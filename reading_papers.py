import os
import sqlite3
import json
import logging
from datetime import datetime
from tqdm import tqdm
from langchain_community.document_loaders import PDFPlumberLoader
from volcenginesdkarkruntime import Ark

# é…ç½®å¢å¼ºæ—¥å¿—ç³»ç»Ÿ
# ä¿®æ”¹æ—¥å¿—é…ç½®éƒ¨åˆ†
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("paper_analysis.log", encoding='utf-8'),  # æ·»åŠ ç¼–ç å‚æ•°
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# å…¶ä»–åŸæœ‰é…ç½®ä¿æŒä¸å˜
os.environ['ARK_API_KEY'] = '89f37545-ebf9-45b4-a537-fb281f2204c9'

client = Ark(
    base_url="https://ark.cn-beijing.volces.com/api/v3",
    api_key=os.environ.get("ARK_API_KEY")
)


def chat_v3(full_text, questions):
    results = []
    for question in questions:
        try:
            sys_template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šæ–‡æ¡£åˆ†æåŠ©æ‰‹ï¼Œè¯·æ ¹æ®ä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ã€‚
            è‹¥æ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜"æ ¹æ®æ–‡æ¡£å†…å®¹æ— æ³•å›ç­”è¯¥é—®é¢˜"ã€‚
            è¯·æ ¹æ®é—®é¢˜è¦æ±‚ç»™å‡ºç®€æ´æˆ–è€…æ˜¯å…·ä½“è¯¦ç»†çš„å›ç­”"""

            user_question = f"""æ–‡æ¡£å†…å®¹ï¼š{full_text[:33000]} 
            é—®é¢˜ï¼š{question}
            è¯·æŒ‰é—®é¢˜è¦æ±‚ç›´æ¥ç»™å‡ºç­”æ¡ˆ"""

            completion = client.chat.completions.create(
                model="ep-20250206224347-2mshv",
                messages=[
                    {"role": "system", "content": sys_template},
                    {"role": "user", "content": user_question},
                ],
            )
            result = completion.choices[0].message.content
            results.append(result)
            logger.info(f"æˆåŠŸå¤„ç†é—®é¢˜: {question[:30]}...")  # æˆªæ–­é•¿é—®é¢˜
        except Exception as e:
            logger.error(f"å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {str(e)}")
            results.append("ERROR: å¤„ç†å¤±è´¥")
    return results





# å…¶ä»–é…ç½®ä¿æŒä¸å˜...

def update_analyse_database(questions, col_names, db_path: str = "papers.db"):
    """æ›´æ–°æ•°æ®åº“å¹¶å®æ—¶ç”ŸæˆJSONè¾“å‡º"""
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    # ç”Ÿæˆå”¯ä¸€ç»“æœæ–‡ä»¶å
    output_file = f"results/analysis_{datetime.now().strftime('%Y%m%d%H%M%S')}.json"
    os.makedirs("results", exist_ok=True)

    all_results = []
    stats = {
        "total": 0,
        "success": 0,
        "failed": 0,
        "current_batch": 0
    }

    try:
        # ... [åŸæœ‰è¡¨ç»“æ„æ£€æŸ¥ä»£ç ä¿æŒä¸å˜] ...
        # æ£€æŸ¥å¹¶æ·»åŠ pdfåˆ—ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        cursor.execute("PRAGMA table_info(papers)")
        columns = [col[1] for col in cursor.fetchall()]

        for col in col_names:
            if col not in columns:
                cursor.execute(f"ALTER TABLE papers ADD COLUMN {col} TEXT")
                conn.commit()

        # è·å–å¾…å¤„ç†è®ºæ–‡
        cursor.execute("""
            SELECT title, pdf, first_submitted_date 
            FROM papers 
            WHERE question_detail IS NULL 
              AND pdf is NOT NULL
              AND abstract LIKE '%personality%'
            ORDER BY first_submitted_date DESC
        """)
        pending_papers = cursor.fetchall()
        stats["total"] = len(pending_papers)

        for idx, (title, pdf, date) in enumerate(tqdm(pending_papers, desc="åˆ†æè®ºæ–‡"), 1):
            paper_data = {
                "meta": {
                    "title": title,
                    "pdf_path": pdf,
                    "submit_date": date,
                    "process_time": datetime.now().isoformat(),
                    "status": "pending"
                },
                "answers": {}
            }

            try:
                # PDFå†…å®¹åŠ è½½
                full_text = load_pdf_content(pdf)
                if not full_text:
                    logger.warning(f"ğŸŸ¡ å†…å®¹ç©ºå€¼è·³è¿‡ | {title}")
                    stats["failed"] += 1
                    continue

                # å¤§æ¨¡å‹åˆ†æ
                results = chat_v3(full_text, questions)

                # æ„å»ºæ•°æ®ç»“æ„
                for col_name, result in zip(col_names, results):
                    paper_data["answers"][col_name] = result

                # æ›´æ–°æ•°æ®åº“
                update_params = [*results, title]
                cursor.execute(
                    f"UPDATE papers SET {', '.join([f'{col}=?' for col in col_names])} WHERE title = ?",
                    update_params
                )
                conn.commit()

                # æ›´æ–°ç»“æœé›†
                paper_data["meta"]["status"] = "success"
                all_results.append(paper_data)
                stats["success"] += 1

                # å®æ—¶å†™å…¥JSON
                with open(output_file, "w", encoding="utf-8") as f:
                    json.dump(all_results, f, ensure_ascii=False, indent=2)

                # è®°å½•è®ºæ–‡æ‘˜è¦æ—¥å¿—
                logger.info(f"\nğŸ”µ è®ºæ–‡åˆ†æå®Œæˆ [{idx}/{stats['total']}]")
                logger.info(f"  æ ‡é¢˜ï¼š{title}")
                logger.info(f"  çŠ¶æ€ï¼šâœ… æˆåŠŸå…¥åº“")
                logger.info(f"  å…³é”®ç»“æœï¼š")
                for q_name in ["question_title", "question_dataset"]:
                    if ans := paper_data["answers"].get(q_name):
                        logger.info(f"  - {q_name}: {ans[:80]}...")

            except Exception as e:
                paper_data["meta"]["status"] = f"failed: {str(e)}"
                all_results.append(paper_data)
                stats["failed"] += 1
                logger.error(f"ğŸ”´ å¤„ç†å¤±è´¥ | {title}\n{str(e)}", exc_info=True)
                conn.rollback()

            # é˜¶æ®µæ€§è¿›åº¦æŠ¥å‘Š
            if idx % 5 == 0 or idx == stats["total"]:
                progress = f"\nğŸ“Š å¤„ç†è¿›åº¦ [{idx}/{stats['total']}]\n"
                progress += f"âœ… æˆåŠŸï¼š{stats['success']} ç¯‡\n"
                progress += f"âŒ å¤±è´¥ï¼š{stats['failed']} ç¯‡\n"
                progress += f"ğŸ“‚ æœ€æ–°ç»“æœæ–‡ä»¶ï¼š{output_file}"
                logger.info(progress)

                # æ›´æ–°å®æ—¶ç»“æœæ–‡ä»¶
                with open(output_file, "w", encoding="utf-8") as f:
                    json.dump(all_results, f, ensure_ascii=False, indent=2)

        # æœ€ç»ˆæ€»ç»“æŠ¥å‘Š
        final_report = f"\nğŸ‰ åˆ†æä»»åŠ¡å®Œæˆï¼\n"
        final_report += f"â¤ å…±å¤„ç†è®ºæ–‡ï¼š{stats['total']} ç¯‡\n"
        final_report += f"â¤ æˆåŠŸåˆ†ææ•°ï¼š{stats['success']} ç¯‡ ({stats['success'] / stats['total']:.1%})\n"
        final_report += f"â¤ å¤±è´¥åˆ†ææ•°ï¼š{stats['failed']} ç¯‡\n"
        final_report += f"â¤ ç»“æœæ–‡ä»¶è·¯å¾„ï¼š{os.path.abspath(output_file)}"
        logger.info(final_report)

    finally:
        conn.close()

def load_pdf_content(file_path):
    """åŠ è½½å¹¶è¿”å›PDFå…¨æ–‡å†…å®¹"""
    try:
        loader = PDFPlumberLoader(file_path)
        documents = loader.load()
        content = "\n".join([doc.page_content for doc in documents])
        logger.info(f"æˆåŠŸåŠ è½½PDF: {file_path} [{len(content)}å­—ç¬¦]")
        return content
    except Exception as e:
        logger.error(f"åŠ è½½PDFå¤±è´¥: {file_path} - {str(e)}")
        return None


if __name__ == "__main__":
    # ç¤ºä¾‹é—®é¢˜
    questions = [
        """
        è®ºæ–‡æ ‡é¢˜å’Œæ ‡é¢˜çš„ä¸­æ–‡ç¿»è¯‘æ˜¯ä»€ä¹ˆï¼Ÿè¯·æŒ‰ä»¥ä¸‹æ ¼å¼åˆ†åˆ«ç›´æ¥ç»™å‡º:
        è‹±æ–‡: [è‹±æ–‡æ ‡é¢˜]
        ä¸­æ–‡: [æ ‡é¢˜ä¸­æ–‡ç¿»è¯‘]
        """
        ,

        """
        è®ºæ–‡æ‘˜è¦å’Œä½ å¯¹æ‘˜è¦çš„ä¸­æ–‡ç¿»è¯‘æ˜¯ä»€ä¹ˆï¼Ÿè¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹æ ¼å¼åˆ†åˆ«å®Œæ•´åœ°ç»™å‡º:
        è‹±æ–‡: [è‹±æ–‡æ‘˜è¦]
        ä¸­æ–‡: [æ‘˜è¦çš„ä¸­æ–‡ç¿»è¯‘]
        """
        ,

        """
        æ–‡æ¡£çš„ä¸»è¦ç ”ç©¶å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿå›ç­”è¿™ä¸ªé—®é¢˜æ˜¯å¯ç›¸å¯¹è¯¦ç»†ä¸€äº›ï¼Œå¹¶ä¾§é‡å…³æ³¨å’Œè€ƒè™‘åˆ†ææ˜¯å¦åŒ…å«æœ‰â€œäººæ ¼ã€æ€§æ ¼(personality)â€çš„éƒ¨åˆ†,å‰ææ˜¯æœ‰å¯¹åº”çš„å†…å®¹,è¯·ä½ æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š
        ä¸»è¦å†…å®¹: [æ–‡æœ¬æè¿°]
        å…³äºäººæ ¼ã€æ€§æ ¼: [æ–‡æœ¬æè¿°]

        è¯·æ³¨æ„ï¼Œå…³äºäººæ ¼ã€æ€§æ ¼çš„å†…å®¹å¦‚æœæ²¡æœ‰ï¼Œåˆ™æ ‡ä¸ºâ€œæ— â€
        """

        ,
        """
        è®ºæ–‡ä½¿ç”¨çš„æ•°æ®é›†æœ‰å“ªäº›ï¼Ÿå¦‚æœæ²¡æœ‰ï¼Œåˆ™å›ç­”â€œæ— â€ï¼Œå¦‚æœæœ‰è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹æ ¼å¼åˆ—å‡ºï¼š
        æ•°æ®é›†: [æ•°æ®é›†åç§°]
        æ•°æ®é›†è§„æ¨¡: [æ•°æ®é›†è§„æ¨¡]
        æ•°æ®é›†å½¢å¼: [æ•°æ®é›†å½¢å¼]
        åœ°å€: [æ•°æ®é›†url]

        å¦‚æœä¸Šè¿°æ ¼å¼å¯¹åº”çš„é¡¹ï¼ˆè§„æ¨¡ã€å½¢å¼ã€åœ°å€ï¼‰æ²¡æœ‰ï¼Œåˆ™æ ‡ä¸ºâ€œæ— â€ï¼Œè¯·æ³¨æ„æ•°æ®é›†åç§°ä¸èƒ½ä¸ºâ€œæ— â€
        """
    ]

    col_names = [
        "question_title",
        "question_abs",
        "question_detail",
        "question_dataset"
    ]

    update_analyse_database(questions, col_names)