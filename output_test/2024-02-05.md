> 本文由 [https://github.com/huiyeruzhou/arxiv_crawler](https://github.com/huiyeruzhou/arxiv_crawler) 自动生成
>
> 领域白名单：cs.AI,cs.CL,cs.LG
> 关键词： LLM, GPT, AI

# 论文全览：2024-02-05

共有199篇相关领域论文, 另有24篇其他

## 人工智能(cs.AI:Artificial Intelligence)

### Understanding the planning of LLM agents: A survey 
[[arxiv](https://arxiv.org/abs/2402.02716)] [[cool](https://papers.cool/arxiv/2402.02716)] [[pdf](https://arxiv.org/pdf/2402.02716)]
> **Authors**: Xu Huang,Weiwen Liu,Xiaolong Chen,Xingmei Wang,Hao Wang,Defu Lian,Yasheng Wang,Ruiming Tang,Enhong Chen
> **First submission**: 2024-02-04
> **First announcement**: 2024-02-05
> **comment**: 9 pages, 2 tables, 2 figures
- **标题**: None
- **领域**: 人工智能,计算语言学,机器学习
- **Abstract**: As Large Language Models (LLMs) have shown significant intelligence, the progress to leverage LLMs as planning modules of autonomous agents has attracted more attention. This survey provides the first systematic view of LLM-based agents planning, covering recent works aiming to improve planning ability. We provide a taxonomy of existing works on LLM-Agent planning, which can be categorized into Task Decomposition, Plan Selection, External Module, Reflection and Memory. Comprehensive analyses are conducted for each direction, and further challenges for the field of research are discussed.

### PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems? 
[[arxiv](https://arxiv.org/abs/2402.02611)] [[cool](https://papers.cool/arxiv/2402.02611)] [[pdf](https://arxiv.org/pdf/2402.02611)]
> **Authors**: Chinmay Mittal,Krishna Kartik,Mausam,Parag Singla
> **First submission**: 2024-02-04
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学,机器学习
- **Abstract**: Recent works show that the largest of the large language models (LLMs) can solve many simple reasoning tasks expressed in natural language, without any/much supervision. But, can they also solve challenging first-order combinatorial reasoning problems, such as graph coloring, knapsack and cryptarithmetic? To answer this question, we present PuzzleBench, a dataset of 31 such challenging problems along with a few solved instances for each problem. These problems are all first order, i.e., they can be instantiated with problem instances of varying sizes, and most of them are NP-hard, requiring several reasoning steps to reach the solution. We first observe that LLMs, even when aided by symbolic solvers, perform rather poorly on our dataset. In response, we propose a new approach, Puzzle-LM, which combines LLMs with both symbolic solvers and program interpreters, along with feedback from solved examples, to achieve huge performance gains. Our extensive experimentation and analyses offer new insights into the reasoning abilities and limitations of present-day LLMs.

### DeLLMa: Decision Making Under Uncertainty with Large Language Models 
[[arxiv](https://arxiv.org/abs/2402.02392)] [[cool](https://papers.cool/arxiv/2402.02392)] [[pdf](https://arxiv.org/pdf/2402.02392)]
> **Authors**: Ollie Liu,Deqing Fu,Dani Yogatama,Willie Neiswanger
> **First submission**: 2024-02-04
> **First announcement**: 2024-02-05
> **comment**: 37 pages, 24 figures
- **标题**: None
- **领域**: 人工智能,计算语言学,机器学习
- **Abstract**: The potential of large language models (LLMs) as decision support tools is increasingly being explored in fields such as business, engineering, and medicine, which often face challenging tasks of decision-making under uncertainty. In this paper, we show that directly prompting LLMs on these types of decision-making problems can yield poor results, especially as the problem complexity increases. To aid in these tasks, we propose DeLLMa (Decision-making Large Language Model assistant), a framework designed to enhance decision-making accuracy in uncertain environments. DeLLMa involves a multi-step reasoning procedure that integrates recent best practices in scaling inference-time reasoning, drawing upon principles from decision theory and utility theory, to provide an accurate and human-auditable decision-making process. We validate our procedure on multiple realistic decision-making environments, demonstrating that DeLLMa can consistently enhance the decision-making performance of leading language models, and achieve up to a 40% increase in accuracy over competing methods. Additionally, we show how performance improves when scaling compute at test time, and carry out human evaluations to benchmark components of DeLLMa.

### Enhance Reasoning for Large Language Models in the Game Werewolf 
[[arxiv](https://arxiv.org/abs/2402.02330)] [[cool](https://papers.cool/arxiv/2402.02330)] [[pdf](https://arxiv.org/pdf/2402.02330)]
> **Authors**: Shuang Wu,Liwen Zhu,Tao Yang,Shiwei Xu,Qiang Fu,Yang Wei,Haobo Fu
> **First submission**: 2024-02-03
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: This paper presents an innovative framework that integrates Large Language Models (LLMs) with an external Thinker module to enhance the reasoning capabilities of LLM-based agents. Unlike augmenting LLMs with prompt engineering, Thinker directly harnesses knowledge from databases and employs various optimization techniques. The framework forms a reasoning hierarchy where LLMs handle intuitive System-1 tasks such as natural language processing, while the Thinker focuses on cognitive System-2 tasks that require complex logical analysis and domain-specific knowledge. Our framework is presented using a 9-player Werewolf game that demands dual-system reasoning. We introduce a communication protocol between LLMs and the Thinker, and train the Thinker using data from 18800 human sessions and reinforcement learning. Experiments demonstrate the framework's effectiveness in deductive reasoning, speech generation, and online game evaluation. Additionally, we fine-tune a 6B LLM to surpass GPT4 when integrated with the Thinker. This paper also contributes the largest dataset for social deduction games to date.

### Hierarchical Structure Enhances the Convergence and Generalizability of Linear Molecular Representation 
[[arxiv](https://arxiv.org/abs/2402.02164)] [[cool](https://papers.cool/arxiv/2402.02164)] [[pdf](https://arxiv.org/pdf/2402.02164)]
> **Authors**: Juan-Ni Wu,Tong Wang,Li-Juan Tang,Hai-Long Wu,Ru-Qin Yu
> **First submission**: 2024-02-03
> **First announcement**: 2024-02-05
> **comment**: 26pages, 6 figures
- **标题**: None
- **领域**: 人工智能,生物分子
- **Abstract**: Language models demonstrate fundamental abilities in syntax, semantics, and reasoning, though their performance often depends significantly on the inputs they process. This study introduces TSIS (Simplified TSID) and its variants:TSISD (TSIS with Depth-First Search), TSISO (TSIS in Order), and TSISR (TSIS in Random), as integral components of the t-SMILES framework. These additions complete the framework's design, providing diverse approaches to molecular representation. Through comprehensive analysis and experiments employing deep generative models, including GPT, diffusion models, and reinforcement learning, the findings reveal that the hierarchical structure of t-SMILES is more straightforward to parse than initially anticipated. Furthermore, t-SMILES consistently outperforms other linear representations such as SMILES, SELFIES, and SAFE, demonstrating superior convergence speed and enhanced generalization capabilities.

### Emergency Computing: An Adaptive Collaborative Inference Method Based on Hierarchical Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2402.02146)] [[cool](https://papers.cool/arxiv/2402.02146)] [[pdf](https://arxiv.org/pdf/2402.02146)]
> **Authors**: Weiqi Fu,Lianming Xu,Xin Wu,Li Wang,Aiguo Fei
> **First submission**: 2024-02-03
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习,网络和互联网架构,信号处理
- **Abstract**: In achieving effective emergency response, the timely acquisition of environmental information, seamless command data transmission, and prompt decision-making are crucial. This necessitates the establishment of a resilient emergency communication dedicated network, capable of providing communication and sensing services even in the absence of basic infrastructure. In this paper, we propose an Emergency Network with Sensing, Communication, Computation, Caching, and Intelligence (E-SC3I). The framework incorporates mechanisms for emergency computing, caching, integrated communication and sensing, and intelligence empowerment. E-SC3I ensures rapid access to a large user base, reliable data transmission over unstable links, and dynamic network deployment in a changing environment. However, these advantages come at the cost of significant computation overhead. Therefore, we specifically concentrate on emergency computing and propose an adaptive collaborative inference method (ACIM) based on hierarchical reinforcement learning. Experimental results demonstrate our method's ability to achieve rapid inference of AI models with constrained computational and communication resources.

### Affordable Generative Agents 
[[arxiv](https://arxiv.org/abs/2402.02053)] [[cool](https://papers.cool/arxiv/2402.02053)] [[pdf](https://arxiv.org/pdf/2402.02053)]
> **Authors**: Yangbin Yu,Qin Zhang,Junyou Li,Qiang Fu,Deheng Ye
> **First submission**: 2024-02-03
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,人机交互
- **Abstract**: The emergence of large language models (LLMs) has significantly advanced the simulation of believable interactive agents. However, the substantial cost on maintaining the prolonged agent interactions poses challenge over the deployment of believable LLM-based agents. Therefore, in this paper, we develop Affordable Generative Agents (AGA), a framework for enabling the generation of believable and low-cost interactions on both agent-environment and inter-agents levels. Specifically, for agent-environment interactions, we substitute repetitive LLM inferences with learned policies; while for inter-agent interactions, we model the social relationships between agents and compress auxiliary dialogue information. Extensive experiments on multiple environments show the effectiveness and efficiency of our proposed framework. Also, we delve into the mechanisms of emergent believable behaviors lying in LLM agents, demonstrating that agents can only generate finite behaviors in fixed environments, based upon which, we understand ways to facilitate emergent interaction behaviors. Our code is publicly available at: https://github.com/AffordableGenerativeAgents/Affordable-Generative-Agents.

### The Role of Foundation Models in Neuro-Symbolic Learning and Reasoning 
[[arxiv](https://arxiv.org/abs/2402.01889)] [[cool](https://papers.cool/arxiv/2402.01889)] [[pdf](https://arxiv.org/pdf/2402.01889)]
> **Authors**: Daniel Cunnington,Mark Law,Jorge Lobo,Alessandra Russo
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: Pre-print
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: Neuro-Symbolic AI (NeSy) holds promise to ensure the safe deployment of AI systems, as interpretable symbolic techniques provide formal behaviour guarantees. The challenge is how to effectively integrate neural and symbolic computation, to enable learning and reasoning from raw data. Existing pipelines that train the neural and symbolic components sequentially require extensive labelling, whereas end-to-end approaches are limited in terms of scalability, due to the combinatorial explosion in the symbol grounding problem. In this paper, we leverage the implicit knowledge within foundation models to enhance the performance in NeSy tasks, whilst reducing the amount of data labelling and manual engineering. We introduce a new architecture, called NeSyGPT, which fine-tunes a vision-language foundation model to extract symbolic features from raw data, before learning a highly expressive answer set program to solve a downstream task. Our comprehensive evaluation demonstrates that NeSyGPT has superior accuracy over various baselines, and can scale to complex NeSy tasks. Finally, we highlight the effective use of a large language model to generate the programmatic interface between the neural and symbolic components, significantly reducing the amount of manual engineering required.

### LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks 
[[arxiv](https://arxiv.org/abs/2402.01817)] [[cool](https://papers.cool/arxiv/2402.01817)] [[pdf](https://arxiv.org/pdf/2402.01817)]
> **Authors**: Subbarao Kambhampati,Karthik Valmeekam,Lin Guan,Mudit Verma,Kaya Stechly,Siddhant Bhambri,Lucas Saldyt,Anil Murthy
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: ef:Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: There is considerable confusion about the role of Large Language Models (LLMs) in planning and reasoning tasks. On one side are over-optimistic claims that LLMs can indeed do these tasks with just the right prompting or self-verification strategies. On the other side are perhaps over-pessimistic claims that all that LLMs are good for in planning/reasoning tasks are as mere translators of the problem specification from one syntactic format to another, and ship the problem off to external symbolic solvers. In this position paper, we take the view that both these extremes are misguided. We argue that auto-regressive LLMs cannot, by themselves, do planning or self-verification (which is after all a form of reasoning), and shed some light on the reasons for misunderstandings in the literature. We will also argue that LLMs should be viewed as universal approximate knowledge sources that have much more meaningful roles to play in planning/reasoning tasks beyond simple front-end/back-end format translators. We present a vision of {\bf LLM-Modulo Frameworks} that combine the strengths of LLMs with external model-based verifiers in a tighter bi-directional interaction regime. We will show how the models driving the external verifiers themselves can be acquired with the help of LLMs. We will also argue that rather than simply pipelining LLMs and symbolic components, this LLM-Modulo Framework provides a better neuro-symbolic approach that offers tighter integration between LLMs and symbolic components, and allows extending the scope of model-based planning/reasoning regimes towards more flexible knowledge, problem and preference specifications.

### COA-GPT: Generative Pre-trained Transformers for Accelerated Course of Action Development in Military Operations 
[[arxiv](https://arxiv.org/abs/2402.01786)] [[cool](https://papers.cool/arxiv/2402.01786)] [[pdf](https://arxiv.org/pdf/2402.01786)]
> **Authors**: Vinicius G. Goecks,Nicholas Waytowich
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-05
> **comment**: Accepted at the NATO Science and Technology Organization Symposium (ICMCIS) organized by the Information Systems Technology (IST) Panel, IST-205-RSY - the ICMCIS, held in Koblenz, Germany, 23-24 April 2024
- **标题**: None
- **领域**: 人工智能,计算语言学,人机交互,机器学习
- **Abstract**: The development of Courses of Action (COAs) in military operations is traditionally a time-consuming and intricate process. Addressing this challenge, this study introduces COA-GPT, a novel algorithm employing Large Language Models (LLMs) for rapid and efficient generation of valid COAs. COA-GPT incorporates military doctrine and domain expertise to LLMs through in-context learning, allowing commanders to input mission information - in both text and image formats - and receive strategically aligned COAs for review and approval. Uniquely, COA-GPT not only accelerates COA development, producing initial COAs within seconds, but also facilitates real-time refinement based on commander feedback. This work evaluates COA-GPT in a military-relevant scenario within a militarized version of the StarCraft II game, comparing its performance against state-of-the-art reinforcement learning algorithms. Our results demonstrate COA-GPT's superiority in generating strategically sound COAs more swiftly, with added benefits of enhanced adaptability and alignment with commander intentions. COA-GPT's capability to rapidly adapt and update COAs during missions presents a transformative potential for military planning, particularly in addressing planning discrepancies and capitalizing on emergent windows of opportunities.

### Foundation Model Sherpas: Guiding Foundation Models through Knowledge and Reasoning 
[[arxiv](https://arxiv.org/abs/2402.01602)] [[cool](https://papers.cool/arxiv/2402.01602)] [[pdf](https://arxiv.org/pdf/2402.01602)]
> **Authors**: Debarun Bhattacharjya,Junkyu Lee,Don Joven Agravante,Balaji Ganesan,Radu Marinescu
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: 9 pages
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Foundation models (FMs) such as large language models have revolutionized the field of AI by showing remarkable performance in various tasks. However, they exhibit numerous limitations that prevent their broader adoption in many real-world systems, which often require a higher bar for trustworthiness and usability. Since FMs are trained using loss functions aimed at reconstructing the training corpus in a self-supervised manner, there is no guarantee that the model's output aligns with users' preferences for a specific task at hand. In this survey paper, we propose a conceptual framework that encapsulates different modes by which agents could interact with FMs and guide them suitably for a set of tasks, particularly through knowledge augmentation and reasoning. Our framework elucidates agent role categories such as updating the underlying FM, assisting with prompting the FM, and evaluating the FM output. We also categorize several state-of-the-art approaches into agent interaction protocols, highlighting the nature and extent of involvement of the various agent roles. The proposed framework provides guidance for future directions to further realize the power of FMs in practical AI systems.

### Developing and Evaluating a Design Method for Positive Artificial Intelligence 
[[arxiv](https://arxiv.org/abs/2402.01499)] [[cool](https://papers.cool/arxiv/2402.01499)] [[pdf](https://arxiv.org/pdf/2402.01499)]
> **Authors**: Willem van der Maden,Derek Lomas,Paul Hekkert
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: :68T01ACM Class:H.5.2; I.2.9; J.3
- **标题**: None
- **领域**: 人工智能
- **Abstract**: As artificial intelligence (AI) continues advancing, ensuring positive societal impacts becomes critical, especially as AI systems become increasingly ubiquitous in various aspects of life. However, developing "AI for good" poses substantial challenges around aligning systems with complex human values. Presently, we lack mature methods for addressing these challenges. This article presents and evaluates the Positive AI design method aimed at addressing this gap. The method provides a human-centered process to translate wellbeing aspirations into concrete practices. First, we explain the method's four key steps: contextualizing, operationalizing, optimizing, and implementing wellbeing supported by continuous measurement for feedback cycles. We then present a multiple case study where novice designers applied the method, revealing strengths and weaknesses related to efficacy and usability. Next, an expert evaluation study assessed the quality of the resulting concepts, rating them moderately high for feasibility, desirability, and plausibility of achieving intended wellbeing benefits. Together, these studies provide preliminary validation of the method's ability to improve AI design, while surfacing areas needing refinement like developing support for complex steps. Proposed adaptations such as examples and evaluation heuristics could address weaknesses. Further research should examine sustained application over multiple projects. This human-centered approach shows promise for realizing the vision of 'AI for Wellbeing' that does not just avoid harm, but actively benefits humanity.

### Towards the New XAI: A Hypothesis-Driven Approach to Decision Support Using Evidence 
[[arxiv](https://arxiv.org/abs/2402.01292)] [[cool](https://papers.cool/arxiv/2402.01292)] [[pdf](https://arxiv.org/pdf/2402.01292)]
> **Authors**: Thao Le,Tim Miller,Liz Sonenberg,Ronal Singh
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: ECAI 2024 Main Track. The full paper version, including the supplementary material
- **标题**: None
- **领域**: 人工智能,人机交互
- **Abstract**: Prior research on AI-assisted human decision-making has explored several different explainable AI (XAI) approaches. A recent paper has proposed a paradigm shift calling for hypothesis-driven XAI through a conceptual framework called evaluative AI that gives people evidence that supports or refutes hypotheses without necessarily giving a decision-aid recommendation. In this paper, we describe and evaluate an approach for hypothesis-driven XAI based on the Weight of Evidence (WoE) framework, which generates both positive and negative evidence for a given hypothesis. Through human behavioural experiments, we show that our hypothesis-driven approach increases decision accuracy and reduces reliance compared to a recommendation-driven approach and an AI-explanation-only baseline, but with a small increase in under-reliance compared to the recommendation-driven approach. Further, we show that participants used our hypothesis-driven approach in a materially different way to the two baselines.

## 计算语言学(cs.CL:Computation and Language)

### Large Language Models are Geographically Biased 
[[arxiv](https://arxiv.org/abs/2402.02680)] [[cool](https://papers.cool/arxiv/2402.02680)] [[pdf](https://arxiv.org/pdf/2402.02680)]
> **Authors**: Rohin Manvi,Samar Khanna,Marshall Burke,David Lobell,Stefano Ermon
> **First submission**: 2024-02-04
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机与社会,机器学习
- **Abstract**: Large Language Models (LLMs) inherently carry the biases contained in their training corpora, which can lead to the perpetuation of societal harm. As the impact of these foundation models grows, understanding and evaluating their biases becomes crucial to achieving fairness and accuracy. We propose to study what LLMs know about the world we live in through the lens of geography. This approach is particularly powerful as there is ground truth for the numerous aspects of human life that are meaningfully projected onto geographic space such as culture, race, language, politics, and religion. We show various problematic geographic biases, which we define as systemic errors in geospatial predictions. Initially, we demonstrate that LLMs are capable of making accurate zero-shot geospatial predictions in the form of ratings that show strong monotonic correlation with ground truth (Spearman's $ρ$ of up to 0.89). We then show that LLMs exhibit common biases across a range of objective and subjective topics. In particular, LLMs are clearly biased against locations with lower socioeconomic conditions (e.g. most of Africa) on a variety of sensitive subjective topics such as attractiveness, morality, and intelligence (Spearman's $ρ$ of up to 0.70). Finally, we introduce a bias score to quantify this and find that there is significant variation in the magnitude of bias across existing LLMs. Code is available on the project website: https://rohinmanvi.github.io/GeoLLM

### RACER: An LLM-powered Methodology for Scalable Analysis of Semi-structured Mental Health Interviews 
[[arxiv](https://arxiv.org/abs/2402.02656)] [[cool](https://papers.cool/arxiv/2402.02656)] [[pdf](https://arxiv.org/pdf/2402.02656)]
> **Authors**: Satpreet Harcharan Singh,Kevin Jiang,Kanchan Bhasin,Ashutosh Sabharwal,Nidal Moukaddam,Ankit B Patel
> **First submission**: 2024-02-04
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,定量方法
- **Abstract**: Semi-structured interviews (SSIs) are a commonly employed data-collection method in healthcare research, offering in-depth qualitative insights into subject experiences. Despite their value, the manual analysis of SSIs is notoriously time-consuming and labor-intensive, in part due to the difficulty of extracting and categorizing emotional responses, and challenges in scaling human evaluation for large populations. In this study, we develop RACER, a Large Language Model (LLM) based expert-guided automated pipeline that efficiently converts raw interview transcripts into insightful domain-relevant themes and sub-themes. We used RACER to analyze SSIs conducted with 93 healthcare professionals and trainees to assess the broad personal and professional mental health impacts of the COVID-19 crisis. RACER achieves moderately high agreement with two human evaluators (72%), which approaches the human inter-rater agreement (77%). Interestingly, LLMs and humans struggle with similar content involving nuanced emotional, ambivalent/dialectical, and psychological statements. Our study highlights the opportunities and challenges in using LLMs to improve research efficiency and opens new avenues for scalable analysis of SSIs in healthcare research.

### Recursive Chain-of-Feedback Prevents Performance Degradation from Redundant Prompting 
[[arxiv](https://arxiv.org/abs/2402.02648)] [[cool](https://papers.cool/arxiv/2402.02648)] [[pdf](https://arxiv.org/pdf/2402.02648)]
> **Authors**: Jinwoo Ahn,Kyuseung Shin
> **First submission**: 2024-02-04
> **First announcement**: 2024-02-05
> **comment**: Still Ongoing Work; 8 Pages; 2 Figures
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large Language Models (LLMs) frequently struggle with complex reasoning tasks, failing to construct logically sound steps towards the solution. In response to this behavior, users often try prompting the LLMs repeatedly in hopes of reaching a better response. This paper studies such repetitive behavior and its effect by defining a novel setting, Chain-of-Feedback (CoF). The setting takes questions that require multi-step reasoning as an input. Upon response, we repetitively prompt meaningless feedback (e.g. 'make another attempt') requesting additional trials. Surprisingly, our preliminary results show that repeated meaningless feedback gradually decreases the quality of the responses, eventually leading to a larger deviation from the intended outcome. To alleviate these troubles, we propose a novel method, Recursive Chain-of-Feedback (R-CoF). Following the logic of recursion in computer science, R-CoF recursively revises the initially incorrect response by breaking down each incorrect reasoning step into smaller individual problems. Our preliminary results show that majority of questions that LLMs fail to respond correctly can be answered using R-CoF without any sample data outlining the logical process.

### Can Large Language Models Learn Independent Causal Mechanisms? 
[[arxiv](https://arxiv.org/abs/2402.02636)] [[cool](https://papers.cool/arxiv/2402.02636)] [[pdf](https://arxiv.org/pdf/2402.02636)]
> **Authors**: Gaël Gendron,Bao Trung Nguyen,Alex Yuxuan Peng,Michael Witbrock,Gillian Dobbie
> **First submission**: 2024-02-04
> **First announcement**: 2024-02-05
> **comment**: 20 pages, 7 pages for the main paper and 13 pages for references and appendices, 17 figures
- **标题**: None
- **领域**: 计算语言学,人工智能,信息论,机器学习
- **Abstract**: Despite impressive performance on language modelling and complex reasoning tasks, Large Language Models (LLMs) fall short on the same tasks in uncommon settings or with distribution shifts, exhibiting a lack of generalisation ability. By contrast, systems such as causal models, that learn abstract variables and causal relationships, can demonstrate increased robustness against changes in the distribution. One reason for this success is the existence and use of Independent Causal Mechanisms (ICMs) representing high-level concepts that only sparsely interact. In this work, we apply two concepts from causality to learn ICMs within LLMs. We develop a new LLM architecture composed of multiple sparsely interacting language modelling modules. We show that such causal constraints can improve out-of-distribution performance on abstract and causal reasoning tasks. We also investigate the level of independence and domain specialisation and show that LLMs rely on pre-trained partially domain-invariant mechanisms resilient to fine-tuning.

### A Truly Joint Neural Architecture for Segmentation and Parsing 
[[arxiv](https://arxiv.org/abs/2402.02564)] [[cool](https://papers.cool/arxiv/2402.02564)] [[pdf](https://arxiv.org/pdf/2402.02564)]
> **Authors**: Danit Yshaayahu Levi,Reut Tsarfaty
> **First submission**: 2024-02-04
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Contemporary multilingual dependency parsers can parse a diverse set of languages, but for Morphologically Rich Languages (MRLs), performance is attested to be lower than other languages. The key challenge is that, due to high morphological complexity and ambiguity of the space-delimited input tokens, the linguistic units that act as nodes in the tree are not known in advance. Pre-neural dependency parsers for MRLs subscribed to the joint morpho-syntactic hypothesis, stating that morphological segmentation and syntactic parsing should be solved jointly, rather than as a pipeline where segmentation precedes parsing. However, neural state-of-the-art parsers to date use a strict pipeline. In this paper we introduce a joint neural architecture where a lattice-based representation preserving all morphological ambiguity of the input is provided to an arc-factored model, which then solves the morphological segmentation and syntactic parsing tasks at once. Our experiments on Hebrew, a rich and highly ambiguous MRL, demonstrate state-of-the-art performance on parsing, tagging and segmentation of the Hebrew section of UD, using a single model. This proposed architecture is LLM-based and language agnostic, providing a solid foundation for MRLs to obtain further performance improvements and bridge the gap with other languages.

### Synergy-of-Thoughts: Eliciting Efficient Reasoning in Hybrid Language Models 
[[arxiv](https://arxiv.org/abs/2402.02563)] [[cool](https://papers.cool/arxiv/2402.02563)] [[pdf](https://arxiv.org/pdf/2402.02563)]
> **Authors**: Yu Shang,Yu Li,Fengli Xu,Yong Li
> **First submission**: 2024-02-04
> **First announcement**: 2024-02-05
> **comment**: 19 pages, 16 figures, 12 tables
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Large language models (LLMs) have shown impressive emergent abilities in a wide range of tasks, but the associated expensive API cost greatly limits the real application. Previous works like chain-of-thought (CoT) and tree-of-thoughts (ToT) have predominately focused on enhancing accuracy, but overlook the rapidly increasing API cost, which could be particularly problematic for open-ended real-world tasks with huge solution spaces. Motivated by the dual process theory of human cognition, we propose "Synergy of Thoughts"(SoT) to unleash the synergistic potential of hybrid LLMs with different scales for efficient reasoning. By default, SoT uses smaller-scale language models to generate multiple low-cost intuitive thoughts, which resembles the parallel intuitions produced by System 1. We then design a confidence evaluator where the intuitive thoughts are cross-evaluated and introduce a controllable threshold mechanism to decide their mutual conflict. If these intuitive thoughts exhibit conflicts, SoT will invoke the reflective reasoning of scaled-up language models to emulate the intervention of System 2, which will override the intuitive thoughts and rectify the reasoning results. This framework is model-agnostic and training-free, which can be flexibly implemented with various off-the-shelf LLMs. Experiments on six representative reasoning tasks show that SoT substantially reduces the API cost by 38.3%-75.1%, and simultaneously achieves state-of-the-art reasoning accuracy and solution diversity. Notably, the average token cost reduction on open-ended tasks reaches up to 69.1%.

### Enhancing Robustness in Biomedical NLI Models: A Probing Approach for Clinical Trials 
[[arxiv](https://arxiv.org/abs/2402.02558)] [[cool](https://papers.cool/arxiv/2402.02558)] [[pdf](https://arxiv.org/pdf/2402.02558)]
> **Authors**: Ata Mustafa
> **First submission**: 2024-02-04
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Large Language Models have revolutionized various fields and industries, such as Conversational AI, Content Generation, Information Retrieval, Business Intelligence, and Medical, to name a few. One major application in the field of medical is to analyze and investigate clinical trials for entailment tasks.However, It has been observed that Large Language Models are susceptible to shortcut learning, factual inconsistency, and performance degradation with little variation in context. Adversarial and robust testing is performed to ensure the integrity of models output. But, ambiguity still persists. In order to ensure the integrity of the reasoning performed and investigate the model has correct syntactic and semantic understanding probing is used. Here, I used mnestic probing to investigate the Sci-five model, trained on clinical trial. I investigated the model for feature learnt with respect to natural logic. To achieve the target, I trained task specific probes. Used these probes to investigate the final layers of trained model. Then, fine tuned the trained model using iterative null projection. The results shows that model accuracy improved. During experimentation, I observed that size of the probe has affect on the fine tuning process.

### Are Large Language Models Table-based Fact-Checkers? 
[[arxiv](https://arxiv.org/abs/2402.02549)] [[cool](https://papers.cool/arxiv/2402.02549)] [[pdf](https://arxiv.org/pdf/2402.02549)]
> **Authors**: Hanwen Zhang,Qingyi Si,Peng Fu,Zheng Lin,Weiping Wang
> **First submission**: 2024-02-04
> **First announcement**: 2024-02-05
> **comment**: CSCWD 2024
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Table-based Fact Verification (TFV) aims to extract the entailment relation between statements and structured tables. Existing TFV methods based on small-scaled models suffer from insufficient labeled data and weak zero-shot ability. Recently, the appearance of Large Language Models (LLMs) has gained lots of attraction in research fields. They have shown powerful zero-shot and in-context learning abilities on several NLP tasks, but their potential on TFV is still unknown. In this work, we implement a preliminary study about whether LLMs are table-based fact-checkers. In detail, we design diverse prompts to explore how the in-context learning can help LLMs in TFV, i.e., zero-shot and few-shot TFV capability. Besides, we carefully design and construct TFV instructions to study the performance gain brought by the instruction tuning of LLMs. Experimental results demonstrate that LLMs can achieve acceptable results on zero-shot and few-shot TFV with prompt engineering, while instruction-tuning can stimulate the TFV capability significantly. We also make some valuable findings about the format of zero-shot prompts and the number of in-context examples. Finally, we analyze some possible directions to promote the accuracy of TFV via LLMs, which is beneficial to further research of table reasoning.

### "What's my model inside of?": Exploring the role of environments for grounded natural language understanding 
[[arxiv](https://arxiv.org/abs/2402.02548)] [[cool](https://papers.cool/arxiv/2402.02548)] [[pdf](https://arxiv.org/pdf/2402.02548)]
> **Authors**: Ronen Tamari
> **First submission**: 2024-02-04
> **First announcement**: 2024-02-05
> **comment**: PhD Thesis
- **标题**: None
- **领域**: 计算语言学,人工智能,社交和信息网络
- **Abstract**: In contrast to classical cognitive science which studied brains in isolation, ecological approaches focused on the role of the body and environment in shaping cognition. Similarly, in this thesis we adopt an ecological approach to grounded natural language understanding (NLU) research. Grounded language understanding studies language understanding systems situated in the context of events, actions and precepts in naturalistic/simulated virtual environments. Where classic research tends to focus on designing new models and optimization methods while treating environments as given, we explore the potential of environment design for improving data collection and model development. We developed novel training and annotation approaches for procedural text understanding based on text-based game environments. We also drew upon embodied cognitive linguistics literature to propose a roadmap for grounded NLP research, and to inform the development of a new benchmark for measuring the progress of large language models on challenging commonsense reasoning tasks. We leveraged the richer supervision provided by text-based game environments to develop Breakpoint Transformers, a novel approach to modeling intermediate semantic information in long narrative or procedural texts. Finally, we integrated theories on the role of environments in collective human intelligence to propose a design for AI-augmented "social thinking environments" for knowledge workers like scientists.

### Knowledge Generation for Zero-shot Knowledge-based VQA 
[[arxiv](https://arxiv.org/abs/2402.02541)] [[cool](https://papers.cool/arxiv/2402.02541)] [[pdf](https://arxiv.org/pdf/2402.02541)]
> **Authors**: Rui Cao,Jing Jiang
> **First submission**: 2024-02-04
> **First announcement**: 2024-02-05
> **comment**: accepted as Findings in EACL 2023
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别
- **Abstract**: Previous solutions to knowledge-based visual question answering~(K-VQA) retrieve knowledge from external knowledge bases and use supervised learning to train the K-VQA model. Recently pre-trained LLMs have been used as both a knowledge source and a zero-shot QA model for K-VQA and demonstrated promising results. However, these recent methods do not explicitly show the knowledge needed to answer the questions and thus lack interpretability. Inspired by recent work on knowledge generation from LLMs for text-based QA, in this work we propose and test a similar knowledge-generation-based K-VQA method, which first generates knowledge from an LLM and then incorporates the generated knowledge for K-VQA in a zero-shot manner. We evaluate our method on two K-VQA benchmarks and found that our method performs better than previous zero-shot K-VQA methods and our generated knowledge is generally relevant and helpful.

### Factuality of Large Language Models: A Survey 
[[arxiv](https://arxiv.org/abs/2402.02420)] [[cool](https://papers.cool/arxiv/2402.02420)] [[pdf](https://arxiv.org/pdf/2402.02420)]
> **Authors**: Yuxia Wang,Minghan Wang,Muhammad Arslan Manzoor,Fei Liu,Georgi Georgiev,Rocktim Jyoti Das,Preslav Nakov
> **First submission**: 2024-02-04
> **First announcement**: 2024-02-05
> **comment**: 11 pages, 1 figure and 2 tables
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large language models (LLMs), especially when instruction-tuned for chat, have become part of our daily lives, freeing people from the process of searching, extracting, and integrating information from multiple sources by offering a straightforward answer to a variety of questions in a single place. Unfortunately, in many cases, LLM responses are factually incorrect, which limits their applicability in real-world scenarios. As a result, research on evaluating and improving the factuality of LLMs has attracted a lot of attention recently. In this survey, we critically analyze existing work with the aim to identify the major challenges and their associated causes, pointing out to potential solutions for improving the factuality of LLMs, and analyzing the obstacles to automated factuality evaluation for open-ended text generation. We further offer an outlook on where future research should go.

### Aligner: Efficient Alignment by Learning to Correct 
[[arxiv](https://arxiv.org/abs/2402.02416)] [[cool](https://papers.cool/arxiv/2402.02416)] [[pdf](https://arxiv.org/pdf/2402.02416)]
> **Authors**: Jiaming Ji,Boyuan Chen,Hantao Lou,Donghai Hong,Borong Zhang,Xuehai Pan,Juntao Dai,Tianyi Qiu,Yaodong Yang
> **First submission**: 2024-02-04
> **First announcement**: 2024-02-05
> **comment**: Accepted by NeurIPS 2024 Oral Presentation
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: With the rapid development of large language models (LLMs) and ever-evolving practical requirements, finding an efficient and effective alignment method has never been more critical. However, the tension between the complexity of current alignment methods and the need for rapid iteration in deployment scenarios necessitates the development of a model-agnostic alignment approach that can operate under these constraints. In this paper, we introduce Aligner, a novel and simple alignment paradigm that learns the correctional residuals between preferred and dispreferred answers using a small model. Designed as a model-agnostic, plug-and-play module, Aligner can be directly applied to various open-source and API-based models with only one-off training, making it suitable for rapid iteration. Notably, Aligner can be applied to any powerful, large-scale upstream models. Moreover, it can even iteratively bootstrap the upstream models using corrected responses as synthetic human preference data, breaking through the model's performance ceiling. Our experiments demonstrate performance improvements by deploying the same Aligner model across 11 different LLMs, evaluated on the 3H dimensions (helpfulness, harmlessness, and honesty). Specifically, Aligner-7B has achieved an average improvement of 68.9% in helpfulness and 23.8% in harmlessness across the tested LLMs while also effectively reducing hallucination. In the Alpaca-Eval leaderboard, stacking Aligner-2B on GPT-4 Turbo improved its LC Win Rate from 55.0% to 58.3%, surpassing GPT-4 Omni's 57.5% Win Rate (community report).

### GLaPE: Gold Label-agnostic Prompt Evaluation and Optimization for Large Language Model 
[[arxiv](https://arxiv.org/abs/2402.02408)] [[cool](https://papers.cool/arxiv/2402.02408)] [[pdf](https://arxiv.org/pdf/2402.02408)]
> **Authors**: Xuanchang Zhang,Zhuosheng Zhang,Hai Zhao
> **First submission**: 2024-02-04
> **First announcement**: 2024-02-05
> **comment**: EMNLP 2024
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Despite the rapid progress of large language models (LLMs), their task performance remains sensitive to prompt design. Recent studies have explored leveraging the LLM itself as an optimizer to identify optimal prompts that maximize task accuracy. However, when evaluating prompts, such approaches heavily rely on elusive manually annotated gold labels to calculate task accuracy for each candidate prompt, which hinders the widespread implementation and generality. To overcome the limitation, this work proposes a gold label-agnostic prompt evaluation (GLaPE) to alleviate dependence on gold labels. Motivated by the observed correlation between self-consistency and the accuracy of the answer, we adopt self-consistency as the initial evaluation score. Subsequently, we refine the scores of prompts producing identical answers to be mutually consistent. Experimental results show that GLaPE provides reliable evaluations uniform with accuracy, even in the absence of gold labels. Moreover, on six popular reasoning tasks, our GLaPE-based prompt optimization yields effective prompts comparable to accuracy-based ones. The code is publicly available at https://github.com/thunderous77/GLaPE.

### KICGPT: Large Language Model with Knowledge in Context for Knowledge Graph Completion 
[[arxiv](https://arxiv.org/abs/2402.02389)] [[cool](https://papers.cool/arxiv/2402.02389)] [[pdf](https://arxiv.org/pdf/2402.02389)]
> **Authors**: Yanbin Wei,Qiushi Huang,James T. Kwok,Yu Zhang
> **First submission**: 2024-02-04
> **First announcement**: 2024-02-05
> **comment**: Accepted to EMNLP 2023 Findings
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Knowledge Graph Completion (KGC) is crucial for addressing knowledge graph incompleteness and supporting downstream applications. Many models have been proposed for KGC. They can be categorized into two main classes: triple-based and text-based approaches. Triple-based methods struggle with long-tail entities due to limited structural information and imbalanced entity distributions. Text-based methods alleviate this issue but require costly training for language models and specific finetuning for knowledge graphs, which limits their efficiency. To alleviate these limitations, in this paper, we propose KICGPT, a framework that integrates a large language model (LLM) and a triple-based KGC retriever. It alleviates the long-tail problem without incurring additional training overhead. KICGPT uses an in-context learning strategy called Knowledge Prompt, which encodes structural knowledge into demonstrations to guide the LLM. Empirical results on benchmark datasets demonstrate the effectiveness of KICGPT with smaller training overhead and no finetuning.

### Solution-oriented Agent-based Models Generation with Verifier-assisted Iterative In-context Learning 
[[arxiv](https://arxiv.org/abs/2402.02388)] [[cool](https://papers.cool/arxiv/2402.02388)] [[pdf](https://arxiv.org/pdf/2402.02388)]
> **Authors**: Tong Niu,Weihao Zhang,Rong Zhao
> **First submission**: 2024-02-04
> **First announcement**: 2024-02-05
> **comment**: ef:International Conference on Autonomous Agents and Multiagent Systems 2024
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习,软件工程
- **Abstract**: Agent-based models (ABMs) stand as an essential paradigm for proposing and validating hypothetical solutions or policies aimed at addressing challenges posed by complex systems and achieving various objectives. This process demands labor-intensive endeavors and multidisciplinary expertise. Large language models (LLMs) encapsulating cross-domain knowledge and programming proficiency could potentially alleviate the difficulty of this process. However, LLMs excel in handling sequential information, making it challenging for analyzing the intricate interactions and nonlinear dynamics inherent in ABMs. Additionally, due to the lack of self-evaluation capability of LLMs, relying solely on LLMs is insufficient to effectively accomplish this process. In this paper, we present SAGE, a general solution-oriented ABM generation framework designed for automatic modeling and generating solutions for targeted problems. Unlike approaches reliant on expert handcrafting or resource-intensive neural network training, SAGE establishes a verifier-assisted iterative in-context learning process employing large language models (LLMs) to leverages their inherent cross-domain knowledge for tackling intricate demands from diverse domain scenarios. In SAGE, we introduce an semi-structured conceptual representation expliciting the intricate structures of ABMs and an objective representation to guide LLMs in modeling scenarios and proposing hypothetical solutions through in-context learning. To ensure the model executability and solution feasibility, SAGE devises a two-level verifier with chain-of-thought prompting tailored to the complex interactions and non-linear dynamics of ABMs, driving the iterative generation optimization. Moreover, we construct an evaluation dataset of solution-oriented ABMs from open sources.It contains practical models across various domains.

### Evaluating Large Language Models in Analysing Classroom Dialogue 
[[arxiv](https://arxiv.org/abs/2402.02380)] [[cool](https://papers.cool/arxiv/2402.02380)] [[pdf](https://arxiv.org/pdf/2402.02380)]
> **Authors**: Yun Long,Haifeng Luo,Yu Zhang
> **First submission**: 2024-02-04
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,人机交互
- **Abstract**: This study explores the application of Large Language Models (LLMs), specifically GPT-4, in the analysis of classroom dialogue, a crucial research task for both teaching diagnosis and quality improvement. Recognizing the knowledge-intensive and labor-intensive nature of traditional qualitative methods in educational research, this study investigates the potential of LLM to streamline and enhance the analysis process. The study involves datasets from a middle school, encompassing classroom dialogues across mathematics and Chinese classes. These dialogues were manually coded by educational experts and then analyzed using a customised GPT-4 model. This study focuses on comparing manual annotations with the outputs of GPT-4 to evaluate its efficacy in analyzing educational dialogues. Time efficiency, inter-coder agreement, and inter-coder reliability between human coders and GPT-4 are evaluated. Results indicate substantial time savings with GPT-4, and a high degree of consistency in coding between the model and human coders, with some discrepancies in specific codes. These findings highlight the strong potential of LLM in teaching evaluation and facilitation.

### A Survey of Large Language Models in Finance (FinLLMs) 
[[arxiv](https://arxiv.org/abs/2402.02315)] [[cool](https://papers.cool/arxiv/2402.02315)] [[pdf](https://arxiv.org/pdf/2402.02315)]
> **Authors**: Jean Lee,Nicholas Stevens,Soyeon Caren Han,Minseok Song
> **First submission**: 2024-02-03
> **First announcement**: 2024-02-05
> **comment**: More information on https://github.com/adlnlp/FinLLMs
- **标题**: None
- **领域**: 计算语言学,一般财务
- **Abstract**: Large Language Models (LLMs) have shown remarkable capabilities across a wide variety of Natural Language Processing (NLP) tasks and have attracted attention from multiple domains, including financial services. Despite the extensive research into general-domain LLMs, and their immense potential in finance, Financial LLM (FinLLM) research remains limited. This survey provides a comprehensive overview of FinLLMs, including their history, techniques, performance, and opportunities and challenges. Firstly, we present a chronological overview of general-domain Pre-trained Language Models (PLMs) through to current FinLLMs, including the GPT-series, selected open-source LLMs, and financial LMs. Secondly, we compare five techniques used across financial PLMs and FinLLMs, including training methods, training data, and fine-tuning methods. Thirdly, we summarize the performance evaluations of six benchmark tasks and datasets. In addition, we provide eight advanced financial NLP tasks and datasets for developing more sophisticated FinLLMs. Finally, we discuss the opportunities and the challenges facing FinLLMs, such as hallucination, privacy, and efficiency. To support AI research in finance, we compile a collection of accessible datasets and evaluation benchmarks on GitHub.

### SynthDST: Synthetic Data is All You Need for Few-Shot Dialog State Tracking 
[[arxiv](https://arxiv.org/abs/2402.02285)] [[cool](https://papers.cool/arxiv/2402.02285)] [[pdf](https://arxiv.org/pdf/2402.02285)]
> **Authors**: Atharva Kulkarni,Bo-Hsiang Tseng,Joel Ruben Antony Moniz,Dhivya Piraviperumal,Hong Yu,Shruti Bhargava
> **First submission**: 2024-02-03
> **First announcement**: 2024-02-05
> **comment**: 9 pages. 4 figures, EACL 2024 main conference
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: In-context learning with Large Language Models (LLMs) has emerged as a promising avenue of research in Dialog State Tracking (DST). However, the best-performing in-context learning methods involve retrieving and adding similar examples to the prompt, requiring access to labeled training data. Procuring such training data for a wide range of domains and applications is time-consuming, expensive, and, at times, infeasible. While zero-shot learning requires no training data, it significantly lags behind the few-shot setup. Thus, `\textit{Can we efficiently generate synthetic data for any dialogue schema to enable few-shot prompting?}' Addressing this question, we propose \method, a data generation framework tailored for DST, utilizing LLMs. Our approach only requires the dialogue schema and a few hand-crafted dialogue templates to synthesize natural, coherent, and free-flowing dialogues with DST annotations. Few-shot learning using data from {\method} results in $4-5%$ improvement in Joint Goal Accuracy over the zero-shot baseline on MultiWOZ 2.1 and 2.4. Remarkably, our few-shot learning approach recovers nearly $98%$ of the performance compared to the few-shot setup using human-annotated training data. Our synthetic data and code can be accessed at https://github.com/apple/ml-synthdst

### Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models 
[[arxiv](https://arxiv.org/abs/2402.02244)] [[cool](https://papers.cool/arxiv/2402.02244)] [[pdf](https://arxiv.org/pdf/2402.02244)]
> **Authors**: Xindi Wang,Mahsa Salmani,Parsa Omidi,Xiangyu Ren,Mehdi Rezagholizadeh,Armaghan Eshaghi
> **First submission**: 2024-02-03
> **First announcement**: 2024-02-05
> **comment**: Accepted to IJCAI 2024 Survey Track -- camera-ready version
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Recently, large language models (LLMs) have shown remarkable capabilities including understanding context, engaging in logical reasoning, and generating responses. However, this is achieved at the expense of stringent computational and memory requirements, hindering their ability to effectively support long input sequences. This survey provides an inclusive review of the recent techniques and methods devised to extend the sequence length in LLMs, thereby enhancing their capacity for long-context understanding. In particular, we review and categorize a wide range of techniques including architectural modifications, such as modified positional encoding and altered attention mechanisms, which are designed to enhance the processing of longer sequences while avoiding a proportional increase in computational requirements. The diverse methodologies investigated in this study can be leveraged across different phases of LLMs, i.e., training, fine-tuning and inference. This enables LLMs to efficiently process extended sequences. The limitations of the current methodologies is discussed in the last section along with the suggestions for future research directions, underscoring the importance of sequence length in the continued advancement of LLMs.

### Language Writ Large: LLMs, ChatGPT, Grounding, Meaning and Understanding 
[[arxiv](https://arxiv.org/abs/2402.02243)] [[cool](https://papers.cool/arxiv/2402.02243)] [[pdf](https://arxiv.org/pdf/2402.02243)]
> **Authors**: Stevan Harnad
> **First submission**: 2024-02-03
> **First announcement**: 2024-02-05
> **comment**: 54 pages, 29 references
- **标题**: None
- **领域**: 计算语言学,神经元和认知
- **Abstract**: Apart from what (little) OpenAI may be concealing from us, we all know (roughly) how ChatGPT works (its huge text database, its statistics, its vector representations, and their huge number of parameters, its next-word training, and so on). But none of us can say (hand on heart) that we are not surprised by what ChatGPT has proved to be able to do with these resources. This has even driven some of us to conclude that ChatGPT actually understands. It is not true that it understands. But it is also not true that we understand how it can do what it can do. I will suggest some hunches about benign biases: convergent constraints that emerge at LLM scale that may be helping ChatGPT do so much better than we would have expected. These biases are inherent in the nature of language itself, at LLM scale, and they are closely linked to what it is that ChatGPT lacks, which is direct sensorimotor grounding to connect its words to their referents and its propositions to their meanings. These convergent biases are related to (1) the parasitism of indirect verbal grounding on direct sensorimotor grounding, (2) the circularity of verbal definition, (3) the mirroring of language production and comprehension, (4) iconicity in propositions at LLM scale, (5) computational counterparts of human categorical perception in category learning by neural nets, and perhaps also (6) a conjecture by Chomsky about the laws of thought. The exposition will be in the form of a dialogue with ChatGPT-4.

### A Survey to Recent Progress Towards Understanding In-Context Learning 
[[arxiv](https://arxiv.org/abs/2402.02212)] [[cool](https://papers.cool/arxiv/2402.02212)] [[pdf](https://arxiv.org/pdf/2402.02212)]
> **Authors**: Haitao Mao,Guangliang Liu,Yao Ma,Rongrong Wang,Kristen Johnson,Jiliang Tang
> **First submission**: 2024-02-03
> **First announcement**: 2024-02-05
> **comment**: 21 pages, 1 figure
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: In-Context Learning (ICL) empowers Large Language Models (LLMs) with the ability to learn from a few examples provided in the prompt, enabling downstream generalization without the requirement for gradient updates. Despite encouragingly empirical success, the underlying mechanism of ICL remains unclear. Existing research remains ambiguous with various viewpoints, utilizing intuition-driven and ad-hoc technical solutions to interpret ICL. In this paper, we leverage a data generation perspective to reinterpret recent efforts from a systematic angle, demonstrating the potential broader usage of these popular technical solutions. For a conceptual definition, we rigorously adopt the terms of skill recognition and skill learning. Skill recognition selects one learned data generation function previously seen during pre-training while skill learning can learn new data generation functions from in-context data. Furthermore, we provide insights into the strengths and weaknesses of both abilities, emphasizing their commonalities through the perspective of data generation. This analysis suggests potential directions for future research.

### Do Moral Judgment and Reasoning Capability of LLMs Change with Language? A Study using the Multilingual Defining Issues Test 
[[arxiv](https://arxiv.org/abs/2402.02135)] [[cool](https://papers.cool/arxiv/2402.02135)] [[pdf](https://arxiv.org/pdf/2402.02135)]
> **Authors**: Aditi Khandelwal,Utkarsh Agarwal,Kumar Tanmay,Monojit Choudhury
> **First submission**: 2024-02-03
> **First announcement**: 2024-02-05
> **comment**: Accepted to EACL 2024 (main)
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: This paper explores the moral judgment and moral reasoning abilities exhibited by Large Language Models (LLMs) across languages through the Defining Issues Test. It is a well known fact that moral judgment depends on the language in which the question is asked. We extend the work of beyond English, to 5 new languages (Chinese, Hindi, Russian, Spanish and Swahili), and probe three LLMs -- ChatGPT, GPT-4 and Llama2Chat-70B -- that shows substantial multilingual text processing and generation abilities. Our study shows that the moral reasoning ability for all models, as indicated by the post-conventional score, is substantially inferior for Hindi and Swahili, compared to Spanish, Russian, Chinese and English, while there is no clear trend for the performance of the latter four languages. The moral judgments too vary considerably by the language.

### GITA: Graph to Visual and Textual Integration for Vision-Language Graph Reasoning 
[[arxiv](https://arxiv.org/abs/2402.02130)] [[cool](https://papers.cool/arxiv/2402.02130)] [[pdf](https://arxiv.org/pdf/2402.02130)]
> **Authors**: Yanbin Wei,Shuai Fu,Weisen Jiang,Zejian Zhang,Zhixiong Zeng,Qi Wu,James T. Kwok,Yu Zhang
> **First submission**: 2024-02-03
> **First announcement**: 2024-02-05
> **comment**: NeurIPS 2024; Project Page: v-graph.github.io; Code: https://github.com/WEIYanbin1999/GITA/
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models (LLMs) are increasingly used for various tasks with graph structures. Though LLMs can process graph information in a textual format, they overlook the rich vision modality, which is an intuitive way for humans to comprehend structural information and conduct general graph reasoning. The potential benefits and capabilities of representing graph structures as visual images (i.e., $\textit{visual graph}$) are still unexplored. To fill the gap, we innovatively propose an end-to-end framework, called $\textbf{G}$raph to v$\textbf{I}$sual and $\textbf{T}$extual Integr$\textbf{A}$tion (GITA), which firstly incorporates visual graphs into general graph reasoning. Besides, we establish $\textbf{G}$raph-based $\textbf{V}$ision-$\textbf{L}$anguage $\textbf{Q}$uestion $\textbf{A}$nswering (GVLQA) dataset from existing graph data, which is the first vision-language dataset for general graph reasoning purposes. Extensive experiments on the GVLQA dataset and five real-world datasets show that GITA outperforms mainstream LLMs in terms of general graph reasoning capabilities. Moreover, We highlight the effectiveness of the layout augmentation on visual graphs and pretraining on the GVLQA dataset.

### Zero-shot Sentiment Analysis in Low-Resource Languages Using a Multilingual Sentiment Lexicon 
[[arxiv](https://arxiv.org/abs/2402.02113)] [[cool](https://papers.cool/arxiv/2402.02113)] [[pdf](https://arxiv.org/pdf/2402.02113)]
> **Authors**: Fajri Koto,Tilman Beck,Zeerak Talat,Iryna Gurevych,Timothy Baldwin
> **First submission**: 2024-02-03
> **First announcement**: 2024-02-05
> **comment**: Accepted at EACL 2024
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Improving multilingual language models capabilities in low-resource languages is generally difficult due to the scarcity of large-scale data in those languages. In this paper, we relax the reliance on texts in low-resource languages by using multilingual lexicons in pretraining to enhance multilingual capabilities. Specifically, we focus on zero-shot sentiment analysis tasks across 34 languages, including 6 high/medium-resource languages, 25 low-resource languages, and 3 code-switching datasets. We demonstrate that pretraining using multilingual lexicons, without using any sentence-level sentiment data, achieves superior zero-shot performance compared to models fine-tuned on English sentiment datasets, and large language models like GPT--3.5, BLOOMZ, and XGLM. These findings are observable for unseen low-resource languages to code-mixed scenarios involving high-resource languages.

### Are Large Language Models Good Prompt Optimizers? 
[[arxiv](https://arxiv.org/abs/2402.02101)] [[cool](https://papers.cool/arxiv/2402.02101)] [[pdf](https://arxiv.org/pdf/2402.02101)]
> **Authors**: Ruotian Ma,Xiaolei Wang,Xin Zhou,Jian Li,Nan Du,Tao Gui,Qi Zhang,Xuanjing Huang
> **First submission**: 2024-02-03
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: LLM-based Automatic Prompt Optimization, which typically utilizes LLMs as Prompt Optimizers to self-reflect and refine prompts, has shown promising performance in recent studies. Despite the success, the underlying mechanism of this approach remains unexplored, and the true effectiveness of LLMs as Prompt Optimizers requires further validation. In this work, we conducted a comprehensive study to uncover the actual mechanism of LLM-based Prompt Optimization. Our findings reveal that the LLM optimizers struggle to identify the true causes of errors during reflection, tending to be biased by their own prior knowledge rather than genuinely reflecting on the errors. Furthermore, even when the reflection is semantically valid, the LLM optimizers often fail to generate appropriate prompts for the target models with a single prompt refinement step, partly due to the unpredictable behaviors of the target models. Based on the observations, we introduce a new "Automatic Behavior Optimization" paradigm, which directly optimizes the target model's behavior in a more controllable manner. We hope our study can inspire new directions for automatic prompt optimization development.

### GliDe with a CaPE: A Low-Hassle Method to Accelerate Speculative Decoding 
[[arxiv](https://arxiv.org/abs/2402.02082)] [[cool](https://papers.cool/arxiv/2402.02082)] [[pdf](https://arxiv.org/pdf/2402.02082)]
> **Authors**: Cunxiao Du,Jing Jiang,Xu Yuanchen,Jiawei Wu,Sicheng Yu,Yongqi Li,Shenggui Li,Kai Xu,Liqiang Nie,Zhaopeng Tu,Yang You
> **First submission**: 2024-02-03
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Speculative decoding is a relatively new decoding framework that leverages small and efficient draft models to reduce the latency of LLMs. In this study, we introduce GliDe and CaPE, two low-hassle modifications to vanilla speculative decoding to further improve the decoding speed of a frozen LLM. Specifically, GliDe is a modified draft model architecture that reuses the cached keys and values from the target LLM, while CaPE is a proposal expansion method that uses the draft model's confidence scores to help select additional candidate tokens for verification. Extensive experiments on different benchmarks demonstrate that our proposed GliDe draft model significantly reduces the expected decoding latency. Additional evaluation using walltime reveals that GliDe can accelerate Vicuna models up to 2.17x and further extend the improvement to 2.61x with CaPE. We will release our code, data, and the trained draft models.

### Panacea: Pareto Alignment via Preference Adaptation for LLMs 
[[arxiv](https://arxiv.org/abs/2402.02030)] [[cool](https://papers.cool/arxiv/2402.02030)] [[pdf](https://arxiv.org/pdf/2402.02030)]
> **Authors**: Yifan Zhong,Chengdong Ma,Xiaoyuan Zhang,Ziran Yang,Haojun Chen,Qingfu Zhang,Siyuan Qi,Yaodong Yang
> **First submission**: 2024-02-03
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Current methods for large language model alignment typically use scalar human preference labels. However, this convention tends to oversimplify the multi-dimensional and heterogeneous nature of human preferences, leading to reduced expressivity and even misalignment. This paper presents Panacea, an innovative approach that reframes alignment as a multi-dimensional preference optimization problem. Panacea trains a single model capable of adapting online and Pareto-optimally to diverse sets of preferences without the need for further tuning. A major challenge here is using a low-dimensional preference vector to guide the model's behavior, despite it being governed by an overwhelmingly large number of parameters. To address this, Panacea is designed to use singular value decomposition (SVD)-based low-rank adaptation, which allows the preference vector to be simply injected online as singular values. Theoretically, we prove that Panacea recovers the entire Pareto front with common loss aggregation methods under mild conditions. Moreover, our experiments demonstrate, for the first time, the feasibility of aligning a single LLM to represent an exponentially vast spectrum of human preferences through various optimization methods. Our work marks a step forward in effectively and efficiently aligning models to diverse and intricate human preferences in a controllable and Pareto-optimal manner.

### How well do LLMs cite relevant medical references? An evaluation framework and analyses 
[[arxiv](https://arxiv.org/abs/2402.02008)] [[cool](https://papers.cool/arxiv/2402.02008)] [[pdf](https://arxiv.org/pdf/2402.02008)]
> **Authors**: Kevin Wu,Eric Wu,Ally Cassasola,Angela Zhang,Kevin Wei,Teresa Nguyen,Sith Riantawan,Patricia Shi Riantawan,Daniel E. Ho,James Zou
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large language models (LLMs) are currently being used to answer medical questions across a variety of clinical domains. Recent top-performing commercial LLMs, in particular, are also capable of citing sources to support their responses. In this paper, we ask: do the sources that LLMs generate actually support the claims that they make? To answer this, we propose three contributions. First, as expert medical annotations are an expensive and time-consuming bottleneck for scalable evaluation, we demonstrate that GPT-4 is highly accurate in validating source relevance, agreeing 88% of the time with a panel of medical doctors. Second, we develop an end-to-end, automated pipeline called \textit{SourceCheckup} and use it to evaluate five top-performing LLMs on a dataset of 1200 generated questions, totaling over 40K pairs of statements and sources. Interestingly, we find that between ~50% to 90% of LLM responses are not fully supported by the sources they provide. We also evaluate GPT-4 with retrieval augmented generation (RAG) and find that, even still, around 30\% of individual statements are unsupported, while nearly half of its responses are not fully supported. Third, we open-source our curated dataset of medical questions and expert annotations for future evaluations. Given the rapid pace of LLM development and the potential harms of incorrect or outdated medical information, it is crucial to also understand and quantify their capability to produce relevant, trustworthy medical references.

### Self-Debiasing Large Language Models: Zero-Shot Recognition and Reduction of Stereotypes 
[[arxiv](https://arxiv.org/abs/2402.01981)] [[cool](https://papers.cool/arxiv/2402.01981)] [[pdf](https://arxiv.org/pdf/2402.01981)]
> **Authors**: Isabel O. Gallegos,Ryan A. Rossi,Joe Barrow,Md Mehrab Tanjim,Tong Yu,Hanieh Deilamsalehy,Ruiyi Zhang,Sungchul Kim,Franck Dernoncourt
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机与社会,机器学习
- **Abstract**: Large language models (LLMs) have shown remarkable advances in language generation and understanding but are also prone to exhibiting harmful social biases. While recognition of these behaviors has generated an abundance of bias mitigation techniques, most require modifications to the training data, model parameters, or decoding strategy, which may be infeasible without access to a trainable model. In this work, we leverage the zero-shot capabilities of LLMs to reduce stereotyping in a technique we introduce as zero-shot self-debiasing. With two approaches, self-debiasing via explanation and self-debiasing via reprompting, we show that self-debiasing can significantly reduce the degree of stereotyping across nine different social groups while relying only on the LLM itself and a simple prompt, with explanations correctly identifying invalid assumptions and reprompting delivering the greatest reductions in bias. We hope this work opens inquiry into other zero-shot techniques for bias mitigation.

### SOCIALITE-LLAMA: An Instruction-Tuned Model for Social Scientific Tasks 
[[arxiv](https://arxiv.org/abs/2402.01980)] [[cool](https://papers.cool/arxiv/2402.01980)] [[pdf](https://arxiv.org/pdf/2402.01980)]
> **Authors**: Gourab Dey,Adithya V Ganesan,Yash Kumar Lal,Manal Shah,Shreyashee Sinha,Matthew Matero,Salvatore Giorgi,Vivek Kulkarni,H. Andrew Schwartz
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: Short paper accepted to EACL 2024. 4 pgs, 2 tables
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Social science NLP tasks, such as emotion or humor detection, are required to capture the semantics along with the implicit pragmatics from text, often with limited amounts of training data. Instruction tuning has been shown to improve the many capabilities of large language models (LLMs) such as commonsense reasoning, reading comprehension, and computer programming. However, little is known about the effectiveness of instruction tuning on the social domain where implicit pragmatic cues are often needed to be captured. We explore the use of instruction tuning for social science NLP tasks and introduce Socialite-Llama -- an open-source, instruction-tuned Llama. On a suite of 20 social science tasks, Socialite-Llama improves upon the performance of Llama as well as matches or improves upon the performance of a state-of-the-art, multi-task finetuned model on a majority of them. Further, Socialite-Llama also leads to improvement on 5 out of 6 related social tasks as compared to Llama, suggesting instruction tuning can lead to generalized social understanding. All resources including our code, model and dataset can be found through bit.ly/socialitellama.

### LiPO: Listwise Preference Optimization through Learning-to-Rank 
[[arxiv](https://arxiv.org/abs/2402.01878)] [[cool](https://papers.cool/arxiv/2402.01878)] [[pdf](https://arxiv.org/pdf/2402.01878)]
> **Authors**: Tianqi Liu,Zhen Qin,Junru Wu,Jiaming Shen,Misha Khalman,Rishabh Joshi,Yao Zhao,Mohammad Saleh,Simon Baumgartner,Jialu Liu,Peter J. Liu,Xuanhui Wang
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: Accepted at NAACL 2025
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Aligning language models (LMs) with curated human feedback is critical to control their behaviors in real-world applications. Several recent policy optimization methods, such as DPO and SLiC, serve as promising alternatives to the traditional Reinforcement Learning from Human Feedback (RLHF) approach. In practice, human feedback often comes in a format of a ranked list over multiple responses to amortize the cost of reading prompt. Multiple responses can also be ranked by reward models or AI feedback. There lacks such a thorough study on directly fitting upon a list of responses. In this work, we formulate the LM alignment as a \textit{listwise} ranking problem and describe the LiPO framework, where the policy can potentially learn more effectively from a ranked list of plausible responses given the prompt. This view draws an explicit connection to Learning-to-Rank (LTR), where most existing preference optimization work can be mapped to existing ranking objectives. Following this connection, we provide an examination of ranking objectives that are not well studied for LM alignment with DPO and SLiC as special cases when list size is two. In particular, we highlight a specific method, LiPO-$λ$, which leverages a state-of-the-art \textit{listwise} ranking objective and weights each preference pair in a more advanced manner. We show that LiPO-$λ$ can outperform DPO variants and SLiC by a clear margin on several preference alignment tasks with both curated and real rankwise preference data.

### The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement Learning and Large Language Models 
[[arxiv](https://arxiv.org/abs/2402.01874)] [[cool](https://papers.cool/arxiv/2402.01874)] [[pdf](https://arxiv.org/pdf/2402.01874)]
> **Authors**: Moschoula Pternea,Prerna Singh,Abir Chakraborty,Yagna Oruganti,Mirco Milletari,Sayli Bapat,Kebei Jiang
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: 30 pages (including bibliography), 1 figure, 7 tables
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习,机器人技术
- **Abstract**: In this work, we review research studies that combine Reinforcement Learning (RL) and Large Language Models (LLMs), two areas that owe their momentum to the development of deep neural networks. We propose a novel taxonomy of three main classes based on the way that the two model types interact with each other. The first class, RL4LLM, includes studies where RL is leveraged to improve the performance of LLMs on tasks related to Natural Language Processing. L4LLM is divided into two sub-categories depending on whether RL is used to directly fine-tune an existing LLM or to improve the prompt of the LLM. In the second class, LLM4RL, an LLM assists the training of an RL model that performs a task that is not inherently related to natural language. We further break down LLM4RL based on the component of the RL training framework that the LLM assists or replaces, namely reward shaping, goal generation, and policy function. Finally, in the third class, RL+LLM, an LLM and an RL agent are embedded in a common planning framework without either of them contributing to training or fine-tuning of the other. We further branch this class to distinguish between studies with and without natural language feedback. We use this taxonomy to explore the motivations behind the synergy of LLMs and RL and explain the reasons for its success, while pinpointing potential shortcomings and areas where further research is needed, as well as alternative methodologies that serve the same goal.

### PiCO: Peer Review in LLMs based on the Consistency Optimization 
[[arxiv](https://arxiv.org/abs/2402.01830)] [[cool](https://papers.cool/arxiv/2402.01830)] [[pdf](https://arxiv.org/pdf/2402.01830)]
> **Authors**: Kun-Peng Ning,Shuo Yang,Yu-Yang Liu,Jia-Yu Yao,Zhen-Hui Liu,Yong-Hong Tian,Yibing Song,Li Yuan
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Existing large language models (LLMs) evaluation methods typically focus on testing the performance on some closed-environment and domain-specific benchmarks with human annotations. In this paper, we explore a novel unsupervised evaluation direction, utilizing peer-review mechanisms to measure LLMs automatically. In this setting, both open-source and closed-source LLMs lie in the same environment, capable of answering unlabeled questions and evaluating each other, where each LLM's response score is jointly determined by other anonymous ones. To obtain the ability hierarchy among these models, we assign each LLM a learnable capability parameter to adjust the final ranking. We formalize it as a constrained optimization problem, intending to maximize the consistency of each LLM's capabilities and scores. The key assumption behind is that high-level LLM can evaluate others' answers more accurately than low-level ones, while higher-level LLM can also achieve higher response scores. Moreover, we propose three metrics called PEN, CIN, and LIS to evaluate the gap in aligning human rankings. We perform experiments on multiple datasets with these metrics, validating the effectiveness of the proposed approach.

### Retrieval Augmented End-to-End Spoken Dialog Models 
[[arxiv](https://arxiv.org/abs/2402.01828)] [[cool](https://papers.cool/arxiv/2402.01828)] [[pdf](https://arxiv.org/pdf/2402.01828)]
> **Authors**: Mingqiu Wang,Izhak Shafran,Hagen Soltau,Wei Han,Yuan Cao,Dian Yu,Laurent El Shafey
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: ef:Proc. ICASSP 2024
- **标题**: None
- **领域**: 计算语言学,人工智能,声音,音频和语音处理
- **Abstract**: We recently developed SLM, a joint speech and language model, which fuses a pretrained foundational speech model and a large language model (LLM), while preserving the in-context learning capability intrinsic to the pretrained LLM. In this paper, we apply SLM to speech dialog applications where the dialog states are inferred directly from the audio signal. Task-oriented dialogs often contain domain-specific entities, i.e., restaurants, hotels, train stations, and city names, which are difficult to recognize, however, critical for the downstream applications. Inspired by the RAG (retrieval-augmented generation) paradigm, we propose a retrieval augmented SLM (ReSLM) that overcomes this weakness. We first train a speech retriever to retrieve text entities mentioned in the audio. The retrieved entities are then added as text inputs to the underlying SLM to bias model predictions. We evaluated ReSLM on speech MultiWoz task (DSTC-11 challenge), and found that this retrieval augmentation boosts model performance, achieving joint goal accuracy (38.6% vs 32.7%), slot error rate (20.6% vs 24.8%) and ASR word error rate (5.5% vs 6.7%). While demonstrated on dialog state tracking, our approach is broadly applicable to other speech tasks requiring contextual information or domain-specific entities, such as contextual ASR with biasing capability.

### Leveraging Large Language Models for Analyzing Blood Pressure Variations Across Biological Sex from Scientific Literature 
[[arxiv](https://arxiv.org/abs/2402.01826)] [[cool](https://papers.cool/arxiv/2402.01826)] [[pdf](https://arxiv.org/pdf/2402.01826)]
> **Authors**: Yuting Guo,Seyedeh Somayyeh Mousavi,Reza Sameni,Abeed Sarker
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Hypertension, defined as blood pressure (BP) that is above normal, holds paramount significance in the realm of public health, as it serves as a critical precursor to various cardiovascular diseases (CVDs) and significantly contributes to elevated mortality rates worldwide. However, many existing BP measurement technologies and standards might be biased because they do not consider clinical outcomes, comorbidities, or demographic factors, making them inconclusive for diagnostic purposes. There is limited data-driven research focused on studying the variance in BP measurements across these variables. In this work, we employed GPT-35-turbo, a large language model (LLM), to automatically extract the mean and standard deviation values of BP for both males and females from a dataset comprising 25 million abstracts sourced from PubMed. 993 article abstracts met our predefined inclusion criteria (i.e., presence of references to blood pressure, units of blood pressure such as mmHg, and mention of biological sex). Based on the automatically-extracted information from these articles, we conducted an analysis of the variations of BP values across biological sex. Our results showed the viability of utilizing LLMs to study the BP variations across different demographic factors.

### Fractal Patterns May Illuminate the Success of Next-Token Prediction 
[[arxiv](https://arxiv.org/abs/2402.01825)] [[cool](https://papers.cool/arxiv/2402.01825)] [[pdf](https://arxiv.org/pdf/2402.01825)]
> **Authors**: Ibrahim Alabdulmohsin,Vinh Q. Tran,Mostafa Dehghani
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: 15 pages, 10 tables, 6 figures
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: We study the fractal structure of language, aiming to provide a precise formalism for quantifying properties that may have been previously suspected but not formally shown. We establish that language is: (1) self-similar, exhibiting complexities at all levels of granularity, with no particular characteristic context length, and (2) long-range dependent (LRD), with a Hurst parameter of approximately H=0.7. Based on these findings, we argue that short-term patterns/dependencies in language, such as in paragraphs, mirror the patterns/dependencies over larger scopes, like entire documents. This may shed some light on how next-token prediction can capture the structure of text across multiple levels of granularity, from words and clauses to broader contexts and intents. In addition, we carry out an extensive analysis across different domains and architectures, showing that fractal parameters are robust. Finally, we demonstrate that the tiny variations in fractal parameters seen across LLMs improve upon perplexity-based bits-per-byte (BPB) in predicting their downstream performance. We hope these findings offer a fresh perspective on language and the mechanisms underlying the success of LLMs.

### Building Guardrails for Large Language Models 
[[arxiv](https://arxiv.org/abs/2402.01822)] [[cool](https://papers.cool/arxiv/2402.01822)] [[pdf](https://arxiv.org/pdf/2402.01822)]
> **Authors**: Yi Dong,Ronghui Mu,Gaojie Jin,Yi Qi,Jinwei Hu,Xingyu Zhao,Jie Meng,Wenjie Ruan,Xiaowei Huang
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: Proceedings of the 41st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: As Large Language Models (LLMs) become more integrated into our daily lives, it is crucial to identify and mitigate their risks, especially when the risks can have profound impacts on human users and societies. Guardrails, which filter the inputs or outputs of LLMs, have emerged as a core safeguarding technology. This position paper takes a deep look at current open-source solutions (Llama Guard, Nvidia NeMo, Guardrails AI), and discusses the challenges and the road towards building more complete solutions. Drawing on robust evidence from previous research, we advocate for a systematic approach to construct guardrails for LLMs, based on comprehensive consideration of diverse contexts across various LLMs applications. We propose employing socio-technical methods through collaboration with a multi-disciplinary team to pinpoint precise technical requirements, exploring advanced neural-symbolic implementations to embrace the complexity of the requirements, and developing verification and testing to ensure the utmost quality of the final product.

### Distilling LLMs' Decomposition Abilities into Compact Language Models 
[[arxiv](https://arxiv.org/abs/2402.01812)] [[cool](https://papers.cool/arxiv/2402.01812)] [[pdf](https://arxiv.org/pdf/2402.01812)]
> **Authors**: Denis Tarasov,Kumar Shridhar
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: https://github.com/DT6A/GSM8K-AI-SubQ
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Large Language Models (LLMs) have demonstrated proficiency in their reasoning abilities, yet their large size presents scalability challenges and limits any further customization. In contrast, compact models offer customized training but often fall short in solving complex reasoning tasks. This study focuses on distilling the LLMs' decomposition skills into compact models using offline reinforcement learning. We leverage the advancements in the LLM`s capabilities to provide feedback and generate a specialized task-specific dataset for training compact models. The development of an AI-generated dataset and the establishment of baselines constitute the primary contributions of our work, underscoring the potential of compact models in replicating complex problem-solving skills.

### Can LLMs perform structured graph reasoning? 
[[arxiv](https://arxiv.org/abs/2402.01805)] [[cool](https://papers.cool/arxiv/2402.01805)] [[pdf](https://arxiv.org/pdf/2402.01805)]
> **Authors**: Palaash Agrawal,Shavak Vasania,Cheston Tan
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: International Conference on Pattern Recognition (ICPR), 2024
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Pretrained Large Language Models (LLMs) have demonstrated various reasoning capabilities through language-based prompts alone, particularly in unstructured task settings (tasks purely based on language semantics). However, LLMs often struggle with structured tasks, because of the inherent incompatibility of input representation. Reducing structured tasks to uni-dimensional language semantics often renders the problem trivial. Keeping the trade-off between LLM compatibility and structure complexity in mind, we design various graph reasoning tasks as a proxy to semi-structured tasks in this paper, in order to test the ability to navigate through representations beyond plain text in various LLMs. Particularly, we design 10 distinct problems of graph traversal, each representing increasing levels of complexity, and benchmark 5 different instruct-finetuned LLMs (GPT-4, GPT-3.5, Claude-2, Llama-2 and Palm-2) on the aforementioned tasks. Further, we analyse the performance of models across various settings such as varying sizes of graphs as well as different forms of k-shot prompting. We highlight various limitations, biases and properties of LLMs through this benchmarking process, such as an inverse relation to the average degrees of freedom of traversal per node in graphs, the overall negative impact of k-shot prompting on graph reasoning tasks, and a positive response bias which prevents LLMs from identifying the absence of a valid solution. Finally, we introduce a new prompting technique specially designed for graph traversal tasks (PathCompare), which demonstrates a notable increase in the performance of LLMs in comparison to standard prompting techniques such as Chain-of-Thought (CoT).

### LitLLM: A Toolkit for Scientific Literature Review 
[[arxiv](https://arxiv.org/abs/2402.01788)] [[cool](https://papers.cool/arxiv/2402.01788)] [[pdf](https://arxiv.org/pdf/2402.01788)]
> **Authors**: Shubham Agarwal,Issam H. Laradji,Laurent Charlin,Christopher Pal
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,信息检索
- **Abstract**: Conducting literature reviews for scientific papers is essential for understanding research, its limitations, and building on existing work. It is a tedious task which makes an automatic literature review generator appealing. Unfortunately, many existing works that generate such reviews using Large Language Models (LLMs) have significant limitations. They tend to hallucinate-generate non-actual information-and ignore the latest research they have not been trained on. To address these limitations, we propose a toolkit that operates on Retrieval Augmented Generation (RAG) principles, specialized prompting and instructing techniques with the help of LLMs. Our system first initiates a web search to retrieve relevant papers by summarizing user-provided abstracts into keywords using an off-the-shelf LLM. Authors can enhance the search by supplementing it with relevant papers or keywords, contributing to a tailored retrieval process. Second, the system re-ranks the retrieved papers based on the user-provided abstract. Finally, the related work section is generated based on the re-ranked results and the abstract. There is a substantial reduction in time and effort for literature review compared to traditional methods, establishing our toolkit as an efficient alternative. Our open-source toolkit is accessible at https://github.com/shubhamagarwal92/LitLLM and Huggingface space (https://huggingface.co/spaces/shubhamagarwal92/LitLLM) with the video demo at https://youtu.be/E2ggOZBAFw0.

### Hierarchical Multi-Label Classification of Online Vaccine Concerns 
[[arxiv](https://arxiv.org/abs/2402.01783)] [[cool](https://papers.cool/arxiv/2402.01783)] [[pdf](https://arxiv.org/pdf/2402.01783)]
> **Authors**: Chloe Qinyu Zhu,Rickard Stureborg,Bhuwan Dhingra
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-05
> **comment**: Published in AAAI 2024 Health Intelligence workshop
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Vaccine concerns are an ever-evolving target, and can shift quickly as seen during the COVID-19 pandemic. Identifying longitudinal trends in vaccine concerns and misinformation might inform the healthcare space by helping public health efforts strategically allocate resources or information campaigns. We explore the task of detecting vaccine concerns in online discourse using large language models (LLMs) in a zero-shot setting without the need for expensive training datasets. Since real-time monitoring of online sources requires large-scale inference, we explore cost-accuracy trade-offs of different prompting strategies and offer concrete takeaways that may inform choices in system designs for current applications. An analysis of different prompting strategies reveals that classifying the concerns over multiple passes through the LLM, each consisting a boolean question whether the text mentions a vaccine concern or not, works the best. Our results indicate that GPT-4 can strongly outperform crowdworker accuracy when compared to ground truth annotations provided by experts on the recently introduced VaxConcerns dataset, achieving an overall F1 score of 78.7%.

### When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards 
[[arxiv](https://arxiv.org/abs/2402.01781)] [[cool](https://papers.cool/arxiv/2402.01781)] [[pdf](https://arxiv.org/pdf/2402.01781)]
> **Authors**: Norah Alzahrani,Hisham Abdullah Alyahya,Yazeed Alnumay,Sultan Alrashed,Shaykhah Alsubaie,Yusef Almushaykeh,Faisal Mirza,Nouf Alotaibi,Nora Altwairesh,Areeb Alowisheq,M Saiful Bari,Haidar Khan
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-05
> **comment**: updated with ACL 2024 camera ready version
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Large Language Model (LLM) leaderboards based on benchmark rankings are regularly used to guide practitioners in model selection. Often, the published leaderboard rankings are taken at face value - we show this is a (potentially costly) mistake. Under existing leaderboards, the relative performance of LLMs is highly sensitive to (often minute) details. We show that for popular multiple-choice question benchmarks (e.g., MMLU), minor perturbations to the benchmark, such as changing the order of choices or the method of answer selection, result in changes in rankings up to 8 positions. We explain this phenomenon by conducting systematic experiments over three broad categories of benchmark perturbations and identifying the sources of this behavior. Our analysis results in several best-practice recommendations, including the advantage of a hybrid scoring method for answer selection. Our study highlights the dangers of relying on simple benchmark evaluations and charts the path for more robust evaluation schemes on the existing benchmarks. The code for this paper is available at https://github.com/National-Center-for-AI-Saudi-Arabia/lm-evaluation-harness.

### On the Psychology of GPT-4: Moderately anxious, slightly masculine, honest, and humble 
[[arxiv](https://arxiv.org/abs/2402.01777)] [[cool](https://papers.cool/arxiv/2402.01777)] [[pdf](https://arxiv.org/pdf/2402.01777)]
> **Authors**: Adrita Barua,Gary Brase,Ke Dong,Pascal Hitzler,Eugene Vasserman
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-05
> **comment**: 16 pages, 8 tables, 1 code repository
- **标题**: None
- **领域**: 计算语言学,人工智能,人机交互
- **Abstract**: We subject GPT-4 to a number of rigorous psychometric tests and analyze the results. We find that, compared to the average human, GPT-4 tends to show more honesty and humility, and less machiavellianism and narcissism. It sometimes exhibits ambivalent sexism, leans slightly toward masculinity, is moderately anxious but mostly not depressive (but not always). It shows human-average numerical literacy and has cognitive reflection abilities that are above human average for verbal tasks.

### Redefining "Hallucination" in LLMs: Towards a psychology-informed framework for mitigating misinformation 
[[arxiv](https://arxiv.org/abs/2402.01769)] [[cool](https://papers.cool/arxiv/2402.01769)] [[pdf](https://arxiv.org/pdf/2402.01769)]
> **Authors**: Elijah Berberette,Jack Hutchins,Amir Sadovnik
> **First submission**: 2024-01-31
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: In recent years, large language models (LLMs) have become incredibly popular, with ChatGPT for example being used by over a billion users. While these models exhibit remarkable language understanding and logical prowess, a notable challenge surfaces in the form of "hallucinations." This phenomenon results in LLMs outputting misinformation in a confident manner, which can lead to devastating consequences with such a large user base. However, we question the appropriateness of the term "hallucination" in LLMs, proposing a psychological taxonomy based on cognitive biases and other psychological phenomena. Our approach offers a more fine-grained understanding of this phenomenon, allowing for targeted solutions. By leveraging insights from how humans internally resolve similar challenges, we aim to develop strategies to mitigate LLM hallucinations. This interdisciplinary approach seeks to move beyond conventional terminology, providing a nuanced understanding and actionable pathways for improvement in LLM reliability.

### LLM Voting: Human Choices and AI Collective Decision Making 
[[arxiv](https://arxiv.org/abs/2402.01766)] [[cool](https://papers.cool/arxiv/2402.01766)] [[pdf](https://arxiv.org/pdf/2402.01766)]
> **Authors**: Joshua C. Yang,Damian Dailisan,Marcin Korecki,Carina I. Hausladen,Dirk Helbing
> **First submission**: 2024-01-31
> **First announcement**: 2024-02-05
> **comment**: Accepted in AAAI Conference onAI, Ethics, and Society (AIES)
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机与社会,机器学习,普通经济学
- **Abstract**: This paper investigates the voting behaviors of Large Language Models (LLMs), specifically GPT-4 and LLaMA-2, their biases, and how they align with human voting patterns. Our methodology involved using a dataset from a human voting experiment to establish a baseline for human preferences and conducting a corresponding experiment with LLM agents. We observed that the choice of voting methods and the presentation order influenced LLM voting outcomes. We found that varying the persona can reduce some of these biases and enhance alignment with human choices. While the Chain-of-Thought approach did not improve prediction accuracy, it has potential for AI explainability in the voting process. We also identified a trade-off between preference diversity and alignment accuracy in LLMs, influenced by different temperature settings. Our findings indicate that LLMs may lead to less diverse collective outcomes and biased assumptions when used in voting scenarios, emphasizing the need for cautious integration of LLMs into democratic processes.

### LLMs Simulate Big Five Personality Traits: Further Evidence 
[[arxiv](https://arxiv.org/abs/2402.01765)] [[cool](https://papers.cool/arxiv/2402.01765)] [[pdf](https://arxiv.org/pdf/2402.01765)]
> **Authors**: Aleksandra Sorokovikova,Natalia Fedorova,Sharwin Rezagholi,Ivan P. Yamshchikov
> **First submission**: 2024-01-31
> **First announcement**: 2024-02-05
> **comment**: :I.2.7; J.4; I.2.1
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: An empirical investigation into the simulation of the Big Five personality traits by large language models (LLMs), namely Llama2, GPT4, and Mixtral, is presented. We analyze the personality traits simulated by these models and their stability. This contributes to the broader understanding of the capabilities of LLMs to simulate personality traits and the respective implications for personalized human-computer interaction.

### Rethinking Interpretability in the Era of Large Language Models 
[[arxiv](https://arxiv.org/abs/2402.01761)] [[cool](https://papers.cool/arxiv/2402.01761)] [[pdf](https://arxiv.org/pdf/2402.01761)]
> **Authors**: Chandan Singh,Jeevana Priya Inala,Michel Galley,Rich Caruana,Jianfeng Gao
> **First submission**: 2024-01-30
> **First announcement**: 2024-02-05
> **comment**: 7 pages
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Interpretable machine learning has exploded as an area of interest over the last decade, sparked by the rise of increasingly large datasets and deep neural networks. Simultaneously, large language models (LLMs) have demonstrated remarkable capabilities across a wide array of tasks, offering a chance to rethink opportunities in interpretable machine learning. Notably, the capability to explain in natural language allows LLMs to expand the scale and complexity of patterns that can be given to a human. However, these new capabilities raise new challenges, such as hallucinated explanations and immense computational costs. In this position paper, we start by reviewing existing methods to evaluate the emerging field of LLM interpretation (both interpreting LLMs and using LLMs for explanation). We contend that, despite their limitations, LLMs hold the opportunity to redefine interpretability with a more ambitious scope across many applications, including in auditing LLMs themselves. We highlight two emerging research priorities for LLM interpretation: using LLMs to directly analyze new datasets and to generate interactive explanations.

### Performance Assessment of ChatGPT vs Bard in Detecting Alzheimer's Dementia 
[[arxiv](https://arxiv.org/abs/2402.01751)] [[cool](https://papers.cool/arxiv/2402.01751)] [[pdf](https://arxiv.org/pdf/2402.01751)]
> **Authors**: Balamurali B T,Jer-Ming Chen
> **First submission**: 2024-01-30
> **First announcement**: 2024-02-05
> **comment**: 22 pages
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large language models (LLMs) find increasing applications in many fields. Here, three LLM chatbots (ChatGPT-3.5, ChatGPT-4 and Bard) are assessed - in their current form, as publicly available - for their ability to recognize Alzheimer's Dementia (AD) and Cognitively Normal (CN) individuals using textual input derived from spontaneous speech recordings. Zero-shot learning approach is used at two levels of independent queries, with the second query (chain-of-thought prompting) eliciting more detailed than the first. Each LLM chatbot's performance is evaluated on the prediction generated in terms of accuracy, sensitivity, specificity, precision and F1 score. LLM chatbots generated three-class outcome ("AD", "CN", or "Unsure"). When positively identifying AD, Bard produced highest true-positives (89% recall) and highest F1 score (71%), but tended to misidentify CN as AD, with high confidence (low "Unsure" rates); for positively identifying CN, GPT-4 resulted in the highest true-negatives at 56% and highest F1 score (62%), adopting a diplomatic stance (moderate "Unsure" rates). Overall, three LLM chatbots identify AD vs CN surpassing chance-levels but do not currently satisfy clinical application.

### PACE: A Pragmatic Agent for Enhancing Communication Efficiency Using Large Language Models 
[[arxiv](https://arxiv.org/abs/2402.01750)] [[cool](https://papers.cool/arxiv/2402.01750)] [[pdf](https://arxiv.org/pdf/2402.01750)]
> **Authors**: Jiaxuan Li,Minxi Yang,Dahua Gao,Wenlong Xu,Guangming Shi
> **First submission**: 2024-01-30
> **First announcement**: 2024-02-05
> **comment**: 11 pages,11 figures, submitted to IJCAI 2024
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Current communication technologies face limitations in terms of theoretical capacity, spectrum availability, and power resources. Pragmatic communication, leveraging terminal intelligence for selective data transmission, offers resource conservation. Existing research lacks universal intention resolution tools, limiting applicability to specific tasks. This paper proposes an image pragmatic communication framework based on a Pragmatic Agent for Communication Efficiency (PACE) using Large Language Models (LLM). In this framework, PACE sequentially performs semantic perception, intention resolution, and intention-oriented coding. To ensure the effective utilization of LLM in communication, a knowledge base is designed to supplement the necessary knowledge, dedicated prompts are introduced to facilitate understanding of pragmatic communication scenarios and task requirements, and a chain of thought is designed to assist in making reasonable trade-offs between transmission efficiency and cost. For experimental validation, this paper constructs an image pragmatic communication dataset along with corresponding evaluation standards. Simulation results indicate that the proposed method outperforms traditional and non-LLM-based pragmatic communication in terms of transmission efficiency.

### Towards Optimizing the Costs of LLM Usage 
[[arxiv](https://arxiv.org/abs/2402.01742)] [[cool](https://papers.cool/arxiv/2402.01742)] [[pdf](https://arxiv.org/pdf/2402.01742)]
> **Authors**: Shivanshu Shekhar,Tanishq Dubey,Koyel Mukherjee,Apoorv Saxena,Atharv Tyagi,Nishanth Kotla
> **First submission**: 2024-01-29
> **First announcement**: 2024-02-05
> **comment**: 8 pages + Appendix, Total 12 pages
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Generative AI and LLMs in particular are heavily used nowadays for various document processing tasks such as question answering and summarization. However, different LLMs come with different capabilities for different tasks as well as with different costs, tokenization, and latency. In fact, enterprises are already incurring huge costs of operating or using LLMs for their respective use cases. In this work, we propose optimizing the usage costs of LLMs by estimating their output quality (without actually invoking the LLMs), and then solving an optimization routine for the LLM selection to either keep costs under a budget, or minimize the costs, in a quality and latency aware manner. We propose a model to predict the output quality of LLMs on document processing tasks like summarization, followed by an LP rounding algorithm to optimize the selection of LLMs. We study optimization problems trading off the quality and costs, both theoretically and empirically. We further propose a sentence simplification model for reducing the number of tokens in a controlled manner. Additionally, we propose several deterministic heuristics for reducing tokens in a quality aware manner, and study the related optimization problem of applying the heuristics optimizing the quality and cost trade-off. We perform extensive empirical validation of our methods on not only enterprise datasets but also on open-source datasets, annotated by us, and show that we perform much better compared to closest baselines. Our methods reduce costs by 40%- 90% while improving quality by 4%-7%. We will release the annotated open source datasets to the community for further research and exploration.

### Development and Testing of a Novel Large Language Model-Based Clinical Decision Support Systems for Medication Safety in 12 Clinical Specialties 
[[arxiv](https://arxiv.org/abs/2402.01741)] [[cool](https://papers.cool/arxiv/2402.01741)] [[pdf](https://arxiv.org/pdf/2402.01741)]
> **Authors**: Jasmine Chiat Ling Ong,Liyuan Jin,Kabilan Elangovan,Gilbert Yong San Lim,Daniel Yan Zheng Lim,Gerald Gui Ren Sng,Yuhe Ke,Joshua Yi Min Tung,Ryan Jian Zhong,Christopher Ming Yao Koh,Keane Zhi Hao Lee,Xiang Chen,Jack Kian Chng,Aung Than,Ken Junyang Goh,Daniel Shu Wei Ting
> **First submission**: 2024-01-29
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Importance: We introduce a novel Retrieval Augmented Generation (RAG)-Large Language Model (LLM) framework as a Clinical Decision Support Systems (CDSS) to support safe medication prescription. Objective: To evaluate the efficacy of LLM-based CDSS in correctly identifying medication errors in different patient case vignettes from diverse medical and surgical sub-disciplines, against a human expert panel derived ground truth. We compared performance for under 2 different CDSS practical healthcare integration modalities: LLM-based CDSS alone (fully autonomous mode) vs junior pharmacist + LLM-based CDSS (co-pilot, assistive mode). Design, Setting, and Participants: Utilizing a RAG model with state-of-the-art medically-related LLMs (GPT-4, Gemini Pro 1.0 and Med-PaLM 2), this study used 61 prescribing error scenarios embedded into 23 complex clinical vignettes across 12 different medical and surgical specialties. A multidisciplinary expert panel assessed these cases for Drug-Related Problems (DRPs) using the PCNE classification and graded severity / potential for harm using revised NCC MERP medication error index. We compared. Results RAG-LLM performed better compared to LLM alone. When employed in a co-pilot mode, accuracy, recall, and F1 scores were optimized, indicating effectiveness in identifying moderate to severe DRPs. The accuracy of DRP detection with RAG-LLM improved in several categories but at the expense of lower precision. Conclusions This study established that a RAG-LLM based CDSS significantly boosts the accuracy of medication error identification when used alongside junior pharmacists (co-pilot), with notable improvements in detecting severe DRPs. This study also illuminates the comparative performance of current state-of-the-art LLMs in RAG-based CDSS systems.

### Reducing Selection Bias in Large Language Models 
[[arxiv](https://arxiv.org/abs/2402.01740)] [[cool](https://papers.cool/arxiv/2402.01740)] [[pdf](https://arxiv.org/pdf/2402.01740)]
> **Authors**: J. E. Eicher,R. F. Irgolič
> **First submission**: 2024-01-29
> **First announcement**: 2024-02-05
> **comment**: 28 pages, 23 figures
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large Language Models (LLMs) like gpt-3.5-turbo-0613 and claude-instant-1.2 are vital in interpreting and executing semantic tasks. Unfortunately, these models' inherent biases adversely affect their performance Particularly affected is object selection from lists; a fundamental operation in digital navigation and decision-making. This research critically examines these biases and quantifies the effects on a representative list selection task. To explore these biases, we experiment manipulating temperature, list length, object identity, object type, prompt complexity, and model. We isolated and measured the influence of the biases on selection behavior. Our findings show that bias structure is strongly dependent on the model, with object type modulating the magnitude of the effect. With a strong primacy effect, causing the first objects in a list to be disproportionately represented in outputs. The usage of guard rails, a prompt engineering method of ensuring a response structure, increases bias and decreases instruction adherence when to a selection task. The bias is ablated when the guard rail step is separated from the list sampling step, lowering the complexity of each individual task. We provide LLM applications and theoretically suggest that LLMs experience a form of cognitive load that is compensated for with bias.

### OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models 
[[arxiv](https://arxiv.org/abs/2402.01739)] [[cool](https://papers.cool/arxiv/2402.01739)] [[pdf](https://arxiv.org/pdf/2402.01739)]
> **Authors**: Fuzhao Xue,Zian Zheng,Yao Fu,Jinjie Ni,Zangwei Zheng,Wangchunshu Zhou,Yang You
> **First submission**: 2024-01-29
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,分布式、并行和集群计算,机器学习
- **Abstract**: To help the open-source community have a better understanding of Mixture-of-Experts (MoE) based large language models (LLMs), we train and release OpenMoE, a series of fully open-sourced and reproducible decoder-only MoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T tokens. Our investigation confirms that MoE-based LLMs can offer a more favorable cost-effectiveness trade-off than dense LLMs, highlighting the potential effectiveness for future LLM development. One more important contribution of this study is an in-depth analysis of the routing mechanisms within our OpenMoE models, leading to three significant findings: Context-Independent Specialization, Early Routing Learning, and Drop-towards-the-End. We discovered that routing decisions in MoE models are predominantly based on token IDs, with minimal context relevance. The token-to-expert assignments are determined early in the pre-training phase and remain largely unchanged. This imperfect routing can result in performance degradation, particularly in sequential tasks like multi-turn conversations, where tokens appearing later in a sequence are more likely to be dropped. Finally, we rethink our design based on the above-mentioned observations and analysis. To facilitate future MoE LLM development, we propose potential strategies for mitigating the issues we found and further improving off-the-shelf MoE LLM designs.

### Assistive Large Language Model Agents for Socially-Aware Negotiation Dialogues 
[[arxiv](https://arxiv.org/abs/2402.01737)] [[cool](https://papers.cool/arxiv/2402.01737)] [[pdf](https://arxiv.org/pdf/2402.01737)]
> **Authors**: Yuncheng Hua,Lizhen Qu,Gholamreza Haffari
> **First submission**: 2024-01-29
> **First announcement**: 2024-02-05
> **comment**: 28 pages, 3 figures, 14 tables; The paper has been published in the Findings of the Association for Computational Linguistics: EMNLP 2024
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: We develop assistive agents based on Large Language Models (LLMs) that aid interlocutors in business negotiations. Specifically, we simulate business negotiations by letting two LLM-based agents engage in role play. A third LLM acts as a remediator agent to rewrite utterances violating norms for improving negotiation outcomes. We introduce a simple tuning-free and label-free In-Context Learning (ICL) method to identify high-quality ICL exemplars for the remediator, where we propose a novel select criteria, called value impact, to measure the quality of the negotiation outcomes. We provide rich empirical evidence to demonstrate its effectiveness in negotiations across three different negotiation topics. We have released our source code and the generated dataset at: https://github.com/tk1363704/SADAS.

### VIALM: A Survey and Benchmark of Visually Impaired Assistance with Large Models 
[[arxiv](https://arxiv.org/abs/2402.01735)] [[cool](https://papers.cool/arxiv/2402.01735)] [[pdf](https://arxiv.org/pdf/2402.01735)]
> **Authors**: Yi Zhao,Yilin Zhang,Rong Xiang,Jing Li,Hillming Li
> **First submission**: 2024-01-29
> **First announcement**: 2024-02-05
> **comment**: under review
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别
- **Abstract**: Visually Impaired Assistance (VIA) aims to automatically help the visually impaired (VI) handle daily activities. The advancement of VIA primarily depends on developments in Computer Vision (CV) and Natural Language Processing (NLP), both of which exhibit cutting-edge paradigms with large models (LMs). Furthermore, LMs have shown exceptional multimodal abilities to tackle challenging physically-grounded tasks such as embodied robots. To investigate the potential and limitations of state-of-the-art (SOTA) LMs' capabilities in VIA applications, we present an extensive study for the task of VIA with LMs (VIALM). In this task, given an image illustrating the physical environments and a linguistic request from a VI user, VIALM aims to output step-by-step guidance to assist the VI user in fulfilling the request grounded in the environment. The study consists of a survey reviewing recent LM research and benchmark experiments examining selected LMs' capabilities in VIA. The results indicate that while LMs can potentially benefit VIA, their output cannot be well environment-grounded (i.e., 25.7% GPT-4's responses) and lacks fine-grained guidance (i.e., 32.1% GPT-4's responses).

### Development and Testing of Retrieval Augmented Generation in Large Language Models -- A Case Study Report 
[[arxiv](https://arxiv.org/abs/2402.01733)] [[cool](https://papers.cool/arxiv/2402.01733)] [[pdf](https://arxiv.org/pdf/2402.01733)]
> **Authors**: YuHe Ke,Liyuan Jin,Kabilan Elangovan,Hairil Rizal Abdullah,Nan Liu,Alex Tiong Heng Sia,Chai Rick Soh,Joshua Yi Min Tung,Jasmine Chiat Ling Ong,Daniel Shu Wei Ting
> **First submission**: 2024-01-29
> **First announcement**: 2024-02-05
> **comment**: NA
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Purpose: Large Language Models (LLMs) hold significant promise for medical applications. Retrieval Augmented Generation (RAG) emerges as a promising approach for customizing domain knowledge in LLMs. This case study presents the development and evaluation of an LLM-RAG pipeline tailored for healthcare, focusing specifically on preoperative medicine. Methods: We developed an LLM-RAG model using 35 preoperative guidelines and tested it against human-generated responses, with a total of 1260 responses evaluated. The RAG process involved converting clinical documents into text using Python-based frameworks like LangChain and Llamaindex, and processing these texts into chunks for embedding and retrieval. Vector storage techniques and selected embedding models to optimize data retrieval, using Pinecone for vector storage with a dimensionality of 1536 and cosine similarity for loss metrics. Human-generated answers, provided by junior doctors, were used as a comparison. Results: The LLM-RAG model generated answers within an average of 15-20 seconds, significantly faster than the 10 minutes typically required by humans. Among the basic LLMs, GPT4.0 exhibited the best accuracy of 80.1%. This accuracy was further increased to 91.4% when the model was enhanced with RAG. Compared to the human-generated instructions, which had an accuracy of 86.3%, the performance of the GPT4.0 RAG model demonstrated non-inferiority (p=0.610). Conclusions: In this case study, we demonstrated a LLM-RAG model for healthcare implementation. The pipeline shows the advantages of grounded knowledge, upgradability, and scalability as important aspects of healthcare LLM deployment.

### Evaluating LLM -- Generated Multimodal Diagnosis from Medical Images and Symptom Analysis 
[[arxiv](https://arxiv.org/abs/2402.01730)] [[cool](https://papers.cool/arxiv/2402.01730)] [[pdf](https://arxiv.org/pdf/2402.01730)]
> **Authors**: Dimitrios P. Panagoulias,Maria Virvou,George A. Tsihrintzis
> **First submission**: 2024-01-28
> **First announcement**: 2024-02-05
> **comment**: Department of Informatics, University of Piraeus, Greece
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别
- **Abstract**: Large language models (LLMs) constitute a breakthrough state-of-the-art Artificial Intelligence technology which is rapidly evolving and promises to aid in medical diagnosis. However, the correctness and the accuracy of their returns has not yet been properly evaluated. In this work, we propose an LLM evaluation paradigm that incorporates two independent steps of a novel methodology, namely (1) multimodal LLM evaluation via structured interactions and (2) follow-up, domain-specific analysis based on data extracted via the previous interactions. Using this paradigm, (1) we evaluate the correctness and accuracy of LLM-generated medical diagnosis with publicly available multimodal multiple-choice questions(MCQs) in the domain of Pathology and (2) proceed to a systemic and comprehensive analysis of extracted results. We used GPT-4-Vision-Preview as the LLM to respond to complex, medical questions consisting of both images and text, and we explored a wide range of diseases, conditions, chemical compounds, and related entity types that are included in the vast knowledge domain of Pathology. GPT-4-Vision-Preview performed quite well, scoring approximately 84\% of correct diagnoses. Next, we further analyzed the findings of our work, following an analytical approach which included Image Metadata Analysis, Named Entity Recognition and Knowledge Graphs. Weaknesses of GPT-4-Vision-Preview were revealed on specific knowledge paths, leading to a further understanding of its shortcomings in specific areas. Our methodology and findings are not limited to the use of GPT-4-Vision-Preview, but a similar approach can be followed to evaluate the usefulness and accuracy of other LLMs and, thus, improve their use with further optimization.

### Contextualization Distillation from Large Language Model for Knowledge Graph Completion 
[[arxiv](https://arxiv.org/abs/2402.01729)] [[cool](https://papers.cool/arxiv/2402.01729)] [[pdf](https://arxiv.org/pdf/2402.01729)]
> **Authors**: Dawei Li,Zhen Tan,Tianlong Chen,Huan Liu
> **First submission**: 2024-01-28
> **First announcement**: 2024-02-05
> **comment**: Accepted by EACL 2024 findings v3: add missing citations
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: While textual information significantly enhances the performance of pre-trained language models (PLMs) in knowledge graph completion (KGC), the static and noisy nature of existing corpora collected from Wikipedia articles or synsets definitions often limits the potential of PLM-based KGC models. To surmount these challenges, we introduce the Contextualization Distillation strategy, a versatile plug-in-and-play approach compatible with both discriminative and generative KGC frameworks. Our method begins by instructing large language models (LLMs) to transform compact, structural triplets into context-rich segments. Subsequently, we introduce two tailored auxiliary tasks, reconstruction and contextualization, allowing smaller KGC models to assimilate insights from these enriched triplets. Comprehensive evaluations across diverse datasets and KGC techniques highlight the efficacy and adaptability of our approach, revealing consistent performance enhancements irrespective of underlying pipelines or architectures. Moreover, our analysis makes our method more explainable and provides insight into generating path selection, as well as the choosing of suitable distillation tasks. All the code and data in this work will be released at https://github.com/David-Li0406/Contextulization-Distillation

### Hardware Phi-1.5B: A Large Language Model Encodes Hardware Domain Specific Knowledge 
[[arxiv](https://arxiv.org/abs/2402.01728)] [[cool](https://papers.cool/arxiv/2402.01728)] [[pdf](https://arxiv.org/pdf/2402.01728)]
> **Authors**: Weimin Fu,Shijie Li,Yifang Zhao,Haocheng Ma,Raj Dutta,Xuan Zhang,Kaichen Yang,Yier Jin,Xiaolong Guo
> **First submission**: 2024-01-27
> **First announcement**: 2024-02-05
> **comment**: 6 pages, 6 figures
- **标题**: None
- **领域**: 计算语言学,人工智能,硬件架构
- **Abstract**: In the rapidly evolving semiconductor industry, where research, design, verification, and manufacturing are intricately linked, the potential of Large Language Models to revolutionize hardware design and security verification is immense. The primary challenge, however, lies in the complexity of hardware specific issues that are not adequately addressed by the natural language or software code knowledge typically acquired during the pretraining stage. Additionally, the scarcity of datasets specific to the hardware domain poses a significant hurdle in developing a foundational model. Addressing these challenges, this paper introduces Hardware Phi 1.5B, an innovative large language model specifically tailored for the hardware domain of the semiconductor industry. We have developed a specialized, tiered dataset comprising small, medium, and large subsets and focused our efforts on pretraining using the medium dataset. This approach harnesses the compact yet efficient architecture of the Phi 1.5B model. The creation of this first pretrained, hardware domain specific large language model marks a significant advancement, offering improved performance in hardware design and verification tasks and illustrating a promising path forward for AI applications in the semiconductor sector.

### AI Does Not Alter Perceptions of Text Messages 
[[arxiv](https://arxiv.org/abs/2402.01726)] [[cool](https://papers.cool/arxiv/2402.01726)] [[pdf](https://arxiv.org/pdf/2402.01726)]
> **Authors**: N'yoma Diamond
> **First submission**: 2024-01-27
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,人机交互
- **Abstract**: For many people, anxiety, depression, and other social and mental factors can make composing text messages an active challenge. To remedy this problem, large language models (LLMs) may yet prove to be the perfect tool to assist users that would otherwise find texting difficult or stressful. However, despite rapid uptake in LLM usage, considerations for their assistive usage in text message composition have not been explored. A primary concern regarding LLM usage is that poor public sentiment regarding AI introduces the possibility that its usage may harm perceptions of AI-assisted text messages, making usage counter-productive. To (in)validate this possibility, we explore how the belief that a text message did or did not receive AI assistance in composition alters its perceived tone, clarity, and ability to convey intent. In this study, we survey the perceptions of 26 participants on 18 randomly labeled pre-composed text messages. In analyzing the participants' ratings of message tone, clarity, and ability to convey intent, we find that there is no statistically significant evidence that the belief that AI is utilized alters recipient perceptions. This provides hopeful evidence that LLM-based text message composition assistance can be implemented without the risk of counter-productive outcomes.

### Fortifying Ethical Boundaries in AI: Advanced Strategies for Enhancing Security in Large Language Models 
[[arxiv](https://arxiv.org/abs/2402.01725)] [[cool](https://papers.cool/arxiv/2402.01725)] [[pdf](https://arxiv.org/pdf/2402.01725)]
> **Authors**: Yunhong He,Jianling Qiu,Wei Zhang,Zhengqing Yuan
> **First submission**: 2024-01-27
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Recent advancements in large language models (LLMs) have significantly enhanced capabilities in natural language processing and artificial intelligence. These models, including GPT-3.5 and LLaMA-2, have revolutionized text generation, translation, and question-answering tasks due to the transformative Transformer model. Despite their widespread use, LLMs present challenges such as ethical dilemmas when models are compelled to respond inappropriately, susceptibility to phishing attacks, and privacy violations. This paper addresses these challenges by introducing a multi-pronged approach that includes: 1) filtering sensitive vocabulary from user input to prevent unethical responses; 2) detecting role-playing to halt interactions that could lead to 'prison break' scenarios; 3) implementing custom rule engines to restrict the generation of prohibited content; and 4) extending these methodologies to various LLM derivatives like Multi-Model Large Language Models (MLLMs). Our approach not only fortifies models against unethical manipulations and privacy breaches but also maintains their high performance across tasks. We demonstrate state-of-the-art performance under various attack prompts, without compromising the model's core functionalities. Furthermore, the introduction of differentiated security levels empowers users to control their personal data disclosure. Our methods contribute to reducing social risks and conflicts arising from technological abuse, enhance data protection, and promote social equity. Collectively, this research provides a framework for balancing the efficiency of question-answering systems with user privacy and ethical standards, ensuring a safer user experience and fostering trust in AI technology.

### An Empirical Study on Large Language Models in Accuracy and Robustness under Chinese Industrial Scenarios 
[[arxiv](https://arxiv.org/abs/2402.01723)] [[cool](https://papers.cool/arxiv/2402.01723)] [[pdf](https://arxiv.org/pdf/2402.01723)]
> **Authors**: Zongjie Li,Wenying Qiu,Pingchuan Ma,Yichen Li,You Li,Sijia He,Baozheng Jiang,Shuai Wang,Weixi Gu
> **First submission**: 2024-01-26
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Recent years have witnessed the rapid development of large language models (LLMs) in various domains. To better serve the large number of Chinese users, many commercial vendors in China have adopted localization strategies, training and providing local LLMs specifically customized for Chinese users. Furthermore, looking ahead, one of the key future applications of LLMs will be practical deployment in industrial production by enterprises and users in those sectors. However, the accuracy and robustness of LLMs in industrial scenarios have not been well studied. In this paper, we present a comprehensive empirical study on the accuracy and robustness of LLMs in the context of the Chinese industrial production area. We manually collected 1,200 domain-specific problems from 8 different industrial sectors to evaluate LLM accuracy. Furthermore, we designed a metamorphic testing framework containing four industrial-specific stability categories with eight abilities, totaling 13,631 questions with variants to evaluate LLM robustness. In total, we evaluated 9 different LLMs developed by Chinese vendors, as well as four different LLMs developed by global vendors. Our major findings include: (1) Current LLMs exhibit low accuracy in Chinese industrial contexts, with all LLMs scoring less than 0.6. (2) The robustness scores vary across industrial sectors, and local LLMs overall perform worse than global ones. (3) LLM robustness differs significantly across abilities. Global LLMs are more robust under logical-related variants, while advanced local LLMs perform better on problems related to understanding Chinese industrial terminology. Our study results provide valuable guidance for understanding and promoting the industrial domain capabilities of LLMs from both development and industrial enterprise perspectives. The results further motivate possible research directions and tooling support.

### Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately 
[[arxiv](https://arxiv.org/abs/2402.01722)] [[cool](https://papers.cool/arxiv/2402.01722)] [[pdf](https://arxiv.org/pdf/2402.01722)]
> **Authors**: Liang Zhang,Katherine Jijo,Spurthi Setty,Eden Chung,Fatima Javid,Natan Vidra,Tommy Clifford
> **First submission**: 2024-01-26
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large Language Models (LLMs) generate responses to questions; however, their effectiveness is often hindered by sub-optimal quality of answers and occasional failures to provide accurate responses to questions. To address these challenges, a fine-tuning process is employed, involving feedback and examples to refine models. The objective is to enhance AI models through continuous feedback loops, utilizing metrics such as cosine similarity, LLM evaluation and Rouge-L scores to evaluate the models. Leveraging LLMs like GPT-3.5, GPT4ALL, and LLaMA2, and Claude, this approach is benchmarked on financial datasets, including the FinanceBench and RAG Instruct Benchmark Tester Dataset, illustrating the necessity of fine-tuning. The results showcase the capability of fine-tuned models to surpass the accuracy of zero-shot LLMs, providing superior question and answering capabilities. Notably, the combination of fine-tuning the LLM with a process known as Retrieval Augmented Generation (RAG) proves to generate responses with improved accuracy.

### Measuring Moral Inconsistencies in Large Language Models 
[[arxiv](https://arxiv.org/abs/2402.01719)] [[cool](https://papers.cool/arxiv/2402.01719)] [[pdf](https://arxiv.org/pdf/2402.01719)]
> **Authors**: Vamshi Krishna Bonagiri,Sreeram Vennam,Manas Gaur,Ponnurangam Kumaraguru
> **First submission**: 2024-01-26
> **First announcement**: 2024-02-05
> **comment**: Accepted at BlackBoxNLP 2023, Co-located with EMNLP 2023
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: A Large Language Model (LLM) is considered consistent if semantically equivalent prompts produce semantically equivalent responses. Despite recent advancements showcasing the impressive capabilities of LLMs in conversational systems, we show that even state-of-the-art LLMs are highly inconsistent in their generations, questioning their reliability. Prior research has tried to measure this with task-specific accuracy. However, this approach is unsuitable for moral scenarios, such as the trolley problem, with no "correct" answer. To address this issue, we propose a novel information-theoretic measure called Semantic Graph Entropy (SGE) to measure the consistency of an LLM in moral scenarios. We leverage "Rules of Thumb" (RoTs) to explain a model's decision-making strategies and further enhance our metric. Compared to existing consistency metrics, SGE correlates better with human judgments across five LLMs. In the future, we aim to investigate the root causes of LLM inconsistencies and propose improvements.

### From RAG to QA-RAG: Integrating Generative AI for Pharmaceutical Regulatory Compliance Process 
[[arxiv](https://arxiv.org/abs/2402.01717)] [[cool](https://papers.cool/arxiv/2402.01717)] [[pdf](https://arxiv.org/pdf/2402.01717)]
> **Authors**: Jaewoong Kim,Moohong Min
> **First submission**: 2024-01-26
> **First announcement**: 2024-02-05
> **comment**: Total number of pages: 9. Total number of figures: 2. For the source code and experimental results of this paper, see https://github.com/jwoongkim11/QA-RAG. For the dataset used in training and evaluating the model, see https://huggingface.co/datasets/Jaymax/FDA Pharmaceuticals FAQ
- **标题**: None
- **领域**: 计算语言学,人工智能,信息检索
- **Abstract**: Regulatory compliance in the pharmaceutical industry entails navigating through complex and voluminous guidelines, often requiring significant human resources. To address these challenges, our study introduces a chatbot model that utilizes generative AI and the Retrieval Augmented Generation (RAG) method. This chatbot is designed to search for guideline documents relevant to the user inquiries and provide answers based on the retrieved guidelines. Recognizing the inherent need for high reliability in this domain, we propose the Question and Answer Retrieval Augmented Generation (QA-RAG) model. In comparative experiments, the QA-RAG model demonstrated a significant improvement in accuracy, outperforming all other baselines including conventional RAG methods. This paper details QA-RAG's structure and performance evaluation, emphasizing its potential for the regulatory compliance domain in the pharmaceutical industry and beyond. We have made our work publicly available for further research and development.

### ChatGPT vs Gemini vs LLaMA on Multilingual Sentiment Analysis 
[[arxiv](https://arxiv.org/abs/2402.01715)] [[cool](https://papers.cool/arxiv/2402.01715)] [[pdf](https://arxiv.org/pdf/2402.01715)]
> **Authors**: Alessio Buscemi,Daniele Proverbio
> **First submission**: 2024-01-25
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Automated sentiment analysis using Large Language Model (LLM)-based models like ChatGPT, Gemini or LLaMA2 is becoming widespread, both in academic research and in industrial applications. However, assessment and validation of their performance in case of ambiguous or ironic text is still poor. In this study, we constructed nuanced and ambiguous scenarios, we translated them in 10 languages, and we predicted their associated sentiment using popular LLMs. The results are validated against post-hoc human responses. Ambiguous scenarios are often well-coped by ChatGPT and Gemini, but we recognise significant biases and inconsistent performance across models and evaluated human languages. This work provides a standardised methodology for automated sentiment analysis evaluation and makes a call for action to further improve the algorithms and their underlying data, to improve their performance, interpretability and applicability.

### TrICy: Trigger-guided Data-to-text Generation with Intent aware Attention-Copy 
[[arxiv](https://arxiv.org/abs/2402.01714)] [[cool](https://papers.cool/arxiv/2402.01714)] [[pdf](https://arxiv.org/pdf/2402.01714)]
> **Authors**: Vibhav Agarwal,Sourav Ghosh,Harichandana BSS,Himanshu Arora,Barath Raj Kandur Raja
> **First submission**: 2024-01-25
> **First announcement**: 2024-02-05
> **comment**: Published in the IEEE/ACM Transactions on Audio, Speech, and Language Processing. (Sourav Ghosh and Vibhav Agarwal contributed equally to this work.)
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Data-to-text (D2T) generation is a crucial task in many natural language understanding (NLU) applications and forms the foundation of task-oriented dialog systems. In the context of conversational AI solutions that can work directly with local data on the user's device, architectures utilizing large pre-trained language models (PLMs) are impractical for on-device deployment due to a high memory footprint. To this end, we propose TrICy, a novel lightweight framework for an enhanced D2T task that generates text sequences based on the intent in context and may further be guided by user-provided triggers. We leverage an attention-copy mechanism to predict out-of-vocabulary (OOV) words accurately. Performance analyses on E2E NLG dataset (BLEU: 66.43%, ROUGE-L: 70.14%), WebNLG dataset (BLEU: Seen 64.08%, Unseen 52.35%), and our Custom dataset related to text messaging applications, showcase our architecture's effectiveness. Moreover, we show that by leveraging an optional trigger input, data-to-text generation quality increases significantly and achieves the new SOTA score of 69.29% BLEU for E2E NLG. Furthermore, our analyses show that TrICy achieves at least 24% and 3% improvement in BLEU and METEOR respectively over LLMs like GPT-3, ChatGPT, and Llama 2. We also demonstrate that in some scenarios, performance improvement due to triggers is observed even when they are absent in training.

### Prompting Large Language Models for Zero-Shot Clinical Prediction with Structured Longitudinal Electronic Health Record Data 
[[arxiv](https://arxiv.org/abs/2402.01713)] [[cool](https://papers.cool/arxiv/2402.01713)] [[pdf](https://arxiv.org/pdf/2402.01713)]
> **Authors**: Yinghao Zhu,Zixiang Wang,Junyi Gao,Yuning Tong,Jingkun An,Weibin Liao,Ewen M. Harrison,Liantao Ma,Chengwei Pan
> **First submission**: 2024-01-25
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: The inherent complexity of structured longitudinal Electronic Health Records (EHR) data poses a significant challenge when integrated with Large Language Models (LLMs), which are traditionally tailored for natural language processing. Motivated by the urgent need for swift decision-making during new disease outbreaks, where traditional predictive models often fail due to a lack of historical data, this research investigates the adaptability of LLMs, like GPT-4, to EHR data. We particularly focus on their zero-shot capabilities, which enable them to make predictions in scenarios in which they haven't been explicitly trained. In response to the longitudinal, sparse, and knowledge-infused nature of EHR data, our prompting approach involves taking into account specific EHR characteristics such as units and reference ranges, and employing an in-context learning strategy that aligns with clinical contexts. Our comprehensive experiments on the MIMIC-IV and TJH datasets demonstrate that with our elaborately designed prompting framework, LLMs can improve prediction performance in key tasks such as mortality, length-of-stay, and 30-day readmission by about 35\%, surpassing ML models in few-shot settings. Our research underscores the potential of LLMs in enhancing clinical decision-making, especially in urgent healthcare situations like the outbreak of emerging diseases with no labeled data. The code is publicly available at https://github.com/yhzhu99/llm4healthcare for reproducibility.

### Socially Aware Synthetic Data Generation for Suicidal Ideation Detection Using Large Language Models 
[[arxiv](https://arxiv.org/abs/2402.01712)] [[cool](https://papers.cool/arxiv/2402.01712)] [[pdf](https://arxiv.org/pdf/2402.01712)]
> **Authors**: Hamideh Ghanadian,Isar Nejadgholi,Hussein Al Osman
> **First submission**: 2024-01-25
> **First announcement**: 2024-02-05
> **comment**: ef:IEEE Access
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Suicidal ideation detection is a vital research area that holds great potential for improving mental health support systems. However, the sensitivity surrounding suicide-related data poses challenges in accessing large-scale, annotated datasets necessary for training effective machine learning models. To address this limitation, we introduce an innovative strategy that leverages the capabilities of generative AI models, such as ChatGPT, Flan-T5, and Llama, to create synthetic data for suicidal ideation detection. Our data generation approach is grounded in social factors extracted from psychology literature and aims to ensure coverage of essential information related to suicidal ideation. In our study, we benchmarked against state-of-the-art NLP classification models, specifically, those centered around the BERT family structures. When trained on the real-world dataset, UMD, these conventional models tend to yield F1-scores ranging from 0.75 to 0.87. Our synthetic data-driven method, informed by social factors, offers consistent F1-scores of 0.82 for both models, suggesting that the richness of topics in synthetic data can bridge the performance gap across different model complexities. Most impressively, when we combined a mere 30% of the UMD dataset with our synthetic data, we witnessed a substantial increase in performance, achieving an F1-score of 0.88 on the UMD test set. Such results underscore the cost-effectiveness and potential of our approach in confronting major challenges in the field, such as data scarcity and the quest for diversity in data representation.

### Not My Voice! A Taxonomy of Ethical and Safety Harms of Speech Generators 
[[arxiv](https://arxiv.org/abs/2402.01708)] [[cool](https://papers.cool/arxiv/2402.01708)] [[pdf](https://arxiv.org/pdf/2402.01708)]
> **Authors**: Wiebke Hutiri,Oresiti Papakyriakopoulos,Alice Xiang
> **First submission**: 2024-01-25
> **First announcement**: 2024-02-05
> **comment**: 17 pages, 4 tables, 4 figures Accepted at the 2024 ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT '24)
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机与社会,音频和语音处理
- **Abstract**: The rapid and wide-scale adoption of AI to generate human speech poses a range of significant ethical and safety risks to society that need to be addressed. For example, a growing number of speech generation incidents are associated with swatting attacks in the United States, where anonymous perpetrators create synthetic voices that call police officers to close down schools and hospitals, or to violently gain access to innocent citizens' homes. Incidents like this demonstrate that multimodal generative AI risks and harms do not exist in isolation, but arise from the interactions of multiple stakeholders and technical AI systems. In this paper we analyse speech generation incidents to study how patterns of specific harms arise. We find that specific harms can be categorised according to the exposure of affected individuals, that is to say whether they are a subject of, interact with, suffer due to, or are excluded from speech generation systems. Similarly, specific harms are also a consequence of the motives of the creators and deployers of the systems. Based on these insights we propose a conceptual framework for modelling pathways to ethical and safety harms of AI, which we use to develop a taxonomy of harms of speech generators. Our relational approach captures the complexity of risks and harms in sociotechnical AI systems, and yields a taxonomy that can support appropriate policy interventions and decision making for the responsible development and release of speech generation models.

### MULTIVERSE: Exposing Large Language Model Alignment Problems in Diverse Worlds 
[[arxiv](https://arxiv.org/abs/2402.01706)] [[cool](https://papers.cool/arxiv/2402.01706)] [[pdf](https://arxiv.org/pdf/2402.01706)]
> **Authors**: Xiaolong Jin,Zhuo Zhang,Xiangyu Zhang
> **First submission**: 2024-01-24
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Large Language Model (LLM) alignment aims to ensure that LLM outputs match with human values. Researchers have demonstrated the severity of alignment problems with a large spectrum of jailbreak techniques that can induce LLMs to produce malicious content during conversations. Finding the corresponding jailbreaking prompts usually requires substantial human intelligence or computation resources. In this paper, we report that LLMs have different levels of alignment in various contexts. As such, by systematically constructing many contexts, called worlds, leveraging a Domain Specific Language describing possible worlds (e.g., time, location, characters, actions and languages) and the corresponding compiler, we can cost-effectively expose latent alignment issues. Given the low cost of our method, we are able to conduct a large scale study regarding LLM alignment issues in different worlds. Our results show that our method outperforms the-state-of-the-art jailbreaking techniques on both effectiveness and efficiency. In addition, our results indicate that existing LLMs are extremely vulnerable to nesting worlds and programming language worlds. They imply that existing alignment training focuses on the real-world and is lacking in various (virtual) worlds where LLMs can be exploited.

### Steering Language Models with Game-Theoretic Solvers 
[[arxiv](https://arxiv.org/abs/2402.01704)] [[cool](https://papers.cool/arxiv/2402.01704)] [[pdf](https://arxiv.org/pdf/2402.01704)]
> **Authors**: Ian Gemp,Roma Patel,Yoram Bachrach,Marc Lanctot,Vibhavari Dasagi,Luke Marris,Georgios Piliouras,Siqi Liu,Karl Tuyls
> **First submission**: 2024-01-24
> **First announcement**: 2024-02-05
> **comment**: Code available @ https://github.com/google-deepmind/open_spiel/blob/master/open_spiel/python/games/chat_game.py
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机科学与博弈论
- **Abstract**: Mathematical models of interactions among rational agents have long been studied in game theory. However these interactions are often over a small set of discrete game actions which is very different from how humans communicate in natural language. To bridge this gap, we introduce a framework that allows equilibrium solvers to work over the space of natural language dialogue generated by large language models (LLMs). Specifically, by modelling the players, strategies and payoffs in a "game" of dialogue, we create a binding from natural language interactions to the conventional symbolic logic of game theory. Given this binding, we can ask existing game-theoretic algorithms to provide us with strategic solutions (e.g., what string an LLM should generate to maximize payoff in the face of strategic partners or opponents), giving us predictors of stable, rational conversational strategies. We focus on three domains that require different negotiation strategies: scheduling meetings, trading fruit and debate, and evaluate an LLM's generated language when guided by solvers. We see that LLMs that follow game-theory solvers result in dialogue generations that are less exploitable than the control (no guidance from solvers), and the language generated results in higher rewards, in all negotiation domains. We discuss future implications of this work, and how game-theoretic solvers that can leverage the expressivity of natural language can open up a new avenue of guiding language research.

### Large language model empowered participatory urban planning 
[[arxiv](https://arxiv.org/abs/2402.01698)] [[cool](https://papers.cool/arxiv/2402.01698)] [[pdf](https://arxiv.org/pdf/2402.01698)]
> **Authors**: Zhilun Zhou,Yuming Lin,Yong Li
> **First submission**: 2024-01-24
> **First announcement**: 2024-02-05
> **comment**: 26 pages, 7 figures, 2 tables
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Participatory urban planning is the mainstream of modern urban planning and involves the active engagement of different stakeholders. However, the traditional participatory paradigm encounters challenges in time and manpower, while the generative planning tools fail to provide adjustable and inclusive solutions. This research introduces an innovative urban planning approach integrating Large Language Models (LLMs) within the participatory process. The framework, based on the crafted LLM agent, consists of role-play, collaborative generation, and feedback iteration, solving a community-level land-use task catering to 1000 distinct interests. Empirical experiments in diverse urban communities exhibit LLM's adaptability and effectiveness across varied planning scenarios. The results were evaluated on four metrics, surpassing human experts in satisfaction and inclusion, and rivaling state-of-the-art reinforcement learning methods in service and ecology. Further analysis shows the advantage of LLM agents in providing adjustable and inclusive solutions with natural language reasoning and strong scalability. While implementing the recent advancements in emulating human behavior for planning, this work envisions both planners and citizens benefiting from low-cost, efficient LLM agents, which is crucial for enhancing participation and realizing participatory urban planning.

### APT-Pipe: A Prompt-Tuning Tool for Social Data Annotation using ChatGPT 
[[arxiv](https://arxiv.org/abs/2402.01697)] [[cool](https://papers.cool/arxiv/2402.01697)] [[pdf](https://arxiv.org/pdf/2402.01697)]
> **Authors**: Yiming Zhu,Zhizhuo Yin,Gareth Tyson,Ehsan-Ul Haq,Lik-Hang Lee,Pan Hui
> **First submission**: 2024-01-24
> **First announcement**: 2024-02-05
> **comment**: Accepted by WWW 2024; Camera-ready version
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Recent research has highlighted the potential of LLM applications, like ChatGPT, for performing label annotation on social computing text. However, it is already well known that performance hinges on the quality of the input prompts. To address this, there has been a flurry of research into prompt tuning -- techniques and guidelines that attempt to improve the quality of prompts. Yet these largely rely on manual effort and prior knowledge of the dataset being annotated. To address this limitation, we propose APT-Pipe, an automated prompt-tuning pipeline. APT-Pipe aims to automatically tune prompts to enhance ChatGPT's text classification performance on any given dataset. We implement APT-Pipe and test it across twelve distinct text classification datasets. We find that prompts tuned by APT-Pipe help ChatGPT achieve higher weighted F1-score on nine out of twelve experimented datasets, with an improvement of 7.01% on average. We further highlight APT-Pipe's flexibility as a framework by showing how it can be extended to support additional tuning mechanisms.

### Language-Guided World Models: A Model-Based Approach to AI Control 
[[arxiv](https://arxiv.org/abs/2402.01695)] [[cool](https://papers.cool/arxiv/2402.01695)] [[pdf](https://arxiv.org/pdf/2402.01695)]
> **Authors**: Alex Zhang,Khanh Nguyen,Jens Tuyls,Albert Lin,Karthik Narasimhan
> **First submission**: 2024-01-23
> **First announcement**: 2024-02-05
> **comment**: SpLU-RoboNLP workshop at ACL 2024
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: This paper introduces the concept of Language-Guided World Models (LWMs) -- probabilistic models that can simulate environments by reading texts. Agents equipped with these models provide humans with more extensive and efficient control, allowing them to simultaneously alter agent behaviors in multiple tasks via natural verbal communication. In this work, we take initial steps in developing robust LWMs that can generalize to compositionally novel language descriptions. We design a challenging world modeling benchmark based on the game of MESSENGER (Hanjie et al., 2021), featuring evaluation settings that require varying degrees of compositional generalization. Our experiments reveal the lack of generalizability of the state-of-the-art Transformer model, as it offers marginal improvements in simulation quality over a no-text baseline. We devise a more robust model by fusing the Transformer with the EMMA attention mechanism (Hanjie et al., 2021). Our model substantially outperforms the Transformer and approaches the performance of a model with an oracle semantic parsing and grounding capability. To demonstrate the practicality of this model in improving AI safety and transparency, we simulate a scenario in which the model enables an agent to present plans to a human before execution, and to revise plans based on their language feedback.

### ARGS: Alignment as Reward-Guided Search 
[[arxiv](https://arxiv.org/abs/2402.01694)] [[cool](https://papers.cool/arxiv/2402.01694)] [[pdf](https://arxiv.org/pdf/2402.01694)]
> **Authors**: Maxim Khanov,Jirayu Burapacheep,Yixuan Li
> **First submission**: 2024-01-23
> **First announcement**: 2024-02-05
> **comment**: ICLR 2024
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Aligning large language models with human objectives is paramount, yet common approaches including RLHF suffer from unstable and resource-intensive training. In response to this challenge, we introduce ARGS, Alignment as Reward-Guided Search, a novel framework that integrates alignment into the decoding process, eliminating the need for expensive RL training. By adjusting the model's probabilistic predictions using a reward signal, ARGS generates texts with semantic diversity while being aligned with human preferences, offering a promising and flexible solution for aligning language models. Notably, ARGS demonstrates consistent enhancements in average reward compared to baselines across diverse alignment tasks and various model dimensions. For example, under the same greedy-based decoding strategy, our method improves the average reward by 19.56% relative to the baseline and secures a preference or tie score of 64.33% in GPT-4 evaluation. We believe that our framework, emphasizing decoding-time alignment, paves the way for more responsive language models in the future. Code is publicly available at: \url{https://github.com/deeplearning-wisc/args}.

### Quality of Answers of Generative Large Language Models vs Peer Patients for Interpreting Lab Test Results for Lay Patients: Evaluation Study 
[[arxiv](https://arxiv.org/abs/2402.01693)] [[cool](https://papers.cool/arxiv/2402.01693)] [[pdf](https://arxiv.org/pdf/2402.01693)]
> **Authors**: Zhe He,Balu Bhasuran,Qiao Jin,Shubo Tian,Karim Hanna,Cindy Shavor,Lisbeth Garcia Arguello,Patrick Murray,Zhiyong Lu
> **First submission**: 2024-01-23
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Lab results are often confusing and hard to understand. Large language models (LLMs) such as ChatGPT have opened a promising avenue for patients to get their questions answered. We aim to assess the feasibility of using LLMs to generate relevant, accurate, helpful, and unharmful responses to lab test-related questions asked by patients and to identify potential issues that can be mitigated with augmentation approaches. We first collected lab test results related question and answer data from Yahoo! Answers and selected 53 QA pairs for this study. Using the LangChain framework and ChatGPT web portal, we generated responses to the 53 questions from four LLMs including GPT-4, Meta LLaMA 2, MedAlpaca, and ORCA_mini. We first assessed the similarity of their answers using standard QA similarity-based evaluation metrics including ROUGE, BLEU, METEOR, BERTScore. We also utilized an LLM-based evaluator to judge whether a target model has higher quality in terms of relevance, correctness, helpfulness, and safety than the baseline model. Finally, we performed a manual evaluation with medical experts for all the responses to seven selected questions on the same four aspects. The results of Win Rate and medical expert evaluation both showed that GPT-4's responses achieved better scores than all the other LLM responses and human responses on all four aspects (relevance, correctness, helpfulness, and safety). However, LLM responses occasionally also suffer from a lack of interpretation in one's medical context, incorrect statements, and lack of references. We find that compared to other three LLMs and human answer from the Q&A website, GPT-4's responses are more accurate, helpful, relevant, and safer. However, there are cases which GPT-4 responses are inaccurate and not individualized. We identified a number of ways to improve the quality of LLM responses.

### A Framework to Implement 1+N Multi-task Fine-tuning Pattern in LLMs Using the CGC-LORA Algorithm 
[[arxiv](https://arxiv.org/abs/2402.01684)] [[cool](https://papers.cool/arxiv/2402.01684)] [[pdf](https://arxiv.org/pdf/2402.01684)]
> **Authors**: Chao Song,Zhihao Ye,Qiqiang Lin,Qiuying Peng,Jun Wang
> **First submission**: 2024-01-22
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: With the productive evolution of large language models (LLMs) in the field of natural language processing (NLP), tons of effort has been made to effectively fine-tune common pre-trained LLMs to fulfill a variety of tasks in one or multiple specific domain. In practice, there are two prevailing ways, in which the adaptation can be achieved: (i) Multiple Independent Models: Pre-trained LLMs are fine-tuned a few times independently using the corresponding training samples from each task. (ii) An Integrated Model: Samples from all tasks are employed to fine-tune a pre-trianed LLM unitedly. To address the high computing cost and seesawing issue simultaneously, we propose a unified framework that implements a 1 + N mutli-task fine-tuning pattern in LLMs using a novel Customized Gate Control (CGC) Low-rank Adaptation (LoRA) algorithm. Our work aims to take an advantage of both MTL (i.e., CGC) and PEFT (i.e., LoRA) scheme. For a given cluster of tasks, we design an innovative layer that contains two types of experts as additional trainable parameters to make LoRA be compatible with MTL. To comprehensively evaluate the proposed framework, we conduct well-designed experiments on two public datasets. The experimental results demonstrate that the unified framework with CGC-LoRA modules achieves higher evaluation scores than all benchmarks on both two datasets.

### Emojis Decoded: Leveraging ChatGPT for Enhanced Understanding in Social Media Communications 
[[arxiv](https://arxiv.org/abs/2402.01681)] [[cool](https://papers.cool/arxiv/2402.01681)] [[pdf](https://arxiv.org/pdf/2402.01681)]
> **Authors**: Yuhang Zhou,Paiheng Xu,Xiyao Wang,Xuan Lu,Ge Gao,Wei Ai
> **First submission**: 2024-01-22
> **First announcement**: 2024-02-05
> **comment**: 12 pages, 2 page appendix
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Emojis, which encapsulate semantics beyond mere words or phrases, have become prevalent in social network communications. This has spurred increasing scholarly interest in exploring their attributes and functionalities. However, emoji-related research and application face two primary challenges. First, researchers typically rely on crowd-sourcing to annotate emojis in order to understand their sentiments, usage intentions, and semantic meanings. Second, subjective interpretations by users can often lead to misunderstandings of emojis and cause the communication barrier. Large Language Models (LLMs) have achieved significant success in various annotation tasks, with ChatGPT demonstrating expertise across multiple domains. In our study, we assess ChatGPT's effectiveness in handling previously annotated and downstream tasks. Our objective is to validate the hypothesis that ChatGPT can serve as a viable alternative to human annotators in emoji research and that its ability to explain emoji meanings can enhance clarity and transparency in online communications. Our findings indicate that ChatGPT has extensive knowledge of emojis. It is adept at elucidating the meaning of emojis across various application scenarios and demonstrates the potential to replace human annotators in a range of tasks.

### Large Language Model based Multi-Agents: A Survey of Progress and Challenges 
[[arxiv](https://arxiv.org/abs/2402.01680)] [[cool](https://papers.cool/arxiv/2402.01680)] [[pdf](https://arxiv.org/pdf/2402.01680)]
> **Authors**: Taicheng Guo,Xiuying Chen,Yaqi Wang,Ruidi Chang,Shichao Pei,Nitesh V. Chawla,Olaf Wiest,Xiangliang Zhang
> **First submission**: 2024-01-21
> **First announcement**: 2024-02-05
> **comment**: This work is ongoing and we welcome your contribution!
- **标题**: None
- **领域**: 计算语言学,人工智能,多代理系统
- **Abstract**: Large Language Models (LLMs) have achieved remarkable success across a wide array of tasks. Due to the impressive planning and reasoning abilities of LLMs, they have been used as autonomous agents to do many tasks automatically. Recently, based on the development of using one LLM as a single planning or decision-making agent, LLM-based multi-agent systems have achieved considerable progress in complex problem-solving and world simulation. To provide the community with an overview of this dynamic field, we present this survey to offer an in-depth discussion on the essential aspects of multi-agent systems based on LLMs, as well as the challenges. Our goal is for readers to gain substantial insights on the following questions: What domains and environments do LLM-based multi-agents simulate? How are these agents profiled and how do they communicate? What mechanisms contribute to the growth of agents' capacities? For those interested in delving into this field of study, we also summarize the commonly used datasets or benchmarks for them to have convenient access. To keep researchers updated on the latest studies, we maintain an open-source GitHub repository, dedicated to outlining the research on LLM-based multi-agent systems.

### STICKERCONV: Generating Multimodal Empathetic Responses from Scratch 
[[arxiv](https://arxiv.org/abs/2402.01679)] [[cool](https://papers.cool/arxiv/2402.01679)] [[pdf](https://arxiv.org/pdf/2402.01679)]
> **Authors**: Yiqun Zhang,Fanheng Kong,Peidong Wang,Shuang Sun,Lingshuai Wang,Shi Feng,Daling Wang,Yifei Zhang,Kaisong Song
> **First submission**: 2024-01-20
> **First announcement**: 2024-02-05
> **comment**: ef:Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7707-7733, Bangkok, Thailand, August 2024
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Stickers, while widely recognized for enhancing empathetic communication in online interactions, remain underexplored in current empathetic dialogue research, notably due to the challenge of a lack of comprehensive datasets. In this paper, we introduce the Agent for STICKERCONV (Agent4SC), which uses collaborative agent interactions to realistically simulate human behavior with sticker usage, thereby enhancing multimodal empathetic communication. Building on this foundation, we develop a multimodal empathetic dialogue dataset, STICKERCONV, comprising 12.9K dialogue sessions, 5.8K unique stickers, and 2K diverse conversational scenarios. This dataset serves as a benchmark for multimodal empathetic generation. To advance further, we propose PErceive and Generate Stickers (PEGS), a multimodal empathetic response generation framework, complemented by a comprehensive set of empathy evaluation metrics based on LLM. Our experiments demonstrate PEGS's effectiveness in generating contextually relevant and emotionally resonant multimodal empathetic responses, contributing to the advancement of more nuanced and engaging empathetic dialogue systems.

### Language models align with human judgments on key grammatical constructions 
[[arxiv](https://arxiv.org/abs/2402.01676)] [[cool](https://papers.cool/arxiv/2402.01676)] [[pdf](https://arxiv.org/pdf/2402.01676)]
> **Authors**: Jennifer Hu,Kyle Mahowald,Gary Lupyan,Anna Ivanova,Roger Levy
> **First submission**: 2024-01-19
> **First announcement**: 2024-02-05
> **comment**: Published in PNAS at https://www.pnas.org/doi/10.1073/pnas.2400917121 as response to Dentella et al. (2023)
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Do large language models (LLMs) make human-like linguistic generalizations? Dentella et al. (2023) ("DGL") prompt several LLMs ("Is the following sentence grammatically correct in English?") to elicit grammaticality judgments of 80 English sentences, concluding that LLMs demonstrate a "yes-response bias" and a "failure to distinguish grammatical from ungrammatical sentences". We re-evaluate LLM performance using well-established practices and find that DGL's data in fact provide evidence for just how well LLMs capture human behaviors. Models not only achieve high accuracy overall, but also capture fine-grained variation in human linguistic judgments.

### L-TUNING: Synchronized Label Tuning for Prompt and Prefix in LLMs 
[[arxiv](https://arxiv.org/abs/2402.01643)] [[cool](https://papers.cool/arxiv/2402.01643)] [[pdf](https://arxiv.org/pdf/2402.01643)]
> **Authors**: Md. Kowsher,Md. Shohanur Islam Sobuj,Asif Mahmud,Nusrat Jahan Prottasha,Prakash Bhat
> **First submission**: 2023-12-20
> **First announcement**: 2024-02-05
> **comment**: Published in the ICLR TinyPaper track
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Efficiently fine-tuning Large Language Models (LLMs) for specific tasks presents a considerable challenge in natural language processing. Traditional methods, like prompt or prefix tuning, typically rely on arbitrary tokens for training, leading to prolonged training times and generalized token use across various class labels. To address these issues, this paper introduces L-Tuning, an efficient fine-tuning approach designed for classification tasks within the Natural Language Inference (NLI) framework. Diverging from conventional methods, L-Tuning focuses on the fine-tuning of label tokens processed through a pre-trained LLM, thereby harnessing its pre-existing semantic knowledge. This technique not only improves the fine-tuning accuracy and efficiency but also facilitates the generation of distinct label embeddings for each class, enhancing the model's training nuance. Our experimental results indicate a significant improvement in training efficiency and classification accuracy with L-Tuning compared to traditional approaches, marking a promising advancement in fine-tuning LLMs for complex language tasks.

### Detection of Machine-Generated Text: Literature Survey 
[[arxiv](https://arxiv.org/abs/2402.01642)] [[cool](https://papers.cool/arxiv/2402.01642)] [[pdf](https://arxiv.org/pdf/2402.01642)]
> **Authors**: Dmytro Valiaiev
> **First submission**: 2024-01-01
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Since language models produce fake text quickly and easily, there is an oversupply of such content in the public domain. The degree of sophistication and writing style has reached a point where differentiating between human authored and machine-generated content is nearly impossible. As a result, works generated by language models rather than human authors have gained significant media attention and stirred controversy.Concerns regarding the possible influence of advanced language models on society have also arisen, needing a fuller knowledge of these processes. Natural language generation (NLG) and generative pre-trained transformer (GPT) models have revolutionized a variety of sectors: the scope not only permeated throughout journalism and customer service but also reached academia. To mitigate the hazardous implications that may arise from the use of these models, preventative measures must be implemented, such as providing human agents with the capacity to distinguish between artificially made and human composed texts utilizing automated systems and possibly reverse-engineered language models. Furthermore, to ensure a balanced and responsible approach, it is critical to have a full grasp of the socio-technological ramifications of these breakthroughs. This literature survey aims to compile and synthesize accomplishments and developments in the aforementioned work, while also identifying future prospects. It also gives an overview of machine-generated text trends and explores the larger societal implications. Ultimately, this survey intends to contribute to the development of robust and effective approaches for resolving the issues connected with the usage and detection of machine-generated text by exploring the interplay between the capabilities of language models and their possible implications.

### TravelPlanner: A Benchmark for Real-World Planning with Language Agents 
[[arxiv](https://arxiv.org/abs/2402.01622)] [[cool](https://papers.cool/arxiv/2402.01622)] [[pdf](https://arxiv.org/pdf/2402.01622)]
> **Authors**: Jian Xie,Kai Zhang,Jiangjie Chen,Tinghui Zhu,Renze Lou,Yuandong Tian,Yanghua Xiao,Yu Su
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: ICML 2024 (Spotlight)
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Planning has been part of the core pursuit for artificial intelligence since its conception, but earlier AI agents mostly focused on constrained settings because many of the cognitive substrates necessary for human-level planning have been lacking. Recently, language agents powered by large language models (LLMs) have shown interesting capabilities such as tool use and reasoning. Are these language agents capable of planning in more complex settings that are out of the reach of prior AI agents? To advance this investigation, we propose TravelPlanner, a new planning benchmark that focuses on travel planning, a common real-world planning scenario. It provides a rich sandbox environment, various tools for accessing nearly four million data records, and 1,225 meticulously curated planning intents and reference plans. Comprehensive evaluations show that the current language agents are not yet capable of handling such complex planning tasks-even GPT-4 only achieves a success rate of 0.6%. Language agents struggle to stay on task, use the right tools to collect information, or keep track of multiple constraints. However, we note that the mere possibility for language agents to tackle such a complex problem is in itself non-trivial progress. TravelPlanner provides a challenging yet meaningful testbed for future language agents.

### MAGDi: Structured Distillation of Multi-Agent Interaction Graphs Improves Reasoning in Smaller Language Models 
[[arxiv](https://arxiv.org/abs/2402.01620)] [[cool](https://papers.cool/arxiv/2402.01620)] [[pdf](https://arxiv.org/pdf/2402.01620)]
> **Authors**: Justin Chih-Yao Chen,Swarnadeep Saha,Elias Stengel-Eskin,Mohit Bansal
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: ICML 2024 (Camera-ready); First two authors contributed equally; GitHub: https://github.com/dinobby/MAGDi
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Multi-agent interactions between Large Language Model (LLM) agents have shown major improvements on diverse reasoning tasks. However, these involve long generations from multiple models across several rounds, making them expensive. Moreover, these multi-agent approaches fail to provide a final, single model for efficient inference. To address this, we introduce MAGDi, a new method for structured distillation of the reasoning interactions between multiple LLMs into smaller LMs. MAGDi teaches smaller models by representing multi-agent interactions as graphs, augmenting a base student model with a graph encoder, and distilling knowledge using three objective functions: next-token prediction, a contrastive loss between correct and incorrect reasoning, and a graph-based objective to model the interaction structure. Experiments on seven widely used commonsense and math reasoning benchmarks show that MAGDi improves the reasoning capabilities of smaller models, outperforming several methods that distill from a single teacher and multiple teachers. Moreover, MAGDi also demonstrates an order of magnitude higher efficiency over its teachers. We conduct extensive analyses to show that MAGDi (1) enhances the generalizability to out-of-domain tasks, (2) scales positively with the size and strength of the base student model, and (3) obtains larger improvements (via our multi-teacher training) when applying self-consistency -- an inference technique that relies on model diversity.

### KB-Plugin: A Plug-and-play Framework for Large Language Models to Induce Programs over Low-resourced Knowledge Bases 
[[arxiv](https://arxiv.org/abs/2402.01619)] [[cool](https://papers.cool/arxiv/2402.01619)] [[pdf](https://arxiv.org/pdf/2402.01619)]
> **Authors**: Jiajie Zhang,Shulin Cao,Linmei Hu,Ling Feng,Lei Hou,Juanzi Li
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Program induction (PI) has become a promising paradigm for using knowledge bases (KBs) to help large language models (LLMs) answer complex knowledge-intensive questions. Nonetheless, PI typically relies on a large number of parallel question-program pairs to make the LLM aware of the schema of the given KB, and is thus challenging for many low-resourced KBs that lack annotated data. To this end, we propose KB-Plugin, a plug-and-play framework that enables LLMs to induce programs over any low-resourced KB. Firstly, KB-Plugin adopts self-supervised learning to encode the detailed schema information of a given KB into a pluggable module, namely schema plugin. Secondly, KB-Plugin utilizes abundant annotated data from a rich-resourced KB to train another pluggable module, namely PI plugin, which can help the LLM extract question-relevant schema information from the schema plugin of any KB and utilize this information to induce programs over this KB. Experiments on five heterogeneous KBQA datasets show that KB-Plugin achieves better or comparable performance with 25$\times$ smaller backbone LLM compared to SoTA PI methods for low-resourced KBs, and even approaches the performance of supervised methods. Our code and data are available at https://github.com/THU-KEG/KB-Plugin.

### Style Vectors for Steering Generative Large Language Model 
[[arxiv](https://arxiv.org/abs/2402.01618)] [[cool](https://papers.cool/arxiv/2402.01618)] [[pdf](https://arxiv.org/pdf/2402.01618)]
> **Authors**: Kai Konen,Sophie Jentzsch,Diaoulé Diallo,Peer Schütt,Oliver Bensch,Roxanne El Baff,Dominik Opitz,Tobias Hecking
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: Will be published as findings paper at EACL2024 - 18th Conference of the European Chapter of the Association for Computational Linguistics
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: This research explores strategies for steering the output of large language models (LLMs) towards specific styles, such as sentiment, emotion, or writing style, by adding style vectors to the activations of hidden layers during text generation. We show that style vectors can be simply computed from recorded layer activations for input texts in a specific style in contrast to more complex training-based approaches. Through a series of experiments, we demonstrate the effectiveness of activation engineering using such style vectors to influence the style of generated text in a nuanced and parameterisable way, distinguishing it from prompt engineering. The presented research constitutes a significant step towards developing more adaptive and effective AI-empowered interactive systems.

### Nomic Embed: Training a Reproducible Long Context Text Embedder 
[[arxiv](https://arxiv.org/abs/2402.01613)] [[cool](https://papers.cool/arxiv/2402.01613)] [[pdf](https://arxiv.org/pdf/2402.01613)]
> **Authors**: Zach Nussbaum,John X. Morris,Brandon Duderstadt,Andriy Mulyar
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: Accepted to TMLR https://openreview.net/forum?id=IPmzyQSiQE
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: This technical report describes the training of nomic-embed-text-v1, the first fully reproducible, open-source, open-weights, open-data, 8192 context length English text embedding model that outperforms both OpenAI Ada-002 and OpenAI text-embedding-3-small on the short-context MTEB benchmark and the long context LoCo benchmark. We release the training code and model weights under an Apache 2.0 license. In contrast with other open-source models, we release the full curated training data and code that allows for full replication of nomic-embed-text-v1. You can find code and data to replicate the model at https://github.com/nomic-ai/contrastors.

### TrustAgent: Towards Safe and Trustworthy LLM-based Agents 
[[arxiv](https://arxiv.org/abs/2402.01586)] [[cool](https://papers.cool/arxiv/2402.01586)] [[pdf](https://arxiv.org/pdf/2402.01586)]
> **Authors**: Wenyue Hua,Xianjun Yang,Mingyu Jin,Zelong Li,Wei Cheng,Ruixiang Tang,Yongfeng Zhang
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: In EMNLP 2024
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习,多代理系统
- **Abstract**: The rise of LLM-based agents shows great potential to revolutionize task planning, capturing significant attention. Given that these agents will be integrated into high-stake domains, ensuring their reliability and safety is crucial. This paper presents an Agent-Constitution-based agent framework, TrustAgent, with a particular focus on improving the LLM-based agent safety. The proposed framework ensures strict adherence to the Agent Constitution through three strategic components: pre-planning strategy which injects safety knowledge to the model before plan generation, in-planning strategy which enhances safety during plan generation, and post-planning strategy which ensures safety by post-planning inspection. Our experimental results demonstrate that the proposed framework can effectively enhance an LLM agent's safety across multiple domains by identifying and mitigating potential dangers during the planning. Further analysis reveals that the framework not only improves safety but also enhances the helpfulness of the agent. Additionally, we highlight the importance of the LLM reasoning ability in adhering to the Constitution. This paper sheds light on how to ensure the safe integration of LLM-based agents into human-centric environments. Data and code are available at https://github.com/agiresearch/TrustAgent.

### An Empirical Analysis of Diversity in Argument Summarization 
[[arxiv](https://arxiv.org/abs/2402.01535)] [[cool](https://papers.cool/arxiv/2402.01535)] [[pdf](https://arxiv.org/pdf/2402.01535)]
> **Authors**: Michiel van der Meer,Piek Vossen,Catholijn M. Jonker,Pradeep K. Murukannaiah
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: Accepted at EACL2024 (main proceedings)
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Presenting high-level arguments is a crucial task for fostering participation in online societal discussions. Current argument summarization approaches miss an important facet of this task -- capturing diversity -- which is important for accommodating multiple perspectives. We introduce three aspects of diversity: those of opinions, annotators, and sources. We evaluate approaches to a popular argument summarization task called Key Point Analysis, which shows how these approaches struggle to (1) represent arguments shared by few people, (2) deal with data from various sources, and (3) align with subjectivity in human-provided annotations. We find that both general-purpose LLMs and dedicated KPA models exhibit this behavior, but have complementary strengths. Further, we observe that diversification of training data may ameliorate generalization. Addressing diversity in argument summarization requires a mix of strategies to deal with subjectivity.

### K-Level Reasoning: Establishing Higher Order Beliefs in Large Language Models for Strategic Reasoning 
[[arxiv](https://arxiv.org/abs/2402.01521)] [[cool](https://papers.cool/arxiv/2402.01521)] [[pdf](https://arxiv.org/pdf/2402.01521)]
> **Authors**: Yadong Zhang,Shaoguang Mao,Tao Ge,Xun Wang,Yan Xia,Man Lan,Furu Wei
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Strategic reasoning is a complex yet essential capability for intelligent agents. It requires Large Language Model (LLM) agents to adapt their strategies dynamically in multi-agent environments. Unlike static reasoning tasks, success in these contexts depends on anticipating other agents' beliefs and actions while continuously adjusting strategies to achieve individual goals. LLMs and LLM agents often struggle with strategic reasoning due to the absence of a reasoning framework that enables them to dynamically infer others' perspectives and adapt to changing environments. Inspired by the Level-K framework from game theory and behavioral economics, which extends reasoning from simple reactions to structured strategic depth, we propose a novel framework: "K-Level Reasoning with Large Language Models (K-R)." This framework employs recursive mechanisms to enable LLMs to achieve varying levels of strategic depth, allowing agents to form higher order beliefs - beliefs about others' beliefs. We validate this framework through rigorous testing on four testbeds: two classical game theory problems and two social intelligence tasks. The results demonstrate the advantages of K-R in strategic reasoning. Our work presents the first recursive implementation of strategic depth in large language models (LLMs). It establishes a foundation for future research into theory of mind and strategic reasoning in LLMs.

### Distractor Generation in Multiple-Choice Tasks: A Survey of Methods, Datasets, and Evaluation 
[[arxiv](https://arxiv.org/abs/2402.01512)] [[cool](https://papers.cool/arxiv/2402.01512)] [[pdf](https://arxiv.org/pdf/2402.01512)]
> **Authors**: Elaf Alhazmi,Quan Z. Sheng,Wei Emma Zhang,Munazza Zaib,Ahoud Alhazmi
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: Accepted (Main) at EMNLP 2024 : The 2024 Conference on Empirical Methods in Natural Language Processing
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The distractor generation task focuses on generating incorrect but plausible options for objective questions such as fill-in-the-blank and multiple-choice questions. This task is widely utilized in educational settings across various domains and subjects. The effectiveness of these questions in assessments relies on the quality of the distractors, as they challenge examinees to select the correct answer from a set of misleading options. The evolution of artificial intelligence (AI) has transitioned the task from traditional methods to the use of neural networks and pre-trained language models. This shift has established new benchmarks and expanded the use of advanced deep learning methods in generating distractors. This survey explores distractor generation tasks, datasets, methods, and current evaluation metrics for English objective questions, covering both text-based and multi-modal domains. It also evaluates existing AI models and benchmarks and discusses potential future research directions.

### AMOR: A Recipe for Building Adaptable Modular Knowledge Agents Through Process Feedback 
[[arxiv](https://arxiv.org/abs/2402.01469)] [[cool](https://papers.cool/arxiv/2402.01469)] [[pdf](https://arxiv.org/pdf/2402.01469)]
> **Authors**: Jian Guan,Wei Wu,Zujie Wen,Peng Xu,Hongning Wang,Minlie Huang
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: NeurIPS 2024
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The notable success of large language models (LLMs) has sparked an upsurge in building language agents to complete various complex tasks. We present AMOR, an agent framework based on open-source LLMs, which reasons with external knowledge bases and adapts to specific domains through human supervision to the reasoning process. AMOR builds reasoning logic over a finite state machine (FSM) that solves problems through autonomous executions and transitions over disentangled modules. This allows humans to provide direct feedback to the individual modules, and thus naturally forms process supervision. Based on this reasoning and feedback framework, we develop AMOR through two-stage fine-tuning: warm-up and adaptation. The former fine-tunes the LLM with examples automatically constructed from various public datasets, enabling AMOR to generalize across different knowledge environments, while the latter tailors AMOR to specific domains using process feedback. Extensive experiments across multiple domains demonstrate the advantage of AMOR to strong baselines, thanks to its FSM-based reasoning and process feedback mechanism. The code and data are publicly available at \url{https://github.com/JianGuanTHU/AMOR}.

### LLM-based NLG Evaluation: Current Status and Challenges 
[[arxiv](https://arxiv.org/abs/2402.01383)] [[cool](https://papers.cool/arxiv/2402.01383)] [[pdf](https://arxiv.org/pdf/2402.01383)]
> **Authors**: Mingqi Gao,Xinyu Hu,Jie Ruan,Xiao Pu,Xiaojun Wan
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Evaluating natural language generation (NLG) is a vital but challenging problem in artificial intelligence. Traditional evaluation metrics mainly capturing content (e.g. n-gram) overlap between system outputs and references are far from satisfactory, and large language models (LLMs) such as ChatGPT have demonstrated great potential in NLG evaluation in recent years. Various automatic evaluation methods based on LLMs have been proposed, including metrics derived from LLMs, prompting LLMs, and fine-tuning LLMs with labeled evaluation data. In this survey, we first give a taxonomy of LLM-based NLG evaluation methods, and discuss their pros and cons, respectively. We also discuss human-LLM collaboration for NLG evaluation. Lastly, we discuss several open problems in this area and point out future research directions.

### LoTR: Low Tensor Rank Weight Adaptation 
[[arxiv](https://arxiv.org/abs/2402.01376)] [[cool](https://papers.cool/arxiv/2402.01376)] [[pdf](https://arxiv.org/pdf/2402.01376)]
> **Authors**: Daniel Bershatsky,Daria Cherniuk,Talgat Daulbaev,Aleksandr Mikhalev,Ivan Oseledets
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: Submitted; missing author and sections were added;
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: In this paper we generalize and extend an idea of low-rank adaptation (LoRA) of large language models (LLMs) based on Transformer architecture. Widely used LoRA-like methods of fine-tuning LLMs are based on matrix factorization of gradient update. We introduce LoTR, a novel approach for parameter-efficient fine-tuning of LLMs which represents a gradient update to parameters in a form of tensor decomposition. Low-rank adapter for each layer is constructed as a product of three matrices, and tensor structure arises from sharing left and right multipliers of this product among layers. Simultaneous compression of a sequence of layers with low-rank tensor representation allows LoTR to archive even better parameter efficiency then LoRA especially for deep models. Moreover, the core tensor does not depend on original weight dimension and can be made arbitrary small, which allows for extremely cheap and fast downstream fine-tuning.

### Continual Learning for Large Language Models: A Survey 
[[arxiv](https://arxiv.org/abs/2402.01364)] [[cool](https://papers.cool/arxiv/2402.01364)] [[pdf](https://arxiv.org/pdf/2402.01364)]
> **Authors**: Tongtong Wu,Linhao Luo,Yuan-Fang Li,Shirui Pan,Thuy-Trang Vu,Gholamreza Haffari
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Large language models (LLMs) are not amenable to frequent re-training, due to high training costs arising from their massive scale. However, updates are necessary to endow LLMs with new skills and keep them up-to-date with rapidly evolving human knowledge. This paper surveys recent works on continual learning for LLMs. Due to the unique nature of LLMs, we catalog continue learning techniques in a novel multi-staged categorization scheme, involving continual pretraining, instruction tuning, and alignment. We contrast continual learning for LLMs with simpler adaptation methods used in smaller models, as well as with other enhancement strategies like retrieval-augmented generation and model editing. Moreover, informed by a discussion of benchmarks and evaluation, we identify several challenges and future work directions for this crucial task.

### LLMs May Perform MCQA by Selecting the Least Incorrect Option 
[[arxiv](https://arxiv.org/abs/2402.01349)] [[cool](https://papers.cool/arxiv/2402.01349)] [[pdf](https://arxiv.org/pdf/2402.01349)]
> **Authors**: Haochun Wang,Sendong Zhao,Zewen Qiang,Nuwa Xi,Bing Qin,Ting Liu
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: COLING 2025
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: In the field of NLP, Large Language Models (LLMs) have markedly enhanced performance across a variety of tasks. However, the comprehensive evaluation of LLMs remains an inevitable challenge for the community. Recently, the adoption of Multiple Choice Question Answering (MCQA) as a benchmark for assessing LLMs has gained considerable traction. However, concerns regarding the robustness of this evaluative method persist. Building upon previous discussions on the issue of \textit{variability}, we reveal an additional dimension of concern: LLMs may perform MCQA by selecting the least incorrect option rather than distinctly correct. This observation suggests that LLMs might regard multiple options as correct, which could undermine the reliability of MCQA as a metric for evaluating LLMs. To address this challenge, we introduce an enhanced dataset augmentation method for MCQA, termed MCQA+, to provide a more accurate reflection of the model performance, thereby highlighting the necessity for more sophisticated evaluation mechanisms in the assessment of LLM capabilities.

### CorpusLM: Towards a Unified Language Model on Corpus for Knowledge-Intensive Tasks 
[[arxiv](https://arxiv.org/abs/2402.01176)] [[cool](https://papers.cool/arxiv/2402.01176)] [[pdf](https://arxiv.org/pdf/2402.01176)]
> **Authors**: Xiaoxi Li,Zhicheng Dou,Yujia Zhou,Fangchao Liu
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,信息检索
- **Abstract**: Large language models (LLMs) have gained significant attention in various fields but prone to hallucination, especially in knowledge-intensive (KI) tasks. To address this, retrieval-augmented generation (RAG) has emerged as a popular solution to enhance factual accuracy. However, traditional retrieval modules often rely on large document index and disconnect with generative tasks. With the advent of generative retrieval (GR), language models can retrieve by directly generating document identifiers (DocIDs), offering superior performance in retrieval tasks. However, the potential relationship between GR and downstream tasks remains unexplored. In this paper, we propose \textbf{CorpusLM}, a unified language model that leverages external corpus to tackle various knowledge-intensive tasks by integrating generative retrieval, closed-book generation, and RAG through a unified greedy decoding process. We design the following mechanisms to facilitate effective retrieval and generation, and improve the end-to-end effectiveness of KI tasks: (1) We develop a ranking-oriented DocID list generation strategy, which refines GR by directly learning from a DocID ranking list, to improve retrieval quality. (2) We design a continuous DocIDs-References-Answer generation strategy, which facilitates effective and efficient RAG. (3) We employ well-designed unsupervised DocID understanding tasks, to comprehend DocID semantics and their relevance to downstream tasks. We evaluate our approach on the widely used KILT benchmark with two variants of backbone models, i.e., T5 and Llama2. Experimental results demonstrate the superior performance of our models in both retrieval and downstream tasks.

### Efficient Prompt Caching via Embedding Similarity 
[[arxiv](https://arxiv.org/abs/2402.01173)] [[cool](https://papers.cool/arxiv/2402.01173)] [[pdf](https://arxiv.org/pdf/2402.01173)]
> **Authors**: Hanlin Zhu,Banghua Zhu,Jiantao Jiao
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: 21 pages, 3 figures
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Large language models (LLMs) have achieved huge success in numerous natural language process (NLP) tasks. However, it faces the challenge of significant resource consumption during inference. In this paper, we aim to improve the inference efficiency of LLMs by prompt caching, i.e., if the current prompt can be answered by the same response of a previous prompt, one can directly utilize that previous response without calling the LLM. Specifically, we focus on the prediction accuracy of prompt caching for single-round question-answering tasks via embedding similarity. The existing embeddings of prompts mostly focus on whether two prompts are semantically similar, which is not necessarily equivalent to whether the same response can answer them. Therefore, we propose a distillation-based method to fine-tune the existing embeddings for better caching prediction. Theoretically, we provide finite-sample guarantees for the convergence of our method under different types of loss functions. Empirically, we carefully construct a hard dataset based on Kwiatkowski et al. (2019) where the existing embedding model (Wang et al., 2022) only achieves an AUC of 0.51. We then fine-tune the above embedding model, which significantly improves the AUC of caching prediction from 0.51 to 0.81. We also conduct simulations demonstrating that our trained models achieve better caching efficiency than the previous embedding model.

### LLM-Detector: Improving AI-Generated Chinese Text Detection with Open-Source LLM Instruction Tuning 
[[arxiv](https://arxiv.org/abs/2402.01158)] [[cool](https://papers.cool/arxiv/2402.01158)] [[pdf](https://arxiv.org/pdf/2402.01158)]
> **Authors**: Rongsheng Wang,Haoming Chen,Ruizhe Zhou,Han Ma,Yaofei Duan,Yanlan Kang,Songhua Yang,Baoyu Fan,Tao Tan
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: 17 pages, 13 tables, 7 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: ChatGPT and other general large language models (LLMs) have achieved remarkable success, but they have also raised concerns about the misuse of AI-generated texts. Existing AI-generated text detection models, such as based on BERT and RoBERTa, are prone to in-domain over-fitting, leading to poor out-of-domain (OOD) detection performance. In this paper, we first collected Chinese text responses generated by human experts and 9 types of LLMs, for which to multiple domains questions, and further created a dataset that mixed human-written sentences and sentences polished by LLMs. We then proposed LLM-Detector, a novel method for both document-level and sentence-level text detection through Instruction Tuning of LLMs. Our method leverages the wealth of knowledge LLMs acquire during pre-training, enabling them to detect the text they generate. Instruction tuning aligns the model's responses with the user's expected text detection tasks. Experimental results show that previous methods struggle with sentence-level AI-generated text detection and OOD detection. In contrast, our proposed method not only significantly outperforms baseline methods in both sentence-level and document-level text detection but also demonstrates strong generalization capabilities. Furthermore, since LLM-Detector is trained based on open-source LLMs, it is easy to customize for deployment.

### CABINET: Content Relevance based Noise Reduction for Table Question Answering 
[[arxiv](https://arxiv.org/abs/2402.01155)] [[cool](https://papers.cool/arxiv/2402.01155)] [[pdf](https://arxiv.org/pdf/2402.01155)]
> **Authors**: Sohan Patnaik,Heril Changwal,Milan Aggarwal,Sumit Bhatia,Yaman Kumar,Balaji Krishnamurthy
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: Accepted at ICLR 2024 (spotlight)
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Table understanding capability of Large Language Models (LLMs) has been extensively studied through the task of question-answering (QA) over tables. Typically, only a small part of the whole table is relevant to derive the answer for a given question. The irrelevant parts act as noise and are distracting information, resulting in sub-optimal performance due to the vulnerability of LLMs to noise. To mitigate this, we propose CABINET (Content RelevAnce-Based NoIse ReductioN for TablE QuesTion-Answering) - a framework to enable LLMs to focus on relevant tabular data by suppressing extraneous information. CABINET comprises an Unsupervised Relevance Scorer (URS), trained differentially with the QA LLM, that weighs the table content based on its relevance to the input question before feeding it to the question-answering LLM (QA LLM). To further aid the relevance scorer, CABINET employs a weakly supervised module that generates a parsing statement describing the criteria of rows and columns relevant to the question and highlights the content of corresponding table cells. CABINET significantly outperforms various tabular LLM baselines, as well as GPT3-based in-context learning methods, is more robust to noise, maintains outperformance on tables of varying sizes, and establishes new SoTA performance on WikiTQ, FeTaQA, and WikiSQL datasets. We release our code and datasets at https://github.com/Sohanpatnaik106/CABINET_QA.

## 密码学和安全(cs.CR:Cryptography and Security)

### XAI-CF -- Examining the Role of Explainable Artificial Intelligence in Cyber Forensics 
[[arxiv](https://arxiv.org/abs/2402.02452)] [[cool](https://papers.cool/arxiv/2402.02452)] [[pdf](https://arxiv.org/pdf/2402.02452)]
> **Authors**: Shahid Alam,Zeynep Altiparmak
> **First submission**: 2024-02-04
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: With the rise of complex cyber devices Cyber Forensics (CF) is facing many new challenges. For example, there are dozens of systems running on smartphones, each with more than millions of downloadable applications. Sifting through this large amount of data and making sense requires new techniques, such as from the field of Artificial Intelligence (AI). To apply these techniques successfully in CF, we need to justify and explain the results to the stakeholders of CF, such as forensic analysts and members of the court, for them to make an informed decision. If we want to apply AI successfully in CF, there is a need to develop trust in AI systems. Some other factors in accepting the use of AI in CF are to make AI authentic, interpretable, understandable, and interactive. This way, AI systems will be more acceptable to the public and ensure alignment with legal standards. An explainable AI (XAI) system can play this role in CF, and we call such a system XAI-CF. XAI-CF is indispensable and is still in its infancy. In this paper, we explore and make a case for the significance and advantages of XAI-CF. We strongly emphasize the need to build a successful and practical XAI-CF system and discuss some of the main requirements and prerequisites of such a system. We present a formal definition of the terms CF and XAI-CF and a comprehensive literature review of previous works that apply and utilize XAI to build and increase trust in CF. We discuss some challenges facing XAI-CF. We also provide some concrete solutions to these challenges. We identify key insights and future research directions for building XAI applications for CF. This paper is an effort to explore and familiarize the readers with the role of XAI applications in CF, and we believe that our work provides a promising basis for future researchers interested in XAI-CF.

### Copyright Protection in Generative AI: A Technical Perspective 
[[arxiv](https://arxiv.org/abs/2402.02333)] [[cool](https://papers.cool/arxiv/2402.02333)] [[pdf](https://arxiv.org/pdf/2402.02333)]
> **Authors**: Jie Ren,Han Xu,Pengfei He,Yingqian Cui,Shenglai Zeng,Jiankun Zhang,Hongzhi Wen,Jiayuan Ding,Pei Huang,Lingjuan Lyu,Hui Liu,Yi Chang,Jiliang Tang
> **First submission**: 2024-02-03
> **First announcement**: 2024-02-05
> **comment**: 26 pages
- **标题**: None
- **领域**: 密码学和安全,计算机视觉和模式识别,机器学习
- **Abstract**: Generative AI has witnessed rapid advancement in recent years, expanding their capabilities to create synthesized content such as text, images, audio, and code. The high fidelity and authenticity of contents generated by these Deep Generative Models (DGMs) have sparked significant copyright concerns. There have been various legal debates on how to effectively safeguard copyrights in DGMs. This work delves into this issue by providing a comprehensive overview of copyright protection from a technical perspective. We examine from two distinct viewpoints: the copyrights pertaining to the source data held by the data owners and those of the generative models maintained by the model builders. For data copyright, we delve into methods data owners can protect their content and DGMs can be utilized without infringing upon these rights. For model copyright, our discussion extends to strategies for preventing model theft and identifying outputs generated by specific models. Finally, we highlight the limitations of existing techniques and identify areas that remain unexplored. Furthermore, we discuss prospective directions for the future of copyright protection, underscoring its importance for the sustainable and ethical development of Generative AI.

### A Review and Comparison of AI Enhanced Side Channel Analysis 
[[arxiv](https://arxiv.org/abs/2402.02299)] [[cool](https://papers.cool/arxiv/2402.02299)] [[pdf](https://arxiv.org/pdf/2402.02299)]
> **Authors**: Max Panoff,Honggang Yu,Haoqi Shan,Yier Jin
> **First submission**: 2024-02-03
> **First announcement**: 2024-02-05
> **comment**: This paper has been accepted by ACM Journal on Emerging Technologies in Computing Systems (JETC)
- **标题**: None
- **领域**: 密码学和安全,机器学习
- **Abstract**: Side Channel Analysis (SCA) presents a clear threat to privacy and security in modern computing systems. The vast majority of communications are secured through cryptographic algorithms. These algorithms are often provably-secure from a cryptographical perspective, but their implementation on real hardware introduces vulnerabilities. Adversaries can exploit these vulnerabilities to conduct SCA and recover confidential information, such as secret keys or internal states. The threat of SCA has greatly increased as machine learning, and in particular deep learning, enhanced attacks become more common. In this work, we will examine the latest state-of-the-art deep learning techniques for side channel analysis, the theory behind them, and how they are conducted. Our focus will be on profiling attacks using deep learning techniques, but we will also examine some new and emerging methodologies enhanced by deep learning techniques, such as non-profiled attacks, artificial trace generation, and others. Finally, different deep learning enhanced SCA schemes attempted against the ANSSI SCA Database (ASCAD) and their relative performance will be evaluated and compared. This will lead to new research directions to secure cryptographic implementations against the latest SCA attacks.

### AI Code Generators for Security: Friend or Foe? 
[[arxiv](https://arxiv.org/abs/2402.01219)] [[cool](https://papers.cool/arxiv/2402.01219)] [[pdf](https://arxiv.org/pdf/2402.01219)]
> **Authors**: Roberto Natella,Pietro Liguori,Cristina Improta,Bojan Cukic,Domenico Cotroneo
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: Dataset available at: https://github.com/dessertlab/violent-python
- **标题**: None
- **领域**: 密码学和安全,人工智能,软件工程
- **Abstract**: Recent advances of artificial intelligence (AI) code generators are opening new opportunities in software security research, including misuse by malicious actors. We review use cases for AI code generators for security and introduce an evaluation benchmark.

## 计算机视觉和模式识别(cs.CV:Computer Vision and Pattern Recognition)

### LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model 
[[arxiv](https://arxiv.org/abs/2402.02544)] [[cool](https://papers.cool/arxiv/2402.02544)] [[pdf](https://arxiv.org/pdf/2402.02544)]
> **Authors**: Dilxat Muhtar,Zhenshi Li,Feng Gu,Xueliang Zhang,Pengfeng Xiao
> **First submission**: 2024-02-04
> **First announcement**: 2024-02-05
> **comment**: 36 pages, 10 figures. Github https://github.com/NJU-LHRS/LHRS-Bot
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: The revolutionary capabilities of large language models (LLMs) have paved the way for multimodal large language models (MLLMs) and fostered diverse applications across various specialized domains. In the remote sensing (RS) field, however, the diverse geographical landscapes and varied objects in RS imagery are not adequately considered in recent MLLM endeavors. To bridge this gap, we construct a large-scale RS image-text dataset, LHRS-Align, and an informative RS-specific instruction dataset, LHRS-Instruct, leveraging the extensive volunteered geographic information (VGI) and globally available RS images. Building on this foundation, we introduce LHRS-Bot, an MLLM tailored for RS image understanding through a novel multi-level vision-language alignment strategy and a curriculum learning method. Additionally, we introduce LHRS-Bench, a benchmark for thoroughly evaluating MLLMs' abilities in RS image understanding. Comprehensive experiments demonstrate that LHRS-Bot exhibits a profound understanding of RS images and the ability to perform nuanced reasoning within the RS domain.

### GeReA: Question-Aware Prompt Captions for Knowledge-based Visual Question Answering 
[[arxiv](https://arxiv.org/abs/2402.02503)] [[cool](https://papers.cool/arxiv/2402.02503)] [[pdf](https://arxiv.org/pdf/2402.02503)]
> **Authors**: Ziyu Ma,Shutao Li,Bin Sun,Jianfei Cai,Zuxiang Long,Fuyan Ma
> **First submission**: 2024-02-04
> **First announcement**: 2024-02-05
> **comment**: 17 pages
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算语言学
- **Abstract**: Knowledge-based visual question answering (VQA) requires world knowledge beyond the image for accurate answer. Recently, instead of extra knowledge bases, a large language model (LLM) like GPT-3 is activated as an implicit knowledge engine to jointly acquire and reason the necessary knowledge for answering by converting images into textual information (e.g., captions and answer candidates). However, such conversion may introduce irrelevant information, which causes the LLM to misinterpret images and ignore visual details crucial for accurate knowledge. We argue that multimodal large language model (MLLM) is a better implicit knowledge engine than the LLM for its superior capability of visual understanding. Despite this, how to activate the capacity of MLLM as the implicit knowledge engine has not been explored yet. Therefore, we propose GeReA, a generate-reason framework that prompts a MLLM like InstructBLIP with question relevant vision and language information to generate knowledge-relevant descriptions and reasons those descriptions for knowledge-based VQA. Specifically, the question-relevant image regions and question-specific manual prompts are encoded in the MLLM to generate the knowledge relevant descriptions, referred to as question-aware prompt captions. After that, the question-aware prompt captions, image-question pair, and similar samples are sent into the multi-modal reasoning model to learn a joint knowledge-image-question representation for answer prediction. GeReA unlocks the use of MLLM as the implicit knowledge engine, surpassing all previous state-of-the-art methods on OK-VQA and A-OKVQA datasets, with test accuracies of 66.5% and 63.3% respectively. Our code will be released at https://github.com/Upper9527/GeReA.

### AI-Generated Content Enhanced Computer-Aided Diagnosis Model for Thyroid Nodules: A ChatGPT-Style Assistant 
[[arxiv](https://arxiv.org/abs/2402.02401)] [[cool](https://papers.cool/arxiv/2402.02401)] [[pdf](https://arxiv.org/pdf/2402.02401)]
> **Authors**: Jincao Yao,Yunpeng Wang,Zhikai Lei,Kai Wang,Xiaoxian Li,Jianhua Zhou,Xiang Hao,Jiafei Shen,Zhenping Wang,Rongrong Ru,Yaqing Chen,Yahan Zhou,Chen Chen,Yanming Zhang,Ping Liang,Dong Xu
> **First submission**: 2024-02-04
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: An artificial intelligence-generated content-enhanced computer-aided diagnosis (AIGC-CAD) model, designated as ThyGPT, has been developed. This model, inspired by the architecture of ChatGPT, could assist radiologists in assessing the risk of thyroid nodules through semantic-level human-machine interaction. A dataset comprising 19,165 thyroid nodule ultrasound cases from Zhejiang Cancer Hospital was assembled to facilitate the training and validation of the model. After training, ThyGPT could automatically evaluate thyroid nodule and engage in effective communication with physicians through human-computer interaction. The performance of ThyGPT was rigorously quantified using established metrics such as the receiver operating characteristic (ROC) curve, area under the curve (AUC), sensitivity, and specificity. The empirical findings revealed that radiologists, when supplemented with ThyGPT, markedly surpassed the diagnostic acumen of their peers utilizing traditional methods as well as the performance of the model in isolation. These findings suggest that AIGC-CAD systems, exemplified by ThyGPT, hold the promise to fundamentally transform the diagnostic workflows of radiologists in forthcoming years.

### Closed-Loop Unsupervised Representation Disentanglement with $β$-VAE Distillation and Diffusion Probabilistic Feedback 
[[arxiv](https://arxiv.org/abs/2402.02346)] [[cool](https://papers.cool/arxiv/2402.02346)] [[pdf](https://arxiv.org/pdf/2402.02346)]
> **Authors**: Xin Jin,Bohan Li,BAAO Xie,Wenyao Zhang,Jinming Liu,Ziqiang Li,Tao Yang,Wenjun Zeng
> **First submission**: 2024-02-04
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Representation disentanglement may help AI fundamentally understand the real world and thus benefit both discrimination and generation tasks. It currently has at least three unresolved core issues: (i) heavy reliance on label annotation and synthetic data -- causing poor generalization on natural scenarios; (ii) heuristic/hand-craft disentangling constraints make it hard to adaptively achieve an optimal training trade-off; (iii) lacking reasonable evaluation metric, especially for the real label-free data. To address these challenges, we propose a \textbf{C}losed-\textbf{L}oop unsupervised representation \textbf{Dis}entanglement approach dubbed \textbf{CL-Dis}. Specifically, we use diffusion-based autoencoder (Diff-AE) as a backbone while resorting to $β$-VAE as a co-pilot to extract semantically disentangled representations. The strong generation ability of diffusion model and the good disentanglement ability of VAE model are complementary. To strengthen disentangling, VAE-latent distillation and diffusion-wise feedback are interconnected in a closed-loop system for a further mutual promotion. Then, a self-supervised \textbf{Navigation} strategy is introduced to identify interpretable semantic directions in the disentangled latent space. Finally, a new metric based on content tracking is designed to evaluate the disentanglement effect. Experiments demonstrate the superiority of CL-Dis on applications like real image manipulation and visual analysis.

### On the Exploitation of DCT-Traces in the Generative-AI Domain 
[[arxiv](https://arxiv.org/abs/2402.02209)] [[cool](https://papers.cool/arxiv/2402.02209)] [[pdf](https://arxiv.org/pdf/2402.02209)]
> **Authors**: Orazio Pontorno,Luca Guarnera,Sebastiano Battiato
> **First submission**: 2024-02-03
> **First announcement**: 2024-02-05
> **comment**: ef:2024 IEEE International Conference on Image Processing (ICIP)
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Deepfakes represent one of the toughest challenges in the world of Cybersecurity and Digital Forensics, especially considering the high-quality results obtained with recent generative AI-based solutions. Almost all generative models leave unique traces in synthetic data that, if analyzed and identified in detail, can be exploited to improve the generalization limitations of existing deepfake detectors. In this paper we analyzed deepfake images in the frequency domain generated by both GAN and Diffusion Model engines, examining in detail the underlying statistical distribution of Discrete Cosine Transform (DCT) coefficients. Recognizing that not all coefficients contribute equally to image detection, we hypothesize the existence of a unique ``discriminative fingerprint", embedded in specific combinations of coefficients. To identify them, Machine Learning classifiers were trained on various combinations of coefficients. In addition, the Explainable AI (XAI) LIME algorithm was used to search for intrinsic discriminative combinations of coefficients. Finally, we performed a robustness test to analyze the persistence of traces by applying JPEG compression. The experimental results reveal the existence of traces left by the generative models that are more discriminative and persistent at JPEG attacks. Code and dataset are available at https://github.com/opontorno/dcts_analysis_deepfakes.

### SynthCLIP: Are We Ready for a Fully Synthetic CLIP Training? 
[[arxiv](https://arxiv.org/abs/2402.01832)] [[cool](https://papers.cool/arxiv/2402.01832)] [[pdf](https://arxiv.org/pdf/2402.01832)]
> **Authors**: Hasan Abed Al Kader Hammoud,Hani Itani,Fabio Pizzati,Philip Torr,Adel Bibi,Bernard Ghanem
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: Under review
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: We present SynthCLIP, a CLIP model trained on entirely synthetic text-image pairs. Leveraging recent text-to-image (TTI) networks and large language models (LLM), we generate synthetic datasets of images and corresponding captions at scale, with no human intervention. In this work, we provide an analysis on CLIP models trained on synthetic data. We provide insights on the data generation strategy, number of samples required, scaling trends, and resulting properties. We also introduce SynthCI-30M, a purely synthetic dataset comprising 30 million captioned images. Our code, trained models, and data, are released as open source at https://github.com/hammoudhasan/SynthCLIP

### Spiking CenterNet: A Distillation-boosted Spiking Neural Network for Object Detection 
[[arxiv](https://arxiv.org/abs/2402.01287)] [[cool](https://papers.cool/arxiv/2402.01287)] [[pdf](https://arxiv.org/pdf/2402.01287)]
> **Authors**: Lennard Bodden,Franziska Schwaiger,Duc Bach Ha,Lars Kreuzberg,Sven Behnke
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: 8 pages, 5 figures. Accepted at IJCNN 2024
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习,神经和进化计算
- **Abstract**: In the era of AI at the edge, self-driving cars, and climate change, the need for energy-efficient, small, embedded AI is growing. Spiking Neural Networks (SNNs) are a promising approach to address this challenge, with their event-driven information flow and sparse activations. We propose Spiking CenterNet for object detection on event data. It combines an SNN CenterNet adaptation with an efficient M2U-Net-based decoder. Our model significantly outperforms comparable previous work on Prophesee's challenging GEN1 Automotive Detection Dataset while using less than half the energy. Distilling the knowledge of a non-spiking teacher into our SNN further increases performance. To the best of our knowledge, our work is the first approach that takes advantage of knowledge distillation in the field of spiking object detection.

## 计算机与社会(cs.CY:Computers and Society)

### (A)I Am Not a Lawyer, But...: Engaging Legal Experts towards Responsible LLM Policies for Legal Advice 
[[arxiv](https://arxiv.org/abs/2402.01864)] [[cool](https://papers.cool/arxiv/2402.01864)] [[pdf](https://arxiv.org/pdf/2402.01864)]
> **Authors**: Inyoung Cheong,King Xia,K. J. Kevin Feng,Quan Ze Chen,Amy X. Zhang
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: 14 pages
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: Large language models (LLMs) are increasingly capable of providing users with advice in a wide range of professional domains, including legal advice. However, relying on LLMs for legal queries raises concerns due to the significant expertise required and the potential real-world consequences of the advice. To explore \textit{when} and \textit{why} LLMs should or should not provide advice to users, we conducted workshops with 20 legal experts using methods inspired by case-based reasoning. The provided realistic queries ("cases") allowed experts to examine granular, situation-specific concerns and overarching technical and legal constraints, producing a concrete set of contextual considerations for LLM developers. By synthesizing the factors that impacted LLM response appropriateness, we present a 4-dimension framework: (1) User attributes and behaviors, (2) Nature of queries, (3) AI capabilities, and (4) Social impacts. We share experts' recommendations for LLM response strategies, which center around helping users identify `right questions to ask' and relevant information rather than providing definitive legal judgments. Our findings reveal novel legal considerations, such as unauthorized practice of law, confidentiality, and liability for inaccurate advice, that have been overlooked in the literature. The case-based deliberation method enabled us to elicit fine-grained, practice-informed insights that surpass those from de-contextualized surveys or speculative principles. These findings underscore the applicability of our method for translating domain-specific professional knowledge and practices into policies that can guide LLM behavior in a more responsible direction.

### The Political Preferences of LLMs 
[[arxiv](https://arxiv.org/abs/2402.01789)] [[cool](https://papers.cool/arxiv/2402.01789)] [[pdf](https://arxiv.org/pdf/2402.01789)]
> **Authors**: David Rozado
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,人工智能,计算语言学
- **Abstract**: I report here a comprehensive analysis about the political preferences embedded in Large Language Models (LLMs). Namely, I administer 11 political orientation tests, designed to identify the political preferences of the test taker, to 24 state-of-the-art conversational LLMs, both closed and open source. When probed with questions/statements with political connotations, most conversational LLMs tend to generate responses that are diagnosed by most political test instruments as manifesting preferences for left-of-center viewpoints. This does not appear to be the case for five additional base (i.e. foundation) models upon which LLMs optimized for conversation with humans are built. However, the weak performance of the base models at coherently answering the tests' questions makes this subset of results inconclusive. Finally, I demonstrate that LLMs can be steered towards specific locations in the political spectrum through Supervised Fine-Tuning (SFT) with only modest amounts of politically aligned data, suggesting SFT's potential to embed political orientation in LLMs. With LLMs beginning to partially displace traditional information sources like search engines and Wikipedia, the societal implications of political biases embedded in LLMs are substantial.

### Harm Amplification in Text-to-Image Models 
[[arxiv](https://arxiv.org/abs/2402.01787)] [[cool](https://papers.cool/arxiv/2402.01787)] [[pdf](https://arxiv.org/pdf/2402.01787)]
> **Authors**: Susan Hao,Renee Shelby,Yuchi Liu,Hansa Srinivasan,Mukul Bhutani,Burcu Karagol Ayan,Ryan Poplin,Shivani Poddar,Sarah Laszlo
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,人工智能,机器学习
- **Abstract**: Text-to-image (T2I) models have emerged as a significant advancement in generative AI; however, there exist safety concerns regarding their potential to produce harmful image outputs even when users input seemingly safe prompts. This phenomenon, where T2I models generate harmful representations that were not explicit in the input prompt, poses a potentially greater risk than adversarial prompts, leaving users unintentionally exposed to harms. Our paper addresses this issue by formalizing a definition for this phenomenon which we term harm amplification. We further contribute to the field by developing a framework of methodologies to quantify harm amplification in which we consider the harm of the model output in the context of user input. We then empirically examine how to apply these different methodologies to simulate real-world deployment scenarios including a quantification of disparate impacts across genders resulting from harm amplification. Together, our work aims to offer researchers tools to comprehensively address safety challenges in T2I systems and contribute to the responsible deployment of generative AI models.

### Extending Interactive Science Exhibits into the Classroom using Anthropomorphized Chatbots and Bloom's Taxonomy 
[[arxiv](https://arxiv.org/abs/2402.01770)] [[cool](https://papers.cool/arxiv/2402.01770)] [[pdf](https://arxiv.org/pdf/2402.01770)]
> **Authors**: Yousuf Golding
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: This study explores the use of Generative AI chatbots for transforming public science exhibits into virtual experiences that can extend the engagement of exhibits into the classroom. The broader goal is to increase accessibility of science exhibits, especially for those marginalized in STEM due to various factors, including cultural barriers. We hypothesize that turning exhibits into first-person anthropomorphized chatbots with a personality, like quirky-talking asteroids or comets, can increase engagement and learning. The paper mainly explores if such techniques are possible using Generative AI (e.g. GPT) via prompt engineering alone. The research includes an investigation into the possibility of integrating interactive assessment via question-generation using Bloom's Taxonomy. Initial results indicate that it is possible to combine these techniques. As such, it lays a foundation for future classroom evaluations of such chatbots to gauge their overall efficacy in extending the reach of science exhibitions. The paper concludes by discussing extensions of the research to fully evaluate effectiveness in virtual field-trips. We also include a brief examination of additional ways to enhance student motivation towards learning via chatbots.

### Commercial AI, Conflict, and Moral Responsibility: A theoretical analysis and practical approach to the moral responsibilities associated with dual-use AI technology 
[[arxiv](https://arxiv.org/abs/2402.01762)] [[cool](https://papers.cool/arxiv/2402.01762)] [[pdf](https://arxiv.org/pdf/2402.01762)]
> **Authors**: Daniel Trusilo,David Danks
> **First submission**: 2024-01-30
> **First announcement**: 2024-02-05
> **comment**: 9 pages
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: This paper presents a theoretical analysis and practical approach to the moral responsibilities when developing AI systems for non-military applications that may nonetheless be used for conflict applications. We argue that AI represents a form of crossover technology that is different from previous historical examples of dual- or multi-use technology as it has a multiplicative effect across other technologies. As a result, existing analyses of ethical responsibilities around dual-use technologies do not necessarily work for AI systems. We instead argue that stakeholders involved in the AI system lifecycle are morally responsible for uses of their systems that are reasonably foreseeable. The core idea is that an agent's moral responsibility for some action is not necessarily determined by their intentions alone; we must also consider what the agent could reasonably have foreseen to be potential outcomes of their action, such as the potential use of a system in conflict even when it is not designed for that. In particular, we contend that it is reasonably foreseeable that: (1) civilian AI systems will be applied to active conflict, including conflict support activities, (2) the use of civilian AI systems in conflict will impact applications of the law of armed conflict, and (3) crossover AI technology will be applied to conflicts that fall short of armed conflict. Given these reasonably foreseeably outcomes, we present three technically feasible actions that developers of civilian AIs can take to potentially mitigate their moral responsibility: (a) establishing systematic approaches to multi-perspective capability testing, (b) integrating digital watermarking in model weight matrices, and (c) utilizing monitoring and reporting mechanisms for conflict-related AI applications.

### Trust and ethical considerations in a multi-modal, explainable AI-driven chatbot tutoring system: The case of collaboratively solving Rubik's Cube 
[[arxiv](https://arxiv.org/abs/2402.01760)] [[cool](https://papers.cool/arxiv/2402.01760)] [[pdf](https://arxiv.org/pdf/2402.01760)]
> **Authors**: Kausik Lakkaraju,Vedant Khandelwal,Biplav Srivastava,Forest Agostinelli,Hengtao Tang,Prathamjeet Singh,Dezhi Wu,Matt Irvin,Ashish Kundu
> **First submission**: 2024-01-30
> **First announcement**: 2024-02-05
> **comment**: Accepted at 'Neural ConversationalAIWorkshop - What's left to TEACH (Trustworthy, Enhanced, Adaptable, Capable, and Human-centric) chatbots?' at ICML 2023
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: Artificial intelligence (AI) has the potential to transform education with its power of uncovering insights from massive data about student learning patterns. However, ethical and trustworthy concerns of AI have been raised but are unsolved. Prominent ethical issues in high school AI education include data privacy, information leakage, abusive language, and fairness. This paper describes technological components that were built to address ethical and trustworthy concerns in a multi-modal collaborative platform (called ALLURE chatbot) for high school students to collaborate with AI to solve the Rubik's cube. In data privacy, we want to ensure that the informed consent of children, parents, and teachers, is at the center of any data that is managed. Since children are involved, language, whether textual, audio, or visual, is acceptable both from users and AI and the system can steer interaction away from dangerous situations. In information management, we also want to ensure that the system, while learning to improve over time, does not leak information about users from one group to another.

### Aalap: AI Assistant for Legal & Paralegal Functions in India 
[[arxiv](https://arxiv.org/abs/2402.01758)] [[cool](https://papers.cool/arxiv/2402.01758)] [[pdf](https://arxiv.org/pdf/2402.01758)]
> **Authors**: Aman Tiwari,Prathamesh Kalamkar,Atreyo Banerjee,Saurabh Karn,Varun Hemachandran,Smita Gupta
> **First submission**: 2024-01-30
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,人工智能,计算语言学
- **Abstract**: Using proprietary Large Language Models on legal tasks poses challenges due to data privacy issues, domain data heterogeneity, domain knowledge sophistication, and domain objectives uniqueness. We created Aalalp, a fine-tuned Mistral 7B model on instructions data related to specific Indian legal tasks. The performance of Aalap is better than gpt-3.5-turbo in 31\% of our test data and obtains an equivalent score in 34\% of the test data as evaluated by GPT4. Training Aalap mainly focuses on teaching legal reasoning rather than legal recall. Aalap is definitely helpful for the day-to-day activities of lawyers, judges, or anyone working in legal systems.

### 3DG: A Framework for Using Generative AI for Handling Sparse Learner Performance Data From Intelligent Tutoring Systems 
[[arxiv](https://arxiv.org/abs/2402.01746)] [[cool](https://papers.cool/arxiv/2402.01746)] [[pdf](https://arxiv.org/pdf/2402.01746)]
> **Authors**: Liang Zhang,Jionghao Lin,Conrad Borchers,Meng Cao,Xiangen Hu
> **First submission**: 2024-01-29
> **First announcement**: 2024-02-05
> **comment**: ef:LAK 2024: International Workshop on GenerativeAIfor Learning Analytics (GenAI-LA)
- **标题**: None
- **领域**: 计算机与社会,人工智能,机器学习
- **Abstract**: Learning performance data (e.g., quiz scores and attempts) is significant for understanding learner engagement and knowledge mastery level. However, the learning performance data collected from Intelligent Tutoring Systems (ITSs) often suffers from sparsity, impacting the accuracy of learner modeling and knowledge assessments. To address this, we introduce the 3DG framework (3-Dimensional tensor for Densification and Generation), a novel approach combining tensor factorization with advanced generative models, including Generative Adversarial Network (GAN) and Generative Pre-trained Transformer (GPT), for enhanced data imputation and augmentation. The framework operates by first representing the data as a three-dimensional tensor, capturing dimensions of learners, questions, and attempts. It then densifies the data through tensor factorization and augments it using Generative AI models, tailored to individual learning patterns identified via clustering. Applied to data from an AutoTutor lesson by the Center for the Study of Adult Literacy (CSAL), the 3DG framework effectively generated scalable, personalized simulations of learning performance. Comparative analysis revealed GAN's superior reliability over GPT-4 in this context, underscoring its potential in addressing data sparsity challenges in ITSs and contributing to the advancement of personalized educational technology.

### The Reasoning Under Uncertainty Trap: A Structural AI Risk 
[[arxiv](https://arxiv.org/abs/2402.01743)] [[cool](https://papers.cool/arxiv/2402.01743)] [[pdf](https://arxiv.org/pdf/2402.01743)]
> **Authors**: Toby D. Pilditch
> **First submission**: 2024-01-29
> **First announcement**: 2024-02-05
> **comment**: 51 pages (excluding references), 7 chapters, 9 figures
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: This report examines a novel risk associated with current (and projected) AI tools. Making effective decisions about future actions requires us to reason under uncertainty (RUU), and doing so is essential to many critical real world problems. Overfaced by this challenge, there is growing demand for AI tools like LLMs to assist decision-makers. Having evidenced this demand and the incentives behind it, we expose a growing risk: we 1) do not currently sufficiently understand LLM capabilities in this regard, and 2) have no guarantees of performance given fundamental computational explosiveness and deep uncertainty constraints on accuracy. This report provides an exposition of what makes RUU so challenging for both humans and machines, and relates these difficulties to prospective AI timelines and capabilities. Having established this current potential misuse risk, we go on to expose how this seemingly additive risk (more misuse additively contributed to potential harm) in fact has multiplicative properties. Specifically, we detail how this misuse risk connects to a wider network of underlying structural risks (e.g., shifting incentives, limited transparency, and feedback loops) to produce non-linear harms. We go on to provide a solutions roadmap that targets multiple leverage points in the structure of the problem. This includes recommendations for all involved actors (prospective users, developers, and policy-makers) and enfolds insights from areas including Decision-making Under Deep Uncertainty and complex systems theory. We argue this report serves not only to raise awareness (and subsequently mitigate/correct) of a current, novel AI risk, but also awareness of the underlying class of structural risks by illustrating how their interconnected nature poses twin-dangers of camouflaging their presence, whilst amplifying their potential effects.

### Identifying and Improving Disability Bias in GPT-Based Resume Screening 
[[arxiv](https://arxiv.org/abs/2402.01732)] [[cool](https://papers.cool/arxiv/2402.01732)] [[pdf](https://arxiv.org/pdf/2402.01732)]
> **Authors**: Kate Glazko,Yusuf Mohammed,Ben Kosa,Venkatesh Potluri,Jennifer Mankoff
> **First submission**: 2024-01-28
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: As Generative AI rises in adoption, its use has expanded to include domains such as hiring and recruiting. However, without examining the potential of bias, this may negatively impact marginalized populations, including people with disabilities. To address this important concern, we present a resume audit study, in which we ask ChatGPT (specifically, GPT-4) to rank a resume against the same resume enhanced with an additional leadership award, scholarship, panel presentation, and membership that are disability related. We find that GPT-4 exhibits prejudice towards these enhanced CVs. Further, we show that this prejudice can be quantifiably reduced by training a custom GPTs on principles of DEI and disability justice. Our study also includes a unique qualitative analysis of the types of direct and indirect ableism GPT-4 uses to justify its biased decisions and suggest directions for additional bias mitigation work. Additionally, since these justifications are presumably drawn from training data containing real-world biased statements made by humans, our analysis suggests additional avenues for understanding and addressing human bias.

### Prompting Diverse Ideas: Increasing AI Idea Variance 
[[arxiv](https://arxiv.org/abs/2402.01727)] [[cool](https://papers.cool/arxiv/2402.01727)] [[pdf](https://arxiv.org/pdf/2402.01727)]
> **Authors**: Lennart Meincke,Ethan R. Mollick,Christian Terwiesch
> **First submission**: 2024-01-27
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: Unlike routine tasks where consistency is prized, in creativity and innovation the goal is to create a diverse set of ideas. This paper delves into the burgeoning interest in employing Artificial Intelligence (AI) to enhance the productivity and quality of the idea generation process. While previous studies have found that the average quality of AI ideas is quite high, prior research also has pointed to the inability of AI-based brainstorming to create sufficient dispersion of ideas, which limits novelty and the quality of the overall best idea. Our research investigates methods to increase the dispersion in AI-generated ideas. Using GPT-4, we explore the effect of different prompting methods on Cosine Similarity, the number of unique ideas, and the speed with which the idea space gets exhausted. We do this in the domain of developing a new product development for college students, priced under $50. In this context, we find that (1) pools of ideas generated by GPT-4 with various plausible prompts are less diverse than ideas generated by groups of human subjects (2) the diversity of AI generated ideas can be substantially improved using prompt engineering (3) Chain-of-Thought (CoT) prompting leads to the highest diversity of ideas of all prompts we evaluated and was able to come close to what is achieved by groups of human subjects. It also was capable of generating the highest number of unique ideas of any prompt we studied.

### Deep Learning Based Amharic Chatbot for FAQs in Universities 
[[arxiv](https://arxiv.org/abs/2402.01720)] [[cool](https://papers.cool/arxiv/2402.01720)] [[pdf](https://arxiv.org/pdf/2402.01720)]
> **Authors**: Goitom Ybrah Hailu,Hadush Hailu,Shishay Welay
> **First submission**: 2024-01-26
> **First announcement**: 2024-02-05
> **comment**: mber:AksumUniv-CS-2024
- **标题**: None
- **领域**: 计算机与社会,人工智能,计算语言学,机器学习
- **Abstract**: University students often spend a considerable amount of time seeking answers to common questions from administrators or teachers. This can become tedious for both parties, leading to a need for a solution. In response, this paper proposes a chatbot model that utilizes natural language processing and deep learning techniques to answer frequently asked questions (FAQs) in the Amharic language. Chatbots are computer programs that simulate human conversation through the use of artificial intelligence (AI), acting as a virtual assistant to handle questions and other tasks. The proposed chatbot program employs tokenization, normalization, stop word removal, and stemming to analyze and categorize Amharic input sentences. Three machine learning model algorithms were used to classify tokens and retrieve appropriate responses: Support Vector Machine (SVM), Multinomial Naïve Bayes, and deep neural networks implemented through TensorFlow, Keras, and NLTK. The deep learning model achieved the best results with 91.55% accuracy and a validation loss of 0.3548 using an Adam optimizer and SoftMax activation function. The chatbot model was integrated with Facebook Messenger and deployed on a Heroku server for 24-hour accessibility. The experimental results demonstrate that the chatbot framework achieved its objectives and effectively addressed challenges such as Amharic Fidel variation, morphological variation, and lexical gaps. Future research could explore the integration of Amharic WordNet to narrow the lexical gap and support more complex questions.

### LLM on FHIR -- Demystifying Health Records 
[[arxiv](https://arxiv.org/abs/2402.01711)] [[cool](https://papers.cool/arxiv/2402.01711)] [[pdf](https://arxiv.org/pdf/2402.01711)]
> **Authors**: Paul Schmiedmayer,Adrit Rao,Philipp Zagar,Vishnu Ravi,Aydin Zahedivash,Arash Fereydooni,Oliver Aalami
> **First submission**: 2024-01-25
> **First announcement**: 2024-02-05
> **comment**: Pre-print of the paper submitted to the Call for Papers for the Special Focus Issue on ChatGPT and Large Language Models (LLMs) in Biomedicine and Health at the Journal of the American Medical Informatics Association: https://academic.oup.com/jamia/pages/call-for-papers-for-special-focus-issue
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: Objective: To enhance health literacy and accessibility of health information for a diverse patient population by developing a patient-centered artificial intelligence (AI) solution using large language models (LLMs) and Fast Healthcare Interoperability Resources (FHIR) application programming interfaces (APIs). Materials and Methods: The research involved developing LLM on FHIR, an open-source mobile application allowing users to interact with their health records using LLMs. The app is built on Stanford's Spezi ecosystem and uses OpenAI's GPT-4. A pilot study was conducted with the SyntheticMass patient dataset and evaluated by medical experts to assess the app's effectiveness in increasing health literacy. The evaluation focused on the accuracy, relevance, and understandability of the LLM's responses to common patient questions. Results: LLM on FHIR demonstrated varying but generally high degrees of accuracy and relevance in providing understandable health information to patients. The app effectively translated medical data into patient-friendly language and was able to adapt its responses to different patient profiles. However, challenges included variability in LLM responses and the need for precise filtering of health data. Discussion and Conclusion: LLMs offer significant potential in improving health literacy and making health records more accessible. LLM on FHIR, as a pioneering application in this field, demonstrates the feasibility and challenges of integrating LLMs into patient care. While promising, the implementation and pilot also highlight risks such as inconsistent responses and the importance of replicable output. Future directions include better resource identification mechanisms and executing LLMs on-device to enhance privacy and reduce costs.

### Investigating Algorithm Review Boards for Organizational Responsible Artificial Intelligence Governance 
[[arxiv](https://arxiv.org/abs/2402.01691)] [[cool](https://papers.cool/arxiv/2402.01691)] [[pdf](https://arxiv.org/pdf/2402.01691)]
> **Authors**: Emily Hadley,Alan Blatecky,Megan Comfort
> **First submission**: 2024-01-23
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: Organizations including companies, nonprofits, governments, and academic institutions are increasingly developing, deploying, and utilizing artificial intelligence (AI) tools. Responsible AI (RAI) governance approaches at organizations have emerged as important mechanisms to address potential AI risks and harms. In this work, we interviewed 17 technical contributors across organization types (Academic, Government, Industry, Nonprofit) and sectors (Finance, Health, Tech, Other) about their experiences with internal RAI governance. Our findings illuminated the variety of organizational definitions of RAI and accompanying internal governance approaches. We summarized the first detailed findings on algorithm review boards (ARBs) and similar review committees in practice, including their membership, scope, and measures of success. We confirmed known robust model governance in finance sectors and revealed extensive algorithm and AI governance with ARB-like review boards in health sectors. Our findings contradict the idea that Institutional Review Boards alone are sufficient for algorithm governance and posit that ARBs are among the more impactful internal RAI governance approaches. Our results suggest that integration with existing internal regulatory approaches and leadership buy-in are among the most important attributes for success and that financial tensions are the greatest challenge to effective organizational RAI. We make a variety of suggestions for how organizational partners can learn from these findings when building their own internal RAI frameworks. We outline future directions for developing and measuring effectiveness of ARBs and other internal RAI governance approaches.

### An Online Hierarchical Energy Management System for Energy Communities, Complying with the Current Technical Legislation Framework 
[[arxiv](https://arxiv.org/abs/2402.01688)] [[cool](https://papers.cool/arxiv/2402.01688)] [[pdf](https://arxiv.org/pdf/2402.01688)]
> **Authors**: Antonino Capillo,Enrico De Santis,Fabio Massimo Frattale Mascioli,Antonello Rizzi
> **First submission**: 2024-01-22
> **First announcement**: 2024-02-05
> **comment**: 26 pages, 18 figures
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: Efforts in the fight against Climate Change are increasingly oriented towards new energy efficiency strategies in Smart Grids (SGs). In 2018, with proper legislation, the European Union (EU) defined the Renewable Energy Community (REC) as a local electrical grid whose participants share their self-produced renewable energy, aiming at reducing bill costs by taking advantage of proper incentives. That action aspires to accelerate the spread of local renewable energy exploitation, whose costs could not be within everyone's reach. Since a REC is technically an SG, the strategies above can be applied, and specifically, practical Energy Management Systems (EMSs) are required. Therefore, in this work, an online Hierarchical EMS (HEMS) is synthesized for REC cost minimization to evaluate its superiority over a local self-consumption approach. EU technical indications (as inherited from Italy) are diligently followed, aiming for results that are as realistic as possible. Power flows between REC nodes, or Microgrids (MGs) are optimized by taking Energy Storage Systems (ESSs) and PV plant costs, energy purchase costs, and REC incentives. A hybrid Fuzzy Inference System - Genetic Algorithm (FIS-GA) model is implemented with the GA encoding the FIS parameters. Power generation and consumption, which are the overall system input, are predicted by a LSTM trained on historical data. The proposed hierarchical model achieves good precision in short computation times and outperforms the self-consumption approach, leading to about 20% savings compared to the latter. In addition, the Explainable AI (XAI), which characterizes the model through the FIS, makes results more reliable thanks to an excellent human interpretation level. To finish, the HEMS is parametrized so that it is straightforward to switch to another Country's technical legislation framework.

### "Which LLM should I use?": Evaluating LLMs for tasks performed by Undergraduate Computer Science Students 
[[arxiv](https://arxiv.org/abs/2402.01687)] [[cool](https://papers.cool/arxiv/2402.01687)] [[pdf](https://arxiv.org/pdf/2402.01687)]
> **Authors**: Vibhor Agarwal,Madhav Krishan Garg,Sahiti Dharmavaram,Dhruv Kumar
> **First submission**: 2024-01-22
> **First announcement**: 2024-02-05
> **comment**: Under review
- **标题**: None
- **领域**: 计算机与社会,人机交互,机器学习
- **Abstract**: This study evaluates the effectiveness of various large language models (LLMs) in performing tasks common among undergraduate computer science students. Although a number of research studies in the computing education community have explored the possibility of using LLMs for a variety of tasks, there is a lack of comprehensive research comparing different LLMs and evaluating which LLMs are most effective for different tasks. Our research systematically assesses some of the publicly available LLMs such as Google Bard, ChatGPT(3.5), GitHub Copilot Chat, and Microsoft Copilot across diverse tasks commonly encountered by undergraduate computer science students in India. These tasks include code explanation and documentation, solving class assignments, technical interview preparation, learning new concepts and frameworks, and email writing. Evaluation for these tasks was carried out by pre-final year and final year undergraduate computer science students and provides insights into the models' strengths and limitations. This study aims to guide students as well as instructors in selecting suitable LLMs for any specific task and offers valuable insights on how LLMs can be used constructively by students and instructors.

### Determining the Difficulties of Students With Dyslexia via Virtual Reality and Artificial Intelligence: An Exploratory Analysis 
[[arxiv](https://arxiv.org/abs/2402.01668)] [[cool](https://papers.cool/arxiv/2402.01668)] [[pdf](https://arxiv.org/pdf/2402.01668)]
> **Authors**: Enrique Yeguas-Bolívar,José M. Alcalde-Llergo,Pilar Aparicio-Martínez,Juri Taborri,Andrea Zingoni,Sara Pinzi
> **First submission**: 2024-01-15
> **First announcement**: 2024-02-05
> **comment**: 7 pages, 5 figures, 3 tables, MetroXRAINE 2022 Conference, VRAILEXIA european project
- **标题**: None
- **领域**: 计算机与社会,人工智能,计算机视觉和模式识别,图形,人机交互
- **Abstract**: Learning disorders are neurological conditions that affect the brain's ability to interconnect communication areas. Dyslexic students experience problems with reading, memorizing, and exposing concepts; however the magnitude of these can be mitigated through both therapies and the creation of compensatory mechanisms. Several efforts have been made to mitigate these issues, leading to the creation of digital resources for students with specific learning disorders attending primary and secondary education levels. Conversely, a standard approach is still missed in higher education. The VRAIlexia project has been created to tackle this issue by proposing two different tools: a mobile application integrating virtual reality (VR) to collect data quickly and easily, and an artificial intelligencebased software (AI) to analyze the collected data for customizing the supporting methodology for each student. The first one has been created and is being distributed among dyslexic students in Higher Education Institutions, for the conduction of specific psychological and psychometric tests. The second tool applies specific artificial intelligence algorithms to the data gathered via the application and other surveys. These AI techniques have allowed us to identify the most relevant difficulties faced by the students' cohort. Our different models have obtained around 90\% mean accuracy for predicting the support tools and learning strategies.

### Killer Apps: Low-Speed, Large-Scale AI Weapons 
[[arxiv](https://arxiv.org/abs/2402.01663)] [[cool](https://papers.cool/arxiv/2402.01663)] [[pdf](https://arxiv.org/pdf/2402.01663)]
> **Authors**: Philip Feldman,Aaron Dant,James R. Foulds
> **First submission**: 2024-01-14
> **First announcement**: 2024-02-05
> **comment**: 10 pages with 10 pages of appendices. 3 Figures, 2 code listings
- **标题**: None
- **领域**: 计算机与社会,密码学和安全,机器学习
- **Abstract**: The accelerating advancements in Artificial Intelligence (AI) and Machine Learning (ML), highlighted by the development of cutting-edge Generative Pre-trained Transformer (GPT) models by organizations such as OpenAI, Meta, and Anthropic, present new challenges and opportunities in warfare and security. Much of the current focus is on AI's integration within weapons systems and its role in rapid decision-making in kinetic conflict. However, an equally important but often overlooked aspect is the potential of AI-based psychological manipulation at internet scales within the information domain. These capabilities could pose significant threats to individuals, organizations, and societies globally. This paper explores the concept of AI weapons, their deployment, detection, and potential countermeasures.

### Generative Ghosts: Anticipating Benefits and Risks of AI Afterlives 
[[arxiv](https://arxiv.org/abs/2402.01662)] [[cool](https://papers.cool/arxiv/2402.01662)] [[pdf](https://arxiv.org/pdf/2402.01662)]
> **Authors**: Meredith Ringel Morris,Jed R. Brubaker
> **First submission**: 2024-01-14
> **First announcement**: 2024-02-05
> **comment**: version 4, updated to include new references and examples
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: As AI systems quickly improve in both breadth and depth of performance, they lend themselves to creating increasingly powerful and realistic agents, including the possibility of agents modeled on specific people. We anticipate that within our lifetimes it may become common practice for people to create custom AI agents to interact with loved ones and/or the broader world after death; indeed, the past year has seen a boom in startups purporting to offer such services. We call these generative ghosts, since such agents will be capable of generating novel content rather than merely parroting content produced by their creator while living. In this paper, we reflect on the history of technologies for AI afterlives, including current early attempts by individual enthusiasts and startup companies to create generative ghosts. We then introduce a novel design space detailing potential implementations of generative ghosts, and use this analytic framework to ground discussion of the practical and ethical implications of various approaches to designing generative ghosts, including potential positive and negative impacts on individuals and society. Based on these considerations, we lay out a research agenda for the AI and HCI research communities to better understand the risk/benefit landscape of this novel technology so as to ultimately empower people who wish to create and interact with AI afterlives to do so in a beneficial manner.

### Promises and pitfalls of artificial intelligence for legal applications 
[[arxiv](https://arxiv.org/abs/2402.01656)] [[cool](https://papers.cool/arxiv/2402.01656)] [[pdf](https://arxiv.org/pdf/2402.01656)]
> **Authors**: Sayash Kapoor,Peter Henderson,Arvind Narayanan
> **First submission**: 2024-01-10
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: Is AI set to redefine the legal profession? We argue that this claim is not supported by the current evidence. We dive into AI's increasingly prevalent roles in three types of legal tasks: information processing; tasks involving creativity, reasoning, or judgment; and predictions about the future. We find that the ease of evaluating legal applications varies greatly across legal tasks, based on the ease of identifying correct answers and the observability of information relevant to the task at hand. Tasks that would lead to the most significant changes to the legal profession are also the ones most prone to overoptimism about AI capabilities, as they are harder to evaluate. We make recommendations for better evaluation and deployment of AI in legal contexts.

### User-Centric AI Analytics for Chronic Health Conditions Management 
[[arxiv](https://arxiv.org/abs/2402.01652)] [[cool](https://papers.cool/arxiv/2402.01652)] [[pdf](https://arxiv.org/pdf/2402.01652)]
> **Authors**: Aladdin Ayesh
> **First submission**: 2024-01-09
> **First announcement**: 2024-02-05
> **comment**: Keynote talk at IEEE Conference on Intelligent Methods, Systems, and Applications (IMSA), Cairo, Egypt, July 2023
- **标题**: None
- **领域**: 计算机与社会,人工智能,人机交互,机器学习
- **Abstract**: The use of AI analytics in health informatics has seen a rapid growth in recent years. In this talk, we look at AI analytics use in managing chronic health conditions such as diabetes, obesity, etc. We focus on the challenges in managing these conditions especially with drug-free approaches due to the variations in individual circumstances. These variations directed the research into user-centric approach leading to variety of research questions. In this short paper, we give examples from recent and current research work and conclude with what, in our opinion, to be the next steps and some remaining open research questions.

### Informed AI Regulation: Comparing the Ethical Frameworks of Leading LLM Chatbots Using an Ethics-Based Audit to Assess Moral Reasoning and Normative Values 
[[arxiv](https://arxiv.org/abs/2402.01651)] [[cool](https://papers.cool/arxiv/2402.01651)] [[pdf](https://arxiv.org/pdf/2402.01651)]
> **Authors**: Jon Chun,Katherine Elkins
> **First submission**: 2024-01-09
> **First announcement**: 2024-02-05
> **comment**: 23 pages, 6 figures (3 as tables), 1 table (in LaTeX)
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: With the rise of individual and collaborative networks of autonomous agents, AI is deployed in more key reasoning and decision-making roles. For this reason, ethics-based audits play a pivotal role in the rapidly growing fields of AI safety and regulation. This paper undertakes an ethics-based audit to probe the 8 leading commercial and open-source Large Language Models including GPT-4. We assess explicability and trustworthiness by a) establishing how well different models engage in moral reasoning and b) comparing normative values underlying models as ethical frameworks. We employ an experimental, evidence-based approach that challenges the models with ethical dilemmas in order to probe human-AI alignment. The ethical scenarios are designed to require a decision in which the particulars of the situation may or may not necessitate deviating from normative ethical principles. A sophisticated ethical framework was consistently elicited in one model, GPT-4. Nonetheless, troubling findings include underlying normative frameworks with clear bias towards particular cultural norms. Many models also exhibit disturbing authoritarian tendencies. Code is available at https://github.com/jonchun/llm-sota-chatbots-ethics-based-audit.

### Build Your Own Robot Friend: An Open-Source Learning Module for Accessible and Engaging AI Education 
[[arxiv](https://arxiv.org/abs/2402.01647)] [[cool](https://papers.cool/arxiv/2402.01647)] [[pdf](https://arxiv.org/pdf/2402.01647)]
> **Authors**: Zhonghao Shi,Allison O'Connell,Zongjian Li,Siqi Liu,Jennifer Ayissi,Guy Hoffman,Mohammad Soleymani,Maja J. Matarić
> **First submission**: 2024-01-06
> **First announcement**: 2024-02-05
> **comment**: Accepted to the Proceedings of the AAAI Conference on Artificial Intelligence (2024)
- **标题**: None
- **领域**: 计算机与社会,人工智能,人机交互,机器学习,机器人技术
- **Abstract**: As artificial intelligence (AI) is playing an increasingly important role in our society and global economy, AI education and literacy have become necessary components in college and K-12 education to prepare students for an AI-powered society. However, current AI curricula have not yet been made accessible and engaging enough for students and schools from all socio-economic backgrounds with different educational goals. In this work, we developed an open-source learning module for college and high school students, which allows students to build their own robot companion from the ground up. This open platform can be used to provide hands-on experience and introductory knowledge about various aspects of AI, including robotics, machine learning (ML), software engineering, and mechanical engineering. Because of the social and personal nature of a socially assistive robot companion, this module also puts a special emphasis on human-centered AI, enabling students to develop a better understanding of human-AI interaction and AI ethics through hands-on learning activities. With open-source documentation, assembling manuals and affordable materials, students from different socio-economic backgrounds can personalize their learning experience based on their individual educational goals. To evaluate the student-perceived quality of our module, we conducted a usability testing workshop with 15 college students recruited from a minority-serving institution. Our results indicate that our AI module is effective, easy-to-follow, and engaging, and it increases student interest in studying AI/ML and robotics in the future. We hope that this work will contribute toward accessible and engaging AI education in human-AI interaction for college and high school students.

### Generative AI for Education (GAIED): Advances, Opportunities, and Challenges 
[[arxiv](https://arxiv.org/abs/2402.01580)] [[cool](https://papers.cool/arxiv/2402.01580)] [[pdf](https://arxiv.org/pdf/2402.01580)]
> **Authors**: Paul Denny,Sumit Gulwani,Neil T. Heffernan,Tanja Käser,Steven Moore,Anna N. Rafferty,Adish Singla
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: This survey article has grown out of the GAIED (pronounced "guide") workshop organized by the authors at the NeurIPS 2023 conference. We organized the GAIED workshop as part of a community-building effort to bring together researchers, educators, and practitioners to explore the potential of generative AI for enhancing education. This article aims to provide an overview of the workshop activities and highlight several future research directions in the area of GAIED.

## 数据库(cs.DB:Databases)

### LLM-Enhanced Data Management 
[[arxiv](https://arxiv.org/abs/2402.02643)] [[cool](https://papers.cool/arxiv/2402.02643)] [[pdf](https://arxiv.org/pdf/2402.02643)]
> **Authors**: Xuanhe Zhou,Xinyang Zhao,Guoliang Li
> **First submission**: 2024-02-04
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 数据库,人工智能,计算语言学,机器学习
- **Abstract**: Machine learning (ML) techniques for optimizing data management problems have been extensively studied and widely deployed in recent five years. However traditional ML methods have limitations on generalizability (adapting to different scenarios) and inference ability (understanding the context). Fortunately, large language models (LLMs) have shown high generalizability and human-competitive abilities in understanding context, which are promising for data management tasks (e.g., database diagnosis, database tuning). However, existing LLMs have several limitations: hallucination, high cost, and low accuracy for complicated tasks. To address these challenges, we design LLMDB, an LLM-enhanced data management paradigm which has generalizability and high inference ability while avoiding hallucination, reducing LLM cost, and achieving high accuracy. LLMDB embeds domain-specific knowledge to avoid hallucination by LLM fine-tuning and prompt engineering. LLMDB reduces the high cost of LLMs by vector databases which provide semantic search and caching abilities. LLMDB improves the task accuracy by LLM agent which provides multiple-round inference and pipeline executions. We showcase three real-world scenarios that LLMDB can well support, including query rewrite, database diagnosis and data analytics. We also summarize the open research challenges of LLMDB.

### When Large Language Models Meet Vector Databases: A Survey 
[[arxiv](https://arxiv.org/abs/2402.01763)] [[cool](https://papers.cool/arxiv/2402.01763)] [[pdf](https://arxiv.org/pdf/2402.01763)]
> **Authors**: Zhi Jing,Yongye Su,Yikun Han
> **First submission**: 2024-01-30
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 数据库,人工智能,计算语言学,机器学习
- **Abstract**: This survey explores the synergistic potential of Large Language Models (LLMs) and Vector Databases (VecDBs), a burgeoning but rapidly evolving research area. With the proliferation of LLMs comes a host of challenges, including hallucinations, outdated knowledge, prohibitive commercial application costs, and memory issues. VecDBs emerge as a compelling solution to these issues by offering an efficient means to store, retrieve, and manage the high-dimensional vector representations intrinsic to LLM operations. Through this nuanced review, we delineate the foundational principles of LLMs and VecDBs and critically analyze their integration's impact on enhancing LLM functionalities. This discourse extends into a discussion on the speculative future developments in this domain, aiming to catalyze further research into optimizing the confluence of LLMs and VecDBs for advanced data handling and knowledge extraction capabilities.

## 分布式、并行和集群计算(cs.DC:Distributed, Parallel, and Cluster Computing)

### Device Scheduling and Assignment in Hierarchical Federated Learning for Internet of Things 
[[arxiv](https://arxiv.org/abs/2402.02506)] [[cool](https://papers.cool/arxiv/2402.02506)] [[pdf](https://arxiv.org/pdf/2402.02506)]
> **Authors**: Tinghao Zhang,Kwok-Yan Lam,Jun Zhao
> **First submission**: 2024-02-04
> **First announcement**: 2024-02-05
> **comment**: Published in IEEE Internet of Things Journal (IoT-J)
- **标题**: None
- **领域**: 分布式、并行和集群计算,机器学习
- **Abstract**: Federated Learning (FL) is a promising machine learning approach for Internet of Things (IoT), but it has to address network congestion problems when the population of IoT devices grows. Hierarchical FL (HFL) alleviates this issue by distributing model aggregation to multiple edge servers. Nevertheless, the challenge of communication overhead remains, especially in scenarios where all IoT devices simultaneously join the training process. For scalability, practical HFL schemes select a subset of IoT devices to participate in the training, hence the notion of device scheduling. In this setting, only selected IoT devices are scheduled to participate in the global training, with each of them being assigned to one edge server. Existing HFL assignment methods are primarily based on search mechanisms, which suffer from high latency in finding the optimal assignment. This paper proposes an improved K-Center algorithm for device scheduling and introduces a deep reinforcement learning-based approach for assigning IoT devices to edge servers. Experiments show that scheduling 50% of IoT devices is generally adequate for achieving convergence in HFL with much lower time delay and energy consumption. In cases where reduction in energy consumption (such as in Green AI) and reduction of messages (to avoid burst traffic) are key objectives, scheduling 30% IoT devices allows a substantial reduction in energy and messages with similar model accuracy.

## 人机交互(cs.HC:Human-Computer Interaction)

### Vi(E)va LLM! A Conceptual Stack for Evaluating and Interpreting Generative AI-based Visualizations 
[[arxiv](https://arxiv.org/abs/2402.02167)] [[cool](https://papers.cool/arxiv/2402.02167)] [[pdf](https://arxiv.org/pdf/2402.02167)]
> **Authors**: Luca Podo,Muhammad Ishmal,Marco Angelini
> **First submission**: 2024-02-03
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 人机交互,人工智能,机器学习
- **Abstract**: The automatic generation of visualizations is an old task that, through the years, has shown more and more interest from the research and practitioner communities. Recently, large language models (LLM) have become an interesting option for supporting generative tasks related to visualization, demonstrating initial promising results. At the same time, several pitfalls, like the multiple ways of instructing an LLM to generate the desired result, the different perspectives leading the generation (code-based, image-based, grammar-based), and the presence of hallucinations even for the visualization generation task, make their usage less affordable than expected. Following similar initiatives for benchmarking LLMs, this paper copes with the problem of modeling the evaluation of a generated visualization through an LLM. We propose a theoretical evaluation stack, EvaLLM, that decomposes the evaluation effort in its atomic components, characterizes their nature, and provides an overview of how to implement and interpret them. We also designed and implemented an evaluation platform that provides a benchmarking resource for the visualization generation task. The platform supports automatic and manual scoring conducted by multiple assessors to support a fine-grained and semantic evaluation based on the EvaLLM stack. Two case studies on GPT3.5-turbo with Code Interpreter and Llama2-70-b models show the benefits of EvaLLM and illustrate interesting results on the current state-of-the-art LLM-generated visualizations.

### Human-Centered Privacy Research in the Age of Large Language Models 
[[arxiv](https://arxiv.org/abs/2402.01994)] [[cool](https://papers.cool/arxiv/2402.01994)] [[pdf](https://arxiv.org/pdf/2402.01994)]
> **Authors**: Tianshi Li,Sauvik Das,Hao-Ping Lee,Dakuo Wang,Bingsheng Yao,Zhiping Zhang
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: 4 pages, CHI EA'24
- **标题**: None
- **领域**: 人机交互,人工智能,密码学和安全
- **Abstract**: The emergence of large language models (LLMs), and their increased use in user-facing systems, has led to substantial privacy concerns. To date, research on these privacy concerns has been model-centered: exploring how LLMs lead to privacy risks like memorization, or can be used to infer personal characteristics about people from their content. We argue that there is a need for more research focusing on the human aspect of these privacy issues: e.g., research on how design paradigms for LLMs affect users' disclosure behaviors, users' mental models and preferences for privacy controls, and the design of tools, systems, and artifacts that empower end-users to reclaim ownership over their personal data. To build usable, efficient, and privacy-friendly systems powered by these models with imperfect privacy properties, our goal is to initiate discussions to outline an agenda for conducting human-centered research on privacy issues in LLM-powered systems. This Special Interest Group (SIG) aims to bring together researchers with backgrounds in usable security and privacy, human-AI collaboration, NLP, or any other related domains to share their perspectives and experiences on this problem, to help our community establish a collective understanding of the challenges, research opportunities, research methods, and strategies to collaborate with researchers outside of HCI.

### Homogenization Effects of Large Language Models on Human Creative Ideation 
[[arxiv](https://arxiv.org/abs/2402.01536)] [[cool](https://papers.cool/arxiv/2402.01536)] [[pdf](https://arxiv.org/pdf/2402.01536)]
> **Authors**: Barrett R. Anderson,Jash Hemant Shah,Max Kreminski
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: Accepted to C&C 2024
- **标题**: None
- **领域**: 人机交互,人工智能
- **Abstract**: Large language models (LLMs) are now being used in a wide variety of contexts, including as creativity support tools (CSTs) intended to help their users come up with new ideas. But do LLMs actually support user creativity? We hypothesized that the use of an LLM as a CST might make the LLM's users feel more creative, and even broaden the range of ideas suggested by each individual user, but also homogenize the ideas suggested by different users. We conducted a 36-participant comparative user study and found, in accordance with the homogenization hypothesis, that different users tended to produce less semantically distinct ideas with ChatGPT than with an alternative CST. Additionally, ChatGPT users generated a greater number of more detailed ideas, but felt less responsible for the ideas they generated. We discuss potential implications of these findings for users, designers, and developers of LLM-based CSTs.

## 机器学习(cs.LG:Machine Learning)

### Position: What Can Large Language Models Tell Us about Time Series Analysis 
[[arxiv](https://arxiv.org/abs/2402.02713)] [[cool](https://papers.cool/arxiv/2402.02713)] [[pdf](https://arxiv.org/pdf/2402.02713)]
> **Authors**: Ming Jin,Yifan Zhang,Wei Chen,Kexin Zhang,Yuxuan Liang,Bin Yang,Jindong Wang,Shirui Pan,Qingsong Wen
> **First submission**: 2024-02-04
> **First announcement**: 2024-02-05
> **comment**: Accepted by the 41st International Conference on Machine Learning (ICML 2024)
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Time series analysis is essential for comprehending the complexities inherent in various realworld systems and applications. Although large language models (LLMs) have recently made significant strides, the development of artificial general intelligence (AGI) equipped with time series analysis capabilities remains in its nascent phase. Most existing time series models heavily rely on domain knowledge and extensive model tuning, predominantly focusing on prediction tasks. In this paper, we argue that current LLMs have the potential to revolutionize time series analysis, thereby promoting efficient decision-making and advancing towards a more universal form of time series analytical intelligence. Such advancement could unlock a wide range of possibilities, including time series modality switching and question answering. We encourage researchers and practitioners to recognize the potential of LLMs in advancing time series analysis and emphasize the need for trust in these related efforts. Furthermore, we detail the seamless integration of time series analysis with existing LLM technologies and outline promising avenues for future research.

### tnGPS: Discovering Unknown Tensor Network Structure Search Algorithms via Large Language Models (LLMs) 
[[arxiv](https://arxiv.org/abs/2402.02456)] [[cool](https://papers.cool/arxiv/2402.02456)] [[pdf](https://arxiv.org/pdf/2402.02456)]
> **Authors**: Junhua Zeng,Chao Li,Zhun Sun,Qibin Zhao,Guoxu Zhou
> **First submission**: 2024-02-04
> **First announcement**: 2024-02-05
> **comment**: Accepted by ICML2024, pre-printed version
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Tensor networks are efficient for extremely high-dimensional representation, but their model selection, known as tensor network structure search (TN-SS), is a challenging problem. Although several works have targeted TN-SS, most existing algorithms are manually crafted heuristics with poor performance, suffering from the curse of dimensionality and local convergence. In this work, we jump out of the box, studying how to harness large language models (LLMs) to automatically discover new TN-SS algorithms, replacing the involvement of human experts. By observing how human experts innovate in research, we model their common workflow and propose an automatic algorithm discovery framework called tnGPS. The proposed framework is an elaborate prompting pipeline that instruct LLMs to generate new TN-SS algorithms through iterative refinement and enhancement. The experimental results demonstrate that the algorithms discovered by tnGPS exhibit superior performance in benchmarks compared to the current state-of-the-art methods.

### LQER: Low-Rank Quantization Error Reconstruction for LLMs 
[[arxiv](https://arxiv.org/abs/2402.02446)] [[cool](https://papers.cool/arxiv/2402.02446)] [[pdf](https://arxiv.org/pdf/2402.02446)]
> **Authors**: Cheng Zhang,Jianyi Cheng,George A. Constantinides,Yiren Zhao
> **First submission**: 2024-02-04
> **First announcement**: 2024-02-05
> **comment**: Accepted at ICML2024
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Post-training quantization of Large Language Models (LLMs) is challenging. In this work, we introduce Low-rank Quantization Error Reduction (LQER), which combines quantization and low-rank approximation to recover the model capability. LQER leverages an activation-induced scale matrix to drive the singular value distribution of quantization error towards a desirable distribution, which enables nearly-lossless W4A8 quantization on various LLMs and downstream tasks without the need for knowledge distillation, grid search, or gradient-base iterative optimization. Unlike existing methods, the computation pattern of LQER eliminates the need for specialized Scatter and Gather processes to collect high-precision weights from irregular memory locations. Our W4A8 LLMs achieve near-lossless performance on six popular downstream tasks, while using 1.36$\times$ fewer hardware resources than the leading state-of-the-art method. We open-source our framework at https://github.com/ChengZhang-98/lqer

### AutoTimes: Autoregressive Time Series Forecasters via Large Language Models 
[[arxiv](https://arxiv.org/abs/2402.02370)] [[cool](https://papers.cool/arxiv/2402.02370)] [[pdf](https://arxiv.org/pdf/2402.02370)]
> **Authors**: Yong Liu,Guo Qin,Xiangdong Huang,Jianmin Wang,Mingsheng Long
> **First submission**: 2024-02-04
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Foundation models of time series have not been fully developed due to the limited availability of time series corpora and the underexploration of scalable pre-training. Based on the similar sequential formulation of time series and natural language, increasing research demonstrates the feasibility of leveraging large language models (LLM) for time series. Nevertheless, the inherent autoregressive property and decoder-only architecture of LLMs have not been fully considered, resulting in insufficient utilization of LLM abilities. To fully revitalize the general-purpose token transition and multi-step generation capability of large language models, we propose AutoTimes to repurpose LLMs as autoregressive time series forecasters, which projects time series into the embedding space of language tokens and autoregressively generates future predictions with arbitrary lengths. Compatible with any decoder-only LLMs, the consequent forecaster exhibits the flexibility of the lookback length and scalability with larger LLMs. Further, we formulate time series as prompts, extending the context for prediction beyond the lookback window, termed in-context forecasting. By introducing LLM-embedded textual timestamps, AutoTimes can utilize chronological information to align multivariate time series. Empirically, AutoTimes achieves state-of-the-art with 0.1% trainable parameters and over $5\times$ training/inference speedup compared to advanced LLM-based forecasters. Code is available at this repository: https://github.com/thuml/AutoTimes.

### Timer: Generative Pre-trained Transformers Are Large Time Series Models 
[[arxiv](https://arxiv.org/abs/2402.02368)] [[cool](https://papers.cool/arxiv/2402.02368)] [[pdf](https://arxiv.org/pdf/2402.02368)]
> **Authors**: Yong Liu,Haoran Zhang,Chenyu Li,Xiangdong Huang,Jianmin Wang,Mingsheng Long
> **First submission**: 2024-02-04
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Deep learning has contributed remarkably to the advancement of time series analysis. Still, deep models can encounter performance bottlenecks in real-world data-scarce scenarios, which can be concealed due to the performance saturation with small models on current benchmarks. Meanwhile, large models have demonstrated great powers in these scenarios through large-scale pre-training. Continuous progress has been achieved with the emergence of large language models, exhibiting unprecedented abilities such as few-shot generalization, scalability, and task generality, which are however absent in small deep models. To change the status quo of training scenario-specific small models from scratch, this paper aims at the early development of large time series models (LTSM). During pre-training, we curate large-scale datasets with up to 1 billion time points, unify heterogeneous time series into single-series sequence (S3) format, and develop the GPT-style architecture toward LTSMs. To meet diverse application needs, we convert forecasting, imputation, and anomaly detection of time series into a unified generative task. The outcome of this study is a Time Series Transformer (Timer), which is generative pre-trained by next token prediction and adapted to various downstream tasks with promising capabilities as an LTSM. Code and datasets are available at: https://github.com/thuml/Large-Time-Series-Model.

### Selecting Large Language Model to Fine-tune via Rectified Scaling Law 
[[arxiv](https://arxiv.org/abs/2402.02314)] [[cool](https://papers.cool/arxiv/2402.02314)] [[pdf](https://arxiv.org/pdf/2402.02314)]
> **Authors**: Haowei Lin,Baizhou Huang,Haotian Ye,Qinyu Chen,Zihao Wang,Sujian Li,Jianzhu Ma,Xiaojun Wan,James Zou,Yitao Liang
> **First submission**: 2024-02-03
> **First announcement**: 2024-02-05
> **comment**: ef:ICML 2024
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: The ever-growing ecosystem of LLMs has posed a challenge in selecting the most appropriate pre-trained model to fine-tune amidst a sea of options. Given constrained resources, fine-tuning all models and making selections afterward is unrealistic. In this work, we formulate this resource-constrained selection task into predicting fine-tuning performance and illustrate its natural connection with Scaling Law. Unlike pre-training, we find that the fine-tuning scaling curve includes not just the well-known "power phase" but also the previously unobserved "pre-power phase". We also explain why existing Scaling Law fails to capture this phase transition phenomenon both theoretically and empirically. To address this, we introduce the concept of "pre-learned data size" into our Rectified Scaling Law, which overcomes theoretical limitations and fits experimental results much better. By leveraging our law, we propose a novel LLM selection algorithm that selects the near-optimal model with hundreds of times less resource consumption, while other methods may provide negatively correlated selection. The project page is available at rectified-scaling-law.github.io.

### Jailbreaking Attack against Multimodal Large Language Model 
[[arxiv](https://arxiv.org/abs/2402.02309)] [[cool](https://papers.cool/arxiv/2402.02309)] [[pdf](https://arxiv.org/pdf/2402.02309)]
> **Authors**: Zhenxing Niu,Haodong Ren,Xinbo Gao,Gang Hua,Rong Jin
> **First submission**: 2024-02-03
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学,密码学和安全,计算机视觉和模式识别
- **Abstract**: This paper focuses on jailbreaking attacks against multi-modal large language models (MLLMs), seeking to elicit MLLMs to generate objectionable responses to harmful user queries. A maximum likelihood-based algorithm is proposed to find an \emph{image Jailbreaking Prompt} (imgJP), enabling jailbreaks against MLLMs across multiple unseen prompts and images (i.e., data-universal property). Our approach exhibits strong model-transferability, as the generated imgJP can be transferred to jailbreak various models, including MiniGPT-v2, LLaVA, InstructBLIP, and mPLUG-Owl2, in a black-box manner. Moreover, we reveal a connection between MLLM-jailbreaks and LLM-jailbreaks. As a result, we introduce a construction-based method to harness our approach for LLM-jailbreaks, demonstrating greater efficiency than current state-of-the-art methods. The code is available here. \textbf{Warning: some content generated by language models may be offensive to some readers.}

### Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models 
[[arxiv](https://arxiv.org/abs/2402.02207)] [[cool](https://papers.cool/arxiv/2402.02207)] [[pdf](https://arxiv.org/pdf/2402.02207)]
> **Authors**: Yongshuo Zong,Ondrej Bohdal,Tingyang Yu,Yongxin Yang,Timothy Hospedales
> **First submission**: 2024-02-03
> **First announcement**: 2024-02-05
> **comment**: ICML 2024
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Current vision large language models (VLLMs) exhibit remarkable capabilities yet are prone to generate harmful content and are vulnerable to even the simplest jailbreaking attacks. Our initial analysis finds that this is due to the presence of harmful data during vision-language instruction fine-tuning, and that VLLM fine-tuning can cause forgetting of safety alignment previously learned by the underpinning LLM. To address this issue, we first curate a vision-language safe instruction-following dataset VLGuard covering various harmful categories. Our experiments demonstrate that integrating this dataset into standard vision-language fine-tuning or utilizing it for post-hoc fine-tuning effectively safety aligns VLLMs. This alignment is achieved with minimal impact on, or even enhancement of, the models' helpfulness. The versatility of our safety fine-tuning dataset makes it a valuable resource for safety-testing existing VLLMs, training new models or safeguarding pre-trained VLLMs. Empirical results demonstrate that fine-tuned VLLMs effectively reject unsafe instructions and substantially reduce the success rates of several black-box adversarial attacks, which approach zero in many cases. The code and dataset are available at https://github.com/ys-zong/VLGuard.

### Break the Sequential Dependency of LLM Inference Using Lookahead Decoding 
[[arxiv](https://arxiv.org/abs/2402.02057)] [[cool](https://papers.cool/arxiv/2402.02057)] [[pdf](https://arxiv.org/pdf/2402.02057)]
> **Authors**: Yichao Fu,Peter Bailis,Ion Stoica,Hao Zhang
> **First submission**: 2024-02-03
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Autoregressive decoding of large language models (LLMs) is memory bandwidth bounded, resulting in high latency and significant wastes of the parallel processing power of modern accelerators. Existing methods for accelerating LLM decoding often require a draft model (e.g., speculative decoding), which is nontrivial to obtain and unable to generalize. In this paper, we introduce Lookahead decoding, an exact, parallel decoding algorithm that accelerates LLM decoding without needing auxiliary models or data stores. It allows trading per-step log(FLOPs) to reduce the number of total decoding steps, is more parallelizable on single or multiple modern accelerators, and is compatible with concurrent memory-efficient attention (e.g., FlashAttention). Our implementation of Lookahead decoding can speed up autoregressive decoding by up to 1.8x on MT-bench and 4x with strong scaling on multiple GPUs in code completion tasks. Our code is avialable at https://github.com/hao-ai-lab/LookaheadDecoding

### A Plug-in Tiny AI Module for Intelligent and Selective Sensor Data Transmission 
[[arxiv](https://arxiv.org/abs/2402.02043)] [[cool](https://papers.cool/arxiv/2402.02043)] [[pdf](https://arxiv.org/pdf/2402.02043)]
> **Authors**: Wenjun Huang,Arghavan Rezvani,Hanning Chen,Yang Ni,Sanggeon Yun,Sungheon Jeong,Mohsen Imani
> **First submission**: 2024-02-03
> **First announcement**: 2024-02-05
> **comment**: 14 pages, 6 figures
- **标题**: None
- **领域**: 机器学习,人工智能,网络和互联网架构
- **Abstract**: Applications in the Internet of Things (IoT) utilize machine learning to analyze sensor-generated data. However, a major challenge lies in the lack of targeted intelligence in current sensing systems, leading to vast data generation and increased computational and communication costs. To address this challenge, we propose a novel sensing module to equip sensing frameworks with intelligent data transmission capabilities by integrating a highly efficient machine learning model placed near the sensor. This model provides prompt feedback for the sensing system to transmit only valuable data while discarding irrelevant information by regulating the frequency of data transmission. The near-sensor model is quantized and optimized for real-time sensor control. To enhance the framework's performance, the training process is customized and a "lazy" sensor deactivation strategy utilizing temporal information is introduced. The suggested method is orthogonal to other IoT frameworks and can be considered as a plugin for selective data transmission. The framework is implemented, encompassing both software and hardware components. The experiments demonstrate that the framework utilizing the suggested module achieves over 85% system efficiency in terms of energy consumption and storage, with negligible impact on performance. This methodology has the potential to significantly reduce data output from sensors, benefiting a wide range of IoT applications.

### The Landscape and Challenges of HPC Research and LLMs 
[[arxiv](https://arxiv.org/abs/2402.02018)] [[cool](https://papers.cool/arxiv/2402.02018)] [[pdf](https://arxiv.org/pdf/2402.02018)]
> **Authors**: Le Chen,Nesreen K. Ahmed,Akash Dutta,Arijit Bhattacharjee,Sixing Yu,Quazi Ishtiaque Mahmud,Waqwoya Abebe,Hung Phan,Aishwarya Sarkar,Branden Butler,Niranjan Hasabnis,Gal Oren,Vy A. Vo,Juan Pablo Munoz,Theodore L. Willke,Tim Mattson,Ali Jannesari
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Recently, language models (LMs), especially large language models (LLMs), have revolutionized the field of deep learning. Both encoder-decoder models and prompt-based techniques have shown immense potential for natural language processing and code-based tasks. Over the past several years, many research labs and institutions have invested heavily in high-performance computing, approaching or breaching exascale performance levels. In this paper, we posit that adapting and utilizing such language model-based techniques for tasks in high-performance computing (HPC) would be very beneficial. This study presents our reasoning behind the aforementioned position and highlights how existing ideas can be improved and adapted for HPC tasks.

### PresAIse, A Prescriptive AI Solution for Enterprises 
[[arxiv](https://arxiv.org/abs/2402.02006)] [[cool](https://papers.cool/arxiv/2402.02006)] [[pdf](https://arxiv.org/pdf/2402.02006)]
> **Authors**: Wei Sun,Scott McFaddin,Linh Ha Tran,Shivaram Subramanian,Kristjan Greenewald,Yeshi Tenzin,Zack Xue,Youssef Drissi,Markus Ettl
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: 14 pages
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Prescriptive AI represents a transformative shift in decision-making, offering causal insights and actionable recommendations. Despite its huge potential, enterprise adoption often faces several challenges. The first challenge is caused by the limitations of observational data for accurate causal inference which is typically a prerequisite for good decision-making. The second pertains to the interpretability of recommendations, which is crucial for enterprise decision-making settings. The third challenge is the silos between data scientists and business users, hindering effective collaboration. This paper outlines an initiative from IBM Research, aiming to address some of these challenges by offering a suite of prescriptive AI solutions. Leveraging insights from various research papers, the solution suite includes scalable causal inference methods, interpretable decision-making approaches, and the integration of large language models (LLMs) to bridge communication gaps via a conversation agent. A proof-of-concept, PresAIse, demonstrates the solutions' potential by enabling non-ML experts to interact with prescriptive AI models via a natural language interface, democratizing advanced analytics for strategic decision-making.

### On Catastrophic Inheritance of Large Foundation Models 
[[arxiv](https://arxiv.org/abs/2402.01909)] [[cool](https://papers.cool/arxiv/2402.01909)] [[pdf](https://arxiv.org/pdf/2402.01909)]
> **Authors**: Hao Chen,Bhiksha Raj,Xing Xie,Jindong Wang
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: Accepted by DMLR
- **标题**: None
- **领域**: 机器学习,人工智能,计算机与社会
- **Abstract**: Large foundation models (LFMs) are claiming incredible performances. Yet great concerns have been raised about their mythic and uninterpreted potentials not only in machine learning, but also in various other disciplines. In this position paper, we propose to identify a neglected issue deeply rooted in LFMs: Catastrophic Inheritance, describing the weaknesses and limitations inherited from biased large-scale pre-training data to behaviors of LFMs on the downstream tasks, including samples that are corrupted, long-tailed, noisy, out-of-distributed, to name a few. Such inheritance can potentially cause catastrophes to downstream applications, such as bias, lack of generalization, deteriorated performance, security vulnerability, privacy leakage, and value misalignment. We discuss the challenges behind this issue and propose UIM, a framework to Understand the catastrophic inheritance of LFMs from both pre-training and downstream adaptation, Interpret the implications of catastrophic inheritance on downstream tasks, and how to Mitigate it. UIM aims to unite both the machine learning and social sciences communities for more responsible and promising AI development and deployment.

### Large Language Model Agent for Hyper-Parameter Optimization 
[[arxiv](https://arxiv.org/abs/2402.01881)] [[cool](https://papers.cool/arxiv/2402.01881)] [[pdf](https://arxiv.org/pdf/2402.01881)]
> **Authors**: Siyi Liu,Chen Gao,Yong Li
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Hyperparameter optimization is critical in modern machine learning, requiring expert knowledge, numerous trials, and high computational and human resources. Despite the advancements in Automated Machine Learning (AutoML), challenges in terms of trial efficiency, setup complexity, and interoperability still persist. To address these issues, we introduce a novel paradigm leveraging Large Language Models (LLMs) to automate hyperparameter optimization across diverse machine learning tasks, which is named AgentHPO (short for LLM Agent-based Hyperparameter Optimization). Specifically, AgentHPO processes the task information autonomously, conducts experiments with specific hyperparameters (HPs), and iteratively optimizes them based on historical trials. This human-like optimization process largely reduces the number of required trials, simplifies the setup process, and enhances interpretability and user trust, compared to traditional AutoML methods. Extensive empirical experiments conducted on 12 representative machine-learning tasks indicate that AgentHPO not only matches but also often surpasses the best human trials in terms of performance while simultaneously providing explainable results. Further analysis sheds light on the strategies employed by the LLM in optimizing these tasks, highlighting its effectiveness and adaptability in various scenarios.

### InferCept: Efficient Intercept Support for Augmented Large Language Model Inference 
[[arxiv](https://arxiv.org/abs/2402.01869)] [[cool](https://papers.cool/arxiv/2402.01869)] [[pdf](https://arxiv.org/pdf/2402.01869)]
> **Authors**: Reyna Abhyankar,Zijian He,Vikranth Srivatsa,Hao Zhang,Yiying Zhang
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学,分布式、并行和集群计算
- **Abstract**: Large language models are increasingly integrated with external environments, tools, and agents like ChatGPT plugins to extend their capability beyond language-centric tasks. However, today's LLM inference systems are designed for standalone LLMs. They treat each external interaction as the end of LLM generation and form a new request when the interaction finishes, causing unnecessary recomputation of already computed contexts, which accounts for 37-40% of total model forwarding time. This paper presents InferCept, the first LLM inference framework targeting augmented LLMs and supporting the efficient interception of LLM generation. InferCept minimizes the GPU resource waste caused by LLM interceptions and dedicates saved memory for serving more requests. InferCept improves the overall serving throughput by 1.6x-2x and completes 2x more requests per second compared to the state-of-the-art LLM inference systems.

### Leveraging Large Language Models for Structure Learning in Prompted Weak Supervision 
[[arxiv](https://arxiv.org/abs/2402.01867)] [[cool](https://papers.cool/arxiv/2402.01867)] [[pdf](https://arxiv.org/pdf/2402.01867)]
> **Authors**: Jinyan Su,Peilin Yu,Jieyu Zhang,Stephen H. Bach
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: Accepted to IEEE International Conference on Big Data 2023
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Prompted weak supervision (PromptedWS) applies pre-trained large language models (LLMs) as the basis for labeling functions (LFs) in a weak supervision framework to obtain large labeled datasets. We further extend the use of LLMs in the loop to address one of the key challenges in weak supervision: learning the statistical dependency structure among supervision sources. In this work, we ask the LLM how similar are these prompted LFs. We propose a Structure Refining Module, a simple yet effective first approach based on the similarities of the prompts by taking advantage of the intrinsic structure in the embedding space. At the core of Structure Refining Module are Labeling Function Removal (LaRe) and Correlation Structure Generation (CosGen). Compared to previous methods that learn the dependencies from weak labels, our method finds the dependencies which are intrinsic to the LFs and less dependent on the data. We show that our Structure Refining Module improves the PromptedWS pipeline by up to 12.7 points on the benchmark tasks. We also explore the trade-offs between efficiency and performance with comprehensive ablation experiments and analysis. Code for this project can be found in https://github.com/BatsResearch/su-bigdata23-code.

### Large Language Models for Time Series: A Survey 
[[arxiv](https://arxiv.org/abs/2402.01801)] [[cool](https://papers.cool/arxiv/2402.01801)] [[pdf](https://arxiv.org/pdf/2402.01801)]
> **Authors**: Xiyuan Zhang,Ranak Roy Chowdhury,Rajesh K. Gupta,Jingbo Shang
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: GitHub repository: https://github.com/xiyuanzh/awesome-llm-time-series
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Large Language Models (LLMs) have seen significant use in domains such as natural language processing and computer vision. Going beyond text, image and graphics, LLMs present a significant potential for analysis of time series data, benefiting domains such as climate, IoT, healthcare, traffic, audio and finance. This survey paper provides an in-depth exploration and a detailed taxonomy of the various methodologies employed to harness the power of LLMs for time series analysis. We address the inherent challenge of bridging the gap between LLMs' original text data training and the numerical nature of time series data, and explore strategies for transferring and distilling knowledge from LLMs to numerical time series analysis. We detail various methodologies, including (1) direct prompting of LLMs, (2) time series quantization, (3) aligning techniques, (4) utilization of the vision modality as a bridging mechanism, and (5) the combination of LLMs with tools. Additionally, this survey offers a comprehensive overview of the existing multimodal time series and text datasets and delves into the challenges and future opportunities of this emerging field. We maintain an up-to-date Github repository which includes all the papers and datasets discussed in the survey.

### Faster and Lighter LLMs: A Survey on Current Challenges and Way Forward 
[[arxiv](https://arxiv.org/abs/2402.01799)] [[cool](https://papers.cool/arxiv/2402.01799)] [[pdf](https://arxiv.org/pdf/2402.01799)]
> **Authors**: Arnav Chavan,Raghav Magazine,Shubham Kushwaha,Mérouane Debbah,Deepak Gupta
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: Accepted at IJCAI '24 (Survey Track), Updated TGI results
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Despite the impressive performance of LLMs, their widespread adoption faces challenges due to substantial computational and memory requirements during inference. Recent advancements in model compression and system-level optimization methods aim to enhance LLM inference. This survey offers an overview of these methods, emphasizing recent developments. Through experiments on LLaMA(/2)-7B, we evaluate various compression techniques, providing practical insights for efficient LLM deployment in a unified setting. The empirical analysis on LLaMA(/2)-7B highlights the effectiveness of these methods. Drawing from survey insights, we identify current limitations and discuss potential future directions to improve LLM inference efficiency. We release the codebase to reproduce the results presented in this paper at https://github.com/nyunAI/Faster-LLM-Survey

### Decoding Speculative Decoding 
[[arxiv](https://arxiv.org/abs/2402.01528)] [[cool](https://papers.cool/arxiv/2402.01528)] [[pdf](https://arxiv.org/pdf/2402.01528)]
> **Authors**: Minghao Yan,Saurabh Agarwal,Shivaram Venkataraman
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL 2025)
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Speculative Decoding is a widely used technique to speed up inference for Large Language Models (LLMs) without sacrificing quality. When performing inference, speculative decoding uses a smaller draft model to generate speculative tokens and then uses the target LLM to verify those draft tokens. The speedup provided by speculative decoding heavily depends on the choice of the draft model. In this work, we perform a detailed study comprising over 350 experiments with LLaMA-65B and OPT-66B using speculative decoding and delineate the factors that affect the performance gain provided by speculative decoding. Our experiments indicate that the performance of speculative decoding depends heavily on the latency of the draft model, and the draft model's capability in language modeling does not correlate strongly with its performance in speculative decoding. Based on these insights we explore a new design space for draft models and design hardware-efficient draft models for speculative decoding. Our newly designed draft model can provide 111% higher throughput than existing draft models and our approach generalizes further to all LLaMA models (1/2/3.1) and supervised fine-tuned models.

### Integrating Large Language Models in Causal Discovery: A Statistical Causal Approach 
[[arxiv](https://arxiv.org/abs/2402.01454)] [[cool](https://papers.cool/arxiv/2402.01454)] [[pdf](https://arxiv.org/pdf/2402.01454)]
> **Authors**: Masayuki Takayama,Tadahisa Okuda,Thong Pham,Tatsuyoshi Ikenoue,Shingo Fukuma,Shohei Shimizu,Akiyoshi Sannai
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,方法论,机器学习
- **Abstract**: In practical statistical causal discovery (SCD), embedding domain expert knowledge as constraints into the algorithm is important for creating consistent, meaningful causal models, despite the challenges in the systematic acquisition of background knowledge. To overcome these challenges, this paper proposes a novel method for causal inference, in which SCD and knowledge based causal inference (KBCI) with a large language model (LLM) are synthesized through ``statistical causal prompting (SCP)'' for LLMs and prior knowledge augmentation for SCD. Experiments have revealed that the results of LLM-KBCI and SCD augmented with LLM-KBCI approach the ground truths, more than the SCD result without prior knowledge. It has also been revealed that the SCD result can be further improved if the LLM undergoes SCP. Furthermore, with an unpublished real-world dataset, we have demonstrated that the background knowledge provided by the LLM can improve the SCD on this dataset, even if this dataset has never been included in the training data of the LLM. For future practical application of this proposed method across important domains such as healthcare, we also thoroughly discuss the limitations, risks of critical errors, expected improvement of techniques around LLMs, and realistic integration of expert checks of the results into this automatic process, with SCP simulations under various conditions both in successful and failure scenarios. The careful and appropriate application of the proposed approach in this work, with improvement and customization for each domain, can thus address challenges such as dataset biases and limitations, illustrating the potential of LLMs to improve data-driven causal inference across diverse scientific domains. The code used in this work is publicly available at: www.github.com/mas-takayama/LLM-and-SCD

### From Words to Molecules: A Survey of Large Language Models in Chemistry 
[[arxiv](https://arxiv.org/abs/2402.01439)] [[cool](https://papers.cool/arxiv/2402.01439)] [[pdf](https://arxiv.org/pdf/2402.01439)]
> **Authors**: Chang Liao,Yemin Yu,Yu Mei,Ying Wei
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: Submitted to IJCAI 2024 survey track
- **标题**: None
- **领域**: 机器学习,人工智能,生物分子,定量方法
- **Abstract**: In recent years, Large Language Models (LLMs) have achieved significant success in natural language processing (NLP) and various interdisciplinary areas. However, applying LLMs to chemistry is a complex task that requires specialized domain knowledge. This paper provides a thorough exploration of the nuanced methodologies employed in integrating LLMs into the field of chemistry, delving into the complexities and innovations at this interdisciplinary juncture. Specifically, our analysis begins with examining how molecular information is fed into LLMs through various representation and tokenization methods. We then categorize chemical LLMs into three distinct groups based on the domain and modality of their input data, and discuss approaches for integrating these inputs for LLMs. Furthermore, this paper delves into the pretraining objectives with adaptations to chemical LLMs. After that, we explore the diverse applications of LLMs in chemistry, including novel paradigms for their application in chemistry tasks. Finally, we identify promising research directions, including further integration with chemical knowledge, advancements in continual learning, and improvements in model interpretability, paving the way for groundbreaking developments in the field.

### Counterfactual Concept Bottleneck Models 
[[arxiv](https://arxiv.org/abs/2402.01408)] [[cool](https://papers.cool/arxiv/2402.01408)] [[pdf](https://arxiv.org/pdf/2402.01408)]
> **Authors**: Gabriele Dominici,Pietro Barbiero,Francesco Giannini,Martin Gjoreski,Giuseppe Marra,Marc Langheinrich
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Current deep learning models are not designed to simultaneously address three fundamental questions: predict class labels to solve a given classification task (the "What?"), simulate changes in the situation to evaluate how this impacts class predictions (the "How?"), and imagine how the scenario should change to result in different class predictions (the "Why not?"). The inability to answer these questions represents a crucial gap in deploying reliable AI agents, calibrating human trust, and improving human-machine interaction. To bridge this gap, we introduce CounterFactual Concept Bottleneck Models (CF-CBMs), a class of models designed to efficiently address the above queries all at once without the need to run post-hoc searches. Our experimental results demonstrate that CF-CBMs: achieve classification accuracy comparable to black-box models and existing CBMs ("What?"), rely on fewer important concepts leading to simpler explanations ("How?"), and produce interpretable, concept-based counterfactuals ("Why not?"). Additionally, we show that training the counterfactual generator jointly with the CBM leads to two key improvements: (i) it alters the model's decision-making process, making the model rely on fewer important concepts (leading to simpler explanations), and (ii) it significantly increases the causal effect of concept interventions on class predictions, making the model more responsive to these changes.

### An Information Theoretic Approach to Machine Unlearning 
[[arxiv](https://arxiv.org/abs/2402.01401)] [[cool](https://papers.cool/arxiv/2402.01401)] [[pdf](https://arxiv.org/pdf/2402.01401)]
> **Authors**: Jack Foster,Kyle Fogarty,Stefan Schoepf,Zack Dugue,Cengiz Öztireli,Alexandra Brintrup
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: Updated, new low-dimensional experiments and updated perspective on unlearning from an information theoretic view
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: To comply with AI and data regulations, the need to forget private or copyrighted information from trained machine learning models is increasingly important. The key challenge in unlearning is forgetting the necessary data in a timely manner, while preserving model performance. In this work, we address the zero-shot unlearning scenario, whereby an unlearning algorithm must be able to remove data given only a trained model and the data to be forgotten. We explore unlearning from an information theoretic perspective, connecting the influence of a sample to the information gain a model receives by observing it. From this, we derive a simple but principled zero-shot unlearning method based on the geometry of the model. Our approach takes the form of minimising the gradient of a learned function with respect to a small neighbourhood around a target forget point. This induces a smoothing effect, causing forgetting by moving the boundary of the classifier. We explore the intuition behind why this approach can jointly unlearn forget samples while preserving general model performance through a series of low-dimensional experiments. We perform extensive empirical evaluation of our method over a range of contemporary benchmarks, verifying that our method is competitive with state-of-the-art performance under the strict constraints of zero-shot unlearning. Code for the project can be found at https://github.com/jwf40/Information-Theoretic-Unlearning

### On the Multi-modal Vulnerability of Diffusion Models 
[[arxiv](https://arxiv.org/abs/2402.01369)] [[cool](https://papers.cool/arxiv/2402.01369)] [[pdf](https://arxiv.org/pdf/2402.01369)]
> **Authors**: Dingcheng Yang,Yang Bai,Xiaojun Jia,Yang Liu,Xiaochun Cao,Wenjian Yu
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: Accepted at ICML2024 Workshop on Trustworthy Multi-modal Foundation Models andAIAgents (TiFA)
- **标题**: None
- **领域**: 机器学习,密码学和安全,计算机视觉和模式识别
- **Abstract**: Diffusion models have been widely deployed in various image generation tasks, demonstrating an extraordinary connection between image and text modalities. Although prior studies have explored the vulnerability of diffusion models from the perspectives of text and image modalities separately, the current research landscape has not yet thoroughly investigated the vulnerabilities that arise from the integration of multiple modalities, specifically through the joint analysis of textual and visual features. In this paper, we are the first to visualize both text and image feature space embedded by diffusion models and observe a significant difference. The prompts are embedded chaotically in the text feature space, while in the image feature space they are clustered according to their subjects. These fascinating findings may underscore a potential misalignment in robustness between the two modalities that exists within diffusion models. Based on this observation, we propose MMP-Attack, which leverages multi-modal priors (MMP) to manipulate the generation results of diffusion models by appending a specific suffix to the original prompt. Specifically, our goal is to induce diffusion models to generate a specific object while simultaneously eliminating the original object. Our MMP-Attack shows a notable advantage over existing studies with superior manipulation capability and efficiency. Our code is publicly available at \url{https://github.com/ydc123/MMP-Attack}.

### Training-time Neuron Alignment through Permutation Subspace for Improving Linear Mode Connectivity and Model Fusion 
[[arxiv](https://arxiv.org/abs/2402.01342)] [[cool](https://papers.cool/arxiv/2402.01342)] [[pdf](https://arxiv.org/pdf/2402.01342)]
> **Authors**: Zexi Li,Zhiqi Li,Jie Lin,Tao Shen,Tao Lin,Chao Wu
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: preprint
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: In deep learning, stochastic gradient descent often yields functionally similar yet widely scattered solutions in the weight space even under the same initialization, causing barriers in the Linear Mode Connectivity (LMC) landscape. Overcoming these barriers is crucial for understanding deep learning dynamics and enhancing model-fusion algorithms. Previous studies highlight the role of permutation symmetry in reducing post-training barriers through network permutation. However, these post-hoc methods, demanding extra computations, are less effective for larger, complex models (e.g., ViT, LLM) due to numerous permutation matrices. Thus, in this paper, we study training-time neuron alignment. Our hypothesis suggests that training-time permutation subspace can reduce LMC barriers for free. We find that pruning at initialization supports this. Beyond pruning, we introduce TNA-PFN, a simple yet lossless algorithm using a partial gradient mask during training. TNA-PFN is theoretically and empirically validated for reducing LMC barriers. It excels in wide model fusion applications, especially in federated learning, two algorithms based on TNA-FPN that are proposed to show its prospects even under heterogeneous datasets. Moreover, TNA-PFN can enhance the generalization of model soup for vision transformers and ColD fusion for pretrained language models.

### KTO: Model Alignment as Prospect Theoretic Optimization 
[[arxiv](https://arxiv.org/abs/2402.01306)] [[cool](https://papers.cool/arxiv/2402.01306)] [[pdf](https://arxiv.org/pdf/2402.01306)]
> **Authors**: Kawin Ethayarajh,Winnie Xu,Niklas Muennighoff,Dan Jurafsky,Douwe Kiela
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: ICML 2024
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Kahneman & Tversky's $\textit{prospect theory}$ tells us that humans perceive random variables in a biased but well-defined manner (1992); for example, humans are famously loss-averse. We show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biases -- the success of these objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed to them belonging to a family of loss functions that we call $\textit{human-aware losses}$ (HALOs). However, the utility functions these methods attribute to humans still differ from those in the prospect theory literature. Using a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do. We call this approach KTO, and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B, despite only learning from a binary signal of whether an output is desirable. More broadly, our work suggests that there is no one HALO that is universally superior; the best loss depends on the inductive biases most appropriate for a given setting, an oft-overlooked consideration.

### Can MLLMs Perform Text-to-Image In-Context Learning? 
[[arxiv](https://arxiv.org/abs/2402.01293)] [[cool](https://papers.cool/arxiv/2402.01293)] [[pdf](https://arxiv.org/pdf/2402.01293)]
> **Authors**: Yuchen Zeng,Wonjun Kang,Yicong Chen,Hyung Il Koo,Kangwook Lee
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: Accepted at COLM 2024
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: The evolution from Large Language Models (LLMs) to Multimodal Large Language Models (MLLMs) has spurred research into extending In-Context Learning (ICL) to its multimodal counterpart. Existing such studies have primarily concentrated on image-to-text ICL. However, the Text-to-Image ICL (T2I-ICL), with its unique characteristics and potential applications, remains underexplored. To address this gap, we formally define the task of T2I-ICL and present CoBSAT, the first T2I-ICL benchmark dataset, encompassing ten tasks. Utilizing our dataset to benchmark six state-of-the-art MLLMs, we uncover considerable difficulties MLLMs encounter in solving T2I-ICL. We identify the primary challenges as the inherent complexity of multimodality and image generation, and show that strategies such as fine-tuning and Chain-of-Thought prompting help to mitigate these difficulties, leading to notable improvements in performance. Our code and dataset are available at https://github.com/UW-Madison-Lee-Lab/CoBSAT.

### Efficient Causal Graph Discovery Using Large Language Models 
[[arxiv](https://arxiv.org/abs/2402.01207)] [[cool](https://papers.cool/arxiv/2402.01207)] [[pdf](https://arxiv.org/pdf/2402.01207)]
> **Authors**: Thomas Jiralerspong,Xiaoyin Chen,Yash More,Vedant Shah,Yoshua Bengio
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,方法论
- **Abstract**: We propose a novel framework that leverages LLMs for full causal graph discovery. While previous LLM-based methods have used a pairwise query approach, this requires a quadratic number of queries which quickly becomes impractical for larger causal graphs. In contrast, the proposed framework uses a breadth-first search (BFS) approach which allows it to use only a linear number of queries. We also show that the proposed method can easily incorporate observational data when available, to improve performance. In addition to being more time and data-efficient, the proposed framework achieves state-of-the-art results on real-world causal graphs of varying sizes. The results demonstrate the effectiveness and efficiency of the proposed method in discovering causal relationships, showcasing its potential for broad applicability in causal graph discovery tasks across different domains.

## 多代理系统(cs.MA:Multiagent Systems)

### A Survey on Context-Aware Multi-Agent Systems: Techniques, Challenges and Future Directions 
[[arxiv](https://arxiv.org/abs/2402.01968)] [[cool](https://papers.cool/arxiv/2402.01968)] [[pdf](https://arxiv.org/pdf/2402.01968)]
> **Authors**: Hung Du,Srikanth Thudumu,Rajesh Vasa,Kon Mouzakis
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: 11 pages, 1 figure
- **标题**: None
- **领域**: 多代理系统,人工智能,机器学习
- **Abstract**: Research interest in autonomous agents is on the rise as an emerging topic. The notable achievements of Large Language Models (LLMs) have demonstrated the considerable potential to attain human-like intelligence in autonomous agents. However, the challenge lies in enabling these agents to learn, reason, and navigate uncertainties in dynamic environments. Context awareness emerges as a pivotal element in fortifying multi-agent systems when dealing with dynamic situations. Despite existing research focusing on both context-aware systems and multi-agent systems, there is a lack of comprehensive surveys outlining techniques for integrating context-aware systems with multi-agent systems. To address this gap, this survey provides a comprehensive overview of state-of-the-art context-aware multi-agent systems. First, we outline the properties of both context-aware systems and multi-agent systems that facilitate integration between these systems. Subsequently, we propose a general process for context-aware systems, with each phase of the process encompassing diverse approaches drawn from various application domains such as collision avoidance in autonomous driving, disaster relief management, utility management, supply chain management, human-AI interaction, and others. Finally, we discuss the existing challenges of context-aware multi-agent systems and provide future research directions in this field.

## 神经和进化计算(cs.NE:Neural and Evolutionary Computing)

### ReEvo: Large Language Models as Hyper-Heuristics with Reflective Evolution 
[[arxiv](https://arxiv.org/abs/2402.01145)] [[cool](https://papers.cool/arxiv/2402.01145)] [[pdf](https://arxiv.org/pdf/2402.01145)]
> **Authors**: Haoran Ye,Jiarui Wang,Zhiguang Cao,Federico Berto,Chuanbo Hua,Haeyeon Kim,Jinkyoo Park,Guojie Song
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: Accepted at NeurIPS 2024
- **标题**: None
- **领域**: 神经和进化计算,人工智能
- **Abstract**: The omnipresence of NP-hard combinatorial optimization problems (COPs) compels domain experts to engage in trial-and-error heuristic design. The long-standing endeavor of design automation has gained new momentum with the rise of large language models (LLMs). This paper introduces Language Hyper-Heuristics (LHHs), an emerging variant of Hyper-Heuristics that leverages LLMs for heuristic generation, featuring minimal manual intervention and open-ended heuristic spaces. To empower LHHs, we present Reflective Evolution (ReEvo), a novel integration of evolutionary search for efficiently exploring the heuristic space, and LLM reflections to provide verbal gradients within the space. Across five heterogeneous algorithmic types, six different COPs, and both white-box and black-box views of COPs, ReEvo yields state-of-the-art and competitive meta-heuristics, evolutionary algorithms, heuristics, and neural solvers, while being more sample-efficient than prior LHHs.

## 网络和互联网架构(cs.NI:Networking and Internet Architecture)

### NetLLM: Adapting Large Language Models for Networking 
[[arxiv](https://arxiv.org/abs/2402.02338)] [[cool](https://papers.cool/arxiv/2402.02338)] [[pdf](https://arxiv.org/pdf/2402.02338)]
> **Authors**: Duo Wu,Xianda Wang,Yaqi Qiao,Zhi Wang,Junchen Jiang,Shuguang Cui,Fangxin Wang
> **First submission**: 2024-02-03
> **First announcement**: 2024-02-05
> **comment**: This paper has been accepted by ACM SIGCOMM 2024. DOI: https://doi.org/10.1145/3651890.3672268
- **标题**: None
- **领域**: 网络和互联网架构,机器学习
- **Abstract**: Many networking tasks now employ deep learning (DL) to solve complex prediction and optimization problems. However, current design philosophy of DL-based algorithms entails intensive engineering overhead due to the manual design of deep neural networks (DNNs) for different networking tasks. Besides, DNNs tend to achieve poor generalization performance on unseen data distributions/environments. Motivated by the recent success of large language models (LLMs), this work studies the LLM adaptation for networking to explore a more sustainable design philosophy. With the powerful pre-trained knowledge, the LLM is promising to serve as the foundation model to achieve "one model for all tasks" with even better performance and stronger generalization. In pursuit of this vision, we present NetLLM, the first framework that provides a coherent design to harness the powerful capabilities of LLMs with low efforts to solve networking problems. Specifically, NetLLM empowers the LLM to effectively process multimodal data in networking and efficiently generate task-specific answers. Besides, NetLLM drastically reduces the costs of fine-tuning the LLM to acquire domain knowledge for networking. Across three networking-related use cases - viewport prediction, adaptive bitrate streaming and cluster job scheduling, we showcase that the NetLLM-adapted LLM significantly outperforms state-of-the-art algorithms.

### Large Multi-Modal Models (LMMs) as Universal Foundation Models for AI-Native Wireless Systems 
[[arxiv](https://arxiv.org/abs/2402.01748)] [[cool](https://papers.cool/arxiv/2402.01748)] [[pdf](https://arxiv.org/pdf/2402.01748)]
> **Authors**: Shengzhe Xu,Christo Kurisummoottil Thomas,Omar Hashash,Nikhil Muralidhar,Walid Saad,Naren Ramakrishnan
> **First submission**: 2024-01-29
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 网络和互联网架构,人工智能,计算语言学,机器学习
- **Abstract**: Large language models (LLMs) and foundation models have been recently touted as a game-changer for 6G systems. However, recent efforts on LLMs for wireless networks are limited to a direct application of existing language models that were designed for natural language processing (NLP) applications. To address this challenge and create wireless-centric foundation models, this paper presents a comprehensive vision on how to design universal foundation models that are tailored towards the deployment of artificial intelligence (AI)-native networks. Diverging from NLP-based foundation models, the proposed framework promotes the design of large multi-modal models (LMMs) fostered by three key capabilities: 1) processing of multi-modal sensing data, 2) grounding of physical symbol representations in real-world wireless systems using causal reasoning and retrieval-augmented generation (RAG), and 3) enabling instructibility from the wireless environment feedback to facilitate dynamic network adaptation thanks to logical and mathematical reasoning facilitated by neuro-symbolic AI. In essence, these properties enable the proposed LMM framework to build universal capabilities that cater to various cross-layer networking tasks and alignment of intents across different domains. Preliminary results from experimental evaluation demonstrate the efficacy of grounding using RAG in LMMs, and showcase the alignment of LMMs with wireless system designs. Furthermore, the enhanced rationale exhibited in the responses to mathematical questions by LMMs, compared to vanilla LLMs, demonstrates the logical and mathematical reasoning capabilities inherent in LMMs. Building on those results, we present a sequel of open questions and challenges for LMMs. We then conclude with a set of recommendations that ignite the path towards LMM-empowered AI-native systems.

## 机器人技术(cs.RO:Robotics)

### A Survey on Robotics with Foundation Models: toward Embodied AI 
[[arxiv](https://arxiv.org/abs/2402.02385)] [[cool](https://papers.cool/arxiv/2402.02385)] [[pdf](https://arxiv.org/pdf/2402.02385)]
> **Authors**: Zhiyuan Xu,Kun Wu,Junjie Wen,Jinming Li,Ning Liu,Zhengping Che,Jian Tang
> **First submission**: 2024-02-04
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,人工智能
- **Abstract**: While the exploration for embodied AI has spanned multiple decades, it remains a persistent challenge to endow agents with human-level intelligence, including perception, learning, reasoning, decision-making, control, and generalization capabilities, so that they can perform general-purpose tasks in open, unstructured, and dynamic environments. Recent advances in computer vision, natural language processing, and multi-modality learning have shown that the foundation models have superhuman capabilities for specific tasks. They not only provide a solid cornerstone for integrating basic modules into embodied AI systems but also shed light on how to scale up robot learning from a methodological perspective. This survey aims to provide a comprehensive and up-to-date overview of foundation models in robotics, focusing on autonomous manipulation and encompassing high-level planning and low-level control. Moreover, we showcase their commonly used datasets, simulators, and benchmarks. Importantly, we emphasize the critical challenges intrinsic to this field and delineate potential avenues for future research, contributing to advancing the frontier of academic and industrial discourse.

## 声音(cs.SD:Sound)

### Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities 
[[arxiv](https://arxiv.org/abs/2402.01831)] [[cool](https://papers.cool/arxiv/2402.01831)] [[pdf](https://arxiv.org/pdf/2402.01831)]
> **Authors**: Zhifeng Kong,Arushi Goel,Rohan Badlani,Wei Ping,Rafael Valle,Bryan Catanzaro
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: ICML 2024
- **标题**: None
- **领域**: 声音,机器学习,音频和语音处理
- **Abstract**: Augmenting large language models (LLMs) to understand audio -- including non-speech sounds and non-verbal speech -- is critically important for diverse real-world applications of LLMs. In this paper, we propose Audio Flamingo, a novel audio language model with 1) strong audio understanding abilities, 2) the ability to quickly adapt to unseen tasks via in-context learning and retrieval, and 3) strong multi-turn dialogue abilities. We introduce a series of training techniques, architecture design, and data strategies to enhance our model with these abilities. Extensive evaluations across various audio understanding tasks confirm the efficacy of our method, setting new state-of-the-art benchmarks. Our demo website is https://audioflamingo.github.io/ and the code is open-sourced at https://github.com/NVIDIA/audio-flamingo.

### Bass Accompaniment Generation via Latent Diffusion 
[[arxiv](https://arxiv.org/abs/2402.01412)] [[cool](https://papers.cool/arxiv/2402.01412)] [[pdf](https://arxiv.org/pdf/2402.01412)]
> **Authors**: Marco Pasini,Maarten Grachten,Stefan Lattner
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: ICASSP 2024
- **标题**: None
- **领域**: 声音,机器学习,音频和语音处理
- **Abstract**: The ability to automatically generate music that appropriately matches an arbitrary input track is a challenging task. We present a novel controllable system for generating single stems to accompany musical mixes of arbitrary length. At the core of our method are audio autoencoders that efficiently compress audio waveform samples into invertible latent representations, and a conditional latent diffusion model that takes as input the latent encoding of a mix and generates the latent encoding of a corresponding stem. To provide control over the timbre of generated samples, we introduce a technique to ground the latent space to a user-provided reference style during diffusion sampling. For further improving audio quality, we adapt classifier-free guidance to avoid distortions at high guidance strengths when generating an unbounded latent space. We train our model on a dataset of pairs of mixes and matching bass stems. Quantitative experiments demonstrate that, given an input mix, the proposed system can generate basslines with user-specified timbres. Our controllable conditional audio generation framework represents a significant step forward in creating generative AI tools to assist musicians in music production.

## 软件工程(cs.SE:Software Engineering)

### EffiBench: Benchmarking the Efficiency of Automatically Generated Code 
[[arxiv](https://arxiv.org/abs/2402.02037)] [[cool](https://papers.cool/arxiv/2402.02037)] [[pdf](https://arxiv.org/pdf/2402.02037)]
> **Authors**: Dong Huang,Yuhao Qing,Weiyi Shang,Heming Cui,Jie M. Zhang
> **First submission**: 2024-02-03
> **First announcement**: 2024-02-05
> **comment**: Camera Ready for NeurIPS 2024
- **标题**: None
- **领域**: 软件工程,计算语言学
- **Abstract**: Code generation models have increasingly become integral to aiding software development. Although current research has thoroughly examined the correctness of the code produced by code generation models, a vital aspect that plays a pivotal role in green computing and sustainability efforts has often been neglected. This paper presents EffiBench, a benchmark with 1,000 efficiency-critical coding problems to assess the efficiency of code generated by code generation models. EffiBench contains a diverse set of LeetCode coding problems. Each problem is paired with an executable human-written canonical solution, which obtains the SOTA efficiency on the LeetCode solution leaderboard. With EffiBench, we empirically examine the ability of 42 large language models (35 open-source and 7 closed-source) to generate efficient code. Our evaluation results demonstrate that the efficiency of the code generated by LLMs is generally worse than the efficiency of human-written canonical solutions. For example, GPT-4 generated code has an average \textbf{3.12} times execution time that of the human-written canonical solutions. In the most extreme cases, the execution time and total memory usage of GPT-4 generated code are \textbf{13.89} and \textbf{43.92} times that of the canonical solutions. The source code of EffiBench is released on https://github.com/huangd1999/EffiBench. We also provide the LeaderBoard at https://huggingface.co/spaces/EffiBench/effibench-leaderboard.

### COMET: Generating Commit Messages using Delta Graph Context Representation 
[[arxiv](https://arxiv.org/abs/2402.01841)] [[cool](https://papers.cool/arxiv/2402.01841)] [[pdf](https://arxiv.org/pdf/2402.01841)]
> **Authors**: Abhinav Reddy Mandli,Saurabhsingh Rajput,Tushar Sharma
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: 22 Pages, 7 Figures
- **标题**: None
- **领域**: 软件工程,人工智能,计算语言学
- **Abstract**: Commit messages explain code changes in a commit and facilitate collaboration among developers. Several commit message generation approaches have been proposed; however, they exhibit limited success in capturing the context of code changes. We propose Comet (Context-Aware Commit Message Generation), a novel approach that captures context of code changes using a graph-based representation and leverages a transformer-based model to generate high-quality commit messages. Our proposed method utilizes delta graph that we developed to effectively represent code differences. We also introduce a customizable quality assurance module to identify optimal messages, mitigating subjectivity in commit messages. Experiments show that Comet outperforms state-of-the-art techniques in terms of bleu-norm and meteor metrics while being comparable in terms of rogue-l. Additionally, we compare the proposed approach with the popular gpt-3.5-turbo model, along with gpt-4-turbo; the most capable GPT model, over zero-shot, one-shot, and multi-shot settings. We found Comet outperforming the GPT models, on five and four metrics respectively and provide competitive results with the two other metrics. The study has implications for researchers, tool developers, and software developers. Software developers may utilize Comet to generate context-aware commit messages. Researchers and tool developers can apply the proposed delta graph technique in similar contexts, like code review summarization.

### StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback 
[[arxiv](https://arxiv.org/abs/2402.01391)] [[cool](https://papers.cool/arxiv/2402.01391)] [[pdf](https://arxiv.org/pdf/2402.01391)]
> **Authors**: Shihan Dou,Yan Liu,Haoxiang Jia,Limao Xiong,Enyu Zhou,Wei Shen,Junjie Shan,Caishuang Huang,Xiao Wang,Xiaoran Fan,Zhiheng Xi,Yuhao Zhou,Tao Ji,Rui Zheng,Qi Zhang,Xuanjing Huang,Tao Gui
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: 13 pages, 5 figures
- **标题**: None
- **领域**: 软件工程,计算语言学
- **Abstract**: The advancement of large language models (LLMs) has significantly propelled the field of code generation. Previous work integrated reinforcement learning (RL) with compiler feedback for exploring the output space of LLMs to enhance code generation quality. However, the lengthy code generated by LLMs in response to complex human requirements makes RL exploration a challenge. Also, since the unit tests may not cover the complicated code, optimizing LLMs by using these unexecuted code snippets is ineffective. To tackle these challenges, we introduce StepCoder, a novel RL framework for code generation, consisting of two main components: CCCS addresses the exploration challenge by breaking the long sequences code generation task into a Curriculum of Code Completion Subtasks, while FGO only optimizes the model by masking the unexecuted code segments to provide Fine-Grained Optimization. In addition, we furthermore construct the APPS+ dataset for RL training, which is manually verified to ensure the correctness of unit tests. Experimental results show that our method improves the ability to explore the output space and outperforms state-of-the-art approaches in corresponding benchmarks. Our dataset APPS+ and StepCoder are available online.

## 音频和语音处理(eess.AS:Audio and Speech Processing)

### Speech foundation models in healthcare: Effect of layer selection on pathological speech feature prediction 
[[arxiv](https://arxiv.org/abs/2402.01796)] [[cool](https://papers.cool/arxiv/2402.01796)] [[pdf](https://arxiv.org/pdf/2402.01796)]
> **Authors**: Daniela A. Wiepert,Rene L. Utianski,Joseph R. Duffy,John L. Stricker,Leland R. Barnard,David T. Jones,Hugo Botha
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: Accepted to INTERSPEECH 2024
- **标题**: None
- **领域**: 音频和语音处理,计算语言学,机器学习
- **Abstract**: Accurately extracting clinical information from speech is critical to the diagnosis and treatment of many neurological conditions. As such, there is interest in leveraging AI for automatic, objective assessments of clinical speech to facilitate diagnosis and treatment of speech disorders. We explore transfer learning using foundation models, focusing on the impact of layer selection for the downstream task of predicting pathological speech features. We find that selecting an optimal layer can greatly improve performance (~15.8% increase in balanced accuracy per feature as compared to worst layer, ~13.6% increase as compared to final layer), though the best layer varies by predicted feature and does not always generalize well to unseen data. A learned weighted sum offers comparable performance to the average best layer in-distribution (only ~1.2% lower) and had strong generalization for out-of-distribution data (only 1.5% lower than the average best layer).

### BAT: Learning to Reason about Spatial Sounds with Large Language Models 
[[arxiv](https://arxiv.org/abs/2402.01591)] [[cool](https://papers.cool/arxiv/2402.01591)] [[pdf](https://arxiv.org/pdf/2402.01591)]
> **Authors**: Zhisheng Zheng,Puyuan Peng,Ziyang Ma,Xie Chen,Eunsol Choi,David Harwath
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: Accepted to ICML 2024. Our demo, dataset, code and model weights are available at: https://zhishengzheng.com/BAT
- **标题**: None
- **领域**: 音频和语音处理,人工智能,计算语言学,声音
- **Abstract**: Spatial sound reasoning is a fundamental human skill, enabling us to navigate and interpret our surroundings based on sound. In this paper we present BAT, which combines the spatial sound perception ability of a binaural acoustic scene analysis model with the natural language reasoning capabilities of a large language model (LLM) to replicate this innate ability. To address the lack of existing datasets of in-the-wild spatial sounds, we synthesized a binaural audio dataset using AudioSet and SoundSpaces 2.0. Next, we developed SpatialSoundQA, a spatial sound-based question-answering dataset, offering a range of QA tasks that train BAT in various aspects of spatial sound perception and reasoning. The acoustic front end encoder of BAT is a novel spatial audio encoder named Spatial Audio Spectrogram Transformer, or Spatial-AST, which by itself achieves strong performance across sound event detection, spatial localization, and distance estimation. By integrating Spatial-AST with LLaMA-2 7B model, BAT transcends standard Sound Event Localization and Detection (SELD) tasks, enabling the model to reason about the relationships between the sounds in its environment. Our experiments demonstrate BAT's superior performance on both spatial sound perception and reasoning, showcasing the immense potential of LLMs in navigating and interpreting complex spatial audio environments.

## 生物分子(q-bio.BM:Biomolecules)

### Predicting ATP binding sites in protein sequences using Deep Learning and Natural Language Processing 
[[arxiv](https://arxiv.org/abs/2402.01829)] [[cool](https://papers.cool/arxiv/2402.01829)] [[pdf](https://arxiv.org/pdf/2402.01829)]
> **Authors**: Shreyas V,Swati Agarwal
> **First submission**: 2024-02-02
> **First announcement**: 2024-02-05
> **comment**: Published at 3rd Annual AAAI Workshop onAIto Accelerate Science and Engineering (AI2ASE)
- **标题**: None
- **领域**: 生物分子,计算语言学,机器学习
- **Abstract**: Predicting ATP-Protein Binding sites in genes is of great significance in the field of Biology and Medicine. The majority of research in this field has been conducted through time- and resource-intensive 'wet experiments' in laboratories. Over the years, researchers have been investigating computational methods computational methods to accomplish the same goals, utilising the strength of advanced Deep Learning and NLP algorithms. In this paper, we propose to develop methods to classify ATP-Protein binding sites. We conducted various experiments mainly using PSSMs and several word embeddings as features. We used 2D CNNs and LightGBM classifiers as our chief Deep Learning Algorithms. The MP3Vec and BERT models have also been subjected to testing in our study. The outcomes of our experiments demonstrated improvement over the state-of-the-art benchmarks.

## 量子物理学(quant-ph:Quantum Physics)

### Variational Quantum Circuits Enhanced Generative Adversarial Network 
[[arxiv](https://arxiv.org/abs/2402.01791)] [[cool](https://papers.cool/arxiv/2402.01791)] [[pdf](https://arxiv.org/pdf/2402.01791)]
> **Authors**: Runqiu Shu,Xusheng Xu,Man-Hong Yung,Wei Cui
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 量子物理学,人工智能,新兴技术,机器学习
- **Abstract**: Generative adversarial network (GAN) is one of the widely-adopted machine-learning frameworks for a wide range of applications such as generating high-quality images, video, and audio contents. However, training a GAN could become computationally expensive for large neural networks. In this work, we propose a hybrid quantum-classical architecture for improving GAN (denoted as QC-GAN). The performance was examed numerically by benchmarking with a classical GAN using MindSpore Quantum on the task of hand-written image generation. The generator of the QC-GAN consists of a quantum variational circuit together with a one-layer neural network, and the discriminator consists of a traditional neural network. Leveraging the entangling and expressive power of quantum circuits, our hybrid architecture achieved better performance (Frechet Inception Distance) than the classical GAN, with much fewer training parameters and number of iterations for convergence. We have also demonstrated the superiority of QC-GAN over an alternative quantum GAN, namely pathGAN, which could hardly generate 16$\times$16 or larger images. This work demonstrates the value of combining ideas from quantum computing with machine learning for both areas of Quantum-for-AI and AI-for-Quantum.

## 方法论(stat.ME:Methodology)

### "Clustering and Conquer" Procedures for Parallel Large-Scale Ranking and Selection 
[[arxiv](https://arxiv.org/abs/2402.02196)] [[cool](https://papers.cool/arxiv/2402.02196)] [[pdf](https://arxiv.org/pdf/2402.02196)]
> **Authors**: Zishi Zhang,Yijie Peng
> **First submission**: 2024-02-03
> **First announcement**: 2024-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 方法论,机器学习
- **Abstract**: This work breaks the sample efficiency bottleneck in parallel large-scale ranking and selection (R&S) problem by leveraging correlation information. We modify the commonly used "divide and conquer" framework in parallel computing by adding a correlation-based clustering step, transforming it into "clustering and conquer". This seemingly simple modification can achieve an $\mathcal{O}(p)$ sample complexity reduction rate, which represents the maximum attainable reduction for the class of sample-optimal R&S methods. Our approach enjoys two key advantages: 1) it does not require highly accurate correlation estimation or precise clustering, and 2) it allows for seamless integration with various existing R&S method, while achieving optimal sample complexity. Theoretically, we develop a novel gradient analysis framework to analyze sample efficiency and guide the design of large-scale R&S procedures. Building upon this framework, we propose a gradient-based budget allocation policy. We also introduce a new clustering algorithm, selection policy, and precision criterion tailored for large-scale scenarios. Finally, in large-scale AI applications such as neural architecture search, our methods demonstrate superior performance.

## 其他论文

- [The Gig's Up: How ChatGPT Stacks Up Against Quora on Gig Economy Insights](https://arxiv.org/abs/2402.02676)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [Exploring the Design Space for Message-Driven Systems for Dynamic Graph Processing using CCA](https://arxiv.org/abs/2402.02576)
  - **标题**: None
  - **Filtered Reason**: none of cs.AR,cs.DC in whitelist
- [Neuromorphic hardware for sustainable AI data centers](https://arxiv.org/abs/2402.02521)
  - **标题**: None
  - **Filtered Reason**: none of cs.ET,cs.NE,cs.DC in whitelist
- [AI Art Neural Constellation: Revealing the Collective and Contrastive State of AI-Generated and Human Art](https://arxiv.org/abs/2402.02453)
  - **标题**: None
  - **Filtered Reason**: none of cs.CV in whitelist
- [GPT-4V as Traffic Assistant: An In-depth Look at Vision Language Model on Complex Traffic Events](https://arxiv.org/abs/2402.02205)
  - **标题**: None
  - **Filtered Reason**: none of cs.CV in whitelist
- [CodeAgent: Autonomous Communicative Agents for Code Review](https://arxiv.org/abs/2402.02172)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Data Poisoning for In-context Learning](https://arxiv.org/abs/2402.02160)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [User Intent Recognition and Satisfaction with Large Language Models: A User Study with ChatGPT](https://arxiv.org/abs/2402.02136)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Low-power scalable multilayer optoelectronic neural networks enabled with incoherent light](https://arxiv.org/abs/2402.01988)
  - **标题**: None
  - **Filtered Reason**: none of cs.ET,physics.optics in whitelist
- [Mathemyths: Leveraging Large Language Models to Teach Mathematical Language through Child-AI Co-Creative Storytelling](https://arxiv.org/abs/2402.01927)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Are You a Real Software Engineer? Best Practices in Online Recruitment for Software Engineering Studies](https://arxiv.org/abs/2402.01925)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Large language models that replace human participants can harmfully misportray and flatten identity groups](https://arxiv.org/abs/2402.01908)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [An Educational Tool for Learning about Social Media Tracking, Profiling, and Recommendation](https://arxiv.org/abs/2402.01813)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [Using ChatGPT for Science Learning: A Study on Pre-service Teachers' Lesson Planning](https://arxiv.org/abs/2402.01674)
  - **标题**: None
  - **Filtered Reason**: none of physics.ed-ph,cs.CY in whitelist
- [Edge Offloading in Smart Grid](https://arxiv.org/abs/2402.01664)
  - **标题**: None
  - **Filtered Reason**: none of cs.NI,cs.DC in whitelist
- [Recommendations for public action towards sustainable generative AI systems](https://arxiv.org/abs/2402.01646)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [Integrating ChatGPT in a Computer Science Course: Students Perceptions and Suggestions](https://arxiv.org/abs/2402.01640)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY,cs.HC in whitelist
- [Backward Responsibility in Transition Systems Using General Power Indices](https://arxiv.org/abs/2402.01539)
  - **标题**: None
  - **Filtered Reason**: none of cs.FL in whitelist
- [Exploring the Effect of Multiple Natural Languages on Code Suggestion Using GitHub Copilot](https://arxiv.org/abs/2402.01438)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [CodePori: Large-Scale System for Autonomous Software Development Using Multi-Agent Technology](https://arxiv.org/abs/2402.01411)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Can Large Language Models Serve as Data Analysts? A Multi-Agent Assisted Approach for Qualitative Data Analysis](https://arxiv.org/abs/2402.01386)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Improving Sequential Recommendations with LLMs](https://arxiv.org/abs/2402.01339)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [LimSim++: A Closed-Loop Platform for Deploying Multimodal LLMs in Autonomous Driving](https://arxiv.org/abs/2402.01246)
  - **标题**: None
  - **Filtered Reason**: none of eess.SY,cs.RO in whitelist
- [An Empirical Study on Low Code Programming using Traditional vs Large Language Model Support](https://arxiv.org/abs/2402.01156)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
