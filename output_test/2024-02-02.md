> 本文由 [https://github.com/huiyeruzhou/arxiv_crawler](https://github.com/huiyeruzhou/arxiv_crawler) 自动生成
>
> 领域白名单：cs.AI,cs.CL,cs.LG
> 关键词： LLM, GPT, AI

# 论文全览：2024-02-02

共有94篇相关领域论文, 另有18篇其他

## 人工智能(cs.AI:Artificial Intelligence)

### PokeLLMon: A Human-Parity Agent for Pokemon Battles with Large Language Models 
[[arxiv](https://arxiv.org/abs/2402.01118)] [[cool](https://papers.cool/arxiv/2402.01118)] [[pdf](https://arxiv.org/pdf/2402.01118)]
> **Authors**: Sihao Hu,Tiansheng Huang,Ling Liu
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: 10 pages
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: We introduce PokeLLMon, the first LLM-embodied agent that achieves human-parity performance in tactical battle games, as demonstrated in Pokemon battles. The design of PokeLLMon incorporates three key strategies: (i) In-context reinforcement learning that instantly consumes text-based feedback derived from battles to iteratively refine the policy; (ii) Knowledge-augmented generation that retrieves external knowledge to counteract hallucination and enables the agent to act timely and properly; (iii) Consistent action generation to mitigate the panic switching phenomenon when the agent faces a powerful opponent and wants to elude the battle. We show that online battles against human demonstrates PokeLLMon's human-like battle strategies and just-in-time decision making, achieving 49% of win rate in the Ladder competitions and 56% of win rate in the invited battles. Our implementation and playable battle logs are available at: https://github.com/git-disl/PokeLLMon.

### Real Sparks of Artificial Intelligence and the Importance of Inner Interpretability 
[[arxiv](https://arxiv.org/abs/2402.00901)] [[cool](https://papers.cool/arxiv/2402.00901)] [[pdf](https://arxiv.org/pdf/2402.00901)]
> **Authors**: Alex Grzankowski
> **First submission**: 2024-01-31
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: The present paper looks at one of the most thorough articles on the intelligence of GPT, research conducted by engineers at Microsoft. Although there is a great deal of value in their work, I will argue that, for familiar philosophical reasons, their methodology, !Blackbox Interpretability"#is wrongheaded. But there is a better way. There is an exciting and emerging discipline of !Inner Interpretability"#(and specifically Mechanistic Interpretability) that aims to uncover the internal activations and weights of models in order to understand what they represent and the algorithms they implement. In my view, a crucial mistake in Black-box Interpretability is the failure to appreciate that how processes are carried out matters when it comes to intelligence and understanding. I can#t pretend to have a full story that provides both necessary and sufficient conditions for being intelligent, but I do think that Inner Interpretability dovetails nicely with plausible philosophical views of what intelligence requires. So the conclusion is modest, but the important point in my view is seeing how to get the research on the right track. Towards the end of the paper, I will show how some of the philosophical concepts can be used to further refine how Inner Interpretability is approached, so the paper helps draw out a profitable, future two-way exchange between Philosophers and Computer Scientists.

### Intent Assurance using LLMs guided by Intent Drift 
[[arxiv](https://arxiv.org/abs/2402.00715)] [[cool](https://papers.cool/arxiv/2402.00715)] [[pdf](https://arxiv.org/pdf/2402.00715)]
> **Authors**: Kristina Dzeparoska,Ali Tizghadam,Alberto Leon-Garcia
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,网络和互联网架构,方法论
- **Abstract**: Intent-Based Networking (IBN) presents a paradigm shift for network management, by promising to align intents and business objectives with network operations--in an automated manner. However, its practical realization is challenging: 1) processing intents, i.e., translate, decompose and identify the logic to fulfill the intent, and 2) intent conformance, that is, considering dynamic networks, the logic should be adequately adapted to assure intents. To address the latter, intent assurance is tasked with continuous verification and validation, including taking the necessary actions to align the operational and target states. In this paper, we define an assurance framework that allows us to detect and act when intent drift occurs. To do so, we leverage AI-driven policies, generated by Large Language Models (LLMs) which can quickly learn the necessary in-context requirements, and assist with the fulfillment and assurance of intents.

### Learning Planning-based Reasoning by Trajectories Collection and Process Reward Synthesizing 
[[arxiv](https://arxiv.org/abs/2402.00658)] [[cool](https://papers.cool/arxiv/2402.00658)] [[pdf](https://arxiv.org/pdf/2402.00658)]
> **Authors**: Fangkai Jiao,Chengwei Qin,Zhengyuan Liu,Nancy F. Chen,Shafiq Joty
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: 17 pages, 9 figures. EMNLP 2024
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: Large Language Models (LLMs) have demonstrated significant potential in handling complex reasoning tasks through step-by-step rationale generation. However, recent studies have raised concerns regarding the hallucination and flaws in their reasoning process. Substantial efforts are being made to improve the reliability and faithfulness of the generated rationales. Some approaches model reasoning as planning, while others focus on annotating for process supervision. Nevertheless, the planning-based search process often results in high latency due to the frequent assessment of intermediate reasoning states and the extensive exploration space. Additionally, supervising the reasoning process with human annotation is costly and challenging to scale for LLM training. To address these issues, in this paper, we propose a framework to learn planning-based reasoning through Direct Preference Optimization (DPO) on collected trajectories, which are ranked according to synthesized process rewards. Our results on challenging logical reasoning benchmarks demonstrate the effectiveness of our learning framework, showing that our 7B model can surpass the strong counterparts like GPT-3.5-Turbo.

### Computational Experiments Meet Large Language Model Based Agents: A Survey and Perspective 
[[arxiv](https://arxiv.org/abs/2402.00262)] [[cool](https://papers.cool/arxiv/2402.00262)] [[pdf](https://arxiv.org/pdf/2402.00262)]
> **Authors**: Qun Ma,Xiao Xue,Deyu Zhou,Xiangning Yu,Donghua Liu,Xuwen Zhang,Zihan Zhao,Yifan Shen,Peilin Ji,Juanjuan Li,Gang Wang,Wanpeng Ma
> **First submission**: 2024-01-31
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Computational experiments have emerged as a valuable method for studying complex systems, involving the algorithmization of counterfactuals. However, accurately representing real social systems in Agent-based Modeling (ABM) is challenging due to the diverse and intricate characteristics of humans, including bounded rationality and heterogeneity. To address this limitation, the integration of Large Language Models (LLMs) has been proposed, enabling agents to possess anthropomorphic abilities such as complex reasoning and autonomous learning. These agents, known as LLM-based Agent, offer the potential to enhance the anthropomorphism lacking in ABM. Nonetheless, the absence of explicit explainability in LLMs significantly hinders their application in the social sciences. Conversely, computational experiments excel in providing causal analysis of individual behaviors and complex phenomena. Thus, combining computational experiments with LLM-based Agent holds substantial research potential. This paper aims to present a comprehensive exploration of this fusion. Primarily, it outlines the historical development of agent structures and their evolution into artificial societies, emphasizing their importance in computational experiments. Then it elucidates the advantages that computational experiments and LLM-based Agents offer each other, considering the perspectives of LLM-based Agent for computational experiments and vice versa. Finally, this paper addresses the challenges and future trends in this research domain, offering guidance for subsequent related studies.

### Zero-shot Sequential Neuro-symbolic Reasoning for Automatically Generating Architecture Schematic Designs 
[[arxiv](https://arxiv.org/abs/2402.00052)] [[cool](https://papers.cool/arxiv/2402.00052)] [[pdf](https://arxiv.org/pdf/2402.00052)]
> **Authors**: Milin Kodnongbua,Lawrence H. Curtis,Adriana Schulz
> **First submission**: 2024-01-25
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算机视觉和模式识别,图形
- **Abstract**: This paper introduces a novel automated system for generating architecture schematic designs aimed at streamlining complex decision-making at the multifamily real estate development project's outset. Leveraging the combined strengths of generative AI (neuro reasoning) and mathematical program solvers (symbolic reasoning), the method addresses both the reliance on expert insights and technical challenges in architectural schematic design. To address the large-scale and interconnected nature of design decisions needed for designing a whole building, we proposed a novel sequential neuro-symbolic reasoning approach, emulating traditional architecture design processes from initial concept to detailed layout. To remove the need to hand-craft a cost function to approximate the desired objectives, we propose a solution that uses neuro reasoning to generate constraints and cost functions that the symbolic solvers can use to solve. We also incorporate feedback loops for each design stage to ensure a tight integration between neuro and symbolic reasoning. Developed using GPT-4 without further training, our method's effectiveness is validated through comparative studies with real-world buildings. Our method can generate various building designs in accordance with the understanding of the neighborhood, showcasing its potential to transform the realm of architectural schematic design.

### IICONGRAPH: improved Iconographic and Iconological Statements in Knowledge Graphs 
[[arxiv](https://arxiv.org/abs/2402.00048)] [[cool](https://papers.cool/arxiv/2402.00048)] [[pdf](https://arxiv.org/pdf/2402.00048)]
> **Authors**: Bruno Sartini
> **First submission**: 2024-01-24
> **First announcement**: 2024-02-02
> **comment**: 18 pages
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Iconography and iconology are fundamental domains when it comes to understanding artifacts of cultural heritage. Iconography deals with the study and interpretation of visual elements depicted in artifacts and their symbolism, while iconology delves deeper, exploring the underlying cultural and historical meanings. Despite the advances in representing cultural heritage with Linked Open Data (LOD), recent studies show persistent gaps in the representation of iconographic and iconological statements in current knowledge graphs (KGs). To address them, this paper presents IICONGRAPH, a KG that was created by refining and extending the iconographic and iconological statements of ArCo (the Italian KG of cultural heritage) and Wikidata. The development of IICONGRAPH was also driven by a series of requirements emerging from research case studies that were unattainable in the non-reengineered versions of the KGs. The evaluation results demonstrate that IICONGRAPH not only outperforms ArCo and Wikidata through domain-specific assessments from the literature but also serves as a robust platform for addressing the formulated research questions. IICONGRAPH is released and documented in accordance with the FAIR principles to guarantee the resource's reusability. The algorithms used to create it and assess the research questions have also been made available to ensure transparency and reproducibility. While future work focuses on ingesting more data into the KG, and on implementing it as a backbone of LLM-based question answering systems, the current version of IICONGRAPH still emerges as a valuable asset, contributing to the evolving landscape of cultural heritage representation within Knowledge Graphs, the Semantic Web, and beyond.

### Maintaining User Trust Through Multistage Uncertainty Aware Inference 
[[arxiv](https://arxiv.org/abs/2402.00015)] [[cool](https://papers.cool/arxiv/2402.00015)] [[pdf](https://arxiv.org/pdf/2402.00015)]
> **Authors**: Chandan Agrawal,Ashish Papanai,Jerome White
> **First submission**: 2023-12-28
> **First announcement**: 2024-02-02
> **comment**: ef:Presented at DeployableAIWorkshop at AAAI-2024
- **标题**: None
- **领域**: 人工智能,计算机视觉和模式识别
- **Abstract**: This paper describes and evaluates a multistage approach to AI deployment. Each stage involves a more accurate method of inference, yet engaging each comes with an increasing cost. In outlining the architecture, we present a method for quantifying model uncertainty that facilitates confident deferral decisions. The architecture is currently under active deployment to thousands of cotton farmers across India. The broader idea however is applicable to a growing sector of AI deployments in challenging low resources settings.

## 硬件架构(cs.AR:Hardware Architecture)

### Using the Abstract Computer Architecture Description Language to Model AI Hardware Accelerators 
[[arxiv](https://arxiv.org/abs/2402.00069)] [[cool](https://papers.cool/arxiv/2402.00069)] [[pdf](https://arxiv.org/pdf/2402.00069)]
> **Authors**: Mika Markus Müller,Alexander Richard Manfred Borst,Konstantin Lübeck,Alexander Louis-Ferdinand Jung,Oliver Bringmann
> **First submission**: 2024-01-30
> **First announcement**: 2024-02-02
> **comment**: Accepted Version for: MBMV'24
- **标题**: None
- **领域**: 硬件架构,人工智能
- **Abstract**: Artificial Intelligence (AI) has witnessed remarkable growth, particularly through the proliferation of Deep Neural Networks (DNNs). These powerful models drive technological advancements across various domains. However, to harness their potential in real-world applications, specialized hardware accelerators are essential. This demand has sparked a market for parameterizable AI hardware accelerators offered by different vendors. Manufacturers of AI-integrated products face a critical challenge: selecting an accelerator that aligns with their product's performance requirements. The decision involves choosing the right hardware and configuring a suitable set of parameters. However, comparing different accelerator design alternatives remains a complex task. Often, engineers rely on data sheets, spreadsheet calculations, or slow black-box simulators, which only offer a coarse understanding of the performance characteristics. The Abstract Computer Architecture Description Language (ACADL) is a concise formalization of computer architecture block diagrams, which helps to communicate computer architecture on different abstraction levels and allows for inferring performance characteristics. In this paper, we demonstrate how to use the ACADL to model AI hardware accelerators, use their ACADL description to map DNNs onto them, and explain the timing simulation semantics to gather performance results.

## 计算语言学(cs.CL:Computation and Language)

### DTS-SQL: Decomposed Text-to-SQL with Small Large Language Models 
[[arxiv](https://arxiv.org/abs/2402.01117)] [[cool](https://papers.cool/arxiv/2402.01117)] [[pdf](https://arxiv.org/pdf/2402.01117)]
> **Authors**: Mohammadreza Pourreza,Davood Rafiei
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,数据库,人机交互
- **Abstract**: Leading models for the text-to-SQL task heavily rely on proprietary Large Language Models (LLMs), posing concerns over data privacy. Closing the performance gap between small open-source models and large proprietary models is crucial to mitigate this reliance. To this end, we introduce a novel two-stage fine-tuning approach that decomposes the task into two simpler tasks. Through comprehensive evaluation on two large cross-domain datasets and two small LLMs, we show that this approach improves execution accuracy by 3 to 7 percent, effectively aligning the performance of open-source models with their proprietary counterparts.

### Interpretation of Intracardiac Electrograms Through Textual Representations 
[[arxiv](https://arxiv.org/abs/2402.01115)] [[cool](https://papers.cool/arxiv/2402.01115)] [[pdf](https://arxiv.org/pdf/2402.01115)]
> **Authors**: William Jongwon Han,Diana Gomez,Avi Alok,Chaojing Duan,Michael A. Rosenberg,Douglas Weber,Emerson Liu,Ding Zhao
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: 17 pages, 7 figures; Accepted to CHIL 2024
- **标题**: None
- **领域**: 计算语言学,信号处理
- **Abstract**: Understanding the irregular electrical activity of atrial fibrillation (AFib) has been a key challenge in electrocardiography. For serious cases of AFib, catheter ablations are performed to collect intracardiac electrograms (EGMs). EGMs offer intricately detailed and localized electrical activity of the heart and are an ideal modality for interpretable cardiac studies. Recent advancements in artificial intelligence (AI) has allowed some works to utilize deep learning frameworks to interpret EGMs during AFib. Additionally, language models (LMs) have shown exceptional performance in being able to generalize to unseen domains, especially in healthcare. In this study, we are the first to leverage pretrained LMs for finetuning of EGM interpolation and AFib classification via masked language modeling. We formulate the EGM as a textual sequence and present competitive performances on AFib classification compared against other representations. Lastly, we provide a comprehensive interpretability study to provide a multi-perspective intuition of the model's behavior, which could greatly benefit the clinical use.

### Reasoning Capacity in Multi-Agent Systems: Limitations, Challenges and Human-Centered Solutions 
[[arxiv](https://arxiv.org/abs/2402.01108)] [[cool](https://papers.cool/arxiv/2402.01108)] [[pdf](https://arxiv.org/pdf/2402.01108)]
> **Authors**: Pouya Pezeshkpour,Eser Kandogan,Nikita Bhutani,Sajjadur Rahman,Tom Mitchell,Estevam Hruschka
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Remarkable performance of large language models (LLMs) in a variety of tasks brings forth many opportunities as well as challenges of utilizing them in production settings. Towards practical adoption of LLMs, multi-agent systems hold great promise to augment, integrate, and orchestrate LLMs in the larger context of enterprise platforms that use existing proprietary data and models to tackle complex real-world tasks. Despite the tremendous success of these systems, current approaches rely on narrow, single-focus objectives for optimization and evaluation, often overlooking potential constraints in real-world scenarios, including restricted budgets, resources and time. Furthermore, interpreting, analyzing, and debugging these systems requires different components to be evaluated in relation to one another. This demand is currently not feasible with existing methodologies. In this postion paper, we introduce the concept of reasoning capacity as a unifying criterion to enable integration of constraints during optimization and establish connections among different components within the system, which also enable a more holistic and comprehensive approach to evaluation. We present a formal definition of reasoning capacity and illustrate its utility in identifying limitations within each component of the system. We then argue how these limitations can be addressed with a self-reflective process wherein human-feedback is used to alleviate shortcomings in reasoning and enhance overall consistency of the system.

### Evaluation Methodology for Large Language Models for Multilingual Document Question and Answer 
[[arxiv](https://arxiv.org/abs/2402.01065)] [[cool](https://papers.cool/arxiv/2402.01065)] [[pdf](https://arxiv.org/pdf/2402.01065)]
> **Authors**: Adar Kahana,Jaya Susan Mathew,Said Bleik,Jeremy Reynolds,Oren Elisha
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: With the widespread adoption of Large Language Models (LLMs), in this paper we investigate the multilingual capability of these models. Our preliminary results show that, translating the native language context, question and answer into a high resource language produced the best results.

### Plan-Grounded Large Language Models for Dual Goal Conversational Settings 
[[arxiv](https://arxiv.org/abs/2402.01053)] [[cool](https://papers.cool/arxiv/2402.01053)] [[pdf](https://arxiv.org/pdf/2402.01053)]
> **Authors**: Diogo Glória-Silva,Rafael Ferreira,Diogo Tavares,David Semedo,João Magalhães
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Training Large Language Models (LLMs) to follow user instructions has been shown to supply the LLM with ample capacity to converse fluently while being aligned with humans. Yet, it is not completely clear how an LLM can lead a plan-grounded conversation in mixed-initiative settings where instructions flow in both directions of the conversation, i.e. both the LLM and the user provide instructions to one another. In this paper, we tackle a dual goal mixed-initiative conversational setting where the LLM not only grounds the conversation on an arbitrary plan but also seeks to satisfy both a procedural plan and user instructions. The LLM is then responsible for guiding the user through the plan and, at the same time, adapting to new circumstances, answering questions, and activating safety guardrails when needed. We propose a novel LLM that grounds the dialogue on a procedural plan, can take the dialogue initiative, and enforces guardrails on the system's behavior, while also improving the LLM's responses to unexpected user behavior. Experiments in controlled settings and with real users show that the best-performing model, which we call PlanLLM, achieves a 2.1x improvement over a strong baseline. Moreover, experiments also show good generalization to unseen domains.

### Generation, Distillation and Evaluation of Motivational Interviewing-Style Reflections with a Foundational Language Model 
[[arxiv](https://arxiv.org/abs/2402.01051)] [[cool](https://papers.cool/arxiv/2402.01051)] [[pdf](https://arxiv.org/pdf/2402.01051)]
> **Authors**: Andrew Brown,Jiading Zhu,Mohamed Abdelwahab,Alec Dong,Cindy Wang,Jonathan Rose
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: Accepted to EACL 2024 Long Paper
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Foundational Language Models are capable of performing many tasks at a high level but are difficult to deploy in many applications because of their size and proprietary ownership. Many will be motivated to distill specific capabilities of foundational models into smaller models that can be owned and controlled. In the development of a therapeutic chatbot, we wish to distill a capability known as reflective listening, in which a therapist produces reflections of client speech. These reflections either restate what a client has said, or connect what was said to a relevant observation, idea or guess that encourages and guides the client to continue contemplation. In this paper, we present a method for distilling the generation of reflections from a Foundational Language Model (GPT-4) into smaller models. We first show that GPT-4, using zero-shot prompting, can generate reflections at near 100% success rate, superior to all previous methods. Using reflections generated by GPT-4, we fine-tune different sizes of the GPT-2 family. The GPT-2-small model achieves 83% success on a hold-out test set and the GPT-2 XL achieves 90% success. We also show that GPT-4 can help in the labor-intensive task of evaluating the quality of the distilled models, using it as a zero-shot classifier. Using triple-human review as a guide, the classifier achieves a Cohen-Kappa of 0.66, a substantial inter-rater reliability figure.

### Getting the most out of your tokenizer for pre-training and domain adaptation 
[[arxiv](https://arxiv.org/abs/2402.01035)] [[cool](https://papers.cool/arxiv/2402.01035)] [[pdf](https://arxiv.org/pdf/2402.01035)]
> **Authors**: Gautier Dagan,Gabriel Synnaeve,Baptiste Rozière
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Tokenization is an understudied and often neglected component of modern LLMs. Most published works use a single tokenizer for all experiments, often borrowed from another model, without performing ablations or analysis to optimize tokenization. Moreover, the tokenizer is generally kept unchanged when fine-tuning a base model. In this paper, we show that the size, pre-tokenization regular expression, and training data of a tokenizer can significantly impact the model's generation speed, effective context size, memory usage, and downstream performance. We train specialized Byte-Pair Encoding code tokenizers, and conduct extensive ablations on the impact of tokenizer design on the performance of LLMs for code generation tasks such as HumanEval and MBPP, and provide recommendations for tokenizer hyper-parameters selection and switching the tokenizer in a pre-trained LLM. We perform our experiments on models trained from scratch and from pre-trained models, verifying their applicability to a wide range of use-cases. We find that when fine-tuning on more than 50 billion tokens, we can specialize the tokenizer of a pre-trained LLM to obtain large gains in generation speed and effective context size.

### Executable Code Actions Elicit Better LLM Agents 
[[arxiv](https://arxiv.org/abs/2402.01030)] [[cool](https://papers.cool/arxiv/2402.01030)] [[pdf](https://arxiv.org/pdf/2402.01030)]
> **Authors**: Xingyao Wang,Yangyi Chen,Lifan Yuan,Yizhe Zhang,Yunzhu Li,Hao Peng,Heng Ji
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: Accepted by ICML 2024; Code, data, model, and demo are available at https://github.com/xingyaoww/code-act
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large Language Model (LLM) agents, capable of performing a broad range of actions, such as invoking tools and controlling robots, show great potential in tackling real-world challenges. LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools). This work proposes to use executable Python code to consolidate LLM agents' actions into a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions. Our extensive analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that CodeAct outperforms widely used alternatives (up to 20% higher success rate). The encouraging performance of CodeAct motivates us to build an open-source LLM agent that interacts with environments by executing interpretable code and collaborates with users using natural language. To this end, we collect an instruction-tuning dataset CodeActInstruct that consists of 7k multi-turn interactions using CodeAct. We show that it can be used with existing data to improve models in agent-oriented tasks without compromising their general capability. CodeActAgent, finetuned from Llama2 and Mistral, is integrated with Python interpreter and uniquely tailored to perform sophisticated tasks (e.g., model training) using existing libraries and autonomously self-debug.

### HR-MultiWOZ: A Task Oriented Dialogue (TOD) Dataset for HR LLM Agent 
[[arxiv](https://arxiv.org/abs/2402.01018)] [[cool](https://papers.cool/arxiv/2402.01018)] [[pdf](https://arxiv.org/pdf/2402.01018)]
> **Authors**: Weijie Xu,Zicheng Huang,Wenxiang Hu,Xi Fang,Rajesh Kumar Cherukuri,Naumaan Nayyar,Lorenzo Malandri,Srinivasan H. Sengamedu
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: 13 pages, 9 figures
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Recent advancements in Large Language Models (LLMs) have been reshaping Natural Language Processing (NLP) task in several domains. Their use in the field of Human Resources (HR) has still room for expansions and could be beneficial for several time consuming tasks. Examples such as time-off submissions, medical claims filing, and access requests are noteworthy, but they are by no means the sole instances. However, the aforementioned developments must grapple with the pivotal challenge of constructing a high-quality training dataset. On one hand, most conversation datasets are solving problems for customers not employees. On the other hand, gathering conversations with HR could raise privacy concerns. To solve it, we introduce HR-Multiwoz, a fully-labeled dataset of 550 conversations spanning 10 HR domains to evaluate LLM Agent. Our work has the following contributions: (1) It is the first labeled open-sourced conversation dataset in the HR domain for NLP research. (2) It provides a detailed recipe for the data generation procedure along with data analysis and human evaluations. The data generation pipeline is transferable and can be easily adapted for labeled conversation data generation in other domains. (3) The proposed data-collection pipeline is mostly based on LLMs with minimal human involvement for annotation, which is time and cost-efficient.

### SPARQL Generation with Entity Pre-trained GPT for KG Question Answering 
[[arxiv](https://arxiv.org/abs/2402.00969)] [[cool](https://papers.cool/arxiv/2402.00969)] [[pdf](https://arxiv.org/pdf/2402.00969)]
> **Authors**: Diego Bustamante,Hideaki Takeda
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: 7 pages, 1 figure, 2 tables. For the implementation, see https://github.com/DiegoEmilio01/SPARQL-generation-with-entity-pre-trained-GPT-for-KG-Question-Answering
- **标题**: None
- **领域**: 计算语言学,人工智能,数据库,信息检索
- **Abstract**: Knowledge Graphs popularity has been rapidly growing in last years. All that knowledge is available for people to query it through the many online databases on the internet. Though, it would be a great achievement if non-programmer users could access whatever information they want to know. There has been a lot of effort oriented to solve this task using natural language processing tools and creativity encouragement by way of many challenges. Our approach focuses on assuming a correct entity linking on the natural language questions and training a GPT model to create SPARQL queries from them. We managed to isolate which property of the task can be the most difficult to solve at few or zero-shot and we proposed pre-training on all entities (under CWA) to improve the performance. We obtained a 62.703% accuracy of exact SPARQL matches on testing at 3-shots, a F1 of 0.809 on the entity linking challenge and a F1 of 0.009 on the question answering challenge.

### Exploring Spatial Schema Intuitions in Large Language and Vision Models 
[[arxiv](https://arxiv.org/abs/2402.00956)] [[cool](https://papers.cool/arxiv/2402.00956)] [[pdf](https://arxiv.org/pdf/2402.00956)]
> **Authors**: Philipp Wicke,Lennart Wachowiak
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: ACL Findings 2024
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Despite the ubiquity of large language models (LLMs) in AI research, the question of embodiment in LLMs remains underexplored, distinguishing them from embodied systems in robotics where sensory perception directly informs physical action. Our investigation navigates the intriguing terrain of whether LLMs, despite their non-embodied nature, effectively capture implicit human intuitions about fundamental, spatial building blocks of language. We employ insights from spatial cognitive foundations developed through early sensorimotor experiences, guiding our exploration through the reproduction of three psycholinguistic experiments. Surprisingly, correlations between model outputs and human responses emerge, revealing adaptability without a tangible connection to embodied experiences. Notable distinctions include polarized language model responses and reduced correlations in vision language models. This research contributes to a nuanced understanding of the interplay between language, spatial experiences, and the computations made by large language models. More at https://cisnlp.github.io/Spatial_Schemas/

### Security and Privacy Challenges of Large Language Models: A Survey 
[[arxiv](https://arxiv.org/abs/2402.00888)] [[cool](https://papers.cool/arxiv/2402.00888)] [[pdf](https://arxiv.org/pdf/2402.00888)]
> **Authors**: Badhan Chandra Das,M. Hadi Amini,Yanzhao Wu
> **First submission**: 2024-01-29
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,密码学和安全
- **Abstract**: Large Language Models (LLMs) have demonstrated extraordinary capabilities and contributed to multiple fields, such as generating and summarizing text, language translation, and question-answering. Nowadays, LLM is becoming a very popular tool in computerized language processing tasks, with the capability to analyze complicated linguistic patterns and provide relevant and appropriate responses depending on the context. While offering significant advantages, these models are also vulnerable to security and privacy attacks, such as jailbreaking attacks, data poisoning attacks, and Personally Identifiable Information (PII) leakage attacks. This survey provides a thorough review of the security and privacy challenges of LLMs for both training data and users, along with the application-based risks in various domains, such as transportation, education, and healthcare. We assess the extent of LLM vulnerabilities, investigate emerging security and privacy attacks for LLMs, and review the potential defense mechanisms. Additionally, the survey outlines existing research gaps in this domain and highlights future research directions.

### Can Large Language Models Understand Context? 
[[arxiv](https://arxiv.org/abs/2402.00858)] [[cool](https://papers.cool/arxiv/2402.00858)] [[pdf](https://arxiv.org/pdf/2402.00858)]
> **Authors**: Yilun Zhu,Joel Ruben Antony Moniz,Shruti Bhargava,Jiarui Lu,Dhivya Piraviperumal,Site Li,Yuan Zhang,Hong Yu,Bo-Hsiang Tseng
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: Findings of EACL 2024
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Understanding context is key to understanding human language, an ability which Large Language Models (LLMs) have been increasingly seen to demonstrate to an impressive extent. However, though the evaluation of LLMs encompasses various domains within the realm of Natural Language Processing, limited attention has been paid to probing their linguistic capability of understanding contextual features. This paper introduces a context understanding benchmark by adapting existing datasets to suit the evaluation of generative models. This benchmark comprises of four distinct tasks and nine datasets, all featuring prompts designed to assess the models' ability to understand context. First, we evaluate the performance of LLMs under the in-context learning pretraining scenario. Experimental results indicate that pre-trained dense models struggle with understanding more nuanced contextual features when compared to state-of-the-art fine-tuned models. Second, as LLM compression holds growing significance in both research and real-world applications, we assess the context understanding of quantized models under in-context-learning settings. We find that 3-bit post-training quantization leads to varying degrees of performance reduction on our benchmark. We conduct an extensive analysis of these scenarios to substantiate our experimental results.

### Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight in the Real World for Meeting Summarization? 
[[arxiv](https://arxiv.org/abs/2402.00841)] [[cool](https://papers.cool/arxiv/2402.00841)] [[pdf](https://arxiv.org/pdf/2402.00841)]
> **Authors**: Xue-Yong Fu,Md Tahmid Rahman Laskar,Elena Khasanova,Cheng Chen,Shashi Bhushan TN
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: Accepted by NAACL 2024 (Industry Track). The first two authors contributed equally to this work
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models (LLMs) have demonstrated impressive capabilities to solve a wide range of tasks without being explicitly fine-tuned on task-specific datasets. However, deploying LLMs in the real world is not trivial, as it requires substantial computing resources. In this paper, we investigate whether smaller, compact LLMs are a good alternative to the comparatively Larger LLMs2 to address significant costs associated with utilizing LLMs in the real world. In this regard, we study the meeting summarization task in a real-world industrial environment and conduct extensive experiments by comparing the performance of fine-tuned compact LLMs (e.g., FLAN-T5, TinyLLaMA, LiteLLaMA) with zero-shot larger LLMs (e.g., LLaMA-2, GPT-3.5, PaLM-2). We observe that most smaller LLMs, even after fine-tuning, fail to outperform larger zero-shot LLMs in meeting summarization datasets. However, a notable exception is FLAN-T5 (780M parameters), which performs on par or even better than many zero-shot Larger LLMs (from 7B to above 70B parameters), while being significantly smaller. This makes compact LLMs like FLAN-T5 a suitable cost-efficient solution for real-world industrial deployment.

### Health-LLM: Personalized Retrieval-Augmented Disease Prediction System 
[[arxiv](https://arxiv.org/abs/2402.00746)] [[cool](https://papers.cool/arxiv/2402.00746)] [[pdf](https://arxiv.org/pdf/2402.00746)]
> **Authors**: Qinkai Yu,Mingyu Jin,Dong Shu,Chong Zhang,Lizhou Fan,Wenyue Hua,Suiyuan Zhu,Yanda Meng,Zhenting Wang,Mengnan Du,Yongfeng Zhang
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Recent advancements in artificial intelligence (AI), especially large language models (LLMs), have significantly advanced healthcare applications and demonstrated potentials in intelligent medical treatment. However, there are conspicuous challenges such as vast data volumes and inconsistent symptom characterization standards, preventing full integration of healthcare AI systems with individual patients' needs. To promote professional and personalized healthcare, we propose an innovative framework, Heath-LLM, which combines large-scale feature extraction and medical knowledge trade-off scoring. Compared to traditional health management applications, our system has three main advantages: (1) It integrates health reports and medical knowledge into a large model to ask relevant questions to large language model for disease prediction; (2) It leverages a retrieval augmented generation (RAG) mechanism to enhance feature extraction; (3) It incorporates a semi-automated feature updating framework that can merge and delete features to improve accuracy of disease prediction. We experiment on a large number of health reports to assess the effectiveness of Health-LLM system. The results indicate that the proposed system surpasses the existing ones and has the potential to significantly advance disease prediction and personalized health management.

### Enhancing Ethical Explanations of Large Language Models through Iterative Symbolic Refinement 
[[arxiv](https://arxiv.org/abs/2402.00745)] [[cool](https://papers.cool/arxiv/2402.00745)] [[pdf](https://arxiv.org/pdf/2402.00745)]
> **Authors**: Xin Quan,Marco Valentino,Louise A. Dennis,André Freitas
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: Camera-ready for EACL 2024
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: An increasing amount of research in Natural Language Inference (NLI) focuses on the application and evaluation of Large Language Models (LLMs) and their reasoning capabilities. Despite their success, however, LLMs are still prone to factual errors and inconsistencies in their explanations, offering limited control and interpretability for inference in complex domains. In this paper, we focus on ethical NLI, investigating how hybrid neuro-symbolic techniques can enhance the logical validity and alignment of ethical explanations produced by LLMs. Specifically, we present an abductive-deductive framework named Logic-Explainer, which integrates LLMs with an external backward-chaining solver to refine step-wise natural language explanations and jointly verify their correctness, reduce incompleteness and minimise redundancy. An extensive empirical analysis demonstrates that Logic-Explainer can improve explanations generated via in-context learning methods and Chain-of-Thought (CoT) on challenging ethical NLI tasks, while, at the same time, producing formal proofs describing and supporting models' reasoning. As ethical NLI requires commonsense reasoning to identify underlying moral violations, our results suggest the effectiveness of neuro-symbolic methods for multi-step NLI more broadly, opening new opportunities to enhance the logical consistency, reliability, and alignment of LLMs.

### Improving Weak-to-Strong Generalization with Scalable Oversight and Ensemble Learning 
[[arxiv](https://arxiv.org/abs/2402.00667)] [[cool](https://papers.cool/arxiv/2402.00667)] [[pdf](https://arxiv.org/pdf/2402.00667)]
> **Authors**: Jitao Sang,Yuhang Wang,Jing Zhang,Yanxu Zhu,Chao Kong,Junhong Ye,Shuyu Wei,Jinlin Xiao
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: This paper presents a follow-up study to OpenAI's recent superalignment work on Weak-to-Strong Generalization (W2SG). Superalignment focuses on ensuring that high-level AI systems remain consistent with human values and intentions when dealing with complex, high-risk tasks. The W2SG framework has opened new possibilities for empirical research in this evolving field. Our study simulates two phases of superalignment under the W2SG framework: the development of general superhuman models and the progression towards superintelligence. In the first phase, based on human supervision, the quality of weak supervision is enhanced through a combination of scalable oversight and ensemble learning, reducing the capability gap between weak teachers and strong students. In the second phase, an automatic alignment evaluator is employed as the weak supervisor. By recursively updating this auto aligner, the capabilities of the weak teacher models are synchronously enhanced, achieving weak-to-strong supervision over stronger student models.We also provide an initial validation of the proposed approach for the first phase. Using the SciQ task as example, we explore ensemble learning for weak teacher models through bagging and boosting. Scalable oversight is explored through two auxiliary settings: human-AI interaction and AI-AI debate. Additionally, the paper discusses the impact of improved weak supervision on enhancing weak-to-strong generalization based on in-context learning. Experiment code and dataset will be released at https://github.com/ADaM-BJTU/W2SG.

### Actor Identification in Discourse: A Challenge for LLMs? 
[[arxiv](https://arxiv.org/abs/2402.00620)] [[cool](https://papers.cool/arxiv/2402.00620)] [[pdf](https://arxiv.org/pdf/2402.00620)]
> **Authors**: Ana Barić,Sean Papay,Sebastian Padó
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: Proceedings of the EACL 2024 workshop on Computational Models of Discourse (St. Julian's, Malta)
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The identification of political actors who put forward claims in public debate is a crucial step in the construction of discourse networks, which are helpful to analyze societal debates. Actor identification is, however, rather challenging: Often, the locally mentioned speaker of a claim is only a pronoun ("He proposed that [claim]"), so recovering the canonical actor name requires discourse understanding. We compare a traditional pipeline of dedicated NLP components (similar to those applied to the related task of coreference) with a LLM, which appears a good match for this generation task. Evaluating on a corpus of German actors in newspaper reports, we find surprisingly that the LLM performs worse. Further analysis reveals that the LLM is very good at identifying the right reference, but struggles to generate the correct canonical form. This points to an underlying issue in LLMs with controlling generated output. Indeed, a hybrid model combining the LLM with a classifier to normalize its output substantially outperforms both initial models.

### Superfiltering: Weak-to-Strong Data Filtering for Fast Instruction-Tuning 
[[arxiv](https://arxiv.org/abs/2402.00530)] [[cool](https://papers.cool/arxiv/2402.00530)] [[pdf](https://arxiv.org/pdf/2402.00530)]
> **Authors**: Ming Li,Yong Zhang,Shwai He,Zhitao Li,Hongyu Zhao,Jianzong Wang,Ning Cheng,Tianyi Zhou
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: ACL2024 main, Camera-ready
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Instruction tuning is critical to improve LLMs but usually suffers from low-quality and redundant data. Data filtering for instruction tuning has proved important in improving both the efficiency and performance of the tuning process. But it also leads to extra cost and computation due to the involvement of LLMs in this process. To reduce the filtering cost, we study Superfiltering: Can we use a smaller and weaker model to select data for finetuning a larger and stronger model? Despite the performance gap between weak and strong language models, we find their highly consistent capability to perceive instruction difficulty and data selection results. This enables us to use a much smaller and more efficient model to filter the instruction data used to train a larger language model. Not only does it largely speed up the data filtering, but the filtered-data-finetuned LLM achieves even better performance on standard benchmarks. Extensive experiments validate the efficacy and efficiency of our approach.

### SA-MDKIF: A Scalable and Adaptable Medical Domain Knowledge Injection Framework for Large Language Models 
[[arxiv](https://arxiv.org/abs/2402.00474)] [[cool](https://papers.cool/arxiv/2402.00474)] [[pdf](https://arxiv.org/pdf/2402.00474)]
> **Authors**: Tianhan Xu,Zhe Hu,Ling Chen,Bin Li
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Recent advances in large language models (LLMs) have demonstrated exceptional performance in various natural language processing (NLP) tasks. However, their effective application in the medical domain is hampered by a lack of medical domain knowledge. In this study, we present SA-MDKIF, a scalable and adaptable framework that aims to inject medical knowledge into general-purpose LLMs through instruction tuning, thereby enabling adaptability for various downstream tasks. SA-MDKIF consists of two stages: skill training and skill adaptation. In the first stage, we define 12 basic medical skills and use AdaLoRA to train these skills based on uniformly formatted instructional datasets that we have constructed. In the next stage, we train the skill router using task-specific downstream data and use this router to integrate the acquired skills with LLMs during inference. Experimental results on 9 different medical tasks show that SA-MDKIF improves performance by 10-20% compared to the original LLMs. Notably, this improvement is particularly pronounced for unseen medical tasks, showing an improvement of up to 30%.

### Improving Dialog Safety using Socially Aware Contrastive Learning 
[[arxiv](https://arxiv.org/abs/2402.00446)] [[cool](https://papers.cool/arxiv/2402.00446)] [[pdf](https://arxiv.org/pdf/2402.00446)]
> **Authors**: Souvik Das,Rohini K. Srihari
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: SCI-CHAT@EACL2024
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: State-of-the-art conversational AI systems raise concerns due to their potential risks of generating unsafe, toxic, unethical, or dangerous content. Previous works have developed datasets to teach conversational agents the appropriate social paradigms to respond effectively to specifically designed hazardous content. However, models trained on these adversarial datasets still struggle to recognize subtle unsafe situations that appear naturally in conversations or introduce an inappropriate response in a casual context. To understand the extent of this problem, we study prosociality in both adversarial and casual dialog contexts and audit the response quality of general-purpose language models in terms of propensity to produce unsafe content. We propose a dual-step fine-tuning process to address these issues using a socially aware n-pair contrastive loss. Subsequently, we train a base model that integrates prosocial behavior by leveraging datasets like Moral Integrity Corpus (MIC) and ProsocialDialog. Experimental results on several dialog datasets demonstrate the effectiveness of our approach in generating socially appropriate responses.

### From PARIS to LE-PARIS: Toward Patent Response Automation with Recommender Systems and Collaborative Large Language Models 
[[arxiv](https://arxiv.org/abs/2402.00421)] [[cool](https://papers.cool/arxiv/2402.00421)] [[pdf](https://arxiv.org/pdf/2402.00421)]
> **Authors**: Jung-Mei Chu,Hao-Cheng Lo,Jieh Hsiang,Chun-Chieh Cho
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: 28 pages, 5 figures, typos corrected, references added, under review
- **标题**: None
- **领域**: 计算语言学,人机交互,信息检索,机器学习
- **Abstract**: In patent prosecution, timely and effective responses to Office Actions (OAs) are crucial for securing patents. However, past automation and artificial intelligence research have largely overlooked this aspect. To bridge this gap, our study introduces the Patent Office Action Response Intelligence System (PARIS) and its advanced version, the Large Language Model (LLM) Enhanced PARIS (LE-PARIS). These systems are designed to enhance the efficiency of patent attorneys in handling OA responses through collaboration with AI. The systems' key features include the construction of an OA Topics Database, development of Response Templates, and implementation of Recommender Systems and LLM-based Response Generation. To validate the effectiveness of the systems, we have employed a multi-paradigm analysis using the USPTO Office Action database and longitudinal data based on attorney interactions with our systems over six years. Through five studies, we have examined the constructiveness of OA topics (studies 1 and 2) using topic modeling and our proposed Delphi process, the efficacy of our proposed hybrid LLM-based recommender system tailored for OA responses (study 3), the quality of generated responses (study 4), and the systems' practical value in real-world scenarios through user studies (study 5). The results indicate that both PARIS and LE-PARIS significantly achieve key metrics and have a positive impact on attorney performance.

### Prompt-Time Symbolic Knowledge Capture with Large Language Models 
[[arxiv](https://arxiv.org/abs/2402.00414)] [[cool](https://papers.cool/arxiv/2402.00414)] [[pdf](https://arxiv.org/pdf/2402.00414)]
> **Authors**: Tolga Çöplü,Arto Bendiken,Andrii Skomorokhov,Eduard Bateiko,Stephen Cobb,Joshua J. Bouw
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: 8 pages, 5 figures, 1 table preprint. Under review
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Augmenting large language models (LLMs) with user-specific knowledge is crucial for real-world applications, such as personal AI assistants. However, LLMs inherently lack mechanisms for prompt-driven knowledge capture. This paper investigates utilizing the existing LLM capabilities to enable prompt-driven knowledge capture, with a particular emphasis on knowledge graphs. We address this challenge by focusing on prompt-to-triple (P2T) generation. We explore three methods: zero-shot prompting, few-shot prompting, and fine-tuning, and then assess their performance via a specialized synthetic dataset. Our code and datasets are publicly available at https://github.com/HaltiaAI/paper-PTSKC.

### Hidding the Ghostwriters: An Adversarial Evaluation of AI-Generated Student Essay Detection 
[[arxiv](https://arxiv.org/abs/2402.00412)] [[cool](https://papers.cool/arxiv/2402.00412)] [[pdf](https://arxiv.org/pdf/2402.00412)]
> **Authors**: Xinlin Peng,Ying Zhou,Ben He,Le Sun,Yingfei Sun
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: Accepted by EMNLP 2023 Main conference, Oral Presentation
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large language models (LLMs) have exhibited remarkable capabilities in text generation tasks. However, the utilization of these models carries inherent risks, including but not limited to plagiarism, the dissemination of fake news, and issues in educational exercises. Although several detectors have been proposed to address these concerns, their effectiveness against adversarial perturbations, specifically in the context of student essay writing, remains largely unexplored. This paper aims to bridge this gap by constructing AIG-ASAP, an AI-generated student essay dataset, employing a range of text perturbation methods that are expected to generate high-quality essays while evading detection. Through empirical experiments, we assess the performance of current AIGC detectors on the AIG-ASAP dataset. The results reveal that the existing detectors can be easily circumvented using straightforward automatic adversarial attacks. Specifically, we explore word substitution and sentence substitution perturbation methods that effectively evade detection while maintaining the quality of the generated essays. This highlights the urgent need for more accurate and robust methods to detect AI-generated student essays in the education domain.

### Investigating Bias Representations in Llama 2 Chat via Activation Steering 
[[arxiv](https://arxiv.org/abs/2402.00402)] [[cool](https://papers.cool/arxiv/2402.00402)] [[pdf](https://arxiv.org/pdf/2402.00402)]
> **Authors**: Dawn Lu,Nina Rimsky
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: We address the challenge of societal bias in Large Language Models (LLMs), focusing on the Llama 2 7B Chat model. As LLMs are increasingly integrated into decision-making processes with substantial societal impact, it becomes imperative to ensure these models do not reinforce existing biases. Our approach employs activation steering to probe for and mitigate biases related to gender, race, and religion. This method manipulates model activations to direct responses towards or away from biased outputs, utilizing steering vectors derived from the StereoSet dataset and custom GPT4 generated gender bias prompts. Our findings reveal inherent gender bias in Llama 2 7B Chat, persisting even after Reinforcement Learning from Human Feedback (RLHF). We also observe a predictable negative correlation between bias and the model's tendency to refuse responses. Significantly, our study uncovers that RLHF tends to increase the similarity in the model's representation of different forms of societal biases, which raises questions about the model's nuanced understanding of different forms of bias. This work also provides valuable insights into effective red-teaming strategies for LLMs using activation steering, particularly emphasizing the importance of integrating a refusal vector.

### What Does the Bot Say? Opportunities and Risks of Large Language Models in Social Media Bot Detection 
[[arxiv](https://arxiv.org/abs/2402.00371)] [[cool](https://papers.cool/arxiv/2402.00371)] [[pdf](https://arxiv.org/pdf/2402.00371)]
> **Authors**: Shangbin Feng,Herun Wan,Ningnan Wang,Zhaoxuan Tan,Minnan Luo,Yulia Tsvetkov
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: ACL 2024
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Social media bot detection has always been an arms race between advancements in machine learning bot detectors and adversarial bot strategies to evade detection. In this work, we bring the arms race to the next level by investigating the opportunities and risks of state-of-the-art large language models (LLMs) in social bot detection. To investigate the opportunities, we design novel LLM-based bot detectors by proposing a mixture-of-heterogeneous-experts framework to divide and conquer diverse user information modalities. To illuminate the risks, we explore the possibility of LLM-guided manipulation of user textual and structured information to evade detection. Extensive experiments with three LLMs on two datasets demonstrate that instruction tuning on merely 1,000 annotated examples produces specialized LLMs that outperform state-of-the-art baselines by up to 9.1% on both datasets, while LLM-guided manipulation strategies could significantly bring down the performance of existing bot detectors by up to 29.6% and harm the calibration and reliability of bot detection systems.

### Don't Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration 
[[arxiv](https://arxiv.org/abs/2402.00367)] [[cool](https://papers.cool/arxiv/2402.00367)] [[pdf](https://arxiv.org/pdf/2402.00367)]
> **Authors**: Shangbin Feng,Weijia Shi,Yike Wang,Wenxuan Ding,Vidhisha Balachandran,Yulia Tsvetkov
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: ACL 2024
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Despite efforts to expand the knowledge of large language models (LLMs), knowledge gaps -- missing or outdated information in LLMs -- might always persist given the evolving nature of knowledge. In this work, we study approaches to identify LLM knowledge gaps and abstain from answering questions when knowledge gaps are present. We first adapt existing approaches to model calibration or adaptation through fine-tuning/prompting and analyze their ability to abstain from generating low-confidence outputs. Motivated by their failures in self-reflection and over-reliance on held-out sets, we propose two novel approaches that are based on model collaboration, i.e., LLMs probing other LLMs for knowledge gaps, either cooperatively or competitively. Extensive experiments with three LLMs on four QA tasks featuring diverse knowledge domains demonstrate that both cooperative and competitive approaches to unveiling LLM knowledge gaps achieve up to 19.3% improvements on abstain accuracy against the strongest baseline. Further analysis reveals that our proposed mechanisms could help identify failure cases in retrieval augmentation and pinpoint knowledge gaps in multi-hop reasoning.

### Does DetectGPT Fully Utilize Perturbation? Bridging Selective Perturbation to Fine-tuned Contrastive Learning Detector would be Better 
[[arxiv](https://arxiv.org/abs/2402.00263)] [[cool](https://papers.cool/arxiv/2402.00263)] [[pdf](https://arxiv.org/pdf/2402.00263)]
> **Authors**: Shengchao Liu,Xiaoming Liu,Yichen Wang,Zehua Cheng,Chengzhengxu Li,Zhaohan Zhang,Yu Lan,Chao Shen
> **First submission**: 2024-01-31
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The burgeoning generative capabilities of large language models (LLMs) have raised growing concerns about abuse, demanding automatic machine-generated text detectors. DetectGPT, a zero-shot metric-based detector, first introduces perturbation and shows great performance improvement. However, in DetectGPT, the random perturbation strategy could introduce noise, and logit regression depends on the threshold, harming the generalizability and applicability of individual or small-batch inputs. Hence, we propose a novel fine-tuned detector, Pecola, bridging metric-based and fine-tuned methods by contrastive learning on selective perturbation. Selective strategy retains important tokens during perturbation and weights for multi-pair contrastive learning. The experiments show that Pecola outperforms the state-of-the-art (SOTA) by 1.20% in accuracy on average on four public datasets. And we further analyze the effectiveness, robustness, and generalization of the method.

### Emergency Department Decision Support using Clinical Pseudo-notes 
[[arxiv](https://arxiv.org/abs/2402.00160)] [[cool](https://papers.cool/arxiv/2402.00160)] [[pdf](https://arxiv.org/pdf/2402.00160)]
> **Authors**: Simon A. Lee,Sujay Jain,Alex Chen,Kyoka Ono,Jennifer Fang,Akos Rudas,Jeffrey N. Chiang
> **First submission**: 2024-01-31
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: In this work, we introduce the Multiple Embedding Model for EHR (MEME), an approach that serializes multimodal EHR tabular data into text using pseudo-notes, mimicking clinical text generation. This conversion not only preserves better representations of categorical data and learns contexts but also enables the effective employment of pretrained foundation models for rich feature representation. To address potential issues with context length, our framework encodes embeddings for each EHR modality separately. We demonstrate the effectiveness of MEME by applying it to several decision support tasks within the Emergency Department across multiple hospital systems. Our findings indicate that MEME outperforms traditional machine learning, EHR-specific foundation models, and general LLMs, highlighting its potential as a general and extendible EHR representation strategy.

### Large Language Models for Mathematical Reasoning: Progresses and Challenges 
[[arxiv](https://arxiv.org/abs/2402.00157)] [[cool](https://papers.cool/arxiv/2402.00157)] [[pdf](https://arxiv.org/pdf/2402.00157)]
> **Authors**: Janice Ahn,Rishu Verma,Renze Lou,Di Liu,Rui Zhang,Wenpeng Yin
> **First submission**: 2024-01-31
> **First announcement**: 2024-02-02
> **comment**: EACL 2024 Student Research Workshop, 8 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Mathematical reasoning serves as a cornerstone for assessing the fundamental cognitive capabilities of human intelligence. In recent times, there has been a notable surge in the development of Large Language Models (LLMs) geared towards the automated resolution of mathematical problems. However, the landscape of mathematical problem types is vast and varied, with LLM-oriented techniques undergoing evaluation across diverse datasets and settings. This diversity makes it challenging to discern the true advancements and obstacles within this burgeoning field. This survey endeavors to address four pivotal dimensions: i) a comprehensive exploration of the various mathematical problems and their corresponding datasets that have been investigated; ii) an examination of the spectrum of LLM-oriented techniques that have been proposed for mathematical problem-solving; iii) an overview of factors and concerns affecting LLMs in solving math; and iv) an elucidation of the persisting challenges within this domain. To the best of our knowledge, this survey stands as one of the first extensive examinations of the landscape of LLMs in the realm of mathematics, providing a holistic perspective on the current state, accomplishments, and future challenges in this rapidly evolving field.

### Making a Long Story Short in Conversation Modeling 
[[arxiv](https://arxiv.org/abs/2402.00143)] [[cool](https://papers.cool/arxiv/2402.00143)] [[pdf](https://arxiv.org/pdf/2402.00143)]
> **Authors**: Yufei Tao,Tiernan Mines,Ameeta Agrawal
> **First submission**: 2024-01-31
> **First announcement**: 2024-02-02
> **comment**: This paper was accepted by TEICAI workshop at EACL 2024
- **标题**: None
- **领域**: 计算语言学,人机交互
- **Abstract**: Conversation systems accommodate diverse users with unique personalities and distinct writing styles. Within the domain of multi-turn dialogue modeling, this work studies the impact of varied utterance lengths on the quality of subsequent responses generated by conversation models. Using GPT-3 as the base model, multiple dialogue datasets, and several metrics, we conduct a thorough exploration of this aspect of conversational models. Our analysis sheds light on the complex relationship between utterance lengths and the quality of follow-up responses generated by dialogue systems. Empirical findings suggests that, for certain types of conversations, utterance lengths can be reduced by up to 72% without any noticeable difference in the quality of follow-up responses.

## 密码学和安全(cs.CR:Cryptography and Security)

### Institutional Platform for Secure Self-Service Large Language Model Exploration 
[[arxiv](https://arxiv.org/abs/2402.00913)] [[cool](https://papers.cool/arxiv/2402.00913)] [[pdf](https://arxiv.org/pdf/2402.00913)]
> **Authors**: V. K. Cody Bumgardner,Mitchell A. Klusty,W. Vaiden Logan,Samuel E. Armstrong,Caroline N. Leach,Kenneth L. Calvert,Caylin Hickey,Jeff Talbert
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: 10 pages 5 figures, 1 table
- **标题**: None
- **领域**: 密码学和安全,人工智能,计算语言学
- **Abstract**: This paper introduces a user-friendly platform developed by the University of Kentucky Center for Applied AI, designed to make large, customized language models (LLMs) more accessible. By capitalizing on recent advancements in multi-LoRA inference, the system efficiently accommodates custom adapters for a diverse range of users and projects. The paper outlines the system's architecture and key features, encompassing dataset curation, model training, secure inference, and text-based feature extraction. We illustrate the establishment of a tenant-aware computational network using agent-based methods, securely utilizing islands of isolated resources as a unified system. The platform strives to deliver secure LLM services, emphasizing process and data isolation, end-to-end encryption, and role-based resource authentication. This contribution aligns with the overarching goal of enabling simplified access to cutting-edge AI models and technology in support of scientific discovery.

### An Early Categorization of Prompt Injection Attacks on Large Language Models 
[[arxiv](https://arxiv.org/abs/2402.00898)] [[cool](https://papers.cool/arxiv/2402.00898)] [[pdf](https://arxiv.org/pdf/2402.00898)]
> **Authors**: Sippo Rossi,Alisia Marianne Michel,Raghava Rao Mukkamala,Jason Bennett Thatcher
> **First submission**: 2024-01-31
> **First announcement**: 2024-02-02
> **comment**: 21 pages double spacing
- **标题**: None
- **领域**: 密码学和安全,计算语言学,机器学习
- **Abstract**: Large language models and AI chatbots have been at the forefront of democratizing artificial intelligence. However, the releases of ChatGPT and other similar tools have been followed by growing concerns regarding the difficulty of controlling large language models and their outputs. Currently, we are witnessing a cat-and-mouse game where users attempt to misuse the models with a novel attack called prompt injections. In contrast, the developers attempt to discover the vulnerabilities and block the attacks simultaneously. In this paper, we provide an overview of these emergent threats and present a categorization of prompt injections, which can guide future research on prompt injections and act as a checklist of vulnerabilities in the development of LLM interfaces. Moreover, based on previous literature and our own empirical research, we discuss the implications of prompt injections to LLM end users, developers, and researchers.

### Privacy and Security Implications of Cloud-Based AI Services : A Survey 
[[arxiv](https://arxiv.org/abs/2402.00896)] [[cool](https://papers.cool/arxiv/2402.00896)] [[pdf](https://arxiv.org/pdf/2402.00896)]
> **Authors**: Alka Luqman,Riya Mahesh,Anupam Chattopadhyay
> **First submission**: 2024-01-31
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能,机器学习
- **Abstract**: This paper details the privacy and security landscape in today's cloud ecosystem and identifies that there is a gap in addressing the risks introduced by machine learning models. As machine learning algorithms continue to evolve and find applications across diverse domains, the need to categorize and quantify privacy and security risks becomes increasingly critical. With the emerging trend of AI-as-a-Service (AIaaS), machine learned AI models (or ML models) are deployed on the cloud by model providers and used by model consumers. We first survey the AIaaS landscape to document the various kinds of liabilities that ML models, especially Deep Neural Networks pose and then introduce a taxonomy to bridge this gap by holistically examining the risks that creators and consumers of ML models are exposed to and their known defences till date. Such a structured approach will be beneficial for ML model providers to create robust solutions. Likewise, ML model consumers will find it valuable to evaluate such solutions and understand the implications of their engagement with such services. The proposed taxonomies provide a foundational basis for solutions in private, secure and robust ML, paving the way for more transparent and resilient AI systems.

### Large Language Models in Cybersecurity: State-of-the-Art 
[[arxiv](https://arxiv.org/abs/2402.00891)] [[cool](https://papers.cool/arxiv/2402.00891)] [[pdf](https://arxiv.org/pdf/2402.00891)]
> **Authors**: Farzad Nourmohammadzadeh Motlagh,Mehrdad Hajizadeh,Mehryar Majd,Pejman Najafi,Feng Cheng,Christoph Meinel
> **First submission**: 2024-01-30
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能,计算语言学,机器学习
- **Abstract**: The rise of Large Language Models (LLMs) has revolutionized our comprehension of intelligence bringing us closer to Artificial Intelligence. Since their introduction, researchers have actively explored the applications of LLMs across diverse fields, significantly elevating capabilities. Cybersecurity, traditionally resistant to data-driven solutions and slow to embrace machine learning, stands out as a domain. This study examines the existing literature, providing a thorough characterization of both defensive and adversarial applications of LLMs within the realm of cybersecurity. Our review not only surveys and categorizes the current landscape but also identifies critical research gaps. By evaluating both offensive and defensive applications, we aim to provide a holistic understanding of the potential risks and opportunities associated with LLM-driven cybersecurity.

### X-CBA: Explainability Aided CatBoosted Anomal-E for Intrusion Detection System 
[[arxiv](https://arxiv.org/abs/2402.00839)] [[cool](https://papers.cool/arxiv/2402.00839)] [[pdf](https://arxiv.org/pdf/2402.00839)]
> **Authors**: Kiymet Kaya,Elif Ak,Sumeyye Bas,Berk Canberk,Sule Gunduz Oguducu
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: ef:ICC 2024 - IEEE International Conference on Communications
- **标题**: None
- **领域**: 密码学和安全,人工智能,机器学习,网络和互联网架构
- **Abstract**: The effectiveness of Intrusion Detection Systems (IDS) is critical in an era where cyber threats are becoming increasingly complex. Machine learning (ML) and deep learning (DL) models provide an efficient and accurate solution for identifying attacks and anomalies in computer networks. However, using ML and DL models in IDS has led to a trust deficit due to their non-transparent decision-making. This transparency gap in IDS research is significant, affecting confidence and accountability. To address, this paper introduces a novel Explainable IDS approach, called X-CBA, that leverages the structural advantages of Graph Neural Networks (GNNs) to effectively process network traffic data, while also adapting a new Explainable AI (XAI) methodology. Unlike most GNN-based IDS that depend on labeled network traffic and node features, thereby overlooking critical packet-level information, our approach leverages a broader range of traffic data through network flows, including edge attributes, to improve detection capabilities and adapt to novel threats. Through empirical testing, we establish that our approach not only achieves high accuracy with 99.47% in threat detection but also advances the field by providing clear, actionable explanations of its analytical outcomes. This research also aims to bridge the current gap and facilitate the broader integration of ML/DL technologies in cybersecurity defenses by offering a local and global explainability solution that is both precise and interpretable.

### Ocassionally Secure: A Comparative Analysis of Code Generation Assistants 
[[arxiv](https://arxiv.org/abs/2402.00689)] [[cool](https://papers.cool/arxiv/2402.00689)] [[pdf](https://arxiv.org/pdf/2402.00689)]
> **Authors**: Ran Elgedawy,John Sadik,Senjuti Dutta,Anuj Gautam,Konstantinos Georgiou,Farzin Gholamrezae,Fujiao Ji,Kyungchan Lim,Qian Liu,Scott Ruoti
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: 12 pages, 2 figures
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Large Language Models (LLMs) are being increasingly utilized in various applications, with code generations being a notable example. While previous research has shown that LLMs have the capability to generate both secure and insecure code, the literature does not take into account what factors help generate secure and effective code. Therefore in this paper we focus on identifying and understanding the conditions and contexts in which LLMs can be effectively and safely deployed in real-world scenarios to generate quality code. We conducted a comparative analysis of four advanced LLMs--GPT-3.5 and GPT-4 using ChatGPT and Bard and Gemini from Google--using 9 separate tasks to assess each model's code generation capabilities. We contextualized our study to represent the typical use cases of a real-life developer employing LLMs for everyday tasks as work. Additionally, we place an emphasis on security awareness which is represented through the use of two distinct versions of our developer persona. In total, we collected 61 code outputs and analyzed them across several aspects: functionality, security, performance, complexity, and reliability. These insights are crucial for understanding the models' capabilities and limitations, guiding future development and practical applications in the field of automated code generation.

## 计算机视觉和模式识别(cs.CV:Computer Vision and Pattern Recognition)

### AI-generated faces influence gender stereotypes and racial homogenization 
[[arxiv](https://arxiv.org/abs/2402.01002)] [[cool](https://papers.cool/arxiv/2402.01002)] [[pdf](https://arxiv.org/pdf/2402.01002)]
> **Authors**: Nouar AlDahoul,Talal Rahwan,Yasir Zaki
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: 47 pages, 19 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Text-to-image generative AI models such as Stable Diffusion are used daily by millions worldwide. However, the extent to which these models exhibit racial and gender stereotypes is not yet fully understood. Here, we document significant biases in Stable Diffusion across six races, two genders, 32 professions, and eight attributes. Additionally, we examine the degree to which Stable Diffusion depicts individuals of the same race as being similar to one another. This analysis reveals significant racial homogenization, e.g., depicting nearly all Middle Eastern men as bearded, brown-skinned, and wearing traditional attire. We then propose debiasing solutions that allow users to specify the desired distributions of race and gender when generating images while minimizing racial homogenization. Finally, using a preregistered survey experiment, we find evidence that being presented with inclusive AI-generated faces reduces people's racial and gender biases, while being presented with non-inclusive ones increases such biases, regardless of whether the images are labeled as AI-generated. Taken together, our findings emphasize the need to address biases and stereotypes in text-to-image models.

### A Cost-Efficient Approach for Creating Virtual Fitting Room using Generative Adversarial Networks (GANs) 
[[arxiv](https://arxiv.org/abs/2402.00994)] [[cool](https://papers.cool/arxiv/2402.00994)] [[pdf](https://arxiv.org/pdf/2402.00994)]
> **Authors**: Kirolos Attallah,Girgis Zaky,Nourhan Abdelrhim,Kyrillos Botros,Amjad Dife,Nermin Negied
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: ef:International Journal of Advanced Computer Science and Applications(IJACSA), Volume 15 Issue 1, 2024
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Customers all over the world want to see how the clothes fit them or not before purchasing. Therefore, customers by nature prefer brick-and-mortar clothes shopping so they can try on products before purchasing them. But after the Pandemic of COVID19 many sellers either shifted to online shopping or closed their fitting rooms which made the shopping process hesitant and doubtful. The fact that the clothes may not be suitable for their buyers after purchase led us to think about using new AI technologies to create an online platform or a virtual fitting room (VFR) in the form of a mobile application and a deployed model using a webpage that can be embedded later to any online store where they can try on any number of cloth items without physically trying them. Besides, it will save much searching time for their needs. Furthermore, it will reduce the crowding and headache in the physical shops by applying the same technology using a special type of mirror that will enable customers to try on faster. On the other hand, from business owners' perspective, this project will highly increase their online sales, besides, it will save the quality of the products by avoiding physical trials issues. The main approach used in this work is applying Generative Adversarial Networks (GANs) combined with image processing techniques to generate one output image from two input images which are the person image and the cloth image. This work achieved results that outperformed the state-of-the-art approaches found in literature.

### Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks 
[[arxiv](https://arxiv.org/abs/2402.00626)] [[cool](https://papers.cool/arxiv/2402.00626)] [[pdf](https://arxiv.org/pdf/2402.00626)]
> **Authors**: Maan Qraitem,Nazia Tasnim,Piotr Teterwak,Kate Saenko,Bryan A. Plummer
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,密码学和安全,机器学习
- **Abstract**: Typographic attacks, adding misleading text to images, can deceive vision-language models (LVLMs). The susceptibility of recent large LVLMs like GPT4-V to such attacks is understudied, raising concerns about amplified misinformation in personal assistant applications. Previous attacks use simple strategies, such as random misleading words, which don't fully exploit LVLMs' language reasoning abilities. We introduce an experimental setup for testing typographic attacks on LVLMs and propose two novel self-generated attacks: (1) Class-based attacks, where the model identifies a similar class to deceive itself, and (2) Reasoned attacks, where an advanced LVLM suggests an attack combining a deceiving class and description. Our experiments show these attacks significantly reduce classification performance by up to 60\% and are effective across different models, including InstructBLIP and MiniGPT4. Code: https://github.com/mqraitem/Self-Gen-Typo-Attack

### A Survey on Hallucination in Large Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2402.00253)] [[cool](https://papers.cool/arxiv/2402.00253)] [[pdf](https://arxiv.org/pdf/2402.00253)]
> **Authors**: Hanchao Liu,Wenyuan Xue,Yifei Chen,Dapeng Chen,Xiutian Zhao,Ke Wang,Liping Hou,Rongjun Li,Wei Peng
> **First submission**: 2024-01-31
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算语言学,机器学习
- **Abstract**: Recent development of Large Vision-Language Models (LVLMs) has attracted growing attention within the AI landscape for its practical implementation potential. However, ``hallucination'', or more specifically, the misalignment between factual visual content and corresponding textual generation, poses a significant challenge of utilizing LVLMs. In this comprehensive survey, we dissect LVLM-related hallucinations in an attempt to establish an overview and facilitate future mitigation. Our scrutiny starts with a clarification of the concept of hallucinations in LVLMs, presenting a variety of hallucination symptoms and highlighting the unique challenges inherent in LVLM hallucinations. Subsequently, we outline the benchmarks and methodologies tailored specifically for evaluating hallucinations unique to LVLMs. Additionally, we delve into an investigation of the root causes of these hallucinations, encompassing insights from the training data and model components. We also critically review existing methods for mitigating hallucinations. The open questions and future directions pertaining to hallucinations within LVLMs are discussed to conclude this survey.

## 计算机与社会(cs.CY:Computers and Society)

### Catalyzing Equity in STEM Teams: Harnessing Generative AI for Inclusion and Diversity 
[[arxiv](https://arxiv.org/abs/2402.00037)] [[cool](https://papers.cool/arxiv/2402.00037)] [[pdf](https://arxiv.org/pdf/2402.00037)]
> **Authors**: Nia Nixon,Yiwen Lin,Lauren Snow
> **First submission**: 2024-01-08
> **First announcement**: 2024-02-02
> **comment**: 21 pages, 0 figure, to be published in Policy Insights from Behavioral and Brain Sciences
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: Collaboration is key to STEM, where multidisciplinary team research can solve complex problems. However, inequality in STEM fields hinders their full potential, due to persistent psychological barriers in underrepresented students' experience. This paper documents teamwork in STEM and explores the transformative potential of computational modeling and generative AI in promoting STEM-team diversity and inclusion. Leveraging generative AI, this paper outlines two primary areas for advancing diversity, equity, and inclusion. First, formalizing collaboration assessment with inclusive analytics can capture fine-grained learner behavior. Second, adaptive, personalized AI systems can support diversity and inclusion in STEM teams. Four policy recommendations highlight AI's capacity: formalized collaborative skill assessment, inclusive analytics, funding for socio-cognitive research, human-AI teaming for inclusion training. Researchers, educators, policymakers can build an equitable STEM ecosystem. This roadmap advances AI-enhanced collaboration, offering a vision for the future of STEM where diverse voices are actively encouraged and heard within collaborative scientific endeavors.

### Exploring Public Opinion on Responsible AI Through The Lens of Cultural Consensus Theory 
[[arxiv](https://arxiv.org/abs/2402.00029)] [[cool](https://papers.cool/arxiv/2402.00029)] [[pdf](https://arxiv.org/pdf/2402.00029)]
> **Authors**: Necdet Gurkan,Jordan W. Suchow
> **First submission**: 2024-01-06
> **First announcement**: 2024-02-02
> **comment**: ef:Proceedings of the 57th Hawaii International Conference on System Sciences, 713-723 (2024)
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: As the societal implications of Artificial Intelligence (AI) continue to grow, the pursuit of responsible AI necessitates public engagement in its development and governance processes. This involvement is crucial for capturing diverse perspectives and promoting equitable practices and outcomes. We applied Cultural Consensus Theory (CCT) to a nationally representative survey dataset on various aspects of AI to discern beliefs and attitudes about responsible AI in the United States. Our results offer valuable insights by identifying shared and contrasting views on responsible AI. Furthermore, these findings serve as critical reference points for developers and policymakers, enabling them to more effectively consider individual variances and group-level cultural perspectives when making significant decisions and addressing the public's concerns.

### Deploying ADVISER: Impact and Lessons from Using Artificial Intelligence for Child Vaccination Uptake in Nigeria 
[[arxiv](https://arxiv.org/abs/2402.00017)] [[cool](https://papers.cool/arxiv/2402.00017)] [[pdf](https://arxiv.org/pdf/2402.00017)]
> **Authors**: Opadele Kehinde,Ruth Abdul,Bose Afolabi,Parminder Vir,Corinne Namblard,Ayan Mukhopadhyay,Abiodun Adereni
> **First submission**: 2023-12-30
> **First announcement**: 2024-02-02
> **comment**: Accepted for publication at the AAAI Conference on Artificial Intelligence (AAAI-24)
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: More than 5 million children under five years die from largely preventable or treatable medical conditions every year, with an overwhelmingly large proportion of deaths occurring in underdeveloped countries with low vaccination uptake. One of the United Nations' sustainable development goals (SDG 3) aims to end preventable deaths of newborns and children under five years of age. We focus on Nigeria, where the rate of infant mortality is appalling. In particular, low vaccination uptake in Nigeria is a major driver of more than 2,000 daily deaths of children under the age of five years. In this paper, we describe our collaboration with government partners in Nigeria to deploy ADVISER: AI-Driven Vaccination Intervention Optimiser. The framework, based on an integer linear program that seeks to maximize the cumulative probability of successful vaccination, is the first successful deployment of an AI-enabled toolchain for optimizing the allocation of health interventions in Nigeria. In this paper, we provide a background of the ADVISER framework and present results, lessons, and success stories of deploying ADVISER to more than 13,000 families in the state of Oyo, Nigeria.

### Choosing the Right Path for AI Integration in Engineering Companies: A Strategic Guide 
[[arxiv](https://arxiv.org/abs/2402.00011)] [[cool](https://papers.cool/arxiv/2402.00011)] [[pdf](https://arxiv.org/pdf/2402.00011)]
> **Authors**: Rimma Dzhusupova,Jan Bosch,Helena Holmstrom Olsson
> **First submission**: 2023-12-25
> **First announcement**: 2024-02-02
> **comment**: mber:JSS_111945
- **标题**: None
- **领域**: 计算机与社会,机器学习,软件工程
- **Abstract**: The Engineering, Procurement and Construction (EPC) businesses operating within the energy sector are recognizing the increasing importance of Artificial Intelligence (AI). Many EPC companies and their clients have realized the benefits of applying AI to their businesses in order to reduce manual work, drive productivity, and streamline future operations of engineered installations in a highly competitive industry. The current AI market offers various solutions and services to support this industry, but organizations must understand how to acquire AI technology in the most beneficial way based on their business strategy and available resources. This paper presents a framework for EPC companies in their transformation towards AI. Our work is based on examples of project execution of AI-based products development at one of the biggest EPC contractors worldwide and on insights from EPC vendor companies already integrating AI into their engineering solutions. The paper covers the entire life cycle of building AI solutions, from initial business understanding to deployment and further evolution. The framework identifies how various factors influence the choice of approach toward AI project development within large international engineering corporations. By presenting a practical guide for optimal approach selection, this paper contributes to the research in AI project management and organizational strategies for integrating AI technology into businesses. The framework might also help engineering companies choose the optimum AI approach to create business value.

## 人机交互(cs.HC:Human-Computer Interaction)

### Are Generative AI systems Capable of Supporting Information Needs of Patients? 
[[arxiv](https://arxiv.org/abs/2402.00234)] [[cool](https://papers.cool/arxiv/2402.00234)] [[pdf](https://arxiv.org/pdf/2402.00234)]
> **Authors**: Shreya Rajagopal,Subhashis Hazarika,Sookyung Kim,Yan-ming Chiou,Jae Ho Sohn,Hari Subramonyam,Shiwali Mohan
> **First submission**: 2024-01-31
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 人机交互,人工智能,计算语言学,机器学习
- **Abstract**: Patients managing a complex illness such as cancer face a complex information challenge where they not only must learn about their illness but also how to manage it. Close interaction with healthcare experts (radiologists, oncologists) can improve patient learning and thereby, their disease outcome. However, this approach is resource intensive and takes expert time away from other critical tasks. Given the recent advancements in Generative AI models aimed at improving the healthcare system, our work investigates whether and how generative visual question answering systems can responsibly support patient information needs in the context of radiology imaging data. We conducted a formative need-finding study in which participants discussed chest computed tomography (CT) scans and associated radiology reports of a fictitious close relative with a cardiothoracic radiologist. Using thematic analysis of the conversation between participants and medical experts, we identified commonly occurring themes across interactions, including clarifying medical terminology, locating the problems mentioned in the report in the scanned image, understanding disease prognosis, discussing the next diagnostic steps, and comparing treatment options. Based on these themes, we evaluated two state-of-the-art generative visual language models against the radiologist's responses. Our results reveal variability in the quality of responses generated by the models across various themes. We highlight the importance of patient-facing generative AI systems to accommodate a diverse range of conversational themes, catering to the real-world informational needs of patients.

## 信息检索(cs.IR:Information Retrieval)

### A Multi-Agent Conversational Recommender System 
[[arxiv](https://arxiv.org/abs/2402.01135)] [[cool](https://papers.cool/arxiv/2402.01135)] [[pdf](https://arxiv.org/pdf/2402.01135)]
> **Authors**: Jiabao Fang,Shen Gao,Pengjie Ren,Xiuying Chen,Suzan Verberne,Zhaochun Ren
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,计算语言学
- **Abstract**: Due to strong capabilities in conducting fluent, multi-turn conversations with users, Large Language Models (LLMs) have the potential to further improve the performance of Conversational Recommender System (CRS). Unlike the aimless chit-chat that LLM excels at, CRS has a clear target. So it is imperative to control the dialogue flow in the LLM to successfully recommend appropriate items to the users. Furthermore, user feedback in CRS can assist the system in better modeling user preferences, which has been ignored by existing studies. However, simply prompting LLM to conduct conversational recommendation cannot address the above two key challenges. In this paper, we propose Multi-Agent Conversational Recommender System (MACRS) which contains two essential modules. First, we design a multi-agent act planning framework, which can control the dialogue flow based on four LLM-based agents. This cooperative multi-agent framework will generate various candidate responses based on different dialogue acts and then choose the most appropriate response as the system response, which can help MACRS plan suitable dialogue acts. Second, we propose a user feedback-aware reflection mechanism which leverages user feedback to reason errors made in previous turns to adjust the dialogue act planning, and higher-level user information from implicit semantics. We conduct extensive experiments based on user simulator to demonstrate the effectiveness of MACRS in recommendation and user preferences collection. Experimental results illustrate that MACRS demonstrates an improvement in user interaction experience compared to directly using LLMs.

## 机器学习(cs.LG:Machine Learning)

### Vaccine: Perturbation-aware Alignment for Large Language Models against Harmful Fine-tuning Attack 
[[arxiv](https://arxiv.org/abs/2402.01109)] [[cool](https://papers.cool/arxiv/2402.01109)] [[pdf](https://arxiv.org/pdf/2402.01109)]
> **Authors**: Tiansheng Huang,Sihao Hu,Ling Liu
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: Rejected by ICML2024. Accepted by NeurIPS2024
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: The new paradigm of finetuning-as-a-service introduces a new attack surface for Large Language Models (LLMs): a few harmful data uploaded by users can easily trick the finetuning to produce an alignment-broken model. We conduct an empirical analysis and uncover a \textit{harmful embedding drift} phenomenon, showing a probable cause of the alignment-broken effect. Inspired by our findings, we propose Vaccine, a perturbation-aware alignment technique to mitigate the security risk of users finetuning. The core idea of Vaccine is to produce invariant hidden embeddings by progressively adding crafted perturbation to them in the alignment phase. This enables the embeddings to withstand harmful perturbation from un-sanitized user data in the finetuning phase. Our results on open source mainstream LLMs (e.g., Llama2, Opt, Vicuna) demonstrate that Vaccine can boost the robustness of alignment against harmful prompts induced embedding drift while reserving reasoning ability towards benign prompts. Our code is available at \url{https://github.com/git-disl/Vaccine}.

### Compositional Generative Modeling: A Single Model is Not All You Need 
[[arxiv](https://arxiv.org/abs/2402.01103)] [[cool](https://papers.cool/arxiv/2402.01103)] [[pdf](https://arxiv.org/pdf/2402.01103)]
> **Authors**: Yilun Du,Leslie Kaelbling
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: ICML 2024 (Position Track)
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别,机器人技术
- **Abstract**: Large monolithic generative models trained on massive amounts of data have become an increasingly dominant approach in AI research. In this paper, we argue that we should instead construct large generative systems by composing smaller generative models together. We show how such a compositional generative approach enables us to learn distributions in a more data-efficient manner, enabling generalization to parts of the data distribution unseen at training time. We further show how this enables us to program and construct new generative models for tasks completely unseen at training. Finally, we show that in many cases, we can discover separate compositional components from data.

### Trustworthy Distributed AI Systems: Robustness, Privacy, and Governance 
[[arxiv](https://arxiv.org/abs/2402.01096)] [[cool](https://papers.cool/arxiv/2402.01096)] [[pdf](https://arxiv.org/pdf/2402.01096)]
> **Authors**: Wenqi Wei,Ling Liu
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: Manuscript accepted to ACM Computing Surveys
- **标题**: None
- **领域**: 机器学习,人工智能,密码学和安全,分布式、并行和集群计算
- **Abstract**: Emerging Distributed AI systems are revolutionizing big data computing and data processing capabilities with growing economic and societal impact. However, recent studies have identified new attack surfaces and risks caused by security, privacy, and fairness issues in AI systems. In this paper, we review representative techniques, algorithms, and theoretical foundations for trustworthy distributed AI through robustness guarantee, privacy protection, and fairness awareness in distributed learning. We first provide a brief overview of alternative architectures for distributed learning, discuss inherent vulnerabilities for security, privacy, and fairness of AI algorithms in distributed learning, and analyze why these problems are present in distributed learning regardless of specific architectures. Then we provide a unique taxonomy of countermeasures for trustworthy distributed AI, covering (1) robustness to evasion attacks and irregular queries at inference, and robustness to poisoning attacks, Byzantine attacks, and irregular data distribution during training; (2) privacy protection during distributed learning and model inference at deployment; and (3) AI fairness and governance with respect to both data and models. We conclude with a discussion on open challenges and future research directions toward trustworthy distributed AI, such as the need for trustworthy AI policy guidelines, the AI responsibility-utility co-design, and incentives and compliance.

### Chameleon: Foundation Models for Fairness-aware Multi-modal Data Augmentation to Enhance Coverage of Minorities 
[[arxiv](https://arxiv.org/abs/2402.01071)] [[cool](https://papers.cool/arxiv/2402.01071)] [[pdf](https://arxiv.org/pdf/2402.01071)]
> **Authors**: Mahdi Erfanian,H. V. Jagadish,Abolfazl Asudeh
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机与社会,数据库
- **Abstract**: The potential harms of the under-representation of minorities in training data, particularly in multi-modal settings, is a well-recognized concern. While there has been extensive effort in detecting such under-representation, resolution has remained a challenge. With recent advancements in generative AI, large language models and foundation models have emerged as versatile tools across various domains. In this paper, we propose Chameleon, a system that efficiently utilizes these tools to augment a data set with a minimal addition of synthetically generated tuples, in order to enhance the coverage of the under-represented groups. Our system follows a rejection sampling approach to ensure the generated tuples have a high quality and follow the underlying distribution. In order to minimize the rejection chance of the generated tuples, we propose multiple strategies for providing a guide for the foundation model. Our experiment results, in addition to confirming the efficiency of our proposed algorithms, illustrate the effectiveness of our approach, as the unfairness of the model in a downstream task significantly dropped after data repair using Chameleon.

### Self-Supervised Contrastive Pre-Training for Multivariate Point Processes 
[[arxiv](https://arxiv.org/abs/2402.00987)] [[cool](https://papers.cool/arxiv/2402.00987)] [[pdf](https://arxiv.org/pdf/2402.00987)]
> **Authors**: Xiao Shou,Dharmashankar Subramanian,Debarun Bhattacharjya,Tian Gao,Kristin P. Bennet
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Self-supervision is one of the hallmarks of representation learning in the increasingly popular suite of foundation models including large language models such as BERT and GPT-3, but it has not been pursued in the context of multivariate event streams, to the best of our knowledge. We introduce a new paradigm for self-supervised learning for multivariate point processes using a transformer encoder. Specifically, we design a novel pre-training strategy for the encoder where we not only mask random event epochs but also insert randomly sampled "void" epochs where an event does not occur; this differs from the typical discrete-time pretext tasks such as word-masking in BERT but expands the effectiveness of masking to better capture continuous-time dynamics. To improve downstream tasks, we introduce a contrasting module that compares real events to simulated void instances. The pre-trained model can subsequently be fine-tuned on a potentially much smaller event dataset, similar conceptually to the typical transfer of popular pre-trained language models. We demonstrate the effectiveness of our proposed paradigm on the next-event prediction task using synthetic datasets and 3 real applications, observing a relative performance boost of as high as up to 20% compared to state-of-the-art models.

### Addressing Bias Through Ensemble Learning and Regularized Fine-Tuning 
[[arxiv](https://arxiv.org/abs/2402.00910)] [[cool](https://papers.cool/arxiv/2402.00910)] [[pdf](https://arxiv.org/pdf/2402.00910)]
> **Authors**: Ahmed Radwan,Layan Zaafarani,Jetana Abudawood,Faisal AlZahrani,Fares Fourati
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Addressing biases in AI models is crucial for ensuring fair and accurate predictions. However, obtaining large, unbiased datasets for training can be challenging. This paper proposes a comprehensive approach using multiple methods to remove bias in AI models, with only a small dataset and a potentially biased pretrained model. We train multiple models with the counter-bias of the pre-trained model through data splitting, local training, and regularized fine-tuning, gaining potentially counter-biased models. Then, we employ ensemble learning for all models to reach unbiased predictions. To further accelerate the inference time of our ensemble model, we conclude our solution with knowledge distillation that results in a single unbiased neural network. We demonstrate the effectiveness of our approach through experiments on the CIFAR10 and HAM10000 datasets, showcasing promising results. This work contributes to the ongoing effort to create more unbiased and reliable AI models, even with limited data availability.

### Weakly Supervised Learners for Correction of AI Errors with Provable Performance Guarantees 
[[arxiv](https://arxiv.org/abs/2402.00899)] [[cool](https://papers.cool/arxiv/2402.00899)] [[pdf](https://arxiv.org/pdf/2402.00899)]
> **Authors**: Ivan Y. Tyukin,Tatiana Tyukina,Daniel van Helden,Zedong Zheng,Evgeny M. Mirkes,Oliver J. Sutton,Qinghua Zhou,Alexander N. Gorban,Penelope Allison
> **First submission**: 2024-01-31
> **First announcement**: 2024-02-02
> **comment**: :68T05; 68T37
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: We present a new methodology for handling AI errors by introducing weakly supervised AI error correctors with a priori performance guarantees. These AI correctors are auxiliary maps whose role is to moderate the decisions of some previously constructed underlying classifier by either approving or rejecting its decisions. The rejection of a decision can be used as a signal to suggest abstaining from making a decision. A key technical focus of the work is in providing performance guarantees for these new AI correctors through bounds on the probabilities of incorrect decisions. These bounds are distribution agnostic and do not rely on assumptions on the data dimension. Our empirical example illustrates how the framework can be applied to improve the performance of an image classifier in a challenging real-world task where training data are scarce.

### SymbolicAI: A framework for logic-based approaches combining generative models and solvers 
[[arxiv](https://arxiv.org/abs/2402.00854)] [[cool](https://papers.cool/arxiv/2402.00854)] [[pdf](https://arxiv.org/pdf/2402.00854)]
> **Authors**: Marius-Constantin Dinu,Claudiu Leoveanu-Condrei,Markus Holzleitner,Werner Zellinger,Sepp Hochreiter
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: 46 pages, 13 figures, external resources: framework is available at https://github.com/ExtensityAI/symbolicai and benchmark at https://github.com/ExtensityAI/benchmark
- **标题**: None
- **领域**: 机器学习,人工智能,符号计算,软件工程
- **Abstract**: We introduce SymbolicAI, a versatile and modular framework employing a logic-based approach to concept learning and flow management in generative processes. SymbolicAI enables the seamless integration of generative models with a diverse range of solvers by treating large language models (LLMs) as semantic parsers that execute tasks based on both natural and formal language instructions, thus bridging the gap between symbolic reasoning and generative AI. We leverage probabilistic programming principles to tackle complex tasks, and utilize differentiable and classical programming paradigms with their respective strengths. The framework introduces a set of polymorphic, compositional, and self-referential operations for multi-modal data that connects multi-step generative processes and aligns their outputs with user objectives in complex workflows. As a result, we can transition between the capabilities of various foundation models with in-context learning capabilities and specialized, fine-tuned models or solvers proficient in addressing specific problems. Through these operations based on in-context learning our framework enables the creation and evaluation of explainable computational graphs. Finally, we introduce a quality measure and its empirical score for evaluating these computational graphs, and propose a benchmark that compares various state-of-the-art LLMs across a set of complex workflows. We refer to the empirical score as the "Vector Embedding for Relational Trajectory Evaluation through Cross-similarity", or VERTEX score for short. The framework codebase and benchmark are linked below.

### Position: Bayesian Deep Learning is Needed in the Age of Large-Scale AI 
[[arxiv](https://arxiv.org/abs/2402.00809)] [[cool](https://papers.cool/arxiv/2402.00809)] [[pdf](https://arxiv.org/pdf/2402.00809)]
> **Authors**: Theodore Papamarkou,Maria Skoularidou,Konstantina Palla,Laurence Aitchison,Julyan Arbel,David Dunson,Maurizio Filippone,Vincent Fortuin,Philipp Hennig,José Miguel Hernández-Lobato,Aliaksandr Hubin,Alexander Immer,Theofanis Karaletsos,Mohammad Emtiyaz Khan,Agustinus Kristiadi,Yingzhen Li,Stephan Mandt,Christopher Nemeth,Michael A. Osborne,Tim G. J. Rudner,David Rügamer,Yee Whye Teh,Max Welling,Andrew Gordon Wilson,Ruqi Zhang
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: Proceedings of the 41st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: In the current landscape of deep learning research, there is a predominant emphasis on achieving high predictive accuracy in supervised tasks involving large image and language datasets. However, a broader perspective reveals a multitude of overlooked metrics, tasks, and data types, such as uncertainty, active and continual learning, and scientific data, that demand attention. Bayesian deep learning (BDL) constitutes a promising avenue, offering advantages across these diverse settings. This paper posits that BDL can elevate the capabilities of deep learning. It revisits the strengths of BDL, acknowledges existing challenges, and highlights some exciting research avenues aimed at addressing these obstacles. Looking ahead, the discussion focuses on possible ways to combine large-scale foundation models with BDL to unlock their full potential.

### Signal Quality Auditing for Time-series Data 
[[arxiv](https://arxiv.org/abs/2402.00803)] [[cool](https://papers.cool/arxiv/2402.00803)] [[pdf](https://arxiv.org/pdf/2402.00803)]
> **Authors**: Chufan Gao,Nicholas Gisolfi,Artur Dubrawski
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,信号处理
- **Abstract**: Signal quality assessment (SQA) is required for monitoring the reliability of data acquisition systems, especially in AI-driven Predictive Maintenance (PMx) application contexts. SQA is vital for addressing "silent failures" of data acquisition hardware and software, which when unnoticed, misinform the users of data, creating the risk for incorrect decisions with unintended or even catastrophic consequences. We have developed an open-source software implementation of signal quality indices (SQIs) for the analysis of time-series data. We codify a range of SQIs, demonstrate them using established benchmark data, and show that they can be effective for signal quality assessment. We also study alternative approaches to denoising time-series data in an attempt to improve the quality of the already degraded signal, and evaluate them empirically on relevant real-world data. To our knowledge, our software toolkit is the first to provide an open source implementation of a broad range of signal quality assessment and improvement techniques validated on publicly available benchmark data for ease of reproducibility. The generality of our framework can be easily extended to assessing reliability of arbitrary time-series measurements in complex systems, especially when morphological patterns of the waveform shapes and signal periodicity are of key interest in downstream analyses.

### Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents 
[[arxiv](https://arxiv.org/abs/2402.00798)] [[cool](https://papers.cool/arxiv/2402.00798)] [[pdf](https://arxiv.org/pdf/2402.00798)]
> **Authors**: Zelong Li,Wenyue Hua,Hao Wang,He Zhu,Yongfeng Zhang
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,形式语言和自动机理论
- **Abstract**: Recent advancements on Large Language Models (LLMs) enable AI Agents to automatically generate and execute multi-step plans to solve complex tasks. However, since LLM's content generation process is hardly controllable, current LLM-based agents frequently generate invalid or non-executable plans, which jeopardizes the performance of the generated plans and corrupts users' trust in LLM-based agents. In response, this paper proposes a novel "Formal-LLM" framework for LLM-based agents by integrating the expressiveness of natural language and the precision of formal language. Specifically, the framework allows agent developers to express their requirements or constraints for the planning process as an automaton. A stack-based LLM plan generation process is then conducted under the supervision of the automaton to ensure that the generated plan satisfies the constraints, making the planning process controllable. We conduct experiments on both benchmark tasks and practical real-life tasks, and our framework achieves over 50% overall performance increase, which validates the feasibility and effectiveness of employing Formal-LLM to guide the plan generation of agents, preventing the agents from generating invalid and unsuccessful plans. Further, more controllable LLM-based agents can facilitate the broader utilization of LLM in application scenarios where high validity of planning is essential. The source code of this work is available at https://github.com/agiresearch/Formal-LLM.

### LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law 
[[arxiv](https://arxiv.org/abs/2402.00795)] [[cool](https://papers.cool/arxiv/2402.00795)] [[pdf](https://arxiv.org/pdf/2402.00795)]
> **Authors**: Toni J. B. Liu,Nicolas Boullé,Raphaël Sarfati,Christopher J. Earls
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Pretrained large language models (LLMs) are surprisingly effective at performing zero-shot tasks, including time-series forecasting. However, understanding the mechanisms behind such capabilities remains highly challenging due to the complexity of the models. We study LLMs' ability to extrapolate the behavior of dynamical systems whose evolution is governed by principles of physical interest. Our results show that LLaMA 2, a language model trained primarily on texts, achieves accurate predictions of dynamical system time series without fine-tuning or prompt engineering. Moreover, the accuracy of the learned physical rules increases with the length of the input context window, revealing an in-context version of neural scaling law. Along the way, we present a flexible and efficient algorithm for extracting probability density functions of multi-digit numbers directly from LLMs.

### Human Expertise in Algorithmic Prediction 
[[arxiv](https://arxiv.org/abs/2402.00793)] [[cool](https://papers.cool/arxiv/2402.00793)] [[pdf](https://arxiv.org/pdf/2402.00793)]
> **Authors**: Rohan Alur,Manish Raghavan,Devavrat Shah
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: 35 pages, 13 figures
- **标题**: None
- **领域**: 机器学习,人工智能,人机交互
- **Abstract**: We introduce a novel framework for incorporating human expertise into algorithmic predictions. Our approach leverages human judgment to distinguish inputs which are algorithmically indistinguishable, or "look the same" to predictive algorithms. We argue that this framing clarifies the problem of human-AI collaboration in prediction tasks, as experts often form judgments by drawing on information which is not encoded in an algorithm's training data. Algorithmic indistinguishability yields a natural test for assessing whether experts incorporate this kind of "side information", and further provides a simple but principled method for selectively incorporating human feedback into algorithmic predictions. We show that this method provably improves the performance of any feasible algorithmic predictor and precisely quantify this improvement. We find empirically that although algorithms often outperform their human counterparts on average, human judgment can improve algorithmic predictions on specific instances (which can be identified ex-ante). In an X-ray classification task, we find that this subset constitutes nearly $30\%$ of the patient population. Our approach provides a natural way of uncovering this heterogeneity and thus enabling effective human-AI collaboration.

### Dense Reward for Free in Reinforcement Learning from Human Feedback 
[[arxiv](https://arxiv.org/abs/2402.00782)] [[cool](https://papers.cool/arxiv/2402.00782)] [[pdf](https://arxiv.org/pdf/2402.00782)]
> **Authors**: Alex J. Chan,Hao Sun,Samuel Holt,Mihaela van der Schaar
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Reinforcement Learning from Human Feedback (RLHF) has been credited as the key advance that has allowed Large Language Models (LLMs) to effectively follow instructions and produce useful assistance. Classically, this involves generating completions from the LLM in response to a query before using a separate reward model to assign a score to the full completion. As an auto-regressive process, the LLM has to take many "actions" (selecting individual tokens) and only receives a single, sparse reward at the end of an episode, a setup that is known to be difficult to optimise in traditional reinforcement learning. In this work we leverage the fact that the reward model contains more information than just its scalar output, in particular, it calculates an attention map over tokens as part of the transformer architecture. We use these attention weights to redistribute the reward along the whole completion, effectively densifying the signal and highlighting the most important tokens, all without incurring extra computational cost or requiring any additional modelling. We demonstrate that, theoretically, this approach is equivalent to potential-based reward shaping, ensuring that the optimal policy remains unchanged. Empirically, we show that it stabilises training, accelerates the rate of learning, and, in practical cases, may lead to better local optima.

### Unlearnable Algorithms for In-context Learning 
[[arxiv](https://arxiv.org/abs/2402.00751)] [[cool](https://papers.cool/arxiv/2402.00751)] [[pdf](https://arxiv.org/pdf/2402.00751)]
> **Authors**: Andrei Muresanu,Anvith Thudi,Michael R. Zhang,Nicolas Papernot
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,密码学和安全
- **Abstract**: Machine unlearning is a desirable operation as models get increasingly deployed on data with unknown provenance. However, achieving exact unlearning -- obtaining a model that matches the model distribution when the data to be forgotten was never used -- is challenging or inefficient, often requiring significant retraining. In this paper, we focus on efficient unlearning methods for the task adaptation phase of a pretrained large language model (LLM). We observe that an LLM's ability to do in-context learning for task adaptation allows for efficient exact unlearning of task adaptation training data. We provide an algorithm for selecting few-shot training examples to prepend to the prompt given to an LLM (for task adaptation), ERASE, whose unlearning operation cost is independent of model and dataset size, meaning it scales to large models and datasets. We additionally compare our approach to fine-tuning approaches and discuss the trade-offs between the two approaches. This leads us to propose a new holistic measure of unlearning cost which accounts for varying inference costs, and conclude that in-context learning can often be more favourable than fine-tuning for deployments involving unlearning requests.

### Theoretical Understanding of In-Context Learning in Shallow Transformers with Unstructured Data 
[[arxiv](https://arxiv.org/abs/2402.00743)] [[cool](https://papers.cool/arxiv/2402.00743)] [[pdf](https://arxiv.org/pdf/2402.00743)]
> **Authors**: Yue Xing,Xiaofeng Lin,Chenheng Xu,Namjoon Suh,Qifan Song,Guang Cheng
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学,机器学习
- **Abstract**: Large language models (LLMs) are powerful models that can learn concepts at the inference stage via in-context learning (ICL). While theoretical studies, e.g., \cite{zhang2023trained}, attempt to explain the mechanism of ICL, they assume the input $x_i$ and the output $y_i$ of each demonstration example are in the same token (i.e., structured data). However, in real practice, the examples are usually text input, and all words, regardless of their logic relationship, are stored in different tokens (i.e., unstructured data \cite{wibisono2023role}). To understand how LLMs learn from the unstructured data in ICL, this paper studies the role of each component in the transformer architecture and provides a theoretical understanding to explain the success of the architecture. In particular, we consider a simple transformer with one/two attention layers and linear regression tasks for the ICL prediction. We observe that (1) a transformer with two layers of (self-)attentions with a look-ahead attention mask can learn from the prompt in the unstructured data, and (2) positional encoding can match the $x_i$ and $y_i$ tokens to achieve a better ICL performance.

### EE-Tuning: An Economical yet Scalable Solution for Tuning Early-Exit Large Language Models 
[[arxiv](https://arxiv.org/abs/2402.00518)] [[cool](https://papers.cool/arxiv/2402.00518)] [[pdf](https://arxiv.org/pdf/2402.00518)]
> **Authors**: Xuchen Pan,Yanxi Chen,Yaliang Li,Bolin Ding,Jingren Zhou
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: This work introduces EE-Tuning, a lightweight and economical solution to training/tuning early-exit large language models (LLMs). In contrast to the common approach of full-parameter pre-training, EE-Tuning augments any pre-trained (and possibly fine-tuned) standard LLM with additional early-exit layers that are tuned in a parameter-efficient manner, which requires significantly less computational resources and training data. Our implementation of EE-Tuning achieves outstanding training efficiency via extensive performance optimizations, as well as scalability due to its full compatibility with 3D parallelism. Results of systematic experiments validate the efficacy of EE-Tuning, confirming that effective early-exit LLM inference can be achieved with a limited training budget. In hope of making early-exit LLMs accessible to the community, we release the source code of our implementation of EE-Tuning at https://github.com/pan-x-c/EE-LLM.

### Efficient Exploration for LLMs 
[[arxiv](https://arxiv.org/abs/2402.00396)] [[cool](https://papers.cool/arxiv/2402.00396)] [[pdf](https://arxiv.org/pdf/2402.00396)]
> **Authors**: Vikranth Dwaracherla,Seyed Mohammad Asghari,Botao Hao,Benjamin Van Roy
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: Accepted at ICML 2024
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,方法论,机器学习
- **Abstract**: We present evidence of substantial benefit from efficient exploration in gathering human feedback to improve large language models. In our experiments, an agent sequentially generates queries while fitting a reward model to the feedback received. Our best-performing agent generates queries using double Thompson sampling, with uncertainty represented by an epistemic neural network. Our results demonstrate that efficient exploration enables high levels of performance with far fewer queries. Further, both uncertainty estimation and the choice of exploration scheme play critical roles.

### Efficient Non-Parametric Uncertainty Quantification for Black-Box Large Language Models and Decision Planning 
[[arxiv](https://arxiv.org/abs/2402.00251)] [[cool](https://papers.cool/arxiv/2402.00251)] [[pdf](https://arxiv.org/pdf/2402.00251)]
> **Authors**: Yao-Hung Hubert Tsai,Walter Talbott,Jian Zhang
> **First submission**: 2024-01-31
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Step-by-step decision planning with large language models (LLMs) is gaining attention in AI agent development. This paper focuses on decision planning with uncertainty estimation to address the hallucination problem in language models. Existing approaches are either white-box or computationally demanding, limiting use of black-box proprietary LLMs within budgets. The paper's first contribution is a non-parametric uncertainty quantification method for LLMs, efficiently estimating point-wise dependencies between input-decision on the fly with a single inference, without access to token logits. This estimator informs the statistical interpretation of decision trustworthiness. The second contribution outlines a systematic design for a decision-making agent, generating actions like ``turn on the bathroom light'' based on user prompts such as ``take a bath''. Users will be asked to provide preferences when more than one action has high estimated point-wise dependencies. In conclusion, our uncertainty estimation and decision-making agent design offer a cost-efficient approach for AI agent development.

### Explainable AI for survival analysis: a median-SHAP approach 
[[arxiv](https://arxiv.org/abs/2402.00072)] [[cool](https://papers.cool/arxiv/2402.00072)] [[pdf](https://arxiv.org/pdf/2402.00072)]
> **Authors**: Lucile Ter-Minassian,Sahra Ghalebikesabi,Karla Diaz-Ordaz,Chris Holmes
> **First submission**: 2024-01-30
> **First announcement**: 2024-02-02
> **comment**: Accepted to the Interpretable Machine Learning for Healthcare (IMLH) workshop of the ICML 2022 Conference
- **标题**: None
- **领域**: 机器学习,方法论,机器学习
- **Abstract**: With the adoption of machine learning into routine clinical practice comes the need for Explainable AI methods tailored to medical applications. Shapley values have sparked wide interest for locally explaining models. Here, we demonstrate their interpretation strongly depends on both the summary statistic and the estimator for it, which in turn define what we identify as an 'anchor point'. We show that the convention of using a mean anchor point may generate misleading interpretations for survival analysis and introduce median-SHAP, a method for explaining black-box models predicting individual survival times.

### Adapting Amidst Degradation: Cross Domain Li-ion Battery Health Estimation via Physics-Guided Test-Time Training 
[[arxiv](https://arxiv.org/abs/2402.00068)] [[cool](https://papers.cool/arxiv/2402.00068)] [[pdf](https://arxiv.org/pdf/2402.00068)]
> **Authors**: Yuyuan Feng,Guosheng Hu,Xiaodong Li,Zhihong Zhang
> **First submission**: 2024-01-30
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Health modeling of lithium-ion batteries (LIBs) is crucial for safe and efficient energy management and carries significant socio-economic implications. Although Machine Learning (ML)-based State of Health (SOH) estimation methods have made significant progress in accuracy, the scarcity of high-quality LIB data remains a major obstacle. Existing transfer learning methods for cross-domain LIB SOH estimation have significantly alleviated the labeling burden of target LIB data, however, they still require sufficient unlabeled target data (UTD) for effective adaptation to the target domain. Collecting this UTD is challenging due to the time-consuming nature of degradation experiments. To address this issue, we introduce a practical Test-Time Training framework, BatteryTTT, which adapts the model continually using each UTD collected amidst degradation, thereby significantly reducing data collection time. To fully utilize each UTD, BatteryTTT integrates the inherent physical laws of modern LIBs into self-supervised learning, termed Physcics-Guided Test-Time Training. Additionally, we explore the potential of large language models (LLMs) in battery sequence modeling by evaluating their performance in SOH estimation through model reprogramming and prefix prompt adaptation. The combination of BatteryTTT and LLM modeling, termed GPT4Battery, achieves state-of-the-art generalization results across current LIB benchmarks. Furthermore, we demonstrate the practical value and scalability of our approach by deploying it in our real-world battery management system (BMS) for 300Ah large-scale energy storage LIBs.

### TrackGPT -- A generative pre-trained transformer for cross-domain entity trajectory forecasting 
[[arxiv](https://arxiv.org/abs/2402.00066)] [[cool](https://papers.cool/arxiv/2402.00066)] [[pdf](https://arxiv.org/pdf/2402.00066)]
> **Authors**: Nicholas Stroh
> **First submission**: 2024-01-29
> **First announcement**: 2024-02-02
> **comment**: 16 pages, 8 figures
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: The forecasting of entity trajectories at future points in time is a critical capability gap in applications across both Commercial and Defense sectors. Transformers, and specifically Generative Pre-trained Transformer (GPT) networks have recently revolutionized several fields of Artificial Intelligence, most notably Natural Language Processing (NLP) with the advent of Large Language Models (LLM) like OpenAI's ChatGPT. In this research paper, we introduce TrackGPT, a GPT-based model for entity trajectory forecasting that has shown utility across both maritime and air domains, and we expect to perform well in others. TrackGPT stands as a pioneering GPT model capable of producing accurate predictions across diverse entity time series datasets, demonstrating proficiency in generating both long-term forecasts with sustained accuracy and short-term forecasts with high precision. We present benchmarks against state-of-the-art deep learning techniques, showing that TrackGPT's forecasting capability excels in terms of accuracy, reliability, and modularity. Importantly, TrackGPT achieves these results while remaining domain-agnostic and requiring minimal data features (only location and time) compared to models achieving similar performance. In conclusion, our findings underscore the immense potential of applying GPT architectures to the task of entity trajectory forecasting, exemplified by the innovative TrackGPT model.

## 多媒体(cs.MM:Multimedia)

### Detecting Multimedia Generated by Large AI Models: A Survey 
[[arxiv](https://arxiv.org/abs/2402.00045)] [[cool](https://papers.cool/arxiv/2402.00045)] [[pdf](https://arxiv.org/pdf/2402.00045)]
> **Authors**: Li Lin,Neeraj Gupta,Yue Zhang,Hainan Ren,Chun-Hao Liu,Feng Ding,Xin Wang,Xin Li,Luisa Verdoliva,Shu Hu
> **First submission**: 2024-01-22
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 多媒体,人工智能,机器学习
- **Abstract**: The rapid advancement of Large AI Models (LAIMs), particularly diffusion models and large language models, has marked a new era where AI-generated multimedia is increasingly integrated into various aspects of daily life. Although beneficial in numerous fields, this content presents significant risks, including potential misuse, societal disruptions, and ethical concerns. Consequently, detecting multimedia generated by LAIMs has become crucial, with a marked rise in related research. Despite this, there remains a notable gap in systematic surveys that focus specifically on detecting LAIM-generated multimedia. Addressing this, we provide the first survey to comprehensively cover existing research on detecting multimedia (such as text, images, videos, audio, and multimodal content) created by LAIMs. Specifically, we introduce a novel taxonomy for detection methods, categorized by media modality, and aligned with two perspectives: pure detection (aiming to enhance detection performance) and beyond detection (adding attributes like generalizability, robustness, and interpretability to detectors). Additionally, we have presented a brief overview of generation mechanisms, public datasets, and online detection tools to provide a valuable resource for researchers and practitioners in this field. Furthermore, we identify current challenges in detection and propose directions for future research that address unexplored, ongoing, and emerging issues in detecting multimedia generated by LAIMs. Our aim for this survey is to fill an academic gap and contribute to global AI security efforts, helping to ensure the integrity of information in the digital realm. The project link is https://github.com/Purdue-M2/Detect-LAIM-generated-Multimedia-Survey.

## 神经和进化计算(cs.NE:Neural and Evolutionary Computing)

### SCAPE: Searching Conceptual Architecture Prompts using Evolution 
[[arxiv](https://arxiv.org/abs/2402.00089)] [[cool](https://papers.cool/arxiv/2402.00089)] [[pdf](https://arxiv.org/pdf/2402.00089)]
> **Authors**: Soo Ling Lim,Peter J Bentley,Fuyuki Ishikawa
> **First submission**: 2024-01-31
> **First announcement**: 2024-02-02
> **comment**: 8 pages
- **标题**: None
- **领域**: 神经和进化计算,人工智能
- **Abstract**: Conceptual architecture involves a highly creative exploration of novel ideas, often taken from other disciplines as architects consider radical new forms, materials, textures and colors for buildings. While today's generative AI systems can produce remarkable results, they lack the creativity demonstrated for decades by evolutionary algorithms. SCAPE, our proposed tool, combines evolutionary search with generative AI, enabling users to explore creative and good quality designs inspired by their initial input through a simple point and click interface. SCAPE injects randomness into generative AI, and enables memory, making use of the built-in language skills of GPT-4 to vary prompts via text-based mutation and crossover. We demonstrate that compared to DALL-E 3, SCAPE enables a 67% improvement in image novelty, plus improvements in quality and effectiveness of use; we show that in just three iterations SCAPE has a 24% image novelty increase enabling effective exploration, plus optimization of images by users. We use more than 20 independent architects to assess SCAPE, who provide markedly positive feedback.

### Evolution-Bootstrapped Simulation: Artificial or Human Intelligence: Which Came First? 
[[arxiv](https://arxiv.org/abs/2402.00030)] [[cool](https://papers.cool/arxiv/2402.00030)] [[pdf](https://arxiv.org/pdf/2402.00030)]
> **Authors**: Paul Alexander Bilokon
> **First submission**: 2024-01-06
> **First announcement**: 2024-02-02
> **comment**: 6 pages, no figures
- **标题**: None
- **领域**: 神经和进化计算,人工智能,种群与进化
- **Abstract**: Humans have created artificial intelligence (AI), not the other way around. This statement is deceptively obvious. In this note, we decided to challenge this statement as a small, lighthearted Gedankenexperiment. We ask a simple question: in a world driven by evolution by natural selection, would neural networks or humans be likely to evolve first? We compare the Solomonoff--Kolmogorov--Chaitin complexity of the two and find neural networks (even LLMs) to be significantly simpler than humans. Further, we claim that it is unnecessary for any complex human-made equipment to exist for there to be neural networks. Neural networks may have evolved as naturally occurring objects before humans did as a form of chemical reaction-based or enzyme-based computation. Now that we know that neural networks can pass the Turing test and suspect that they may be capable of superintelligence, we ask whether the natural evolution of neural networks could lead from pure evolution by natural selection to what we call evolution-bootstrapped simulation. The evolution of neural networks does not involve irreducible complexity; would easily allow irreducible complexity to exist in the evolution-bootstrapped simulation; is a falsifiable scientific hypothesis; and is independent of / orthogonal to the issue of intelligent design.

## 网络和互联网架构(cs.NI:Networking and Internet Architecture)

### On the Interplay of Artificial Intelligence and Space-Air-Ground Integrated Networks: A Survey 
[[arxiv](https://arxiv.org/abs/2402.00881)] [[cool](https://papers.cool/arxiv/2402.00881)] [[pdf](https://arxiv.org/pdf/2402.00881)]
> **Authors**: Adilya Bakambekova,Nour Kouzayha,Tareq Al-Naffouri
> **First submission**: 2024-01-20
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 网络和互联网架构,人工智能
- **Abstract**: Space-Air-Ground Integrated Networks (SAGINs), which incorporate space and aerial networks with terrestrial wireless systems, are vital enablers of the emerging sixth-generation (6G) wireless networks. Besides bringing significant benefits to various applications and services, SAGINs are envisioned to extend high-speed broadband coverage to remote areas, such as small towns or mining sites, or areas where terrestrial infrastructure cannot reach, such as airplanes or maritime use cases. However, due to the limited power and storage resources, as well as other constraints introduced by the design of terrestrial networks, SAGINs must be intelligently configured and controlled to satisfy the envisioned requirements. Meanwhile, Artificial Intelligence (AI) is another critical enabler of 6G. Due to massive amounts of available data, AI has been leveraged to address pressing challenges of current and future wireless networks. By adding AI and facilitating the decision-making and prediction procedures, SAGINs can effectively adapt to their surrounding environment, thus enhancing the performance of various metrics. In this work, we aim to investigate the interplay of AI and SAGINs by providing a holistic overview of state-of-the-art research in AI-enabled SAGINs. Specifically, we present a comprehensive overview of some potential applications of AI in SAGINs. We also cover open issues in employing AI and detail the contributions of SAGINs in the development of AI. Finally, we highlight some limitations of the existing research works and outline potential future research directions.

### Building Blocks to Empower Cognitive Internet with Hybrid Edge Cloud 
[[arxiv](https://arxiv.org/abs/2402.00876)] [[cool](https://papers.cool/arxiv/2402.00876)] [[pdf](https://arxiv.org/pdf/2402.00876)]
> **Authors**: Siavash Alamouti,Fay Arjomandi,Michel Burger,Bashar Altakrouri
> **First submission**: 2024-01-10
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 网络和互联网架构,人工智能
- **Abstract**: As we transition from the mobile internet to the 'Cognitive Internet,' a significant shift occurs in how we engage with technology and intelligence. We contend that the Cognitive Internet goes beyond the Cognitive Internet of Things (Cognitive IoT), enabling connected objects to independently acquire knowledge and understanding. Unlike the Mobile Internet and Cognitive IoT, the Cognitive Internet integrates collaborative intelligence throughout the network, blending the cognitive IoT realm with system-wide collaboration and human intelligence. This integrated intelligence facilitates interactions between devices, services, entities, and individuals across diverse domains while preserving decision-making autonomy and accommodating various identities. The paper delves into the foundational elements, distinct characteristics, benefits, and industrial impact of the 'Cognitive Internet' paradigm. It highlights the importance of adaptable AI infrastructures and hybrid edge cloud (HEC) platforms in enabling this shift. This evolution brings forth cognitive services, a Knowledge as a Service (KaaS) economy, enhanced decision-making autonomy, sustainable digital progress, advancements in data management, processing techniques, and a stronger emphasis on privacy. In essence, this paper serves as a crucial resource for understanding and leveraging the transformative potential of HEC for Cognitive Internet. Supported by case studies, forward-looking perspectives, and real-world applications, it provides comprehensive insights into this emerging paradigm.

## 机器人技术(cs.RO:Robotics)

### Human-mediated Large Language Models for Robotic Intervention in Children with Autism Spectrum Disorders 
[[arxiv](https://arxiv.org/abs/2402.00260)] [[cool](https://papers.cool/arxiv/2402.00260)] [[pdf](https://arxiv.org/pdf/2402.00260)]
> **Authors**: Ruchik Mishra,Karla Conn Welch,Dan O Popa
> **First submission**: 2024-01-31
> **First announcement**: 2024-02-02
> **comment**: This work is submitted for possible publication
- **标题**: None
- **领域**: 机器人技术,人工智能
- **Abstract**: The robotic intervention for individuals with Autism Spectrum Disorder (ASD) has generally used pre-defined scripts to deliver verbal content during one-to-one therapy sessions. This practice restricts the use of robots to limited, pre-mediated instructional curricula. In this paper, we increase robot autonomy in one such robotic intervention for children with ASD by implementing perspective-taking teaching. Our approach uses large language models (LLM) to generate verbal content as texts and then deliver it to the child via robotic speech. In the proposed pipeline, we teach perspective-taking through which our robot takes up three roles: initiator, prompter, and reinforcer. We adopted the GPT-2 + BART pipelines to generate social situations, ask questions (as initiator), and give options (as prompter) when required. The robot encourages the child by giving positive reinforcement for correct answers (as a reinforcer). In addition to our technical contribution, we conducted ten-minute sessions with domain experts simulating an actual perspective teaching session, with the researcher acting as a child participant. These sessions validated our robotic intervention pipeline through surveys, including those from NASA TLX and GodSpeed. We used BERTScore to compare our GPT-2 + BART pipeline with an all GPT-2 and found the performance of the former to be better. Based on the responses by the domain experts, the robot session demonstrated higher performance with no additional increase in mental or physical demand, temporal demand, effort, or frustration compared to a no-robot session. We also concluded that the domain experts perceived the robot as ideally safe, likable, and reliable.

### Training microrobots to swim by a large language model 
[[arxiv](https://arxiv.org/abs/2402.00044)] [[cool](https://papers.cool/arxiv/2402.00044)] [[pdf](https://arxiv.org/pdf/2402.00044)]
> **Authors**: Zhuoqun Xu,Lailai Zhu
> **First submission**: 2024-01-21
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,机器学习
- **Abstract**: Machine learning and artificial intelligence have recently represented a popular paradigm for designing and optimizing robotic systems across various scales. Recent studies have showcased the innovative application of large language models (LLMs) in industrial control [1] and in directing legged walking robots [2]. In this study, we utilize an LLM, GPT-4, to train two prototypical microrobots for swimming in viscous fluids. Adopting a few-shot learning approach, we develop a minimal, unified prompt composed of only five sentences. The same concise prompt successfully guides two distinct articulated microrobots -- the three-link swimmer and the three-sphere swimmer -- in mastering their signature strokes. These strokes, initially conceptualized by physicists, are now effectively interpreted and applied by the LLM, enabling the microrobots to circumvent the physical constraints inherent to micro-locomotion. Remarkably, our LLM-based decision-making strategy substantially surpasses a traditional reinforcement learning method in terms of training speed. We discuss the nuanced aspects of prompt design, particularly emphasizing the reduction of monetary expenses of using GPT-4.

## 声音(cs.SD:Sound)

### BATON: Aligning Text-to-Audio Model with Human Preference Feedback 
[[arxiv](https://arxiv.org/abs/2402.00744)] [[cool](https://papers.cool/arxiv/2402.00744)] [[pdf](https://arxiv.org/pdf/2402.00744)]
> **Authors**: Huan Liao,Haonan Han,Kai Yang,Tianjiao Du,Rui Yang,Zunnan Xu,Qinmei Xu,Jingquan Liu,Jiasheng Lu,Xiu Li
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 声音,计算语言学,音频和语音处理
- **Abstract**: With the development of AI-Generated Content (AIGC), text-to-audio models are gaining widespread attention. However, it is challenging for these models to generate audio aligned with human preference due to the inherent information density of natural language and limited model understanding ability. To alleviate this issue, we formulate the BATON, a framework designed to enhance the alignment between generated audio and text prompt using human preference feedback. Our BATON comprises three key stages: Firstly, we curated a dataset containing both prompts and the corresponding generated audio, which was then annotated based on human feedback. Secondly, we introduced a reward model using the constructed dataset, which can mimic human preference by assigning rewards to input text-audio pairs. Finally, we employed the reward model to fine-tune an off-the-shelf text-to-audio model. The experiment results demonstrate that our BATON can significantly improve the generation quality of the original text-to-audio models, concerning audio integrity, temporal relationship, and alignment with human preference.

## 软件工程(cs.SE:Software Engineering)

### Large Language Models Based Fuzzing Techniques: A Survey 
[[arxiv](https://arxiv.org/abs/2402.00350)] [[cool](https://papers.cool/arxiv/2402.00350)] [[pdf](https://arxiv.org/pdf/2402.00350)]
> **Authors**: Linghan Huang,Peizhou Zhao,Huaming Chen,Lei Ma
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: 9 pages submission under review
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: In the modern era where software plays a pivotal role, software security and vulnerability analysis have become essential for software development. Fuzzing test, as an efficient software testing method, are widely used in various domains. Moreover, the rapid development of Large Language Models (LLMs) has facilitated their application in the field of software testing, demonstrating remarkable performance. Considering that existing fuzzing test techniques are not entirely automated and software vulnerabilities continue to evolve, there is a growing trend towards employing fuzzing test generated based on large language models. This survey provides a systematic overview of the approaches that fuse LLMs and fuzzing tests for software testing. In this paper, a statistical analysis and discussion of the literature in three areas, namely LLMs, fuzzing test, and fuzzing test generated based on LLMs, are conducted by summarising the state-of-the-art methods up until 2024. Our survey also investigates the potential for widespread deployment and application of fuzzing test techniques generated by LLMs in the future.

### Code-Aware Prompting: A study of Coverage Guided Test Generation in Regression Setting using LLM 
[[arxiv](https://arxiv.org/abs/2402.00097)] [[cool](https://papers.cool/arxiv/2402.00097)] [[pdf](https://arxiv.org/pdf/2402.00097)]
> **Authors**: Gabriel Ryan,Siddhartha Jain,Mingyue Shang,Shiqi Wang,Xiaofei Ma,Murali Krishna Ramanathan,Baishakhi Ray
> **First submission**: 2024-01-31
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,机器学习
- **Abstract**: Testing plays a pivotal role in ensuring software quality, yet conventional Search Based Software Testing (SBST) methods often struggle with complex software units, achieving suboptimal test coverage. Recent works using large language models (LLMs) for test generation have focused on improving generation quality through optimizing the test generation context and correcting errors in model outputs, but use fixed prompting strategies that prompt the model to generate tests without additional guidance. As a result LLM-generated testsuites still suffer from low coverage. In this paper, we present SymPrompt, a code-aware prompting strategy for LLMs in test generation. SymPrompt's approach is based on recent work that demonstrates LLMs can solve more complex logical problems when prompted to reason about the problem in a multi-step fashion. We apply this methodology to test generation by deconstructing the testsuite generation process into a multi-stage sequence, each of which is driven by a specific prompt aligned with the execution paths of the method under test, and exposing relevant type and dependency focal context to the model. Our approach enables pretrained LLMs to generate more complete test cases without any additional training. We implement SymPrompt using the TreeSitter parsing framework and evaluate on a benchmark challenging methods from open source Python projects. SymPrompt enhances correct test generations by a factor of 5 and bolsters relative coverage by 26% for CodeGen2. Notably, when applied to GPT-4, SymPrompt improves coverage by over 2x compared to baseline prompting strategies.

### ChIRAAG: ChatGPT Informed Rapid and Automated Assertion Generation 
[[arxiv](https://arxiv.org/abs/2402.00093)] [[cool](https://papers.cool/arxiv/2402.00093)] [[pdf](https://arxiv.org/pdf/2402.00093)]
> **Authors**: Bhabesh Mali,Karthik Maddala,Vatsal Gupta,Sweeya Reddy,Chandan Karfa,Ramesh Karri
> **First submission**: 2024-01-31
> **First announcement**: 2024-02-02
> **comment**: 4 pages, 2 figures and 2 tables
- **标题**: None
- **领域**: 软件工程,机器学习
- **Abstract**: System Verilog Assertion (SVA) formulation -- a critical yet complex task is a prerequisite in the Assertion Based Verification (ABV) process. Traditionally, SVA formulation involves expert-driven interpretation of specifications, which is time-consuming and prone to human error. Recently, LLM-informed automatic assertion generation is gaining interest. We designed a novel framework called ChIRAAG, based on OpenAI GPT4, to generate SVA from natural language specifications of a design. ChIRAAG constitutes the systematic breakdown of design specifications into a standardized format, further generating assertions from formatted specifications using LLM. Furthermore, we used few test cases to validate the LLM-generated assertions. Automatic feedback of log messages from the simulation tool to the LLM ensures that the framework can generate correct SVAs. In our experiments, only 27% of LLM-generated raw assertions had errors, which was rectified in few iterations based on the simulation log. Our results on OpenTitan designs show that LLMs can streamline and assist engineers in the assertion generation process, reshaping verification workflows.

## 图像和视频处理(eess.IV:Image and Video Processing)

### Unconditional Latent Diffusion Models Memorize Patient Imaging Data: Implications for Openly Sharing Synthetic Data 
[[arxiv](https://arxiv.org/abs/2402.01054)] [[cool](https://papers.cool/arxiv/2402.01054)] [[pdf](https://arxiv.org/pdf/2402.01054)]
> **Authors**: Salman Ul Hassan Dar,Marvin Seyfarth,Isabelle Ayx,Theano Papavassiliu,Stefan O. Schoenberg,Robert Malte Siepmann,Fabian Christopher Laqua,Jannik Kahmann,Norbert Frey,Bettina Baeßler,Sebastian Foersch,Daniel Truhn,Jakob Nikolas Kather,Sandy Engelhardt
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习
- **Abstract**: AI models present a wide range of applications in the field of medicine. However, achieving optimal performance requires access to extensive healthcare data, which is often not readily available. Furthermore, the imperative to preserve patient privacy restricts patient data sharing with third parties and even within institutes. Recently, generative AI models have been gaining traction for facilitating open-data sharing by proposing synthetic data as surrogates of real patient data. Despite the promise, some of these models are susceptible to patient data memorization, where models generate patient data copies instead of novel synthetic samples. Considering the importance of the problem, surprisingly it has received relatively little attention in the medical imaging community. To this end, we assess memorization in unconditional latent diffusion models. We train latent diffusion models on CT, MR, and X-ray datasets for synthetic data generation. We then detect the amount of training data memorized utilizing our novel self-supervised copy detection approach and further investigate various factors that can influence memorization. Our findings show a surprisingly high degree of patient data memorization across all datasets. Comparison with non-diffusion generative models, such as autoencoders and generative adversarial networks, indicates that while latent diffusion models are more susceptible to memorization, overall they outperform non-diffusion models in synthesis quality. Further analyses reveal that using augmentation strategies, small architecture, and increasing dataset can reduce memorization while over-training the models can enhance it. Collectively, our results emphasize the importance of carefully training generative models on private medical imaging datasets, and examining the synthetic data to ensure patient privacy before sharing it for medical research and applications.

### Detecting Brain Tumors through Multimodal Neural Networks 
[[arxiv](https://arxiv.org/abs/2402.00038)] [[cool](https://papers.cool/arxiv/2402.00038)] [[pdf](https://arxiv.org/pdf/2402.00038)]
> **Authors**: Antonio Curci,Andrea Esposito
> **First submission**: 2024-01-10
> **First announcement**: 2024-02-02
> **comment**: Presented at NeroPRAI 2024 (co-located with ICPRAM 2024). This version did not undergo peer review: refer to the open access version of record (see DOI)
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习,定量方法
- **Abstract**: Tumors can manifest in various forms and in different areas of the human body. Brain tumors are specifically hard to diagnose and treat because of the complexity of the organ in which they develop. Detecting them in time can lower the chances of death and facilitate the therapy process for patients. The use of Artificial Intelligence (AI) and, more specifically, deep learning, has the potential to significantly reduce costs in terms of time and resources for the discovery and identification of tumors from images obtained through imaging techniques. This research work aims to assess the performance of a multimodal model for the classification of Magnetic Resonance Imaging (MRI) scans processed as grayscale images. The results are promising, and in line with similar works, as the model reaches an accuracy of around 98\%. We also highlight the need for explainability and transparency to ensure human control and safety.

## 高能物理 - 理论(hep-th:High Energy Physics - Theory)

### NCoder -- A Quantum Field Theory approach to encoding data 
[[arxiv](https://arxiv.org/abs/2402.00944)] [[cool](https://papers.cool/arxiv/2402.00944)] [[pdf](https://arxiv.org/pdf/2402.00944)]
> **Authors**: David S. Berman,Marc S. Klinger,Alexander G. Stapleton
> **First submission**: 2024-02-01
> **First announcement**: 2024-02-02
> **comment**: 29 pages. v2 Fixed minor typos
- **标题**: None
- **领域**: 高能物理 - 理论,无序系统和神经网络,人工智能
- **Abstract**: In this paper we present a novel approach to interpretable AI inspired by Quantum Field Theory (QFT) which we call the NCoder. The NCoder is a modified autoencoder neural network whose latent layer is prescribed to be a subset of $n$-point correlation functions. Regarding images as draws from a lattice field theory, this architecture mimics the task of perturbatively constructing the effective action of the theory order by order in an expansion using Feynman diagrams. Alternatively, the NCoder may be regarded as simulating the procedure of statistical inference whereby high dimensional data is first summarized in terms of several lower dimensional summary statistics (here the $n$-point correlation functions), and subsequent out-of-sample data is generated by inferring the data generating distribution from these statistics. In this way the NCoder suggests a fascinating correspondence between perturbative renormalizability and the sufficiency of models. We demonstrate the efficacy of the NCoder by applying it to the generation of MNIST images, and find that generated images can be correctly classified using only information from the first three $n$-point functions of the image distribution.

## 生物分子(q-bio.BM:Biomolecules)

### Can Large Language Models Understand Molecules? 
[[arxiv](https://arxiv.org/abs/2402.00024)] [[cool](https://papers.cool/arxiv/2402.00024)] [[pdf](https://arxiv.org/pdf/2402.00024)]
> **Authors**: Shaghayegh Sadeghi,Alan Bui,Ali Forooghi,Jianguo Lu,Alioune Ngom
> **First submission**: 2024-01-05
> **First announcement**: 2024-02-02
> **comment**: No comments
- **标题**: None
- **领域**: 生物分子,人工智能,计算语言学,机器学习
- **Abstract**: Purpose: Large Language Models (LLMs) like GPT (Generative Pre-trained Transformer) from OpenAI and LLaMA (Large Language Model Meta AI) from Meta AI are increasingly recognized for their potential in the field of cheminformatics, particularly in understanding Simplified Molecular Input Line Entry System (SMILES), a standard method for representing chemical structures. These LLMs also have the ability to decode SMILES strings into vector representations. Method: We investigate the performance of GPT and LLaMA compared to pre-trained models on SMILES in embedding SMILES strings on downstream tasks, focusing on two key applications: molecular property prediction and drug-drug interaction prediction. Results: We find that SMILES embeddings generated using LLaMA outperform those from GPT in both molecular property and DDI prediction tasks. Notably, LLaMA-based SMILES embeddings show results comparable to pre-trained models on SMILES in molecular prediction tasks and outperform the pre-trained models for the DDI prediction tasks. Conclusion: The performance of LLMs in generating SMILES embeddings shows great potential for further investigation of these models for molecular embedding. We hope our study bridges the gap between LLMs and molecular embedding, motivating additional research into the potential of LLMs in the molecular representation field. GitHub: https://github.com/sshaghayeghs/LLaMA-VS-GPT

## 其他定量生物学(q-bio.OT:Other Quantitative Biology)

### The whack-a-mole governance challenge for AI-enabled synthetic biology: literature review and emerging frameworks 
[[arxiv](https://arxiv.org/abs/2402.00312)] [[cool](https://papers.cool/arxiv/2402.00312)] [[pdf](https://arxiv.org/pdf/2402.00312)]
> **Authors**: Trond Arne Undheim
> **First submission**: 2024-01-31
> **First announcement**: 2024-02-02
> **comment**: ef:Front. Bioeng. Biotechnol. 12:1359768.
- **标题**: None
- **领域**: 其他定量生物学,人工智能
- **Abstract**: AI-enabled synthetic biology has tremendous potential but also significantly increases biorisks and brings about a new set of dual use concerns. The picture is complicated given the vast innovations envisioned to emerge by combining emerging technologies, as AI-enabled synthetic biology potentially scales up bioengineering into industrial biomanufacturing. However, the literature review indicates that goals such as maintaining a reasonable scope for innovation, or more ambitiously to foster a huge bioeconomy don't necessarily contrast with biosafety, but need to go hand in hand. This paper presents a literature review of the issues and describes emerging frameworks for policy and practice that transverse the options of command-and control, stewardship, bottom-up, and laissez-faire governance. How to achieve early warning systems that enable prevention and mitigation of future AI-enabled biohazards from the lab, from deliberate misuse, or from the public realm, will constantly need to evolve, and adaptive, interactive approaches should emerge. Although biorisk is subject to an established governance regime, and scientists generally adhere to biosafety protocols, even experimental, but legitimate use by scientists could lead to unexpected developments. Recent advances in chatbots enabled by generative AI have revived fears that advanced biological insight can more easily get into the hands of malignant individuals or organizations. Given these sets of issues, society needs to rethink how AI-enabled synthetic biology should be governed. The suggested way to visualize the challenge at hand is whack-a-mole governance, although the emerging solutions are perhaps not so different either.

## 其他论文

- [A Single Simple Patch is All You Need for AI-generated Image Detection](https://arxiv.org/abs/2402.01123)
  - **标题**: None
  - **Filtered Reason**: none of cs.CV in whitelist
- [IMUGPT 2.0: Language-Based Cross Modality Transfer for Sensor-Based Human Activity Recognition](https://arxiv.org/abs/2402.01049)
  - **标题**: None
  - **Filtered Reason**: none of cs.CV in whitelist
- [VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image Segmentation and Classification](https://arxiv.org/abs/2402.01034)
  - **标题**: None
  - **Filtered Reason**: none of eess.IV,cs.CV in whitelist
- [algoXSSF: Detection and analysis of cross-site request forgery (XSRF) and cross-site scripting (XSS) attacks via Machine learning algorithms](https://arxiv.org/abs/2402.01012)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [Semantic Constraint Inference for Web Form Test Generation](https://arxiv.org/abs/2402.00950)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Fine-Tuning and Prompt Engineering for Large Language Models-based Code Review Automation](https://arxiv.org/abs/2402.00905)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Utilizing Large Language Models to Translate RFC Protocol Specifications to CPSA Definitions](https://arxiv.org/abs/2402.00890)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE,cs.CR,cs.NI in whitelist
- [Common errors in Generative AI systems used for knowledge extraction in the climate action domain](https://arxiv.org/abs/2402.00830)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [To Search or To Gen? Exploring the Synergy between Generative AI and Web Search in Programming](https://arxiv.org/abs/2402.00764)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [BIOMERO: BioImage analysis in OMERO](https://arxiv.org/abs/2402.00734)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Comparative Study of Large Language Model Architectures on Frontier](https://arxiv.org/abs/2402.00691)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC in whitelist
- [Responsible developments and networking research: a reflection beyond a paper ethical statement](https://arxiv.org/abs/2402.00442)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [AssertLLM: Generating and Evaluating Hardware Verification Assertions from Design Specifications via Multi-LLMs](https://arxiv.org/abs/2402.00386)
  - **标题**: None
  - **Filtered Reason**: none of cs.AR in whitelist
- [An Exam-based Evaluation Approach Beyond Traditional Relevance Judgments](https://arxiv.org/abs/2402.00309)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [Effective Bug Detection in Graph Database Engines: An LLM-based Approach](https://arxiv.org/abs/2402.00292)
  - **标题**: None
  - **Filtered Reason**: none of cs.DB in whitelist
- [Towards AI-Assisted Synthesis of Verified Dafny Methods](https://arxiv.org/abs/2402.00247)
  - **标题**: None
  - **Filtered Reason**: none of cs.PL,cs.SE in whitelist
- [Design and Implementation of Hardware Accelerators for Neural Processing Applications](https://arxiv.org/abs/2402.00051)
  - **标题**: None
  - **Filtered Reason**: none of eess.SY,cs.NE in whitelist
- [No More Trade-Offs. GPT and Fully Informative Privacy Policies](https://arxiv.org/abs/2402.00013)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR,cs.CY in whitelist
