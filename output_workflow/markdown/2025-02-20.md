> 本文由 [https://github.com/huiyeruzhou/arxiv_crawler](https://github.com/huiyeruzhou/arxiv_crawler) 自动生成
>
> 领域白名单：cs.AI,cs.CL,cs.LG,cs.CV
> 关键词： LLM, GPT, AI, language+model, deep+learning, transformer, neural+network, machine+learning

# 论文全览：2025-02-20

共有287篇相关领域论文, 另有32篇其他

## 天体物理学仪器和方法(astro-ph.IM:Instrumentation and Methods for Astrophysics)

### Evaluation of EAS directions based on TAIGA HiSCORE data using fully connected neural networks 
[[arxiv](https://arxiv.org/abs/2502.13851)] [[cool](https://papers.cool/arxiv/2502.13851)] [[pdf](https://arxiv.org/pdf/2502.13851)]
> **Authors**: A. P. Kryukov,S. P. Polyakov,Yu. Yu. Dubenskaya,E. O. Gres,E. B. Postnikov,P. A. Volchugov,D. P. Zhurov
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: The work was reported on the 8th International Conference onDeepLearningin Computational Physics (DLCP2025), June 19-21, 2024, Moscow, Russia (https://dlcp2024.sinp.msu.ru/). To bee published in Moscow University Physics Bulletin
- **标题**: None
- **领域**: 天体物理学仪器和方法,高能天体物理现象,机器学习
- **Abstract**: The direction of extensive air showers can be used to determine the source of gamma quanta and plays an important role in estimating the energy of the primary particle. The data from an array of non-imaging Cherenkov detector stations HiSCORE in the TAIGA experiment registering the number of photoelectrons and detection time can be used to estimate the shower direction with high accuracy. In this work, we use artificial neural networks trained on Monte Carlo-simulated TAIGA HiSCORE data for gamma quanta to obtain shower direction estimates. The neural networks are multilayer perceptrons with skip connections using partial data from several HiSCORE stations as inputs; composite estimates are derived from multiple individual estimates by the neural networks. We apply a two-stage algorithm in which the direction estimates obtained in the first stage are used to transform the input data and refine the estimates. The mean error of the final estimates is less than 0.25 degrees. The approach will be used for multimodal analysis of the data from several types of detectors used in the TAIGA experiment.

## 材料科学(cond-mat.mtrl-sci:Materials Science)

### OBELiX: A Curated Dataset of Crystal Structures and Experimentally Measured Ionic Conductivities for Lithium Solid-State Electrolytes 
[[arxiv](https://arxiv.org/abs/2502.14234)] [[cool](https://papers.cool/arxiv/2502.14234)] [[pdf](https://arxiv.org/pdf/2502.14234)]
> **Authors**: Félix Therrien,Jamal Abou Haibeh,Divya Sharma,Rhiannon Hendley,Alex Hernández-García,Sun Sun,Alain Tchagang,Jiang Su,Samuel Huberman,Yoshua Bengio,Hongyu Guo,Homin Shin
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 8 pages, 3 figures and 2 tables
- **标题**: None
- **领域**: 材料科学,机器学习
- **Abstract**: Solid-state electrolyte batteries are expected to replace liquid electrolyte lithium-ion batteries in the near future thanks to their higher theoretical energy density and improved safety. However, their adoption is currently hindered by their lower effective ionic conductivity, a quantity that governs charge and discharge rates. Identifying highly ion-conductive materials using conventional theoretical calculations and experimental validation is both time-consuming and resource-intensive. While machine learning holds the promise to expedite this process, relevant ionic conductivity and structural data is scarce. Here, we present OBELiX, a domain-expert-curated database of $\sim$600 synthesized solid electrolyte materials and their experimentally measured room temperature ionic conductivities gathered from literature. Each material is described by their measured composition, space group and lattice parameters. A full-crystal description in the form of a crystallographic information file (CIF) is provided for ~320 structures for which atomic positions were available. We discuss various statistics and features of the dataset and provide training and testing splits that avoid data leakage. Finally, we benchmark seven existing ML models on the task of predicting ionic conductivity and discuss their performance. The goal of this work is to facilitate the use of machine learning for solid-state electrolyte materials discovery.

### AI-Driven Discovery of High Performance Polymer Electrodes for Next-Generation Batteries 
[[arxiv](https://arxiv.org/abs/2502.13899)] [[cool](https://papers.cool/arxiv/2502.13899)] [[pdf](https://arxiv.org/pdf/2502.13899)]
> **Authors**: Subhash V. S. Ganti,Lukas Woelfel,Christopher Kuenneth
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 33 pages, 10 figures, 3 tables
- **标题**: None
- **领域**: 材料科学,机器学习,应用物理
- **Abstract**: The use of transition group metals in electric batteries requires extensive usage of critical elements like lithium, cobalt and nickel, which poses significant environmental challenges. Replacing these metals with redox-active organic materials offers a promising alternative, thereby reducing the carbon footprint of batteries by one order of magnitude. However, this approach faces critical obstacles, including the limited availability of suitable redox-active organic materials and issues such as lower electronic conductivity, voltage, specific capacity, and long-term stability. To overcome the limitations for lower voltage and specific capacity, a machine learning (ML) driven battery informatics framework is developed and implemented. This framework utilizes an extensive battery dataset and advanced ML techniques to accelerate and enhance the identification, optimization, and design of redox-active organic materials. In this contribution, a data-fusion ML coupled meta learning model capable of predicting the battery properties, voltage and specific capacity, for various organic negative electrodes and charge carriers (positive electrode materials) combinations is presented. The ML models accelerate experimentation, facilitate the inverse design of battery materials, and identify suitable candidates from three extensive material libraries to advance sustainable energy-storage technologies.

## 人工智能(cs.AI:Artificial Intelligence)

### Investigating the Impact of LLM Personality on Cognitive Bias Manifestation in Automated Decision-Making Tasks 
[[arxiv](https://arxiv.org/abs/2502.14219)] [[cool](https://papers.cool/arxiv/2502.14219)] [[pdf](https://arxiv.org/pdf/2502.14219)]
> **Authors**: Jiangen He,Jiqun Liu
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Large Language Models (LLMs) are increasingly used in decision-making, yet their susceptibility to cognitive biases remains a pressing challenge. This study explores how personality traits influence these biases and evaluates the effectiveness of mitigation strategies across various model architectures. Our findings identify six prevalent cognitive biases, while the sunk cost and group attribution biases exhibit minimal impact. Personality traits play a crucial role in either amplifying or reducing biases, significantly affecting how LLMs respond to debiasing techniques. Notably, Conscientiousness and Agreeableness may generally enhance the efficacy of bias mitigation strategies, suggesting that LLMs exhibiting these traits are more receptive to corrective measures. These findings address the importance of personality-driven bias dynamics and highlight the need for targeted mitigation approaches to improve fairness and reliability in AI-assisted decision-making.

### Giving AI Personalities Leads to More Human-Like Reasoning 
[[arxiv](https://arxiv.org/abs/2502.14155)] [[cool](https://papers.cool/arxiv/2502.14155)] [[pdf](https://arxiv.org/pdf/2502.14155)]
> **Authors**: Animesh Nighojkar,Bekhzodbek Moydinboyev,My Duong,John Licato
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学,计算机与社会
- **Abstract**: In computational cognitive modeling, capturing the full spectrum of human judgment and decision-making processes, beyond just optimal behaviors, is a significant challenge. This study explores whether Large Language Models (LLMs) can emulate the breadth of human reasoning by predicting both intuitive, fast System 1 and deliberate, slow System 2 processes. We investigate the potential of AI to mimic diverse reasoning behaviors across a human population, addressing what we call the "full reasoning spectrum problem". We designed reasoning tasks using a novel generalization of the Natural Language Inference (NLI) format to evaluate LLMs' ability to replicate human reasoning. The questions were crafted to elicit both System 1 and System 2 responses. Human responses were collected through crowd-sourcing and the entire distribution was modeled, rather than just the majority of the answers. We used personality-based prompting inspired by the Big Five personality model to elicit AI responses reflecting specific personality traits, capturing the diversity of human reasoning, and exploring how personality traits influence LLM outputs. Combined with genetic algorithms to optimize the weighting of these prompts, this method was tested alongside traditional machine learning models. The results show that LLMs can mimic human response distributions, with open-source models like Llama and Mistral outperforming proprietary GPT models. Personality-based prompting, especially when optimized with genetic algorithms, significantly enhanced LLMs' ability to predict human response distributions, suggesting that capturing suboptimal, naturalistic reasoning may require modeling techniques incorporating diverse reasoning styles and psychological profiles. The study concludes that personality-based prompting combined with genetic algorithms is promising for enhancing AI's 'human-ness' in reasoning.

### Explainable Distributed Constraint Optimization Problems 
[[arxiv](https://arxiv.org/abs/2502.14102)] [[cool](https://papers.cool/arxiv/2502.14102)] [[pdf](https://arxiv.org/pdf/2502.14102)]
> **Authors**: Ben Rachmut,Stylianos Loukas Vasileiou,Nimrod Meir Weinstein,Roie Zivan,William Yeoh
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: The Distributed Constraint Optimization Problem (DCOP) formulation is a powerful tool to model cooperative multi-agent problems that need to be solved distributively. A core assumption of existing approaches is that DCOP solutions can be easily understood, accepted, and adopted, which may not hold, as evidenced by the large body of literature on Explainable AI. In this paper, we propose the Explainable DCOP (X-DCOP) model, which extends a DCOP to include its solution and a contrastive query for that solution. We formally define some key properties that contrastive explanations must satisfy for them to be considered as valid solutions to X-DCOPs as well as theoretical results on the existence of such valid explanations. To solve X-DCOPs, we propose a distributed framework as well as several optimizations and suboptimal variants to find valid explanations. We also include a human user study that showed that users, not surprisingly, prefer shorter explanations over longer ones. Our empirical evaluations showed that our approach can scale to large problems, and the different variants provide different options for trading off explanation lengths for smaller runtimes. Thus, our model and algorithmic contributions extend the state of the art by reducing the barrier for users to understand DCOP solutions, facilitating their adoption in more real-world applications.

### Investigating Non-Transitivity in LLM-as-a-Judge 
[[arxiv](https://arxiv.org/abs/2502.14074)] [[cool](https://papers.cool/arxiv/2502.14074)] [[pdf](https://arxiv.org/pdf/2502.14074)]
> **Authors**: Yi Xu,Laura Ruis,Tim Rocktäschel,Robert Kirk
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 8 pages, 6 figures, 2 tables (30 pages, 11 figures, 8 tables including references and appendices)
- **标题**: None
- **领域**: 人工智能,计算语言学,机器学习
- **Abstract**: Automatic evaluation methods based on large language models (LLMs) are emerging as the standard tool for assessing the instruction-following abilities of LLM-based agents. The most common method in this paradigm, pairwise comparisons with a baseline model, critically depends on the assumption of transitive preferences. However, the validity of this assumption remains largely unexplored. In this study, we investigate the presence of non-transitivity within the AlpacaEval framework and analyze its effects on model rankings. We find that LLM judges exhibit non-transitive preferences, leading to rankings that are sensitive to the choice of the baseline model. To mitigate this issue, we show that round-robin tournaments combined with Bradley-Terry models of preference can produce more reliable rankings. Notably, our method increases both the Spearman correlation and the Kendall correlation with Chatbot Arena (95.0% -> 96.4% and 82.1% -> 86.3% respectively). To address the computational cost of round-robin tournaments, we propose Swiss-Wise Iterative Matchmaking (Swim) tournaments, using a dynamic matching strategy to capture the benefits of round-robin tournaments while maintaining computational efficiency.

### Neurosymbolic artificial intelligence via large language models and coherence-driven inference 
[[arxiv](https://arxiv.org/abs/2502.13953)] [[cool](https://papers.cool/arxiv/2502.13953)] [[pdf](https://arxiv.org/pdf/2502.13953)]
> **Authors**: Steve Huntsman,Jewell Thomas
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: We devise an algorithm to generate sets of propositions that objectively instantiate graphs that support coherence-driven inference. We then benchmark the ability of large language models (LLMs) to reconstruct coherence graphs from (a straightforward transformation of) propositions expressed in natural language, with promising results from a single prompt to models optimized for reasoning. Combining coherence-driven inference with consistency evaluations by neural models may advance the state of the art in machine cognition.

### AdaptiveStep: Automatically Dividing Reasoning Step through Model Confidence 
[[arxiv](https://arxiv.org/abs/2502.13943)] [[cool](https://papers.cool/arxiv/2502.13943)] [[pdf](https://arxiv.org/pdf/2502.13943)]
> **Authors**: Yuliang Liu,Junjie Lu,Zhaoling Chen,Chaofeng Qu,Jason Klein Liu,Chonghan Liu,Zefan Cai,Yunhui Xia,Li Zhao,Jiang Bian,Chuheng Zhang,Wei Shen,Zhouhan Lin
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 17 pages
- **标题**: None
- **领域**: 人工智能,计算语言学,机器学习
- **Abstract**: Current approaches for training Process Reward Models (PRMs) often involve breaking down responses into multiple reasoning steps using rule-based techniques, such as using predefined placeholder tokens or setting the reasoning step's length into a fixed size. These approaches overlook the fact that specific words do not typically mark true decision points in a text. To address this, we propose AdaptiveStep, a method that divides reasoning steps based on the model's confidence in predicting the next word. This division method provides more decision-making information at each step, enhancing downstream tasks, such as reward model learning. Moreover, our method does not require manual annotation. We demonstrate its effectiveness through experiments with AdaptiveStep-trained PRMs in mathematical reasoning and code generation tasks. Experimental results indicate that the outcome PRM achieves state-of-the-art Best-of-N performance, surpassing greedy search strategy with token-level value-guided decoding, while also reducing construction costs by over 30% compared to existing open-source PRMs. In addition, we provide a thorough analysis and case study on the PRM's performance, transferability, and generalization capabilities.

### Proving Olympiad Inequalities by Synergizing LLMs and Symbolic Reasoning 
[[arxiv](https://arxiv.org/abs/2502.13834)] [[cool](https://papers.cool/arxiv/2502.13834)] [[pdf](https://arxiv.org/pdf/2502.13834)]
> **Authors**: Zenan Li,Zhaoyu Li,Wen Tang,Xian Zhang,Yuan Yao,Xujie Si,Fan Yang,Kaiyu Yang,Xiaoxing Ma
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: Published as a conference paper at ICLR 2025. Code is available at https://github.com/Lizn-zn/NeqLIPS/
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Large language models (LLMs) can prove mathematical theorems formally by generating proof steps (\textit{a.k.a.} tactics) within a proof system. However, the space of possible tactics is vast and complex, while the available training data for formal proofs is limited, posing a significant challenge to LLM-based tactic generation. To address this, we introduce a neuro-symbolic tactic generator that synergizes the mathematical intuition learned by LLMs with domain-specific insights encoded by symbolic methods. The key aspect of this integration is identifying which parts of mathematical reasoning are best suited to LLMs and which to symbolic methods. While the high-level idea of neuro-symbolic integration is broadly applicable to various mathematical problems, in this paper, we focus specifically on Olympiad inequalities (Figure~1). We analyze how humans solve these problems and distill the techniques into two types of tactics: (1) scaling, handled by symbolic methods, and (2) rewriting, handled by LLMs. In addition, we combine symbolic tools with LLMs to prune and rank the proof goals for efficient proof search. We evaluate our framework on 161 challenging inequalities from multiple mathematics competitions, achieving state-of-the-art performance and significantly outperforming existing LLM and symbolic approaches without requiring additional training data.

### Scoring Verifiers: Evaluating Synthetic Verification in Code and Reasoning 
[[arxiv](https://arxiv.org/abs/2502.13820)] [[cool](https://papers.cool/arxiv/2502.13820)] [[pdf](https://arxiv.org/pdf/2502.13820)]
> **Authors**: Aleksander Ficek,Somshubra Majumdar,Vahid Noroozi,Boris Ginsburg
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学,机器学习,软件工程
- **Abstract**: Code verification has recently found great success as a critical component in training large scale reasoning models for coding. Synthetic techniques such as self-generated test cases and reward models provide a way to enhance code capabilities beyond predefined tests. Building on these advancements, we propose new benchmarks designed to systematically evaluate the impact of synthetic verification methods on assessing solution correctness. We introduce HE-R, HE-R+, MBPP-R, and MBPP-R+, which transform existing coding benchmarks into scoring and ranking datasets to evaluate the effectiveness of synthetic verifiers. Using these benchmarks, we analyze synthetic verification methods in standard, reasoning-based, and reward-based LLMs. Our results show that recent reasoning models significantly improve test case generation and that scaling test cases enhances verification accuracy.

### A consensus set for the aggregation of partial rankings: the case of the Optimal Set of Bucket Orders Problem 
[[arxiv](https://arxiv.org/abs/2502.13769)] [[cool](https://papers.cool/arxiv/2502.13769)] [[pdf](https://arxiv.org/pdf/2502.13769)]
> **Authors**: Juan A. Aledo,José A. Gámez,Alejandro Rosete
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 26 pages, 2 figures
- **标题**: None
- **领域**: 人工智能
- **Abstract**: In rank aggregation problems (RAP), the solution is usually a consensus ranking that generalizes a set of input orderings. There are different variants that differ not only in terms of the type of rankings that are used as input and output, but also in terms of the objective function employed to evaluate the quality of the desired output ranking. In contrast, in some machine learning tasks (e.g. subgroup discovery) or multimodal optimization tasks, attention is devoted to obtaining several models/results to account for the diversity in the input data or across the search landscape. Thus, in this paper we propose to provide, as the solution to an RAP, a set of rankings to better explain the preferences expressed in the input orderings. We exemplify our proposal through the Optimal Bucket Order Problem (OBOP), an RAP which consists in finding a single consensus ranking (with ties) that generalizes a set of input rankings codified as a precedence matrix. To address this, we introduce the Optimal Set of Bucket Orders Problem (OSBOP), a generalization of the OBOP that aims to produce not a single ranking as output but a set of consensus rankings. Experimental results are presented to illustrate this proposal, showing how, by providing a set of consensus rankings, the fitness of the solution significantly improves with respect to the one of the original OBOP, without losing comprehensibility.

### Inference of Abstraction for Grounded Predicate Logic 
[[arxiv](https://arxiv.org/abs/2502.13743)] [[cool](https://papers.cool/arxiv/2502.13743)] [[pdf](https://arxiv.org/pdf/2502.13743)]
> **Authors**: Hiroyuki Kido
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: An important open question in AI is what simple and natural principle enables a machine to reason logically for meaningful abstraction with grounded symbols. This paper explores a conceptually new approach to combining probabilistic reasoning and predicative symbolic reasoning over data. We return to the era of reasoning with a full joint distribution before the advent of Bayesian networks. We then discuss that a full joint distribution over models of exponential size in propositional logic and of infinite size in predicate logic should be simply derived from a full joint distribution over data of linear size. We show that the same process is not only enough to generalise the logical consequence relation of predicate logic but also to provide a new perspective to rethink well-known limitations such as the undecidability of predicate logic, the symbol grounding problem and the principle of explosion. The reproducibility of this theoretical work is fully demonstrated by the included proofs.

### SPPD: Self-training with Process Preference Learning Using Dynamic Value Margin 
[[arxiv](https://arxiv.org/abs/2502.13516)] [[cool](https://papers.cool/arxiv/2502.13516)] [[pdf](https://arxiv.org/pdf/2502.13516)]
> **Authors**: Hao Yi,Qingyang Li,Yulan Hu,Fuzheng Zhang,Di Zhang,Yong Liu
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Recently, enhancing the numerical and logical reasoning capability of Large Language Models (LLMs) has emerged as a research hotspot. Existing methods face several limitations: inference-phase techniques (e.g., Chain of Thoughts) rely on prompt selection and the pretrained knowledge; sentence-level Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) struggle with step-wise mathematical correctness and depend on stronger models distillation or human annotations; while Reinforcement Learning (RL) approaches incur high GPU memory costs and unstable training. To address these, we propose \textbf{S}elf-training framework integrating \textbf{P}rocess \textbf{P}reference learning using \textbf{D}ynamic value margin (SPPD). SPPD leverages a process-based Markov Decision Process (MDP) and Bellman optimality equation to derive \textbf{dynamic value margin} on step-level preference optimization, which employs tree-based self-sampling on model responses \textbf{without any distillation} from other models. Furthermore, we theoretically prove that SPPD is \textbf{equivalent to on-policy policy gradient methods} under reward constraints. Experiments on 7B-scale models demonstrate superior performance across in-domain and out-domain mathematical benchmarks. We open-source our code at \href{https://anonymous.4open.science/r/SSDPO-D-DCDD}{https://anonymous.4open.science/r/SPPD-DCDD}.

### Integration of Agentic AI with 6G Networks for Mission-Critical Applications: Use-case and Challenges 
[[arxiv](https://arxiv.org/abs/2502.13476)] [[cool](https://papers.cool/arxiv/2502.13476)] [[pdf](https://arxiv.org/pdf/2502.13476)]
> **Authors**: Sunder Ali Khowaja,Kapal Dev,Muhammad Salman Pathan,Engin Zeydan,Merouane Debbah
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: FEMA [https://www.fema.gov/openfema-data-page/disaster-declarations-summaries-v2] National Oceanic and Atmospheric Administration [https://www.ncdc.noaa.gov/stormevents/details.jsp] packages Pytorch [https://pytorch.org/] RLib [https://docs.ray.io/en/latest/rllib/index.html] Neo4j [https://neo4j.com/] Apache Kafka [https://kafka.apache.org/]
- **标题**: None
- **领域**: 人工智能,网络和互联网架构
- **Abstract**: We are in a transformative era, and advances in Artificial Intelligence (AI), especially the foundational models, are constantly in the news. AI has been an integral part of many applications that rely on automation for service delivery, and one of them is mission-critical public safety applications. The problem with AI-oriented mission-critical applications is the humanin-the-loop system and the lack of adaptability to dynamic conditions while maintaining situational awareness. Agentic AI (AAI) has gained a lot of attention recently due to its ability to analyze textual data through a contextual lens while quickly adapting to conditions. In this context, this paper proposes an AAI framework for mission-critical applications. We propose a novel framework with a multi-layer architecture to realize the AAI. We also present a detailed implementation of AAI layer that bridges the gap between network infrastructure and missioncritical applications. Our preliminary analysis shows that the AAI reduces initial response time by 5.6 minutes on average, while alert generation time is reduced by 15.6 seconds on average and resource allocation is improved by up to 13.4%. We also show that the AAI methods improve the number of concurrent operations by 40, which reduces the recovery time by up to 5.2 minutes. Finally, we highlight some of the issues and challenges that need to be considered when implementing AAI frameworks.

### Vision-Based Generic Potential Function for Policy Alignment in Multi-Agent Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.13430)] [[cool](https://papers.cool/arxiv/2502.13430)] [[pdf](https://arxiv.org/pdf/2502.13430)]
> **Authors**: Hao Ma,Shijie Wang,Zhiqiang Pu,Siyao Zhao,Xiaolin Ai
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: Guiding the policy of multi-agent reinforcement learning to align with human common sense is a difficult problem, largely due to the complexity of modeling common sense as a reward, especially in complex and long-horizon multi-agent tasks. Recent works have shown the effectiveness of reward shaping, such as potential-based rewards, to enhance policy alignment. The existing works, however, primarily rely on experts to design rule-based rewards, which are often labor-intensive and lack a high-level semantic understanding of common sense. To solve this problem, we propose a hierarchical vision-based reward shaping method. At the bottom layer, a visual-language model (VLM) serves as a generic potential function, guiding the policy to align with human common sense through its intrinsic semantic understanding. To help the policy adapts to uncertainty and changes in long-horizon tasks, the top layer features an adaptive skill selection module based on a visual large language model (vLLM). The module uses instructions, video replays, and training records to dynamically select suitable potential function from a pre-designed pool. Besides, our method is theoretically proven to preserve the optimal policy. Extensive experiments conducted in the Google Research Football environment demonstrate that our method not only achieves a higher win rate but also effectively aligns the policy with human common sense.

## 硬件架构(cs.AR:Hardware Architecture)

### NVR: Vector Runahead on NPUs for Sparse Memory Access 
[[arxiv](https://arxiv.org/abs/2502.13873)] [[cool](https://papers.cool/arxiv/2502.13873)] [[pdf](https://arxiv.org/pdf/2502.13873)]
> **Authors**: Hui Wang,Zhengpeng Zhao,Jing Wang,Yushu Du,Yuan Cheng,Bing Guo,He Xiao,Chenhao Ma,Xiaomeng Han,Dean You,Jiapeng Guan,Ran Wei,Dawei Yang,Zhe Jiang
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 硬件架构,人工智能
- **Abstract**: Deep Neural Networks are increasingly leveraging sparsity to reduce the scaling up of model parameter size. However, reducing wall-clock time through sparsity and pruning remains challenging due to irregular memory access patterns, leading to frequent cache misses. In this paper, we present NPU Vector Runahead (NVR), a prefetching mechanism tailored for NPUs to address cache miss problems in sparse DNN workloads. Rather than optimising memory patterns with high overhead and poor portability, NVR adapts runahead execution to the unique architecture of NPUs. NVR provides a general micro-architectural solution for sparse DNN workloads without requiring compiler or algorithmic support, operating as a decoupled, speculative, lightweight hardware sub-thread alongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an average 90% reduction in cache misses compared to SOTA prefetching in general-purpose processors, delivering 4x average speedup on sparse workloads versus NPUs without prefetching. Moreover, we investigate the advantages of incorporating a small cache (16KB) into the NPU combined with NVR. Our evaluation shows that expanding this modest cache delivers 5x higher performance benefits than increasing the L2 cache size by the same amount.

## 计算语言学(cs.CL:Computation and Language)

### Does Time Have Its Place? Temporal Heads: Where Language Models Recall Time-specific Information 
[[arxiv](https://arxiv.org/abs/2502.14258)] [[cool](https://papers.cool/arxiv/2502.14258)] [[pdf](https://arxiv.org/pdf/2502.14258)]
> **Authors**: Yein Park,Chanwoong Yoon,Jungwoo Park,Minbyul Jeong,Jaewoo Kang
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: While the ability of language models to elicit facts has been widely investigated, how they handle temporally changing facts remains underexplored. We discover Temporal Heads, specific attention heads primarily responsible for processing temporal knowledge through circuit analysis. We confirm that these heads are present across multiple models, though their specific locations may vary, and their responses differ depending on the type of knowledge and its corresponding years. Disabling these heads degrades the model's ability to recall time-specific knowledge while maintaining its general capabilities without compromising time-invariant and question-answering performances. Moreover, the heads are activated not only numeric conditions ("In 2004") but also textual aliases ("In the year ..."), indicating that they encode a temporal dimension beyond simple numerical representation. Furthermore, we expand the potential of our findings by demonstrating how temporal knowledge can be edited by adjusting the values of these heads.

### Effects of Prompt Length on Domain-specific Tasks for Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.14255)] [[cool](https://papers.cool/arxiv/2502.14255)] [[pdf](https://arxiv.org/pdf/2502.14255)]
> **Authors**: Qibang Liu,Wenzhe Wang,Jeffrey Willard
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,新兴技术,机器学习
- **Abstract**: In recent years, Large Language Models have garnered significant attention for their strong performance in various natural language tasks, such as machine translation and question answering. These models demonstrate an impressive ability to generalize across diverse tasks. However, their effectiveness in tackling domain-specific tasks, such as financial sentiment analysis and monetary policy understanding, remains a topic of debate, as these tasks often require specialized knowledge and precise reasoning. To address such challenges, researchers design various prompts to unlock the models' abilities. By carefully crafting input prompts, researchers can guide these models to produce more accurate responses. Consequently, prompt engineering has become a key focus of study. Despite the advancements in both models and prompt engineering, the relationship between the two-specifically, how prompt design impacts models' ability to perform domain-specific tasks-remains underexplored. This paper aims to bridge this research gap.

### Mitigating Lost-in-Retrieval Problems in Retrieval Augmented Multi-Hop Question Answering 
[[arxiv](https://arxiv.org/abs/2502.14245)] [[cool](https://papers.cool/arxiv/2502.14245)] [[pdf](https://arxiv.org/pdf/2502.14245)]
> **Authors**: Rongzhi Zhu,Xiangyu Liu,Zequn Sun,Yiwei Wang,Wei Hu
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: In this paper, we identify a critical problem, "lost-in-retrieval", in retrieval-augmented multi-hop question answering (QA): the key entities are missed in LLMs' sub-question decomposition. "Lost-in-retrieval" significantly degrades the retrieval performance, which disrupts the reasoning chain and leads to the incorrect answers. To resolve this problem, we propose a progressive retrieval and rewriting method, namely ChainRAG, which sequentially handles each sub-question by completing missing key entities and retrieving relevant sentences from a sentence graph for answer generation. Each step in our retrieval and rewriting process builds upon the previous one, creating a seamless chain that leads to accurate retrieval and answers. Finally, all retrieved sentences and sub-question answers are integrated to generate a comprehensive answer to the original question. We evaluate ChainRAG on three multi-hop QA datasets$\unicode{x2013}$MuSiQue, 2Wiki, and HotpotQA$\unicode{x2013}$using three large language models: GPT4o-mini, Qwen2.5-72B, and GLM-4-Plus. Empirical results demonstrate that ChainRAG consistently outperforms baselines in both effectiveness and efficiency.

### Transfer-Prompting: Enhancing Cross-Task Adaptation in Large Language Models via Dual-Stage Prompts Optimization 
[[arxiv](https://arxiv.org/abs/2502.14211)] [[cool](https://papers.cool/arxiv/2502.14211)] [[pdf](https://arxiv.org/pdf/2502.14211)]
> **Authors**: Yupeng Chang,Yi Chang,Yuan Wu
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 17 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) face significant challenges when balancing multiple high-level objectives, such as generating coherent, relevant, and high-quality responses while maintaining efficient task adaptation across diverse tasks. To address these challenges, we introduce Transfer-Prompting, a novel two-stage framework designed to enhance cross-task adaptation in prompt generation. The framework comprises two key components: (1) source prompt construction, which refines the original prompts on source task datasets to generate source prompts with enhanced generalization ability, and (2) target prompt generation, which enhances cross-task adaptation of target prompts by fine-tuning a set of high-scored source prompts on task-specific datasets. In each optimization cycle, a reference LLM generates candidate prompts based on historical prompt-score pairs and task descriptions in our designed reference prompt. These candidate prompts are refined iteratively, while a scorer LLM evaluates their effectiveness using the multi-dimensional metrics designed in the objective prompts evaluator-a novel contribution in this work that provides a holistic evaluation of prompt quality and task performance. This feedback loop facilitates continuous refinement, optimizing both prompt quality and task-specific outcomes. We validate Transfer-Prompting through extensive experiments across 25 LLMs, including 7 foundational models and 18 specialized models, evaluated on 9 diverse datasets. The results demonstrate that Transfer-Prompting significantly improves task-specific performance, highlighting its potential for enhancing cross-task adaptation in LLMs. The code is available at https://github.com/llm172/Transfer-Prompting.

### On-the-fly Preference Alignment via Principle-Guided Decoding 
[[arxiv](https://arxiv.org/abs/2502.14204)] [[cool](https://papers.cool/arxiv/2502.14204)] [[pdf](https://arxiv.org/pdf/2502.14204)]
> **Authors**: Mingye Zhu,Yi Liu,Lei Zhang,Junbo Guo,Zhendong Mao
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: Accepted to ICLR 2025
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: With the rapidly expanding landscape of large language models, aligning model generations with human values and preferences is becoming increasingly important. Popular alignment methods, such as Reinforcement Learning from Human Feedback, have shown significant success in guiding models with greater control. However, these methods require considerable computational resources, which is inefficient, and substantial collection of training data to accommodate the diverse and pluralistic nature of human preferences, which is impractical. These limitations significantly constrain the scope and efficacy of both task-specific and general preference alignment methods. In this work, we introduce On-the-fly Preference Alignment via Principle-Guided Decoding (OPAD) to directly align model outputs with human preferences during inference, eliminating the need for fine-tuning. Our approach involves first curating a surrogate solution to an otherwise infeasible optimization problem and then designing a principle-guided reward function based on this surrogate. The final aligned policy is derived by maximizing this customized reward, which exploits the discrepancy between the constrained policy and its unconstrained counterpart. OPAD directly modifies the model's predictions during inference, ensuring principle adherence without incurring the computational overhead of retraining or fine-tuning. Experiments show that OPAD achieves competitive or superior performance in both general and personalized alignment tasks, demonstrating its efficiency and effectiveness compared to state-of-the-art baselines.

### NLP-AKG: Few-Shot Construction of NLP Academic Knowledge Graph Based on LLM 
[[arxiv](https://arxiv.org/abs/2502.14192)] [[cool](https://papers.cool/arxiv/2502.14192)] [[pdf](https://arxiv.org/pdf/2502.14192)]
> **Authors**: Jiayin Lan,Jiaqi Li,Baoxin Wang,Ming Liu,Dayong Wu,Shijin Wang,Bing Qin
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,数字图书馆
- **Abstract**: Large language models (LLMs) have been widely applied in question answering over scientific research papers. To enhance the professionalism and accuracy of responses, many studies employ external knowledge augmentation. However, existing structures of external knowledge in scientific literature often focus solely on either paper entities or domain concepts, neglecting the intrinsic connections between papers through shared domain concepts. This results in less comprehensive and specific answers when addressing questions that combine papers and concepts. To address this, we propose a novel knowledge graph framework that captures deep conceptual relations between academic papers, constructing a relational network via intra-paper semantic elements and inter-paper citation relations. Using a few-shot knowledge graph construction method based on LLM, we develop NLP-AKG, an academic knowledge graph for the NLP domain, by extracting 620,353 entities and 2,271,584 relations from 60,826 papers in ACL Anthology. Based on this, we propose a 'sub-graph community summary' method and validate its effectiveness on three NLP scientific literature question answering datasets.

### QUAD-LLM-MLTC: Large Language Models Ensemble Learning for Healthcare Text Multi-Label Classification 
[[arxiv](https://arxiv.org/abs/2502.14189)] [[cool](https://papers.cool/arxiv/2502.14189)] [[pdf](https://arxiv.org/pdf/2502.14189)]
> **Authors**: Hajar Sakai,Sarah S. Lam
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The escalating volume of collected healthcare textual data presents a unique challenge for automated Multi-Label Text Classification (MLTC), which is primarily due to the scarcity of annotated texts for training and their nuanced nature. Traditional machine learning models often fail to fully capture the array of expressed topics. However, Large Language Models (LLMs) have demonstrated remarkable effectiveness across numerous Natural Language Processing (NLP) tasks in various domains, which show impressive computational efficiency and suitability for unsupervised learning through prompt engineering. Consequently, these LLMs promise an effective MLTC of medical narratives. However, when dealing with various labels, different prompts can be relevant depending on the topic. To address these challenges, the proposed approach, QUAD-LLM-MLTC, leverages the strengths of four LLMs: GPT-4o, BERT, PEGASUS, and BART. QUAD-LLM-MLTC operates in a sequential pipeline in which BERT extracts key tokens, PEGASUS augments textual data, GPT-4o classifies, and BART provides topics' assignment probabilities, which results in four classifications, all in a 0-shot setting. The outputs are then combined using ensemble learning and processed through a meta-classifier to produce the final MLTC result. The approach is evaluated using three samples of annotated texts, which contrast it with traditional and single-model methods. The results show significant improvements across the majority of the topics in the classification's F1 score and consistency (F1 and Micro-F1 scores of 78.17% and 80.16% with standard deviations of 0.025 and 0.011, respectively). This research advances MLTC using LLMs and provides an efficient and scalable solution to rapidly categorize healthcare-related text data without further training.

### Enhancing Conversational Agents with Theory of Mind: Aligning Beliefs, Desires, and Intentions for Human-Like Interaction 
[[arxiv](https://arxiv.org/abs/2502.14171)] [[cool](https://papers.cool/arxiv/2502.14171)] [[pdf](https://arxiv.org/pdf/2502.14171)]
> **Authors**: Mehdi Jafari,Devin Yuncheng Hua,Hao Xue,Flora Salim
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Natural language interaction with agentic Artificial Intelligence (AI), driven by Large Language Models (LLMs), is expected to remain a dominant paradigm in the near future. While humans instinctively align their communication with mental states -- an ability known as Theory of Mind (ToM), current LLM powered systems exhibit significant limitations in this regard. This study examines the extent to which open source language models (LLaMA) can capture and preserve ToM related information and how effectively it contributes to consistent ToM reasoning in generated responses. We further investigate whether explicit manipulation of ToM related components, such as beliefs, desires, and intentions, can enhance response alignment. Experiments on two LLaMA 3 variants demonstrate that incorporating ToM informed alignment improves response quality, achieving win rates of 67 and 63 percent for the 3B and 8B models, respectively. These findings highlight the potential of ToM driven strategies to improve alignment in LLM based conversational agents.

### LLM-Enhanced Dialogue Management for Full-Duplex Spoken Dialogue Systems 
[[arxiv](https://arxiv.org/abs/2502.14145)] [[cool](https://papers.cool/arxiv/2502.14145)] [[pdf](https://arxiv.org/pdf/2502.14145)]
> **Authors**: Hao Zhang,Weiwei Li,Rilin Chen,Vinay Kothapally,Meng Yu,Dong Yu
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: In submission to INTERSPEECH 2025
- **标题**: None
- **领域**: 计算语言学,音频和语音处理
- **Abstract**: Achieving full-duplex communication in spoken dialogue systems (SDS) requires real-time coordination between listening, speaking, and thinking. This paper proposes a semantic voice activity detection (VAD) module as a dialogue manager (DM) to efficiently manage turn-taking in full-duplex SDS. Implemented as a lightweight (0.5B) LLM fine-tuned on full-duplex conversation data, the semantic VAD predicts four control tokens to regulate turn-switching and turn-keeping, distinguishing between intentional and unintentional barge-ins while detecting query completion for handling user pauses and hesitations. By processing input speech in short intervals, the semantic VAD enables real-time decision-making, while the core dialogue engine (CDE) is only activated for response generation, reducing computational overhead. This design allows independent DM optimization without retraining the CDE, balancing interaction accuracy and inference efficiency for scalable, next-generation full-duplex SDS.

### UM_FHS at TREC 2024 PLABA: Exploration of Fine-tuning and AI agent approach for plain language adaptations of biomedical text 
[[arxiv](https://arxiv.org/abs/2502.14144)] [[cool](https://papers.cool/arxiv/2502.14144)] [[pdf](https://arxiv.org/pdf/2502.14144)]
> **Authors**: Primoz Kocbek,Leon Kopitar,Zhihong Zhang,Emirhan Aydin,Maxim Topaz,Gregor Stiglic
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 10 pages, 2 figures, to be published in the 33rd Text REtrieval Conference (TREC 2024) proceedings
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: This paper describes our submissions to the TREC 2024 PLABA track with the aim to simplify biomedical abstracts for a K8-level audience (13-14 years old students). We tested three approaches using OpenAI's gpt-4o and gpt-4o-mini models: baseline prompt engineering, a two-AI agent approach, and fine-tuning. Adaptations were evaluated using qualitative metrics (5-point Likert scales for simplicity, accuracy, completeness, and brevity) and quantitative readability scores (Flesch-Kincaid grade level, SMOG Index). Results indicated that the two-agent approach and baseline prompt engineering with gpt-4o-mini models show superior qualitative performance, while fine-tuned models excelled in accuracy and completeness but were less simple. The evaluation results demonstrated that prompt engineering with gpt-4o-mini outperforms iterative improvement strategies via two-agent approach as well as fine-tuning with gpt-4o. We intend to expand our investigation of the results and explore advanced evaluations.

### Self-Regularization with Latent Space Explanations for Controllable LLM-based Classification 
[[arxiv](https://arxiv.org/abs/2502.14133)] [[cool](https://papers.cool/arxiv/2502.14133)] [[pdf](https://arxiv.org/pdf/2502.14133)]
> **Authors**: Xuansheng Wu,Wenhao Yu,Xiaoming Zhai,Ninghao Liu
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: Pre-print, 15 pages, 4 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Modern text classification methods heavily rely on contextual embeddings from large language models (LLMs). Compared to human-engineered features, these embeddings provide automatic and effective representations for classification model training. However, they also introduce a challenge: we lose the ability to manually remove unintended features, such as sensitive or task-irrelevant features, to guarantee regulatory compliance or improve the generalizability of classification models. This limitation arises because LLM embeddings are opaque and difficult to interpret. In this paper, we propose a novel framework to identify and regularize unintended features in the LLM latent space. Specifically, we first pre-train a sparse autoencoder (SAE) to extract interpretable features from LLM latent spaces. To ensure the SAE can capture task-specific features, we further fine-tune it on task-specific datasets. In training the classification model, we propose a simple and effective regularizer, by minimizing the similarity between the classifier weights and the identified unintended feature, to remove the impacts of these unintended features toward classification. We evaluate the proposed framework on three real-world tasks, including toxic chat detection, reward modeling, and disease diagnosis. Results show that the proposed framework can significantly improve the classifier's generalizability by regularizing those features that are not semantically correlated to each task. This work pioneers controllable text classification on LLM latent spaces by leveraging interpreted features to address generalizability, fairness, and privacy challenges. We will release our code and data once accepted.

### Can Community Notes Replace Professional Fact-Checkers? 
[[arxiv](https://arxiv.org/abs/2502.14132)] [[cool](https://papers.cool/arxiv/2502.14132)] [[pdf](https://arxiv.org/pdf/2502.14132)]
> **Authors**: Nadav Borenstein,Greta Warren,Desmond Elliott,Isabelle Augenstein
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Two commonly-employed strategies to combat the rise of misinformation on social media are (i) fact-checking by professional organisations and (ii) community moderation by platform users. Policy changes by Twitter/X and, more recently, Meta, signal a shift away from partnerships with fact-checking organisations and towards an increased reliance on crowdsourced community notes. However, the extent and nature of dependencies between fact-checking and helpful community notes remain unclear. To address these questions, we use language models to annotate a large corpus of Twitter/X community notes with attributes such as topic, cited sources, and whether they refute claims tied to broader misinformation narratives. Our analysis reveals that community notes cite fact-checking sources up to five times more than previously reported. Fact-checking is especially crucial for notes on posts linked to broader narratives, which are twice as likely to reference fact-checking sources compared to other sources. In conclusion, our results show that successful community moderation heavily relies on professional fact-checking.

### Which of These Best Describes Multiple Choice Evaluation with LLMs? A) Forced B) Flawed C) Fixable D) All of the Above 
[[arxiv](https://arxiv.org/abs/2502.14127)] [[cool](https://papers.cool/arxiv/2502.14127)] [[pdf](https://arxiv.org/pdf/2502.14127)]
> **Authors**: Nishant Balepur,Rachel Rudinger,Jordan Lee Boyd-Graber
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: In-progress preprint
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Multiple choice question answering (MCQA) is popular for LLM evaluation due to its simplicity and human-like testing, but we argue for its reform. We first reveal flaws in MCQA's format, as it struggles to: 1) test generation/subjectivity; 2) match LLM use cases; and 3) fully test knowledge. We instead advocate for generative formats based on human testing-where LLMs construct and explain answers-better capturing user needs and knowledge while remaining easy to score. We then show even when MCQA is a useful format, its datasets suffer from: leakage; unanswerability; shortcuts; and saturation. In each issue, we give fixes from education, like rubrics to guide MCQ writing; scoring methods to bridle guessing; and Item Response Theory to build harder MCQs. Lastly, we discuss LLM errors in MCQA-robustness, biases, and unfaithful explanations-showing how our prior solutions better measure or address these issues. While we do not need to desert MCQA, we encourage more efforts in refining the task based on educational testing, advancing evaluations.

### Benchmarking LLMs for Political Science: A United Nations Perspective 
[[arxiv](https://arxiv.org/abs/2502.14122)] [[cool](https://papers.cool/arxiv/2502.14122)] [[pdf](https://arxiv.org/pdf/2502.14122)]
> **Authors**: Yueqing Liang,Liangwei Yang,Chen Wang,Congying Xia,Rui Meng,Xiongxiao Xu,Haoran Wang,Ali Payani,Kai Shu
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,计算机与社会,新兴技术
- **Abstract**: Large Language Models (LLMs) have achieved significant advances in natural language processing, yet their potential for high-stake political decision-making remains largely unexplored. This paper addresses the gap by focusing on the application of LLMs to the United Nations (UN) decision-making process, where the stakes are particularly high and political decisions can have far-reaching consequences. We introduce a novel dataset comprising publicly available UN Security Council (UNSC) records from 1994 to 2024, including draft resolutions, voting records, and diplomatic speeches. Using this dataset, we propose the United Nations Benchmark (UNBench), the first comprehensive benchmark designed to evaluate LLMs across four interconnected political science tasks: co-penholder judgment, representative voting simulation, draft adoption prediction, and representative statement generation. These tasks span the three stages of the UN decision-making process--drafting, voting, and discussing--and aim to assess LLMs' ability to understand and simulate political dynamics. Our experimental analysis demonstrates the potential and challenges of applying LLMs in this domain, providing insights into their strengths and limitations in political science. This work contributes to the growing intersection of AI and political science, opening new avenues for research and practical applications in global governance. The UNBench Repository can be accessed at: https://github.com/yueqingliang1/UNBench.

### Meaning Beyond Truth Conditions: Evaluating Discourse Level Understanding via Anaphora Accessibility 
[[arxiv](https://arxiv.org/abs/2502.14119)] [[cool](https://papers.cool/arxiv/2502.14119)] [[pdf](https://arxiv.org/pdf/2502.14119)]
> **Authors**: Xiaomeng Zhu,Zhenghao Zhou,Simon Charlow,Robert Frank
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: We present a hierarchy of natural language understanding abilities and argue for the importance of moving beyond assessments of understanding at the lexical and sentence levels to the discourse level. We propose the task of anaphora accessibility as a diagnostic for assessing discourse understanding, and to this end, present an evaluation dataset inspired by theoretical research in dynamic semantics. We evaluate human and LLM performance on our dataset and find that LLMs and humans align on some tasks and diverge on others. Such divergence can be explained by LLMs' reliance on specific lexical items during language comprehension, in contrast to human sensitivity to structural abstractions.

### Towards Context-Robust LLMs: A Gated Representation Fine-tuning Approach 
[[arxiv](https://arxiv.org/abs/2502.14100)] [[cool](https://papers.cool/arxiv/2502.14100)] [[pdf](https://arxiv.org/pdf/2502.14100)]
> **Authors**: Shenglai Zeng,Pengfei He,Kai Guo,Tianqi Zheng,Hanqing Lu,Yue Xing,Hui Liu
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,信息检索
- **Abstract**: Large Language Models (LLMs) enhanced with external contexts, such as through retrieval-augmented generation (RAG), often face challenges in handling imperfect evidence. They tend to over-rely on external knowledge, making them vulnerable to misleading and unhelpful contexts. To address this, we propose the concept of context-robust LLMs, which can effectively balance internal knowledge with external context, similar to human cognitive processes. Specifically, context-robust LLMs should rely on external context only when lacking internal knowledge, identify contradictions between internal and external knowledge, and disregard unhelpful contexts. To achieve this goal, we introduce Grft, a lightweight and plug-and-play gated representation fine-tuning approach. Grft consists of two key components: a gating mechanism to detect and filter problematic inputs, and low-rank representation adapters to adjust hidden representations. By training a lightweight intervention function with only 0.0004\% of model size on fewer than 200 examples, Grft can effectively adapt LLMs towards context-robust behaviors.

### Retrieving Versus Understanding Extractive Evidence in Few-Shot Learning 
[[arxiv](https://arxiv.org/abs/2502.14095)] [[cool](https://papers.cool/arxiv/2502.14095)] [[pdf](https://arxiv.org/pdf/2502.14095)]
> **Authors**: Karl Elbakian,Samuel Carton
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 9 pages, 8 figures, Accepted to AAAI 2025 Main Conference (AIAlignment Track)
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: A key aspect of alignment is the proper use of within-document evidence to construct document-level decisions. We analyze the relationship between the retrieval and interpretation of within-document evidence for large language model in a few-shot setting. Specifically, we measure the extent to which model prediction errors are associated with evidence retrieval errors with respect to gold-standard human-annotated extractive evidence for five datasets, using two popular closed proprietary models. We perform two ablation studies to investigate when both label prediction and evidence retrieval errors can be attributed to qualities of the relevant evidence. We find that there is a strong empirical relationship between model prediction and evidence retrieval error, but that evidence retrieval error is mostly not associated with evidence interpretation error--a hopeful sign for downstream applications built on this mechanism.

### Navigating Semantic Relations: Challenges for Language Models in Abstract Common-Sense Reasoning 
[[arxiv](https://arxiv.org/abs/2502.14086)] [[cool](https://papers.cool/arxiv/2502.14086)] [[pdf](https://arxiv.org/pdf/2502.14086)]
> **Authors**: Cole Gawin,Yidan Sun,Mayank Kejriwal
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 5 pages, 3 figures, ACM Web Conference 2025
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large language models (LLMs) have achieved remarkable performance in generating human-like text and solving reasoning tasks of moderate complexity, such as question-answering and mathematical problem-solving. However, their capabilities in tasks requiring deeper cognitive skills, such as common-sense understanding and abstract reasoning, remain under-explored. In this paper, we systematically evaluate abstract common-sense reasoning in LLMs using the ConceptNet knowledge graph. We propose two prompting approaches: instruct prompting, where models predict plausible semantic relationships based on provided definitions, and few-shot prompting, where models identify relations using examples as guidance. Our experiments with the gpt-4o-mini model show that in instruct prompting, consistent performance is obtained when ranking multiple relations but with substantial decline when the model is restricted to predicting only one relation. In few-shot prompting, the model's accuracy improves significantly when selecting from five relations rather than the full set, although with notable bias toward certain relations. These results suggest significant gaps still, even in commercially used LLMs' abstract common-sense reasoning abilities, compared to human-level understanding. However, the findings also highlight the promise of careful prompt engineering, based on selective retrieval, for obtaining better performance.

### Are Rules Meant to be Broken? Understanding Multilingual Moral Reasoning as a Computational Pipeline with UniMoral 
[[arxiv](https://arxiv.org/abs/2502.14083)] [[cool](https://papers.cool/arxiv/2502.14083)] [[pdf](https://arxiv.org/pdf/2502.14083)]
> **Authors**: Shivani Kumar,David Jurgens
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 21 pages, 10 figures, 8 tables
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Moral reasoning is a complex cognitive process shaped by individual experiences and cultural contexts and presents unique challenges for computational analysis. While natural language processing (NLP) offers promising tools for studying this phenomenon, current research lacks cohesion, employing discordant datasets and tasks that examine isolated aspects of moral reasoning. We bridge this gap with UniMoral, a unified dataset integrating psychologically grounded and social-media-derived moral dilemmas annotated with labels for action choices, ethical principles, contributing factors, and consequences, alongside annotators' moral and cultural profiles. Recognizing the cultural relativity of moral reasoning, UniMoral spans six languages, Arabic, Chinese, English, Hindi, Russian, and Spanish, capturing diverse socio-cultural contexts. We demonstrate UniMoral's utility through a benchmark evaluations of three large language models (LLMs) across four tasks: action prediction, moral typology classification, factor attribution analysis, and consequence generation. Key findings reveal that while implicitly embedded moral contexts enhance the moral reasoning capability of LLMs, there remains a critical need for increasingly specialized approaches to further advance moral reasoning in these models.

### RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache Compression 
[[arxiv](https://arxiv.org/abs/2502.14051)] [[cool](https://papers.cool/arxiv/2502.14051)] [[pdf](https://arxiv.org/pdf/2502.14051)]
> **Authors**: Payman Behnam,Yaosheng Fu,Ritchie Zhao,Po-An Tsai,Zhiding Yu,Alexey Tumanov
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Transformer-based Large Language Models rely critically on KV cache to efficiently handle extended contexts during the decode phase. Yet, the size of the KV cache grows proportionally with the input length, burdening both memory bandwidth and capacity as decoding progresses. To address this challenge, we present RocketKV, a training-free KV cache compression strategy designed specifically to reduce both memory bandwidth and capacity demand of KV cache during the decode phase. RocketKV contains two consecutive stages. In the first stage, it performs coarse-grain KV cache eviction on the input sequence tokens with SnapKV++, a method improved upon SnapKV by introducing adaptive pooling size and full compatibility with grouped-query attention. In the second stage, it adopts a hybrid attention method to conduct fine-grain top-k sparse attention, approximating the attention scores by leveraging both head and sequence dimensional reductions. Combining these two stages, RocketKV achieves significant KV cache fetching bandwidth and storage savings while maintaining comparable accuracy to full KV cache attention. We show that RocketKV provides end-to-end speedup by up to 3$\times$ as well as peak memory reduction by up to 31% in the decode phase on an NVIDIA H100 GPU compared to the full KV cache baseline, while achieving negligible accuracy loss on a variety of long-context tasks.

### Diversity-driven Data Selection for Language Model Tuning through Sparse Autoencoder 
[[arxiv](https://arxiv.org/abs/2502.14050)] [[cool](https://papers.cool/arxiv/2502.14050)] [[pdf](https://arxiv.org/pdf/2502.14050)]
> **Authors**: Xianjun Yang,Shaoliang Nie,Lijuan Liu,Suchin Gururangan,Ujjwal Karn,Rui Hou,Madian Khabsa,Yuning Mao
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Current pre-trained large language models typically need instruction tuning to align with human preferences. However, instruction tuning data is often quantity-saturated due to the large volume of data collection and fast model iteration, leaving coreset data selection important but underexplored. On the other hand, existing quality-driven data selection methods such as LIMA (NeurIPS 2023 (Zhou et al., 2024)) and AlpaGasus (ICLR 2024 (Chen et al.)) generally ignore the equal importance of data diversity and complexity. In this work, we aim to design a diversity-aware data selection strategy and creatively propose using sparse autoencoders to tackle the challenge of data diversity measure. In addition, sparse autoencoders can also provide more interpretability of model behavior and explain, e.g., the surprising effectiveness of selecting the longest response (ICML 2024 (Zhao et al.)). Using effective data selection, we experimentally prove that models trained on our selected data can outperform other methods in terms of model capabilities, reduce training cost, and potentially gain more control over model behaviors.

### Semantic Decomposition and Selective Context Filtering -- Text Processing Techniques for Context-Aware NLP-Based Systems 
[[arxiv](https://arxiv.org/abs/2502.14048)] [[cool](https://papers.cool/arxiv/2502.14048)] [[pdf](https://arxiv.org/pdf/2502.14048)]
> **Authors**: Karl John Villardar
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: :I.2.7; I.7.0
- **标题**: None
- **领域**: 计算语言学,人工智能,人机交互
- **Abstract**: In this paper, we present two techniques for use in context-aware systems: Semantic Decomposition, which sequentially decomposes input prompts into a structured and hierarchal information schema in which systems can parse and process easily, and Selective Context Filtering, which enables systems to systematically filter out specific irrelevant sections of contextual information that is fed through a system's NLP-based pipeline. We will explore how context-aware systems and applications can utilize these two techniques in order to implement dynamic LLM-to-system interfaces, improve an LLM's ability to generate more contextually cohesive user-facing responses, and optimize complex automated workflows and pipelines.

### DiffSampling: Enhancing Diversity and Accuracy in Neural Text Generation 
[[arxiv](https://arxiv.org/abs/2502.14037)] [[cool](https://papers.cool/arxiv/2502.14037)] [[pdf](https://arxiv.org/pdf/2502.14037)]
> **Authors**: Giorgio Franceschelli,Mirco Musolesi
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Despite their increasing performance, large language models still tend to reproduce training data, generate several repetitions, and focus on the most common grammatical structures and words. A possible cause is the decoding strategy adopted: the most common ones either consider only the most probable tokens, reducing output diversity, or increase the likelihood of unlikely tokens at the cost of output accuracy and correctness. In this paper, we propose a family of three new decoding methods by leveraging a mathematical analysis of the token probability distribution. In particular, the difference between consecutive, sorted probabilities can be used to avoid incorrect tokens and increase the chance of low-probable but accurate words. Experiments concerning math problem solving, extreme summarization, and the divergent association task show that our approach consistently performs at least as well as current alternatives in terms of quality and diversity.

### MaskPrune: Mask-based LLM Pruning for Layer-wise Uniform Structures 
[[arxiv](https://arxiv.org/abs/2502.14008)] [[cool](https://papers.cool/arxiv/2502.14008)] [[pdf](https://arxiv.org/pdf/2502.14008)]
> **Authors**: Jiayu Qin,Jianchao Tan,Kefeng Zhang,Xunliang Cai,Wei Wang
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: The remarkable performance of large language models (LLMs) in various language tasks has attracted considerable attention. However, the ever-increasing size of these models presents growing challenges for deployment and inference. Structured pruning, an effective model compression technique, is gaining increasing attention due to its ability to enhance inference efficiency. Nevertheless, most previous optimization-based structured pruning methods sacrifice the uniform structure across layers for greater flexibility to maintain performance. The heterogeneous structure hinders the effective utilization of off-the-shelf inference acceleration techniques and impedes efficient configuration for continued training. To address this issue, we propose a novel masking learning paradigm based on minimax optimization to obtain the uniform pruned structure by optimizing the masks under sparsity regularization. Extensive experimental results demonstrate that our method can maintain high performance while ensuring the uniformity of the pruned model structure, thereby outperforming existing SOTA methods.

### MuDAF: Long-Context Multi-Document Attention Focusing through Contrastive Learning on Attention Heads 
[[arxiv](https://arxiv.org/abs/2502.13963)] [[cool](https://papers.cool/arxiv/2502.13963)] [[pdf](https://arxiv.org/pdf/2502.13963)]
> **Authors**: Weihao Liu,Ning Wu,Shiping Yang,Wenbiao Ding,Shining Liang,Ming Gong,Dongmei Zhang
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 18 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models (LLMs) frequently show distracted attention due to irrelevant information in the input, which severely impairs their long-context capabilities. Inspired by recent studies on the effectiveness of retrieval heads in long-context factutality, we aim at addressing this distraction issue through improving such retrieval heads directly. We propose Multi-Document Attention Focusing (MuDAF), a novel method that explicitly optimizes the attention distribution at the head level through contrastive learning. According to the experimental results, MuDAF can significantly improve the long-context question answering performance of LLMs, especially in multi-document question answering. Extensive evaluations on retrieval scores and attention visualizations show that MuDAF possesses great potential in making attention heads more focused on relevant information and reducing attention distractions.

### Is That Your Final Answer? Test-Time Scaling Improves Selective Question Answering 
[[arxiv](https://arxiv.org/abs/2502.13962)] [[cool](https://papers.cool/arxiv/2502.13962)] [[pdf](https://arxiv.org/pdf/2502.13962)]
> **Authors**: William Jurayj,Jeffrey Cheng,Benjamin Van Durme
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Scaling the test-time compute of large language models has demonstrated impressive performance on reasoning benchmarks. However, existing evaluations of test-time scaling make the strong assumption that a reasoning system should always give an answer to any question provided. This overlooks concerns about whether a model is confident in its answer, and whether it is appropriate to always provide a response. To address these concerns, we extract confidence scores during reasoning for thresholding model responses. We find that increasing compute budget at inference time not only helps models answer more questions correctly, but also increases confidence in correct responses. We then extend the current paradigm of zero-risk responses during evaluation by considering settings with non-zero levels of response risk, and suggest a recipe for reporting evaluations under these settings.

### LIDDIA: Language-based Intelligent Drug Discovery Agent 
[[arxiv](https://arxiv.org/abs/2502.13959)] [[cool](https://papers.cool/arxiv/2502.13959)] [[pdf](https://arxiv.org/pdf/2502.13959)]
> **Authors**: Reza Averly,Frazier N. Baker,Xia Ning
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: Preprint
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Drug discovery is a long, expensive, and complex process, relying heavily on human medicinal chemists, who can spend years searching the vast space of potential therapies. Recent advances in artificial intelligence for chemistry have sought to expedite individual drug discovery tasks; however, there remains a critical need for an intelligent agent that can navigate the drug discovery process. Towards this end, we introduce LIDDiA, an autonomous agent capable of intelligently navigating the drug discovery process in silico. By leveraging the reasoning capabilities of large language models, LIDDiA serves as a low-cost and highly-adaptable tool for autonomous drug discovery. We comprehensively examine LIDDiA, demonstrating that (1) it can generate molecules meeting key pharmaceutical criteria on over 70% of 30 clinically relevant targets, (2) it intelligently balances exploration and exploitation in the chemical space, and (3) it can identify promising novel drug candidates on EGFR, a critical target for cancers.

### RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision 
[[arxiv](https://arxiv.org/abs/2502.13957)] [[cool](https://papers.cool/arxiv/2502.13957)] [[pdf](https://arxiv.org/pdf/2502.13957)]
> **Authors**: Guangzhi Xiong,Qiao Jin,Xiao Wang,Yin Fang,Haolin Liu,Yifan Yang,Fangyuan Chen,Zhixing Song,Dengyu Wang,Minjia Zhang,Zhiyong Lu,Aidong Zhang
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Retrieval-augmented generation (RAG) has shown great potential for knowledge-intensive tasks, but its traditional architectures rely on static retrieval, limiting their effectiveness for complex questions that require sequential information-seeking. While agentic reasoning and search offer a more adaptive approach, most existing methods depend heavily on prompt engineering. In this work, we introduce RAG-Gym, a unified optimization framework that enhances information-seeking agents through fine-grained process supervision at each search step. We also propose ReSearch, a novel agent architecture that synergizes answer reasoning and search query generation within the RAG-Gym framework. Experiments on four challenging datasets show that RAG-Gym improves performance by up to 25.6\% across various agent architectures, with ReSearch consistently outperforming existing baselines. Further analysis highlights the effectiveness of advanced LLMs as process reward judges and the transferability of trained reward models as verifiers for different LLMs. Additionally, we examine the scaling properties of training and inference in agentic RAG. The project homepage is available at https://rag-gym.github.io/.

### Latent Distribution Decoupling: A Probabilistic Framework for Uncertainty-Aware Multimodal Emotion Recognition 
[[arxiv](https://arxiv.org/abs/2502.13954)] [[cool](https://papers.cool/arxiv/2502.13954)] [[pdf](https://arxiv.org/pdf/2502.13954)]
> **Authors**: Jingwang Huang,Jiang Zhong,Qin Lei,Jinpeng Gao,Yuming Yang,Sirui Wang,Peiguang Li,Kaiwen Wei
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Multimodal multi-label emotion recognition (MMER) aims to identify the concurrent presence of multiple emotions in multimodal data. Existing studies primarily focus on improving fusion strategies and modeling modality-to-label dependencies. However, they often overlook the impact of \textbf{aleatoric uncertainty}, which is the inherent noise in the multimodal data and hinders the effectiveness of modality fusion by introducing ambiguity into feature representations. To address this issue and effectively model aleatoric uncertainty, this paper proposes Latent emotional Distribution Decomposition with Uncertainty perception (LDDU) framework from a novel perspective of latent emotional space probabilistic modeling. Specifically, we introduce a contrastive disentangled distribution mechanism within the emotion space to model the multimodal data, allowing for the extraction of semantic features and uncertainty. Furthermore, we design an uncertainty-aware fusion multimodal method that accounts for the dispersed distribution of uncertainty and integrates distribution information. Experimental results show that LDDU achieves state-of-the-art performance on the CMU-MOSEI and M$^3$ED datasets, highlighting the importance of uncertainty modeling in MMER. Code is available at https://github.com/201983290498/lddu\_mmer.git.

### Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety Mechanisms Tend to Be Anchored in The Template Region 
[[arxiv](https://arxiv.org/abs/2502.13946)] [[cool](https://papers.cool/arxiv/2502.13946)] [[pdf](https://arxiv.org/pdf/2502.13946)]
> **Authors**: Chak Tou Leong,Qingyu Yin,Jian Wang,Wenjie Li
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,密码学和安全
- **Abstract**: The safety alignment of large language models (LLMs) remains vulnerable, as their initial behavior can be easily jailbroken by even relatively simple attacks. Since infilling a fixed template between the input instruction and initial model output is a common practice for existing LLMs, we hypothesize that this template is a key factor behind their vulnerabilities: LLMs' safety-related decision-making overly relies on the aggregated information from the template region, which largely influences these models' safety behavior. We refer to this issue as template-anchored safety alignment. In this paper, we conduct extensive experiments and verify that template-anchored safety alignment is widespread across various aligned LLMs. Our mechanistic analyses demonstrate how it leads to models' susceptibility when encountering inference-time jailbreak attacks. Furthermore, we show that detaching safety mechanisms from the template region is promising in mitigating vulnerabilities to jailbreak attacks. We encourage future research to develop more robust safety alignment techniques that reduce reliance on the template region.

### Beyond Single Frames: Can LMMs Comprehend Temporal and Contextual Narratives in Image Sequences? 
[[arxiv](https://arxiv.org/abs/2502.13925)] [[cool](https://papers.cool/arxiv/2502.13925)] [[pdf](https://arxiv.org/pdf/2502.13925)]
> **Authors**: Xiaochen Wang,Heming Xia,Jialin Song,Longyu Guan,Yixin Yang,Qingxiu Dong,Weiyao Luo,Yifan Pu,Yiru Wang,Xiangdi Meng,Wenjie Li,Zhifang Sui
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Multimodal Models (LMMs) have achieved remarkable success across various visual-language tasks. However, existing benchmarks predominantly focus on single-image understanding, leaving the analysis of image sequences largely unexplored. To address this limitation, we introduce StripCipher, a comprehensive benchmark designed to evaluate capabilities of LMMs to comprehend and reason over sequential images. StripCipher comprises a human-annotated dataset and three challenging subtasks: visual narrative comprehension, contextual frame prediction, and temporal narrative reordering. Our evaluation of $16$ state-of-the-art LMMs, including GPT-4o and Qwen2.5VL, reveals a significant performance gap compared to human capabilities, particularly in tasks that require reordering shuffled sequential images. For instance, GPT-4o achieves only 23.93% accuracy in the reordering subtask, which is 56.07% lower than human performance. Further quantitative analysis discuss several factors, such as input format of images, affecting the performance of LLMs in sequential understanding, underscoring the fundamental challenges that remain in the development of LMMs.

### LongPO: Long Context Self-Evolution of Large Language Models through Short-to-Long Preference Optimization 
[[arxiv](https://arxiv.org/abs/2502.13922)] [[cool](https://papers.cool/arxiv/2502.13922)] [[pdf](https://arxiv.org/pdf/2502.13922)]
> **Authors**: Guanzheng Chen,Xin Li,Michael Qizhe Shieh,Lidong Bing
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: ICLR 2025
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Large Language Models (LLMs) have demonstrated remarkable capabilities through pretraining and alignment. However, superior short-context LLMs may underperform in long-context scenarios due to insufficient long-context alignment. This alignment process remains challenging due to the impracticality of human annotation for extended contexts and the difficulty in balancing short- and long-context performance. To address these challenges, we introduce LongPO, that enables short-context LLMs to self-evolve to excel on long-context tasks by internally transferring short-context capabilities. LongPO harnesses LLMs to learn from self-generated short-to-long preference data, comprising paired responses generated for identical instructions with long-context inputs and their compressed short-context counterparts, respectively. This preference reveals capabilities and potentials of LLMs cultivated during short-context alignment that may be diminished in under-aligned long-context scenarios. Additionally, LongPO incorporates a short-to-long KL constraint to mitigate short-context performance decline during long-context alignment. When applied to Mistral-7B-Instruct-v0.2 from 128K to 512K context lengths, LongPO fully retains short-context performance and largely outperforms naive SFT and DPO in both long- and short-context tasks. Specifically, LongPO-trained models can achieve results on long-context benchmarks comparable to, or even surpassing, those of superior LLMs (e.g., GPT-4-128K) that involve extensive long-context annotation and larger parameter scales. Our code is available at https://github.com/DAMO-NLP-SG/LongPO.

### TESS 2: A Large-Scale Generalist Diffusion Language Model 
[[arxiv](https://arxiv.org/abs/2502.13917)] [[cool](https://papers.cool/arxiv/2502.13917)] [[pdf](https://arxiv.org/pdf/2502.13917)]
> **Authors**: Jaesung Tae,Hamish Ivison,Sachin Kumar,Arman Cohan
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: preprint
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: We introduce TESS 2, a general instruction-following diffusion language model that outperforms contemporary instruction-tuned diffusion models, as well as matches and sometimes exceeds strong autoregressive (AR) models. We train TESS 2 by first adapting a strong AR model via continued pretraining with the usual cross-entropy as diffusion loss, and then performing further instruction tuning. We find that adaptation training as well as the choice of the base model is crucial for training good instruction-following diffusion models. We further propose reward guidance, a novel and modular inference-time guidance procedure to align model outputs without needing to train the underlying model. Finally, we show that TESS 2 further improves with increased inference-time compute, highlighting the utility of diffusion LMs in having fine-grained controllability over the amount of compute used at inference time. Code and models are available at https://github.com/hamishivi/tess-2.

### How Do LLMs Perform Two-Hop Reasoning in Context? 
[[arxiv](https://arxiv.org/abs/2502.13913)] [[cool](https://papers.cool/arxiv/2502.13913)] [[pdf](https://arxiv.org/pdf/2502.13913)]
> **Authors**: Tianyu Guo,Hanlin Zhu,Ruiqi Zhang,Jiantao Jiao,Song Mei,Michael I. Jordan,Stuart Russell
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: "Socrates is human. All humans are mortal. Therefore, Socrates is mortal." This classical example demonstrates two-hop reasoning, where a conclusion logically follows from two connected premises. While transformer-based Large Language Models (LLMs) can make two-hop reasoning, they tend to collapse to random guessing when faced with distracting premises. To understand the underlying mechanism, we train a three-layer transformer on synthetic two-hop reasoning tasks. The training dynamics show two stages: a slow learning phase, where the 3-layer transformer performs random guessing like LLMs, followed by an abrupt phase transitions, where the 3-layer transformer suddenly reaches $100%$ accuracy. Through reverse engineering, we explain the inner mechanisms for how models learn to randomly guess between distractions initially, and how they learn to ignore distractions eventually. We further propose a three-parameter model that supports the causal claims for the mechanisms to the training dynamics of the transformer. Finally, experiments on LLMs suggest that the discovered mechanisms generalize across scales. Our methodologies provide new perspectives for scientific understandings of LLMs and our findings provide new insights into how reasoning emerges during training.

### DataSciBench: An LLM Agent Benchmark for Data Science 
[[arxiv](https://arxiv.org/abs/2502.13897)] [[cool](https://papers.cool/arxiv/2502.13897)] [[pdf](https://arxiv.org/pdf/2502.13897)]
> **Authors**: Dan Zhang,Sining Zhoubian,Min Cai,Fengzu Li,Lekang Yang,Wei Wang,Tianjiao Dong,Ziniu Hu,Jie Tang,Yisong Yue
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 40 pages, 7 figures, 6 tables
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: This paper presents DataSciBench, a comprehensive benchmark for evaluating Large Language Model (LLM) capabilities in data science. Recent related benchmarks have primarily focused on single tasks, easily obtainable ground truth, and straightforward evaluation metrics, which limits the scope of tasks that can be evaluated. In contrast, DataSciBench is constructed based on a more comprehensive and curated collection of natural and challenging prompts for uncertain ground truth and evaluation metrics. We develop a semi-automated pipeline for generating ground truth (GT) and validating evaluation metrics. This pipeline utilizes and implements an LLM-based self-consistency and human verification strategy to produce accurate GT by leveraging collected prompts, predefined task types, and aggregate functions (metrics). Furthermore, we propose an innovative Task - Function - Code (TFC) framework to assess each code execution outcome based on precisely defined metrics and programmatic rules. Our experimental framework involves testing 6 API-based models, 8 open-source general models, and 9 open-source code generation models using the diverse set of prompts we have gathered. This approach aims to provide a more comprehensive and rigorous evaluation of LLMs in data science, revealing their strengths and weaknesses. Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models. We release all code and data at https://github.com/THUDM/DataSciBench.

### PSCon: Toward Conversational Product Search 
[[arxiv](https://arxiv.org/abs/2502.13881)] [[cool](https://papers.cool/arxiv/2502.13881)] [[pdf](https://arxiv.org/pdf/2502.13881)]
> **Authors**: Jie Zou,Mohammad Aliannejadi,Evangelos Kanoulas,Shuxi Han,Heli Ma,Zheng Wang,Yang Yang,Heng Tao Shen
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 11 pages
- **标题**: None
- **领域**: 计算语言学,人工智能,信息检索
- **Abstract**: Conversational Product Search (CPS) is confined to simulated conversations due to the lack of real-world CPS datasets that reflect human-like language. Additionally, current conversational datasets are limited to support cross-market and multi-lingual usage. In this paper, we introduce a new CPS data collection protocol and present PSCon, a novel CPS dataset designed to assist product search via human-like conversations. The dataset is constructed using a coached human-to-human data collection protocol and supports two languages and dual markets. Also, the dataset enables thorough exploration of six subtasks of CPS: user intent detection, keyword extraction, system action prediction, question selection, item ranking, and response generation. Furthermore, we also offer an analysis of the dataset and propose a benchmark model on the proposed CPS dataset.

### Fine-grained Fallacy Detection with Human Label Variation 
[[arxiv](https://arxiv.org/abs/2502.13853)] [[cool](https://papers.cool/arxiv/2502.13853)] [[pdf](https://arxiv.org/pdf/2502.13853)]
> **Authors**: Alan Ramponi,Agnese Daffara,Sara Tonelli
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: NAACL 2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: We introduce Faina, the first dataset for fallacy detection that embraces multiple plausible answers and natural disagreement. Faina includes over 11K span-level annotations with overlaps across 20 fallacy types on social media posts in Italian about migration, climate change, and public health given by two expert annotators. Through an extensive annotation study that allowed discussion over multiple rounds, we minimize annotation errors whilst keeping signals of human label variation. Moreover, we devise a framework that goes beyond "single ground truth" evaluation and simultaneously accounts for multiple (equally reliable) test sets and the peculiarities of the task, i.e., partial span matches, overlaps, and the varying severity of labeling errors. Our experiments across four fallacy detection setups show that multi-task and multi-label transformer-based approaches are strong baselines across all settings. We release our data, code, and annotation guidelines to foster research on fallacy detection and human label variation more broadly.

### DH-RAG: A Dynamic Historical Context-Powered Retrieval-Augmented Generation Method for Multi-Turn Dialogue 
[[arxiv](https://arxiv.org/abs/2502.13847)] [[cool](https://papers.cool/arxiv/2502.13847)] [[pdf](https://arxiv.org/pdf/2502.13847)]
> **Authors**: Feiyuan Zhang,Dezhi Zhu,James Ming,Yilun Jin,Di Chai,Liu Yang,Han Tian,Zhaoxin Fan,Kai Chen
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Retrieval-Augmented Generation (RAG) systems have shown substantial benefits in applications such as question answering and multi-turn dialogue \citep{lewis2020retrieval}. However, traditional RAG methods, while leveraging static knowledge bases, often overlook the potential of dynamic historical information in ongoing conversations. To bridge this gap, we introduce DH-RAG, a Dynamic Historical Context-Powered Retrieval-Augmented Generation Method for Multi-Turn Dialogue. DH-RAG is inspired by human cognitive processes that utilize both long-term memory and immediate historical context in conversational responses \citep{stafford1987conversational}. DH-RAG is structured around two principal components: a History-Learning based Query Reconstruction Module, designed to generate effective queries by synthesizing current and prior interactions, and a Dynamic History Information Updating Module, which continually refreshes historical context throughout the dialogue. The center of DH-RAG is a Dynamic Historical Information database, which is further refined by three strategies within the Query Reconstruction Module: Historical Query Clustering, Hierarchical Matching, and Chain of Thought Tracking. Experimental evaluations show that DH-RAG significantly surpasses conventional models on several benchmarks, enhancing response relevance, coherence, and dialogue quality.

### Inner Thinking Transformer: Leveraging Dynamic Depth Scaling to Foster Adaptive Internal Thinking 
[[arxiv](https://arxiv.org/abs/2502.13842)] [[cool](https://papers.cool/arxiv/2502.13842)] [[pdf](https://arxiv.org/pdf/2502.13842)]
> **Authors**: Yilong Chen,Junyuan Shang,Zhenyu Zhang,Yanxi Xie,Jiawei Sheng,Tingwen Liu,Shuohuan Wang,Yu Sun,Hua Wu,Haifeng Wang
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 15 pages, 11 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) face inherent performance bottlenecks under parameter constraints, particularly in processing critical tokens that demand complex reasoning. Empirical analysis reveals challenging tokens induce abrupt gradient spikes across layers, exposing architectural stress points in standard Transformers. Building on this insight, we propose Inner Thinking Transformer (ITT), which reimagines layer computations as implicit thinking steps. ITT dynamically allocates computation through Adaptive Token Routing, iteratively refines representations via Residual Thinking Connections, and distinguishes reasoning phases using Thinking Step Encoding. ITT enables deeper processing of critical tokens without parameter expansion. Evaluations across 162M-466M parameter models show ITT achieves 96.5\% performance of a 466M Transformer using only 162M parameters, reduces training data by 43.2\%, and outperforms Transformer/Loop variants in 11 benchmarks. By enabling elastic computation allocation during inference, ITT balances performance and efficiency through architecture-aware optimization of implicit thinking pathways.

### From Tools to Teammates: Evaluating LLMs in Multi-Session Coding Interactions 
[[arxiv](https://arxiv.org/abs/2502.13791)] [[cool](https://papers.cool/arxiv/2502.13791)] [[pdf](https://arxiv.org/pdf/2502.13791)]
> **Authors**: Nathanaël Carraz Rakotonirina,Mohammed Hamdy,Jon Ander Campos,Lucas Weber,Alberto Testoni,Marzieh Fadaee,Sandro Pezzelle,Marco Del Tredici
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models (LLMs) are increasingly used in working environments for a wide range of tasks, excelling at solving individual problems in isolation. However, are they also able to effectively collaborate over long-term interactions? To investigate this, we introduce MemoryCode, a synthetic multi-session dataset designed to test LLMs' ability to track and execute simple coding instructions amid irrelevant information, simulating a realistic setting. While all the models we tested handle isolated instructions well, even the performance of state-of-the-art models like GPT-4o deteriorates when instructions are spread across sessions. Our analysis suggests this is due to their failure to retrieve and integrate information over long instruction chains. Our results highlight a fundamental limitation of current LLMs, restricting their ability to collaborate effectively in long interactions.

### Translation in the Hands of Many:Centering Lay Users in Machine Translation Interactions 
[[arxiv](https://arxiv.org/abs/2502.13780)] [[cool](https://papers.cool/arxiv/2502.13780)] [[pdf](https://arxiv.org/pdf/2502.13780)]
> **Authors**: Beatrice Savoldi,Alan Ramponi,Matteo Negri,Luisa Bentivogli
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,计算机与社会
- **Abstract**: Converging societal and technical factors have transformed language technologies into user-facing applications employed across languages. Machine Translation (MT) has become a global tool, with cross-lingual services now also supported by dialogue systems powered by multilingual Large Language Models (LLMs). This accessibility has expanded MT's reach to a vast base of lay users, often with little to no expertise in the languages or the technology itself. Despite this, the understanding of MT consumed by this diverse group of users -- their needs, experiences, and interactions with these systems -- remains limited. This paper traces the shift in MT user profiles, focusing on non-expert users and how their engagement with these systems may change with LLMs. We identify three key factors -- usability, trust, and literacy -- that shape these interactions and must be addressed to align MT with user needs. By exploring these dimensions, we offer insights to guide future MT with a user-centered approach.

### EHOP: A Dataset of Everyday NP-Hard Optimization Problems 
[[arxiv](https://arxiv.org/abs/2502.13776)] [[cool](https://papers.cool/arxiv/2502.13776)] [[pdf](https://arxiv.org/pdf/2502.13776)]
> **Authors**: Alex Duchnowski,Ellie Pavlick,Alexander Koller
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 18 pages, 3 figures
- **标题**: None
- **领域**: 计算语言学,计算复杂度
- **Abstract**: We introduce the dataset of Everyday Hard Optimization Problems (EHOP), a collection of NP-hard optimization problems expressed in natural language. EHOP includes problem formulations that could be found in computer science textbooks, versions that are dressed up as problems that could arise in real life, and variants of well-known problems with inverted rules. We find that state-of-the-art LLMs, across multiple prompting strategies, systematically solve textbook problems more accurately than their real-life and inverted counterparts. We argue that this constitutes evidence that LLMs adapt solutions seen during training, rather than leveraging reasoning abilities that would enable them to generalize to novel problems.

### VITAL: A New Dataset for Benchmarking Pluralistic Alignment in Healthcare 
[[arxiv](https://arxiv.org/abs/2502.13775)] [[cool](https://papers.cool/arxiv/2502.13775)] [[pdf](https://arxiv.org/pdf/2502.13775)]
> **Authors**: Anudeex Shetty,Amin Beheshti,Mark Dras,Usman Naseem
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: Under review
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Alignment techniques have become central to ensuring that Large Language Models (LLMs) generate outputs consistent with human values. However, existing alignment paradigms often model an averaged or monolithic preference, failing to account for the diversity of perspectives across cultures, demographics, and communities. This limitation is particularly critical in health-related scenarios, where plurality is essential due to the influence of culture, religion, personal values, and conflicting opinions. Despite progress in pluralistic alignment, no prior work has focused on health, likely due to the unavailability of publicly available datasets. To address this gap, we introduce VITAL, a new benchmark dataset comprising 13.1K value-laden situations and 5.4K multiple-choice questions focused on health, designed to assess and benchmark pluralistic alignment methodologies. Through extensive evaluation of eight LLMs of varying sizes, we demonstrate that existing pluralistic alignment techniques fall short in effectively accommodating diverse healthcare beliefs, underscoring the need for tailored AI alignment in specific domains. This work highlights the limitations of current approaches and lays the groundwork for developing health-specific alignment solutions.

### GIMMICK -- Globally Inclusive Multimodal Multitask Cultural Knowledge Benchmarking 
[[arxiv](https://arxiv.org/abs/2502.13766)] [[cool](https://papers.cool/arxiv/2502.13766)] [[pdf](https://arxiv.org/pdf/2502.13766)]
> **Authors**: Florian Schneider,Carolin Holtermann,Chris Biemann,Anne Lauscher
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Vision-Language Models (LVLMs) have recently gained attention due to their distinctive performance and broad applicability. While it has been previously shown that their efficacy in usage scenarios involving non-Western contexts falls short, existing studies are limited in scope, covering just a narrow range of cultures, focusing exclusively on a small number of cultural aspects, or evaluating a limited selection of models on a single task only. Towards globally inclusive LVLM research, we introduce GIMMICK, an extensive multimodal benchmark designed to assess a broad spectrum of cultural knowledge across 144 countries representing six global macro-regions. GIMMICK comprises six tasks built upon three new datasets that span 728 unique cultural events or facets on which we evaluated 20 LVLMs and 11 LLMs, including five proprietary and 26 open-weight models of all sizes. We systematically examine (1) regional cultural biases, (2) the influence of model size, (3) input modalities, and (4) external cues. Our analyses reveal strong biases toward Western cultures across models and tasks and highlight strong correlations between model size and performance, as well as the effectiveness of multimodal input and external geographic cues. We further find that models have more knowledge of tangible than intangible aspects (e.g., food vs. rituals) and that they excel in recognizing broad cultural origins but struggle with a more nuanced understanding.

### SCALAR: Scientific Citation-based Live Assessment of Long-context Academic Reasoning 
[[arxiv](https://arxiv.org/abs/2502.13753)] [[cool](https://papers.cool/arxiv/2502.13753)] [[pdf](https://arxiv.org/pdf/2502.13753)]
> **Authors**: Renxi Wang,Honglin Mu,Liqun Ma,Lizhi Lin,Yunlong Feng,Timothy Baldwin,Xudong Han,Haonan Li
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Evaluating large language models' (LLMs) long-context understanding capabilities remains challenging. We present SCALAR (Scientific Citation-based Live Assessment of Long-context Academic Reasoning), a novel benchmark that leverages academic papers and their citation networks. SCALAR features automatic generation of high-quality ground truth labels without human annotation, controllable difficulty levels, and a dynamic updating mechanism that prevents data contamination. Using ICLR 2025 papers, we evaluate 8 state-of-the-art LLMs, revealing key insights about their capabilities and limitations in processing long scientific documents across different context lengths and reasoning types. Our benchmark provides a reliable and sustainable way to track progress in long-context understanding as LLM capabilities evolve.

### Enhancing Input-Label Mapping in In-Context Learning with Contrastive Decoding 
[[arxiv](https://arxiv.org/abs/2502.13738)] [[cool](https://papers.cool/arxiv/2502.13738)] [[pdf](https://arxiv.org/pdf/2502.13738)]
> **Authors**: Keqin Peng,Liang Ding,Yuanxin Ouyang,Meng Fang,Yancheng Yuan,Dacheng Tao
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) excel at a range of tasks through in-context learning (ICL), where only a few task examples guide their predictions. However, prior research highlights that LLMs often overlook input-label mapping information in ICL, relying more on their pre-trained knowledge. To address this issue, we introduce In-Context Contrastive Decoding (ICCD), a novel method that emphasizes input-label mapping by contrasting the output distributions between positive and negative in-context examples. Experiments on 7 natural language understanding (NLU) tasks show that our ICCD method brings consistent and significant improvement (up to +2.1 improvement on average) upon 6 different scales of LLMs without requiring additional training. Our approach is versatile, enhancing performance with various demonstration selection methods, demonstrating its broad applicability and effectiveness. The code and scripts will be publicly released.

### Adapting Large Language Models for Time Series Modeling via a Novel Parameter-efficient Adaptation Method 
[[arxiv](https://arxiv.org/abs/2502.13725)] [[cool](https://papers.cool/arxiv/2502.13725)] [[pdf](https://arxiv.org/pdf/2502.13725)]
> **Authors**: Juyuan Zhang,Wei Zhu,Jiechao Gao
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Time series modeling holds significant importance in many real-world applications and has been extensively studied. While pre-trained foundation models have made impressive strides in the fields of natural language processing (NLP) and computer vision (CV), their development in time series domains has been constrained by data sparsity. A series of recent studies have demonstrated that large language models (LLMs) possess robust pattern recognition and reasoning abilities over complex sequences of tokens. However, the current literature have yet striked a high-quality balance between (a) effectively aligning the time series and natural language modalities, and (b) keeping the inference efficiency. To address the above issues, we now propose the Time-LlaMA framework. Time-LlaMA first converts the time series input into token embeddings through a linear tokenization mechanism. Second, the time series token embeddings are aligned with the text prompts. Third, to further adapt the LLM backbone for time series modeling, we have developed a dynamic low-rank adaptation technique (D-LoRA). D-LoRA dynamically chooses the most suitable LoRA modules at each layer of the Transformer backbone for each time series input, enhancing the model's predictive capabilities. Our experimental results on an extensive collection of challenging real-world time series tasks confirm that our proposed method achieves the state-of-the-art (SOTA) performance.

### Direct Value Optimization: Improving Chain-of-Thought Reasoning in LLMs with Refined Values 
[[arxiv](https://arxiv.org/abs/2502.13723)] [[cool](https://papers.cool/arxiv/2502.13723)] [[pdf](https://arxiv.org/pdf/2502.13723)]
> **Authors**: Hongbo Zhang,Han Cui,Guangsheng Bao,Linyi Yang,Jun Wang,Yue Zhang
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: preprint
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: We introduce Direct Value Optimization (DVO), an innovative reinforcement learning framework for enhancing large language models in complex reasoning tasks. Unlike traditional methods relying on preference labels, DVO utilizes value signals at individual reasoning steps, optimizing models via a mean squared error loss. The key benefit of DVO lies in its fine-grained supervision, circumventing the need for labor-intensive human annotations. Target values within the DVO are estimated using either Monte Carlo Tree Search or an outcome value model. Our empirical analysis on both mathematical and commonsense reasoning tasks shows that DVO consistently outperforms existing offline preference optimization techniques, even with fewer training steps. These findings underscore the importance of value signals in advancing reasoning capabilities and highlight DVO as a superior methodology under scenarios lacking explicit human preference information.

### Multi-Scale and Multi-Objective Optimization for Cross-Lingual Aspect-Based Sentiment Analysis 
[[arxiv](https://arxiv.org/abs/2502.13718)] [[cool](https://papers.cool/arxiv/2502.13718)] [[pdf](https://arxiv.org/pdf/2502.13718)]
> **Authors**: Chengyan Wu,Bolei Ma,Ningyuan Deng,Yanqing He,Yun Xue
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Aspect-based sentiment analysis (ABSA) is a sequence labeling task that has garnered growing research interest in multilingual contexts. However, recent studies lack more robust feature alignment and finer aspect-level alignment. In this paper, we propose a novel framework, Multi-Scale and Multi-Objective optimization (MSMO) for cross-lingual ABSA. During multi-scale alignment, we achieve cross-lingual sentence-level and aspect-level alignment, aligning features of aspect terms in different contextual environments. Specifically, we introduce code-switched bilingual sentences into the language discriminator and consistency training modules to enhance the model's robustness. During multi-objective optimization, we design two optimization objectives: supervised training and consistency training, aiming to enhance cross-lingual semantic alignment. To further improve model performance, we incorporate distilled knowledge of the target language into the model. Results show that MSMO significantly enhances cross-lingual ABSA by achieving state-of-the-art performance across multiple languages and models.

### Is This Collection Worth My LLM's Time? Automatically Measuring Information Potential in Text Corpora 
[[arxiv](https://arxiv.org/abs/2502.13691)] [[cool](https://papers.cool/arxiv/2502.13691)] [[pdf](https://arxiv.org/pdf/2502.13691)]
> **Authors**: Tristan Karch,Luca Engel,Philippe Schwaller,Frédéric Kaplan
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: As large language models (LLMs) converge towards similar capabilities, the key to advancing their performance lies in identifying and incorporating valuable new information sources. However, evaluating which text collections are worth the substantial investment required for digitization, preprocessing, and integration into LLM systems remains a significant challenge. We present a novel approach to this challenge: an automated pipeline that evaluates the potential information gain from text collections without requiring model training or fine-tuning. Our method generates multiple choice questions (MCQs) from texts and measures an LLM's performance both with and without access to the source material. The performance gap between these conditions serves as a proxy for the collection's information potential. We validate our approach using three strategically selected datasets: EPFL PhD manuscripts (likely containing novel specialized knowledge), Wikipedia articles (presumably part of training data), and a synthetic baseline dataset. Our results demonstrate that this method effectively identifies collections containing valuable novel information, providing a practical tool for prioritizing data acquisition and integration efforts.

### MoM: Linear Sequence Modeling with Mixture-of-Memories 
[[arxiv](https://arxiv.org/abs/2502.13685)] [[cool](https://papers.cool/arxiv/2502.13685)] [[pdf](https://arxiv.org/pdf/2502.13685)]
> **Authors**: Jusen Du,Weigao Sun,Disen Lan,Jiaxi Hu,Yu Cheng
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: Technical report, 14 pages
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Linear sequence modeling methods, such as linear attention, state space modeling, and linear RNNs, offer significant efficiency improvements by reducing the complexity of training and inference. However, these methods typically compress the entire input sequence into a single fixed-size memory state, which leads to suboptimal performance on recall-intensive downstream tasks. Drawing inspiration from neuroscience, particularly the brain's ability to maintain robust long-term memory while mitigating "memory interference", we introduce a novel architecture called Mixture-of-Memories (MoM). MoM utilizes multiple independent memory states, with a router network directing input tokens to specific memory states. This approach greatly enhances the overall memory capacity while minimizing memory interference. As a result, MoM performs exceptionally well on recall-intensive tasks, surpassing existing linear sequence modeling techniques. Despite incorporating multiple memory states, the computation of each memory state remains linear in complexity, allowing MoM to retain the linear-complexity advantage during training, while constant-complexity during inference. Our experimental results show that MoM significantly outperforms current linear sequence models on downstream language tasks, particularly recall-intensive tasks, and even achieves performance comparable to Transformer models. The code is released at https://github.com/OpenSparseLLMs/MoM and is also released as a part of https://github.com/OpenSparseLLMs/Linear-MoE.

### SCOPE: A Self-supervised Framework for Improving Faithfulness in Conditional Text Generation 
[[arxiv](https://arxiv.org/abs/2502.13674)] [[cool](https://papers.cool/arxiv/2502.13674)] [[pdf](https://arxiv.org/pdf/2502.13674)]
> **Authors**: Song Duong,Florian Le Bronnec,Alexandre Allauzen,Vincent Guigue,Alberto Lumbreras,Laure Soulier,Patrick Gallinari
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 10 pages, ICLR 2025 conference
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models (LLMs), when used for conditional text generation, often produce hallucinations, i.e., information that is unfaithful or not grounded in the input context. This issue arises in typical conditional text generation tasks, such as text summarization and data-to-text generation, where the goal is to produce fluent text based on contextual input. When fine-tuned on specific domains, LLMs struggle to provide faithful answers to a given context, often adding information or generating errors. One underlying cause of this issue is that LLMs rely on statistical patterns learned from their training data. This reliance can interfere with the model's ability to stay faithful to a provided context, leading to the generation of ungrounded information. We build upon this observation and introduce a novel self-supervised method for generating a training set of unfaithful samples. We then refine the model using a training process that encourages the generation of grounded outputs over unfaithful ones, drawing on preference-based training. Our approach leads to significantly more grounded text generation, outperforming existing self-supervised techniques in faithfulness, as evaluated through automatic metrics, LLM-based assessments, and human evaluations.

### Refining Sentence Embedding Model through Ranking Sentences Generation with Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.13656)] [[cool](https://papers.cool/arxiv/2502.13656)] [[pdf](https://arxiv.org/pdf/2502.13656)]
> **Authors**: Liyang He,Chenglong Liu,Rui Li,Zhenya Huang,Shulan Ruan,Jun Zhou,Enhong Chen
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Sentence embedding is essential for many NLP tasks, with contrastive learning methods achieving strong performance using annotated datasets like NLI. Yet, the reliance on manual labels limits scalability. Recent studies leverage large language models (LLMs) to generate sentence pairs, reducing annotation dependency. However, they overlook ranking information crucial for fine-grained semantic distinctions. To tackle this challenge, we propose a method for controlling the generation direction of LLMs in the latent space. Unlike unconstrained generation, the controlled approach ensures meaningful semantic divergence. Then, we refine exist sentence embedding model by integrating ranking information and semantic information. Experiments on multiple benchmarks demonstrate that our method achieves new SOTA performance with a modest cost in ranking sentence synthesis.

### C2T: A Classifier-Based Tree Construction Method in Speculative Decoding 
[[arxiv](https://arxiv.org/abs/2502.13652)] [[cool](https://papers.cool/arxiv/2502.13652)] [[pdf](https://arxiv.org/pdf/2502.13652)]
> **Authors**: Feiye Huo,Jianchao Tan,Kefeng Zhang,Xunliang Cai,Shengli Sun
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: The growing scale of Large Language Models (LLMs) has exacerbated inference latency and computational costs. Speculative decoding methods, which aim to mitigate these issues, often face inefficiencies in the construction of token trees and the verification of candidate tokens. Existing strategies, including chain mode, static tree, and dynamic tree approaches, have limitations in accurately preparing candidate token trees for verification. We propose a novel method named C2T that adopts a lightweight classifier to generate and prune token trees dynamically. Our classifier considers additional feature variables beyond the commonly used joint probability to predict the confidence score for each draft token to determine whether it is the candidate token for verification. This method outperforms state-of-the-art (SOTA) methods such as EAGLE-2 on multiple benchmarks, by reducing the total number of candidate tokens by 25% while maintaining or even improving the acceptance length.

### Reliability Across Parametric and External Knowledge: Understanding Knowledge Handling in LLMs 
[[arxiv](https://arxiv.org/abs/2502.13648)] [[cool](https://papers.cool/arxiv/2502.13648)] [[pdf](https://arxiv.org/pdf/2502.13648)]
> **Authors**: Youna Kim,Minjoon Choi,Sungmin Cho,Hyuhng Joon Kim,Sang-goo Lee,Taeuk Kim
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: under-review
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models (LLMs) enhance their problem-solving capability by leveraging both parametric and external knowledge. Beyond leveraging external knowledge to improve response accuracy, they require key capabilities for reliable knowledge-handling: resolving conflicts between knowledge sources, avoiding distraction from uninformative external knowledge, and abstaining when sufficient knowledge is unavailable. Prior studies have examined these scenarios in isolation or with limited scope. To systematically evaluate these capabilities, we introduce a comprehensive framework for analyzing knowledge-handling based on two key dimensions: the presence of parametric knowledge and the informativeness of external knowledge. Through analysis, we identify biases in knowledge utilization and examine how the ability to handle one scenario impacts performance in others. Furthermore, we demonstrate that training on data constructed based on the knowledge-handling scenarios improves LLMs' reliability in integrating and utilizing knowledge.

### Instruction Tuning on Public Government and Cultural Data for Low-Resource Language: a Case Study in Kazakh 
[[arxiv](https://arxiv.org/abs/2502.13647)] [[cool](https://papers.cool/arxiv/2502.13647)] [[pdf](https://arxiv.org/pdf/2502.13647)]
> **Authors**: Nurkhan Laiyk,Daniil Orel,Rituraj Joshi,Maiya Goloburda,Yuxia Wang,Preslav Nakov,Fajri Koto
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Instruction tuning in low-resource languages remains underexplored due to limited text data, particularly in government and cultural domains. To address this, we introduce and open-source a large-scale (10,600 samples) instruction-following (IFT) dataset, covering key institutional and cultural knowledge relevant to Kazakhstan. Our dataset enhances LLMs' understanding of procedural, legal, and structural governance topics. We employ LLM-assisted data generation, comparing open-weight and closed-weight models for dataset construction, and select GPT-4o as the backbone. Each entity of our dataset undergoes full manual verification to ensure high quality. We also show that fine-tuning Qwen, Falcon, and Gemma on our dataset leads to consistent performance improvements in both multiple-choice and generative tasks, demonstrating the potential of LLM-assisted instruction tuning for low-resource languages.

### D.Va: Validate Your Demonstration First Before You Use It 
[[arxiv](https://arxiv.org/abs/2502.13646)] [[cool](https://papers.cool/arxiv/2502.13646)] [[pdf](https://arxiv.org/pdf/2502.13646)]
> **Authors**: Qi Zhang,Zhiqing Xiao,Ruixuan Xiao,Lirong Gao,Junbo Zhao
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 14 pages, 6 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: In-context learning (ICL) has demonstrated significant potential in enhancing the capabilities of large language models (LLMs) during inference. It's well-established that ICL heavily relies on selecting effective demonstrations to generate outputs that better align with the expected results. As for demonstration selection, previous approaches have typically relied on intuitive metrics to evaluate the effectiveness of demonstrations, which often results in limited robustness and poor cross-model generalization capabilities. To tackle these challenges, we propose a novel method, \textbf{D}emonstration \textbf{VA}lidation (\textbf{D.Va}), which integrates a demonstration validation perspective into this field. By introducing the demonstration validation mechanism, our method effectively identifies demonstrations that are both effective and highly generalizable. \textbf{D.Va} surpasses all existing demonstration selection techniques across both natural language understanding (NLU) and natural language generation (NLG) tasks. Additionally, we demonstrate the robustness and generalizability of our approach across various language models with different retrieval models.

### Measuring the Effect of Transcription Noise on Downstream Language Understanding Tasks 
[[arxiv](https://arxiv.org/abs/2502.13645)] [[cool](https://papers.cool/arxiv/2502.13645)] [[pdf](https://arxiv.org/pdf/2502.13645)]
> **Authors**: Ori Shapira,Shlomo E. Chazan,Amir DN Cohen
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: With the increasing prevalence of recorded human speech, spoken language understanding (SLU) is essential for its efficient processing. In order to process the speech, it is commonly transcribed using automatic speech recognition technology. This speech-to-text transition introduces errors into the transcripts, which subsequently propagate to downstream NLP tasks, such as dialogue summarization. While it is known that transcript noise affects downstream tasks, a systematic approach to analyzing its effects across different noise severities and types has not been addressed. We propose a configurable framework for assessing task models in diverse noisy settings, and for examining the impact of transcript-cleaning techniques. The framework facilitates the investigation of task model behavior, which can in turn support the development of effective SLU solutions. We exemplify the utility of our framework on three SLU tasks and four task models, offering insights regarding the effect of transcript noise on tasks in general and models in particular. For instance, we find that task models can tolerate a certain level of noise, and are affected differently by the types of errors in the transcript.

### Qorgau: Evaluating LLM Safety in Kazakh-Russian Bilingual Contexts 
[[arxiv](https://arxiv.org/abs/2502.13640)] [[cool](https://papers.cool/arxiv/2502.13640)] [[pdf](https://arxiv.org/pdf/2502.13640)]
> **Authors**: Maiya Goloburda,Nurkhan Laiyk,Diana Turmakhan,Yuxia Wang,Mukhammed Togmanov,Jonibek Mansurov,Askhat Sametov,Nurdaulet Mukhituly,Minghan Wang,Daniil Orel,Zain Muhammad Mujahid,Fajri Koto,Timothy Baldwin,Preslav Nakov
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) are known to have the potential to generate harmful content, posing risks to users. While significant progress has been made in developing taxonomies for LLM risks and safety evaluation prompts, most studies have focused on monolingual contexts, primarily in English. However, language- and region-specific risks in bilingual contexts are often overlooked, and core findings can diverge from those in monolingual settings. In this paper, we introduce Qorgau, a novel dataset specifically designed for safety evaluation in Kazakh and Russian, reflecting the unique bilingual context in Kazakhstan, where both Kazakh (a low-resource language) and Russian (a high-resource language) are spoken. Experiments with both multilingual and language-specific LLMs reveal notable differences in safety performance, emphasizing the need for tailored, region-specific datasets to ensure the responsible and safe deployment of LLMs in countries like Kazakhstan. Warning: this paper contains example data that may be offensive, harmful, or biased.

### Non-Euclidean Hierarchical Representational Learning using Hyperbolic Graph Neural Networks for Environmental Claim Detection 
[[arxiv](https://arxiv.org/abs/2502.13628)] [[cool](https://papers.cool/arxiv/2502.13628)] [[pdf](https://arxiv.org/pdf/2502.13628)]
> **Authors**: Darpan Aswal,Manjira Sinha
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Transformer-based models dominate NLP tasks like sentiment analysis, machine translation, and claim verification. However, their massive computational demands and lack of interpretability pose challenges for real-world applications requiring efficiency and transparency. In this work, we explore Graph Neural Networks (GNNs) and Hyperbolic Graph Neural Networks (HGNNs) as lightweight yet effective alternatives for Environmental Claim Detection, reframing it as a graph classification problem. We construct dependency parsing graphs to explicitly model syntactic structures, using simple word embeddings (word2vec) for node features with dependency relations encoded as edge features. Our results demonstrate that these graph-based models achieve comparable or superior performance to state-of-the-art transformers while using 30x fewer parameters. This efficiency highlights the potential of structured, interpretable, and computationally efficient graph-based approaches.

### REFIND: Retrieval-Augmented Factuality Hallucination Detection in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.13622)] [[cool](https://papers.cool/arxiv/2502.13622)] [[pdf](https://arxiv.org/pdf/2502.13622)]
> **Authors**: DongGeon Lee,Hwanjo Yu
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Hallucinations in large language model (LLM) outputs severely limit their reliability in knowledge-intensive tasks such as question answering. To address this challenge, we introduce REFIND (Retrieval-augmented Factuality hallucINation Detection), a novel framework that detects hallucinated spans within LLM outputs by directly leveraging retrieved documents. As part of the REFIND, we propose the Context Sensitivity Ratio (CSR), a novel metric that quantifies the sensitivity of LLM outputs to retrieved evidence. This innovative approach enables REFIND to efficiently and accurately detect hallucinations, setting it apart from existing methods. In the evaluation, REFIND demonstrated robustness across nine languages, including low-resource settings, and significantly outperformed baseline models, achieving superior IoU scores in identifying hallucinated spans. This work highlights the effectiveness of quantifying context sensitivity for hallucination detection, thereby paving the way for more reliable and trustworthy LLM applications across diverse languages.

### Complex Ontology Matching with Large Language Model Embeddings 
[[arxiv](https://arxiv.org/abs/2502.13619)] [[cool](https://papers.cool/arxiv/2502.13619)] [[pdf](https://arxiv.org/pdf/2502.13619)]
> **Authors**: Guilherme Sousa,Rinaldo Lima,Cassia Trojahn
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Ontology, and more broadly, Knowledge Graph Matching is a challenging task in which expressiveness has not been fully addressed. Despite the increasing use of embeddings and language models for this task, approaches for generating expressive correspondences still do not take full advantage of these models, in particular, large language models (LLMs). This paper proposes to integrate LLMs into an approach for generating expressive correspondences based on alignment need and ABox-based relation discovery. The generation of correspondences is performed by matching similar surroundings of instance sub-graphs. The integration of LLMs results in different architectural modifications, including label similarity, sub-graph matching, and entity matching. The performance word embeddings, sentence embeddings, and LLM-based embeddings, was compared. The results demonstrate that integrating LLMs surpasses all other models, enhancing the baseline version of the approach with a 45\% increase in F-measure.

### BeamLoRA: Beam-Constraint Low-Rank Adaptation 
[[arxiv](https://arxiv.org/abs/2502.13604)] [[cool](https://papers.cool/arxiv/2502.13604)] [[pdf](https://arxiv.org/pdf/2502.13604)]
> **Authors**: Naibin Gu,Zhenyu Zhang,Xiyu Liu,Peng Fu,Zheng Lin,Shuohuan Wang,Yu Sun,Hua Wu,Weiping Wang,Haifeng Wang
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Due to the demand for efficient fine-tuning of large language models, Low-Rank Adaptation (LoRA) has been widely adopted as one of the most effective parameter-efficient fine-tuning methods. Nevertheless, while LoRA improves efficiency, there remains room for improvement in accuracy. Herein, we adopt a novel perspective to assess the characteristics of LoRA ranks. The results reveal that different ranks within the LoRA modules not only exhibit varying levels of importance but also evolve dynamically throughout the fine-tuning process, which may limit the performance of LoRA. Based on these findings, we propose BeamLoRA, which conceptualizes each LoRA module as a beam where each rank naturally corresponds to a potential sub-solution, and the fine-tuning process becomes a search for the optimal sub-solution combination. BeamLoRA dynamically eliminates underperforming sub-solutions while expanding the parameter space for promising ones, enhancing performance with a fixed rank. Extensive experiments across three base models and 12 datasets spanning math reasoning, code generation, and commonsense reasoning demonstrate that BeamLoRA consistently enhances the performance of LoRA, surpassing the other baseline methods.

### Efficient Safety Retrofitting Against Jailbreaking for LLMs 
[[arxiv](https://arxiv.org/abs/2502.13603)] [[cool](https://papers.cool/arxiv/2502.13603)] [[pdf](https://arxiv.org/pdf/2502.13603)]
> **Authors**: Dario Garcia-Gasulla,Adrian Tormos,Anna Arias-Duart,Daniel Hinjos,Oscar Molina-Sedano,Ashwin Kumar Gururajan,Maria Eugenia Cardello
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Direct Preference Optimization (DPO) is an efficient alignment technique that steers LLMs towards preferable outputs by training on preference data, bypassing the need for explicit reward models. Its simplicity enables easy adaptation to various domains and safety requirements. This paper examines DPO's effectiveness in model safety against jailbreaking attacks while minimizing data requirements and training costs. We introduce Egida, a dataset expanded from multiple sources, which includes 27 different safety topics and 18 different attack styles, complemented with synthetic and human labels. This data is used to boost the safety of state-of-the-art LLMs (Llama-3.1-8B/70B-Instruct, Qwen-2.5-7B/72B-Instruct) across topics and attack styles. In addition to safety evaluations, we assess their post-alignment performance degradation in general purpose tasks, and their tendency to over refusal. Following the proposed methodology, trained models reduce their Attack Success Rate by 10%-30%, using small training efforts (2,000 samples) with low computational cost (3\$ for 8B models, 20\$ for 72B models). Safety aligned models generalize to unseen topics and attack styles, with the most successful attack style reaching a success rate around 5%. Size and family are found to strongly influence model malleability towards safety, pointing at the importance of pre-training choices. To validate our findings, a large independent assessment of human preference agreement with Llama-Guard-3-8B is conducted by the authors and the associated dataset Egida-HSafe is released. Overall, this study illustrates how affordable and accessible it is to enhance LLM safety using DPO while outlining its current limitations. All datasets and models are released to enable reproducibility and further research.

### MMTEB: Massive Multilingual Text Embedding Benchmark 
[[arxiv](https://arxiv.org/abs/2502.13595)] [[cool](https://papers.cool/arxiv/2502.13595)] [[pdf](https://arxiv.org/pdf/2502.13595)]
> **Authors**: Kenneth Enevoldsen,Isaac Chung,Imene Kerboua,Márton Kardos,Ashwin Mathur,David Stap,Jay Gala,Wissam Siblini,Dominik Krzemiński,Genta Indra Winata,Saba Sturua,Saiteja Utpala,Mathieu Ciancone,Marion Schaeffer,Gabriel Sequeira,Diganta Misra,Shreeya Dhakal,Jonathan Rystrøm,Roman Solomatin,Ömer Çağatan,Akash Kundu,Martin Bernstorff,Shitao Xiao,Akshita Sukhlecha,Bhavish Pahwa, et al. (61 additional authors not shown)
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: Accepted for ICLR: https://openreview.net/forum?id=zl3pfz4VCV
- **标题**: None
- **领域**: 计算语言学,人工智能,信息检索
- **Abstract**: Text embeddings are typically evaluated on a limited set of tasks, which are constrained by language, domain, and task diversity. To address these limitations and provide a more comprehensive evaluation, we introduce the Massive Multilingual Text Embedding Benchmark (MMTEB) - a large-scale, community-driven expansion of MTEB, covering over 500 quality-controlled evaluation tasks across 250+ languages. MMTEB includes a diverse set of challenging, novel tasks such as instruction following, long-document retrieval, and code retrieval, representing the largest multilingual collection of evaluation tasks for embedding models to date. Using this collection, we develop several highly multilingual benchmarks, which we use to evaluate a representative set of models. We find that while large language models (LLMs) with billions of parameters can achieve state-of-the-art performance on certain language subsets and task categories, the best-performing publicly available model is multilingual-e5-large-instruct with only 560 million parameters. To facilitate accessibility and reduce computational cost, we introduce a novel downsampling method based on inter-task correlation, ensuring a diverse selection while preserving relative model rankings. Furthermore, we optimize tasks such as retrieval by sampling hard negatives, creating smaller but effective splits. These optimizations allow us to introduce benchmarks that drastically reduce computational demands. For instance, our newly introduced zero-shot English benchmark maintains a ranking order similar to the full-scale version but at a fraction of the computational cost.

### Don't Stop the Multi-Party! On Generating Synthetic Multi-Party Conversations with Constraints 
[[arxiv](https://arxiv.org/abs/2502.13592)] [[cool](https://papers.cool/arxiv/2502.13592)] [[pdf](https://arxiv.org/pdf/2502.13592)]
> **Authors**: Nicolò Penzo,Marco Guerini,Bruno Lepri,Goran Glavaš,Sara Tonelli
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Multi-Party Conversations (MPCs) are widely studied across disciplines, with social media as a primary data source due to their accessibility. However, these datasets raise privacy concerns and often reflect platform-specific properties. For example, interactions between speakers may be limited due to rigid platform structures (e.g., threads, tree-like discussions), which yield overly simplistic interaction patterns (e.g., as a consequence of ``reply-to'' links). This work explores the feasibility of generating diverse MPCs with instruction-tuned Large Language Models (LLMs) by providing deterministic constraints such as dialogue structure and participants' stance. We investigate two complementary strategies of leveraging LLMs in this context: (i.) LLMs as MPC generators, where we task the LLM to generate a whole MPC at once and (ii.) LLMs as MPC parties, where the LLM generates one turn of the conversation at a time, provided the conversation history. We next introduce an analytical framework to evaluate compliance with the constraints, content quality, and interaction complexity for both strategies. Finally, we assess the quality of obtained MPCs via human annotation and LLM-as-a-judge evaluations. We find stark differences among LLMs, with only some being able to generate high-quality MPCs. We also find that turn-by-turn generation yields better conformance to constraints and higher linguistic variability than generating MPCs in one pass. Nonetheless, our structural and qualitative evaluation indicates that both generation strategies can yield high-quality MPCs.

### Extracting Social Connections from Finnish Karelian Refugee Interviews Using LLMs 
[[arxiv](https://arxiv.org/abs/2502.13566)] [[cool](https://papers.cool/arxiv/2502.13566)] [[pdf](https://arxiv.org/pdf/2502.13566)]
> **Authors**: Joonatan Laato,Jenna Kanerva,John Loehr,Virpi Lummaa,Filip Ginter
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: Published at Proceedings of Fifth Conference on Computational Humanities Research (CHR'2024), December 2024 https://ceur-ws.org/Vol-3834/paper52.pdf
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: We performed a zero-shot information extraction study on a historical collection of 89,339 brief Finnish-language interviews of refugee families relocated post-WWII from Finnish Eastern Karelia. Our research objective is two-fold. First, we aim to extract social organizations and hobbies from the free text of the interviews, separately for each family member. These can act as a proxy variable indicating the degree of social integration of refugees in their new environment. Second, we aim to evaluate several alternative ways to approach this task, comparing a number of generative models and a supervised learning approach, to gain a broader insight into the relative merits of these different approaches and their applicability in similar studies. We find that the best generative model (GPT-4) is roughly on par with human performance, at an F-score of 88.8%. Interestingly, the best open generative model (Llama-3-70B-Instruct) reaches almost the same performance, at 87.7% F-score, demonstrating that open models are becoming a viable alternative for some practical tasks even on non-English data. Additionally, we test a supervised learning alternative, where we fine-tune a Finnish BERT model (FinBERT) using GPT-4 generated training data. By this method, we achieved an F-score of 84.1% already with 6K interviews up to an F-score of 86.3% with 30k interviews. Such an approach would be particularly appealing in cases where the computational resources are limited, or there is a substantial mass of data to process.

### PRIV-QA: Privacy-Preserving Question Answering for Cloud Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.13564)] [[cool](https://papers.cool/arxiv/2502.13564)] [[pdf](https://arxiv.org/pdf/2502.13564)]
> **Authors**: Guangwei Li,Yuansen Zhang,Yinggui Wang,Shoumeng Yan,Lei Wang,Tao Wei
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The rapid development of large language models (LLMs) is redefining the landscape of human-computer interaction, and their integration into various user-service applications is becoming increasingly prevalent. However, transmitting user data to cloud-based LLMs presents significant risks of data breaches and unauthorized access to personal identification information. In this paper, we propose a privacy preservation pipeline for protecting privacy and sensitive information during interactions between users and LLMs in practical LLM usage scenarios. We construct SensitiveQA, the first privacy open-ended question-answering dataset. It comprises 57k interactions in Chinese and English, encompassing a diverse range of user-sensitive information within the conversations. Our proposed solution employs a multi-stage strategy aimed at preemptively securing user information while simultaneously preserving the response quality of cloud-based LLMs. Experimental validation underscores our method's efficacy in balancing privacy protection with maintaining robust interaction quality. The code and dataset are available at https://github.com/ligw1998/PRIV-QA.

### STaR-SQL: Self-Taught Reasoner for Text-to-SQL 
[[arxiv](https://arxiv.org/abs/2502.13550)] [[cool](https://papers.cool/arxiv/2502.13550)] [[pdf](https://arxiv.org/pdf/2502.13550)]
> **Authors**: Mingqian He,Yongliang Shen,Wenqi Zhang,Qiuying Peng,Jun Wang,Weiming Lu
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Generating step-by-step "chain-of-thought" rationales has proven effective for improving the performance of large language models on complex reasoning tasks. However, applying such techniques to structured tasks, such as text-to-SQL, remains largely unexplored. In this paper, we introduce Self-Taught Reasoner for text-to-SQL (STaR-SQL), a novel approach that reframes SQL query generation as a reasoning-driven process. Our method prompts the LLM to produce detailed reasoning steps for SQL queries and fine-tunes it on rationales that lead to correct outcomes. Unlike traditional methods, STaR-SQL dedicates additional test-time computation to reasoning, thereby positioning LLMs as spontaneous reasoners rather than mere prompt-based agents. To further scale the inference process, we incorporate an outcome-supervised reward model (ORM) as a verifier, which enhances SQL query accuracy. Experimental results on the challenging Spider benchmark demonstrate that STaR-SQL significantly improves text-to-SQL performance, achieving an execution accuracy of 86.6%. This surpasses a few-shot baseline by 31.6% and a baseline fine-tuned to predict answers directly by 18.0%. Additionally, STaR-SQL outperforms agent-like prompting methods that leverage more powerful yet closed-source models such as GPT-4. These findings underscore the potential of reasoning-augmented training for structured tasks and open the door to extending self-improving reasoning models to text-to-SQL generation and beyond.

### Detecting Linguistic Bias in Government Documents Using Large language Models 
[[arxiv](https://arxiv.org/abs/2502.13548)] [[cool](https://papers.cool/arxiv/2502.13548)] [[pdf](https://arxiv.org/pdf/2502.13548)]
> **Authors**: Milena de Swart,Floris den Hengst,Jieying Chen
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: to appear in Proceedings of the ACM Web Conference 2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: This paper addresses the critical need for detecting bias in government documents, an underexplored area with significant implications for governance. Existing methodologies often overlook the unique context and far-reaching impacts of governmental documents, potentially obscuring embedded biases that shape public policy and citizen-government interactions. To bridge this gap, we introduce the Dutch Government Data for Bias Detection (DGDB), a dataset sourced from the Dutch House of Representatives and annotated for bias by experts. We fine-tune several BERT-based models on this dataset and compare their performance with that of generative language models. Additionally, we conduct a comprehensive error analysis that includes explanations of the models' predictions. Our findings demonstrate that fine-tuned models achieve strong performance and significantly outperform generative language models, indicating the effectiveness of DGDB for bias detection. This work underscores the importance of labeled datasets for bias detection in various languages and contributes to more equitable governance practices.

### From Sub-Ability Diagnosis to Human-Aligned Generation: Bridging the Gap for Text Length Control via MARKERGEN 
[[arxiv](https://arxiv.org/abs/2502.13544)] [[cool](https://papers.cool/arxiv/2502.13544)] [[pdf](https://arxiv.org/pdf/2502.13544)]
> **Authors**: Peiwen Yuan,Chuyi Tan,Shaoxiong Feng,Yiwei Li,Xinglin Wang,Yueqi Zhang,Jiayi Shi,Boyuan Pan,Yao Hu,Kan Li
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Despite the rapid progress of large language models (LLMs), their length-controllable text generation (LCTG) ability remains below expectations, posing a major limitation for practical applications. Existing methods mainly focus on end-to-end training to reinforce adherence to length constraints. However, the lack of decomposition and targeted enhancement of LCTG sub-abilities restricts further progress. To bridge this gap, we conduct a bottom-up decomposition of LCTG sub-abilities with human patterns as reference and perform a detailed error analysis. On this basis, we propose MarkerGen, a simple-yet-effective plug-and-play approach that:(1) mitigates LLM fundamental deficiencies via external tool integration;(2) conducts explicit length modeling with dynamically inserted markers;(3) employs a three-stage generation scheme to better align length constraints while maintaining content quality. Comprehensive experiments demonstrate that MarkerGen significantly improves LCTG across various settings, exhibiting outstanding effectiveness and generalizability.

### Activation-aware Probe-Query: Effective Key-Value Retrieval for Long-Context LLMs Inference 
[[arxiv](https://arxiv.org/abs/2502.13542)] [[cool](https://papers.cool/arxiv/2502.13542)] [[pdf](https://arxiv.org/pdf/2502.13542)]
> **Authors**: Qingfa Xiao,Jiachuan Wang,Haoyang Li,Cheng Deng,Jiaqi Tang,Shuangyin Li,Yongqi Zhang,Jun Wang,Lei Chen
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Recent advances in large language models (LLMs) have showcased exceptional performance in long-context tasks, while facing significant inference efficiency challenges with limited GPU memory. Existing solutions first proposed the sliding-window approach to accumulate a set of historical \textbf{key-value} (KV) pairs for reuse, then further improvements selectively retain its subsets at each step. However, due to the sparse attention distribution across a long context, it is hard to identify and recall relevant KV pairs, as the attention is distracted by massive candidate pairs. Additionally, we found it promising to select representative tokens as probe-Query in each sliding window to effectively represent the entire context, which is an approach overlooked by existing methods. Thus, we propose \textbf{ActQKV}, a training-free, \textbf{Act}ivation-aware approach that dynamically determines probe-\textbf{Q}uery and leverages it to retrieve the relevant \textbf{KV} pairs for inference. Specifically, ActQKV monitors a token-level indicator, Activation Bias, within each context window, enabling the proper construction of probe-Query for retrieval at pre-filling stage. To accurately recall the relevant KV pairs and minimize the irrelevant ones, we design a dynamic KV cut-off mechanism guided by information density across layers at the decoding stage. Experiments on the Long-Bench and $\infty$ Benchmarks demonstrate its state-of-the-art performance with competitive inference quality and resource efficiency.

### Shall Your Data Strategy Work? Perform a Swift Study 
[[arxiv](https://arxiv.org/abs/2502.13514)] [[cool](https://papers.cool/arxiv/2502.13514)] [[pdf](https://arxiv.org/pdf/2502.13514)]
> **Authors**: Minlong Peng,Jingyi Yang,Zhongjun He,Hua Wu
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 8 pages 5 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: This work presents a swift method to assess the efficacy of particular types of instruction-tuning data, utilizing just a handful of probe examples and eliminating the need for model retraining. This method employs the idea of gradient-based data influence estimation, analyzing the gradient projections of probe examples from the chosen strategy onto evaluation examples to assess its advantages. Building upon this method, we conducted three swift studies to investigate the potential of Chain-of-thought (CoT) data, query clarification data, and response evaluation data in enhancing model generalization. Subsequently, we embarked on a validation study to corroborate the findings of these swift studies. In this validation study, we developed training datasets tailored to each studied strategy and compared model performance with and without the use of these datasets. The results of the validation study aligned with the findings of the swift studies, validating the efficacy of our proposed method.

### Unlocking Multimodal Integration in EHRs: A Prompt Learning Framework for Language and Time Series Fusion 
[[arxiv](https://arxiv.org/abs/2502.13509)] [[cool](https://papers.cool/arxiv/2502.13509)] [[pdf](https://arxiv.org/pdf/2502.13509)]
> **Authors**: Shuai Niu,Jing Ma,Hongzhan Lin,Liang Bai,Zhihua Wang,Wei Bi,Yida Xu,Guo Li,Xian Yang
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 13 pages, 5 figures
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Large language models (LLMs) have shown remarkable performance in vision-language tasks, but their application in the medical field remains underexplored, particularly for integrating structured time series data with unstructured clinical notes. In clinical practice, dynamic time series data such as lab test results capture critical temporal patterns, while clinical notes provide rich semantic context. Merging these modalities is challenging due to the inherent differences between continuous signals and discrete text. To bridge this gap, we introduce ProMedTS, a novel self-supervised multimodal framework that employs prompt-guided learning to unify these heterogeneous data types. Our approach leverages lightweight anomaly detection to generate anomaly captions that serve as prompts, guiding the encoding of raw time series data into informative embeddings. These embeddings are aligned with textual representations in a shared latent space, preserving fine-grained temporal nuances alongside semantic insights. Furthermore, our framework incorporates tailored self-supervised objectives to enhance both intra- and inter-modal alignment. We evaluate ProMedTS on disease diagnosis tasks using real-world datasets, and the results demonstrate that our method consistently outperforms state-of-the-art approaches.

### PLDR-LLMs Learn A Generalizable Tensor Operator That Can Replace Its Own Deep Neural Net At Inference 
[[arxiv](https://arxiv.org/abs/2502.13502)] [[cool](https://papers.cool/arxiv/2502.13502)] [[pdf](https://arxiv.org/pdf/2502.13502)]
> **Authors**: Burc Gokden
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 15 pages, 1 figure, 12 tables, more ablation data included
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: We show that Large Language Model from Power Law Decoder Representations (PLDR-LLM) is a foundational model whose deductive outputs are invariant tensors up to a small perturbation. PLDR-LLM learns a singularity condition for the deductive outputs that enable the once-inferred energy-curvature tensor $\mathbf{G}_{LM}$ to replace the deep neural network of power law graph attention (PLGA) generating the deductive outputs at inference. We demonstrate that a cache for $\mathbf{G}_{LM}$ (G-cache) and KV-cache can be implemented in a straightforward manner to improve the inference time. The invariance and generalizable nature of deductive outputs is at a very high fidelity where deductive outputs have same RMSE and determinant values up to 15 decimal places after caching, and zero-shot benchmark scores remain unchanged. Ablation studies show that learned deductive outputs have distinct loss and accuracy characteristics from models pretrained with transferred, randomly initialized or identity tensors as a constant tensor operator and an LLM with scaled-dot product attention (SDPA) is a special case of PLDR-LLM where $\mathbf{G}_{LM}$ is predefined as identity. The observed invariance characteristic introduces a novel asymmetry between training and inference phases with caching. We outline observed common characteristics of the deductive outputs for the learned singularity condition. We provide an implementation of a training and inference framework for PLDR-LLM with KV-cache and G-cache.

### Towards Geo-Culturally Grounded LLM Generations 
[[arxiv](https://arxiv.org/abs/2502.13497)] [[cool](https://papers.cool/arxiv/2502.13497)] [[pdf](https://arxiv.org/pdf/2502.13497)]
> **Authors**: Piyawat Lertvittayakumjorn,David Kinney,Vinodkumar Prabhakaran,Donald Martin Jr.,Sunipa Dev
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Generative large language models (LLMs) have been demonstrated to have gaps in diverse, cultural knowledge across the globe. We investigate the effect of retrieval augmented generation and search-grounding techniques on the ability of LLMs to display familiarity with a diverse range of national cultures. Specifically, we compare the performance of standard LLMs, LLMs augmented with retrievals from a bespoke knowledge base (i.e., KB grounding), and LLMs augmented with retrievals from a web search (i.e., search grounding) on a series of cultural familiarity benchmarks. We find that search grounding significantly improves the LLM performance on multiple-choice benchmarks that test propositional knowledge (e.g., the norms, artifacts, and institutions of national cultures), while KB grounding's effectiveness is limited by inadequate knowledge base coverage and a suboptimal retriever. However, search grounding also increases the risk of stereotypical judgments by language models, while failing to improve evaluators' judgments of cultural familiarity in a human evaluation with adequate statistical power. These results highlight the distinction between propositional knowledge about a culture and open-ended cultural fluency when it comes to evaluating the cultural familiarity of generative LLMs.

### What are Models Thinking about? Understanding Large Language Model Hallucinations "Psychology" through Model Inner State Analysis 
[[arxiv](https://arxiv.org/abs/2502.13490)] [[cool](https://papers.cool/arxiv/2502.13490)] [[pdf](https://arxiv.org/pdf/2502.13490)]
> **Authors**: Peiran Wang,Yang Liu,Yunfei Lu,Jue Hong,Ye Wu
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large language model (LLM) systems suffer from the models' unstable ability to generate valid and factual content, resulting in hallucination generation. Current hallucination detection methods heavily rely on out-of-model information sources, such as RAG to assist the detection, thus bringing heavy additional latency. Recently, internal states of LLMs' inference have been widely used in numerous research works, such as prompt injection detection, etc. Considering the interpretability of LLM internal states and the fact that they do not require external information sources, we introduce such states into LLM hallucination detection. In this paper, we systematically analyze different internal states' revealing features during inference forward and comprehensively evaluate their ability in hallucination detection. Specifically, we cut the forward process of a large language model into three stages: understanding, query, generation, and extracting the internal state from these stages. By analyzing these states, we provide a deep understanding of why the hallucinated content is generated and what happened in the internal state of the models. Then, we introduce these internal states into hallucination detection and conduct comprehensive experiments to discuss the advantages and limitations.

### Transferring Textual Preferences to Vision-Language Understanding through Model Merging 
[[arxiv](https://arxiv.org/abs/2502.13487)] [[cool](https://papers.cool/arxiv/2502.13487)] [[pdf](https://arxiv.org/pdf/2502.13487)]
> **Authors**: Chen-An Li,Tzu-Han Lin,Yun-Nung Chen,Hung-yi Lee
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: Preprint. Under Review
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别,机器学习
- **Abstract**: Large vision-language models (LVLMs) perform outstandingly across various multimodal tasks. However, their ability to evaluate generated content remains limited, and training vision-language reward models (VLRMs) with preference data is computationally expensive. This paper explores a training-free alternative by merging text-based reward models (RMs) with LVLMs to create VLRMs. Our approach shows that integrating these models leads to improved performance over LVLMs' scoring and text-based RMs, offering an efficient method for incorporating textual preferences into LVLMs.

### LLM should think and action as a human 
[[arxiv](https://arxiv.org/abs/2502.13475)] [[cool](https://papers.cool/arxiv/2502.13475)] [[pdf](https://arxiv.org/pdf/2502.13475)]
> **Authors**: Haun Leung,ZiNan Wang
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 12 pages, 4 figures, 1 table
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: It is popular lately to train large language models to be used as chat assistants, but in the conversation between the user and the chat assistant, there are prompts, require multi-turns between the chat assistant and the user. However, there are a number of issues with the multi-turns conversation: The response of the chat assistant is prone to errors and can't help users achieve their goals, and as the number of conversation turns increases, the probability of errors will also increase; It is difficult for chat assistant to generate responses with different processes based on actual needs for the same prompt; Chat assistant require the use of tools, but the current approach is not elegant and efficient, and the number of tool calls is limited. The main reason for these issues is that large language models don't have the thinking ability as a human, lack the reasoning ability and planning ability, and lack the ability to execute plans. To solve these issues, we propose a thinking method based on a built-in chain of thought: In the multi-turns conversation, for each user prompt, the large language model thinks based on elements such as chat history, thinking context, action calls, memory and knowledge, makes detailed reasoning and planning, and actions according to the plan. We also explored how the large language model enhances thinking ability through this thinking method: Collect training datasets according to the thinking method and fine tune the large language model through supervised learning; Train a consistency reward model and use it as a reward function to fine tune the large language model using reinforcement learning, and the reinforced large language model outputs according to this way of thinking. Our experimental results show that the reasoning ability and planning ability of the large language model are enhanced, and the issues in the multi-turns conversation are solved.

### Towards Lightweight, Adaptive and Attribute-Aware Multi-Aspect Controllable Text Generation with Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.13474)] [[cool](https://papers.cool/arxiv/2502.13474)] [[pdf](https://arxiv.org/pdf/2502.13474)]
> **Authors**: Chenyu Zhu,Yefeng Liu,Chenyang Lyu,Xue Yang,Guanhua Chen,Longyue Wang,Weihua Luo,Kaifu Zhang
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 17 pages,9 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Multi-aspect controllable text generation aims to control text generation in attributes from multiple aspects, making it a complex but powerful task in natural language processing. Supervised fine-tuning methods are often employed for this task due to their simplicity and effectiveness. However, they still have some limitations: low rank adaptation (LoRA) only fine-tunes a few parameters and has suboptimal control effects, while full fine-tuning (FFT) requires significant computational resources and is susceptible to overfitting, particularly when data is limited. Moreover, existing works typically train multi-aspect controllable text generation models using only single-aspect annotated data, which results in discrepancies in data distribution; at the same time, accurately generating text with specific attributes is a challenge that requires strong attribute-aware capabilities. To address these limitations, we propose a lightweight, adaptive and attribute-aware framework for multi-aspect controllable text generation. Our framework can dynamically adjust model parameters according to different aspects of data to achieve controllable text generation, aiming to optimize performance across multiple aspects. Experimental results show that our framework outperforms other strong baselines, achieves state-of-the-art performance, adapts well to data discrepancies, and is more accurate in attribute perception.

### FlexDuo: A Pluggable System for Enabling Full-Duplex Capabilities in Speech Dialogue Systems 
[[arxiv](https://arxiv.org/abs/2502.13472)] [[cool](https://papers.cool/arxiv/2502.13472)] [[pdf](https://arxiv.org/pdf/2502.13472)]
> **Authors**: Borui Liao,Yulong Xu,Jiao Ou,Kaiyuan Yang,Weihua Jian,Pengfei Wan,Di Zhang
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人机交互
- **Abstract**: Full-Duplex Speech Dialogue Systems (Full-Duplex SDS) have significantly enhanced the naturalness of human-machine interaction by enabling real-time bidirectional communication. However, existing approaches face challenges such as difficulties in independent module optimization and contextual noise interference due to highly coupled architectural designs and oversimplified binary state modeling. This paper proposes FlexDuo, a flexible full-duplex control module that decouples duplex control from spoken dialogue systems through a plug-and-play architectural design. Furthermore, inspired by human information-filtering mechanisms in conversations, we introduce an explicit Idle state. On one hand, the Idle state filters redundant noise and irrelevant audio to enhance dialogue quality. On the other hand, it establishes a semantic integrity-based buffering mechanism, reducing the risk of mutual interruptions while ensuring accurate response transitions. Experimental results on the Fisher corpus demonstrate that FlexDuo reduces the false interruption rate by 24.9% and improves response accuracy by 7.6% compared to integrated full-duplex dialogue system baselines. It also outperforms voice activity detection (VAD) controlled baseline systems in both Chinese and English dialogue quality. The proposed modular architecture and state-based dialogue model provide a novel technical pathway for building flexible and efficient duplex dialogue systems.

### Estimating Commonsense Plausibility through Semantic Shifts 
[[arxiv](https://arxiv.org/abs/2502.13464)] [[cool](https://papers.cool/arxiv/2502.13464)] [[pdf](https://arxiv.org/pdf/2502.13464)]
> **Authors**: Wanqing Cui,Keping Bi,Jiafeng Guo,Xueqi Cheng
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Commonsense plausibility estimation is critical for evaluating language models (LMs), yet existing generative approaches--reliant on likelihoods or verbalized judgments--struggle with fine-grained discrimination. In this paper, we propose ComPaSS, a novel discriminative framework that quantifies commonsense plausibility by measuring semantic shifts when augmenting sentences with commonsense-related information. Plausible augmentations induce minimal shifts in semantics, while implausible ones result in substantial deviations. Evaluations on two types of fine-grained commonsense plausibility estimation tasks across different backbones, including LLMs and vision-language models (VLMs), show that ComPaSS consistently outperforms baselines. It demonstrates the advantage of discriminative approaches over generative methods in fine-grained commonsense plausibility evaluation. Experiments also show that (1) VLMs yield superior performance to LMs, when integrated with ComPaSS, on vision-grounded commonsense tasks. (2) contrastive pre-training sharpens backbone models' ability to capture semantic nuances, thereby further enhancing ComPaSS.

### ThinkGuard: Deliberative Slow Thinking Leads to Cautious Guardrails 
[[arxiv](https://arxiv.org/abs/2502.13458)] [[cool](https://papers.cool/arxiv/2502.13458)] [[pdf](https://arxiv.org/pdf/2502.13458)]
> **Authors**: Xiaofei Wen,Wenxuan Zhou,Wenjie Jacky Mo,Muhao Chen
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,密码学和安全,机器学习
- **Abstract**: Ensuring the safety of large language models (LLMs) is critical as they are deployed in real-world applications. Existing guardrails rely on rule-based filtering or single-pass classification, limiting their ability to handle nuanced safety violations. To address this, we propose ThinkGuard, a critique-augmented guardrail model that distills knowledge from high-capacity LLMs by generating structured critiques alongside safety labels. Fine-tuned on critique-augmented data, the captured deliberative thinking ability drastically enhances the guardrail's cautiousness and interpretability. Evaluated on multiple safety benchmarks, ThinkGuard achieves the highest average F1 and AUPRC, outperforming all baselines. Compared to LLaMA Guard 3, ThinkGuard improves accuracy by 16.1% and macro F1 by 27.0%. Moreover, it surpasses label-only fine-tuned models, confirming that structured critiques enhance both classification precision and nuanced safety reasoning while maintaining computational efficiency.

### TreeCut: A Synthetic Unanswerable Math Word Problem Dataset for LLM Hallucination Evaluation 
[[arxiv](https://arxiv.org/abs/2502.13442)] [[cool](https://papers.cool/arxiv/2502.13442)] [[pdf](https://arxiv.org/pdf/2502.13442)]
> **Authors**: Jialin Ouyang
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Large language models (LLMs) now achieve near-human performance on standard math word problem benchmarks (e.g., GSM8K), yet their true reasoning ability remains disputed. A key concern is that models often produce confident, yet unfounded, answers to unanswerable problems. We introduce TreeCut, a synthetic dataset that systematically generates infinite unanswerable math word problems and their answerable counterparts, by representing each question as a tree and removing chosen necessary conditions. Experiments show TreeCut effectively induce hallucinations in large language models, including GPT-4o and o3-mini, with rates of 61% and 42% in their respective worst-case scenarios. Further analysis highlights that deeper or more complex trees, composite item names, and removing necessary condition near the middle of a path all increase the likelihood of hallucinations, underscoring the persistent challenges LLMs face in identifying unanswerable math problems.

### The Self-Improvement Paradox: Can Language Models Bootstrap Reasoning Capabilities without External Scaffolding? 
[[arxiv](https://arxiv.org/abs/2502.13441)] [[cool](https://papers.cool/arxiv/2502.13441)] [[pdf](https://arxiv.org/pdf/2502.13441)]
> **Authors**: Yutao Sun,Mingshuai Chen,Tiancheng Zhao,Ruochen Xu,Zilun Zhang,Jianwei Yin
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Self-improving large language models (LLMs) -- i.e., to improve the performance of an LLM by fine-tuning it with synthetic data generated by itself -- is a promising way to advance the capabilities of LLMs while avoiding extensive supervision. Existing approaches to self-improvement often rely on external supervision signals in the form of seed data and/or assistance from third-party models. This paper presents Crescent -- a simple yet effective framework for generating high-quality synthetic question-answer data in a fully autonomous manner. Crescent first elicits the LLM to generate raw questions via a bait prompt, then diversifies these questions leveraging a rejection sampling-based self-deduplication, and finally feeds the questions to the LLM and collects the corresponding answers by means of majority voting. We show that Crescent sheds light on the potential of true self-improvement with zero external supervision signals for math reasoning; in particular, Crescent-generated question-answer pairs suffice to (i) improve the reasoning capabilities of an LLM while preserving its general performance (especially in the 0-shot setting); and (ii) distil LLM knowledge to weaker models more effectively than existing methods based on seed-dataset augmentation.

## 密码学和安全(cs.CR:Cryptography and Security)

### Multi-Faceted Studies on Data Poisoning can Advance LLM Development 
[[arxiv](https://arxiv.org/abs/2502.14182)] [[cool](https://papers.cool/arxiv/2502.14182)] [[pdf](https://arxiv.org/pdf/2502.14182)]
> **Authors**: Pengfei He,Yue Xing,Han Xu,Zhen Xiang,Jiliang Tang
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,机器学习
- **Abstract**: The lifecycle of large language models (LLMs) is far more complex than that of traditional machine learning models, involving multiple training stages, diverse data sources, and varied inference methods. While prior research on data poisoning attacks has primarily focused on the safety vulnerabilities of LLMs, these attacks face significant challenges in practice. Secure data collection, rigorous data cleaning, and the multistage nature of LLM training make it difficult to inject poisoned data or reliably influence LLM behavior as intended. Given these challenges, this position paper proposes rethinking the role of data poisoning and argue that multi-faceted studies on data poisoning can advance LLM development. From a threat perspective, practical strategies for data poisoning attacks can help evaluate and address real safety risks to LLMs. From a trustworthiness perspective, data poisoning can be leveraged to build more robust LLMs by uncovering and mitigating hidden biases, harmful outputs, and hallucinations. Moreover, from a mechanism perspective, data poisoning can provide valuable insights into LLMs, particularly the interplay between data and model behavior, driving a deeper understanding of their underlying mechanisms.

### CND-IDS: Continual Novelty Detection for Intrusion Detection Systems 
[[arxiv](https://arxiv.org/abs/2502.14094)] [[cool](https://papers.cool/arxiv/2502.14094)] [[pdf](https://arxiv.org/pdf/2502.14094)]
> **Authors**: Sean Fuhrman,Onat Gungor,Tajana Rosing
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: Accepted by the 62nd Design Automation Conference (DAC 2025)
- **标题**: None
- **领域**: 密码学和安全,机器学习
- **Abstract**: Intrusion detection systems (IDS) play a crucial role in IoT and network security by monitoring system data and alerting to suspicious activities. Machine learning (ML) has emerged as a promising solution for IDS, offering highly accurate intrusion detection. However, ML-IDS solutions often overlook two critical aspects needed to build reliable systems: continually changing data streams and a lack of attack labels. Streaming network traffic and associated cyber attacks are continually changing, which can degrade the performance of deployed ML models. Labeling attack data, such as zero-day attacks, in real-world intrusion scenarios may not be feasible, making the use of ML solutions that do not rely on attack labels necessary. To address both these challenges, we propose CND-IDS, a continual novelty detection IDS framework which consists of (i) a learning-based feature extractor that continuously updates new feature representations of the system data, and (ii) a novelty detector that identifies new cyber attacks by leveraging principal component analysis (PCA) reconstruction. Our results on realistic intrusion datasets show that CND-IDS achieves up to 6.1x F-score improvement, and up to 6.5x improved forward transfer over the SOTA unsupervised continual learning algorithm. Our code will be released upon acceptance.

### Secure Federated Data Distillation 
[[arxiv](https://arxiv.org/abs/2502.13728)] [[cool](https://papers.cool/arxiv/2502.13728)] [[pdf](https://arxiv.org/pdf/2502.13728)]
> **Authors**: Marco Arazzi,Mert Cihangiroglu,Serena Nicolazzo,Antonino Nocera
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Dataset Distillation (DD) is a powerful technique for reducing large datasets into compact, representative synthetic datasets, accelerating Machine Learning training. However, traditional DD methods operate in a centralized manner, which poses significant privacy threats and reduces its applicability. To mitigate these risks, we propose a Secure Federated Data Distillation framework (SFDD) to decentralize the distillation process while preserving privacy.Unlike existing Federated Distillation techniques that focus on training global models with distilled knowledge, our approach aims to produce a distilled dataset without exposing local contributions. We leverage the gradient-matching-based distillation method, adapting it for a distributed setting where clients contribute to the distillation process without sharing raw data. The central aggregator iteratively refines a synthetic dataset by integrating client-side updates while ensuring data confidentiality. To make our approach resilient to inference attacks perpetrated by the server that could exploit gradient updates to reconstruct private data, we create an optimized Local Differential Privacy approach, called LDPO-RLD (Label Differential Privacy Obfuscation via Randomized Linear Dispersion). Furthermore, we assess the framework's resilience against malicious clients executing backdoor attacks and demonstrate robustness under the assumption of a sufficient number of participating clients. Our experimental results demonstrate the effectiveness of SFDD and that the proposed defense concretely mitigates the identified vulnerabilities, with minimal impact on the performance of the distilled dataset. By addressing the interplay between privacy and federation in dataset distillation, this work advances the field of privacy-preserving Machine Learning making our SFDD framework a viable solution for sensitive data-sharing applications.

### Exploiting Prefix-Tree in Structured Output Interfaces for Enhancing Jailbreak Attacking 
[[arxiv](https://arxiv.org/abs/2502.13527)] [[cool](https://papers.cool/arxiv/2502.13527)] [[pdf](https://arxiv.org/pdf/2502.13527)]
> **Authors**: Yanzeng Li,Yunfan Xiong,Jialun Zhong,Jinchao Zhang,Jie Zhou,Lei Zou
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: The rise of Large Language Models (LLMs) has led to significant applications but also introduced serious security threats, particularly from jailbreak attacks that manipulate output generation. These attacks utilize prompt engineering and logit manipulation to steer models toward harmful content, prompting LLM providers to implement filtering and safety alignment strategies. We investigate LLMs' safety mechanisms and their recent applications, revealing a new threat model targeting structured output interfaces, which enable attackers to manipulate the inner logit during LLM generation, requiring only API access permissions. To demonstrate this threat model, we introduce a black-box attack framework called AttackPrefixTree (APT). APT exploits structured output interfaces to dynamically construct attack patterns. By leveraging prefixes of models' safety refusal response and latent harmful outputs, APT effectively bypasses safety measures. Experiments on benchmark datasets indicate that this approach achieves higher attack success rate than existing methods. This work highlights the urgent need for LLM providers to enhance security protocols to address vulnerabilities arising from the interaction between safety patterns and structured outputs.

### Poisoned Source Code Detection in Code Models 
[[arxiv](https://arxiv.org/abs/2502.13459)] [[cool](https://papers.cool/arxiv/2502.13459)] [[pdf](https://arxiv.org/pdf/2502.13459)]
> **Authors**: Ehab Ghannoum,Mohammad Ghafari
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: Accepted for Publication in the Journal of Systems and Software (JSS)
- **标题**: None
- **领域**: 密码学和安全,机器学习
- **Abstract**: Deep learning models have gained popularity for conducting various tasks involving source code. However, their black-box nature raises concerns about potential risks. One such risk is a poisoning attack, where an attacker intentionally contaminates the training set with malicious samples to mislead the model's predictions in specific scenarios. To protect source code models from poisoning attacks, we introduce CodeGarrison (CG), a hybrid deep-learning model that relies on code embeddings to identify poisoned code samples. We evaluated CG against the state-of-the-art technique ONION for detecting poisoned samples generated by DAMP, MHM, ALERT, as well as a novel poisoning technique named CodeFooler. Results showed that CG significantly outperformed ONION with an accuracy of 93.5%. We also tested CG's robustness against unknown attacks and achieved an average accuracy of 85.6% in identifying poisoned samples across the four attacks mentioned above.

## 计算机视觉和模式识别(cs.CV:Computer Vision and Pattern Recognition)

### Designing Parameter and Compute Efficient Diffusion Transformers using Distillation 
[[arxiv](https://arxiv.org/abs/2502.14226)] [[cool](https://papers.cool/arxiv/2502.14226)] [[pdf](https://arxiv.org/pdf/2502.14226)]
> **Authors**: Vignesh Sundaresha
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 4 pages
- **标题**: None
- **领域**: 计算机视觉和模式识别,图像和视频处理
- **Abstract**: Diffusion Transformers (DiTs) with billions of model parameters form the backbone of popular image and video generation models like DALL.E, Stable-Diffusion and SORA. Though these models are necessary in many low-latency applications like Augmented/Virtual Reality, they cannot be deployed on resource-constrained Edge devices (like Apple Vision Pro or Meta Ray-Ban glasses) due to their huge computational complexity. To overcome this, we turn to knowledge distillation and perform a thorough design-space exploration to achieve the best DiT for a given parameter size. In particular, we provide principles for how to choose design knobs such as depth, width, attention heads and distillation setup for a DiT. During the process, a three-way trade-off emerges between model performance, size and speed that is crucial for Edge implementation of diffusion. We also propose two distillation approaches - Teaching Assistant (TA) method and Multi-In-One (MI1) method - to perform feature distillation in the DiT context. Unlike existing solutions, we demonstrate and benchmark the efficacy of our approaches on practical Edge devices such as NVIDIA Jetson Orin Nano.

### H3DE-Net: Efficient and Accurate 3D Landmark Detection in Medical Imaging 
[[arxiv](https://arxiv.org/abs/2502.14221)] [[cool](https://papers.cool/arxiv/2502.14221)] [[pdf](https://arxiv.org/pdf/2502.14221)]
> **Authors**: Zhen Huang,Ronghao Xu,Xiaoqian Zhou,Yangbo Wei,Suhua Wang,Xiaoxin Sun,Han Li,Qingsong Yao
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: 3D landmark detection is a critical task in medical image analysis, and accurately detecting anatomical landmarks is essential for subsequent medical imaging tasks. However, mainstream deep learning methods in this field struggle to simultaneously capture fine-grained local features and model global spatial relationships, while maintaining a balance between accuracy and computational efficiency. Local feature extraction requires capturing fine-grained anatomical details, while global modeling requires understanding the spatial relationships within complex anatomical structures. The high-dimensional nature of 3D volume further exacerbates these challenges, as landmarks are sparsely distributed, leading to significant computational costs. Therefore, achieving efficient and precise 3D landmark detection remains a pressing challenge in medical image analysis. In this work, We propose a \textbf{H}ybrid \textbf{3}D \textbf{DE}tection \textbf{Net}(H3DE-Net), a novel framework that combines CNNs for local feature extraction with a lightweight attention mechanism designed to efficiently capture global dependencies in 3D volumetric data. This mechanism employs a hierarchical routing strategy to reduce computational cost while maintaining global context modeling. To our knowledge, H3DE-Net is the first 3D landmark detection model that integrates such a lightweight attention mechanism with CNNs. Additionally, integrating multi-scale feature fusion further enhances detection accuracy and robustness. Experimental results on a public CT dataset demonstrate that H3DE-Net achieves state-of-the-art(SOTA) performance, significantly improving accuracy and robustness, particularly in scenarios with missing landmarks or complex anatomical variations. We aready open-source our project, including code, data and model weights.

### Bridging Text and Vision: A Multi-View Text-Vision Registration Approach for Cross-Modal Place Recognition 
[[arxiv](https://arxiv.org/abs/2502.14195)] [[cool](https://papers.cool/arxiv/2502.14195)] [[pdf](https://arxiv.org/pdf/2502.14195)]
> **Authors**: Tianyi Shang,Zhenyu Li,Pengjie Xu,Jinwei Qiao,Gang Chen,Zihan Ruan,Weijun Hu
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 8 pages, 4 figures, conference
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器人技术
- **Abstract**: Mobile robots necessitate advanced natural language understanding capabilities to accurately identify locations and perform tasks such as package delivery. However, traditional visual place recognition (VPR) methods rely solely on single-view visual information and cannot interpret human language descriptions. To overcome this challenge, we bridge text and vision by proposing a multiview (360° views of the surroundings) text-vision registration approach called Text4VPR for place recognition task, which is the first method that exclusively utilizes textual descriptions to match a database of images. Text4VPR employs the frozen T5 language model to extract global textual embeddings. Additionally, it utilizes the Sinkhorn algorithm with temperature coefficient to assign local tokens to their respective clusters, thereby aggregating visual descriptors from images. During the training stage, Text4VPR emphasizes the alignment between individual text-image pairs for precise textual description. In the inference stage, Text4VPR uses the Cascaded Cross-Attention Cosine Alignment (CCCA) to address the internal mismatch between text and image groups. Subsequently, Text4VPR performs precisely place match based on the descriptions of text-image groups. On Street360Loc, the first text to image VPR dataset we created, Text4VPR builds a robust baseline, achieving a leading top-1 accuracy of 57% and a leading top-10 accuracy of 92% within a 5-meter radius on the test set, which indicates that localization from textual descriptions to images is not only feasible but also holds significant potential for further advancement, as shown in Figure 1.

### Multimodal RewardBench: Holistic Evaluation of Reward Models for Vision Language Models 
[[arxiv](https://arxiv.org/abs/2502.14191)] [[cool](https://papers.cool/arxiv/2502.14191)] [[pdf](https://arxiv.org/pdf/2502.14191)]
> **Authors**: Michihiro Yasunaga,Luke Zettlemoyer,Marjan Ghazvininejad
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: Dataset available at https://github.com/facebookresearch/multimodal_rewardbench
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Reward models play an essential role in training vision-language models (VLMs) by assessing output quality to enable aligning with human preferences. Despite their importance, the research community lacks comprehensive open benchmarks for evaluating multimodal reward models in VLMs. To address this gap, we introduce Multimodal RewardBench, an expert-annotated benchmark covering six domains: general correctness, preference, knowledge, reasoning, safety, and visual question-answering. Our dataset comprises 5,211 annotated (prompt, chosen response, rejected response) triplets collected from various VLMs. In evaluating a range of VLM judges, we find that even the top-performing models, Gemini 1.5 Pro and Claude 3.5 Sonnet, achieve only 72% overall accuracy. Notably, most models struggle in the reasoning and safety domains. These findings suggest that Multimodal RewardBench offers a challenging testbed for advancing reward model development across multiple domains. We release the benchmark at https://github.com/facebookresearch/multimodal_rewardbench.

### Stereo Image Coding for Machines with Joint Visual Feature Compression 
[[arxiv](https://arxiv.org/abs/2502.14190)] [[cool](https://papers.cool/arxiv/2502.14190)] [[pdf](https://arxiv.org/pdf/2502.14190)]
> **Authors**: Dengchao Jin,Jianjun Lei,Bo Peng,Zhaoqing Pan,Nam Ling,Qingming Huang
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,图像和视频处理
- **Abstract**: 2D image coding for machines (ICM) has achieved great success in coding efficiency, while less effort has been devoted to stereo image fields. To promote the efficiency of stereo image compression (SIC) and intelligent analysis, the stereo image coding for machines (SICM) is formulated and explored in this paper. More specifically, a machine vision-oriented stereo feature compression network (MVSFC-Net) is proposed for SICM, where the stereo visual features are effectively extracted, compressed, and transmitted for 3D visual task. To efficiently compress stereo visual features in MVSFC-Net, a stereo multi-scale feature compression (SMFC) module is designed to gradually transform sparse stereo multi-scale features into compact joint visual representations by removing spatial, inter-view, and cross-scale redundancies simultaneously. Experimental results show that the proposed MVSFC-Net obtains superior compression efficiency as well as 3D visual task performance, when compared with the existing ICM anchors recommended by MPEG and the state-of-the-art SIC method.

### Bayesian SegNet for Semantic Segmentation with Improved Interpretation of Microstructural Evolution During Irradiation of Materials 
[[arxiv](https://arxiv.org/abs/2502.14184)] [[cool](https://papers.cool/arxiv/2502.14184)] [[pdf](https://arxiv.org/pdf/2502.14184)]
> **Authors**: Marjolein Oostrom,Alex Hagen,Nicole LaHaye,Karl Pazdernik
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Understanding the relationship between the evolution of microstructures of irradiated LiAlO2 pellets and tritium diffusion, retention and release could improve predictions of tritium-producing burnable absorber rod performance. Given expert-labeled segmented images of irradiated and unirradiated pellets, we trained Deep Convolutional Neural Networks to segment images into defect, grain, and boundary classes. Qualitative microstructural information was calculated from these segmented images to facilitate the comparison of unirradiated and irradiated pellets. We tested modifications to improve the sensitivity of the model, including incorporating meta-data into the model and utilizing uncertainty quantification. The predicted segmentation was similar to the expert-labeled segmentation for most methods of microstructural qualification, including pixel proportion, defect area, and defect density. Overall, the high performance metrics for the best models for both irradiated and unirradiated images shows that utilizing neural network models is a viable alternative to expert-labeled images.

### Deep learning based infrared small object segmentation: Challenges and future directions 
[[arxiv](https://arxiv.org/abs/2502.14168)] [[cool](https://papers.cool/arxiv/2502.14168)] [[pdf](https://arxiv.org/pdf/2502.14168)]
> **Authors**: Zhengeng Yang,Hongshan Yu,Jianjun Zhang,Qiang Tang,Ajmal Mian
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: This is a submitted version of a paper accepted by Information Fusion. If you want a better reading experience, please refer to the final published version of Information Fusion
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Infrared sensing is a core method for supporting unmanned systems, such as autonomous vehicles and drones. Recently, infrared sensors have been widely deployed on mobile and stationary platforms for detection and classification of objects from long distances and in wide field of views. Given its success in the vision image analysis domain, deep learning has also been applied for object recognition in infrared images. However, techniques that have proven successful in visible light perception face new challenges in the infrared domain. These challenges include extremely low signal-to-noise ratios in infrared images, very small and blurred objects of interest, and limited availability of labeled/unlabeled training data due to the specialized nature of infrared sensors. Numerous methods have been proposed in the literature for the detection and classification of small objects in infrared images achieving varied levels of success. There is a need for a survey paper that critically analyzes existing techniques in this domain, identifies unsolved challenges and provides future research directions. This paper fills the gap and offers a concise and insightful review of deep learning-based methods. It also identifies the challenges faced by existing infrared object segmentation methods and provides a structured review of existing infrared perception methods from the perspective of these challenges and highlights the motivations behind the various approaches. Finally, this review suggests promising future directions based on recent advancements within this domain.

### PitVQA++: Vector Matrix-Low-Rank Adaptation for Open-Ended Visual Question Answering in Pituitary Surgery 
[[arxiv](https://arxiv.org/abs/2502.14149)] [[cool](https://papers.cool/arxiv/2502.14149)] [[pdf](https://arxiv.org/pdf/2502.14149)]
> **Authors**: Runlong He,Danyal Z. Khan,Evangelos B. Mazomenos,Hani J. Marcus,Danail Stoyanov,Matthew J. Clarkson,Mobarakol Islam
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 9 pages
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Vision-Language Models (VLMs) in visual question answering (VQA) offer a unique opportunity to enhance intra-operative decision-making, promote intuitive interactions, and significantly advancing surgical education. However, the development of VLMs for surgical VQA is challenging due to limited datasets and the risk of overfitting and catastrophic forgetting during full fine-tuning of pretrained weights. While parameter-efficient techniques like Low-Rank Adaptation (LoRA) and Matrix of Rank Adaptation (MoRA) address adaptation challenges, their uniform parameter distribution overlooks the feature hierarchy in deep networks, where earlier layers, that learn general features, require more parameters than later ones. This work introduces PitVQA++ with an open-ended PitVQA dataset and vector matrix-low-rank adaptation (Vector-MoLoRA), an innovative VLM fine-tuning approach for adapting GPT-2 to pituitary surgery. Open-Ended PitVQA comprises around 101,803 frames from 25 procedural videos with 745,972 question-answer sentence pairs, covering key surgical elements such as phase and step recognition, context understanding, tool detection, localization, and interactions recognition. Vector-MoLoRA incorporates the principles of LoRA and MoRA to develop a matrix-low-rank adaptation strategy that employs vector ranking to allocate more parameters to earlier layers, gradually reducing them in the later layers. Our approach, validated on the Open-Ended PitVQA and EndoVis18-VQA datasets, effectively mitigates catastrophic forgetting while significantly enhancing performance over recent baselines. Furthermore, our risk-coverage analysis highlights its enhanced reliability and trustworthiness in handling uncertain predictions. Our source code and dataset is available at~\url{https://github.com/HRL-Mike/PitVQA-Plus}.

### Token Adaptation via Side Graph Convolution for Efficient Fine-tuning of 3D Point Cloud Transformers 
[[arxiv](https://arxiv.org/abs/2502.14142)] [[cool](https://papers.cool/arxiv/2502.14142)] [[pdf](https://arxiv.org/pdf/2502.14142)]
> **Authors**: Takahiko Furuya
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Parameter-efficient fine-tuning (PEFT) of pre-trained 3D point cloud Transformers has emerged as a promising technique for 3D point cloud analysis. While existing PEFT methods attempt to minimize the number of tunable parameters, they often suffer from high temporal and spatial computational costs during fine-tuning. This paper proposes a novel PEFT algorithm called Side Token Adaptation on a neighborhood Graph (STAG) to achieve superior temporal and spatial efficiency. STAG employs a graph convolutional side network operating in parallel with a frozen backbone Transformer to adapt tokens to downstream tasks. Through efficient graph convolution, parameter sharing, and reduced gradient computation, STAG significantly reduces both temporal and spatial costs for fine-tuning. We also present Point Cloud Classification 13 (PCC13), a new benchmark comprising diverse publicly available 3D point cloud datasets to facilitate comprehensive evaluation. Extensive experiments using multiple pre-trained models and PCC13 demonstrates the effectiveness of STAG. Specifically, STAG maintains classification accuracy comparable to existing methods while reducing tunable parameters to only 0.43M and achieving significant reductions in both computation time and memory consumption for fine-tuning. Code and benchmark will be available at: https://github.com/takahikof/STAG.

### Modular Prompt Learning Improves Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2502.14125)] [[cool](https://papers.cool/arxiv/2502.14125)] [[pdf](https://arxiv.org/pdf/2502.14125)]
> **Authors**: Zhenhan Huang,Tejaswini Pedapati,Pin-Yu Chen,Jianxi Gao
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 2025 IEEE International Conference on Acoustics, Speech, and Signal Processing
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Pre-trained vision-language models are able to interpret visual concepts and language semantics. Prompt learning, a method of constructing prompts for text encoders or image encoders, elicits the potentials of pre-trained models and readily adapts them to new scenarios. Compared to fine-tuning, prompt learning enables the model to achieve comparable or better performance using fewer trainable parameters. Besides, prompt learning freezes the pre-trained model and avoids the catastrophic forgetting issue in the fine-tuning. Continuous prompts inserted into the input of every transformer layer (i.e. deep prompts) can improve the performances of pre-trained models on downstream tasks. For i-th transformer layer, the inserted prompts replace previously inserted prompts in the $(i-1)$-th layer. Although the self-attention mechanism contextualizes newly inserted prompts for the current layer and embeddings from the previous layer's output, removing all inserted prompts from the previous layer inevitably loses information contained in the continuous prompts. In this work, we propose Modular Prompt Learning (MPL) that is designed to promote the preservation of information contained in the inserted prompts. We evaluate the proposed method on base-to-new generalization and cross-dataset tasks. On average of 11 datasets, our method achieves 0.7% performance gain on the base-to-new generalization task compared to the state-of-the-art method. The largest improvement on the individual dataset is 10.7% (EuroSAT dataset).

### Object-centric Binding in Contrastive Language-Image Pretraining 
[[arxiv](https://arxiv.org/abs/2502.14113)] [[cool](https://papers.cool/arxiv/2502.14113)] [[pdf](https://arxiv.org/pdf/2502.14113)]
> **Authors**: Rim Assouel,Pietro Astolfi,Florian Bordes,Michal Drozdzal,Adriana Romero-Soriano
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Recent advances in vision language models (VLM) have been driven by contrastive models such as CLIP, which learn to associate visual information with their corresponding text descriptions. However, these models have limitations in understanding complex compositional scenes involving multiple objects and their spatial relationships. To address these challenges, we propose a novel approach that diverges from commonly used strategies, which rely on the design of hard-negative augmentations. Instead, our work focuses on integrating inductive biases into pre-trained CLIP-like models to improve their compositional understanding without using any additional hard-negatives. To that end, we introduce a binding module that connects a scene graph, derived from a text description, with a slot-structured image representation, facilitating a structured similarity assessment between the two modalities. We also leverage relationships as text-conditioned visual constraints, thereby capturing the intricate interactions between objects and their contextual relationships more effectively. Our resulting model not only enhances the performance of CLIP-based models in multi-object compositional understanding but also paves the way towards more accurate and sample-efficient image-text matching of complex scenes.

### Point Cloud Geometry Scalable Coding Using a Resolution and Quality-conditioned Latents Probability Estimator 
[[arxiv](https://arxiv.org/abs/2502.14099)] [[cool](https://papers.cool/arxiv/2502.14099)] [[pdf](https://arxiv.org/pdf/2502.14099)]
> **Authors**: Daniele Mari,André F. R. Guarda,Nuno M. M. Rodrigues,Simone Milani,Fernando Pereira
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: Submitted to IEEE and currently under review
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: In the current age, users consume multimedia content in very heterogeneous scenarios in terms of network, hardware, and display capabilities. A naive solution to this problem is to encode multiple independent streams, each covering a different possible requirement for the clients, with an obvious negative impact in both storage and computational requirements. These drawbacks can be avoided by using codecs that enable scalability, i.e., the ability to generate a progressive bitstream, containing a base layer followed by multiple enhancement layers, that allow decoding the same bitstream serving multiple reconstructions and visualization specifications. While scalable coding is a well-known and addressed feature in conventional image and video codecs, this paper focuses on a new and very different problem, notably the development of scalable coding solutions for deep learning-based Point Cloud (PC) coding. The peculiarities of this 3D representation make it hard to implement flexible solutions that do not compromise the other functionalities of the codec. This paper proposes a joint quality and resolution scalability scheme, named Scalable Resolution and Quality Hyperprior (SRQH), that, contrary to previous solutions, can model the relationship between latents obtained with models trained for different RD tradeoffs and/or at different resolutions. Experimental results obtained by integrating SRQH in the emerging JPEG Pleno learning-based PC coding standard show that SRQH allows decoding the PC at different qualities and resolutions with a single bitstream while incurring only in a limited RD penalty and increment in complexity w.r.t. non-scalable JPEG PCC that would require one bitstream per coding configuration.

### Regression in EO: Are VLMs Up to the Challenge? 
[[arxiv](https://arxiv.org/abs/2502.14088)] [[cool](https://papers.cool/arxiv/2502.14088)] [[pdf](https://arxiv.org/pdf/2502.14088)]
> **Authors**: Xizhe Xue,Xiao Xiang Zhu
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Earth Observation (EO) data encompass a vast range of remotely sensed information, featuring multi-sensor and multi-temporal, playing an indispensable role in understanding our planet's dynamics. Recently, Vision Language Models (VLMs) have achieved remarkable success in perception and reasoning tasks, bringing new insights and opportunities to the EO field. However, the potential for EO applications, especially for scientific regression related applications remains largely unexplored. This paper bridges that gap by systematically examining the challenges and opportunities of adapting VLMs for EO regression tasks. The discussion first contrasts the distinctive properties of EO data with conventional computer vision datasets, then identifies four core obstacles in applying VLMs to EO regression: 1) the absence of dedicated benchmarks, 2) the discrete-versus-continuous representation mismatch, 3) cumulative error accumulation, and 4) the suboptimal nature of text-centric training objectives for numerical tasks. Next, a series of methodological insights and potential subtle pitfalls are explored. Lastly, we offer some promising future directions for designing robust, domain-aware solutions. Our findings highlight the promise of VLMs for scientific regression in EO, setting the stage for more precise and interpretable modeling of critical environmental processes.

### A Racing Dataset and Baseline Model for Track Detection in Autonomous Racing 
[[arxiv](https://arxiv.org/abs/2502.14068)] [[cool](https://papers.cool/arxiv/2502.14068)] [[pdf](https://arxiv.org/pdf/2502.14068)]
> **Authors**: Shreya Ghosh,Yi-Huan Chen,Ching-Hsiang Huang,Abu Shafin Mohammad Mahdee Jameel,Chien Chou Ho,Aly El Gamal,Samuel Labi
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: Currently Under Review
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,图像和视频处理
- **Abstract**: A significant challenge in racing-related research is the lack of publicly available datasets containing raw images with corresponding annotations for the downstream task. In this paper, we introduce RoRaTrack, a novel dataset that contains annotated multi-camera image data from racing scenarios for track detection. The data is collected on a Dallara AV-21 at a racing circuit in Indiana, in collaboration with the Indy Autonomous Challenge (IAC). RoRaTrack addresses common problems such as blurriness due to high speed, color inversion from the camera, and absence of lane markings on the track. Consequently, we propose RaceGAN, a baseline model based on a Generative Adversarial Network (GAN) that effectively addresses these challenges. The proposed model demonstrates superior performance compared to current state-of-the-art machine learning models in track detection. The dataset and code for this work are available at github.com/RaceGAN.

### EfficientPose 6D: Scalable and Efficient 6D Object Pose Estimation 
[[arxiv](https://arxiv.org/abs/2502.14061)] [[cool](https://papers.cool/arxiv/2502.14061)] [[pdf](https://arxiv.org/pdf/2502.14061)]
> **Authors**: Zixuan Fang,Thomas Pöllabauer,Tristan Wirth,Sarah Berkei,Volker Knauthe,Arjan Kuijper
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: In industrial applications requiring real-time feedback, such as quality control and robotic manipulation, the demand for high-speed and accurate pose estimation remains critical. Despite advances improving speed and accuracy in pose estimation, finding a balance between computational efficiency and accuracy poses significant challenges in dynamic environments. Most current algorithms lack scalability in estimation time, especially for diverse datasets, and the state-of-the-art (SOTA) methods are often too slow. This study focuses on developing a fast and scalable set of pose estimators based on GDRNPP to meet or exceed current benchmarks in accuracy and robustness, particularly addressing the efficiency-accuracy trade-off essential in real-time scenarios. We propose the AMIS algorithm to tailor the utilized model according to an application-specific trade-off between inference time and accuracy. We further show the effectiveness of the AMIS-based model choice on four prominent benchmark datasets (LM-O, YCB-V, T-LESS, and ITODD).

### Enhancing Cognition and Explainability of Multimodal Foundation Models with Self-Synthesized Data 
[[arxiv](https://arxiv.org/abs/2502.14044)] [[cool](https://papers.cool/arxiv/2502.14044)] [[pdf](https://arxiv.org/pdf/2502.14044)]
> **Authors**: Yucheng Shi,Quanzheng Li,Jin Sun,Xiang Li,Ninghao Liu
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: Accepted by ICLR 2025. Code: https://github.com/sycny/SelfSynthX
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Large Multimodal Models (LMMs), or Vision-Language Models (VLMs), have shown impressive capabilities in a wide range of visual tasks. However, they often struggle with fine-grained visual reasoning, failing to identify domain-specific objectives and provide justifiable explanations for their predictions. To address the above challenge, we propose a novel visual rejection sampling framework to improve the cognition and explainability of LMMs using self-synthesized data. Specifically, visual fine-tuning requires images, queries, and target answers. Our approach begins by synthesizing interpretable answers that include human-verifiable visual features. These features are based on expert-defined concepts, and carefully selected based on their alignment with the image content. After each round of fine-tuning, we apply a reward model-free filtering mechanism to select the highest-quality interpretable answers for the next round of tuning. This iterative process of synthetic data generation and fine-tuning progressively improves the model's ability to generate accurate and reasonable explanations. Experimental results demonstrate the effectiveness of our method in improving both the accuracy and explainability of specialized visual classification tasks.

### FlexTok: Resampling Images into 1D Token Sequences of Flexible Length 
[[arxiv](https://arxiv.org/abs/2502.13967)] [[cool](https://papers.cool/arxiv/2502.13967)] [[pdf](https://arxiv.org/pdf/2502.13967)]
> **Authors**: Roman Bachmann,Jesse Allardice,David Mizrahi,Enrico Fini,Oğuzhan Fatih Kar,Elmira Amirloo,Alaaeldin El-Nouby,Amir Zamir,Afshin Dehghan
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: Project page at https://flextok.epfl.ch/
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Image tokenization has enabled major advances in autoregressive image generation by providing compressed, discrete representations that are more efficient to process than raw pixels. While traditional approaches use 2D grid tokenization, recent methods like TiTok have shown that 1D tokenization can achieve high generation quality by eliminating grid redundancies. However, these methods typically use a fixed number of tokens and thus cannot adapt to an image's inherent complexity. We introduce FlexTok, a tokenizer that projects 2D images into variable-length, ordered 1D token sequences. For example, a 256x256 image can be resampled into anywhere from 1 to 256 discrete tokens, hierarchically and semantically compressing its information. By training a rectified flow model as the decoder and using nested dropout, FlexTok produces plausible reconstructions regardless of the chosen token sequence length. We evaluate our approach in an autoregressive generation setting using a simple GPT-style Transformer. On ImageNet, this approach achieves an FID<2 across 8 to 128 tokens, outperforming TiTok and matching state-of-the-art methods with far fewer tokens. We further extend the model to support to text-conditioned image generation and examine how FlexTok relates to traditional 2D tokenization. A key finding is that FlexTok enables next-token prediction to describe images in a coarse-to-fine "visual vocabulary", and that the number of tokens to generate depends on the complexity of the generation task.

### IP-Composer: Semantic Composition of Visual Concepts 
[[arxiv](https://arxiv.org/abs/2502.13951)] [[cool](https://papers.cool/arxiv/2502.13951)] [[pdf](https://arxiv.org/pdf/2502.13951)]
> **Authors**: Sara Dorfman,Dana Cohen-Bar,Rinon Gal,Daniel Cohen-Or
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: Project Page: https://ip-composer.github.io/IP-Composer/
- **标题**: None
- **领域**: 计算机视觉和模式识别,图形
- **Abstract**: Content creators often draw inspiration from multiple visual sources, combining distinct elements to craft new compositions. Modern computational approaches now aim to emulate this fundamental creative process. Although recent diffusion models excel at text-guided compositional synthesis, text as a medium often lacks precise control over visual details. Image-based composition approaches can capture more nuanced features, but existing methods are typically limited in the range of concepts they can capture, and require expensive training procedures or specialized data. We present IP-Composer, a novel training-free approach for compositional image generation that leverages multiple image references simultaneously, while using natural language to describe the concept to be extracted from each image. Our method builds on IP-Adapter, which synthesizes novel images conditioned on an input image's CLIP embedding. We extend this approach to multiple visual inputs by crafting composite embeddings, stitched from the projections of multiple input images onto concept-specific CLIP-subspaces identified through text. Through comprehensive evaluation, we show that our approach enables more precise control over a larger range of visual concept compositions.

### A Chain-of-Thought Subspace Meta-Learning for Few-shot Image Captioning with Large Vision and Language Models 
[[arxiv](https://arxiv.org/abs/2502.13942)] [[cool](https://papers.cool/arxiv/2502.13942)] [[pdf](https://arxiv.org/pdf/2502.13942)]
> **Authors**: Hao Huang,Shuaihang Yuan,Yu Hao,Congcong Wen,Yi Fang
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 11 pages, 3 figures, 5 tables
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: A large-scale vision and language model that has been pretrained on massive data encodes visual and linguistic prior, which makes it easier to generate images and language that are more natural and realistic. Despite this, there is still a significant domain gap between the modalities of vision and language, especially when training data is scarce in few-shot settings, where only very limited data are available for training. In order to mitigate this issue, a multi-modal meta-learning framework has been proposed to bridge the gap between two frozen pretrained large vision and language models by introducing a tunable prompt connecting these two large models. For few-shot image captioning, the existing multi-model meta-learning framework utilizes a one-step prompting scheme to accumulate the visual features of input images to guide the language model, which struggles to generate accurate image descriptions with only a few training samples. Instead, we propose a chain-of-thought (CoT) meta-learning scheme as a multi-step image captioning procedure to better imitate how humans describe images. In addition, we further propose to learn different meta-parameters of the model corresponding to each CoT step in distinct subspaces to avoid interference. We evaluated our method on three commonly used image captioning datasets, i.e., MSCOCO, Flickr8k, and Flickr30k, under few-shot settings. The results of our experiments indicate that our chain-of-thought subspace meta-learning strategy is superior to the baselines in terms of performance across different datasets measured by different metrics.

### Image compositing is all you need for data augmentation 
[[arxiv](https://arxiv.org/abs/2502.13936)] [[cool](https://papers.cool/arxiv/2502.13936)] [[pdf](https://arxiv.org/pdf/2502.13936)]
> **Authors**: Ang Jia Ning Shermaine,Michalis Lazarou,Tania Stathaki
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: Accepted in VISAPP 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: This paper investigates the impact of various data augmentation techniques on the performance of object detection models. Specifically, we explore classical augmentation methods, image compositing, and advanced generative models such as Stable Diffusion XL and ControlNet. The objective of this work is to enhance model robustness and improve detection accuracy, particularly when working with limited annotated data. Using YOLOv8, we fine-tune the model on a custom dataset consisting of commercial and military aircraft, applying different augmentation strategies. Our experiments show that image compositing offers the highest improvement in detection performance, as measured by precision, recall, and mean Average Precision (mAP@0.50). Other methods, including Stable Diffusion XL and ControlNet, also demonstrate significant gains, highlighting the potential of advanced data augmentation techniques for object detection tasks. The results underline the importance of dataset diversity and augmentation in achieving better generalization and performance in real-world applications. Future work will explore the integration of semi-supervised learning methods and further optimizations to enhance model performance across larger and more complex datasets.

### Continually Learning Structured Visual Representations via Network Refinement with Rerelation 
[[arxiv](https://arxiv.org/abs/2502.13935)] [[cool](https://papers.cool/arxiv/2502.13935)] [[pdf](https://arxiv.org/pdf/2502.13935)]
> **Authors**: Zeki Doruk Erden,Boi Faltings
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: Current machine learning paradigm relies on continuous representations like neural networks, which iteratively adjust parameters to approximate outcomes rather than directly learning the structure of problem. This spreads information across the network, causing issues like information loss and incomprehensibility Building on prior work in environment dynamics modeling, we propose a method that learns visual space in a structured, continual manner. Our approach refines networks to capture the core structure of objects while representing significant subvariants in structure efficiently. We demonstrate this with 2D shape detection, showing incremental learning on MNIST without overwriting knowledge and creating compact, comprehensible representations. These results offer a promising step toward a transparent, continually learning alternative to traditional neural networks for visual processing.

### Symmetrical Visual Contrastive Optimization: Aligning Vision-Language Models with Minimal Contrastive Images 
[[arxiv](https://arxiv.org/abs/2502.13928)] [[cool](https://papers.cool/arxiv/2502.13928)] [[pdf](https://arxiv.org/pdf/2502.13928)]
> **Authors**: Shengguang Wu,Fan-Yun Sun,Kaiyue Wen,Nick Haber
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: Project Website: https://s-vco.github.io/
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习
- **Abstract**: Recent studies have shown that Large Vision-Language Models (VLMs) tend to neglect image content and over-rely on language-model priors, resulting in errors in visually grounded tasks and hallucinations. We hypothesize that this issue arises because existing VLMs are not explicitly trained to generate texts that are accurately grounded in fine-grained image details. To enhance visual feedback during VLM training, we propose S-VCO (Symmetrical Visual Contrastive Optimization), a novel finetuning objective that steers the model toward capturing important visual details and aligning them with corresponding text tokens. To further facilitate this detailed alignment, we introduce MVC, a paired image-text dataset built by automatically filtering and augmenting visual counterfactual data to challenge the model with hard contrastive cases involving Minimal Visual Contrasts. Experiments show that our method consistently improves VLM performance across diverse benchmarks covering various abilities and domains, achieving up to a 22% reduction in hallucinations, and significant gains in vision-centric and general tasks. Notably, these improvements become increasingly pronounced in benchmarks with higher visual dependency. In short, S-VCO offers a significant enhancement of VLM's visually-dependent task performance while retaining or even improving the model's general abilities. We opensource our code at https://s-vco.github.io/

### Qwen2.5-VL Technical Report 
[[arxiv](https://arxiv.org/abs/2502.13923)] [[cool](https://papers.cool/arxiv/2502.13923)] [[pdf](https://arxiv.org/pdf/2502.13923)]
> **Authors**: Shuai Bai,Keqin Chen,Xuejing Liu,Jialin Wang,Wenbin Ge,Sibo Song,Kai Dang,Peng Wang,Shijie Wang,Jun Tang,Humen Zhong,Yuanzhi Zhu,Mingkun Yang,Zhaohai Li,Jianqiang Wan,Pengfei Wang,Wei Ding,Zheren Fu,Yiheng Xu,Jiabo Ye,Xi Zhang,Tianbao Xie,Zesen Cheng,Hang Zhang,Zhibo Yang, et al. (2 additional authors not shown)
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算语言学
- **Abstract**: We introduce Qwen2.5-VL, the latest flagship model of Qwen vision-language series, which demonstrates significant advancements in both foundational capabilities and innovative functionalities. Qwen2.5-VL achieves a major leap forward in understanding and interacting with the world through enhanced visual recognition, precise object localization, robust document parsing, and long-video comprehension. A standout feature of Qwen2.5-VL is its ability to localize objects using bounding boxes or points accurately. It provides robust structured data extraction from invoices, forms, and tables, as well as detailed analysis of charts, diagrams, and layouts. To handle complex inputs, Qwen2.5-VL introduces dynamic resolution processing and absolute time encoding, enabling it to process images of varying sizes and videos of extended durations (up to hours) with second-level event localization. This allows the model to natively perceive spatial scales and temporal dynamics without relying on traditional normalization techniques. By training a native dynamic-resolution Vision Transformer (ViT) from scratch and incorporating Window Attention, we reduce computational overhead while maintaining native resolution. As a result, Qwen2.5-VL excels not only in static image and document understanding but also as an interactive visual agent capable of reasoning, tool usage, and task execution in real-world scenarios such as operating computers and mobile devices. Qwen2.5-VL is available in three sizes, addressing diverse use cases from edge AI to high-performance computing. The flagship Qwen2.5-VL-72B model matches state-of-the-art models like GPT-4o and Claude 3.5 Sonnet, particularly excelling in document and diagram understanding. Additionally, Qwen2.5-VL maintains robust linguistic performance, preserving the core language competencies of the Qwen2.5 LLM.

### Multi-view Video-Pose Pretraining for Operating Room Surgical Activity Recognition 
[[arxiv](https://arxiv.org/abs/2502.13883)] [[cool](https://papers.cool/arxiv/2502.13883)] [[pdf](https://arxiv.org/pdf/2502.13883)]
> **Authors**: Idris Hamoud,Vinkle Srivastav,Muhammad Abdullah Jamal,Didier Mutter,Omid Mohareri,Nicolas Padoy
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Understanding the workflow of surgical procedures in complex operating rooms requires a deep understanding of the interactions between clinicians and their environment. Surgical activity recognition (SAR) is a key computer vision task that detects activities or phases from multi-view camera recordings. Existing SAR models often fail to account for fine-grained clinician movements and multi-view knowledge, or they require calibrated multi-view camera setups and advanced point-cloud processing to obtain better results. In this work, we propose a novel calibration-free multi-view multi-modal pretraining framework called Multiview Pretraining for Video-Pose Surgical Activity Recognition PreViPS, which aligns 2D pose and vision embeddings across camera views. Our model follows CLIP-style dual-encoder architecture: one encoder processes visual features, while the other encodes human pose embeddings. To handle the continuous 2D human pose coordinates, we introduce a tokenized discrete representation to convert the continuous 2D pose coordinates into discrete pose embeddings, thereby enabling efficient integration within the dual-encoder framework. To bridge the gap between these two modalities, we propose several pretraining objectives using cross- and in-modality geometric constraints within the embedding space and incorporating masked pose token prediction strategy to enhance representation learning. Extensive experiments and ablation studies demonstrate improvements over the strong baselines, while data-efficiency experiments on two distinct operating room datasets further highlight the effectiveness of our approach. We highlight the benefits of our approach for surgical activity recognition in both multi-view and single-view settings, showcasing its practical applicability in complex surgical environments. Code will be made available at: https://github.com/CAMMA-public/PreViPS.

### MagicGeo: Training-Free Text-Guided Geometric Diagram Generation 
[[arxiv](https://arxiv.org/abs/2502.13855)] [[cool](https://papers.cool/arxiv/2502.13855)] [[pdf](https://arxiv.org/pdf/2502.13855)]
> **Authors**: Junxiao Wang,Ting Zhang,Heng Yu,Jingdong Wang,Hua Huang
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Geometric diagrams are critical in conveying mathematical and scientific concepts, yet traditional diagram generation methods are often manual and resource-intensive. While text-to-image generation has made strides in photorealistic imagery, creating accurate geometric diagrams remains a challenge due to the need for precise spatial relationships and the scarcity of geometry-specific datasets. This paper presents MagicGeo, a training-free framework for generating geometric diagrams from textual descriptions. MagicGeo formulates the diagram generation process as a coordinate optimization problem, ensuring geometric correctness through a formal language solver, and then employs coordinate-aware generation. The framework leverages the strong language translation capability of large language models, while formal mathematical solving ensures geometric correctness. We further introduce MagicGeoBench, a benchmark dataset of 220 geometric diagram descriptions, and demonstrate that MagicGeo outperforms current methods in both qualitative and quantitative evaluations. This work provides a scalable, accurate solution for automated diagram generation, with significant implications for educational and academic applications.

### Building Age Estimation: A New Multi-Modal Benchmark Dataset and Community Challenge 
[[arxiv](https://arxiv.org/abs/2502.13818)] [[cool](https://papers.cool/arxiv/2502.13818)] [[pdf](https://arxiv.org/pdf/2502.13818)]
> **Authors**: Nikolaos Dionelis,Nicolas Longépé,Alessandra Feliciotti,Mattia Marconcini,Devis Peressutti,Nika Oman Kadunc,JaeWan Park,Hagai Raja Sinulingga,Steve Andreas Immanuel,Ba Tran,Caroline Arnold
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 6 pages, 12 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Estimating the construction year of buildings is of great importance for sustainability. Sustainable buildings minimize energy consumption and are a key part of responsible and sustainable urban planning and development to effectively combat climate change. By using Artificial Intelligence (AI) and recently proposed Transformer models, we are able to estimate the construction epoch of buildings from a multi-modal dataset. In this paper, we introduce a new benchmark multi-modal dataset, i.e. the Map your City Dataset (MyCD), containing top-view Very High Resolution (VHR) images, Earth Observation (EO) multi-spectral data from the Copernicus Sentinel-2 satellite constellation, and street-view images in many different cities in Europe, co-localized with respect to the building under study and labelled with the construction epoch. We assess EO generalization performance on new/ previously unseen cities that have been held-out from training and appear only during inference. In this work, we present the community-based data challenge we organized based on MyCD. The ESA AI4EO Challenge MapYourCity was opened in 2024 for 4 months. Here, we present the Top-4 performing models, and the main evaluation results. During inference, the performance of the models using both all three input modalities and only the two top-view modalities, i.e. without the street-view images, is examined. The evaluation results show that the models are effective and can achieve good performance on this difficult real-world task of estimating the age of buildings, even on previously unseen cities, as well as even using only the two top-view modalities (i.e. VHR and Sentinel-2) during inference.

### From Correctness to Comprehension: AI Agents for Personalized Error Diagnosis in Education 
[[arxiv](https://arxiv.org/abs/2502.13789)] [[cool](https://papers.cool/arxiv/2502.13789)] [[pdf](https://arxiv.org/pdf/2502.13789)]
> **Authors**: Yi-Fan Zhang,Hang Li,Dingjie Song,Lichao Sun,Tianlong Xu,Qingsong Wen
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Large Language Models (LLMs), such as GPT-4, have demonstrated impressive mathematical reasoning capabilities, achieving near-perfect performance on benchmarks like GSM8K. However, their application in personalized education remains limited due to an overemphasis on correctness over error diagnosis and feedback generation. Current models fail to provide meaningful insights into the causes of student mistakes, limiting their utility in educational contexts. To address these challenges, we present three key contributions. First, we introduce \textbf{MathCCS} (Mathematical Classification and Constructive Suggestions), a multi-modal benchmark designed for systematic error analysis and tailored feedback. MathCCS includes real-world problems, expert-annotated error categories, and longitudinal student data. Evaluations of state-of-the-art models, including \textit{Qwen2-VL}, \textit{LLaVA-OV}, \textit{Claude-3.5-Sonnet} and \textit{GPT-4o}, reveal that none achieved classification accuracy above 30\% or generated high-quality suggestions (average scores below 4/10), highlighting a significant gap from human-level performance. Second, we develop a sequential error analysis framework that leverages historical data to track trends and improve diagnostic precision. Finally, we propose a multi-agent collaborative framework that combines a Time Series Agent for historical analysis and an MLLM Agent for real-time refinement, enhancing error classification and feedback generation. Together, these contributions provide a robust platform for advancing personalized education, bridging the gap between current AI capabilities and the demands of real-world teaching.

### An Overall Real-Time Mechanism for Classification and Quality Evaluation of Rice 
[[arxiv](https://arxiv.org/abs/2502.13764)] [[cool](https://papers.cool/arxiv/2502.13764)] [[pdf](https://arxiv.org/pdf/2502.13764)]
> **Authors**: Wanke Xia,Ruoxin Peng,Haoqi Chu,Xinlei Zhu,Zhiyu Yang,Yaojun Wang
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Rice is one of the most widely cultivated crops globally and has been developed into numerous varieties. The quality of rice during cultivation is primarily determined by its cultivar and characteristics. Traditionally, rice classification and quality assessment rely on manual visual inspection, a process that is both time-consuming and prone to errors. However, with advancements in machine vision technology, automating rice classification and quality evaluation based on its cultivar and characteristics has become increasingly feasible, enhancing both accuracy and efficiency. This study proposes a real-time evaluation mechanism for comprehensive rice grain assessment, integrating a one-stage object detection approach, a deep convolutional neural network, and traditional machine learning techniques. The proposed framework enables rice variety identification, grain completeness grading, and grain chalkiness evaluation. The rice grain dataset used in this study comprises approximately 20,000 images from six widely cultivated rice varieties in China. Experimental results demonstrate that the proposed mechanism achieves a mean average precision (mAP) of 99.14% in the object detection task and an accuracy of 97.89% in the classification task. Furthermore, the framework attains an average accuracy of 97.56% in grain completeness grading within the same rice variety, contributing to an effective quality evaluation system.

### Capturing Rich Behavior Representations: A Dynamic Action Semantic-Aware Graph Transformer for Video Captioning 
[[arxiv](https://arxiv.org/abs/2502.13754)] [[cool](https://papers.cool/arxiv/2502.13754)] [[pdf](https://arxiv.org/pdf/2502.13754)]
> **Authors**: Caihua Liu,Xu Li,Wenjing Xue,Wei Tang,Xia Feng
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 5 pages, 3 figures, published ICASSP
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Existing video captioning methods merely provide shallow or simplistic representations of object behaviors, resulting in superficial and ambiguous descriptions. However, object behavior is dynamic and complex. To comprehensively capture the essence of object behavior, we propose a dynamic action semantic-aware graph transformer. Firstly, a multi-scale temporal modeling module is designed to flexibly learn long and short-term latent action features. It not only acquires latent action features across time scales, but also considers local latent action details, enhancing the coherence and sensitiveness of latent action representations. Secondly, a visual-action semantic aware module is proposed to adaptively capture semantic representations related to object behavior, enhancing the richness and accurateness of action representations. By harnessing the collaborative efforts of these two modules,we can acquire rich behavior representations to generate human-like natural descriptions. Finally, this rich behavior representations and object representations are used to construct a temporal objects-action graph, which is fed into the graph transformer to model the complex temporal dependencies between objects and actions. To avoid adding complexity in the inference phase, the behavioral knowledge of the objects will be distilled into a simple network through knowledge distillation. The experimental results on MSVD and MSR-VTT datasets demonstrate that the proposed method achieves significant performance improvements across multiple metrics.

### CARE: Confidence-Aware Regression Estimation of building density fine-tuning EO Foundation Models 
[[arxiv](https://arxiv.org/abs/2502.13734)] [[cool](https://papers.cool/arxiv/2502.13734)] [[pdf](https://arxiv.org/pdf/2502.13734)]
> **Authors**: Nikolaos Dionelis,Jente Bosmans,Nicolas Longépé
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 5 pages, 3 figures, Submitted
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Performing accurate confidence quantification and assessment is important for deep neural networks to predict their failures, improve their performance and enhance their capabilities in real-world applications, for their practical deployment in real life. For pixel-wise regression tasks, confidence quantification and assessment has not been well addressed in the literature, in contrast to classification tasks like semantic segmentation. The softmax output layer is not used in deep neural networks that solve pixel-wise regression problems. In this paper, to address these problems, we develop, train and evaluate the proposed model Confidence-Aware Regression Estimation (CARE). Our model CARE computes and assigns confidence to regression output results. We focus on solving regression problems as downstream tasks of an AI Foundation Model for Earth Observation (EO). We evaluate the proposed model CARE and experimental results on data from the Copernicus Sentinel-2 satellite constellation for estimating the density of buildings show that the proposed method can be successfully applied to regression problems. We also show that our approach outperforms other methods.

### Medical Image Classification with KAN-Integrated Transformers and Dilated Neighborhood Attention 
[[arxiv](https://arxiv.org/abs/2502.13693)] [[cool](https://papers.cool/arxiv/2502.13693)] [[pdf](https://arxiv.org/pdf/2502.13693)]
> **Authors**: Omid Nejati Manzari,Hojat Asgariandehkordi,Taha Koleilat,Yiming Xiao,Hassan Rivaz
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Convolutional networks, transformers, hybrid models, and Mamba-based architectures have demonstrated strong performance across various medical image classification tasks. However, these methods were primarily designed to classify clean images using labeled data. In contrast, real-world clinical data often involve image corruptions that are unique to multi-center studies and stem from variations in imaging equipment across manufacturers. In this paper, we introduce the Medical Vision Transformer (MedViTV2), a novel architecture incorporating Kolmogorov-Arnold Network (KAN) layers into the transformer architecture for the first time, aiming for generalized medical image classification. We have developed an efficient KAN block to reduce computational load while enhancing the accuracy of the original MedViT. Additionally, to counteract the fragility of our MedViT when scaled up, we propose an enhanced Dilated Neighborhood Attention (DiNA), an adaptation of the efficient fused dot-product attention kernel capable of capturing global context and expanding receptive fields to scale the model effectively and addressing feature collapse issues. Moreover, a hierarchical hybrid strategy is introduced to stack our Local Feature Perception and Global Feature Perception blocks in an efficient manner, which balances local and global feature perceptions to boost performance. Extensive experiments on 17 medical image classification datasets and 12 corrupted medical image datasets demonstrate that MedViTV2 achieved state-of-the-art results in 27 out of 29 experiments with reduced computational complexity. MedViTV2 is 44\% more computationally efficient than the previous version and significantly enhances accuracy, achieving improvements of 4.6\% on MedMNIST, 5.8\% on NonMNIST, and 13.4\% on the MedMNIST-C benchmark.

### Exploring Mutual Cross-Modal Attention for Context-Aware Human Affordance Generation 
[[arxiv](https://arxiv.org/abs/2502.13637)] [[cool](https://papers.cool/arxiv/2502.13637)] [[pdf](https://arxiv.org/pdf/2502.13637)]
> **Authors**: Prasun Roy,Saumik Bhattacharya,Subhankar Ghosh,Umapada Pal,Michael Blumenstein
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 11 pages
- **标题**: None
- **领域**: 计算机视觉和模式识别,多媒体
- **Abstract**: Human affordance learning investigates contextually relevant novel pose prediction such that the estimated pose represents a valid human action within the scene. While the task is fundamental to machine perception and automated interactive navigation agents, the exponentially large number of probable pose and action variations make the problem challenging and non-trivial. However, the existing datasets and methods for human affordance prediction in 2D scenes are significantly limited in the literature. In this paper, we propose a novel cross-attention mechanism to encode the scene context for affordance prediction by mutually attending spatial feature maps from two different modalities. The proposed method is disentangled among individual subtasks to efficiently reduce the problem complexity. First, we sample a probable location for a person within the scene using a variational autoencoder (VAE) conditioned on the global scene context encoding. Next, we predict a potential pose template from a set of existing human pose candidates using a classifier on the local context encoding around the predicted location. In the subsequent steps, we use two VAEs to sample the scale and deformation parameters for the predicted pose template by conditioning on the local context and template class. Our experiments show significant improvements over the previous baseline of human affordance injection into complex 2D scenes.

### CardiacMamba: A Multimodal RGB-RF Fusion Framework with State Space Models for Remote Physiological Measurement 
[[arxiv](https://arxiv.org/abs/2502.13624)] [[cool](https://papers.cool/arxiv/2502.13624)] [[pdf](https://arxiv.org/pdf/2502.13624)]
> **Authors**: Zheng Wu,Yiping Xie,Bo Zhao,Jiguang He,Fei Luo,Ning Deng,Zitong Yu
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Heart rate (HR) estimation via remote photoplethysmography (rPPG) offers a non-invasive solution for health monitoring. However, traditional single-modality approaches (RGB or Radio Frequency (RF)) face challenges in balancing robustness and accuracy due to lighting variations, motion artifacts, and skin tone bias. In this paper, we propose CardiacMamba, a multimodal RGB-RF fusion framework that leverages the complementary strengths of both modalities. It introduces the Temporal Difference Mamba Module (TDMM) to capture dynamic changes in RF signals using timing differences between frames, enhancing the extraction of local and global features. Additionally, CardiacMamba employs a Bidirectional SSM for cross-modal alignment and a Channel-wise Fast Fourier Transform (CFFT) to effectively capture and refine the frequency domain characteristics of RGB and RF signals, ultimately improving heart rate estimation accuracy and periodicity detection. Extensive experiments on the EquiPleth dataset demonstrate state-of-the-art performance, achieving marked improvements in accuracy and robustness. CardiacMamba significantly mitigates skin tone bias, reducing performance disparities across demographic groups, and maintains resilience under missing-modality scenarios. By addressing critical challenges in fairness, adaptability, and precision, the framework advances rPPG technology toward reliable real-world deployment in healthcare. The codes are available at: https://github.com/WuZheng42/CardiacMamba.

### MobileViM: A Light-weight and Dimension-independent Vision Mamba for 3D Medical Image Analysis 
[[arxiv](https://arxiv.org/abs/2502.13524)] [[cool](https://papers.cool/arxiv/2502.13524)] [[pdf](https://arxiv.org/pdf/2502.13524)]
> **Authors**: Wei Dai,Steven Wang,Jun Liu
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: The code is accessible through: https://github.com/anthonyweidai/MobileViM_3D/
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习,网络和互联网架构
- **Abstract**: Efficient evaluation of three-dimensional (3D) medical images is crucial for diagnostic and therapeutic practices in healthcare. Recent years have seen a substantial uptake in applying deep learning and computer vision to analyse and interpret medical images. Traditional approaches, such as convolutional neural networks (CNNs) and vision transformers (ViTs), face significant computational challenges, prompting the need for architectural advancements. Recent efforts have led to the introduction of novel architectures like the ``Mamba'' model as alternative solutions to traditional CNNs or ViTs. The Mamba model excels in the linear processing of one-dimensional data with low computational demands. However, Mamba's potential for 3D medical image analysis remains underexplored and could face significant computational challenges as the dimension increases. This manuscript presents MobileViM, a streamlined architecture for efficient segmentation of 3D medical images. In the MobileViM network, we invent a new dimension-independent mechanism and a dual-direction traversing approach to incorporate with a vision-Mamba-based framework. MobileViM also features a cross-scale bridging technique to improve efficiency and accuracy across various medical imaging modalities. With these enhancements, MobileViM achieves segmentation speeds exceeding 90 frames per second (FPS) on a single graphics processing unit (i.e., NVIDIA RTX 4090). This performance is over 24 FPS faster than the state-of-the-art deep learning models for processing 3D images with the same computational resources. In addition, experimental evaluations demonstrate that MobileViM delivers superior performance, with Dice similarity scores reaching 92.72%, 86.69%, 80.46%, and 77.43% for PENGWIN, BraTS2024, ATLAS, and Toothfairy2 datasets, respectively, which significantly surpasses existing models.

### Enhancing Chest X-ray Classification through Knowledge Injection in Cross-Modality Learning 
[[arxiv](https://arxiv.org/abs/2502.13447)] [[cool](https://papers.cool/arxiv/2502.13447)] [[pdf](https://arxiv.org/pdf/2502.13447)]
> **Authors**: Yang Yan,Bingqing Yue,Qiaxuan Li,Man Huang,Jingyu Chen,Zhenzhong Lan
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: Accepted by ICASSP'25
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算语言学
- **Abstract**: The integration of artificial intelligence in medical imaging has shown tremendous potential, yet the relationship between pre-trained knowledge and performance in cross-modality learning remains unclear. This study investigates how explicitly injecting medical knowledge into the learning process affects the performance of cross-modality classification, focusing on Chest X-ray (CXR) images. We introduce a novel Set Theory-based knowledge injection framework that generates captions for CXR images with controllable knowledge granularity. Using this framework, we fine-tune CLIP model on captions with varying levels of medical information. We evaluate the model's performance through zero-shot classification on the CheXpert dataset, a benchmark for CXR classification. Our results demonstrate that injecting fine-grained medical knowledge substantially improves classification accuracy, achieving 72.5\% compared to 49.9\% when using human-generated captions. This highlights the crucial role of domain-specific knowledge in medical cross-modality learning. Furthermore, we explore the influence of knowledge density and the use of domain-specific Large Language Models (LLMs) for caption generation, finding that denser knowledge and specialized LLMs contribute to enhanced performance. This research advances medical image analysis by demonstrating the effectiveness of knowledge injection for improving automated CXR classification, paving the way for more accurate and reliable diagnostic tools.

## 计算机与社会(cs.CY:Computers and Society)

### Personalized Education with Generative AI and Digital Twins: VR, RAG, and Zero-Shot Sentiment Analysis for Industry 4.0 Workforce Development 
[[arxiv](https://arxiv.org/abs/2502.14080)] [[cool](https://papers.cool/arxiv/2502.14080)] [[pdf](https://arxiv.org/pdf/2502.14080)]
> **Authors**: Yu-Zheng Lin,Karan Petal,Ahmed H Alhamadah,Sujan Ghimire,Matthew William Redondo,David Rafael Vidal Corona,Jesus Pacheco,Soheil Salehi,Pratik Satam
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: The Fourth Industrial Revolution (4IR) technologies, such as cloud computing, machine learning, and AI, have improved productivity but introduced challenges in workforce training and reskilling. This is critical given existing workforce shortages, especially in marginalized communities like Underrepresented Minorities (URM), who often lack access to quality education. Addressing these challenges, this research presents gAI-PT4I4, a Generative AI-based Personalized Tutor for Industrial 4.0, designed to personalize 4IR experiential learning. gAI-PT4I4 employs sentiment analysis to assess student comprehension, leveraging generative AI and finite automaton to tailor learning experiences. The framework integrates low-fidelity Digital Twins for VR-based training, featuring an Interactive Tutor - a generative AI assistant providing real-time guidance via audio and text. It uses zero-shot sentiment analysis with LLMs and prompt engineering, achieving 86\% accuracy in classifying student-teacher interactions as positive or negative. Additionally, retrieval-augmented generation (RAG) enables personalized learning content grounded in domain-specific knowledge. To adapt training dynamically, finite automaton structures exercises into states of increasing difficulty, requiring 80\% task-performance accuracy for progression. Experimental evaluation with 22 volunteers showed improved accuracy exceeding 80\%, reducing training time. Finally, this paper introduces a Multi-Fidelity Digital Twin model, aligning Digital Twin complexity with Bloom's Taxonomy and Kirkpatrick's model, providing a scalable educational framework.

## 数据库(cs.DB:Databases)

### Enhancing Pavement Sensor Data Acquisition for AI-Driven Transportation Research 
[[arxiv](https://arxiv.org/abs/2502.14222)] [[cool](https://papers.cool/arxiv/2502.14222)] [[pdf](https://arxiv.org/pdf/2502.14222)]
> **Authors**: Manish Kumar Krishne Gowda,Andrew Balmos,Shin Boonam,James V. Krogmeier
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: This paper was accepted for presentation at the 104th TRB Annual Meeting, held on January 5-9, 2025, in Washington, D.C., and was presented during the poster session on January 8, 2025
- **标题**: None
- **领域**: 数据库,人工智能,信号处理
- **Abstract**: Effective strategies for sensor data management are essential for advancing transportation research, especially in the current data-driven era, due to the advent of novel applications in artificial intelligence. This paper presents comprehensive guidelines for managing transportation sensor data, encompassing both archived static data and real-time data streams. The real-time system architecture integrates various applications with data acquisition systems (DAQ). By deploying the in-house designed, open-source Avena software platform alongside the NATS messaging system as a secure communication broker, reliable data exchange is ensured. While robust databases like TimescaleDB facilitate organized storage, visualization platforms like Grafana provide real-time monitoring capabilities. In contrast, static data standards address the challenges in handling unstructured, voluminous datasets. The standards advocate for a combination of cost-effective bulk cloud storage for unprocessed sensor data and relational databases for recording summarized analyses. They highlight the role of cloud data transfer tools like FME for efficient migration of sensor data from local storages onto the cloud. Further, integration of robust visualization tools into the framework helps in deriving patterns and trends from these complex datasets. The proposals were applied to INDOT's real-world case studies involving the I-65 and I-69 Greenfield districts. For real-time data collection, Campbell Scientific DAQ systems were used, enabling continuous generation and monitoring of sensor metrics. In the case of the archived I-69 database, summary data was compiled in Oracle, while the unprocessed data was stored in SharePoint. The results underline the effectiveness of the proposed guidelines and motivate their adoption in research projects.

### AnDB: Breaking Boundaries with an AI-Native Database for Universal Semantic Analysis 
[[arxiv](https://arxiv.org/abs/2502.13805)] [[cool](https://papers.cool/arxiv/2502.13805)] [[pdf](https://arxiv.org/pdf/2502.13805)]
> **Authors**: Tianqing Wang,Xun Xue,Guoliang Li,Yong Wang
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 4 pages, 5 figures, conference
- **标题**: None
- **领域**: 数据库,人工智能,机器学习
- **Abstract**: In this demonstration, we present AnDB, an AI-native database that supports traditional OLTP workloads and innovative AI-driven tasks, enabling unified semantic analysis across structured and unstructured data. While structured data analytics is mature, challenges remain in bridging the semantic gap between user queries and unstructured data. AnDB addresses these issues by leveraging cutting-edge AI-native technologies, allowing users to perform semantic queries using intuitive SQL-like statements without requiring AI expertise. This approach eliminates the ambiguity of traditional text-to-SQL systems and provides a seamless end-to-end optimization for analyzing all data types. AnDB automates query processing by generating multiple execution plans and selecting the optimal one through its optimizer, which balances accuracy, execution time, and financial cost based on user policies and internal optimizing mechanisms. AnDB future-proofs data management infrastructure, empowering users to effectively and efficiently harness the full potential of all kinds of data without starting from scratch.

## 数据结构和算法(cs.DS:Data Structures and Algorithms)

### A Query-Driven Approach to Space-Efficient Range Searching 
[[arxiv](https://arxiv.org/abs/2502.13653)] [[cool](https://papers.cool/arxiv/2502.13653)] [[pdf](https://arxiv.org/pdf/2502.13653)]
> **Authors**: Dimitris Fotakis,Andreas Kalavas,Ioannis Psarros
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 16 pages, 2 figures
- **标题**: None
- **领域**: 数据结构和算法,计算几何,机器学习
- **Abstract**: We initiate a study of a query-driven approach to designing partition trees for range-searching problems. Our model assumes that a data structure is to be built for an unknown query distribution that we can access through a sampling oracle, and must be selected such that it optimizes a meaningful performance parameter on expectation. Our first contribution is to show that a near-linear sample of queries allows the construction of a partition tree with a near-optimal expected number of nodes visited during querying. We enhance this approach by treating node processing as a classification problem, leveraging fast classifiers like shallow neural networks to obtain experimentally efficient query times. Our second contribution is to develop partition trees using sparse geometric separators. Our preprocessing algorithm, based on a sample of queries, builds a balanced tree with nodes associated with separators that minimize query stabs on expectation; this yields both fast processing of each node and a small number of visited nodes, significantly reducing query time.

## 图形(cs.GR:Graphics)

### Pandora3D: A Comprehensive Framework for High-Quality 3D Shape and Texture Generation 
[[arxiv](https://arxiv.org/abs/2502.14247)] [[cool](https://papers.cool/arxiv/2502.14247)] [[pdf](https://arxiv.org/pdf/2502.14247)]
> **Authors**: Jiayu Yang,Taizhang Shang,Weixuan Sun,Xibin Song,Ziang Cheng,Senbo Wang,Shenzhou Chen,Weizhe Liu,Hongdong Li,Pan Ji
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: Tencent XR 3D Gen
- **标题**: None
- **领域**: 图形,人工智能,计算机视觉和模式识别
- **Abstract**: This report presents a comprehensive framework for generating high-quality 3D shapes and textures from diverse input prompts, including single images, multi-view images, and text descriptions. The framework consists of 3D shape generation and texture generation. (1). The 3D shape generation pipeline employs a Variational Autoencoder (VAE) to encode implicit 3D geometries into a latent space and a diffusion network to generate latents conditioned on input prompts, with modifications to enhance model capacity. An alternative Artist-Created Mesh (AM) generation approach is also explored, yielding promising results for simpler geometries. (2). Texture generation involves a multi-stage process starting with frontal images generation followed by multi-view images generation, RGB-to-PBR texture conversion, and high-resolution multi-view texture refinement. A consistency scheduler is plugged into every stage, to enforce pixel-wise consistency among multi-view textures during inference, ensuring seamless integration. The pipeline demonstrates effective handling of diverse input formats, leveraging advanced neural architectures and novel methodologies to produce high-quality 3D content. This report details the system architecture, experimental results, and potential future directions to improve and expand the framework. The source code and pretrained weights are released at: https://github.com/Tencent/Tencent-XR-3DGen.

### Appeal prediction for AI up-scaled Images 
[[arxiv](https://arxiv.org/abs/2502.14013)] [[cool](https://papers.cool/arxiv/2502.14013)] [[pdf](https://arxiv.org/pdf/2502.14013)]
> **Authors**: Steve Göring,Rasmus Merten,Alexander Raake
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 图形,人工智能,图像和视频处理
- **Abstract**: DNN- or AI-based up-scaling algorithms are gaining in popularity due to the improvements in machine learning. Various up-scaling models using CNNs, GANs or mixed approaches have been published. The majority of models are evaluated using PSRN and SSIM or only a few example images. However, a performance evaluation with a wide range of real-world images and subjective evaluation is missing, which we tackle in the following paper. For this reason, we describe our developed dataset, which uses 136 base images and five different up-scaling methods, namely Real-ESRGAN, BSRGAN, waifu2x, KXNet, and Lanczos. Overall the dataset consists of 1496 annotated images. The labeling of our dataset focused on image appeal and has been performed using crowd-sourcing employing our open-source tool AVRate Voyager. We evaluate the appeal of the different methods, and the results indicate that Real-ESRGAN and BSRGAN are the best. Furthermore, we train a DNN to detect which up-scaling method has been used, the trained models have a good overall performance in our evaluation. In addition to this, we evaluate state-of-the-art image appeal and quality models, here none of the models showed a high prediction performance, therefore we also trained two own approaches. The first uses transfer learning and has the best performance, and the second model uses signal-based features and a random forest model with good overall performance. We share the data and implementation to allow further research in the context of open science.

### Inter3D: A Benchmark and Strong Baseline for Human-Interactive 3D Object Reconstruction 
[[arxiv](https://arxiv.org/abs/2502.14004)] [[cool](https://papers.cool/arxiv/2502.14004)] [[pdf](https://arxiv.org/pdf/2502.14004)]
> **Authors**: Gan Chen,Ying He,Mulin Yu,F. Richard Yu,Gang Xu,Fei Ma,Ming Li,Guang Zhou
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 图形,机器学习
- **Abstract**: Recent advancements in implicit 3D reconstruction methods, e.g., neural rendering fields and Gaussian splatting, have primarily focused on novel view synthesis of static or dynamic objects with continuous motion states. However, these approaches struggle to efficiently model a human-interactive object with n movable parts, requiring 2^n separate models to represent all discrete states. To overcome this limitation, we propose Inter3D, a new benchmark and approach for novel state synthesis of human-interactive objects. We introduce a self-collected dataset featuring commonly encountered interactive objects and a new evaluation pipeline, where only individual part states are observed during training, while part combination states remain unseen. We also propose a strong baseline approach that leverages Space Discrepancy Tensors to efficiently modelling all states of an object. To alleviate the impractical constraints on camera trajectories across training states, we propose a Mutual State Regularization mechanism to enhance the spatial density consistency of movable parts. In addition, we explore two occupancy grid sampling strategies to facilitate training efficiency. We conduct extensive experiments on the proposed benchmark, showcasing the challenges of the task and the superiority of our approach.

### FantasyID: Face Knowledge Enhanced ID-Preserving Video Generation 
[[arxiv](https://arxiv.org/abs/2502.13995)] [[cool](https://papers.cool/arxiv/2502.13995)] [[pdf](https://arxiv.org/pdf/2502.13995)]
> **Authors**: Yunpeng Zhang,Qiang Wang,Fan Jiang,Yaqi Fan,Mu Xu,Yonggang Qi
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 图形,计算机视觉和模式识别
- **Abstract**: Tuning-free approaches adapting large-scale pre-trained video diffusion models for identity-preserving text-to-video generation (IPT2V) have gained popularity recently due to their efficacy and scalability. However, significant challenges remain to achieve satisfied facial dynamics while keeping the identity unchanged. In this work, we present a novel tuning-free IPT2V framework by enhancing face knowledge of the pre-trained video model built on diffusion transformers (DiT), dubbed FantasyID. Essentially, 3D facial geometry prior is incorporated to ensure plausible facial structures during video synthesis. To prevent the model from learning copy-paste shortcuts that simply replicate reference face across frames, a multi-view face augmentation strategy is devised to capture diverse 2D facial appearance features, hence increasing the dynamics over the facial expressions and head poses. Additionally, after blending the 2D and 3D features as guidance, instead of naively employing cross-attention to inject guidance cues into DiT layers, a learnable layer-aware adaptive mechanism is employed to selectively inject the fused features into each individual DiT layers, facilitating balanced modeling of identity preservation and motion dynamics. Experimental results validate our model's superiority over the current tuning-free IPT2V methods.

### GPU-Friendly Laplacian Texture Blending 
[[arxiv](https://arxiv.org/abs/2502.13945)] [[cool](https://papers.cool/arxiv/2502.13945)] [[pdf](https://arxiv.org/pdf/2502.13945)]
> **Authors**: Bartlomiej Wronski
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 19 pages, 13 figures, Journal of Computer Graphics Techniques (JCGT)
- **标题**: None
- **领域**: 图形,计算机视觉和模式识别
- **Abstract**: Texture and material blending is one of the leading methods for adding variety to rendered virtual worlds, creating composite materials, and generating procedural content. When done naively, it can introduce either visible seams or contrast loss, leading to an unnatural look not representative of blended textures. Earlier work proposed addressing this problem through careful manual parameter tuning, lengthy per-texture statistics precomputation, look-up tables, or training deep neural networks. In this work, we propose an alternative approach based on insights from image processing and Laplacian pyramid blending. Our approach does not require any precomputation or increased memory usage (other than the presence of a regular, non-Laplacian, texture mipmap chain), does not produce ghosting, preserves sharp local features, and can run in real time on the GPU at the cost of a few additional lower mipmap texture taps.

## 计算机科学与博弈论(cs.GT:Computer Science and Game Theory)

### Efficient Inverse Multiagent Learning 
[[arxiv](https://arxiv.org/abs/2502.14160)] [[cool](https://papers.cool/arxiv/2502.14160)] [[pdf](https://arxiv.org/pdf/2502.14160)]
> **Authors**: Denizalp Goktas,Amy Greenwald,Sadie Zhao,Alec Koppel,Sumitra Ganesh
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: Paper was submitted to the International Conference onLearningRepresentations (2024) under the title of "Generative Adversarial Inverse MultiagentLearning", and renamed for the camera-ready submission as "Efficient Inverse MultiagentLearning"
- **标题**: None
- **领域**: 计算机科学与博弈论,人工智能,机器学习,理论经济学
- **Abstract**: In this paper, we study inverse game theory (resp. inverse multiagent learning) in which the goal is to find parameters of a game's payoff functions for which the expected (resp. sampled) behavior is an equilibrium. We formulate these problems as generative-adversarial (i.e., min-max) optimization problems, for which we develop polynomial-time algorithms to solve, the former of which relies on an exact first-order oracle, and the latter, a stochastic one. We extend our approach to solve inverse multiagent simulacral learning in polynomial time and number of samples. In these problems, we seek a simulacrum, meaning parameters and an associated equilibrium that replicate the given observations in expectation. We find that our approach outperforms the widely-used ARIMA method in predicting prices in Spanish electricity markets based on time-series data.

## 人机交互(cs.HC:Human-Computer Interaction)

### Exploring Personalized Health Support through Data-Driven, Theory-Guided LLMs: A Case Study in Sleep Health 
[[arxiv](https://arxiv.org/abs/2502.13920)] [[cool](https://papers.cool/arxiv/2502.13920)] [[pdf](https://arxiv.org/pdf/2502.13920)]
> **Authors**: Xingbo Wang,Janessa Griffith,Daniel A. Adler,Joey Castillo,Tanzeem Choudhury,Fei Wang
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: Accepted to CHI Conference on Human Factors in Computing Systems (CHI 2025)
- **标题**: None
- **领域**: 人机交互,计算语言学
- **Abstract**: Despite the prevalence of sleep-tracking devices, many individuals struggle to translate data into actionable improvements in sleep health. Current methods often provide data-driven suggestions but may not be feasible and adaptive to real-life constraints and individual contexts. We present HealthGuru, a novel large language model-powered chatbot to enhance sleep health through data-driven, theory-guided, and adaptive recommendations with conversational behavior change support. HealthGuru's multi-agent framework integrates wearable device data, contextual information, and a contextual multi-armed bandit model to suggest tailored sleep-enhancing activities. The system facilitates natural conversations while incorporating data-driven insights and theoretical behavior change techniques. Our eight-week in-the-wild deployment study with 16 participants compared HealthGuru to a baseline chatbot. Results show improved metrics like sleep duration and activity scores, higher quality responses, and increased user motivation for behavior change with HealthGuru. We also identify challenges and design considerations for personalization and user engagement in health chatbots.

### Hidden Darkness in LLM-Generated Designs: Exploring Dark Patterns in Ecommerce Web Components Generated by LLMs 
[[arxiv](https://arxiv.org/abs/2502.13499)] [[cool](https://papers.cool/arxiv/2502.13499)] [[pdf](https://arxiv.org/pdf/2502.13499)]
> **Authors**: Ziwei Chen,Jiawen Shen,Luna,Kristen Vaccaro
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 15 pages
- **标题**: None
- **领域**: 人机交互,人工智能,机器学习
- **Abstract**: Recent work has highlighted the risks of LLM-generated content for a wide range of harmful behaviors, including incorrect and harmful code. In this work, we extend this by studying whether LLM-generated web design contains dark patterns. This work evaluated designs of ecommerce web components generated by four popular LLMs: Claude, GPT, Gemini, and Llama. We tested 13 commonly used ecommerce components (e.g., search, product reviews) and used them as prompts to generate a total of 312 components across all models. Over one-third of generated components contain at least one dark pattern. The majority of dark pattern strategies involve hiding crucial information, limiting users' actions, and manipulating them into making decisions through a sense of urgency. Dark patterns are also more frequently produced in components that are related to company interests. These findings highlight the need for interventions to prevent dark patterns during front-end code generation with LLMs and emphasize the importance of expanding ethical design education to a broader audience.

## 信息检索(cs.IR:Information Retrieval)

### Lost in Sequence: Do Large Language Models Understand Sequential Recommendation? 
[[arxiv](https://arxiv.org/abs/2502.13909)] [[cool](https://papers.cool/arxiv/2502.13909)] [[pdf](https://arxiv.org/pdf/2502.13909)]
> **Authors**: Sein Kim,Hongseok Kang,Kibum Kim,Jiwan Kim,Donghyun Kim,Minchul Yang,Kwangjin Oh,Julian McAuley,Chanyoung Park
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,人工智能
- **Abstract**: Large Language Models (LLMs) have recently emerged as promising tools for recommendation thanks to their advanced textual understanding ability and context-awareness. Despite the current practice of training and evaluating LLM-based recommendation (LLM4Rec) models under a sequential recommendation scenario, we found that whether these models understand the sequential information inherent in users' item interaction sequences has been largely overlooked. In this paper, we first demonstrate through a series of experiments that existing LLM4Rec models do not fully capture sequential information both during training and inference. Then, we propose a simple yet effective LLM-based sequential recommender, called LLM-SRec, a method that enhances the integration of sequential information into LLMs by distilling the user representations extracted from a pre-trained CF-SRec model into LLMs. Our extensive experiments show that LLM-SRec enhances LLMs' ability to understand users' item interaction sequences, ultimately leading to improved recommendation performance. Furthermore, unlike existing LLM4Rec models that require fine-tuning of LLMs, LLM-SRec achieves state-of-the-art performance by training only a few lightweight MLPs, highlighting its practicality in real-world applications. Our code is available at https://github.com/Sein-Kim/LLM-SRec.

### Enhancing LLM-Based Recommendations Through Personalized Reasoning 
[[arxiv](https://arxiv.org/abs/2502.13845)] [[cool](https://papers.cool/arxiv/2502.13845)] [[pdf](https://arxiv.org/pdf/2502.13845)]
> **Authors**: Jiahao Liu,Xueshuo Yan,Dongsheng Li,Guangping Zhang,Hansu Gu,Peng Zhang,Tun Lu,Li Shang,Ning Gu
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 7 pages, under review
- **标题**: None
- **领域**: 信息检索,人工智能
- **Abstract**: Current recommendation systems powered by large language models (LLMs) often underutilize their reasoning capabilities due to a lack of explicit logical structuring. To address this limitation, we introduce CoT-Rec, a framework that integrates Chain-of-Thought (CoT) reasoning into LLM-driven recommendations by incorporating two crucial processes: user preference analysis and item perception evaluation. CoT-Rec operates in two key phases: (1) personalized data extraction, where user preferences and item perceptions are identified, and (2) personalized data application, where this information is leveraged to refine recommendations. Our experimental analysis demonstrates that CoT-Rec improves recommendation accuracy by making better use of LLMs' reasoning potential. The implementation is publicly available at https://anonymous.4open.science/r/CoT-Rec.

### Enhancing Cross-Domain Recommendations with Memory-Optimized LLM-Based User Agents 
[[arxiv](https://arxiv.org/abs/2502.13843)] [[cool](https://papers.cool/arxiv/2502.13843)] [[pdf](https://arxiv.org/pdf/2502.13843)]
> **Authors**: Jiahao Liu,Shengkang Gu,Dongsheng Li,Guangping Zhang,Mingzhe Han,Hansu Gu,Peng Zhang,Tun Lu,Li Shang,Ning Gu
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 6 pages, under review
- **标题**: None
- **领域**: 信息检索,人工智能
- **Abstract**: Large Language Model (LLM)-based user agents have emerged as a powerful tool for improving recommender systems by simulating user interactions. However, existing methods struggle with cross-domain scenarios due to inefficient memory structures, leading to irrelevant information retention and failure to account for social influence factors such as popularity. To address these limitations, we introduce AgentCF++, a novel framework featuring a dual-layer memory architecture and a two-step fusion mechanism to filter domain-specific preferences effectively. Additionally, we propose interest groups with shared memory, allowing the model to capture the impact of popularity trends on users with similar interests. Through extensive experiments on multiple cross-domain datasets, AgentCF++ demonstrates superior performance over baseline models, highlighting its effectiveness in refining user behavior simulation for recommender systems. Our code is available at https://anonymous.4open.science/r/AgentCF-plus.

### ActionPiece: Contextually Tokenizing Action Sequences for Generative Recommendation 
[[arxiv](https://arxiv.org/abs/2502.13581)] [[cool](https://papers.cool/arxiv/2502.13581)] [[pdf](https://arxiv.org/pdf/2502.13581)]
> **Authors**: Yupeng Hou,Jianmo Ni,Zhankui He,Noveen Sachdeva,Wang-Cheng Kang,Ed H. Chi,Julian McAuley,Derek Zhiyuan Cheng
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,机器学习
- **Abstract**: Generative recommendation (GR) is an emerging paradigm where user actions are tokenized into discrete token patterns and autoregressively generated as predictions. However, existing GR models tokenize each action independently, assigning the same fixed tokens to identical actions across all sequences without considering contextual relationships. This lack of context-awareness can lead to suboptimal performance, as the same action may hold different meanings depending on its surrounding context. To address this issue, we propose ActionPiece to explicitly incorporate context when tokenizing action sequences. In ActionPiece, each action is represented as a set of item features, which serve as the initial tokens. Given the action sequence corpora, we construct the vocabulary by merging feature patterns as new tokens, based on their co-occurrence frequency both within individual sets and across adjacent sets. Considering the unordered nature of feature sets, we further introduce set permutation regularization, which produces multiple segmentations of action sequences with the same semantics. Experiments on public datasets demonstrate that ActionPiece consistently outperforms existing action tokenization methods, improving NDCG@$10$ by $6.00\%$ to $12.82\%$.

## 机器学习(cs.LG:Machine Learning)

### LabTOP: A Unified Model for Lab Test Outcome Prediction on Electronic Health Records 
[[arxiv](https://arxiv.org/abs/2502.14259)] [[cool](https://papers.cool/arxiv/2502.14259)] [[pdf](https://arxiv.org/pdf/2502.14259)]
> **Authors**: Sujeong Im,Jungwoo Oh,Edward Choi
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 11 pages for main text, 4 pages for appendix
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Lab tests are fundamental for diagnosing diseases and monitoring patient conditions. However, frequent testing can be burdensome for patients, and test results may not always be immediately available. To address these challenges, we propose LabTOP, a unified model that predicts lab test outcomes by leveraging a language modeling approach on EHR data. Unlike conventional methods that estimate only a subset of lab tests or classify discrete value ranges, LabTOP performs continuous numerical predictions for a diverse range of lab items. We evaluate LabTOP on three publicly available EHR datasets and demonstrate that it outperforms existing methods, including traditional machine learning models and state-of-the-art large language models. We also conduct extensive ablation studies to confirm the effectiveness of our design choices. We believe that LabTOP will serve as an accurate and generalizable framework for lab test outcome prediction, with potential applications in clinical decision support and early detection of critical conditions.

### SleepGMUformer: A gated multimodal temporal neural network for sleep staging 
[[arxiv](https://arxiv.org/abs/2502.14227)] [[cool](https://papers.cool/arxiv/2502.14227)] [[pdf](https://arxiv.org/pdf/2502.14227)]
> **Authors**: Chenjun Zhao,Xuesen Niu,Xinglin Yu,Long Chen,Na Lv,Huiyu Zhou,Aite Zhao
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Sleep staging is a key method for assessing sleep quality and diagnosing sleep disorders. However, current deep learning methods face challenges: 1) postfusion techniques ignore the varying contributions of different modalities; 2) unprocessed sleep data can interfere with frequency-domain information. To tackle these issues, this paper proposes a gated multimodal temporal neural network for multidomain sleep data, including heart rate, motion, steps, EEG (Fpz-Cz, Pz-Oz), and EOG from WristHR-Motion-Sleep and SleepEDF-78. The model integrates: 1) a pre-processing module for feature alignment, missing value handling, and EEG de-trending; 2) a feature extraction module for complex sleep features in the time dimension; and 3) a dynamic fusion module for real-time modality weighting.Experiments show classification accuracies of 85.03% on SleepEDF-78 and 94.54% on WristHR-Motion-Sleep datasets. The model handles heterogeneous datasets and outperforms state-of-the-art models by 1.00%-4.00%.

### Rethinking Spiking Neural Networks from an Ensemble Learning Perspective 
[[arxiv](https://arxiv.org/abs/2502.14218)] [[cool](https://papers.cool/arxiv/2502.14218)] [[pdf](https://arxiv.org/pdf/2502.14218)]
> **Authors**: Yongqi Ding,Lin Zuo,Mengmeng Jing,Pei He,Hanpu Deng
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: Published as a conference paper at ICLR 2025
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Spiking neural networks (SNNs) exhibit superior energy efficiency but suffer from limited performance. In this paper, we consider SNNs as ensembles of temporal subnetworks that share architectures and weights, and highlight a crucial issue that affects their performance: excessive differences in initial states (neuronal membrane potentials) across timesteps lead to unstable subnetwork outputs, resulting in degraded performance. To mitigate this, we promote the consistency of the initial membrane potential distribution and output through membrane potential smoothing and temporally adjacent subnetwork guidance, respectively, to improve overall stability and performance. Moreover, membrane potential smoothing facilitates forward propagation of information and backward propagation of gradients, mitigating the notorious temporal gradient vanishing problem. Our method requires only minimal modification of the spiking neurons without adapting the network structure, making our method generalizable and showing consistent performance gains in 1D speech, 2D object, and 3D point cloud recognition tasks. In particular, on the challenging CIFAR10-DVS dataset, we achieved 83.20\% accuracy with only four timesteps. This provides valuable insights into unleashing the potential of SNNs.

### Asymmetric Co-Training for Source-Free Few-Shot Domain Adaptation 
[[arxiv](https://arxiv.org/abs/2502.14214)] [[cool](https://papers.cool/arxiv/2502.14214)] [[pdf](https://arxiv.org/pdf/2502.14214)]
> **Authors**: Gengxu Li,Yuan Wu
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 13 pages
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: Source-free unsupervised domain adaptation (SFUDA) has gained significant attention as an alternative to traditional unsupervised domain adaptation (UDA), which relies on the constant availability of labeled source data. However, SFUDA approaches come with inherent limitations that are frequently overlooked. These challenges include performance degradation when the unlabeled target data fails to meet critical assumptions, such as having a closed-set label distribution identical to that of the source domain, or when sufficient unlabeled target data is unavailable-a common situation in real-world applications. To address these issues, we propose an asymmetric co-training (ACT) method specifically designed for the SFFSDA scenario. SFFSDA presents a more practical alternative to SFUDA, as gathering a few labeled target instances is more feasible than acquiring large volumes of unlabeled target data in many real-world contexts. Our ACT method begins by employing a weak-strong augmentation to enhance data diversity. Then we use a two-step optimization process to train the target model. In the first step, we optimize the label smoothing cross-entropy loss, the entropy of the class-conditional distribution, and the reverse-entropy loss to bolster the model's discriminative ability while mitigating overfitting. The second step focuses on reducing redundancy in the output space by minimizing classifier determinacy disparity. Extensive experiments across four benchmarks demonstrate the superiority of our ACT approach, which outperforms state-of-the-art SFUDA methods and transfer learning techniques. Our findings suggest that adapting a source pre-trained model using only a small amount of labeled target data offers a practical and dependable solution. The code is available at https://github.com/gengxuli/ACT.

### A Non-Asymptotic Theory of Seminorm Lyapunov Stability: From Deterministic to Stochastic Iterative Algorithms 
[[arxiv](https://arxiv.org/abs/2502.14208)] [[cool](https://papers.cool/arxiv/2502.14208)] [[pdf](https://arxiv.org/pdf/2502.14208)]
> **Authors**: Zaiwei Chen,Sheng Zhang,Zhe Zhang,Shaan Ul Haque,Siva Theja Maguluri
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,优化与控制,机器学习
- **Abstract**: We study the problem of solving fixed-point equations for seminorm-contractive operators and establish foundational results on the non-asymptotic behavior of iterative algorithms in both deterministic and stochastic settings. Specifically, in the deterministic setting, we prove a fixed-point theorem for seminorm-contractive operators, showing that iterates converge geometrically to the kernel of the seminorm. In the stochastic setting, we analyze the corresponding stochastic approximation (SA) algorithm under seminorm-contractive operators and Markovian noise, providing a finite-sample analysis for various stepsize choices. A benchmark for equation solving is linear systems of equations, where the convergence behavior of fixed-point iteration is closely tied to the stability of linear dynamical systems. In this special case, our results provide a complete characterization of system stability with respect to a seminorm, linking it to the solution of a Lyapunov equation in terms of positive semi-definite matrices. In the stochastic setting, we establish a finite-sample analysis for linear Markovian SA without requiring the Hurwitzness assumption. Our theoretical results offer a unified framework for deriving finite-sample bounds for various reinforcement learning algorithms in the average reward setting, including TD($λ$) for policy evaluation (which is a special case of solving a Poisson equation) and Q-learning for control.

### Accurate Forgetting for Heterogeneous Federated Continual Learning 
[[arxiv](https://arxiv.org/abs/2502.14205)] [[cool](https://papers.cool/arxiv/2502.14205)] [[pdf](https://arxiv.org/pdf/2502.14205)]
> **Authors**: Abudukelimu Wuerkaixi,Sen Cui,Jingfeng Zhang,Kunda Yan,Bo Han,Gang Niu,Lei Fang,Changshui Zhang,Masashi Sugiyama
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: published in ICLR 2024
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Recent years have witnessed a burgeoning interest in federated learning (FL). However, the contexts in which clients engage in sequential learning remain under-explored. Bridging FL and continual learning (CL) gives rise to a challenging practical problem: federated continual learning (FCL). Existing research in FCL primarily focuses on mitigating the catastrophic forgetting issue of continual learning while collaborating with other clients. We argue that the forgetting phenomena are not invariably detrimental. In this paper, we consider a more practical and challenging FCL setting characterized by potentially unrelated or even antagonistic data/tasks across different clients. In the FL scenario, statistical heterogeneity and data noise among clients may exhibit spurious correlations which result in biased feature learning. While existing CL strategies focus on a complete utilization of previous knowledge, we found that forgetting biased information is beneficial in our study. Therefore, we propose a new concept accurate forgetting (AF) and develop a novel generative-replay method~\method~which selectively utilizes previous knowledge in federated networks. We employ a probabilistic framework based on a normalizing flow model to quantify the credibility of previous knowledge. Comprehensive experiments affirm the superiority of our method over baselines.

### Adaptive Sparsified Graph Learning Framework for Vessel Behavior Anomalies 
[[arxiv](https://arxiv.org/abs/2502.14197)] [[cool](https://papers.cool/arxiv/2502.14197)] [[pdf](https://arxiv.org/pdf/2502.14197)]
> **Authors**: Jeehong Kim,Minchan Kim,Jaeseong Ju,Youngseok Hwang,Wonhee Lee,Hyunwoo Park
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: Anomaly Detection in Scientific Domains AAAI Workshop
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Graph neural networks have emerged as a powerful tool for learning spatiotemporal interactions. However, conventional approaches often rely on predefined graphs, which may obscure the precise relationships being modeled. Additionally, existing methods typically define nodes based on fixed spatial locations, a strategy that is ill-suited for dynamic environments like maritime environments. Our method introduces an innovative graph representation where timestamps are modeled as distinct nodes, allowing temporal dependencies to be explicitly captured through graph edges. This setup is extended to construct a multi-ship graph that effectively captures spatial interactions while preserving graph sparsity. The graph is processed using Graph Convolutional Network layers to capture spatiotemporal patterns, with a forecasting layer for feature prediction and a Variational Graph Autoencoder for reconstruction, enabling robust anomaly detection.

### Federated Fine-Tuning of Large Language Models: Kahneman-Tversky vs. Direct Preference Optimization 
[[arxiv](https://arxiv.org/abs/2502.14187)] [[cool](https://papers.cool/arxiv/2502.14187)] [[pdf](https://arxiv.org/pdf/2502.14187)]
> **Authors**: Fernando Spadea,Oshani Seneviratne
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: We evaluate Kahneman-Tversky Optimization (KTO) as a fine-tuning method for large language models (LLMs) in federated learning (FL) settings, comparing it against Direct Preference Optimization (DPO). Using Alpaca-7B as the base model, we fine-tune on a realistic dataset under both methods and evaluate performance using MT-Bench-1, Vicuna, and AdvBench benchmarks. Additionally, we introduce a redistributed dataset setup, where only KTO is applicable due to its ability to handle single-response feedback, unlike DPO's reliance on paired responses. Our results demonstrate that KTO, in both its original (KTOO) and redistributed (KTOR) configurations, consistently outperforms DPO across all benchmarks. In the redistributed setup, KTO further validates its flexibility and resilience by maintaining superior performance in scenarios where DPO cannot be applied. These findings establish KTO as a robust and scalable fine-tuning method for FL, motivating its adoption for privacy-preserving, decentralized, and heterogeneous environments.

### Type 1 Diabetes Management using GLIMMER: Glucose Level Indicator Model with Modified Error Rate 
[[arxiv](https://arxiv.org/abs/2502.14183)] [[cool](https://papers.cool/arxiv/2502.14183)] [[pdf](https://arxiv.org/pdf/2502.14183)]
> **Authors**: Saman Khamesian,Asiful Arefeen,Adela Grando,Bithika Thompson,Hassan Ghasemzadeh
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Managing Type 1 Diabetes (T1D) demands constant vigilance as individuals strive to regulate their blood glucose levels to avert the dangers of dysglycemia (hyperglycemia or hypoglycemia). Despite the advent of sophisticated technologies such as automated insulin delivery (AID) systems, achieving optimal glycemic control remains a formidable task. AID systems integrate continuous subcutaneous insulin infusion (CSII) and continuous glucose monitors (CGM) data, offering promise in reducing variability and increasing glucose time-in-range. However, these systems often fail to prevent dysglycemia, partly due to limitations in prediction algorithms that lack the precision to avert abnormal glucose events. This gap highlights the need for proactive behavioral adjustments. We address this need with GLIMMER, Glucose Level Indicator Model with Modified Error Rate, a machine learning approach for forecasting blood glucose levels. GLIMMER categorizes glucose values into normal and abnormal ranges and devises a novel custom loss function to prioritize accuracy in dysglycemic events where patient safety is critical. To evaluate the potential of GLIMMER for T1D management, we both use a publicly available dataset and collect new data involving 25 patients with T1D. In predicting next-hour glucose values, GLIMMER achieved a root mean square error (RMSE) of 23.97 (+/-3.77) and a mean absolute error (MAE) of 15.83 (+/-2.09) mg/dL. These results reflect a 23% improvement in RMSE and a 31% improvement in MAE compared to the best-reported error rates.

### On the logical skills of large language models: evaluations using arbitrarily complex first-order logic problems 
[[arxiv](https://arxiv.org/abs/2502.14180)] [[cool](https://papers.cool/arxiv/2502.14180)] [[pdf](https://arxiv.org/pdf/2502.14180)]
> **Authors**: Shokhrukh Ibragimov,Arnulf Jentzen,Benno Kuckuck
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 67 pages, 24 figures
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: We present a method of generating first-order logic statements whose complexity can be controlled along multiple dimensions. We use this method to automatically create several datasets consisting of questions asking for the truth or falsity of first-order logic statements in Zermelo-Fraenkel set theory. While the resolution of these questions does not require any knowledge beyond basic notation of first-order logic and set theory, it does require a degree of planning and logical reasoning, which can be controlled up to arbitrarily high difficulty by the complexity of the generated statements. Furthermore, we do extensive evaluations of the performance of various large language models, including recent models such as DeepSeek-R1 and OpenAI's o3-mini, on these datasets. All of the datasets along with the code used for generating them, as well as all data from the evaluations is publicly available at https://github.com/bkuckuck/logical-skills-of-llms.

### InstaSHAP: Interpretable Additive Models Explain Shapley Values Instantly 
[[arxiv](https://arxiv.org/abs/2502.14177)] [[cool](https://papers.cool/arxiv/2502.14177)] [[pdf](https://arxiv.org/pdf/2502.14177)]
> **Authors**: James Enouen,Yan Liu
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: In recent years, the Shapley value and SHAP explanations have emerged as one of the most dominant paradigms for providing post-hoc explanations of black-box models. Despite their well-founded theoretical properties, many recent works have focused on the limitations in both their computational efficiency and their representation power. The underlying connection with additive models, however, is left critically under-emphasized in the current literature. In this work, we find that a variational perspective linking GAM models and SHAP explanations is able to provide deep insights into nearly all recent developments. In light of this connection, we borrow in the other direction to develop a new method to train interpretable GAM models which are automatically purified to compute the Shapley value in a single forward pass. Finally, we provide theoretical results showing the limited representation power of GAM models is the same Achilles' heel existing in SHAP and discuss the implications for SHAP's modern usage in CV and NLP.

### Blockchain-based Framework for Scalable and Incentivized Federated Learning 
[[arxiv](https://arxiv.org/abs/2502.14170)] [[cool](https://papers.cool/arxiv/2502.14170)] [[pdf](https://arxiv.org/pdf/2502.14170)]
> **Authors**: Bijun Wu,Oshani Seneviratne
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,分布式、并行和集群计算
- **Abstract**: Federated Learning (FL) enables collaborative model training without sharing raw data, preserving privacy while harnessing distributed datasets. However, traditional FL systems often rely on centralized aggregating mechanisms, introducing trust issues, single points of failure, and limited mechanisms for incentivizing meaningful client contributions. These challenges are exacerbated as FL scales to train resource-intensive models, such as large language models (LLMs), requiring scalable, decentralized solutions. This paper presents a blockchain-based FL framework that addresses these limitations by integrating smart contracts and a novel hybrid incentive mechanism. The framework automates critical FL tasks, including client registration, update validation, reward distribution, and maintaining a transparent global state. The hybrid incentive mechanism combines on-chain alignment-based rewards, off-chain fairness checks, and consistency multipliers to ensure fairness, transparency, and sustained engagement. We evaluate the framework through gas cost analysis, demonstrating its feasibility for different scales of federated learning scenarios.

### Dual-level Mixup for Graph Few-shot Learning with Fewer Tasks 
[[arxiv](https://arxiv.org/abs/2502.14158)] [[cool](https://papers.cool/arxiv/2502.14158)] [[pdf](https://arxiv.org/pdf/2502.14158)]
> **Authors**: Yonghao Liu,Mengyu Li,Fausto Giunchiglia,Lan Huang,Ximing Li,Xiaoyue Feng,Renchu Guan
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: WWW25
- **标题**: None
- **领域**: 机器学习,社交和信息网络
- **Abstract**: Graph neural networks have been demonstrated as a powerful paradigm for effectively learning graph-structured data on the web and mining content from it.Current leading graph models require a large number of labeled samples for training, which unavoidably leads to overfitting in few-shot scenarios. Recent research has sought to alleviate this issue by simultaneously leveraging graph learning and meta-learning paradigms. However, these graph meta-learning models assume the availability of numerous meta-training tasks to learn transferable meta-knowledge. Such assumption may not be feasible in the real world due to the difficulty of constructing tasks and the substantial costs involved. Therefore, we propose a SiMple yet effectIve approach for graph few-shot Learning with fEwer tasks, named SMILE. We introduce a dual-level mixup strategy, encompassing both within-task and across-task mixup, to simultaneously enrich the available nodes and tasks in meta-learning. Moreover, we explicitly leverage the prior information provided by the node degrees in the graph to encode expressive node representations. Theoretically, we demonstrate that SMILE can enhance the model generalization ability. Empirically, SMILE consistently outperforms other competitive models by a large margin across all evaluated datasets with in-domain and cross-domain settings. Our anonymous code can be found here.

### Learning the P2D Model for Lithium-Ion Batteries with SOH Detection 
[[arxiv](https://arxiv.org/abs/2502.14147)] [[cool](https://papers.cool/arxiv/2502.14147)] [[pdf](https://arxiv.org/pdf/2502.14147)]
> **Authors**: Maricela Best McKay,Bhushan Gopaluni,Brian Wetton
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 18 pages, 5 figures
- **标题**: None
- **领域**: 机器学习,化学物理
- **Abstract**: Lithium ion batteries are widely used in many applications. Battery management systems control their optimal use and charging and predict when the battery will cease to deliver the required output on a planned duty or driving cycle. Such systems use a simulation of a mathematical model of battery performance. These models can be electrochemical or data-driven. Electrochemical models for batteries running at high currents are mathematically and computationally complex. In this work, we show that a well-regarded electrochemical model, the Pseudo Two Dimensional (P2D) model, can be replaced by a computationally efficient Convolutional Neural Network (CNN) surrogate model fit to accurately simulated data from a class of random driving cycles. We demonstrate that a CNN is an ideal choice for accurately capturing Lithium ion concentration profiles. Additionally, we show how the neural network model can be adjusted to correspond to battery changes in State of Health (SOH).

### Efficient and Optimal Policy Gradient Algorithm for Corrupted Multi-armed Bandits 
[[arxiv](https://arxiv.org/abs/2502.14146)] [[cool](https://papers.cool/arxiv/2502.14146)] [[pdf](https://arxiv.org/pdf/2502.14146)]
> **Authors**: Jiayuan Liu,Siwei Wang,Zhixuan Fang
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: In this paper, we consider the stochastic multi-armed bandits problem with adversarial corruptions, where the random rewards of the arms are partially modified by an adversary to fool the algorithm. We apply the policy gradient algorithm SAMBA to this setting, and show that it is computationally efficient, and achieves a state-of-the-art $O(K\log T/Δ) + O(C/Δ)$ regret upper bound, where $K$ is the number of arms, $C$ is the unknown corruption level, $Δ$ is the minimum expected reward gap between the best arm and other ones, and $T$ is the time horizon. Compared with the best existing efficient algorithm (e.g., CBARBAR), whose regret upper bound is $O(K\log^2 T/Δ) + O(C)$, we show that SAMBA reduces one $\log T$ factor in the regret bound, while maintaining the corruption-dependent term to be linear with $C$. This is indeed asymptotically optimal. We also conduct simulations to demonstrate the effectiveness of SAMBA, and the results show that SAMBA outperforms existing baselines.

### Cluster Analysis and Concept Drift Detection in Malware 
[[arxiv](https://arxiv.org/abs/2502.14135)] [[cool](https://papers.cool/arxiv/2502.14135)] [[pdf](https://arxiv.org/pdf/2502.14135)]
> **Authors**: Aniket Mishra,Mark Stamp
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,密码学和安全
- **Abstract**: Concept drift refers to gradual or sudden changes in the properties of data that affect the accuracy of machine learning models. In this paper, we address the problem of concept drift detection in the malware domain. Specifically, we propose and analyze a clustering-based approach to detecting concept drift. Using a subset of the KronoDroid dataset, malware samples are partitioned into temporal batches and analyzed using MiniBatch $K$-Means clustering. The silhouette coefficient is used as a metric to identify points in time where concept drift has likely occurred. To verify our drift detection results, we train learning models under three realistic scenarios, which we refer to as static training, periodic retraining, and drift-aware retraining. In each scenario, we consider four supervised classifiers, namely, Multilayer Perceptron (MLP), Support Vector Machine (SVM), Random Forest, and XGBoost. Experimental results demonstrate that drift-aware retraining guided by silhouette coefficient thresholding achieves classification accuracy far superior to static models, and generally within 1% of periodic retraining, while also being far more efficient than periodic retraining. These results provide strong evidence that our clustering-based approach is effective at detecting concept drift, while also illustrating a highly practical and efficient fully automated approach to improved malware classification via concept drift detection.

### Gradients can train reward models: An Empirical Risk Minimization Approach for Offline Inverse RL and Dynamic Discrete Choice Model 
[[arxiv](https://arxiv.org/abs/2502.14131)] [[cool](https://papers.cool/arxiv/2502.14131)] [[pdf](https://arxiv.org/pdf/2502.14131)]
> **Authors**: Enoch H. Kang,Hema Yoganarasimhan,Lalit Jain
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计量经济学
- **Abstract**: We study the problem of estimating Dynamic Discrete Choice (DDC) models, also known as offline Maximum Entropy-Regularized Inverse Reinforcement Learning (offline MaxEnt-IRL) in machine learning. The objective is to recover reward or $Q^*$ functions that govern agent behavior from offline behavior data. In this paper, we propose a globally convergent gradient-based method for solving these problems without the restrictive assumption of linearly parameterized rewards. The novelty of our approach lies in introducing the Empirical Risk Minimization (ERM) based IRL/DDC framework, which circumvents the need for explicit state transition probability estimation in the Bellman equation. Furthermore, our method is compatible with non-parametric estimation techniques such as neural networks. Therefore, the proposed method has the potential to be scaled to high-dimensional, infinite state spaces. A key theoretical insight underlying our approach is that the Bellman residual satisfies the Polyak-Lojasiewicz (PL) condition -- a property that, while weaker than strong convexity, is sufficient to ensure fast global convergence guarantees. Through a series of synthetic experiments, we demonstrate that our approach consistently outperforms benchmark methods and state-of-the-art alternatives.

### Understanding SGD with Exponential Moving Average: A Case Study in Linear Regression 
[[arxiv](https://arxiv.org/abs/2502.14123)] [[cool](https://papers.cool/arxiv/2502.14123)] [[pdf](https://arxiv.org/pdf/2502.14123)]
> **Authors**: Xuheng Li,Quanquan Gu
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 34 pages, 4 figures
- **标题**: None
- **领域**: 机器学习,优化与控制,机器学习
- **Abstract**: Exponential moving average (EMA) has recently gained significant popularity in training modern deep learning models, especially diffusion-based generative models. However, there have been few theoretical results explaining the effectiveness of EMA. In this paper, to better understand EMA, we establish the risk bound of online SGD with EMA for high-dimensional linear regression, one of the simplest overparameterized learning tasks that shares similarities with neural networks. Our results indicate that (i) the variance error of SGD with EMA is always smaller than that of SGD without averaging, and (ii) unlike SGD with iterate averaging from the beginning, the bias error of SGD with EMA decays exponentially in every eigen-subspace of the data covariance matrix. Additionally, we develop proof techniques applicable to the analysis of a broad class of averaging schemes.

### A Supervised Machine-Learning Approach For Turboshaft Engine Dynamic Modeling Under Real Flight Conditions 
[[arxiv](https://arxiv.org/abs/2502.14120)] [[cool](https://papers.cool/arxiv/2502.14120)] [[pdf](https://arxiv.org/pdf/2502.14120)]
> **Authors**: Damiano Paniccia,Francesco Aldo Tucci,Joel Guerrero,Luigi Capone,Nicoletta Sanguini,Tommaso Benacchio,Luigi Bottasso
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 26 pages, 14 figures, submitted to the Aeronautical Journal
- **标题**: None
- **领域**: 机器学习,系统与控制
- **Abstract**: Rotorcraft engines are highly complex, nonlinear thermodynamic systems that operate under varying environmental and flight conditions. Simulating their dynamics is crucial for design, fault diagnostics, and deterioration control phases, and requires robust and reliable control systems to estimate engine performance throughout flight envelope. However, the development of detailed physical models of the engine based on numerical simulations is a very challenging task due to the complex and entangled physics driving the engine. In this scenario, data-driven machine-learning techniques are of great interest to the aircraft engine community, due to their ability to describe nonlinear systems' dynamic behavior and enable online performance estimation, achieving excellent results with accuracy competitive with the state of the art. In this work, we explore different Neural Network architectures to model the turboshaft engine of Leonardo's AW189P4 prototype, aiming to predict the engine torque. The models are trained on an extensive database of real flight tests featuring a variety of operational maneuvers performed under different flight conditions, providing a comprehensive representation of the engine's performance. To complement the neural network approach, we apply Sparse Identification of Nonlinear Dynamics (SINDy) to derive a low-dimensional dynamical model from the available data, describing the relationship between fuel flow and engine torque. The resulting model showcases SINDy's capability to recover the actual physics underlying the engine dynamics and demonstrates its potential for investigating more complex aspects of the engine. The results prove that data-driven engine models can exploit a wider range of parameters than standard transfer function-based approaches, enabling the use of trained schemes to simulate nonlinear effects in different engines and helicopters.

### Chasing the Timber Trail: Machine Learning to Reveal Harvest Location Misrepresentation 
[[arxiv](https://arxiv.org/abs/2502.14115)] [[cool](https://papers.cool/arxiv/2502.14115)] [[pdf](https://arxiv.org/pdf/2502.14115)]
> **Authors**: Shailik Sarkar,Raquib Bin Yousuf,Linhan Wang,Brian Mayer,Thomas Mortier,Victor Deklerck,Jakub Truszkowski,John C. Simeone,Marigold Norman,Jade Saunders,Chang-Tien Lu,Naren Ramakrishnan
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 9 pages, 5 figures
- **标题**: None
- **领域**: 机器学习,计算工程、金融和科学,计算机与社会
- **Abstract**: Illegal logging poses a significant threat to global biodiversity, climate stability, and depresses international prices for legal wood harvesting and responsible forest products trade, affecting livelihoods and communities across the globe. Stable isotope ratio analysis (SIRA) is rapidly becoming an important tool for determining the harvest location of traded, organic, products. The spatial pattern in stable isotope ratio values depends on factors such as atmospheric and environmental conditions and can thus be used for geographical identification. We present here the results of a deployed machine learning pipeline where we leverage both isotope values and atmospheric variables to determine timber harvest location. Additionally, the pipeline incorporates uncertainty estimation to facilitate the interpretation of harvest location determination for analysts. We present our experiments on a collection of oak (Quercus spp.) tree samples from its global range. Our pipeline outperforms comparable state-of-the-art models determining geographic harvest origin of commercially traded wood products, and has been used by European enforcement agencies to identify illicit Russian and Belarusian timber entering the EU market. We also identify opportunities for further advancement of our framework and how it can be generalized to help identify the origin of falsely labeled organic products throughout the supply chain.

### Zero loss guarantees and explicit minimizers for generic overparametrized Deep Learning networks 
[[arxiv](https://arxiv.org/abs/2502.14114)] [[cool](https://papers.cool/arxiv/2502.14114)] [[pdf](https://arxiv.org/pdf/2502.14114)]
> **Authors**: Thomas Chen,Andrew G. Moore
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: AMS Latex, 9 pages
- **标题**: None
- **领域**: 机器学习,人工智能,偏微分方程分析,优化与控制,机器学习
- **Abstract**: We determine sufficient conditions for overparametrized deep learning (DL) networks to guarantee the attainability of zero loss in the context of supervised learning, for the $\mathcal{L}^2$ cost and {\em generic} training data. We present an explicit construction of the zero loss minimizers without invoking gradient descent. On the other hand, we point out that increase of depth can deteriorate the efficiency of cost minimization using a gradient descent algorithm by analyzing the conditions for rank loss of the training Jacobian. Our results clarify key aspects on the dichotomy between zero loss reachability in underparametrized versus overparametrized DL.

### Aligned Multi Objective Optimization 
[[arxiv](https://arxiv.org/abs/2502.14096)] [[cool](https://papers.cool/arxiv/2502.14096)] [[pdf](https://arxiv.org/pdf/2502.14096)]
> **Authors**: Yonathan Efroni,Ben Kertzu,Daniel Jiang,Jalaj Bhandari,Zheqing,Zhu,Karen Ullrich
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,优化与控制
- **Abstract**: To date, the multi-objective optimization literature has mainly focused on conflicting objectives, studying the Pareto front, or requiring users to balance tradeoffs. Yet, in machine learning practice, there are many scenarios where such conflict does not take place. Recent findings from multi-task learning, reinforcement learning, and LLMs training show that diverse related tasks can enhance performance across objectives simultaneously. Despite this evidence, such phenomenon has not been examined from an optimization perspective. This leads to a lack of generic gradient-based methods that can scale to scenarios with a large number of related objectives. To address this gap, we introduce the Aligned Multi-Objective Optimization framework, propose new algorithms for this setting, and provide theoretical guarantees of their superior performance compared to naive approaches.

### Learning from End User Data with Shuffled Differential Privacy over Kernel Densities 
[[arxiv](https://arxiv.org/abs/2502.14087)] [[cool](https://papers.cool/arxiv/2502.14087)] [[pdf](https://arxiv.org/pdf/2502.14087)]
> **Authors**: Tal Wagner
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: ICLR 2025
- **标题**: None
- **领域**: 机器学习,密码学和安全,数据结构和算法
- **Abstract**: We study a setting of collecting and learning from private data distributed across end users. In the shuffled model of differential privacy, the end users partially protect their data locally before sharing it, and their data is also anonymized during its collection to enhance privacy. This model has recently become a prominent alternative to central DP, which requires full trust in a central data curator, and local DP, where fully local data protection takes a steep toll on downstream accuracy. Our main technical result is a shuffled DP protocol for privately estimating the kernel density function of a distributed dataset, with accuracy essentially matching central DP. We use it to privately learn a classifier from the end user data, by learning a private density function per class. Moreover, we show that the density function itself can recover the semantic content of its class, despite having been learned in the absence of any unprotected data. Our experiments show the favorable downstream performance of our approach, and highlight key downstream considerations and trade-offs in a practical ML deployment of shuffled DP.

### Towards Vector Optimization on Low-Dimensional Vector Symbolic Architecture 
[[arxiv](https://arxiv.org/abs/2502.14075)] [[cool](https://papers.cool/arxiv/2502.14075)] [[pdf](https://arxiv.org/pdf/2502.14075)]
> **Authors**: Shijin Duan,Yejia Liu,Gaowen Liu,Ramana Rao Kompella,Shaolei Ren,Xiaolin Xu
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 10 pages, 2 figures. Accepted in CPAL 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Vector Symbolic Architecture (VSA) is emerging in machine learning due to its efficiency, but they are hindered by issues of hyperdimensionality and accuracy. As a promising mitigation, the Low-Dimensional Computing (LDC) method significantly reduces the vector dimension by ~100 times while maintaining accuracy, by employing a gradient-based optimization. Despite its potential, LDC optimization for VSA is still underexplored. Our investigation into vector updates underscores the importance of stable, adaptive dynamics in LDC training. We also reveal the overlooked yet critical roles of batch normalization (BN) and knowledge distillation (KD) in standard approaches. Besides the accuracy boost, BN does not add computational overhead during inference, and KD significantly enhances inference confidence. Through extensive experiments and ablation studies across multiple benchmarks, we provide a thorough evaluation of our approach and extend the interpretability of binary neural network optimization similar to LDC, previously unaddressed in BNN literature.

### Towards a Learning Theory of Representation Alignment 
[[arxiv](https://arxiv.org/abs/2502.14047)] [[cool](https://papers.cool/arxiv/2502.14047)] [[pdf](https://arxiv.org/pdf/2502.14047)]
> **Authors**: Francesco Insulla,Shuo Huang,Lorenzo Rosasco
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: It has recently been argued that AI models' representations are becoming aligned as their scale and performance increase. Empirical analyses have been designed to support this idea and conjecture the possible alignment of different representations toward a shared statistical model of reality. In this paper, we propose a learning-theoretic perspective to representation alignment. First, we review and connect different notions of alignment based on metric, probabilistic, and spectral ideas. Then, we focus on stitching, a particular approach to understanding the interplay between different representations in the context of a task. Our main contribution here is relating properties of stitching to the kernel alignment of the underlying representation. Our results can be seen as a first step toward casting representation alignment as a learning-theoretic problem.

### Position: There are no Champions in Long-Term Time Series Forecasting 
[[arxiv](https://arxiv.org/abs/2502.14045)] [[cool](https://papers.cool/arxiv/2502.14045)] [[pdf](https://arxiv.org/pdf/2502.14045)]
> **Authors**: Lorenzo Brigato,Rafael Morand,Knut Strømmen,Maria Panagiotou,Markus Schmidt,Stavroula Mougiakakou
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: Pre-print
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Recent advances in long-term time series forecasting have introduced numerous complex prediction models that consistently outperform previously published architectures. However, this rapid progression raises concerns regarding inconsistent benchmarking and reporting practices, which may undermine the reliability of these comparisons. Our position emphasizes the need to shift focus away from pursuing ever-more complex models and towards enhancing benchmarking practices through rigorous and standardized evaluation methods. To support our claim, we first perform a broad, thorough, and reproducible evaluation of the top-performing models on the most popular benchmark by training 3,500+ networks over 14 datasets. Then, through a comprehensive analysis, we find that slight changes to experimental setups or current evaluation metrics drastically shift the common belief that newly published results are advancing the state of the art. Our findings suggest the need for rigorous and standardized evaluation methods that enable more substantiated claims, including reproducible hyperparameter setups and statistical testing.

### Asking for Help Enables Safety Guarantees Without Sacrificing Effectiveness 
[[arxiv](https://arxiv.org/abs/2502.14043)] [[cool](https://papers.cool/arxiv/2502.14043)] [[pdf](https://arxiv.org/pdf/2502.14043)]
> **Authors**: Benjamin Plaut,Juan Liévano-Karim,Stuart Russell
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Most reinforcement learning algorithms with regret guarantees rely on a critical assumption: that all errors are recoverable. Recent work by Plaut et al. discarded this assumption and presented algorithms that avoid "catastrophe" (i.e., irreparable errors) by asking for help. However, they provided only safety guarantees and did not consider reward maximization. We prove that any algorithm that avoids catastrophe in their setting also guarantees high reward (i.e., sublinear regret) in any Markov Decision Process (MDP), including MDPs with irreversible costs. This constitutes the first no-regret guarantee for general MDPs. More broadly, our result may be the first formal proof that it is possible for an agent to obtain high reward while becoming self-sufficient in an unknown, unbounded, and high-stakes environment without causing catastrophe or requiring resets.

### Dynamic Activation with Knowledge Distillation for Energy-Efficient Spiking NN Ensembles 
[[arxiv](https://arxiv.org/abs/2502.14023)] [[cool](https://papers.cool/arxiv/2502.14023)] [[pdf](https://arxiv.org/pdf/2502.14023)]
> **Authors**: Orestis Konstantaropoulos,Theodoris Mallios,Maria Papadopouli
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别,神经和进化计算
- **Abstract**: While foundation AI models excel at tasks like classification and decision-making, their high energy consumption makes them unsuitable for energy-constrained applications. Inspired by the brain's efficiency, spiking neural networks (SNNs) have emerged as a viable alternative due to their event-driven nature and compatibility with neuromorphic chips. This work introduces a novel system that combines knowledge distillation and ensemble learning to bridge the performance gap between artificial neural networks (ANNs) and SNNs. A foundation AI model acts as a teacher network, guiding smaller student SNNs organized into an ensemble, called Spiking Neural Ensemble (SNE). SNE enables the disentanglement of the teacher's knowledge, allowing each student to specialize in predicting a distinct aspect of it, while processing the same input. The core innovation of SNE is the adaptive activation of a subset of SNN models of an ensemble, leveraging knowledge-distillation, enhanced with an informed-partitioning (disentanglement) of the teacher's feature space. By dynamically activating only a subset of these student SNNs, the system balances accuracy and energy efficiency, achieving substantial energy savings with minimal accuracy loss. Moreover, SNE is significantly more efficient than the teacher network, reducing computational requirements by up to 20x with only a 2% drop in accuracy on the CIFAR-10 dataset. This disentanglement procedure achieves an accuracy improvement of up to 2.4% on the CIFAR-10 dataset compared to other partitioning schemes. Finally, we comparatively analyze SNE performance under noisy conditions, demonstrating enhanced robustness compared to its ANN teacher. In summary, SNE offers a promising new direction for energy-constrained applications.

### I Want 'Em All (At Once) -- Ultrametric Cluster Hierarchies 
[[arxiv](https://arxiv.org/abs/2502.14018)] [[cool](https://papers.cool/arxiv/2502.14018)] [[pdf](https://arxiv.org/pdf/2502.14018)]
> **Authors**: Andrew Draganov,Pascal Weber,Rasmus Skibdahl Melanchton Jørgensen,Anna Beer,Claudia Plant,Ira Assent
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Hierarchical clustering is a powerful tool for exploratory data analysis, organizing data into a tree of clusterings from which a partition can be chosen. This paper generalizes these ideas by proving that, for any reasonable hierarchy, one can optimally solve any center-based clustering objective over it (such as $k$-means). Moreover, these solutions can be found exceedingly quickly and are themselves necessarily hierarchical. Thus, given a cluster tree, we show that one can quickly access a plethora of new, equally meaningful hierarchies. Just as in standard hierarchical clustering, one can then choose any desired partition from these new hierarchies. We conclude by verifying the utility of our proposed techniques across datasets, hierarchies, and partitioning schemes.

### DFDT: Dynamic Fast Decision Tree for IoT Data Stream Mining on Edge Devices 
[[arxiv](https://arxiv.org/abs/2502.14011)] [[cool](https://papers.cool/arxiv/2502.14011)] [[pdf](https://arxiv.org/pdf/2502.14011)]
> **Authors**: Afonso Lourenço,João Rodrigo,João Gama,Goreti Marreiros
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,网络和互联网架构
- **Abstract**: The Internet of Things generates massive data streams, with edge computing emerging as a key enabler for online IoT applications and 5G networks. Edge solutions facilitate real-time machine learning inference, but also require continuous adaptation to concept drifts. Ensemble-based solutions improve predictive performance, but incur higher resource consumption, latency, and memory demands. This paper presents DFDT: Dynamic Fast Decision Tree, a novel algorithm designed for energy-efficient memory-constrained data stream mining. DFDT improves hoeffding tree growth efficiency by dynamically adjusting grace periods, tie thresholds, and split evaluations based on incoming data. It incorporates stricter evaluation rules (based on entropy, information gain, and leaf instance count), adaptive expansion modes, and a leaf deactivation mechanism to manage memory, allowing more computation on frequently visited nodes while conserving energy on others. Experiments show that the proposed framework can achieve increased predictive performance (0.43 vs 0.29 ranking) with constrained memory and a fraction of the runtime of VFDT or SVFDT.

### Which Attention Heads Matter for In-Context Learning? 
[[arxiv](https://arxiv.org/abs/2502.14010)] [[cool](https://papers.cool/arxiv/2502.14010)] [[pdf](https://arxiv.org/pdf/2502.14010)]
> **Authors**: Kayo Yin,Jacob Steinhardt
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Large language models (LLMs) exhibit impressive in-context learning (ICL) capability, enabling them to perform new tasks using only a few demonstrations in the prompt. Two different mechanisms have been proposed to explain ICL: induction heads that find and copy relevant tokens, and function vector (FV) heads whose activations compute a latent encoding of the ICL task. To better understand which of the two distinct mechanisms drives ICL, we study and compare induction heads and FV heads in 12 language models. Through detailed ablations, we discover that few-shot ICL performance depends primarily on FV heads, especially in larger models. In addition, we uncover that FV and induction heads are connected: many FV heads start as induction heads during training before transitioning to the FV mechanism. This leads us to speculate that induction facilitates learning the more complex FV mechanism that ultimately drives ICL.

### Smaller But Better: Unifying Layout Generation with Smaller Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.14005)] [[cool](https://papers.cool/arxiv/2502.14005)] [[pdf](https://arxiv.org/pdf/2502.14005)]
> **Authors**: Peirong Zhang,Jiaxin Zhang,Jiahuan Cao,Hongliang Li,Lianwen Jin
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We propose LGGPT, an LLM-based model tailored for unified layout generation. First, we propose Arbitrary Layout Instruction (ALI) and Universal Layout Response (ULR) as the uniform I/O template. ALI accommodates arbitrary layout generation task inputs across multiple layout domains, enabling LGGPT to unify both task-generic and domain-generic layout generation hitherto unexplored. Collectively, ALI and ULR boast a succinct structure that forgoes superfluous tokens typically found in existing HTML-based formats, facilitating efficient instruction tuning and boosting unified generation performance. In addition, we propose an Interval Quantization Encoding (IQE) strategy that compresses ALI into a more condensed structure. IQE precisely preserves valid layout clues while eliminating the less informative placeholders, facilitating LGGPT to capture complex and variable layout generation conditions during the unified training process. Experimental results demonstrate that LGGPT achieves superior or on par performance compared to existing methods. Notably, LGGPT strikes a prominent balance between proficiency and efficiency with a compact 1.5B parameter LLM, which beats prior 7B or 175B models even in the most extensive and challenging unified scenario. Furthermore, we underscore the necessity of employing LLMs for unified layout generation and suggest that 1.5B could be an optimal parameter size by comparing LLMs of varying scales. Code is available at https://github.com/NiceRingNode/LGGPT.

### Rectified Lagrangian for Out-of-Distribution Detection in Modern Hopfield Networks 
[[arxiv](https://arxiv.org/abs/2502.14003)] [[cool](https://papers.cool/arxiv/2502.14003)] [[pdf](https://arxiv.org/pdf/2502.14003)]
> **Authors**: Ryo Moriai,Nakamasa Inoue,Masayuki Tanaka,Rei Kawakami,Satoshi Ikehata,Ikuro Sato
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: Accepted to AAAI 2025
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Modern Hopfield networks (MHNs) have recently gained significant attention in the field of artificial intelligence because they can store and retrieve a large set of patterns with an exponentially large memory capacity. A MHN is generally a dynamical system defined with Lagrangians of memory and feature neurons, where memories associated with in-distribution (ID) samples are represented by attractors in the feature space. One major problem in existing MHNs lies in managing out-of-distribution (OOD) samples because it was originally assumed that all samples are ID samples. To address this, we propose the rectified Lagrangian (RegLag), a new Lagrangian for memory neurons that explicitly incorporates an attractor for OOD samples in the dynamical system of MHNs. RecLag creates a trivial point attractor for any interaction matrix, enabling OOD detection by identifying samples that fall into this attractor as OOD. The interaction matrix is optimized so that the probability densities can be estimated to identify ID/OOD. We demonstrate the effectiveness of RecLag-based MHNs compared to energy-based OOD detection methods, including those using state-of-the-art Hopfield energies, across nine image datasets.

### Beyond Single-Value Metrics: Evaluating and Enhancing LLM Unlearning with Cognitive Diagnosis 
[[arxiv](https://arxiv.org/abs/2502.13996)] [[cool](https://papers.cool/arxiv/2502.13996)] [[pdf](https://arxiv.org/pdf/2502.13996)]
> **Authors**: Yicheng Lang,Kehan Guo,Yue Huang,Yujun Zhou,Haomin Zhuang,Tianyu Yang,Yao Su,Xiangliang Zhang
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Due to the widespread use of LLMs and the rising critical ethical and safety concerns, LLM unlearning methods have been developed to remove harmful knowledge and undesirable capabilities. In this context, evaluations are mostly based on single-value metrics such as QA accuracy. However, these metrics often fail to capture the nuanced retention of harmful knowledge components, making it difficult to assess the true effectiveness of unlearning. To address this issue, we propose UNCD (UNlearning evaluation via Cognitive Diagnosis), a novel framework that leverages Cognitive Diagnosis Modeling for fine-grained evaluation of LLM unlearning. Our dedicated benchmark, UNCD-Cyber, provides a detailed assessment of the removal of dangerous capabilities. Moreover, we introduce UNCD-Agent, which refines unlearning by diagnosing knowledge remnants and generating targeted unlearning data. Extensive experiments across eight unlearning methods and two base models demonstrate that UNCD not only enhances evaluation but also effectively facilitates the removal of harmful LLM abilities.

### Autellix: An Efficient Serving Engine for LLM Agents as General Programs 
[[arxiv](https://arxiv.org/abs/2502.13965)] [[cool](https://papers.cool/arxiv/2502.13965)] [[pdf](https://arxiv.org/pdf/2502.13965)]
> **Authors**: Michael Luo,Xiaoxiang Shi,Colin Cai,Tianjun Zhang,Justin Wong,Yichuan Wang,Chi Wang,Yanping Huang,Zhifeng Chen,Joseph E. Gonzalez,Ion Stoica
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,分布式、并行和集群计算
- **Abstract**: Large language model (LLM) applications are evolving beyond simple chatbots into dynamic, general-purpose agentic programs, which scale LLM calls and output tokens to help AI agents reason, explore, and solve complex tasks. However, existing LLM serving systems ignore dependencies between programs and calls, missing significant opportunities for optimization. Our analysis reveals that programs submitted to LLM serving engines experience long cumulative wait times, primarily due to head-of-line blocking at both the individual LLM request and the program. To address this, we introduce Autellix, an LLM serving system that treats programs as first-class citizens to minimize their end-to-end latencies. Autellix intercepts LLM calls submitted by programs, enriching schedulers with program-level context. We propose two scheduling algorithms-for single-threaded and distributed programs-that preempt and prioritize LLM calls based on their programs' previously completed calls. Our evaluation demonstrates that across diverse LLMs and agentic workloads, Autellix improves throughput of programs by 4-15x at the same latency compared to state-of-the-art systems, such as vLLM.

### Exploring Code Language Models for Automated HLS-based Hardware Generation: Benchmark, Infrastructure and Analysis 
[[arxiv](https://arxiv.org/abs/2502.13921)] [[cool](https://papers.cool/arxiv/2502.13921)] [[pdf](https://arxiv.org/pdf/2502.13921)]
> **Authors**: Jiahao Gai,Hao,Chen,Zhican Wang,Hongyu Zhou,Wanru Zhao,Nicholas Lane,Hongxiang Fan
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: Paper accepted by ASP-DAC'25
- **标题**: None
- **领域**: 机器学习,硬件架构,软件工程
- **Abstract**: Recent advances in code generation have illuminated the potential of employing large language models (LLMs) for general-purpose programming languages such as Python and C++, opening new opportunities for automating software development and enhancing programmer productivity. The potential of LLMs in software programming has sparked significant interest in exploring automated hardware generation and automation. Although preliminary endeavors have been made to adopt LLMs in generating hardware description languages (HDLs), several challenges persist in this direction. First, the volume of available HDL training data is substantially smaller compared to that for software programming languages. Second, the pre-trained LLMs, mainly tailored for software code, tend to produce HDL designs that are more error-prone. Third, the generation of HDL requires a significantly higher number of tokens compared to software programming, leading to inefficiencies in cost and energy consumption. To tackle these challenges, this paper explores leveraging LLMs to generate High-Level Synthesis (HLS)-based hardware design. Although code generation for domain-specific programming languages is not new in the literature, we aim to provide experimental results, insights, benchmarks, and evaluation infrastructure to investigate the suitability of HLS over low-level HDLs for LLM-assisted hardware design generation. To achieve this, we first finetune pre-trained models for HLS-based hardware generation, using a collected dataset with text prompts and corresponding reference HLS designs. An LLM-assisted framework is then proposed to automate end-to-end hardware code generation, which also investigates the impact of chain-of-thought and feedback loops promoting techniques on HLS-design generation. Limited by the timeframe of this research, we plan to evaluate more advanced reasoning models in the future.

### Playing Hex and Counter Wargames using Reinforcement Learning and Recurrent Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.13918)] [[cool](https://papers.cool/arxiv/2502.13918)] [[pdf](https://arxiv.org/pdf/2502.13918)]
> **Authors**: Guilherme Palma,Pedro A. Santos,João Dias
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: :I.2.6
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Hex and Counter Wargames are adversarial two-player simulations of real military conflicts requiring complex strategic decision-making. Unlike classical board games, these games feature intricate terrain/unit interactions, unit stacking, large maps of varying sizes, and simultaneous move and combat decisions involving hundreds of units. This paper introduces a novel system designed to address the strategic complexity of Hex and Counter Wargames by integrating cutting-edge advancements in Recurrent Neural Networks with AlphaZero, a reliable modern Reinforcement Learning algorithm. The system utilizes a new Neural Network architecture developed from existing research, incorporating innovative state and action representations tailored to these specific game environments. With minimal training, our solution has shown promising results in typical scenarios, demonstrating the ability to generalize across different terrain and tactical situations. Additionally, we explore the system's potential to scale to larger map sizes. The developed system is openly accessible, facilitating continued research and exploration within this challenging domain.

### Partially Observable Gaussian Process Network and Doubly Stochastic Variational Inference 
[[arxiv](https://arxiv.org/abs/2502.13905)] [[cool](https://papers.cool/arxiv/2502.13905)] [[pdf](https://arxiv.org/pdf/2502.13905)]
> **Authors**: Saksham Kiroriwal,Julius Pfrommer,Jürgen Beyerer
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 8 pages, 6 figures
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: To reduce the curse of dimensionality for Gaussian processes (GP), they can be decomposed into a Gaussian Process Network (GPN) of coupled subprocesses with lower dimensionality. In some cases, intermediate observations are available within the GPN. However, intermediate observations are often indirect, noisy, and incomplete in most real-world systems. This work introduces the Partially Observable Gaussian Process Network (POGPN) to model real-world process networks. We model a joint distribution of latent functions of subprocesses and make inferences using observations from all subprocesses. POGPN incorporates observation lenses (observation likelihoods) into the well-established inference method of deep Gaussian processes. We also introduce two training methods for POPGN to make inferences on the whole network using node observations. The application to benchmark problems demonstrates how incorporating partial observations during training and inference can improve the predictive performance of the overall network, offering a promising outlook for its practical application.

### Optimistically Optimistic Exploration for Provably Efficient Infinite-Horizon Reinforcement and Imitation Learning 
[[arxiv](https://arxiv.org/abs/2502.13900)] [[cool](https://papers.cool/arxiv/2502.13900)] [[pdf](https://arxiv.org/pdf/2502.13900)]
> **Authors**: Antoine Moulin,Gergely Neu,Luca Viano
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We study the problem of reinforcement learning in infinite-horizon discounted linear Markov decision processes (MDPs), and propose the first computationally efficient algorithm achieving near-optimal regret guarantees in this setting. Our main idea is to combine two classic techniques for optimistic exploration: additive exploration bonuses applied to the reward function, and artificial transitions made to an absorbing state with maximal return. We show that, combined with a regularized approximate dynamic-programming scheme, the resulting algorithm achieves a regret of order $\tilde{\mathcal{O}} (\sqrt{d^3 (1 - γ)^{- 7 / 2} T})$, where $T$ is the total number of sample transitions, $γ\in (0,1)$ is the discount factor, and $d$ is the feature dimensionality. The results continue to hold against adversarial reward sequences, enabling application of our method to the problem of imitation learning in linear MDPs, where we achieve state-of-the-art results.

### Geometric Principles for Machine Learning of Dynamical Systems 
[[arxiv](https://arxiv.org/abs/2502.13895)] [[cool](https://papers.cool/arxiv/2502.13895)] [[pdf](https://arxiv.org/pdf/2502.13895)]
> **Authors**: Zack Xuereb Conti,David J Wagg,Nick Pepper
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Mathematical descriptions of dynamical systems are deeply rooted in topological spaces defined by non-Euclidean geometry. This paper proposes leveraging structure-rich geometric spaces for machine learning to achieve structural generalization when modeling physical systems from data, in contrast to embedding physics bias within model-free architectures. We consider model generalization to be a function of symmetry, invariance and uniqueness, defined as a topological mapping from state space dynamics to the parameter space. We illustrate this view through the machine learning of linear time-invariant dynamical systems, whose dynamics reside on the symmetric positive definite manifold.

### Refining embeddings with fill-tuning: data-efficient generalised performance improvements for materials foundation models 
[[arxiv](https://arxiv.org/abs/2502.13886)] [[cool](https://papers.cool/arxiv/2502.13886)] [[pdf](https://arxiv.org/pdf/2502.13886)]
> **Authors**: Matthew P. Wilson,Edward O. Pyzer-Knapp,Nicolas Galichet,Luke Dicks
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 8 pages, 4 figures
- **标题**: None
- **领域**: 机器学习,计算工程、金融和科学
- **Abstract**: Pretrained foundation models learn embeddings that can be used for a wide range of downstream tasks. These embeddings optimise general performance, and if insufficiently accurate at a specific task the model can be fine-tuned to improve performance. For all current methodologies this operation necessarily degrades performance on all out-of-distribution tasks. In this work we present 'fill-tuning', a novel methodology to generate datasets for continued pretraining of foundation models that are not suited to a particular downstream task, but instead aim to correct poor regions of the embedding. We present the application of roughness analysis to latent space topologies and illustrate how it can be used to propose data that will be most valuable to improving the embedding. We apply fill-tuning to a set of state-of-the-art materials foundation models trained on $O(10^9)$ data points and show model improvement of almost 1% in all downstream tasks with the addition of only 100 data points. This method provides a route to the general improvement of foundation models at the computational cost of fine-tuning.

### SPEX: Scaling Feature Interaction Explanations for LLMs 
[[arxiv](https://arxiv.org/abs/2502.13870)] [[cool](https://papers.cool/arxiv/2502.13870)] [[pdf](https://arxiv.org/pdf/2502.13870)]
> **Authors**: Justin Singh Kang,Landon Butler,Abhineet Agarwal,Yigit Efe Erginbas,Ramtin Pedarsani,Kannan Ramchandran,Bin Yu
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,信息论
- **Abstract**: Large language models (LLMs) have revolutionized machine learning due to their ability to capture complex interactions between input features. Popular post-hoc explanation methods like SHAP provide marginal feature attributions, while their extensions to interaction importances only scale to small input lengths ($\approx 20$). We propose Spectral Explainer (SPEX), a model-agnostic interaction attribution algorithm that efficiently scales to large input lengths ($\approx 1000)$. SPEX exploits underlying natural sparsity among interactions -- common in real-world data -- and applies a sparse Fourier transform using a channel decoding algorithm to efficiently identify important interactions. We perform experiments across three difficult long-context datasets that require LLMs to utilize interactions between inputs to complete the task. For large inputs, SPEX outperforms marginal attribution methods by up to 20% in terms of faithfully reconstructing LLM outputs. Further, SPEX successfully identifies key features and interactions that strongly influence model output. For one of our datasets, HotpotQA, SPEX provides interactions that align with human annotations. Finally, we use our model-agnostic approach to generate explanations to demonstrate abstract reasoning in closed-source LLMs (GPT-4o mini) and compositional reasoning in vision-language models.

### Quantifying Memorization and Retriever Performance in Retrieval-Augmented Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2502.13836)] [[cool](https://papers.cool/arxiv/2502.13836)] [[pdf](https://arxiv.org/pdf/2502.13836)]
> **Authors**: Peter Carragher,Abhinand Jha,R Raghav,Kathleen M. Carley
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Large Language Models (LLMs) demonstrate remarkable capabilities in question answering (QA), but metrics for assessing their reliance on memorization versus retrieval remain underdeveloped. Moreover, while finetuned models are state-of-the-art on closed-domain tasks, general-purpose models like GPT-4o exhibit strong zero-shot performance. This raises questions about the trade-offs between memorization, generalization, and retrieval. In this work, we analyze the extent to which multimodal retrieval-augmented VLMs memorize training data compared to baseline VLMs. Using the WebQA benchmark, we contrast finetuned models with baseline VLMs on multihop retrieval and question answering, examining the impact of finetuning on data memorization. To quantify memorization in end-to-end retrieval and QA systems, we propose several proxy metrics by investigating instances where QA succeeds despite retrieval failing. Our results reveal the extent to which finetuned models rely on memorization. In contrast, retrieval-augmented VLMs have lower memorization scores, at the cost of accuracy (72% vs 52% on WebQA test set). As such, our measures pose a challenge for future work to reconcile memorization and generalization in both Open-Domain QA and joint Retrieval-QA tasks.

### Contrastive Learning-Based privacy metrics in Tabular Synthetic Datasets 
[[arxiv](https://arxiv.org/abs/2502.13833)] [[cool](https://papers.cool/arxiv/2502.13833)] [[pdf](https://arxiv.org/pdf/2502.13833)]
> **Authors**: Milton Nicolás Plasencia Palacios,Sebastiano Saccani,Gabriele Sgroi,Alexander Boudewijn,Luca Bortolussi
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,密码学和安全
- **Abstract**: Synthetic data has garnered attention as a Privacy Enhancing Technology (PET) in sectors such as healthcare and finance. When using synthetic data in practical applications, it is important to provide protection guarantees. In the literature, two family of approaches are proposed for tabular data: on the one hand, Similarity-based methods aim at finding the level of similarity between training and synthetic data. Indeed, a privacy breach can occur if the generated data is consistently too similar or even identical to the train data. On the other hand, Attack-based methods conduce deliberate attacks on synthetic datasets. The success rates of these attacks reveal how secure the synthetic datasets are. In this paper, we introduce a contrastive method that improves privacy assessment of synthetic datasets by embedding the data in a more representative space. This overcomes obstacles surrounding the multitude of data types and attributes. It also makes the use of intuitive distance metrics possible for similarity measurements and as an attack vector. In a series of experiments with publicly available datasets, we compare the performances of similarity-based and attack-based methods, both with and without use of the contrastive learning-based embeddings. Our results show that relatively efficient, easy to implement privacy metrics can perform equally well as more advanced metrics explicitly modeling conditions for privacy referred to by the GDPR.

### Bayesian Physics Informed Neural Networks for Linear Inverse problems 
[[arxiv](https://arxiv.org/abs/2502.13827)] [[cool](https://papers.cool/arxiv/2502.13827)] [[pdf](https://arxiv.org/pdf/2502.13827)]
> **Authors**: Ali Mohammad-Djafari
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-20
> **comment**: 9 pages
- **标题**: None
- **领域**: 机器学习,数值分析
- **Abstract**: Inverse problems arise almost everywhere in science and engineering where we need to infer on a quantity from indirect observation. The cases of medical, biomedical, and industrial imaging systems are the typical examples. A very high overview of classification of the inverse problems method can be: i) Analytical, ii) Regularization, and iii) Bayesian inference methods. Even if there are straight links between them, we can say that the Bayesian inference based methods are the most powerful, as they give the possibility of accounting for prior knowledge and can account for errors and uncertainties in general. One of the main limitations stay in computational costs in particular for high dimensional imaging systems. Neural Networks (NN), and in particular Deep NNs (DNN), have been considered as a way to push farther this limit. Physics Informed Neural Networks (PINN) concept integrates physical laws with deep learning techniques to enhance the speed, accuracy and efficiency of the above mentioned problems. In this work, a new Bayesian framework for the concept of PINN (BPINN) is presented and discussed which includes the deterministic one if we use the Maximum A Posteriori (MAP) estimation framework. We consider two cases of supervised and unsupervised for training step, obtain the expressions of the posterior probability of the unknown variables, and deduce the posterior laws of the NN's parameters. We also discuss about the challenges of implementation of these methods in real applications.

### Mixup Regularization: A Probabilistic Perspective 
[[arxiv](https://arxiv.org/abs/2502.13825)] [[cool](https://papers.cool/arxiv/2502.13825)] [[pdf](https://arxiv.org/pdf/2502.13825)]
> **Authors**: Yousef El-Laham,Niccolo Dalmasso,Svitlana Vyetrenko,Vamsi Potluru,Manuela Veloso
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: In recent years, mixup regularization has gained popularity as an effective way to improve the generalization performance of deep learning models by training on convex combinations of training data. While many mixup variants have been explored, the proper adoption of the technique to conditional density estimation and probabilistic machine learning remains relatively unexplored. This work introduces a novel framework for mixup regularization based on probabilistic fusion that is better suited for conditional density estimation tasks. For data distributed according to a member of the exponential family, we show that likelihood functions can be analytically fused using log-linear pooling. We further propose an extension of probabilistic mixup, which allows for fusion of inputs at an arbitrary intermediate layer of the neural network. We provide a theoretical analysis comparing our approach to standard mixup variants. Empirical results on synthetic and real datasets demonstrate the benefits of our proposed framework compared to existing mixup variants.

### On the Duality between Gradient Transformations and Adapters 
[[arxiv](https://arxiv.org/abs/2502.13811)] [[cool](https://papers.cool/arxiv/2502.13811)] [[pdf](https://arxiv.org/pdf/2502.13811)]
> **Authors**: Lucas Torroba-Hennigen,Hunter Lang,Han Guo,Yoon Kim
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 17 pages, 2 figures
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: We study memory-efficient optimization of neural networks with linear gradient transformations, where the gradients are linearly mapped to a lower dimensional space than the full parameter space, thus saving memory required for gradient accumulation and optimizer state persistence. The model parameters are updated by first performing an optimization step in the lower dimensional space and then going back into the original parameter space via the linear map's transpose. We show that optimizing the model in this transformed space is equivalent to reparameterizing the original model through a linear adapter that additively modifies the model parameters, and then only optimizing the adapter's parameters. When the transformation is Kronecker-factored, this establishes an equivalence between GaLore and one-sided LoRA. We show that this duality between gradient transformations and adapter-based reparameterizations unifies existing approaches to memory-efficient training and suggests new techniques for improving training efficiency and memory use.

### Learning to explore when mistakes are not allowed 
[[arxiv](https://arxiv.org/abs/2502.13801)] [[cool](https://papers.cool/arxiv/2502.13801)] [[pdf](https://arxiv.org/pdf/2502.13801)]
> **Authors**: Charly Pecqueux-Guézénec,Stéphane Doncieux,Nicolas Perrin-Gilbert
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 12 pages, 13 figures, Published as an extended abstract at AAMAS 2025
- **标题**: None
- **领域**: 机器学习,系统与控制
- **Abstract**: Goal-Conditioned Reinforcement Learning (GCRL) provides a versatile framework for developing unified controllers capable of handling wide ranges of tasks, exploring environments, and adapting behaviors. However, its reliance on trial-and-error poses challenges for real-world applications, as errors can result in costly and potentially damaging consequences. To address the need for safer learning, we propose a method that enables agents to learn goal-conditioned behaviors that explore without the risk of making harmful mistakes. Exploration without risks can seem paradoxical, but environment dynamics are often uniform in space, therefore a policy trained for safety without exploration purposes can still be exploited globally. Our proposed approach involves two distinct phases. First, during a pretraining phase, we employ safe reinforcement learning and distributional techniques to train a safety policy that actively tries to avoid failures in various situations. In the subsequent safe exploration phase, a goal-conditioned (GC) policy is learned while ensuring safety. To achieve this, we implement an action-selection mechanism leveraging the previously learned distributional safety critics to arbitrate between the safety policy and the GC policy, ensuring safe exploration by switching to the safety policy when needed. We evaluate our method in simulated environments and demonstrate that it not only provides substantial coverage of the goal space but also reduces the occurrence of mistakes to a minimum, in stark contrast to traditional GCRL approaches. Additionally, we conduct an ablation study and analyze failure modes, offering insights for future research directions.

### LESA: Learnable LLM Layer Scaling-Up 
[[arxiv](https://arxiv.org/abs/2502.13794)] [[cool](https://papers.cool/arxiv/2502.13794)] [[pdf](https://arxiv.org/pdf/2502.13794)]
> **Authors**: Yifei Yang,Zouying Cao,Xinbei Ma,Yao Yao,Libo Qin,Zhi Chen,Hai Zhao
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Training Large Language Models (LLMs) from scratch requires immense computational resources, making it prohibitively expensive. Model scaling-up offers a promising solution by leveraging the parameters of smaller models to create larger ones. However, existing depth scaling-up methods rely on empirical heuristic rules for layer duplication, which result in poorer initialization and slower convergence during continual pre-training. We propose \textbf{LESA}, a novel learnable method for depth scaling-up. By concatenating parameters from each layer and applying Singular Value Decomposition, we uncover latent patterns between layers, suggesting that inter-layer parameters can be learned. LESA uses a neural network to predict the parameters inserted between adjacent layers, enabling better initialization and faster training. Experiments show that LESA outperforms existing baselines, achieving superior performance with less than half the computational cost during continual pre-training. Extensive analyses demonstrate its effectiveness across different model sizes and tasks.

### Herglotz-NET: Implicit Neural Representation of Spherical Data with Harmonic Positional Encoding 
[[arxiv](https://arxiv.org/abs/2502.13777)] [[cool](https://papers.cool/arxiv/2502.13777)] [[pdf](https://arxiv.org/pdf/2502.13777)]
> **Authors**: Théo Hanon,Nicolas Mil-Homens Cavaco,John Kiely,Laurent Jacques
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: Keywords: Herglotz, spherical harmonics, spectral analysis, implicitneuralrepresentation. Remarks: 4 pages + 1 reference page, 4 figures (submitted to SAMPTA2025)
- **标题**: None
- **领域**: 机器学习,信号处理
- **Abstract**: Representing and processing data in spherical domains presents unique challenges, primarily due to the curvature of the domain, which complicates the application of classical Euclidean techniques. Implicit neural representations (INRs) have emerged as a promising alternative for high-fidelity data representation; however, to effectively handle spherical domains, these methods must be adapted to the inherent geometry of the sphere to maintain both accuracy and stability. In this context, we propose Herglotz-NET (HNET), a novel INR architecture that employs a harmonic positional encoding based on complex Herglotz mappings. This encoding yields a well-posed representation on the sphere with interpretable and robust spectral properties. Moreover, we present a unified expressivity analysis showing that any spherical-based INR satisfying a mild condition exhibits a predictable spectral expansion that scales with network depth. Our results establish HNET as a scalable and flexible framework for accurate modeling of spherical data.

### RobustX: Robust Counterfactual Explanations Made Easy 
[[arxiv](https://arxiv.org/abs/2502.13751)] [[cool](https://papers.cool/arxiv/2502.13751)] [[pdf](https://arxiv.org/pdf/2502.13751)]
> **Authors**: Junqi Jiang,Luca Marzari,Aaryan Purohit,Francesco Leofante
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: The increasing use of Machine Learning (ML) models to aid decision-making in high-stakes industries demands explainability to facilitate trust. Counterfactual Explanations (CEs) are ideally suited for this, as they can offer insights into the predictions of an ML model by illustrating how changes in its input data may lead to different outcomes. However, for CEs to realise their explanatory potential, significant challenges remain in ensuring their robustness under slight changes in the scenario being explained. Despite the widespread recognition of CEs' robustness as a fundamental requirement, a lack of standardised tools and benchmarks hinders a comprehensive and effective comparison of robust CE generation methods. In this paper, we introduce RobustX, an open-source Python library implementing a collection of CE generation and evaluation methods, with a focus on the robustness property. RobustX provides interfaces to several existing methods from the literature, enabling streamlined access to state-of-the-art techniques. The library is also easily extensible, allowing fast prototyping of novel robust CE generation and evaluation methods.

### Reverse Markov Learning: Multi-Step Generative Models for Complex Distributions 
[[arxiv](https://arxiv.org/abs/2502.13747)] [[cool](https://papers.cool/arxiv/2502.13747)] [[pdf](https://arxiv.org/pdf/2502.13747)]
> **Authors**: Xinwei Shen,Nicolai Meinshausen,Tong Zhang
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,方法论,机器学习
- **Abstract**: Learning complex distributions is a fundamental challenge in contemporary applications. Generative models, such as diffusion models, have demonstrated remarkable success in overcoming many limitations of traditional statistical methods. Shen and Meinshausen (2024) introduced engression, a generative approach based on scoring rules that maps noise (and covariates, if available) directly to data. While effective, engression struggles with highly complex distributions, such as those encountered in image data. In this work, we extend engression to improve its capability in learning complex distributions. We propose a framework that defines a general forward process transitioning from the target distribution to a known distribution (e.g., Gaussian) and then learns a reverse Markov process using multiple engression models. This reverse process reconstructs the target distribution step by step. Our approach supports general forward processes, allows for dimension reduction, and naturally discretizes the generative process. As a special case, when using a diffusion-based forward process, our framework offers a method to discretize the training and inference of diffusion models efficiently. Empirical evaluations on simulated and climate data validate our theoretical insights, demonstrating the effectiveness of our approach in capturing complex distributions.

### Homophily Heterogeneity Matters in Graph Federated Learning: A Spectrum Sharing and Complementing Perspective 
[[arxiv](https://arxiv.org/abs/2502.13732)] [[cool](https://papers.cool/arxiv/2502.13732)] [[pdf](https://arxiv.org/pdf/2502.13732)]
> **Authors**: Wentao Yu
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 15 pages
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Since heterogeneity presents a fundamental challenge in graph federated learning, many existing methods are proposed to deal with node feature heterogeneity and structure heterogeneity. However, they overlook the critical homophily heterogeneity, which refers to the substantial variation in homophily levels across graph data from different clients. The homophily level represents the proportion of edges connecting nodes that belong to the same class. Due to adapting to their local homophily, local models capture inconsistent spectral properties across different clients, significantly reducing the effectiveness of collaboration. Specifically, local models trained on graphs with high homophily tend to capture low-frequency information, whereas local models trained on graphs with low homophily tend to capture high-frequency information. To effectively deal with homophily heterophily, we introduce the spectral Graph Neural Network (GNN) and propose a novel Federated learning method by mining Graph Spectral Properties (FedGSP). On one hand, our proposed FedGSP enables clients to share generic spectral properties (i.e., low-frequency information), allowing all clients to benefit through collaboration. On the other hand, inspired by our theoretical findings, our proposed FedGSP allows clients to complement non-generic spectral properties by acquiring the spectral properties they lack (i.e., high-frequency information), thereby obtaining additional information gain. Extensive experiments conducted on six homophilic and five heterophilic graph datasets, across both non-overlapping and overlapping settings, validate the superiority of our method over eleven state-of-the-art methods. Notably, our FedGSP outperforms the second-best method by an average margin of 3.28% on all heterophilic datasets.

### Emergence of the Primacy Effect in Structured State-Space Models 
[[arxiv](https://arxiv.org/abs/2502.13729)] [[cool](https://papers.cool/arxiv/2502.13729)] [[pdf](https://arxiv.org/pdf/2502.13729)]
> **Authors**: Takashi Morita
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,神经和进化计算,神经元和认知
- **Abstract**: Human and animal memory for sequentially presented items is well-documented to be more accurate for those at the beginning and end of the sequence, phenomena known as the primacy and recency effects, respectively. By contrast, artificial neural network (ANN) models are typically designed with a memory that decays monotonically over time. Accordingly, ANNs are expected to show the recency effect but not the primacy effect. Contrary to this theoretical expectation, however, the present study reveals a counterintuitive finding: a recently developed ANN architecture, called structured state-space models, exhibits the primacy effect when trained and evaluated on a synthetic task that mirrors psychological memory experiments. Given that this model was originally designed for recovering neuronal activity patterns observed in biological brains, this result provides a novel perspective on the psychological primacy effect while also posing a non-trivial puzzle for the current theories in machine learning.

### Learning Novel Transformer Architecture for Time-series Forecasting 
[[arxiv](https://arxiv.org/abs/2502.13721)] [[cool](https://papers.cool/arxiv/2502.13721)] [[pdf](https://arxiv.org/pdf/2502.13721)]
> **Authors**: Juyuan Zhang,Wei Zhu,Jiechao Gao
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Despite the success of Transformer-based models in the time-series prediction (TSP) tasks, the existing Transformer architecture still face limitations and the literature lacks comprehensive explorations into alternative architectures. To address these challenges, we propose AutoFormer-TS, a novel framework that leverages a comprehensive search space for Transformer architectures tailored to TSP tasks. Our framework introduces a differentiable neural architecture search (DNAS) method, AB-DARTS, which improves upon existing DNAS approaches by enhancing the identification of optimal operations within the architecture. AutoFormer-TS systematically explores alternative attention mechanisms, activation functions, and encoding operations, moving beyond the traditional Transformer design. Extensive experiments demonstrate that AutoFormer-TS consistently outperforms state-of-the-art baselines across various TSP benchmarks, achieving superior forecasting accuracy while maintaining reasonable training efficiency.

### Tight Generalization Bounds for Large-Margin Halfspaces 
[[arxiv](https://arxiv.org/abs/2502.13692)] [[cool](https://papers.cool/arxiv/2502.13692)] [[pdf](https://arxiv.org/pdf/2502.13692)]
> **Authors**: Kasper Green Larsen,Natascha Schalburg
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,统计理论
- **Abstract**: We prove the first generalization bound for large-margin halfspaces that is asymptotically tight in the tradeoff between the margin, the fraction of training points with the given margin, the failure probability and the number of training points.

### Generalization error bound for denoising score matching under relaxed manifold assumption 
[[arxiv](https://arxiv.org/abs/2502.13662)] [[cool](https://papers.cool/arxiv/2502.13662)] [[pdf](https://arxiv.org/pdf/2502.13662)]
> **Authors**: Konstantin Yakovlev,Nikita Puchkin
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 70 pages; refined rates of convergence in the bounded case
- **标题**: None
- **领域**: 机器学习,统计理论,机器学习
- **Abstract**: We examine theoretical properties of the denoising score matching estimate. We model the density of observations with a nonparametric Gaussian mixture. We significantly relax the standard manifold assumption allowing the samples step away from the manifold. At the same time, we are still able to leverage a nice distribution structure. We derive non-asymptotic bounds on the approximation and generalization errors of the denoising score matching estimate. The rates of convergence are determined by the intrinsic dimension. Furthermore, our bounds remain valid even if we allow the ambient dimension grow polynomially with the sample size.

### Towards Invariance to Node Identifiers in Graph Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.13660)] [[cool](https://papers.cool/arxiv/2502.13660)] [[pdf](https://arxiv.org/pdf/2502.13660)]
> **Authors**: Maya Bechler-Speicher,Moshe Eliasof,Carola-Bibiane Schonlieb,Ran Gilad-Bachrach,Amir Globerson
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: arXiv admin note: text overlap with arXiv:2411.02271
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Message-Passing Graph Neural Networks (GNNs) are known to have limited expressive power, due to their message passing structure. One mechanism for circumventing this limitation is to add unique node identifiers (IDs), which break the symmetries that underlie the expressivity limitation. In this work, we highlight a key limitation of the ID framework, and propose an approach for addressing it. We begin by observing that the final output of the GNN should clearly not depend on the specific IDs used. We then show that in practice this does not hold, and thus the learned network does not possess this desired structural property. Such invariance to node IDs may be enforced in several ways, and we discuss their theoretical properties. We then propose a novel regularization method that effectively enforces ID invariance to the network. Extensive evaluations on both real-world and synthetic tasks demonstrate that our approach significantly improves ID invariance and, in turn, often boosts generalization performance.

### Integrating Inverse and Forward Modeling for Sparse Temporal Data from Sensor Networks 
[[arxiv](https://arxiv.org/abs/2502.13638)] [[cool](https://papers.cool/arxiv/2502.13638)] [[pdf](https://arxiv.org/pdf/2502.13638)]
> **Authors**: Julian Vexler,Björn Vieten,Martin Nelke,Stefan Kramer
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: We present CavePerception, a framework for the analysis of sparse data from sensor networks that incorporates elements of inverse modeling and forward modeling. By integrating machine learning with physical modeling in a hypotheses space, we aim to improve the interpretability of sparse, noisy, and potentially incomplete sensor data. The framework assumes data from a two-dimensional sensor network laid out in a graph structure that detects certain objects, with certain motion patterns. Examples of such sensors are magnetometers. Given knowledge about the objects and the way they act on the sensors, one can develop a data generator that produces data from simulated motions of the objects across the sensor field. The framework uses the simulated data to infer object behaviors across the sensor network. The approach is experimentally tested on real-world data, where magnetometers are used on an airport to detect and identify aircraft motions. Experiments demonstrate the value of integrating inverse and forward modeling, enabling intelligent systems to better understand and predict complex, sensor-driven events.

### Concept Layers: Enhancing Interpretability and Intervenability via LLM Conceptualization 
[[arxiv](https://arxiv.org/abs/2502.13632)] [[cool](https://papers.cool/arxiv/2502.13632)] [[pdf](https://arxiv.org/pdf/2502.13632)]
> **Authors**: Or Raphael Bidusa,Shaul Markovitch
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: The opaque nature of Large Language Models (LLMs) has led to significant research efforts aimed at enhancing their interpretability, primarily through post-hoc methods. More recent in-hoc approaches, such as Concept Bottleneck Models (CBMs), offer both interpretability and intervenability by incorporating explicit concept representations. However, these methods suffer from key limitations, including reliance on labeled concept datasets and significant architectural modifications that challenges re-integration into existing system pipelines. In this work, we introduce a new methodology for incorporating interpretability and intervenability into an existing model by integrating Concept Layers (CLs) into its architecture. Our approach projects the model's internal vector representations into a conceptual, explainable vector space before reconstructing and feeding them back into the model. Furthermore, we eliminate the need for a human-selected concept set by algorithmically searching an ontology for a set of concepts that can be either task-specific or task-agnostic. We evaluate CLs across multiple tasks, demonstrating that they maintain the original model's performance and agreement while enabling meaningful interventions. Additionally, we present a proof of concept showcasing an intervenability interface, allowing users to adjust model behavior dynamically, such as mitigating biases during inference.

### Toward Robust Non-Transferable Learning: A Survey and Benchmark 
[[arxiv](https://arxiv.org/abs/2502.13593)] [[cool](https://papers.cool/arxiv/2502.13593)] [[pdf](https://arxiv.org/pdf/2502.13593)]
> **Authors**: Ziming Hong,Yongli Xiang,Tongliang Liu
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,密码学和安全,计算机视觉和模式识别
- **Abstract**: Over the past decades, researchers have primarily focused on improving the generalization abilities of models, with limited attention given to regulating such generalization. However, the ability of models to generalize to unintended data (e.g., harmful or unauthorized data) can be exploited by malicious adversaries in unforeseen ways, potentially resulting in violations of model ethics. Non-transferable learning (NTL), a task aimed at reshaping the generalization abilities of deep learning models, was proposed to address these challenges. While numerous methods have been proposed in this field, a comprehensive review of existing progress and a thorough analysis of current limitations remain lacking. In this paper, we bridge this gap by presenting the first comprehensive survey on NTL and introducing NTLBench, the first benchmark to evaluate NTL performance and robustness within a unified framework. Specifically, we first introduce the task settings, general framework, and criteria of NTL, followed by a summary of NTL approaches. Furthermore, we emphasize the often-overlooked issue of robustness against various attacks that can destroy the non-transferable mechanism established by NTL. Experiments conducted via NTLBench verify the limitations of existing NTL methods in robustness. Finally, we discuss the practical applications of NTL, along with its future directions and associated challenges.

### Multi-Target Radar Search and Track Using Sequence-Capable Deep Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.13584)] [[cool](https://papers.cool/arxiv/2502.13584)] [[pdf](https://arxiv.org/pdf/2502.13584)]
> **Authors**: Jan-Hendrik Ewers,David Cormack,Joe Gibbs,David Anderson
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: Accepted for RLDM 2025, submitted to IEEE SSP 2025
- **标题**: None
- **领域**: 机器学习,系统与控制
- **Abstract**: The research addresses sensor task management for radar systems, focusing on efficiently searching and tracking multiple targets using reinforcement learning. The approach develops a 3D simulation environment with an active electronically scanned array radar, using a multi-target tracking algorithm to improve observation data quality. Three neural network architectures were compared including an approach using fated recurrent units with multi-headed self-attention. Two pre-training techniques were applied: behavior cloning to approximate a random search strategy and an auto-encoder to pre-train the feature extractor. Experimental results revealed that search performance was relatively consistent across most methods. The real challenge emerged in simultaneously searching and tracking targets. The multi-headed self-attention architecture demonstrated the most promising results, highlighting the potential of sequence-capable architectures in handling dynamic tracking scenarios. The key contribution lies in demonstrating how reinforcement learning can optimize sensor management, potentially improving radar systems' ability to identify and track multiple targets in complex environments.

### Unraveling the Localized Latents: Learning Stratified Manifold Structures in LLM Embedding Space with Sparse Mixture-of-Experts 
[[arxiv](https://arxiv.org/abs/2502.13577)] [[cool](https://papers.cool/arxiv/2502.13577)] [[pdf](https://arxiv.org/pdf/2502.13577)]
> **Authors**: Xin Li,Anand Sarwate
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: However, real-world data often exhibit complex local structures that can be challenging for single-model approaches with a smooth global manifold in the embedding space to unravel. In this work, we conjecture that in the latent space of these large language models, the embeddings live in a local manifold structure with different dimensions depending on the perplexities and domains of the input data, commonly referred to as a Stratified Manifold structure, which in combination form a structured space known as a Stratified Space. To investigate the validity of this structural claim, we propose an analysis framework based on a Mixture-of-Experts (MoE) model where each expert is implemented with a simple dictionary learning algorithm at varying sparsity levels. By incorporating an attention-based soft-gating network, we verify that our model learns specialized sub-manifolds for an ensemble of input data sources, reflecting the semantic stratification in LLM embedding space. We further analyze the intrinsic dimensions of these stratified sub-manifolds and present extensive statistics on expert assignments, gating entropy, and inter-expert distances. Our experimental results demonstrate that our method not only validates the claim of a stratified manifold structure in the LLM embedding space, but also provides interpretable clusters that align with the intrinsic semantic variations of the input data.

### Beyond One-Size-Fits-All: Tailored Benchmarks for Efficient Evaluation 
[[arxiv](https://arxiv.org/abs/2502.13576)] [[cool](https://papers.cool/arxiv/2502.13576)] [[pdf](https://arxiv.org/pdf/2502.13576)]
> **Authors**: Peiwen Yuan,Yueqi Zhang,Shaoxiong Feng,Yiwei Li,Xinglin Wang,Jiayi Shi,Chuyi Tan,Boyuan Pan,Yao Hu,Kan Li
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Evaluating models on large benchmarks is very resource-intensive, especially during the period of rapid model evolution. Existing efficient evaluation methods estimate the performance of target models by testing them only on a small and static coreset of the benchmark, which is derived from the publicly available evaluation results of source models. These methods rely on the assumption that target models have high prediction consistency with source models. However, we demonstrate that it doesn't generalize well in practice. To alleviate the inconsistency issue, we present TailoredBench, a method that conducts customized evaluation tailored to each target model. Specifically, a Global-coreset is first constructed as a probe to identify the most consistent source models for each target model with an adaptive source model selection strategy. Afterwards, a scalable K-Medoids clustering algorithm is proposed to extend the Global-coreset to a tailored Native-coreset for each target model. According to the predictions on Native-coresets, we obtain the performance of target models on the whole benchmark with a calibrated estimation strategy. Comprehensive experiments on 5 benchmarks across over 300 models demonstrate that compared to best performing baselines, TailoredBench achieves an average reduction of 31.4% in MAE of accuracy estimates under the same inference budgets, showcasing strong effectiveness and generalizability.

### ETS: Efficient Tree Search for Inference-Time Scaling 
[[arxiv](https://arxiv.org/abs/2502.13575)] [[cool](https://papers.cool/arxiv/2502.13575)] [[pdf](https://arxiv.org/pdf/2502.13575)]
> **Authors**: Coleman Hooper,Sehoon Kim,Suhong Moon,Kerem Dilmen,Monishwaran Maheswaran,Nicholas Lee,Michael W. Mahoney,Sophia Shao,Kurt Keutzer,Amir Gholami
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 11 pages
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Test-time compute scaling has emerged as a new axis along which to improve model accuracy, where additional computation is used at inference time to allow the model to think longer for more challenging problems. One promising approach for test-time compute scaling is search against a process reward model, where a model generates multiple potential candidates at each step of the search, and these partial trajectories are then scored by a separate reward model in order to guide the search process. The diversity of trajectories in the tree search process affects the accuracy of the search, since increasing diversity promotes more exploration. However, this diversity comes at a cost, as divergent trajectories have less KV sharing, which means they consume more memory and slow down the search process. Previous search methods either do not perform sufficient exploration, or else explore diverse trajectories but have high latency. We address this challenge by proposing Efficient Tree Search (ETS), which promotes KV sharing by pruning redundant trajectories while maintaining necessary diverse trajectories. ETS incorporates a linear programming cost model to promote KV cache sharing by penalizing the number of nodes retained, while incorporating a semantic coverage term into the cost model to ensure that we retain trajectories which are semantically different. We demonstrate how ETS can achieve 1.8$\times$ reduction in average KV cache size during the search process, leading to 1.4$\times$ increased throughput relative to prior state-of-the-art methods, with minimal accuracy degradation and without requiring any custom kernel implementation. Code is available at: https://github.com/SqueezeAILab/ETS.

### Noise May Contain Transferable Knowledge: Understanding Semi-supervised Heterogeneous Domain Adaptation from an Empirical Perspective 
[[arxiv](https://arxiv.org/abs/2502.13573)] [[cool](https://papers.cool/arxiv/2502.13573)] [[pdf](https://arxiv.org/pdf/2502.13573)]
> **Authors**: Yuan Yao,Xiaopu Zhang,Yu Zhang,Jian Jin,Qiang Yang
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Semi-supervised heterogeneous domain adaptation (SHDA) addresses learning across domains with distinct feature representations and distributions, where source samples are labeled while most target samples are unlabeled, with only a small fraction labeled. Moreover, there is no one-to-one correspondence between source and target samples. Although various SHDA methods have been developed to tackle this problem, the nature of the knowledge transferred across heterogeneous domains remains unclear. This paper delves into this question from an empirical perspective. We conduct extensive experiments on about 330 SHDA tasks, employing two supervised learning methods and seven representative SHDA methods. Surprisingly, our observations indicate that both the category and feature information of source samples do not significantly impact the performance of the target domain. Additionally, noise drawn from simple distributions, when used as source samples, may contain transferable knowledge. Based on this insight, we perform a series of experiments to uncover the underlying principles of transferable knowledge in SHDA. Specifically, we design a unified Knowledge Transfer Framework (KTF) for SHDA. Based on the KTF, we find that the transferable knowledge in SHDA primarily stems from the transferability and discriminability of the source domain. Consequently, ensuring those properties in source samples, regardless of their origin (e.g., image, text, noise), can enhance the effectiveness of knowledge transfer in SHDA tasks. The codes and datasets are available at https://github.com/yyyaoyuan/SHDA.

### LSR-Adapt: Ultra-Efficient Parameter Tuning with Matrix Low Separation Rank Kernel Adaptation 
[[arxiv](https://arxiv.org/abs/2502.13568)] [[cool](https://papers.cool/arxiv/2502.13568)] [[pdf](https://arxiv.org/pdf/2502.13568)]
> **Authors**: Xin Li,Anand Sarwate
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Imposing an effective structural assumption on neural network weight matrices has been the major paradigm for designing Parameter-Efficient Fine-Tuning (PEFT) systems for adapting modern large pre-trained models to various downstream tasks. However, low rank based adaptation has become increasingly challenging due to the sheer scale of modern large language models. In this paper, we propose an effective kernelization to further reduce the number of parameters required for adaptation tasks. Specifically, from the classical idea in numerical analysis regarding matrix Low-Separation-Rank (LSR) representations, we develop a kernel using this representation for the low rank adapter matrices of the linear layers from large networks, named the Low Separation Rank Adaptation (LSR-Adapt) kernel. With the ultra-efficient kernel representation of the low rank adapter matrices, we manage to achieve state-of-the-art performance with even higher accuracy with almost half the number of parameters as compared to conventional low rank based methods. This structural assumption also opens the door to further GPU-side optimizations due to the highly parallelizable nature of Kronecker computations.

### Are Large Language Models In-Context Graph Learners? 
[[arxiv](https://arxiv.org/abs/2502.13562)] [[cool](https://papers.cool/arxiv/2502.13562)] [[pdf](https://arxiv.org/pdf/2502.13562)]
> **Authors**: Jintang Li,Ruofan Wu,Yuchang Zhu,Huizhe Zhang,Liang Chen,Zibin Zheng
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: Preprint, under review
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Large language models (LLMs) have demonstrated remarkable in-context reasoning capabilities across a wide range of tasks, particularly with unstructured inputs such as language or images. However, LLMs struggle to handle structured data, such as graphs, due to their lack of understanding of non-Euclidean structures. As a result, without additional fine-tuning, their performance significantly lags behind that of graph neural networks (GNNs) in graph learning tasks. In this paper, we show that learning on graph data can be conceptualized as a retrieval-augmented generation (RAG) process, where specific instances (e.g., nodes or edges) act as queries, and the graph itself serves as the retrieved context. Building on this insight, we propose a series of RAG frameworks to enhance the in-context learning capabilities of LLMs for graph learning tasks. Comprehensive evaluations demonstrate that our proposed RAG frameworks significantly improve LLM performance on graph-based tasks, particularly in scenarios where a pretrained LLM must be used without modification or accessed via an API.

### Democratizing Large Language Model-Based Graph Data Augmentation via Latent Knowledge Graphs 
[[arxiv](https://arxiv.org/abs/2502.13555)] [[cool](https://papers.cool/arxiv/2502.13555)] [[pdf](https://arxiv.org/pdf/2502.13555)]
> **Authors**: Yushi Feng,Tsai Hor Chan,Guosheng Yin,Lequan Yu
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Data augmentation is necessary for graph representation learning due to the scarcity and noise present in graph data. Most of the existing augmentation methods overlook the context information inherited from the dataset as they rely solely on the graph structure for augmentation. Despite the success of some large language model-based (LLM) graph learning methods, they are mostly white-box which require access to the weights or latent features from the open-access LLMs, making them difficult to be democratized for everyone as existing LLMs are mostly closed-source for commercial considerations. To overcome these limitations, we propose a black-box context-driven graph data augmentation approach, with the guidance of LLMs -- DemoGraph. Leveraging the text prompt as context-related information, we task the LLM with generating knowledge graphs (KGs), which allow us to capture the structural interactions from the text outputs. We then design a dynamic merging schema to stochastically integrate the LLM-generated KGs into the original graph during training. To control the sparsity of the augmented graph, we further devise a granularity-aware prompting strategy and an instruction fine-tuning module, which seamlessly generates text prompts according to different granularity levels of the dataset. Extensive experiments on various graph learning tasks validate the effectiveness of our method over existing graph data augmentation methods. Notably, our approach excels in scenarios involving electronic health records (EHRs), which validates its maximal utilization of contextual knowledge, leading to enhanced predictive performance and interpretability.

### Train Small, Infer Large: Memory-Efficient LoRA Training for Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.13533)] [[cool](https://papers.cool/arxiv/2502.13533)] [[pdf](https://arxiv.org/pdf/2502.13533)]
> **Authors**: Jun Zhang,Jue Wang,Huan Li,Lidan Shou,Ke Chen,Yang You,Guiming Xie,Xuejian Gong,Kunlong Zhou
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: Accepted at ICLR 2025
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Large Language Models (LLMs) have significantly advanced natural language processing with exceptional task generalization capabilities. Low-Rank Adaption (LoRA) offers a cost-effective fine-tuning solution, freezing the original model parameters and training only lightweight, low-rank adapter matrices. However, the memory footprint of LoRA is largely dominated by the original model parameters. To mitigate this, we propose LoRAM, a memory-efficient LoRA training scheme founded on the intuition that many neurons in over-parameterized LLMs have low training utility but are essential for inference. LoRAM presents a unique twist: it trains on a pruned (small) model to obtain pruned low-rank matrices, which are then recovered and utilized with the original (large) model for inference. Additionally, minimal-cost continual pre-training, performed by the model publishers in advance, aligns the knowledge discrepancy between pruned and original models. Our extensive experiments demonstrate the efficacy of LoRAM across various pruning strategies and downstream tasks. For a model with 70 billion parameters, LoRAM enables training on a GPU with only 20G HBM, replacing an A100-80G GPU for LoRA training and 15 GPUs for full fine-tuning. Specifically, QLoRAM implemented by structured pruning combined with 4-bit quantization, for LLaMA-3.1-70B (LLaMA-2-70B), reduces the parameter storage cost that dominates the memory usage in low-rank matrix training by 15.81$\times$ (16.95$\times$), while achieving dominant performance gains over both the original LLaMA-3.1-70B (LLaMA-2-70B) and LoRA-trained LLaMA-3.1-8B (LLaMA-2-13B).

### AS-GCL: Asymmetric Spectral Augmentation on Graph Contrastive Learning 
[[arxiv](https://arxiv.org/abs/2502.13525)] [[cool](https://papers.cool/arxiv/2502.13525)] [[pdf](https://arxiv.org/pdf/2502.13525)]
> **Authors**: Ruyue Liu,Rong Yin,Yong Liu,Xiaoshuai Hao,Haichao Shi,Can Ma,Weiping Wang
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: Accepted by TMM
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Graph Contrastive Learning (GCL) has emerged as the foremost approach for self-supervised learning on graph-structured data. GCL reduces reliance on labeled data by learning robust representations from various augmented views. However, existing GCL methods typically depend on consistent stochastic augmentations, which overlook their impact on the intrinsic structure of the spectral domain, thereby limiting the model's ability to generalize effectively. To address these limitations, we propose a novel paradigm called AS-GCL that incorporates asymmetric spectral augmentation for graph contrastive learning. A typical GCL framework consists of three key components: graph data augmentation, view encoding, and contrastive loss. Our method introduces significant enhancements to each of these components. Specifically, for data augmentation, we apply spectral-based augmentation to minimize spectral variations, strengthen structural invariance, and reduce noise. With respect to encoding, we employ parameter-sharing encoders with distinct diffusion operators to generate diverse, noise-resistant graph views. For contrastive loss, we introduce an upper-bound loss function that promotes generalization by maintaining a balanced distribution of intra- and inter-class distance. To our knowledge, we are the first to encode augmentation views of the spectral domain using asymmetric encoders. Extensive experiments on eight benchmark datasets across various node-level tasks demonstrate the advantages of the proposed method.

### Enhancing Machine Learning Potentials through Transfer Learning across Chemical Elements 
[[arxiv](https://arxiv.org/abs/2502.13522)] [[cool](https://papers.cool/arxiv/2502.13522)] [[pdf](https://arxiv.org/pdf/2502.13522)]
> **Authors**: Sebastien Röcken,Julija Zavadlav
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,材料科学
- **Abstract**: Machine Learning Potentials (MLPs) can enable simulations of ab initio accuracy at orders of magnitude lower computational cost. However, their effectiveness hinges on the availability of considerable datasets to ensure robust generalization across chemical space and thermodynamic conditions. The generation of such datasets can be labor-intensive, highlighting the need for innovative methods to train MLPs in data-scarce scenarios. Here, we introduce transfer learning of potential energy surfaces between chemically similar elements. Specifically, we leverage the trained MLP for silicon to initialize and expedite the training of an MLP for germanium. Utilizing classical force field and ab initio datasets, we demonstrate that transfer learning surpasses traditional training from scratch in force prediction, leading to more stable simulations and improved temperature transferability. These advantages become even more pronounced as the training dataset size decreases. The out-of-target property analysis shows that transfer learning leads to beneficial but sometimes adversarial effects. Our findings demonstrate that transfer learning across chemical elements is a promising technique for developing accurate and numerically stable MLPs, particularly in a data-scarce regime.

### Smoothed Normalization for Efficient Distributed Private Optimization 
[[arxiv](https://arxiv.org/abs/2502.13482)] [[cool](https://papers.cool/arxiv/2502.13482)] [[pdf](https://arxiv.org/pdf/2502.13482)]
> **Authors**: Egor Shulgin,Sarit Khirirat,Peter Richtárik
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 36 pages
- **标题**: None
- **领域**: 机器学习,密码学和安全,分布式、并行和集群计算,优化与控制,机器学习
- **Abstract**: Federated learning enables training machine learning models while preserving the privacy of participants. Surprisingly, there is no differentially private distributed method for smooth, non-convex optimization problems. The reason is that standard privacy techniques require bounding the participants' contributions, usually enforced via $\textit{clipping}$ of the updates. Existing literature typically ignores the effect of clipping by assuming the boundedness of gradient norms or analyzes distributed algorithms with clipping but ignores DP constraints. In this work, we study an alternative approach via $\textit{smoothed normalization}$ of the updates motivated by its favorable performance in the single-node setting. By integrating smoothed normalization with an error-feedback mechanism, we design a new distributed algorithm $α$-$\sf NormEC$. We prove that our method achieves a superior convergence rate over prior works. By extending $α$-$\sf NormEC$ to the DP setting, we obtain the first differentially private distributed optimization algorithm with provable convergence guarantees. Finally, our empirical results from neural network training indicate robust convergence of $α$-$\sf NormEC$ across different parameter settings.

### Some Insights of Construction of Feature Graph to Learn Pairwise Feature Interactions with Graph Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.13471)] [[cool](https://papers.cool/arxiv/2502.13471)] [[pdf](https://arxiv.org/pdf/2502.13471)]
> **Authors**: Phaphontee Yamchote,Saw Nay Htet Win,Chainarong Amornbunchornvej,Thanapon Noraset
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: This is the draft before submitting to any journal
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Feature interaction is crucial in predictive machine learning models, as it captures the relationships between features that influence model performance. In this work, we focus on pairwise interactions and investigate their importance in constructing feature graphs for Graph Neural Networks (GNNs). Rather than proposing new methods, we leverage existing GNN models and tools to explore the relationship between feature graph structures and their effectiveness in modeling interactions. Through experiments on synthesized datasets, we uncover that edges between interacting features are important for enabling GNNs to model feature interactions effectively. We also observe that including non-interaction edges can act as noise, degrading model performance. Furthermore, we provide theoretical support for sparse feature graph selection using the Minimum Description Length (MDL) principle. We prove that feature graphs retaining only necessary interaction edges yield a more efficient and interpretable representation than complete graphs, aligning with Occam's Razor. Our findings offer both theoretical insights and practical guidelines for designing feature graphs that improve the performance and interpretability of GNN models.

### Continuous K-Max Bandits 
[[arxiv](https://arxiv.org/abs/2502.13467)] [[cool](https://papers.cool/arxiv/2502.13467)] [[pdf](https://arxiv.org/pdf/2502.13467)]
> **Authors**: Yu Chen,Siwei Wang,Longbo Huang,Wei Chen
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We study the $K$-Max combinatorial multi-armed bandits problem with continuous outcome distributions and weak value-index feedback: each base arm has an unknown continuous outcome distribution, and in each round the learning agent selects $K$ arms, obtains the maximum value sampled from these $K$ arms as reward and observes this reward together with the corresponding arm index as feedback. This setting captures critical applications in recommendation systems, distributed computing, server scheduling, etc. The continuous $K$-Max bandits introduce unique challenges, including discretization error from continuous-to-discrete conversion, non-deterministic tie-breaking under limited feedback, and biased estimation due to partial observability. Our key contribution is the computationally efficient algorithm DCK-UCB, which combines adaptive discretization with bias-corrected confidence bounds to tackle these challenges. For general continuous distributions, we prove that DCK-UCB achieves a $\widetilde{\mathcal{O}}(T^{3/4})$ regret upper bound, establishing the first sublinear regret guarantee for this setting. Furthermore, we identify an important special case with exponential distributions under full-bandit feedback. In this case, our proposed algorithm MLE-Exp enables $\widetilde{\mathcal{O}}(\sqrt{T})$ regret upper bound through maximal log-likelihood estimation, achieving near-minimax optimality.

### Provably Efficient Multi-Objective Bandit Algorithms under Preference-Centric Customization 
[[arxiv](https://arxiv.org/abs/2502.13457)] [[cool](https://papers.cool/arxiv/2502.13457)] [[pdf](https://arxiv.org/pdf/2502.13457)]
> **Authors**: Linfeng Cao,Ming Shi,Ness B. Shroff
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Multi-objective multi-armed bandit (MO-MAB) problems traditionally aim to achieve Pareto optimality. However, real-world scenarios often involve users with varying preferences across objectives, resulting in a Pareto-optimal arm that may score high for one user but perform quite poorly for another. This highlights the need for customized learning, a factor often overlooked in prior research. To address this, we study a preference-aware MO-MAB framework in the presence of explicit user preference. It shifts the focus from achieving Pareto optimality to further optimizing within the Pareto front under preference-centric customization. To our knowledge, this is the first theoretical study of customized MO-MAB optimization with explicit user preferences. Motivated by practical applications, we explore two scenarios: unknown preference and hidden preference, each presenting unique challenges for algorithm design and analysis. At the core of our algorithms are preference estimation and preference-aware optimization mechanisms to adapt to user preferences effectively. We further develop novel analytical techniques to establish near-optimal regret of the proposed algorithms. Strong empirical performance confirm the effectiveness of our approach.

### Interleaved Gibbs Diffusion for Constrained Generation 
[[arxiv](https://arxiv.org/abs/2502.13450)] [[cool](https://papers.cool/arxiv/2502.13450)] [[pdf](https://arxiv.org/pdf/2502.13450)]
> **Authors**: Gautham Govind Anil,Sachin Yadav,Dheeraj Nagaraj,Karthikeyan Shanmugam,Prateek Jain
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: We introduce Interleaved Gibbs Diffusion (IGD), a novel generative modeling framework for mixed continuous-discrete data, focusing on constrained generation problems. Prior works on discrete and continuous-discrete diffusion models assume factorized denoising distribution for fast generation, which can hinder the modeling of strong dependencies between random variables encountered in constrained generation. IGD moves beyond this by interleaving continuous and discrete denoising algorithms via a discrete time Gibbs sampling type Markov chain. IGD provides flexibility in the choice of denoisers, allows conditional generation via state-space doubling and inference time scaling via the ReDeNoise method. Empirical evaluations on three challenging tasks-solving 3-SAT, generating molecule structures, and generating layouts-demonstrate state-of-the-art performance. Notably, IGD achieves a 7% improvement on 3-SAT out of the box and achieves state-of-the-art results in molecule generation without relying on equivariant diffusion or domain-specific architectures. We explore a wide range of modeling, and interleaving strategies along with hyperparameters in each of these problems.

### Mol-LLaMA: Towards General Understanding of Molecules in Large Molecular Language Model 
[[arxiv](https://arxiv.org/abs/2502.13449)] [[cool](https://papers.cool/arxiv/2502.13449)] [[pdf](https://arxiv.org/pdf/2502.13449)]
> **Authors**: Dongki Kim,Wonbin Lee,Sung Ju Hwang
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: Project Page: https://mol-llama.github.io/
- **标题**: None
- **领域**: 机器学习,化学物理
- **Abstract**: Understanding molecules is key to understanding organisms and driving advances in drug discovery, requiring interdisciplinary knowledge across chemistry and biology. Although large molecular language models have achieved notable success in interpreting molecular structures, their instruction datasets are limited to the specific knowledge from task-oriented datasets and do not fully cover the fundamental characteristics of molecules, hindering their abilities as general-purpose molecular assistants. To address this issue, we propose Mol-LLaMA, a large molecular language model that grasps the general knowledge centered on molecules via multi-modal instruction tuning. To this end, we design key data types that encompass the fundamental features of molecules, incorporating essential knowledge from molecular structures. In addition, to improve understanding of molecular features, we introduce a module that integrates complementary information from different molecular encoders, leveraging the distinct advantages of different molecular representations. Our experimental results demonstrate that Mol-LLaMA is capable of comprehending the general features of molecules and generating relevant responses to users' queries with detailed explanations, implying its potential as a general-purpose assistant for molecular analysis. Our project page is at https://mol-llama.github.io/.

## 多代理系统(cs.MA:Multiagent Systems)

### Multi-Agent Risks from Advanced AI 
[[arxiv](https://arxiv.org/abs/2502.14143)] [[cool](https://papers.cool/arxiv/2502.14143)] [[pdf](https://arxiv.org/pdf/2502.14143)]
> **Authors**: Lewis Hammond,Alan Chan,Jesse Clifton,Jason Hoelscher-Obermaier,Akbir Khan,Euan McLean,Chandler Smith,Wolfram Barfuss,Jakob Foerster,Tomáš Gavenčiak,The Anh Han,Edward Hughes,Vojtěch Kovařík,Jan Kulveit,Joel Z. Leibo,Caspar Oesterheld,Christian Schroeder de Witt,Nisarg Shah,Michael Wellman,Paolo Bova,Theodor Cimpeanu,Carson Ezell,Quentin Feuillade-Montixi,Matija Franklin,Esben Kran, et al. (19 additional authors not shown)
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: CooperativeAIFoundation, Technical Report #1
- **标题**: None
- **领域**: 多代理系统,人工智能,计算机与社会,新兴技术,机器学习
- **Abstract**: The rapid development of advanced AI agents and the imminent deployment of many instances of these agents will give rise to multi-agent systems of unprecedented complexity. These systems pose novel and under-explored risks. In this report, we provide a structured taxonomy of these risks by identifying three key failure modes (miscoordination, conflict, and collusion) based on agents' incentives, as well as seven key risk factors (information asymmetries, network effects, selection pressures, destabilising dynamics, commitment problems, emergent agency, and multi-agent security) that can underpin them. We highlight several important instances of each risk, as well as promising directions to help mitigate them. By anchoring our analysis in a range of real-world examples and experimental evidence, we illustrate the distinct challenges posed by multi-agent systems and their implications for the safety, governance, and ethics of advanced AI.

### Human-Artificial Interaction in the Age of Agentic AI: A System-Theoretical Approach 
[[arxiv](https://arxiv.org/abs/2502.14000)] [[cool](https://papers.cool/arxiv/2502.14000)] [[pdf](https://arxiv.org/pdf/2502.14000)]
> **Authors**: Uwe M. Borghoff,Paolo Bottoni,Remo Pareschi
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 27 pages, 10 figures
- **标题**: None
- **领域**: 多代理系统,人工智能,人机交互
- **Abstract**: This paper presents a novel perspective on human-computer interaction (HCI), framing it as a dynamic interplay between human and computational agents within a networked system. Going beyond traditional interface-based approaches, we emphasize the importance of coordination and communication among heterogeneous agents with different capabilities, roles, and goals. A key distinction is made between multi-agent systems (MAS) and Centaurian systems, which represent two different paradigms of human-AI collaboration. MAS maintain agent autonomy, with structured protocols enabling cooperation, while Centaurian systems deeply integrate human and AI capabilities, creating unified decision-making entities. To formalize these interactions, we introduce a framework for communication spaces, structured into surface, observation, and computation layers, ensuring seamless integration between MAS and Centaurian architectures, where colored Petri nets effectively represent structured Centaurian systems and high-level reconfigurable networks address the dynamic nature of MAS. Our research has practical applications in autonomous robotics, human-in-the-loop decision making, and AI-driven cognitive architectures, and provides a foundation for next-generation hybrid intelligence systems that balance structured coordination with emergent behavior.

## 机器人技术(cs.RO:Robotics)

### Mem2Ego: Empowering Vision-Language Models with Global-to-Ego Memory for Long-Horizon Embodied Navigation 
[[arxiv](https://arxiv.org/abs/2502.14254)] [[cool](https://papers.cool/arxiv/2502.14254)] [[pdf](https://arxiv.org/pdf/2502.14254)]
> **Authors**: Lingfeng Zhang,Yuecheng Liu,Zhanguang Zhang,Matin Aghaei,Yaochen Hu,Hongjian Gu,Mohammad Ali Alomrani,David Gamaliel Arcos Bravo,Raika Karimi,Atia Hamidizadeh,Haoping Xu,Guowei Huang,Zhanpeng Zhang,Tongtong Cao,Weichao Qiu,Xingyue Quan,Jianye Hao,Yuzheng Zhuang,Yingxue Zhang
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,人工智能
- **Abstract**: Recent advancements in Large Language Models (LLMs) and Vision-Language Models (VLMs) have made them powerful tools in embodied navigation, enabling agents to leverage commonsense and spatial reasoning for efficient exploration in unfamiliar environments. Existing LLM-based approaches convert global memory, such as semantic or topological maps, into language descriptions to guide navigation. While this improves efficiency and reduces redundant exploration, the loss of geometric information in language-based representations hinders spatial reasoning, especially in intricate environments. To address this, VLM-based approaches directly process ego-centric visual inputs to select optimal directions for exploration. However, relying solely on a first-person perspective makes navigation a partially observed decision-making problem, leading to suboptimal decisions in complex environments. In this paper, we present a novel vision-language model (VLM)-based navigation framework that addresses these challenges by adaptively retrieving task-relevant cues from a global memory module and integrating them with the agent's egocentric observations. By dynamically aligning global contextual information with local perception, our approach enhances spatial reasoning and decision-making in long-horizon tasks. Experimental results demonstrate that the proposed method surpasses previous state-of-the-art approaches in object navigation tasks, providing a more effective and scalable solution for embodied navigation.

### Real-Time Sampling-based Online Planning for Drone Interception 
[[arxiv](https://arxiv.org/abs/2502.14231)] [[cool](https://papers.cool/arxiv/2502.14231)] [[pdf](https://arxiv.org/pdf/2502.14231)]
> **Authors**: Gilhyun Ryou,Lukas Lao Beyer,Sertac Karaman
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: Accepted at ICRA 2025. Supplementary video: https://youtu.be/dDdshfEAZpg
- **标题**: None
- **领域**: 机器人技术,机器学习,系统与控制
- **Abstract**: This paper studies high-speed online planning in dynamic environments. The problem requires finding time-optimal trajectories that conform to system dynamics, meeting computational constraints for real-time adaptation, and accounting for uncertainty from environmental changes. To address these challenges, we propose a sampling-based online planning algorithm that leverages neural network inference to replace time-consuming nonlinear trajectory optimization, enabling rapid exploration of multiple trajectory options under uncertainty. The proposed method is applied to the drone interception problem, where a defense drone must intercept a target while avoiding collisions and handling imperfect target predictions. The algorithm efficiently generates trajectories toward multiple potential target drone positions in parallel. It then assesses trajectory reachability by comparing traversal times with the target drone's predicted arrival time, ultimately selecting the minimum-time reachable trajectory. Through extensive validation in both simulated and real-world environments, we demonstrate our method's capability for high-rate online planning and its adaptability to unpredictable movements in unstructured settings.

### Hybrid Visual Servoing of Tendon-driven Continuum Robots 
[[arxiv](https://arxiv.org/abs/2502.14092)] [[cool](https://papers.cool/arxiv/2502.14092)] [[pdf](https://arxiv.org/pdf/2502.14092)]
> **Authors**: Rana Danesh,Farrokh Janabi-Sharifi,Farhad Aghili
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,计算机视觉和模式识别,系统与控制
- **Abstract**: This paper introduces a novel Hybrid Visual Servoing (HVS) approach for controlling tendon-driven continuum robots (TDCRs). The HVS system combines Image-Based Visual Servoing (IBVS) with Deep Learning-Based Visual Servoing (DLBVS) to overcome the limitations of each method and improve overall performance. IBVS offers higher accuracy and faster convergence in feature-rich environments, while DLBVS enhances robustness against disturbances and offers a larger workspace. By enabling smooth transitions between IBVS and DLBVS, the proposed HVS ensures effective control in dynamic, unstructured environments. The effectiveness of this approach is validated through simulations and real-world experiments, demonstrating that HVS achieves reduced iteration time, faster convergence, lower final error, and smoother performance compared to DLBVS alone, while maintaining DLBVS's robustness in challenging conditions such as occlusions, lighting changes, actuator noise, and physical impacts.

### A Training-Free Framework for Precise Mobile Manipulation of Small Everyday Objects 
[[arxiv](https://arxiv.org/abs/2502.13964)] [[cool](https://papers.cool/arxiv/2502.13964)] [[pdf](https://arxiv.org/pdf/2502.13964)]
> **Authors**: Arjun Gupta,Rishik Sathua,Saurabh Gupta
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: Project webpage: https://arjung128.github.io/svm
- **标题**: None
- **领域**: 机器人技术,人工智能,计算机视觉和模式识别,机器学习
- **Abstract**: Many everyday mobile manipulation tasks require precise interaction with small objects, such as grasping a knob to open a cabinet or pressing a light switch. In this paper, we develop Servoing with Vision Models (SVM), a closed-loop training-free framework that enables a mobile manipulator to tackle such precise tasks involving the manipulation of small objects. SVM employs an RGB-D wrist camera and uses visual servoing for control. Our novelty lies in the use of state-of-the-art vision models to reliably compute 3D targets from the wrist image for diverse tasks and under occlusion due to the end-effector. To mitigate occlusion artifacts, we employ vision models to out-paint the end-effector thereby significantly enhancing target localization. We demonstrate that aided by out-painting methods, open-vocabulary object detectors can serve as a drop-in module to identify semantic targets (e.g. knobs) and point tracking methods can reliably track interaction sites indicated by user clicks. This training-free method obtains an 85% zero-shot success rate on manipulating unseen objects in novel environments in the real world, outperforming an open-loop control method and an imitation learning baseline trained on 1000+ demonstrations by an absolute success rate of 50%.

### NavigateDiff: Visual Predictors are Zero-Shot Navigation Assistants 
[[arxiv](https://arxiv.org/abs/2502.13894)] [[cool](https://papers.cool/arxiv/2502.13894)] [[pdf](https://arxiv.org/pdf/2502.13894)]
> **Authors**: Yiran Qin,Ao Sun,Yuze Hong,Benyou Wang,Ruimao Zhang
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: Accepted to ICRA2025
- **标题**: None
- **领域**: 机器人技术,计算机视觉和模式识别
- **Abstract**: Navigating unfamiliar environments presents significant challenges for household robots, requiring the ability to recognize and reason about novel decoration and layout. Existing reinforcement learning methods cannot be directly transferred to new environments, as they typically rely on extensive mapping and exploration, leading to time-consuming and inefficient. To address these challenges, we try to transfer the logical knowledge and the generalization ability of pre-trained foundation models to zero-shot navigation. By integrating a large vision-language model with a diffusion network, our approach named \mname ~constructs a visual predictor that continuously predicts the agent's potential observations in the next step which can assist robots generate robust actions. Furthermore, to adapt the temporal property of navigation, we introduce temporal historical information to ensure that the predicted image is aligned with the navigation scene. We then carefully designed an information fusion framework that embeds the predicted future frames as guidance into goal-reaching policy to solve downstream image navigation tasks. This approach enhances navigation control and generalization across both simulated and real-world environments. Through extensive experimentation, we demonstrate the robustness and versatility of our method, showcasing its potential to improve the efficiency and effectiveness of robotic navigation in diverse settings.

### MILE: Model-based Intervention Learning 
[[arxiv](https://arxiv.org/abs/2502.13519)] [[cool](https://papers.cool/arxiv/2502.13519)] [[pdf](https://arxiv.org/pdf/2502.13519)]
> **Authors**: Yigit Korkmaz,Erdem Bıyık
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: International Conference on Robotics and Automation (ICRA)
- **标题**: None
- **领域**: 机器人技术,人工智能,机器学习
- **Abstract**: Imitation learning techniques have been shown to be highly effective in real-world control scenarios, such as robotics. However, these approaches not only suffer from compounding error issues but also require human experts to provide complete trajectories. Although there exist interactive methods where an expert oversees the robot and intervenes if needed, these extensions usually only utilize the data collected during intervention periods and ignore the feedback signal hidden in non-intervention timesteps. In this work, we create a model to formulate how the interventions occur in such cases, and show that it is possible to learn a policy with just a handful of expert interventions. Our key insight is that it is possible to get crucial information about the quality of the current state and the optimality of the chosen action from expert feedback, regardless of the presence or the absence of intervention. We evaluate our method on various discrete and continuous simulation environments, a real-world robotic manipulation task, as well as a human subject study. Videos and the code can be found at https://liralab.usc.edu/mile .

### Improving Collision-Free Success Rate For Object Goal Visual Navigation Via Two-Stage Training With Collision Prediction 
[[arxiv](https://arxiv.org/abs/2502.13498)] [[cool](https://papers.cool/arxiv/2502.13498)] [[pdf](https://arxiv.org/pdf/2502.13498)]
> **Authors**: Shiwei Lian,Feitian Zhang
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,计算机视觉和模式识别
- **Abstract**: The object goal visual navigation is the task of navigating to a specific target object using egocentric visual observations. Recent end-to-end navigation models based on deep reinforcement learning have achieved remarkable performance in finding and reaching target objects. However, the collision problem of these models during navigation remains unresolved, since the collision is typically neglected when evaluating the success. Although incorporating a negative reward for collision during training appears straightforward, it results in a more conservative policy, thereby limiting the agent's ability to reach targets. In addition, many of these models utilize only RGB observations, further increasing the difficulty of collision avoidance without depth information. To address these limitations, a new concept -- collision-free success is introduced to evaluate the ability of navigation models to find a collision-free path towards the target object. A two-stage training method with collision prediction is proposed to improve the collision-free success rate of the existing navigation models using RGB observations. In the first training stage, the collision prediction module supervises the agent's collision states during exploration to learn to predict the possible collision. In the second stage, leveraging the trained collision prediction, the agent learns to navigate to the target without collision. The experimental results in the AI2-THOR environment demonstrate that the proposed method greatly improves the collision-free success rate of different navigation models and outperforms other comparable collision-avoidance methods.

## 声音(cs.SD:Sound)

### Semi-supervised classification of bird vocalizations 
[[arxiv](https://arxiv.org/abs/2502.13440)] [[cool](https://papers.cool/arxiv/2502.13440)] [[pdf](https://arxiv.org/pdf/2502.13440)]
> **Authors**: Simen Hexeberg,Mandar Chitre,Matthias Hoffmann-Kuhnt,Bing Wen Low
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 声音,人工智能,计算机视觉和模式识别,音频和语音处理,定量方法
- **Abstract**: Changes in bird populations can indicate broader changes in ecosystems, making birds one of the most important animal groups to monitor. Combining machine learning and passive acoustics enables continuous monitoring over extended periods without direct human involvement. However, most existing techniques require extensive expert-labeled datasets for training and cannot easily detect time-overlapping calls in busy soundscapes. We propose a semi-supervised acoustic bird detector designed to allow both the detection of time-overlapping calls (when separated in frequency) and the use of few labeled training samples. The classifier is trained and evaluated on a combination of community-recorded open-source data and long-duration soundscape recordings from Singapore. It achieves a mean F0.5 score of 0.701 across 315 classes from 110 bird species on a hold-out test set, with an average of 11 labeled training samples per class. It outperforms the state-of-the-art BirdNET classifier on a test set of 103 bird species despite significantly fewer labeled training samples. The detector is further tested on 144 microphone-hours of continuous soundscape data. The rich soundscape in Singapore makes suppression of false positives a challenge on raw, continuous data streams. Nevertheless, we demonstrate that achieving high precision in such environments with minimal labeled training data is possible.

## 软件工程(cs.SE:Software Engineering)

### Towards Secure Program Partitioning for Smart Contracts with LLM's In-Context Learning 
[[arxiv](https://arxiv.org/abs/2502.14215)] [[cool](https://papers.cool/arxiv/2502.14215)] [[pdf](https://arxiv.org/pdf/2502.14215)]
> **Authors**: Ye Liu,Yuqing Niu,Chengyan Ma,Ruidong Han,Wei Ma,Yi Li,Debin Gao,David Lo
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: Smart contracts are highly susceptible to manipulation attacks due to the leakage of sensitive information. Addressing manipulation vulnerabilities is particularly challenging because they stem from inherent data confidentiality issues rather than straightforward implementation bugs. To tackle this by preventing sensitive information leakage, we present PartitionGPT, the first LLM-driven approach that combines static analysis with the in-context learning capabilities of large language models (LLMs) to partition smart contracts into privileged and normal codebases, guided by a few annotated sensitive data variables. We evaluated PartitionGPT on 18 annotated smart contracts containing 99 sensitive functions. The results demonstrate that PartitionGPT successfully generates compilable, and verified partitions for 78% of the sensitive functions while reducing approximately 30% code compared to function-level partitioning approach. Furthermore, we evaluated PartitionGPT on nine real-world manipulation attacks that lead to a total loss of 25 million dollars, PartitionGPT effectively prevents eight cases, highlighting its potential for broad applicability and the necessity for secure program partitioning during smart contract development to diminish manipulation vulnerabilities.

### Do LLMs Consider Security? An Empirical Study on Responses to Programming Questions 
[[arxiv](https://arxiv.org/abs/2502.14202)] [[cool](https://papers.cool/arxiv/2502.14202)] [[pdf](https://arxiv.org/pdf/2502.14202)]
> **Authors**: Amirali Sajadi,Binh Le,Anh Nguyen,Kostadin Damevski,Preetha Chatterjee
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,人工智能,计算语言学,机器学习
- **Abstract**: The widespread adoption of conversational LLMs for software development has raised new security concerns regarding the safety of LLM-generated content. Our motivational study outlines ChatGPT's potential in volunteering context-specific information to the developers, promoting safe coding practices. Motivated by this finding, we conduct a study to evaluate the degree of security awareness exhibited by three prominent LLMs: Claude 3, GPT-4, and Llama 3. We prompt these LLMs with Stack Overflow questions that contain vulnerable code to evaluate whether they merely provide answers to the questions or if they also warn users about the insecure code, thereby demonstrating a degree of security awareness. Further, we assess whether LLM responses provide information about the causes, exploits, and the potential fixes of the vulnerability, to help raise users' awareness. Our findings show that all three models struggle to accurately detect and warn users about vulnerabilities, achieving a detection rate of only 12.6% to 40% across our datasets. We also observe that the LLMs tend to identify certain types of vulnerabilities related to sensitive information exposure and improper input neutralization much more frequently than other types, such as those involving external control of file names or paths. Furthermore, when LLMs do issue security warnings, they often provide more information on the causes, exploits, and fixes of vulnerabilities compared to Stack Overflow responses. Finally, we provide an in-depth discussion on the implications of our findings and present a CLI-based prompting tool that can be used to generate significantly more secure LLM responses.

### Where's the Bug? Attention Probing for Scalable Fault Localization 
[[arxiv](https://arxiv.org/abs/2502.13966)] [[cool](https://papers.cool/arxiv/2502.13966)] [[pdf](https://arxiv.org/pdf/2502.13966)]
> **Authors**: Adam Stein,Arthur Wayne,Aaditya Naik,Mayur Naik,Eric Wong
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 14 pages, 5 figures
- **标题**: None
- **领域**: 软件工程,机器学习
- **Abstract**: Ensuring code correctness remains a challenging problem even as large language models (LLMs) become increasingly capable at code-related tasks. While LLM-based program repair systems can propose bug fixes using only a user's bug report, their effectiveness is fundamentally limited by their ability to perform fault localization (FL), a challenging problem for both humans and LLMs. Existing FL approaches rely on executable test cases, require training on costly and often noisy line-level annotations, or demand resource-intensive LLMs. In this paper, we present Bug Attention Probe (BAP), a method which learns state-of-the-art fault localization without any direct localization labels, outperforming traditional FL baselines and prompting of large-scale LLMs. We evaluate our approach across a variety of code settings, including real-world Java bugs from the standard Defects4J dataset as well as seven other datasets which span a diverse set of bug types and languages. Averaged across all eight datasets, BAP improves by 34.6% top-1 accuracy compared to the strongest baseline and 93.4% over zero-shot prompting GPT-4o. BAP is also significantly more efficient than prompting, outperforming large open-weight models at a small fraction of the computational cost.

### AI Software Engineer: Programming with Trust 
[[arxiv](https://arxiv.org/abs/2502.13767)] [[cool](https://papers.cool/arxiv/2502.13767)] [[pdf](https://arxiv.org/pdf/2502.13767)]
> **Authors**: Abhik Roychoudhury,Corina Pasareanu,Michael Pradel,Baishakhi Ray
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 5 pages
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: Large Language Models (LLMs) have shown surprising proficiency in generating code snippets, promising to automate large parts of software engineering via artificial intelligence (AI). We argue that successfully deploying AI software engineers requires a level of trust equal to or even greater than the trust established by human-driven software engineering practices. The recent trend toward LLM agents offers a path toward integrating the power of LLMs to create new code with the power of analysis tools to increase trust in the code. This opinion piece comments on whether LLM agents could dominate software engineering workflows in the future and whether the focus of programming will shift from programming at scale to programming with trust.

### An LLM-based Agent for Reliable Docker Environment Configuration 
[[arxiv](https://arxiv.org/abs/2502.13681)] [[cool](https://papers.cool/arxiv/2502.13681)] [[pdf](https://arxiv.org/pdf/2502.13681)]
> **Authors**: Ruida Hu,Chao Peng,Xinchen Wang,Cuiyun Gao
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,人工智能,计算语言学,机器学习
- **Abstract**: Environment configuration is a critical yet time-consuming step in software development, especially when dealing with unfamiliar code repositories. While Large Language Models (LLMs) demonstrate the potential to accomplish software engineering tasks, existing methods for environment configuration often rely on manual efforts or fragile scripts, leading to inefficiencies and unreliable outcomes. We introduce Repo2Run, the first LLM-based agent designed to fully automate environment configuration and generate executable Dockerfiles for arbitrary Python repositories. We address two major challenges: (1) enabling the LLM agent to configure environments within isolated Docker containers, and (2) ensuring the successful configuration process is recorded and accurately transferred to a Dockerfile without error. To achieve this, we propose atomic configuration synthesis, featuring a dual-environment architecture (internal and external environment) with a rollback mechanism to prevent environment "pollution" from failed commands, guaranteeing atomic execution (execute fully or not at all) and a Dockerfile generator to transfer successful configuration steps into runnable Dockerfiles. We evaluate Repo2Run~on our proposed benchmark of 420 recent Python repositories with unit tests, where it achieves an 86.0% success rate, outperforming the best baseline by 63.9%.

## 社交和信息网络(cs.SI:Social and Information Networks)

### Diffusion Model Agnostic Social Influence Maximization in Hyperbolic Space 
[[arxiv](https://arxiv.org/abs/2502.13571)] [[cool](https://papers.cool/arxiv/2502.13571)] [[pdf](https://arxiv.org/pdf/2502.13571)]
> **Authors**: Hongliang Qiao
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 10 pages, 4 figures
- **标题**: None
- **领域**: 社交和信息网络,机器学习
- **Abstract**: The Influence Maximization (IM) problem aims to find a small set of influential users to maximize their influence spread in a social network. Traditional methods rely on fixed diffusion models with known parameters, limiting their generalization to real-world scenarios. In contrast, graph representation learning-based methods have gained wide attention for overcoming this limitation by learning user representations to capture influence characteristics. However, existing studies are built on Euclidean space, which fails to effectively capture the latent hierarchical features of social influence distribution. As a result, users' influence spread cannot be effectively measured through the learned representations. To alleviate these limitations, we propose HIM, a novel diffusion model agnostic method that leverages hyperbolic representation learning to estimate users' potential influence spread from social propagation data. HIM consists of two key components. First, a hyperbolic influence representation module encodes influence spread patterns from network structure and historical influence activations into expressive hyperbolic user representations. Hence, the influence magnitude of users can be reflected through the geometric properties of hyperbolic space, where highly influential users tend to cluster near the space origin. Second, a novel adaptive seed selection module is developed to flexibly and effectively select seed users using the positional information of learned user representations. Extensive experiments on five network datasets demonstrate the superior effectiveness and efficiency of our method for the IM problem with unknown diffusion model parameters, highlighting its potential for large-scale real-world social networks.

## 音频和语音处理(eess.AS:Audio and Speech Processing)

### Gesture-Aware Zero-Shot Speech Recognition for Patients with Language Disorders 
[[arxiv](https://arxiv.org/abs/2502.13983)] [[cool](https://papers.cool/arxiv/2502.13983)] [[pdf](https://arxiv.org/pdf/2502.13983)]
> **Authors**: Seungbae Kim,Daeun Lee,Brielle Stark,Jinyoung Han
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 音频和语音处理,人工智能
- **Abstract**: Individuals with language disorders often face significant communication challenges due to their limited language processing and comprehension abilities, which also affect their interactions with voice-assisted systems that mostly rely on Automatic Speech Recognition (ASR). Despite advancements in ASR that address disfluencies, there has been little attention on integrating non-verbal communication methods, such as gestures, which individuals with language disorders substantially rely on to supplement their communication. Recognizing the need to interpret the latent meanings of visual information not captured by speech alone, we propose a gesture-aware ASR system utilizing a multimodal large language model with zero-shot learning for individuals with speech impairments. Our experiment results and analyses show that including gesture information significantly enhances semantic understanding. This study can help develop effective communication technologies, specifically designed to meet the unique needs of individuals with language impairments.

### Benchmarking Automatic Speech Recognition coupled LLM Modules for Medical Diagnostics 
[[arxiv](https://arxiv.org/abs/2502.13982)] [[cool](https://papers.cool/arxiv/2502.13982)] [[pdf](https://arxiv.org/pdf/2502.13982)]
> **Authors**: Kabir Kumar
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 音频和语音处理,机器学习
- **Abstract**: Natural Language Processing (NLP) and Voice Recognition agents are rapidly evolving healthcare by enabling efficient, accessible, and professional patient support while automating grunt work. This report serves as my self project wherein models finetuned on medical call recordings are analysed through a two-stage system: Automatic Speech Recognition (ASR) for speech transcription and a Large Language Model (LLM) for context-aware, professional responses. ASR, finetuned on phone call recordings provides generalised transcription of diverse patient speech over call, while the LLM matches transcribed text to medical diagnosis. A novel audio preprocessing strategy, is deployed to provide invariance to incoming recording/call data, laden with sufficient augmentation with noise/clipping to make the pipeline robust to the type of microphone and ambient conditions the patient might have while calling/recording.

### Adopting Whisper for Confidence Estimation 
[[arxiv](https://arxiv.org/abs/2502.13446)] [[cool](https://papers.cool/arxiv/2502.13446)] [[pdf](https://arxiv.org/pdf/2502.13446)]
> **Authors**: Vaibhav Aggarwal,Shabari S Nair,Yash Verma,Yash Jogi
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: Accepted at IEEE ICASSP 2025
- **标题**: None
- **领域**: 音频和语音处理,机器学习
- **Abstract**: Recent research on word-level confidence estimation for speech recognition systems has primarily focused on lightweight models known as Confidence Estimation Modules (CEMs), which rely on hand-engineered features derived from Automatic Speech Recognition (ASR) outputs. In contrast, we propose a novel end-to-end approach that leverages the ASR model itself (Whisper) to generate word-level confidence scores. Specifically, we introduce a method in which the Whisper model is fine-tuned to produce scalar confidence scores given an audio input and its corresponding hypothesis transcript. Our experiments demonstrate that the fine-tuned Whisper-tiny model, comparable in size to a strong CEM baseline, achieves similar performance on the in-domain dataset and surpasses the CEM baseline on eight out-of-domain datasets, whereas the fine-tuned Whisper-large model consistently outperforms the CEM baseline by a substantial margin across all datasets.

## 图像和视频处理(eess.IV:Image and Video Processing)

### MambaLiteSR: Image Super-Resolution with Low-Rank Mamba using Knowledge Distillation 
[[arxiv](https://arxiv.org/abs/2502.14090)] [[cool](https://papers.cool/arxiv/2502.14090)] [[pdf](https://arxiv.org/pdf/2502.14090)]
> **Authors**: Romina Aalishah,Mozhgan Navardi,Tinoosh Mohsenin
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: Special Session: GenerativeAIon Edge, 26th International Symposium on Quality Electronic Design (ISQED'25)
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: Generative Artificial Intelligence (AI) has gained significant attention in recent years, revolutionizing various applications across industries. Among these, advanced vision models for image super-resolution are in high demand, particularly for deployment on edge devices where real-time processing is crucial. However, deploying such models on edge devices is challenging due to limited computing power and memory. In this paper, we present MambaLiteSR, a novel lightweight image Super-Resolution (SR) model that utilizes the architecture of Vision Mamba. It integrates State Space Blocks and a reconstruction module for efficient feature extraction. To optimize efficiency without affecting performance, MambaLiteSR employs knowledge distillation to transfer key insights from a larger Mamba-based teacher model to a smaller student model via hyperparameter tuning. Through mathematical analysis of model parameters and their impact on PSNR, we identify key factors and adjust them accordingly. Our comprehensive evaluation shows that MambaLiteSR outperforms state-of-the-art edge SR methods by reducing power consumption while maintaining competitive PSNR and SSIM scores across benchmark datasets. It also reduces power usage during training via low-rank approximation. Moreover, MambaLiteSR reduces parameters with minimal performance loss, enabling efficient deployment of generative AI models on resource-constrained devices. Deployment on the embedded NVIDIA Jetson Orin Nano confirms the superior balance of MambaLiteSR size, latency, and efficiency. Experiments show that MambaLiteSR achieves performance comparable to both the baseline and other edge models while using 15% fewer parameters. It also improves power consumption by up to 58% compared to state-of-the-art SR edge models, all while maintaining low energy use during training.

### Benchmarking Self-Supervised Methods for Accelerated MRI Reconstruction 
[[arxiv](https://arxiv.org/abs/2502.14009)] [[cool](https://papers.cool/arxiv/2502.14009)] [[pdf](https://arxiv.org/pdf/2502.14009)]
> **Authors**: Andrew Wang,Mike Davies
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: Preprint: Work in Progress
- **标题**: None
- **领域**: 图像和视频处理,机器学习
- **Abstract**: Reconstructing MRI from highly undersampled measurements is crucial for accelerating medical imaging, but is challenging due to the ill-posedness of the inverse problem. While supervised deep learning approaches have shown remarkable success, they rely on fully-sampled ground truth data, which is often impractical or impossible to obtain. Recently, numerous self-supervised methods have emerged that do not require ground truth, however, the lack of systematic comparison and standard experimental setups have hindered research. We present the first comprehensive review of loss functions from all feedforward self-supervised methods and the first benchmark on accelerated MRI reconstruction without ground truth, showing that there is a wide range in performance across methods. In addition, we propose Multi-Operator Equivariant Imaging (MO-EI), a novel framework that builds on the imaging model considered in existing methods to outperform all state-of-the-art and approaches supervised performance. Finally, to facilitate reproducible benchmarking, we provide implementations of all methods in the DeepInverse library (https://deepinv.github.io) and easy-to-use demo code at https://andrewwango.github.io/deepinv-selfsup-fastmri.

### A Baseline Method for Removing Invisible Image Watermarks using Deep Image Prior 
[[arxiv](https://arxiv.org/abs/2502.13998)] [[cool](https://papers.cool/arxiv/2502.13998)] [[pdf](https://arxiv.org/pdf/2502.13998)]
> **Authors**: Hengyue Liang,Taihui Li,Ju Sun
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,人工智能,密码学和安全,计算机视觉和模式识别
- **Abstract**: Image watermarks have been considered a promising technique to help detect AI-generated content, which can be used to protect copyright or prevent fake image abuse. In this work, we present a black-box method for removing invisible image watermarks, without the need of any dataset of watermarked images or any knowledge about the watermark system. Our approach is simple to implement: given a single watermarked image, we regress it by deep image prior (DIP). We show that from the intermediate steps of DIP one can reliably find an evasion image that can remove invisible watermarks while preserving high image quality. Due to its unique working mechanism and practical effectiveness, we advocate including DIP as a baseline invasion method for benchmarking the robustness of watermarking systems. Finally, by showing the limited ability of DIP and other existing black-box methods in evading training-based visible watermarks, we discuss the positive implications on the practical use of training-based visible watermarks to prevent misinformation abuse.

### Remote Sensing Semantic Segmentation Quality Assessment based on Vision Language Model 
[[arxiv](https://arxiv.org/abs/2502.13990)] [[cool](https://papers.cool/arxiv/2502.13990)] [[pdf](https://arxiv.org/pdf/2502.13990)]
> **Authors**: Huiying Shi,Zhihong Tan,Zhihan Zhang,Hongchen Wei,Yaosi Hu,Yingxue Zhang,Zhenzhong Chen
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-20
> **comment**: 16 pages,6 figures
- **标题**: None
- **领域**: 图像和视频处理,机器学习
- **Abstract**: The complexity of scenes and variations in image quality result in significant variability in the performance of semantic segmentation methods of remote sensing imagery (RSI) in supervised real-world scenarios. This makes the evaluation of semantic segmentation quality in such scenarios an issue to be resolved. However, most of the existing evaluation metrics are developed based on expert-labeled object-level annotations, which are not applicable in such scenarios. To address this issue, we propose RS-SQA, an unsupervised quality assessment model for RSI semantic segmentation based on vision language model (VLM). This framework leverages a pre-trained RS VLM for semantic understanding and utilizes intermediate features from segmentation methods to extract implicit information about segmentation quality. Specifically, we introduce CLIP-RS, a large-scale pre-trained VLM trained with purified text to reduce textual noise and capture robust semantic information in the RS domain. Feature visualizations confirm that CLIP-RS can effectively differentiate between various levels of segmentation quality. Semantic features and low-level segmentation features are effectively integrated through a semantic-guided approach to enhance evaluation accuracy. To further support the development of RS semantic segmentation quality assessment, we present RS-SQED, a dedicated dataset sampled from four major RS semantic segmentation datasets and annotated with segmentation accuracy derived from the inference results of 8 representative segmentation methods. Experimental results on the established dataset demonstrate that RS-SQA significantly outperforms state-of-the-art quality assessment models. This provides essential support for predicting segmentation accuracy and high-quality semantic segmentation interpretation, offering substantial practical value.

### Regularização, aprendizagem profunda e interdisciplinaridade em problemas inversos mal-postos 
[[arxiv](https://arxiv.org/abs/2502.13976)] [[cool](https://papers.cool/arxiv/2502.13976)] [[pdf](https://arxiv.org/pdf/2502.13976)]
> **Authors**: Roberto Gutierrez Beraldo,Ricardo Suyama
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-20
> **comment**: 200 pages, in Portugueselanguage, 54 figures
- **标题**: None
- **领域**: 图像和视频处理,机器学习
- **Abstract**: In this book, written in Portuguese, we discuss what ill-posed problems are and how the regularization method is used to solve them. In the form of questions and answers, we reflect on the origins and future of regularization, relating the similarities and differences of its meaning in different areas, including inverse problems, statistics, machine learning, and deep learning.

### RestoreGrad: Signal Restoration Using Conditional Denoising Diffusion Models with Jointly Learned Prior 
[[arxiv](https://arxiv.org/abs/2502.13574)] [[cool](https://papers.cool/arxiv/2502.13574)] [[pdf](https://arxiv.org/pdf/2502.13574)]
> **Authors**: Ching-Hua Lee,Chouchang Yang,Jaejin Cho,Yashas Malur Saidutta,Rakshith Sharma Srinivasa,Yilin Shen,Hongxia Jin
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,机器学习,音频和语音处理
- **Abstract**: Denoising diffusion probabilistic models (DDPMs) can be utilized for recovering a clean signal from its degraded observation(s) by conditioning the model on the degraded signal. The degraded signals are themselves contaminated versions of the clean signals; due to this correlation, they may encompass certain useful information about the target clean data distribution. However, existing adoption of the standard Gaussian as the prior distribution in turn discards such information, resulting in sub-optimal performance. In this paper, we propose to improve conditional DDPMs for signal restoration by leveraging a more informative prior that is jointly learned with the diffusion model. The proposed framework, called RestoreGrad, seamlessly integrates DDPMs into the variational autoencoder framework and exploits the correlation between the degraded and clean signals to encode a better diffusion prior. On speech and image restoration tasks, we show that RestoreGrad demonstrates faster convergence (5-10 times fewer training steps) to achieve better quality of restored signals over existing DDPM baselines, and improved robustness to using fewer sampling steps in inference time (2-2.5 times fewer), advocating the advantages of leveraging jointly learned prior for efficiency improvements in the diffusion process.

## 信号处理(eess.SP:Signal Processing)

### IncepFormerNet: A multi-scale multi-head attention network for SSVEP classification 
[[arxiv](https://arxiv.org/abs/2502.13972)] [[cool](https://papers.cool/arxiv/2502.13972)] [[pdf](https://arxiv.org/pdf/2502.13972)]
> **Authors**: Yan Huang,Yongru Chen,Lei Cao,Yongnian Cao,Xuechun Yang,Yilin Dong,Tianyu Liu
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 信号处理,人工智能,机器学习
- **Abstract**: In recent years, deep learning (DL) models have shown outstanding performance in EEG classification tasks, particularly in Steady-State Visually Evoked Potential(SSVEP)-based Brain-Computer-Interfaces(BCI)systems. DL methods have been successfully applied to SSVEP-BCI. This study proposes a new model called IncepFormerNet, which is a hybrid of the Inception and Transformer architectures. IncepFormerNet adeptly extracts multi-scale temporal information from time series data using parallel convolution kernels of varying sizes, accurately capturing the subtle variations and critical features within SSVEP signals.Furthermore, the model integrates the multi-head attention mechanism from the Transformer architecture, which not only provides insights into global dependencies but also significantly enhances the understanding and representation of complex patterns.Additionally, it takes advantage of filter bank techniques to extract features based on the spectral characteristics of SSVEP data. To validate the effectiveness of the proposed model, we conducted experiments on two public datasets, . The experimental results show that IncepFormerNet achieves an accuracy of 87.41 on Dataset 1 and 71.97 on Dataset 2 using a 1.0-second time window. To further verify the superiority of the proposed model, we compared it with other deep learning models, and the results indicate that our method achieves significantly higher accuracy than the others.The source codes in this work are available at: https://github.com/CECNL/SSVEP-DAN.

### Bridging Simulation and Reality: A 3D Clustering-Based Deep Learning Model for UAV-Based RF Source Localization 
[[arxiv](https://arxiv.org/abs/2502.13969)] [[cool](https://papers.cool/arxiv/2502.13969)] [[pdf](https://arxiv.org/pdf/2502.13969)]
> **Authors**: Saad Masrur,Ismail Guvenc
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-20
> **comment**: This paper has been submitted to IEEE ICC 2025
- **标题**: None
- **领域**: 信号处理,人工智能
- **Abstract**: Localization of radio frequency (RF) sources has critical applications, including search and rescue, jammer detection, and monitoring of hostile activities. Unmanned aerial vehicles (UAVs) offer significant advantages for RF source localization (RFSL) over terrestrial methods, leveraging autonomous 3D navigation and improved signal capture at higher altitudes. Recent advancements in deep learning (DL) have further enhanced localization accuracy, particularly for outdoor scenarios. DL models often face challenges in real-world performance, as they are typically trained on simulated datasets that fail to replicate real-world conditions fully. To address this, we first propose the Enhanced Two-Ray propagation model, reducing the simulation-to-reality gap by improving the accuracy of propagation environment modeling. For RFSL, we propose the 3D Cluster-Based RealAdaptRNet, a DL-based method leveraging 3D clustering-based feature extraction for robust localization. Experimental results demonstrate that the proposed Enhanced Two-Ray model provides superior accuracy in simulating real-world propagation scenarios compared to conventional free-space and two-ray models. Notably, the 3D Cluster-Based RealAdaptRNet, trained entirely on simulated datasets, achieves exceptional performance when validated in real-world environments using the AERPAW physical testbed, with an average localization error of 18.2 m. The proposed approach is computationally efficient, utilizing 33.5 times fewer parameters, and demonstrates strong generalization capabilities across diverse trajectories, making it highly suitable for real-world applications.

## 系统与控制(eess.SY:Systems and Control)

### Highly Dynamic and Flexible Spatio-Temporal Spectrum Management with AI-Driven O-RAN: A Multi-Granularity Marketplace Framework 
[[arxiv](https://arxiv.org/abs/2502.13891)] [[cool](https://papers.cool/arxiv/2502.13891)] [[pdf](https://arxiv.org/pdf/2502.13891)]
> **Authors**: Mehdi Rasti,Elaheh Ataeebojd,Shiva Kazemi Taskooh,Mehdi Monemi,Siavash Razmi,Matti Latva-aho
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 系统与控制,机器学习
- **Abstract**: Current spectrum-sharing frameworks struggle with adaptability, often being either static or insufficiently dynamic. They primarily emphasize temporal sharing while overlooking spatial and spectral dimensions. We propose an adaptive, AI-driven spectrum-sharing framework within the O-RAN architecture, integrating discriminative and generative AI (GenAI) to forecast spectrum needs across multiple timescales and spatial granularities. A marketplace model, managed by an authorized spectrum broker, enables operators to trade spectrum dynamically, balancing static assignments with real-time trading. GenAI enhances traffic prediction, spectrum estimation, and allocation, optimizing utilization while reducing costs. This modular, flexible approach fosters operator collaboration, maximizing efficiency and revenue. A key research challenge is refining allocation granularity and spatio-temporal dynamics beyond existing models.

### Kernel Mean Embedding Topology: Weak and Strong Forms for Stochastic Kernels and Implications for Model Learning 
[[arxiv](https://arxiv.org/abs/2502.13486)] [[cool](https://papers.cool/arxiv/2502.13486)] [[pdf](https://arxiv.org/pdf/2502.13486)]
> **Authors**: Naci Saldi,Serdar Yuksel
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 35 pages
- **标题**: None
- **领域**: 系统与控制,机器学习,优化与控制,统计理论
- **Abstract**: We introduce a novel topology, called Kernel Mean Embedding Topology, for stochastic kernels, in a weak and strong form. This topology, defined on the spaces of Bochner integrable functions from a signal space to a space of probability measures endowed with a Hilbert space structure, allows for a versatile formulation. This construction allows one to obtain both a strong and weak formulation. (i) For its weak formulation, we highlight the utility on relaxed policy spaces, and investigate connections with the Young narrow topology and Borkar (or $w^*$)-topology, and establish equivalence properties. We report that, while both the $w^*$-topology and kernel mean embedding topology are relatively compact, they are not closed. Conversely, while the Young narrow topology is closed, it lacks relative compactness. (ii) We show that the strong form provides an appropriate formulation for placing topologies on spaces of models characterized by stochastic kernels with explicit robustness and learning theoretic implications on optimal stochastic control under discounted or average cost criteria. (iii) We show that this topology possesses several properties making it ideal to study optimality, approximations, robustness and continuity properties. In particular, the kernel mean embedding topology has a Hilbert space structure, which is particularly useful for approximating stochastic kernels through simulation data.

## 范畴论(math.CT:Category Theory)

### Learning Is a Kan Extension 
[[arxiv](https://arxiv.org/abs/2502.13810)] [[cool](https://papers.cool/arxiv/2502.13810)] [[pdf](https://arxiv.org/pdf/2502.13810)]
> **Authors**: Matthew Pugh,Jo Grundy,Corina Cirstea,Nick Harris
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 范畴论,机器学习
- **Abstract**: Previous work has demonstrated that efficient algorithms exist for computing Kan extensions and that some Kan extensions have interesting similarities to various machine learning algorithms. This paper closes the gap by proving that all error minimisation algorithms may be presented as a Kan extension. This result provides a foundation for future work to investigate the optimisation of machine learning algorithms through their presentation as Kan extensions. A corollary of this representation of error-minimising algorithms is a presentation of error from the perspective of lossy and lossless transformations of data.

## 优化与控制(math.OC:Optimization and Control)

### Sample Complexity of Linear Quadratic Regulator Without Initial Stability 
[[arxiv](https://arxiv.org/abs/2502.14210)] [[cool](https://papers.cool/arxiv/2502.14210)] [[pdf](https://arxiv.org/pdf/2502.14210)]
> **Authors**: Amirreza Neshaei Moghaddam,Alex Olshevsky,Bahman Gharesifard
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 优化与控制,机器学习,系统与控制
- **Abstract**: Inspired by REINFORCE, we introduce a novel receding-horizon algorithm for the Linear Quadratic Regulator (LQR) problem with unknown parameters. Unlike prior methods, our algorithm avoids reliance on two-point gradient estimates while maintaining the same order of sample complexity. Furthermore, it eliminates the restrictive requirement of starting with a stable initial policy, broadening its applicability. Beyond these improvements, we introduce a refined analysis of error propagation through the contraction of the Riemannian distance over the Riccati operator. This refinement leads to a better sample complexity and ensures improved convergence guarantees. Numerical simulations validate the theoretical results, demonstrating the method's practical feasibility and performance in realistic scenarios.

### Weighted Low-rank Approximation via Stochastic Gradient Descent on Manifolds 
[[arxiv](https://arxiv.org/abs/2502.14174)] [[cool](https://papers.cool/arxiv/2502.14174)] [[pdf](https://arxiv.org/pdf/2502.14174)]
> **Authors**: Conglong Xu,Peiqi Yang,Hao Wu
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 优化与控制,人工智能,机器学习,机器学习
- **Abstract**: We solve a regularized weighted low-rank approximation problem by a stochastic gradient descent on a manifold. To guarantee the convergence of our stochastic gradient descent, we establish a convergence theorem on manifolds for retraction-based stochastic gradient descents admitting confinements. On sample data from the Netflix Prize training dataset, our algorithm outperforms the existing stochastic gradient descent on Euclidean spaces. We also compare the accelerated line search on this manifold to the existing accelerated line search on Euclidean spaces.

### Population Dynamics Control with Partial Observations 
[[arxiv](https://arxiv.org/abs/2502.14079)] [[cool](https://papers.cool/arxiv/2502.14079)] [[pdf](https://arxiv.org/pdf/2502.14079)]
> **Authors**: Zhou Lu,Y. Jennifer Sun,Zhiyu Zhang
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 优化与控制,机器学习
- **Abstract**: We study the problem of controlling population dynamics, a class of linear dynamical systems evolving on the probability simplex, from the perspective of online non-stochastic control. While Golowich et.al. 2024 analyzed the fully observable setting, we focus on the more realistic, partially observable case, where only a low-dimensional representation of the state is accessible. In classical non-stochastic control, inputs are set as linear combinations of past disturbances. However, under partial observations, disturbances cannot be directly computed. To address this, Simchowitz et.al. 2020 proposed to construct oblivious signals, which are counterfactual observations with zero control, as a substitute. This raises several challenges in our setting: (1) how to construct oblivious signals under simplex constraints, where zero control is infeasible; (2) how to design a sufficiently expressive convex controller parameterization tailored to these signals; and (3) how to enforce the simplex constraint on control when projections may break the convexity of cost functions. Our main contribution is a new controller that achieves the optimal $\tilde{O}(\sqrt{T})$ regret with respect to a natural class of mixing linear dynamic controllers. To tackle these challenges, we construct signals based on hypothetical observations under a constant control adapted to the simplex domain, and introduce a new controller parameterization that approximates general control policies linear in non-oblivious observations. Furthermore, we employ a novel convex extension surrogate loss, inspired by Lattimore 2024, to bypass the projection-induced convexity issue.

## 大气和海洋物理(physics.ao-ph:Atmospheric and Oceanic Physics)

### A Study on Monthly Marine Heatwave Forecasts in New Zealand: An Investigation of Imbalanced Regression Loss Functions with Neural Network Models 
[[arxiv](https://arxiv.org/abs/2502.13495)] [[cool](https://papers.cool/arxiv/2502.13495)] [[pdf](https://arxiv.org/pdf/2502.13495)]
> **Authors**: Ding Ning,Varvara Vetrova,Sébastien Delaux,Rachael Tappenden,Karin R. Bryan,Yun Sing Koh
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: The paper contains 32 pages for the main text
- **标题**: None
- **领域**: 大气和海洋物理,机器学习,应用领域
- **Abstract**: Marine heatwaves (MHWs) are extreme ocean-temperature events with significant impacts on marine ecosystems and related industries. Accurate forecasts (one to six months ahead) of MHWs would aid in mitigating these impacts. However, forecasting MHWs presents a challenging imbalanced regression task due to the rarity of extreme temperature anomalies in comparison to more frequent moderate conditions. In this study, we examine monthly MHW forecasts for 12 locations around New Zealand. We use a fully-connected neural network and compare standard and specialized regression loss functions, including the mean squared error (MSE), the mean absolute error (MAE), the Huber, the weighted MSE, the focal-R, the balanced MSE, and a proposed scaling-weighted MSE. Results show that (i) short lead times (one month) are considerably more predictable than three- and six-month leads, (ii) models trained with the standard MSE or MAE losses excel at forecasting average conditions but struggle to capture extremes, and (iii) specialized loss functions such as the balanced MSE and our scaling-weighted MSE substantially improve forecasting of MHW and suspected MHW events. These findings underscore the importance of tailored loss functions for imbalanced regression, particularly in forecasting rare but impactful events such as MHWs.

## 基因组学(q-bio.GN:Genomics)

### Helix-mRNA: A Hybrid Foundation Model For Full Sequence mRNA Therapeutics 
[[arxiv](https://arxiv.org/abs/2502.13785)] [[cool](https://papers.cool/arxiv/2502.13785)] [[pdf](https://arxiv.org/pdf/2502.13785)]
> **Authors**: Matthew Wood,Mathieu Klop,Maxime Allard
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 8 pages, 3 figures, 3 tables
- **标题**: None
- **领域**: 基因组学,人工智能
- **Abstract**: mRNA-based vaccines have become a major focus in the pharmaceutical industry. The coding sequence as well as the Untranslated Regions (UTRs) of an mRNA can strongly influence translation efficiency, stability, degradation, and other factors that collectively determine a vaccine's effectiveness. However, optimizing mRNA sequences for those properties remains a complex challenge. Existing deep learning models often focus solely on coding region optimization, overlooking the UTRs. We present Helix-mRNA, a structured state-space-based and attention hybrid model to address these challenges. In addition to a first pre-training, a second pre-training stage allows us to specialise the model with high-quality data. We employ single nucleotide tokenization of mRNA sequences with codon separation, ensuring prior biological and structural information from the original mRNA sequence is not lost. Our model, Helix-mRNA, outperforms existing methods in analysing both UTRs and coding region properties. It can process sequences 6x longer than current approaches while using only 10% of the parameters of existing foundation models. Its predictive capabilities extend to all mRNA regions. We open-source the model (https://github.com/helicalAI/helical) and model weights (https://huggingface.co/helical-ai/helix-mRNA).

## 神经元和认知(q-bio.NC:Neurons and Cognition)

### LaVCa: LLM-assisted Visual Cortex Captioning 
[[arxiv](https://arxiv.org/abs/2502.13606)] [[cool](https://papers.cool/arxiv/2502.13606)] [[pdf](https://arxiv.org/pdf/2502.13606)]
> **Authors**: Takuya Matsuyama,Shinji Nishimoto,Yu Takagi
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 33 pages
- **标题**: None
- **领域**: 神经元和认知,人工智能,计算语言学,计算机视觉和模式识别,机器学习
- **Abstract**: Understanding the property of neural populations (or voxels) in the human brain can advance our comprehension of human perceptual and cognitive processing capabilities and contribute to developing brain-inspired computer models. Recent encoding models using deep neural networks (DNNs) have successfully predicted voxel-wise activity. However, interpreting the properties that explain voxel responses remains challenging because of the black-box nature of DNNs. As a solution, we propose LLM-assisted Visual Cortex Captioning (LaVCa), a data-driven approach that uses large language models (LLMs) to generate natural-language captions for images to which voxels are selective. By applying LaVCa for image-evoked brain activity, we demonstrate that LaVCa generates captions that describe voxel selectivity more accurately than the previously proposed method. Furthermore, the captions generated by LaVCa quantitatively capture more detailed properties than the existing method at both the inter-voxel and intra-voxel levels. Furthermore, a more detailed analysis of the voxel-specific properties generated by LaVCa reveals fine-grained functional differentiation within regions of interest (ROIs) in the visual cortex and voxels that simultaneously represent multiple distinct concepts. These findings offer profound insights into human visual representations by assigning detailed captions throughout the visual cortex while highlighting the potential of LLM-based methods in understanding brain representations. Please check out our webpage at https://sites.google.com/view/lavca-llm/

## 风险管理(q-fin.RM:Risk Management)

### Utilizing Effective Dynamic Graph Learning to Shield Financial Stability from Risk Propagation 
[[arxiv](https://arxiv.org/abs/2502.13979)] [[cool](https://papers.cool/arxiv/2502.13979)] [[pdf](https://arxiv.org/pdf/2502.13979)]
> **Authors**: Guanyuan Yu,Qing Li,Yu Zhao,Jun Wang,YiJun Chen,Shaolei Chen
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 风险管理,人工智能,机器学习
- **Abstract**: Financial risks can propagate across both tightly coupled temporal and spatial dimensions, posing significant threats to financial stability. Moreover, risks embedded in unlabeled data are often difficult to detect. To address these challenges, we introduce GraphShield, a novel approach with three key innovations: Enhanced Cross-Domain Infor mation Learning: We propose a dynamic graph learning module to improve information learning across temporal and spatial domains. Advanced Risk Recognition: By leveraging the clustering characteristics of risks, we construct a risk recognizing module to enhance the identification of hidden threats. Risk Propagation Visualization: We provide a visualization tool for quantifying and validating nodes that trigger widespread cascading risks. Extensive experiments on two real-world and two open-source datasets demonstrate the robust performance of our framework. Our approach represents a significant advancement in leveraging artificial intelligence to enhance financial stability, offering a powerful solution to mitigate the spread of risks within financial networks.

## 统计金融(q-fin.ST:Statistical Finance)

### Deep Learning for VWAP Execution in Crypto Markets: Beyond the Volume Curve 
[[arxiv](https://arxiv.org/abs/2502.13722)] [[cool](https://papers.cool/arxiv/2502.13722)] [[pdf](https://arxiv.org/pdf/2502.13722)]
> **Authors**: Remi Genet
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 统计金融,机器学习
- **Abstract**: Volume-Weighted Average Price (VWAP) is arguably the most prevalent benchmark for trade execution as it provides an unbiased standard for comparing performance across market participants. However, achieving VWAP is inherently challenging due to its dependence on two dynamic factors, volumes and prices. Traditional approaches typically focus on forecasting the market's volume curve, an assumption that may hold true under steady conditions but becomes suboptimal in more volatile environments or markets such as cryptocurrency where prediction error margins are higher. In this study, I propose a deep learning framework that directly optimizes the VWAP execution objective by bypassing the intermediate step of volume curve prediction. Leveraging automatic differentiation and custom loss functions, my method calibrates order allocation to minimize VWAP slippage, thereby fully addressing the complexities of the execution problem. My results demonstrate that this direct optimization approach consistently achieves lower VWAP slippage compared to conventional methods, even when utilizing a naive linear model presented in arXiv:2410.21448. They validate the observation that strategies optimized for VWAP performance tend to diverge from accurate volume curve predictions and thus underscore the advantage of directly modeling the execution objective. This research contributes a more efficient and robust framework for VWAP execution in volatile markets, illustrating the potential of deep learning in complex financial systems where direct objective optimization is crucial. Although my empirical analysis focuses on cryptocurrency markets, the underlying principles of the framework are readily applicable to other asset classes such as equities.

## 量子物理学(quant-ph:Quantum Physics)

### Towards efficient quantum algorithms for diffusion probability models 
[[arxiv](https://arxiv.org/abs/2502.14252)] [[cool](https://papers.cool/arxiv/2502.14252)] [[pdf](https://arxiv.org/pdf/2502.14252)]
> **Authors**: Yunfei Wang,Ruoxi Jiang,Yingda Fan,Xiaowei Jia,Jens Eisert,Junyu Liu,Jin-Peng Liu
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 6+20 pages, 2 figures
- **标题**: None
- **领域**: 量子物理学,机器学习
- **Abstract**: A diffusion probabilistic model (DPM) is a generative model renowned for its ability to produce high-quality outputs in tasks such as image and audio generation. However, training DPMs on large, high-dimensional datasets such as high-resolution images or audio incurs significant computational, energy, and hardware costs. In this work, we introduce efficient quantum algorithms for implementing DPMs through various quantum ODE solvers. These algorithms highlight the potential of quantum Carleman linearization for diverse mathematical structures, leveraging state-of-the-art quantum linear system solvers (QLSS) or linear combination of Hamiltonian simulations (LCHS). Specifically, we focus on two approaches: DPM-solver-$k$ which employs exact $k$-th order derivatives to compute a polynomial approximation of $ε_θ(x_λ,λ)$; and UniPC which uses finite difference of $ε_θ(x_λ,λ)$ at different points $(x_{s_m}, λ_{s_m})$ to approximate higher-order derivatives. As such, this work represents one of the most direct and pragmatic applications of quantum algorithms to large-scale machine learning models, presumably talking substantial steps towards demonstrating the practical utility of quantum computing.

### Solving the Encoding Bottleneck: Of the HHL Algorithm, By the HHL Algorithm 
[[arxiv](https://arxiv.org/abs/2502.13534)] [[cool](https://papers.cool/arxiv/2502.13534)] [[pdf](https://arxiv.org/pdf/2502.13534)]
> **Authors**: Guang Ping He
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 5 pages
- **标题**: None
- **领域**: 量子物理学,人工智能,机器学习
- **Abstract**: The Harrow-Hassidim-Lloyd (HHL) algorithm offers exponential speedup for solving the quantum linear-system problem. But some caveats for the speedup could be hard to met. One of the difficulties is the encoding bottleneck, i.e., the efficient preparation of the initial quantum state. To prepare an arbitrary $N$-dimensional state exactly, existing state-preparation approaches generally require a runtime of $O(N)$, which will ruin the speedup of the HHL algorithm. Here we show that the states can be prepared approximately with a runtime of $O(poly(\log N))$ by employing a slightly modified version of the HHL algorithm itself. Thus, applying this approach to prepare the initial state of the original HHL algorithm can preserve the exponential speedup advantage. It can also serve as a standalone solution for other applications demanding rapid state preparation.

## 机器学习(stat.ML:Machine Learning)

### Finite Sample Analysis of Distributional TD Learning with Linear Function Approximation 
[[arxiv](https://arxiv.org/abs/2502.14172)] [[cool](https://papers.cool/arxiv/2502.14172)] [[pdf](https://arxiv.org/pdf/2502.14172)]
> **Authors**: Yang Peng,Kaicheng Jin,Liangyu Zhang,Zhihua Zhang
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 57 pages
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: In this paper, we investigate the finite-sample statistical rates of distributional temporal difference (TD) learning with linear function approximation. The aim of distributional TD learning is to estimate the return distribution of a discounted Markov decision process for a given policy π. Prior works on statistical analysis of distributional TD learning mainly focus on the tabular case. In contrast, we first consider the linear function approximation setting and derive sharp finite-sample rates. Our theoretical results demonstrate that the sample complexity of linear distributional TD learning matches that of the classic linear TD learning. This implies that, with linear function approximation, learning the full distribution of the return using streaming data is no more difficult than learning its expectation (i.e. the value function). To derive tight sample complexity bounds, we conduct a fine-grained analysis of the linear-categorical Bellman equation, and employ the exponential stability arguments for products of random matrices. Our findings provide new insights into the statistical efficiency of distributional reinforcement learning algorithms.

### Prediction-Powered Adaptive Shrinkage Estimation 
[[arxiv](https://arxiv.org/abs/2502.14166)] [[cool](https://papers.cool/arxiv/2502.14166)] [[pdf](https://arxiv.org/pdf/2502.14166)]
> **Authors**: Sida Li,Nikolaos Ignatiadis
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习,方法论
- **Abstract**: Prediction-Powered Inference (PPI) is a powerful framework for enhancing statistical estimates by combining limited gold-standard data with machine learning (ML) predictions. While prior work has demonstrated PPI's benefits for individual statistical tasks, modern applications require answering numerous parallel statistical questions. We introduce Prediction-Powered Adaptive Shrinkage (PAS), a method that bridges PPI with empirical Bayes shrinkage to improve the estimation of multiple means. PAS debiases noisy ML predictions within each task and then borrows strength across tasks by using those same predictions as a reference point for shrinkage. The amount of shrinkage is determined by minimizing an unbiased estimate of risk, and we prove that this tuning strategy is asymptotically optimal. Experiments on both synthetic and real-world datasets show that PAS adapts to the reliability of the ML predictions and outperforms traditional and modern baselines in large-scale applications.

### Multi-Objective Bayesian Optimization for Networked Black-Box Systems: A Path to Greener Profits and Smarter Designs 
[[arxiv](https://arxiv.org/abs/2502.14121)] [[cool](https://papers.cool/arxiv/2502.14121)] [[pdf](https://arxiv.org/pdf/2502.14121)]
> **Authors**: Akshay Kudva,Wei-Ting Tang,Joel A. Paulson
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Designing modern industrial systems requires balancing several competing objectives, such as profitability, resilience, and sustainability, while accounting for complex interactions between technological, economic, and environmental factors. Multi-objective optimization (MOO) methods are commonly used to navigate these tradeoffs, but selecting the appropriate algorithm to tackle these problems is often unclear, particularly when system representations vary from fully equation-based (white-box) to entirely data-driven (black-box) models. While grey-box MOO methods attempt to bridge this gap, they typically impose rigid assumptions on system structure, requiring models to conform to the underlying structural assumptions of the solver rather than the solver adapting to the natural representation of the system of interest. In this chapter, we introduce a unifying approach to grey-box MOO by leveraging network representations, which provide a general and flexible framework for modeling interconnected systems as a series of function nodes that share various inputs and outputs. Specifically, we propose MOBONS, a novel Bayesian optimization-inspired algorithm that can efficiently optimize general function networks, including those with cyclic dependencies, enabling the modeling of feedback loops, recycle streams, and multi-scale simulations - features that existing methods fail to capture. Furthermore, MOBONS incorporates constraints, supports parallel evaluations, and preserves the sample efficiency of Bayesian optimization while leveraging network structure for improved scalability. We demonstrate the effectiveness of MOBONS through two case studies, including one related to sustainable process design. By enabling efficient MOO under general graph representations, MOBONS has the potential to significantly enhance the design of more profitable, resilient, and sustainable engineering systems.

### Conformal Prediction under Lévy-Prokhorov Distribution Shifts: Robustness to Local and Global Perturbations 
[[arxiv](https://arxiv.org/abs/2502.14105)] [[cool](https://papers.cool/arxiv/2502.14105)] [[pdf](https://arxiv.org/pdf/2502.14105)]
> **Authors**: Liviu Aolaritei,Michael I. Jordan,Youssef Marzouk,Zheyu Oliver Wang,Julie Zhu
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习,统计理论,方法论
- **Abstract**: Conformal prediction provides a powerful framework for constructing prediction intervals with finite-sample guarantees, yet its robustness under distribution shifts remains a significant challenge. This paper addresses this limitation by modeling distribution shifts using Lévy-Prokhorov (LP) ambiguity sets, which capture both local and global perturbations. We provide a self-contained overview of LP ambiguity sets and their connections to popular metrics such as Wasserstein and Total Variation. We show that the link between conformal prediction and LP ambiguity sets is a natural one: by propagating the LP ambiguity set through the scoring function, we reduce complex high-dimensional distribution shifts to manageable one-dimensional distribution shifts, enabling exact quantification of worst-case quantiles and coverage. Building on this analysis, we construct robust conformal prediction intervals that remain valid under distribution shifts, explicitly linking LP parameters to interval width and confidence levels. Experimental results on real-world datasets demonstrate the effectiveness of the proposed approach.

### New Lower Bounds for Stochastic Non-Convex Optimization through Divergence Composition 
[[arxiv](https://arxiv.org/abs/2502.14060)] [[cool](https://papers.cool/arxiv/2502.14060)] [[pdf](https://arxiv.org/pdf/2502.14060)]
> **Authors**: El Mehdi Saad,Weicheng Lee,Francesco Orabona
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习,优化与控制
- **Abstract**: We study fundamental limits of first-order stochastic optimization in a range of nonconvex settings, including L-smooth functions satisfying Quasar-Convexity (QC), Quadratic Growth (QG), and Restricted Secant Inequalities (RSI). While the convergence properties of standard algorithms are well-understood in deterministic regimes, significantly fewer results address the stochastic case, where only unbiased and noisy gradients are available. We establish new lower bounds on the number of noisy gradient queries to minimize these classes of functions, also showing that they are tight (up to a logarithmic factor) in all the relevant quantities characterizing each class. Our approach reformulates the optimization task as a function identification problem, leveraging divergence composition arguments to construct a challenging subclass that leads to sharp lower bounds. Furthermore, we present a specialized algorithm in the one-dimensional setting that achieves faster rates, suggesting that certain dimensional thresholds are intrinsic to the complexity of non-convex stochastic optimization.

### Towards a perturbation-based explanation for medical AI as differentiable programs 
[[arxiv](https://arxiv.org/abs/2502.14001)] [[cool](https://papers.cool/arxiv/2502.14001)] [[pdf](https://arxiv.org/pdf/2502.14001)]
> **Authors**: Takeshi Abe,Yoshiyuki Asai
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 7 pages, 1 figure
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Recent advancement in machine learning algorithms reaches a point where medical devices can be equipped with artificial intelligence (AI) models for diagnostic support and routine automation in clinical settings. In medicine and healthcare, there is a particular demand for sufficient and objective explainability of the outcome generated by AI models. However, AI models are generally considered as black boxes due to their complexity, and the computational process leading to their response is often opaque. Although several methods have been proposed to explain the behavior of models by evaluating the importance of each feature in discrimination and prediction, they may suffer from biases and opacities arising from the scale and sampling protocol of the dataset used for training or testing. To overcome the shortcomings of existing methods, we explore an alternative approach to provide an objective explanation of AI models that can be defined independently of the learning process and does not require additional data. As a preliminary study for this direction of research, this work examines a numerical availability of the Jacobian matrix of deep learning models that measures how stably a model responses against small perturbations added to the input. The indicator, if available, are calculated from a trained AI model for a given target input. This is a first step towards a perturbation-based explanation, which will assist medical practitioners in understanding and interpreting the response of the AI model in its clinical application.

### The Computational Advantage of Depth: Learning High-Dimensional Hierarchical Functions with Gradient Descent 
[[arxiv](https://arxiv.org/abs/2502.13961)] [[cool](https://papers.cool/arxiv/2502.13961)] [[pdf](https://arxiv.org/pdf/2502.13961)]
> **Authors**: Yatin Dandi,Luca Pesce,Lenka Zdeborová,Florent Krzakala
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Understanding the advantages of deep neural networks trained by gradient descent (GD) compared to shallow models remains an open theoretical challenge. While the study of multi-index models with Gaussian data in high dimensions has provided analytical insights into the benefits of GD-trained neural networks over kernels, the role of depth in improving sample complexity and generalization in GD-trained networks remains poorly understood. In this paper, we introduce a class of target functions (single and multi-index Gaussian hierarchical targets) that incorporate a hierarchy of latent subspace dimensionalities. This framework enables us to analytically study the learning dynamics and generalization performance of deep networks compared to shallow ones in the high-dimensional limit. Specifically, our main theorem shows that feature learning with GD reduces the effective dimensionality, transforming a high-dimensional problem into a sequence of lower-dimensional ones. This enables learning the target function with drastically less samples than with shallow networks. While the results are proven in a controlled training setting, we also discuss more common training procedures and argue that they learn through the same mechanisms. These findings open the way to further quantitative studies of the crucial role of depth in learning hierarchical structures with deep networks.

### Uncertainty quantification for Markov chains with application to temporal difference learning 
[[arxiv](https://arxiv.org/abs/2502.13822)] [[cool](https://papers.cool/arxiv/2502.13822)] [[pdf](https://arxiv.org/pdf/2502.13822)]
> **Authors**: Weichen Wu,Yuting Wei,Alessandro Rinaldo
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Markov chains are fundamental to statistical machine learning, underpinning key methodologies such as Markov Chain Monte Carlo (MCMC) sampling and temporal difference (TD) learning in reinforcement learning (RL). Given their widespread use, it is crucial to establish rigorous probabilistic guarantees on their convergence, uncertainty, and stability. In this work, we develop novel, high-dimensional concentration inequalities and Berry-Esseen bounds for vector- and matrix-valued functions of Markov chains, addressing key limitations in existing theoretical tools for handling dependent data. We leverage these results to analyze the TD learning algorithm, a widely used method for policy evaluation in RL. Our analysis yields a sharp high-probability consistency guarantee that matches the asymptotic variance up to logarithmic factors. Furthermore, we establish a $O(T^{-\frac{1}{4}}\log T)$ distributional convergence rate for the Gaussian approximation of the TD estimator, measured in convex distance. These findings provide new insights into statistical inference for RL algorithms, bridging the gaps between classical stochastic approximation theory and modern reinforcement learning applications.

### Identifying metric structures of deep latent variable models 
[[arxiv](https://arxiv.org/abs/2502.13757)] [[cool](https://papers.cool/arxiv/2502.13757)] [[pdf](https://arxiv.org/pdf/2502.13757)]
> **Authors**: Stas Syrota,Yevgen Zainchkovskyy,Johnny Xi,Benjamin Bloem-Reddy,Søren Hauberg
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Deep latent variable models learn condensed representations of data that, hopefully, reflect the inner workings of the studied phenomena. Unfortunately, these latent representations are not statistically identifiable, meaning they cannot be uniquely determined. Domain experts, therefore, need to tread carefully when interpreting these. Current solutions limit the lack of identifiability through additional constraints on the latent variable model, e.g. by requiring labeled training data, or by restricting the expressivity of the model. We change the goal: instead of identifying the latent variables, we identify relationships between them such as meaningful distances, angles, and volumes. We prove this is feasible under very mild model conditions and without additional labeled data. We empirically demonstrate that our theory results in more reliable latent distances, offering a principled path forward in extracting trustworthy conclusions from deep latent variable models.

### Graph Signal Inference by Learning Narrowband Spectral Kernels 
[[arxiv](https://arxiv.org/abs/2502.13686)] [[cool](https://papers.cool/arxiv/2502.13686)] [[pdf](https://arxiv.org/pdf/2502.13686)]
> **Authors**: Osman Furkan Kar,Gülce Turhan,Elif Vural
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: While a common assumption in graph signal analysis is the smoothness of the signals or the band-limitedness of their spectrum, in many instances the spectrum of real graph data may be concentrated at multiple regions of the spectrum, possibly including mid-to-high-frequency components. In this work, we propose a novel graph signal model where the signal spectrum is represented through the combination of narrowband kernels in the graph frequency domain. We then present an algorithm that jointly learns the model by optimizing the kernel parameters and the signal representation coefficients from a collection of graph signals. Our problem formulation has the flexibility of permitting the incorporation of signals possibly acquired on different graphs into the learning algorithm. We then theoretically study the signal reconstruction performance of the proposed method, by also elaborating on when joint learning on multiple graphs is preferable to learning an individual model on each graph. Experimental results on several graph data sets shows that the proposed method offers quite satisfactory signal interpolation accuracy in comparison with a variety of reference approaches in the literature.

### An Efficient Permutation-Based Kernel Two-Sample Test 
[[arxiv](https://arxiv.org/abs/2502.13570)] [[cool](https://papers.cool/arxiv/2502.13570)] [[pdf](https://arxiv.org/pdf/2502.13570)]
> **Authors**: Antoine Chatalic,Marco Letizia,Nicolas Schreuder,and Lorenzo Rosasco
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-20
> **comment**: 23 pages, 2 figures
- **标题**: None
- **领域**: 机器学习,机器学习,统计理论,方法论
- **Abstract**: Two-sample hypothesis testing-determining whether two sets of data are drawn from the same distribution-is a fundamental problem in statistics and machine learning with broad scientific applications. In the context of nonparametric testing, maximum mean discrepancy (MMD) has gained popularity as a test statistic due to its flexibility and strong theoretical foundations. However, its use in large-scale scenarios is plagued by high computational costs. In this work, we use a Nyström approximation of the MMD to design a computationally efficient and practical testing algorithm while preserving statistical guarantees. Our main result is a finite-sample bound on the power of the proposed test for distributions that are sufficiently separated with respect to the MMD. The derived separation rate matches the known minimax optimal rate in this setting. We support our findings with a series of numerical experiments, emphasizing realistic scientific data.

## 其他论文

- [Feedforward in Generative AI: Opportunities for a Design Space](https://arxiv.org/abs/2502.14229)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Adaptive Convolution for CNN-based Speech Enhancement Models](https://arxiv.org/abs/2502.14224)
  - **标题**: None
  - **Filtered Reason**: none of cs.SD,eess.AS in whitelist
- [Ask Me Anything: Exploring children's attitudes toward an age-tailored AI-powered chatbot](https://arxiv.org/abs/2502.14217)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Less is More: On the Importance of Data Quality for Unit Test Generation](https://arxiv.org/abs/2502.14212)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR,cs.SE in whitelist
- [Collaborative Retrieval for Large Language Model-based Conversational Recommender Systems](https://arxiv.org/abs/2502.14137)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [SALTY: Explainable Artificial Intelligence Guided Structural Analysis for Hardware Trojan Detection](https://arxiv.org/abs/2502.14116)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [Goggin's corrected Kalman Filter: Guarantees and Filtering Regimes](https://arxiv.org/abs/2502.14053)
  - **标题**: None
  - **Filtered Reason**: none of cs.IT in whitelist
- [A Matter of Perspective(s): Contrasting Human and LLM Argumentation in Subjective Decision-Making on Subtle Sexism](https://arxiv.org/abs/2502.14052)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [A General Framework for Augmenting Lossy Compressors with Topological Guarantees](https://arxiv.org/abs/2502.14022)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC,cs.IT in whitelist
- [SelfAge: Personalized Facial Age Transformation Using Self-reference Images](https://arxiv.org/abs/2502.13987)
  - **标题**: None
  - **Filtered Reason**: none of eess.IV,cs.GR in whitelist
- [Bounded Synthesis of Synchronized Distributed Models from Lightweight Specifications](https://arxiv.org/abs/2502.13955)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Judging the Judges: A Collection of LLM-Generated Relevance Judgements](https://arxiv.org/abs/2502.13908)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [Audio-Based Classification of Insect Species Using Machine Learning Models: Cicada, Beetle, Termite, and Cricket](https://arxiv.org/abs/2502.13893)
  - **标题**: None
  - **Filtered Reason**: none of cs.SD,eess.AS in whitelist
- [The KnowWhereGraph: A Large-Scale Geo-Knowledge Graph for Interdisciplinary Knowledge Discovery and Geo-Enrichment](https://arxiv.org/abs/2502.13874)
  - **标题**: None
  - **Filtered Reason**: none of cs.DB in whitelist
- [ArtMentor: AI-Assisted Evaluation of Artworks to Explore Multimodal Large Language Models Capabilities](https://arxiv.org/abs/2502.13832)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Binary VPN Traffic Detection Using Wavelet Features and Machine Learning](https://arxiv.org/abs/2502.13804)
  - **标题**: None
  - **Filtered Reason**: none of cs.NI in whitelist
- [Generative Large Recommendation Models: Emerging Trends in LLMs for Recommendation](https://arxiv.org/abs/2502.13783)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [User Agency and System Automation in Interactive Intelligent Systems](https://arxiv.org/abs/2502.13779)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Unsupervised Graph Embeddings for Session-based Recommendation with Item Features](https://arxiv.org/abs/2502.13763)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [TALKPLAY: Multimodal Music Recommendation with Large Language Models](https://arxiv.org/abs/2502.13713)
  - **标题**: None
  - **Filtered Reason**: none of cs.SD,cs.IR,eess.AS in whitelist
- [Active Illumination for Visual Ego-Motion Estimation in the Dark](https://arxiv.org/abs/2502.13708)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [User Association and Coordinated Beamforming in Cognitive Aerial-Terrestrial Networks: A Safe Reinforcement Learning Approach](https://arxiv.org/abs/2502.13663)
  - **标题**: None
  - **Filtered Reason**: none of eess.SP,cs.IT in whitelist
- [First Glimpse on Physical Layer Security in Internet of Vehicles: Transformed from Communication Interference to Sensing Interference](https://arxiv.org/abs/2502.13634)
  - **标题**: None
  - **Filtered Reason**: none of cs.IT in whitelist
- [AI-Empowered Catalyst Discovery: A Survey from Classical Machine Learning Approaches to Large Language Models](https://arxiv.org/abs/2502.13626)
  - **标题**: None
  - **Filtered Reason**: none of cs.CE in whitelist
- [Improving the Sparse Structure Learning of Spiking Neural Networks from the View of Compression Efficiency](https://arxiv.org/abs/2502.13572)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Bursting Filter Bubble: Enhancing Serendipity Recommendations with Aligned Large Language Models](https://arxiv.org/abs/2502.13539)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [VLAS: Vision-Language-Action Model With Speech Instructions For Customized Robot Manipulation](https://arxiv.org/abs/2502.13508)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [Reproducing NevIR: Negation in Neural Information Retrieval](https://arxiv.org/abs/2502.13506)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [LLM4Tag: Automatic Tagging System for Information Retrieval via Large Language Models](https://arxiv.org/abs/2502.13481)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [MapNav: A Novel Memory Representation via Annotated Semantic Maps for VLM-based Vision-and-Language Navigation](https://arxiv.org/abs/2502.13451)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [On Qualitative Preference in Alternating-time Temporal Logic with Strategy Contexts](https://arxiv.org/abs/2502.13436)
  - **标题**: None
  - **Filtered Reason**: none of cs.MA,cs.LO in whitelist
- [MATS: An Audio Language Model under Text-only Supervision](https://arxiv.org/abs/2502.13433)
  - **标题**: None
  - **Filtered Reason**: none of cs.SD,eess.AS in whitelist
