> 本文由 [https://github.com/huiyeruzhou/arxiv_crawler](https://github.com/huiyeruzhou/arxiv_crawler) 自动生成
>
> 领域白名单：cs.AI,cs.CL,cs.LG,cs.CV
> 关键词： LLM, GPT, AI, language+model, deep+learning, transformer, neural+network, machine+learning

# 论文全览：2025-02-21

共有354篇相关领域论文, 另有37篇其他

## 天体物理学仪器和方法(astro-ph.IM:Instrumentation and Methods for Astrophysics)

### Reducing false positives in strong lens detection through effective augmentation and ensemble learning 
[[arxiv](https://arxiv.org/abs/2502.14936)] [[cool](https://papers.cool/arxiv/2502.14936)] [[pdf](https://arxiv.org/pdf/2502.14936)]
> **Authors**: Samira Rezaei,Amirmohammad Chegeni,Bharath Chowdhary Nagam,J. P. McKean,Mitra Baratchi,Koen Kuijken,Léon V. E. Koopmans
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 15 pages, 14 figures, 7 tables, Accepted for publication in MNRAS
- **标题**: None
- **领域**: 天体物理学仪器和方法,宇宙学和非银河系天体物理学,星系天体物理学,计算机视觉和模式识别
- **Abstract**: This research studies the impact of high-quality training datasets on the performance of Convolutional Neural Networks (CNNs) in detecting strong gravitational lenses. We stress the importance of data diversity and representativeness, demonstrating how variations in sample populations influence CNN performance. In addition to the quality of training data, our results highlight the effectiveness of various techniques, such as data augmentation and ensemble learning, in reducing false positives while maintaining model completeness at an acceptable level. This enhances the robustness of gravitational lens detection models and advancing capabilities in this field. Our experiments, employing variations of DenseNet and EfficientNet, achieved a best false positive rate (FP rate) of $10^{-4}$, while successfully identifying over 88 per cent of genuine gravitational lenses in the test dataset. This represents an 11-fold reduction in the FP rate compared to the original training dataset. Notably, this substantial enhancement in the FP rate is accompanied by only a 2.3 per cent decrease in the number of true positive samples. Validated on the KiDS dataset, our findings offer insights applicable to ongoing missions, like Euclid.

## 太阳和恒星天体物理学(astro-ph.SR:Solar and Stellar Astrophysics)

### An Interpretable Machine Learning Approach to Understanding the Relationships between Solar Flares and Source Active Regions 
[[arxiv](https://arxiv.org/abs/2502.15066)] [[cool](https://papers.cool/arxiv/2502.15066)] [[pdf](https://arxiv.org/pdf/2502.15066)]
> **Authors**: Huseyin Cavus,Jason T. L. Wang,Teja P. S. Singampalli,Gani Caglar Coban,Hongyang Zhang,Abd-ur Raheem,Haimin Wang
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 16 pages, 9 figures
- **标题**: None
- **领域**: 太阳和恒星天体物理学,机器学习,空间物理学
- **Abstract**: Solar flares are defined as outbursts on the surface of the Sun. They occur when energy accumulated in magnetic fields enclosing solar active regions (ARs) is abruptly expelled. Solar flares and associated coronal mass ejections are sources of space weather that adversely impact devices at or near Earth, including the obstruction of high-frequency radio waves utilized for communication and the deterioration of power grid operations. Tracking and delivering early and precise predictions of solar flares is essential for readiness and catastrophe risk mitigation. This paper employs the random forest (RF) model to address the binary classification task, analyzing the links between solar flares and their originating ARs with observational data gathered from 2011 to 2021 by SolarMonitor.org and the XRT flare database. We seek to identify the physical features of a source AR that significantly influence its potential to trigger >=C-class flares. We found that the features of AR_Type_Today, Hale_Class_Yesterday are the most and the least prepotent features, respectively. NoS_Difference has a remarkable effect in decision-making in both global and local interpretations.

## 材料科学(cond-mat.mtrl-sci:Materials Science)

### Towards an automated workflow in materials science for combining multi-modal simulative and experimental information using data mining and large language models 
[[arxiv](https://arxiv.org/abs/2502.14904)] [[cool](https://papers.cool/arxiv/2502.14904)] [[pdf](https://arxiv.org/pdf/2502.14904)]
> **Authors**: Balduin Katzer,Steffen Klinder,Katrin Schulz
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 材料科学,机器学习
- **Abstract**: To retrieve and compare scientific data of simulations and experiments in materials science, data needs to be easily accessible and machine readable to qualify and quantify various materials science phenomena. The recent progress in open science leverages the accessibility to data. However, a majority of information is encoded within scientific documents limiting the capability of finding suitable literature as well as material properties. This manuscript showcases an automated workflow, which unravels the encoded information from scientific literature to a machine readable data structure of texts, figures, tables, equations and meta-data, using natural language processing and language as well as vision transformer models to generate a machine-readable database. The machine-readable database can be enriched with local data, as e.g. unpublished or private material data, leading to knowledge synthesis. The study shows that such an automated workflow accelerates information retrieval, proximate context detection and material property extraction from multi-modal input data exemplarily shown for the research field of microstructural analyses of face-centered cubic single crystals. Ultimately, a Retrieval-Augmented Generation (RAG) based Large Language Model (LLM) enables a fast and efficient question answering chat bot.

## 人工智能(cs.AI:Artificial Intelligence)

### Measuring AI agent autonomy: Towards a scalable approach with code inspection 
[[arxiv](https://arxiv.org/abs/2502.15212)] [[cool](https://papers.cool/arxiv/2502.15212)] [[pdf](https://arxiv.org/pdf/2502.15212)]
> **Authors**: Peter Cihon,Merlin Stein,Gagan Bansal,Sam Manning,Kevin Xu
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: NeurIPS Socially ResponsibleLanguageModelling Research (SoLaR) Workshop 2024
- **标题**: None
- **领域**: 人工智能
- **Abstract**: AI agents are AI systems that can achieve complex goals autonomously. Assessing the level of agent autonomy is crucial for understanding both their potential benefits and risks. Current assessments of autonomy often focus on specific risks and rely on run-time evaluations -- observations of agent actions during operation. We introduce a code-based assessment of autonomy that eliminates the need to run an AI agent to perform specific tasks, thereby reducing the costs and risks associated with run-time evaluations. Using this code-based framework, the orchestration code used to run an AI agent can be scored according to a taxonomy that assesses attributes of autonomy: impact and oversight. We demonstrate this approach with the AutoGen framework and select applications.

### The Imitation Game for Educational AI 
[[arxiv](https://arxiv.org/abs/2502.15127)] [[cool](https://papers.cool/arxiv/2502.15127)] [[pdf](https://arxiv.org/pdf/2502.15127)]
> **Authors**: Shashank Sonkar,Naiming Liu,Xinghe Chen,Richard G. Baraniuk
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,人机交互
- **Abstract**: As artificial intelligence systems become increasingly prevalent in education, a fundamental challenge emerges: how can we verify if an AI truly understands how students think and reason? Traditional evaluation methods like measuring learning gains require lengthy studies confounded by numerous variables. We present a novel evaluation framework based on a two-phase Turing-like test. In Phase 1, students provide open-ended responses to questions, revealing natural misconceptions. In Phase 2, both AI and human experts, conditioned on each student's specific mistakes, generate distractors for new related questions. By analyzing whether students select AI-generated distractors at rates similar to human expert-generated ones, we can validate if the AI models student cognition. We prove this evaluation must be conditioned on individual responses - unconditioned approaches merely target common misconceptions. Through rigorous statistical sampling theory, we establish precise requirements for high-confidence validation. Our research positions conditioned distractor generation as a probe into an AI system's fundamental ability to model student thinking - a capability that enables adapting tutoring, feedback, and assessments to each student's specific needs.

### GenAI vs. Human Fact-Checkers: Accurate Ratings, Flawed Rationales 
[[arxiv](https://arxiv.org/abs/2502.14943)] [[cool](https://papers.cool/arxiv/2502.14943)] [[pdf](https://arxiv.org/pdf/2502.14943)]
> **Authors**: Yuehong Cassandra Tai,Khushi Navin Patni,Nicholas Daniel Hemauer,Bruce Desmarais,Yu-Ru Lin
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: Accepted for publication in the 17th ACM Web Science Conference 2025
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: Despite recent advances in understanding the capabilities and limits of generative artificial intelligence (GenAI) models, we are just beginning to understand their capacity to assess and reason about the veracity of content. We evaluate multiple GenAI models across tasks that involve the rating of, and perceived reasoning about, the credibility of information. The information in our experiments comes from content that subnational U.S. politicians post to Facebook. We find that GPT-4o, one of the most used AI models in consumer applications, outperforms other models, but all models exhibit only moderate agreement with human coders. Importantly, even when GenAI models accurately identify low-credibility content, their reasoning relies heavily on linguistic features and ``hard'' criteria, such as the level of detail, source reliability, and language formality, rather than an understanding of veracity. We also assess the effectiveness of summarized versus full content inputs, finding that summarized content holds promise for improving efficiency without sacrificing accuracy. While GenAI has the potential to support human fact-checkers in scaling misinformation detection, our results caution against relying solely on these models.

### Optimizing Model Selection for Compound AI Systems 
[[arxiv](https://arxiv.org/abs/2502.14815)] [[cool](https://papers.cool/arxiv/2502.14815)] [[pdf](https://arxiv.org/pdf/2502.14815)]
> **Authors**: Lingjiao Chen,Jared Quincy Davis,Boris Hanin,Peter Bailis,Matei Zaharia,James Zou,Ion Stoica
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学,机器学习,多代理系统
- **Abstract**: Compound AI systems that combine multiple LLM calls, such as self-refine and multi-agent-debate, achieve strong performance on many AI tasks. We address a core question in optimizing compound systems: for each LLM call or module in the system, how should one decide which LLM to use? We show that these LLM choices have a large effect on quality, but the search space is exponential. We propose LLMSelector, an efficient framework for model selection in compound systems, which leverages two key empirical insights: (i) end-to-end performance is often monotonic in how well each module performs, with all other modules held fixed, and (ii) per-module performance can be estimated accurately by an LLM. Building upon these insights, LLMSelector iteratively selects one module and allocates to it the model with the highest module-wise performance, as estimated by an LLM, until no further gain is possible. LLMSelector is applicable to any compound system with a bounded number of modules, and its number of API calls scales linearly with the number of modules, achieving high-quality model allocation both empirically and theoretically. Experiments with popular compound systems such as multi-agent debate and self-refine using LLMs such as GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 show that LLMSelector confers 5%-70% accuracy gains compared to using the same LLM for all modules.

### EquivaMap: Leveraging LLMs for Automatic Equivalence Checking of Optimization Formulations 
[[arxiv](https://arxiv.org/abs/2502.14760)] [[cool](https://papers.cool/arxiv/2502.14760)] [[pdf](https://arxiv.org/pdf/2502.14760)]
> **Authors**: Haotian Zhai,Connor Lawless,Ellen Vitercik,Liu Leqi
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习,优化与控制
- **Abstract**: A fundamental problem in combinatorial optimization is identifying equivalent formulations, which can lead to more efficient solution strategies and deeper insights into a problem's computational complexity. The need to automatically identify equivalence between problem formulations has grown as optimization copilots--systems that generate problem formulations from natural language descriptions--have proliferated. However, existing approaches to checking formulation equivalence lack grounding, relying on simple heuristics which are insufficient for rigorous validation. Inspired by Karp reductions, in this work we introduce quasi-Karp equivalence, a formal criterion for determining when two optimization formulations are equivalent based on the existence of a mapping between their decision variables. We propose EquivaMap, a framework that leverages large language models to automatically discover such mappings, enabling scalable and reliable equivalence verification. To evaluate our approach, we construct the first open-source dataset of equivalent optimization formulations, generated by applying transformations such as adding slack variables or valid inequalities to existing formulations. Empirically, EquivaMap significantly outperforms existing methods, achieving substantial improvements in correctly identifying formulation equivalence.

### From Knowledge Generation to Knowledge Verification: Examining the BioMedical Generative Capabilities of ChatGPT 
[[arxiv](https://arxiv.org/abs/2502.14714)] [[cool](https://papers.cool/arxiv/2502.14714)] [[pdf](https://arxiv.org/pdf/2502.14714)]
> **Authors**: Ahmed Abdeen Hamed,Byung Suk Lee
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 26 pages, 6 figures, In Review with a Cell Press Journal
- **标题**: None
- **领域**: 人工智能,计算语言学,信息检索
- **Abstract**: The generative capabilities of LLM models present opportunities in accelerating tasks and concerns with the authenticity of the knowledge it produces. To address the concerns, we present a computational approach that systematically evaluates the factual accuracy of biomedical knowledge that an LLM model has been prompted to generate. Our approach encompasses two processes: the generation of disease-centric associations and the verification of them using the semantic knowledge of the biomedical ontologies. Using ChatGPT as the select LLM model, we designed a set of prompt-engineering processes to generate linkages between diseases, drugs, symptoms, and genes to establish grounds for assessments. Experimental results demonstrate high accuracy in identifying disease terms (88%-97%), drug names (90%-91%), and genetic information (88%-98%). The symptom term identification accuracy was notably lower (49%-61%), as verified against the DOID, ChEBI, SYMPTOM, and GO ontologies accordingly. The verification of associations reveals literature coverage rates of (89%-91%) among disease-drug and disease-gene associations. The low identification accuracy for symptom terms also contributed to the verification of symptom-related associations (49%-62%).

### A Statistical Case Against Empirical Human-AI Alignment 
[[arxiv](https://arxiv.org/abs/2502.14581)] [[cool](https://papers.cool/arxiv/2502.14581)] [[pdf](https://arxiv.org/pdf/2502.14581)]
> **Authors**: Julian Rodemann,Esteban Garces Arias,Christoph Luther,Christoph Jansen,Thomas Augustin
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 24 pages, 2 figures, 5 tables
- **标题**: None
- **领域**: 人工智能,计算语言学,机器学习,其他统计数据
- **Abstract**: Empirical human-AI alignment aims to make AI systems act in line with observed human behavior. While noble in its goals, we argue that empirical alignment can inadvertently introduce statistical biases that warrant caution. This position paper thus advocates against naive empirical alignment, offering prescriptive alignment and a posteriori empirical alignment as alternatives. We substantiate our principled argument by tangible examples like human-centric decoding of language models.

### Plan-over-Graph: Towards Parallelable LLM Agent Schedule 
[[arxiv](https://arxiv.org/abs/2502.14563)] [[cool](https://papers.cool/arxiv/2502.14563)] [[pdf](https://arxiv.org/pdf/2502.14563)]
> **Authors**: Shiqi Zhang,Xinbei Ma,Zouying Cao,Zhuosheng Zhang,Hai Zhao
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Large Language Models (LLMs) have demonstrated exceptional abilities in reasoning for task planning. However, challenges remain under-explored for parallel schedules. This paper introduces a novel paradigm, plan-over-graph, in which the model first decomposes a real-life textual task into executable subtasks and constructs an abstract task graph. The model then understands this task graph as input and generates a plan for parallel execution. To enhance the planning capability of complex, scalable graphs, we design an automated and controllable pipeline to generate synthetic graphs and propose a two-stage training scheme. Experimental results show that our plan-over-graph method significantly improves task performance on both API-based LLMs and trainable open-sourced LLMs. By normalizing complex tasks as graphs, our method naturally supports parallel execution, demonstrating global efficiency. The code and data are available at https://github.com/zsq259/Plan-over-Graph.

### Statistical Scenario Modelling and Lookalike Distributions for Multi-Variate AI Risk 
[[arxiv](https://arxiv.org/abs/2502.14491)] [[cool](https://papers.cool/arxiv/2502.14491)] [[pdf](https://arxiv.org/pdf/2502.14491)]
> **Authors**: Elija Perrier
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: Under review
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Evaluating AI safety requires statistically rigorous methods and risk metrics for understanding how the use of AI affects aggregated risk. However, much AI safety literature focuses upon risks arising from AI models in isolation, lacking consideration of how modular use of AI affects risk distribution of workflow components or overall risk metrics. There is also a lack of statistical grounding enabling sensitisation of risk models in the presence of absence of AI to estimate causal contributions of AI. This is in part due to the dearth of AI impact data upon which to fit distributions. In this work, we address these gaps in two ways. First, we demonstrate how scenario modelling (grounded in established statistical techniques such as Markov chains, copulas and Monte Carlo simulation) can be used to model AI risk holistically. Second, we show how lookalike distributions from phenomena analogous to AI can be used to estimate AI impacts in the absence of directly observable data. We demonstrate the utility of our methods for benchmarking cumulative AI risk via risk analysis of a logistic scenario simulations.

### Narrative-Driven Travel Planning: Geoculturally-Grounded Script Generation with Evolutionary Itinerary Optimization 
[[arxiv](https://arxiv.org/abs/2502.14456)] [[cool](https://papers.cool/arxiv/2502.14456)] [[pdf](https://arxiv.org/pdf/2502.14456)]
> **Authors**: Ran Ding,Ziyu Zhang,Ying Zhu,Ziqian Kong,Peilan Xu
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: To enhance tourists' experiences and immersion, this paper proposes a narrative-driven travel planning framework called NarrativeGuide, which generates a geoculturally-grounded narrative script for travelers, offering a novel, role-playing experience for their journey. In the initial stage, NarrativeGuide constructs a knowledge graph for attractions within a city, then configures the worldview, character setting, and exposition based on the knowledge graph. Using this foundation, the knowledge graph is combined to generate an independent scene unit for each attraction. During the itinerary planning stage, NarrativeGuide models narrative-driven travel planning as an optimization problem, utilizing a genetic algorithm (GA) to refine the itinerary. Before evaluating the candidate itinerary, transition scripts are generated for each pair of adjacent attractions, which, along with the scene units, form a complete script. The weighted sum of script coherence, travel time, and attraction scores is then used as the fitness value to update the candidate solution set. Experimental results across four cities, i.e., Nanjing and Yangzhou in China, Paris in France, and Berlin in Germany, demonstrate significant improvements in narrative coherence and cultural fit, alongside a notable reduction in travel time and an increase in the quality of visited attractions. Our study highlights that incorporating external evolutionary optimization effectively addresses the limitations of large language models in travel planning.

### HPS: Hard Preference Sampling for Human Preference Alignment 
[[arxiv](https://arxiv.org/abs/2502.14400)] [[cool](https://papers.cool/arxiv/2502.14400)] [[pdf](https://arxiv.org/pdf/2502.14400)]
> **Authors**: Xiandong Zou,Wanyu Lin,Yuchen Li,Pan Zhou
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Aligning Large Language Model (LLM) responses with human preferences is vital for building safe and controllable AI systems. While preference optimization methods based on Plackett-Luce (PL) and Bradley-Terry (BT) models have shown promise, they face challenges such as poor handling of harmful content, inefficient use of dispreferred responses, and, specifically for PL, high computational costs. To address these issues, we propose Hard Preference Sampling (HPS), a novel framework for robust and efficient human preference alignment. HPS introduces a training loss that prioritizes the most preferred response while rejecting all dispreferred and harmful ones. It emphasizes "hard" dispreferred responses--those closely resembling preferred ones--to enhance the model's rejection capabilities. By leveraging a single-sample Monte Carlo sampling strategy, HPS reduces computational overhead while maintaining alignment quality. Theoretically, HPS improves sample efficiency over existing PL methods and maximizes the reward margin between preferred and dispreferred responses, ensuring clearer distinctions. Experiments on HH-RLHF and PKU-Safety datasets validate HPS's effectiveness, achieving comparable BLEU and reward scores while greatly improving reward margins and thus reducing harmful content generation.

### Retrieval-Augmented Process Reward Model for Generalizable Mathematical Reasoning 
[[arxiv](https://arxiv.org/abs/2502.14361)] [[cool](https://papers.cool/arxiv/2502.14361)] [[pdf](https://arxiv.org/pdf/2502.14361)]
> **Authors**: Jiachen Zhu,Congmin Zheng,Jianghao Lin,Kounianhua Du,Ying Wen,Yong Yu,Jun Wang,Weinan Zhang
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,信息检索
- **Abstract**: While large language models (LLMs) have significantly advanced mathematical reasoning, Process Reward Models (PRMs) have been developed to evaluate the logical validity of reasoning steps. However, PRMs still struggle with out-of-distribution (OOD) challenges. This paper identifies key OOD issues, including step OOD, caused by differences in reasoning patterns across model types and sizes, and question OOD, which arises from dataset shifts between training data and real-world problems. To address these issues, we introduce Retrieval-Augmented Process Reward Model (RetrievalPRM), a novel framework designed to tackle these OOD issues. By utilizing a two-stage retrieval-enhanced mechanism, RetrievalPRM retrieves semantically similar questions and steps as a warmup, enhancing PRM's ability to evaluate target steps and improving generalization and reasoning consistency across different models and problem types. Our extensive experiments demonstrate that RetrievalPRM outperforms existing baselines across multiple real-world datasets. Our open-source contributions include a retrieval-enhanced dataset, a tuning framework for PRM training, and the RetrievalPRM model, establishing a new standard for PRM performance.

### FlowAgent: Achieving Compliance and Flexibility for Workflow Agents 
[[arxiv](https://arxiv.org/abs/2502.14345)] [[cool](https://papers.cool/arxiv/2502.14345)] [[pdf](https://arxiv.org/pdf/2502.14345)]
> **Authors**: Yuchen Shi,Siqi Cai,Zihan Xu,Yuei Qin,Gang Li,Hang Shao,Jiawei Chen,Deqing Yang,Ke Li,Xing Sun
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 8 pages
- **标题**: None
- **领域**: 人工智能
- **Abstract**: The integration of workflows with large language models (LLMs) enables LLM-based agents to execute predefined procedures, enhancing automation in real-world applications. Traditional rule-based methods tend to limit the inherent flexibility of LLMs, as their predefined execution paths restrict the models' action space, particularly when the unexpected, out-of-workflow (OOW) queries are encountered. Conversely, prompt-based methods allow LLMs to fully control the flow, which can lead to diminished enforcement of procedural compliance. To address these challenges, we introduce FlowAgent, a novel agent framework designed to maintain both compliance and flexibility. We propose the Procedure Description Language (PDL), which combines the adaptability of natural language with the precision of code to formulate workflows. Building on PDL, we develop a comprehensive framework that empowers LLMs to manage OOW queries effectively, while keeping the execution path under the supervision of a set of controllers. Additionally, we present a new evaluation methodology to rigorously assess an LLM agent's ability to handle OOW scenarios, going beyond routine flow compliance tested in existing benchmarks. Experiments on three datasets demonstrate that FlowAgent not only adheres to workflows but also effectively manages OOW queries, highlighting its dual strengths in compliance and flexibility. The code is available at https://github.com/Lightblues/FlowAgent.

### SPRIG: Stackelberg Perception-Reinforcement Learning with Internal Game Dynamics 
[[arxiv](https://arxiv.org/abs/2502.14264)] [[cool](https://papers.cool/arxiv/2502.14264)] [[pdf](https://arxiv.org/pdf/2502.14264)]
> **Authors**: Fernando Martinez-Lopez,Juntao Chen,Yingdong Lu
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: To appear in: AAAI 2025 Workshop on Planning and ReinforcementLearning(PRL) - Bridging the Gap BetweenAIPlanning and ReinforcementLearning
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Deep reinforcement learning agents often face challenges to effectively coordinate perception and decision-making components, particularly in environments with high-dimensional sensory inputs where feature relevance varies. This work introduces SPRIG (Stackelberg Perception-Reinforcement learning with Internal Game dynamics), a framework that models the internal perception-policy interaction within a single agent as a cooperative Stackelberg game. In SPRIG, the perception module acts as a leader, strategically processing raw sensory states, while the policy module follows, making decisions based on extracted features. SPRIG provides theoretical guarantees through a modified Bellman operator while preserving the benefits of modern policy optimization. Experimental results on the Atari BeamRider environment demonstrate SPRIG's effectiveness, achieving around 30% higher returns than standard PPO through its game-theoretical balance of feature extraction and decision-making.

## 计算复杂度(cs.CC:Computational Complexity)

### Low degree conjecture implies sharp computational thresholds in stochastic block model 
[[arxiv](https://arxiv.org/abs/2502.15024)] [[cool](https://papers.cool/arxiv/2502.15024)] [[pdf](https://arxiv.org/pdf/2502.15024)]
> **Authors**: Jingqiu Ding,Yiding Hua,Lucas Slot,David Steurer
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 33 pages
- **标题**: None
- **领域**: 计算复杂度,机器学习,统计理论,计算
- **Abstract**: We investigate implications of the (extended) low-degree conjecture (recently formalized in [MW23]) in the context of the symmetric stochastic block model. Assuming the conjecture holds, we establish that no polynomial-time algorithm can weakly recover community labels below the Kesten-Stigum (KS) threshold. In particular, we rule out polynomial-time estimators that, with constant probability, achieve correlation with the true communities that is significantly better than random. Whereas, above the KS threshold, polynomial-time algorithms are known to achieve constant correlation with the true communities with high probability[Mas14,AS15]. To our knowledge, we provide the first rigorous evidence for the sharp transition in recovery rate for polynomial-time algorithms at the KS threshold. Notably, under a stronger version of the low-degree conjecture, our lower bound remains valid even when the number of blocks diverges. Furthermore, our results provide evidence of a computational-to-statistical gap in learning the parameters of stochastic block models. In contrast to prior work, which either (i) rules out polynomial-time algorithms for hypothesis testing with 1-o(1) success probability [Hopkins18, BBK+21a] under the low-degree conjecture, or (ii) rules out low-degree polynomials for learning the edge connection probability matrix [LG23], our approach provides stronger lower bounds on the recovery and learning problem. Our proof combines low-degree lower bounds from [Hopkins18, BBK+21a] with graph splitting and cross-validation techniques. In order to rule out general recovery algorithms, we employ the correlation preserving projection method developed in [HS17].

## 计算工程、金融和科学(cs.CE:Computational Engineering, Finance, and Science)

### Revisiting Financial Sentiment Analysis: A Language Model Approach 
[[arxiv](https://arxiv.org/abs/2502.14897)] [[cool](https://papers.cool/arxiv/2502.14897)] [[pdf](https://arxiv.org/pdf/2502.14897)]
> **Authors**: Hamid Moradi-Kamali,Mohammad-Hossein Rajabi-Ghozlou,Mahdi Ghazavi,Ali Soltani,Amirreza Sattarzadeh,Reza Entezari-Maleki
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-21
> **comment**: 13 pages, 6 figures
- **标题**: None
- **领域**: 计算工程、金融和科学,计算语言学,机器学习,统计金融
- **Abstract**: Financial Sentiment Analysis (FSA) traditionally relies on human-annotated sentiment labels to infer investor sentiment and forecast market movements. However, inferring the potential market impact of words based on human-perceived intentions is inherently challenging. We hypothesize that the historical market reactions to words, offer a more reliable indicator of their potential impact on markets than subjective sentiment interpretations by human annotators. To test this hypothesis, a market-derived labeling approach is proposed to assign tweet labels based on ensuing short-term price trends, enabling the language model to capture the relationship between textual signals and market dynamics directly. A domain-specific language model was fine-tuned on these labels, achieving up to an 11% improvement in short-term trend prediction accuracy over traditional sentiment-based benchmarks. Moreover, by incorporating market and temporal context through prompt-tuning, the proposed context-aware language model demonstrated an accuracy of 89.6% on a curated dataset of 227 impactful Bitcoin-related news events with significant market impacts. Aggregating daily tweet predictions into trading signals, our method outperformed traditional fusion models (which combine sentiment-based and price-based predictions). It challenged the assumption that sentiment-based signals are inferior to price-based predictions in forecasting market movements. Backtesting these signals across three distinct market regimes yielded robust Sharpe ratios of up to 5.07 in trending markets and 3.73 in neutral markets. Our findings demonstrate that language models can serve as effective short-term market predictors. This paradigm shift underscores the untapped capabilities of language models in financial decision-making and opens new avenues for market prediction applications.

### A Neural Operator-Based Emulator for Regional Shallow Water Dynamics 
[[arxiv](https://arxiv.org/abs/2502.14782)] [[cool](https://papers.cool/arxiv/2502.14782)] [[pdf](https://arxiv.org/pdf/2502.14782)]
> **Authors**: Peter Rivera-Casillas,Sourav Dutta,Shukai Cai,Mark Loveland,Kamaljyoti Nath,Khemraj Shukla,Corey Trahan,Jonghyun Lee,Matthew Farthing,Clint Dawson
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算工程、金融和科学,机器学习,计算物理,地球物理学
- **Abstract**: Coastal regions are particularly vulnerable to the impacts of rising sea levels and extreme weather events. Accurate real-time forecasting of hydrodynamic processes in these areas is essential for infrastructure planning and climate adaptation. In this study, we present the Multiple-Input Temporal Operator Network (MITONet), a novel autoregressive neural emulator that employs dimensionality reduction to efficiently approximate high-dimensional numerical solvers for complex, nonlinear problems that are governed by time-dependent, parameterized partial differential equations. Although MITONet is applicable to a wide range of problems, we showcase its capabilities by forecasting regional tide-driven dynamics described by the two-dimensional shallow-water equations, while incorporating initial conditions, boundary conditions, and a varying domain parameter. We demonstrate MITONet's performance in a real-world application, highlighting its ability to make accurate predictions by extrapolating both in time and parametric space.

## 计算语言学(cs.CL:Computation and Language)

### Unveiling Attractor Cycles in Large Language Models: A Dynamical Systems View of Successive Paraphrasing 
[[arxiv](https://arxiv.org/abs/2502.15208)] [[cool](https://papers.cool/arxiv/2502.15208)] [[pdf](https://arxiv.org/pdf/2502.15208)]
> **Authors**: Zhilin Wang,Yafu Li,Jianhao Yan,Yu Cheng,Yue Zhang
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 9 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Dynamical systems theory provides a framework for analyzing iterative processes and evolution over time. Within such systems, repetitive transformations can lead to stable configurations, known as attractors, including fixed points and limit cycles. Applying this perspective to large language models (LLMs), which iteratively map input text to output text, provides a principled approach to characterizing long-term behaviors. Successive paraphrasing serves as a compelling testbed for exploring such dynamics, as paraphrases re-express the same underlying meaning with linguistic variation. Although LLMs are expected to explore a diverse set of paraphrases in the text space, our study reveals that successive paraphrasing converges to stable periodic states, such as 2-period attractor cycles, limiting linguistic diversity. This phenomenon is attributed to the self-reinforcing nature of LLMs, as they iteratively favour and amplify certain textual forms over others. This pattern persists with increasing generation randomness or alternating prompts and LLMs. These findings underscore inherent constraints in LLM generative capability, while offering a novel dynamical systems perspective for studying their expressive potential.

### TETRIS: Optimal Draft Token Selection for Batch Speculative Decoding 
[[arxiv](https://arxiv.org/abs/2502.15197)] [[cool](https://papers.cool/arxiv/2502.15197)] [[pdf](https://arxiv.org/pdf/2502.15197)]
> **Authors**: Zhaoxuan Wu,Zijian Zhou,Arun Verma,Alok Prakash,Daniela Rus,Bryan Kian Hsiang Low
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 15 pages, 10 figures, 5 tables
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: We propose TETRIS, a novel method that optimizes the total throughput of batch speculative decoding in multi-request settings. Unlike existing methods that optimize for a single request or a group of requests as a whole, TETRIS actively selects the most promising draft tokens (for every request in a batch) to be accepted when verified in parallel, resulting in fewer rejected tokens and hence less wasted computing resources. Such an effective resource utilization to achieve fast inference in large language models (LLMs) is especially important to service providers with limited inference capacity. Compared to baseline speculative decoding, TETRIS yields a consistently higher acceptance rate and more effective utilization of the limited inference capacity. We show theoretically and empirically that TETRIS outperforms baseline speculative decoding and existing methods that dynamically select draft tokens, leading to a more efficient batch inference in LLMs.

### Scale-Free Graph-Language Models 
[[arxiv](https://arxiv.org/abs/2502.15189)] [[cool](https://papers.cool/arxiv/2502.15189)] [[pdf](https://arxiv.org/pdf/2502.15189)]
> **Authors**: Jianglin Lu,Yixuan Liu,Yitian Zhang,Yun Fu
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Graph-language models (GLMs) have demonstrated great potential in graph-based semi-supervised learning. A typical GLM consists of two key stages: graph generation and text embedding, which are usually implemented by inferring a latent graph and finetuning a language model (LM), respectively. However, the former often relies on artificial assumptions about the underlying edge distribution, while the latter requires extensive data annotations. To tackle these challenges, this paper introduces a novel GLM that integrates graph generation and text embedding within a unified framework. Specifically, for graph generation, we leverage an inherent characteristic of real edge distribution--the scale-free property--as a structural prior. We unexpectedly find that this natural property can be effectively approximated by a simple k-nearest neighbor (KNN) graph. For text embedding, we develop a graph-based pseudo-labeler that utilizes scale-free graphs to provide complementary supervision for improved LM finetuning. Extensive experiments on representative datasets validate our findings on the scale-free structural approximation of KNN graphs and demonstrate the effectiveness of integrating graph generation and text embedding with a real structural prior. Our code is available at https://github.com/Jianglin954/SFGL.

### mStyleDistance: Multilingual Style Embeddings and their Evaluation 
[[arxiv](https://arxiv.org/abs/2502.15168)] [[cool](https://papers.cool/arxiv/2502.15168)] [[pdf](https://arxiv.org/pdf/2502.15168)]
> **Authors**: Justin Qiu,Jiacheng Zhu,Ajay Patel,Marianna Apidianaki,Chris Callison-Burch
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: arXiv admin note: substantial text overlap with arXiv:2410.12757
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Style embeddings are useful for stylistic analysis and style transfer; however, only English style embeddings have been made available. We introduce Multilingual StyleDistance (mStyleDistance), a multilingual style embedding model trained using synthetic data and contrastive learning. We train the model on data from nine languages and create a multilingual STEL-or-Content benchmark (Wegmann et al., 2022) that serves to assess the embeddings' quality. We also employ our embeddings in an authorship verification task involving different languages. Our results show that mStyleDistance embeddings outperform existing models on these multilingual style benchmarks and generalize well to unseen features and languages. We make our model publicly available at https://huggingface.co/StyleDistance/mstyledistance .

### Extreme Speech Classification in the Era of LLMs: Exploring Open-Source and Proprietary Models 
[[arxiv](https://arxiv.org/abs/2502.15155)] [[cool](https://papers.cool/arxiv/2502.15155)] [[pdf](https://arxiv.org/pdf/2502.15155)]
> **Authors**: Sarthak Mahajan,Nimmi Rangaswamy
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: Accepted to 7th International Conference on information systems and management science (ISMS), 2024
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: In recent years, widespread internet adoption and the growth in userbase of various social media platforms have led to an increase in the proliferation of extreme speech online. While traditional language models have demonstrated proficiency in distinguishing between neutral text and non-neutral text (i.e. extreme speech), categorizing the diverse types of extreme speech presents significant challenges. The task of extreme speech classification is particularly nuanced, as it requires a deep understanding of socio-cultural contexts to accurately interpret the intent of the language used by the speaker. Even human annotators often disagree on the appropriate classification of such content, emphasizing the complex and subjective nature of this task. The use of human moderators also presents a scaling issue, necessitating the need for automated systems for extreme speech classification. The recent launch of ChatGPT has drawn global attention to the potential applications of Large Language Models (LLMs) across a diverse variety of tasks. Trained on vast and diverse corpora, and demonstrating the ability to effectively capture and encode contextual information, LLMs emerge as highly promising tools for tackling this specific task of extreme speech classification. In this paper, we leverage the Indian subset of the extreme speech dataset from Maronikolakis et al. (2022) to develop an effective classification framework using LLMs. We evaluate open-source Llama models against closed-source OpenAI models, finding that while pre-trained LLMs show moderate efficacy, fine-tuning with domain-specific data significantly enhances performance, highlighting their adaptability to linguistic and contextual nuances. Although GPT-based models outperform Llama models in zero-shot settings, the performance gap disappears after fine-tuning.

### Investigating the Adaptive Robustness with Knowledge Conflicts in LLM-based Multi-Agent Systems 
[[arxiv](https://arxiv.org/abs/2502.15153)] [[cool](https://papers.cool/arxiv/2502.15153)] [[pdf](https://arxiv.org/pdf/2502.15153)]
> **Authors**: Tianjie Ju,Bowen Wang,Hao Fei,Mong-Li Lee,Wynne Hsu,Yun Li,Qianren Wang,Pengzhou Cheng,Zongru Wu,Zhuosheng Zhang,Gongshen Liu
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: Working in progress
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Recent advances in Large Language Models (LLMs) have upgraded them from sophisticated text generators to autonomous agents capable of corporation and tool use in multi-agent systems (MASs). However, the robustness of these LLM-based MASs, especially under knowledge conflicts, remains unclear. In this paper, we design four comprehensive metrics to investigate the robustness of MASs when facing mild or task-critical knowledge conflicts. We first analyze mild knowledge conflicts introduced by heterogeneous agents and find that they do not harm system robustness but instead improve collaborative decision-making. Next, we investigate task-critical knowledge conflicts by synthesizing knowledge conflicts and embedding them into one of the agents. Our results show that these conflicts have surprisingly little to no impact on MAS robustness. Furthermore, we observe that MASs demonstrate certain self-repairing capabilities by reducing their reliance on knowledge conflicts and adopting alternative solution paths to maintain stability. Finally, we conduct ablation studies on the knowledge conflict number, agent number, and interaction rounds, finding that the self-repairing capability of MASs has intrinsic limits, and all findings hold consistently across various factors. Our code is publicly available at https://github.com/wbw625/MultiAgentRobustness.

### Latent Factor Models Meets Instructions:Goal-conditioned Latent Factor Discovery without Task Supervision 
[[arxiv](https://arxiv.org/abs/2502.15147)] [[cool](https://papers.cool/arxiv/2502.15147)] [[pdf](https://arxiv.org/pdf/2502.15147)]
> **Authors**: Zhouhang Xie,Tushar Khot,Bhavana Dalvi Mishra,Harshit Surana,Julian McAuley,Peter Clark,Bodhisattwa Prasad Majumder
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: NAACL 2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Instruction-following LLMs have recently allowed systems to discover hidden concepts from a collection of unstructured documents based on a natural language description of the purpose of the discovery (i.e., goal). Still, the quality of the discovered concepts remains mixed, as it depends heavily on LLM's reasoning ability and drops when the data is noisy or beyond LLM's knowledge. We present Instruct-LF, a goal-oriented latent factor discovery system that integrates LLM's instruction-following ability with statistical models to handle large, noisy datasets where LLM reasoning alone falls short. Instruct-LF uses LLMs to propose fine-grained, goal-related properties from documents, estimates their presence across the dataset, and applies gradient-based optimization to uncover hidden factors, where each factor is represented by a cluster of co-occurring properties. We evaluate latent factors produced by Instruct-LF on movie recommendation, text-world navigation, and legal document categorization tasks. These interpretable representations improve downstream task performance by 5-52% than the best baselines and were preferred 1.8 times as often as the best alternative, on average, in human evaluation.

### Do LLMs Make Mistakes Like Students? Exploring Natural Alignment between Language Models and Human Error Patterns 
[[arxiv](https://arxiv.org/abs/2502.15140)] [[cool](https://papers.cool/arxiv/2502.15140)] [[pdf](https://arxiv.org/pdf/2502.15140)]
> **Authors**: Naiming Liu,Shashank Sonkar,Richard G. Baraniuk
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人机交互
- **Abstract**: Large Language Models (LLMs) have demonstrated remarkable capabilities in various educational tasks, yet their alignment with human learning patterns, particularly in predicting which incorrect options students are most likely to select in multiple-choice questions (MCQs), remains underexplored. Our work investigates the relationship between LLM generation likelihood and student response distributions in MCQs with a specific focus on distractor selections. We collect a comprehensive dataset of MCQs with real-world student response distributions to explore two fundamental research questions: (1). RQ1 - Do the distractors that students more frequently select correspond to those that LLMs assign higher generation likelihood to? (2). RQ2 - When an LLM selects a incorrect choice, does it choose the same distractor that most students pick? Our experiments reveals moderate correlations between LLM-assigned probabilities and student selection patterns for distractors in MCQs. Additionally, when LLMs make mistakes, they are more likley to select the same incorrect answers that commonly mislead students, which is a pattern consistent across both small and large language models. Our work provides empirical evidence that despite LLMs' strong performance on generating educational content, there remains a gap between LLM's underlying reasoning process and human cognitive processes in identifying confusing distractors. Our findings also have significant implications for educational assessment development. The smaller language models could be efficiently utilized for automated distractor generation as they demonstrate similar patterns in identifying confusing answer choices as larger language models. This observed alignment between LLMs and student misconception patterns opens new opportunities for generating high-quality distractors that complement traditional human-designed distractors.

### Chain-of-Rank: Enhancing Large Language Models for Domain-Specific RAG in Edge Device 
[[arxiv](https://arxiv.org/abs/2502.15134)] [[cool](https://papers.cool/arxiv/2502.15134)] [[pdf](https://arxiv.org/pdf/2502.15134)]
> **Authors**: Juntae Lee,Jihwan Bang,Seunghan Yang,Kyuhong Shim,Simyung Chang
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: NAACL 2025 (Findings)
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Retrieval-augmented generation (RAG) with large language models (LLMs) is especially valuable in specialized domains, where precision is critical. To more specialize the LLMs into a target domain, domain-specific RAG has recently been developed by allowing the LLM to access the target domain early via finetuning. The domain-specific RAG makes more sense in resource-constrained environments like edge devices, as they should perform a specific task (e.g. personalization) reliably using only small-scale LLMs. While the domain-specific RAG is well-aligned with edge devices in this respect, it often relies on widely-used reasoning techniques like chain-of-thought (CoT). The reasoning step is useful to understand the given external knowledge, and yet it is computationally expensive and difficult for small-scale LLMs to learn it. Tackling this, we propose the Chain of Rank (CoR) which shifts the focus from intricate lengthy reasoning to simple ranking of the reliability of input external documents. Then, CoR reduces computational complexity while maintaining high accuracy, making it particularly suited for resource-constrained environments. We attain the state-of-the-art (SOTA) results in benchmarks, and analyze its efficacy.

### CoT-ICL Lab: A Petri Dish for Studying Chain-of-Thought Learning from In-Context Demonstrations 
[[arxiv](https://arxiv.org/abs/2502.15132)] [[cool](https://papers.cool/arxiv/2502.15132)] [[pdf](https://arxiv.org/pdf/2502.15132)]
> **Authors**: Vignesh Kothapalli,Hamed Firooz,Maziar Sanjabi
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 23 pages, 27 figures, 3 tables, code at https://github.com/kvignesh1420/cot-icl-lab
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: We introduce CoT-ICL Lab, a framework and methodology to generate synthetic tokenized datasets and systematically study chain-of-thought (CoT) in-context learning (ICL) in language models. CoT-ICL Lab allows fine grained control over the complexity of in-context examples by decoupling (1) the causal structure involved in chain token generation from (2) the underlying token processing functions. We train decoder-only transformers (up to 700M parameters) on these datasets and show that CoT accelerates the accuracy transition to higher values across model sizes. In particular, we find that model depth is crucial for leveraging CoT with limited in-context examples, while more examples help shallow models match deeper model performance. Additionally, limiting the diversity of token processing functions throughout training improves causal structure learning via ICL. We also interpret these transitions by analyzing transformer embeddings and attention maps. Overall, CoT-ICL Lab serves as a simple yet powerful testbed for theoretical and empirical insights into ICL and CoT in language models.

### Unveiling Reasoning Thresholds in Language Models: Scaling, Fine-Tuning, and Interpretability through Attention Maps 
[[arxiv](https://arxiv.org/abs/2502.15120)] [[cool](https://papers.cool/arxiv/2502.15120)] [[pdf](https://arxiv.org/pdf/2502.15120)]
> **Authors**: Yen-Che Hsiao,Abhishek Dutta
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: This study investigates the in-context learning capabilities of various decoder-only transformer-based language models with different model sizes and training data, including GPT2, SmolLM2, OpenELM, TinyLlama, Stable LM, and Gemma 2. We identify a critical parameter threshold (~1.6 billion), beyond which reasoning performance improves significantly in tasks such as commonsense reasoning in multiple-choice question answering and deductive reasoning. Specifically, models above this threshold achieve better success rates in chain-of-thought (CoT) prompting for deductive reasoning tasks, especially those requiring longer reasoning chains, such as proof by contradiction and disjunction elimination. To address limitations in sub-threshold models, we demonstrate that fine-tuning with task-specific exemplars substantially enhances reasoning performance, enabling accurate CoT generation even without additional exemplars in the prompt for tasks with shorter reasoning chains. Finally, our analysis of attention maps reveals that models capable of generating correct CoTs exhibit higher token-level attention scores on subsequent correct tokens and the correct parts of speech, providing interpretability insights into reasoning processes. These findings collectively advance understanding of reasoning capabilities in decoder-only transformer-based models. The code is available at: https://github.com/AnnonymousForPapers/CoT_Reasoning_Test.

### Social Genome: Grounded Social Reasoning Abilities of Multimodal Models 
[[arxiv](https://arxiv.org/abs/2502.15109)] [[cool](https://papers.cool/arxiv/2502.15109)] [[pdf](https://arxiv.org/pdf/2502.15109)]
> **Authors**: Leena Mathur,Marian Qian,Paul Pu Liang,Louis-Philippe Morency
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: Under Review, 22 pages
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Social reasoning abilities are crucial for AI systems to effectively interpret and respond to multimodal human communication and interaction within social contexts. We introduce Social Genome, the first benchmark for fine-grained, grounded social reasoning abilities of multimodal models. Social Genome contains 272 videos of interactions and 1,486 human-annotated reasoning traces related to inferences about these interactions. These traces contain 5,777 reasoning steps that reference evidence from visual cues, verbal cues, vocal cues, and external knowledge (contextual knowledge external to videos). Social Genome is also the first modeling challenge to study external knowledge in social reasoning. Social Genome computes metrics to holistically evaluate semantic and structural qualities of model-generated social reasoning traces. We demonstrate the utility of Social Genome through experiments with state-of-the-art models, identifying performance gaps and opportunities for future research to improve the grounded social reasoning abilities of multimodal models.

### LUME: LLM Unlearning with Multitask Evaluations 
[[arxiv](https://arxiv.org/abs/2502.15097)] [[cool](https://papers.cool/arxiv/2502.15097)] [[pdf](https://arxiv.org/pdf/2502.15097)]
> **Authors**: Anil Ramakrishna,Yixin Wan,Xiaomeng Jin,Kai-Wei Chang,Zhiqi Bu,Bhanukiran Vinzamuri,Volkan Cevher,Mingyi Hong,Rahul Gupta
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Unlearning aims to remove copyrighted, sensitive, or private content from large language models (LLMs) without a full retraining. In this work, we develop a multi-task unlearning benchmark (LUME) which features three tasks: (1) unlearn synthetically generated creative short novels, (2) unlearn synthetic biographies with sensitive information, and (3) unlearn a collection of public biographies. We further release two fine-tuned LLMs of 1B and 7B parameter sizes as the target models. We conduct detailed evaluations of several recently proposed unlearning algorithms and present results on carefully crafted metrics to understand their behavior and limitations.

### Judging It, Washing It: Scoring and Greenwashing Corporate Climate Disclosures using Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.15094)] [[cool](https://papers.cool/arxiv/2502.15094)] [[pdf](https://arxiv.org/pdf/2502.15094)]
> **Authors**: Marianne Chuang,Gabriel Chuang,Cheryl Chuang,John Chuang
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 16 pages, 12 figures
- **标题**: None
- **领域**: 计算语言学,应用领域
- **Abstract**: We study the use of large language models (LLMs) to both evaluate and greenwash corporate climate disclosures. First, we investigate the use of the LLM-as-a-Judge (LLMJ) methodology for scoring company-submitted reports on emissions reduction targets and progress. Second, we probe the behavior of an LLM when it is prompted to greenwash a response subject to accuracy and length constraints. Finally, we test the robustness of the LLMJ methodology against responses that may be greenwashed using an LLM. We find that two LLMJ scoring systems, numerical rating and pairwise comparison, are effective in distinguishing high-performing companies from others, with the pairwise comparison system showing greater robustness against LLM-greenwashed responses.

### Optimizing Singular Spectrum for Large Language Model Compression 
[[arxiv](https://arxiv.org/abs/2502.15092)] [[cool](https://papers.cool/arxiv/2502.15092)] [[pdf](https://arxiv.org/pdf/2502.15092)]
> **Authors**: Dengjie Li,Tiancheng Shen,Yao Zhou,Baisong Yang,Zhongying Liu,Masheng Yang,Bernard Ghanem,Yibo Yang,Yujie Zhong,Ming-Hsuan Yang
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) have demonstrated remarkable capabilities, yet prohibitive parameter complexity often hinders their deployment. Existing singular value decomposition (SVD) based compression methods simply deem singular values as importance scores of decomposed components. However, this importance ordered by singular values does not necessarily correlate with the performance of a downstream task. In this work, we introduce SoCo (Singular spectrum optimization for large language model Compression), a novel compression framework that learns to rescale the decomposed components of SVD in a data-driven manner. Concretely, we employ a learnable diagonal matrix to assign importance scores for singular spectrum and develop a three-stage training process that progressively refines these scores from initial coarse compression to fine-grained sparsification-thereby striking an effective balance between aggressive model compression and performance preservation. Thanks to the learnable singular spectrum, SoCo adaptively prunes components according to the sparsified importance scores, rather than relying on the fixed order of singular values. More importantly, the remaining components with amplified importance scores can compensate for the loss of the pruned ones. Experimental evaluations across multiple LLMs and benchmarks demonstrate that SoCo surpasses the state-of-the-art methods in model compression.

### Analyze the Neurons, not the Embeddings: Understanding When and Where LLM Representations Align with Humans 
[[arxiv](https://arxiv.org/abs/2502.15090)] [[cool](https://papers.cool/arxiv/2502.15090)] [[pdf](https://arxiv.org/pdf/2502.15090)]
> **Authors**: Masha Fedzechkina,Eleonora Gualdoni,Sinead Williamson,Katherine Metcalf,Skyler Seto,Barry-John Theobald
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Modern large language models (LLMs) achieve impressive performance on some tasks, while exhibiting distinctly non-human-like behaviors on others. This raises the question of how well the LLM's learned representations align with human representations. In this work, we introduce a novel approach to the study of representation alignment: we adopt a method from research on activation steering to identify neurons responsible for specific concepts (e.g., 'cat') and then analyze the corresponding activation patterns. Our findings reveal that LLM representations closely align with human representations inferred from behavioral data. Notably, this alignment surpasses that of word embeddings, which have been center stage in prior work on human and model alignment. Additionally, our approach enables a more granular view of how LLMs represent concepts. Specifically, we show that LLMs organize concepts in a way that reflects hierarchical relationships interpretable to humans (e.g., 'animal'-'dog').

### Is Safety Standard Same for Everyone? User-Specific Safety Evaluation of Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.15086)] [[cool](https://papers.cool/arxiv/2502.15086)] [[pdf](https://arxiv.org/pdf/2502.15086)]
> **Authors**: Yeonjun In,Wonjoong Kim,Kanghoon Yoon,Sungchul Kim,Mehrab Tanjim,Kibum Kim,Chanyoung Park
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: Under review
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: As the use of large language model (LLM) agents continues to grow, their safety vulnerabilities have become increasingly evident. Extensive benchmarks evaluate various aspects of LLM safety by defining the safety relying heavily on general standards, overlooking user-specific standards. However, safety standards for LLM may vary based on a user-specific profiles rather than being universally consistent across all users. This raises a critical research question: Do LLM agents act safely when considering user-specific safety standards? Despite its importance for safe LLM use, no benchmark datasets currently exist to evaluate the user-specific safety of LLMs. To address this gap, we introduce U-SAFEBENCH, the first benchmark designed to assess user-specific aspect of LLM safety. Our evaluation of 18 widely used LLMs reveals current LLMs fail to act safely when considering user-specific safety standards, marking a new discovery in this field. To address this vulnerability, we propose a simple remedy based on chain-of-thought, demonstrating its effectiveness in improving user-specific safety. Our benchmark and code are available at https://github.com/yeonjun-in/U-SafeBench.

### Rare Disease Differential Diagnosis with Large Language Models at Scale: From Abdominal Actinomycosis to Wilson's Disease 
[[arxiv](https://arxiv.org/abs/2502.15069)] [[cool](https://papers.cool/arxiv/2502.15069)] [[pdf](https://arxiv.org/pdf/2502.15069)]
> **Authors**: Elliot Schumacher,Dhruv Naik,Anitha Kannan
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large language models (LLMs) have demonstrated impressive capabilities in disease diagnosis. However, their effectiveness in identifying rarer diseases, which are inherently more challenging to diagnose, remains an open question. Rare disease performance is critical with the increasing use of LLMs in healthcare settings. This is especially true if a primary care physician needs to make a rarer prognosis from only a patient conversation so that they can take the appropriate next step. To that end, several clinical decision support systems are designed to support providers in rare disease identification. Yet their utility is limited due to their lack of knowledge of common disorders and difficulty of use. In this paper, we propose RareScale to combine the knowledge LLMs with expert systems. We use jointly use an expert system and LLM to simulate rare disease chats. This data is used to train a rare disease candidate predictor model. Candidates from this smaller model are then used as additional inputs to black-box LLM to make the final differential diagnosis. Thus, RareScale allows for a balance between rare and common diagnoses. We present results on over 575 rare diseases, beginning with Abdominal Actinomycosis and ending with Wilson's Disease. Our approach significantly improves the baseline performance of black-box LLMs by over 17% in Top-5 accuracy. We also find that our candidate generation performance is high (e.g. 88.8% on gpt-4o generated chats).

### Reducing Hallucinations of Medical Multimodal Large Language Models with Visual Retrieval-Augmented Generation 
[[arxiv](https://arxiv.org/abs/2502.15040)] [[cool](https://papers.cool/arxiv/2502.15040)] [[pdf](https://arxiv.org/pdf/2502.15040)]
> **Authors**: Yun-Wei Chu,Kai Zhang,Christopher Malon,Martin Renqiang Min
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: GenAI4Health - AAAI '25
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Multimodal Large Language Models (MLLMs) have shown impressive performance in vision and text tasks. However, hallucination remains a major challenge, especially in fields like healthcare where details are critical. In this work, we show how MLLMs may be enhanced to support Visual RAG (V-RAG), a retrieval-augmented generation framework that incorporates both text and visual data from retrieved images. On the MIMIC-CXR chest X-ray report generation and Multicare medical image caption generation datasets, we show that Visual RAG improves the accuracy of entity probing, which asks whether a medical entities is grounded by an image. We show that the improvements extend both to frequent and rare entities, the latter of which may have less positive training data. Downstream, we apply V-RAG with entity probing to correct hallucinations and generate more clinically accurate X-ray reports, obtaining a higher RadGraph-F1 score.

### InterFeedback: Unveiling Interactive Intelligence of Large Multimodal Models via Human Feedback 
[[arxiv](https://arxiv.org/abs/2502.15027)] [[cool](https://papers.cool/arxiv/2502.15027)] [[pdf](https://arxiv.org/pdf/2502.15027)]
> **Authors**: Henry Hengyuan Zhao,Wenqi Pei,Yifei Tao,Haiyang Mei,Mike Zheng Shou
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 18 pages, 10 figures
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别,人机交互
- **Abstract**: Existing benchmarks do not test Large Multimodal Models (LMMs) on their interactive intelligence with human users which is vital for developing general-purpose AI assistants. We design InterFeedback, an interactive framework, which can be applied to any LMM and dataset to assess this ability autonomously. On top of this, we introduce InterFeedback-Bench which evaluates interactive intelligence using two representative datasets, MMMU-Pro and MathVerse, to test 10 different open-source LMMs. Additionally, we present InterFeedback-Human, a newly collected dataset of 120 cases designed for manually testing interactive performance in leading models such as OpenAI-o1 and Claude-3.5-Sonnet. Our evaluation results show that even state-of-the-art LMM (like OpenAI-o1) can correct their results through human feedback less than 50%. Our findings point to the need for methods that can enhance the LMMs' capability to interpret and benefit from feedback.

### A Meta-Evaluation of Style and Attribute Transfer Metrics 
[[arxiv](https://arxiv.org/abs/2502.15022)] [[cool](https://papers.cool/arxiv/2502.15022)] [[pdf](https://arxiv.org/pdf/2502.15022)]
> **Authors**: Amalie Brogaard Pauli,Isabelle Augenstein,Ira Assent
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: LLMs make it easy to rewrite text in any style, be it more polite, persuasive, or more positive. We present a large-scale study of evaluation metrics for style and attribute transfer with a focus on content preservation; meaning content not attributed to the style shift is preserved. The de facto evaluation approach uses lexical or semantic similarity metrics often between source sentences and rewrites. While these metrics are not designed to distinguish between style or content differences, empirical meta-evaluation shows a reasonable correlation to human judgment. In fact, recent works find that LLMs prompted as evaluators are only comparable to semantic similarity metrics, even though intuitively, the LLM approach should better fit the task. To investigate this discrepancy, we benchmark 8 metrics for evaluating content preservation on existing datasets and additionally construct a new test set that better aligns with the meta-evaluation aim. Indeed, we then find that the empirical conclusion aligns with the intuition: content preservation metrics for style/attribute transfer must be conditional on the style shift. To support this, we propose a new efficient zero-shot evaluation method using the likelihood of the next token. We hope our meta-evaluation can foster more research on evaluating content preservation metrics, and also to ensure fair evaluation of methods for conducting style transfer.

### Using tournaments to calculate AUROC for zero-shot classification with LLMs 
[[arxiv](https://arxiv.org/abs/2502.15018)] [[cool](https://papers.cool/arxiv/2502.15018)] [[pdf](https://arxiv.org/pdf/2502.15018)]
> **Authors**: Wonjin Yoon,Ian Bulovic,Timothy A. Miller
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models perform surprisingly well on many zero-shot classification tasks, but are difficult to fairly compare to supervised classifiers due to the lack of a modifiable decision boundary. In this work, we propose and evaluate a method that converts binary classification tasks into pairwise comparison tasks, obtaining relative rankings from LLMs. Repeated pairwise comparisons can be used to score instances using the Elo rating system (used in chess and other competitions), inducing a confidence ordering over instances in a dataset. We evaluate scheduling algorithms for their ability to minimize comparisons, and show that our proposed algorithm leads to improved classification performance, while also providing more information than traditional zero-shot classification.

### Obliviate: Efficient Unmemorization for Protecting Intellectual Property in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.15010)] [[cool](https://papers.cool/arxiv/2502.15010)] [[pdf](https://arxiv.org/pdf/2502.15010)]
> **Authors**: Mark Russinovich,Ahmed Salem
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,密码学和安全,机器学习
- **Abstract**: Recent copyright agreements between AI companies and content creators have highlighted the need for precise control over language models' ability to reproduce copyrighted content. While existing approaches rely on either complete concept removal through unlearning or simple output filtering, we propose Obliviate, a novel post-training technique that selectively prevents verbatim reproduction of specific text while preserving semantic understanding. Obliviate operates by selecting tokens within memorized sequences and modifying the model's probability distribution to prevent exact reproduction while maintaining contextual understanding. We evaluate Obliviate on multiple large language models (LLaMA-3.1 8B, LLaMA-3.1-instruct 8B, Qwen-2.5-7B, and Yi-1.5 6B) across both synthetic memorization tasks and organic copyright content. Our results demonstrate that Obliviate achieves orders of magnitude reduction, e.g., 100x, in verbatim memorization while maintaining model performance within 1% of baseline on standard benchmarks (HellaSwag, MMLU, TruthfulQA, and Winogrande). This makes Obliviate particularly suitable for practical deployment scenarios where companies need to efficiently address copyright concerns in pretrained models without compromising their general capabilities.

### Contextualizing Search Queries In-Context Learning for Conversational Rewriting with LLMs 
[[arxiv](https://arxiv.org/abs/2502.15009)] [[cool](https://papers.cool/arxiv/2502.15009)] [[pdf](https://arxiv.org/pdf/2502.15009)]
> **Authors**: Raymond Wilson,Chase Carter,Cole Graham
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Conversational query rewriting is crucial for effective conversational search, yet traditional supervised methods require substantial labeled data, which is scarce in low-resource settings. This paper introduces Prompt-Guided In-Context Learning, a novel approach that leverages the in-context learning capabilities of Large Language Models (LLMs) for few-shot conversational query rewriting. Our method employs carefully designed prompts, incorporating task descriptions, input/output format specifications, and a small set of illustrative examples, to guide pre-trained LLMs to generate context-independent queries without explicit fine-tuning. Extensive experiments on benchmark datasets, TREC and Taskmaster-1, demonstrate that our approach significantly outperforms strong baselines, including supervised models and contrastive co-training methods, across various evaluation metrics such as BLEU, ROUGE-L, Success Rate, and MRR. Ablation studies confirm the importance of in-context examples, and human evaluations further validate the superior fluency, relevance, and context utilization of our generated rewrites. The results highlight the potential of prompt-guided in-context learning as an efficient and effective paradigm for low-resource conversational query rewriting, reducing the reliance on extensive labeled data and complex training procedures.

### LLM-Microscope: Uncovering the Hidden Role of Punctuation in Context Memory of Transformers 
[[arxiv](https://arxiv.org/abs/2502.15007)] [[cool](https://papers.cool/arxiv/2502.15007)] [[pdf](https://arxiv.org/pdf/2502.15007)]
> **Authors**: Anton Razzhigaev,Matvey Mikhalchuk,Temurbek Rahmatullaev,Elizaveta Goncharova,Polina Druzhinina,Ivan Oseledets,Andrey Kuznetsov
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: accepted to NAACL 2025
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: We introduce methods to quantify how Large Language Models (LLMs) encode and store contextual information, revealing that tokens often seen as minor (e.g., determiners, punctuation) carry surprisingly high context. Notably, removing these tokens -- especially stopwords, articles, and commas -- consistently degrades performance on MMLU and BABILong-4k, even if removing only irrelevant tokens. Our analysis also shows a strong correlation between contextualization and linearity, where linearity measures how closely the transformation from one layer's embeddings to the next can be approximated by a single linear mapping. These findings underscore the hidden importance of filler tokens in maintaining context. For further exploration, we present LLM-Microscope, an open-source toolkit that assesses token-level nonlinearity, evaluates contextual memory, visualizes intermediate layer contributions (via an adapted Logit Lens), and measures the intrinsic dimensionality of representations. This toolkit illuminates how seemingly trivial tokens can be critical for long-range understanding.

### Beyond No: Quantifying AI Over-Refusal and Emotional Attachment Boundaries 
[[arxiv](https://arxiv.org/abs/2502.14975)] [[cool](https://papers.cool/arxiv/2502.14975)] [[pdf](https://arxiv.org/pdf/2502.14975)]
> **Authors**: David Noever,Grant Rosario
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: We present an open-source benchmark and evaluation framework for assessing emotional boundary handling in Large Language Models (LLMs). Using a dataset of 1156 prompts across six languages, we evaluated three leading LLMs (GPT-4o, Claude-3.5 Sonnet, and Mistral-large) on their ability to maintain appropriate emotional boundaries through pattern-matched response analysis. Our framework quantifies responses across seven key patterns: direct refusal, apology, explanation, deflection, acknowledgment, boundary setting, and emotional awareness. Results demonstrate significant variation in boundary-handling approaches, with Claude-3.5 achieving the highest overall score (8.69/10) and producing longer, more nuanced responses (86.51 words on average). We identified a substantial performance gap between English (average score 25.62) and non-English interactions (< 0.22), with English responses showing markedly higher refusal rates (43.20% vs. < 1% for non-English). Pattern analysis revealed model-specific strategies, such as Mistral's preference for deflection (4.2%) and consistently low empathy scores across all models (< 0.06). Limitations include potential oversimplification through pattern matching, lack of contextual understanding in response analysis, and binary classification of complex emotional responses. Future work should explore more nuanced scoring methods, expand language coverage, and investigate cultural variations in emotional boundary expectations. Our benchmark and methodology provide a foundation for systematic evaluation of LLM emotional intelligence and boundary-setting capabilities.

### Lost in Space: Optimizing Tokens for Grammar-Constrained Decoding 
[[arxiv](https://arxiv.org/abs/2502.14969)] [[cool](https://papers.cool/arxiv/2502.14969)] [[pdf](https://arxiv.org/pdf/2502.14969)]
> **Authors**: Sil Hamilton,David Mimno
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: General-purpose language models are trained to produce varied natural language outputs, but for some tasks like annotation or classification we need more specific output formats. LLM systems increasingly support structured output, sampling tokens according to a grammar, which enforces a format but which can also reduce performance. We ask whether there are systematic differences between grammars that appear semantically similar to humans. To answer this question, we test four popular model families with five token formats on four NLP benchmarks. All models perform most accurately when instructed to classify with real numbers. Performance also improves by 5%-10% when models are instructed to return tokens incorporating leading whitespace, which we find can help models avoid structural deficiencies in subword token representations. Format-based differences are largest for smaller models that are often used for local laptop-scale inference. We present best practices for researchers using language models as zero-shot classifiers with structured output.

### Learning to Retrieve and Reason on Knowledge Graph through Active Self-Reflection 
[[arxiv](https://arxiv.org/abs/2502.14932)] [[cool](https://papers.cool/arxiv/2502.14932)] [[pdf](https://arxiv.org/pdf/2502.14932)]
> **Authors**: Han Zhang,Langshi Zhou,Hanfang Yang
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Extensive research has investigated the integration of large language models (LLMs) with knowledge graphs to enhance the reasoning process. However, understanding how models perform reasoning utilizing structured graph knowledge remains underexplored. Most existing approaches rely on LLMs or retrievers to make binary judgments regarding the utilization of knowledge, which is too coarse. Meanwhile, there is still a lack of feedback mechanisms for reflection and correction throughout the entire reasoning path. This paper proposes an Active self-Reflection framework for knowledge Graph reasoning ARG, introducing for the first time an end-to-end training approach to achieve iterative reasoning grounded on structured graphs. Within the framework, the model leverages special tokens to \textit{actively} determine whether knowledge retrieval is necessary, performs \textit{reflective} critique based on the retrieved knowledge, and iteratively reasons over the knowledge graph. The reasoning paths generated by the model exhibit high interpretability, enabling deeper exploration of the model's understanding of structured knowledge. Ultimately, the proposed model achieves outstanding results compared to existing baselines in knowledge graph reasoning tasks.

### A Tale of Two Structures: Do LLMs Capture the Fractal Complexity of Language? 
[[arxiv](https://arxiv.org/abs/2502.14924)] [[cool](https://papers.cool/arxiv/2502.14924)] [[pdf](https://arxiv.org/pdf/2502.14924)]
> **Authors**: Ibrahim Alabdulmohsin,Andreas Steiner
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Language exhibits a fractal structure in its information-theoretic complexity (i.e. bits per token), with self-similarity across scales and long-range dependence (LRD). In this work, we investigate whether large language models (LLMs) can replicate such fractal characteristics and identify conditions-such as temperature setting and prompting method-under which they may fail. Moreover, we find that the fractal parameters observed in natural language are contained within a narrow range, whereas those of LLMs' output vary widely, suggesting that fractal parameters might prove helpful in detecting a non-trivial portion of LLM-generated texts. Notably, these findings, and many others reported in this work, are robust to the choice of the architecture; e.g. Gemini 1.0 Pro, Mistral-7B and Gemma-2B. We also release a dataset comprising of over 240,000 articles generated by various LLMs (both pretrained and instruction-tuned) with different decoding temperatures and prompting methods, along with their corresponding human-generated texts. We hope that this work highlights the complex interplay between fractal properties, prompting, and statistical mimicry in LLMs, offering insights for generating, evaluating and detecting synthetic texts.

### AI Thinking as a Meaning-Centered Framework: Reimagining Language Technologies Through Community Agency 
[[arxiv](https://arxiv.org/abs/2502.14923)] [[cool](https://papers.cool/arxiv/2502.14923)] [[pdf](https://arxiv.org/pdf/2502.14923)]
> **Authors**: Jose F Quesada
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-21
> **comment**: LT4All 2025.LanguageTechnologies for All - 2025. Advancing Humanism throughLanguageTechnologies. Paris (FR), UNESCO Headquarters, 24-26 February 2025
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: While language technologies have advanced significantly, current approaches fail to address the complex sociocultural dimensions of linguistic preservation. AI Thinking proposes a meaning-centered framework that would transform technological development from creating tools FOR communities to co-creating solutions WITH them. This approach recognizes that meaningful solutions emerge through the interplay of cultural understanding, community agency, and technological innovation. The proposal articulates a holistic methodology and a five-layer technological ecosystem where communities maintain control over their linguistic and cultural knowledge representation. This systematic integration of community needs, cultural preservation, and advanced capabilities could revolutionize how we approach linguistic diversity preservation in the digital age.

### SIFT: Grounding LLM Reasoning in Contexts via Stickers 
[[arxiv](https://arxiv.org/abs/2502.14922)] [[cool](https://papers.cool/arxiv/2502.14922)] [[pdf](https://arxiv.org/pdf/2502.14922)]
> **Authors**: Zihao Zeng,Xuyao Huang,Boxiu Li,Zhijie Deng
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: This paper identifies the misinterpretation of the context can be a significant issue during the reasoning process of large language models, spanning from smaller models like Llama3.2-3B-Instruct to cutting-edge ones like DeepSeek-R1. For example, in the phrase "10 dollars per kilo," LLMs might not recognize that "per" means "for each," leading to calculation errors. We introduce a novel, post-training approach called **Stick to the Facts (SIFT)** to tackle this. SIFT leverages increasing inference-time compute to ground LLM reasoning in contexts. At the core of SIFT lies the *Sticker*, which is generated by the model itself to explicitly emphasize the key information within the context. Given the curated Sticker, SIFT generates two predictions -- one from the original query and one from the query augmented with the Sticker. If they differ, the Sticker is sequentially refined via *forward* optimization (to better align the extracted facts with the query) and *inverse* generation (to conform with the model's inherent tendencies) for more faithful reasoning outcomes. Studies across diverse models (from 3B to 100B+) and benchmarks (e.g., GSM8K, MATH-500) reveal consistent performance improvements. Notably, SIFT improves the pass@1 accuracy of DeepSeek-R1 on AIME2024 from 78.33% to **85.67**%, establishing a new state-of-the-art in the open-source community. The code is available at https://github.com/zhijie-group/SIFT.

### The Canary's Echo: Auditing Privacy Risks of LLM-Generated Synthetic Text 
[[arxiv](https://arxiv.org/abs/2502.14921)] [[cool](https://papers.cool/arxiv/2502.14921)] [[pdf](https://arxiv.org/pdf/2502.14921)]
> **Authors**: Matthieu Meeus,Lukas Wutschitz,Santiago Zanella-Béguelin,Shruti Tople,Reza Shokri
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,密码学和安全,机器学习
- **Abstract**: How much information about training samples can be gleaned from synthetic data generated by Large Language Models (LLMs)? Overlooking the subtleties of information flow in synthetic data generation pipelines can lead to a false sense of privacy. In this paper, we design membership inference attacks (MIAs) that target data used to fine-tune pre-trained LLMs that are then used to synthesize data, particularly when the adversary does not have access to the fine-tuned model but only to the synthetic data. We show that such data-based MIAs do significantly better than a random guess, meaning that synthetic data leaks information about the training data. Further, we find that canaries crafted to maximize vulnerability to model-based MIAs are sub-optimal for privacy auditing when only synthetic data is released. Such out-of-distribution canaries have limited influence on the model's output when prompted to generate useful, in-distribution synthetic data, which drastically reduces their vulnerability. To tackle this problem, we leverage the mechanics of auto-regressive models to design canaries with an in-distribution prefix and a high-perplexity suffix that leave detectable traces in synthetic data. This enhances the power of data-based MIAs and provides a better assessment of the privacy risks of releasing synthetic data generated by LLMs.

### MKE-Coder: Multi-Axial Knowledge with Evidence Verification in ICD Coding for Chinese EMRs 
[[arxiv](https://arxiv.org/abs/2502.14916)] [[cool](https://papers.cool/arxiv/2502.14916)] [[pdf](https://arxiv.org/pdf/2502.14916)]
> **Authors**: Xinxin You,Xien Liu,Xue Yang,Ziyi Wang,Ji Wu
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: The task of automatically coding the International Classification of Diseases (ICD) in the medical field has been well-established and has received much attention. Automatic coding of the ICD in the medical field has been successful in English but faces challenges when dealing with Chinese electronic medical records (EMRs). The first issue lies in the difficulty of extracting disease code-related information from Chinese EMRs, primarily due to the concise writing style and specific internal structure of the EMRs. The second problem is that previous methods have failed to leverage the disease-based multi-axial knowledge and lack of association with the corresponding clinical evidence. This paper introduces a novel framework called MKE-Coder: Multi-axial Knowledge with Evidence verification in ICD coding for Chinese EMRs. Initially, we identify candidate codes for the diagnosis and categorize each of them into knowledge under four coding axes.Subsequently, we retrieve corresponding clinical evidence from the comprehensive content of EMRs and filter credible evidence through a scoring model. Finally, to ensure the validity of the candidate code, we propose an inference module based on the masked language modeling strategy. This module verifies that all the axis knowledge associated with the candidate code is supported by evidence and provides recommendations accordingly. To evaluate the performance of our framework, we conduct experiments using a large-scale Chinese EMR dataset collected from various hospitals. The experimental results demonstrate that MKE-Coder exhibits significant superiority in the task of automatic ICD coding based on Chinese EMRs. In the practical evaluation of our method within simulated real coding scenarios, it has been demonstrated that our approach significantly aids coders in enhancing both their coding accuracy and speed.

### OpenSearch-SQL: Enhancing Text-to-SQL with Dynamic Few-shot and Consistency Alignment 
[[arxiv](https://arxiv.org/abs/2502.14913)] [[cool](https://papers.cool/arxiv/2502.14913)] [[pdf](https://arxiv.org/pdf/2502.14913)]
> **Authors**: Xiangjin Xie,Guangwei Xu,Lingyan Zhao,Ruijie Guo
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-21
> **comment**: 15 pages
- **标题**: None
- **领域**: 计算语言学,人工智能,信息检索
- **Abstract**: Although multi-agent collaborative Large Language Models (LLMs) have achieved significant breakthroughs in the Text-to-SQL task, their performance is still constrained by various factors. These factors include the incompleteness of the framework, failure to follow instructions, and model hallucination problems. To address these problems, we propose OpenSearch-SQL, which divides the Text-to-SQL task into four main modules: Preprocessing, Extraction, Generation, and Refinement, along with an Alignment module based on a consistency alignment mechanism. This architecture aligns the inputs and outputs of agents through the Alignment module, reducing failures in instruction following and hallucination. Additionally, we designed an intermediate language called SQL-Like and optimized the structured CoT based on SQL-Like. Meanwhile, we developed a dynamic few-shot strategy in the form of self-taught Query-CoT-SQL. These methods have significantly improved the performance of LLMs in the Text-to-SQL task. In terms of model selection, we directly applied the base LLMs without any post-training, thereby simplifying the task chain and enhancing the framework's portability. Experimental results show that OpenSearch-SQL achieves an execution accuracy(EX) of 69.3% on the BIRD development set, 72.28% on the test set, and a reward-based validity efficiency score (R-VES) of 69.36%, with all three metrics ranking first at the time of submission. These results demonstrate the comprehensive advantages of the proposed method in both effectiveness and efficiency.

### Universal Semantic Embeddings of Chemical Elements for Enhanced Materials Inference and Discovery 
[[arxiv](https://arxiv.org/abs/2502.14912)] [[cool](https://papers.cool/arxiv/2502.14912)] [[pdf](https://arxiv.org/pdf/2502.14912)]
> **Authors**: Yunze Jia,Yuehui Xian,Yangyang Xu,Pengfei Dang,Xiangdong Ding,Jun Sun,Yumei Zhou,Dezhen Xue
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-21
> **comment**: 5 figures
- **标题**: None
- **领域**: 计算语言学,材料科学,机器学习
- **Abstract**: We present a framework for generating universal semantic embeddings of chemical elements to advance materials inference and discovery. This framework leverages ElementBERT, a domain-specific BERT-based natural language processing model trained on 1.29 million abstracts of alloy-related scientific papers, to capture latent knowledge and contextual relationships specific to alloys. These semantic embeddings serve as robust elemental descriptors, consistently outperforming traditional empirical descriptors with significant improvements across multiple downstream tasks. These include predicting mechanical and transformation properties, classifying phase structures, and optimizing materials properties via Bayesian optimization. Applications to titanium alloys, high-entropy alloys, and shape memory alloys demonstrate up to 23% gains in prediction accuracy. Our results show that ElementBERT surpasses general-purpose BERT variants by encoding specialized alloy knowledge. By bridging contextual insights from scientific literature with quantitative inference, our framework accelerates the discovery and optimization of advanced materials, with potential applications extending beyond alloys to other material classes.

### Batayan: A Filipino NLP benchmark for evaluating Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.14911)] [[cool](https://papers.cool/arxiv/2502.14911)] [[pdf](https://arxiv.org/pdf/2502.14911)]
> **Authors**: Jann Railey Montalan,Jimson Paulo Layacan,David Demitri Africa,Richell Isaiah Flores,Michael T. Lopez II,Theresa Denise Magsajo,Anjanette Cayabyab,William Chandra Tjhi
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-21
> **comment**: Submitted to ACL 2025
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Recent advances in large language models (LLMs) have demonstrated remarkable capabilities on widely benchmarked high-resource languages; however, linguistic nuances of under-resourced languages remain unexplored. We introduce Batayan, a holistic Filipino benchmark designed to systematically evaluate LLMs across three key natural language processing (NLP) competencies: understanding, reasoning, and generation. Batayan consolidates eight tasks, covering both Tagalog and code-switched Taglish utterances. Our rigorous, native-speaker-driven annotation process ensures fluency and authenticity to the complex morphological and syntactic structures of Filipino, alleviating a pervasive translationese bias in existing Filipino corpora. We report empirical results on a variety of multilingual LLMs, highlighting significant performance gaps that signal the under-representation of Filipino in pretraining corpora, the unique hurdles in modeling Filipino's rich morphology and construction, and the importance of explicit Filipino language support and instruction tuning. Moreover, we discuss the practical challenges encountered in dataset construction and propose principled solutions for building culturally and linguistically-faithful resources in under-represented languages. We also provide a public benchmark and leaderboard as a clear foundation for iterative, community-driven progress in Filipino NLP.

### EvoP: Robust LLM Inference via Evolutionary Pruning 
[[arxiv](https://arxiv.org/abs/2502.14910)] [[cool](https://papers.cool/arxiv/2502.14910)] [[pdf](https://arxiv.org/pdf/2502.14910)]
> **Authors**: Shangyu Wu,Hongchao Du,Ying Xiong,Shuai Chen,Tei-wei Kuo,Nan Guan,Chun Jason Xue
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large Language Models (LLMs) have achieved remarkable success in natural language processing tasks, but their massive size and computational demands hinder their deployment in resource-constrained environments. Existing structured pruning methods address this issue by removing redundant structures (e.g., elements, channels, layers) from the model. However, these methods employ a heuristic pruning strategy, which leads to suboptimal performance. Besides, they also ignore the data characteristics when pruning the model. To overcome these limitations, we propose EvoP, an evolutionary pruning framework for robust LLM inference. EvoP first presents a cluster-based calibration dataset sampling (CCDS) strategy for creating a more diverse calibration dataset. EvoP then introduces an evolutionary pruning pattern searching (EPPS) method to find the optimal pruning pattern. Compared to existing structured pruning techniques, EvoP achieves the best performance while maintaining the best efficiency. Experiments across different LLMs and different downstream tasks validate the effectiveness of the proposed EvoP, making it a practical and scalable solution for deploying LLMs in real-world applications.

### GneissWeb: Preparing High Quality Data for LLMs at Scale 
[[arxiv](https://arxiv.org/abs/2502.14907)] [[cool](https://papers.cool/arxiv/2502.14907)] [[pdf](https://arxiv.org/pdf/2502.14907)]
> **Authors**: Hajar Emami Gohari,Swanand Ravindra Kadhe,Syed Yousaf Shah. Constantin Adam,Abdulhamid Adebayo,Praneet Adusumilli,Farhan Ahmed,Nathalie Baracaldo Angel,Santosh Borse,Yuan-Chi Chang,Xuan-Hong Dang,Nirmit Desai,Ravital Eres,Ran Iwamoto,Alexei Karve,Yan Koyfman,Wei-Han Lee,Changchang Liu,Boris Lublinsky,Takuyo Ohko,Pablo Pesce,Maroun Touma,Shiqiang Wang,Shalisha Witherspoon,Herbert Woisetschlager,David Wood, et al. (6 additional authors not shown)
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Data quantity and quality play a vital role in determining the performance of Large Language Models (LLMs). High-quality data, in particular, can significantly boost the LLM's ability to generalize on a wide range of downstream tasks. Large pre-training datasets for leading LLMs remain inaccessible to the public, whereas many open datasets are small in size (less than 5 trillion tokens), limiting their suitability for training large models. In this paper, we introduce GneissWeb, a large dataset yielding around 10 trillion tokens that caters to the data quality and quantity requirements of training LLMs. Our GneissWeb recipe that produced the dataset consists of sharded exact sub-string deduplication and a judiciously constructed ensemble of quality filters. GneissWeb achieves a favorable trade-off between data quality and quantity, producing models that outperform models trained on state-of-the-art open large datasets (5+ trillion tokens). We show that models trained using GneissWeb dataset outperform those trained on FineWeb-V1.1.0 by 2.73 percentage points in terms of average score computed on a set of 11 commonly used benchmarks (both zero-shot and few-shot) for pre-training dataset evaluation. When the evaluation set is extended to 20 benchmarks (both zero-shot and few-shot), models trained using GneissWeb still achieve a 1.75 percentage points advantage over those trained on FineWeb-V1.1.0.

### Beyond Words: Exploring Cultural Value Sensitivity in Multimodal Models 
[[arxiv](https://arxiv.org/abs/2502.14906)] [[cool](https://papers.cool/arxiv/2502.14906)] [[pdf](https://arxiv.org/pdf/2502.14906)]
> **Authors**: Srishti Yadav,Zhi Zhang,Daniel Hershcovich,Ekaterina Shutova
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-21
> **comment**: ef:NAACL 2025
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Investigating value alignment in Large Language Models (LLMs) based on cultural context has become a critical area of research. However, similar biases have not been extensively explored in large vision-language models (VLMs). As the scale of multimodal models continues to grow, it becomes increasingly important to assess whether images can serve as reliable proxies for culture and how these values are embedded through the integration of both visual and textual data. In this paper, we conduct a thorough evaluation of multimodal model at different scales, focusing on their alignment with cultural values. Our findings reveal that, much like LLMs, VLMs exhibit sensitivity to cultural values, but their performance in aligning with these values is highly context-dependent. While VLMs show potential in improving value understanding through the use of images, this alignment varies significantly across contexts highlighting the complexities and underexplored challenges in the alignment of multimodal models.

### Think Inside the JSON: Reinforcement Strategy for Strict LLM Schema Adherence 
[[arxiv](https://arxiv.org/abs/2502.14905)] [[cool](https://papers.cool/arxiv/2502.14905)] [[pdf](https://arxiv.org/pdf/2502.14905)]
> **Authors**: Bhavik Agarwal,Ishan Joshi,Viktoria Rojkova
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: In this paper, we address the challenge of enforcing strict schema adherence in large language model (LLM) generation by leveraging LLM reasoning capabilities. Building on the DeepSeek R1 reinforcement learning framework, our approach trains structured reasoning skills of a 1.5B parameter model through a novel pipeline that combines synthetic reasoning dataset construction with custom reward functions under Group Relative Policy Optimization (GRPO). Specifically, we first perform R1 reinforcement learning on a 20K sample unstructured-to-structured dataset, mirroring the original DeepSeek R1 methods, to establish core reasoning abilities. Subsequently, we performed supervised fine-tuning on a separate 10K reasoning sample dataset, focusing on refining schema adherence for downstream tasks. Despite the relatively modest training scope, requiring approximately 20 hours on an 8xH100 GPU cluster for GRPO training and 3 hours on 1xA100 for SFT, our model demonstrates robust performance in enforcing schema consistency. We compare our ThinkJSON approach against the original DeepSeek R1 (671B), distilled versions of DeepSeek R1 (Qwen-1.5B and Qwen-7B), and Gemini 2.0 Flash (70B), showcasing its effectiveness in real-world applications. Our results underscore the practical utility of a resource-efficient framework for schema-constrained text generation.

### PathRAG: Pruning Graph-based Retrieval Augmented Generation with Relational Paths 
[[arxiv](https://arxiv.org/abs/2502.14902)] [[cool](https://papers.cool/arxiv/2502.14902)] [[pdf](https://arxiv.org/pdf/2502.14902)]
> **Authors**: Boyu Chen,Zirui Guo,Zidan Yang,Yuluo Chen,Junze Chen,Zhenghao Liu,Chuan Shi,Cheng Yang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,信息检索
- **Abstract**: Retrieval-augmented generation (RAG) improves the response quality of large language models (LLMs) by retrieving knowledge from external databases. Typical RAG approaches split the text database into chunks, organizing them in a flat structure for efficient searches. To better capture the inherent dependencies and structured relationships across the text database, researchers propose to organize textual information into an indexing graph, known asgraph-based RAG. However, we argue that the limitation of current graph-based RAG methods lies in the redundancy of the retrieved information, rather than its insufficiency. Moreover, previous methods use a flat structure to organize retrieved information within the prompts, leading to suboptimal performance. To overcome these limitations, we propose PathRAG, which retrieves key relational paths from the indexing graph, and converts these paths into textual form for prompting LLMs. Specifically, PathRAG effectively reduces redundant information with flow-based pruning, while guiding LLMs to generate more logical and coherent responses with path-based prompting. Experimental results show that PathRAG consistently outperforms state-of-the-art baselines across six datasets and five evaluation dimensions. The code is available at the following link: https://github.com/BUPT-GAMMA/PathRAG

### Reading the unreadable: Creating a dataset of 19th century English newspapers using image-to-text language models 
[[arxiv](https://arxiv.org/abs/2502.14901)] [[cool](https://papers.cool/arxiv/2502.14901)] [[pdf](https://arxiv.org/pdf/2502.14901)]
> **Authors**: Jonathan Bourne
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,数字图书馆,机器学习
- **Abstract**: Oscar Wilde said, "The difference between literature and journalism is that journalism is unreadable, and literature is not read." Unfortunately, The digitally archived journalism of Oscar Wilde's 19th century often has no or poor quality Optical Character Recognition (OCR), reducing the accessibility of these archives and making them unreadable both figuratively and literally. This paper helps address the issue by performing OCR on "The Nineteenth Century Serials Edition" (NCSE), an 84k-page collection of 19th-century English newspapers and periodicals, using Pixtral 12B, a pre-trained image-to-text language model. The OCR capability of Pixtral was compared to 4 other OCR approaches, achieving a median character error rate of 1%, 5x lower than the next best model. The resulting NCSE v2.0 dataset features improved article identification, high-quality OCR, and text classified into four types and seventeen topics. The dataset contains 1.4 million entries, and 321 million words. Example use cases demonstrate analysis of topic similarity, readability, and event tracking. NCSE v2.0 is freely available to encourage historical and sociological research. As a result, 21st-century readers can now share Oscar Wilde's disappointment with 19th-century journalistic standards, reading the unreadable from the comfort of their own computers.

### Can AI mimic the human ability to define neologisms? 
[[arxiv](https://arxiv.org/abs/2502.14900)] [[cool](https://papers.cool/arxiv/2502.14900)] [[pdf](https://arxiv.org/pdf/2502.14900)]
> **Authors**: Georgios P. Georgiou
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: One ongoing debate in linguistics is whether Artificial Intelligence (AI) can effectively mimic human performance in language-related tasks. While much research has focused on various linguistic abilities of AI, little attention has been given to how it defines neologisms formed through different word formation processes. This study addresses this gap by examining the degree of agreement between human and AI-generated responses in defining three types of Greek neologisms: blends, compounds, and derivatives. The study employed an online experiment in which human participants selected the most appropriate definitions for neologisms, while ChatGPT received identical prompts. The results revealed fair agreement between human and AI responses for blends and derivatives but no agreement for compounds. However, when considering the majority response among humans, agreement with AI was high for blends and derivatives. These findings highlight the complexity of human language and the challenges AI still faces in capturing its nuances. In particular, they suggest a need for integrating more advanced semantic networks and contextual learning mechanisms into AI models to improve their interpretation of complex word formations, especially compounds.

### Retrieval-augmented systems can be dangerous medical communicators 
[[arxiv](https://arxiv.org/abs/2502.14898)] [[cool](https://papers.cool/arxiv/2502.14898)] [[pdf](https://arxiv.org/pdf/2502.14898)]
> **Authors**: Lionel Wong,Ayman Ali,Raymond Xiong,Shannon Zeijang Shen,Yoon Kim,Monica Agrawal
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-21
> **comment**: Preprint
- **标题**: None
- **领域**: 计算语言学,人工智能,信息检索
- **Abstract**: Patients have long sought health information online, and increasingly, they are turning to generative AI to answer their health-related queries. Given the high stakes of the medical domain, techniques like retrieval-augmented generation and citation grounding have been widely promoted as methods to reduce hallucinations and improve the accuracy of AI-generated responses and have been widely adopted into search engines. This paper argues that even when these methods produce literally accurate content drawn from source documents sans hallucinations, they can still be highly misleading. Patients may derive significantly different interpretations from AI-generated outputs than they would from reading the original source material, let alone consulting a knowledgeable clinician. Through a large-scale query analysis on topics including disputed diagnoses and procedure safety, we support our argument with quantitative and qualitative evidence of the suboptimal answers resulting from current systems. In particular, we highlight how these models tend to decontextualize facts, omit critical relevant sources, and reinforce patient misconceptions or biases. We propose a series of recommendations -- such as the incorporation of communication pragmatics and enhanced comprehension of source documents -- that could help mitigate these issues and extend beyond the medical domain.

### LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention 
[[arxiv](https://arxiv.org/abs/2502.14866)] [[cool](https://papers.cool/arxiv/2502.14866)] [[pdf](https://arxiv.org/pdf/2502.14866)]
> **Authors**: Shang Yang,Junxian Guo,Haotian Tang,Qinghao Hu,Guangxuan Xiao,Jiaming Tang,Yujun Lin,Zhijian Liu,Yao Lu,Song Han
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: Accepted by MLSys 2025. Code available at: https://github.com/mit-han-lab/omniserve
- **标题**: None
- **领域**: 计算语言学,人工智能,分布式、并行和集群计算,机器学习,表现
- **Abstract**: Large language models (LLMs) have shown remarkable potential in processing long sequences, yet efficiently serving these long-context models remains challenging due to the quadratic computational complexity of attention in the prefilling stage and the large memory footprint of the KV cache in the decoding stage. To address these issues, we introduce LServe, an efficient system that accelerates long-sequence LLM serving via hybrid sparse attention. This method unifies different hardware-friendly, structured sparsity patterns for both prefilling and decoding attention into a single framework, where computations on less important tokens are skipped block-wise. LServe demonstrates the compatibility of static and dynamic sparsity in long-context LLM attention. This design enables multiplicative speedups by combining these optimizations. Specifically, we convert half of the attention heads to nearly free streaming heads in both the prefilling and decoding stages. Additionally, we find that only a constant number of KV pages is required to preserve long-context capabilities, irrespective of context length. We then design a hierarchical KV page selection policy that dynamically prunes KV pages based on query-centric similarity. On average, LServe accelerates LLM prefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is released at https://github.com/mit-han-lab/omniserve.

### Interpretable Text Embeddings and Text Similarity Explanation: A Primer 
[[arxiv](https://arxiv.org/abs/2502.14862)] [[cool](https://papers.cool/arxiv/2502.14862)] [[pdf](https://arxiv.org/pdf/2502.14862)]
> **Authors**: Juri Opitz,Lucas Möller,Andrianos Michail,Simon Clematide
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,信息检索
- **Abstract**: Text embeddings and text embedding models are a backbone of many AI and NLP systems, particularly those involving search. However, interpretability challenges persist, especially in explaining obtained similarity scores, which is crucial for applications requiring transparency. In this paper, we give a structured overview of interpretability methods specializing in explaining those similarity scores, an emerging research area. We study the methods' individual ideas and techniques, evaluating their potential for improving interpretability of text embeddings and explaining predicted similarities.

### Aligning LLMs to Ask Good Questions A Case Study in Clinical Reasoning 
[[arxiv](https://arxiv.org/abs/2502.14860)] [[cool](https://papers.cool/arxiv/2502.14860)] [[pdf](https://arxiv.org/pdf/2502.14860)]
> **Authors**: Shuyue Stella Li,Jimin Mun,Faeze Brahman,Jonathan S. Ilgen,Yulia Tsvetkov,Maarten Sap
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 22 pages, 8 figures, 8 tables
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) often fail to ask effective questions under uncertainty, making them unreliable in domains where proactive information-gathering is essential for decisionmaking. We present ALFA, a framework that improves LLM question-asking by (i) decomposing the notion of a "good" question into a set of theory-grounded attributes (e.g., clarity, relevance), (ii) controllably synthesizing attribute-specific question variations, and (iii) aligning models via preference-based optimization to explicitly learn to ask better questions along these fine-grained attributes. Focusing on clinical reasoning as a case study, we introduce the MediQ-AskDocs dataset, composed of 17k real-world clinical interactions augmented with 80k attribute-specific preference pairs of follow-up questions, as well as a novel expert-annotated interactive healthcare QA task to evaluate question-asking abilities. Models aligned with ALFA reduce diagnostic errors by 56.6% on MediQ-AskDocs compared to SOTA instruction-tuned LLMs, with a question-level win-rate of 64.4% and strong generalizability. Our findings suggest that explicitly guiding question-asking with structured, fine-grained attributes offers a scalable path to improve LLMs, especially in expert application domains.

### FR-Spec: Accelerating Large-Vocabulary Language Models via Frequency-Ranked Speculative Sampling 
[[arxiv](https://arxiv.org/abs/2502.14856)] [[cool](https://papers.cool/arxiv/2502.14856)] [[pdf](https://arxiv.org/pdf/2502.14856)]
> **Authors**: Weilin Zhao,Tengyu Pan,Xu Han,Yudi Zhang,Ao Sun,Yuxiang Huang,Kaihuo Zhang,Weilun Zhao,Yuxuan Li,Jianyong Wang,Zhiyuan Liu,Maosong Sun
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Speculative sampling has emerged as an important technique for accelerating the auto-regressive generation process of large language models (LLMs) by utilizing a draft-then-verify mechanism to produce multiple tokens per forward pass. While state-of-the-art speculative sampling methods use only a single layer and a language modeling (LM) head as the draft model to achieve impressive layer compression, their efficiency gains are substantially reduced for large-vocabulary LLMs, such as Llama-3-8B with a vocabulary of 128k tokens. To address this, we present FR-Spec, a frequency-ranked speculative sampling framework that optimizes draft candidate selection through vocabulary space compression. By constraining the draft search to a frequency-prioritized token subset, our method reduces LM Head computation overhead by 75% while ensuring the equivalence of the final output distribution. Experiments across multiple datasets demonstrate an average of 1.12$\times$ speedup over the state-of-the-art speculative sampling method EAGLE-2.

### CLIPPER: Compression enables long-context synthetic data generation 
[[arxiv](https://arxiv.org/abs/2502.14854)] [[cool](https://papers.cool/arxiv/2502.14854)] [[pdf](https://arxiv.org/pdf/2502.14854)]
> **Authors**: Chau Minh Pham,Yapei Chang,Mohit Iyyer
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: LLM developers are increasingly reliant on synthetic data, but generating high-quality data for complex long-context reasoning tasks remains challenging. We introduce CLIPPER, a compression-based approach for generating synthetic data tailored to narrative claim verification - a task that requires reasoning over a book to verify a given claim. Instead of generating claims directly from the raw text of the book, which results in artifact-riddled claims, CLIPPER first compresses the book into chapter outlines and book summaries and then uses these intermediate representations to generate complex claims and corresponding chain-of-thoughts. Compared to naive approaches, CLIPPER produces claims that are more valid, grounded, and complex. Using CLIPPER, we construct a dataset of 19K synthetic book claims paired with their source texts and chain-of-thought reasoning, and use it to fine-tune three open-weight models. Our best model achieves breakthrough results on narrative claim verification (from 28% to 76% accuracy on our test set) and sets a new state-of-the-art for sub-10B models on the NoCha leaderboard. Further analysis shows that our models generate more detailed and grounded chain-of-thought reasoning while also improving performance on other narrative understanding tasks (e.g., NarrativeQA).

### GATE: Graph-based Adaptive Tool Evolution Across Diverse Tasks 
[[arxiv](https://arxiv.org/abs/2502.14848)] [[cool](https://papers.cool/arxiv/2502.14848)] [[pdf](https://arxiv.org/pdf/2502.14848)]
> **Authors**: Jianwen Luo,Yiming Huang,Jinxiang Meng,Fangyu Lei,Shizhu He,Xiao Liu,Shanshan Jiang,Bin Dong,Jun Zhao,Kang Liu
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 8 pages of main text, 38 pages of appendices
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models (LLMs) have shown great promise in tool-making, yet existing frameworks often struggle to efficiently construct reliable toolsets and are limited to single-task settings. To address these challenges, we propose GATE (Graph-based Adaptive Tool Evolution), an adaptive framework that dynamically constructs and evolves a hierarchical graph of reusable tools across multiple scenarios. We evaluate GATE on open-ended tasks (Minecraft), agent-based tasks (TextCraft, DABench), and code generation tasks (MATH, Date, TabMWP). Our results show that GATE achieves up to 4.3x faster milestone completion in Minecraft compared to the previous SOTA, and provides an average improvement of 9.23% over existing tool-making methods in code generation tasks and 10.03% in agent tasks. GATE demonstrates the power of adaptive evolution, balancing tool quantity, complexity, and functionality while maintaining high efficiency. Code and data are available at \url{https://github.com/ayanami2003/GATE}.

### Revealing and Mitigating Over-Attention in Knowledge Editing 
[[arxiv](https://arxiv.org/abs/2502.14838)] [[cool](https://papers.cool/arxiv/2502.14838)] [[pdf](https://arxiv.org/pdf/2502.14838)]
> **Authors**: Pinzheng Wang,Zecheng Tang,Keyan Zhou,Juntao Li,Qiaoming Zhu,Min Zhang
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large Language Models have demonstrated superior performance across a wide range of tasks, but they still exhibit undesirable errors due to incorrect knowledge learned from the training data. To avoid this, knowledge editing methods emerged to precisely edit the specific model knowledge via efficiently modifying a very small percentage of parameters. % However, those methods can lead to the problem of Specificity Failure: when the content related to the edited knowledge occurs in the context, it can inadvertently corrupt other pre-existing knowledge. However, those methods can lead to the problem of Specificity Failure, where the existing knowledge and capabilities are severely degraded due to editing. Our preliminary indicates that Specificity Failure primarily stems from the model's attention heads assigning excessive attention scores to entities related to the edited knowledge, thereby unduly focusing on specific snippets within the context, which we denote as the Attention Drift phenomenon. To mitigate such Attention Drift issue, we introduce a simple yet effective method Selective Attention Drift Restriction}(SADR), which introduces an additional regularization term during the knowledge editing process to restrict changes in the attention weight distribution, thereby preventing undue focus on the edited entity. Experiments on five frequently used strong LLMs demonstrate the effectiveness of our method, where SADR can significantly mitigate Specificity Failure in the predominant knowledge editing tasks.

### Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent Attention in Any Transformer-based LLMs 
[[arxiv](https://arxiv.org/abs/2502.14837)] [[cool](https://papers.cool/arxiv/2502.14837)] [[pdf](https://arxiv.org/pdf/2502.14837)]
> **Authors**: Tao Ji,Bin Guo,Yuanbin Wu,Qipeng Guo,Lixing Shen,Zhan Chen,Xipeng Qiu,Qi Zhang,Tao Gui
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 16 pages, 8 figures
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Multi-head Latent Attention (MLA) is an innovative architecture proposed by DeepSeek, designed to ensure efficient and economical inference by significantly compressing the Key-Value (KV) cache into a latent vector. Compared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its variants such as Grouped-Query Attention (GQA) exhibit significant cost disadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA without pre-training from scratch is both meaningful and challenging. This paper proposes the first data-efficient fine-tuning method for transitioning from MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE, we remove RoPE from dimensions of queries and keys that contribute less to the attention scores, for low-rank approximation, we introduce joint SVD approximations based on the pre-trained parameters of keys and values. These carefully designed strategies enable MHA2MLA to recover performance using only a small fraction (0.3% to 0.6%) of the data, significantly reducing inference costs while seamlessly integrating with compression techniques such as KV cache quantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%, with only a 0.5% drop in LongBench performance.

### Middle-Layer Representation Alignment for Cross-Lingual Transfer in Fine-Tuned LLMs 
[[arxiv](https://arxiv.org/abs/2502.14830)] [[cool](https://papers.cool/arxiv/2502.14830)] [[pdf](https://arxiv.org/pdf/2502.14830)]
> **Authors**: Danni Liu,Jan Niehues
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: While large language models demonstrate remarkable capabilities at task-specific applications through fine-tuning, extending these benefits across diverse languages is essential for broad accessibility. However, effective cross-lingual transfer is hindered by LLM performance gaps across languages and the scarcity of fine-tuning data in many languages. Through analysis of LLM internal representations from over 1,000+ language pairs, we discover that middle layers exhibit the strongest potential for cross-lingual alignment. Building on this finding, we propose a middle-layer alignment objective integrated into task-specific training. Our experiments on slot filling, machine translation, and structured text generation show consistent improvements in cross-lingual transfer, especially to lower-resource languages. The method is robust to the choice of alignment languages and generalizes to languages unseen during alignment. Furthermore, we show that separately trained alignment modules can be merged with existing task-specific modules, improving cross-lingual capabilities without full re-training. Our code is publicly available (https://github.com/dannigt/mid-align).

### Measuring Faithfulness of Chains of Thought by Unlearning Reasoning Steps 
[[arxiv](https://arxiv.org/abs/2502.14829)] [[cool](https://papers.cool/arxiv/2502.14829)] [[pdf](https://arxiv.org/pdf/2502.14829)]
> **Authors**: Martin Tutek,Fateme Hashemi Chaleshtori,Ana Marasović,Yonatan Belinkov
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: When prompted to think step-by-step, language models (LMs) produce a chain of thought (CoT), a sequence of reasoning steps that the model supposedly used to produce its prediction. However, despite much work on CoT prompting, it is unclear if CoT reasoning is faithful to the models' parameteric beliefs. We introduce a framework for measuring parametric faithfulness of generated reasoning, and propose Faithfulness by Unlearning Reasoning steps (FUR), an instance of this framework. FUR erases information contained in reasoning steps from model parameters. We perform experiments unlearning CoTs of four LMs prompted on four multi-choice question answering (MCQA) datasets. Our experiments show that FUR is frequently able to change the underlying models' prediction by unlearning key steps, indicating when a CoT is parametrically faithful. Further analysis shows that CoTs generated by models post-unlearning support different answers, hinting at a deeper effect of unlearning. Importantly, CoT steps identified as important by FUR do not align well with human notions of plausbility, emphasizing the need for specialized alignment

### eC-Tab2Text: Aspect-Based Text Generation from e-Commerce Product Tables 
[[arxiv](https://arxiv.org/abs/2502.14820)] [[cool](https://papers.cool/arxiv/2502.14820)] [[pdf](https://arxiv.org/pdf/2502.14820)]
> **Authors**: Luis Antonio Gutiérrez Guanilo,Mir Tafseer Nayeem,Cristian López,Davood Rafiei
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: NAACL 2025 (Industry Track)
- **标题**: None
- **领域**: 计算语言学,人工智能,数据库,人机交互
- **Abstract**: Large Language Models (LLMs) have demonstrated exceptional versatility across diverse domains, yet their application in e-commerce remains underexplored due to a lack of domain-specific datasets. To address this gap, we introduce eC-Tab2Text, a novel dataset designed to capture the intricacies of e-commerce, including detailed product attributes and user-specific queries. Leveraging eC-Tab2Text, we focus on text generation from product tables, enabling LLMs to produce high-quality, attribute-specific product reviews from structured tabular data. Fine-tuned models were rigorously evaluated using standard Table2Text metrics, alongside correctness, faithfulness, and fluency assessments. Our results demonstrate substantial improvements in generating contextually accurate reviews, highlighting the transformative potential of tailored datasets and fine-tuning methodologies in optimizing e-commerce workflows. This work highlights the potential of LLMs in e-commerce workflows and the essential role of domain-specific datasets in tailoring them to industry-specific challenges.

### From RAG to Memory: Non-Parametric Continual Learning for Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.14802)] [[cool](https://papers.cool/arxiv/2502.14802)] [[pdf](https://arxiv.org/pdf/2502.14802)]
> **Authors**: Bernal Jiménez Gutiérrez,Yiheng Shu,Weijian Qi,Sizhe Zhou,Yu Su
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: Code and data to be released at: https://github.com/OSU-NLP-Group/HippoRAG
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Our ability to continuously acquire, organize, and leverage knowledge is a key feature of human intelligence that AI systems must approximate to unlock their full potential. Given the challenges in continual learning with large language models (LLMs), retrieval-augmented generation (RAG) has become the dominant way to introduce new information. However, its reliance on vector retrieval hinders its ability to mimic the dynamic and interconnected nature of human long-term memory. Recent RAG approaches augment vector embeddings with various structures like knowledge graphs to address some of these gaps, namely sense-making and associativity. However, their performance on more basic factual memory tasks drops considerably below standard RAG. We address this unintended deterioration and propose HippoRAG 2, a framework that outperforms standard RAG comprehensively on factual, sense-making, and associative memory tasks. HippoRAG 2 builds upon the Personalized PageRank algorithm used in HippoRAG and enhances it with deeper passage integration and more effective online use of an LLM. This combination pushes this RAG system closer to the effectiveness of human long-term memory, achieving a 7% improvement in associative memory tasks over the state-of-the-art embedding model while also exhibiting superior factual knowledge and sense-making memory capabilities. This work paves the way for non-parametric continual learning for LLMs. Our code and data will be released at https://github.com/OSU-NLP-Group/HippoRAG.

### Rapid Word Learning Through Meta In-Context Learning 
[[arxiv](https://arxiv.org/abs/2502.14791)] [[cool](https://papers.cool/arxiv/2502.14791)] [[pdf](https://arxiv.org/pdf/2502.14791)]
> **Authors**: Wentao Wang,Guangyuan Jiang,Tal Linzen,Brenden M. Lake
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Humans can quickly learn a new word from a few illustrative examples, and then systematically and flexibly use it in novel contexts. Yet the abilities of current language models for few-shot word learning, and methods for improving these abilities, are underexplored. In this study, we introduce a novel method, Meta-training for IN-context learNing Of Words (Minnow). This method trains language models to generate new examples of a word's usage given a few in-context examples, using a special placeholder token to represent the new word. This training is repeated on many new words to develop a general word-learning ability. We find that training models from scratch with Minnow on human-scale child-directed language enables strong few-shot word learning, comparable to a large language model (LLM) pre-trained on orders of magnitude more data. Furthermore, through discriminative and generative evaluations, we demonstrate that finetuning pre-trained LLMs with Minnow improves their ability to discriminate between new words, identify syntactic categories of new words, and generate reasonable new usages and definitions for new words, based on one or a few in-context examples. These findings highlight the data efficiency of Minnow and its potential to improve language model performance in word learning tasks.

### ReVision: A Dataset and Baseline VLM for Privacy-Preserving Task-Oriented Visual Instruction Rewriting 
[[arxiv](https://arxiv.org/abs/2502.14780)] [[cool](https://papers.cool/arxiv/2502.14780)] [[pdf](https://arxiv.org/pdf/2502.14780)]
> **Authors**: Abhijit Mishra,Richard Noh,Hsiang Fu,Mingda Li,Minji Kim
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 12 pages, 7 figures, 3 tables
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别
- **Abstract**: Efficient and privacy-preserving multimodal interaction is essential as AR, VR, and modern smartphones with powerful cameras become primary interfaces for human-computer communication. Existing powerful large vision-language models (VLMs) enabling multimodal interaction often rely on cloud-based processing, raising significant concerns about (1) visual privacy by transmitting sensitive vision data to servers, and (2) their limited real-time, on-device usability. This paper explores Visual Instruction Rewriting, a novel approach that transforms multimodal instructions into text-only commands, allowing seamless integration of lightweight on-device instruction rewriter VLMs (250M parameters) with existing conversational AI systems, enhancing vision data privacy. To achieve this, we present a dataset of over 39,000 examples across 14 domains and develop a compact VLM, pretrained on image captioning datasets and fine-tuned for instruction rewriting. Experimental results, evaluated through NLG metrics such as BLEU, METEOR, and ROUGE, along with semantic parsing analysis, demonstrate that even a quantized version of the model (<500MB storage footprint) can achieve effective instruction rewriting, thus enabling privacy-focused, multimodal AI applications.

### Harnessing PDF Data for Improving Japanese Large Multimodal Models 
[[arxiv](https://arxiv.org/abs/2502.14778)] [[cool](https://papers.cool/arxiv/2502.14778)] [[pdf](https://arxiv.org/pdf/2502.14778)]
> **Authors**: Jeonghun Baek,Akiko Aizawa,Kiyoharu Aizawa
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 15 pages, 8 figures
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别
- **Abstract**: Large Multimodal Models (LMMs) have demonstrated strong performance in English, but their effectiveness in Japanese remains limited due to the lack of high-quality training data. Current Japanese LMMs often rely on translated English datasets, restricting their ability to capture Japan-specific cultural knowledge. To address this, we explore the potential of Japanese PDF data as a training resource, an area that remains largely underutilized. We introduce a fully automated pipeline that leverages pretrained models to extract image-text pairs from PDFs through layout analysis, OCR, and vision-language pairing, removing the need for manual annotation. Additionally, we construct instruction data from extracted image-text pairs to enrich the training data. To evaluate the effectiveness of PDF-derived data, we train Japanese LMMs and assess their performance on the Japanese LMM Benchmark. Our results demonstrate substantial improvements, with performance gains ranging from 3.9% to 13.8% on Heron-Bench. Further analysis highlights the impact of PDF-derived data on various factors, such as model size and language models, reinforcing its value as a multimodal resource for Japanese LMMs. We plan to make the source code and data publicly available upon acceptance.

### SurveyX: Academic Survey Automation via Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.14776)] [[cool](https://papers.cool/arxiv/2502.14776)] [[pdf](https://arxiv.org/pdf/2502.14776)]
> **Authors**: Xun Liang,Jiawei Yang,Yezhaohui Wang,Chen Tang,Zifan Zheng,Shichao Song,Zehao Lin,Yebin Yang,Simin Niu,Hanyu Wang,Bo Tang,Feiyu Xiong,Keming Mao,Zhiyu li
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 15 pages, 16 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models (LLMs) have demonstrated exceptional comprehension capabilities and a vast knowledge base, suggesting that LLMs can serve as efficient tools for automated survey generation. However, recent research related to automated survey generation remains constrained by some critical limitations like finite context window, lack of in-depth content discussion, and absence of systematic evaluation frameworks. Inspired by human writing processes, we propose SurveyX, an efficient and organized system for automated survey generation that decomposes the survey composing process into two phases: the Preparation and Generation phases. By innovatively introducing online reference retrieval, a pre-processing method called AttributeTree, and a re-polishing process, SurveyX significantly enhances the efficacy of survey composition. Experimental evaluation results show that SurveyX outperforms existing automated survey generation systems in content quality (0.259 improvement) and citation quality (1.76 enhancement), approaching human expert performance across multiple evaluation dimensions. Examples of surveys generated by SurveyX are available on www.surveyx.cn

### Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.14768)] [[cool](https://papers.cool/arxiv/2502.14768)] [[pdf](https://arxiv.org/pdf/2502.14768)]
> **Authors**: Tian Xie,Zitian Gao,Qingnan Ren,Haoming Luo,Yuqian Hong,Bryan Dai,Joey Zhou,Kai Qiu,Zhirong Wu,Chong Luo
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Inspired by the success of DeepSeek-R1, we explore the potential of rule-based reinforcement learning (RL) in large reasoning models. To analyze reasoning dynamics, we use synthetic logic puzzles as training data due to their controllable complexity and straightforward answer verification. We make some key technical contributions that lead to effective and stable RL training: a system prompt that emphasizes the thinking and answering process, a stringent format reward function that penalizes outputs for taking shortcuts, and a straightforward training recipe that achieves stable convergence. Our 7B model develops advanced reasoning skills-such as reflection, verification, and summarization-that are absent from the logic corpus. Remarkably, after training on just 5K logic problems, it demonstrates generalization abilities to the challenging math benchmarks AIME and AMC.

### Tree-of-Debate: Multi-Persona Debate Trees Elicit Critical Thinking for Scientific Comparative Analysis 
[[arxiv](https://arxiv.org/abs/2502.14767)] [[cool](https://papers.cool/arxiv/2502.14767)] [[pdf](https://arxiv.org/pdf/2502.14767)]
> **Authors**: Priyanka Kargupta,Ishika Agarwal,Tal August,Jiawei Han
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: Code available at: https://github.com/pkargupta/tree-of-debate
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: With the exponential growth of research facilitated by modern technology and improved accessibility, scientific discoveries have become increasingly fragmented within and across fields. This makes it challenging to assess the significance, novelty, incremental findings, and equivalent ideas between related works, particularly those from different research communities. Large language models (LLMs) have recently demonstrated strong quantitative and qualitative reasoning abilities, and multi-agent LLM debates have shown promise in handling complex reasoning tasks by exploring diverse perspectives and reasoning paths. Inspired by this, we introduce Tree-of-Debate (ToD), a framework which converts scientific papers into LLM personas that debate their respective novelties. To emphasize structured, critical reasoning rather than focusing solely on outcomes, ToD dynamically constructs a debate tree, enabling fine-grained analysis of independent novelty arguments within scholarly articles. Through experiments on scientific literature across various domains, evaluated by expert researchers, we demonstrate that ToD generates informative arguments, effectively contrasts papers, and supports researchers in their literature review.

### Step-by-Step Fact Verification System for Medical Claims with Explainable Reasoning 
[[arxiv](https://arxiv.org/abs/2502.14765)] [[cool](https://papers.cool/arxiv/2502.14765)] [[pdf](https://arxiv.org/pdf/2502.14765)]
> **Authors**: Juraj Vladika,Ivana Hacajová,Florian Matthes
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: Accepted to NAACL 2025 (Main)
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Fact verification (FV) aims to assess the veracity of a claim based on relevant evidence. The traditional approach for automated FV includes a three-part pipeline relying on short evidence snippets and encoder-only inference models. More recent approaches leverage the multi-turn nature of LLMs to address FV as a step-by-step problem where questions inquiring additional context are generated and answered until there is enough information to make a decision. This iterative method makes the verification process rational and explainable. While these methods have been tested for encyclopedic claims, exploration on domain-specific and realistic claims is missing. In this work, we apply an iterative FV system on three medical fact-checking datasets and evaluate it with multiple settings, including different LLMs, external web search, and structured reasoning using logic predicates. We demonstrate improvements in the final performance over traditional approaches and the high potential of step-by-step FV systems for domain-specific claims.

### On the Influence of Context Size and Model Choice in Retrieval-Augmented Generation Systems 
[[arxiv](https://arxiv.org/abs/2502.14759)] [[cool](https://papers.cool/arxiv/2502.14759)] [[pdf](https://arxiv.org/pdf/2502.14759)]
> **Authors**: Juraj Vladika,Florian Matthes
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: Accepted to Findings of NAACL 2025
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Retrieval-augmented generation (RAG) has emerged as an approach to augment large language models (LLMs) by reducing their reliance on static knowledge and improving answer factuality. RAG retrieves relevant context snippets and generates an answer based on them. Despite its increasing industrial adoption, systematic exploration of RAG components is lacking, particularly regarding the ideal size of provided context, and the choice of base LLM and retrieval method. To help guide development of robust RAG systems, we evaluate various context sizes, BM25 and semantic search as retrievers, and eight base LLMs. Moving away from the usual RAG evaluation with short answers, we explore the more challenging long-form question answering in two domains, where a good answer has to utilize the entire context. Our findings indicate that final QA performance improves steadily with up to 15 snippets but stagnates or declines beyond that. Finally, we show that different general-purpose LLMs excel in the biomedical domain than the encyclopedic one, and that open-domain evidence retrieval in large corpora is challenging.

### TritonBench: Benchmarking Large Language Model Capabilities for Generating Triton Operators 
[[arxiv](https://arxiv.org/abs/2502.14752)] [[cool](https://papers.cool/arxiv/2502.14752)] [[pdf](https://arxiv.org/pdf/2502.14752)]
> **Authors**: Jianling Li,Shangzhan Li,Zhenye Gao,Qi Shi,Yuxuan Li,Zefan Wang,Jiacheng Huang,Haojie Wang,Jianrong Wang,Xu Han,Zhiyuan Liu,Maosong Sun
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Triton, a high-level Python-like language designed for building efficient GPU kernels, is widely adopted in deep learning frameworks due to its portability, flexibility, and accessibility. However, programming and parallel optimization still require considerable trial and error from Triton developers. Despite advances in large language models (LLMs) for conventional code generation, these models struggle to generate accurate, performance-optimized Triton code, as they lack awareness of its specifications and the complexities of GPU programming. More critically, there is an urgent need for systematic evaluations tailored to Triton. In this work, we introduce TritonBench, the first comprehensive benchmark for Triton operator generation. TritonBench features two evaluation channels: a curated set of 184 real-world operators from GitHub and a collection of operators aligned with PyTorch interfaces. Unlike conventional code benchmarks prioritizing functional correctness, TritonBench also profiles efficiency performance on widely deployed GPUs aligned with industry applications. Our study reveals that current state-of-the-art code LLMs struggle to generate efficient Triton operators, highlighting a significant gap in high-performance code generation. TritonBench will be available at https://github.com/thunlp/TritonBench.

### Large Language Models Struggle to Describe the Haystack without Human Help: Human-in-the-loop Evaluation of LLMs 
[[arxiv](https://arxiv.org/abs/2502.14748)] [[cool](https://papers.cool/arxiv/2502.14748)] [[pdf](https://arxiv.org/pdf/2502.14748)]
> **Authors**: Zongxia Li,Lorena Calvo-Bartolomé,Alexander Hoyle,Paiheng Xu,Alden Dima,Juan Francisco Fung,Jordan Boyd-Graber
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 21 Pages.LLMfor Data Exploration and content analysis
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: A common use of NLP is to facilitate the understanding of large document collections, with a shift from using traditional topic models to Large Language Models. Yet the effectiveness of using LLM for large corpus understanding in real-world applications remains under-explored. This study measures the knowledge users acquire with unsupervised, supervised LLM-based exploratory approaches or traditional topic models on two datasets. While LLM-based methods generate more human-readable topics and show higher average win probabilities than traditional models for data exploration, they produce overly generic topics for domain-specific datasets that do not easily allow users to learn much about the documents. Adding human supervision to the LLM generation process improves data exploration by mitigating hallucination and over-genericity but requires greater human effort. In contrast, traditional. models like Latent Dirichlet Allocation (LDA) remain effective for exploration but are less user-friendly. We show that LLMs struggle to describe the haystack of large corpora without human help, particularly domain-specific data, and face scaling and hallucination limitations due to context length constraints. Dataset available at https://huggingface. co/datasets/zli12321/Bills.

### HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language Models via Monitoring Hidden States 
[[arxiv](https://arxiv.org/abs/2502.14744)] [[cool](https://papers.cool/arxiv/2502.14744)] [[pdf](https://arxiv.org/pdf/2502.14744)]
> **Authors**: Yilei Jiang,Xinyan Gao,Tianshuo Peng,Yingshui Tan,Xiaoyong Zhu,Bo Zheng,Xiangyu Yue
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The integration of additional modalities increases the susceptibility of large vision-language models (LVLMs) to safety risks, such as jailbreak attacks, compared to their language-only counterparts. While existing research primarily focuses on post-hoc alignment techniques, the underlying safety mechanisms within LVLMs remain largely unexplored. In this work , we investigate whether LVLMs inherently encode safety-relevant signals within their internal activations during inference. Our findings reveal that LVLMs exhibit distinct activation patterns when processing unsafe prompts, which can be leveraged to detect and mitigate adversarial inputs without requiring extensive fine-tuning. Building on this insight, we introduce HiddenDetect, a novel tuning-free framework that harnesses internal model activations to enhance safety. Experimental results show that {HiddenDetect} surpasses state-of-the-art methods in detecting jailbreak attacks against LVLMs. By utilizing intrinsic safety-aware patterns, our method provides an efficient and scalable solution for strengthening LVLM robustness against multimodal threats. Our code will be released publicly at https://github.com/leigest519/HiddenDetect.

### SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines 
[[arxiv](https://arxiv.org/abs/2502.14739)] [[cool](https://papers.cool/arxiv/2502.14739)] [[pdf](https://arxiv.org/pdf/2502.14739)]
> **Authors**: M-A-P Team,Xinrun Du,Yifan Yao,Kaijing Ma,Bingli Wang,Tianyu Zheng,Kang Zhu,Minghao Liu,Yiming Liang,Xiaolong Jin,Zhenlin Wei,Chujie Zheng,Kaixin Deng,Shian Jia,Sichao Jiang,Yiyan Liao,Rui Li,Qinrui Li,Sirun Li,Yizhi Li,Yunwen Li,Dehua Ma,Yuansheng Ni,Haoran Que,Qiyao Wang, et al. (71 additional authors not shown)
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) have demonstrated remarkable proficiency in mainstream academic disciplines such as mathematics, physics, and computer science. However, human knowledge encompasses over 200 specialized disciplines, far exceeding the scope of existing benchmarks. The capabilities of LLMs in many of these specialized fields-particularly in light industry, agriculture, and service-oriented disciplines-remain inadequately evaluated. To address this gap, we present SuperGPQA, a comprehensive benchmark that evaluates graduate-level knowledge and reasoning capabilities across 285 disciplines. Our benchmark employs a novel Human-LLM collaborative filtering mechanism to eliminate trivial or ambiguous questions through iterative refinement based on both LLM responses and expert feedback. Our experimental results reveal significant room for improvement in the performance of current state-of-the-art LLMs across diverse knowledge domains (e.g., the reasoning-focused model DeepSeek-R1 achieved the highest accuracy of 61.82% on SuperGPQA), highlighting the considerable gap between current model capabilities and artificial general intelligence. Additionally, we present comprehensive insights from our management of a large-scale annotation process, involving over 80 expert annotators and an interactive Human-LLM collaborative system, offering valuable methodological guidance for future research initiatives of comparable scope.

### Sentence Smith: Formally Controllable Text Transformation and its Application to Evaluation of Text Embedding Models 
[[arxiv](https://arxiv.org/abs/2502.14734)] [[cool](https://papers.cool/arxiv/2502.14734)] [[pdf](https://arxiv.org/pdf/2502.14734)]
> **Authors**: Hongji Li,Andrianos Michail,Reto Gubelmann,Simon Clematide,Juri Opitz
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: We propose the Sentence Smith framework that enables controlled and specified manipulation of text meaning. It consists of three main steps: 1. Parsing a sentence into a semantic graph, 2. Applying human-designed semantic manipulation rules, and 3. Generating text from the manipulated graph. A final filtering step (4.) ensures the validity of the applied transformation. To demonstrate the utility of Sentence Smith in an application study, we use it to generate hard negative pairs that challenge text embedding models. Since the controllable generation makes it possible to clearly isolate different types of semantic shifts, we can gain deeper insights into the specific strengths and weaknesses of widely used text embedding models, also addressing an issue in current benchmarking where linguistic phenomena remain opaque. Human validation confirms that the generations produced by Sentence Smith are highly accurate.

### Entity Framing and Role Portrayal in the News 
[[arxiv](https://arxiv.org/abs/2502.14718)] [[cool](https://papers.cool/arxiv/2502.14718)] [[pdf](https://arxiv.org/pdf/2502.14718)]
> **Authors**: Tarek Mahmoud,Zhuohan Xie,Dimitar Dimitrov,Nikolaos Nikolaidis,Purificação Silvano,Roman Yangarber,Shivam Sharma,Elisa Sartori,Nicolas Stefanovitch,Giovanni Da San Martino,Jakub Piskorski,Preslav Nakov
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 23 pages, 12 figures. Submitted to ACL Rolling Review (ARR)
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: We introduce a novel multilingual hierarchical corpus annotated for entity framing and role portrayal in news articles. The dataset uses a unique taxonomy inspired by storytelling elements, comprising 22 fine-grained roles, or archetypes, nested within three main categories: protagonist, antagonist, and innocent. Each archetype is carefully defined, capturing nuanced portrayals of entities such as guardian, martyr, and underdog for protagonists; tyrant, deceiver, and bigot for antagonists; and victim, scapegoat, and exploited for innocents. The dataset includes 1,378 recent news articles in five languages (Bulgarian, English, Hindi, European Portuguese, and Russian) focusing on two critical domains of global significance: the Ukraine-Russia War and Climate Change. Over 5,800 entity mentions have been annotated with role labels. This dataset serves as a valuable resource for research into role portrayal and has broader implications for news analysis. We describe the characteristics of the dataset and the annotation process, and we report evaluation results on fine-tuned state-of-the-art multilingual transformers and hierarchical zero-shot learning using LLMs at the level of a document, a paragraph, and a sentence.

### Data-Efficient Pretraining with Group-Level Data Influence Modeling 
[[arxiv](https://arxiv.org/abs/2502.14709)] [[cool](https://papers.cool/arxiv/2502.14709)] [[pdf](https://arxiv.org/pdf/2502.14709)]
> **Authors**: Zichun Yu,Fei Peng,Jie Lei,Arnold Overwijk,Wen-tau Yih,Chenyan Xiong
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Data-efficient pretraining has shown tremendous potential to elevate scaling laws. This paper argues that effective pretraining data should be curated at the group level, treating a set of data points as a whole rather than as independent contributors. To achieve that, we propose Group-Level Data Influence Modeling (Group-MATES), a novel data-efficient pretraining method that captures and optimizes group-level data utility. Specifically, Group-MATES collects oracle group-level influences by locally probing the pretraining model with data sets. It then fine-tunes a relational data influence model to approximate oracles as relationship-weighted aggregations of individual influences. The fine-tuned model selects the data subset by maximizing its group-level influence prediction, with influence-aware clustering to enable efficient inference. Experiments on the DCLM benchmark demonstrate that Group-MATES achieves a 10% relative core score improvement on 22 downstream tasks over DCLM-Baseline and 5% over individual-influence-based methods, establishing a new state-of-the-art. Further analyses highlight the effectiveness of relational data influence models in capturing intricate interactions between data points.

### I-MCTS: Enhancing Agentic AutoML via Introspective Monte Carlo Tree Search 
[[arxiv](https://arxiv.org/abs/2502.14693)] [[cool](https://papers.cool/arxiv/2502.14693)] [[pdf](https://arxiv.org/pdf/2502.14693)]
> **Authors**: Zujie Liang,Feng Wei,Wujiang Xu,Lin Chen,Yuxi Qian,Xinhui Wu
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Recent advancements in large language models (LLMs) have shown remarkable potential in automating machine learning tasks. However, existing LLM-based agents often struggle with low-diversity and suboptimal code generation. While recent work has introduced Monte Carlo Tree Search (MCTS) to address these issues, limitations persist in the quality and diversity of thoughts generated, as well as in the scalar value feedback mechanisms used for node selection. In this study, we introduce Introspective Monte Carlo Tree Search (I-MCTS), a novel approach that iteratively expands tree nodes through an introspective process that meticulously analyzes solutions and results from parent and sibling nodes. This facilitates a continuous refinement of the node in the search tree, thereby enhancing the overall decision-making process. Furthermore, we integrate a Large Language Model (LLM)-based value model to facilitate direct evaluation of each node's solution prior to conducting comprehensive computational rollouts. A hybrid rewarding mechanism is implemented to seamlessly transition the Q-value from LLM-estimated scores to actual performance scores. This allows higher-quality nodes to be traversed earlier. Applied to the various ML tasks, our approach demonstrates a 6% absolute improvement in performance compared to the strong open-source AutoML agents, showcasing its effectiveness in enhancing agentic AutoML systems. Resource available at https://github.com/jokieleung/I-MCTS

### Bridging the Gap: Transforming Natural Language Questions into SQL Queries via Abstract Query Pattern and Contextual Schema Markup 
[[arxiv](https://arxiv.org/abs/2502.14682)] [[cool](https://papers.cool/arxiv/2502.14682)] [[pdf](https://arxiv.org/pdf/2502.14682)]
> **Authors**: Yonghui Kong,Hongbing Hu,Dan Zhang,Siyuan Chai,Fan Zhang,Wei Wang
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models have demonstrated excellent performance in many tasks, including Text-to-SQL, due to their powerful in-context learning capabilities. They are becoming the mainstream approach for Text-to-SQL. However, these methods still have a significant gap compared to human performance, especially on complex questions. As the complexity of questions increases, the gap between questions and SQLs increases. We identify two important gaps: the structural mapping gap and the lexical mapping gap. To tackle these two gaps, we propose PAS-SQL, an efficient SQL generation pipeline based on LLMs, which alleviates gaps through Abstract Query Pattern (AQP) and Contextual Schema Markup (CSM). AQP aims to obtain the structural pattern of the question by removing database-related information, which enables us to find structurally similar demonstrations. CSM aims to associate database-related text span in the question with specific tables or columns in the database, which alleviates the lexical mapping gap. Experimental results on the Spider and BIRD datasets demonstrate the effectiveness of our proposed method. Specifically, PAS-SQL + GPT-4o sets a new state-of-the-art on the Spider benchmark with an execution accuracy of 87.9\%, and achieves leading results on the BIRD dataset with an execution accuracy of 64.67\%.

### How to Get Your LLM to Generate Challenging Problems for Evaluation 
[[arxiv](https://arxiv.org/abs/2502.14678)] [[cool](https://papers.cool/arxiv/2502.14678)] [[pdf](https://arxiv.org/pdf/2502.14678)]
> **Authors**: Arkil Patel,Siva Reddy,Dzmitry Bahdanau
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The pace of evolution of Large Language Models (LLMs) necessitates new approaches for rigorous and comprehensive evaluation. Traditional human annotation is increasingly impracticable due to the complexities and costs involved in generating high-quality, challenging problems. In this work, we introduce CHASE, a unified framework to synthetically generate challenging problems using LLMs without human involvement. For a given task, our approach builds a hard problem in a bottom-up manner from simpler components. Moreover, our framework decomposes the generation process into independently verifiable sub-tasks, thereby ensuring a high level of quality and correctness. We implement CHASE to create evaluation benchmarks across three diverse domains: (1) document-based question answering, (2) repository-level code completion, and (3) math reasoning. The performance of state-of-the-art LLMs on these synthetic benchmarks lies in the range of 40-60% accuracy, thereby demonstrating the effectiveness of our framework at generating challenging problems. We publicly release our benchmarks and code.

### Data-Constrained Synthesis of Training Data for De-Identification 
[[arxiv](https://arxiv.org/abs/2502.14677)] [[cool](https://papers.cool/arxiv/2502.14677)] [[pdf](https://arxiv.org/pdf/2502.14677)]
> **Authors**: Thomas Vakili,Aron Henriksson,Hercules Dalianis
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: Under review
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Many sensitive domains -- such as the clinical domain -- lack widely available datasets due to privacy risks. The increasing generative capabilities of large language models (LLMs) have made synthetic datasets a viable path forward. In this study, we domain-adapt LLMs to the clinical domain and generate synthetic clinical texts that are machine-annotated with tags for personally identifiable information using capable encoder-based NER models. The synthetic corpora are then used to train synthetic NER models. The results show that training NER models using synthetic corpora incurs only a small drop in predictive performance. The limits of this process are investigated in a systematic ablation study -- using both Swedish and Spanish data. Our analysis shows that smaller datasets can be sufficient for domain-adapting LLMs for data synthesis. Instead, the effectiveness of this process is almost entirely contingent on the performance of the machine-annotating NER models trained using the original data.

### Explanations of Deep Language Models Explain Language Representations in the Brain 
[[arxiv](https://arxiv.org/abs/2502.14671)] [[cool](https://papers.cool/arxiv/2502.14671)] [[pdf](https://arxiv.org/pdf/2502.14671)]
> **Authors**: Maryam Rahimi,Yadollah Yaghoobzadeh,Mohammad Reza Daliri
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,神经元和认知
- **Abstract**: Recent advances in artificial intelligence have given rise to large language models (LLMs) that not only achieve human-like performance but also share computational principles with the brain's language processing mechanisms. While previous research has primarily focused on aligning LLMs' internal representations with neural activity, we introduce a novel approach that leverages explainable AI (XAI) methods to forge deeper connections between the two domains. Using attribution methods, we quantified how preceding words contribute to an LLM's next-word predictions and employed these explanations to predict fMRI recordings from participants listening to the same narratives. Our findings demonstrate that attribution methods robustly predict brain activity across the language network, surpassing traditional internal representations in early language areas. This alignment is hierarchical: early-layer explanations correspond to the initial stages of language processing in the brain, while later layers align with more advanced stages. Moreover, the layers more influential on LLM next-word prediction$\unicode{x2014}$those with higher attribution scores$\unicode{x2014}$exhibited stronger alignment with neural activity. This work establishes a bidirectional bridge between AI and neuroscience. First, we demonstrate that attribution methods offer a powerful lens for investigating the neural mechanisms of language comprehension, revealing how meaning emerges from preceding context. Second, we propose using brain alignment as a metric to evaluate the validity of attribution methods, providing a framework for assessing their biological plausibility.

### AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via GRPO 
[[arxiv](https://arxiv.org/abs/2502.14669)] [[cool](https://papers.cool/arxiv/2502.14669)] [[pdf](https://arxiv.org/pdf/2502.14669)]
> **Authors**: Alan Dao,Dinh Bach Vu
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models (LLMs) have demonstrated impressive capabilities in language processing, yet they often struggle with tasks requiring genuine visual spatial reasoning. In this paper, we introduce a novel two-stage training framework designed to equip standard LLMs with visual reasoning abilities for maze navigation. First, we leverage Supervised Fine Tuning (SFT) on a curated dataset of tokenized maze representations to teach the model to predict step-by-step movement commands. Next, we apply Group Relative Policy Optimization (GRPO)-a technique used in DeepSeekR1-with a carefully crafted reward function to refine the model's sequential decision-making and encourage emergent chain-of-thought behaviors. Experimental results on synthetically generated mazes show that while a baseline model fails to navigate the maze, the SFT-trained model achieves 86% accuracy, and further GRPO fine-tuning boosts accuracy to 93%. Qualitative analyses reveal that GRPO fosters more robust and self-corrective reasoning, highlighting the potential of our approach to bridge the gap between language models and visual spatial tasks. These findings offer promising implications for applications in robotics, autonomous navigation, and other domains that require integrated visual and sequential reasoning.

### InstructAgent: Building User Controllable Recommender via LLM Agent 
[[arxiv](https://arxiv.org/abs/2502.14662)] [[cool](https://papers.cool/arxiv/2502.14662)] [[pdf](https://arxiv.org/pdf/2502.14662)]
> **Authors**: Wujiang Xu,Yunxiao Shi,Zujie Liang,Xuying Ning,Kai Mei,Kun Wang,Xi Zhu,Min Xu,Yongfeng Zhang
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: WWW2025@HCRS
- **标题**: None
- **领域**: 计算语言学,信息检索
- **Abstract**: Traditional recommender systems usually take the user-platform paradigm, where users are directly exposed under the control of the platform's recommendation algorithms. However, the defect of recommendation algorithms may put users in very vulnerable positions under this paradigm. First, many sophisticated models are often designed with commercial objectives in mind, focusing on the platform's benefits, which may hinder their ability to protect and capture users' true interests. Second, these models are typically optimized using data from all users, which may overlook individual user's preferences. Due to these shortcomings, users may experience several disadvantages under the traditional user-platform direct exposure paradigm, such as lack of control over the recommender system, potential manipulation by the platform, echo chamber effects, or lack of personalization for less active users due to the dominance of active users during collaborative learning. Therefore, there is an urgent need to develop a new paradigm to protect user interests and alleviate these issues. Recently, some researchers have introduced LLM agents to simulate user behaviors, these approaches primarily aim to optimize platform-side performance, leaving core issues in recommender systems unresolved. To address these limitations, we propose a new user-agent-platform paradigm, where agent serves as the protective shield between user and recommender system that enables indirect exposure. To this end, we first construct four recommendation datasets, denoted as $\dataset$, along with user instructions for each record.

### Edit Once, Update Everywhere: A Simple Framework for Cross-Lingual Knowledge Synchronization in LLMs 
[[arxiv](https://arxiv.org/abs/2502.14645)] [[cool](https://papers.cool/arxiv/2502.14645)] [[pdf](https://arxiv.org/pdf/2502.14645)]
> **Authors**: Yuchen Wu,Liang Ding,Li Shen,Dacheng Tao
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Knowledge editing allows for efficient adaptation of large language models (LLMs) to new information or corrections without requiring full retraining. However, prior methods typically focus on either single-language editing or basic multilingual editing, failing to achieve true cross-linguistic knowledge synchronization. To address this, we present a simple and practical state-of-the-art (SOTA) recipe Cross-Lingual Knowledge Democracy Edit (X-KDE), designed to propagate knowledge from a dominant language to other languages effectively. Our X-KDE comprises two stages: (i) Cross-lingual Edition Instruction Tuning (XE-IT), which fine-tunes the model on a curated parallel dataset to modify in-scope knowledge while preserving unrelated information, and (ii) Target-language Preference Optimization (TL-PO), which applies advanced optimization techniques to ensure consistency across languages, fostering the transfer of updates. Additionally, we contribute a high-quality, cross-lingual dataset, specifically designed to enhance knowledge transfer across languages. Extensive experiments on the Bi-ZsRE and MzsRE benchmarks show that X-KDE significantly enhances cross-lingual performance, achieving an average improvement of +8.19%, while maintaining high accuracy in monolingual settings.

### LIFT: Improving Long Context Understanding of Large Language Models through Long Input Fine-Tuning 
[[arxiv](https://arxiv.org/abs/2502.14644)] [[cool](https://papers.cool/arxiv/2502.14644)] [[pdf](https://arxiv.org/pdf/2502.14644)]
> **Authors**: Yansheng Mao,Yufei Xu,Jiaqi Li,Fanxu Meng,Haotong Yang,Zilong Zheng,Xiyuan Wang,Muhan Zhang
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: arXiv admin note: text overlap with arXiv:2412.13626
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Long context understanding remains challenging for large language models due to their limited context windows. This paper presents Long Input Fine-Tuning (LIFT), a novel framework for long-context modeling that can improve the long-context performance of arbitrary (short-context) LLMs by dynamically adapting model parameters based on the long input. Importantly, LIFT, rather than endlessly extending the context window size to accommodate increasingly longer inputs in context, chooses to store and absorb the long input in parameter. By fine-tuning the long input into model parameters, LIFT allows short-context LLMs to answer questions even when the required information is not provided in the context during inference. Furthermore, to enhance LIFT performance while maintaining the original in-context learning (ICL) capabilities, we introduce Gated Memory, a specialized attention adapter that automatically balances long input memorization and ICL. We provide a comprehensive analysis of the strengths and limitations of LIFT on long context understanding, offering valuable directions for future research.

### Length-Controlled Margin-Based Preference Optimization without Reference Model 
[[arxiv](https://arxiv.org/abs/2502.14643)] [[cool](https://papers.cool/arxiv/2502.14643)] [[pdf](https://arxiv.org/pdf/2502.14643)]
> **Authors**: Gengxu Li,Tingyu Xia,Yi Chang,Yuan Wu
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Direct Preference Optimization (DPO) is a widely adopted offline algorithm for preference-based reinforcement learning from human feedback (RLHF), designed to improve training simplicity and stability by redefining reward functions. However, DPO is hindered by several limitations, including length bias, memory inefficiency, and probability degradation. To address these challenges, we propose Length-Controlled Margin-Based Preference Optimization (LMPO), a more efficient and robust alternative. LMPO introduces a uniform reference model as an upper bound for the DPO loss, enabling a more accurate approximation of the original optimization objective. Additionally, an average log-probability optimization strategy is employed to minimize discrepancies between training and inference phases. A key innovation of LMPO lies in its Length-Controlled Margin-Based loss function, integrated within the Bradley-Terry framework. This loss function regulates response length while simultaneously widening the margin between preferred and rejected outputs. By doing so, it mitigates probability degradation for both accepted and discarded responses, addressing a significant limitation of existing methods. We evaluate LMPO against state-of-the-art preference optimization techniques on two open-ended large language models, Mistral and LLaMA3, across six conditional benchmarks. Our experimental results demonstrate that LMPO effectively controls response length, reduces probability degradation, and outperforms existing approaches. The code is available at \url{https://github.com/gengxuli/LMPO}.

### How Far are LLMs from Being Our Digital Twins? A Benchmark for Persona-Based Behavior Chain Simulation 
[[arxiv](https://arxiv.org/abs/2502.14642)] [[cool](https://papers.cool/arxiv/2502.14642)] [[pdf](https://arxiv.org/pdf/2502.14642)]
> **Authors**: Rui Li,Heming Xia,Xinfeng Yuan,Qingxiu Dong,Lei Sha,Wenjie Li,Zhifang Sui
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Recently, LLMs have garnered increasing attention across academic disciplines for their potential as human digital twins, virtual proxies designed to replicate individuals and autonomously perform tasks such as decision-making, problem-solving, and reasoning on their behalf. However, current evaluations of LLMs primarily emphasize dialogue simulation while overlooking human behavior simulation, which is crucial for digital twins. To address this gap, we introduce BehaviorChain, the first benchmark for evaluating LLMs' ability to simulate continuous human behavior. BehaviorChain comprises diverse, high-quality, persona-based behavior chains, totaling 15,846 distinct behaviors across 1,001 unique personas, each with detailed history and profile metadata. For evaluation, we integrate persona metadata into LLMs and employ them to iteratively infer contextually appropriate behaviors within dynamic scenarios provided by BehaviorChain. Comprehensive evaluation results demonstrated that even state-of-the-art models struggle with accurately simulating continuous human behavior.

### NAVIG: Natural Language-guided Analysis with Vision Language Models for Image Geo-localization 
[[arxiv](https://arxiv.org/abs/2502.14638)] [[cool](https://papers.cool/arxiv/2502.14638)] [[pdf](https://arxiv.org/pdf/2502.14638)]
> **Authors**: Zheyuan Zhang,Runze Li,Tasnim Kabir,Jordan Boyd-Graber
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,计算机视觉和模式识别
- **Abstract**: Image geo-localization is the task of predicting the specific location of an image and requires complex reasoning across visual, geographical, and cultural contexts. While prior Vision Language Models (VLMs) have the best accuracy at this task, there is a dearth of high-quality datasets and models for analytical reasoning. We first create NaviClues, a high-quality dataset derived from GeoGuessr, a popular geography game, to supply examples of expert reasoning from language. Using this dataset, we present Navig, a comprehensive image geo-localization framework integrating global and fine-grained image information. By reasoning with language, Navig reduces the average distance error by 14% compared to previous state-of-the-art models while requiring fewer than 1000 training samples. Our dataset and code are available at https://github.com/SparrowZheyuan18/Navig/.

### Multi-Record Web Page Information Extraction From News Websites 
[[arxiv](https://arxiv.org/abs/2502.14625)] [[cool](https://papers.cool/arxiv/2502.14625)] [[pdf](https://arxiv.org/pdf/2502.14625)]
> **Authors**: Alexander Kustenkov,Maksim Varlamov,Alexander Yatskov
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,信息检索
- **Abstract**: In this paper, we focused on the problem of extracting information from web pages containing many records, a task of growing importance in the era of massive web data. Recently, the development of neural network methods has improved the quality of information extraction from web pages. Nevertheless, most of the research and datasets are aimed at studying detailed pages. This has left multi-record "list pages" relatively understudied, despite their widespread presence and practical significance. To address this gap, we created a large-scale, open-access dataset specifically designed for list pages. This is the first dataset for this task in the Russian language. Our dataset contains 13,120 web pages with news lists, significantly exceeding existing datasets in both scale and complexity. Our dataset contains attributes of various types, including optional and multi-valued, providing a realistic representation of real-world list pages. These features make our dataset a valuable resource for studying information extraction from pages containing many records. Furthermore, we proposed our own multi-stage information extraction methods. In this work, we explore and demonstrate several strategies for applying MarkupLM to the specific challenges of multi-record web pages. Our experiments validate the advantages of our methods. By releasing our dataset to the public, we aim to advance the field of information extraction from multi-record pages.

### Exploring RWKV for Sentence Embeddings: Layer-wise Analysis and Baseline Comparison for Semantic Similarity 
[[arxiv](https://arxiv.org/abs/2502.14620)] [[cool](https://papers.cool/arxiv/2502.14620)] [[pdf](https://arxiv.org/pdf/2502.14620)]
> **Authors**: Xinghan Pan
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 17 pages, 3 tables, preprint on ArXiV, includes detailed analysis of RWKV for semantic similarity tasks
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: This paper investigates the efficacy of RWKV, a novel language model architecture known for its linear attention mechanism, for generating sentence embeddings in a zero-shot setting. I conduct a layer-wise analysis to evaluate the semantic similarity captured by embeddings from different hidden layers of a pre-trained RWKV model. The performance is assessed on the Microsoft Research Paraphrase Corpus (MRPC) dataset using Spearman correlation and compared against a GloVe-based baseline. My results indicate that while RWKV embeddings capture some semantic relatedness, they underperform compared to the GloVe baseline in terms of Spearman correlation. I also analyze the inference time and GPU memory usage, highlighting the computational trade-offs associated with RWKV embeddings. The findings suggest that while RWKV offers potential advantages in terms of linear scaling, its zero-shot sentence embedding quality for semantic similarity tasks requires further investigation and potential task-specific fine-tuning to match or exceed simpler baselines.

### FIND: Fine-grained Information Density Guided Adaptive Retrieval-Augmented Generation for Disease Diagnosis 
[[arxiv](https://arxiv.org/abs/2502.14614)] [[cool](https://papers.cool/arxiv/2502.14614)] [[pdf](https://arxiv.org/pdf/2502.14614)]
> **Authors**: Mingyi Jia,Junwen Duan,Yan Song,Jianxin Wang
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Retrieval-Augmented Large Language Models (LLMs), which integrate external knowledge into LLMs, have shown remarkable performance in various medical domains, including clinical diagnosis. However, existing RAG methods struggle to effectively assess task difficulty to make retrieval decisions, thereby failing to meet the clinical requirements for balancing efficiency and accuracy. So in this paper, we propose FIND (\textbf{F}ine-grained \textbf{In}formation \textbf{D}ensity Guided Adaptive RAG), a novel framework that improves the reliability of RAG in disease diagnosis scenarios. FIND incorporates a fine-grained adaptive control module to determine whether retrieval is necessary based on the information density of the input. By optimizing the retrieval process and implementing a knowledge filtering module, FIND ensures that the retrieval is better suited to clinical scenarios. Experiments on three Chinese electronic medical record datasets demonstrate that FIND significantly outperforms various baseline methods, highlighting its effectiveness in clinical diagnosis tasks.

### Behavioral Analysis of Information Salience in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.14613)] [[cool](https://papers.cool/arxiv/2502.14613)] [[pdf](https://arxiv.org/pdf/2502.14613)]
> **Authors**: Jan Trienes,Jörg Schlötterer,Junyi Jessy Li,Christin Seifert
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models (LLMs) excel at text summarization, a task that requires models to select content based on its importance. However, the exact notion of salience that LLMs have internalized remains unclear. To bridge this gap, we introduce an explainable framework to systematically derive and investigate information salience in LLMs through their summarization behavior. Using length-controlled summarization as a behavioral probe into the content selection process, and tracing the answerability of Questions Under Discussion throughout, we derive a proxy for how models prioritize information. Our experiments on 13 models across four datasets reveal that LLMs have a nuanced, hierarchical notion of salience, generally consistent across model families and sizes. While models show highly consistent behavior and hence salience patterns, this notion of salience cannot be accessed through introspection, and only weakly correlates with human perceptions of information salience.

### Can LLMs Predict Citation Intent? An Experimental Analysis of In-context Learning and Fine-tuning on Open LLMs 
[[arxiv](https://arxiv.org/abs/2502.14561)] [[cool](https://papers.cool/arxiv/2502.14561)] [[pdf](https://arxiv.org/pdf/2502.14561)]
> **Authors**: Paris Koloveas,Serafeim Chatzopoulos,Thanasis Vergoulis,Christos Tryfonopoulos
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,数字图书馆
- **Abstract**: This work investigates the ability of open Large Language Models (LLMs) to predict citation intent through in-context learning and fine-tuning. Unlike traditional approaches that rely on pre-trained models like SciBERT, which require extensive domain-specific pretraining and specialized architectures, we demonstrate that general-purpose LLMs can be adapted to this task with minimal task-specific data. We evaluate twelve model variations across five prominent open LLM families using zero, one, few, and many-shot prompting to assess performance across scenarios. Our experimental study identifies the top-performing model through extensive experimentation of in-context learning-related parameters, which we fine-tune to further enhance task performance. The results highlight the strengths and limitations of LLMs in recognizing citation intents, providing valuable insights for model selection and prompt engineering. Additionally, we make our end-to-end evaluation framework and models openly available for future use.

### Multiscale Byte Language Models -- A Hierarchical Architecture for Causal Million-Length Sequence Modeling 
[[arxiv](https://arxiv.org/abs/2502.14553)] [[cool](https://papers.cool/arxiv/2502.14553)] [[pdf](https://arxiv.org/pdf/2502.14553)]
> **Authors**: Eric Egli,Matteo Manica,Jannis Born
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: Under Review
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Bytes form the basis of the digital world and thus are a promising building block for multimodal foundation models. Recently, Byte Language Models (BLMs) have emerged to overcome tokenization, yet the excessive length of bytestreams requires new architectural paradigms. Therefore, we present the Multiscale Byte Language Model (MBLM), a model-agnostic hierarchical decoder stack that allows training with context windows of $5$M bytes on single GPU in full model precision. We thoroughly examine MBLM's performance with Transformer and Mamba blocks on both unimodal and multimodal tasks. Our experiments demonstrate that hybrid architectures are efficient in handling extremely long byte sequences during training while achieving near-linear generational efficiency. To the best of our knowledge, we present the first evaluation of BLMs on visual Q\&A tasks and find that, despite serializing images and the absence of an encoder, a MBLM with pure next token prediction can match custom CNN-LSTM architectures with designated classification heads. We show that MBLMs exhibit strong adaptability in integrating diverse data representations, including pixel and image filestream bytes, underlining their potential toward omnimodal foundation models. Source code is publicly available at: https://github.com/ai4sd/multiscale-byte-lm

### LLM-based User Profile Management for Recommender System 
[[arxiv](https://arxiv.org/abs/2502.14541)] [[cool](https://papers.cool/arxiv/2502.14541)] [[pdf](https://arxiv.org/pdf/2502.14541)]
> **Authors**: Seunghwan Bang,Hwanjun Song
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: Submitted to ACL 2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The rapid advancement of Large Language Models (LLMs) has opened new opportunities in recommender systems by enabling zero-shot recommendation without conventional training. Despite their potential, most existing works rely solely on users' purchase histories, leaving significant room for improvement by incorporating user-generated textual data, such as reviews and product descriptions. Addressing this gap, we propose PURE, a novel LLM-based recommendation framework that builds and maintains evolving user profiles by systematically extracting and summarizing key information from user reviews. PURE consists of three core components: a Review Extractor for identifying user preferences and key product features, a Profile Updater for refining and updating user profiles, and a Recommender for generating personalized recommendations using the most current profile. To evaluate PURE, we introduce a continuous sequential recommendation task that reflects real-world scenarios by adding reviews over time and updating predictions incrementally. Our experimental results on Amazon datasets demonstrate that PURE outperforms existing LLM-based methods, effectively leveraging long-term user information while managing token limitations.

### LoRA-GGPO: Mitigating Double Descent in LoRA Fine-Tuning via Gradient-Guided Perturbation Optimization 
[[arxiv](https://arxiv.org/abs/2502.14538)] [[cool](https://papers.cool/arxiv/2502.14538)] [[pdf](https://arxiv.org/pdf/2502.14538)]
> **Authors**: Yupeng Chang,Chenlu Guo,Yi Chang,Yuan Wu
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models (LLMs) have achieved remarkable success in natural language processing, but their full fine-tuning remains resource-intensive. Parameter-Efficient Fine-Tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), have emerged as a practical solution by approximating parameter updates with low-rank matrices. However, LoRA often exhibits a "double descent" phenomenon during fine-tuning, where model performance degrades due to overfitting and limited expressiveness caused by low-rank constraints. To address this issue, we propose LoRA-GGPO (Gradient-Guided Perturbation Optimization), a novel method that leverages gradient and weight norms to generate targeted perturbations. By optimizing the sharpness of the loss landscape, LoRA-GGPO guides the model toward flatter minima, mitigating the double descent problem and improving generalization. Extensive experiments on natural language understanding (NLU) and generation (NLG) tasks demonstrate that LoRA-GGPO outperforms LoRA and its state-of-the-art variants. Furthermore, extended experiments specifically designed to analyze the double descent phenomenon confirm that LoRA-GGPO effectively alleviates this issue, producing more robust and generalizable models. Our work provides a robust and efficient solution for fine-tuning LLMs, with broad applicability in real-world scenarios. The code is available at https://github.com/llm172/LoRA-GGPO.

### CORBA: Contagious Recursive Blocking Attacks on Multi-Agent Systems Based on Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.14529)] [[cool](https://papers.cool/arxiv/2502.14529)] [[pdf](https://arxiv.org/pdf/2502.14529)]
> **Authors**: Zhenhong Zhou,Zherui Li,Jie Zhang,Yuanhe Zhang,Kun Wang,Yang Liu,Qing Guo
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large Language Model-based Multi-Agent Systems (LLM-MASs) have demonstrated remarkable real-world capabilities, effectively collaborating to complete complex tasks. While these systems are designed with safety mechanisms, such as rejecting harmful instructions through alignment, their security remains largely unexplored. This gap leaves LLM-MASs vulnerable to targeted disruptions. In this paper, we introduce Contagious Recursive Blocking Attacks (Corba), a novel and simple yet highly effective attack that disrupts interactions between agents within an LLM-MAS. Corba leverages two key properties: its contagious nature allows it to propagate across arbitrary network topologies, while its recursive property enables sustained depletion of computational resources. Notably, these blocking attacks often involve seemingly benign instructions, making them particularly challenging to mitigate using conventional alignment methods. We evaluate Corba on two widely-used LLM-MASs, namely, AutoGen and Camel across various topologies and commercial models. Additionally, we conduct more extensive experiments in open-ended interactive LLM-MASs, demonstrating the effectiveness of Corba in complex topology structures and open-source models. Our code is available at: https://github.com/zhrli324/Corba.

### MultiSlav: Using Cross-Lingual Knowledge Transfer to Combat the Curse of Multilinguality 
[[arxiv](https://arxiv.org/abs/2502.14509)] [[cool](https://papers.cool/arxiv/2502.14509)] [[pdf](https://arxiv.org/pdf/2502.14509)]
> **Authors**: Artur Kot,Mikołaj Koszowski,Wojciech Chojnowski,Mieszko Rutkowski,Artur Nowakowski,Kamil Guttmann,Mikołaj Pokrywka
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Does multilingual Neural Machine Translation (NMT) lead to The Curse of the Multlinguality or provides the Cross-lingual Knowledge Transfer within a language family? In this study, we explore multiple approaches for extending the available data-regime in NMT and we prove cross-lingual benefits even in 0-shot translation regime for low-resource languages. With this paper, we provide state-of-the-art open-source NMT models for translating between selected Slavic languages. We released our models on the HuggingFace Hub (https://hf.co/collections/allegro/multislav-6793d6b6419e5963e759a683) under the CC BY 4.0 license. Slavic language family comprises morphologically rich Central and Eastern European languages. Although counting hundreds of millions of native speakers, Slavic Neural Machine Translation is under-studied in our opinion. Recently, most NMT research focuses either on: high-resource languages like English, Spanish, and German - in WMT23 General Translation Task 7 out of 8 task directions are from or to English; massively multilingual models covering multiple language groups; or evaluation techniques.

### Can LLMs Simulate L2-English Dialogue? An Information-Theoretic Analysis of L1-Dependent Biases 
[[arxiv](https://arxiv.org/abs/2502.14507)] [[cool](https://papers.cool/arxiv/2502.14507)] [[pdf](https://arxiv.org/pdf/2502.14507)]
> **Authors**: Rena Gao,Xuetong Wu,Tatsuki Kuribayashi,Mingrui Ye,Siya Qi,Carsten Roever,Yuanxing Liu,Zheng Yuan,Jey Han Lau
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: This study evaluates Large Language Models' (LLMs) ability to simulate non-native-like English use observed in human second language (L2) learners interfered with by their native first language (L1). In dialogue-based interviews, we prompt LLMs to mimic L2 English learners with specific L1s (e.g., Japanese, Thai, Urdu) across seven languages, comparing their outputs to real L2 learner data. Our analysis examines L1-driven linguistic biases, such as reference word usage and avoidance behaviors, using information-theoretic and distributional density measures. Results show that modern LLMs (e.g., Qwen2.5, LLAMA3.3, DeepseekV3, GPT-4o) replicate L1-dependent patterns observed in human L2 data, with distinct influences from various languages (e.g., Japanese, Korean, and Mandarin significantly affect tense agreement, and Urdu influences noun-verb collocations). Our results reveal the potential of LLMs for L2 dialogue generation and evaluation for future educational applications.

### How Much Knowledge Can You Pack into a LoRA Adapter without Harming LLM? 
[[arxiv](https://arxiv.org/abs/2502.14502)] [[cool](https://papers.cool/arxiv/2502.14502)] [[pdf](https://arxiv.org/pdf/2502.14502)]
> **Authors**: Sergey Pletenev,Maria Marina,Daniil Moskovskiy,Vasily Konovalov,Pavel Braslavski,Alexander Panchenko,Mikhail Salnikov
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The performance of Large Language Models (LLMs) on many tasks is greatly limited by the knowledge learned during pre-training and stored in the model's parameters. Low-rank adaptation (LoRA) is a popular and efficient training technique for updating or domain-specific adaptation of LLMs. In this study, we investigate how new facts can be incorporated into the LLM using LoRA without compromising the previously learned knowledge. We fine-tuned Llama-3.1-8B-instruct using LoRA with varying amounts of new knowledge. Our experiments have shown that the best results are obtained when the training data contains a mixture of known and new facts. However, this approach is still potentially harmful because the model's performance on external question-answering benchmarks declines after such fine-tuning. When the training data is biased towards certain entities, the model tends to regress to few overrepresented answers. In addition, we found that the model becomes more confident and refuses to provide an answer in only few cases. These findings highlight the potential pitfalls of LoRA-based LLM updates and underscore the importance of training data composition and tuning parameters to balance new knowledge integration and general model capabilities.

### Towards a Perspectivist Turn in Argument Quality Assessment 
[[arxiv](https://arxiv.org/abs/2502.14501)] [[cool](https://papers.cool/arxiv/2502.14501)] [[pdf](https://arxiv.org/pdf/2502.14501)]
> **Authors**: Julia Romberg,Maximilian Maurer,Henning Wachsmuth,Gabriella Lapesa
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: Accepted to NAACL 2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The assessment of argument quality depends on well-established logical, rhetorical, and dialectical properties that are unavoidably subjective: multiple valid assessments may exist, there is no unequivocal ground truth. This aligns with recent paths in machine learning, which embrace the co-existence of different perspectives. However, this potential remains largely unexplored in NLP research on argument quality. One crucial reason seems to be the yet unexplored availability of suitable datasets. We fill this gap by conducting a systematic review of argument quality datasets. We assign them to a multi-layered categorization targeting two aspects: (a) What has been annotated: we collect the quality dimensions covered in datasets and consolidate them in an overarching taxonomy, increasing dataset comparability and interoperability. (b) Who annotated: we survey what information is given about annotators, enabling perspectivist research and grounding our recommendations for future actions. To this end, we discuss datasets suitable for developing perspectivist models (i.e., those containing individual, non-aggregated annotations), and we showcase the importance of a controlled selection of annotators in a pilot study.

### MLGym: A New Framework and Benchmark for Advancing AI Research Agents 
[[arxiv](https://arxiv.org/abs/2502.14499)] [[cool](https://papers.cool/arxiv/2502.14499)] [[pdf](https://arxiv.org/pdf/2502.14499)]
> **Authors**: Deepak Nathani,Lovish Madaan,Nicholas Roberts,Nikolay Bashlykov,Ajay Menon,Vincent Moens,Amar Budhiraja,Despoina Magka,Vladislav Vorotilov,Gaurav Chaurasia,Dieuwke Hupkes,Ricardo Silveira Cabral,Tatiana Shavrina,Jakob Foerster,Yoram Bachrach,William Yang Wang,Roberta Raileanu
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 35 pages, 12 figures, 10 tables
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: We introduce Meta MLGym and MLGym-Bench, a new framework and benchmark for evaluating and developing LLM agents on AI research tasks. This is the first Gym environment for machine learning (ML) tasks, enabling research on reinforcement learning (RL) algorithms for training such agents. MLGym-bench consists of 13 diverse and open-ended AI research tasks from diverse domains such as computer vision, natural language processing, reinforcement learning, and game theory. Solving these tasks requires real-world AI research skills such as generating new ideas and hypotheses, creating and processing data, implementing ML methods, training models, running experiments, analyzing the results, and iterating through this process to improve on a given task. We evaluate a number of frontier large language models (LLMs) on our benchmarks such as Claude-3.5-Sonnet, Llama-3.1 405B, GPT-4o, o1-preview, and Gemini-1.5 Pro. Our MLGym framework makes it easy to add new tasks, integrate and evaluate models or agents, generate synthetic data at scale, as well as develop new learning algorithms for training agents on AI research tasks. We find that current frontier models can improve on the given baselines, usually by finding better hyperparameters, but do not generate novel hypotheses, algorithms, architectures, or substantial improvements. We open-source our framework and benchmark to facilitate future research in advancing the AI research capabilities of LLM agents.

### Enhancing Language Multi-Agent Learning with Multi-Agent Credit Re-Assignment for Interactive Environment Generalization 
[[arxiv](https://arxiv.org/abs/2502.14496)] [[cool](https://papers.cool/arxiv/2502.14496)] [[pdf](https://arxiv.org/pdf/2502.14496)]
> **Authors**: Zhitao He,Zijun Liu,Peng Li,May Fung,Ming Yan,Ji Zhang,Fei Huang,Yang Liu
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 24 pages, under review
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: LLM-based agents have made significant advancements in interactive environments, such as mobile operations and web browsing, and other domains beyond computer using. Current multi-agent systems universally excel in performance, compared to single agents, but struggle with generalization across environments due to predefined roles and inadequate strategies for generalizing language agents. The challenge of achieving both strong performance and good generalization has hindered the progress of multi-agent systems for interactive environments. To address these issues, we propose CollabUIAgents, a multi-agent reinforcement learning framework with a novel multi-agent credit re-assignment (CR) strategy, assigning process rewards with LLMs rather than environment-specific rewards and learning with synthesized preference data, in order to foster generalizable, collaborative behaviors among the role-free agents' policies. Empirical results show that our framework improves both performance and cross-environment generalizability of multi-agent systems. Moreover, our 7B-parameter system achieves results on par with or exceed strong closed-source models, and the LLM that guides the CR. We also provide insights in using granular CR rewards effectively for environment generalization, and accommodating trained LLMs in multi-agent systems.

### StructFlowBench: A Structured Flow Benchmark for Multi-turn Instruction Following 
[[arxiv](https://arxiv.org/abs/2502.14494)] [[cool](https://papers.cool/arxiv/2502.14494)] [[pdf](https://arxiv.org/pdf/2502.14494)]
> **Authors**: Jinnan Li,Jinzhe Li,Yue Wang,Yi Chang,Yuan Wu
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 18 pages, 8 figures, 8 tables
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Multi-turn instruction following capability constitutes a core competency of large language models (LLMs) in real-world applications. Existing evaluation benchmarks predominantly focus on fine-grained constraint satisfaction and domain-specific capability assessment, yet overlook the crucial structural dependency between dialogue turns that distinguishes multi-turn from single-turn interactions. This structural dependency not only reflects user intent but also establishes a second dimension for instruction following evaluation beyond constraint satisfaction. To address this gap, we propose StructFlowBench, a multi-turn instruction following benchmark with structural flow modeling. The benchmark innovatively defines a structural flow framework comprising six fundamental inter-turn relationships, which not only introduces novel structural constraints for model evaluation but also serves as generation parameters for creating customized dialogue flows tailored to specific scenarios. Adopting established LLM-based automatic evaluation methodologies, we conduct systematic evaluations of 13 leading open-source and closed-source LLMs. Experimental results reveal significant deficiencies in current models' comprehension of multi-turn dialogue structures. The code is available at \url{https://github.com/MLGroupJLU/StructFlowBench}.

### NLoRA: Nyström-Initiated Low-Rank Adaptation for Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.14482)] [[cool](https://papers.cool/arxiv/2502.14482)] [[pdf](https://arxiv.org/pdf/2502.14482)]
> **Authors**: Chenlu Guo,Yuan Wu,Yi Chang
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Parameter-efficient fine-tuning (PEFT) is essential for adapting large language models (LLMs), with low-rank adaptation (LoRA) being the most popular approach. However, LoRA suffers from slow convergence, and some recent LoRA variants, such as PiSSA, primarily rely on Singular Value Decomposition (SVD) for initialization, leading to expensive computation. To mitigate these problems, we use the Nyström method, which follows a three-matrix manipulation. We first introduce StructuredLoRA (SLoRA), which investigates adding a small intermediate matrix between the low-rank matrices A and B. Secondly, we propose NyströmLoRA (NLoRA), which leverages Nyström-based initialization for SLoRA to improve its effectiveness and efficiency. Finally, we propose IntermediateTune (IntTune), which explores fine-tuning exclusively on the intermediate matrix of NLoRA to further boost LLM efficiency. We evaluate our methods on five natural language generation (NLG) tasks and eight natural language understanding (NLU) tasks. On GSM8K, SLoRA and NLoRA achieve accuracies of 56.48% and 57.70%, surpassing LoRA by 33.52% and 36.41%, with only 3.67 million additional trainable parameters. IntTune improves average NLG performance over LoRA by 7.45% while using only 1.25% of its parameters. These results demonstrate the efficiency and effectiveness of our approach in enhancing model performance with minimal parameter overhead.

### Unshackling Context Length: An Efficient Selective Attention Approach through Query-Key Compression 
[[arxiv](https://arxiv.org/abs/2502.14477)] [[cool](https://papers.cool/arxiv/2502.14477)] [[pdf](https://arxiv.org/pdf/2502.14477)]
> **Authors**: Haoyu Wang,Tong Teng,Tianyu Guo,An Xiao,Duyu Tang,Hanting Chen,Yunhe Wang
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 14 pages,2 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Handling long-context sequences efficiently remains a significant challenge in large language models (LLMs). Existing methods for token selection in sequence extrapolation either employ a permanent eviction strategy or select tokens by chunk, which may lead to the loss of critical information. We propose Efficient Selective Attention (ESA), a novel approach that extends context length by efficiently selecting the most critical tokens at the token level to compute attention. ESA reduces the computational complexity of token selection by compressing query and key vectors into lower-dimensional representations. We evaluate ESA on long sequence benchmarks with maximum lengths up to 256k using open-source LLMs with context lengths of 8k and 32k. ESA outperforms other selective attention methods, especially in tasks requiring the retrieval of multiple pieces of information, achieving comparable performance to full-attention extrapolation methods across various tasks, with superior results in certain tasks.

### Argument-Based Comparative Question Answering Evaluation Benchmark 
[[arxiv](https://arxiv.org/abs/2502.14476)] [[cool](https://papers.cool/arxiv/2502.14476)] [[pdf](https://arxiv.org/pdf/2502.14476)]
> **Authors**: Irina Nikishina,Saba Anwar,Nikolay Dolgov,Maria Manina,Daria Ignatenko,Viktor Moskvoretskii,Artem Shelmanov,Tim Baldwin,Chris Biemann
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 8 pages, 7 Tables, 13 Figures, 18 pages with Appendix
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: In this paper, we aim to solve the problems standing in the way of automatic comparative question answering. To this end, we propose an evaluation framework to assess the quality of comparative question answering summaries. We formulate 15 criteria for assessing comparative answers created using manual annotation and annotation from 6 large language models and two comparative question asnwering datasets. We perform our tests using several LLMs and manual annotation under different settings and demonstrate the constituency of both evaluations. Our results demonstrate that the Llama-3 70B Instruct model demonstrates the best results for summary evaluation, while GPT-4 is the best for answering comparative questions. All used data, code, and evaluation results are publicly available\footnote{\url{https://anonymous.4open.science/r/cqa-evaluation-benchmark-4561/README.md}}.

### Enhancing Smart Environments with Context-Aware Chatbots using Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.14469)] [[cool](https://papers.cool/arxiv/2502.14469)] [[pdf](https://arxiv.org/pdf/2502.14469)]
> **Authors**: Aurora Polo-Rodríguez,Laura Fiorini,Erika Rovini,Filippo Cavallo,Javier Medina-Quero
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 11 pages, 3 figures
- **标题**: None
- **领域**: 计算语言学,人工智能,社交和信息网络
- **Abstract**: This work presents a novel architecture for context-aware interactions within smart environments, leveraging Large Language Models (LLMs) to enhance user experiences. Our system integrates user location data obtained through UWB tags and sensor-equipped smart homes with real-time human activity recognition (HAR) to provide a comprehensive understanding of user context. This contextual information is then fed to an LLM-powered chatbot, enabling it to generate personalised interactions and recommendations based on the user's current activity and environment. This approach moves beyond traditional static chatbot interactions by dynamically adapting to the user's real-time situation. A case study conducted from a real-world dataset demonstrates the feasibility and effectiveness of our proposed architecture, showcasing its potential to create more intuitive and helpful interactions within smart homes. The results highlight the significant benefits of integrating LLM with real-time activity and location data to deliver personalised and contextually relevant user experiences.

### Optimal word order for non-causal text generation with Large Language Models: the Spanish case 
[[arxiv](https://arxiv.org/abs/2502.14451)] [[cool](https://papers.cool/arxiv/2502.14451)] [[pdf](https://arxiv.org/pdf/2502.14451)]
> **Authors**: Andrea Busto-Castiñeira,Silvia García-Méndez,Francisco de Arriba-Pérez,Francisco J. González-Castaño
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Natural Language Generation (NLG) popularity has increased owing to the progress in Large Language Models (LLMs), with zero-shot inference capabilities. However, most neural systems utilize decoder-only causal (unidirectional) transformer models, which are effective for English but may reduce the richness of languages with less strict word order, subject omission, or different relative clause attachment preferences. This is the first work that analytically addresses optimal text generation order for non-causal language models. We present a novel Viterbi algorithm-based methodology for maximum likelihood word order estimation. We analyze the non-causal most-likelihood order probability for NLG in Spanish and, then, the probability of generating the same phrases with Spanish causal NLG. This comparative analysis reveals that causal NLG prefers English-like SVO structures. We also analyze the relationship between optimal generation order and causal left-to-right generation order using Spearman's rank correlation. Our results demonstrate that the ideal order predicted by the maximum likelihood estimator is not closely related to the causal order and may be influenced by the syntactic structure of the target sentence.

### PredictaBoard: Benchmarking LLM Score Predictability 
[[arxiv](https://arxiv.org/abs/2502.14445)] [[cool](https://papers.cool/arxiv/2502.14445)] [[pdf](https://arxiv.org/pdf/2502.14445)]
> **Authors**: Lorenzo Pacchiardi,Konstantinos Voudouris,Ben Slater,Fernando Martínez-Plumed,José Hernández-Orallo,Lexin Zhou,Wout Schellaert
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Despite possessing impressive skills, Large Language Models (LLMs) often fail unpredictably, demonstrating inconsistent success in even basic common sense reasoning tasks. This unpredictability poses a significant challenge to ensuring their safe deployment, as identifying and operating within a reliable "safe zone" is essential for mitigating risks. To address this, we present PredictaBoard, a novel collaborative benchmarking framework designed to evaluate the ability of score predictors (referred to as assessors) to anticipate LLM errors on specific task instances (i.e., prompts) from existing datasets. PredictaBoard evaluates pairs of LLMs and assessors by considering the rejection rate at different tolerance errors. As such, PredictaBoard stimulates research into developing better assessors and making LLMs more predictable, not only with a higher average performance. We conduct illustrative experiments using baseline assessors and state-of-the-art LLMs. PredictaBoard highlights the critical need to evaluate predictability alongside performance, paving the way for safer AI systems where errors are not only minimised but also anticipated and effectively mitigated. Code for our benchmark can be found at https://github.com/Kinds-of-Intelligence-CFI/PredictaBoard

### Natural Language Generation 
[[arxiv](https://arxiv.org/abs/2502.14437)] [[cool](https://papers.cool/arxiv/2502.14437)] [[pdf](https://arxiv.org/pdf/2502.14437)]
> **Authors**: Ehud Reiter
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: This is a preprint of the following work: Ehud Reiter, NaturalLanguageGeneration, 2024, Springer reproduced with permission of Springer Nature Switzerland AG. The final authenticated version is available online at: http://dx.doi.org/10.1007/978-3-031-68582-8
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: This book provides a broad overview of Natural Language Generation (NLG), including technology, user requirements, evaluation, and real-world applications. The focus is on concepts and insights which hopefully will remain relevant for many years, not on the latest LLM innovations. It draws on decades of work by the author and others on NLG. The book has the following chapters: Introduction to NLG; Rule-Based NLG; Machine Learning and Neural NLG; Requirements; Evaluation; Safety, Maintenance, and Testing; and Applications. All chapters include examples and anecdotes from the author's personal experiences, and end with a Further Reading section. The book should be especially useful to people working on applied NLG, including NLG researchers, people in other fields who want to use NLG, and commercial developers. It will not however be useful to people who want to understand the latest LLM technology. There is a companion site with more information at https://ehudreiter.com/book/

### Early-Exit and Instant Confidence Translation Quality Estimation 
[[arxiv](https://arxiv.org/abs/2502.14429)] [[cool](https://papers.cool/arxiv/2502.14429)] [[pdf](https://arxiv.org/pdf/2502.14429)]
> **Authors**: Vilém Zouhar,Maike Züfle,Beni Egressy,Julius Cheng,Jan Niehues
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Quality estimation is omnipresent in machine translation, for both evaluation and generation. Unfortunately, quality estimation models are often opaque and computationally expensive, making them impractical to be part of large-scale pipelines. In this work, we tackle two connected challenges: (1) reducing the cost of quality estimation at scale, and (2) developing an inexpensive uncertainty estimation method for quality estimation. To address the latter, we introduce Instant Confidence COMET, an uncertainty-aware quality estimation model that matches the performance of previous approaches at a fraction of their costs. We extend this to Early-Exit COMET, a quality estimation model that can compute quality scores and associated confidences already at early model layers, allowing us to early-exit computations and reduce evaluation costs. We also apply our model to machine translation reranking. We combine Early-Exit COMET with an upper confidence bound bandit algorithm to find the best candidate from a large pool without having to run the full evaluation model on all candidates. In both cases (evaluation and reranking) our methods reduce the required compute by 50% with very little degradation in performance.

### Token-Level Density-Based Uncertainty Quantification Methods for Eliciting Truthfulness of Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.14427)] [[cool](https://papers.cool/arxiv/2502.14427)] [[pdf](https://arxiv.org/pdf/2502.14427)]
> **Authors**: Artem Vazhentsev,Lyudmila Rvanova,Ivan Lazichny,Alexander Panchenko,Maxim Panov,Timothy Baldwin,Artem Shelmanov
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Uncertainty quantification (UQ) is a prominent approach for eliciting truthful answers from large language models (LLMs). To date, information-based and consistency-based UQ have been the dominant UQ methods for text generation via LLMs. Density-based methods, despite being very effective for UQ in text classification with encoder-based models, have not been very successful with generative LLMs. In this work, we adapt Mahalanobis Distance (MD) - a well-established UQ technique in classification tasks - for text generation and introduce a new supervised UQ method. Our method extracts token embeddings from multiple layers of LLMs, computes MD scores for each token, and uses linear regression trained on these features to provide robust uncertainty scores. Through extensive experiments on eleven datasets, we demonstrate that our approach substantially improves over existing UQ methods, providing accurate and computationally efficient uncertainty scores for both sequence-level selective generation and claim-level fact-checking tasks. Our method also exhibits strong generalization to out-of-domain data, making it suitable for a wide range of LLM-based applications.

### A Survey on Data Contamination for Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.14425)] [[cool](https://papers.cool/arxiv/2502.14425)] [[pdf](https://arxiv.org/pdf/2502.14425)]
> **Authors**: Yuxing Cheng,Yi Chang,Yuan Wu
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Recent advancements in Large Language Models (LLMs) have demonstrated significant progress in various areas, such as text generation and code synthesis. However, the reliability of performance evaluation has come under scrutiny due to data contamination-the unintended overlap between training and test datasets. This overlap has the potential to artificially inflate model performance, as LLMs are typically trained on extensive datasets scraped from publicly available sources. These datasets often inadvertently overlap with the benchmarks used for evaluation, leading to an overestimation of the models' true generalization capabilities. In this paper, we first examine the definition and impacts of data contamination. Secondly, we review methods for contamination-free evaluation, focusing on three strategies: data updating-based methods, data rewriting-based methods, and prevention-based methods. Specifically, we highlight dynamic benchmarks and LLM-driven evaluation methods. Finally, we categorize contamination detecting methods based on model information dependency: white-Box, gray-Box, and black-Box detection approaches. Our survey highlights the requirements for more rigorous evaluation protocols and proposes future directions for addressing data contamination challenges.

### Unstructured Evidence Attribution for Long Context Query Focused Summarization 
[[arxiv](https://arxiv.org/abs/2502.14409)] [[cool](https://papers.cool/arxiv/2502.14409)] [[pdf](https://arxiv.org/pdf/2502.14409)]
> **Authors**: Dustin Wright,Zain Muhammad Mujahid,Lu Wang,Isabelle Augenstein,David Jurgens
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 24 pages; 21 figures; 5 tables
- **标题**: None
- **领域**: 计算语言学,信息检索
- **Abstract**: Large language models (LLMs) are capable of generating coherent summaries from very long contexts given a user query. Extracting and properly citing evidence spans could help improve the transparency and reliability of these summaries. At the same time, LLMs suffer from positional biases in terms of which information they understand and attend to, which could affect evidence citation. Whereas previous work has focused on evidence citation with predefined levels of granularity (e.g. sentence, paragraph, document, etc.), we propose the task of long-context query focused summarization with unstructured evidence citation. We show how existing systems struggle to generate and properly cite unstructured evidence from their context, and that evidence tends to be "lost-in-the-middle". To help mitigate this, we create the Summaries with Unstructured Evidence Text dataset (SUnsET), a synthetic dataset generated using a novel domain-agnostic pipeline which can be used as supervision to adapt LLMs to this task. We demonstrate across 5 LLMs of different sizes and 4 datasets with varying document types and lengths that LLMs adapted with SUnsET data generate more relevant and factually consistent evidence than their base models, extract evidence from more diverse locations in their context, and can generate more relevant and consistent summaries.

### Enhancing Portuguese Variety Identification with Cross-Domain Approaches 
[[arxiv](https://arxiv.org/abs/2502.14394)] [[cool](https://papers.cool/arxiv/2502.14394)] [[pdf](https://arxiv.org/pdf/2502.14394)]
> **Authors**: Hugo Sousa,Rúben Almeida,Purificação Silvano,Inês Cantante,Ricardo Campos,Alípio Jorge
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: AAAI 2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Recent advances in natural language processing have raised expectations for generative models to produce coherent text across diverse language varieties. In the particular case of the Portuguese language, the predominance of Brazilian Portuguese corpora online introduces linguistic biases in these models, limiting their applicability outside of Brazil. To address this gap and promote the creation of European Portuguese resources, we developed a cross-domain language variety identifier (LVI) to discriminate between European and Brazilian Portuguese. Motivated by the findings of our literature review, we compiled the PtBrVarId corpus, a cross-domain LVI dataset, and study the effectiveness of transformer-based LVI classifiers for cross-domain scenarios. Although this research focuses on two Portuguese varieties, our contribution can be extended to other varieties and languages. We open source the code, corpus, and models to foster further research in this task.

### Leveraging Small LLMs for Argument Mining in Education: Argument Component Identification, Classification, and Assessment 
[[arxiv](https://arxiv.org/abs/2502.14389)] [[cool](https://papers.cool/arxiv/2502.14389)] [[pdf](https://arxiv.org/pdf/2502.14389)]
> **Authors**: Lucile Favero,Juan Antonio Pérez-Ortiz,Tanja Käser,Nuria Oliver
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人机交互
- **Abstract**: Argument mining algorithms analyze the argumentative structure of essays, making them a valuable tool for enhancing education by providing targeted feedback on the students' argumentation skills. While current methods often use encoder or encoder-decoder deep learning architectures, decoder-only models remain largely unexplored, offering a promising research direction. This paper proposes leveraging open-source, small Large Language Models (LLMs) for argument mining through few-shot prompting and fine-tuning. These models' small size and open-source nature ensure accessibility, privacy, and computational efficiency, enabling schools and educators to adopt and deploy them locally. Specifically, we perform three tasks: segmentation of student essays into arguments, classification of the arguments by type, and assessment of their quality. We empirically evaluate the models on the Feedback Prize - Predicting Effective Arguments dataset of grade 6-12 students essays and demonstrate how fine-tuned small LLMs outperform baseline methods in segmenting the essays and determining the argument types while few-shot prompting yields comparable performance to that of the baselines in assessing quality. This work highlights the educational potential of small, open-source LLMs to provide real-time, personalized feedback, enhancing independent learning and writing skills while ensuring low computational cost and privacy.

### Tradutor: Building a Variety Specific Translation Model 
[[arxiv](https://arxiv.org/abs/2502.14385)] [[cool](https://papers.cool/arxiv/2502.14385)] [[pdf](https://arxiv.org/pdf/2502.14385)]
> **Authors**: Hugo Sousa,Satya Almasian,Ricardo Campos,Alípio Jorge
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: AAAI 2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Language models have become foundational to many widely used systems. However, these seemingly advantageous models are double-edged swords. While they excel in tasks related to resource-rich languages like English, they often lose the fine nuances of language forms, dialects, and varieties that are inherent to languages spoken in multiple regions of the world. Languages like European Portuguese are neglected in favor of their more popular counterpart, Brazilian Portuguese, leading to suboptimal performance in various linguistic tasks. To address this gap, we introduce the first open-source translation model specifically tailored for European Portuguese, along with a novel dataset specifically designed for this task. Results from automatic evaluations on two benchmark datasets demonstrate that our best model surpasses existing open-source translation systems for Portuguese and approaches the performance of industry-leading closed-source systems for European Portuguese. By making our dataset, models, and code publicly available, we aim to support and encourage further research, fostering advancements in the representation of underrepresented language varieties.

### Rumor Detection by Multi-task Suffix Learning based on Time-series Dual Sentiments 
[[arxiv](https://arxiv.org/abs/2502.14383)] [[cool](https://papers.cool/arxiv/2502.14383)] [[pdf](https://arxiv.org/pdf/2502.14383)]
> **Authors**: Zhiwei Liu,Kailai Yang,Eduard Hovy,Sophia Ananiadou
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: work in progress
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The widespread dissemination of rumors on social media has a significant impact on people's lives, potentially leading to public panic and fear. Rumors often evoke specific sentiments, resonating with readers and prompting sharing. To effectively detect and track rumors, it is essential to observe the fine-grained sentiments of both source and response message pairs as the rumor evolves over time. However, current rumor detection methods fail to account for this aspect. In this paper, we propose MSuf, the first multi-task suffix learning framework for rumor detection and tracking using time series dual (coupled) sentiments. MSuf includes three modules: (1) an LLM to extract sentiment intensity features and sort them chronologically; (2) a module that fuses the sorted sentiment features with their source text word embeddings to obtain an aligned embedding; (3) two hard prompts are combined with the aligned vector to perform rumor detection and sentiment analysis using one frozen LLM. MSuf effectively enhances the performance of LLMs for rumor detection with only minimal parameter fine-tuning. Evaluating MSuf on four rumor detection benchmarks, we find significant improvements compared to other emotion-based methods.

### Affinity and Diversity: A Unified Metric for Demonstration Selection via Internal Representations 
[[arxiv](https://arxiv.org/abs/2502.14380)] [[cool](https://papers.cool/arxiv/2502.14380)] [[pdf](https://arxiv.org/pdf/2502.14380)]
> **Authors**: Mariko Kato,Hakaze Cho,Yoshihiro Sakai,Naoya Inoue
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 8 pages, 10 figures
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: The performance of In-Context Learning (ICL) is highly sensitive to the selected demonstrations. Existing approaches to demonstration selection optimize different objectives, yielding inconsistent results. To address this, we propose a unified metric--affinity and diversity--that leverages ICL model's internal representations. Our experiments show that both affinity and diversity strongly correlate with test accuracies, indicating their effectiveness for demonstration selection. Moreover, we show that our proposed metrics align well with various previous works to unify the inconsistency.

### A Similarity Paradigm Through Textual Regularization Without Forgetting 
[[arxiv](https://arxiv.org/abs/2502.14376)] [[cool](https://papers.cool/arxiv/2502.14376)] [[pdf](https://arxiv.org/pdf/2502.14376)]
> **Authors**: Fangming Cui,Jan Fong,Rongfei Zeng,Xinmei Tian,Jun Yu
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,计算机视觉和模式识别
- **Abstract**: Prompt learning has emerged as a promising method for adapting pre-trained visual-language models (VLMs) to a range of downstream tasks. While optimizing the context can be effective for improving performance on specific tasks, it can often lead to poor generalization performance on unseen classes or datasets sampled from different distributions. It may be attributed to the fact that textual prompts tend to overfit downstream data distributions, leading to the forgetting of generalized knowledge derived from hand-crafted prompts. In this paper, we propose a novel method called Similarity Paradigm with Textual Regularization (SPTR) for prompt learning without forgetting. SPTR is a two-pronged design based on hand-crafted prompts that is an inseparable framework. 1) To avoid forgetting general textual knowledge, we introduce the optimal transport as a textual regularization to finely ensure approximation with hand-crafted features and tuning textual features. 2) In order to continuously unleash the general ability of multiple hand-crafted prompts, we propose a similarity paradigm for natural alignment score and adversarial alignment score to improve model robustness for generalization. Both modules share a common objective in addressing generalization issues, aiming to maximize the generalization capability derived from multiple hand-crafted prompts. Four representative tasks (i.e., non-generalization few-shot learning, base-to-novel generalization, cross-dataset generalization, domain generalization) across 11 datasets demonstrate that SPTR outperforms existing prompt learning methods.

### Entropy-UID: A Method for Optimizing Information Density 
[[arxiv](https://arxiv.org/abs/2502.14366)] [[cool](https://papers.cool/arxiv/2502.14366)] [[pdf](https://arxiv.org/pdf/2502.14366)]
> **Authors**: Xinpeng Shou
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 5pages, 1 figures, submitting to ACL 2025
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Balanced and efficient information flow is essential for optimizing language generation models. In this work, we propose Entropy-UID, a new token selection method that balances entropy and Uniform Information Density (UID) principles for enhanced efficiency of text generation. Our approach adaptively adjusts token selection by jointly minimizing entropy and surprisal, promoting more even information distribution across generated sequences. Theoretical validation demonstrates that Entropy-UID optimally reduces information spikes while maintaining fluency and coherence. The method has been evulated using information-theoretic metrics on multiple benchmark datasets, including WikiText-2, OpenWebText, and WMT. Experimental results show that Entropy-UID achieves lower surprisal and entropy variance compared to standard GPT-2 and alternative heuristics, leading to more balanced and human-like text generation. Our findings point towards the potential of leveraging information-theoretic constraints to refine token selection strategies in autoregressive language models.

### Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests 
[[arxiv](https://arxiv.org/abs/2502.14359)] [[cool](https://papers.cool/arxiv/2502.14359)] [[pdf](https://arxiv.org/pdf/2502.14359)]
> **Authors**: Filippo Momentè,Alessandro Suglia,Mario Giulianelli,Ambra Ferrari,Alexander Koller,Oliver Lemon,David Schlangen,Raquel Fernández,Raffaella Bernardi
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: We examine three evaluation paradigms: large question-answering benchmarks (e.g., MMLU and BBH), interactive games (e.g., Signalling Games or Taboo), and cognitive tests (e.g., for working memory or theory of mind). First, we investigate which of the former two-benchmarks or games-is most effective at discriminating LLMs of varying quality. Then, inspired by human cognitive assessments, we compile a suite of targeted tests that measure cognitive abilities deemed essential for effective language use, and we investigate their correlation with model performance in benchmarks and games. Our analyses reveal that interactive games are superior to standard benchmarks in discriminating models. Causal and logical reasoning correlate with both static and interactive tests, while differences emerge regarding core executive functions and social/emotional skills, which correlate more with games. We advocate the development of new interactive benchmarks and targeted cognitive tasks inspired by assessing human abilities but designed specifically for LLMs.

### Full-Step-DPO: Self-Supervised Preference Optimization with Step-wise Rewards for Mathematical Reasoning 
[[arxiv](https://arxiv.org/abs/2502.14356)] [[cool](https://papers.cool/arxiv/2502.14356)] [[pdf](https://arxiv.org/pdf/2502.14356)]
> **Authors**: Huimin Xu,Xin Mao,Feng-Lin Li,Xiaobao Wu,Wang Chen,Wei Zhang,Anh Tuan Luu
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Direct Preference Optimization (DPO) often struggles with long-chain mathematical reasoning. Existing approaches, such as Step-DPO, typically improve this by focusing on the first erroneous step in the reasoning chain. However, they overlook all other steps and rely heavily on humans or GPT-4 to identify erroneous steps. To address these issues, we propose Full-Step-DPO, a novel DPO framework tailored for mathematical reasoning. Instead of optimizing only the first erroneous step, it leverages step-wise rewards from the entire reasoning chain. This is achieved by training a self-supervised process reward model, which automatically scores each step, providing rewards while avoiding reliance on external signals. Furthermore, we introduce a novel step-wise DPO loss, which dynamically updates gradients based on these step-wise rewards. This endows stronger reasoning capabilities to language models. Extensive evaluations on both in-domain and out-of-domain mathematical reasoning benchmarks across various base language models, demonstrate that Full-Step-DPO achieves superior performance compared to state-of-the-art baselines.

### SR-LLM: Rethinking the Structured Representation in Large Language Model 
[[arxiv](https://arxiv.org/abs/2502.14352)] [[cool](https://papers.cool/arxiv/2502.14352)] [[pdf](https://arxiv.org/pdf/2502.14352)]
> **Authors**: Jiahuan Zhang,Tianheng Wang,Hanqing Wu,Ziyi Huang,Yulong Wu,Dongbai Chen,Linfeng Song,Yue Zhang,Guozheng Rao,Kaicheng Yu
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Structured representations, exemplified by Abstract Meaning Representation (AMR), have long been pivotal in computational linguistics. However, their role remains ambiguous in the Large Language Models (LLMs) era. Initial attempts to integrate structured representation into LLMs via a zero-shot setting yielded inferior performance. We hypothesize that such a decline stems from the structure information being passed into LLMs in a code format unfamiliar to LLMs' training corpora. Consequently, we propose SR-LLM, an innovative framework with two settings to explore a superior way of integrating structured representation with LLMs from training-free and training-dependent perspectives. The former integrates structural information through natural language descriptions in LLM prompts, whereas its counterpart augments the model's inference capability through fine-tuning on linguistically described structured representations. Performance improvements were observed in widely downstream datasets, with particularly notable gains of 3.17% and 12.38% in PAWS. To the best of our knowledge, this work represents the pioneering demonstration that leveraging structural representations can substantially enhance LLMs' inference capability. We hope that our work sheds light and encourages future research to enhance the reasoning and interoperability of LLMs by structure data.

### Earlier Tokens Contribute More: Learning Direct Preference Optimization From Temporal Decay Perspective 
[[arxiv](https://arxiv.org/abs/2502.14340)] [[cool](https://papers.cool/arxiv/2502.14340)] [[pdf](https://arxiv.org/pdf/2502.14340)]
> **Authors**: Ruichen Shao,Bei Li,Gangao Liu,Yang Chen,Xiang Zhou,Jingang Wang,Xunliang Cai,Peng Li
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: Accepted by ICLR 2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Direct Preference Optimization (DPO) has gained attention as an efficient alternative to reinforcement learning from human feedback (RLHF) for aligning large language models (LLMs) with human preferences. Despite its advantages, DPO suffers from a length bias, generating responses longer than those from the reference model. Existing solutions like SimPO and SamPO address this issue but uniformly treat the contribution of rewards across sequences, overlooking temporal dynamics. To this end, we propose an enhanced preference optimization method that incorporates a temporal decay factor controlled by a gamma parameter. This dynamic weighting mechanism adjusts the influence of each reward based on its position in the sequence, prioritizing earlier tokens that are more critical for alignment. By adaptively focusing on more relevant feedback, our approach mitigates overfitting to less pertinent data and remains responsive to evolving human preferences. Experimental results on several benchmarks show that our approach consistently outperforms vanilla DPO by 5.9-8.8 points on AlpacaEval 2 and 3.3-9.7 points on Arena-Hard across different model architectures and sizes. Furthermore, additional experiments on mathematical and reasoning benchmarks (MMLU, GSM8K, and MATH) confirm that our method enhances performance without compromising general capabilities. Our codebase would be available at \url{https://github.com/LotuSrc/D2PO}.

### A Survey on Feedback-based Multi-step Reasoning for Large Language Models on Mathematics 
[[arxiv](https://arxiv.org/abs/2502.14333)] [[cool](https://papers.cool/arxiv/2502.14333)] [[pdf](https://arxiv.org/pdf/2502.14333)]
> **Authors**: Ting-Ruen Wei,Haowei Liu,Xuyang Wu,Yi Fang
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Recent progress in large language models (LLM) found chain-of-thought prompting strategies to improve the reasoning ability of LLMs by encouraging problem solving through multiple steps. Therefore, subsequent research aimed to integrate the multi-step reasoning process into the LLM itself through process rewards as feedback and achieved improvements over prompting strategies. Due to the cost of step-level annotation, some turn to outcome rewards as feedback. Aside from these training-based approaches, training-free techniques leverage frozen LLMs or external tools for feedback at each step to enhance the reasoning process. With the abundance of work in mathematics due to its logical nature, we present a survey of strategies utilizing feedback at the step and outcome levels to enhance multi-step math reasoning for LLMs. As multi-step reasoning emerges a crucial component in scaling LLMs, we hope to establish its foundation for easier understanding and empower further research.

### Line Goes Up? Inherent Limitations of Benchmarks for Evaluating Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.14318)] [[cool](https://papers.cool/arxiv/2502.14318)] [[pdf](https://arxiv.org/pdf/2502.14318)]
> **Authors**: James Fodor
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 10 pages
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Large language models (LLMs) regularly demonstrate new and impressive performance on a wide range of language, knowledge, and reasoning benchmarks. Such rapid progress has led many commentators to argue that LLM general cognitive capabilities have likewise rapidly improved, with the implication that such models are becoming progressively more capable on various real-world tasks. Here I summarise theoretical and empirical considerations to challenge this narrative. I argue that inherent limitations with the benchmarking paradigm, along with specific limitations of existing benchmarks, render benchmark performance highly unsuitable as a metric for generalisable competence over cognitive tasks. I also contend that alternative methods for assessing LLM capabilities, including adversarial stimuli and interpretability techniques, have shown that LLMs do not have robust competence in many language and reasoning tasks, and often fail to learn representations which facilitate generalisable inferences. I conclude that benchmark performance should not be used as a reliable indicator of general LLM cognitive capabilities.

### ParallelComp: Parallel Long-Context Compressor for Length Extrapolation 
[[arxiv](https://arxiv.org/abs/2502.14317)] [[cool](https://papers.cool/arxiv/2502.14317)] [[pdf](https://arxiv.org/pdf/2502.14317)]
> **Authors**: Jing Xiong,Jianghan Shen,Chuanyang Zheng,Zhongwei Wan,Chenyang Zhao,Chiwun Yang,Fanghua Ye,Hongxia Yang,Lingpeng Kong,Ngai Wong
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: We will release the code soon
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Efficiently handling long contexts is crucial for large language models (LLMs). While rotary position embeddings (RoPEs) enhance length generalization, effective length extrapolation remains challenging and often requires costly fine-tuning. In contrast, recent training-free approaches suffer from the attention sink phenomenon, leading to severe performance degradation. In this paper, we introduce ParallelComp, a novel training-free method for long-context extrapolation that extends LLMs' context length from 4K to 128K while maintaining high throughput and preserving perplexity, and integrates seamlessly with Flash Attention. Our analysis offers new insights into attention biases in parallel attention mechanisms and provides practical solutions to tackle these challenges. To mitigate the attention sink issue, we propose an attention calibration strategy that reduces biases, ensuring more stable long-range attention. Additionally, we introduce a chunk eviction strategy to efficiently manage ultra-long contexts on a single A100 80GB GPU. To further enhance efficiency, we propose a parallel KV cache eviction technique, which improves chunk throughput by 1.76x, thereby achieving a 23.50x acceleration in the prefilling stage with negligible performance loss due to attention calibration. Furthermore, ParallelComp achieves 91.17% of GPT-4's performance on long-context tasks using an 8B model trained on 8K-length context, outperforming powerful closed-source models such as Claude-2 and Kimi-Chat.

### Unveiling Cultural Blind Spots: Analyzing the Limitations of mLLMs in Procedural Text Comprehension 
[[arxiv](https://arxiv.org/abs/2502.14315)] [[cool](https://papers.cool/arxiv/2502.14315)] [[pdf](https://arxiv.org/pdf/2502.14315)]
> **Authors**: Amir Hossein Yari,Fajri Koto
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Despite the impressive performance of multilingual large language models (mLLMs) in various natural language processing tasks, their ability to understand procedural texts, particularly those with culture-specific content, remains largely unexplored. Texts describing cultural procedures, including rituals, traditional craftsmanship, and social etiquette, require an inherent understanding of cultural context, presenting a significant challenge for mLLMs. In this work, we introduce CAPTex, a benchmark designed to evaluate mLLMs' ability to process and reason about culturally diverse procedural texts across multiple languages using various methodologies to assess their performance. Our findings indicate that (1) mLLMs face difficulties with culturally contextualized procedural texts, showing notable performance declines in low-resource languages, (2) model performance fluctuates across cultural domains, with some areas presenting greater difficulties, and (3) language models exhibit better performance on multiple-choice tasks within conversational frameworks compared to direct questioning. These results underscore the current limitations of mLLMs in handling culturally nuanced procedural texts and highlight the need for culturally aware benchmarks like CAPTex to enhance their adaptability and comprehension across diverse linguistic and cultural landscapes.

### MedHallu: A Comprehensive Benchmark for Detecting Medical Hallucinations in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.14302)] [[cool](https://papers.cool/arxiv/2502.14302)] [[pdf](https://arxiv.org/pdf/2502.14302)]
> **Authors**: Shrey Pandit,Jiawei Xu,Junyuan Hong,Zhangyang Wang,Tianlong Chen,Kaidi Xu,Ying Ding
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: Code and dataset are available at https://medhallu.github.io/
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Advancements in Large Language Models (LLMs) and their increasing use in medical question-answering necessitate rigorous evaluation of their reliability. A critical challenge lies in hallucination, where models generate plausible yet factually incorrect outputs. In the medical domain, this poses serious risks to patient safety and clinical decision-making. To address this, we introduce MedHallu, the first benchmark specifically designed for medical hallucination detection. MedHallu comprises 10,000 high-quality question-answer pairs derived from PubMedQA, with hallucinated answers systematically generated through a controlled pipeline. Our experiments show that state-of-the-art LLMs, including GPT-4o, Llama-3.1, and the medically fine-tuned UltraMedical, struggle with this binary hallucination detection task, with the best model achieving an F1 score as low as 0.625 for detecting "hard" category hallucinations. Using bidirectional entailment clustering, we show that harder-to-detect hallucinations are semantically closer to ground truth. Through experiments, we also show incorporating domain-specific knowledge and introducing a "not sure" category as one of the answer categories improves the precision and F1 scores by up to 38% relative to baselines.

### SEA-HELM: Southeast Asian Holistic Evaluation of Language Models 
[[arxiv](https://arxiv.org/abs/2502.14301)] [[cool](https://papers.cool/arxiv/2502.14301)] [[pdf](https://arxiv.org/pdf/2502.14301)]
> **Authors**: Yosephine Susanto,Adithya Venkatadri Hulagadri,Jann Railey Montalan,Jian Gang Ngui,Xian Bin Yong,Weiqi Leong,Hamsawardhini Rengarajan,Peerat Limkonchotiwat,Yifan Mai,William Chandra Tjhi
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: With the rapid emergence of novel capabilities in Large Language Models (LLMs), the need for rigorous multilingual and multicultural benchmarks that are integrated has become more pronounced. Though existing LLM benchmarks are capable of evaluating specific capabilities of LLMs in English as well as in various mid- to low-resource languages, including those in the Southeast Asian (SEA) region, a comprehensive and authentic evaluation suite for the SEA languages has not been developed thus far. Here, we present SEA-HELM, a holistic linguistic and cultural LLM evaluation suite that emphasizes SEA languages, comprising five core pillars: (1) NLP Classics, (2) LLM-specifics, (3) SEA Linguistics, (4) SEA Culture, (5) Safety. SEA-HELM currently supports Filipino, Indonesian, Tamil, Thai, and Vietnamese. We also introduce the SEA-HELM leaderboard, which allows users to understand models' multilingual and multicultural performance in a systematic and user-friendly manner.

### Drift: Decoding-time Personalized Alignments with Implicit User Preferences 
[[arxiv](https://arxiv.org/abs/2502.14289)] [[cool](https://papers.cool/arxiv/2502.14289)] [[pdf](https://arxiv.org/pdf/2502.14289)]
> **Authors**: Minbeom Kim,Kang-il Lee,Seongho Joo,Hwaran Lee,Kyomin Jung
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 19 pages, 6 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Personalized alignments for individual users have been a long-standing goal in large language models (LLMs). We introduce Drift, a novel framework that personalizes LLMs at decoding time with implicit user preferences. Traditional Reinforcement Learning from Human Feedback (RLHF) requires thousands of annotated examples and expensive gradient updates. In contrast, Drift personalizes LLMs in a training-free manner, using only a few dozen examples to steer a frozen model through efficient preference modeling. Our approach models user preferences as a composition of predefined, interpretable attributes and aligns them at decoding time to enable personalized generation. Experiments on both a synthetic persona dataset (Perspective) and a real human-annotated dataset (PRISM) demonstrate that Drift significantly outperforms RLHF baselines while using only 50-100 examples. Our results and analysis show that Drift is both computationally efficient and interpretable.

### Vulnerability of Text-to-Image Models to Prompt Template Stealing: A Differential Evolution Approach 
[[arxiv](https://arxiv.org/abs/2502.14285)] [[cool](https://papers.cool/arxiv/2502.14285)] [[pdf](https://arxiv.org/pdf/2502.14285)]
> **Authors**: Yurong Wu,Fangwen Mu,Qiuhong Zhang,Jinjing Zhao,Xinrun Xu,Lingrui Mei,Yang Wu,Lin Shi,Junjie Wang,Zhiming Ding,Yiwei Wang
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 14 pages,8 figures,4 tables
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Prompt trading has emerged as a significant intellectual property concern in recent years, where vendors entice users by showcasing sample images before selling prompt templates that can generate similar images. This work investigates a critical security vulnerability: attackers can steal prompt templates using only a limited number of sample images. To investigate this threat, we introduce Prism, a prompt-stealing benchmark consisting of 50 templates and 450 images, organized into Easy and Hard difficulty levels. To identify the vulnerabity of VLMs to prompt stealing, we propose EvoStealer, a novel template stealing method that operates without model fine-tuning by leveraging differential evolution algorithms. The system first initializes population sets using multimodal large language models (MLLMs) based on predefined patterns, then iteratively generates enhanced offspring through MLLMs. During evolution, EvoStealer identifies common features across offspring to derive generalized templates. Our comprehensive evaluation conducted across open-source (INTERNVL2-26B) and closed-source models (GPT-4o and GPT-4o-mini) demonstrates that EvoStealer's stolen templates can reproduce images highly similar to originals and effectively generalize to other subjects, significantly outperforming baseline methods with an average improvement of over 10%. Moreover, our cost analysis reveals that EvoStealer achieves template stealing with negligible computational expenses. Our code and dataset are available at https://github.com/whitepagewu/evostealer.

### EpMAN: Episodic Memory AttentioN for Generalizing to Longer Contexts 
[[arxiv](https://arxiv.org/abs/2502.14280)] [[cool](https://papers.cool/arxiv/2502.14280)] [[pdf](https://arxiv.org/pdf/2502.14280)]
> **Authors**: Subhajit Chaudhury,Payel Das,Sarathkrishna Swaminathan,Georgios Kollias,Elliot Nelson,Khushbu Pahwa,Tejaswini Pedapati,Igor Melnyk,Matthew Riemer
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Recent advances in Large Language Models (LLMs) have yielded impressive successes on many language tasks. However, efficient processing of long contexts using LLMs remains a significant challenge. We introduce \textbf{EpMAN} -- a method for processing long contexts in an \textit{episodic memory} module while \textit{holistically attending to} semantically relevant context chunks. The output of \textit{episodic attention} is then used to reweigh the decoder's self-attention to the stored KV cache of the context during training and generation. When an LLM decoder is trained using \textbf{EpMAN}, its performance on multiple challenging single-hop long-context recall and question-answering benchmarks is found to be stronger and more robust across the range from 16k to 256k tokens than baseline decoders trained with self-attention, and popular retrieval-augmented generation frameworks.

### Fact or Guesswork? Evaluating Large Language Model's Medical Knowledge with Structured One-Hop Judgment 
[[arxiv](https://arxiv.org/abs/2502.14275)] [[cool](https://papers.cool/arxiv/2502.14275)] [[pdf](https://arxiv.org/pdf/2502.14275)]
> **Authors**: Jiaxi Li,Yiwei Wang,Kai Zhang,Yujun Cai,Bryan Hooi,Nanyun Peng,Kai-Wei Chang,Jin Lu
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 15 pages, 11 figures
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Large language models (LLMs) have been widely adopted in various downstream task domains. However, their ability to directly recall and apply factual medical knowledge remains under-explored. Most existing medical QA benchmarks assess complex reasoning or multi-hop inference, making it difficult to isolate LLMs' inherent medical knowledge from their reasoning capabilities. Given the high-stakes nature of medical applications, where incorrect information can have critical consequences, it is essential to evaluate how well LLMs encode, retain, and recall fundamental medical facts. To bridge this gap, we introduce the Medical Knowledge Judgment, a dataset specifically designed to measure LLMs' one-hop factual medical knowledge. MKJ is constructed from the Unified Medical Language System (UMLS), a large-scale repository of standardized biomedical vocabularies and knowledge graphs. We frame knowledge assessment as a binary judgment task, requiring LLMs to verify the correctness of medical statements extracted from reliable and structured knowledge sources. Our experiments reveal that LLMs struggle with factual medical knowledge retention, exhibiting significant performance variance across different semantic categories, particularly for rare medical conditions. Furthermore, LLMs show poor calibration, often being overconfident in incorrect answers. To mitigate these issues, we explore retrieval-augmented generation, demonstrating its effectiveness in improving factual accuracy and reducing uncertainty in medical decision-making.

### Capturing Nuanced Preferences: Preference-Aligned Distillation for Small Language Models 
[[arxiv](https://arxiv.org/abs/2502.14272)] [[cool](https://papers.cool/arxiv/2502.14272)] [[pdf](https://arxiv.org/pdf/2502.14272)]
> **Authors**: Yanggan Gu,Junzhuo Li,Sirui Huang,Xin Zou,Zhenghua Li,Xuming Hu
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: Under review
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Aligning small language models (SLMs) with human values typically involves distilling preference knowledge from large language models (LLMs). However, existing distillation methods model preference knowledge in teacher LLMs by comparing pairwise responses, overlooking the extent of difference between responses. This limitation hinders student SLMs from capturing the nuanced preferences for multiple responses. In this paper, we propose a Preference-Aligned Distillation (PAD) framework, which models teacher's preference knowledge as a probability distribution over all potential preferences, thereby providing more nuanced supervisory signals. Our insight in developing PAD is rooted in the demonstration that language models can serve as reward functions, reflecting their intrinsic preferences. Based on this, PAD comprises three key steps: (1) sampling diverse responses using high-temperature; (2) computing rewards for both teacher and student to construct their intrinsic preference; and (3) training the student's intrinsic preference distribution to align with the teacher's. Experiments on four mainstream alignment benchmarks demonstrate that PAD consistently and significantly outperforms existing approaches, achieving over 20\% improvement on AlpacaEval 2 and Arena-Hard, indicating superior alignment with human preferences. Notably, on MT-Bench, using the \textsc{Gemma} model family, the student trained by PAD surpasses its teacher, further validating the effectiveness of our PAD.

### PaperHelper: Knowledge-Based LLM QA Paper Reading Assistant 
[[arxiv](https://arxiv.org/abs/2502.14271)] [[cool](https://papers.cool/arxiv/2502.14271)] [[pdf](https://arxiv.org/pdf/2502.14271)]
> **Authors**: Congrui Yin,Evan Wei,Zhongxing Zhang,Zaifu Zhan
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: In the paper, we introduce a paper reading assistant, PaperHelper, a potent tool designed to enhance the capabilities of researchers in efficiently browsing and understanding scientific literature. Utilizing the Retrieval-Augmented Generation (RAG) framework, PaperHelper effectively minimizes hallucinations commonly encountered in large language models (LLMs), optimizing the extraction of accurate, high-quality knowledge. The implementation of advanced technologies such as RAFT and RAG Fusion significantly boosts the performance, accuracy, and reliability of the LLMs-based literature review process. Additionally, PaperHelper features a user-friendly interface that facilitates the batch downloading of documents and uses the Mermaid format to illustrate structural relationships between documents. Experimental results demonstrate that PaperHelper, based on a fine-tuned GPT-4 API, achieves an F1 Score of 60.04, with a latency of only 5.8 seconds, outperforming the basic RAG model by 7\% in F1 Score.

### MCQA-Eval: Efficient Confidence Evaluation in NLG with Gold-Standard Correctness Labels 
[[arxiv](https://arxiv.org/abs/2502.14268)] [[cool](https://papers.cool/arxiv/2502.14268)] [[pdf](https://arxiv.org/pdf/2502.14268)]
> **Authors**: Xiaoou Liu,Zhen Lin,Longchao Da,Chacha Chen,Shubhendu Trivedi,Hua Wei
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large Language Models (LLMs) require robust confidence estimation, particularly in critical domains like healthcare and law where unreliable outputs can lead to significant consequences. Despite much recent work in confidence estimation, current evaluation frameworks rely on correctness functions -- various heuristics that are often noisy, expensive, and possibly introduce systematic biases. These methodological weaknesses tend to distort evaluation metrics and thus the comparative ranking of confidence measures. We introduce MCQA-Eval, an evaluation framework for assessing confidence measures in Natural Language Generation (NLG) that eliminates dependence on an explicit correctness function by leveraging gold-standard correctness labels from multiple-choice datasets. MCQA-Eval enables systematic comparison of both internal state-based white-box (e.g. logit-based) and consistency-based black-box confidence measures, providing a unified evaluation methodology across different approaches. Through extensive experiments on multiple LLMs and widely used QA datasets, we report that MCQA-Eval provides efficient and more reliable assessments of confidence estimation methods than existing approaches.

## 密码学和安全(cs.CR:Cryptography and Security)

### Graph in the Vault: Protecting Edge GNN Inference with Trusted Execution Environment 
[[arxiv](https://arxiv.org/abs/2502.15012)] [[cool](https://papers.cool/arxiv/2502.15012)] [[pdf](https://arxiv.org/pdf/2502.15012)]
> **Authors**: Ruyi Ding,Tianhong Xu,Aidong Adam Ding,Yunsi Fei
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: This work is accepted by DAC 2025
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Wide deployment of machine learning models on edge devices has rendered the model intellectual property (IP) and data privacy vulnerable. We propose GNNVault, the first secure Graph Neural Network (GNN) deployment strategy based on Trusted Execution Environment (TEE). GNNVault follows the design of 'partition-before-training' and includes a private GNN rectifier to complement with a public backbone model. This way, both critical GNN model parameters and the private graph used during inference are protected within secure TEE compartments. Real-world implementations with Intel SGX demonstrate that GNNVault safeguards GNN inference against state-of-the-art link stealing attacks with negligible accuracy degradation (<2%).

### CyberSentinel: An Emergent Threat Detection System for AI Security 
[[arxiv](https://arxiv.org/abs/2502.14966)] [[cool](https://papers.cool/arxiv/2502.14966)] [[pdf](https://arxiv.org/pdf/2502.14966)]
> **Authors**: Krti Tallam
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: The rapid advancement of artificial intelligence (AI) has significantly expanded the attack surface for AI-driven cybersecurity threats, necessitating adaptive defense strategies. This paper introduces CyberSentinel, a unified, single-agent system for emergent threat detection, designed to identify and mitigate novel security risks in real time. CyberSentinel integrates: (1) Brute-force attack detection through SSH log analysis, (2) Phishing threat assessment using domain blacklists and heuristic URL scoring, and (3) Emergent threat detection via machine learning-based anomaly detection. By continuously adapting to evolving adversarial tactics, CyberSentinel strengthens proactive cybersecurity defense, addressing critical vulnerabilities in AI security.

### A Survey of Safety on Large Vision-Language Models: Attacks, Defenses and Evaluations 
[[arxiv](https://arxiv.org/abs/2502.14881)] [[cool](https://papers.cool/arxiv/2502.14881)] [[pdf](https://arxiv.org/pdf/2502.14881)]
> **Authors**: Mang Ye,Xuankun Rong,Wenke Huang,Bo Du,Nenghai Yu,Dacheng Tao
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-21
> **comment**: 22 pages, 2 figures
- **标题**: None
- **领域**: 密码学和安全,计算机视觉和模式识别
- **Abstract**: With the rapid advancement of Large Vision-Language Models (LVLMs), ensuring their safety has emerged as a crucial area of research. This survey provides a comprehensive analysis of LVLM safety, covering key aspects such as attacks, defenses, and evaluation methods. We introduce a unified framework that integrates these interrelated components, offering a holistic perspective on the vulnerabilities of LVLMs and the corresponding mitigation strategies. Through an analysis of the LVLM lifecycle, we introduce a classification framework that distinguishes between inference and training phases, with further subcategories to provide deeper insights. Furthermore, we highlight limitations in existing research and outline future directions aimed at strengthening the robustness of LVLMs. As part of our research, we conduct a set of safety evaluations on the latest LVLM, Deepseek Janus-Pro, and provide a theoretical analysis of the results. Our findings provide strategic recommendations for advancing LVLM safety and ensuring their secure and reliable deployment in high-stakes, real-world applications. This survey aims to serve as a cornerstone for future research, facilitating the development of models that not only push the boundaries of multimodal intelligence but also adhere to the highest standards of security and ethical integrity. Furthermore, to aid the growing research in this field, we have created a public repository to continuously compile and update the latest work on LVLM safety: https://github.com/XuankunRong/Awesome-LVLM-Safety .

### How Jailbreak Defenses Work and Ensemble? A Mechanistic Investigation 
[[arxiv](https://arxiv.org/abs/2502.14486)] [[cool](https://papers.cool/arxiv/2502.14486)] [[pdf](https://arxiv.org/pdf/2502.14486)]
> **Authors**: Zhuohang Long,Siyuan Wang,Shujun Liu,Yuhang Lai,Xuanjing Huang,Zhongyu Wei
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能,计算语言学
- **Abstract**: Jailbreak attacks, where harmful prompts bypass generative models' built-in safety, raise serious concerns about model vulnerability. While many defense methods have been proposed, the trade-offs between safety and helpfulness, and their application to Large Vision-Language Models (LVLMs), are not well understood. This paper systematically examines jailbreak defenses by reframing the standard generation task as a binary classification problem to assess model refusal tendencies for both harmful and benign queries. We identify two key defense mechanisms: safety shift, which increases refusal rates across all queries, and harmfulness discrimination, which improves the model's ability to distinguish between harmful and benign inputs. Using these mechanisms, we develop two ensemble defense strategies-inter-mechanism ensembles and intra-mechanism ensembles-to balance safety and helpfulness. Experiments on the MM-SafetyBench and MOSSBench datasets with LLaVA-1.5 models show that these strategies effectively improve model safety or optimize the trade-off between safety and helpfulness.

### μRL: Discovering Transient Execution Vulnerabilities Using Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.14307)] [[cool](https://papers.cool/arxiv/2502.14307)] [[pdf](https://arxiv.org/pdf/2502.14307)]
> **Authors**: M. Caner Tol,Kemal Derya,Berk Sunar
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,硬件架构,机器学习
- **Abstract**: We propose using reinforcement learning to address the challenges of discovering microarchitectural vulnerabilities, such as Spectre and Meltdown, which exploit subtle interactions in modern processors. Traditional methods like random fuzzing fail to efficiently explore the vast instruction space and often miss vulnerabilities that manifest under specific conditions. To overcome this, we introduce an intelligent, feedback-driven approach using RL. Our RL agents interact with the processor, learning from real-time feedback to prioritize instruction sequences more likely to reveal vulnerabilities, significantly improving the efficiency of the discovery process. We also demonstrate that RL systems adapt effectively to various microarchitectures, providing a scalable solution across processor generations. By automating the exploration process, we reduce the need for human intervention, enabling continuous learning that uncovers hidden vulnerabilities. Additionally, our approach detects subtle signals, such as timing anomalies or unusual cache behavior, that may indicate microarchitectural weaknesses. This proposal advances hardware security testing by introducing a more efficient, adaptive, and systematic framework for protecting modern processors. When unleashed on Intel Skylake-X and Raptor Lake microarchitectures, our RL agent was indeed able to generate instruction sequences that cause significant observable byte leakages through transient execution without generating any $μ$code assists, faults or interrupts. The newly identified leaky sequences stem from a variety of Intel instructions, e.g. including SERIALIZE, VERR/VERW, CLMUL, MMX-x87 transitions, LSL+RDSCP and LAR. These initial results give credence to the proposed approach.

## 计算机视觉和模式识别(cs.CV:Computer Vision and Pattern Recognition)

### Image Translation-Based Unsupervised Cross-Modality Domain Adaptation for Medical Image Segmentation 
[[arxiv](https://arxiv.org/abs/2502.15193)] [[cool](https://papers.cool/arxiv/2502.15193)] [[pdf](https://arxiv.org/pdf/2502.15193)]
> **Authors**: Tao Yang,Lisheng Wang
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 5 pages, 1 figure
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Supervised deep learning usually faces more challenges in medical images than in natural images. Since annotations in medical images require the expertise of doctors and are more time-consuming and expensive. Thus, some researchers turn to unsupervised learning methods, which usually face inevitable performance drops. In addition, medical images may have been acquired at different medical centers with different scanners and under different image acquisition protocols, so the modalities of the medical images are often inconsistent. This modality difference (domain shift) also reduces the applicability of deep learning methods. In this regard, we propose an unsupervised crossmodality domain adaptation method based on image translation by transforming the source modality image with annotation into the unannotated target modality and using its annotation to achieve supervised learning of the target modality. In addition, the subtle differences between translated pseudo images and real images are overcome by self-training methods to further improve the task performance of deep learning. The proposed method showed mean Dice Similarity Coefficient (DSC) and Average Symmetric Surface Distance (ASSD) of $0.8351 \pm 0.1152$ and $1.6712 \pm 2.1948$ for vestibular schwannoma (VS), $0.8098 \pm 0.0233$ and $0.2317 \pm 0.1577$ for cochlea on the VS and cochlea segmentation task of the Cross-Modality Domain Adaptation (crossMoDA 2022) challenge validation phase leaderboard.

### Hierarchical Context Transformer for Multi-level Semantic Scene Understanding 
[[arxiv](https://arxiv.org/abs/2502.15184)] [[cool](https://papers.cool/arxiv/2502.15184)] [[pdf](https://arxiv.org/pdf/2502.15184)]
> **Authors**: Luoying Hao,Yan Hu,Yang Yue,Li Wu,Huazhu Fu,Jinming Duan,Jiang Liu
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: This paper has been accepted by the IEEE TCSVT
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: A comprehensive and explicit understanding of surgical scenes plays a vital role in developing context-aware computer-assisted systems in the operating theatre. However, few works provide systematical analysis to enable hierarchical surgical scene understanding. In this work, we propose to represent the tasks set [phase recognition --> step recognition --> action and instrument detection] as multi-level semantic scene understanding (MSSU). For this target, we propose a novel hierarchical context transformer (HCT) network and thoroughly explore the relations across the different level tasks. Specifically, a hierarchical relation aggregation module (HRAM) is designed to concurrently relate entries inside multi-level interaction information and then augment task-specific features. To further boost the representation learning of the different tasks, inter-task contrastive learning (ICL) is presented to guide the model to learn task-wise features via absorbing complementary information from other tasks. Furthermore, considering the computational costs of the transformer, we propose HCT+ to integrate the spatial and temporal adapter to access competitive performance on substantially fewer tunable parameters. Extensive experiments on our cataract dataset and a publicly available endoscopic PSI-AVA dataset demonstrate the outstanding performance of our method, consistently exceeding the state-of-the-art methods by a large margin. The code is available at https://github.com/Aurora-hao/HCT.

### Methods and Trends in Detecting Generated Images: A Comprehensive Review 
[[arxiv](https://arxiv.org/abs/2502.15176)] [[cool](https://papers.cool/arxiv/2502.15176)] [[pdf](https://arxiv.org/pdf/2502.15176)]
> **Authors**: Arpan Mahara,Naphtali Rishe
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 30 pages, 4 Figures, 10 Tables
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: The proliferation of generative models, such as Generative Adversarial Networks (GANs), Diffusion Models, and Variational Autoencoders (VAEs), has enabled the synthesis of high-quality multimedia data. However, these advancements have also raised significant concerns regarding adversarial attacks, unethical usage, and societal harm. Recognizing these challenges, researchers have increasingly focused on developing methodologies to detect synthesized data effectively, aiming to mitigate potential risks. Prior reviews have primarily focused on deepfake detection and often lack coverage of recent advancements in synthetic image detection, particularly methods leveraging multimodal frameworks for improved forensic analysis. To address this gap, the present survey provides a comprehensive review of state-of-the-art methods for detecting and classifying synthetic images generated by advanced generative AI models. This review systematically examines core detection methodologies, identifies commonalities among approaches, and categorizes them into meaningful taxonomies. Furthermore, given the crucial role of large-scale datasets in this field, we present an overview of publicly available datasets that facilitate further research and benchmarking in synthetic data detection.

### M3-AGIQA: Multimodal, Multi-Round, Multi-Aspect AI-Generated Image Quality Assessment 
[[arxiv](https://arxiv.org/abs/2502.15167)] [[cool](https://papers.cool/arxiv/2502.15167)] [[pdf](https://arxiv.org/pdf/2502.15167)]
> **Authors**: Chuan Cui,Kejiang Chen,Zhihua Wei,Wen Shen,Weiming Zhang,Nenghai Yu
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 14 pages, 5 figures. This work has been submitted to the IEEE for possible publication
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: The rapid advancement of AI-generated image (AGI) models has introduced significant challenges in evaluating their quality, which requires considering multiple dimensions such as perceptual quality, prompt correspondence, and authenticity. To address these challenges, we propose M3-AGIQA, a comprehensive framework for AGI quality assessment that is Multimodal, Multi-Round, and Multi-Aspect. Our approach leverages the capabilities of Multimodal Large Language Models (MLLMs) as joint text and image encoders and distills advanced captioning capabilities from online MLLMs into a local model via Low-Rank Adaptation (LoRA) fine-tuning. The framework includes a structured multi-round evaluation mechanism, where intermediate image descriptions are generated to provide deeper insights into the quality, correspondence, and authenticity aspects. To align predictions with human perceptual judgments, a predictor constructed by an xLSTM and a regression head is incorporated to process sequential logits and predict Mean Opinion Scores (MOSs). Extensive experiments conducted on multiple benchmark datasets demonstrate that M3-AGIQA achieves state-of-the-art performance, effectively capturing nuanced aspects of AGI quality. Furthermore, cross-dataset validation confirms its strong generalizability. The code is available at https://github.com/strawhatboy/M3-AGIQA.

### TransMamba: Fast Universal Architecture Adaption from Transformers to Mamba 
[[arxiv](https://arxiv.org/abs/2502.15130)] [[cool](https://papers.cool/arxiv/2502.15130)] [[pdf](https://arxiv.org/pdf/2502.15130)]
> **Authors**: Xiuwei Chen,Sihao Lin,Xiao Dong,Zisheng Chen,Meng Cao,Jianhua Han,Hang Xu,Xiaodan Liang
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Transformers have been favored in both uni-modal and multi-modal foundation models for their flexible scalability in attention modules. Consequently, a number of pre-trained Transformer models, e.g., LLaVA, CLIP, and DEIT, are publicly available. Recent research has introduced subquadratic architectures like Mamba, which enables global awareness with linear complexity. Nevertheless, training specialized subquadratic architectures from scratch for certain tasks is both resource-intensive and time-consuming. As a motivator, we explore cross-architecture training to transfer the ready knowledge in existing Transformer models to alternative architecture Mamba, termed TransMamba. Our approach employs a two-stage strategy to expedite training new Mamba models, ensuring effectiveness in across uni-modal and cross-modal tasks. Concerning architecture disparities, we project the intermediate features into an aligned latent space before transferring knowledge. On top of that, a Weight Subcloning and Adaptive Bidirectional distillation method (WSAB) is introduced for knowledge transfer without limitations on varying layer counts. For cross-modal learning, we propose a cross-Mamba module that integrates language awareness into Mamba's visual features, enhancing the cross-modal interaction capabilities of Mamba architecture. Despite using less than 75% of the training data typically required for training from scratch, TransMamba boasts substantially stronger performance across various network architectures and downstream tasks, including image classification, visual question answering, and text-video retrieval. The code will be publicly available.

### DAM-Seg: Anatomically accurate cardiac segmentation using Dense Associative Networks 
[[arxiv](https://arxiv.org/abs/2502.15128)] [[cool](https://papers.cool/arxiv/2502.15128)] [[pdf](https://arxiv.org/pdf/2502.15128)]
> **Authors**: Zahid Ullah,Jihie Kim
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 12 pages, 7 figures, 5 tables
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Deep learning-based cardiac segmentation has seen significant advancements over the years. Many studies have tackled the challenge of anatomically incorrect segmentation predictions by introducing auxiliary modules. These modules either post-process segmentation outputs or enforce consistency between specific points to ensure anatomical correctness. However, such approaches often increase network complexity, require separate training for these modules, and may lack robustness in scenarios with poor visibility. To address these limitations, we propose a novel transformer-based architecture that leverages dense associative networks to learn and retain specific patterns inherent to cardiac inputs. Unlike traditional methods, our approach restricts the network to memorize a limited set of patterns. During forward propagation, a weighted sum of these patterns is used to enforce anatomical correctness in the output. Since these patterns are input-independent, the model demonstrates enhanced robustness, even in cases with poor visibility. The proposed pipeline was evaluated on two publicly available datasets, CAMUS and CardiacNet. Experimental results indicate that our model consistently outperforms baseline approaches across all metrics, highlighting its effectiveness and reliability for cardiac segmentation tasks.

### Can Hallucination Correction Improve Video-Language Alignment? 
[[arxiv](https://arxiv.org/abs/2502.15079)] [[cool](https://papers.cool/arxiv/2502.15079)] [[pdf](https://arxiv.org/pdf/2502.15079)]
> **Authors**: Lingjun Zhao,Mingyang Xie,Paola Cascante-Bonilla,Hal Daumé III,Kwonjoon Lee
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **Abstract**: Large Vision-Language Models often generate hallucinated content that is not grounded in its visual inputs. While prior work focuses on mitigating hallucinations, we instead explore leveraging hallucination correction as a training objective to improve video-language alignment. We introduce HACA, a self-training framework learning to correct hallucinations in descriptions that do not align with the video content. By identifying and correcting inconsistencies, HACA enhances the model's ability to align video and textual representations for spatio-temporal reasoning. Our experimental results show consistent gains in video-caption binding and text-to-video retrieval tasks, demonstrating that hallucination correction-inspired tasks serve as an effective strategy for improving vision and language alignment.

### Hardware-Friendly Static Quantization Method for Video Diffusion Transformers 
[[arxiv](https://arxiv.org/abs/2502.15077)] [[cool](https://papers.cool/arxiv/2502.15077)] [[pdf](https://arxiv.org/pdf/2502.15077)]
> **Authors**: Sanghyun Yi,Qingfeng Liu,Mostafa El-Khamy
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Diffusion Transformers for video generation have gained significant research interest since the impressive performance of SORA. Efficient deployment of such generative-AI models on GPUs has been demonstrated with dynamic quantization. However, resource-constrained devices cannot support dynamic quantization, and need static quantization of the models for their efficient deployment on AI processors. In this paper, we propose a novel method for the post-training quantization of OpenSora\cite{opensora}, a Video Diffusion Transformer, without relying on dynamic quantization techniques. Our approach employs static quantization, achieving video quality comparable to FP16 and dynamically quantized ViDiT-Q methods, as measured by CLIP, and VQA metrics. In particular, we utilize per-step calibration data to adequately provide a post-training statically quantized model for each time step, incorporating channel-wise quantization for weights and tensor-wise quantization for activations. By further applying the smooth-quantization technique, we can obtain high-quality video outputs with the statically quantized models. Extensive experimental results demonstrate that static quantization can be a viable alternative to dynamic quantization for video diffusion transformers, offering a more efficient approach without sacrificing performance.

### Simpler Fast Vision Transformers with a Jumbo CLS Token 
[[arxiv](https://arxiv.org/abs/2502.15021)] [[cool](https://papers.cool/arxiv/2502.15021)] [[pdf](https://arxiv.org/pdf/2502.15021)]
> **Authors**: Anthony Fuller,Yousef Yassin,Daniel G. Kyrollos,Evan Shelhamer,James R. Green
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: We introduce a simple enhancement to the global processing of vision transformers (ViTs) to improve accuracy while maintaining throughput. Our approach, Jumbo, creates a wider CLS token, which is split to match the patch token width before attention, processed with self-attention, and reassembled. After attention, Jumbo applies a dedicated, wider FFN to this token. Jumbo significantly improves over ViT+Registers on ImageNet-1K at high speeds (by 3.2% for ViT-tiny and 13.5% for ViT-nano); these Jumbo models even outperform specialized compute-efficient models while preserving the architectural advantages of plain ViTs. Although Jumbo sees no gains for ViT-small on ImageNet-1K, it gains 3.4% on ImageNet-21K over ViT+Registers. Both findings indicate that Jumbo is most helpful when the ViT is otherwise too narrow for the task. Finally, we show that Jumbo can be easily adapted to excel on data beyond images, e.g., time series.

### LAVID: An Agentic LVLM Framework for Diffusion-Generated Video Detection 
[[arxiv](https://arxiv.org/abs/2502.14994)] [[cool](https://papers.cool/arxiv/2502.14994)] [[pdf](https://arxiv.org/pdf/2502.14994)]
> **Authors**: Qingyuan Liu,Yun-Yun Tsai,Ruijian Zha,Victoria Li,Pengyuan Shi,Chengzhi Mao,Junfeng Yang
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: The impressive achievements of generative models in creating high-quality videos have raised concerns about digital integrity and privacy vulnerabilities. Recent works of AI-generated content detection have been widely studied in the image field (e.g., deepfake), yet the video field has been unexplored. Large Vision Language Model (LVLM) has become an emerging tool for AI-generated content detection for its strong reasoning and multimodal capabilities. It breaks the limitations of traditional deep learning based methods faced with like lack of transparency and inability to recognize new artifacts. Motivated by this, we propose LAVID, a novel LVLMs-based ai-generated video detection with explicit knowledge enhancement. Our insight list as follows: (1) The leading LVLMs can call external tools to extract useful information to facilitate its own video detection task; (2) Structuring the prompt can affect LVLM's reasoning ability to interpret information in video content. Our proposed pipeline automatically selects a set of explicit knowledge tools for detection, and then adaptively adjusts the structure prompt by self-rewriting. Different from prior SOTA that trains additional detectors, our method is fully training-free and only requires inference of the LVLM for detection. To facilitate our research, we also create a new benchmark \vidfor with high-quality videos generated from multiple sources of video generation tools. Evaluation results show that LAVID improves F1 scores by 6.2 to 30.2% over the top baselines on our datasets across four SOTA LVLMs.

### Few-shot Species Range Estimation 
[[arxiv](https://arxiv.org/abs/2502.14977)] [[cool](https://papers.cool/arxiv/2502.14977)] [[pdf](https://arxiv.org/pdf/2502.14977)]
> **Authors**: Christian Lange,Max Hamilton,Elijah Cole,Alexander Shepard,Samuel Heinrich,Angela Zhu,Subhransu Maji,Grant Van Horn,Oisin Mac Aodha
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Knowing where a particular species can or cannot be found on Earth is crucial for ecological research and conservation efforts. By mapping the spatial ranges of all species, we would obtain deeper insights into how global biodiversity is affected by climate change and habitat loss. However, accurate range estimates are only available for a relatively small proportion of all known species. For the majority of the remaining species, we often only have a small number of records denoting the spatial locations where they have previously been observed. We outline a new approach for few-shot species range estimation to address the challenge of accurately estimating the range of a species from limited data. During inference, our model takes a set of spatial locations as input, along with optional metadata such as text or an image, and outputs a species encoding that can be used to predict the range of a previously unseen species in feed-forward manner. We validate our method on two challenging benchmarks, where we obtain state-of-the-art range estimation performance, in a fraction of the compute time, compared to recent alternative approaches.

### KITAB-Bench: A Comprehensive Multi-Domain Benchmark for Arabic OCR and Document Understanding 
[[arxiv](https://arxiv.org/abs/2502.14949)] [[cool](https://papers.cool/arxiv/2502.14949)] [[pdf](https://arxiv.org/pdf/2502.14949)]
> **Authors**: Ahmed Heakl,Abdullah Sohail,Mukul Ranjan,Rania Hossam,Ghazi Ahmed,Mohamed El-Geish,Omar Maher,Zhiqiang Shen,Fahad Khan,Salman Khan
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 17 pages, 5 figures, ACL 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,人机交互,机器学习
- **Abstract**: With the growing adoption of Retrieval-Augmented Generation (RAG) in document processing, robust text recognition has become increasingly critical for knowledge extraction. While OCR (Optical Character Recognition) for English and other languages benefits from large datasets and well-established benchmarks, Arabic OCR faces unique challenges due to its cursive script, right-to-left text flow, and complex typographic and calligraphic features. We present KITAB-Bench, a comprehensive Arabic OCR benchmark that fills the gaps in current evaluation systems. Our benchmark comprises 8,809 samples across 9 major domains and 36 sub-domains, encompassing diverse document types including handwritten text, structured tables, and specialized coverage of 21 chart types for business intelligence. Our findings show that modern vision-language models (such as GPT-4, Gemini, and Qwen) outperform traditional OCR approaches (like EasyOCR, PaddleOCR, and Surya) by an average of 60% in Character Error Rate (CER). Furthermore, we highlight significant limitations of current Arabic OCR models, particularly in PDF-to-Markdown conversion, where the best model Gemini-2.0-Flash achieves only 65% accuracy. This underscores the challenges in accurately recognizing Arabic text, including issues with complex fonts, numeral recognition errors, word elongation, and table structure detection. This work establishes a rigorous evaluation framework that can drive improvements in Arabic document analysis methods and bridge the performance gap with English OCR technologies.

### Online hand gesture recognition using Continual Graph Transformers 
[[arxiv](https://arxiv.org/abs/2502.14939)] [[cool](https://papers.cool/arxiv/2502.14939)] [[pdf](https://arxiv.org/pdf/2502.14939)]
> **Authors**: Rim Slama,Wael Rabah,Hazem Wannous
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Online continuous action recognition has emerged as a critical research area due to its practical implications in real-world applications, such as human-computer interaction, healthcare, and robotics. Among various modalities, skeleton-based approaches have gained significant popularity, demonstrating their effectiveness in capturing 3D temporal data while ensuring robustness to environmental variations. However, most existing works focus on segment-based recognition, making them unsuitable for real-time, continuous recognition scenarios. In this paper, we propose a novel online recognition system designed for real-time skeleton sequence streaming. Our approach leverages a hybrid architecture combining Spatial Graph Convolutional Networks (S-GCN) for spatial feature extraction and a Transformer-based Graph Encoder (TGE) for capturing temporal dependencies across frames. Additionally, we introduce a continual learning mechanism to enhance model adaptability to evolving data distributions, ensuring robust recognition in dynamic environments. We evaluate our method on the SHREC'21 benchmark dataset, demonstrating its superior performance in online hand gesture recognition. Our approach not only achieves state-of-the-art accuracy but also significantly reduces false positive rates, making it a compelling solution for real-time applications. The proposed system can be seamlessly integrated into various domains, including human-robot collaboration and assistive technologies, where natural and intuitive interaction is crucial.

### RAPTOR: Refined Approach for Product Table Object Recognition 
[[arxiv](https://arxiv.org/abs/2502.14918)] [[cool](https://papers.cool/arxiv/2502.14918)] [[pdf](https://arxiv.org/pdf/2502.14918)]
> **Authors**: Eliott Thomas,Mickael Coustaty,Aurelie Joseph,Gaspar Deloin,Elodie Carel,Vincent Poulain D'Andecy,Jean-Marc Ogier
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-21
> **comment**: Accepted for WACVW 2025 (VisionDocs)
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,信息检索
- **Abstract**: Extracting tables from documents is a critical task across various industries, especially on business documents like invoices and reports. Existing systems based on DEtection TRansformer (DETR) such as TAble TRansformer (TATR), offer solutions for Table Detection (TD) and Table Structure Recognition (TSR) but face challenges with diverse table formats and common errors like incorrect area detection and overlapping columns. This research introduces RAPTOR, a modular post-processing system designed to enhance state-of-the-art models for improved table extraction, particularly for product tables. RAPTOR addresses recurrent TD and TSR issues, improving both precision and structural predictions. For TD, we use DETR (trained on ICDAR 2019) and TATR (trained on PubTables-1M and FinTabNet), while TSR only relies on TATR. A Genetic Algorithm is incorporated to optimize RAPTOR's module parameters, using a private dataset of product tables to align with industrial needs. We evaluate our method on two private datasets of product tables, the public DOCILE dataset (which contains tables similar to our target product tables), and the ICDAR 2013 and ICDAR 2019 datasets. The results demonstrate that while our approach excels at product tables, it also maintains reasonable performance across diverse table formats. An ablation study further validates the contribution of each module in our system.

### Sce2DriveX: A Generalized MLLM Framework for Scene-to-Drive Learning 
[[arxiv](https://arxiv.org/abs/2502.14917)] [[cool](https://papers.cool/arxiv/2502.14917)] [[pdf](https://arxiv.org/pdf/2502.14917)]
> **Authors**: Rui Zhao,Qirui Yuan,Jinyu Li,Haofeng Hu,Yun Li,Chengyuan Zheng,Fei Gao
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: End-to-end autonomous driving, which directly maps raw sensor inputs to low-level vehicle controls, is an important part of Embodied AI. Despite successes in applying Multimodal Large Language Models (MLLMs) for high-level traffic scene semantic understanding, it remains challenging to effectively translate these conceptual semantics understandings into low-level motion control commands and achieve generalization and consensus in cross-scene driving. We introduce Sce2DriveX, a human-like driving chain-of-thought (CoT) reasoning MLLM framework. Sce2DriveX utilizes multimodal joint learning from local scene videos and global BEV maps to deeply understand long-range spatiotemporal relationships and road topology, enhancing its comprehensive perception and reasoning capabilities in 3D dynamic/static scenes and achieving driving generalization across scenes. Building on this, it reconstructs the implicit cognitive chain inherent in human driving, covering scene understanding, meta-action reasoning, behavior interpretation analysis, motion planning and control, thereby further bridging the gap between autonomous driving and human thought processes. To elevate model performance, we have developed the first extensive Visual Question Answering (VQA) driving instruction dataset tailored for 3D spatial understanding and long-axis task reasoning. Extensive experiments demonstrate that Sce2DriveX achieves state-of-the-art performance from scene understanding to end-to-end driving, as well as robust generalization on the CARLA Bench2Drive benchmark.

### What Is a Good Caption? A Comprehensive Visual Caption Benchmark for Evaluating Both Correctness and Coverage of MLLMs 
[[arxiv](https://arxiv.org/abs/2502.14914)] [[cool](https://papers.cool/arxiv/2502.14914)] [[pdf](https://arxiv.org/pdf/2502.14914)]
> **Authors**: Zhihang Liu,Chen-Wei Xie,Bin Wen,Feiwu Yu,Jixuan Chen,Boqiang Zhang,Nianzu Yang,Pandeng Li,Yun Zheng,Hongtao Xie
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-21
> **comment**: Work in progress
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算语言学,机器学习
- **Abstract**: Recent advancements in Multimodal Large Language Models (MLLMs) have rendered traditional visual captioning benchmarks obsolete, as they primarily evaluate short descriptions with outdated metrics. While recent benchmarks address these limitations by decomposing captions into visual elements and adopting model-based evaluation, they remain incomplete-overlooking critical aspects, while providing vague, non-explanatory scores. To bridge this gap, we propose CV-CapBench, a Comprehensive Visual Caption Benchmark that systematically evaluates caption quality across 6 views and 13 dimensions. CV-CapBench introduces precision, recall, and hit rate metrics for each dimension, uniquely assessing both correctness and coverage. Experiments on leading MLLMs reveal significant capability gaps, particularly in dynamic and knowledge-intensive dimensions. These findings provide actionable insights for future research. The code and data will be released.

### KOALA: Knowledge Conflict Augmentations for Robustness in Vision Language Models 
[[arxiv](https://arxiv.org/abs/2502.14908)] [[cool](https://papers.cool/arxiv/2502.14908)] [[pdf](https://arxiv.org/pdf/2502.14908)]
> **Authors**: Peter Carragher,Nikitha Rao,Abhinand Jha,R Raghav,Kathleen M. Carley
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习
- **Abstract**: The robustness of large language models (LLMs) against knowledge conflicts in unimodal question answering systems has been well studied. However, the effect of conflicts in information sources on vision language models (VLMs) in multimodal settings has not yet been explored. In this work, we propose \segsub, a framework that applies targeted perturbations to image sources to study and improve the robustness of VLMs against three different types of knowledge conflicts, namely parametric, source, and counterfactual conflicts. Contrary to prior findings that showed that LLMs are sensitive to parametric conflicts arising from textual perturbations, we find VLMs are largely robust to image perturbation. On the other hand, VLMs perform poorly on counterfactual examples (<30% accuracy) and fail to reason over source conflicts (<1% accuracy). We also find a link between hallucinations and image context, with GPT-4o prone to hallucination when presented with highly contextualized counterfactual examples. While challenges persist with source conflicts, finetuning models significantly improves reasoning over counterfactual samples. Our findings highlight the need for VLM training methodologies that enhance their reasoning capabilities, particularly in addressing complex knowledge conflicts between multimodal sources.

### UPCMR: A Universal Prompt-guided Model for Random Sampling Cardiac MRI Reconstruction 
[[arxiv](https://arxiv.org/abs/2502.14899)] [[cool](https://papers.cool/arxiv/2502.14899)] [[pdf](https://arxiv.org/pdf/2502.14899)]
> **Authors**: Donghang Lyu,Chinmay Rao,Marius Staring,Matthias J. P. van Osch,Mariya Doneva,Hildo J. Lamb,Nicola Pezzotti
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-21
> **comment**: Accepted paper for STACOM 2024
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: Cardiac magnetic resonance imaging (CMR) is vital for diagnosing heart diseases, but long scan time remains a major drawback. To address this, accelerated imaging techniques have been introduced by undersampling k-space, which reduces the quality of the resulting images. Recent deep learning advancements aim to speed up scanning while preserving quality, but adapting to various sampling modes and undersampling factors remains challenging. Therefore, building a universal model is a promising direction. In this work, we introduce UPCMR, a universal unrolled model designed for CMR reconstruction. This model incorporates two kinds of learnable prompts, undersampling-specific prompt and spatial-specific prompt, and integrates them with a UNet structure in each block. Overall, by using the CMRxRecon2024 challenge dataset for training and validation, the UPCMR model highly enhances reconstructed image quality across all random sampling scenarios through an effective training strategy compared to some traditional methods, demonstrating strong adaptability potential for this task.

### A Comprehensive Survey on Concept Erasure in Text-to-Image Diffusion Models 
[[arxiv](https://arxiv.org/abs/2502.14896)] [[cool](https://papers.cool/arxiv/2502.14896)] [[pdf](https://arxiv.org/pdf/2502.14896)]
> **Authors**: Changhoon Kim,Yanjun Qi
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Text-to-Image (T2I) models have made remarkable progress in generating high-quality, diverse visual content from natural language prompts. However, their ability to reproduce copyrighted styles, sensitive imagery, and harmful content raises significant ethical and legal concerns. Concept erasure offers a proactive alternative to external filtering by modifying T2I models to prevent the generation of undesired content. In this survey, we provide a structured overview of concept erasure, categorizing existing methods based on their optimization strategies and the architectural components they modify. We categorize concept erasure methods into fine-tuning for parameter updates, closed-form solutions for efficient edits, and inference-time interventions for content restriction without weight modification. Additionally, we explore adversarial attacks that bypass erasure techniques and discuss emerging defenses. To support further research, we consolidate key datasets, evaluation metrics, and benchmarks for assessing erasure effectiveness and model robustness. This survey serves as a comprehensive resource, offering insights into the evolving landscape of concept erasure, its challenges, and future directions.

### FOCUS on Contamination: A Geospatial Deep Learning Framework with a Noise-Aware Loss for Surface Water PFAS Prediction 
[[arxiv](https://arxiv.org/abs/2502.14894)] [[cool](https://papers.cool/arxiv/2502.14894)] [[pdf](https://arxiv.org/pdf/2502.14894)]
> **Authors**: Jowaria Khan,Alexa Friedman,Sydney Evans,Runzi Wang,Kaley Beins,David Andrews,Elizabeth Bondi-Kelly
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-21
> **comment**: :I.2.1; I.2.10; I.4.6; I.4.9; I.4.10; J.2
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,计算机与社会,机器学习
- **Abstract**: Per and polyfluoroalkyl substances (PFAS), chemicals found in products like non-stick cookware, are unfortunately persistent environmental pollutants with severe health risks. Accurately mapping PFAS contamination is crucial for guiding targeted remediation efforts and protecting public and environmental health, yet detection across large regions remains challenging due to the cost of testing and the difficulty of simulating their spread. In this work, we introduce FOCUS, a geospatial deep learning framework with a label noise-aware loss function, to predict PFAS contamination in surface water over large regions. By integrating hydrological flow data, land cover information, and proximity to known PFAS sources, our approach leverages both spatial and environmental context to improve prediction accuracy. We evaluate the performance of our approach through extensive ablation studies and comparative analyses against baselines like sparse segmentation, as well as existing scientific methods, including Kriging and pollutant transport simulations. Results highlight our framework's potential for scalable PFAS monitoring.

### NOTA: Multimodal Music Notation Understanding for Visual Large Language Model 
[[arxiv](https://arxiv.org/abs/2502.14893)] [[cool](https://papers.cool/arxiv/2502.14893)] [[pdf](https://arxiv.org/pdf/2502.14893)]
> **Authors**: Mingni Tang,Jiajia Li,Lu Yang,Zhiqiang Zhang,Jinghao Tian,Zuchao Li,Lefei Zhang,Ping Wang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习,声音,音频和语音处理
- **Abstract**: Symbolic music is represented in two distinct forms: two-dimensional, visually intuitive score images, and one-dimensional, standardized text annotation sequences. While large language models have shown extraordinary potential in music, current research has primarily focused on unimodal symbol sequence text. Existing general-domain visual language models still lack the ability of music notation understanding. Recognizing this gap, we propose NOTA, the first large-scale comprehensive multimodal music notation dataset. It consists of 1,019,237 records, from 3 regions of the world, and contains 3 tasks. Based on the dataset, we trained NotaGPT, a music notation visual large language model. Specifically, we involve a pre-alignment training phase for cross-modal alignment between the musical notes depicted in music score images and their textual representation in ABC notation. Subsequent training phases focus on foundational music information extraction, followed by training on music notation analysis. Experimental results demonstrate that our NotaGPT-7B achieves significant improvement on music understanding, showcasing the effectiveness of NOTA and the training pipeline. Our datasets are open-sourced at https://huggingface.co/datasets/MYTH-Lab/NOTA-dataset.

### WeedVision: Multi-Stage Growth and Classification of Weeds using DETR and RetinaNet for Precision Agriculture 
[[arxiv](https://arxiv.org/abs/2502.14890)] [[cool](https://papers.cool/arxiv/2502.14890)] [[pdf](https://arxiv.org/pdf/2502.14890)]
> **Authors**: Taminul Islam,Toqi Tahamid Sarker,Khaled R Ahmed,Cristiana Bernardi Rankrape,Karla Gage
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-21
> **comment**: Accepted and Presented to ICMLA, 2024
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Weed management remains a critical challenge in agriculture, where weeds compete with crops for essential resources, leading to significant yield losses. Accurate detection of weeds at various growth stages is crucial for effective management yet challenging for farmers, as it requires identifying different species at multiple growth phases. This research addresses these challenges by utilizing advanced object detection models, specifically, the Detection Transformer (DETR) with a ResNet50 backbone and RetinaNet with a ResNeXt101 backbone, to identify and classify 16 weed species of economic concern across 174 classes, spanning their 11 weeks growth stages from seedling to maturity. A robust dataset comprising 203,567 images was developed, meticulously labeled by species and growth stage. The models were rigorously trained and evaluated, with RetinaNet demonstrating superior performance, achieving a mean Average Precision (mAP) of 0.907 on the training set and 0.904 on the test set, compared to DETR's mAP of 0.854 and 0.840, respectively. RetinaNet also outperformed DETR in recall and inference speed of 7.28 FPS, making it more suitable for real time applications. Both models showed improved accuracy as plants matured. This research provides crucial insights for developing precise, sustainable, and automated weed management strategies, paving the way for real time species specific detection systems and advancing AI-assisted agriculture through continued innovation in model development and early detection accuracy.

### Narrowing Information Bottleneck Theory for Multimodal Image-Text Representations Interpretability 
[[arxiv](https://arxiv.org/abs/2502.14889)] [[cool](https://papers.cool/arxiv/2502.14889)] [[pdf](https://arxiv.org/pdf/2502.14889)]
> **Authors**: Zhiyu Zhu,Zhibo Jin,Jiayu Zhang,Nan Yang,Jiahao Huang,Jianlong Zhou,Fang Chen
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-21
> **comment**: Accepted by ICLR 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: The task of identifying multimodal image-text representations has garnered increasing attention, particularly with models such as CLIP (Contrastive Language-Image Pretraining), which demonstrate exceptional performance in learning complex associations between images and text. Despite these advancements, ensuring the interpretability of such models is paramount for their safe deployment in real-world applications, such as healthcare. While numerous interpretability methods have been developed for unimodal tasks, these approaches often fail to transfer effectively to multimodal contexts due to inherent differences in the representation structures. Bottleneck methods, well-established in information theory, have been applied to enhance CLIP's interpretability. However, they are often hindered by strong assumptions or intrinsic randomness. To overcome these challenges, we propose the Narrowing Information Bottleneck Theory, a novel framework that fundamentally redefines the traditional bottleneck approach. This theory is specifically designed to satisfy contemporary attribution axioms, providing a more robust and reliable solution for improving the interpretability of multimodal models. In our experiments, compared to state-of-the-art methods, our approach enhances image interpretability by an average of 9%, text interpretability by an average of 58.83%, and accelerates processing speed by 63.95%. Our code is publicly accessible at https://github.com/LMBTough/NIB.

### The Multi-Faceted Monosemanticity in Multimodal Representations 
[[arxiv](https://arxiv.org/abs/2502.14888)] [[cool](https://papers.cool/arxiv/2502.14888)] [[pdf](https://arxiv.org/pdf/2502.14888)]
> **Authors**: Hanqi Yan,Xiangxiang Cui,Lu Yin,Paul Pu Liang,Yulan He,Yifei Wang
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: In this paper, we leverage recent advancements in feature monosemanticity to extract interpretable features from deep multimodal models, offering a data-driven understanding of modality gaps. Specifically, we investigate CLIP (Contrastive Language-Image Pretraining), a prominent visual-language representation model trained on extensive image-text pairs. Building upon interpretability tools developed for single-modal models, we extend these methodologies to assess multi-modal interpretability of CLIP features. Additionally, we introduce the Modality Dominance Score (MDS) to attribute the interpretability of each feature to its respective modality. Next, we transform CLIP features into a more interpretable space, enabling us to categorize them into three distinct classes: vision features (single-modal), language features (single-modal), and visual-language features (cross-modal). Our findings reveal that this categorization aligns closely with human cognitive understandings of different modalities. We also demonstrate significant use cases of this modality-specific features including detecting gender bias, adversarial attack defense and text-to-image model editing. These results indicate that large-scale multimodal models, equipped with task-agnostic interpretability tools, offer valuable insights into key connections and distinctions between different modalities.

### Vision-Enhanced Time Series Forecasting via Latent Diffusion Models 
[[arxiv](https://arxiv.org/abs/2502.14887)] [[cool](https://papers.cool/arxiv/2502.14887)] [[pdf](https://arxiv.org/pdf/2502.14887)]
> **Authors**: Weilin Ruan,Siru Zhong,Haomin Wen,Yuxuan Liang
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Diffusion models have recently emerged as powerful frameworks for generating high-quality images. While recent studies have explored their application to time series forecasting, these approaches face significant challenges in cross-modal modeling and transforming visual information effectively to capture temporal patterns. In this paper, we propose LDM4TS, a novel framework that leverages the powerful image reconstruction capabilities of latent diffusion models for vision-enhanced time series forecasting. Instead of introducing external visual data, we are the first to use complementary transformation techniques to convert time series into multi-view visual representations, allowing the model to exploit the rich feature extraction capabilities of the pre-trained vision encoder. Subsequently, these representations are reconstructed using a latent diffusion model with a cross-modal conditioning mechanism as well as a fusion module. Experimental results demonstrate that LDM4TS outperforms various specialized forecasting models for time series forecasting tasks.

### Surgical Scene Understanding in the Era of Foundation AI Models: A Comprehensive Review 
[[arxiv](https://arxiv.org/abs/2502.14886)] [[cool](https://papers.cool/arxiv/2502.14886)] [[pdf](https://arxiv.org/pdf/2502.14886)]
> **Authors**: Ufaq Khan,Umair Nawaz,Adnan Qayyum,Shazad Ashraf,Muhammad Bilal,Junaid Qadir
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Recent advancements in machine learning (ML) and deep learning (DL), particularly through the introduction of foundational models (FMs), have significantly enhanced surgical scene understanding within minimally invasive surgery (MIS). This paper surveys the integration of state-of-the-art ML and DL technologies, including Convolutional Neural Networks (CNNs), Vision Transformers (ViTs), and foundational models like the Segment Anything Model (SAM), into surgical workflows. These technologies improve segmentation accuracy, instrument tracking, and phase recognition in surgical endoscopic video analysis. The paper explores the challenges these technologies face, such as data variability and computational demands, and discusses ethical considerations and integration hurdles in clinical settings. Highlighting the roles of FMs, we bridge the technological capabilities with clinical needs and outline future research directions to enhance the adaptability, efficiency, and ethical alignment of AI applications in surgery. Our findings suggest that substantial progress has been made; however, more focused efforts are required to achieve seamless integration of these technologies into clinical workflows, ensuring they complement surgical practice by enhancing precision, reducing risks, and optimizing patient outcomes.

### SEM-CLIP: Precise Few-Shot Learning for Nanoscale Defect Detection in Scanning Electron Microscope Image 
[[arxiv](https://arxiv.org/abs/2502.14884)] [[cool](https://papers.cool/arxiv/2502.14884)] [[pdf](https://arxiv.org/pdf/2502.14884)]
> **Authors**: Qian Jin,Yuqi Jiang,Xudong Lu,Yumeng Liu,Yining Chen,Dawei Gao,Qi Sun,Cheng Zhuo
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-21
> **comment**: Published in ACM/IEEE International Conference on Computer-Aided Design (ICCAD), 2024
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: In the field of integrated circuit manufacturing, the detection and classification of nanoscale wafer defects are critical for subsequent root cause analysis and yield enhancement. The complex background patterns observed in scanning electron microscope (SEM) images and the diverse textures of the defects pose significant challenges. Traditional methods usually suffer from insufficient data, labels, and poor transferability. In this paper, we propose a novel few-shot learning approach, SEM-CLIP, for accurate defect classification and segmentation. SEM-CLIP customizes the Contrastive Language-Image Pretraining (CLIP) model to better focus on defect areas and minimize background distractions, thereby enhancing segmentation accuracy. We employ text prompts enriched with domain knowledge as prior information to assist in precise analysis. Additionally, our approach incorporates feature engineering with textual guidance to categorize defects more effectively. SEM-CLIP requires little annotated data, substantially reducing labor demands in the semiconductor industry. Extensive experimental validation demonstrates that our model achieves impressive classification and segmentation results under few-shot learning scenarios.

### Can LVLMs and Automatic Metrics Capture Underlying Preferences of Blind and Low-Vision Individuals for Navigational Aid? 
[[arxiv](https://arxiv.org/abs/2502.14883)] [[cool](https://papers.cool/arxiv/2502.14883)] [[pdf](https://arxiv.org/pdf/2502.14883)]
> **Authors**: Na Min An,Eunki Kim,Wan Ju Kang,Sangryul Kim,Hyunjung Shim,James Thorne
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-21
> **comment**: 26 pages, 12 figures, 14 tables
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Vision is a primary means of how humans perceive the environment, but Blind and Low-Vision (BLV) people need assistance understanding their surroundings, especially in unfamiliar environments. The emergence of semantic-based systems as assistance tools for BLV users has motivated many researchers to explore responses from Large Vision-Language Models (LVLMs). However, it has yet been studied preferences of BLV users on diverse types/styles of responses from LVLMs, specifically for navigational aid. To fill this gap, we first construct Eye4B dataset, consisting of human-validated 1.1k curated outdoor/indoor scenes with 5-10 relevant requests per scene. Then, we conduct an in-depth user study with eight BLV users to evaluate their preferences on six LVLMs from five perspectives: Afraidness, Nonactionability, Sufficiency, and Conciseness. Finally, we introduce Eye4B benchmark for evaluating alignment between widely used model-based image-text metrics and our collected BLV preferences. Our work can be set as a guideline for developing BLV-aware LVLMs towards a Barrier-Free AI system.

### From 16-Bit to 1-Bit: Visual KV Cache Quantization for Memory-Efficient Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.14882)] [[cool](https://papers.cool/arxiv/2502.14882)] [[pdf](https://arxiv.org/pdf/2502.14882)]
> **Authors**: Zeliang Zhang,Yifan Zhu,Susan Liang,Zhiyuan Wang,Jiani Liu,Haiting Lin,Mingjie Zhao,Chenliang Xu,Kun Wan,Wentian Zhao
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Multimodal Large Language Models (MLLMs) have achieved remarkable success across various applications, yet their computational overhead during deployment remains a critical challenge. While Key-Value (KV) caching improves inference efficiency by trading memory for computation, the growing memory footprint from storing extensive KV caches reduces throughput and limits long-term execution on devices with constrained GPU memory. Existing approaches primarily focus on dropping unimportant tokens to reduce the KV cache size, mitigating memory constraints at the cost of potential information loss. In contrast, we propose a simple yet effective visual quantization strategy that preserves all visual tokens while significantly reducing memory consumption. To achieve an extreme quantization ratio, i.e., 1-bit quantization, we propose group-specific quantization and quantile-based quantization approaches, motivated by the inherent patterns of the KV cache. Our method is plug-and-play, enabling seamless integration into various MLLMs to improve memory efficiency without architectural modifications. Extensive experiments demonstrate that our approach effectively reduces memory overhead while maintaining computational efficiency and preserving multimodal performance.

### KKA: Improving Vision Anomaly Detection through Anomaly-related Knowledge from Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.14880)] [[cool](https://papers.cool/arxiv/2502.14880)] [[pdf](https://arxiv.org/pdf/2502.14880)]
> **Authors**: Dong Chen,Zhengqing Hu,Peiguang Fan,Yueting Zhuang,Yafei Li,Qidong Liu,Xiaoheng Jiang,Mingliang Xu
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Vision anomaly detection, particularly in unsupervised settings, often struggles to distinguish between normal samples and anomalies due to the wide variability in anomalies. Recently, an increasing number of studies have focused on generating anomalies to help detectors learn more effective boundaries between normal samples and anomalies. However, as the generated anomalies are often derived from random factors, they frequently lack realism. Additionally, randomly generated anomalies typically offer limited support in constructing effective boundaries, as most differ substantially from normal samples and lie far from the boundary. To address these challenges, we propose Key Knowledge Augmentation (KKA), a method that extracts anomaly-related knowledge from large language models (LLMs). More specifically, KKA leverages the extensive prior knowledge of LLMs to generate meaningful anomalies based on normal samples. Then, KKA classifies the generated anomalies as easy anomalies and hard anomalies according to their similarity to normal samples. Easy anomalies exhibit significant differences from normal samples, whereas hard anomalies closely resemble normal samples. KKA iteratively updates the generated anomalies, and gradually increasing the proportion of hard anomalies to enable the detector to learn a more effective boundary. Experimental results show that the proposed method significantly improves the performance of various vision anomaly detectors while maintaining low generation costs. The code for CMG can be found at https://github.com/Anfeather/KKA.

### Time Travel: A Comprehensive Benchmark to Evaluate LMMs on Historical and Cultural Artifacts 
[[arxiv](https://arxiv.org/abs/2502.14865)] [[cool](https://papers.cool/arxiv/2502.14865)] [[pdf](https://arxiv.org/pdf/2502.14865)]
> **Authors**: Sara Ghaboura,Ketan More,Ritesh Thawkar,Wafa Alghallabi,Omkar Thawakar,Fahad Shahbaz Khan,Hisham Cholakkal,Salman Khan,Rao Muhammad Anwer
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 4 pages, 6 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Understanding historical and cultural artifacts demands human expertise and advanced computational techniques, yet the process remains complex and time-intensive. While large multimodal models offer promising support, their evaluation and improvement require a standardized benchmark. To address this, we introduce TimeTravel, a benchmark of 10,250 expert-verified samples spanning 266 distinct cultures across 10 major historical regions. Designed for AI-driven analysis of manuscripts, artworks, inscriptions, and archaeological discoveries, TimeTravel provides a structured dataset and robust evaluation framework to assess AI models' capabilities in classification, interpretation, and historical comprehension. By integrating AI with historical research, TimeTravel fosters AI-powered tools for historians, archaeologists, researchers, and cultural tourists to extract valuable insights while ensuring technology contributes meaningfully to historical discovery and cultural heritage preservation. We evaluate contemporary AI models on TimeTravel, highlighting their strengths and identifying areas for improvement. Our goal is to establish AI as a reliable partner in preserving cultural heritage, ensuring that technological advancements contribute meaningfully to historical discovery. Our code is available at: \url{https://github.com/mbzuai-oryx/TimeTravel}.

### Scaling Text-Rich Image Understanding via Code-Guided Synthetic Multimodal Data Generation 
[[arxiv](https://arxiv.org/abs/2502.14846)] [[cool](https://papers.cool/arxiv/2502.14846)] [[pdf](https://arxiv.org/pdf/2502.14846)]
> **Authors**: Yue Yang,Ajay Patel,Matt Deitke,Tanmay Gupta,Luca Weihs,Andrew Head,Mark Yatskar,Chris Callison-Burch,Ranjay Krishna,Aniruddha Kembhavi,Christopher Clark
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 20 pages, 19 figures, 9 tables, website: https://yueyang1996.github.io/cosyn/
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算语言学
- **Abstract**: Reasoning about images with rich text, such as charts and documents, is a critical application of vision-language models (VLMs). However, VLMs often struggle in these domains due to the scarcity of diverse text-rich vision-language data. To address this challenge, we present CoSyn, a framework that leverages the coding capabilities of text-only large language models (LLMs) to automatically create synthetic text-rich multimodal data. Given input text describing a target domain (e.g., "nutrition fact labels"), CoSyn prompts an LLM to generate code (Python, HTML, LaTeX, etc.) for rendering synthetic images. With the underlying code as textual representations of the synthetic images, CoSyn can generate high-quality instruction-tuning data, again relying on a text-only LLM. Using CoSyn, we constructed a dataset comprising 400K images and 2.7M rows of vision-language instruction-tuning data. Comprehensive experiments on seven benchmarks demonstrate that models trained on our synthetic data achieve state-of-the-art performance among competitive open-source models, including Llama 3.2, and surpass proprietary models such as GPT-4V and Gemini 1.5 Flash. Furthermore, CoSyn can produce synthetic pointing data, enabling VLMs to ground information within input images, showcasing its potential for developing multimodal agents capable of acting in real-world environments.

### LongWriter-V: Enabling Ultra-Long and High-Fidelity Generation in Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2502.14834)] [[cool](https://papers.cool/arxiv/2502.14834)] [[pdf](https://arxiv.org/pdf/2502.14834)]
> **Authors**: Shangqing Tu,Yucheng Wang,Daniel Zhang-Li,Yushi Bai,Jifan Yu,Yuhao Wu,Lei Hou,Huiqin Liu,Zhiyuan Liu,Bin Xu,Juanzi Li
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **Abstract**: Existing Large Vision-Language Models (LVLMs) can process inputs with context lengths up to 128k visual and text tokens, yet they struggle to generate coherent outputs beyond 1,000 words. We find that the primary limitation is the absence of long output examples during supervised fine-tuning (SFT). To tackle this issue, we introduce LongWriter-V-22k, a SFT dataset comprising 22,158 examples, each with multiple input images, an instruction, and corresponding outputs ranging from 0 to 10,000 words. Moreover, to achieve long outputs that maintain high-fidelity to the input images, we employ Direct Preference Optimization (DPO) to the SFT model. Given the high cost of collecting human feedback for lengthy outputs (e.g., 3,000 words), we propose IterDPO, which breaks long outputs into segments and uses iterative corrections to form preference pairs with the original outputs. Additionally, we develop MMLongBench-Write, a benchmark featuring six tasks to evaluate the long-generation capabilities of VLMs. Our 7B parameter model, trained with LongWriter-V-22k and IterDPO, achieves impressive performance on this benchmark, outperforming larger proprietary models like GPT-4o. Code and data: https://github.com/THU-KEG/LongWriter-V

### Improving the Diffusability of Autoencoders 
[[arxiv](https://arxiv.org/abs/2502.14831)] [[cool](https://papers.cool/arxiv/2502.14831)] [[pdf](https://arxiv.org/pdf/2502.14831)]
> **Authors**: Ivan Skorokhodov,Sharath Girish,Benran Hu,Willi Menapace,Yanyu Li,Rameen Abdal,Sergey Tulyakov,Aliaksandr Siarohin
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 26 pages, 22 figures, 9 tables
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: Latent diffusion models have emerged as the leading approach for generating high-quality images and videos, utilizing compressed latent representations to reduce the computational burden of the diffusion process. While recent advancements have primarily focused on scaling diffusion backbones and improving autoencoder reconstruction quality, the interaction between these components has received comparatively less attention. In this work, we perform a spectral analysis of modern autoencoders and identify inordinate high-frequency components in their latent spaces, which are especially pronounced in the autoencoders with a large bottleneck channel size. We hypothesize that this high-frequency component interferes with the coarse-to-fine nature of the diffusion synthesis process and hinders the generation quality. To mitigate the issue, we propose scale equivariance: a simple regularization strategy that aligns latent and RGB spaces across frequencies by enforcing scale equivariance in the decoder. It requires minimal code changes and only up to 20K autoencoder fine-tuning steps, yet significantly improves generation quality, reducing FID by 19% for image generation on ImageNet-1K 256x256 and FVD by at least 44% for video generation on Kinetics-700 17x256x256.

### Exploring Advanced Techniques for Visual Question Answering: A Comprehensive Comparison 
[[arxiv](https://arxiv.org/abs/2502.14827)] [[cool](https://papers.cool/arxiv/2502.14827)] [[pdf](https://arxiv.org/pdf/2502.14827)]
> **Authors**: Aiswarya Baby,Tintu Thankom Koshy
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 8 pages, No figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,新兴技术,机器学习
- **Abstract**: Visual Question Answering (VQA) has emerged as a pivotal task in the intersection of computer vision and natural language processing, requiring models to understand and reason about visual content in response to natural language questions. Analyzing VQA datasets is essential for developing robust models that can handle the complexities of multimodal reasoning. Several approaches have been developed to examine these datasets, each offering distinct perspectives on question diversity, answer distribution, and visual-textual correlations. Despite significant progress, existing VQA models face challenges related to dataset bias, limited model complexity, commonsense reasoning gaps, rigid evaluation methods, and generalization to real world scenarios. This paper presents a comprehensive comparative study of five advanced VQA models: ABC-CNN, KICNLE, Masked Vision and Language Modeling, BLIP-2, and OFA, each employing distinct methodologies to address these challenges.

### A Survey on Text-Driven 360-Degree Panorama Generation 
[[arxiv](https://arxiv.org/abs/2502.14799)] [[cool](https://papers.cool/arxiv/2502.14799)] [[pdf](https://arxiv.org/pdf/2502.14799)]
> **Authors**: Hai Wang,Xiaoyu Xiang,Weihao Xia,Jing-Hao Xue
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: The advent of text-driven 360-degree panorama generation, enabling the synthesis of 360-degree panoramic images directly from textual descriptions, marks a transformative advancement in immersive visual content creation. This innovation significantly simplifies the traditionally complex process of producing such content. Recent progress in text-to-image diffusion models has accelerated the rapid development in this emerging field. This survey presents a comprehensive review of text-driven 360-degree panorama generation, offering an in-depth analysis of state-of-the-art algorithms and their expanding applications in 360-degree 3D scene generation. Furthermore, we critically examine current limitations and propose promising directions for future research. A curated project page with relevant resources and research papers is available at https://littlewhitesea.github.io/Text-Driven-Pano-Gen/.

### SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features 
[[arxiv](https://arxiv.org/abs/2502.14786)] [[cool](https://papers.cool/arxiv/2502.14786)] [[pdf](https://arxiv.org/pdf/2502.14786)]
> **Authors**: Michael Tschannen,Alexey Gritsenko,Xiao Wang,Muhammad Ferjad Naeem,Ibrahim Alabdulmohsin,Nikhil Parthasarathy,Talfan Evans,Lucas Beyer,Ye Xia,Basil Mustafa,Olivier Hénaff,Jeremiah Harmsen,Andreas Steiner,Xiaohua Zhai
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: Modelcheckpoints are available at https://github.com/google-research/big_vision/tree/main/big_vision/configs/proj/image_text/README_siglip2.md
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: We introduce SigLIP 2, a family of new multilingual vision-language encoders that build on the success of the original SigLIP. In this second iteration, we extend the original image-text training objective with several prior, independently developed techniques into a unified recipe -- this includes captioning-based pretraining, self-supervised losses (self-distillation, masked prediction) and online data curation. With these changes, SigLIP 2 models outperform their SigLIP counterparts at all model scales in core capabilities, including zero-shot classification, image-text retrieval, and transfer performance when extracting visual representations for Vision-Language Models (VLMs). Furthermore, the new training recipe leads to significant improvements on localization and dense prediction tasks. We also train variants which support multiple resolutions and preserve the input's native aspect ratio. Finally, we train on a more diverse data-mixture that includes de-biasing techniques, leading to much better multilingual understanding and improved fairness. To allow users to trade off inference cost with performance, we release model checkpoints at four sizes: ViT-B (86M), L (303M), So400m (400M), and g (1B).

### DC-ControlNet: Decoupling Inter- and Intra-Element Conditions in Image Generation with Diffusion Models 
[[arxiv](https://arxiv.org/abs/2502.14779)] [[cool](https://papers.cool/arxiv/2502.14779)] [[pdf](https://arxiv.org/pdf/2502.14779)]
> **Authors**: Hongji Yang,Wencheng Han,Yucheng Zhou,Jianbing Shen
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: In this paper, we introduce DC (Decouple)-ControlNet, a highly flexible and precisely controllable framework for multi-condition image generation. The core idea behind DC-ControlNet is to decouple control conditions, transforming global control into a hierarchical system that integrates distinct elements, contents, and layouts. This enables users to mix these individual conditions with greater flexibility, leading to more efficient and accurate image generation control. Previous ControlNet-based models rely solely on global conditions, which affect the entire image and lack the ability of element- or region-specific control. This limitation reduces flexibility and can cause condition misunderstandings in multi-conditional image generation. To address these challenges, we propose both intra-element and Inter-element Controllers in DC-ControlNet. The Intra-Element Controller handles different types of control signals within individual elements, accurately describing the content and layout characteristics of the object. For interactions between elements, we introduce the Inter-Element Controller, which accurately handles multi-element interactions and occlusion based on user-defined relationships. Extensive evaluations show that DC-ControlNet significantly outperforms existing ControlNet models and Layout-to-Image generative models in terms of control flexibility and precision in multi-condition control.

### Multi-dataset synergistic in supervised learning to pre-label structural components in point clouds from shell construction scenes 
[[arxiv](https://arxiv.org/abs/2502.14721)] [[cool](https://papers.cool/arxiv/2502.14721)] [[pdf](https://arxiv.org/pdf/2502.14721)]
> **Authors**: Lukas Rauch,Thomas Braml
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 18 pages, 8 figures, 7 tables
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: The significant effort required to annotate data for new training datasets hinders computer vision research and machine learning in the construction industry. This work explores adapting standard datasets and the latest transformer model architectures for point cloud semantic segmentation in the context of shell construction sites. Unlike common approaches focused on object segmentation of building interiors and furniture, this study addressed the challenges of segmenting complex structural components in Architecture, Engineering, and Construction (AEC). We establish a baseline through supervised training and a custom validation dataset, evaluate the cross-domain inference with large-scale indoor datasets, and utilize transfer learning to maximize segmentation performance with minimal new data. The findings indicate that with minimal fine-tuning, pre-trained transformer architectures offer an effective strategy for building component segmentation. Our results are promising for automating the annotation of new, previously unseen data when creating larger training resources and for the segmentation of frequently recurring objects.

### Self-supervised Monocular Depth Estimation Robust to Reflective Surface Leveraged by Triplet Mining 
[[arxiv](https://arxiv.org/abs/2502.14573)] [[cool](https://papers.cool/arxiv/2502.14573)] [[pdf](https://arxiv.org/pdf/2502.14573)]
> **Authors**: Wonhyeok Choi,Kyumin Hwang,Wei Peng,Minwoo Choi,Sunghoon Im
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: Accepted at ICLR 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Self-supervised monocular depth estimation (SSMDE) aims to predict the dense depth map of a monocular image, by learning depth from RGB image sequences, eliminating the need for ground-truth depth labels. Although this approach simplifies data acquisition compared to supervised methods, it struggles with reflective surfaces, as they violate the assumptions of Lambertian reflectance, leading to inaccurate training on such surfaces. To tackle this problem, we propose a novel training strategy for an SSMDE by leveraging triplet mining to pinpoint reflective regions at the pixel level, guided by the camera geometry between different viewpoints. The proposed reflection-aware triplet mining loss specifically penalizes the inappropriate photometric error minimization on the localized reflective regions while preserving depth accuracy in non-reflective areas. We also incorporate a reflection-aware knowledge distillation method that enables a student model to selectively learn the pixel-level knowledge from reflective and non-reflective regions. This results in robust depth estimation across areas. Evaluation results on multiple datasets demonstrate that our method effectively enhances depth quality on reflective surfaces and outperforms state-of-the-art SSMDE baselines.

### PLPHP: Per-Layer Per-Head Vision Token Pruning for Efficient Large Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2502.14504)] [[cool](https://papers.cool/arxiv/2502.14504)] [[pdf](https://arxiv.org/pdf/2502.14504)]
> **Authors**: Yu Meng,Kaiyuan Li,Chenran Huang,Chen Gao,Xinlei Chen,Yong Li,Xiaoping Zhang
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 12 pages, 8 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities across a range of multimodal tasks. However, their inference efficiency is constrained by the large number of visual tokens processed during decoding. To address this challenge, we propose Per-Layer Per-Head Vision Token Pruning (PLPHP), a two-level fine-grained pruning method including Layer-Level Retention Rate Allocation and Head-Level Vision Token Pruning. Motivated by the Vision Token Re-attention phenomenon across decoder layers, we dynamically adjust token retention rates layer by layer. Layers that exhibit stronger attention to visual information preserve more vision tokens, while layers with lower vision attention are aggressively pruned. Furthermore, PLPHP applies pruning at the attention head level, enabling different heads within the same layer to independently retain critical context. Experiments on multiple benchmarks demonstrate that PLPHP delivers an 18% faster decoding speed and reduces the Key-Value Cache (KV Cache) size by over 50%, all at the cost of 0.46% average performance drop, while also achieving notable performance improvements in multi-image tasks. These results highlight the effectiveness of fine-grained token pruning and contribute to advancing the efficiency and scalability of LVLMs. Our source code will be made publicly available.

### LXLv2: Enhanced LiDAR Excluded Lean 3D Object Detection with Fusion of 4D Radar and Camera 
[[arxiv](https://arxiv.org/abs/2502.14503)] [[cool](https://papers.cool/arxiv/2502.14503)] [[pdf](https://arxiv.org/pdf/2502.14503)]
> **Authors**: Weiyi Xiong,Zean Zou,Qiuchi Zhao,Fengchun He,Bing Zhu
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: Accepted by IEEE Robotics and Automation Letters
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: As the previous state-of-the-art 4D radar-camera fusion-based 3D object detection method, LXL utilizes the predicted image depth distribution maps and radar 3D occupancy grids to assist the sampling-based image view transformation. However, the depth prediction lacks accuracy and consistency, and the concatenation-based fusion in LXL impedes the model robustness. In this work, we propose LXLv2, where modifications are made to overcome the limitations and improve the performance. Specifically, considering the position error in radar measurements, we devise a one-to-many depth supervision strategy via radar points, where the radar cross section (RCS) value is further exploited to adjust the supervision area for object-level depth consistency. Additionally, a channel and spatial attention-based fusion module named CSAFusion is introduced to improve feature adaptiveness. Experimental results on the View-of-Delft and TJ4DRadSet datasets show that the proposed LXLv2 can outperform LXL in detection accuracy, inference speed and robustness, demonstrating the effectiveness of the model.

### CrossFuse: Learning Infrared and Visible Image Fusion by Cross-Sensor Top-K Vision Alignment and Beyond 
[[arxiv](https://arxiv.org/abs/2502.14493)] [[cool](https://papers.cool/arxiv/2502.14493)] [[pdf](https://arxiv.org/pdf/2502.14493)]
> **Authors**: Yukai Shi,Cidan Shi,Zhipeng Weng,Yin Tian,Xiaoyu Xian,Liang Lin
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: IEEE T-CSVT. We mainly discuss the out-of-distribution challenges in infrared and visible image fusion
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Infrared and visible image fusion (IVIF) is increasingly applied in critical fields such as video surveillance and autonomous driving systems. Significant progress has been made in deep learning-based fusion methods. However, these models frequently encounter out-of-distribution (OOD) scenes in real-world applications, which severely impact their performance and reliability. Therefore, addressing the challenge of OOD data is crucial for the safe deployment of these models in open-world environments. Unlike existing research, our focus is on the challenges posed by OOD data in real-world applications and on enhancing the robustness and generalization of models. In this paper, we propose an infrared-visible fusion framework based on Multi-View Augmentation. For external data augmentation, Top-k Selective Vision Alignment is employed to mitigate distribution shifts between datasets by performing RGB-wise transformations on visible images. This strategy effectively introduces augmented samples, enhancing the adaptability of the model to complex real-world scenarios. Additionally, for internal data augmentation, self-supervised learning is established using Weak-Aggressive Augmentation. This enables the model to learn more robust and general feature representations during the fusion process, thereby improving robustness and generalization. Extensive experiments demonstrate that the proposed method exhibits superior performance and robustness across various conditions and environments. Our approach significantly enhances the reliability and stability of IVIF tasks in practical applications.

### Exploiting Deblurring Networks for Radiance Fields 
[[arxiv](https://arxiv.org/abs/2502.14454)] [[cool](https://papers.cool/arxiv/2502.14454)] [[pdf](https://arxiv.org/pdf/2502.14454)]
> **Authors**: Haeyun Choi,Heemin Yang,Janghyeok Han,Sunghyun Cho
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: In this paper, we propose DeepDeblurRF, a novel radiance field deblurring approach that can synthesize high-quality novel views from blurred training views with significantly reduced training time. DeepDeblurRF leverages deep neural network (DNN)-based deblurring modules to enjoy their deblurring performance and computational efficiency. To effectively combine DNN-based deblurring and radiance field construction, we propose a novel radiance field (RF)-guided deblurring and an iterative framework that performs RF-guided deblurring and radiance field construction in an alternating manner. Moreover, DeepDeblurRF is compatible with various scene representations, such as voxel grids and 3D Gaussians, expanding its applicability. We also present BlurRF-Synth, the first large-scale synthetic dataset for training radiance field deblurring frameworks. We conduct extensive experiments on both camera motion blur and defocus blur, demonstrating that DeepDeblurRF achieves state-of-the-art novel-view synthesis quality with significantly reduced training time.

### Stochastic Resonance Improves the Detection of Low Contrast Images in Deep Learning Models 
[[arxiv](https://arxiv.org/abs/2502.14442)] [[cool](https://papers.cool/arxiv/2502.14442)] [[pdf](https://arxiv.org/pdf/2502.14442)]
> **Authors**: Siegfried Ludwig
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: MSc Course Project
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Stochastic resonance describes the utility of noise in improving the detectability of weak signals in certain types of systems. It has been observed widely in natural and engineered settings, but its utility in image classification with rate-based neural networks has not been studied extensively. In this analysis a simple LSTM recurrent neural network is trained for digit recognition and classification. During the test phase, image contrast is reduced to a point where the model fails to recognize the presence of a stimulus. Controlled noise is added to partially recover classification performance. The results indicate the presence of stochastic resonance in rate-based recurrent neural networks.

### Daily Land Surface Temperature Reconstruction in Landsat Cross-Track Areas Using Deep Ensemble Learning With Uncertainty Quantification 
[[arxiv](https://arxiv.org/abs/2502.14433)] [[cool](https://papers.cool/arxiv/2502.14433)] [[pdf](https://arxiv.org/pdf/2502.14433)]
> **Authors**: Shengjie Liu,Siqin Wang,Lu Zhang
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Many real-world applications rely on land surface temperature (LST) data at high spatiotemporal resolution. In complex urban areas, LST exhibits significant variations, fluctuating dramatically within and across city blocks. Landsat provides high spatial resolution data at 100 meters but is limited by long revisit time, with cloud cover further disrupting data collection. Here, we propose DELAG, a deep ensemble learning method that integrates annual temperature cycles and Gaussian processes, to reconstruct Landsat LST in complex urban areas. Leveraging the cross-track characteristics and dual-satellite operation of Landsat since 2021, we further enhance data availability to 4 scenes every 16 days. We select New York City, London and Hong Kong from three different continents as study areas. Experiments show that DELAG successfully reconstructed LST in the three cities under clear-sky (RMSE = 0.73-0.96 K) and heavily-cloudy (RMSE = 0.84-1.62 K) situations, superior to existing methods. Additionally, DELAG can quantify uncertainty that enhances LST reconstruction reliability. We further tested the reconstructed LST to estimate near-surface air temperature, achieving results (RMSE = 1.48-2.11 K) comparable to those derived from clear-sky LST (RMSE = 1.63-2.02 K). The results demonstrate the successful reconstruction through DELAG and highlight the broader applications of LST reconstruction for estimating accurate air temperature. Our study thus provides a novel and practical method for Landsat LST reconstruction, particularly suited for complex urban areas within Landsat cross-track areas, taking one step toward addressing complex climate events at high spatiotemporal resolution.

### Evaluating Precise Geolocation Inference Capabilities of Vision Language Models 
[[arxiv](https://arxiv.org/abs/2502.14412)] [[cool](https://papers.cool/arxiv/2502.14412)] [[pdf](https://arxiv.org/pdf/2502.14412)]
> **Authors**: Neel Jay,Hieu Minh Nguyen,Trung Dung Hoang,Jacob Haimes
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: AAAI 2025 Workshop DATASAFE
- **标题**: None
- **领域**: 计算机视觉和模式识别,密码学和安全,机器学习
- **Abstract**: The prevalence of Vision-Language Models (VLMs) raises important questions about privacy in an era where visual information is increasingly available. While foundation VLMs demonstrate broad knowledge and learned capabilities, we specifically investigate their ability to infer geographic location from previously unseen image data. This paper introduces a benchmark dataset collected from Google Street View that represents its global distribution of coverage. Foundation models are evaluated on single-image geolocation inference, with many achieving median distance errors of <300 km. We further evaluate VLM "agents" with access to supplemental tools, observing up to a 30.6% decrease in distance error. Our findings establish that modern foundation VLMs can act as powerful image geolocation tools, without being specifically trained for this task. When coupled with increasing accessibility of these models, our findings have greater implications for online privacy. We discuss these risks, as well as future work in this area.

### RelaCtrl: Relevance-Guided Efficient Control for Diffusion Transformers 
[[arxiv](https://arxiv.org/abs/2502.14377)] [[cool](https://papers.cool/arxiv/2502.14377)] [[pdf](https://arxiv.org/pdf/2502.14377)]
> **Authors**: Ke Cao,Jing Wang,Ao Ma,Jiasong Feng,Zhanjie Zhang,Xuanhua He,Shanyuan Liu,Bo Cheng,Dawei Leng,Yuhui Yin,Jie Zhang
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: Homepage: https://360cvgroup.github.io/RelaCtrl/ Github: https://github.com/360CVGroup/RelaCtrl
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: The Diffusion Transformer plays a pivotal role in advancing text-to-image and text-to-video generation, owing primarily to its inherent scalability. However, existing controlled diffusion transformer methods incur significant parameter and computational overheads and suffer from inefficient resource allocation due to their failure to account for the varying relevance of control information across different transformer layers. To address this, we propose the Relevance-Guided Efficient Controllable Generation framework, RelaCtrl, enabling efficient and resource-optimized integration of control signals into the Diffusion Transformer. First, we evaluate the relevance of each layer in the Diffusion Transformer to the control information by assessing the "ControlNet Relevance Score"-i.e., the impact of skipping each control layer on both the quality of generation and the control effectiveness during inference. Based on the strength of the relevance, we then tailor the positioning, parameter scale, and modeling capacity of the control layers to reduce unnecessary parameters and redundant computations. Additionally, to further improve efficiency, we replace the self-attention and FFN in the commonly used copy block with the carefully designed Two-Dimensional Shuffle Mixer (TDSM), enabling efficient implementation of both the token mixer and channel mixer. Both qualitative and quantitative experimental results demonstrate that our approach achieves superior performance with only 15% of the parameters and computational complexity compared to PixArt-delta.

### CrossVTON: Mimicking the Logic Reasoning on Cross-category Virtual Try-on guided by Tri-zone Priors 
[[arxiv](https://arxiv.org/abs/2502.14373)] [[cool](https://papers.cool/arxiv/2502.14373)] [[pdf](https://arxiv.org/pdf/2502.14373)]
> **Authors**: Donghao Luo,Yujie Liang,Xu Peng,Xiaobin Hu,Boyuan Jiang,Chengming Xu,Taisong Jin,Chengjie Wang,Yanwei Fu
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Despite remarkable progress in image-based virtual try-on systems, generating realistic and robust fitting images for cross-category virtual try-on remains a challenging task. The primary difficulty arises from the absence of human-like reasoning, which involves addressing size mismatches between garments and models while recognizing and leveraging the distinct functionalities of various regions within the model images. To address this issue, we draw inspiration from human cognitive processes and disentangle the complex reasoning required for cross-category try-on into a structured framework. This framework systematically decomposes the model image into three distinct regions: try-on, reconstruction, and imagination zones. Each zone plays a specific role in accommodating the garment and facilitating realistic synthesis. To endow the model with robust reasoning capabilities for cross-category scenarios, we propose an iterative data constructor. This constructor encompasses diverse scenarios, including intra-category try-on, any-to-dress transformations (replacing any garment category with a dress), and dress-to-any transformations (replacing a dress with another garment category). Utilizing the generated dataset, we introduce a tri-zone priors generator that intelligently predicts the try-on, reconstruction, and imagination zones by analyzing how the input garment is expected to align with the model image. Guided by these tri-zone priors, our proposed method, CrossVTON, achieves state-of-the-art performance, surpassing existing baselines in both qualitative and quantitative evaluations. Notably, it demonstrates superior capability in handling cross-category virtual try-on, meeting the complex demands of real-world applications.

### Weed Detection using Convolutional Neural Network 
[[arxiv](https://arxiv.org/abs/2502.14360)] [[cool](https://papers.cool/arxiv/2502.14360)] [[pdf](https://arxiv.org/pdf/2502.14360)]
> **Authors**: Santosh Kumar Tripathi,Shivendra Pratap Singh,Devansh Sharma,Harshavardhan U Patekar
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: In this paper we use convolutional neural networks (CNNs) for weed detection in agricultural land. We specifically investigate the application of two CNN layer types, Conv2d and dilated Conv2d, for weed detection in crop fields. The suggested method extracts features from the input photos using pre-trained models, which are subsequently adjusted for weed detection. The findings of the experiment, which used a sizable collection of dataset consisting of 15336 segments, being 3249 of soil, 7376 of soybean, 3520 grass and 1191 of broadleaf weeds. show that the suggested approach can accurately and successfully detect weeds at an accuracy of 94%. This study has significant ramifications for lowering the usage of toxic herbicides and increasing the effectiveness of weed management in agriculture.

### Towards Accurate Binary Spiking Neural Networks: Learning with Adaptive Gradient Modulation Mechanism 
[[arxiv](https://arxiv.org/abs/2502.14344)] [[cool](https://papers.cool/arxiv/2502.14344)] [[pdf](https://arxiv.org/pdf/2502.14344)]
> **Authors**: Yu Liang,Wenjie Wei,Ammar Belatreche,Honglin Cao,Zijian Zhou,Shuai Wang,Malu Zhang,Yang Yang
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 9 pages, 8 figures, AAAI conference
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Binary Spiking Neural Networks (BSNNs) inherit the eventdriven paradigm of SNNs, while also adopting the reduced storage burden of binarization techniques. These distinct advantages grant BSNNs lightweight and energy-efficient characteristics, rendering them ideal for deployment on resource-constrained edge devices. However, due to the binary synaptic weights and non-differentiable spike function, effectively training BSNNs remains an open question. In this paper, we conduct an in-depth analysis of the challenge for BSNN learning, namely the frequent weight sign flipping problem. To mitigate this issue, we propose an Adaptive Gradient Modulation Mechanism (AGMM), which is designed to reduce the frequency of weight sign flipping by adaptively adjusting the gradients during the learning process. The proposed AGMM can enable BSNNs to achieve faster convergence speed and higher accuracy, effectively narrowing the gap between BSNNs and their full-precision equivalents. We validate AGMM on both static and neuromorphic datasets, and results indicate that it achieves state-of-the-art results among BSNNs. This work substantially reduces storage demands and enhances SNNs' inherent energy efficiency, making them highly feasible for resource-constrained environments.

### A Collaborative Jade Recognition System for Mobile Devices Based on Lightweight and Large Models 
[[arxiv](https://arxiv.org/abs/2502.14332)] [[cool](https://papers.cool/arxiv/2502.14332)] [[pdf](https://arxiv.org/pdf/2502.14332)]
> **Authors**: Zhenyu Wang,Wenjia Li,Pengyu Zhu
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,信息检索
- **Abstract**: With the widespread adoption and development of mobile devices, vision-based recognition applications have become a hot topic in research. Jade, as an important cultural heritage and artistic item, has significant applications in fields such as jewelry identification and cultural relic preservation. However, existing jade recognition systems still face challenges in mobile implementation, such as limited computing resources, real-time requirements, and accuracy issues. To address these challenges, this paper proposes a jade recognition system based on size model collaboration, aiming to achieve efficient and accurate jade identification using mobile devices such as smartphones.First, we design a size model based on multi-scale image processing, extracting key visual information by analyzing jade's dimensions, shapes, and surface textures. Then, a collaborative multi-model classification framework is built by combining deep learning and traditional computer vision algorithms. This framework can effectively select and adjust models based on different jade characteristics, providing high accuracy results across various environments and devices.Experimental results show that the proposed system can provide high recognition accuracy and fast processing time on mobile devices, while consuming relatively low computational resources. The system not only holds great application potential but also provides new ideas and technical support for the intelligent development of jade identification.

### OrchardDepth: Precise Metric Depth Estimation of Orchard Scene from Monocular Camera Images 
[[arxiv](https://arxiv.org/abs/2502.14279)] [[cool](https://papers.cool/arxiv/2502.14279)] [[pdf](https://arxiv.org/pdf/2502.14279)]
> **Authors**: Zhichao Zheng,Henry Williams,Bruce A MacDonald
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 10 pages, 5 figures, Australasian Conference on Robotics and Automation, ACRA, 2024
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Monocular depth estimation is a rudimentary task in robotic perception. Recently, with the development of more accurate and robust neural network models and different types of datasets, monocular depth estimation has significantly improved performance and efficiency. However, most of the research in this area focuses on very concentrated domains. In particular, most of the benchmarks in outdoor scenarios belong to urban environments for the improvement of autonomous driving devices, and these benchmarks have a massive disparity with the orchard/vineyard environment, which is hardly helpful for research in the primary industry. Therefore, we propose OrchardDepth, which fills the gap in the estimation of the metric depth of the monocular camera in the orchard/vineyard environment. In addition, we present a new retraining method to improve the training result by monitoring the consistent regularization between dense depth maps and sparse points. Our method improves the RMSE of depth estimation in the orchard environment from 1.5337 to 0.6738, proving our method's validation.

### LLM-EvRep: Learning an LLM-Compatible Event Representation Using a Self-Supervised Framework 
[[arxiv](https://arxiv.org/abs/2502.14273)] [[cool](https://papers.cool/arxiv/2502.14273)] [[pdf](https://arxiv.org/pdf/2502.14273)]
> **Authors**: Zongyou Yu,Qiang Qu,Qian Zhang,Nan Zhang,Xiaoming Chen
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 6 pages, 2 figures,Companion Proceedings of the ACM Web Conference 2025 (WWW Companion '25)
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,多媒体
- **Abstract**: Recent advancements in event-based recognition have demonstrated significant promise, yet most existing approaches rely on extensive training, limiting their adaptability for efficient processing of event-driven visual content. Meanwhile, large language models (LLMs) have exhibited remarkable zero-shot capabilities across diverse domains, but their application to event-based visual recognition remains largely unexplored. To bridge this gap, we propose \textbf{LLM-EvGen}, an event representation generator that produces LLM-compatible event representations \textbf{LLM-EvRep}, thereby enhancing the performance of LLMs on event recognition tasks. The generator is trained using a self-supervised framework, aligning the generated representations with semantic consistency and structural fidelity. Comprehensive experiments were conducted on three datasets: N-ImageNet, N-Caltech101, and N-MNIST. The results demonstrate that our method, \textbf{LLM-EvRep}, outperforms the event-to-video method, E2VID, by 15.93\%, 0.82\%, and 50.21\%, respectively, in recognition tasks when evaluated using GPT-4o.

## 计算机与社会(cs.CY:Computers and Society)

### Why do Experts Disagree on Existential Risk and P(doom)? A Survey of AI Experts 
[[arxiv](https://arxiv.org/abs/2502.14870)] [[cool](https://papers.cool/arxiv/2502.14870)] [[pdf](https://arxiv.org/pdf/2502.14870)]
> **Authors**: Severin Field
> **First submission**: 2025-01-24
> **First announcement**: 2025-02-21
> **comment**: In submission toAIand Ethics Journal. 24 pages total, 15 pages of writing with 9 pages of appendices
- **标题**: None
- **领域**: 计算机与社会,人工智能,人机交互
- **Abstract**: The development of artificial general intelligence (AGI) is likely to be one of humanity's most consequential technological advancements. Leading AI labs and scientists have called for the global prioritization of AI safety citing existential risks comparable to nuclear war. However, research on catastrophic risks and AI alignment is often met with skepticism, even by experts. Furthermore, online debate over the existential risk of AI has begun to turn tribal (e.g. name-calling such as "doomer" or "accelerationist"). Until now, no systematic study has explored the patterns of belief and the levels of familiarity with AI safety concepts among experts. I surveyed 111 AI experts on their familiarity with AI safety concepts, key objections to AI safety, and reactions to safety arguments. My findings reveal that AI experts cluster into two viewpoints -- an "AI as controllable tool" and an "AI as uncontrollable agent" perspective -- diverging in beliefs toward the importance of AI safety. While most experts (78%) agreed or strongly agreed that "technical AI researchers should be concerned about catastrophic risks", many were unfamiliar with specific AI safety concepts. For example, only 21% of surveyed experts had heard of "instrumental convergence," a fundamental concept in AI safety predicting that advanced AI systems will tend to pursue common sub-goals (such as self-preservation). The least concerned participants were the least familiar with concepts like this, suggesting that effective communication of AI safety should begin with establishing clear conceptual foundations in the field.

### Envisioning Stakeholder-Action Pairs to Mitigate Negative Impacts of AI: A Participatory Approach to Inform Policy Making 
[[arxiv](https://arxiv.org/abs/2502.14869)] [[cool](https://papers.cool/arxiv/2502.14869)] [[pdf](https://arxiv.org/pdf/2502.14869)]
> **Authors**: Julia Barnett,Kimon Kieslich,Natali Helberger,Nicholas Diakopoulos
> **First submission**: 2025-01-24
> **First announcement**: 2025-02-21
> **comment**: 14 pages + supplementary information and appendix
- **标题**: None
- **领域**: 计算机与社会,人工智能,人机交互
- **Abstract**: The potential for negative impacts of AI has rapidly become more pervasive around the world, and this has intensified a need for responsible AI governance. While many regulatory bodies endorse risk-based approaches and a multitude of risk mitigation practices are proposed by companies and academic scholars, these approaches are commonly expert-centered and thus lack the inclusion of a significant group of stakeholders. Ensuring that AI policies align with democratic expectations requires methods that prioritize the voices and needs of those impacted. In this work we develop a participative and forward-looking approach to inform policy-makers and academics that grounds the needs of lay stakeholders at the forefront and enriches the development of risk mitigation strategies. Our approach (1) maps potential mitigation and prevention strategies of negative AI impacts that assign responsibility to various stakeholders, (2) explores the importance and prioritization thereof in the eyes of laypeople, and (3) presents these insights in policy fact sheets, i.e., a digestible format for informing policy processes. We emphasize that this approach is not targeted towards replacing policy-makers; rather our aim is to present an informative method that enriches mitigation strategies and enables a more participatory approach to policy development.

### Unlocking the Black Box: Analysing the EU Artificial Intelligence Act's Framework for Explainability in AI 
[[arxiv](https://arxiv.org/abs/2502.14868)] [[cool](https://papers.cool/arxiv/2502.14868)] [[pdf](https://arxiv.org/pdf/2502.14868)]
> **Authors**: Georgios Pavlidis
> **First submission**: 2025-01-24
> **First announcement**: 2025-02-21
> **comment**: ef:Law Innovation and Technology, Vol. 16 (1), 2024, pp. 293-308
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: The lack of explainability of Artificial Intelligence (AI) is one of the first obstacles that the industry and regulators must overcome to mitigate the risks associated with the technology. The need for eXplainable AI (XAI) is evident in fields where accountability, ethics and fairness are critical, such as healthcare, credit scoring, policing and the criminal justice system. At the EU level, the notion of explainability is one of the fundamental principles that underpin the AI Act, though the exact XAI techniques and requirements are still to be determined and tested in practice. This paper explores various approaches and techniques that promise to advance XAI, as well as the challenges of implementing the principle of explainability in AI governance and policies. Finally, the paper examines the integration of XAI into EU law, emphasising the issues of standard setting, oversight, and enforcement.

## 数据库(cs.DB:Databases)

### LEDD: Large Language Model-Empowered Data Discovery in Data Lakes 
[[arxiv](https://arxiv.org/abs/2502.15182)] [[cool](https://papers.cool/arxiv/2502.15182)] [[pdf](https://arxiv.org/pdf/2502.15182)]
> **Authors**: Qi An,Chihua Ying,Yuqing Zhu,Yihao Xu,Manwei Zhang,Jianmin Wang
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 数据库,人工智能
- **Abstract**: Data discovery in data lakes with ever increasing datasets has long been recognized as a big challenge in the realm of data management, especially for semantic search of and hierarchical global catalog generation of tables. While large language models (LLMs) facilitate the processing of data semantics, challenges remain in architecting an end-to-end system that comprehensively exploits LLMs for the two semantics-related tasks. In this demo, we propose LEDD, an end-to-end system with an extensible architecture that leverages LLMs to provide hierarchical global catalogs with semantic meanings and semantic table search for data lakes. Specifically, LEDD can return semantically related tables based on natural-language specification. These features make LEDD an ideal foundation for downstream tasks such as model training and schema linking for text-to-SQL tasks. LEDD also provides a simple Python interface to facilitate the extension and the replacement of data discovery algorithms.

### Real-Time Device Reach Forecasting Using HLL and MinHash Data Sketches 
[[arxiv](https://arxiv.org/abs/2502.14785)] [[cool](https://papers.cool/arxiv/2502.14785)] [[pdf](https://arxiv.org/pdf/2502.14785)]
> **Authors**: Chandrashekar Muniyappa,Kendall Willets,Sriraman Krishnamoorthy
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: :60G25ACM Class:I.5.3
- **标题**: None
- **领域**: 数据库,人工智能,机器学习
- **Abstract**: Predicting the right number of TVs (Device Reach) in real-time based on a user-specified targeting attributes is imperative for running multi-million dollar ADs business. The traditional approach of SQL queries to join billions of records across multiple targeting dimensions is extremely slow. As a workaround, many applications will have an offline process to crunch these numbers and present the results after many hours. In our case, the solution was an offline process taking 24 hours to onboard a customer resulting in a potential loss of business. To solve this problem, we have built a new real-time prediction system using MinHash and HyperLogLog (HLL) data sketches to compute the device reach at runtime when a user makes a request. However, existing MinHash implementations do not solve the complex problem of multilevel aggregation and intersection. This work will show how we have solved this problem, in addition, we have improved MinHash algorithm to run 4 times faster using Single Instruction Multiple Data (SIMD) vectorized operations for high speed and accuracy with constant space to process billions of records. Finally, by experiments, we prove that the results are as accurate as traditional offline prediction system with an acceptable error rate of 5%.

### SQL4NN: Validation and expressive querying of models as data 
[[arxiv](https://arxiv.org/abs/2502.14745)] [[cool](https://papers.cool/arxiv/2502.14745)] [[pdf](https://arxiv.org/pdf/2502.14745)]
> **Authors**: Mark Gerarts,Juno Steegmans,Jan Van den Bussche
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 数据库,机器学习
- **Abstract**: We consider machine learning models, learned from data, to be an important, intensional, kind of data in themselves. As such, various analysis tasks on models can be thought of as queries over this intensional data, often combined with extensional data such as data for training or validation. We demonstrate that relational database systems and SQL can actually be well suited for many such tasks.

### Optimize Cardinality Estimation Model Pretraining by Simplifying the Training Datasets 
[[arxiv](https://arxiv.org/abs/2502.14350)] [[cool](https://papers.cool/arxiv/2502.14350)] [[pdf](https://arxiv.org/pdf/2502.14350)]
> **Authors**: Boyang Fang
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 数据库,机器学习
- **Abstract**: The cardinality estimation is a key aspect of query optimization research, and its performance has significantly improved with the integration of machine learning. To overcome the "cold start" problem or the lack of model transferability in learned cardinality estimators, some pre-training cardinality estimation models have been proposed that use learning across multiple datasets and corresponding workloads. These models typically train on a dataset created by uniformly sampling from many datasets, but this approach may not be optimal. By applying the Group Distributionally Robust Optimization (Group DRO) algorithm to training datasets, we find that some specific training datasets contribute more significantly to model performance than others. Based on this observation, we conduct extensive experiments to delve deeper into pre-training cardinality estimators. Our results show how the performance of these models can be influenced by the datasets and corresponding workloads. Finally, we introduce a simplified training dataset, which has been reduced to a fraction of the size of existing pretraining datasets. Sufficient experimental results demonstrate that the pre-trained cardinality estimator based on this simplified dataset can still achieve comparable performance to existing models in zero-shot setups.

## 数据结构和算法(cs.DS:Data Structures and Algorithms)

### Efficient Multivariate Robust Mean Estimation Under Mean-Shift Contamination 
[[arxiv](https://arxiv.org/abs/2502.14772)] [[cool](https://papers.cool/arxiv/2502.14772)] [[pdf](https://arxiv.org/pdf/2502.14772)]
> **Authors**: Ilias Diakonikolas,Giannis Iakovidis,Daniel M. Kane,Thanasis Pittas
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 数据结构和算法,机器学习,统计理论,机器学习
- **Abstract**: We study the algorithmic problem of robust mean estimation of an identity covariance Gaussian in the presence of mean-shift contamination. In this contamination model, we are given a set of points in $\mathbb{R}^d$ generated i.i.d. via the following process. For a parameter $α<1/2$, the $i$-th sample $x_i$ is obtained as follows: with probability $1-α$, $x_i$ is drawn from $\mathcal{N}(μ, I)$, where $μ\in \mathbb{R}^d$ is the target mean; and with probability $α$, $x_i$ is drawn from $\mathcal{N}(z_i, I)$, where $z_i$ is unknown and potentially arbitrary. Prior work characterized the information-theoretic limits of this task. Specifically, it was shown that, in contrast to Huber contamination, in the presence of mean-shift contamination consistent estimation is possible. On the other hand, all known robust estimators in the mean-shift model have running times exponential in the dimension. Here we give the first computationally efficient algorithm for high-dimensional robust mean estimation with mean-shift contamination that can tolerate a constant fraction of outliers. In particular, our algorithm has near-optimal sample complexity, runs in sample-polynomial time, and approximates the target mean to any desired accuracy. Conceptually, our result contributes to a growing body of work that studies inference with respect to natural noise models lying in between fully adversarial and random settings.

## 图形(cs.GR:Graphics)

### Dynamic Concepts Personalization from Single Videos 
[[arxiv](https://arxiv.org/abs/2502.14844)] [[cool](https://papers.cool/arxiv/2502.14844)] [[pdf](https://arxiv.org/pdf/2502.14844)]
> **Authors**: Rameen Abdal,Or Patashnik,Ivan Skorokhodov,Willi Menapace,Aliaksandr Siarohin,Sergey Tulyakov,Daniel Cohen-Or,Kfir Aberman
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: Webpage: https://snap-research.github.io/dynamic_concepts/
- **标题**: None
- **领域**: 图形,计算机视觉和模式识别,机器学习
- **Abstract**: Personalizing generative text-to-image models has seen remarkable progress, but extending this personalization to text-to-video models presents unique challenges. Unlike static concepts, personalizing text-to-video models has the potential to capture dynamic concepts, i.e., entities defined not only by their appearance but also by their motion. In this paper, we introduce Set-and-Sequence, a novel framework for personalizing Diffusion Transformers (DiTs)-based generative video models with dynamic concepts. Our approach imposes a spatio-temporal weight space within an architecture that does not explicitly separate spatial and temporal features. This is achieved in two key stages. First, we fine-tune Low-Rank Adaptation (LoRA) layers using an unordered set of frames from the video to learn an identity LoRA basis that represents the appearance, free from temporal interference. In the second stage, with the identity LoRAs frozen, we augment their coefficients with Motion Residuals and fine-tune them on the full video sequence, capturing motion dynamics. Our Set-and-Sequence framework results in a spatio-temporal weight space that effectively embeds dynamic concepts into the video model's output domain, enabling unprecedented editability and compositionality while setting a new benchmark for personalizing dynamic concepts.

### Single-image Reflectance and Transmittance Estimation from Any Flatbed Scanner 
[[arxiv](https://arxiv.org/abs/2502.14462)] [[cool](https://papers.cool/arxiv/2502.14462)] [[pdf](https://arxiv.org/pdf/2502.14462)]
> **Authors**: Carlos Rodriguez-Pardo,David Pascual-Hernandez,Javier Rodriguez-Vazquez,Jorge Lopez-Moreno,Elena Garces
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: Accepted to Computers & Graphics
- **标题**: None
- **领域**: 图形,人工智能,计算机视觉和模式识别,机器学习
- **Abstract**: Flatbed scanners have emerged as promising devices for high-resolution, single-image material capture. However, existing approaches assume very specific conditions, such as uniform diffuse illumination, which are only available in certain high-end devices, hindering their scalability and cost. In contrast, in this work, we introduce a method inspired by intrinsic image decomposition, which accurately removes both shading and specularity, effectively allowing captures with any flatbed scanner. Further, we extend previous work on single-image material reflectance capture with the estimation of opacity and transmittance, critical components of full material appearance (SVBSDF), improving the results for any material captured with a flatbed scanner, at a very high resolution and accuracy

## 人机交互(cs.HC:Human-Computer Interaction)

### BP-GPT: Auditory Neural Decoding Using fMRI-prompted LLM 
[[arxiv](https://arxiv.org/abs/2502.15172)] [[cool](https://papers.cool/arxiv/2502.15172)] [[pdf](https://arxiv.org/pdf/2502.15172)]
> **Authors**: Xiaoyu Chen,Changde Du,Che Liu,Yizhe Wang,Huiguang He
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: arXiv admin note: substantial text overlap with arXiv:2405.07840
- **标题**: None
- **领域**: 人机交互,计算语言学
- **Abstract**: Decoding language information from brain signals represents a vital research area within brain-computer interfaces, particularly in the context of deciphering the semantic information from the fMRI signal. Although existing work uses LLM to achieve this goal, their method does not use an end-to-end approach and avoids the LLM in the mapping of fMRI-to-text, leaving space for the exploration of the LLM in auditory decoding. In this paper, we introduce a novel method, the Brain Prompt GPT (BP-GPT). By using the brain representation that is extracted from the fMRI as a prompt, our method can utilize GPT-2 to decode fMRI signals into stimulus text. Further, we introduce the text prompt and align the fMRI prompt to it. By introducing the text prompt, our BP-GPT can extract a more robust brain prompt and promote the decoding of pre-trained LLM. We evaluate our BP-GPT on the open-source auditory semantic decoding dataset and achieve a significant improvement up to 4.61 on METEOR and 2.43 on BERTScore across all the subjects compared to the state-of-the-art method. The experimental results demonstrate that using brain representation as a prompt to further drive LLM for auditory neural decoding is feasible and effective. The code is available at https://github.com/1994cxy/BP-GPT.

### The Impact and Feasibility of Self-Confidence Shaping for AI-Assisted Decision-Making 
[[arxiv](https://arxiv.org/abs/2502.14311)] [[cool](https://papers.cool/arxiv/2502.14311)] [[pdf](https://arxiv.org/pdf/2502.14311)]
> **Authors**: Takehiro Takayanagi,Ryuji Hashimoto,Chung-Chi Chen,Kiyoshi Izumi
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 人机交互,计算语言学,计算机与社会
- **Abstract**: In AI-assisted decision-making, it is crucial but challenging for humans to appropriately rely on AI, especially in high-stakes domains such as finance and healthcare. This paper addresses this problem from a human-centered perspective by presenting an intervention for self-confidence shaping, designed to calibrate self-confidence at a targeted level. We first demonstrate the impact of self-confidence shaping by quantifying the upper-bound improvement in human-AI team performance. Our behavioral experiments with 121 participants show that self-confidence shaping can improve human-AI team performance by nearly 50% by mitigating both over- and under-reliance on AI. We then introduce a self-confidence prediction task to identify when our intervention is needed. Our results show that simple machine-learning models achieve 67% accuracy in predicting self-confidence. We further illustrate the feasibility of such interventions. The observed relationship between sentiment and self-confidence suggests that modifying sentiment could be a viable strategy for shaping self-confidence. Finally, we outline future research directions to support the deployment of self-confidence shaping in a real-world scenario for effective human-AI collaboration.

## 信息检索(cs.IR:Information Retrieval)

### EAGER-LLM: Enhancing Large Language Models as Recommenders through Exogenous Behavior-Semantic Integration 
[[arxiv](https://arxiv.org/abs/2502.14735)] [[cool](https://papers.cool/arxiv/2502.14735)] [[pdf](https://arxiv.org/pdf/2502.14735)]
> **Authors**: Minjie Hong,Yan Xia,Zehan Wang,Jieming Zhu,Ye Wang,Sihang Cai,Xiaoda Yang,Quanyu Dai,Zhenhua Dong,Zhimeng Zhang,Zhou Zhao
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 9 pages, 6 figures, accpeted by WWW 2025
- **标题**: None
- **领域**: 信息检索,人工智能
- **Abstract**: Large language models (LLMs) are increasingly leveraged as foundational backbones in the development of advanced recommender systems, offering enhanced capabilities through their extensive knowledge and reasoning. Existing llm-based recommender systems (RSs) often face challenges due to the significant differences between the linguistic semantics of pre-trained LLMs and the collaborative semantics essential for RSs. These systems use pre-trained linguistic semantics but learn collaborative semantics from scratch via the llm-Backbone. However, LLMs are not designed for recommendations, leading to inefficient collaborative learning, weak result correlations, and poor integration of traditional RS features. To address these challenges, we propose EAGER-LLM, a decoder-only llm-based generative recommendation framework that integrates endogenous and exogenous behavioral and semantic information in a non-intrusive manner. Specifically, we propose 1)dual-source knowledge-rich item indices that integrates indexing sequences for exogenous signals, enabling efficient link-wide processing; 2)non-invasive multiscale alignment reconstruction tasks guide the model toward a deeper understanding of both collaborative and semantic signals; 3)an annealing adapter designed to finely balance the model's recommendation performance with its comprehension capabilities. We demonstrate EAGER-LLM's effectiveness through rigorous testing on three public benchmarks.

### Efficient AI in Practice: Training and Deployment of Efficient LLMs for Industry Applications 
[[arxiv](https://arxiv.org/abs/2502.14305)] [[cool](https://papers.cool/arxiv/2502.14305)] [[pdf](https://arxiv.org/pdf/2502.14305)]
> **Authors**: Kayhan Behdin,Yun Dai,Ata Fatahibaarzi,Aman Gupta,Qingquan Song,Shao Tang,Hejian Sang,Gregory Dexter,Sirou Zhu,Siyu Zhu,Tejas Dharamsi,Maziar Sanjabi,Vignesh Kothapalli,Hamed Firooz,Zhoutong Fu,Yihan Cao,Pin-Lun Hsu,Fedor Borisyuk,Zhipeng Wang,Rahul Mazumder,Natesh Pillai,Luke Simon
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,机器学习
- **Abstract**: Large language models (LLMs) have demonstrated remarkable performance across a wide range of industrial applications, from search and recommendations to generative tasks. Although scaling laws indicate that larger models generally yield better generalization and performance, their substantial computational requirements often render them impractical for many real-world scenarios at scale. In this paper, we present methods and insights for training small language models (SLMs) that deliver high performance and efficiency in deployment. We focus on two key techniques: (1) knowledge distillation and (2) model compression via quantization and pruning. These approaches enable SLMs to retain much of the quality of their larger counterparts while significantly reducing training, serving costs, and latency. We detail the impact of these techniques on a variety of use cases at a large professional social network platform and share deployment lessons - including hardware optimization strategies that enhance speed and throughput for both predictive and reasoning-based applications.

### Evaluating Sakana's AI Scientist for Autonomous Research: Wishful Thinking or an Emerging Reality Towards 'Artificial Research Intelligence' (ARI)? 
[[arxiv](https://arxiv.org/abs/2502.14297)] [[cool](https://papers.cool/arxiv/2502.14297)] [[pdf](https://arxiv.org/pdf/2502.14297)]
> **Authors**: Joeran Beel,Min-Yen Kan,Moritz Baumgart
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 16 pages
- **标题**: None
- **领域**: 信息检索,人工智能,机器学习
- **Abstract**: A major step toward Artificial General Intelligence (AGI) and Super Intelligence is AI's ability to autonomously conduct research - what we term Artificial Research Intelligence (ARI). If machines could generate hypotheses, conduct experiments, and write research papers without human intervention, it would transform science. Sakana recently introduced the 'AI Scientist', claiming to conduct research autonomously, i.e. they imply to have achieved what we term Artificial Research Intelligence (ARI). The AI Scientist gained much attention, but a thorough independent evaluation has yet to be conducted. Our evaluation of the AI Scientist reveals critical shortcomings. The system's literature reviews produced poor novelty assessments, often misclassifying established concepts (e.g., micro-batching for stochastic gradient descent) as novel. It also struggles with experiment execution: 42% of experiments failed due to coding errors, while others produced flawed or misleading results. Code modifications were minimal, averaging 8% more characters per iteration, suggesting limited adaptability. Generated manuscripts were poorly substantiated, with a median of five citations, most outdated (only five of 34 from 2020 or later). Structural errors were frequent, including missing figures, repeated sections, and placeholder text like 'Conclusions Here'. Some papers contained hallucinated numerical results. Despite these flaws, the AI Scientist represents a leap forward in research automation. It generates full research manuscripts with minimal human input, challenging expectations of AI-driven science. Many reviewers might struggle to distinguish its work from human researchers. While its quality resembles a rushed undergraduate paper, its speed and cost efficiency are unprecedented, producing a full paper for USD 6 to 15 with 3.5 hours of human involvement, far outpacing traditional researchers.

## 机器学习(cs.LG:Machine Learning)

### PairBench: A Systematic Framework for Selecting Reliable Judge VLMs 
[[arxiv](https://arxiv.org/abs/2502.15210)] [[cool](https://papers.cool/arxiv/2502.15210)] [[pdf](https://arxiv.org/pdf/2502.15210)]
> **Authors**: Aarash Feizi,Sai Rajeswar,Adriana Romero-Soriano,Reihaneh Rabbany,Spandana Gella,Valentina Zantedeschi,João Monteiro
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: As large vision language models (VLMs) are increasingly used as automated evaluators, understanding their ability to effectively compare data pairs as instructed in the prompt becomes essential. To address this, we present PairBench, a low-cost framework that systematically evaluates VLMs as customizable similarity tools across various modalities and scenarios. Through PairBench, we introduce four metrics that represent key desiderata of similarity scores: alignment with human annotations, consistency for data pairs irrespective of their order, smoothness of similarity distributions, and controllability through prompting. Our analysis demonstrates that no model, whether closed- or open-source, is superior on all metrics; the optimal choice depends on an auto evaluator's desired behavior (e.g., a smooth vs. a sharp judge), highlighting risks of widespread adoption of VLMs as evaluators without thorough assessment. For instance, the majority of VLMs struggle with maintaining symmetric similarity scores regardless of order. Additionally, our results show that the performance of VLMs on the metrics in PairBench closely correlates with popular benchmarks, showcasing its predictive power in ranking models.

### Graph-Based Deep Learning on Stereo EEG for Predicting Seizure Freedom in Epilepsy Patients 
[[arxiv](https://arxiv.org/abs/2502.15198)] [[cool](https://papers.cool/arxiv/2502.15198)] [[pdf](https://arxiv.org/pdf/2502.15198)]
> **Authors**: Artur Agaronyan,Syeda Abeera Amir,Nunthasiri Wittayanakorn,John Schreiber,Marius G. Linguraru,William Gaillard,Chima Oluigbo,Syed Muhammad Anwar
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,信号处理,神经元和认知
- **Abstract**: Predicting seizure freedom is essential for tailoring epilepsy treatment. But accurate prediction remains challenging with traditional methods, especially with diverse patient populations. This study developed a deep learning-based graph neural network (GNN) model to predict seizure freedom from stereo electroencephalography (sEEG) data in patients with refractory epilepsy. We utilized high-quality sEEG data from 15 pediatric patients to train a deep learning model that can accurately predict seizure freedom outcomes and advance understanding of brain connectivity at the seizure onset zone. Our model integrates local and global connectivity using graph convolutions with multi-scale attention mechanisms to capture connections between difficult-to-study regions such as the thalamus and motor regions. The model achieved an accuracy of 92.4% in binary class analysis, 86.6% in patient-wise analysis, and 81.4% in multi-class analysis. Node and edge-level feature analysis highlighted the anterior cingulate and frontal pole regions as key contributors to seizure freedom outcomes. The nodes identified by our model were also more likely to coincide with seizure onset zones. Our findings underscore the potential of new connectivity-based deep learning models such as GNNs for enhancing the prediction of seizure freedom, predicting seizure onset zones, connectivity analysis of the brain during seizure, as well as informing AI-assisted personalized epilepsy treatment planning.

### Optimizing Product Provenance Verification using Data Valuation Methods 
[[arxiv](https://arxiv.org/abs/2502.15177)] [[cool](https://papers.cool/arxiv/2502.15177)] [[pdf](https://arxiv.org/pdf/2502.15177)]
> **Authors**: Raquib Bin Yousuf,Hoang Anh Just,Shengzhe Xu,Brian Mayer,Victor Deklerck,Jakub Truszkowski,John C. Simeone,Jade Saunders,Chang-Tien Lu,Ruoxi Jia,Naren Ramakrishnan
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机与社会
- **Abstract**: Determining and verifying product provenance remains a critical challenge in global supply chains, particularly as geopolitical conflicts and shifting borders create new incentives for misrepresentation of commodities, such as hiding the origin of illegally harvested timber or stolen agricultural products. Stable Isotope Ratio Analysis (SIRA), combined with Gaussian process regression-based isoscapes, has emerged as a powerful tool for geographic origin verification. However, the effectiveness of these models is often constrained by data scarcity and suboptimal dataset selection. In this work, we introduce a novel data valuation framework designed to enhance the selection and utilization of training data for machine learning models applied in SIRA. By prioritizing high-informative samples, our approach improves model robustness and predictive accuracy across diverse datasets and geographies. We validate our methodology with extensive experiments, demonstrating its potential to significantly enhance provenance verification, mitigate fraudulent trade practices, and strengthen regulatory enforcement of global supply chains.

### Projection Optimization: A General Framework for Multi-Objective and Multi-Group RLHF 
[[arxiv](https://arxiv.org/abs/2502.15145)] [[cool](https://papers.cool/arxiv/2502.15145)] [[pdf](https://arxiv.org/pdf/2502.15145)]
> **Authors**: Nuoya Xiong,Aarti Singh
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Reinforcement Learning with Human Feedback (RLHF) is a widely used fine-tuning approach that aligns machine learning model, particularly Language Model (LM) with human preferences. There are typically multiple objectives driving the preference, hence humans find it easier to express per-objective comparisons rather than a global preference between two choices. Multi-Objective RLHF (MORLHF) aims to use per-objective preference feedback and achieve Pareto optimality among these objectives by aggregating them into a single unified objective for optimization. However, nearly all prior works rely on linear aggregation, which rules out policies that favor specific objectives such as the worst one. The only existing approach using non-linear aggregation is computationally expensive due to its reward-based nature and the need for retraining whenever the aggregation parameters change. In this work, we address this limitation by transforming the non-linear aggregation maximization problem into a series of sub-problems. Each sub-problem involves only linear aggregation, making it computationally efficient to solve. We further extend our framework to handle multi-group scenarios, where each group has distinct weights for the objectives. Our method enables achieving consensus or maximizing the aggregated objective across all groups. Theoretically, we demonstrate that our algorithmic framework achieves sublinear regret and can be easily adapted to a reward-free algorithm. Empirically, leveraging our theoretical insights, we propose a nearly training-free algorithm once the optimal policies for individual objectives are obtained.

### Data Complexity Measures for Quantum Circuits Architecture Recommendation 
[[arxiv](https://arxiv.org/abs/2502.15129)] [[cool](https://papers.cool/arxiv/2502.15129)] [[pdf](https://arxiv.org/pdf/2502.15129)]
> **Authors**: Fernando M de Paula Neto
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,量子物理学
- **Abstract**: Quantum Parametric Circuits are constructed as an alternative to reduce the size of quantum circuits, meaning to decrease the number of quantum gates and, consequently, the depth of these circuits. However, determining the optimal circuit for a given problem remains an open question. Testing various combinations is challenging due to the infinite possibilities. In this work, a quantum circuit recommendation architecture for classification problems is proposed using database complexity measures. A quantum circuit is defined based on a circuit layer and the number of times this layer is iterated. Fourteen databases of varying dimensions and different numbers of classes were used to evaluate six quantum circuits, each with 1, 2, 3, 4, 8, and 16-layer repetitions. Using data complexity measures from the databases, it was possible to identify the optimal circuit capable of solving all problems with up to 100$\%$ accuracy. Furthermore, with a mean absolute error of 0.80 $\pm$ 2.17, one determined the appropriate number of layer repetitions, allowing for an error margin of up to three additional layers. Sixteen distinct machine learning models were employed for the selection of quantum circuits, alongside twelve classical regressor models to dynamically define the number of layers.

### MONSTER: Monash Scalable Time Series Evaluation Repository 
[[arxiv](https://arxiv.org/abs/2502.15122)] [[cool](https://papers.cool/arxiv/2502.15122)] [[pdf](https://arxiv.org/pdf/2502.15122)]
> **Authors**: Angus Dempster,Navid Mohammadi Foumani,Chang Wei Tan,Lynn Miller,Amish Mishra,Mahsa Salehi,Charlotte Pelletier,Daniel F. Schmidt,Geoffrey I. Webb
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 45 pages; 38 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We introduce MONSTER-the MONash Scalable Time Series Evaluation Repository-a collection of large datasets for time series classification. The field of time series classification has benefitted from common benchmarks set by the UCR and UEA time series classification repositories. However, the datasets in these benchmarks are small, with median sizes of 217 and 255 examples, respectively. In consequence they favour a narrow subspace of models that are optimised to achieve low classification error on a wide variety of smaller datasets, that is, models that minimise variance, and give little weight to computational issues such as scalability. Our hope is to diversify the field by introducing benchmarks using larger datasets. We believe that there is enormous potential for new progress in the field by engaging with the theoretical and practical challenges of learning effectively from larger quantities of data.

### Assessing a Single Student's Concentration on Learning Platforms: A Machine Learning-Enhanced EEG-Based Framework 
[[arxiv](https://arxiv.org/abs/2502.15107)] [[cool](https://papers.cool/arxiv/2502.15107)] [[pdf](https://arxiv.org/pdf/2502.15107)]
> **Authors**: Zewen Zhuo,Mohamad Najafi,Hazem Zein,Amine Nait-Ali
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: This study introduces a specialized pipeline designed to classify the concentration state of an individual student during online learning sessions by training a custom-tailored machine learning model. Detailed protocols for acquiring and preprocessing EEG data are outlined, along with the extraction of fifty statistical features from five EEG signal bands: alpha, beta, theta, delta, and gamma. Following feature extraction, a thorough feature selection process was conducted to optimize the data inputs for a personalized analysis. The study also explores the benefits of hyperparameter fine-tuning to enhance the classification accuracy of the student's concentration state. EEG signals were captured from the student using a Muse headband (Gen 2), equipped with five electrodes (TP9, AF7, AF8, TP10, and a reference electrode NZ), during engagement with educational content on computer-based e-learning platforms. Employing a random forest model customized to the student's data, we achieved remarkable classification performance, with test accuracies of 97.6% in the computer-based learning setting and 98% in the virtual reality setting. These results underscore the effectiveness of our approach in delivering personalized insights into student concentration during online educational activities.

### Leveraging ChatGPT for Sponsored Ad Detection and Keyword Extraction in YouTube Videos 
[[arxiv](https://arxiv.org/abs/2502.15102)] [[cool](https://papers.cool/arxiv/2502.15102)] [[pdf](https://arxiv.org/pdf/2502.15102)]
> **Authors**: Brice Valentin Kok-Shun,Johnny Chan
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 6 pages, 4 figures, accepted and presented in the 10th IEEE International Conference on Sustainable Technology and Engineering
- **标题**: None
- **领域**: 机器学习
- **Abstract**: This work-in-progress paper presents a novel approach to detecting sponsored advertisement segments in YouTube videos and comparing the advertisement with the main content. Our methodology involves the collection of 421 auto-generated and manual transcripts which are then fed into a prompt-engineered GPT-4o for ad detection, a KeyBERT for keyword extraction, and another iteration of ChatGPT for category identification. The results revealed a significant prevalence of product-related ads across various educational topics, with ad categories refined using GPT-4o into succinct 9 content and 4 advertisement categories. This approach provides a scalable and efficient alternative to traditional ad detection methods while offering new insights into the types and relevance of ads embedded within educational content. This study highlights the potential of LLMs in transforming ad detection processes and improving our understanding of advertisement strategies in digital media.

### UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning 
[[arxiv](https://arxiv.org/abs/2502.15082)] [[cool](https://papers.cool/arxiv/2502.15082)] [[pdf](https://arxiv.org/pdf/2502.15082)]
> **Authors**: Vaidehi Patil,Elias Stengel-Eskin,Mohit Bansal
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: Code: https://github.com/Vaidehi99/UPCORE
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: User specifications or legal frameworks often require information to be removed from pretrained models, including large language models (LLMs). This requires deleting or "forgetting" a set of data points from an already-trained model, which typically degrades its performance on other data points. Thus, a balance must be struck between removing information and keeping the model's other abilities intact, with a failure to balance this trade-off leading to poor deletion or an unusable model. To this end, we propose UPCORE (Utility-Preserving Coreset Selection), a method-agnostic data selection framework for mitigating collateral damage during unlearning. Finding that the model damage is correlated with the variance of the model's representations on the forget set, we selectively prune the forget set to remove outliers, thereby minimizing model degradation after unlearning. We evaluate UPCORE across three standard unlearning methods consistently achieving a superior balance between the competing objectives of deletion efficacy and model preservation. To better evaluate this trade-off, we introduce a new metric, measuring the area-under-the-curve (AUC) across standard metrics. We find that UPCORE improves both standard metrics and AUC, benefitting from positive transfer between the coreset and pruned points while reducing negative transfer from the forget set to points outside of it.

### More for Keys, Less for Values: Adaptive KV Cache Quantization 
[[arxiv](https://arxiv.org/abs/2502.15075)] [[cool](https://papers.cool/arxiv/2502.15075)] [[pdf](https://arxiv.org/pdf/2502.15075)]
> **Authors**: Mohsen Hariri,Lam Nguyen,Sixu Chen,Shaochen Zhong,Qifan Wang,Xia Hu,Xiaotian Han,Vipin Chaudhary
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: This paper introduces an information-aware quantization framework that adaptively compresses the key-value (KV) cache in large language models (LLMs). Although prior work has underscored the distinct roles of key and value cache during inference, our systematic analysis -- examining singular value distributions, spectral norms, and Frobenius norms -- reveals, for the first time, that key matrices consistently exhibit higher norm values and are more sensitive to quantization than value matrices. Furthermore, our theoretical analysis shows that matrices with higher spectral norms amplify quantization errors more significantly. Motivated by these insights, we propose a mixed-precision quantization strategy, KV-AdaQuant, which allocates more bit-width for keys and fewer for values since key matrices have higher norm values. With the same total KV bit budget, this approach effectively mitigates error propagation across transformer layers while achieving significant memory savings. Our extensive experiments on multiple LLMs (1B--70B) demonstrate that our mixed-precision quantization scheme maintains high model accuracy even under aggressive compression. For instance, using 4-bit for Key and 2-bit for Value achieves an accuracy of 75.2%, whereas reversing the assignment (2-bit for Key and 4-bit for Value) yields only 54.7% accuracy. The code is available at https://tinyurl.com/kv-adaquant

### Visualizing Machine Learning Models for Enhanced Financial Decision-Making and Risk Management 
[[arxiv](https://arxiv.org/abs/2502.15073)] [[cool](https://papers.cool/arxiv/2502.15073)] [[pdf](https://arxiv.org/pdf/2502.15073)]
> **Authors**: Priyam Ganguly,Ramakrishna Garine,Isha Mukherjee
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: This study emphasizes how crucial it is to visualize machine learning models, especially for the banking industry, in order to improve interpretability and support predictions in high stakes financial settings. Visual tools enable performance improvements and support the creation of innovative financial models by offering crucial insights into the algorithmic decision-making processes. Within a financial machine learning framework, the research uses visually guided experiments to make important concepts, such risk assessment and portfolio allocation, more understandable. The study also examines variations in trading tactics and how they relate to risk appetite, coming to the conclusion that the frequency of portfolio rebalancing is negatively correlated with risk tolerance. Finding these ideas is made possible in large part by visualization. The study concludes by presenting a novel method of locally stochastic asset weighing, where visualization facilitates data extraction and validation. This highlights the usefulness of these methods in furthering the field of financial machine learning research.

### GiGL: Large-Scale Graph Neural Networks at Snapchat 
[[arxiv](https://arxiv.org/abs/2502.15054)] [[cool](https://papers.cool/arxiv/2502.15054)] [[pdf](https://arxiv.org/pdf/2502.15054)]
> **Authors**: Tong Zhao,Yozen Liu,Matthew Kolodner,Kyle Montemayor,Elham Ghazizadeh,Ankit Batra,Zihao Fan,Xiaobin Gao,Xuan Guo,Jiwen Ren,Serim Park,Peicheng Yu,Jun Yu,Shubham Vij,Neil Shah
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Recent advances in graph machine learning (ML) with the introduction of Graph Neural Networks (GNNs) have led to a widespread interest in applying these approaches to business applications at scale. GNNs enable differentiable end-to-end (E2E) learning of model parameters given graph structure which enables optimization towards popular node, edge (link) and graph-level tasks. While the research innovation in new GNN layers and training strategies has been rapid, industrial adoption and utility of GNNs has lagged considerably due to the unique scale challenges that large-scale graph ML problems create. In this work, we share our approach to training, inference, and utilization of GNNs at Snapchat. To this end, we present GiGL (Gigantic Graph Learning), an open-source library to enable large-scale distributed graph ML to the benefit of researchers, ML engineers, and practitioners. We use GiGL internally at Snapchat to manage the heavy lifting of GNN workflows, including graph data preprocessing from relational DBs, subgraph sampling, distributed training, inference, and orchestration. GiGL is designed to interface cleanly with open-source GNN modeling libraries prominent in academia like PyTorch Geometric (PyG), while handling scaling and productionization challenges that make it easier for internal practitioners to focus on modeling. GiGL is used in multiple production settings, and has powered over 35 launches across multiple business domains in the last 2 years in the contexts of friend recommendation, content recommendation and advertising. This work details high-level design and tools the library provides, scaling properties, case studies in diverse business settings with industry-scale graphs, and several key lessons learned in employing graph ML at scale on large social data. GiGL is open-sourced at https://github.com/snap-research/GiGL.

### Approximating Latent Manifolds in Neural Networks via Vanishing Ideals 
[[arxiv](https://arxiv.org/abs/2502.15051)] [[cool](https://papers.cool/arxiv/2502.15051)] [[pdf](https://arxiv.org/pdf/2502.15051)]
> **Authors**: Nico Pelleriti,Max Zimmer,Elias Wirth,Sebastian Pokutta
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 26 pages (8 main body, rest appendix and references), 12 figures, 3 tables, 3 algorithms
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Deep neural networks have reshaped modern machine learning by learning powerful latent representations that often align with the manifold hypothesis: high-dimensional data lie on lower-dimensional manifolds. In this paper, we establish a connection between manifold learning and computational algebra by demonstrating how vanishing ideals can characterize the latent manifolds of deep networks. To that end, we propose a new neural architecture that (i) truncates a pretrained network at an intermediate layer, (ii) approximates each class manifold via polynomial generators of the vanishing ideal, and (iii) transforms the resulting latent space into linearly separable features through a single polynomial layer. The resulting models have significantly fewer layers than their pretrained baselines, while maintaining comparable accuracy, achieving higher throughput, and utilizing fewer parameters. Furthermore, drawing on spectral complexity analysis, we derive sharper theoretical guarantees for generalization, showing that our approach can in principle offer tighter bounds than standard deep networks. Numerical experiments confirm the effectiveness and efficiency of the proposed approach.

### GeoAggregator: An Efficient Transformer Model for Geo-Spatial Tabular Data 
[[arxiv](https://arxiv.org/abs/2502.15032)] [[cool](https://papers.cool/arxiv/2502.15032)] [[pdf](https://arxiv.org/pdf/2502.15032)]
> **Authors**: Rui Deng,Ziqi Li,Mingshu Wang
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: Accepted in the main technical track of the AAAI 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Modeling geospatial tabular data with deep learning has become a promising alternative to traditional statistical and machine learning approaches. However, existing deep learning models often face challenges related to scalability and flexibility as datasets grow. To this end, this paper introduces GeoAggregator, an efficient and lightweight algorithm based on transformer architecture designed specifically for geospatial tabular data modeling. GeoAggregators explicitly account for spatial autocorrelation and spatial heterogeneity through Gaussian-biased local attention and global positional awareness. Additionally, we introduce a new attention mechanism that uses the Cartesian product to manage the size of the model while maintaining strong expressive power. We benchmark GeoAggregator against spatial statistical models, XGBoost, and several state-of-the-art geospatial deep learning methods using both synthetic and empirical geospatial datasets. The results demonstrate that GeoAggregators achieve the best or second-best performance compared to their competitors on nearly all datasets. GeoAggregator's efficiency is underscored by its reduced model size, making it both scalable and lightweight. Moreover, ablation experiments offer insights into the effectiveness of the Gaussian bias and Cartesian attention mechanism, providing recommendations for further optimizing the GeoAggregator's performance.

### Interpreting Adversarial Attacks and Defences using Architectures with Enhanced Interpretability 
[[arxiv](https://arxiv.org/abs/2502.15017)] [[cool](https://papers.cool/arxiv/2502.15017)] [[pdf](https://arxiv.org/pdf/2502.15017)]
> **Authors**: Akshay G Rao,Chandrashekhar Lakshminarayanan,Arun Rajkumar
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: Publication accepted at AAAI DeployableAIconference 2025 (proof - https://sites.google.com/view/dai-2025/accepted-papers?authuser=0) Total 17 pages
- **标题**: None
- **领域**: 机器学习,密码学和安全
- **Abstract**: Adversarial attacks in deep learning represent a significant threat to the integrity and reliability of machine learning models. Adversarial training has been a popular defence technique against these adversarial attacks. In this work, we capitalize on a network architecture, namely Deep Linearly Gated Networks (DLGN), which has better interpretation capabilities than regular deep network architectures. Using this architecture, we interpret robust models trained using PGD adversarial training and compare them with standard training. Feature networks in DLGN act as feature extractors, making them the only medium through which an adversary can attack the model. We analyze the feature network of DLGN with fully connected layers with respect to properties like alignment of the hyperplanes, hyperplane relation with PCA, and sub-network overlap among classes and compare these properties between robust and standard models. We also consider this architecture having CNN layers wherein we qualitatively (using visualizations) and quantitatively contrast gating patterns between robust and standard models. We uncover insights into hyperplanes resembling principal components in PGD-AT and STD-TR models, with PGD-AT hyperplanes aligned farther from the data points. We use path activity analysis to show that PGD-AT models create diverse, non-overlapping active subnetworks across classes, preventing attack-induced gating overlaps. Our visualization ideas show the nature of representations learnt by PGD-AT and STD-TR models.

### TimeDistill: Efficient Long-Term Time Series Forecasting with MLP via Cross-Architecture Distillation 
[[arxiv](https://arxiv.org/abs/2502.15016)] [[cool](https://papers.cool/arxiv/2502.15016)] [[pdf](https://arxiv.org/pdf/2502.15016)]
> **Authors**: Juntong Ni,Zewen Liu,Shiyu Wang,Ming Jin,Wei Jin
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Transformer-based and CNN-based methods demonstrate strong performance in long-term time series forecasting. However, their high computational and storage requirements can hinder large-scale deployment. To address this limitation, we propose integrating lightweight MLP with advanced architectures using knowledge distillation (KD). Our preliminary study reveals different models can capture complementary patterns, particularly multi-scale and multi-period patterns in the temporal and frequency domains. Based on this observation, we introduce TimeDistill, a cross-architecture KD framework that transfers these patterns from teacher models (e.g., Transformers, CNNs) to MLP. Additionally, we provide a theoretical analysis, demonstrating that our KD approach can be interpreted as a specialized form of mixup data augmentation. TimeDistill improves MLP performance by up to 18.6%, surpassing teacher models on eight datasets. It also achieves up to 7X faster inference and requires 130X fewer parameters. Furthermore, we conduct extensive evaluations to highlight the versatility and effectiveness of TimeDistill.

### Accelerating Neural Network Training: An Analysis of the AlgoPerf Competition 
[[arxiv](https://arxiv.org/abs/2502.15015)] [[cool](https://papers.cool/arxiv/2502.15015)] [[pdf](https://arxiv.org/pdf/2502.15015)]
> **Authors**: Priya Kasimbeg,Frank Schneider,Runa Eschenhagen,Juhan Bae,Chandramouli Shama Sastry,Mark Saroufim,Boyuan Feng,Less Wright,Edward Z. Yang,Zachary Nado,Sourabh Medapati,Philipp Hennig,Michael Rabbat,George E. Dahl
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: ICLR 2025; 23 pages, 5 figures, 8 tables
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: The goal of the AlgoPerf: Training Algorithms competition is to evaluate practical speed-ups in neural network training achieved solely by improving the underlying training algorithms. In the external tuning ruleset, submissions must provide workload-agnostic hyperparameter search spaces, while in the self-tuning ruleset they must be completely hyperparameter-free. In both rulesets, submissions are compared on time-to-result across multiple deep learning workloads, training on fixed hardware. This paper presents the inaugural AlgoPerf competition's results, which drew 18 diverse submissions from 10 teams. Our investigation reveals several key findings: (1) The winning submission in the external tuning ruleset, using Distributed Shampoo, demonstrates the effectiveness of non-diagonal preconditioning over popular methods like Adam, even when compared on wall-clock runtime. (2) The winning submission in the self-tuning ruleset, based on the Schedule Free AdamW algorithm, demonstrates a new level of effectiveness for completely hyperparameter-free training algorithms. (3) The top-scoring submissions were surprisingly robust to workload changes. We also discuss the engineering challenges encountered in ensuring a fair comparison between different training algorithms. These results highlight both the significant progress so far, and the considerable room for further improvements.

### Towards Physics-Guided Foundation Models 
[[arxiv](https://arxiv.org/abs/2502.15013)] [[cool](https://papers.cool/arxiv/2502.15013)] [[pdf](https://arxiv.org/pdf/2502.15013)]
> **Authors**: Majid Farhadloo,Arun Sharma,Mingzhou Yang,Bharat Jayaprakash,William Northrop,Shashi Shekhar
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Traditional foundation models are pre-trained on broad datasets to reduce the training resources (e.g., time, energy, labeled samples) needed for fine-tuning a wide range of downstream tasks. However, traditional foundation models struggle with out-of-distribution prediction and can produce outputs that are unrealistic and physically infeasible. We propose the notation of physics-guided foundation models (PGFM), that is, foundation models integrated with broad or general domain (e.g., scientific) physical knowledge applicable to a wide range of downstream tasks.

### Understanding the Design Principles of Link Prediction in Directed Settings 
[[arxiv](https://arxiv.org/abs/2502.15008)] [[cool](https://papers.cool/arxiv/2502.15008)] [[pdf](https://arxiv.org/pdf/2502.15008)]
> **Authors**: Jun Zhai,Muberra Ozmen,Thomas Markovich
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Link prediction is a widely studied task in Graph Representation Learning (GRL) for modeling relational data. The early theories in GRL were based on the assumption of a symmetric adjacency matrix, reflecting an undirected setting. As a result, much of the following state-of-the-art research has continued to operate under this symmetry assumption, even though real-world data often involve crucial information conveyed through the direction of relationships. This oversight limits the ability of these models to fully capture the complexity of directed interactions. In this paper, we focus on the challenge of directed link prediction by evaluating key heuristics that have been successful in undirected settings. We propose simple but effective adaptations of these heuristics to the directed link prediction task and demonstrate that these modifications produce competitive performance compared to the leading Graph Neural Networks (GNNs) originally designed for undirected graphs. Through an extensive set of experiments, we derive insights that inform the development of a novel framework for directed link prediction, which not only surpasses baseline methods but also outperforms state-of-the-art GNNs on multiple benchmarks.

### Digital implementations of deep feature extractors are intrinsically informative 
[[arxiv](https://arxiv.org/abs/2502.15004)] [[cool](https://papers.cool/arxiv/2502.15004)] [[pdf](https://arxiv.org/pdf/2502.15004)]
> **Authors**: Max Getter
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 6 pages
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别,泛函分析
- **Abstract**: Rapid information (energy) propagation in deep feature extractors is crucial to balance computational complexity versus expressiveness as a representation of the input. We prove an upper bound for the speed of energy propagation in a unified framework that covers different neural network models, both over Euclidean and non-Euclidean domains. Additional structural information about the signal domain can be used to explicitly determine or improve the rate of decay. To illustrate this, we show global exponential energy decay for a range of 1) feature extractors with discrete-domain input signals, and 2) convolutional neural networks (CNNs) via scattering over locally compact abelian (LCA) groups.

### Generative Modeling of Individual Behavior at Scale 
[[arxiv](https://arxiv.org/abs/2502.14998)] [[cool](https://papers.cool/arxiv/2502.14998)] [[pdf](https://arxiv.org/pdf/2502.14998)]
> **Authors**: Nabil Omi,Lucas Caccia,Anurag Sarkar,Jordan T. Ash,Siddhartha Sen
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: There has been a growing interest in using AI to model human behavior, particularly in domains where humans interact with this technology. While most existing work models human behavior at an aggregate level, our goal is to model behavior at the individual level. Recent approaches to behavioral stylometry -- or the task of identifying a person from their actions alone -- have shown promise in domains like chess, but these approaches are either not scalable (e.g., fine-tune a separate model for each person) or not generative, in that they cannot generate actions. We address these limitations by framing behavioral stylometry as a multi-task learning problem -- where each task represents a distinct person -- and use parameter-efficient fine-tuning (PEFT) methods to learn an explicit style vector for each person. Style vectors are generative: they selectively activate shared "skill" parameters to generate actions in the style of each person. They also induce a latent space that we can interpret and manipulate algorithmically. In particular, we develop a general technique for style steering that allows us to steer a player's style vector towards a desired property. We apply our approach to two very different games, at unprecedented scales: chess (47,864 players) and Rocket League (2,000 players). We also show generality beyond gaming by applying our method to image generation, where we learn style vectors for 10,177 celebrities and use these vectors to steer their images.

### EigenShield: Causal Subspace Filtering via Random Matrix Theory for Adversarially Robust Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2502.14976)] [[cool](https://papers.cool/arxiv/2502.14976)] [[pdf](https://arxiv.org/pdf/2502.14976)]
> **Authors**: Nastaran Darabi,Devashri Naik,Sina Tayebati,Dinithi Jayasuriya,Ranganath Krishnan,Amit Ranjan Trivedi
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,密码学和安全,计算机视觉和模式识别
- **Abstract**: Vision-Language Models (VLMs) inherit adversarial vulnerabilities of Large Language Models (LLMs), which are further exacerbated by their multimodal nature. Existing defenses, including adversarial training, input transformations, and heuristic detection, are computationally expensive, architecture-dependent, and fragile against adaptive attacks. We introduce EigenShield, an inference-time defense leveraging Random Matrix Theory to quantify adversarial disruptions in high-dimensional VLM representations. Unlike prior methods that rely on empirical heuristics, EigenShield employs the spiked covariance model to detect structured spectral deviations. Using a Robustness-based Nonconformity Score (RbNS) and quantile-based thresholding, it separates causal eigenvectors, which encode semantic information, from correlational eigenvectors that are susceptible to adversarial artifacts. By projecting embeddings onto the causal subspace, EigenShield filters adversarial noise without modifying model parameters or requiring adversarial training. This architecture-independent, attack-agnostic approach significantly reduces the attack success rate, establishing spectral analysis as a principled alternative to conventional defenses. Our results demonstrate that EigenShield consistently outperforms all existing defenses, including adversarial training, UNIGUARD, and CIDER.

### P2W: From Power Traces to Weights Matrix -- An Unconventional Transfer Learning Approach 
[[arxiv](https://arxiv.org/abs/2502.14968)] [[cool](https://papers.cool/arxiv/2502.14968)] [[pdf](https://arxiv.org/pdf/2502.14968)]
> **Authors**: Roozbeh Siyadatzadeh,Fatemeh Mehrafrooz,Nele Mentens,Todor Stefanov
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The rapid growth of deploying machine learning (ML) models within embedded systems on a chip (SoCs) has led to transformative shifts in fields like healthcare and autonomous vehicles. One of the primary challenges for training such embedded ML models is the lack of publicly available high-quality training data. Transfer learning approaches address this challenge by utilizing the knowledge encapsulated in an existing ML model as a starting point for training a new ML model. However, existing transfer learning approaches require direct access to the existing model which is not always feasible, especially for ML models deployed on embedded SoCs. Therefore, in this paper, we introduce a novel unconventional transfer learning approach to train a new ML model by extracting and using weights from an existing ML model running on an embedded SoC without having access to the model within the SoC. Our approach captures power consumption measurements from the SoC while it is executing the ML model and translates them to an approximated weights matrix used to initialize the new ML model. This improves the learning efficiency and predictive performance of the new model, especially in scenarios with limited data available to train the model. Our novel approach can effectively increase the accuracy of the new ML model up to 3 times compared to classical training methods using the same amount of limited training data.

### Prompt-to-Leaderboard 
[[arxiv](https://arxiv.org/abs/2502.14855)] [[cool](https://papers.cool/arxiv/2502.14855)] [[pdf](https://arxiv.org/pdf/2502.14855)]
> **Authors**: Evan Frick,Connor Chen,Joseph Tennyson,Tianle Li,Wei-Lin Chiang,Anastasios N. Angelopoulos,Ion Stoica
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Large language model (LLM) evaluations typically rely on aggregated metrics like accuracy or human preference, averaging across users and prompts. This averaging obscures user- and prompt-specific variations in model performance. To address this, we propose Prompt-to-Leaderboard (P2L), a method that produces leaderboards specific to a prompt. The core idea is to train an LLM taking natural language prompts as input to output a vector of Bradley-Terry coefficients which are then used to predict the human preference vote. The resulting prompt-dependent leaderboards allow for unsupervised task-specific evaluation, optimal routing of queries to models, personalization, and automated evaluation of model strengths and weaknesses. Data from Chatbot Arena suggest that P2L better captures the nuanced landscape of language model performance than the averaged leaderboard. Furthermore, our findings suggest that P2L's ability to produce prompt-specific evaluations follows a power law scaling similar to that observed in LLMs themselves. In January 2025, the router we trained based on this methodology achieved the \#1 spot in the Chatbot Arena leaderboard. Our code is available at this GitHub link: https://github.com/lmarena/p2l.

### Generating $π$-Functional Molecules Using STGG+ with Active Learning 
[[arxiv](https://arxiv.org/abs/2502.14842)] [[cool](https://papers.cool/arxiv/2502.14842)] [[pdf](https://arxiv.org/pdf/2502.14842)]
> **Authors**: Alexia Jolicoeur-Martineau,Yan Zhang,Boris Knyazev,Aristide Baratin,Cheng-Hao Liu
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: Code: https://github.com/SamsungSAILMontreal/STGG-AL
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Generating novel molecules with out-of-distribution properties is a major challenge in molecular discovery. While supervised learning methods generate high-quality molecules similar to those in a dataset, they struggle to generalize to out-of-distribution properties. Reinforcement learning can explore new chemical spaces but often conducts 'reward-hacking' and generates non-synthesizable molecules. In this work, we address this problem by integrating a state-of-the-art supervised learning method, STGG+, in an active learning loop. Our approach iteratively generates, evaluates, and fine-tunes STGG+ to continuously expand its knowledge. We denote this approach STGG+AL. We apply STGG+AL to the design of organic $π$-functional materials, specifically two challenging tasks: 1) generating highly absorptive molecules characterized by high oscillator strength and 2) designing absorptive molecules with reasonable oscillator strength in the near-infrared (NIR) range. The generated molecules are validated and rationalized in-silico with time-dependent density functional theory. Our results demonstrate that our method is highly effective in generating novel molecules with high oscillator strength, contrary to existing methods such as reinforcement learning (RL) methods. We open-source our active-learning code along with our Conjugated-xTB dataset containing 2.9 million $π$-conjugated molecules and the function for approximating the oscillator strength and absorption wavelength (based on sTDA-xTB).

### Spatial Distribution-Shift Aware Knowledge-Guided Machine Learning 
[[arxiv](https://arxiv.org/abs/2502.14840)] [[cool](https://papers.cool/arxiv/2502.14840)] [[pdf](https://arxiv.org/pdf/2502.14840)]
> **Authors**: Arun Sharma,Majid Farhadloo,Mingzhou Yang,Ruolei Zeng,Subhankar Ghosh,Shashi Shekhar
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Given inputs of diverse soil characteristics and climate data gathered from various regions, we aimed to build a model to predict accurate land emissions. The problem is important since accurate quantification of the carbon cycle in agroecosystems is crucial for mitigating climate change and ensuring sustainable food production. Predicting accurate land emissions is challenging since calibrating the heterogeneous nature of soil properties, moisture, and environmental conditions is hard at decision-relevant scales. Traditional approaches do not adequately estimate land emissions due to location-independent parameters failing to leverage the spatial heterogeneity and also require large datasets. To overcome these limitations, we proposed Spatial Distribution-Shift Aware Knowledge-Guided Machine Learning (SDSA-KGML), which leverages location-dependent parameters that account for significant spatial heterogeneity in soil moisture from multiple sites within the same region. Experimental results demonstrate that SDSA-KGML models achieve higher local accuracy for the specified states in the Midwest Region.

### Probabilistic Robustness in Deep Learning: A Concise yet Comprehensive Guide 
[[arxiv](https://arxiv.org/abs/2502.14833)] [[cool](https://papers.cool/arxiv/2502.14833)] [[pdf](https://arxiv.org/pdf/2502.14833)]
> **Authors**: Xingyu Zhao
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: This is a preprint of the following chapter: X. Zhao, Probabilistic Robustness inDeepLearning: A Concise yet Comprehensive Guide, published in the book Adversarial Example Detection and Mitigation UsingMachineLearning, edited by Ehsan Nowroozi, Rahim Taheri, Lucas Cordeiro, 2025, Springer Nature. The final authenticated version will available online soon
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Deep learning (DL) has demonstrated significant potential across various safety-critical applications, yet ensuring its robustness remains a key challenge. While adversarial robustness has been extensively studied in worst-case scenarios, probabilistic robustness (PR) offers a more practical perspective by quantifying the likelihood of failures under stochastic perturbations. This paper provides a concise yet comprehensive overview of PR, covering its formal definitions, evaluation and enhancement methods. We introduce a reformulated ''min-max'' optimisation framework for adversarial training specifically designed to improve PR. Furthermore, we explore the integration of PR verification evidence into system-level safety assurance, addressing challenges in translating DL model-level robustness to system-level claims. Finally, we highlight open research questions, including benchmarking PR evaluation methods, extending PR to generative AI tasks, and developing rigorous methodologies and case studies for system-level integration.

### Fundamental Limitations in Defending LLM Finetuning APIs 
[[arxiv](https://arxiv.org/abs/2502.14828)] [[cool](https://papers.cool/arxiv/2502.14828)] [[pdf](https://arxiv.org/pdf/2502.14828)]
> **Authors**: Xander Davies,Eric Winsor,Tomek Korbak,Alexandra Souly,Robert Kirk,Christian Schroeder de Witt,Yarin Gal
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,密码学和安全
- **Abstract**: LLM developers have imposed technical interventions to prevent fine-tuning misuse attacks, attacks where adversaries evade safeguards by fine-tuning the model using a public API. Previous work has established several successful attacks against specific fine-tuning API defences. In this work, we show that defences of fine-tuning APIs that seek to detect individual harmful training or inference samples ('pointwise' detection) are fundamentally limited in their ability to prevent fine-tuning attacks. We construct 'pointwise-undetectable' attacks that repurpose entropy in benign model outputs (e.g. semantic or syntactic variations) to covertly transmit dangerous knowledge. Our attacks are composed solely of unsuspicious benign samples that can be collected from the model before fine-tuning, meaning training and inference samples are all individually benign and low-perplexity. We test our attacks against the OpenAI fine-tuning API, finding they succeed in eliciting answers to harmful multiple-choice questions, and that they evade an enhanced monitoring system we design that successfully detects other fine-tuning attacks. We encourage the community to develop defences that tackle the fundamental limitations we uncover in pointwise fine-tuning API defences.

### Learning from Reward-Free Offline Data: A Case for Planning with Latent Dynamics Models 
[[arxiv](https://arxiv.org/abs/2502.14819)] [[cool](https://papers.cool/arxiv/2502.14819)] [[pdf](https://arxiv.org/pdf/2502.14819)]
> **Authors**: Vlad Sobal,Wancong Zhang,Kynghyun Cho,Randall Balestriero,Tim G. J. Rudner,Yann LeCun
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: Project web page: https://latent-planning.github.io/
- **标题**: None
- **领域**: 机器学习
- **Abstract**: A long-standing goal in AI is to build agents that can solve a variety of tasks across different environments, including previously unseen ones. Two dominant approaches tackle this challenge: (i) reinforcement learning (RL), which learns policies through trial and error, and (ii) optimal control, which plans actions using a learned or known dynamics model. However, their relative strengths and weaknesses remain underexplored in the setting where agents must learn from offline trajectories without reward annotations. In this work, we systematically analyze the performance of different RL and control-based methods under datasets of varying quality. On the RL side, we consider goal-conditioned and zero-shot approaches. On the control side, we train a latent dynamics model using the Joint Embedding Predictive Architecture (JEPA) and use it for planning. We study how dataset properties-such as data diversity, trajectory quality, and environment variability-affect the performance of these approaches. Our results show that model-free RL excels when abundant, high-quality data is available, while model-based planning excels in generalization to novel environment layouts, trajectory stitching, and data-efficiency. Notably, planning with a latent dynamics model emerges as a promising approach for zero-shot generalization from suboptimal data.

### Dynamic Low-Rank Sparse Adaptation for Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.14816)] [[cool](https://papers.cool/arxiv/2502.14816)] [[pdf](https://arxiv.org/pdf/2502.14816)]
> **Authors**: Weizhong Huang,Yuxin Zhang,Xiawu Zheng,Yang Liu,Jing Lin,Yiwu Yao,Rongrong Ji
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: Accepted to ICLR 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Despite the efficacy of network sparsity in alleviating the deployment strain of Large Language Models (LLMs), it endures significant performance degradation. Applying Low-Rank Adaptation (LoRA) to fine-tune the sparse LLMs offers an intuitive approach to counter this predicament, while it holds shortcomings include: 1) The inability to integrate LoRA weights into sparse LLMs post-training, and 2) Insufficient performance recovery at high sparsity ratios. In this paper, we introduce dynamic Low-rank Sparse Adaptation (LoSA), a novel method that seamlessly integrates low-rank adaptation into LLM sparsity within a unified framework, thereby enhancing the performance of sparse LLMs without increasing the inference latency. In particular, LoSA dynamically sparsifies the LoRA outcomes based on the corresponding sparse weights during fine-tuning, thus guaranteeing that the LoRA module can be integrated into the sparse LLMs post-training. Besides, LoSA leverages Representation Mutual Information (RMI) as an indicator to determine the importance of layers, thereby efficiently determining the layer-wise sparsity rates during fine-tuning. Predicated on this, LoSA adjusts the rank of the LoRA module based on the variability in layer-wise reconstruction errors, allocating an appropriate fine-tuning for each layer to reduce the output discrepancies between dense and sparse LLMs. Extensive experiments tell that LoSA can efficiently boost the efficacy of sparse LLMs within a few hours, without introducing any additional inferential burden. For example, LoSA reduced the perplexity of sparse LLaMA-2-7B by 68.73 and increased zero-shot accuracy by 16.32$\%$, achieving a 2.60$\times$ speedup on CPU and 2.23$\times$ speedup on GPU, requiring only 45 minutes of fine-tuning on a single NVIDIA A100 80GB GPU. Code is available at https://github.com/wzhuang-xmu/LoSA.

### PREM: Privately Answering Statistical Queries with Relative Error 
[[arxiv](https://arxiv.org/abs/2502.14809)] [[cool](https://papers.cool/arxiv/2502.14809)] [[pdf](https://arxiv.org/pdf/2502.14809)]
> **Authors**: Badih Ghazi,Cristóbal Guzmán,Pritish Kamath,Alexander Knop,Ravi Kumar,Pasin Manurangsi,Sushant Sachdeva
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We introduce $\mathsf{PREM}$ (Private Relative Error Multiplicative weight update), a new framework for generating synthetic data that achieves a relative error guarantee for statistical queries under $(\varepsilon, δ)$ differential privacy (DP). Namely, for a domain ${\cal X}$, a family ${\cal F}$ of queries $f : {\cal X} \to \{0, 1\}$, and $ζ> 0$, our framework yields a mechanism that on input dataset $D \in {\cal X}^n$ outputs a synthetic dataset $\widehat{D} \in {\cal X}^n$ such that all statistical queries in ${\cal F}$ on $D$, namely $\sum_{x \in D} f(x)$ for $f \in {\cal F}$, are within a $1 \pm ζ$ multiplicative factor of the corresponding value on $\widehat{D}$ up to an additive error that is polynomial in $\log |{\cal F}|$, $\log |{\cal X}|$, $\log n$, $\log(1/δ)$, $1/\varepsilon$, and $1/ζ$. In contrast, any $(\varepsilon, δ)$-DP mechanism is known to require worst-case additive error that is polynomial in at least one of $n, |{\cal F}|$, or $|{\cal X}|$. We complement our algorithm with nearly matching lower bounds.

### An Adversarial Analysis of Thompson Sampling for Full-information Online Learning: from Finite to Infinite Action Spaces 
[[arxiv](https://arxiv.org/abs/2502.14790)] [[cool](https://papers.cool/arxiv/2502.14790)] [[pdf](https://arxiv.org/pdf/2502.14790)]
> **Authors**: Alexander Terenin,Jeffrey Negrea
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机科学与博弈论,统计理论,机器学习
- **Abstract**: We develop an analysis of Thompson sampling for online learning under full feedback - also known as prediction with expert advice - where the learner's prior is defined over the space of an adversary's future actions, rather than the space of experts. We show regret decomposes into regret the learner expected a priori, plus a prior-robustness-type term we call excess regret. In the classical finite-expert setting, this recovers optimal rates. As an initial step towards practical online learning in settings with a potentially-uncountably-infinite number of experts, we show that Thompson sampling with a certain Gaussian process prior widely-used in the Bayesian optimization literature has a $\mathcal{O}(β\sqrt{T\log(1+λ)})$ rate against a $β$-bounded $λ$-Lipschitz adversary.

### Ray-Tracing for Conditionally Activated Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.14788)] [[cool](https://papers.cool/arxiv/2502.14788)] [[pdf](https://arxiv.org/pdf/2502.14788)]
> **Authors**: Claudio Gallicchio,Giuseppe Nuti
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: submitted to workshop
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: In this paper, we introduce a novel architecture for conditionally activated neural networks combining a hierarchical construction of multiple Mixture of Experts (MoEs) layers with a sampling mechanism that progressively converges to an optimized configuration of expert activation. This methodology enables the dynamic unfolding of the network's architecture, facilitating efficient path-specific training. Experimental results demonstrate that this approach achieves competitive accuracy compared to conventional baselines while significantly reducing the parameter count required for inference. Notably, this parameter reduction correlates with the complexity of the input patterns, a property naturally emerging from the network's operational dynamics without necessitating explicit auxiliary penalty functions.

### Sparse Activations as Conformal Predictors 
[[arxiv](https://arxiv.org/abs/2502.14773)] [[cool](https://papers.cool/arxiv/2502.14773)] [[pdf](https://arxiv.org/pdf/2502.14773)]
> **Authors**: Margarida M. Campos,João Calém,Sophia Sklaviadis,Mário A. T. Figueiredo,André F. T. Martins
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: Accepted at AISTATS 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Conformal prediction is a distribution-free framework for uncertainty quantification that replaces point predictions with sets, offering marginal coverage guarantees (i.e., ensuring that the prediction sets contain the true label with a specified probability, in expectation). In this paper, we uncover a novel connection between conformal prediction and sparse softmax-like transformations, such as sparsemax and $γ$-entmax (with $γ> 1$), which may assign nonzero probability only to a subset of labels. We introduce new non-conformity scores for classification that make the calibration process correspond to the widely used temperature scaling method. At test time, applying these sparse transformations with the calibrated temperature leads to a support set (i.e., the set of labels with nonzero probability) that automatically inherits the coverage guarantees of conformal prediction. Through experiments on computer vision and text classification benchmarks, we demonstrate that the proposed method achieves competitive results in terms of coverage, efficiency, and adaptiveness compared to standard non-conformity scores based on softmax.

### Determining Layer-wise Sparsity for Large Language Models Through a Theoretical Perspective 
[[arxiv](https://arxiv.org/abs/2502.14770)] [[cool](https://papers.cool/arxiv/2502.14770)] [[pdf](https://arxiv.org/pdf/2502.14770)]
> **Authors**: Weizhong Huang,Yuxin Zhang,Xiawu Zheng,Fei Chao,Rongrong Ji
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: In this paper, we address the challenge of determining the layer-wise sparsity rates of large language models (LLMs) through a theoretical perspective. Specifically, we identify a critical issue of ''$\textbf{reconstruction error explosion}$'' in existing LLMs sparsification methods. This refers to the cumulative effect of reconstruction errors throughout the sparsification process, where errors from earlier layers propagate and amplify in subsequent layers. As a result, the overall reconstruction error increases significantly, leading to a substantial degradation in model performance. Through theoretical analysis, we derive a simple yet effective approach to layer-wise sparsity allocation that mitigates this issue. Our method uses a monotonically increasing arithmetic progression, reducing the process of determining sparsity rates for multiple layers to the determination of a single common difference hyperparameter. Remarkably, this allows for the optimal layer-wise sparsity rates to be identified with just a few trials. Both our theoretical analysis and experimental results demonstrate that this sparsity allocation scheme is near optimal. Extensive experiments show that our method significantly improves the performance of sparse LLMs across various architectures, outperforming existing layer-wise sparsity methods. Furthermore, it enhances the performance of various compression techniques and is applicable to vision and multimodal models. Notably, our method achieves a reduction of 52.10 in perplexity for the 70$\%$ sparse LLaMA2-7B model obtained via Wanda, improves average zero-shot accuracy by 10.50$\%$, and delivers speedups of 2.63$\times$ and 2.23$\times$ on CPU and GPU, respectively.

### Sculpting [CLS] Features for Pre-Trained Model-Based Class-Incremental Learning 
[[arxiv](https://arxiv.org/abs/2502.14762)] [[cool](https://papers.cool/arxiv/2502.14762)] [[pdf](https://arxiv.org/pdf/2502.14762)]
> **Authors**: Murat Onur Yildirim,Elif Ceren Gok Yildirim,Joaquin Vanschoren
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: Class-incremental learning requires models to continually acquire knowledge of new classes without forgetting old ones. Although pre-trained models have demonstrated strong performance in class-incremental learning, they remain susceptible to catastrophic forgetting when learning new concepts. Excessive plasticity in the models breaks generalizability and causes forgetting, while strong stability results in insufficient adaptation to new classes. This necessitates effective adaptation with minimal modifications to preserve the general knowledge of pre-trained models. To address this challenge, we first introduce a new parameter-efficient fine-tuning module 'Learn and Calibrate', or LuCA, designed to acquire knowledge through an adapter-calibrator couple, enabling effective adaptation with well-refined feature representations. Second, for each learning session, we deploy a sparse LuCA module on top of the last token just before the classifier, which we refer to as 'Token-level Sparse Calibration and Adaptation', or TOSCA. This strategic design improves the orthogonality between the modules and significantly reduces both training and inference complexity. By leaving the generalization capabilities of the pre-trained models intact and adapting exclusively via the last token, our approach achieves a harmonious balance between stability and plasticity. Extensive experiments demonstrate TOSCA's state-of-the-art performance while introducing ~8 times fewer parameters compared to prior methods.

### Not All Data are Good Labels: On the Self-supervised Labeling for Time Series Forecasting 
[[arxiv](https://arxiv.org/abs/2502.14704)] [[cool](https://papers.cool/arxiv/2502.14704)] [[pdf](https://arxiv.org/pdf/2502.14704)]
> **Authors**: Yuxuan Yang,Dalin Zhang,Yuxuan Liang,Hua Lu,Gang Chen,Huan Li
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Time Series Forecasting (TSF) is a crucial task in various domains, yet existing TSF models rely heavily on high-quality data and insufficiently exploit all available data. This paper explores a novel self-supervised approach to re-label time series datasets by inherently constructing candidate datasets. During the optimization of a simple reconstruction network, intermediates are used as pseudo labels in a self-supervised paradigm, improving generalization for any predictor. We introduce the Self-Correction with Adaptive Mask (SCAM), which discards overfitted components and selectively replaces them with pseudo labels generated from reconstructions. Additionally, we incorporate Spectral Norm Regularization (SNR) to further suppress overfitting from a loss landscape perspective. Our experiments on eleven real-world datasets demonstrate that SCAM consistently improves the performance of various backbone models. This work offers a new perspective on constructing datasets and enhancing the generalization of TSF models through self-supervised learning.

### General Uncertainty Estimation with Delta Variances 
[[arxiv](https://arxiv.org/abs/2502.14698)] [[cool](https://papers.cool/arxiv/2502.14698)] [[pdf](https://arxiv.org/pdf/2502.14698)]
> **Authors**: Simon Schmitt,John Shawe-Taylor,Hado van Hasselt
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,应用领域,机器学习
- **Abstract**: Decision makers may suffer from uncertainty induced by limited data. This may be mitigated by accounting for epistemic uncertainty, which is however challenging to estimate efficiently for large neural networks. To this extent we investigate Delta Variances, a family of algorithms for epistemic uncertainty quantification, that is computationally efficient and convenient to implement. It can be applied to neural networks and more general functions composed of neural networks. As an example we consider a weather simulator with a neural-network-based step function inside -- here Delta Variances empirically obtain competitive results at the cost of a single gradient computation. The approach is convenient as it requires no changes to the neural network architecture or training procedure. We discuss multiple ways to derive Delta Variances theoretically noting that special cases recover popular techniques and present a unified perspective on multiple related methods. Finally we observe that this general perspective gives rise to a natural extension and empirically show its benefit.

### seqKAN: Sequence processing with Kolmogorov-Arnold Networks 
[[arxiv](https://arxiv.org/abs/2502.14681)] [[cool](https://papers.cool/arxiv/2502.14681)] [[pdf](https://arxiv.org/pdf/2502.14681)]
> **Authors**: Tatiana Boura,Stasinos Konstantopoulos
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Kolmogorov-Arnold Networks (KANs) have been recently proposed as a machine learning framework that is more interpretable and controllable than the multi-layer perceptron. Various network architectures have been proposed within the KAN framework targeting different tasks and application domains, including sequence processing. This paper proposes seqKAN, a new KAN architecture for sequence processing. Although multiple sequence processing KAN architectures have already been proposed, we argue that seqKAN is more faithful to the core concept of the KAN framework. Furthermore, we empirically demonstrate that it achieves better results. The empirical evaluation is performed on generated data from a complex physics problem on an interpolation and an extrapolation task. Using this dataset we compared seqKAN against a prior KAN network for timeseries prediction, recurrent deep networks, and symbolic regression. seqKAN substantially outperforms all architectures, particularly on the extrapolation dataset, while also being the most transparent.

### Disentangled Latent Spaces for Reduced Order Models using Deterministic Autoencoders 
[[arxiv](https://arxiv.org/abs/2502.14679)] [[cool](https://papers.cool/arxiv/2502.14679)] [[pdf](https://arxiv.org/pdf/2502.14679)]
> **Authors**: Henning Schwarz,Pyei Phyo Lin,Jens-Peter M. Zemke,Thomas Rung
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Data-driven reduced-order models based on autoencoders generally lack interpretability compared to classical methods such as the proper orthogonal decomposition. More interpretability can be gained by disentangling the latent variables and analyzing the resulting modes. For this purpose, probabilistic $β$-variational autoencoders ($β$-VAEs) are frequently used in computational fluid dynamics and other simulation sciences. Using a benchmark periodic flow dataset, we show that competitive results can be achieved using non-probabilistic autoencoder approaches that either promote orthogonality or penalize correlation between latent variables. Compared to probabilistic autoencoders, these approaches offer more robustness with respect to the choice of hyperparameters entering the loss function. We further demonstrate the ability of a non-probabilistic approach to identify a reduced number of active latent variables by introducing a correlation penalty, a function also known from the use of $β$-VAE. The investigated probabilistic and non-probabilistic autoencoder models are finally used for the dimensionality reduction of aircraft ditching loads, which serves as an industrial application in this work.

### Beyond the Surface: Uncovering Implicit Locations with LLMs for Personalized Local News 
[[arxiv](https://arxiv.org/abs/2502.14660)] [[cool](https://papers.cool/arxiv/2502.14660)] [[pdf](https://arxiv.org/pdf/2502.14660)]
> **Authors**: Gali Katz,Hai Sitton,Guy Gonen,Yohay Kaplan
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 10 pages, 2 figures, submitted to kdd
- **标题**: None
- **领域**: 机器学习
- **Abstract**: News recommendation systems personalize homepage content to boost engagement, but factors like content type, editorial stance, and geographic focus impact recommendations. Local newspapers balance coverage across regions, yet identifying local articles is challenging due to implicit location cues like slang or landmarks. Traditional methods, such as Named Entity Recognition (NER) and Knowledge Graphs, infer locations, but Large Language Models (LLMs) offer new possibilities while raising concerns about accuracy and explainability. This paper explores LLMs for local article classification in Taboola's "Homepage For You" system, comparing them to traditional techniques. Key findings: (1) Knowledge Graphs enhance NER models' ability to detect implicit locations, (2) LLMs outperform traditional methods, and (3) LLMs can effectively identify local content without requiring Knowledge Graph integration. Offline evaluations showed LLMs excel at implicit location classification, while online A/B tests showed a significant increased in local views. A scalable pipeline integrating LLM-based location classification boosted local article distribution by 27%, preserving newspapers' brand identity and enhancing homepage personalization.

### Variance Reduction Methods Do Not Need to Compute Full Gradients: Improved Efficiency through Shuffling 
[[arxiv](https://arxiv.org/abs/2502.14648)] [[cool](https://papers.cool/arxiv/2502.14648)] [[pdf](https://arxiv.org/pdf/2502.14648)]
> **Authors**: Daniil Medyakov,Gleb Molodtsov,Savelii Chezhegov,Alexey Rebrikov,Aleksandr Beznosikov
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 30 pages, 6 figures, 1 table
- **标题**: None
- **领域**: 机器学习,优化与控制
- **Abstract**: In today's world, machine learning is hard to imagine without large training datasets and models. This has led to the use of stochastic methods for training, such as stochastic gradient descent (SGD). SGD provides weak theoretical guarantees of convergence, but there are modifications, such as Stochastic Variance Reduced Gradient (SVRG) and StochAstic Recursive grAdient algoritHm (SARAH), that can reduce the variance. These methods require the computation of the full gradient occasionally, which can be time consuming. In this paper, we explore variants of variance reduction algorithms that eliminate the need for full gradient computations. To make our approach memory-efficient and avoid full gradient computations, we use two key techniques: the shuffling heuristic and idea of SAG/SAGA methods. As a result, we improve existing estimates for variance reduction algorithms without the full gradient computations. Additionally, for the non-convex objective function, our estimate matches that of classic shuffling methods, while for the strongly convex one, it is an improvement. We conduct comprehensive theoretical analysis and provide extensive experimental results to validate the efficiency and practicality of our methods for large-scale machine learning problems.

### ReQFlow: Rectified Quaternion Flow for Efficient and High-Quality Protein Backbone Generation 
[[arxiv](https://arxiv.org/abs/2502.14637)] [[cool](https://papers.cool/arxiv/2502.14637)] [[pdf](https://arxiv.org/pdf/2502.14637)]
> **Authors**: Angxiao Yue,Zichong Wang,Hongteng Xu
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Protein backbone generation plays a central role in de novo protein design and is significant for many biological and medical applications. Although diffusion and flow-based generative models provide potential solutions to this challenging task, they often generate proteins with undesired designability and suffer computational inefficiency. In this study, we propose a novel rectified quaternion flow (ReQFlow) matching method for fast and high-quality protein backbone generation. In particular, our method generates a local translation and a 3D rotation from random noise for each residue in a protein chain, which represents each 3D rotation as a unit quaternion and constructs its flow by spherical linear interpolation (SLERP) in an exponential format. We train the model by quaternion flow (QFlow) matching with guaranteed numerical stability and rectify the QFlow model to accelerate its inference and improve the designability of generated protein backbones, leading to the proposed ReQFlow model. Experiments show that ReQFlow achieves state-of-the-art performance in protein backbone generation while requiring much fewer sampling steps and significantly less inference time (e.g., being 37x faster than RFDiffusion and 62x faster than Genie2 when generating a backbone of length 300), demonstrating its effectiveness and efficiency. The code is available at https://github.com/AngxiaoYue/ReQFlow.

### CER: Confidence Enhanced Reasoning in LLMs 
[[arxiv](https://arxiv.org/abs/2502.14634)] [[cool](https://papers.cool/arxiv/2502.14634)] [[pdf](https://arxiv.org/pdf/2502.14634)]
> **Authors**: Ali Razghandi,Seyed Mohammad Hadi Hosseini,Mahdieh Soleymani Baghshah
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Ensuring the reliability of Large Language Models (LLMs) in complex reasoning tasks remains a formidable challenge, particularly in scenarios that demand precise mathematical calculations and knowledge-intensive open-domain generation. In this work, we introduce an uncertainty-aware framework designed to enhance the accuracy of LLM responses by systematically incorporating model confidence at critical decision points. We propose an approach that encourages multi-step reasoning in LLMs and quantify the confidence of intermediate answers such as numerical results in mathematical reasoning and proper nouns in open-domain generation. Then, the overall confidence of each reasoning chain is evaluated based on confidence of these critical intermediate steps. Finally, we aggregate the answer of generated response paths in a way that reflects the reliability of each generated content (as opposed to self-consistency in which each generated chain contributes equally to majority voting). We conducted extensive experiments in five datasets, three mathematical datasets and two open-domain datasets, using four LLMs. The results consistently validate the effectiveness of our novel confidence aggregation method, leading to an accuracy improvement of up to 7.4% and 5.8% over baseline approaches in math and open-domain generation tasks, respectively. Code is publicly available at https://github.com/ Aquasar11/CER.

### Synergistic Fusion of Multi-Source Knowledge via Evidence Theory for High-Entropy Alloy Discovery 
[[arxiv](https://arxiv.org/abs/2502.14631)] [[cool](https://papers.cool/arxiv/2502.14631)] [[pdf](https://arxiv.org/pdf/2502.14631)]
> **Authors**: Minh-Quyet Ha,Dinh-Khiet Le,Duc-Anh Dao,Tien-Sinh Vu,Duong-Nguyen Nguyen,Viet-Cuong Nguyen,Hiori Kino,Van-Nam Huynh,Hieu-Chi Dam
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 13 pages, 7 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Discovering novel high-entropy alloys (HEAs) with desirable properties is challenging due to the vast compositional space and complex phase formation mechanisms. Efficient exploration of this space requires a strategic approach that integrates heterogeneous knowledge sources. Here, we propose a framework that systematically combines knowledge extracted from computational material datasets with domain knowledge distilled from scientific literature using large language models (LLMs). A central feature of this approach is the explicit consideration of element substitutability, identifying chemically similar elements that can be interchanged to potentially stabilize desired HEAs. Dempster-Shafer theory, a mathematical framework for reasoning under uncertainty, is employed to model and combine substitutabilities based on aggregated evidence from multiple sources. The framework predicts the phase stability of candidate HEA compositions and is systematically evaluated on both quaternary alloy systems, demonstrating superior performance compared to baseline machine learning models and methods reliant on single-source evidence in cross-validation experiments. By leveraging multi-source knowledge, the framework retains robust predictive power even when key elements are absent from the training data, underscoring its potential for knowledge transfer and extrapolation. Furthermore, the enhanced interpretability of the methodology offers insights into the fundamental factors governing HEA formation. Overall, this work provides a promising strategy for accelerating HEA discovery by integrating computational and textual knowledge sources, enabling efficient exploration of vast compositional spaces with improved generalization and interpretability.

### PEARL: Towards Permutation-Resilient LLMs 
[[arxiv](https://arxiv.org/abs/2502.14628)] [[cool](https://papers.cool/arxiv/2502.14628)] [[pdf](https://arxiv.org/pdf/2502.14628)]
> **Authors**: Liang Chen,Li Shen,Yang Deng,Xiaoyan Zhao,Bin Liang,Kam-Fai Wong
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: ICLR 2025
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: The in-context learning (ICL) capability of large language models (LLMs) enables them to perform challenging tasks using provided demonstrations. However, ICL is highly sensitive to the ordering of demonstrations, leading to instability in predictions. This paper shows that this vulnerability can be exploited to design a natural attack - difficult for model providers to detect - that achieves nearly 80% success rate on LLaMA-3 by simply permuting the demonstrations. Existing mitigation methods primarily rely on post-processing and fail to enhance the model's inherent robustness to input permutations, raising concerns about safety and reliability of LLMs. To address this issue, we propose Permutation-resilient learning (PEARL), a novel framework based on distributionally robust optimization (DRO), which optimizes model performance against the worst-case input permutation. Specifically, PEARL consists of a permutation-proposal network (P-Net) and the LLM. The P-Net generates the most challenging permutations by treating it as an optimal transport problem, which is solved using an entropy-constrained Sinkhorn algorithm. Through minimax optimization, the P-Net and the LLM iteratively optimize against each other, progressively improving the LLM's robustness. Experiments on synthetic pre-training and real-world instruction tuning tasks demonstrate that PEARL effectively mitigates permutation attacks and enhances performance. Notably, despite being trained on fewer shots and shorter contexts, PEARL achieves performance gains of up to 40% when scaled to many-shot and long-context scenarios, highlighting its efficiency and generalization capabilities.

### Reward Models Identify Consistency, Not Causality 
[[arxiv](https://arxiv.org/abs/2502.14619)] [[cool](https://papers.cool/arxiv/2502.14619)] [[pdf](https://arxiv.org/pdf/2502.14619)]
> **Authors**: Yuhui Xu,Hanze Dong,Lei Wang,Caiming Xiong,Junnan Li
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 16 pages
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Reward models (RMs) play a crucial role in aligning large language models (LLMs) with human preferences and enhancing reasoning quality. Traditionally, RMs are trained to rank candidate outputs based on their correctness and coherence. However, in this work, we present several surprising findings that challenge common assumptions about RM behavior. Our analysis reveals that state-of-the-art reward models prioritize structural consistency over causal correctness. Specifically, removing the problem statement has minimal impact on reward scores, whereas altering numerical values or disrupting the reasoning flow significantly affects RM outputs. Furthermore, RMs exhibit a strong dependence on complete reasoning trajectories truncated or incomplete steps lead to significant variations in reward assignments, indicating that RMs primarily rely on learned reasoning patterns rather than explicit problem comprehension. These findings hold across multiple architectures, datasets, and tasks, leading to three key insights: (1) RMs primarily assess coherence rather than true reasoning quality; (2) The role of explicit problem comprehension in reward assignment is overstated; (3) Current RMs may be more effective at ranking responses than verifying logical validity. Our results suggest a fundamental limitation in existing reward modeling approaches, emphasizing the need for a shift toward causality-aware reward models that go beyond consistency-driven evaluation.

### Noisy Test-Time Adaptation in Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2502.14604)] [[cool](https://papers.cool/arxiv/2502.14604)] [[pdf](https://arxiv.org/pdf/2502.14604)]
> **Authors**: Chentao Cao,Zhun Zhong,Zhanke Zhou,Tongliang Liu,Yang Liu,Kun Zhang,Bo Han
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: ICLR 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Test-time adaptation (TTA) aims to address distribution shifts between source and target data by relying solely on target data during testing. In open-world scenarios, models often encounter noisy samples, i.e., samples outside the in-distribution (ID) label space. Leveraging the zero-shot capability of pre-trained vision-language models (VLMs), this paper introduces Zero-Shot Noisy TTA (ZS-NTTA), focusing on adapting the model to target data with noisy samples during test-time in a zero-shot manner. We find existing TTA methods underperform under ZS-NTTA, often lagging behind even the frozen model. We conduct comprehensive experiments to analyze this phenomenon, revealing that the negative impact of unfiltered noisy data outweighs the benefits of clean data during model updating. Also, adapting a classifier for ID classification and noise detection hampers both sub-tasks. Built on this, we propose a framework that decouples the classifier and detector, focusing on developing an individual detector while keeping the classifier frozen. Technically, we introduce the Adaptive Noise Detector (AdaND), which utilizes the frozen model's outputs as pseudo-labels to train a noise detector. To handle clean data streams, we further inject Gaussian noise during adaptation, preventing the detector from misclassifying clean samples as noisy. Beyond the ZS-NTTA, AdaND can also improve the zero-shot out-of-distribution (ZS-OOD) detection ability of VLMs. Experiments show that AdaND outperforms in both ZS-NTTA and ZS-OOD detection. On ImageNet, AdaND achieves a notable improvement of $8.32\%$ in harmonic mean accuracy ($\text{Acc}_\text{H}$) for ZS-NTTA and $9.40\%$ in FPR95 for ZS-OOD detection, compared to SOTA methods. Importantly, AdaND is computationally efficient and comparable to the model-frozen method. The code is publicly available at: https://github.com/tmlr-group/ZS-NTTA.

### Multi-Class Imbalanced Learning with Support Vector Machines via Differential Evolution 
[[arxiv](https://arxiv.org/abs/2502.14597)] [[cool](https://papers.cool/arxiv/2502.14597)] [[pdf](https://arxiv.org/pdf/2502.14597)]
> **Authors**: Zhong-Liang Zhang,Jie Yang,Jian-Ming Ru,Xiao-Xi Zhao,Xing-Gang Luo
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,神经和进化计算
- **Abstract**: Support vector machine (SVM) is a powerful machine learning algorithm to handle classification tasks. However, the classical SVM is developed for binary problems with the assumption of balanced datasets. Obviously, the multi-class imbalanced classification problems are more complex. In this paper, we propose an improved SVM via Differential Evolution (i-SVM-DE) method to deal with it. An improved SVM (i-SVM) model is proposed to handle the data imbalance by combining cost sensitive technique and separation margin modification in the constraints, which formalize a parameter optimization problem. By using one-versus-one (OVO) scheme, a multi-class problem is decomposed into a number of binary subproblems. A large optimization problem is formalized through concatenating the parameters in the binary subproblems. To find the optimal model effectively and learn the support vectors for each class simultaneously, an improved differential evolution (DE) algorithm is applied to solve this large optimization problem. Instead of the validation set, we propose the fitness functions to evaluate the learned model and obtain the optimal parameters in the search process of DE. A series of experiments are carried out to verify the benefits of our proposed method. The results indicate that i-SVM-DE is statistically superior by comparing with the other baseline methods.

### Moshi Moshi? A Model Selection Hijacking Adversarial Attack 
[[arxiv](https://arxiv.org/abs/2502.14586)] [[cool](https://papers.cool/arxiv/2502.14586)] [[pdf](https://arxiv.org/pdf/2502.14586)]
> **Authors**: Riccardo Petrucci,Luca Pajola,Francesco Marchiori,Luca Pasa,Mauro conti
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,密码学和安全
- **Abstract**: Model selection is a fundamental task in Machine Learning~(ML), focusing on selecting the most suitable model from a pool of candidates by evaluating their performance on specific metrics. This process ensures optimal performance, computational efficiency, and adaptability to diverse tasks and environments. Despite its critical role, its security from the perspective of adversarial ML remains unexplored. This risk is heightened in the Machine-Learning-as-a-Service model, where users delegate the training phase and the model selection process to third-party providers, supplying data and training strategies. Therefore, attacks on model selection could harm both the user and the provider, undermining model performance and driving up operational costs. In this work, we present MOSHI (MOdel Selection HIjacking adversarial attack), the first adversarial attack specifically targeting model selection. Our novel approach manipulates model selection data to favor the adversary, even without prior knowledge of the system. Utilizing a framework based on Variational Auto Encoders, we provide evidence that an attacker can induce inefficiencies in ML deployment. We test our attack on diverse computer vision and speech recognition benchmark tasks and different settings, obtaining an average attack success rate of 75.42%. In particular, our attack causes an average 88.30% decrease in generalization capabilities, an 83.33% increase in latency, and an increase of up to 105.85% in energy consumption. These results highlight the significant vulnerabilities in model selection processes and their potential impact on real-world applications.

### A Theory for Conditional Generative Modeling on Multiple Data Sources 
[[arxiv](https://arxiv.org/abs/2502.14583)] [[cool](https://papers.cool/arxiv/2502.14583)] [[pdf](https://arxiv.org/pdf/2502.14583)]
> **Authors**: Rongzhen Wang,Yan Zhang,Chenyu Zheng,Chongxuan Li,Guoqiang Wu
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 35 pages
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: The success of large generative models has driven a paradigm shift, leveraging massive multi-source data to enhance model capabilities. However, the interaction among these sources remains theoretically underexplored. This paper takes the first step toward a rigorous analysis of multi-source training in conditional generative modeling, where each condition represents a distinct data source. Specifically, we establish a general distribution estimation error bound in average total variation distance for conditional maximum likelihood estimation based on the bracketing number. Our result shows that when source distributions share certain similarities and the model is expressive enough, multi-source training guarantees a sharper bound than single-source training. We further instantiate the general theory on conditional Gaussian estimation and deep generative models including autoregressive and flexible energy-based models, by characterizing their bracketing numbers. The results highlight that the number of sources and similarity among source distributions improve the advantage of multi-source training. Simulations and real-world experiments validate our theory. Code is available at: \url{https://github.com/ML-GSAI/Multi-Source-GM}.

### Factor Graph-based Interpretable Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.14572)] [[cool](https://papers.cool/arxiv/2502.14572)] [[pdf](https://arxiv.org/pdf/2502.14572)]
> **Authors**: Yicong Li,Kuanjiu Zhou,Shuo Yu,Qiang Zhang,Renqiang Luo,Xiaodong Li,Feng Xia
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: The Thirteenth International Conference onLearningRepresentations
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Comprehensible neural network explanations are foundations for a better understanding of decisions, especially when the input data are infused with malicious perturbations. Existing solutions generally mitigate the impact of perturbations through adversarial training, yet they fail to generate comprehensible explanations under unknown perturbations. To address this challenge, we propose AGAIN, a fActor GrAph-based Interpretable neural Network, which is capable of generating comprehensible explanations under unknown perturbations. Instead of retraining like previous solutions, the proposed AGAIN directly integrates logical rules by which logical errors in explanations are identified and rectified during inference. Specifically, we construct the factor graph to express logical rules between explanations and categories. By treating logical rules as exogenous knowledge, AGAIN can identify incomprehensible explanations that violate real-world logic. Furthermore, we propose an interactive intervention switch strategy rectifying explanations based on the logical guidance from the factor graph without learning perturbations, which overcomes the inherent limitation of adversarial training-based methods in defending only against known perturbations. Additionally, we theoretically demonstrate the effectiveness of employing factor graph by proving that the comprehensibility of explanations is strongly correlated with factor graph. Extensive experiments are conducted on three datasets and experimental results illustrate the superior performance of AGAIN compared to state-of-the-art baselines.

### Predicting Filter Medium Performances in Chamber Filter Presses with Digital Twins Using Neural Network Technologies 
[[arxiv](https://arxiv.org/abs/2502.14571)] [[cool](https://papers.cool/arxiv/2502.14571)] [[pdf](https://arxiv.org/pdf/2502.14571)]
> **Authors**: Dennis Teutscher,Tyll Weber-Carstanjen,Stephan Simonis,Mathias J. Krause
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算工程、金融和科学
- **Abstract**: Efficient solid-liquid separation is crucial in industries like mining, but traditional chamber filter presses depend heavily on manual monitoring, leading to inefficiencies, downtime, and resource wastage. This paper introduces a machine learning-powered digital twin framework to improve operational flexibility and predictive control. A key challenge addressed is the degradation of the filter medium due to repeated cycles and clogging, which reduces filtration efficiency. To solve this, a neural network-based predictive model was developed to forecast operational parameters, such as pressure and flow rates, under various conditions. This predictive capability allows for optimized filtration cycles, reduced downtime, and improved process efficiency. Additionally, the model predicts the filter mediums lifespan, aiding in maintenance planning and resource sustainability. The digital twin framework enables seamless data exchange between filter press sensors and the predictive model, ensuring continuous updates to the training data and enhancing accuracy over time. Two neural network architectures, feedforward and recurrent, were evaluated. The recurrent neural network outperformed the feedforward model, demonstrating superior generalization. It achieved a relative $L^2$-norm error of $5\%$ for pressure and $9.3\%$ for flow rate prediction on partially known data. For completely unknown data, the relative errors were $18.4\%$ and $15.4\%$, respectively. Qualitative analysis showed strong alignment between predicted and measured data, with deviations within a confidence band of $8.2\%$ for pressure and $4.8\%$ for flow rate predictions. This work contributes an accurate predictive model, a new approach to predicting filter medium cycle impacts, and a real-time interface for model updates, ensuring adaptability to changing operational conditions.

### ReVISE: Learning to Refine at Test-Time via Intrinsic Self-Verification 
[[arxiv](https://arxiv.org/abs/2502.14565)] [[cool](https://papers.cool/arxiv/2502.14565)] [[pdf](https://arxiv.org/pdf/2502.14565)]
> **Authors**: Hyunseok Lee,Seunghyuk Oh,Jaehyung Kim,Jinwoo Shin,Jihoon Tack
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Self-awareness, i.e., the ability to assess and correct one's own generation, is a fundamental aspect of human intelligence, making its replication in large language models (LLMs) an important yet challenging task. Previous works tackle this by employing extensive reinforcement learning or rather relying on large external verifiers. In this work, we propose Refine via Intrinsic Self-Verification (ReVISE), an efficient and effective framework that enables LLMs to self-correct their outputs through self-verification. The core idea of ReVISE is to enable LLMs to verify their reasoning processes and continually rethink reasoning trajectories based on its verification. We introduce a structured curriculum based upon online preference learning to implement this efficiently. Specifically, as ReVISE involves two challenging tasks (i.e., self-verification and reasoning correction), we tackle each task sequentially using curriculum learning, collecting both failed and successful reasoning paths to construct preference pairs for efficient training. During inference, our approach enjoys natural test-time scaling by integrating self-verification and correction capabilities, further enhanced by our proposed confidence-aware decoding mechanism. Our experiments on various reasoning tasks demonstrate that ReVISE achieves efficient self-correction and significantly improves reasoning performance.

### Less is More: Improving LLM Alignment via Preference Data Selection 
[[arxiv](https://arxiv.org/abs/2502.14560)] [[cool](https://papers.cool/arxiv/2502.14560)] [[pdf](https://arxiv.org/pdf/2502.14560)]
> **Authors**: Xun Deng,Han Zhong,Rui Ai,Fuli Feng,Zheng Wang,Xiangnan He
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Direct Preference Optimization (DPO) has emerged as a promising approach for aligning large language models with human preferences. While prior work mainly extends DPO from the aspect of the objective function, we instead improve DPO from the largely overlooked but critical aspect of data selection. Specifically, we address the issue of parameter shrinkage caused by noisy data by proposing a novel margin-maximization principle for dataset curation in DPO training. To accurately estimate margins for data selection, we propose a dual-margin guided approach that considers both external reward margins and implicit DPO reward margins. Extensive experiments demonstrate that our method reduces computational cost dramatically while improving performance. Remarkably, by using just 10\% of the Ultrafeedback dataset, our approach achieves 3\% to 8\% improvements across various Llama and Mistral series models on the AlpacaEval 2.0 benchmark. Furthermore, our approach seamlessly extends to iterative DPO, yielding a roughly 3\% improvement with 25\% online data, while further reducing training time. These results highlight the potential of data selection strategies for advancing preference optimization.

### Position: Graph Learning Will Lose Relevance Due To Poor Benchmarks 
[[arxiv](https://arxiv.org/abs/2502.14546)] [[cool](https://papers.cool/arxiv/2502.14546)] [[pdf](https://arxiv.org/pdf/2502.14546)]
> **Authors**: Maya Bechler-Speicher,Ben Finkelshtein,Fabrizio Frasca,Luis Müller,Jan Tönshoff,Antoine Siraudin,Viktor Zaverkin,Michael M. Bronstein,Mathias Niepert,Bryan Perozzi,Mikhail Galkin,Christopher Morris
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,神经和进化计算
- **Abstract**: While machine learning on graphs has demonstrated promise in drug design and molecular property prediction, significant benchmarking challenges hinder its further progress and relevance. Current benchmarking practices often lack focus on transformative, real-world applications, favoring narrow domains like two-dimensional molecular graphs over broader, impactful areas such as combinatorial optimization, relational databases, or chip design. Additionally, many benchmark datasets poorly represent the underlying data, leading to inadequate abstractions and misaligned use cases. Fragmented evaluations and an excessive focus on accuracy further exacerbate these issues, incentivizing overfitting rather than fostering generalizable insights. These limitations have prevented the development of truly useful graph foundation models. This position paper calls for a paradigm shift toward more meaningful benchmarks, rigorous evaluation protocols, and stronger collaboration with domain experts to drive impactful and reliable advances in graph learning research, unlocking the potential of graph learning.

### An Entropic Metric for Measuring Calibration of Machine Learning Models 
[[arxiv](https://arxiv.org/abs/2502.14545)] [[cool](https://papers.cool/arxiv/2502.14545)] [[pdf](https://arxiv.org/pdf/2502.14545)]
> **Authors**: Daniel James Sumler,Lee Devlin,Simon Maskell,Richard O. Lane
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Understanding the confidence with which a machine learning model classifies an input datum is an important, and perhaps under-investigated, concept. In this paper, we propose a new calibration metric, the Entropic Calibration Difference (ECD). Based on existing research in the field of state estimation, specifically target tracking (TT), we show how ECD may be applied to binary classification machine learning models. We describe the relative importance of under- and over-confidence and how they are not conflated in the TT literature. Indeed, our metric distinguishes under- from over-confidence. We consider this important given that algorithms that are under-confident are likely to be 'safer' than algorithms that are over-confident, albeit at the expense of also being over-cautious and so statistically inefficient. We demonstrate how this new metric performs on real and simulated data and compare with other metrics for machine learning model probability calibration, including the Expected Calibration Error (ECE) and its signed counterpart, the Expected Signed Calibration Error (ESCE).

### Preordering: A hybrid of correlation clustering and partial ordering 
[[arxiv](https://arxiv.org/abs/2502.14536)] [[cool](https://papers.cool/arxiv/2502.14536)] [[pdf](https://arxiv.org/pdf/2502.14536)]
> **Authors**: Jannik Irmai,Maximilian Moeller,Bjoern Andres
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: Source code: https://github.com/JannikIrmai/preordering-problem
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We discuss the preordering problem, a joint relaxation of the correlation clustering problem and the partial ordering problem. We show that preordering remains NP-hard even for values in $\{-1,0,1\}$. We introduce a linear-time $4$-approximation algorithm and a local search technique. For an integer linear program formulation, we establish a class of non-canonical facets of the associated preorder polytope. By solving a non-canonical linear program relaxation, we obtain non-trivial upper bounds on the objective value. We provide implementations of the algorithms we define, apply these to published social networks and compare the output and efficiency qualitatively and quantitatively.

### Inter-turbine Modelling of Wind-Farm Power using Multi-task Learning 
[[arxiv](https://arxiv.org/abs/2502.14527)] [[cool](https://papers.cool/arxiv/2502.14527)] [[pdf](https://arxiv.org/pdf/2502.14527)]
> **Authors**: Simon M. Brealy,Lawrence A. Bull,Pauline Beltrando,Anders Sommer,Nikolaos Dervilis,Keith Worden
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: Preprint submitted to Mechanical Systems and Signal Processing. A shortened version of this article has submitted to the Wind Energy Science Conference 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Because of the global need to increase power production from renewable energy resources, developments in the online monitoring of the associated infrastructure is of interest to reduce operation and maintenance costs. However, challenges exist for data-driven approaches to this problem, such as incomplete or limited histories of labelled damage-state data, operational and environmental variability, or the desire for the quantification of uncertainty to support risk management. This work first introduces a probabilistic regression model for predicting wind-turbine power, which adjusts for wake effects learnt from data. Spatial correlations in the learned model parameters for different tasks (turbines) are then leveraged in a hierarchical Bayesian model (an approach to multi-task learning) to develop a "metamodel", which can be used to make power-predictions which adjust for turbine location - including on previously unobserved turbines not included in the training data. The results show that the metamodel is able to outperform a series of benchmark models, and demonstrates a novel strategy for making efficient use of data for inference in populations of structures, in particular where correlations exist in the variable(s) of interest (such as those from wind-turbine wake-effects).

### Small Graph Is All You Need: DeepStateGNN for Scalable Traffic Forecasting 
[[arxiv](https://arxiv.org/abs/2502.14525)] [[cool](https://papers.cool/arxiv/2502.14525)] [[pdf](https://arxiv.org/pdf/2502.14525)]
> **Authors**: Yannick Wölker,Arash Hajisafi,Cyrus Shahabi,Matthias Renz
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: Yannick Wölker and Arash Hajisafi contributed equally to this work
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: We propose a novel Graph Neural Network (GNN) model, named DeepStateGNN, for analyzing traffic data, demonstrating its efficacy in two critical tasks: forecasting and reconstruction. Unlike typical GNN methods that treat each traffic sensor as an individual graph node, DeepStateGNN clusters sensors into higher-level graph nodes, dubbed Deep State Nodes, based on various similarity criteria, resulting in a fixed number of nodes in a Deep State graph. The term "Deep State" nodes is a play on words, referencing hidden networks of power that, like these nodes, secretly govern traffic independently of visible sensors. These Deep State Nodes are defined by several similarity factors, including spatial proximity (e.g., sensors located nearby in the road network), functional similarity (e.g., sensors on similar types of freeways), and behavioral similarity under specific conditions (e.g., traffic behavior during rain). This clustering approach allows for dynamic and adaptive node grouping, as sensors can belong to multiple clusters and clusters may evolve over time. Our experimental results show that DeepStateGNN offers superior scalability and faster training, while also delivering more accurate results than competitors. It effectively handles large-scale sensor networks, outperforming other methods in both traffic forecasting and reconstruction accuracy.

### Generative adversarial networks vs large language models: a comparative study on synthetic tabular data generation 
[[arxiv](https://arxiv.org/abs/2502.14523)] [[cool](https://papers.cool/arxiv/2502.14523)] [[pdf](https://arxiv.org/pdf/2502.14523)]
> **Authors**: Austin A. Barr,Robert Rozman,Eddie Guo
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 12 pages, 7 figures, 5 tables
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: We propose a new framework for zero-shot generation of synthetic tabular data. Using the large language model (LLM) GPT-4o and plain-language prompting, we demonstrate the ability to generate high-fidelity tabular data without task-specific fine-tuning or access to real-world data (RWD) for pre-training. To benchmark GPT-4o, we compared the fidelity and privacy of LLM-generated synthetic data against data generated with the conditional tabular generative adversarial network (CTGAN), across three open-access datasets: Iris, Fish Measurements, and Real Estate Valuation. Despite the zero-shot approach, GPT-4o outperformed CTGAN in preserving means, 95% confidence intervals, bivariate correlations, and data privacy of RWD, even at amplified sample sizes. Notably, correlations between parameters were consistently preserved with appropriate direction and strength. However, refinement is necessary to better retain distributional characteristics. These findings highlight the potential of LLMs in tabular data synthesis, offering an accessible alternative to generative adversarial networks and variational autoencoders.

### Investigating the Generalizability of ECG Noise Detection Across Diverse Data Sources and Noise Types 
[[arxiv](https://arxiv.org/abs/2502.14522)] [[cool](https://papers.cool/arxiv/2502.14522)] [[pdf](https://arxiv.org/pdf/2502.14522)]
> **Authors**: Sharmad Kalpande,Nilesh Kumar Sahu,Haroon Lone
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Electrocardiograms (ECGs) are essential for monitoring cardiac health, allowing clinicians to analyze heart rate variability (HRV), detect abnormal rhythms, and diagnose cardiovascular diseases. However, ECG signals, especially those from wearable devices, are often affected by noise artifacts caused by motion, muscle activity, or device-related interference. These artifacts distort R-peaks and the characteristic QRS complex, making HRV analysis unreliable and increasing the risk of misdiagnosis. Despite this, the few existing studies on ECG noise detection have primarily focused on a single dataset, limiting the understanding of how well noise detection models generalize across different datasets. In this paper, we investigate the generalizability of noise detection in ECG using a novel HRV-based approach through cross-dataset experiments on four datasets. Our results show that machine learning achieves an average accuracy of over 90\% and an AUPRC of more than 0.9. These findings suggest that regardless of the ECG data source or the type of noise, the proposed method maintains high accuracy even on unseen datasets, demonstrating the feasibility of generalizability.

### Temporal Misalignment in ANN-SNN Conversion and Its Mitigation via Probabilistic Spiking Neurons 
[[arxiv](https://arxiv.org/abs/2502.14487)] [[cool](https://papers.cool/arxiv/2502.14487)] [[pdf](https://arxiv.org/pdf/2502.14487)]
> **Authors**: Velibor Bojković,Xiaofeng Wu,Bin Gu
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: Spiking Neural Networks (SNNs) offer a more energy-efficient alternative to Artificial Neural Networks (ANNs) by mimicking biological neural principles, establishing them as a promising approach to mitigate the increasing energy demands of large-scale neural models. However, fully harnessing the capabilities of SNNs remains challenging due to their discrete signal processing and temporal dynamics. ANN-SNN conversion has emerged as a practical approach, enabling SNNs to achieve competitive performance on complex machine learning tasks. In this work, we identify a phenomenon in the ANN-SNN conversion framework, termed temporal misalignment, in which random spike rearrangement across SNN layers leads to performance improvements. Based on this observation, we introduce biologically plausible two-phase probabilistic (TPP) spiking neurons, further enhancing the conversion process. We demonstrate the advantages of our proposed method both theoretically and empirically through comprehensive experiments on CIFAR-10/100, CIFAR10-DVS, and ImageNet across a variety of architectures, achieving state-of-the-art results.

### Llamba: Scaling Distilled Recurrent Models for Efficient Language Processing 
[[arxiv](https://arxiv.org/abs/2502.14458)] [[cool](https://papers.cool/arxiv/2502.14458)] [[pdf](https://arxiv.org/pdf/2502.14458)]
> **Authors**: Aviv Bick,Tobias Katsch,Nimit Sohoni,Arjun Desai,Albert Gu
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: We introduce Llamba, a family of efficient recurrent language models distilled from Llama-3.x into the Mamba architecture. The series includes Llamba-1B, Llamba-3B, and Llamba-8B, which achieve higher inference throughput and handle significantly larger batch sizes than Transformer-based models while maintaining comparable benchmark performance. Furthermore, Llamba demonstrates the effectiveness of cross-architecture distillation using MOHAWK (Bick et al., 2024), achieving these results with less than 0.1% of the training data typically used for models of similar size. To take full advantage of their efficiency, we provide an optimized implementation of Llamba for resource-constrained devices such as smartphones and edge platforms, offering a practical and memory-efficient alternative to Transformers. Overall, Llamba improves the tradeoff between speed, memory efficiency, and performance, making high-quality language models more accessible.

### Port-Hamiltonian Neural Networks with Output Error Noise Models 
[[arxiv](https://arxiv.org/abs/2502.14432)] [[cool](https://papers.cool/arxiv/2502.14432)] [[pdf](https://arxiv.org/pdf/2502.14432)]
> **Authors**: Sarvin Moradi,Gerben I. Beintema,Nick Jaensson,Roland Tóth,Maarten Schoukens
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: Preprint submitted to Automatica
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Hamiltonian neural networks (HNNs) represent a promising class of physics-informed deep learning methods that utilize Hamiltonian theory as foundational knowledge within neural networks. However, their direct application to engineering systems is often challenged by practical issues, including the presence of external inputs, dissipation, and noisy measurements. This paper introduces a novel framework that enhances the capabilities of HNNs to address these real-life factors. We integrate port-Hamiltonian theory into the neural network structure, allowing for the inclusion of external inputs and dissipation, while mitigating the impact of measurement noise through an output-error (OE) model structure. The resulting output error port-Hamiltonian neural networks (OE-pHNNs) can be adapted to tackle modeling complex engineering systems with noisy measurements. Furthermore, we propose the identification of OE-pHNNs based on the subspace encoder approach (SUBNET), which efficiently approximates the complete simulation loss using subsections of the data and uses an encoder function to predict initial states. By integrating SUBNET with OE-pHNNs, we achieve consistent models of complex engineering systems under noisy measurements. In addition, we perform a consistency analysis to ensure the reliability of the proposed data-driven model learning method. We demonstrate the effectiveness of our approach on system identification benchmarks, showing its potential as a powerful tool for modeling dynamic systems in real-world applications.

### Cardiac Evidence Backtracking for Eating Behavior Monitoring using Collocative Electrocardiogram Imagining 
[[arxiv](https://arxiv.org/abs/2502.14430)] [[cool](https://papers.cool/arxiv/2502.14430)] [[pdf](https://arxiv.org/pdf/2502.14430)]
> **Authors**: Xu-Lu Zhang,Zhen-Qun Yang,Dong-Mei Jiang,Ga Liao,Qing Li,Ramesh Jain,Xiao-Yong Wei
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算工程、金融和科学
- **Abstract**: Eating monitoring has remained an open challenge in medical research for years due to the lack of non-invasive sensors for continuous monitoring and the reliable methods for automatic behavior detection. In this paper, we present a pilot study using the wearable 24-hour ECG for sensing and tailoring the sophisticated deep learning for ad-hoc and interpretable detection. This is accomplished using a collocative learning framework in which 1) we construct collocative tensors as pseudo-images from 1D ECG signals to improve the feasibility of 2D image-based deep models; 2) we formulate the cardiac logic of analyzing the ECG data in a comparative way as periodic attention regulators so as to guide the deep inference to collect evidence in a human comprehensible manner; and 3) we improve the interpretability of the framework by enabling the backtracking of evidence with a set of methods designed for Class Activation Mapping (CAM) decoding and decision tree/forest generation. The effectiveness of the proposed framework has been validated on the largest ECG dataset of eating behavior with superior performance over conventional models, and its capacity of cardiac evidence mining has also been verified through the consistency of the evidence it backtracked and that of the previous medical studies.

### Towards Efficient Automatic Self-Pruning of Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.14413)] [[cool](https://papers.cool/arxiv/2502.14413)] [[pdf](https://arxiv.org/pdf/2502.14413)]
> **Authors**: Weizhong Huang,Yuxin Zhang,Xiawu Zheng,Fei Chao,Rongrong Ji
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Despite exceptional capabilities, Large Language Models (LLMs) still face deployment challenges due to their enormous size. Post-training structured pruning is a promising solution that prunes LLMs without the need for retraining, reducing computational overhead, and it is hardware-deployment friendly. However, the training-free nature of post-training structured pruning leads to significant performance degradation. We argue that the key to mitigating this issue lies in accurately determining the pruning rate for each layer. Meanwhile, we find that LLMs may have prior knowledge about their own redundancy. Based on this insight, we introduce $\textbf{Self-Pruner}$ an end-to-end automatic self-pruning framework for LLMs, which efficiently search layer-wise pruning rates. Specifically, $\textbf{Self-Pruner}$ leverages LLMs to autonomously execute the entire evolutionary search process to search for pruning rate configurations. In this process, LLMs are used to generate populations, select parent solutions from the current population, and perform crossover and mutation operations to produce offspring solutions. In this way, LLMs automatically generate and evaluate a large number of candidate solutions, effectively converging to find the pruning rate configurations with minimal human intervention. Extensive experiments demonstrate $\textbf{Self-Pruner}$'s better performance compared to existing state-of-the-art methods. Notably, $\textbf{Self-Pruner}$ prunes LLaMA-2-70B to 49B level with only 0.80$\%$ drop in accuracy across seven commonsense reasoning tasks, achieving a 1.39$\times$ speedup on NVIDIA A100 80GB GPU. Further pruning to 35B level resulted in only a 3.80$\%$ decrease in accuracy while obtaining a 1.70$\times$ speedup.

### S*: Test Time Scaling for Code Generation 
[[arxiv](https://arxiv.org/abs/2502.14382)] [[cool](https://papers.cool/arxiv/2502.14382)] [[pdf](https://arxiv.org/pdf/2502.14382)]
> **Authors**: Dacheng Li,Shiyi Cao,Chengkun Cao,Xiuyu Li,Shangyin Tan,Kurt Keutzer,Jiarong Xing,Joseph E. Gonzalez,Ion Stoica
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Increasing test-time compute for LLMs shows promise across domains but remains underexplored in code generation, despite extensive study in math. In this paper, we propose S*, the first hybrid test-time scaling framework that substantially improves the coverage and selection accuracy of generated code. S* extends the existing parallel scaling paradigm with sequential scaling to push performance boundaries. It further leverages a novel selection mechanism that adaptively generates distinguishing inputs for pairwise comparison, combined with execution-grounded information to robustly identify correct solutions. We evaluate across 12 Large Language Models and Large Reasoning Model and show: (1) S* consistently improves performance across model families and sizes, enabling a 3B model to outperform GPT-4o-mini; (2) S* enables non-reasoning models to surpass reasoning models - GPT-4o-mini with S* outperforms o1-preview by 3.7% on LiveCodeBench; (3) S* further boosts state-of-the-art reasoning models - DeepSeek-R1-Distill-Qwen-32B with S* achieves 85.7% on LiveCodeBench, approaching o1 (high) at 88.5%. Code will be available under https://github.com/NovaSky-AI/SkyThought.

### dtaianomaly: A Python library for time series anomaly detection 
[[arxiv](https://arxiv.org/abs/2502.14381)] [[cool](https://papers.cool/arxiv/2502.14381)] [[pdf](https://arxiv.org/pdf/2502.14381)]
> **Authors**: Louis Carpentier,Nick Seeuws,Wannes Meert,Mathias Verbeke
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,数据库
- **Abstract**: dtaianomaly is an open-source Python library for time series anomaly detection, designed to bridge the gap between academic research and real-world applications. Our goal is to (1) accelerate the development of novel state-of-the-art anomaly detection techniques through simple extensibility; (2) offer functionality for large-scale experimental validation; and thereby (3) bring cutting-edge research to business and industry through a standardized API, similar to scikit-learn to lower the entry barrier for both new and experienced users. Besides these key features, dtaianomaly offers (1) a broad range of built-in anomaly detectors, (2) support for time series preprocessing, (3) tools for visual analysis, (4) confidence prediction of anomaly scores, (5) runtime and memory profiling, (6) comprehensive documentation, and (7) cross-platform unit testing. The source code of dtaianomaly, documentation, code examples and installation guides are publicly available at https://github.com/ML-KULeuven/dtaianomaly.

### Achieving adaptivity and optimality for multi-armed bandits using Exponential-Kullback Leiblier Maillard Sampling 
[[arxiv](https://arxiv.org/abs/2502.14379)] [[cool](https://papers.cool/arxiv/2502.14379)] [[pdf](https://arxiv.org/pdf/2502.14379)]
> **Authors**: Hao Qin,Kwang-Sung Jun,Chicheng Zhang
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 12 pages of the main body, 2 figures, 43 pages in total
- **标题**: None
- **领域**: 机器学习,数据结构和算法
- **Abstract**: We study the problem of Multi-Armed Bandits (MAB) with reward distributions belonging to a One-Parameter Exponential Distribution (OPED) family. In the literature, several criteria have been proposed to evaluate the performance of such algorithms, including Asymptotic Optimality (A.O.), Minimax Optimality (M.O.), Sub-UCB, and variance-adaptive worst-case regret bound. Thompson Sampling (TS)-based and Upper Confidence Bound (UCB)-based algorithms have been employed to achieve some of these criteria. However, none of these algorithms simultaneously satisfy all the aforementioned criteria. In this paper, we design an algorithm, Exponential Kullback-Leibler Maillard Sampling (abbrev. \expklms), that can achieve multiple optimality criteria simultaneously, including A.O., M.O. with a logarithmic factor, Sub-UCB, and variance-adaptive worst-case regret bound.

### VFL-RPS: Relevant Participant Selection in Vertical Federated Learning 
[[arxiv](https://arxiv.org/abs/2502.14375)] [[cool](https://papers.cool/arxiv/2502.14375)] [[pdf](https://arxiv.org/pdf/2502.14375)]
> **Authors**: Afsana Khan,Marijn ten Thij,Guangzhi Tang,Anna Wilbik
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Federated Learning (FL) allows collaboration between different parties, while ensuring that the data across these parties is not shared. However, not every collaboration is helpful in terms of the resulting model performance. Therefore, it is an important challenge to select the correct participants in a collaboration. As it currently stands, most of the efforts in participant selection in the literature have focused on Horizontal Federated Learning (HFL), which assumes that all features are the same across all participants, disregarding the possibility of different features across participants which is captured in Vertical Federated Learning (VFL). To close this gap in the literature, we propose a novel method VFL-RPS for participant selection in VFL, as a pre-training step. We have tested our method on several data sets performing both regression and classification tasks, showing that our method leads to comparable results as using all data by only selecting a few participants. In addition, we show that our method outperforms existing methods for participant selection in VFL.

### PPO-MI: Efficient Black-Box Model Inversion via Proximal Policy Optimization 
[[arxiv](https://arxiv.org/abs/2502.14370)] [[cool](https://papers.cool/arxiv/2502.14370)] [[pdf](https://arxiv.org/pdf/2502.14370)]
> **Authors**: Xinpeng Shou
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 6 pages, submitting to ICML 2025
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: Model inversion attacks pose a significant privacy risk by attempting to reconstruct private training data from trained models. Most of the existing methods either depend on gradient estimation or require white-box access to model parameters, which limits their applicability in practical scenarios. In this paper, we propose PPO-MI, a novel reinforcement learning-based framework for black-box model inversion attacks. Our approach formulates the inversion task as a Markov Decision Process, where an agent navigates the latent space of a generative model to reconstruct private training samples using only model predictions. By employing Proximal Policy Optimization (PPO) with a momentum-based state transition mechanism, along with a reward function balancing prediction accuracy and exploration, PPO-MI ensures efficient latent space exploration and high query efficiency. We conduct extensive experiments illustrates that PPO-MI outperforms the existing methods while require less attack knowledge, and it is robust across various model architectures and datasets. These results underline its effectiveness and generalizability in practical black-box scenarios, raising important considerations for the privacy vulnerabilities of deployed machine learning models.

### Is Q-learning an Ill-posed Problem? 
[[arxiv](https://arxiv.org/abs/2502.14365)] [[cool](https://papers.cool/arxiv/2502.14365)] [[pdf](https://arxiv.org/pdf/2502.14365)]
> **Authors**: Philipp Wissmann,Daniel Hein,Steffen Udluft,Thomas Runkler
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: Accepted at ESANN 2025
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: This paper investigates the instability of Q-learning in continuous environments, a challenge frequently encountered by practitioners. Traditionally, this instability is attributed to bootstrapping and regression model errors. Using a representative reinforcement learning benchmark, we systematically examine the effects of bootstrapping and model inaccuracies by incrementally eliminating these potential error sources. Our findings reveal that even in relatively simple benchmarks, the fundamental task of Q-learning - iteratively learning a Q-function from policy-specific target values - can be inherently ill-posed and prone to failure. These insights cast doubt on the reliability of Q-learning as a universal solution for reinforcement learning problems.

### Self-Improvement Towards Pareto Optimality: Mitigating Preference Conflicts in Multi-Objective Alignment 
[[arxiv](https://arxiv.org/abs/2502.14354)] [[cool](https://papers.cool/arxiv/2502.14354)] [[pdf](https://arxiv.org/pdf/2502.14354)]
> **Authors**: Moxin Li,Yuantao Zhang,Wenjie Wang,Wentao Shi,Zhuo Liu,Fuli Feng,Tat-Seng Chua
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: Under review
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Multi-Objective Alignment (MOA) aims to align LLMs' responses with multiple human preference objectives, with Direct Preference Optimization (DPO) emerging as a prominent approach. However, we find that DPO-based MOA approaches suffer from widespread preference conflicts in the data, where different objectives favor different responses. This results in conflicting optimization directions, hindering the optimization on the Pareto Front. To address this, we propose to construct Pareto-optimal responses to resolve preference conflicts. To efficiently obtain and utilize such responses, we propose a self-improving DPO framework that enables LLMs to self-generate and select Pareto-optimal responses for self-supervised preference alignment. Extensive experiments on two datasets demonstrate the superior Pareto Front achieved by our framework compared to various baselines. Code is available at \url{https://github.com/zyttt-coder/SIPO}.

### On Theoretical Limits of Learning with Label Differential Privacy 
[[arxiv](https://arxiv.org/abs/2502.14309)] [[cool](https://papers.cool/arxiv/2502.14309)] [[pdf](https://arxiv.org/pdf/2502.14309)]
> **Authors**: Puning Zhao,Chuan Ma,Li Shen,Shaowei Wang,Rongfei Fan
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,信息论
- **Abstract**: Label differential privacy (DP) is designed for learning problems involving private labels and public features. While various methods have been proposed for learning under label DP, the theoretical limits remain largely unexplored. In this paper, we investigate the fundamental limits of learning with label DP in both local and central models for both classification and regression tasks, characterized by minimax convergence rates. We establish lower bounds by converting each task into a multiple hypothesis testing problem and bounding the test error. Additionally, we develop algorithms that yield matching upper bounds. Our results demonstrate that under label local DP (LDP), the risk has a significantly faster convergence rate than that under full LDP, i.e. protecting both features and labels, indicating the advantages of relaxing the DP definition to focus solely on labels. In contrast, under the label central DP (CDP), the risk is only reduced by a constant factor compared to full DP, indicating that the relaxation of CDP only has limited benefits on the performance.

### Generalization Certificates for Adversarially Robust Bayesian Linear Regression 
[[arxiv](https://arxiv.org/abs/2502.14298)] [[cool](https://papers.cool/arxiv/2502.14298)] [[pdf](https://arxiv.org/pdf/2502.14298)]
> **Authors**: Mahalakshmi Sabanayagam,Russell Tsuchida,Cheng Soon Ong,Debarghya Ghoshdastidar
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: Under review
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Adversarial robustness of machine learning models is critical to ensuring reliable performance under data perturbations. Recent progress has been on point estimators, and this paper considers distributional predictors. First, using the link between exponential families and Bregman divergences, we formulate an adversarial Bregman divergence loss as an adversarial negative log-likelihood. Using the geometric properties of Bregman divergences, we compute the adversarial perturbation for such models in closed-form. Second, under such losses, we introduce \emph{adversarially robust posteriors}, by exploiting the optimization-centric view of generalized Bayesian inference. Third, we derive the \emph{first} rigorous generalization certificates in the context of an adversarial extension of Bayesian linear regression by leveraging the PAC-Bayesian framework. Finally, experiments on real and synthetic datasets demonstrate the superior robustness of the derived adversarially robust posterior over Bayes posterior, and also validate our theoretical guarantees.

### Graph Anomaly Detection via Adaptive Test-time Representation Learning across Out-of-Distribution Domains 
[[arxiv](https://arxiv.org/abs/2502.14293)] [[cool](https://papers.cool/arxiv/2502.14293)] [[pdf](https://arxiv.org/pdf/2502.14293)]
> **Authors**: Delaram Pirhayati,Arlei Silva
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,社交和信息网络
- **Abstract**: Graph Anomaly Detection (GAD) has demonstrated great effectiveness in identifying unusual patterns within graph-structured data. However, while labeled anomalies are often scarce in emerging applications, existing supervised GAD approaches are either ineffective or not applicable when moved across graph domains due to distribution shifts and heterogeneous feature spaces. To address these challenges, we present AdaGraph-T3, a novel test-time training framework for cross-domain GAD. AdaGraph-T3 combines supervised and self-supervised learning during training while adapting to a new domain during test time using only self-supervised learning by leveraging a homophily-based affinity score that captures domain-invariant properties of anomalies. Our framework introduces four key innovations to cross-domain GAD: an effective self-supervision scheme, an attention-based mechanism that dynamically learns edge importance weights during message passing, domain-specific encoders for handling heterogeneous features, and class-aware regularization to address imbalance. Experiments across multiple cross-domain settings demonstrate that AdaGraph-T3 significantly outperforms existing approaches, achieving average improvements of over 6.6% in AUROC and 7.9% in AUPRC compared to the best competing model.

### Correcting Noisy Multilabel Predictions: Modeling Label Noise through Latent Space Shifts 
[[arxiv](https://arxiv.org/abs/2502.14281)] [[cool](https://papers.cool/arxiv/2502.14281)] [[pdf](https://arxiv.org/pdf/2502.14281)]
> **Authors**: Weipeng Huang,Qin Li,Yang Xiao,Cheng Qiao,Tie Cai,Junwei Liao,Neil J. Hurley,Guangyuan Piao
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Noise in data appears to be inevitable in most real-world machine learning applications and would cause severe overfitting problems. Not only can data features contain noise, but labels are also prone to be noisy due to human input. In this paper, rather than noisy label learning in multiclass classifications, we instead focus on the less explored area of noisy label learning for multilabel classifications. Specifically, we investigate the post-correction of predictions generated from classifiers learned with noisy labels. The reasons are two-fold. Firstly, this approach can directly work with the trained models to save computational resources. Secondly, it could be applied on top of other noisy label correction techniques to achieve further improvements. To handle this problem, we appeal to deep generative approaches that are possible for uncertainty estimation. Our model posits that label noise arises from a stochastic shift in the latent variable, providing a more robust and beneficial means for noisy learning. We develop both unsupervised and semi-supervised learning methods for our model. The extensive empirical study presents solid evidence to that our approach is able to consistently improve the independent models and performs better than a number of existing methods across various noisy label settings. Moreover, a comprehensive empirical analysis of the proposed method is carried out to validate its robustness, including sensitivity analysis and an ablation study, among other elements.

### STeCa: Step-level Trajectory Calibration for LLM Agent Learning 
[[arxiv](https://arxiv.org/abs/2502.14276)] [[cool](https://papers.cool/arxiv/2502.14276)] [[pdf](https://arxiv.org/pdf/2502.14276)]
> **Authors**: Hanlin Wang,Jian Wang,Chak Tou Leong,Wenjie Li
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Large language model (LLM)-based agents have shown promise in tackling complex tasks by interacting dynamically with the environment. Existing work primarily focuses on behavior cloning from expert demonstrations and preference learning through exploratory trajectory sampling. However, these methods often struggle in long-horizon tasks, where suboptimal actions accumulate step by step, causing agents to deviate from correct task trajectories. To address this, we highlight the importance of timely calibration and the need to automatically construct calibration trajectories for training agents. We propose Step-Level Trajectory Calibration (STeCa), a novel framework for LLM agent learning. Specifically, STeCa identifies suboptimal actions through a step-level reward comparison during exploration. It constructs calibrated trajectories using LLM-driven reflection, enabling agents to learn from improved decision-making processes. These calibrated trajectories, together with successful trajectory data, are utilized for reinforced training. Extensive experiments demonstrate that STeCa significantly outperforms existing methods. Further analysis highlights that step-level calibration enables agents to complete tasks with greater robustness. Our code and data are available at https://github.com/WangHanLinHenry/STeCa.

### Predicting Fetal Birthweight from High Dimensional Data using Advanced Machine Learning 
[[arxiv](https://arxiv.org/abs/2502.14270)] [[cool](https://papers.cool/arxiv/2502.14270)] [[pdf](https://arxiv.org/pdf/2502.14270)]
> **Authors**: Nachiket Kapure,Harsh Joshi,Rajeshwari Mistri,Parul Kumari,Manasi Mali,Seema Purohit,Neha Sharma,Mrityunjoy Panday,Chittaranjan S. Yajnik
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Birth weight serves as a fundamental indicator of neonatal health, closely linked to both early medical interventions and long-term developmental risks. Traditional predictive models, often constrained by limited feature selection and incomplete datasets, struggle to achieve overlooking complex maternal and fetal interactions in diverse clinical settings. This research explores machine learning to address these limitations, utilizing a structured methodology that integrates advanced imputation strategies, supervised feature selection techniques, and predictive modeling. Given the constraints of the dataset, the research strengthens the role of data preprocessing in improving the model performance. Among the various methodologies explored, tree-based feature selection methods demonstrated superior capability in identifying the most relevant predictors, while ensemble-based regression models proved highly effective in capturing non-linear relationships and complex maternal-fetal interactions within the data. Beyond model performance, the study highlights the clinical significance of key physiological determinants, offering insights into maternal and fetal health factors that influence birth weight, offering insights that extend over statistical modeling. By bridging computational intelligence with perinatal research, this work underscores the transformative role of machine learning in enhancing predictive accuracy, refining risk assessment and informing data-driven decision-making in maternal and neonatal care. Keywords: Birth weight prediction, maternal-fetal health, MICE, BART, Gradient Boosting, neonatal outcomes, Clinipredictive.

## 多代理系统(cs.MA:Multiagent Systems)

### Multi-Agent Coordination across Diverse Applications: A Survey 
[[arxiv](https://arxiv.org/abs/2502.14743)] [[cool](https://papers.cool/arxiv/2502.14743)] [[pdf](https://arxiv.org/pdf/2502.14743)]
> **Authors**: Lijun Sun,Yijun Yang,Qiqi Duan,Yuhui Shi,Chao Lyu,Yu-Cheng Chang,Chin-Teng Lin,Yang Shen
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 23 pages, 4 figures, 2 tables
- **标题**: None
- **领域**: 多代理系统,人工智能
- **Abstract**: Multi-agent coordination studies the underlying mechanism enabling the trending spread of diverse multi-agent systems (MAS) and has received increasing attention, driven by the expansion of emerging applications and rapid AI advances. This survey outlines the current state of coordination research across applications through a unified understanding that answers four fundamental coordination questions: (1) what is coordination; (2) why coordination; (3) who to coordinate with; and (4) how to coordinate. Our purpose is to explore existing ideas and expertise in coordination and their connections across diverse applications, while identifying and highlighting emerging and promising research directions. First, general coordination problems that are essential to varied applications are identified and analyzed. Second, a number of MAS applications are surveyed, ranging from widely studied domains, e.g., search and rescue, warehouse automation and logistics, and transportation systems, to emerging fields including humanoid and anthropomorphic robots, satellite systems, and large language models (LLMs). Finally, open challenges about the scalability, heterogeneity, and learning mechanisms of MAS are analyzed and discussed. In particular, we identify the hybridization of hierarchical and decentralized coordination, human-MAS coordination, and LLM-based MAS as promising future directions.

### Ranking Joint Policies in Dynamic Games using Evolutionary Dynamics 
[[arxiv](https://arxiv.org/abs/2502.14724)] [[cool](https://papers.cool/arxiv/2502.14724)] [[pdf](https://arxiv.org/pdf/2502.14724)]
> **Authors**: Natalia Koliou,George Vouros
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 多代理系统,人工智能,机器学习
- **Abstract**: Game-theoretic solution concepts, such as the Nash equilibrium, have been key to finding stable joint actions in multi-player games. However, it has been shown that the dynamics of agents' interactions, even in simple two-player games with few strategies, are incapable of reaching Nash equilibria, exhibiting complex and unpredictable behavior. Instead, evolutionary approaches can describe the long-term persistence of strategies and filter out transient ones, accounting for the long-term dynamics of agents' interactions. Our goal is to identify agents' joint strategies that result in stable behavior, being resistant to changes, while also accounting for agents' payoffs, in dynamic games. Towards this goal, and building on previous results, this paper proposes transforming dynamic games into their empirical forms by considering agents' strategies instead of agents' actions, and applying the evolutionary methodology $α$-Rank to evaluate and rank strategy profiles according to their long-term dynamics. This methodology not only allows us to identify joint strategies that are strong through agents' long-term interactions, but also provides a descriptive, transparent framework regarding the high ranking of these strategies. Experiments report on agents that aim to collaboratively solve a stochastic version of the graph coloring problem. We consider different styles of play as strategies to define the empirical game, and train policies realizing these strategies, using the DQN algorithm. Then we run simulations to generate the payoff matrix required by $α$-Rank to rank joint strategies.

### Beyond Self-Talk: A Communication-Centric Survey of LLM-Based Multi-Agent Systems 
[[arxiv](https://arxiv.org/abs/2502.14321)] [[cool](https://papers.cool/arxiv/2502.14321)] [[pdf](https://arxiv.org/pdf/2502.14321)]
> **Authors**: Bingyu Yan,Xiaoming Zhang,Litian Zhang,Lian Zhang,Ziyi Zhou,Dezhuang Miao,Chaozhuo Li
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 多代理系统,计算语言学
- **Abstract**: Large Language Models (LLMs) have recently demonstrated remarkable capabilities in reasoning, planning, and decision-making. Building upon these strengths, researchers have begun incorporating LLMs into multi-agent systems (MAS), where agents collaborate or compete through natural language interactions to tackle tasks beyond the scope of single-agent setups. In this survey, we present a communication-centric perspective on LLM-based multi-agent systems, examining key system-level features such as architecture design and communication goals, as well as internal mechanisms like communication strategies, paradigms, objects and content. We illustrate how these communication elements interplay to enable collective intelligence and flexible collaboration. Furthermore, we discuss prominent challenges, including scalability, security, and multimodal integration, and propose directions for future work to advance research in this emerging domain. Ultimately, this survey serves as a catalyst for further innovation, fostering more robust, scalable, and intelligent multi-agent systems across diverse application domains.

## 网络和互联网架构(cs.NI:Networking and Internet Architecture)

### Reinforcement Learning with Graph Attention for Routing and Wavelength Assignment with Lightpath Reuse 
[[arxiv](https://arxiv.org/abs/2502.14741)] [[cool](https://papers.cool/arxiv/2502.14741)] [[pdf](https://arxiv.org/pdf/2502.14741)]
> **Authors**: Michael Doherty,Alejandra Beghelli
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 网络和互联网架构,机器学习,系统与控制
- **Abstract**: Many works have investigated reinforcement learning (RL) for routing and spectrum assignment on flex-grid networks but only one work to date has examined RL for fixed-grid with flex-rate transponders, despite production systems using this paradigm. Flex-rate transponders allow existing lightpaths to accommodate new services, a task we term routing and wavelength assignment with lightpath reuse (RWA-LR). We re-examine this problem and present a thorough benchmarking of heuristic algorithms for RWA-LR, which are shown to have 6% increased throughput when candidate paths are ordered by number of hops, rather than total length. We train an RL agent for RWA-LR with graph attention networks for the policy and value functions to exploit the graph-structured data. We provide details of our methodology and open source all of our code for reproduction. We outperform the previous state-of-the-art RL approach by 2.5% (17.4 Tbps mean additional throughput) and the best heuristic by 1.2% (8.5 Tbps mean additional throughput). This marginal gain highlights the difficulty in learning effective RL policies on long horizon resource allocation tasks.

## 机器人技术(cs.RO:Robotics)

### CurricuVLM: Towards Safe Autonomous Driving via Personalized Safety-Critical Curriculum Learning with Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2502.15119)] [[cool](https://papers.cool/arxiv/2502.15119)] [[pdf](https://arxiv.org/pdf/2502.15119)]
> **Authors**: Zihao Sheng,Zilin Huang,Yansong Qu,Yue Leng,Sruthi Bhavanam,Sikai Chen
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,人工智能,计算机视觉和模式识别
- **Abstract**: Ensuring safety in autonomous driving systems remains a critical challenge, particularly in handling rare but potentially catastrophic safety-critical scenarios. While existing research has explored generating safety-critical scenarios for autonomous vehicle (AV) testing, there is limited work on effectively incorporating these scenarios into policy learning to enhance safety. Furthermore, developing training curricula that adapt to an AV's evolving behavioral patterns and performance bottlenecks remains largely unexplored. To address these challenges, we propose CurricuVLM, a novel framework that leverages Vision-Language Models (VLMs) to enable personalized curriculum learning for autonomous driving agents. Our approach uniquely exploits VLMs' multimodal understanding capabilities to analyze agent behavior, identify performance weaknesses, and dynamically generate tailored training scenarios for curriculum adaptation. Through comprehensive analysis of unsafe driving situations with narrative descriptions, CurricuVLM performs in-depth reasoning to evaluate the AV's capabilities and identify critical behavioral patterns. The framework then synthesizes customized training scenarios targeting these identified limitations, enabling effective and personalized curriculum learning. Extensive experiments on the Waymo Open Motion Dataset show that CurricuVLM outperforms state-of-the-art baselines across both regular and safety-critical scenarios, achieving superior performance in terms of navigation success, driving efficiency, and safety metrics. Further analysis reveals that CurricuVLM serves as a general approach that can be integrated with various RL algorithms to enhance autonomous driving systems. The code and demo video are available at: https://zihaosheng.github.io/CurricuVLM/.

### Humanoid-VLA: Towards Universal Humanoid Control with Visual Integration 
[[arxiv](https://arxiv.org/abs/2502.14795)] [[cool](https://papers.cool/arxiv/2502.14795)] [[pdf](https://arxiv.org/pdf/2502.14795)]
> **Authors**: Pengxiang Ding,Jianfei Ma,Xinyang Tong,Binghong Zou,Xinxin Luo,Yiguo Fan,Ting Wang,Hongchao Lu,Panzhong Mo,Jinxin Liu,Yuefan Wang,Huaicheng Zhou,Wenshuo Feng,Jiacheng Liu,Siteng Huang,Donglin Wang
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,计算机视觉和模式识别
- **Abstract**: This paper addresses the limitations of current humanoid robot control frameworks, which primarily rely on reactive mechanisms and lack autonomous interaction capabilities due to data scarcity. We propose Humanoid-VLA, a novel framework that integrates language understanding, egocentric scene perception, and motion control, enabling universal humanoid control. Humanoid-VLA begins with language-motion pre-alignment using non-egocentric human motion datasets paired with textual descriptions, allowing the model to learn universal motion patterns and action semantics. We then incorporate egocentric visual context through a parameter efficient video-conditioned fine-tuning, enabling context-aware motion generation. Furthermore, we introduce a self-supervised data augmentation strategy that automatically generates pseudoannotations directly derived from motion data. This process converts raw motion sequences into informative question-answer pairs, facilitating the effective use of large-scale unlabeled video data. Built upon whole-body control architectures, extensive experiments show that Humanoid-VLA achieves object interaction and environment exploration tasks with enhanced contextual awareness, demonstrating a more human-like capacity for adaptive and intelligent engagement.

### Watch Less, Feel More: Sim-to-Real RL for Generalizable Articulated Object Manipulation via Motion Adaptation and Impedance Control 
[[arxiv](https://arxiv.org/abs/2502.14457)] [[cool](https://papers.cool/arxiv/2502.14457)] [[pdf](https://arxiv.org/pdf/2502.14457)]
> **Authors**: Tan-Dzung Do,Nandiraju Gireesh,Jilong Wang,He Wang
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,人工智能,机器学习
- **Abstract**: Articulated object manipulation poses a unique challenge compared to rigid object manipulation as the object itself represents a dynamic environment. In this work, we present a novel RL-based pipeline equipped with variable impedance control and motion adaptation leveraging observation history for generalizable articulated object manipulation, focusing on smooth and dexterous motion during zero-shot sim-to-real transfer. To mitigate the sim-to-real gap, our pipeline diminishes reliance on vision by not leveraging the vision data feature (RGBD/pointcloud) directly as policy input but rather extracting useful low-dimensional data first via off-the-shelf modules. Additionally, we experience less sim-to-real gap by inferring object motion and its intrinsic properties via observation history as well as utilizing impedance control both in the simulation and in the real world. Furthermore, we develop a well-designed training setting with great randomization and a specialized reward system (task-aware and motion-aware) that enables multi-staged, end-to-end manipulation without heuristic motion planning. To the best of our knowledge, our policy is the first to report 84\% success rate in the real world via extensive experiments with various unseen objects.

### An Efficient Ground-aerial Transportation System for Pest Control Enabled by AI-based Autonomous Nano-UAVs 
[[arxiv](https://arxiv.org/abs/2502.14455)] [[cool](https://papers.cool/arxiv/2502.14455)] [[pdf](https://arxiv.org/pdf/2502.14455)]
> **Authors**: Luca Crupi,Luca Butera,Alberto Ferrante,Alessandro Giusti,Daniele Palossi
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,人工智能
- **Abstract**: Efficient crop production requires early detection of pest outbreaks and timely treatments; we consider a solution based on a fleet of multiple autonomous miniaturized unmanned aerial vehicles (nano-UAVs) to visually detect pests and a single slower heavy vehicle that visits the detected outbreaks to deliver treatments. To cope with the extreme limitations aboard nano-UAVs, e.g., low-resolution sensors and sub-100 mW computational power budget, we design, fine-tune, and optimize a tiny image-based convolutional neural network (CNN) for pest detection. Despite the small size of our CNN (i.e., 0.58 GOps/inference), on our dataset, it scores a mean average precision (mAP) of 0.79 in detecting harmful bugs, i.e., 14% lower mAP but 32x fewer operations than the best-performing CNN in the literature. Our CNN runs in real-time at 6.8 frame/s, requiring 33 mW on a GWT GAP9 System-on-Chip aboard a Crazyflie nano-UAV. Then, to cope with in-field unexpected obstacles, we leverage a global+local path planner based on the A* algorithm. The global path planner determines the best route for the nano-UAV to sweep the entire area, while the local one runs up to 50 Hz aboard our nano-UAV and prevents collision by adjusting the short-distance path. Finally, we demonstrate with in-simulator experiments that once a 25 nano-UAVs fleet has combed a 200x200 m vineyard, collected information can be used to plan the best path for the tractor, visiting all and only required hotspots. In this scenario, our efficient transportation system, compared to a traditional single-ground vehicle performing both inspection and treatment, can save up to 20 h working time.

### ChatVLA: Unified Multimodal Understanding and Robot Control with Vision-Language-Action Model 
[[arxiv](https://arxiv.org/abs/2502.14420)] [[cool](https://papers.cool/arxiv/2502.14420)] [[pdf](https://arxiv.org/pdf/2502.14420)]
> **Authors**: Zhongyi Zhou,Yichen Zhu,Minjie Zhu,Junjie Wen,Ning Liu,Zhiyuan Xu,Weibin Meng,Ran Cheng,Yaxin Peng,Chaomin Shen,Feifei Feng
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,计算机视觉和模式识别,机器学习
- **Abstract**: Humans possess a unified cognitive ability to perceive, comprehend, and interact with the physical world. Why can't large language models replicate this holistic understanding? Through a systematic analysis of existing training paradigms in vision-language-action models (VLA), we identify two key challenges: spurious forgetting, where robot training overwrites crucial visual-text alignments, and task interference, where competing control and understanding tasks degrade performance when trained jointly. To overcome these limitations, we propose ChatVLA, a novel framework featuring Phased Alignment Training, which incrementally integrates multimodal data after initial control mastery, and a Mixture-of-Experts architecture to minimize task interference. ChatVLA demonstrates competitive performance on visual question-answering datasets and significantly surpasses state-of-the-art vision-language-action (VLA) methods on multimodal understanding benchmarks. Notably, it achieves a six times higher performance on MMMU and scores 47.2% on MMStar with a more parameter-efficient design than ECoT. Furthermore, ChatVLA demonstrates superior performance on 25 real-world robot manipulation tasks compared to existing VLA methods like OpenVLA. Our findings highlight the potential of our unified framework for achieving both robust multimodal understanding and effective robot control.

## 声音(cs.SD:Sound)

### Fundamental Survey on Neuromorphic Based Audio Classification 
[[arxiv](https://arxiv.org/abs/2502.15056)] [[cool](https://papers.cool/arxiv/2502.15056)] [[pdf](https://arxiv.org/pdf/2502.15056)]
> **Authors**: Amlan Basu,Pranav Chaudhari,Gaetano Di Caterina
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 24 Pages, 1 Table
- **标题**: None
- **领域**: 声音,人工智能,音频和语音处理
- **Abstract**: Audio classification is paramount in a variety of applications including surveillance, healthcare monitoring, and environmental analysis. Traditional methods frequently depend on intricate signal processing algorithms and manually crafted features, which may fall short in fully capturing the complexities of audio patterns. Neuromorphic computing, inspired by the architecture and functioning of the human brain, presents a promising alternative for audio classification tasks. This survey provides an exhaustive examination of the current state-of-the-art in neuromorphic-based audio classification. It delves into the crucial components of neuromorphic systems, such as Spiking Neural Networks (SNNs), memristors, and neuromorphic hardware platforms, highlighting their advantages in audio classification. Furthermore, the survey explores various methodologies and strategies employed in neuromorphic audio classification, including event-based processing, spike-based learning, and bio-inspired feature extraction. It examines how these approaches address the limitations of traditional audio classification methods, particularly in terms of energy efficiency, real-time processing, and robustness to environmental noise. Additionally, the paper conducts a comparative analysis of different neuromorphic audio classification models and benchmarks, evaluating their performance metrics, computational efficiency, and scalability. By providing a comprehensive guide for researchers, engineers and practitioners, this survey aims to stimulate further innovation and advancements in the evolving field of neuromorphic audio classification.

### WavRAG: Audio-Integrated Retrieval Augmented Generation for Spoken Dialogue Models 
[[arxiv](https://arxiv.org/abs/2502.14727)] [[cool](https://papers.cool/arxiv/2502.14727)] [[pdf](https://arxiv.org/pdf/2502.14727)]
> **Authors**: Yifu Chen,Shengpeng Ji,Haoxiao Wang,Ziqing Wang,Siyu Chen,Jinzheng He,Jin Xu,Zhou Zhao
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 声音,人工智能,音频和语音处理
- **Abstract**: Retrieval Augmented Generation (RAG) has gained widespread adoption owing to its capacity to empower large language models (LLMs) to integrate external knowledge. However, existing RAG frameworks are primarily designed for text-based LLMs and rely on Automatic Speech Recognition to process speech input, which discards crucial audio information, risks transcription errors, and increases computational overhead. Therefore, we introduce WavRAG, the first retrieval augmented generation framework with native, end-to-end audio support. WavRAG offers two key features: 1) Bypassing ASR, WavRAG directly processes raw audio for both embedding and retrieval. 2) WavRAG integrates audio and text into a unified knowledge representation. Specifically, we propose the WavRetriever to facilitate the retrieval from a text-audio hybrid knowledge base, and further enhance the in-context capabilities of spoken dialogue models through the integration of chain-of-thought reasoning. In comparison to state-of-the-art ASR-Text RAG pipelines, WavRAG achieves comparable retrieval performance while delivering a 10x acceleration. Furthermore, WavRAG's unique text-audio hybrid retrieval capability extends the boundaries of RAG to the audio modality.

## 社交和信息网络(cs.SI:Social and Information Networks)

### A Macro- and Micro-Hierarchical Transfer Learning Framework for Cross-Domain Fake News Detection 
[[arxiv](https://arxiv.org/abs/2502.14403)] [[cool](https://papers.cool/arxiv/2502.14403)] [[pdf](https://arxiv.org/pdf/2502.14403)]
> **Authors**: Xuankai Yang,Yan Wang,Xiuzhen Zhang,Shoujin Wang,Huaxiong Wang,Kwok Yan Lam
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 11 pages, 8 figures, to be published in The 2025 ACM Web Conference (WWW '25)
- **标题**: None
- **领域**: 社交和信息网络,计算语言学,机器学习
- **Abstract**: Cross-domain fake news detection aims to mitigate domain shift and improve detection performance by transferring knowledge across domains. Existing approaches transfer knowledge based on news content and user engagements from a source domain to a target domain. However, these approaches face two main limitations, hindering effective knowledge transfer and optimal fake news detection performance. Firstly, from a micro perspective, they neglect the negative impact of veracity-irrelevant features in news content when transferring domain-shared features across domains. Secondly, from a macro perspective, existing approaches ignore the relationship between user engagement and news content, which reveals shared behaviors of common users across domains and can facilitate more effective knowledge transfer. To address these limitations, we propose a novel macro- and micro- hierarchical transfer learning framework (MMHT) for cross-domain fake news detection. Firstly, we propose a micro-hierarchical disentangling module to disentangle veracity-relevant and veracity-irrelevant features from news content in the source domain for improving fake news detection performance in the target domain. Secondly, we propose a macro-hierarchical transfer learning module to generate engagement features based on common users' shared behaviors in different domains for improving effectiveness of knowledge transfer. Extensive experiments on real-world datasets demonstrate that our framework significantly outperforms the state-of-the-art baselines.

## 理论经济学(econ.TH:Theoretical Economics)

### Human Misperception of Generative-AI Alignment: A Laboratory Experiment 
[[arxiv](https://arxiv.org/abs/2502.14708)] [[cool](https://papers.cool/arxiv/2502.14708)] [[pdf](https://arxiv.org/pdf/2502.14708)]
> **Authors**: Kevin He,Ran Shorrer,Mengjia Xia
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 理论经济学,人工智能,计算机科学与博弈论
- **Abstract**: We conduct an incentivized laboratory experiment to study people's perception of generative artificial intelligence (GenAI) alignment in the context of economic decision-making. Using a panel of economic problems spanning the domains of risk, time preference, social preference, and strategic interactions, we ask human subjects to make choices for themselves and to predict the choices made by GenAI on behalf of a human user. We find that people overestimate the degree of alignment between GenAI's choices and human choices. In every problem, human subjects' average prediction about GenAI's choice is substantially closer to the average human-subject choice than it is to the GenAI choice. At the individual level, different subjects' predictions about GenAI's choice in a given problem are highly correlated with their own choices in the same problem. We explore the implications of people overestimating GenAI alignment in a simple theoretical model.

## 图像和视频处理(eess.IV:Image and Video Processing)

### Lung-DDPM: Semantic Layout-guided Diffusion Models for Thoracic CT Image Synthesis 
[[arxiv](https://arxiv.org/abs/2502.15204)] [[cool](https://papers.cool/arxiv/2502.15204)] [[pdf](https://arxiv.org/pdf/2502.15204)]
> **Authors**: Yifan Jiang,Yannick Lemaréchal,Josée Bafaro,Jessica Abi-Rjeile,Philippe Joubert,Philippe Després,Venkata Manem
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: The code and pretrained models are available at https://github.com/Manem-Lab/Lung-DDPM
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: With the rapid development of artificial intelligence (AI), AI-assisted medical imaging analysis demonstrates remarkable performance in early lung cancer screening. However, the costly annotation process and privacy concerns limit the construction of large-scale medical datasets, hampering the further application of AI in healthcare. To address the data scarcity in lung cancer screening, we propose Lung-DDPM, a thoracic CT image synthesis approach that effectively generates high-fidelity 3D synthetic CT images, which prove helpful in downstream lung nodule segmentation tasks. Our method is based on semantic layout-guided denoising diffusion probabilistic models (DDPM), enabling anatomically reasonable, seamless, and consistent sample generation even from incomplete semantic layouts. Our results suggest that the proposed method outperforms other state-of-the-art (SOTA) generative models in image quality evaluation and downstream lung nodule segmentation tasks. Specifically, Lung-DDPM achieved superior performance on our large validation cohort, with a Fréchet inception distance (FID) of 0.0047, maximum mean discrepancy (MMD) of 0.0070, and mean squared error (MSE) of 0.0024. These results were 7.4$\times$, 3.1$\times$, and 29.5$\times$ better than the second-best competitors, respectively. Furthermore, the lung nodule segmentation model, trained on a dataset combining real and Lung-DDPM-generated synthetic samples, attained a dice coefficient (Dice) of 0.3914 and sensitivity of 0.4393. This represents 8.8\% and 18.6\% improvements in DICE and sensitivity compared to the model trained solely on real samples. The experimental results highlight Lung-DDPM's potential for a broader range of medical imaging applications, such as general tumor segmentation, cancer survival estimation, and risk prediction.

### LUMINA-Net: Low-light Upgrade through Multi-stage Illumination and Noise Adaptation Network for Image Enhancement 
[[arxiv](https://arxiv.org/abs/2502.15186)] [[cool](https://papers.cool/arxiv/2502.15186)] [[pdf](https://arxiv.org/pdf/2502.15186)]
> **Authors**: Namrah Siddiqua,Kim Suneung
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 9 pages, 4 figures
- **标题**: None
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **Abstract**: Low-light image enhancement (LLIE) is a crucial task in computer vision aimed to enhance the visual fidelity of images captured under low-illumination conditions. Conventional methods frequently struggle to mitigate pervasive shortcomings such as noise, over-exposure, and color distortion thereby precipitating a pronounced degradation in image quality. To address these challenges, we propose LUMINA-Net an advanced deep learning framework designed specifically by integrating multi-stage illumination and reflectance modules. First, the illumination module intelligently adjusts brightness and contrast levels while meticulously preserving intricate textural details. Second, the reflectance module incorporates a noise reduction mechanism that leverages spatial attention and channel-wise feature refinement to mitigate noise contamination. Through a comprehensive suite of experiments conducted on LOL and SICE datasets using PSNR, SSIM and LPIPS metrics, surpassing state-of-the-art methodologies and showcasing its efficacy in low-light image enhancement.

### Denoising, segmentation and volumetric rendering of optical coherence tomography angiography (OCTA) image using deep learning techniques: a review 
[[arxiv](https://arxiv.org/abs/2502.14935)] [[cool](https://papers.cool/arxiv/2502.14935)] [[pdf](https://arxiv.org/pdf/2502.14935)]
> **Authors**: Kejie Chen,Xiaochun Yang,Jing Na,Wenbo Wang
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: Optical coherence tomography angiography (OCTA) is a non-invasive imaging technique widely used to study vascular structures and micro-circulation dynamics in the retina and choroid. OCTA has been widely used in clinics for diagnosing ocular disease and monitoring its progression, because OCTA is safer and faster than dye-based angiography while retaining the ability to characterize micro-scale structures. However, OCTA data contains many inherent noises from the devices and acquisition protocols and suffers from various types of artifacts, which impairs diagnostic accuracy and repeatability. Deep learning (DL) based imaging analysis models are able to automatically detect and remove artifacts and noises, and enhance the quality of image data. It is also a powerful tool for segmentation and identification of normal and pathological structures in the images. Thus, the value of OCTA imaging can be significantly enhanced by the DL-based approaches for interpreting and performing measurements and predictions on the OCTA data. In this study, we reviewed literature on the DL models for OCTA images in the latest five years. In particular, we focused on discussing the current problems in the OCTA data and the corresponding design principles of the DL models. We also reviewed the state-of-art DL models for 3D volumetric reconstruction of the vascular networks and pathological structures such as the edema and distorted optic disc. In addition, the publicly available dataset of OCTA images are summarized at the end of this review. Overall, this review can provide valuable insights for engineers to develop novel DL models by utilizing the characteristics of OCTA signals and images. The pros and cons of each DL methods and their applications discussed in this review can be helpful to assist technicians and clinicians to use proper DL models for fundamental research and disease screening.

### Distributed U-net model and Image Segmentation for Lung Cancer Detection 
[[arxiv](https://arxiv.org/abs/2502.14928)] [[cool](https://papers.cool/arxiv/2502.14928)] [[pdf](https://arxiv.org/pdf/2502.14928)]
> **Authors**: Tianzuo Hu
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别,应用领域,方法论
- **Abstract**: Until now, in the wake of the COVID-19 pandemic in 2019, lung diseases, especially diseases such as lung cancer and chronic obstructive pulmonary disease (COPD), have become an urgent global health issue. In order to mitigate the goal problem, early detection and accurate diagnosis of these conditions are critical for effective treatment and improved patient outcomes. To further research and reduce the error rate of hospital diagnoses, this comprehensive study explored the potential of computer-aided design (CAD) systems, especially utilizing advanced deep learning models such as U-Net. And compared with the literature content of other authors, this study explores the capabilities of U-Net in detail, and enhances the ability to simulate CAD systems through the VGG16 algorithm. An extensive dataset consisting of lung CT images and corresponding segmentation masks, curated collaboratively by multiple academic institutions, serves as the basis for empirical validation. In this paper, the efficiency of U-Net model is evaluated rigorously and precisely under multiple hardware configurations, such as single CPU, single GPU, distributed GPU and federated learning, and the effectiveness and development of the method in the segmentation task of lung disease are demonstrated. Empirical results clearly affirm the robust performance of the U-Net model, most effectively utilizing four GPUs for distributed learning, and these results highlight the potential of U-Net-based CAD systems for accurate and timely lung disease detection and diagnosis huge potential.

### Display Field-Of-View Agnostic Robust CT Kernel Synthesis Using Model-Based Deep Learning 
[[arxiv](https://arxiv.org/abs/2502.14920)] [[cool](https://papers.cool/arxiv/2502.14920)] [[pdf](https://arxiv.org/pdf/2502.14920)]
> **Authors**: Hemant Kumar Aggarwal,Antony Jerald,Phaneendra K. Yalavarthy,Rajesh Langoju,Bipul Das
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-21
> **comment**: Accepted at IEEE ISBI 2025
- **标题**: None
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **Abstract**: In X-ray computed tomography (CT) imaging, the choice of reconstruction kernel is crucial as it significantly impacts the quality of clinical images. Different kernels influence spatial resolution, image noise, and contrast in various ways. Clinical applications involving lung imaging often require images reconstructed with both soft and sharp kernels. The reconstruction of images with different kernels requires raw sinogram data and storing images for all kernels increases processing time and storage requirements. The Display Field-of-View (DFOV) adds complexity to kernel synthesis, as data acquired at different DFOVs exhibit varying levels of sharpness and details. This work introduces an efficient, DFOV-agnostic solution for image-based kernel synthesis using model-based deep learning. The proposed method explicitly integrates CT kernel and DFOV characteristics into the forward model. Experimental results on clinical data, along with quantitative analysis of the estimated modulation transfer function using wire phantom data, clearly demonstrate the utility of the proposed method in real-time. Additionally, a comparative study with a direct learning network, that lacks forward model information, shows that the proposed method is more robust to DFOV variations.

### Pulmonary Tuberculosis Edge Diagnosis System Based on MindSpore Framework: Low-cost and High-precision Implementation with Ascend 310 Chip 
[[arxiv](https://arxiv.org/abs/2502.14885)] [[cool](https://papers.cool/arxiv/2502.14885)] [[pdf](https://arxiv.org/pdf/2502.14885)]
> **Authors**: HaoYu Li
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: Pulmonary Tuberculosis (PTB) remains a major challenge for global health, especially in areas with poor medical resources, where access to specialized medical knowledge and diagnostic tools is limited. This paper presents an auxiliary diagnosis system for pulmonary tuberculosis based on Huawei MindSpore framework and Ascend310 edge computing chip. Using MobileNetV3 architecture and Softmax cross entropy loss function with momentum optimizer. The system operates with FP16 hybrid accuracy on the Orange pie AIPro (Atlas 200 DK) edge device and performs well. In the test set containing 4148 chest images, the model accuracy reached 99.1\% (AUC = 0.99), and the equipment cost was controlled within \$150, providing affordable AI-assisted diagnosis scheme for primary care.

### FetalCLIP: A Visual-Language Foundation Model for Fetal Ultrasound Image Analysis 
[[arxiv](https://arxiv.org/abs/2502.14807)] [[cool](https://papers.cool/arxiv/2502.14807)] [[pdf](https://arxiv.org/pdf/2502.14807)]
> **Authors**: Fadillah Maani,Numan Saeed,Tausifa Saleem,Zaid Farooq,Hussain Alasmawi,Werner Diehl,Ameera Mohammad,Gareth Waring,Saudabi Valappi,Leanne Bricker,Mohammad Yaqub
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **Abstract**: Foundation models are becoming increasingly effective in the medical domain, offering pre-trained models on large datasets that can be readily adapted for downstream tasks. Despite progress, fetal ultrasound images remain a challenging domain for foundation models due to their inherent complexity, often requiring substantial additional training and facing limitations due to the scarcity of paired multimodal data. To overcome these challenges, here we introduce FetalCLIP, a vision-language foundation model capable of generating universal representation of fetal ultrasound images. FetalCLIP was pre-trained using a multimodal learning approach on a diverse dataset of 210,035 fetal ultrasound images paired with text. This represents the largest paired dataset of its kind used for foundation model development to date. This unique training approach allows FetalCLIP to effectively learn the intricate anatomical features present in fetal ultrasound images, resulting in robust representations that can be used for a variety of downstream applications. In extensive benchmarking across a range of key fetal ultrasound applications, including classification, gestational age estimation, congenital heart defect (CHD) detection, and fetal structure segmentation, FetalCLIP outperformed all baselines while demonstrating remarkable generalizability and strong performance even with limited labeled data. We plan to release the FetalCLIP model publicly for the benefit of the broader scientific community.

### MedVAE: Efficient Automated Interpretation of Medical Images with Large-Scale Generalizable Autoencoders 
[[arxiv](https://arxiv.org/abs/2502.14753)] [[cool](https://papers.cool/arxiv/2502.14753)] [[pdf](https://arxiv.org/pdf/2502.14753)]
> **Authors**: Maya Varma,Ashwin Kumar,Rogier van der Sluijs,Sophie Ostmeier,Louis Blankemeier,Pierre Chambon,Christian Bluethgen,Jip Prince,Curtis Langlotz,Akshay Chaudhari
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **Abstract**: Medical images are acquired at high resolutions with large fields of view in order to capture fine-grained features necessary for clinical decision-making. Consequently, training deep learning models on medical images can incur large computational costs. In this work, we address the challenge of downsizing medical images in order to improve downstream computational efficiency while preserving clinically-relevant features. We introduce MedVAE, a family of six large-scale 2D and 3D autoencoders capable of encoding medical images as downsized latent representations and decoding latent representations back to high-resolution images. We train MedVAE autoencoders using a novel two-stage training approach with 1,052,730 medical images. Across diverse tasks obtained from 20 medical image datasets, we demonstrate that (1) utilizing MedVAE latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits (up to 70x improvement in throughput) while simultaneously preserving clinically-relevant features and (2) MedVAE can decode latent representations back to high-resolution images with high fidelity. Our work demonstrates that large-scale, generalizable autoencoders can help address critical efficiency challenges in the medical domain. Our code is available at https://github.com/StanfordMIMI/MedVAE.

### TRUSWorthy: Toward Clinically Applicable Deep Learning for Confident Detection of Prostate Cancer in Micro-Ultrasound 
[[arxiv](https://arxiv.org/abs/2502.14707)] [[cool](https://papers.cool/arxiv/2502.14707)] [[pdf](https://arxiv.org/pdf/2502.14707)]
> **Authors**: Mohamed Harmanani,Paul F. R. Wilson,Minh Nguyen Nhat To,Mahdi Gilany,Amoon Jamzad,Fahimeh Fooladgar,Brian Wodlinger,Purang Abolmaesumi,Parvin Mousavi
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: accepted to IJCARS. This preprint has not undergone post-submission improvements or corrections. To access the Version of Record of this article, see the journal reference below
- **标题**: None
- **领域**: 图像和视频处理,机器学习,组织和器官
- **Abstract**: While deep learning methods have shown great promise in improving the effectiveness of prostate cancer (PCa) diagnosis by detecting suspicious lesions from trans-rectal ultrasound (TRUS), they must overcome multiple simultaneous challenges. There is high heterogeneity in tissue appearance, significant class imbalance in favor of benign examples, and scarcity in the number and quality of ground truth annotations available to train models. Failure to address even a single one of these problems can result in unacceptable clinical outcomes.We propose TRUSWorthy, a carefully designed, tuned, and integrated system for reliable PCa detection. Our pipeline integrates self-supervised learning, multiple-instance learning aggregation using transformers, random-undersampled boosting and ensembling: these address label scarcity, weak labels, class imbalance, and overconfidence, respectively. We train and rigorously evaluate our method using a large, multi-center dataset of micro-ultrasound data. Our method outperforms previous state-of-the-art deep learning methods in terms of accuracy and uncertainty calibration, with AUROC and balanced accuracy scores of 79.9% and 71.5%, respectively. On the top 20% of predictions with the highest confidence, we can achieve a balanced accuracy of up to 91%. The success of TRUSWorthy demonstrates the potential of integrated deep learning solutions to meet clinical needs in a highly challenging deployment setting, and is a significant step towards creating a trustworthy system for computer-assisted PCa diagnosis.

### Vision Foundation Models in Medical Image Analysis: Advances and Challenges 
[[arxiv](https://arxiv.org/abs/2502.14584)] [[cool](https://papers.cool/arxiv/2502.14584)] [[pdf](https://arxiv.org/pdf/2502.14584)]
> **Authors**: Pengchen Liang,Bin Pu,Haishan Huang,Yiwei Li,Hualiang Wang,Weibo Ma,Qing Chang
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 17 pages, 1 figure
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: The rapid development of Vision Foundation Models (VFMs), particularly Vision Transformers (ViT) and Segment Anything Model (SAM), has sparked significant advances in the field of medical image analysis. These models have demonstrated exceptional capabilities in capturing long-range dependencies and achieving high generalization in segmentation tasks. However, adapting these large models to medical image analysis presents several challenges, including domain differences between medical and natural images, the need for efficient model adaptation strategies, and the limitations of small-scale medical datasets. This paper reviews the state-of-the-art research on the adaptation of VFMs to medical image segmentation, focusing on the challenges of domain adaptation, model compression, and federated learning. We discuss the latest developments in adapter-based improvements, knowledge distillation techniques, and multi-scale contextual feature modeling, and propose future directions to overcome these bottlenecks. Our analysis highlights the potential of VFMs, along with emerging methodologies such as federated learning and model compression, to revolutionize medical image analysis and enhance clinical applications. The goal of this work is to provide a comprehensive overview of current approaches and suggest key areas for future research that can drive the next wave of innovation in medical image segmentation.

### Reliable Explainability of Deep Learning Spatial-Spectral Classifiers for Improved Semantic Segmentation in Autonomous Driving 
[[arxiv](https://arxiv.org/abs/2502.14416)] [[cool](https://papers.cool/arxiv/2502.14416)] [[pdf](https://arxiv.org/pdf/2502.14416)]
> **Authors**: Jon Gutiérrez-Zaballa,Koldo Basterretxea,Javier Echanobe
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,人工智能,机器学习
- **Abstract**: Integrating hyperspectral imagery (HSI) with deep neural networks (DNNs) can strengthen the accuracy of intelligent vision systems by combining spectral and spatial information, which is useful for tasks like semantic segmentation in autonomous driving. To advance research in such safety-critical systems, determining the precise contribution of spectral information to complex DNNs' output is needed. To address this, several saliency methods, such as class activation maps (CAM), have been proposed primarily for image classification. However, recent studies have raised concerns regarding their reliability. In this paper, we address their limitations and propose an alternative approach by leveraging the data provided by activations and weights from relevant DNN layers to better capture the relationship between input features and predictions. The study aims to assess the superior performance of HSI compared to 3-channel and single-channel DNNs. We also address the influence of spectral signature normalization for enhancing DNN robustness in real-world driving conditions.

### MedFuncta: Modality-Agnostic Representations Based on Efficient Neural Fields 
[[arxiv](https://arxiv.org/abs/2502.14401)] [[cool](https://papers.cool/arxiv/2502.14401)] [[pdf](https://arxiv.org/pdf/2502.14401)]
> **Authors**: Paul Friedrich,Florentin Bieder,Phlippe C. Cattin
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: Code and Dataset: https://github.com/pfriedri/medfuncta
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: Recent research in medical image analysis with deep learning almost exclusively focuses on grid- or voxel-based data representations. We challenge this common choice by introducing MedFuncta, a modality-agnostic continuous data representation based on neural fields. We demonstrate how to scale neural fields from single instances to large datasets by exploiting redundancy in medical signals and by applying an efficient meta-learning approach with a context reduction scheme. We further address the spectral bias in commonly used SIREN activations, by introducing an $ω_0$-schedule, improving reconstruction quality and convergence speed. We validate our proposed approach on a large variety of medical signals of different dimensions and modalities (1D: ECG; 2D: Chest X-ray, Retinal OCT, Fundus Camera, Dermatoscope, Colon Histopathology, Cell Microscopy; 3D: Brain MRI, Lung CT) and successfully demonstrate that we can solve relevant downstream tasks on these representations. We additionally release a large-scale dataset of > 550k annotated neural fields to promote research in this direction.

## 历史与概述(math.HO:History and Overview)

### Is Mathematics Obsolete? 
[[arxiv](https://arxiv.org/abs/2502.14874)] [[cool](https://papers.cool/arxiv/2502.14874)] [[pdf](https://arxiv.org/pdf/2502.14874)]
> **Authors**: Jeremy Avigad
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 历史与概述,人工智能
- **Abstract**: This is an essay about the value of mathematical and symbolic reasoning in the age of AI.

## 数值分析(math.NA:Numerical Analysis)

### Curvature Corrected Nonnegative Manifold Data Factorization 
[[arxiv](https://arxiv.org/abs/2502.15124)] [[cool](https://papers.cool/arxiv/2502.15124)] [[pdf](https://arxiv.org/pdf/2502.15124)]
> **Authors**: Joyce Chew,Willem Diepeveen,Deanna Needell
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: :53Z50
- **标题**: None
- **领域**: 数值分析,机器学习,微分几何
- **Abstract**: Data with underlying nonlinear structure are collected across numerous application domains, necessitating new data processing and analysis methods adapted to nonlinear domain structure. Riemannanian manifolds present a rich environment in which to develop such tools, as manifold-valued data arise in a variety of scientific settings, and Riemannian geometry provides a solid theoretical grounding for geometric data analysis. Low-rank approximations, such as nonnegative matrix factorization (NMF), are the foundation of many Euclidean data analysis methods, so adaptations of these factorizations for manifold-valued data are important building blocks for further development of manifold data analysis. In this work, we propose curvature corrected nonnegative manifold data factorization (CC-NMDF) as a geometry-aware method for extracting interpretable factors from manifold-valued data, analogous to nonnegative matrix factorization. We develop an efficient iterative algorithm for computing CC-NMDF and demonstrate our method on real-world diffusion tensor magnetic resonance imaging data.

### Meshless Shape Optimization using Neural Networks and Partial Differential Equations on Graphs 
[[arxiv](https://arxiv.org/abs/2502.14821)] [[cool](https://papers.cool/arxiv/2502.14821)] [[pdf](https://arxiv.org/pdf/2502.14821)]
> **Authors**: Eloi Martinet,Leon Bungert
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 13 pages, 5 figures, accepted at SSVM 2025
- **标题**: None
- **领域**: 数值分析,机器学习,优化与控制
- **Abstract**: Shape optimization involves the minimization of a cost function defined over a set of shapes, often governed by a partial differential equation (PDE). In the absence of closed-form solutions, one relies on numerical methods to approximate the solution. The level set method -- when coupled with the finite element method -- is one of the most versatile numerical shape optimization approaches but still suffers from the limitations of most mesh-based methods. In this work, we present a fully meshless level set framework that leverages neural networks to parameterize the level set function and employs the graph Laplacian to approximate the underlying PDE. Our approach enables precise computations of geometric quantities such as surface normals and curvature, and allows tackling optimization problems within the class of convex shapes.

## 统计理论(math.ST:Statistics Theory)

### Optimal and Provable Calibration in High-Dimensional Binary Classification: Angular Calibration and Platt Scaling 
[[arxiv](https://arxiv.org/abs/2502.15131)] [[cool](https://papers.cool/arxiv/2502.15131)] [[pdf](https://arxiv.org/pdf/2502.15131)]
> **Authors**: Yufan Li,Pragya Sur
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 统计理论,机器学习,方法论,机器学习
- **Abstract**: We study the fundamental problem of calibrating a linear binary classifier of the form $σ(\hat{w}^\top x)$, where the feature vector $x$ is Gaussian, $σ$ is a link function, and $\hat{w}$ is an estimator of the true linear weight $w^\star$. By interpolating with a noninformative $\textit{chance classifier}$, we construct a well-calibrated predictor whose interpolation weight depends on the angle $\angle(\hat{w}, w_\star)$ between the estimator $\hat{w}$ and the true linear weight $w_\star$. We establish that this angular calibration approach is provably well-calibrated in a high-dimensional regime where the number of samples and features both diverge, at a comparable rate. The angle $\angle(\hat{w}, w_\star)$ can be consistently estimated. Furthermore, the resulting predictor is uniquely $\textit{Bregman-optimal}$, minimizing the Bregman divergence to the true label distribution within a suitable class of calibrated predictors. Our work is the first to provide a calibration strategy that satisfies both calibration and optimality properties provably in high dimensions. Additionally, we identify conditions under which a classical Platt-scaling predictor converges to our Bregman-optimal calibrated solution. Thus, Platt-scaling also inherits these desirable properties provably in high dimensions.

### Do we really need the Rademacher complexities? 
[[arxiv](https://arxiv.org/abs/2502.15118)] [[cool](https://papers.cool/arxiv/2502.15118)] [[pdf](https://arxiv.org/pdf/2502.15118)]
> **Authors**: Daniel Bartl,Shahar Mendelson
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 统计理论,机器学习,机器学习
- **Abstract**: We study the fundamental problem of learning with respect to the squared loss in a convex class. The state-of-the-art sample complexity estimates in this setting rely on Rademacher complexities, which are generally difficult to control. We prove that, contrary to prevailing belief and under minimal assumptions, the sample complexity is not governed by the Rademacher complexities but rather by the behaviour of the limiting gaussian process. In particular, all such learning problems that have the same $L_2$-structure -- even those with heavy-tailed distributions -- share the same sample complexity. This constitutes the first universality result for general convex learning problems. The proof is based on a novel learning procedure, and its performance is studied by combining optimal mean estimation techniques for real-valued random variables with Talagrand's generic chaining method.

## 空间物理学(physics.space-ph:Space Physics)

### Forecasting Local Ionospheric Parameters Using Transformers 
[[arxiv](https://arxiv.org/abs/2502.15093)] [[cool](https://papers.cool/arxiv/2502.15093)] [[pdf](https://arxiv.org/pdf/2502.15093)]
> **Authors**: Daniel J. Alford-Lago,Christopher W. Curtis,Alexander T. Ihler,Katherine A. Zawdie,Douglas P. Drob
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 47 pages, 42 figures
- **标题**: None
- **领域**: 空间物理学,机器学习
- **Abstract**: We present a novel method for forecasting key ionospheric parameters using transformer-based neural networks. The model provides accurate forecasts and uncertainty quantification of the F2-layer peak plasma frequency (foF2), the F2-layer peak density height (hmF2), and total electron content (TEC) for a given geographic location. It supports a number of exogenous variables, including F10.7cm solar flux and disturbance storm time (Dst). We demonstrate how transformers can be trained in a data assimilation-like fashion that use these exogenous variables along with naïve predictions from climatology to generate 24-hour forecasts with non-parametric uncertainty bounds. We call this method the Local Ionospheric Forecast Transformer (LIFT). We demonstrate that the trained model can generalize to new geographic locations and time periods not seen during training, and we compare its performance to that of the International Reference Ionosphere (IRI).

## 神经元和认知(q-bio.NC:Neurons and Cognition)

### Applications of Random Matrix Theory in Machine Learning and Brain Mapping 
[[arxiv](https://arxiv.org/abs/2502.14878)] [[cool](https://papers.cool/arxiv/2502.14878)] [[pdf](https://arxiv.org/pdf/2502.14878)]
> **Authors**: Katrina Lawrence
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 神经元和认知,机器学习,可能性
- **Abstract**: Brain mapping analyzes the wavelengths of brain signals and outputs them in a map, which is then analyzed by a radiologist. Introducing Machine Learning (ML) into the brain mapping process reduces the variable of human error in reading such maps and increases efficiency. A key area of interest is determining the correlation between the functional areas of the brain on a voxel (3-dimensional pixel) wise basis. This leads to determining how a brain is functioning and can be used to detect diseases, disabilities, and sicknesses. As such, random noise presents a challenge in consistently determining the actual signals from the scan. This paper discusses how an algorithm created by Random Matrix Theory (RMT) can be used as a tool for ML, as it detects the correlation of the functional areas of the brain. Random matrices are simulated to represent the voxel signal intensity strength for each time interval where a stimulus is presented in an fMRI scan. Using the Marchenko-Pastur law for Wishart Matrices, a result of RMT, it was found that no matter what type of noise was added to the random matrices, the observed eigenvalue distribution of the Wishart Matrices would converge to the theoretical distribution. This means that RMT is robust and has a high test-re-test reliability. These results further indicate that a strong correlation exists between the eigenvalues, and hence the functional regions of the brain. Any eigenvalue that differs significantly from those predicted from RMT may indicate the discovery of a new discrete brain network.

### Beyond Performance Scores: Directed Functional Connectivity as a Brain-Based Biomarker for Motor Skill Learning and Retention 
[[arxiv](https://arxiv.org/abs/2502.14731)] [[cool](https://papers.cool/arxiv/2502.14731)] [[pdf](https://arxiv.org/pdf/2502.14731)]
> **Authors**: Anil Kamat,Rahul Rahul,Lora Cavuoto,Harry Burke,Matthew Hackett,Jack Norfleet,Steven Schwaitzberg,Suvranu De
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 神经元和认知,机器学习
- **Abstract**: Motor skill acquisition in fields like surgery, robotics, and sports involves learning complex task sequences through extensive training. Traditional performance metrics, like execution time and error rates, offer limited insight as they fail to capture the neural mechanisms underlying skill learning and retention. This study introduces directed functional connectivity (dFC), derived from electroencephalography (EEG), as a novel brain-based biomarker for assessing motor skill learning and retention. For the first time, dFC is applied as a biomarker to map the stages of the Fitts and Posner motor learning model, offering new insights into the neural mechanisms underlying skill acquisition and retention. Unlike traditional measures, it captures both the strength and direction of neural information flow, providing a comprehensive understanding of neural adaptations across different learning stages. The analysis demonstrates that dFC can effectively identify and track the progression through various stages of the Fitts and Posner model. Furthermore, its stability over a six-week washout period highlights its utility in monitoring long-term retention. No significant changes in dFC were observed in a control group, confirming that the observed neural adaptations were specific to training and not due to external factors. By offering a granular view of the learning process at the group and individual levels, dFC facilitates the development of personalized, targeted training protocols aimed at enhancing outcomes in fields where precision and long-term retention are critical, such as surgical education. These findings underscore the value of dFC as a robust biomarker that complements traditional performance metrics, providing a deeper understanding of motor skill learning and retention.

## 定量方法(q-bio.QM:Quantitative Methods)

### Reward-Guided Iterative Refinement in Diffusion Models at Test-Time with Applications to Protein and DNA Design 
[[arxiv](https://arxiv.org/abs/2502.14944)] [[cool](https://papers.cool/arxiv/2502.14944)] [[pdf](https://arxiv.org/pdf/2502.14944)]
> **Authors**: Masatoshi Uehara,Xingyu Su,Yulai Zhao,Xiner Li,Aviv Regev,Shuiwang Ji,Sergey Levine,Tommaso Biancalani
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: Under review. If you have any suggestions/missing references, please let us know
- **标题**: None
- **领域**: 定量方法,人工智能,机器学习,机器学习
- **Abstract**: To fully leverage the capabilities of diffusion models, we are often interested in optimizing downstream reward functions during inference. While numerous algorithms for reward-guided generation have been recently proposed due to their significance, current approaches predominantly focus on single-shot generation, transitioning from fully noised to denoised states. We propose a novel framework for inference-time reward optimization with diffusion models inspired by evolutionary algorithms. Our approach employs an iterative refinement process consisting of two steps in each iteration: noising and reward-guided denoising. This sequential refinement allows for the gradual correction of errors introduced during reward optimization. Besides, we provide a theoretical guarantee for our framework. Finally, we demonstrate its superior empirical performance in protein and cell-type-specific regulatory DNA design. The code is available at \href{https://github.com/masa-ue/ProDifEvo-Refinement}{https://github.com/masa-ue/ProDifEvo-Refinement}.

### Fast and Accurate Blind Flexible Docking 
[[arxiv](https://arxiv.org/abs/2502.14934)] [[cool](https://papers.cool/arxiv/2502.14934)] [[pdf](https://arxiv.org/pdf/2502.14934)]
> **Authors**: Zizhuo Zhang,Lijun Wu,Kaiyuan Gao,Jiangchao Yao,Tao Qin,Bo Han
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 25 pages, Accepted by ICLR 2025
- **标题**: None
- **领域**: 定量方法,人工智能,机器学习
- **Abstract**: Molecular docking that predicts the bound structures of small molecules (ligands) to their protein targets, plays a vital role in drug discovery. However, existing docking methods often face limitations: they either overlook crucial structural changes by assuming protein rigidity or suffer from low computational efficiency due to their reliance on generative models for structure sampling. To address these challenges, we propose FABFlex, a fast and accurate regression-based multi-task learning model designed for realistic blind flexible docking scenarios, where proteins exhibit flexibility and binding pocket sites are unknown (blind). Specifically, FABFlex's architecture comprises three specialized modules working in concert: (1) A pocket prediction module that identifies potential binding sites, addressing the challenges inherent in blind docking scenarios. (2) A ligand docking module that predicts the bound (holo) structures of ligands from their unbound (apo) states. (3) A pocket docking module that forecasts the holo structures of protein pockets from their apo conformations. Notably, FABFlex incorporates an iterative update mechanism that serves as a conduit between the ligand and pocket docking modules, enabling continuous structural refinements. This approach effectively integrates the three subtasks of blind flexible docking-pocket identification, ligand conformation prediction, and protein flexibility modeling-into a unified, coherent framework. Extensive experiments on public benchmark datasets demonstrate that FABFlex not only achieves superior effectiveness in predicting accurate binding modes but also exhibits a significant speed advantage (208 $\times$) compared to existing state-of-the-art methods. Our code is released at https://github.com/tmlr-group/FABFlex.

### Optimizing Gene-Based Testing for Antibiotic Resistance Prediction 
[[arxiv](https://arxiv.org/abs/2502.14919)] [[cool](https://papers.cool/arxiv/2502.14919)] [[pdf](https://arxiv.org/pdf/2502.14919)]
> **Authors**: David Hagerman,Anna Johnning,Roman Naeem,Fredrik Kahl,Erik Kristiansson,Lennart Svensson
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-21
> **comment**: Accepted to AAAI-25 AISI
- **标题**: None
- **领域**: 定量方法,机器学习
- **Abstract**: Antibiotic Resistance (AR) is a critical global health challenge that necessitates the development of cost-effective, efficient, and accurate diagnostic tools. Given the genetic basis of AR, techniques such as Polymerase Chain Reaction (PCR) that target specific resistance genes offer a promising approach for predictive diagnostics using a limited set of key genes. This study introduces GenoARM, a novel framework that integrates reinforcement learning (RL) with transformer-based models to optimize the selection of PCR gene tests and improve AR predictions, leveraging observed metadata for improved accuracy. In our evaluation, we developed several high-performing baselines and compared them using publicly available datasets derived from real-world bacterial samples representing multiple clinically relevant pathogens. The results show that all evaluated methods achieve strong and reliable performance when metadata is not utilized. When metadata is introduced and the number of selected genes increases, GenoARM demonstrates superior performance due to its capacity to approximate rewards for unseen and sparse combinations. Overall, our framework represents a major advancement in optimizing diagnostic tools for AR in clinical settings.

## 量子物理学(quant-ph:Quantum Physics)

### Symmetric observations without symmetric causal explanations 
[[arxiv](https://arxiv.org/abs/2502.14950)] [[cool](https://papers.cool/arxiv/2502.14950)] [[pdf](https://arxiv.org/pdf/2502.14950)]
> **Authors**: Christian William,Patrick Remy,Jean-Daniel Bancal,Yu Cai,Nicolas Brunner,Alejandro Pozas-Kerstjens
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 8 pages, 4 figures, RevTeX 4.2. The computational appendix is available at https://www.github.com/apozas/symmetric-causal
- **标题**: None
- **领域**: 量子物理学,机器学习,统计理论,机器学习
- **Abstract**: Inferring causal models from observed correlations is a challenging task, crucial to many areas of science. In order to alleviate the effort, it is important to know whether symmetries in the observations correspond to symmetries in the underlying realization. Via an explicit example, we answer this question in the negative. We use a tripartite probability distribution over binary events that is realized by using three (different) independent sources of classical randomness. We prove that even removing the condition that the sources distribute systems described by classical physics, the requirements that i) the sources distribute the same physical systems, ii) these physical systems respect relativistic causality, and iii) the correlations are the observed ones, are incompatible.

### Discovering highly efficient low-weight quantum error-correcting codes with reinforcement learning 
[[arxiv](https://arxiv.org/abs/2502.14372)] [[cool](https://papers.cool/arxiv/2502.14372)] [[pdf](https://arxiv.org/pdf/2502.14372)]
> **Authors**: Austin Yubo He,Zi-Wen Liu
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 18 pages, 14 figures, 4 tables
- **标题**: None
- **领域**: 量子物理学,人工智能,信息论,机器学习
- **Abstract**: The realization of scalable fault-tolerant quantum computing is expected to hinge on quantum error-correcting codes. In the quest for more efficient quantum fault tolerance, a critical code parameter is the weight of measurements that extract information about errors to enable error correction: as higher measurement weights require higher implementation costs and introduce more errors, it is important in code design to optimize measurement weight. This underlies the surging interest in quantum low-density parity-check (qLDPC) codes, the study of which has primarily focused on the asymptotic (large-code-limit) properties. In this work, we introduce a versatile and computationally efficient approach to stabilizer code weight reduction based on reinforcement learning (RL), which produces new low-weight codes that substantially outperform the state of the art in practically relevant parameter regimes, extending significantly beyond previously accessible small distances. For example, our approach demonstrates savings in physical qubit overhead compared to existing results by 1 to 2 orders of magnitude for weight 6 codes and brings the overhead into a feasible range for near-future experiments. We also investigate the interplay between code parameters using our RL framework, offering new insights into the potential efficiency and power of practically viable coding strategies. Overall, our results demonstrate how RL can effectively advance the crucial yet challenging problem of quantum code discovery and thereby facilitate a faster path to the practical implementation of fault-tolerant quantum technologies.

## 计算(stat.CO:Computation)

### Provable Quantum Algorithm Advantage for Gaussian Process Quadrature 
[[arxiv](https://arxiv.org/abs/2502.14467)] [[cool](https://papers.cool/arxiv/2502.14467)] [[pdf](https://arxiv.org/pdf/2502.14467)]
> **Authors**: Cristian A. Galvis-Florez,Ahmad Farooq,Simo Särkkä
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 21 pages, 6 figures
- **标题**: None
- **领域**: 计算,机器学习,量子物理学
- **Abstract**: The aim of this paper is to develop novel quantum algorithms for Gaussian process quadrature methods. Gaussian process quadratures are numerical integration methods where Gaussian processes are used as functional priors for the integrands to capture the uncertainty arising from the sparse function evaluations. Quantum computers have emerged as potential replacements for classical computers, offering exponential reductions in the computational complexity of machine learning tasks. In this paper, we combine Gaussian process quadratures and quantum computing by proposing a quantum low-rank Gaussian process quadrature method based on a Hilbert space approximation of the Gaussian process kernel and enhancing the quadrature using a quantum circuit. The method combines the quantum phase estimation algorithm with the quantum principal component analysis technique to extract information up to a desired rank. Then, Hadamard and SWAP tests are implemented to find the expected value and variance that determines the quadrature. We use numerical simulations of a quantum computer to demonstrate the effectiveness of the method. Furthermore, we provide a theoretical complexity analysis that shows a polynomial advantage over classical Gaussian process quadrature methods. The code is available at https://github.com/cagalvisf/Quantum_HSGPQ.

## 机器学习(stat.ML:Machine Learning)

### Variational phylogenetic inference with products over bipartitions 
[[arxiv](https://arxiv.org/abs/2502.15110)] [[cool](https://papers.cool/arxiv/2502.15110)] [[pdf](https://arxiv.org/pdf/2502.15110)]
> **Authors**: Evan Sidrow,Alexandre Bouchard-Côté,Lloyd T. Elliott
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 20 pages, 5 figures
- **标题**: None
- **领域**: 机器学习,机器学习,应用领域
- **Abstract**: Bayesian phylogenetics requires accurate and efficient approximation of posterior distributions over trees. In this work, we develop a variational Bayesian approach for ultrametric phylogenetic trees. We present a novel variational family based on coalescent times of a single-linkage clustering and derive a closed-form density of the resulting distribution over trees. Unlike existing methods for ultrametric trees, our method performs inference over all of tree space, it does not require any Markov chain Monte Carlo subroutines, and our variational family is differentiable. Through experiments on benchmark genomic datasets and an application to SARS-CoV-2, we demonstrate that our method achieves competitive accuracy while requiring significantly fewer gradient evaluations than existing state-of-the-art techniques.

### Modifying Final Splits of Classification Tree for Fine-tuning Subpopulation Target in Policy Making 
[[arxiv](https://arxiv.org/abs/2502.15072)] [[cool](https://papers.cool/arxiv/2502.15072)] [[pdf](https://arxiv.org/pdf/2502.15072)]
> **Authors**: Lei Bill Wang,Zhenbang Jiao,Fangyi Wang
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习,计量经济学
- **Abstract**: Policymakers often use Classification and Regression Trees (CART) to partition populations based on binary outcomes and target subpopulations whose probability of the binary event exceeds a threshold. However, classic CART and knowledge distillation method whose student model is a CART (referred to as KD-CART) do not minimize the misclassification risk associated with classifying the latent probabilities of these binary events. To reduce the misclassification risk, we propose two methods, Penalized Final Split (PFS) and Maximizing Distance Final Split (MDFS). PFS incorporates a tunable penalty into the standard CART splitting criterion function. MDFS maximizes a weighted sum of distances between node means and the threshold. It can point-identify the optimal split under the unique intersect latent probability assumption. In addition, we develop theoretical result for MDFS splitting rule estimation, which has zero asymptotic risk. Through extensive simulation studies, we demonstrate that these methods predominately outperform classic CART and KD-CART in terms of misclassification error. Furthermore, in our empirical evaluations, these methods provide deeper insights than the two baseline methods.

### Multi-Objective Causal Bayesian Optimization 
[[arxiv](https://arxiv.org/abs/2502.14755)] [[cool](https://papers.cool/arxiv/2502.14755)] [[pdf](https://arxiv.org/pdf/2502.14755)]
> **Authors**: Shriya Bhatija,Paul-David Zuercher,Jakob Thumm,Thomas Bohné
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: 17 Pages, 12 Figures
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: In decision-making problems, the outcome of an intervention often depends on the causal relationships between system components and is highly costly to evaluate. In such settings, causal Bayesian optimization (CBO) can exploit the causal relationships between the system variables and sequentially perform interventions to approach the optimum with minimal data. Extending CBO to the multi-outcome setting, we propose Multi-Objective Causal Bayesian Optimization (MO-CBO), a paradigm for identifying Pareto-optimal interventions within a known multi-target causal graph. We first derive a graphical characterization for potentially optimal sets of variables to intervene upon. Showing that any MO-CBO problem can be decomposed into several traditional multi-objective optimization tasks, we then introduce an algorithm that sequentially balances exploration across these tasks using relative hypervolume improvement. The proposed method will be validated on both synthetic and real-world causal graphs, demonstrating its superiority over traditional (non-causal) multi-objective Bayesian optimization in settings where causal information is available.

### Internal Incoherency Scores for Constraint-based Causal Discovery Algorithms 
[[arxiv](https://arxiv.org/abs/2502.14719)] [[cool](https://papers.cool/arxiv/2502.14719)] [[pdf](https://arxiv.org/pdf/2502.14719)]
> **Authors**: Sofia Faltenbacher,Jonas Wahl,Rebecca Herman,Jakob Runge
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: under review
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Causal discovery aims to infer causal graphs from observational or experimental data. Methods such as the popular PC algorithm are based on conditional independence testing and utilize enabling assumptions, such as the faithfulness assumption, for their inferences. In practice, these assumptions, as well as the functional assumptions inherited from the chosen conditional independence test, are typically taken as a given and not further tested for their validity on the data. In this work, we propose internal coherency scores that allow testing for assumption violations and finite sample errors, whenever detectable without requiring ground truth or further statistical tests. We provide a complete classification of erroneous results, including a distinction between detectable and undetectable errors, and prove that the detectable erroneous results can be measured by our scores. We illustrate our coherency scores on the PC algorithm with simulated and real-world datasets, and envision that testing for internal coherency can become a standard tool in applying constraint-based methods, much like a suite of tests is used to validate the assumptions of classical regression analysis.

### Confidence Estimation via Sequential Likelihood Mixing 
[[arxiv](https://arxiv.org/abs/2502.14689)] [[cool](https://papers.cool/arxiv/2502.14689)] [[pdf](https://arxiv.org/pdf/2502.14689)]
> **Authors**: Johannes Kirschner,Andreas Krause,Michele Meziu,Mojmir Mutny
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: We present a universal framework for constructing confidence sets based on sequential likelihood mixing. Building upon classical results from sequential analysis, we provide a unifying perspective on several recent lines of work, and establish fundamental connections between sequential mixing, Bayesian inference and regret inequalities from online estimation. The framework applies to any realizable family of likelihood functions and allows for non-i.i.d. data and anytime validity. Moreover, the framework seamlessly integrates standard approximate inference techniques, such as variational inference and sampling-based methods, and extends to misspecified model classes, while preserving provable coverage guarantees. We illustrate the power of the framework by deriving tighter confidence sequences for classical settings, including sequential linear regression and sparse estimation, with simplified proofs.

### Generalization Error of $f$-Divergence Stabilized Algorithms via Duality 
[[arxiv](https://arxiv.org/abs/2502.14544)] [[cool](https://papers.cool/arxiv/2502.14544)] [[pdf](https://arxiv.org/pdf/2502.14544)]
> **Authors**: Francisco Daunas,Iñaki Esnaola,Samir M. Perlaza,Gholamali Aminian
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: This is new work for ISIT2025. arXiv admin note: text overlap with arXiv:2402.00501
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: The solution to empirical risk minimization with $f$-divergence regularization (ERM-$f$DR) is extended to constrained optimization problems, establishing conditions for equivalence between the solution and constraints. A dual formulation of ERM-$f$DR is introduced, providing a computationally efficient method to derive the normalization function of the ERM-$f$DR solution. This dual approach leverages the Legendre-Fenchel transform and the implicit function theorem, enabling explicit characterizations of the generalization error for general algorithms under mild conditions, and another for ERM-$f$DR solutions.

### Distribution Matching for Self-Supervised Transfer Learning 
[[arxiv](https://arxiv.org/abs/2502.14424)] [[cool](https://papers.cool/arxiv/2502.14424)] [[pdf](https://arxiv.org/pdf/2502.14424)]
> **Authors**: Yuling Jiao,Wensen Ma,Defeng Sun,Hansheng Wang,Yang Wang
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习,方法论
- **Abstract**: In this paper, we propose a novel self-supervised transfer learning method called Distribution Matching (DM), which drives the representation distribution toward a predefined reference distribution while preserving augmentation invariance. The design of DM results in a learned representation space that is intuitively structured and offers easily interpretable hyperparameters. Experimental results across multiple real-world datasets and evaluation metrics demonstrate that DM performs competitively on target classification tasks compared to existing self-supervised transfer learning methods. Additionally, we provide robust theoretical guarantees for DM, including a population theorem and an end-to-end sample theorem. The population theorem bridges the gap between the self-supervised learning task and target classification accuracy, while the sample theorem shows that, even with a limited number of samples from the target domain, DM can deliver exceptional classification performance, provided the unlabeled sample size is sufficiently large.

## 其他论文

- [GNN-Coder: Boosting Semantic Code Retrieval with Combined GNNs and Transformer](https://arxiv.org/abs/2502.15202)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR,cs.SE in whitelist
- [Learning to Collaborate: A Capability Vectors-based Architecture for Adaptive Human-AI Decision Making](https://arxiv.org/abs/2502.15196)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Enhancing Speech Large Language Models with Prompt-Aware Mixture of Audio Encoders](https://arxiv.org/abs/2502.15178)
  - **标题**: None
  - **Filtered Reason**: none of cs.SD,eess.AS in whitelist
- [AccessFixer: Enhancing GUI Accessibility for Low Vision Users With R-GCN Model](https://arxiv.org/abs/2502.15142)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Schemex: Discovering Design Patterns from Examples through Iterative Abstraction and Refinement](https://arxiv.org/abs/2502.15105)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Detecting Student Intent for Chat-Based Intelligent Tutoring Systems](https://arxiv.org/abs/2502.15096)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC,cs.CY in whitelist
- [DDAT: Diffusion Policies Enforcing Dynamically Admissible Robot Trajectories](https://arxiv.org/abs/2502.15043)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO,eess.SY in whitelist
- [Benchmarking Android Malware Detection: Rethinking the Role of Traditional and Deep Learning Models](https://arxiv.org/abs/2502.15041)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [Notions of Stack-manipulating Computation and Relative Monads (Extended Version)](https://arxiv.org/abs/2502.15031)
  - **标题**: None
  - **Filtered Reason**: none of cs.PL in whitelist
- [CHOIR: Chat-based Helper for Organizational Intelligence Repository](https://arxiv.org/abs/2502.15030)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Is Relevance Propagated from Retriever to Generator in RAG?](https://arxiv.org/abs/2502.15025)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [MACPruning: Dynamic Operation Pruning to Mitigate Side-Channel DNN Model Extraction](https://arxiv.org/abs/2502.15020)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [Learning to Solve and Verify: A Self-Play Framework for Code and Test Generation](https://arxiv.org/abs/2502.14948)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Categorical algebra of conditional probability](https://arxiv.org/abs/2502.14941)
  - **标题**: None
  - **Filtered Reason**: none of math.PR,math.ST,math.CT,cs.LO in whitelist
- [Hier-SLAM++: Neuro-Symbolic Semantic SLAM with a Hierarchically Categorical Gaussian Splatting](https://arxiv.org/abs/2502.14931)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [RAGVA: Engineering Retrieval Augmented Generation-based Virtual Assistants in Practice](https://arxiv.org/abs/2502.14930)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [DeepSeek-V3, GPT-4, Phi-4, and LLaMA-3.3 generate correct code for LoRaWAN-related engineering tasks](https://arxiv.org/abs/2502.14926)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [CODEPROMPTZIP: Code-specific Prompt Compression for Retrieval-Augmented Generation in Coding Tasks with LMs](https://arxiv.org/abs/2502.14925)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Red-Teaming LLM Multi-Agent Systems via Communication Attacks](https://arxiv.org/abs/2502.14847)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [MadVoro: Parallel Construction of Voronoi Diagrams in Distributed Memory Systems](https://arxiv.org/abs/2502.14825)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC,cs.CG,astro-ph.IM in whitelist
- [A Survey of Model Architectures in Information Retrieval](https://arxiv.org/abs/2502.14822)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [A Multi-Agent Perspective on Modern Information Retrieval](https://arxiv.org/abs/2502.14796)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [AIdeation: Designing a Human-AI Collaborative Ideation System for Concept Designers](https://arxiv.org/abs/2502.14747)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [SegAug: CTC-Aligned Segmented Augmentation For Robust RNN-Transducer Based Speech Recognition](https://arxiv.org/abs/2502.14685)
  - **标题**: None
  - **Filtered Reason**: none of cs.SD,eess.AS in whitelist
- [Set Visualizations for Comparing and Evaluating Machine Learning Models](https://arxiv.org/abs/2502.14675)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Augmenting Coaching with GenAI: Insights into Use, Effectiveness, and Future Potential](https://arxiv.org/abs/2502.14632)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Partial Incorrectness Logic](https://arxiv.org/abs/2502.14626)
  - **标题**: None
  - **Filtered Reason**: none of cs.PL,cs.LO in whitelist
- [Serving Models, Fast and Slow:Optimizing Heterogeneous LLM Inferencing Workloads at Scale](https://arxiv.org/abs/2502.14617)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC in whitelist
- ["Don't Forget the Teachers": Towards an Educator-Centered Understanding of Harms from Large Language Models in Education](https://arxiv.org/abs/2502.14592)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [madupite: A High-Performance Distributed Solver for Large-Scale Markov Decision Processes](https://arxiv.org/abs/2502.14474)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC in whitelist
- [LLM4FaaS: No-Code Application Development using LLMs and FaaS](https://arxiv.org/abs/2502.14450)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC,cs.SE in whitelist
- [SolSearch: An LLM-Driven Framework for Efficient SAT-Solving Code Generation](https://arxiv.org/abs/2502.14328)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [ChemHTS: Hierarchical Tool Stacking for Enhancing Chemical Agents](https://arxiv.org/abs/2502.14327)
  - **标题**: None
  - **Filtered Reason**: none of cs.CE in whitelist
- [On the Trustworthiness of Generative Foundation Models: Guideline, Assessment, and Perspective](https://arxiv.org/abs/2502.14296)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [DAG: Deep Adaptive and Generative $K$-Free Community Detection on Attributed Graphs](https://arxiv.org/abs/2502.14294)
  - **标题**: None
  - **Filtered Reason**: none of cs.SI in whitelist
- [A Note on Efficient Privacy-Preserving Similarity Search for Encrypted Vectors](https://arxiv.org/abs/2502.14291)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [Are your apps accessible? A GCN-based accessibility checker for low vision users](https://arxiv.org/abs/2502.14288)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
