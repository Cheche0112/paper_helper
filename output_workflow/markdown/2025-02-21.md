> 本文由 [https://github.com/huiyeruzhou/arxiv_crawler](https://github.com/huiyeruzhou/arxiv_crawler) 自动生成
>
> 领域白名单：cs.AI,cs.CL,cs.LG,cs.CV
> 关键词： LLM, GPT, AI, language+model, deep+learning, transformer, neural+network, machine+learning

# 论文全览：2025-02-21

共有2篇相关领域论文, 另有0篇其他

## 计算语言学(cs.CL:Computation and Language)

### Control Illusion: The Failure of Instruction Hierarchies in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.15851)] [[cool](https://papers.cool/arxiv/2502.15851)] [[pdf](https://arxiv.org/pdf/2502.15851)]
> **Authors**: Yilin Geng,Haonan Li,Honglin Mu,Xudong Han,Timothy Baldwin,Omri Abend,Eduard Hovy,Lea Frermann
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large language models (LLMs) are increasingly deployed with hierarchical instruction schemes, where certain instructions (e.g., system-level directives) are expected to take precedence over others (e.g., user messages). Yet, we lack a systematic understanding of how effectively these hierarchical control mechanisms work. We introduce a systematic evaluation framework based on constraint prioritization to assess how well LLMs enforce instruction hierarchies. Our experiments across six state-of-the-art LLMs reveal that models struggle with consistent instruction prioritization, even for simple formatting conflicts. We find that the widely-adopted system/user prompt separation fails to establish a reliable instruction hierarchy, and models exhibit strong inherent biases toward certain constraint types regardless of their priority designation. While controlled prompt engineering and model fine-tuning show modest improvements, our results indicate that instruction hierarchy enforcement is not robustly realized, calling for deeper architectural innovations beyond surface-level modifications.

### Forecasting Frontier Language Model Agent Capabilities 
[[arxiv](https://arxiv.org/abs/2502.15850)] [[cool](https://papers.cool/arxiv/2502.15850)] [[pdf](https://arxiv.org/pdf/2502.15850)]
> **Authors**: Govind Pimpale,Axel Højmark,Jérémy Scheurer,Marius Hobbhahn
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-21
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: As Language Models (LMs) increasingly operate as autonomous agents, accurately forecasting their capabilities becomes crucial for societal preparedness. We evaluate six forecasting methods that predict downstream capabilities of LM agents. We use "one-step" approaches that predict benchmark scores from input metrics like compute or model release date directly or "two-step" approaches that first predict an intermediate metric like the principal component of cross-benchmark performance (PC-1) and human-evaluated competitive Elo ratings. We evaluate our forecasting methods by backtesting them on a dataset of 38 LMs from the OpenLLM 2 leaderboard. We then use the validated two-step approach (Release Date$\to$Elo$\to$Benchmark) to predict LM agent performance for frontier models on three benchmarks: SWE-Bench Verified (software development), Cybench (cybersecurity assessment), and RE-Bench (ML research engineering). Our forecast predicts that by the beginning of 2026, non-specialized LM agents with low capability elicitation will reach a success rate of 54% on SWE-Bench Verified, while state-of-the-art LM agents will reach an 87% success rate. Our approach does not account for recent advances in inference-compute scaling and might thus be too conservative.

## 其他论文

