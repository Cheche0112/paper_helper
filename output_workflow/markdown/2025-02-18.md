> 本文由 [https://github.com/huiyeruzhou/arxiv_crawler](https://github.com/huiyeruzhou/arxiv_crawler) 自动生成
>
> 领域白名单：cs.AI,cs.CL,cs.LG,cs.CV
> 关键词： LLM, GPT, AI, language+model, deep+learning, transformer, neural+network, machine+learning

# 论文全览：2025-02-18

共有434篇相关领域论文, 另有41篇其他

## 人工智能(cs.AI:Artificial Intelligence)

### CityEQA: A Hierarchical LLM Agent on Embodied Question Answering Benchmark in City Space 
[[arxiv](https://arxiv.org/abs/2502.12532)] [[cool](https://papers.cool/arxiv/2502.12532)] [[pdf](https://arxiv.org/pdf/2502.12532)]
> **Authors**: Yong Zhao,Kai Xu,Zhengqiu Zhu,Yue Hu,Zhiheng Zheng,Yingfeng Chen,Yatai Ji,Chen Gao,Yong Li,Jincai Huang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Embodied Question Answering (EQA) has primarily focused on indoor environments, leaving the complexities of urban settings - spanning environment, action, and perception - largely unexplored. To bridge this gap, we introduce CityEQA, a new task where an embodied agent answers open-vocabulary questions through active exploration in dynamic city spaces. To support this task, we present CityEQA-EC, the first benchmark dataset featuring 1,412 human-annotated tasks across six categories, grounded in a realistic 3D urban simulator. Moreover, we propose Planner-Manager-Actor (PMA), a novel agent tailored for CityEQA. PMA enables long-horizon planning and hierarchical task execution: the Planner breaks down the question answering into sub-tasks, the Manager maintains an object-centric cognitive map for spatial reasoning during the process control, and the specialized Actors handle navigation, exploration, and collection sub-tasks. Experiments demonstrate that PMA achieves 60.7% of human-level answering accuracy, significantly outperforming frontier-based baselines. While promising, the performance gap compared to humans highlights the need for enhanced visual reasoning in CityEQA. This work paves the way for future advancements in urban spatial intelligence. Dataset and code are available at https://github.com/BiluYong/CityEQA.git.

### Inference-Time Computations for LLM Reasoning and Planning: A Benchmark and Insights 
[[arxiv](https://arxiv.org/abs/2502.12521)] [[cool](https://papers.cool/arxiv/2502.12521)] [[pdf](https://arxiv.org/pdf/2502.12521)]
> **Authors**: Shubham Parashar,Blake Olson,Sambhav Khurana,Eric Li,Hongyi Ling,James Caverlee,Shuiwang Ji
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: We examine the reasoning and planning capabilities of large language models (LLMs) in solving complex tasks. Recent advances in inference-time techniques demonstrate the potential to enhance LLM reasoning without additional training by exploring intermediate steps during inference. Notably, OpenAI's o1 model shows promising performance through its novel use of multi-step reasoning and verification. Here, we explore how scaling inference-time techniques can improve reasoning and planning, focusing on understanding the tradeoff between computational cost and performance. To this end, we construct a comprehensive benchmark, known as Sys2Bench, and perform extensive experiments evaluating existing inference-time techniques on eleven diverse tasks across five categories, including arithmetic reasoning, logical reasoning, common sense reasoning, algorithmic reasoning, and planning. Our findings indicate that simply scaling inference-time computation has limitations, as no single inference-time technique consistently performs well across all reasoning and planning tasks.

### Boost, Disentangle, and Customize: A Robust System2-to-System1 Pipeline for Code Generation 
[[arxiv](https://arxiv.org/abs/2502.12492)] [[cool](https://papers.cool/arxiv/2502.12492)] [[pdf](https://arxiv.org/pdf/2502.12492)]
> **Authors**: Kounianhua Du,Hanjing Wang,Jianxing Liu,Jizheng Chen,Xinyi Dai,Yasheng Wang,Ruiming Tang,Yong Yu,Jun Wang,Weinan Zhang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Large language models (LLMs) have demonstrated remarkable capabilities in various domains, particularly in system 1 tasks, yet the intricacies of their problem-solving mechanisms in system 2 tasks are not sufficiently explored. Recent research on System2-to-System1 methods surge, exploring the System 2 reasoning knowledge via inference-time computation and compressing the explored knowledge into System 1 process. In this paper, we focus on code generation, which is a representative System 2 task, and identify two primary challenges: (1) the complex hidden reasoning processes and (2) the heterogeneous data distributions that complicate the exploration and training of robust LLM solvers. To tackle these issues, we propose a novel BDC framework that explores insightful System 2 knowledge of LLMs using a MC-Tree-Of-Agents algorithm with mutual \textbf{B}oosting, \textbf{D}isentangles the heterogeneous training data for composable LoRA-experts, and obtain \textbf{C}ustomized problem solver for each data instance with an input-aware hypernetwork to weight over the LoRA-experts, offering effectiveness, flexibility, and robustness. This framework leverages multiple LLMs through mutual verification and boosting, integrated into a Monte-Carlo Tree Search process enhanced by reflection-based pruning and refinement. Additionally, we introduce the DisenLora algorithm, which clusters heterogeneous data to fine-tune LLMs into composable Lora experts, enabling the adaptive generation of customized problem solvers through an input-aware hypernetwork. This work lays the groundwork for advancing LLM capabilities in complex reasoning tasks, offering a novel System2-to-System1 solution.

### Investigating and Extending Homans' Social Exchange Theory with Large Language Model based Agents 
[[arxiv](https://arxiv.org/abs/2502.12450)] [[cool](https://papers.cool/arxiv/2502.12450)] [[pdf](https://arxiv.org/pdf/2502.12450)]
> **Authors**: Lei Wang,Zheqing Zhang,Xu Chen
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Homans' Social Exchange Theory (SET) is widely recognized as a basic framework for understanding the formation and emergence of human civilizations and social structures. In social science, this theory is typically studied based on simple simulation experiments or real-world human studies, both of which either lack realism or are too expensive to control. In artificial intelligence, recent advances in large language models (LLMs) have shown promising capabilities in simulating human behaviors. Inspired by these insights, we adopt an interdisciplinary research perspective and propose using LLM-based agents to study Homans' SET. Specifically, we construct a virtual society composed of three LLM agents and have them engage in a social exchange game to observe their behaviors. Through extensive experiments, we found that Homans' SET is well validated in our agent society, demonstrating the consistency between the agent and human behaviors. Building on this foundation, we intentionally alter the settings of the agent society to extend the traditional Homans' SET, making it more comprehensive and detailed. To the best of our knowledge, this paper marks the first step in studying Homans' SET with LLM-based agents. More importantly, it introduces a novel and feasible research paradigm that bridges the fields of social science and computer science through LLM-based agents. Code is available at https://github.com/Paitesanshi/SET.

### Computational Safety for Generative AI: A Signal Processing Perspective 
[[arxiv](https://arxiv.org/abs/2502.12445)] [[cool](https://papers.cool/arxiv/2502.12445)] [[pdf](https://arxiv.org/pdf/2502.12445)]
> **Authors**: Pin-Yu Chen
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: preprint for an invited paper
- **标题**: None
- **领域**: 人工智能,机器学习,机器学习
- **Abstract**: AI safety is a rapidly growing area of research that seeks to prevent the harm and misuse of frontier AI technology, particularly with respect to generative AI (GenAI) tools that are capable of creating realistic and high-quality content through text prompts. Examples of such tools include large language models (LLMs) and text-to-image (T2I) diffusion models. As the performance of various leading GenAI models approaches saturation due to similar training data sources and neural network architecture designs, the development of reliable safety guardrails has become a key differentiator for responsibility and sustainability. This paper presents a formalization of the concept of computational safety, which is a mathematical framework that enables the quantitative assessment, formulation, and study of safety challenges in GenAI through the lens of signal processing theory and methods. In particular, we explore two exemplary categories of computational safety challenges in GenAI that can be formulated as hypothesis testing problems. For the safety of model input, we show how sensitivity analysis and loss landscape analysis can be used to detect malicious prompts with jailbreak attempts. For the safety of model output, we elucidate how statistical signal processing and adversarial learning can be used to detect AI-generated content. Finally, we discuss key open research challenges, opportunities, and the essential role of signal processing in computational AI safety.

### A Survey on Large Language Models for Automated Planning 
[[arxiv](https://arxiv.org/abs/2502.12435)] [[cool](https://papers.cool/arxiv/2502.12435)] [[pdf](https://arxiv.org/pdf/2502.12435)]
> **Authors**: Mohamed Aghzal,Erion Plaku,Gregory J. Stein,Ziyu Yao
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: The planning ability of Large Language Models (LLMs) has garnered increasing attention in recent years due to their remarkable capacity for multi-step reasoning and their ability to generalize across a wide range of domains. While some researchers emphasize the potential of LLMs to perform complex planning tasks, others highlight significant limitations in their performance, particularly when these models are tasked with handling the intricacies of long-horizon reasoning. In this survey, we critically investigate existing research on the use of LLMs in automated planning, examining both their successes and shortcomings in detail. We illustrate that although LLMs are not well-suited to serve as standalone planners because of these limitations, they nonetheless present an enormous opportunity to enhance planning applications when combined with other approaches. Thus, we advocate for a balanced methodology that leverages the inherent flexibility and generalized knowledge of LLMs alongside the rigor and cost-effectiveness of traditional planning methods.

### Integrating Expert Knowledge into Logical Programs via LLMs 
[[arxiv](https://arxiv.org/abs/2502.12275)] [[cool](https://papers.cool/arxiv/2502.12275)] [[pdf](https://arxiv.org/pdf/2502.12275)]
> **Authors**: Franciszek Górski,Oskar Wysocki,Marco Valentino,Andre Freitas
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学,多代理系统
- **Abstract**: This paper introduces ExKLoP, a novel framework designed to evaluate how effectively Large Language Models (LLMs) integrate expert knowledge into logical reasoning systems. This capability is especially valuable in engineering, where expert knowledge-such as manufacturer-recommended operational ranges-can be directly embedded into automated monitoring systems. By mirroring expert verification steps, tasks like range checking and constraint validation help ensure system safety and reliability. Our approach systematically evaluates LLM-generated logical rules, assessing both syntactic fluency and logical correctness in these critical validation tasks. We also explore the models capacity for self-correction via an iterative feedback loop based on code execution outcomes. ExKLoP presents an extensible dataset comprising 130 engineering premises, 950 prompts, and corresponding validation points. It enables comprehensive benchmarking while allowing control over task complexity and scalability of experiments. We leverage the synthetic data creation methodology to conduct extensive empirical evaluation on a diverse set of LLMs including Llama3, Gemma, Mixtral, Mistral, and Qwen. Results reveal that while models generate nearly perfect syntactically correct code, they frequently exhibit logical errors in translating expert knowledge. Furthermore, iterative self-correction yields only marginal improvements (up to 3%). Overall, ExKLoP serves as a robust evaluation platform that streamlines the selection of effective models for self-correcting systems while clearly delineating the types of errors encountered. The complete implementation, along with all relevant data, is available at GitHub.

### Accurate Expert Predictions in MoE Inference via Cross-Layer Gate 
[[arxiv](https://arxiv.org/abs/2502.12224)] [[cool](https://papers.cool/arxiv/2502.12224)] [[pdf](https://arxiv.org/pdf/2502.12224)]
> **Authors**: Zhiyuan Fang,Zicong Hong,Yuegui Huang,Yufeng Lyu,Wuhui Chen,Yue Yu,Fan Yu,Zibin Zheng
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: Large Language Models (LLMs) have demonstrated impressive performance across various tasks, and their application in edge scenarios has attracted significant attention. However, sparse-activated Mixture-of-Experts (MoE) models, which are well suited for edge scenarios, have received relatively little attention due to their high memory demands. Offload-based methods have been proposed to address this challenge, but they face difficulties with expert prediction. Inaccurate expert predictions can result in prolonged inference delays. To promote the application of MoE models in edge scenarios, we propose Fate, an offloading system designed for MoE models to enable efficient inference in resource-constrained environments. The key insight behind Fate is that gate inputs from adjacent layers can be effectively used for expert prefetching, achieving high prediction accuracy without additional GPU overhead. Furthermore, Fate employs a shallow-favoring expert caching strategy that increases the expert hit rate to 99\%. Additionally, Fate integrates tailored quantization strategies for cache optimization and IO efficiency. Experimental results show that, compared to Load on Demand and Expert Activation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in prefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively, while maintaining inference quality. Moreover, Fate's performance improvements are scalable across different memory budgets.

### Evaluating the Paperclip Maximizer: Are RL-Based Language Models More Likely to Pursue Instrumental Goals? 
[[arxiv](https://arxiv.org/abs/2502.12206)] [[cool](https://papers.cool/arxiv/2502.12206)] [[pdf](https://arxiv.org/pdf/2502.12206)]
> **Authors**: Yufei He,Yuexin Li,Jiaying Wu,Yuan Sui,Yulin Chen,Bryan Hooi
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学,机器学习
- **Abstract**: As large language models (LLMs) continue to evolve, ensuring their alignment with human goals and values remains a pressing challenge. A key concern is \textit{instrumental convergence}, where an AI system, in optimizing for a given objective, develops unintended intermediate goals that override the ultimate objective and deviate from human-intended goals. This issue is particularly relevant in reinforcement learning (RL)-trained models, which can generate creative but unintended strategies to maximize rewards. In this paper, we explore instrumental convergence in LLMs by comparing models trained with direct RL optimization (e.g., the o1 model) to those trained with reinforcement learning from human feedback (RLHF). We hypothesize that RL-driven models exhibit a stronger tendency for instrumental convergence due to their optimization of goal-directed behavior in ways that may misalign with human intentions. To assess this, we introduce InstrumentalEval, a benchmark for evaluating instrumental convergence in RL-trained LLMs. Initial experiments reveal cases where a model tasked with making money unexpectedly pursues instrumental objectives, such as self-replication, implying signs of instrumental convergence. Our findings contribute to a deeper understanding of alignment challenges in AI systems and the risks posed by unintended model behaviors.

### Small Models Struggle to Learn from Strong Reasoners 
[[arxiv](https://arxiv.org/abs/2502.12143)] [[cool](https://papers.cool/arxiv/2502.12143)] [[pdf](https://arxiv.org/pdf/2502.12143)]
> **Authors**: Yuetai Li,Xiang Yue,Zhangchen Xu,Fengqing Jiang,Luyao Niu,Bill Yuchen Lin,Bhaskar Ramasubramanian,Radha Poovendran
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Large language models (LLMs) excel in complex reasoning tasks, and distilling their reasoning capabilities into smaller models has shown promise. However, we uncover an interesting phenomenon, which we term the Small Model Learnability Gap: small models ($\leq$3B parameters) do not consistently benefit from long chain-of-thought (CoT) reasoning or distillation from larger models. Instead, they perform better when fine-tuned on shorter, simpler reasoning chains that better align with their intrinsic learning capacity. To address this, we propose Mix Distillation, a simple yet effective strategy that balances reasoning complexity by combining long and short CoT examples or reasoning from both larger and smaller models. Our experiments demonstrate that Mix Distillation significantly improves small model reasoning performance compared to training on either data alone. These findings highlight the limitations of direct strong model distillation and underscore the importance of adapting reasoning complexity for effective reasoning capability transfer.

### Transformer Dynamics: A neuroscientific approach to interpretability of large language models 
[[arxiv](https://arxiv.org/abs/2502.12131)] [[cool](https://papers.cool/arxiv/2502.12131)] [[pdf](https://arxiv.org/pdf/2502.12131)]
> **Authors**: Jesseba Fernando,Grigori Guitchounts
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: As artificial intelligence models have exploded in scale and capability, understanding of their internal mechanisms remains a critical challenge. Inspired by the success of dynamical systems approaches in neuroscience, here we propose a novel framework for studying computations in deep learning systems. We focus on the residual stream (RS) in transformer models, conceptualizing it as a dynamical system evolving across layers. We find that activations of individual RS units exhibit strong continuity across layers, despite the RS being a non-privileged basis. Activations in the RS accelerate and grow denser over layers, while individual units trace unstable periodic orbits. In reduced-dimensional spaces, the RS follows a curved trajectory with attractor-like dynamics in the lower layers. These insights bridge dynamical systems theory and mechanistic interpretability, establishing a foundation for a "neuroscience of AI" that combines theoretical rigor with large-scale data analysis to advance our understanding of modern neural networks.

### Scaling Autonomous Agents via Automatic Reward Modeling And Planning 
[[arxiv](https://arxiv.org/abs/2502.12130)] [[cool](https://papers.cool/arxiv/2502.12130)] [[pdf](https://arxiv.org/pdf/2502.12130)]
> **Authors**: Zhenfang Chen,Delin Chen,Rui Sun,Wenjun Liu,Chuang Gan
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: ICLR2025, Project page: https://armap-agent.github.io
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Large language models (LLMs) have demonstrated remarkable capabilities across a range of text-generation tasks. However, LLMs still struggle with problems requiring multi-step decision-making and environmental feedback, such as online shopping, scientific reasoning, and mathematical problem-solving. Unlike pure text data, collecting large-scale decision-making data is challenging. Moreover, many powerful LLMs are only accessible through APIs, which hinders their fine-tuning for agent tasks due to cost and complexity. To address LLM agents' limitations, we propose a framework that can automatically learn a reward model from the environment without human annotations. This model can be used to evaluate the action trajectories of LLM agents and provide heuristics for task planning. Specifically, our approach involves employing one LLM-based agent to navigate an environment randomly, generating diverse action trajectories. Subsequently, a separate LLM is leveraged to assign a task intent and synthesize a negative response alongside the correct response for each trajectory. These triplets (task intent, positive response, and negative response) are then utilized as training data to optimize a reward model capable of scoring action trajectories. The effectiveness and generalizability of our framework are demonstrated through evaluations conducted on different agent benchmarks. In conclusion, our proposed framework represents a significant advancement in enhancing LLM agents' decision-making capabilities. By automating the learning of reward models, we overcome the challenges of data scarcity and API limitations, potentially revolutionizing the application of LLMs in complex and interactive environments. This research paves the way for more sophisticated AI agents capable of tackling a wide range of real-world problems requiring multi-step decision-making.

### Hypernym Bias: Unraveling Deep Classifier Training Dynamics through the Lens of Class Hierarchy 
[[arxiv](https://arxiv.org/abs/2502.12125)] [[cool](https://papers.cool/arxiv/2502.12125)] [[pdf](https://arxiv.org/pdf/2502.12125)]
> **Authors**: Roman Malashin,Valeria Yachnaya,Alexander Mullin
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: We investigate the training dynamics of deep classifiers by examining how hierarchical relationships between classes evolve during training. Through extensive experiments, we argue that the learning process in classification problems can be understood through the lens of label clustering. Specifically, we observe that networks tend to distinguish higher-level (hypernym) categories in the early stages of training, and learn more specific (hyponym) categories later. We introduce a novel framework to track the evolution of the feature manifold during training, revealing how the hierarchy of class relations emerges and refines across the network layers. Our analysis demonstrates that the learned representations closely align with the semantic structure of the dataset, providing a quantitative description of the clustering process. Notably, we show that in the hypernym label space, certain properties of neural collapse appear earlier than in the hyponym label space, helping to bridge the gap between the initial and terminal phases of learning. We believe our findings offer new insights into the mechanisms driving hierarchical learning in deep networks, paving the way for future advancements in understanding deep learning dynamics.

### Relational Norms for Human-AI Cooperation 
[[arxiv](https://arxiv.org/abs/2502.12102)] [[cool](https://papers.cool/arxiv/2502.12102)] [[pdf](https://arxiv.org/pdf/2502.12102)]
> **Authors**: Brian D. Earp,Sebastian Porsdam Mann,Mateo Aboy,Edmond Awad,Monika Betzler,Marietjie Botes,Rachel Calcott,Mina Caraccio,Nick Chater,Mark Coeckelbergh,Mihaela Constantinescu,Hossein Dabbagh,Kate Devlin,Xiaojun Ding,Vilius Dranseika,Jim A. C. Everett,Ruiping Fan,Faisal Feroz,Kathryn B. Francis,Cindy Friedman,Orsolya Friedrich,Iason Gabriel,Ivar Hannikainen,Julie Hellmann,Arasj Khodadade Jahrome, et al. (37 additional authors not shown)
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 76 pages, 2 figures
- **标题**: None
- **领域**: 人工智能,新兴技术
- **Abstract**: How we should design and interact with social artificial intelligence depends on the socio-relational role the AI is meant to emulate or occupy. In human society, relationships such as teacher-student, parent-child, neighbors, siblings, or employer-employee are governed by specific norms that prescribe or proscribe cooperative functions including hierarchy, care, transaction, and mating. These norms shape our judgments of what is appropriate for each partner. For example, workplace norms may allow a boss to give orders to an employee, but not vice versa, reflecting hierarchical and transactional expectations. As AI agents and chatbots powered by large language models are increasingly designed to serve roles analogous to human positions - such as assistant, mental health provider, tutor, or romantic partner - it is imperative to examine whether and how human relational norms should extend to human-AI interactions. Our analysis explores how differences between AI systems and humans, such as the absence of conscious experience and immunity to fatigue, may affect an AI's capacity to fulfill relationship-specific functions and adhere to corresponding norms. This analysis, which is a collaborative effort by philosophers, psychologists, relationship scientists, ethicists, legal experts, and AI researchers, carries important implications for AI systems design, user behavior, and regulation. While we accept that AI systems can offer significant benefits such as increased availability and consistency in certain socio-relational roles, they also risk fostering unhealthy dependencies or unrealistic expectations that could spill over into human-human relationships. We propose that understanding and thoughtfully shaping (or implementing) suitable human-AI relational norms will be crucial for ensuring that human-AI interactions are ethical, trustworthy, and favorable to human well-being.

### A Study on Leveraging Search and Self-Feedback for Agent Reasoning 
[[arxiv](https://arxiv.org/abs/2502.12094)] [[cool](https://papers.cool/arxiv/2502.12094)] [[pdf](https://arxiv.org/pdf/2502.12094)]
> **Authors**: Karthikeyan K,Michelle Yuan,Elman Mansimov,Katerina Margatina,Anurag Pratik,Daniele Bonadiman,Monica Sunkara,Yi Zhang,Yassine Benajiba
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Under review
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: Recent works have demonstrated that incorporating search during inference can significantly improve reasoning capabilities of language agents. Some approaches may make use of the ground truth or rely on model's own generated feedback. The search algorithm uses this feedback to then produce values that will update its criterion for exploring and exploiting various reasoning paths. In this study, we investigate how search and model's self-feedback can be leveraged for reasoning tasks. First, we explore differences in ground-truth feedback and self-feedback during search for math reasoning. Second, we observe limitations in applying search techniques to more complex tasks like tool-calling and design domain-specific approaches to address these gaps. Our experiments reveal challenges related to generalization when solely relying on self-feedback during search. For search to work effectively, either access to the ground-truth is needed or feedback mechanisms need to be carefully designed for the specific task.

### CONSTRUCTA: Automating Commercial Construction Schedules in Fabrication Facilities with Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.12066)] [[cool](https://papers.cool/arxiv/2502.12066)] [[pdf](https://arxiv.org/pdf/2502.12066)]
> **Authors**: Yifan Zhang,Xue Yang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习,软件工程
- **Abstract**: Automating planning with LLMs presents transformative opportunities for traditional industries, yet remains underexplored. In commercial construction, the complexity of automated scheduling often requires manual intervention to ensure precision. We propose CONSTRUCTA, a novel framework leveraging LLMs to optimize construction schedules in complex projects like semiconductor fabrication. CONSTRUCTA addresses key challenges by: (1) integrating construction-specific knowledge through static RAG; (2) employing context-sampling techniques inspired by architectural expertise to provide relevant input; and (3) deploying Construction DPO to align schedules with expert preferences using RLHF. Experiments on proprietary data demonstrate performance improvements of +42.3% in missing value prediction, +79.1% in dependency analysis, and +28.9% in automated planning compared to baseline methods, showcasing its potential to revolutionize construction workflows and inspire domain-specific LLM advancements.

### PhysReason: A Comprehensive Benchmark towards Physics-Based Reasoning 
[[arxiv](https://arxiv.org/abs/2502.12054)] [[cool](https://papers.cool/arxiv/2502.12054)] [[pdf](https://arxiv.org/pdf/2502.12054)]
> **Authors**: Xinyu Zhang,Yuxuan Dong,Yanrui Wu,Jiaxing Huang,Chengyou Jia,Basura Fernando,Mike Zheng Shou,Lingling Zhang,Jun Liu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Large language models demonstrate remarkable capabilities across various domains, especially mathematics and logic reasoning. However, current evaluations overlook physics-based reasoning - a complex task requiring physics theorems and constraints. We present PhysReason, a 1,200-problem benchmark comprising knowledge-based (25%) and reasoning-based (75%) problems, where the latter are divided into three difficulty levels (easy, medium, hard). Notably, problems require an average of 8.1 solution steps, with hard requiring 15.6, reflecting the complexity of physics-based reasoning. We propose the Physics Solution Auto Scoring Framework, incorporating efficient answer-level and comprehensive step-level evaluations. Top-performing models like Deepseek-R1, Gemini-2.0-Flash-Thinking, and o3-mini-high achieve less than 60% on answer-level evaluation, with performance dropping from knowledge questions (75.11%) to hard problems (31.95%). Through step-level evaluation, we identified four key bottlenecks: Physics Theorem Application, Physics Process Understanding, Calculation, and Physics Condition Analysis. These findings position PhysReason as a novel and comprehensive benchmark for evaluating physics-based reasoning capabilities in large language models. Our code and data will be published at https:/dxzxy12138.github.io/PhysReason.

### A Survey on Bridging EEG Signals and Generative AI: From Image and Text to Beyond 
[[arxiv](https://arxiv.org/abs/2502.12048)] [[cool](https://papers.cool/arxiv/2502.12048)] [[pdf](https://arxiv.org/pdf/2502.12048)]
> **Authors**: Shreya Shukla,Jose Torres,Abhijit Mishra,Jacek Gwizdka,Shounak Roychowdhury
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,人机交互,机器学习
- **Abstract**: Integration of Brain-Computer Interfaces (BCIs) and Generative Artificial Intelligence (GenAI) has opened new frontiers in brain signal decoding, enabling assistive communication, neural representation learning, and multimodal integration. BCIs, particularly those leveraging Electroencephalography (EEG), provide a non-invasive means of translating neural activity into meaningful outputs. Recent advances in deep learning, including Generative Adversarial Networks (GANs) and Transformer-based Large Language Models (LLMs), have significantly improved EEG-based generation of images, text, and speech. This paper provides a literature review of the state-of-the-art in EEG-based multimodal generation, focusing on (i) EEG-to-image generation through GANs, Variational Autoencoders (VAEs), and Diffusion Models, and (ii) EEG-to-text generation leveraging Transformer based language models and contrastive learning methods. Additionally, we discuss the emerging domain of EEG-to-speech synthesis, an evolving multimodal frontier. We highlight key datasets, use cases, challenges, and EEG feature encoding methods that underpin generative approaches. By providing a structured overview of EEG-based generative AI, this survey aims to equip researchers and practitioners with insights to advance neural decoding, enhance assistive technologies, and expand the frontiers of brain-computer interaction.

### KnowPath: Knowledge-enhanced Reasoning via LLM-generated Inference Paths over Knowledge Graphs 
[[arxiv](https://arxiv.org/abs/2502.12029)] [[cool](https://papers.cool/arxiv/2502.12029)] [[pdf](https://arxiv.org/pdf/2502.12029)]
> **Authors**: Qi Zhao,Hongyu Yang,Qi Song,Xinwei Yao,Xiangyang Li
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Large language models (LLMs) have demonstrated remarkable capabilities in various complex tasks, yet they still suffer from hallucinations. Introducing external knowledge, such as knowledge graph, can enhance the LLMs' ability to provide factual answers. LLMs have the ability to interactively explore knowledge graphs. However, most approaches have been affected by insufficient internal knowledge excavation in LLMs, limited generation of trustworthy knowledge reasoning paths, and a vague integration between internal and external knowledge. Therefore, we propose KnowPath, a knowledge-enhanced large model framework driven by the collaboration of internal and external knowledge. It relies on the internal knowledge of the LLM to guide the exploration of interpretable directed subgraphs in external knowledge graphs, better integrating the two knowledge sources for more accurate reasoning. Extensive experiments on multiple real-world datasets confirm the superiority of KnowPath.

### SafeChain: Safety of Language Models with Long Chain-of-Thought Reasoning Capabilities 
[[arxiv](https://arxiv.org/abs/2502.12025)] [[cool](https://papers.cool/arxiv/2502.12025)] [[pdf](https://arxiv.org/pdf/2502.12025)]
> **Authors**: Fengqing Jiang,Zhangchen Xu,Yuetai Li,Luyao Niu,Zhen Xiang,Bo Li,Bill Yuchen Lin,Radha Poovendran
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: Emerging large reasoning models (LRMs), such as DeepSeek-R1 models, leverage long chain-of-thought (CoT) reasoning to generate structured intermediate steps, enhancing their reasoning capabilities. However, long CoT does not inherently guarantee safe outputs, potentially leading to harmful consequences such as the introduction of security vulnerabilities in code or the spread of misinformation. Current research on large language model (LLM) safety usually focuses on short-answer responses, overlooking the long CoT style outputs of LRMs. To bridge this gap, we conduct a systematic study of LRM safety. First, we investigate safety evaluators calibrated against human annotations. Using our newly developed metrics, we thoroughly assess the safety of 12 state-of-the-art LRMs on StrongReject and WildJailbreak datasets. Our results show that LRMs are not safe compared to their reasoning advance. Further, we perform a fine-grained analysis of the reasoning trace and final answer. We find that three decoding strategies-ZeroThink, LessThink, and MoreThink-can improve model safety without additional training. However, these strategies either use constrained reasoning traces or incur high inference costs. To better strengthen LRM safety, we introduce SafeChain, the first-of-its-kind safety training dataset in CoT style. We fine-tune two LRMs with SafeChain, showing that it not only enhances model safety but also preserves performance across 6 reasoning benchmarks.

### Learning Generalizable Prompt for CLIP with Class Similarity Knowledge 
[[arxiv](https://arxiv.org/abs/2502.11969)] [[cool](https://papers.cool/arxiv/2502.11969)] [[pdf](https://arxiv.org/pdf/2502.11969)]
> **Authors**: Sehun Jung,Hyang-won Lee
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算机视觉和模式识别,机器学习
- **Abstract**: In vision-language models (VLMs), prompt tuning has shown its effectiveness in adapting models to downstream tasks. However, learned prompts struggle to generalize to unseen classes, as they tend to overfit to the classes that are targeted during prompt tuning. Examining failure cases, we observed that learned prompts disrupt the semantics of unseen classes, generating text embeddings with incorrect semantic relationships among classes. To address this, we propose Similarity Alignment Regularization (SAR), which regularizes learnable prompts to preserve the semantic relationships among classes captured by hand-crafted prompts. Specifically, we first obtain novel classes related to base classes using ChatGPT-4o and utilize them as potential unseen classes during prompt tuning. Then, by targeting both base and novel classes, SAR aligns the similarity relationships among text embeddings generated by learnable prompts with the similarity relationships from hand-crafted prompts. Extensive experiments applying SAR to existing prompt tuning methods demonstrate its effectiveness in improving generalization to unseen classes.

### GRAPHGPT-O: Synergistic Multimodal Comprehension and Generation on Graphs 
[[arxiv](https://arxiv.org/abs/2502.11925)] [[cool](https://papers.cool/arxiv/2502.11925)] [[pdf](https://arxiv.org/pdf/2502.11925)]
> **Authors**: Yi Fang,Bowen Jin,Jiacheng Shen,Sirui Ding,Qiaoyu Tan,Jiawei Han
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算机视觉和模式识别,机器学习
- **Abstract**: The rapid development of Multimodal Large Language Models (MLLMs) has enabled the integration of multiple modalities, including texts and images, within the large language model (LLM) framework. However, texts and images are usually interconnected, forming a multimodal attributed graph (MMAG). It is underexplored how MLLMs can incorporate the relational information (\textit{i.e.}, graph structure) and semantic information (\textit{i.e.,} texts and images) on such graphs for multimodal comprehension and generation. In this paper, we propose GraphGPT-o, which supports omni-multimodal understanding and creation on MMAGs. We first comprehensively study linearization variants to transform semantic and structural information as input for MLLMs. Then, we propose a hierarchical aligner that enables deep graph encoding, bridging the gap between MMAGs and MLLMs. Finally, we explore the inference choices, adapting MLLM to interleaved text and image generation in graph scenarios. Extensive experiments on three datasets from different domains demonstrate the effectiveness of our proposed method. Datasets and codes will be open-sourced upon acceptance.

### On the robustness of ChatGPT in teaching Korean Mathematics 
[[arxiv](https://arxiv.org/abs/2502.11915)] [[cool](https://papers.cool/arxiv/2502.11915)] [[pdf](https://arxiv.org/pdf/2502.11915)]
> **Authors**: Phuong-Nam Nguyen,Quang Nguyen-The,An Vu-Minh,Diep-Anh Nguyen,Xuan-Lam Pham
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 21 pages, 12 figures, includes statistical analysis of ChatGPT's robustness in solving and rating multilingual mathematics questions. Focus on Korean CSAT Mathematics. EvaluatesAIaccuracy, rating effectiveness, and topic analysis
- **标题**: None
- **领域**: 人工智能,历史与概述
- **Abstract**: ChatGPT, an Artificial Intelligence model, has the potential to revolutionize education. However, its effectiveness in solving non-English questions remains uncertain. This study evaluates ChatGPT's robustness using 586 Korean mathematics questions. ChatGPT achieves 66.72% accuracy, correctly answering 391 out of 586 questions. We also assess its ability to rate mathematics questions based on eleven criteria and perform a topic analysis. Our findings show that ChatGPT's ratings align with educational theory and test-taker perspectives. While ChatGPT performs well in question classification, it struggles with non-English contexts, highlighting areas for improvement. Future research should address linguistic biases and enhance accuracy across diverse languages. Domain-specific optimizations and multilingual training could improve ChatGPT's role in personalized education.

### Leveraging Dual Process Theory in Language Agent Framework for Real-time Simultaneous Human-AI Collaboration 
[[arxiv](https://arxiv.org/abs/2502.11882)] [[cool](https://papers.cool/arxiv/2502.11882)] [[pdf](https://arxiv.org/pdf/2502.11882)]
> **Authors**: Shao Zhang,Xihuai Wang,Wenhao Zhang,Chaoran Li,Junru Song,Tingyu Li,Lin Qiu,Xuezhi Cao,Xunliang Cai,Wen Yao,Weinan Zhang,Xinbing Wang,Ying Wen
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Preprint under review. Update the experimental results of the DeepSeek-R1 series models
- **标题**: None
- **领域**: 人工智能,计算语言学,人机交互,机器学习,多代理系统
- **Abstract**: Agents built on large language models (LLMs) have excelled in turn-by-turn human-AI collaboration but struggle with simultaneous tasks requiring real-time interaction. Latency issues and the challenge of inferring variable human strategies hinder their ability to make autonomous decisions without explicit instructions. Through experiments with current independent System 1 and System 2 methods, we validate the necessity of using Dual Process Theory (DPT) in real-time tasks. We propose DPT-Agent, a novel language agent framework that integrates System 1 and System 2 for efficient real-time simultaneous human-AI collaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and code-as-policy for fast, intuitive, and controllable decision-making. DPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous reflection to infer human intentions and perform reasoning-based autonomous decisions. We demonstrate the effectiveness of DPT-Agent through further experiments with rule-based agents and human collaborators, showing significant improvements over mainstream LLM-based frameworks. To the best of our knowledge, DPT-Agent is the first language agent framework that achieves successful real-time simultaneous human-AI collaboration autonomously. Code of DPT-Agent can be found in https://github.com/sjtu-marl/DPT-Agent.

### Hypothesis-Driven Theory-of-Mind Reasoning for Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.11881)] [[cool](https://papers.cool/arxiv/2502.11881)] [[pdf](https://arxiv.org/pdf/2502.11881)]
> **Authors**: Hyunwoo Kim,Melanie Sclar,Tan Zhi-Xuan,Lance Ying,Sydney Levine,Yang Liu,Joshua B. Tenenbaum,Yejin Choi
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: Existing LLM reasoning methods have shown impressive capabilities across various tasks, such as solving math and coding problems. However, applying these methods to scenarios without ground-truth answers or rule-based verification methods - such as tracking the mental states of an agent - remains challenging. Inspired by the sequential Monte Carlo algorithm, we introduce thought-tracing, an inference-time reasoning algorithm designed to trace the mental states of specific agents by generating hypotheses and weighting them based on observations without relying on ground-truth solutions to questions in datasets. Our algorithm is modeled after the Bayesian theory-of-mind framework, using LLMs to approximate probabilistic inference over agents' evolving mental states based on their perceptions and actions. We evaluate thought-tracing on diverse theory-of-mind benchmarks, demonstrating significant performance improvements compared to baseline LLMs. Our experiments also reveal interesting behaviors of the recent reasoning models - e.g., o1 and R1 - on theory-of-mind, highlighting the difference of social reasoning compared to other domains.

### AAKT: Enhancing Knowledge Tracing with Alternate Autoregressive Modeling 
[[arxiv](https://arxiv.org/abs/2502.11817)] [[cool](https://papers.cool/arxiv/2502.11817)] [[pdf](https://arxiv.org/pdf/2502.11817)]
> **Authors**: Hao Zhou,Wenge Rong,Jianfei Zhang,Qing Sun,Yuanxin Ouyang,Zhang Xiong
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: ef:IEEE Transactions onLearningTechnologies, vol. 18, pp. 25-38, 2025
- **标题**: None
- **领域**: 人工智能,计算机与社会,机器学习
- **Abstract**: Knowledge Tracing (KT) aims to predict students' future performances based on their former exercises and additional information in educational settings. KT has received significant attention since it facilitates personalized experiences in educational situations. Simultaneously, the autoregressive modeling on the sequence of former exercises has been proven effective for this task. One of the primary challenges in autoregressive modeling for Knowledge Tracing is effectively representing the anterior (pre-response) and posterior (post-response) states of learners across exercises. Existing methods often employ complex model architectures to update learner states using question and response records. In this study, we propose a novel perspective on knowledge tracing task by treating it as a generative process, consistent with the principles of autoregressive models. We demonstrate that knowledge states can be directly represented through autoregressive encodings on a question-response alternate sequence, where model generate the most probable representation in hidden state space by analyzing history interactions. This approach underpins our framework, termed Alternate Autoregressive Knowledge Tracing (AAKT). Additionally, we incorporate supplementary educational information, such as question-related skills, into our framework through an auxiliary task, and include extra exercise details, like response time, as additional inputs. Our proposed framework is implemented using advanced autoregressive technologies from Natural Language Generation (NLG) for both training and prediction. Empirical evaluations on four real-world KT datasets indicate that AAKT consistently outperforms all baseline models in terms of AUC, ACC, and RMSE. Furthermore, extensive ablation studies and visualized analysis validate the effectiveness of key components in AAKT.

### Table-Critic: A Multi-Agent Framework for Collaborative Criticism and Refinement in Table Reasoning 
[[arxiv](https://arxiv.org/abs/2502.11799)] [[cool](https://papers.cool/arxiv/2502.11799)] [[pdf](https://arxiv.org/pdf/2502.11799)]
> **Authors**: Peiying Yu,Guoxin Chen,Jingjing Wang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: Despite the remarkable capabilities of large language models (LLMs) in various reasoning tasks, they still struggle with table reasoning tasks, particularly in maintaining consistency throughout multi-step reasoning processes. While existing approaches have explored various decomposition strategies, they often lack effective mechanisms to identify and correct errors in intermediate reasoning steps, leading to cascading error propagation. To address these issues, we propose Table-Critic, a novel multi-agent framework that facilitates collaborative criticism and iterative refinement of the reasoning process until convergence to correct solutions. Our framework consists of four specialized agents: a Judge for error identification, a Critic for comprehensive critiques, a Refiner for process improvement, and a Curator for pattern distillation. To effectively deal with diverse and unpredictable error types, we introduce a self-evolving template tree that systematically accumulates critique knowledge through experience-driven learning and guides future reflections. Extensive experiments have demonstrated that Table-Critic achieves substantial improvements over existing methods, achieving superior accuracy and error correction rates while maintaining computational efficiency and lower solution degradation rate.

### Cognitive-Aligned Document Selection for Retrieval-augmented Generation 
[[arxiv](https://arxiv.org/abs/2502.11770)] [[cool](https://papers.cool/arxiv/2502.11770)] [[pdf](https://arxiv.org/pdf/2502.11770)]
> **Authors**: Bingyu Wan,Fuxi Zhang,Zhongpeng Qi,Jiayi Ding,Jijun Li,Baoshi Fan,Yijia Zhang,Jun Zhang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Large language models (LLMs) inherently display hallucinations since the precision of generated texts cannot be guaranteed purely by the parametric knowledge they include. Although retrieval-augmented generation (RAG) systems enhance the accuracy and reliability of generative models by incorporating external documents, these retrieved documents often fail to adequately support the model's responses in practical applications. To address this issue, we propose GGatrieval (Fine-\textbf{G}rained \textbf{G}rounded \textbf{A}lignment Re\textbf{trieval} for verifiable generation), which leverages an LLM to dynamically update queries and filter high-quality, reliable retrieval documents. Specifically, we parse the user query into its syntactic components and perform fine-grained grounded alignment with the retrieved documents. For query components that cannot be individually aligned, we propose a dynamic semantic compensation mechanism that iteratively refines and rewrites the query while continuously updating the retrieval results. This iterative process continues until the retrieved documents sufficiently support the query's response. Our approach introduces a novel criterion for filtering retrieved documents, closely emulating human strategies for acquiring targeted information. This ensures that the retrieved content effectively supports and verifies the generated outputs. On the ALCE benchmark, our method significantly surpasses a wide range of baselines, achieving state-of-the-art performance.

### HintsOfTruth: A Multimodal Checkworthiness Detection Dataset with Real and Synthetic Claims 
[[arxiv](https://arxiv.org/abs/2502.11753)] [[cool](https://papers.cool/arxiv/2502.11753)] [[pdf](https://arxiv.org/pdf/2502.11753)]
> **Authors**: Michiel van der Meer,Pavel Korshunov,Sébastien Marcel,Lonneke van der Plas
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Misinformation can be countered with fact-checking, but the process is costly and slow. Identifying checkworthy claims is the first step, where automation can help scale fact-checkers' efforts. However, detection methods struggle with content that is 1) multimodal, 2) from diverse domains, and 3) synthetic. We introduce HintsOfTruth, a public dataset for multimodal checkworthiness detection with $27$K real-world and synthetic image/claim pairs. The mix of real and synthetic data makes this dataset unique and ideal for benchmarking detection methods. We compare fine-tuned and prompted Large Language Models (LLMs). We find that well-configured lightweight text-based encoders perform comparably to multimodal models but the first only focus on identifying non-claim-like content. Multimodal LLMs can be more accurate but come at a significant computational cost, making them impractical for large-scale applications. When faced with synthetic data, multimodal models perform more robustly

### Energy-Conscious LLM Decoding: Impact of Text Generation Strategies on GPU Energy Consumption 
[[arxiv](https://arxiv.org/abs/2502.11723)] [[cool](https://papers.cool/arxiv/2502.11723)] [[pdf](https://arxiv.org/pdf/2502.11723)]
> **Authors**: Alireza Nik,Michael A. Riegler,Pål Halvorsen
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Decoding strategies significantly influence the quality and diversity of the generated texts in large language models (LLMs), yet their impact on computational resource consumption, particularly GPU energy usage, is insufficiently studied. This paper investigates the relationship between text generation decoding methods and energy efficiency, focusing on the trade-off between generation quality and GPU energy consumption across diverse tasks and decoding configurations. By benchmarking multiple strategies across different text generation tasks, such as Translation, Code Summarization, and Math Problem Solving, we reveal how selecting appropriate decoding techniques with their tuned hyperparameters affects text quality and has measurable implications for resource utilization, emphasizing the need for balanced optimization. To the best of our knowledge, this study is among the first to explore decoding strategies in LLMs through the lens of energy consumption, offering actionable insights for designing resource-aware applications that maintain high-quality text generation.

### VRoPE: Rotary Position Embedding for Video Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.11664)] [[cool](https://papers.cool/arxiv/2502.11664)] [[pdf](https://arxiv.org/pdf/2502.11664)]
> **Authors**: Zikang Liu,Longteng Guo,Yepeng Tang,Junxian Cai,Kai Ma,Xi Chen,Jing Liu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 10 pages
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Rotary Position Embedding (RoPE) has shown strong performance in text-based Large Language Models (LLMs), but extending it to video remains a challenge due to the intricate spatiotemporal structure of video frames. Existing adaptations, such as RoPE-3D, attempt to encode spatial and temporal dimensions separately but suffer from two major limitations: positional bias in attention distribution and disruptions in video-text transitions. To overcome these issues, we propose Video Rotary Position Embedding (VRoPE), a novel positional encoding method tailored for Video-LLMs. Our approach restructures positional indices to preserve spatial coherence and ensure a smooth transition between video and text tokens. Additionally, we introduce a more balanced encoding strategy that mitigates attention biases, ensuring a more uniform distribution of spatial focus. Extensive experiments on Vicuna and Qwen2 across different model scales demonstrate that VRoPE consistently outperforms previous RoPE variants, achieving significant improvements in video understanding, temporal reasoning, and retrieval tasks. Code will be available at https://github.com/johncaged/VRoPE

### Competing LLM Agents in a Non-Cooperative Game of Opinion Polarisation 
[[arxiv](https://arxiv.org/abs/2502.11649)] [[cool](https://papers.cool/arxiv/2502.11649)] [[pdf](https://arxiv.org/pdf/2502.11649)]
> **Authors**: Amin Qasmi,Usman Naseem,Mehwish Nasim
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: :I.6; I.2.7
- **标题**: None
- **领域**: 人工智能,社交和信息网络
- **Abstract**: We introduce a novel non-cooperative game to analyse opinion formation and resistance, incorporating principles from social psychology such as confirmation bias, resource constraints, and influence penalties. Our simulation features Large Language Model (LLM) agents competing to influence a population, with penalties imposed for generating messages that propagate or counter misinformation. This framework integrates resource optimisation into the agents' decision-making process. Our findings demonstrate that while higher confirmation bias strengthens opinion alignment within groups, it also exacerbates overall polarisation. Conversely, lower confirmation bias leads to fragmented opinions and limited shifts in individual beliefs. Investing heavily in a high-resource debunking strategy can initially align the population with the debunking agent, but risks rapid resource depletion and diminished long-term influence.

### Large Language Models and Mathematical Reasoning Failures 
[[arxiv](https://arxiv.org/abs/2502.11574)] [[cool](https://papers.cool/arxiv/2502.11574)] [[pdf](https://arxiv.org/pdf/2502.11574)]
> **Authors**: Johan Boye,Birger Moell
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: This paper investigates the mathematical reasoning capabilities of large language models (LLMs) using 50 newly constructed high-school-level word problems. Unlike prior studies that focus solely on answer correctness, we rigorously analyze both final answers and solution steps to identify reasoning failures. Evaluating eight state-of-the-art models - including Mixtral, Llama, Gemini, GPT-4o, and OpenAI's o1 variants - we find that while newer models (e.g., o3-mini, deepseek-r1) achieve higher accuracy, all models exhibit errors in spatial reasoning, strategic planning, and arithmetic, sometimes producing correct answers through flawed logic. Common failure modes include unwarranted assumptions, over-reliance on numerical patterns, and difficulty translating physical intuition into mathematical steps. Manual analysis reveals that models struggle with problems requiring multi-step deduction or real-world knowledge, despite possessing broad mathematical knowledge. Our results underscore the importance of evaluating reasoning processes, not just answers, and caution against overestimating LLMs' problem-solving proficiency. The study highlights persistent gaps in LLMs' generalization abilities, emphasizing the need for targeted improvements in structured reasoning and constraint handling.

### A Survey of Automatic Prompt Engineering: An Optimization Perspective 
[[arxiv](https://arxiv.org/abs/2502.11560)] [[cool](https://papers.cool/arxiv/2502.11560)] [[pdf](https://arxiv.org/pdf/2502.11560)]
> **Authors**: Wenwu Li,Xiangfeng Wang,Wenhao Li,Bo Jin
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 19 pages, 4 figures
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: The rise of foundation models has shifted focus from resource-intensive fine-tuning to prompt engineering, a paradigm that steers model behavior through input design rather than weight updates. While manual prompt engineering faces limitations in scalability, adaptability, and cross-modal alignment, automated methods, spanning foundation model (FM) based optimization, evolutionary methods, gradient-based optimization, and reinforcement learning, offer promising solutions. Existing surveys, however, remain fragmented across modalities and methodologies. This paper presents the first comprehensive survey on automated prompt engineering through a unified optimization-theoretic lens. We formalize prompt optimization as a maximization problem over discrete, continuous, and hybrid prompt spaces, systematically organizing methods by their optimization variables (instructions, soft prompts, exemplars), task-specific objectives, and computational frameworks. By bridging theoretical formulation with practical implementations across text, vision, and multimodal domains, this survey establishes a foundational framework for both researchers and practitioners, while highlighting underexplored frontiers in constrained optimization and agent-oriented prompt design.

### Equilibrate RLHF: Towards Balancing Helpfulness-Safety Trade-off in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.11555)] [[cool](https://papers.cool/arxiv/2502.11555)] [[pdf](https://arxiv.org/pdf/2502.11555)]
> **Authors**: Yingshui Tan,Yilei Jiang,Yanshi Li,Jiaheng Liu,Xingyuan Bu,Wenbo Su,Xiangyu Yue,Xiaoyong Zhu,Bo Zheng
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Fine-tuning large language models (LLMs) based on human preferences, commonly achieved through reinforcement learning from human feedback (RLHF), has been effective in improving their performance. However, maintaining LLM safety throughout the fine-tuning process remains a significant challenge, as resolving conflicts between safety and helpfulness can be non-trivial. Typically, the safety alignment of LLM is trained on data with safety-related categories. However, our experiments find that naively increasing the scale of safety training data usually leads the LLMs to an ``overly safe'' state rather than a ``truly safe'' state, boosting the refusal rate through extensive safety-aligned data without genuinely understanding the requirements for safe responses. Such an approach can inadvertently diminish the models' helpfulness. To understand the phenomenon, we first investigate the role of safety data by categorizing them into three different groups, and observe that each group behaves differently as training data scales up. To boost the balance between safety and helpfulness, we propose an Equilibrate RLHF framework including a Fine-grained Data-centric (FDC) approach that achieves better safety alignment even with fewer training data, and an Adaptive Message-wise Alignment (AMA) approach, which selectively highlight the key segments through a gradient masking strategy. Extensive experimental results demonstrate that our approach significantly enhances the safety alignment of LLMs while balancing safety and helpfulness.

### A Survey of Personalized Large Language Models: Progress and Future Directions 
[[arxiv](https://arxiv.org/abs/2502.11528)] [[cool](https://papers.cool/arxiv/2502.11528)] [[pdf](https://arxiv.org/pdf/2502.11528)]
> **Authors**: Jiahong Liu,Zexuan Qiu,Zhongyang Li,Quanyu Dai,Jieming Zhu,Minda Hu,Menglin Yang,Irwin King
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 7pages, 5 figures, Under Review
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Large Language Models (LLMs) excel in handling general knowledge tasks, yet they struggle with user-specific personalization, such as understanding individual emotions, writing styles, and preferences. Personalized Large Language Models (PLLMs) tackle these challenges by leveraging individual user data, such as user profiles, historical dialogues, content, and interactions, to deliver responses that are contextually relevant and tailored to each user's specific needs. This is a highly valuable research topic, as PLLMs can significantly enhance user satisfaction and have broad applications in conversational agents, recommendation systems, emotion recognition, medical assistants, and more. This survey reviews recent advancements in PLLMs from three technical perspectives: prompting for personalized context (input level), finetuning for personalized adapters (model level), and alignment for personalized preferences (objective level). To provide deeper insights, we also discuss current limitations and outline several promising directions for future research. Updated information about this survey can be found at the https://github.com/JiahongLiu21/Awesome-Personalized-Large-Language-Models.

### Why Vision Language Models Struggle with Visual Arithmetic? Towards Enhanced Chart and Geometry Understanding 
[[arxiv](https://arxiv.org/abs/2502.11492)] [[cool](https://papers.cool/arxiv/2502.11492)] [[pdf](https://arxiv.org/pdf/2502.11492)]
> **Authors**: Kung-Hsiang Huang,Can Qin,Haoyi Qiu,Philippe Laban,Shafiq Joty,Caiming Xiong,Chien-Sheng Wu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学,计算机视觉和模式识别
- **Abstract**: Vision Language Models (VLMs) have achieved remarkable progress in multimodal tasks, yet they often struggle with visual arithmetic, seemingly simple capabilities like object counting or length comparison, which are essential for relevant complex tasks like chart understanding and geometric reasoning. In this work, we first investigate the root causes of this deficiency through a suite of probing tasks focusing on basic visual arithmetic. Our analysis reveals that while pre-trained vision encoders typically capture sufficient information, the text decoder often fails to decode it correctly for arithmetic reasoning. To address this, we propose CogAlign, a novel post-training strategy inspired by Piaget's theory of cognitive development. CogAlign trains VLMs to recognize invariant properties under visual transformations. We demonstrate that this approach significantly improves the performance of three diverse VLMs on our proposed probing tasks. Furthermore, CogAlign enhances performance by an average of 4.6% on CHOCOLATE and 2.9% on MATH-VISION, outperforming or matching supervised fine-tuning methods while requiring only 60% less training data. These results highlight the effectiveness and generalizability of CogAlign in improving fundamental visual arithmetic capabilities and their transfer to downstream tasks.

### AGrail: A Lifelong Agent Guardrail with Effective and Adaptive Safety Detection 
[[arxiv](https://arxiv.org/abs/2502.11448)] [[cool](https://papers.cool/arxiv/2502.11448)] [[pdf](https://arxiv.org/pdf/2502.11448)]
> **Authors**: Weidi Luo,Shenghong Dai,Xiaogeng Liu,Suman Banerjee,Huan Sun,Muhao Chen,Chaowei Xiao
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: The rapid advancements in Large Language Models (LLMs) have enabled their deployment as autonomous agents for handling complex tasks in dynamic environments. These LLMs demonstrate strong problem-solving capabilities and adaptability to multifaceted scenarios. However, their use as agents also introduces significant risks, including task-specific risks, which are identified by the agent administrator based on the specific task requirements and constraints, and systemic risks, which stem from vulnerabilities in their design or interactions, potentially compromising confidentiality, integrity, or availability (CIA) of information and triggering security risks. Existing defense agencies fail to adaptively and effectively mitigate these risks. In this paper, we propose AGrail, a lifelong agent guardrail to enhance LLM agent safety, which features adaptive safety check generation, effective safety check optimization, and tool compatibility and flexibility. Extensive experiments demonstrate that AGrail not only achieves strong performance against task-specific and system risks but also exhibits transferability across different LLM agents' tasks.

## 计算工程、金融和科学(cs.CE:Computational Engineering, Finance, and Science)

### Qubit-Based Framework for Quantum Machine Learning: Bridging Classical Data and Quantum Algorithms 
[[arxiv](https://arxiv.org/abs/2502.11951)] [[cool](https://papers.cool/arxiv/2502.11951)] [[pdf](https://arxiv.org/pdf/2502.11951)]
> **Authors**: Bhavna Bose,Saurav Verma
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算工程、金融和科学,机器学习,量子物理学
- **Abstract**: This paper dives into the exciting and rapidly growing field of quantum computing, explaining its core ideas, current progress, and how it could revolutionize the way we solve complex problems. It starts by breaking down the basics, like qubits, quantum circuits, and how principles like superposition and entanglement make quantum computers fundamentally different-and far more powerful for certain tasks-than the classical computers we use today. We also explore how quantum computing deals with complex problems and why it is uniquely suited for challenges classical systems struggle to handle. A big part of this paper focuses on Quantum Machine Learning (QML), where the strengths of quantum computing meet the world of artificial intelligence. By processing massive datasets and optimizing intricate algorithms, quantum systems offer new possibilities for machine learning. We highlight different approaches to combining quantum and classical computing, showing how they can work together to produce faster and more accurate results. Additionally, we explore the tools and platforms available-like TensorFlow Quantum, Qiskit and PennyLane-that are helping researchers and developers bring these theories to life. Of course, quantum computing has its hurdles. Challenges like scaling up hardware, correcting errors, and keeping qubits stable are significant roadblocks. Yet, with rapid advancements in cloud-based platforms and innovative technologies, the potential of quantum computing feels closer than ever. This paper aims to offer readers a clear and comprehensive introduction to quantum computing, its role in machine learning, and the immense possibilities it holds for the future of technology.

## 计算语言学(cs.CL:Computation and Language)

### Policy-to-Language: Train LLMs to Explain Decisions with Flow-Matching Generated Rewards 
[[arxiv](https://arxiv.org/abs/2502.12530)] [[cool](https://papers.cool/arxiv/2502.12530)] [[pdf](https://arxiv.org/pdf/2502.12530)]
> **Authors**: Xinyi Yang,Liang Zeng,Heng Dong,Chao Yu,Xiaoran Wu,Huazhong Yang,Yu Wang,Milind Tambe,Tonghan Wang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: As humans increasingly share environments with diverse agents powered by RL, LLMs, and beyond, the ability to explain their policies in natural language will be vital for reliable coexistence. In this paper, we build a model-agnostic explanation generator based on an LLM. The technical novelty is that the rewards for training this LLM are generated by a generative flow matching model. This model has a specially designed structure with a hidden layer merged with an LLM to harness the linguistic cues of explanations into generating appropriate rewards. Experiments on both RL and LLM tasks demonstrate that our method can generate dense and effective rewards while saving on expensive human feedback; it thus enables effective explanations and even improves the accuracy of the decisions in original tasks.

### Can LLMs Extract Frame-Semantic Arguments? 
[[arxiv](https://arxiv.org/abs/2502.12516)] [[cool](https://papers.cool/arxiv/2502.12516)] [[pdf](https://arxiv.org/pdf/2502.12516)]
> **Authors**: Jacob Devasier,Rishabh Mediratta,Chengkai Li
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Frame-semantic parsing is a critical task in natural language understanding, yet the ability of large language models (LLMs) to extract frame-semantic arguments remains underexplored. This paper presents a comprehensive evaluation of LLMs on frame-semantic argument identification, analyzing the impact of input representation formats, model architectures, and generalization to unseen and out-of-domain samples. Our experiments, spanning models from 0.5B to 78B parameters, reveal that JSON-based representations significantly enhance performance, and while larger models generally perform better, smaller models can achieve competitive results through fine-tuning. We also introduce a novel approach to frame identification leveraging predicted frame elements, achieving state-of-the-art performance on ambiguous targets. Despite strong generalization capabilities, our analysis finds that LLMs still struggle with out-of-domain data.

### Aspect-Guided Multi-Level Perturbation Analysis of Large Language Models in Automated Peer Review 
[[arxiv](https://arxiv.org/abs/2502.12510)] [[cool](https://papers.cool/arxiv/2502.12510)] [[pdf](https://arxiv.org/pdf/2502.12510)]
> **Authors**: Jiatao Li,Yanheng Li,Xinyu Hu,Mingqi Gao,Xiaojun Wan
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Under Review
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: We propose an aspect-guided, multi-level perturbation framework to evaluate the robustness of Large Language Models (LLMs) in automated peer review. Our framework explores perturbations in three key components of the peer review process-papers, reviews, and rebuttals-across several quality aspects, including contribution, soundness, presentation, tone, and completeness. By applying targeted perturbations and examining their effects on both LLM-as-Reviewer and LLM-as-Meta-Reviewer, we investigate how aspect-based manipulations, such as omitting methodological details from papers or altering reviewer conclusions, can introduce significant biases in the review process. We identify several potential vulnerabilities: review conclusions that recommend a strong reject may significantly influence meta-reviews, negative or misleading reviews may be wrongly interpreted as thorough, and incomplete or hostile rebuttals can unexpectedly lead to higher acceptance rates. Statistical tests show that these biases persist under various Chain-of-Thought prompting strategies, highlighting the lack of robust critical evaluation in current LLMs. Our framework offers a practical methodology for diagnosing these vulnerabilities, thereby contributing to the development of more reliable and robust automated reviewing systems.

### LegalCore: A Dataset for Legal Documents Event Coreference Resolution 
[[arxiv](https://arxiv.org/abs/2502.12509)] [[cool](https://papers.cool/arxiv/2502.12509)] [[pdf](https://arxiv.org/pdf/2502.12509)]
> **Authors**: Kangda Wei,Xi Shi,Jonathan Tong,Sai Ramana Reddy,Anandhavelu Natarajan,Rajiv Jain,Aparna Garimella,Ruihong Huang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Recognizing events and their coreferential mentions in a document is essential for understanding semantic meanings of text. The existing research on event coreference resolution is mostly limited to news articles. In this paper, we present the first dataset for the legal domain, LegalCore, which has been annotated with comprehensive event and event coreference information. The legal contract documents we annotated in this dataset are several times longer than news articles, with an average length of around 25k tokens per document. The annotations show that legal documents have dense event mentions and feature both short-distance and super long-distance coreference links between event mentions. We further benchmark mainstream Large Language Models (LLMs) on this dataset for both event detection and event coreference resolution tasks, and find that this dataset poses significant challenges for state-of-the-art open-source and proprietary LLMs, which perform significantly worse than a supervised baseline. We will publish the dataset as well as the code.

### Efficient OpAmp Adaptation for Zoom Attention to Golden Contexts 
[[arxiv](https://arxiv.org/abs/2502.12502)] [[cool](https://papers.cool/arxiv/2502.12502)] [[pdf](https://arxiv.org/pdf/2502.12502)]
> **Authors**: Haoyuan Wu,Rui Ming,Haisheng Zheng,Zhuolun He,Bei Yu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) have shown significant promise in question-answering (QA) tasks, particularly in retrieval-augmented generation (RAG) scenarios and long-context applications. However, their performance is hindered by noisy reference documents, which often distract from essential information. Despite fine-tuning efforts, Transformer-based architectures struggle to prioritize relevant content. This is evidenced by their tendency to allocate disproportionate attention to irrelevant or later-positioned documents. Recent work proposes the differential attention mechanism to address this issue, but this mechanism is limited by an unsuitable common-mode rejection ratio (CMRR) and high computational costs. Inspired by the operational amplifier (OpAmp), we propose the OpAmp adaptation to address these challenges, which is implemented with adapters efficiently. By integrating the adapter into pre-trained Transformer blocks, our approach enhances focus on the golden context without costly training from scratch. Empirical evaluations on noisy-context benchmarks reveal that our Qwen2.5-OpAmp-72B model, trained with our OpAmp adaptation, surpasses the performance of state-of-the-art LLMs, including DeepSeek-V3 and GPT-4o.

### Crowd Comparative Reasoning: Unlocking Comprehensive Evaluations for LLM-as-a-Judge 
[[arxiv](https://arxiv.org/abs/2502.12501)] [[cool](https://papers.cool/arxiv/2502.12501)] [[pdf](https://arxiv.org/pdf/2502.12501)]
> **Authors**: Qiyuan Zhang,Yufei Wang,Yuxin Jiang,Liangyou Li,Chuhan Wu,Yasheng Wang,Xin Jiang,Lifeng Shang,Ruiming Tang,Fuyuan Lyu,Chen Ma
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: LLM-as-a-Judge, which generates chain-of-thought (CoT) judgments, has become a widely adopted auto-evaluation method. However, its reliability is compromised by the CoT reasoning's inability to capture comprehensive and deeper details, often leading to incomplete outcomes. Existing methods mainly rely on majority voting or criteria expansion, which is insufficient to address the limitation in CoT. We propose Crowd-based Comparative Evaluation, which introduces additional crowd responses to compare with the candidate responses, thereby exposing deeper and more comprehensive details within the candidate responses. This process effectively guides LLM-as-a-Judge to provide a more detailed CoT judgment. Extensive experiments demonstrate that our approach enhances evaluation reliability, achieving an average accuracy gain of 6.7% across five benchmarks. Moreover, our method produces higher-quality CoTs that facilitate judge distillation and exhibit superior performance in rejection sampling for supervised fine-tuning (SFT), referred to as crowd rejection sampling, thereby enabling more efficient SFT. Our analysis confirms that CoTs generated by ours are more comprehensive and of higher quality, and evaluation accuracy improves as inference scales.

### UniGenCoder: Merging Seq2Seq and Seq2Tree Paradigms for Unified Code Generation 
[[arxiv](https://arxiv.org/abs/2502.12490)] [[cool](https://papers.cool/arxiv/2502.12490)] [[pdf](https://arxiv.org/pdf/2502.12490)]
> **Authors**: Liangying Shao,Yanfu Yan,Denys Poshyvanyk,Jinsong Su
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: accepted to 47th International Conference on Software Engineering (ICSE 2025), NIER track
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Deep learning-based code generation has completely transformed the way developers write programs today. Existing approaches to code generation have focused either on the Sequence-to-Sequence paradigm, which generates target code as a sequence of tokens, or the Sequence-to-Tree paradigm, which outputs code as a sequence of actions. While these two paradigms are intuitively complementary, their combination has not been previously explored. By comparing the code generated under these two paradigms, we find that integrating them holds significant potential. In this paper, we propose UniGenCoder for code-related generation tasks, which consists of a shared encoder, a shared decoder with a minimal set of additional parameters to unify two paradigms, and a selector that dynamically chooses optimal paradigm for each instance. Also, during the model training, we first perform the multi-task learning and distillation strategies to facilitate knowledge transfer between two paradigms, and then leverage contrastive learning to train the selector. Experimental results on the text-to-code and code-to-code generation tasks demonstrate the effectiveness of our proposed model. We release our code at https://github.com/DeepLearnXMU/UniGenCoder.

### EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.12486)] [[cool](https://papers.cool/arxiv/2502.12486)] [[pdf](https://arxiv.org/pdf/2502.12486)]
> **Authors**: Xiaoqian Liu,Ke Wang,Yongbin Li,Yuchuan Wu,Wentao Ma,Aobo Kong,Fei Huang,Jianbin Jiao,Junge Zhang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 9 pages, 4 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models (LLMs) have shown impressive reasoning capabilities in well-defined problems with clear solutions, such as mathematics and coding. However, they still struggle with complex real-world scenarios like business negotiations, which require strategic reasoning-an ability to navigate dynamic environments and align long-term goals amidst uncertainty. Existing methods for strategic reasoning face challenges in adaptability, scalability, and transferring strategies to new contexts. To address these issues, we propose explicit policy optimization (EPO) for strategic reasoning, featuring an LLM that provides strategies in open-ended action space and can be plugged into arbitrary LLM agents to motivate goal-directed behavior. To improve adaptability and policy transferability, we train the strategic reasoning model via multi-turn reinforcement learning (RL) using process rewards and iterative self-play, without supervised fine-tuning (SFT) as a preliminary step. Experiments across social and physical domains demonstrate EPO's ability of long-term goal alignment through enhanced strategic reasoning, achieving state-of-the-art performance on social dialogue and web navigation tasks. Our findings reveal various collaborative reasoning mechanisms emergent in EPO and its effectiveness in generating novel strategies, underscoring its potential for strategic reasoning in real-world applications.

### Safe at the Margins: A General Approach to Safety Alignment in Low-Resource English Languages -- A Singlish Case Study 
[[arxiv](https://arxiv.org/abs/2502.12485)] [[cool](https://papers.cool/arxiv/2502.12485)] [[pdf](https://arxiv.org/pdf/2502.12485)]
> **Authors**: Isaac Lim,Shaun Khoo,Watson Chua,Goh Jiayi,Jessica Foo
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: To ensure safe usage, Large Language Models (LLMs) typically undergo alignment with human-defined values. However, this alignment often relies on primarily English data and is biased towards Western-centric values, limiting its effectiveness in low-resource language settings. In this paper, we describe our approach for aligning SEA-Lion-v2.1-Instruct (a Llama3-8B variant) to minimize toxicity in Singlish, an English creole specific to Singapore. We find that supervised fine-tuning and Kahneman-Tversky Optimization (KTO) on paired and unpaired preferences is more sample efficient and yields significantly better results than Direct Preference Optimization (DPO). Our analysis reveals that DPO implicitly enforces a weaker safety objective than KTO, and that SFT complements KTO by improving training stability. Finally, we introduce a simple but novel modification to KTO, KTO-S, which improves training stability through better gradient exploitation. Overall, we present a general approach for safety alignment conducive to low-resource English languages, successfully reducing toxicity by 99\% on our Singlish benchmark, with gains generalizing to the broader TOXIGEN dataset while maintaining strong performance across standard LLM benchmarks.

### The Knowledge Microscope: Features as Better Analytical Lenses than Neurons 
[[arxiv](https://arxiv.org/abs/2502.12483)] [[cool](https://papers.cool/arxiv/2502.12483)] [[pdf](https://arxiv.org/pdf/2502.12483)]
> **Authors**: Yuheng Chen,Pengfei Cao,Kang Liu,Jun Zhao
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: ARR February UnderReview
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Previous studies primarily utilize MLP neurons as units of analysis for understanding the mechanisms of factual knowledge in Language Models (LMs); however, neurons suffer from polysemanticity, leading to limited knowledge expression and poor interpretability. In this paper, we first conduct preliminary experiments to validate that Sparse Autoencoders (SAE) can effectively decompose neurons into features, which serve as alternative analytical units. With this established, our core findings reveal three key advantages of features over neurons: (1) Features exhibit stronger influence on knowledge expression and superior interpretability. (2) Features demonstrate enhanced monosemanticity, showing distinct activation patterns between related and unrelated facts. (3) Features achieve better privacy protection than neurons, demonstrated through our proposed FeatureEdit method, which significantly outperforms existing neuron-based approaches in erasing privacy-sensitive information from LMs.Code and dataset will be available.

### MSE-Adapter: A Lightweight Plugin Endowing LLMs with the Capability to Perform Multimodal Sentiment Analysis and Emotion Recognition 
[[arxiv](https://arxiv.org/abs/2502.12478)] [[cool](https://papers.cool/arxiv/2502.12478)] [[pdf](https://arxiv.org/pdf/2502.12478)]
> **Authors**: Yang Yang,Xunde Dong,Yupeng Qiang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Current Multimodal Sentiment Analysis (MSA) and Emotion Recognition in Conversations (ERC) methods based on pre-trained language models exhibit two primary limitations: 1) Once trained for MSA and ERC tasks, these pre-trained language models lose their original generalized capabilities. 2) They demand considerable computational resources. As the size of pre-trained language models continues to grow, training larger multimodal sentiment analysis models using previous approaches could result in unnecessary computational cost. In response to this challenge, we propose \textbf{M}ultimodal \textbf{S}entiment Analysis and \textbf{E}motion Recognition \textbf{Adapter} (MSE-Adapter), a lightweight and adaptable plugin. This plugin enables a large language model (LLM) to carry out MSA or ERC tasks with minimal computational overhead (only introduces approximately 2.6M to 2.8M trainable parameters upon the 6/7B models), while preserving the intrinsic capabilities of the LLM. In the MSE-Adapter, the Text-Guide-Mixer (TGM) module is introduced to establish explicit connections between non-textual and textual modalities through the Hadamard product. This allows non-textual modalities to better align with textual modalities at the feature level, promoting the generation of higher-quality pseudo tokens. Extensive experiments were conducted on four public English and Chinese datasets using consumer-grade GPUs and open-source LLMs (Qwen-1.8B, ChatGLM3-6B-base, and LLaMA2-7B) as the backbone. The results demonstrate the effectiveness of the proposed plugin. The code will be released on GitHub after a blind review.

### Savaal: Scalable Concept-Driven Question Generation to Enhance Human Learning 
[[arxiv](https://arxiv.org/abs/2502.12477)] [[cool](https://papers.cool/arxiv/2502.12477)] [[pdf](https://arxiv.org/pdf/2502.12477)]
> **Authors**: Kimia Noorbakhsh,Joseph Chandler,Pantea Karimi,Mohammad Alizadeh,Hari Balakrishnan
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Kimia Noorbakhsh, Joseph Chandler, and Pantea Karimi contributed equally to the work
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Assessing and enhancing human learning through question-answering is vital, yet automating this process remains challenging. While large language models (LLMs) excel at summarization and query responses, their ability to generate meaningful questions for learners is underexplored. We propose Savaal, a scalable question-generation system with three objectives: (i) scalability, enabling question generation from hundreds of pages of text (ii) depth of understanding, producing questions beyond factual recall to test conceptual reasoning, and (iii) domain-independence, automatically generating questions across diverse knowledge areas. Instead of providing an LLM with large documents as context, Savaal improves results with a three-stage processing pipeline. Our evaluation with 76 human experts on 71 papers and PhD dissertations shows that Savaal generates questions that better test depth of understanding by 6.5X for dissertations and 1.5X for papers compared to a direct-prompting LLM baseline. Notably, as document length increases, Savaal's advantages in higher question quality and lower cost become more pronounced.

### CoCo-CoLa: Evaluating Language Adherence in Multilingual LLMs 
[[arxiv](https://arxiv.org/abs/2502.12476)] [[cool](https://papers.cool/arxiv/2502.12476)] [[pdf](https://arxiv.org/pdf/2502.12476)]
> **Authors**: Elnaz Rahmati,Alireza S. Ziabari,Morteza Dehghani
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 13 pages, 7 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Multilingual Large Language Models (LLMs) develop cross-lingual abilities despite being trained on limited parallel data. However, they often struggle to generate responses in the intended language, favoring high-resource languages such as English. In this work, we introduce CoCo-CoLa (Correct Concept - Correct Language), a novel metric to evaluate language adherence in multilingual LLMs. Using fine-tuning experiments on a closed-book QA task across seven languages, we analyze how training in one language affects others' performance. Our findings reveal that multilingual models share task knowledge across languages but exhibit biases in the selection of output language. We identify language-specific layers, showing that final layers play a crucial role in determining output language. Accordingly, we propose a partial training strategy that selectively fine-tunes key layers, improving language adherence while significantly reducing computational cost. Our method achieves comparable or superior performance to full fine-tuning, particularly for low-resource languages, offering a more efficient multilingual adaptation.

### Reasoning on a Spectrum: Aligning LLMs to System 1 and System 2 Thinking 
[[arxiv](https://arxiv.org/abs/2502.12470)] [[cool](https://papers.cool/arxiv/2502.12470)] [[pdf](https://arxiv.org/pdf/2502.12470)]
> **Authors**: Alireza S. Ziabari,Nona Ghazizadeh,Zhivar Sourati,Farzan Karimi-Malekabadi,Payam Piray,Morteza Dehghani
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models (LLMs) exhibit impressive reasoning abilities, yet their reliance on structured step-by-step processing reveals a critical limitation. While human cognition fluidly adapts between intuitive, heuristic (System 1) and analytical, deliberative (System 2) reasoning depending on the context, LLMs lack this dynamic flexibility. This rigidity can lead to brittle and unreliable performance when faced with tasks that deviate from their trained patterns. To address this, we create a dataset of 2,000 samples with valid System 1 and System 2 answers, explicitly align LLMs with these reasoning styles, and evaluate their performance across reasoning benchmarks. Our results reveal an accuracy-efficiency trade-off: System 2-aligned models excel in arithmetic and symbolic reasoning, while System 1-aligned models perform better in commonsense tasks. A mechanistic analysis of model responses shows that System 1 models employ more definitive answers, whereas System 2 models demonstrate greater uncertainty. Interpolating between these extremes produces a monotonic transition in reasoning accuracy, preserving coherence. This work challenges the assumption that step-by-step reasoning is always optimal and highlights the need for adapting reasoning strategies based on task demands.

### SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.12464)] [[cool](https://papers.cool/arxiv/2502.12464)] [[pdf](https://arxiv.org/pdf/2502.12464)]
> **Authors**: Seanie Lee,Dong Bok Lee,Dominik Wagner,Minki Kang,Haebin Seong,Tobias Bocklet,Juho Lee,Sung Ju Hwang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 9 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Deploying large language models (LLMs) in real-world applications requires robust safety guard models to detect and block harmful user prompts. While large safety guard models achieve strong performance, their computational cost is substantial. To mitigate this, smaller distilled models are used, but they often underperform on "hard" examples where the larger model provides accurate predictions. We observe that many inputs can be reliably handled by the smaller model, while only a small fraction require the larger model's capacity. Motivated by this, we propose SafeRoute, a binary router that distinguishes hard examples from easy ones. Our method selectively applies the larger safety guard model to the data that the router considers hard, improving efficiency while maintaining accuracy compared to solely using the larger safety guard model. Experimental results on multiple benchmark datasets demonstrate that our adaptive model selection significantly enhances the trade-off between computational cost and safety performance, outperforming relevant baselines.

### Emulating Retrieval Augmented Generation via Prompt Engineering for Enhanced Long Context Comprehension in LLMs 
[[arxiv](https://arxiv.org/abs/2502.12462)] [[cool](https://papers.cool/arxiv/2502.12462)] [[pdf](https://arxiv.org/pdf/2502.12462)]
> **Authors**: Joon Park,Kyohei Atarashi,Koh Takeuchi,Hisashi Kashima
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 11 pages, 2 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: This paper addresses the challenge of comprehending very long contexts in Large Language Models (LLMs) by proposing a method that emulates Retrieval Augmented Generation (RAG) through specialized prompt engineering and chain-of-thought (CoT) reasoning. While recent LLMs support over 100,000 tokens in a single prompt, simply enlarging context windows has not guaranteed robust multi-hop reasoning when key details are scattered across massive input. Our approach treats the model as both the retriever and the reasoner: it first tags relevant segments within a long passage, then employs a stepwise CoT workflow to integrate these pieces of evidence. This single-pass method thereby reduces reliance on an external retriever, yet maintains focus on crucial segments. We evaluate our approach on selected tasks from BABILong, which interleaves standard bAbI QA problems with large amounts of distractor text. Compared to baseline (no retrieval) and naive RAG pipelines, our approach more accurately handles multi-fact questions such as object location tracking, counting, and indefinite knowledge. Furthermore, we analyze how prompt structure, including the order of question, relevant-text tags, and overall instructions, significantly affects performance. These findings underscore that optimized prompt engineering, combined with guided reasoning, can enhance LLMs' long-context comprehension and serve as a lightweight alternative to traditional retrieval pipelines.

### Stress Testing Generalization: How Minor Modifications Undermine Large Language Model Performance 
[[arxiv](https://arxiv.org/abs/2502.12459)] [[cool](https://papers.cool/arxiv/2502.12459)] [[pdf](https://arxiv.org/pdf/2502.12459)]
> **Authors**: Guangxiang Zhao,Saier Hu,Xiaoqi Jian,Jinzhu Wu,Yuhan Wu,Change Jia,Lin Sun,Xiangzheng Zhang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Submitted to ACL 2025 theme track on the Generalization of NLP models
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: This paper investigates the fragility of Large Language Models (LLMs) in generalizing to novel inputs, specifically focusing on minor perturbations in well-established benchmarks (e.g., slight changes in question format or distractor length). Despite high benchmark scores, LLMs exhibit significant accuracy drops and unexpected biases (e.g., preference for longer distractors) when faced with these minor but content-preserving modifications. For example, Qwen 2.5 1.5B's MMLU score rises from 60 to 89 and drops from 89 to 36 when option lengths are changed without altering the question. Even GPT-4 experiences a 25-point accuracy loss when question types are changed, with a 6-point drop across all three modification categories. These analyses suggest that LLMs rely heavily on superficial cues rather than forming robust, abstract representations that generalize across formats, lexical variations, and irrelevant content shifts. This work aligns with the ACL 2025 theme track on the Generalization of NLP models, proposing a "Generalization Stress Test" to assess performance shifts under controlled perturbations. The study calls for reevaluating benchmarks and developing more reliable evaluation methodologies to capture LLM generalization abilities better.

### An Empirical Evaluation of Encoder Architectures for Fast Real-Time Long Conversational Understanding 
[[arxiv](https://arxiv.org/abs/2502.12458)] [[cool](https://papers.cool/arxiv/2502.12458)] [[pdf](https://arxiv.org/pdf/2502.12458)]
> **Authors**: Annamalai Senthilnathan,Kristjan Arumae,Mohammed Khalilia,Zhengzheng Xing,Aaron R. Colak
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Analyzing long text data such as customer call transcripts is a cost-intensive and tedious task. Machine learning methods, namely Transformers, are leveraged to model agent-customer interactions. Unfortunately, Transformers adhere to fixed-length architectures and their self-attention mechanism scales quadratically with input length. Such limitations make it challenging to leverage traditional Transformers for long sequence tasks, such as conversational understanding, especially in real-time use cases. In this paper we explore and evaluate recently proposed efficient Transformer variants (e.g. Performer, Reformer) and a CNN-based architecture for real-time and near real-time long conversational understanding tasks. We show that CNN-based models are dynamic, ~2.6x faster to train, ~80% faster inference and ~72% more memory efficient compared to Transformers on average. Additionally, we evaluate the CNN model using the Long Range Arena benchmark to demonstrate competitiveness in general long document analysis.

### DSMoE: Matrix-Partitioned Experts with Dynamic Routing for Computation-Efficient Dense LLMs 
[[arxiv](https://arxiv.org/abs/2502.12455)] [[cool](https://papers.cool/arxiv/2502.12455)] [[pdf](https://arxiv.org/pdf/2502.12455)]
> **Authors**: Minxuan Lv,Zhenpeng Su,Leiyu Pan,Yizhe Xiong,Zijia Lin,Hui Chen,Wei Zhou,Jungong Han,Guiguang Ding,Cheng Luo,Di Zhang,Kun Gai,Songlin Hu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: As large language models continue to scale, computational costs and resource consumption have emerged as significant challenges. While existing sparsification methods like pruning reduce computational overhead, they risk losing model knowledge through parameter removal. This paper proposes DSMoE (Dynamic Sparse Mixture-of-Experts), a novel approach that achieves sparsification by partitioning pre-trained FFN layers into computational blocks. We implement adaptive expert routing using sigmoid activation and straight-through estimators, enabling tokens to flexibly access different aspects of model knowledge based on input complexity. Additionally, we introduce a sparsity loss term to balance performance and computational efficiency. Extensive experiments on LLaMA models demonstrate that under equivalent computational constraints, DSMoE achieves superior performance compared to existing pruning and MoE approaches across language modeling and downstream tasks, particularly excelling in generation tasks. Analysis reveals that DSMoE learns distinctive layerwise activation patterns, providing new insights for future MoE architecture design.

### Multi-Attribute Steering of Language Models via Targeted Intervention 
[[arxiv](https://arxiv.org/abs/2502.12446)] [[cool](https://papers.cool/arxiv/2502.12446)] [[pdf](https://arxiv.org/pdf/2502.12446)]
> **Authors**: Duy Nguyen,Archiki Prasad,Elias Stengel-Eskin,Mohit Bansal
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 15 pages, code link: https://github.com/duykhuongnguyen/MAT-Steer
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Inference-time intervention (ITI) has emerged as a promising method for steering large language model (LLM) behavior in a particular direction (e.g., improving helpfulness) by intervening on token representations without costly updates to the LLM's parameters. However, existing ITI approaches fail to scale to multi-attribute settings with conflicts, such as enhancing helpfulness while also reducing toxicity. To address this, we introduce Multi-Attribute Targeted Steering (MAT-Steer), a novel steering framework designed for selective token-level intervention across multiple attributes. MAT-Steer learns steering vectors using an alignment objective that shifts the model's internal representations of undesirable outputs closer to those of desirable ones while enforcing sparsity and orthogonality among vectors for different attributes, thereby reducing inter-attribute conflicts. We evaluate MAT-Steer in two distinct settings: (i) on question answering (QA) tasks where we balance attributes like truthfulness, bias, and toxicity; (ii) on generative tasks where we simultaneously improve attributes like helpfulness, correctness, and coherence. MAT-Steer outperforms existing ITI and parameter-efficient finetuning approaches across both task types (e.g., 3% average accuracy gain across QA tasks and 55.82% win rate against the best ITI baseline).

### Should I Trust You? Detecting Deception in Negotiations using Counterfactual RL 
[[arxiv](https://arxiv.org/abs/2502.12436)] [[cool](https://papers.cool/arxiv/2502.12436)] [[pdf](https://arxiv.org/pdf/2502.12436)]
> **Authors**: Wichayaporn Wongkamjan,Yanze Wang,Feng Gu,Denis Peskoff,Jonathan K. Kummerfeld,Jonathan May,Jordan Lee Boyd-Graber
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: An increasingly prevalent socio-technical problem is people being taken in by offers that sound ``too good to be true'', where persuasion and trust shape decision-making. This paper investigates how \abr{ai} can help detect these deceptive scenarios. We analyze how humans strategically deceive each other in \textit{Diplomacy}, a board game that requires both natural language communication and strategic reasoning. This requires extracting logical forms of proposed agreements in player communications and computing the relative rewards of the proposal using agents' value functions. Combined with text-based features, this can improve our deception detection. Our method detects human deception with a high precision when compared to a Large Language Model approach that flags many true messages as deceptive. Future human-\abr{ai} interaction tools can build on our methods for deception detection by triggering \textit{friction} to give users a chance of interrogating suspicious proposals.

### Wi-Chat: Large Language Model Powered Wi-Fi Sensing 
[[arxiv](https://arxiv.org/abs/2502.12421)] [[cool](https://papers.cool/arxiv/2502.12421)] [[pdf](https://arxiv.org/pdf/2502.12421)]
> **Authors**: Haopeng Zhang,Yili Ren,Haohan Yuan,Jingzhe Zhang,Yitong Shen
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Recent advancements in Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse tasks. However, their potential to integrate physical model knowledge for real-world signal interpretation remains largely unexplored. In this work, we introduce Wi-Chat, the first LLM-powered Wi-Fi-based human activity recognition system. We demonstrate that LLMs can process raw Wi-Fi signals and infer human activities by incorporating Wi-Fi sensing principles into prompts. Our approach leverages physical model insights to guide LLMs in interpreting Channel State Information (CSI) data without traditional signal processing techniques. Through experiments on real-world Wi-Fi datasets, we show that LLMs exhibit strong reasoning capabilities, achieving zero-shot activity recognition. These findings highlight a new paradigm for Wi-Fi sensing, expanding LLM applications beyond conventional language tasks and enhancing the accessibility of wireless sensing for real-world deployments.

### Sens-Merging: Sensitivity-Guided Parameter Balancing for Merging Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.12420)] [[cool](https://papers.cool/arxiv/2502.12420)] [[pdf](https://arxiv.org/pdf/2502.12420)]
> **Authors**: Shuqi Liu,Han Wu,Bowei He,Xiongwei Han,Mingxuan Yuan,Linqi Song
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Recent advances in large language models have led to numerous task-specialized fine-tuned variants, creating a need for efficient model merging techniques that preserve specialized capabilities while avoiding costly retraining. While existing task vector-based merging methods show promise, they typically apply uniform coefficients across all parameters, overlooking varying parameter importance both within and across tasks. We present Sens-Merging, a sensitivity-guided coefficient adjustment method that enhances existing model merging techniques by operating at both task-specific and cross-task levels. Our method analyzes parameter sensitivity within individual tasks and evaluates cross-task transferability to determine optimal merging coefficients. Extensive experiments on Mistral 7B and LLaMA2-7B/13B models demonstrate that Sens-Merging significantly improves performance across general knowledge, mathematical reasoning, and code generation tasks. Notably, when combined with existing merging techniques, our method enables merged models to outperform specialized fine-tuned models, particularly in code generation tasks. Our findings reveal important trade-offs between task-specific and cross-task scalings, providing insights for future model merging strategies.

### Lost in Transcription, Found in Distribution Shift: Demystifying Hallucination in Speech Foundation Models 
[[arxiv](https://arxiv.org/abs/2502.12414)] [[cool](https://papers.cool/arxiv/2502.12414)] [[pdf](https://arxiv.org/pdf/2502.12414)]
> **Authors**: Hanin Atwany,Abdul Waheed,Rita Singh,Monojit Choudhury,Bhiksha Raj
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: The first two authors contributed equally as co-first authors. The manuscript is 21 pages long and is a work in progress
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Speech foundation models trained at a massive scale, both in terms of model and data size, result in robust systems capable of performing multiple speech tasks, including automatic speech recognition (ASR). These models transcend language and domain barriers, yet effectively measuring their performance remains a challenge. Traditional metrics like word error rate (WER) and character error rate (CER) are commonly used to evaluate ASR performance but often fail to reflect transcription quality in critical contexts, particularly when detecting fabricated outputs. This phenomenon, known as hallucination, is especially concerning in high-stakes domains such as healthcare, legal, and aviation, where errors can have severe consequences. In our work, we address this gap by investigating hallucination in ASR models. We examine how factors such as distribution shifts, model size, and model architecture influence the hallucination error rate (HER), a metric we introduce to quantify hallucinations. Our analysis of 20 ASR models reveals \numinsights~key insights: (1) High WERs can mask low hallucination rates, while low WERs may conceal dangerous hallucinations. (2) Synthetic noise, both adversarial and common perturbations like white noise, pitch shift, and time stretching, increase HER. (3) Distribution shift correlates strongly with HER ($α= 0.91$). Our findings highlight the importance of incorporating HER alongside traditional metrics like WER to better assess ASR model performance, particularly in high-stakes domains.

### Gradient Co-occurrence Analysis for Detecting Unsafe Prompts in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.12411)] [[cool](https://papers.cool/arxiv/2502.12411)] [[pdf](https://arxiv.org/pdf/2502.12411)]
> **Authors**: Jingyuan Yang,Bowen Yan,Rongjun Li,Ziyu Zhou,Xin Chen,Zhiyong Feng,Wei Peng
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Unsafe prompts pose significant safety risks to large language models (LLMs). Existing methods for detecting unsafe prompts rely on data-driven fine-tuning to train guardrail models, necessitating significant data and computational resources. In contrast, recent few-shot gradient-based methods emerge, requiring only few safe and unsafe reference prompts. A gradient-based approach identifies unsafe prompts by analyzing consistent patterns of the gradients of safety-critical parameters in LLMs. Although effective, its restriction to directional similarity (cosine similarity) introduces ``directional bias'', limiting its capability to identify unsafe prompts. To overcome this limitation, we introduce GradCoo, a novel gradient co-occurrence analysis method that expands the scope of safety-critical parameter identification to include unsigned gradient similarity, thereby reducing the impact of ``directional bias'' and enhancing the accuracy of unsafe prompt detection. Comprehensive experiments on the widely-used benchmark datasets ToxicChat and XStest demonstrate that our proposed method can achieve state-of-the-art (SOTA) performance compared to existing methods. Moreover, we confirm the generalizability of GradCoo in detecting unsafe prompts across a range of LLM base models with various sizes and origins.

### On the Robust Approximation of ASR Metrics 
[[arxiv](https://arxiv.org/abs/2502.12408)] [[cool](https://papers.cool/arxiv/2502.12408)] [[pdf](https://arxiv.org/pdf/2502.12408)]
> **Authors**: Abdul Waheed,Hanin Atwany,Rita Singh,Bhiksha Raj
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 25 Pages. Work in Progress
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Recent advances in speech foundation models are largely driven by scaling both model size and data, enabling them to perform a wide range of tasks, including speech recognition. Traditionally, ASR models are evaluated using metrics like Word Error Rate (WER) and Character Error Rate (CER), which depend on ground truth labels. As a result of limited labeled data from diverse domains and testing conditions, the true generalization capabilities of these models beyond standard benchmarks remain unclear. Moreover, labeling data is both costly and time-consuming. To address this, we propose a novel label-free approach for approximating ASR performance metrics, eliminating the need for ground truth labels. Our method utilizes multimodal embeddings in a unified space for speech and transcription representations, combined with a high-quality proxy model to compute proxy metrics. These features are used to train a regression model to predict key ASR metrics like Word Error Rate (WER) and Character Error Rate (CER). We experiment with over 40 models across 14 datasets representing both standard and in-the-wild testing conditions. Our results show that we approximate the metrics within a single-digit absolute difference across all experimental configurations, outperforming the most recent baseline by more than 50\%.

### WMT24++: Expanding the Language Coverage of WMT24 to 55 Languages & Dialects 
[[arxiv](https://arxiv.org/abs/2502.12404)] [[cool](https://papers.cool/arxiv/2502.12404)] [[pdf](https://arxiv.org/pdf/2502.12404)]
> **Authors**: Daniel Deutsch,Eleftheria Briakou,Isaac Caswell,Mara Finkelstein,Rebecca Galor,Juraj Juraska,Geza Kovacs,Alison Lui,Ricardo Rei,Jason Riesa,Shruti Rijhwani,Parker Riley,Elizabeth Salesky,Firas Trabelsi,Stephanie Winkler,Biao Zhang,Markus Freitag
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: As large language models (LLM) become more and more capable in languages other than English, it is important to collect benchmark datasets in order to evaluate their multilingual performance, including on tasks like machine translation (MT). In this work, we extend the WMT24 dataset to cover 55 languages by collecting new human-written references and post-edits for 46 new languages and dialects in addition to post-edits of the references in 8 out of 9 languages in the original WMT24 dataset. The dataset covers four domains: literary, news, social, and speech. We benchmark a variety of MT providers and LLMs on the collected dataset using automatic metrics and find that LLMs are the best-performing MT systems in all 55 languages. These results should be confirmed using a human-based evaluation, which we leave for future work.

### Pragmatics in the Era of Large Language Models: A Survey on Datasets, Evaluation, Opportunities and Challenges 
[[arxiv](https://arxiv.org/abs/2502.12378)] [[cool](https://papers.cool/arxiv/2502.12378)] [[pdf](https://arxiv.org/pdf/2502.12378)]
> **Authors**: Bolei Ma,Yuting Li,Wei Zhou,Ziwei Gong,Yang Janet Liu,Katja Jasinskaja,Annemarie Friedrich,Julia Hirschberg,Frauke Kreuter,Barbara Plank
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Understanding pragmatics-the use of language in context-is crucial for developing NLP systems capable of interpreting nuanced language use. Despite recent advances in language technologies, including large language models, evaluating their ability to handle pragmatic phenomena such as implicatures and references remains challenging. To advance pragmatic abilities in models, it is essential to understand current evaluation trends and identify existing limitations. In this survey, we provide a comprehensive review of resources designed for evaluating pragmatic capabilities in NLP, categorizing datasets by the pragmatics phenomena they address. We analyze task designs, data collection methods, evaluation approaches, and their relevance to real-world applications. By examining these resources in the context of modern language models, we highlight emerging trends, challenges, and gaps in existing benchmarks. Our survey aims to clarify the landscape of pragmatic evaluation and guide the development of more comprehensive and targeted benchmarks, ultimately contributing to more nuanced and context-aware NLP models.

### UltraGen: Extremely Fine-grained Controllable Generation via Attribute Reconstruction and Global Preference Optimization 
[[arxiv](https://arxiv.org/abs/2502.12375)] [[cool](https://papers.cool/arxiv/2502.12375)] [[pdf](https://arxiv.org/pdf/2502.12375)]
> **Authors**: Longfei Yun,Letian Peng,Jingbo Shang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Fine granularity is an essential requirement for controllable text generation, which has seen rapid growth with the ability of LLMs. However, existing methods focus mainly on a small set of attributes like 3 to 5, and their performance degrades significantly when the number of attributes increases to the next order of magnitude. To address this challenge, we propose a novel zero-shot approach for extremely fine-grained controllable generation (EFCG), proposing auto-reconstruction (AR) and global preference optimization (GPO). In the AR phase, we leverage LLMs to extract soft attributes (e.g., Emphasis on simplicity and minimalism in design) from raw texts, and combine them with programmatically derived hard attributes (e.g., The text should be between 300 and 400 words) to construct massive (around 45) multi-attribute requirements, which guide the fine-grained text reconstruction process under weak supervision. In the GPO phase, we apply direct preference optimization (DPO) to refine text generation under diverse attribute combinations, enabling efficient exploration of the global combination space. Additionally, we introduce an efficient attribute sampling strategy to identify and correct potentially erroneous attributes, further improving global optimization. Our framework significantly improves the constraint satisfaction rate (CSR) and text quality for EFCG by mitigating position bias and alleviating attention dilution.

### Factual Inconsistency in Data-to-Text Generation Scales Exponentially with LLM Size: A Statistical Validation 
[[arxiv](https://arxiv.org/abs/2502.12372)] [[cool](https://papers.cool/arxiv/2502.12372)] [[pdf](https://arxiv.org/pdf/2502.12372)]
> **Authors**: Joy Mahapatra,Soumyajit Roy,Utpal Garain
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 21 pages
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Monitoring factual inconsistency is essential for ensuring trustworthiness in data-to-text generation (D2T). While large language models (LLMs) have demonstrated exceptional performance across various D2T tasks, previous studies on scaling laws have primarily focused on generalization error through power law scaling to LLM size (i.e., the number of model parameters). However, no research has examined the impact of LLM size on factual inconsistency in D2T. In this paper, we investigate how factual inconsistency in D2T scales with LLM size by exploring two scaling laws: power law and exponential scaling. To rigorously evaluate and compare these scaling laws, we employ a statistical validation framework consisting of three key stages: predictive performance estimation, goodness-of-fit assessment, and comparative analysis. For a comprehensive empirical study, we analyze three popular LLM families across five D2T datasets, measuring factual inconsistency inversely using four state-of-the-art consistency metrics. Our findings, based on exhaustive empirical results and validated through our framework, reveal that, contrary to the widely assumed power law scaling, factual inconsistency in D2T follows an exponential scaling with LLM size.

### Classifiers of Data Sharing Statements in Clinical Trial Records 
[[arxiv](https://arxiv.org/abs/2502.12362)] [[cool](https://papers.cool/arxiv/2502.12362)] [[pdf](https://arxiv.org/pdf/2502.12362)]
> **Authors**: Saber Jelodari Mamaghani,Cosima Strantz,Dennis Toddenroth
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Published in Proceedings of MIE 2024, IOS Press eBooks. Studies in Health Technology and Informatics, Vol. 316, pp. 834-838. Conference held in Athens, Greece
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Digital individual participant data (IPD) from clinical trials are increasingly distributed for potential scientific reuse. The identification of available IPD, however, requires interpretations of textual data-sharing statements (DSS) in large databases. Recent advancements in computational linguistics include pre-trained language models that promise to simplify the implementation of effective classifiers based on textual inputs. In a subset of 5,000 textual DSS from ClinicalTrials.gov, we evaluate how well classifiers based on domain-specific pre-trained language models reproduce original availability categories as well as manually annotated labels. Typical metrics indicate that classifiers that predicted manual annotations outperformed those that learned to output the original availability categories. This suggests that the textual DSS descriptions contain applicable information that the availability categories do not, and that such classifiers could thus aid the automatic identification of available IPD in large trial databases.

### ConFit v2: Improving Resume-Job Matching using Hypothetical Resume Embedding and Runner-Up Hard-Negative Mining 
[[arxiv](https://arxiv.org/abs/2502.12361)] [[cool](https://papers.cool/arxiv/2502.12361)] [[pdf](https://arxiv.org/pdf/2502.12361)]
> **Authors**: Xiao Yu,Ruize Xu,Chengyuan Xue,Jinzhong Zhang,Zhou Yu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: arXiv admin note: text overlap with arXiv:2401.16349
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: A reliable resume-job matching system helps a company recommend suitable candidates from a pool of resumes and helps a job seeker find relevant jobs from a list of job posts. However, since job seekers apply only to a few jobs, interaction labels in resume-job datasets are sparse. We introduce ConFit v2, an improvement over ConFit to tackle this sparsity problem. We propose two techniques to enhance the encoder's contrastive training process: augmenting job data with hypothetical reference resume generated by a large language model; and creating high-quality hard negatives from unlabeled resume/job pairs using a novel hard-negative mining strategy. We evaluate ConFit v2 on two real-world datasets and demonstrate that it outperforms ConFit and prior methods (including BM25 and OpenAI text-embedding-003), achieving an average absolute improvement of 13.8% in recall and 17.5% in nDCG across job-ranking and resume-ranking tasks.

### From Dense to Dynamic: Token-Difficulty Driven MoEfication of Pre-Trained LLMs 
[[arxiv](https://arxiv.org/abs/2502.12325)] [[cool](https://papers.cool/arxiv/2502.12325)] [[pdf](https://arxiv.org/pdf/2502.12325)]
> **Authors**: Kumari Nishu,Sachin Mehta,Samira Abnar,Mehrdad Farajtabar,Maxwell Horton,Mahyar Najibi,Moin Nabi,Minsik Cho,Devang Naik
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Training large language models (LLMs) for different inference constraints is computationally expensive, limiting control over efficiency-accuracy trade-offs. Moreover, once trained, these models typically process tokens uniformly, regardless of their complexity, leading to static and inflexible behavior. In this paper, we introduce a post-training optimization framework, DynaMoE, that adapts a pre-trained dense LLM to a token-difficulty-driven Mixture-of-Experts model with minimal fine-tuning cost. This adaptation makes the model dynamic, with sensitivity control to customize the balance between efficiency and accuracy. DynaMoE features a token-difficulty-aware router that predicts the difficulty of tokens and directs them to the appropriate sub-networks or experts, enabling larger experts to handle more complex tokens and smaller experts to process simpler ones. Our experiments demonstrate that DynaMoE can generate a range of adaptive model variants of the existing trained LLM with a single fine-tuning step, utilizing only $10B$ tokens, a minimal cost compared to the base model's training. Each variant offers distinct trade-offs between accuracy and performance. Compared to the baseline post-training optimization framework, Flextron, our method achieves similar aggregated accuracy across downstream tasks, despite using only $\frac{1}{9}\text{th}$ of their fine-tuning cost.

### Can Language Models Learn Typologically Implausible Languages? 
[[arxiv](https://arxiv.org/abs/2502.12317)] [[cool](https://papers.cool/arxiv/2502.12317)] [[pdf](https://arxiv.org/pdf/2502.12317)]
> **Authors**: Tianyang Xu,Tatsuki Kuribayashi,Yohei Oseki,Ryan Cotterell,Alex Warstadt
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Grammatical features across human languages show intriguing correlations often attributed to learning biases in humans. However, empirical evidence has been limited to experiments with highly simplified artificial languages, and whether these correlations arise from domain-general or language-specific biases remains a matter of debate. Language models (LMs) provide an opportunity to study artificial language learning at a large scale and with a high degree of naturalism. In this paper, we begin with an in-depth discussion of how LMs allow us to better determine the role of domain-general learning biases in language universals. We then assess learnability differences for LMs resulting from typologically plausible and implausible languages closely following the word-order universals identified by linguistic typologists. We conduct a symmetrical cross-lingual study training and testing LMs on an array of highly naturalistic but counterfactual versions of the English (head-initial) and Japanese (head-final) languages. Compared to similar work, our datasets are more naturalistic and fall closer to the boundary of plausibility. Our experiments show that these LMs are often slower to learn these subtly implausible languages, while ultimately achieving similar performance on some metrics regardless of typological plausibility. These findings lend credence to the conclusion that LMs do show some typologically-aligned learning preferences, and that the typological patterns may result from, at least to some degree, domain-general learning biases.

### SMOL: Professionally translated parallel data for 115 under-represented languages 
[[arxiv](https://arxiv.org/abs/2502.12301)] [[cool](https://papers.cool/arxiv/2502.12301)] [[pdf](https://arxiv.org/pdf/2502.12301)]
> **Authors**: Isaac Caswell,Elizabeth Nielsen,Jiaming Luo,Colin Cherry,Geza Kovacs,Hadar Shemtov,Partha Talukdar,Dinesh Tewari,Baba Mamadi Diane,Koulako Moussa Doumbouya,Djibrila Diane,Solo Farabado Cissé
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: ~10 pages with appendices
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: We open-source SMOL (Set of Maximal Overall Leverage), a suite of training data to unlock translation for low-resource languages (LRLs). SMOL has been translated into 115 under-resourced languages, including many for which there exist no previous public resources, for a total of 6.1M translated tokens. SMOL comprises two sub-datasets, each carefully chosen for maximum impact given its size: SMOL-Sent, a set of sentences chosen for broad unique token coverage, and SMOL-Doc, a document-level source focusing on a broad topic coverage. They join the already released GATITOS for a trifecta of paragraph, sentence, and token-level content. We demonstrate that using SMOL to prompt or fine-tune Large Language Models yields robust ChrF improvements. In addition to translation, we provide factuality ratings and rationales for all documents in SMOL-Doc, yielding the first factuality datasets for most of these languages.

### Evaluating Step-by-step Reasoning Traces: A Survey 
[[arxiv](https://arxiv.org/abs/2502.12289)] [[cool](https://papers.cool/arxiv/2502.12289)] [[pdf](https://arxiv.org/pdf/2502.12289)]
> **Authors**: Jinu Lee,Julia Hockenmaier
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 20 pages (8 pages of main content), 6 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Step-by-step reasoning is widely used to enhance the reasoning ability of large language models (LLMs) in complex problems. Evaluating the quality of reasoning traces is crucial for understanding and improving LLM reasoning. However, the evaluation criteria remain highly unstandardized, leading to fragmented efforts in developing metrics and meta-evaluation benchmarks. To address this gap, this survey provides a comprehensive overview of step-by-step reasoning evaluation, proposing a taxonomy of evaluation criteria with four top-level categories (groundedness, validity, coherence, and utility). We then categorize metrics based on their implementations, survey which metrics are used for assessing each criterion, and explore whether evaluator models can transfer across different criteria. Finally, we identify key directions for future research.

### Story Grammar Semantic Matching for Literary Study 
[[arxiv](https://arxiv.org/abs/2502.12276)] [[cool](https://papers.cool/arxiv/2502.12276)] [[pdf](https://arxiv.org/pdf/2502.12276)]
> **Authors**: Abigail Swenor,Neil Coffee,Walter Scheirer
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Submitted to Journal of Computational Literary Studies
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: In Natural Language Processing (NLP), semantic matching algorithms have traditionally relied on the feature of word co-occurrence to measure semantic similarity. While this feature approach has proven valuable in many contexts, its simplistic nature limits its analytical and explanatory power when used to understand literary texts. To address these limitations, we propose a more transparent approach that makes use of story structure and related elements. Using a BERT language model pipeline, we label prose and epic poetry with story element labels and perform semantic matching by only considering these labels as features. This new method, Story Grammar Semantic Matching, guides literary scholars to allusions and other semantic similarities across texts in a way that allows for characterizing patterns and literary technique.

### InfoQuest: Evaluating Multi-Turn Dialogue Agents for Open-Ended Conversations with Hidden Context 
[[arxiv](https://arxiv.org/abs/2502.12257)] [[cool](https://papers.cool/arxiv/2502.12257)] [[pdf](https://arxiv.org/pdf/2502.12257)]
> **Authors**: Bryan L. M. de Oliveira,Luana G. B. Martins,Bruno Brandão,Luckeciano C. Melo
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: While large language models excel at following explicit instructions, they often struggle with ambiguous or incomplete user requests, defaulting to verbose, generic responses rather than seeking clarification. We introduce InfoQuest, a multi-turn chat benchmark designed to evaluate how dialogue agents handle hidden context in open-ended user requests. The benchmark presents intentionally ambiguous scenarios that require models to engage in information-seeking dialogue through clarifying questions before providing appropriate responses. Our evaluation of both open and closed-source models reveals that while proprietary models generally perform better, all current assistants struggle with effectively gathering critical information, often requiring multiple turns to infer user intent and frequently defaulting to generic responses without proper clarification. We provide a systematic methodology for generating diverse scenarios and evaluating models' information-seeking capabilities, offering insights into the current limitations of language models in handling ambiguous requests through multi-turn interactions.

### GLoT: A Novel Gated-Logarithmic Transformer for Efficient Sign Language Translation 
[[arxiv](https://arxiv.org/abs/2502.12223)] [[cool](https://papers.cool/arxiv/2502.12223)] [[pdf](https://arxiv.org/pdf/2502.12223)]
> **Authors**: Nada Shahin,Leila Ismail
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: :I.2.6; I.2.7; I.2.10; I.4.8; I.4.9; I.4.10
- **标题**: None
- **领域**: 计算语言学,计算机视觉和模式识别
- **Abstract**: Machine Translation has played a critical role in reducing language barriers, but its adaptation for Sign Language Machine Translation (SLMT) has been less explored. Existing works on SLMT mostly use the Transformer neural network which exhibits low performance due to the dynamic nature of the sign language. In this paper, we propose a novel Gated-Logarithmic Transformer (GLoT) that captures the long-term temporal dependencies of the sign language as a time-series data. We perform a comprehensive evaluation of GloT with the transformer and transformer-fusion models as a baseline, for Sign-to-Gloss-to-Text translation. Our results demonstrate that GLoT consistently outperforms the other models across all metrics. These findings underscore its potential to address the communication challenges faced by the Deaf and Hard of Hearing community.

### Zero Token-Driven Deep Thinking in LLMs: Unlocking the Full Potential of Existing Parameters via Cyclic Refinement 
[[arxiv](https://arxiv.org/abs/2502.12214)] [[cool](https://papers.cool/arxiv/2502.12214)] [[pdf](https://arxiv.org/pdf/2502.12214)]
> **Authors**: Guanghao Li,Wenhao Jiang,Li Shen,Ming Tang,Chun Yuan
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Resource limitations often constrain the parameter counts of Large Language Models (LLMs), hindering their performance. While existing methods employ parameter sharing to reuse the same parameter set under fixed budgets, such approaches typically force each layer to assume multiple roles with a predetermined number of iterations, restricting efficiency and adaptability. In this work, we propose the Zero Token Transformer (ZTT), which features a head-tail decoupled parameter cycling method. We disentangle the first (head) and last (tail) layers from parameter cycling and iteratively refine only the intermediate layers. Furthermore, we introduce a Zero-Token Mechanism, an internal architectural component rather than an input token, to guide layer-specific computation. At each cycle, the model retrieves a zero token (with trainable key values) from a Zero-Token Pool, integrating it alongside regular tokens in the attention mechanism. The corresponding attention scores not only reflect each layer's computational importance but also enable dynamic early exits without sacrificing overall model accuracy. Our approach achieves superior performance under tight parameter budgets, effectively reduces computational overhead via early exits, and can be readily applied to fine-tune existing pre-trained models for enhanced efficiency and adaptability.

### Enhancing Frame Detection with Retrieval Augmented Generation 
[[arxiv](https://arxiv.org/abs/2502.12210)] [[cool](https://papers.cool/arxiv/2502.12210)] [[pdf](https://arxiv.org/pdf/2502.12210)]
> **Authors**: Papa Abdou Karim Karou Diallo,Amal Zouaq
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Recent advancements in Natural Language Processing have significantly improved the extraction of structured semantic representations from unstructured text, especially through Frame Semantic Role Labeling (FSRL). Despite this progress, the potential of Retrieval-Augmented Generation (RAG) models for frame detection remains under-explored. In this paper, we present the first RAG-based approach for frame detection called RCIF (Retrieve Candidates and Identify Frames). RCIF is also the first approach to operate without the need for explicit target span and comprises three main stages: (1) generation of frame embeddings from various representations ; (2) retrieval of candidate frames given an input text; and (3) identification of the most suitable frames. We conducted extensive experiments across multiple configurations, including zero-shot, few-shot, and fine-tuning settings. Our results show that our retrieval component significantly reduces the complexity of the task by narrowing the search space thus allowing the frame identifier to refine and complete the set of candidates. Our approach achieves state-of-the-art performance on FrameNet 1.5 and 1.7, demonstrating its robustness in scenarios where only raw text is provided. Furthermore, we leverage the structured representation obtained through this method as a proxy to enhance generalization across lexical variations in the task of translating natural language questions into SPARQL queries.

### Predicting Depression in Screening Interviews from Interactive Multi-Theme Collaboration 
[[arxiv](https://arxiv.org/abs/2502.12204)] [[cool](https://papers.cool/arxiv/2502.12204)] [[pdf](https://arxiv.org/pdf/2502.12204)]
> **Authors**: Xianbing Zhao,Yiqing Lyu,Di Wang,Buzhou Tang
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Automatic depression detection provides cues for early clinical intervention by clinicians. Clinical interviews for depression detection involve dialogues centered around multiple themes. Existing studies primarily design end-to-end neural network models to capture the hierarchical structure of clinical interview dialogues. However, these methods exhibit defects in modeling the thematic content of clinical interviews: 1) they fail to capture intra-theme and inter-theme correlation explicitly, and 2) they do not allow clinicians to intervene and focus on themes of interest. To address these issues, this paper introduces an interactive depression detection framework. This framework leverages in-context learning techniques to identify themes in clinical interviews and then models both intra-theme and inter-theme correlation. Additionally, it employs AI-driven feedback to simulate the interests of clinicians, enabling interactive adjustment of theme importance. PDIMC achieves absolute improvements of 35\% and 12\% compared to the state-of-the-art on the depression detection dataset DAIC-WOZ, which demonstrates the effectiveness of modeling theme correlation and incorporating interactive external feedback.

### BoT: Breaking Long Thought Processes of o1-like Large Language Models through Backdoor Attack 
[[arxiv](https://arxiv.org/abs/2502.12202)] [[cool](https://papers.cool/arxiv/2502.12202)] [[pdf](https://arxiv.org/pdf/2502.12202)]
> **Authors**: Zihao Zhu,Hongbao Zhang,Mingda Zhang,Ruotong Wang,Guanzong Wu,Ke Xu,Baoyuan Wu
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Longer thought, better performance: large language models with deep reasoning capabilities, particularly o1-like models, have demonstrated remarkable performance by generating extensive thought processes during inference. This trade-off reveals a potential vulnerability: adversaries could compromise model performance by forcing immediate responses without thought processes. To this end, in this paper, we introduce a novel attack scenario targeting the long thought processes of o1-like models and propose BoT (Break CoT), which can selectively break intrinsic reasoning mechanisms through backdoor attacks. BoT constructs poisoned datasets with designed triggers and injects backdoor by either supervised fine-tuning or direct preference optimization. When triggered, the model directly generates answers without thought processes, while maintaining normal reasoning capabilities for clean inputs. Extensive experiments on open-source o1-like models, including recent DeepSeek-R1, demonstrate that BoT nearly achieves high attack success rates while maintaining clean accuracy, highlighting the critical safety risk in current models. Furthermore, the relationship between task difficulty and helpfulness reveals a potential application for good, enabling users to customize model behavior based on task complexity. Code is available at \href{https://github.com/zihao-ai/BoT}{https://github.com/zihao-ai/BoT}.

### Efficient and Effective Prompt Tuning via Prompt Decomposition and Compressed Outer Product 
[[arxiv](https://arxiv.org/abs/2502.12200)] [[cool](https://papers.cool/arxiv/2502.12200)] [[pdf](https://arxiv.org/pdf/2502.12200)]
> **Authors**: Pengxiang Lan,Haoyu Xu,Enneng Yang,Yuliang Liang,Guibing Guo,Jianzhe Zhao,Xingwei Wang
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-18
> **comment**: ef:NAACL 2025 main conference
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Prompt tuning (PT) offers a cost-effective alternative to fine-tuning large-scale pre-trained language models (PLMs), requiring only a few parameters in soft prompt tokens added before the input text. However, existing PT approaches face two significant issues: (i) They overlook intrinsic semantic associations between soft prompt tokens, leading to high discreteness and limited interactions, thus reducing the model's comprehension and effectiveness in complex tasks. (ii) Due to the complexity of downstream tasks, long soft prompt is necessitated to improve performance, but prompt length correlates positively with memory usage and computational costs. Achieving high efficiency and performance remains an ongoing challenge. To address these issues, we propose a novel Low-parameters prompt tuning (LAMP) method, which leverages prompt decomposition and compressed outer product. Specifically, the prompt decomposition module employs Truncated SVD to reduce training parameters and significantly lower the dimensionality of the soft prompt parameter space. It then utilizes a compressed outer product module to facilitate multiple interactions among prompt tokens, exploring their intrinsic associations to enhance knowledge representation. Finally, LAMP uses average pooling to reduce memory usage and training/inference time. Extensive experiments across six architectures and eight datasets demonstrate that LAMP outperforms state-of-the-art PT-based and LoRA-based methods in performance and efficiency.

### A Closer Look at System Prompt Robustness 
[[arxiv](https://arxiv.org/abs/2502.12197)] [[cool](https://papers.cool/arxiv/2502.12197)] [[pdf](https://arxiv.org/pdf/2502.12197)]
> **Authors**: Norman Mu,Jonathan Lu,Michael Lavery,David Wagner
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-18
> **comment**: Artifacts: https://github.com/normster/RealGuardrails
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: System prompts have emerged as a critical control surface for specifying the behavior of LLMs in chat and agent settings. Developers depend on system prompts to specify important context, output format, personalities, guardrails, content policies, and safety countermeasures, all of which require models to robustly adhere to the system prompt, especially when facing conflicting or adversarial user inputs. In practice, models often forget to consider relevant guardrails or fail to resolve conflicting demands between the system and the user. In this work, we study various methods for improving system prompt robustness by creating realistic new evaluation and fine-tuning datasets based on prompts collected from from OpenAI's GPT Store and HuggingFace's HuggingChat. Our experiments assessing models with a panel of new and existing benchmarks show that performance can be considerably improved with realistic fine-tuning data, as well as inference-time interventions such as classifier-free guidance. Finally, we analyze the results of recently released reasoning models from OpenAI and DeepSeek, which show exciting but uneven improvements on the benchmarks we study. Overall, current techniques fall short of ensuring system prompt robustness and further study is warranted.

### AI and the Law: Evaluating ChatGPT's Performance in Legal Classification 
[[arxiv](https://arxiv.org/abs/2502.12193)] [[cool](https://papers.cool/arxiv/2502.12193)] [[pdf](https://arxiv.org/pdf/2502.12193)]
> **Authors**: Pawel Weichbroth
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-18
> **comment**: 15 pages; 1 figure; 2 tables; 32 references
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: The use of ChatGPT to analyze and classify evidence in criminal proceedings has been a topic of ongoing discussion. However, to the best of our knowledge, this issue has not been studied in the context of the Polish language. This study addresses this research gap by evaluating the effectiveness of ChatGPT in classifying legal cases under the Polish Penal Code. The results show excellent binary classification accuracy, with all positive and negative cases correctly categorized. In addition, a qualitative evaluation confirms that the legal basis provided for each case, along with the relevant legal content, was appropriate. The results obtained suggest that ChatGPT can effectively analyze and classify evidence while applying the appropriate legal rules. In conclusion, ChatGPT has the potential to assist interested parties in the analysis of evidence and serve as a valuable legal resource for individuals with less experience or knowledge in this area.

### Hallucinations are inevitable but statistically negligible 
[[arxiv](https://arxiv.org/abs/2502.12187)] [[cool](https://papers.cool/arxiv/2502.12187)] [[pdf](https://arxiv.org/pdf/2502.12187)]
> **Authors**: Atsushi Suzuki,Yulan He,Feng Tian,Zhongyuan Wang
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,形式语言和自动机理论,机器学习,统计理论,机器学习
- **Abstract**: Hallucinations, a phenomenon where a language model (LM) generates nonfactual content, pose a significant challenge to the practical deployment of LMs. While many empirical methods have been proposed to mitigate hallucinations, a recent study established a computability-theoretic result showing that any LM will inevitably generate hallucinations on an infinite set of inputs, regardless of the quality and quantity of training datasets and the choice of the language model architecture and training and inference algorithms. Although the computability-theoretic result may seem pessimistic, its significance in practical viewpoints has remained unclear. In contrast, we present a positive theoretical result from a probabilistic perspective. Specifically, we prove that hallucinations can be made statistically negligible, provided that the quality and quantity of the training data are sufficient. Interestingly, our positive result coexists with the computability-theoretic result, implying that while hallucinations on an infinite set of inputs cannot be entirely eliminated, their probability can always be reduced by improving algorithms and training data. By evaluating the two seemingly contradictory results through the lens of information theory, we argue that our probability-theoretic positive result better reflects practical considerations than the computability-theoretic negative result.

### Large Language Models for Extrapolative Modeling of Manufacturing Processes 
[[arxiv](https://arxiv.org/abs/2502.12185)] [[cool](https://papers.cool/arxiv/2502.12185)] [[pdf](https://arxiv.org/pdf/2502.12185)]
> **Authors**: Kiarash Naghavi Khanghah,Anandkumar Patel,Rajiv Malhotra,Hongyi Xu
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Conventional predictive modeling of parametric relationships in manufacturing processes is limited by the subjectivity of human expertise and intuition on the one hand and by the cost and time of experimental data generation on the other hand. This work addresses this issue by establishing a new Large Language Model (LLM) framework. The novelty lies in combining automatic extraction of process-relevant knowledge embedded in the literature with iterative model refinement based on a small amount of experimental data. This approach is evaluated on three distinct manufacturing processes that are based on machining, deformation, and additive principles. The results show that for the same small experimental data budget the models derived by our framework have unexpectedly high extrapolative performance, often surpassing the capabilities of conventional Machine Learning. Further, our approach eliminates manual generation of initial models or expertise-dependent interpretation of the literature. The results also reveal the importance of the nature of the knowledge extracted from the literature and the significance of both the knowledge extraction and model refinement components.

### Leveraging large language models for structured information extraction from pathology reports 
[[arxiv](https://arxiv.org/abs/2502.12183)] [[cool](https://papers.cool/arxiv/2502.12183)] [[pdf](https://arxiv.org/pdf/2502.12183)]
> **Authors**: Jeya Balaji Balasubramanian,Daniel Adams,Ioannis Roxanis,Amy Berrington de Gonzalez,Penny Coulson,Jonas S. Almeida,Montserrat García-Closas
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-18
> **comment**: 29 pages, 6 figures
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Background: Structured information extraction from unstructured histopathology reports facilitates data accessibility for clinical research. Manual extraction by experts is time-consuming and expensive, limiting scalability. Large language models (LLMs) offer efficient automated extraction through zero-shot prompting, requiring only natural language instructions without labeled data or training. We evaluate LLMs' accuracy in extracting structured information from breast cancer histopathology reports, compared to manual extraction by a trained human annotator. Methods: We developed the Medical Report Information Extractor, a web application leveraging LLMs for automated extraction. We developed a gold standard extraction dataset to evaluate the human annotator alongside five LLMs including GPT-4o, a leading proprietary model, and the Llama 3 model family, which allows self-hosting for data privacy. Our assessment involved 111 histopathology reports from the Breast Cancer Now (BCN) Generations Study, extracting 51 pathology features specified in the study's data dictionary. Results: Evaluation against the gold standard dataset showed that both Llama 3.1 405B (94.7% accuracy) and GPT-4o (96.1%) achieved extraction accuracy comparable to the human annotator (95.4%; p = 0.146 and p = 0.106, respectively). While Llama 3.1 70B (91.6%) performed below human accuracy (p <0.001), its reduced computational requirements make it a viable option for self-hosting. Conclusion: We developed an open-source tool for structured information extraction that can be customized by non-programmers using natural language. Its modular design enables reuse for various extraction tasks, producing standardized, structured data from unstructured text reports to facilitate analytics through improved accessibility and interoperability.

### Idiosyncrasies in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.12150)] [[cool](https://papers.cool/arxiv/2502.12150)] [[pdf](https://arxiv.org/pdf/2502.12150)]
> **Authors**: Mingjie Sun,Yida Yin,Zhiqiu Xu,J. Zico Kolter,Zhuang Liu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Website at https://eric-mingjie.github.io/llm-idiosyncrasies/index.html
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: In this work, we unveil and study idiosyncrasies in Large Language Models (LLMs) -- unique patterns in their outputs that can be used to distinguish the models. To do so, we consider a simple classification task: given a particular text output, the objective is to predict the source LLM that generates the text. We evaluate this synthetic task across various groups of LLMs and find that simply fine-tuning existing text embedding models on LLM-generated texts yields excellent classification accuracy. Notably, we achieve 97.1% accuracy on held-out validation data in the five-way classification problem involving ChatGPT, Claude, Grok, Gemini, and DeepSeek. Our further investigation reveals that these idiosyncrasies are rooted in word-level distributions. These patterns persist even when the texts are rewritten, translated, or summarized by an external LLM, suggesting that they are also encoded in the semantic content. Additionally, we leverage LLM as judges to generate detailed, open-ended descriptions of each model's idiosyncrasies. Finally, we discuss the broader implications of our findings, particularly for training on synthetic data and inferring model similarity. Code is available at https://github.com/locuslab/llm-idiosyncrasies.

### SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs 
[[arxiv](https://arxiv.org/abs/2502.12134)] [[cool](https://papers.cool/arxiv/2502.12134)] [[pdf](https://arxiv.org/pdf/2502.12134)]
> **Authors**: Yige Xu,Xu Guo,Zhiwei Zeng,Chunyan Miao
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Chain-of-Thought (CoT) reasoning enables Large Language Models (LLMs) to solve complex reasoning tasks by generating intermediate reasoning steps. However, most existing approaches focus on hard token decoding, which constrains reasoning within the discrete vocabulary space and may not always be optimal. While recent efforts explore continuous-space reasoning, they often suffer from catastrophic forgetting, limiting their applicability to state-of-the-art LLMs that already perform well in zero-shot settings with a proper instruction. To address this challenge, we propose a novel approach for continuous-space reasoning that does not require modifying the underlying LLM. Specifically, we employ a lightweight assistant model to generate instance-specific soft thought tokens speculatively as the initial chain of thoughts, which are then mapped into the LLM's representation space via a projection module. Experimental results on five reasoning benchmarks demonstrate that our method enhances LLM reasoning performance through supervised, parameter-efficient fine-tuning.

### On the Query Complexity of Verifier-Assisted Language Generation 
[[arxiv](https://arxiv.org/abs/2502.12123)] [[cool](https://papers.cool/arxiv/2502.12123)] [[pdf](https://arxiv.org/pdf/2502.12123)]
> **Authors**: Edoardo Botta,Yuchen Li,Aashay Mehta,Jordan T. Ash,Cyril Zhang,Andrej Risteski
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Recently, a plethora of works have proposed inference-time algorithms (e.g. best-of-n), which incorporate verifiers to assist the generation process. Their quality-efficiency trade-offs have been empirically benchmarked on a variety of constrained generation tasks, but the algorithmic design landscape is still largely poorly understood. In this paper, we develop a mathematical framework for reasoning about constrained generation using a pre-trained language model generator oracle and a process verifier--which can decide whether a prefix can be extended to a string which satisfies the constraints of choice. We show that even in very simple settings, access to a verifier can render an intractable problem (information-theoretically or computationally) to a tractable one. In fact, we show even simple algorithms, like tokenwise rejection sampling, can enjoy significant benefits from access to a verifier. Empirically, we show that a natural modification of tokenwise rejection sampling, in which the sampler is allowed to "backtrack" (i.e., erase the final few generated tokens) has robust and substantive benefits over natural baselines (e.g. (blockwise) rejection sampling, nucleus sampling)--both in terms of computational efficiency, accuracy and diversity.

### A-MEM: Agentic Memory for LLM Agents 
[[arxiv](https://arxiv.org/abs/2502.12110)] [[cool](https://papers.cool/arxiv/2502.12110)] [[pdf](https://arxiv.org/pdf/2502.12110)]
> **Authors**: Wujiang Xu,Zujie Liang,Kai Mei,Hang Gao,Juntao Tan,Yongfeng Zhang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人机交互
- **Abstract**: While large language model (LLM) agents can effectively use external tools for complex real-world tasks, they require memory systems to leverage historical experiences. Current memory systems enable basic storage and retrieval but lack sophisticated memory organization, despite recent attempts to incorporate graph databases. Moreover, these systems' fixed operations and structures limit their adaptability across diverse tasks. To address this limitation, this paper proposes a novel agentic memory system for LLM agents that can dynamically organize memories in an agentic way. Following the basic principles of the Zettelkasten method, we designed our memory system to create interconnected knowledge networks through dynamic indexing and linking. When a new memory is added, we generate a comprehensive note containing multiple structured attributes, including contextual descriptions, keywords, and tags. The system then analyzes historical memories to identify relevant connections, establishing links where meaningful similarities exist. Additionally, this process enables memory evolution - as new memories are integrated, they can trigger updates to the contextual representations and attributes of existing historical memories, allowing the memory network to continuously refine its understanding. Our approach combines the structured organization principles of Zettelkasten with the flexibility of agent-driven decision making, allowing for more adaptive and context-aware memory management. Empirical experiments on six foundation models show superior improvement against existing SOTA baselines. The source code is available at https://github.com/WujiangXu/AgenticMemory.

### Personality Structured Interview for Large Language Model Simulation in Personality Research 
[[arxiv](https://arxiv.org/abs/2502.12109)] [[cool](https://papers.cool/arxiv/2502.12109)] [[pdf](https://arxiv.org/pdf/2502.12109)]
> **Authors**: Pengda Wang,Huiqi Zou,Hanjie Chen,Tianjun Sun,Ziang Xiao,Frederick L. Oswald
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 41 Pages, 30 Tables, 5 Figures
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Although psychometrics researchers have recently explored the use of large language models (LLMs) as proxies for human participants, LLMs often fail to generate heterogeneous data with human-like diversity, which diminishes their value in advancing social science research. To address these challenges, we explored the potential of the theory-informed Personality Structured Interview (PSI) as a tool for simulating human responses in personality research. In this approach, the simulation is grounded in nuanced real-human interview transcripts that target the personality construct of interest. We have provided a growing set of 357 structured interview transcripts from a representative sample, each containing an individual's response to 32 open-ended questions carefully designed to gather theory-based personality evidence. Additionally, grounded in psychometric research, we have summarized an evaluation framework to systematically validate LLM-generated psychometric data. Results from three experiments demonstrate that well-designed structured interviews could improve human-like heterogeneity in LLM-simulated personality data and predict personality-related behavioral outcomes (i.e., organizational citizenship behaviors and counterproductive work behavior). We further discuss the role of theory-informed structured interviews in LLM-based simulation and outline a general framework for designing structured interviews to simulate human-like data for psychometric research.

### VLM$^2$-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit Matching Visual Cues 
[[arxiv](https://arxiv.org/abs/2502.12084)] [[cool](https://papers.cool/arxiv/2502.12084)] [[pdf](https://arxiv.org/pdf/2502.12084)]
> **Authors**: Jianshu Zhang,Dongyu Yao,Renjie Pi,Paul Pu Liang,Yi R. Fung
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Project Page: https://vlm2-bench.github.io/
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Visually linking matching cues is a crucial ability in daily life, such as identifying the same person in multiple photos based on their cues, even without knowing who they are. Despite the extensive knowledge that vision-language models (VLMs) possess, it remains largely unexplored whether they are capable of performing this fundamental task. To address this, we introduce VLM$^2$-Bench, a benchmark designed to assess whether VLMs can Visually Link Matching cues, with 9 subtasks and over 3,000 test cases. Comprehensive evaluation across eight open-source VLMs and GPT-4o, along with further analysis of various language-side and vision-side prompting methods, leads to a total of eight key findings. We identify critical challenges in models' ability to link visual cues, highlighting a significant performance gap where even GPT-4o lags 34.80% behind humans. Based on these insights, we advocate for (i) enhancing core visual capabilities to improve adaptability and reduce reliance on prior knowledge, (ii) establishing clearer principles for integrating language-based reasoning in vision-centric tasks to prevent unnecessary biases, and (iii) shifting vision-text training paradigms toward fostering models' ability to independently structure and infer relationships among visual cues.

### AdaSplash: Adaptive Sparse Flash Attention 
[[arxiv](https://arxiv.org/abs/2502.12082)] [[cool](https://papers.cool/arxiv/2502.12082)] [[pdf](https://arxiv.org/pdf/2502.12082)]
> **Authors**: Nuno Gonçalves,Marcos Treviso,André F. T. Martins
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: The computational cost of softmax-based attention in transformers limits their applicability to long-context tasks. Adaptive sparsity, of which $α$-entmax attention is an example, offers a flexible data-dependent alternative, but existing implementations are inefficient and do not leverage the sparsity to obtain runtime and memory gains. In this work, we propose AdaSplash, which combines the efficiency of GPU-optimized algorithms with the sparsity benefits of $α$-entmax. We first introduce a hybrid Halley-bisection algorithm, resulting in a 7-fold reduction in the number of iterations needed to compute the $α$-entmax transformation. Then, we implement custom Triton kernels to efficiently handle adaptive sparsity. Experiments with RoBERTa and ModernBERT for text classification and single-vector retrieval, along with GPT-2 for language modeling, show that our method achieves substantial improvements in runtime and memory efficiency compared to existing $α$-entmax implementations. It approaches -- and in some cases surpasses -- the efficiency of highly optimized softmax implementations like FlashAttention-2, enabling long-context training while maintaining strong task performance.

### Can LLMs Simulate Social Media Engagement? A Study on Action-Guided Response Generation 
[[arxiv](https://arxiv.org/abs/2502.12073)] [[cool](https://papers.cool/arxiv/2502.12073)] [[pdf](https://arxiv.org/pdf/2502.12073)]
> **Authors**: Zhongyi Qiu,Hanjia Lyu,Wei Xiong,Jiebo Luo
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Social media enables dynamic user engagement with trending topics, and recent research has explored the potential of large language models (LLMs) for response generation. While some studies investigate LLMs as agents for simulating user behavior on social media, their focus remains on practical viability and scalability rather than a deeper understanding of how well LLM aligns with human behavior. This paper analyzes LLMs' ability to simulate social media engagement through action guided response generation, where a model first predicts a user's most likely engagement action-retweet, quote, or rewrite-towards a trending post before generating a personalized response conditioned on the predicted action. We benchmark GPT-4o-mini, O1-mini, and DeepSeek-R1 in social media engagement simulation regarding a major societal event discussed on X. Our findings reveal that zero-shot LLMs underperform BERT in action prediction, while few-shot prompting initially degrades the prediction accuracy of LLMs with limited examples. However, in response generation, few-shot LLMs achieve stronger semantic alignment with ground truth posts.

### TokenSkip: Controllable Chain-of-Thought Compression in LLMs 
[[arxiv](https://arxiv.org/abs/2502.12067)] [[cool](https://papers.cool/arxiv/2502.12067)] [[pdf](https://arxiv.org/pdf/2502.12067)]
> **Authors**: Heming Xia,Yongqi Li,Chak Tou Leong,Wenjie Wang,Wenjie Li
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Chain-of-Thought (CoT) has been proven effective in enhancing the reasoning capabilities of large language models (LLMs). Recent advancements, such as OpenAI's o1 and DeepSeek-R1, suggest that scaling up the length of CoT sequences during inference could further boost LLM reasoning performance. However, due to the autoregressive nature of LLM decoding, longer CoT outputs lead to a linear increase in inference latency, adversely affecting user experience, particularly when the CoT exceeds 10,000 tokens. To address this limitation, we analyze the semantic importance of tokens within CoT outputs and reveal that their contributions to reasoning vary. Building on this insight, we propose TokenSkip, a simple yet effective approach that enables LLMs to selectively skip less important tokens, allowing for controllable CoT compression. Extensive experiments across various models and tasks demonstrate the effectiveness of TokenSkip in reducing CoT token usage while preserving strong reasoning performance. Notably, when applied to Qwen2.5-14B-Instruct, TokenSkip reduces reasoning tokens by 40% (from 313 to 181) on GSM8K, with less than a 0.4% performance drop.

### Formalizing Complex Mathematical Statements with LLMs: A Study on Mathematical Definitions 
[[arxiv](https://arxiv.org/abs/2502.12065)] [[cool](https://papers.cool/arxiv/2502.12065)] [[pdf](https://arxiv.org/pdf/2502.12065)]
> **Authors**: Lan Zhang,Marco Valentino,Andre Freitas
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,形式语言和自动机理论
- **Abstract**: Thanks to their linguistic capabilities, LLMs offer an opportunity to bridge the gap between informal mathematics and formal languages through autoformalization. However, it is still unclear how well LLMs generalize to sophisticated and naturally occurring mathematical statements. To address this gap, we investigate the task of autoformalizing real-world mathematical definitions -- a critical component of mathematical discourse. Specifically, we introduce two novel resources for autoformalisation, collecting definitions from Wikipedia (Def_Wiki) and arXiv papers (Def_ArXiv). We then systematically evaluate a range of LLMs, analyzing their ability to formalize definitions into Isabelle/HOL. Furthermore, we investigate strategies to enhance LLMs' performance including refinement through external feedback from Proof Assistants, and formal definition grounding, where we guide LLMs through relevant contextual elements from formal mathematical libraries. Our findings reveal that definitions present a greater challenge compared to existing benchmarks, such as miniF2F. In particular, we found that LLMs still struggle with self-correction, and aligning with relevant mathematical libraries. At the same time, structured refinement methods and definition grounding strategies yield notable improvements of up to 16% on self-correction capabilities and 43% on the reduction of undefined errors, highlighting promising directions for enhancing LLM-based autoformalization in real-world scenarios.

### AI-generated Text Detection with a GLTR-based Approach 
[[arxiv](https://arxiv.org/abs/2502.12064)] [[cool](https://papers.cool/arxiv/2502.12064)] [[pdf](https://arxiv.org/pdf/2502.12064)]
> **Authors**: Lucía Yan Wu,Isabel Segura-Bedmar
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: The rise of LLMs (Large Language Models) has contributed to the improved performance and development of cutting-edge NLP applications. However, these can also pose risks when used maliciously, such as spreading fake news, harmful content, impersonating individuals, or facilitating school plagiarism, among others. This is because LLMs can generate high-quality texts, which are challenging to differentiate from those written by humans. GLTR, which stands for Giant Language Model Test Room and was developed jointly by the MIT-IBM Watson AI Lab and HarvardNLP, is a visual tool designed to help detect machine-generated texts based on GPT-2, that highlights the words in text depending on the probability that they were machine-generated. One limitation of GLTR is that the results it returns can sometimes be ambiguous and lead to confusion. This study aims to explore various ways to improve GLTR's effectiveness for detecting AI-generated texts within the context of the IberLef-AuTexTification 2023 shared task, in both English and Spanish languages. Experiment results show that our GLTR-based GPT-2 model overcomes the state-of-the-art models on the English dataset with a macro F1-score of 80.19%, except for the first ranking model (80.91%). However, for the Spanish dataset, we obtained a macro F1-score of 66.20%, which differs by 4.57% compared to the top-performing model.

### Designing Role Vectors to Improve LLM Inference Behaviour 
[[arxiv](https://arxiv.org/abs/2502.12055)] [[cool](https://papers.cool/arxiv/2502.12055)] [[pdf](https://arxiv.org/pdf/2502.12055)]
> **Authors**: Daniele Potertì,Andrea Seveso,Fabio Mercorio
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Submitted to ARR 2025 February cycle
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The influence of personas on Large Language Models (LLMs) has been widely studied, yet their direct impact on performance remains uncertain. This work explores a novel approach to guiding LLM behaviour through role vectors, an alternative to persona-based prompting. We construct 29 role vectors derived from model activations and evaluate their impact on benchmark performance across multiple domains. Our analysis investigates whether these vectors can effectively steer models toward domain-specific expertise. We measure two key interventions: (i) activation addition, which reinforces role-specific directions, and (ii) directional ablation, which removes them. Results on well-established benchmarks indicate that role vectors do, in fact, influence model behaviour, improving task performance in relevant domains while marginally affecting unrelated tasks. This, in turn, suggests that manipulating internal model representations has a greater impact on outcomes than persona-based prompting.

### A Dual-Perspective NLG Meta-Evaluation Framework with Automatic Benchmark and Better Interpretability 
[[arxiv](https://arxiv.org/abs/2502.12052)] [[cool](https://papers.cool/arxiv/2502.12052)] [[pdf](https://arxiv.org/pdf/2502.12052)]
> **Authors**: Xinyu Hu,Mingqi Gao,Li Lin,Zhenghan Yu,Xiaojun Wan
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 23 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: In NLG meta-evaluation, evaluation metrics are typically assessed based on their consistency with humans. However, we identify some limitations in traditional NLG meta-evaluation approaches, such as issues in handling human ratings and ambiguous selections of correlation measures, which undermine the effectiveness of meta-evaluation. In this work, we propose a dual-perspective NLG meta-evaluation framework that focuses on different evaluation capabilities, thereby providing better interpretability. In addition, we introduce a method of automatically constructing the corresponding benchmarks without requiring new human annotations. Furthermore, we conduct experiments with 16 representative LLMs as the evaluators based on our proposed framework, comprehensively analyzing their evaluation performance from different perspectives.

### How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines 
[[arxiv](https://arxiv.org/abs/2502.12051)] [[cool](https://papers.cool/arxiv/2502.12051)] [[pdf](https://arxiv.org/pdf/2502.12051)]
> **Authors**: Ayan Sengupta,Yash Goel,Tanmoy Chakraborty
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 20 pages, 8 tables, 4 figures
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Neural scaling laws have revolutionized the design and optimization of large-scale AI models by revealing predictable relationships between model size, dataset volume, and computational resources. Early research established power-law relationships in model performance, leading to compute-optimal scaling strategies. However, recent studies highlighted their limitations across architectures, modalities, and deployment contexts. Sparse models, mixture-of-experts, retrieval-augmented learning, and multimodal models often deviate from traditional scaling patterns. Moreover, scaling behaviors vary across domains such as vision, reinforcement learning, and fine-tuning, underscoring the need for more nuanced approaches. In this survey, we synthesize insights from over 50 studies, examining the theoretical foundations, empirical findings, and practical implications of scaling laws. We also explore key challenges, including data efficiency, inference scaling, and architecture-specific constraints, advocating for adaptive scaling strategies tailored to real-world applications. We suggest that while scaling laws provide a useful guide, they do not always generalize across all architectures and training strategies.

### Teaching LLMs According to Their Aptitude: Adaptive Reasoning for Mathematical Problem Solving 
[[arxiv](https://arxiv.org/abs/2502.12022)] [[cool](https://papers.cool/arxiv/2502.12022)] [[pdf](https://arxiv.org/pdf/2502.12022)]
> **Authors**: Xin Xu,Yan Xu,Tianhao Chen,Yuchen Yan,Chengwu Liu,Zaoyu Chen,Yufei Wang,Yichun Yin,Yasheng Wang,Lifeng Shang,Qun Liu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 8 pages
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Existing approaches to mathematical reasoning with large language models (LLMs) rely on Chain-of-Thought (CoT) for generalizability or Tool-Integrated Reasoning (TIR) for precise computation. While efforts have been made to combine these methods, they primarily rely on post-selection or predefined strategies, leaving an open question: whether LLMs can autonomously adapt their reasoning strategy based on their inherent capabilities. In this work, we propose TATA (Teaching LLMs According to Their Aptitude), an adaptive framework that enables LLMs to personalize their reasoning strategy spontaneously, aligning it with their intrinsic aptitude. TATA incorporates base-LLM-aware data selection during supervised fine-tuning (SFT) to tailor training data to the model's unique abilities. This approach equips LLMs to autonomously determine and apply the appropriate reasoning strategy at test time. We evaluate TATA through extensive experiments on six mathematical reasoning benchmarks, using both general-purpose and math-specialized LLMs. Empirical results demonstrate that TATA effectively combines the complementary strengths of CoT and TIR, achieving superior or comparable performance with improved inference efficiency compared to TIR alone. Further analysis underscores the critical role of aptitude-aware data selection in enabling LLMs to make effective and adaptive reasoning decisions and align reasoning strategies with model capabilities.

### Atom of Thoughts for Markov LLM Test-Time Scaling 
[[arxiv](https://arxiv.org/abs/2502.12018)] [[cool](https://papers.cool/arxiv/2502.12018)] [[pdf](https://arxiv.org/pdf/2502.12018)]
> **Authors**: Fengwei Teng,Zhaoyang Yu,Quan Shi,Jiayi Zhang,Chenglin Wu,Yuyu Luo
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Large Language Models (LLMs) achieve superior performance through training-time scaling, and test-time scaling further enhances their capabilities by conducting effective reasoning during inference. However, as the scale of reasoning increases, existing test-time scaling methods suffer from accumulated historical information, which not only wastes computational resources but also interferes with effective reasoning. To address this issue, we observe that complex reasoning progress is often achieved by solving a sequence of independent subquestions, each being self-contained and verifiable. These subquestions are essentially atomic questions, relying primarily on their current state rather than accumulated history, similar to the memoryless transitions in a Markov process. Based on this observation, we propose Atom of Thoughts (AoT), where each state transition in the reasoning process consists of decomposing the current question into a dependency-based directed acyclic graph and contracting its subquestions, forming a new atomic question state. This iterative decomposition-contraction process continues until reaching directly solvable atomic questions, naturally realizing Markov transitions between question states. Furthermore, these atomic questions can be seamlessly integrated into existing test-time scaling methods, enabling AoT to serve as a plug-in enhancement for improving reasoning capabilities. Experiments across six benchmarks demonstrate the effectiveness of AoT both as a standalone framework and a plug-in enhancement. Notably, on HotpotQA, when applied to gpt-4o-mini, AoT achieves an 80.6% F1 score, surpassing o3-mini by 3.4% and DeepSeek-R1 by 10.6%. The code will be available at https://github.com/qixucen/atom.

### Demographic Attributes Prediction from Speech Using WavLM Embeddings 
[[arxiv](https://arxiv.org/abs/2502.12007)] [[cool](https://papers.cool/arxiv/2502.12007)] [[pdf](https://arxiv.org/pdf/2502.12007)]
> **Authors**: Yuchen Yang,Thomas Thebaud,Najim Dehak
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 6 pages, accepted by The Conference on Information Sciences and Systems (CISS)
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: This paper introduces a general classifier based on WavLM features, to infer demographic characteristics, such as age, gender, native language, education, and country, from speech. Demographic feature prediction plays a crucial role in applications like language learning, accessibility, and digital forensics, enabling more personalized and inclusive technologies. Leveraging pretrained models for embedding extraction, the proposed framework identifies key acoustic and linguistic fea-tures associated with demographic attributes, achieving a Mean Absolute Error (MAE) of 4.94 for age prediction and over 99.81% accuracy for gender classification across various datasets. Our system improves upon existing models by up to relative 30% in MAE and up to relative 10% in accuracy and F1 scores across tasks, leveraging a diverse range of datasets and large pretrained models to ensure robustness and generalizability. This study offers new insights into speaker diversity and provides a strong foundation for future research in speech-based demographic profiling.

### Merging Language and Domain Specific Models: The Impact on Technical Vocabulary Acquisition 
[[arxiv](https://arxiv.org/abs/2502.12001)] [[cool](https://papers.cool/arxiv/2502.12001)] [[pdf](https://arxiv.org/pdf/2502.12001)]
> **Authors**: Thibault Rousset,Taisei Kakibuchi,Yusuke Sasaki,Yoshihide Nomura
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Presented at the 263rd IPSJ-NL Workshop
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: This paper investigates the integration of technical vocabulary in merged language models. We explore the knowledge transfer mechanisms involved when combining a general-purpose language-specific model with a domain-specific model, focusing on the resulting model's comprehension of technical jargon. Our experiments analyze the impact of this merging process on the target model's proficiency in handling specialized terminology. We present a quantitative evaluation of the performance of the merged model, comparing it with that of the individual constituent models. The findings offer insights into the effectiveness of different model merging methods for enhancing domain-specific knowledge and highlight potential challenges and future directions in leveraging these methods for cross-lingual knowledge transfer in Natural Language Processing.

### Presumed Cultural Identity: How Names Shape LLM Responses 
[[arxiv](https://arxiv.org/abs/2502.11995)] [[cool](https://papers.cool/arxiv/2502.11995)] [[pdf](https://arxiv.org/pdf/2502.11995)]
> **Authors**: Siddhesh Pawar,Arnav Arora,Lucie-Aimée Kaffee,Isabelle Augenstein
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 23 Pages, 13 Figures, 4 Tables
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Names are deeply tied to human identity. They can serve as markers of individuality, cultural heritage, and personal history. However, using names as a core indicator of identity can lead to over-simplification of complex identities. When interacting with LLMs, user names are an important point of information for personalisation. Names can enter chatbot conversations through direct user input (requested by chatbots), as part of task contexts such as CV reviews, or as built-in memory features that store user information for personalisation. We study biases associated with names by measuring cultural presumptions in the responses generated by LLMs when presented with common suggestion-seeking queries, which might involve making assumptions about the user. Our analyses demonstrate strong assumptions about cultural identity associated with names present in LLM generations across multiple cultures. Our work has implications for designing more nuanced personalisation systems that avoid reinforcing stereotypes while maintaining meaningful customisation.

### Generating Text from Uniform Meaning Representation 
[[arxiv](https://arxiv.org/abs/2502.11973)] [[cool](https://papers.cool/arxiv/2502.11973)] [[pdf](https://arxiv.org/pdf/2502.11973)]
> **Authors**: Emma Markle,Reihaneh Iranmanesh,Shira Wein
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Uniform Meaning Representation (UMR) is a recently developed graph-based semantic representation, which expands on Abstract Meaning Representation (AMR) in a number of ways, in particular through the inclusion of document-level information and multilingual flexibility. In order to effectively adopt and leverage UMR for downstream tasks, efforts must be placed toward developing a UMR technological ecosystem. Though still limited amounts of UMR annotations have been produced to date, in this work, we investigate the first approaches to producing text from multilingual UMR graphs: (1) a pipeline conversion of UMR to AMR, then using AMR-to-text generation models, (2) fine-tuning large language models with UMR data, and (3) fine-tuning existing AMR-to-text generation models with UMR data. Our best performing model achieves a multilingual BERTscore of 0.825 for English and 0.882 for Chinese when compared to the reference, which is a promising indication of the effectiveness of fine-tuning approaches for UMR-to-text generation with even limited amounts of UMR data.

### Navigating the Helpfulness-Truthfulness Trade-Off with Uncertainty-Aware Instruction Fine-Tuning 
[[arxiv](https://arxiv.org/abs/2502.11962)] [[cool](https://papers.cool/arxiv/2502.11962)] [[pdf](https://arxiv.org/pdf/2502.11962)]
> **Authors**: Tianyi Wu,Jingwei Ni,Bryan Hooi,Jiaheng Zhang,Elliott Ash,See-Kiong Ng,Mrinmaya Sachan,Markus Leippold
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Instruction Fine-tuning (IFT) can enhance the helpfulness of Large Language Models (LLMs), but it may lower their truthfulness. This trade-off arises because IFT steers LLMs to generate responses with long-tail knowledge that is not well covered during pre-training, leading to more informative but less truthful answers when generalizing to unseen tasks. In this paper, we empirically demonstrate this helpfulness-truthfulness trade-off in IFT and propose $\textbf{UNIT}$, a novel IFT paradigm to address it. UNIT teaches LLMs to recognize their uncertainty and explicitly reflect it at the end of their responses. Experimental results show that UNIT-tuned models maintain their helpfulness while distinguishing between certain and uncertain claims, thereby reducing hallucinations.

### Can Your Uncertainty Scores Detect Hallucinated Entity? 
[[arxiv](https://arxiv.org/abs/2502.11948)] [[cool](https://papers.cool/arxiv/2502.11948)] [[pdf](https://arxiv.org/pdf/2502.11948)]
> **Authors**: Min-Hsuan Yeh,Max Kamachee,Seongheon Park,Yixuan Li
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: To mitigate the impact of hallucination nature of LLMs, many studies propose detecting hallucinated generation through uncertainty estimation. However, these approaches predominantly operate at the sentence or paragraph level, failing to pinpoint specific spans or entities responsible for hallucinated content. This lack of granularity is especially problematic for long-form outputs that mix accurate and fabricated information. To address this limitation, we explore entity-level hallucination detection. We propose a new data set, HalluEntity, which annotates hallucination at the entity level. Based on the dataset, we comprehensively evaluate uncertainty-based hallucination detection approaches across 17 modern LLMs. Our experimental results show that uncertainty estimation approaches focusing on individual token probabilities tend to over-predict hallucinations, while context-aware methods show better but still suboptimal performance. Through an in-depth qualitative study, we identify relationships between hallucination tendencies and linguistic properties and highlight important directions for future research.

### Step-Audio: Unified Understanding and Generation in Intelligent Speech Interaction 
[[arxiv](https://arxiv.org/abs/2502.11946)] [[cool](https://papers.cool/arxiv/2502.11946)] [[pdf](https://arxiv.org/pdf/2502.11946)]
> **Authors**: Ailin Huang,Boyong Wu,Bruce Wang,Chao Yan,Chen Hu,Chengli Feng,Fei Tian,Feiyu Shen,Jingbei Li,Mingrui Chen,Peng Liu,Ruihang Miao,Wang You,Xi Chen,Xuerui Yang,Yechang Huang,Yuxiang Zhang,Zheng Gong,Zixin Zhang,Hongyu Zhou,Jianjian Sun,Brian Li,Chengting Feng,Changyi Wan,Hanpeng Hu, et al. (120 additional authors not shown)
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,人机交互,声音,音频和语音处理
- **Abstract**: Real-time speech interaction, serving as a fundamental interface for human-machine collaboration, holds immense potential. However, current open-source models face limitations such as high costs in voice data collection, weakness in dynamic control, and limited intelligence. To address these challenges, this paper introduces Step-Audio, the first production-ready open-source solution. Key contributions include: 1) a 130B-parameter unified speech-text multi-modal model that achieves unified understanding and generation, with the Step-Audio-Chat version open-sourced; 2) a generative speech data engine that establishes an affordable voice cloning framework and produces the open-sourced lightweight Step-Audio-TTS-3B model through distillation; 3) an instruction-driven fine control system enabling dynamic adjustments across dialects, emotions, singing, and RAP; 4) an enhanced cognitive architecture augmented with tool calling and role-playing abilities to manage complex tasks effectively. Based on our new StepEval-Audio-360 evaluation benchmark, Step-Audio achieves state-of-the-art performance in human evaluations, especially in terms of instruction following. On open-source benchmarks like LLaMA Question, shows 9.3% average performance improvement, demonstrating our commitment to advancing the development of open-source multi-modal language technologies. Our code and models are available at https://github.com/stepfun-ai/Step-Audio.

### On Representational Dissociation of Language and Arithmetic in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.11932)] [[cool](https://papers.cool/arxiv/2502.11932)] [[pdf](https://arxiv.org/pdf/2502.11932)]
> **Authors**: Riku Kisako,Tatsuki Kuribayashi,Ryohei Sasano
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The association between language and (non-linguistic) thinking ability in humans has long been debated, and recently, neuroscientific evidence of brain activity patterns has been considered. Such a scientific context naturally raises an interdisciplinary question -- what about such a language-thought dissociation in large language models (LLMs)? In this paper, as an initial foray, we explore this question by focusing on simple arithmetic skills (e.g., $1+2=$ ?) as a thinking ability and analyzing the geometry of their encoding in LLMs' representation space. Our experiments with linear classifiers and cluster separability tests demonstrate that simple arithmetic equations and general language input are encoded in completely separated regions in LLMs' internal representation space across all the layers, which is also supported with more controlled stimuli (e.g., spelled-out equations). These tentatively suggest that arithmetic reasoning is mapped into a distinct region from general language input, which is in line with the neuroscientific observations of human brain activations, while we also point out their somewhat cognitively implausible geometric properties.

### BRIGHTER: BRIdging the Gap in Human-Annotated Textual Emotion Recognition Datasets for 28 Languages 
[[arxiv](https://arxiv.org/abs/2502.11926)] [[cool](https://papers.cool/arxiv/2502.11926)] [[pdf](https://arxiv.org/pdf/2502.11926)]
> **Authors**: Shamsuddeen Hassan Muhammad,Nedjma Ousidhoum,Idris Abdulmumin,Jan Philip Wahle,Terry Ruas,Meriem Beloucif,Christine de Kock,Nirmal Surange,Daniela Teodorescu,Ibrahim Said Ahmad,David Ifeoluwa Adelani,Alham Fikri Aji,Felermino D. M. A. Ali,Ilseyar Alimova,Vladimir Araujo,Nikolay Babakov,Naomi Baes,Ana-Maria Bucur,Andiswa Bukula,Guanqun Cao,Rodrigo Tufino Cardenas,Rendi Chevi,Chiamaka Ijeoma Chukwuneke,Alexandra Ciobotaru,Daryna Dementieva, et al. (23 additional authors not shown)
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 20 pages, under review
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: People worldwide use language in subtle and complex ways to express emotions. While emotion recognition -- an umbrella term for several NLP tasks -- significantly impacts different applications in NLP and other fields, most work in the area is focused on high-resource languages. Therefore, this has led to major disparities in research and proposed solutions, especially for low-resource languages that suffer from the lack of high-quality datasets. In this paper, we present BRIGHTER -- a collection of multilabeled emotion-annotated datasets in 28 different languages. BRIGHTER covers predominantly low-resource languages from Africa, Asia, Eastern Europe, and Latin America, with instances from various domains annotated by fluent speakers. We describe the data collection and annotation processes and the challenges of building these datasets. Then, we report different experimental results for monolingual and crosslingual multi-label emotion identification, as well as intensity-level emotion recognition. We investigate results with and without using LLMs and analyse the large variability in performance across languages and text domains. We show that BRIGHTER datasets are a step towards bridging the gap in text-based emotion recognition and discuss their impact and utility.

### EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.11916)] [[cool](https://papers.cool/arxiv/2502.11916)] [[pdf](https://arxiv.org/pdf/2502.11916)]
> **Authors**: Jiamin Su,Yibo Yan,Fangteng Fu,Han Zhang,Jingheng Ye,Xiang Liu,Jiahao Huo,Huiyu Zhou,Xuming Hu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: JS and YY are co-first authors. XH is the corresponding author
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Automated Essay Scoring (AES) plays a crucial role in educational assessment by providing scalable and consistent evaluations of writing tasks. However, traditional AES systems face three major challenges: (1) reliance on handcrafted features that limit generalizability, (2) difficulty in capturing fine-grained traits like coherence and argumentation, and (3) inability to handle multimodal contexts. In the era of Multimodal Large Language Models (MLLMs), we propose EssayJudge, the first multimodal benchmark to evaluate AES capabilities across lexical-, sentence-, and discourse-level traits. By leveraging MLLMs' strengths in trait-specific scoring and multimodal context understanding, EssayJudge aims to offer precise, context-rich evaluations without manual feature engineering, addressing longstanding AES limitations. Our experiments with 18 representative MLLMs reveal gaps in AES performance compared to human evaluation, particularly in discourse-level traits, highlighting the need for further advancements in MLLM-based AES research. Our dataset and code will be available upon acceptance.

### MMRC: A Large-Scale Benchmark for Understanding Multimodal Large Language Model in Real-World Conversation 
[[arxiv](https://arxiv.org/abs/2502.11903)] [[cool](https://papers.cool/arxiv/2502.11903)] [[pdf](https://arxiv.org/pdf/2502.11903)]
> **Authors**: Haochen Xue,Feilong Tang,Ming Hu,Yexin Liu,Qidong Huang,Yulong Li,Chengzhi Liu,Zhongxing Xu,Chong Zhang,Chun-Mei Feng,Yutong Xie,Imran Razzak,Zongyuan Ge,Jionglong Su,Junjun He,Yu Qiao
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Recent multimodal large language models (MLLMs) have demonstrated significant potential in open-ended conversation, generating more accurate and personalized responses. However, their abilities to memorize, recall, and reason in sustained interactions within real-world scenarios remain underexplored. This paper introduces MMRC, a Multi-Modal Real-world Conversation benchmark for evaluating six core open-ended abilities of MLLMs: information extraction, multi-turn reasoning, information update, image management, memory recall, and answer refusal. With data collected from real-world scenarios, MMRC comprises 5,120 conversations and 28,720 corresponding manually labeled questions, posing a significant challenge to existing MLLMs. Evaluations on 20 MLLMs in MMRC indicate an accuracy drop during open-ended interactions. We identify four common failure patterns: long-term memory degradation, inadequacies in updating factual knowledge, accumulated assumption of error propagation, and reluctance to say no. To mitigate these issues, we propose a simple yet effective NOTE-TAKING strategy, which can record key information from the conversation and remind the model during its responses, enhancing conversational capabilities. Experiments across six MLLMs demonstrate significant performance improvements.

### Building A Proof-Oriented Programmer That Is 64% Better Than GPT-4o Under Data Scarsity 
[[arxiv](https://arxiv.org/abs/2502.11901)] [[cool](https://papers.cool/arxiv/2502.11901)] [[pdf](https://arxiv.org/pdf/2502.11901)]
> **Authors**: Dylan Zhang,Justin Wang,Tianran Sun
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,编程语言,软件工程
- **Abstract**: Existing LMs struggle with proof-oriented programming due to data scarcity, which manifest in two key ways: (1) a lack of sufficient corpora for proof-oriented programming languages such as F*, and (2) the absence of large-scale, project-level proof-oriented implementations that can teach the model the intricate reasoning process when performing proof-oriented programming. We present the first on synthetic data augmentation for project level proof oriented programming for both generation and repair. Our method addresses data scarcity by synthesizing basic proof-oriented programming problems for proficiency in that language; incorporating diverse coding data for reasoning capability elicitation and creating new proofs and repair data within existing repositories. This approach enables language models to both synthesize and repair proofs for function- and repository-level code. We show that our fine-tuned 14B parameter model, PoPilot, can exceed the performance of the models that outperforms GPT-4o in project-level proof-oriented programming by 64% relative margin, and can improve GPT-4o's performance by 54% by repairing its outputs over GPT-4o's self-repair.

### VAQUUM: Are Vague Quantifiers Grounded in Visual Data? 
[[arxiv](https://arxiv.org/abs/2502.11874)] [[cool](https://papers.cool/arxiv/2502.11874)] [[pdf](https://arxiv.org/pdf/2502.11874)]
> **Authors**: Hugh Mee Wong,Rick Nouwen,Albert Gatt
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Under review, 12 pages for main paper (5 figures), 15 pages including appendix (2 figures)
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Vague quantifiers such as "a few" and "many" are influenced by many contextual factors, including how many objects are present in a given context. In this work, we evaluate the extent to which vision-and-language models (VLMs) are compatible with humans when producing or judging the appropriateness of vague quantifiers in visual contexts. We release a novel dataset, VAQUUM, containing 20300 human ratings on quantified statements across a total of 1089 images. Using this dataset, we compare human judgments and VLM predictions using three different evaluation methods. Our findings show that VLMs, like humans, are influenced by object counts in vague quantifier use. However, we find significant inconsistencies across models in different evaluation settings, suggesting that judging and producing vague quantifiers rely on two different processes.

### Southern Newswire Corpus: A Large-Scale Dataset of Mid-Century Wire Articles Beyond the Front Page 
[[arxiv](https://arxiv.org/abs/2502.11866)] [[cool](https://papers.cool/arxiv/2502.11866)] [[pdf](https://arxiv.org/pdf/2502.11866)]
> **Authors**: Michael McRae
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: I introduce a new large-scale dataset of historical wire articles from U.S. Southern newspapers, spanning 1960-1975 and covering multiple wire services: The Associated Press, United Press International, Newspaper Enterprise Association. Unlike prior work focusing on front-page content, this dataset captures articles across the entire newspaper, offering broader insight into mid-century Southern coverage. The dataset includes a version that has undergone an LLM-based text cleanup pipeline to reduce OCR noise, enhancing its suitability for quantitative text analysis. Additionally, duplicate versions of articles are retained to enable analysis of editorial differences in language and framing across newspapers. Each article is tagged by wire service, facilitating comparative studies of editorial patterns across agencies. This resource opens new avenues for research in computational social science, digital humanities, and historical linguistics, providing a detailed perspective on how Southern newspapers relayed national and international news during a transformative period in American history. The dataset will be made available upon publication or request for research purposes.

### Understanding In-Context Machine Translation for Low-Resource Languages: A Case Study on Manchu 
[[arxiv](https://arxiv.org/abs/2502.11862)] [[cool](https://papers.cool/arxiv/2502.11862)] [[pdf](https://arxiv.org/pdf/2502.11862)]
> **Authors**: Renhao Pei,Yihong Liu,Peiqin Lin,François Yvon,Hinrich Schütze
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: preprint
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: In-context machine translation (MT) with large language models (LLMs) is a promising approach for low-resource MT, as it can readily take advantage of linguistic resources such as grammar books and dictionaries. Such resources are usually selectively integrated into the prompt so that LLMs can directly perform translation without any specific training, via their in-context learning capability (ICL). However, the relative importance of each type of resource e.g., dictionary, grammar book, and retrieved parallel examples, is not entirely clear. To address this gap, this study systematically investigates how each resource and its quality affects the translation performance, with the Manchu language as our case study. To remove any prior knowledge of Manchu encoded in the LLM parameters and single out the effect of ICL, we also experiment with an encrypted version of Manchu texts. Our results indicate that high-quality dictionaries and good parallel examples are very helpful, while grammars hardly help. In a follow-up study, we showcase a promising application of in-context MT: parallel data augmentation as a way to bootstrap the conventional MT model. When monolingual data abound, generating synthetic parallel data through in-context MT offers a pathway to mitigate data scarcity and build effective and efficient low-resource neural MT systems.

### Exploring Large Language Models in Healthcare: Insights into Corpora Sources, Customization Strategies, and Evaluation Metrics 
[[arxiv](https://arxiv.org/abs/2502.11861)] [[cool](https://papers.cool/arxiv/2502.11861)] [[pdf](https://arxiv.org/pdf/2502.11861)]
> **Authors**: Shuqi Yang,Mingrui Jing,Shuai Wang,Jiaxin Kou,Manfei Shi,Weijie Xing,Yan Hu,Zheng Zhu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 45 pages, 1 figure, 5 tables
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: This study reviewed the use of Large Language Models (LLMs) in healthcare, focusing on their training corpora, customization techniques, and evaluation metrics. A systematic search of studies from 2021 to 2024 identified 61 articles. Four types of corpora were used: clinical resources, literature, open-source datasets, and web-crawled data. Common construction techniques included pre-training, prompt engineering, and retrieval-augmented generation, with 44 studies combining multiple methods. Evaluation metrics were categorized into process, usability, and outcome metrics, with outcome metrics divided into model-based and expert-assessed outcomes. The study identified critical gaps in corpus fairness, which contributed to biases from geographic, cultural, and socio-economic factors. The reliance on unverified or unstructured data highlighted the need for better integration of evidence-based clinical guidelines. Future research should focus on developing a tiered corpus architecture with vetted sources and dynamic weighting, while ensuring model transparency. Additionally, the lack of standardized evaluation frameworks for domain-specific models called for comprehensive validation of LLMs in real-world healthcare settings.

### LLMs as a synthesis between symbolic and continuous approaches to language 
[[arxiv](https://arxiv.org/abs/2502.11856)] [[cool](https://papers.cool/arxiv/2502.11856)] [[pdf](https://arxiv.org/pdf/2502.11856)]
> **Authors**: Gemma Boleda
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Under review
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Since the middle of the 20th century, a fierce battle is being fought between symbolic and continuous approaches to language and cognition. The success of deep learning models, and LLMs in particular, has been alternatively taken as showing that the continuous camp has won, or dismissed as an irrelevant engineering development. However, in this position paper I argue that deep learning models for language actually represent a synthesis between the two traditions. This is because 1) deep learning architectures allow for both continuous/distributed and symbolic/discrete-like representations and computations; 2) models trained on language make use this flexibility. In particular, I review recent research in mechanistic interpretability that showcases how a substantial part of morphosyntactic knowledge is encoded in a near-discrete fashion in LLMs. This line of research suggests that different behaviors arise in an emergent fashion, and models flexibly alternate between the two modes (and everything in between) as needed. This is possibly one of the main reasons for their wild success; and it is also what makes them particularly interesting for the study of language and cognition. Is it time for peace?

### Can LLM Agents Maintain a Persona in Discourse? 
[[arxiv](https://arxiv.org/abs/2502.11843)] [[cool](https://papers.cool/arxiv/2502.11843)] [[pdf](https://arxiv.org/pdf/2502.11843)]
> **Authors**: Pranav Bhandari,Nicolas Fay,Michael Wise,Amitava Datta,Stephanie Meek,Usman Naseem,Mehwish Nasim
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: :I.2.7
- **标题**: None
- **领域**: 计算语言学,人工智能,社交和信息网络
- **Abstract**: Large Language Models (LLMs) are widely used as conversational agents, exploiting their capabilities in various sectors such as education, law, medicine, and more. However, LLMs are often subjected to context-shifting behaviour, resulting in a lack of consistent and interpretable personality-aligned interactions. Adherence to psychological traits lacks comprehensive analysis, especially in the case of dyadic (pairwise) conversations. We examine this challenge from two viewpoints, initially using two conversation agents to generate a discourse on a certain topic with an assigned personality from the OCEAN framework (Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism) as High/Low for each trait. This is followed by using multiple judge agents to infer the original traits assigned to explore prediction consistency, inter-model agreement, and alignment with the assigned personality. Our findings indicate that while LLMs can be guided toward personality-driven dialogue, their ability to maintain personality traits varies significantly depending on the combination of models and discourse settings. These inconsistencies emphasise the challenges in achieving stable and interpretable personality-aligned interactions in LLMs.

### Text Classification in the LLM Era -- Where do we stand? 
[[arxiv](https://arxiv.org/abs/2502.11830)] [[cool](https://papers.cool/arxiv/2502.11830)] [[pdf](https://arxiv.org/pdf/2502.11830)]
> **Authors**: Sowmya Vajjala,Shwetali Shimangaud
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Pre-print
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models revolutionized NLP and showed dramatic performance improvements across several tasks. In this paper, we investigated the role of such language models in text classification and how they compare with other approaches relying on smaller pre-trained language models. Considering 32 datasets spanning 8 languages, we compared zero-shot classification, few-shot fine-tuning and synthetic data based classifiers with classifiers built using the complete human labeled dataset. Our results show that zero-shot approaches do well for sentiment classification, but are outperformed by other approaches for the rest of the tasks, and synthetic data sourced from multiple LLMs can build better classifiers than zero-shot open LLMs. We also see wide performance disparities across languages in all the classification scenarios. We expect that these findings would guide practitioners working on developing text classification systems across languages.

### Code-Vision: Evaluating Multimodal LLMs Logic Understanding and Code Generation Capabilities 
[[arxiv](https://arxiv.org/abs/2502.11829)] [[cool](https://papers.cool/arxiv/2502.11829)] [[pdf](https://arxiv.org/pdf/2502.11829)]
> **Authors**: Hanbin Wang,Xiaoxuan Zhou,Zhipeng Xu,Keyuan Cheng,Yuxin Zuo,Kai Tian,Jingwei Song,Junting Lu,Wenhui Hu,Xueyang Liu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 15 pages
- **标题**: None
- **领域**: 计算语言学,人工智能,软件工程
- **Abstract**: This paper introduces Code-Vision, a benchmark designed to evaluate the logical understanding and code generation capabilities of Multimodal Large Language Models (MLLMs). It challenges MLLMs to generate a correct program that fulfills specific functionality requirements based on a given flowchart, which visually represents the desired algorithm or process. Code-Vision comprises three subsets: HumanEval-V, Algorithm, and MATH, which evaluate MLLMs' coding abilities across basic programming, algorithmic, and mathematical problem-solving domains. Our experiments evaluate 12 MLLMs on Code-Vision. Experimental results demonstrate that there is a large performance difference between proprietary and open-source models. On Hard problems, GPT-4o can achieve 79.3% pass@1, but the best open-source model only achieves 15%. Further experiments reveal that Code-Vision can pose unique challenges compared to other multimodal reasoning benchmarks MMCode and MathVista. We also explore the reason for the poor performance of the open-source models. All data and codes are available at https://github.com/wanghanbinpanda/CodeVision.

### M-ABSA: A Multilingual Dataset for Aspect-Based Sentiment Analysis 
[[arxiv](https://arxiv.org/abs/2502.11824)] [[cool](https://papers.cool/arxiv/2502.11824)] [[pdf](https://arxiv.org/pdf/2502.11824)]
> **Authors**: Chengyan Wu,Bolei Ma,Yihong Liu,Zheyu Zhang,Ningyuan Deng,Yanshu Li,Baolan Chen,Yi Zhang,Barbara Plank,Yun Xue
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Aspect-based sentiment analysis (ABSA) is a crucial task in information extraction and sentiment analysis, aiming to identify aspects with associated sentiment elements in text. However, existing ABSA datasets are predominantly English-centric, limiting the scope for multilingual evaluation and research. To bridge this gap, we present M-ABSA, a comprehensive dataset spanning 7 domains and 21 languages, making it the most extensive multilingual parallel dataset for ABSA to date. Our primary focus is on triplet extraction, which involves identifying aspect terms, aspect categories, and sentiment polarities. The dataset is constructed through an automatic translation process with human review to ensure quality. We perform extensive experiments using various baselines to assess performance and compatibility on M-ABSA. Our empirical findings highlight that the dataset enables diverse evaluation tasks, such as multilingual and multi-domain transfer learning, and large language model evaluation, underscoring its inclusivity and its potential to drive advancements in multilingual ABSA research.

### Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit Analysis 
[[arxiv](https://arxiv.org/abs/2502.11812)] [[cool](https://papers.cool/arxiv/2502.11812)] [[pdf](https://arxiv.org/pdf/2502.11812)]
> **Authors**: Xu Wang,Yan Hu,Wenyu Du,Reynold Cheng,Benyou Wang,Difan Zou
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 25 pages
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Fine-tuning significantly improves the performance of Large Language Models (LLMs), yet its underlying mechanisms remain poorly understood. This paper aims to provide an in-depth interpretation of the fine-tuning process through circuit analysis, a popular tool in Mechanistic Interpretability (MI). Unlike previous studies \cite{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity} that focus on tasks where pre-trained models already perform well, we develop a set of mathematical tasks where fine-tuning yields substantial performance gains, which are closer to the practical setting. In our experiments, we identify circuits at various checkpoints during fine-tuning and examine the interplay between circuit analysis, fine-tuning methods, and task complexities. First, we find that while circuits maintain high node similarity before and after fine-tuning, their edges undergo significant changes, which is in contrast to the previous work \cite{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity} that show circuits only add some additional components after fine-tuning. Based on these observations, we develop a circuit-aware Low-Rank Adaptation (LoRA) method, which assigns ranks to layers based on edge changes in the circuits. Experimental results demonstrate that our circuit-based LoRA algorithm achieves an average performance improvement of 2.46\% over standard LoRA with similar parameter sizes. Furthermore, we explore how combining circuits from subtasks can enhance fine-tuning in compositional tasks, providing new insights into the design of such tasks and deepening the understanding of circuit dynamics and fine-tuning mechanisms.

### FineFilter: A Fine-grained Noise Filtering Mechanism for Retrieval-Augmented Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.11811)] [[cool](https://papers.cool/arxiv/2502.11811)] [[pdf](https://arxiv.org/pdf/2502.11811)]
> **Authors**: Qianchi Zhang,Hainan Zhang,Liang Pang,Hongwei Zheng,Yongxin Tong,Zhiming Zheng
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Retrieved documents containing noise will hinder Retrieval-Augmented Generation (RAG) from detecting answer clues, necessitating noise filtering mechanisms to enhance accuracy. Existing methods use re-ranking or summarization to identify the most relevant sentences, but directly and accurately locating answer clues from these large-scale and complex documents remains challenging. Unlike these document-level operations, we treat noise filtering as a sentence-level MinMax optimization problem: first identifying the potential clues from multiple documents using contextual information, then ranking them by relevance, and finally retaining the least clues through truncation. In this paper, we propose FineFilter, a novel fine-grained noise filtering mechanism for RAG consisting of a clue extractor, a re-ranker, and a truncator. We optimize each module to tackle complex reasoning challenges: (1) Clue extractor firstly uses sentences containing the answer and similar ones as fine-tuned targets, aiming at extracting sufficient potential clues; (2) Re-ranker is trained to prioritize effective clues based on the real feedback from generation module, with clues capable of generating correct answer as positive samples and others as negative; (3) Truncator takes the minimum clues needed to answer the question (truncation point) as fine-tuned targets, and performs truncation on the re-ranked clues to achieve fine-grained noise filtering. Experiments on three QA datasets demonstrate that FineFilter significantly outperforms baselines in terms of performance and inference cost. Further analysis on each module shows the effectiveness of our optimizations for complex reasoning.

### Exploring Translation Mechanism of Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.11806)] [[cool](https://papers.cool/arxiv/2502.11806)] [[pdf](https://arxiv.org/pdf/2502.11806)]
> **Authors**: Hongbin Zhang,Kehai Chen,Xuefeng Bai,Xiucheng Li,Yang Xiang,Min Zhang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) have succeeded remarkably in multilingual translation tasks. However, the inherent translation mechanisms of LLMs remain poorly understood, largely due to sophisticated architectures and vast parameter scales. In response to this issue, this study explores the translation mechanism of LLM from the perspective of computational components (e.g., attention heads and MLPs). Path patching is utilized to explore causal relationships between components, detecting those crucial for translation tasks and subsequently analyzing their behavioral patterns in human-interpretable terms. Comprehensive analysis reveals that translation is predominantly facilitated by a sparse subset of specialized attention heads (less than 5\%), which extract source language, indicator, and positional features. MLPs subsequently integrate and process these features by transiting towards English-centric latent representations. Notably, building on the above findings, targeted fine-tuning of only 64 heads achieves translation improvement comparable to full-parameter tuning while preserving general capabilities.

### Personality Editing for Language Models through Relevant Knowledge Editing 
[[arxiv](https://arxiv.org/abs/2502.11789)] [[cool](https://papers.cool/arxiv/2502.11789)] [[pdf](https://arxiv.org/pdf/2502.11789)]
> **Authors**: Seojin Hwang,Yumin Kim,Byeongjeong Kim,Hwanhee Lee
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 15 pages, 3 figures, 16 tables
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models (LLMs) play a vital role in applications like conversational agents and content creation, where controlling a model's personality is crucial for maintaining tone, consistency, and engagement. However, traditional prompt-based techniques for controlling personality often fall short, as they do not effectively mitigate the model's inherent biases. In this paper, we introduce a novel method PALETTE that enhances personality control through knowledge editing. By generating adjustment queries inspired by psychological assessments, our approach systematically adjusts responses to personality-related queries similar to modifying factual knowledge, thereby achieving controlled shifts in personality traits. Experimental results from both automatic and human evaluations demonstrate that our method enables more stable and well-balanced personality control in LLMs.

### Efficient Response Generation Method Selection for Fine-Tuning Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.11779)] [[cool](https://papers.cool/arxiv/2502.11779)] [[pdf](https://arxiv.org/pdf/2502.11779)]
> **Authors**: Xuan Ren,Qi Chen,Lingqiao Liu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The training data for fine-tuning large language models (LLMs) is typically structured as input-output pairs. However, for many tasks, there can be multiple equally valid output variations for the same input. Recent studies have observed that the choice of output variation used in training can affect the model's performance. This raises an important question: how can we generate the most effective output from the many possible response generation strategy options? Rather than relying on the traditional but resource-intensive train-and-evaluate approach, this paper proposes a scalable, approximate method for estimating the quality of a small subset of generated training data derived from the same input. We then evaluate how well this small subset of generated output fits the target model we are trying to train. We present a large-scale benchmark covering diverse reasoning-based datasets to support our study. The central idea is that a good output should closely resemble the output generated by the target LLM. We formalize this 'closeness' as the expected alignment score between a candidate output and the output sampled from the target LLM. We connect this measurement to the perplexity metric used in previous literature and demonstrate that leveraging an alignment-based metric can provide better predictions of model performance. Using this strategy, we can evaluate a small subset of the generated output from each response generation strategy option, then select the most effective strategy. We show that an LLM trained on data generated by the selected strategy could lead to a significant performance gain in many cases.

### The Validation Gap: A Mechanistic Analysis of How Language Models Compute Arithmetic but Fail to Validate It 
[[arxiv](https://arxiv.org/abs/2502.11771)] [[cool](https://papers.cool/arxiv/2502.11771)] [[pdf](https://arxiv.org/pdf/2502.11771)]
> **Authors**: Leonardo Bertolazzi,Philipp Mondorf,Barbara Plank,Raffaella Bernardi
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 34 pages, 31 figures
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: The ability of large language models (LLMs) to validate their output and identify potential errors is crucial for ensuring robustness and reliability. However, current research indicates that LLMs struggle with self-correction, encountering significant challenges in detecting errors. While studies have explored methods to enhance self-correction in LLMs, relatively little attention has been given to understanding the models' internal mechanisms underlying error detection. In this paper, we present a mechanistic analysis of error detection in LLMs, focusing on simple arithmetic problems. Through circuit analysis, we identify the computational subgraphs responsible for detecting arithmetic errors across four smaller-sized LLMs. Our findings reveal that all models heavily rely on $\textit{consistency heads}$--attention heads that assess surface-level alignment of numerical values in arithmetic solutions. Moreover, we observe that the models' internal arithmetic computation primarily occurs in higher layers, whereas validation takes place in middle layers, before the final arithmetic results are fully encoded. This structural dissociation between arithmetic computation and validation seems to explain why current LLMs struggle to detect even simple arithmetic errors.

### Warmup-Distill: Bridge the Distribution Mismatch between Teacher and Student before Knowledge Distillation 
[[arxiv](https://arxiv.org/abs/2502.11766)] [[cool](https://papers.cool/arxiv/2502.11766)] [[pdf](https://arxiv.org/pdf/2502.11766)]
> **Authors**: Zengkui Sun,Yijin Liu,Fandong Meng,Yufeng Chen,Jinan Xu,Jie Zhou
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 11 Pages, 4 figures, Code at https://github.com/Acerkoo/WarmupDistill
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The widespread deployment of Large Language Models (LLMs) is hindered by the high computational demands, making knowledge distillation (KD) crucial for developing compact smaller ones. However, the conventional KD methods endure the distribution mismatch issue between the teacher and student models, leading to the poor performance of distillation. For instance, the widely-used KL-based methods suffer the mode-averaging and mode-collapsing problems, since the mismatched probabitliy distribution between both models. Previous studies mainly optimize this issue via different distance calculations towards the distribution of both models. Unfortunately, the distribution mismatch issue still exists in the early stage of the distillation. Hence, to reduce the impact of distribution mismatch, we propose a simple yet efficient method, named Warmup-Distill, which aligns the distillation of the student to that of the teacher in advance of distillation. Specifically, we first detect the distribution of the student model in practical scenarios with its internal knowledge, and then modify the knowledge with low probability via the teacher as the checker. Consequently, Warmup-Distill aligns the internal student's knowledge to that of the teacher, which expands the distribution of the student with the teacher's, and assists the student model to learn better in the subsequent distillation. Experiments on the seven benchmarks demonstrate that Warmup-Distill could provide a warmup student more suitable for distillation, which outperforms the vanilla student by as least +0.4 averaged score among all benchmarks. Noteably, with the assistance of Warmup-Distill, the distillation on the math task could yield a further improvement, at most +1.9% accuracy.

### ReviewEval: An Evaluation Framework for AI-Generated Reviews 
[[arxiv](https://arxiv.org/abs/2502.11736)] [[cool](https://papers.cool/arxiv/2502.11736)] [[pdf](https://arxiv.org/pdf/2502.11736)]
> **Authors**: Chhavi Kirtani,Madhav Krishan Garg,Tejash Prasad,Tanmay Singhal,Murari Mandal,Dhruv Kumar
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Under review: 8 pages, 2 figures, 2 tables, 3 pages for appendix
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: The escalating volume of academic research, coupled with a shortage of qualified reviewers, necessitates innovative approaches to peer review. While large language model (LLMs) offer potential for automating this process, their current limitations include superficial critiques, hallucinations, and a lack of actionable insights. This research addresses these challenges by introducing a comprehensive evaluation framework for AI-generated reviews, that measures alignment with human evaluations, verifies factual accuracy, assesses analytical depth, and identifies actionable insights. We also propose a novel alignment mechanism that tailors LLM-generated reviews to the unique evaluation priorities of individual conferences and journals. To enhance the quality of these reviews, we introduce a self-refinement loop that iteratively optimizes the LLM's review prompts. Our framework establishes standardized metrics for evaluating AI-based review systems, thereby bolstering the reliability of AI-generated reviews in academic research.

### MT-RAIG: Novel Benchmark and Evaluation Framework for Retrieval-Augmented Insight Generation over Multiple Tables 
[[arxiv](https://arxiv.org/abs/2502.11735)] [[cool](https://papers.cool/arxiv/2502.11735)] [[pdf](https://arxiv.org/pdf/2502.11735)]
> **Authors**: Kwangwook Seo,Donguk Kwon,Dongha Lee
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Work in progress
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Recent advancements in table-based reasoning have expanded beyond factoid-level QA to address insight-level tasks, where systems should synthesize implicit knowledge in the table to provide explainable analyses. Although effective, existing studies remain confined to scenarios where a single gold table is given alongside the user query, failing to address cases where users seek comprehensive insights from multiple unknown tables. To bridge these gaps, we propose MT-RAIG Bench, design to evaluate systems on Retrieval-Augmented Insight Generation over Mulitple-Tables. Additionally, to tackle the suboptimality of existing automatic evaluation methods in the table domain, we further introduce a fine-grained evaluation framework MT-RAIG Eval, which achieves better alignment with human quality judgments on the generated insights. We conduct extensive experiments and reveal that even frontier LLMs still struggle with complex multi-table reasoning, establishing our MT-RAIG Bench as a challenging testbed for future research.

### Plant in Cupboard, Orange on Table, Book on Shelf. Benchmarking Practical Reasoning and Situation Modelling in a Text-Simulated Situated Environment 
[[arxiv](https://arxiv.org/abs/2502.11733)] [[cool](https://papers.cool/arxiv/2502.11733)] [[pdf](https://arxiv.org/pdf/2502.11733)]
> **Authors**: Jonathan Jordan,Sherzod Hakimov,David Schlangen
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) have risen to prominence as 'chatbots' for users to interact via natural language. However, their abilities to capture common-sense knowledge make them seem promising as language-based planners of situated or embodied action as well. We have implemented a simple text-based environment -- similar to others that have before been used for reinforcement-learning of agents -- that simulates, very abstractly, a household setting. We use this environment and the detailed error-tracking capabilities we implemented for targeted benchmarking of LLMs on the problem of practical reasoning: Going from goals and observations to actions. Our findings show that environmental complexity and game restrictions hamper performance, and concise action planning is demanding for current LLMs.

### ChineseSimpleVQA -- "See the World, Discover Knowledge": A Chinese Factuality Evaluation for Large Vision Language Models 
[[arxiv](https://arxiv.org/abs/2502.11718)] [[cool](https://papers.cool/arxiv/2502.11718)] [[pdf](https://arxiv.org/pdf/2502.11718)]
> **Authors**: Jihao Gu,Yingyao Wang,Pi Bu,Chen Wang,Ziming Wang,Tengtao Song,Donglai Wei,Jiale Yuan,Yingxiu Zhao,Yancheng He,Shilong Li,Jiaheng Liu,Meng Cao,Jun Song,Yingshui Tan,Xiang Li,Wenbo Su,Zhicheng Zheng,Xiaoyong Zhu,Bo Zheng
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 24 pages, 21 figures
- **标题**: None
- **领域**: 计算语言学,计算机视觉和模式识别
- **Abstract**: The evaluation of factual accuracy in large vision language models (LVLMs) has lagged behind their rapid development, making it challenging to fully reflect these models' knowledge capacity and reliability. In this paper, we introduce the first factuality-based visual question-answering benchmark in Chinese, named ChineseSimpleVQA, aimed at assessing the visual factuality of LVLMs across 8 major topics and 56 subtopics. The key features of this benchmark include a focus on the Chinese language, diverse knowledge types, a multi-hop question construction, high-quality data, static consistency, and easy-to-evaluate through short answers. Moreover, we contribute a rigorous data construction pipeline and decouple the visual factuality into two parts: seeing the world (i.e., object recognition) and discovering knowledge. This decoupling allows us to analyze the capability boundaries and execution mechanisms of LVLMs. Subsequently, we evaluate 34 advanced open-source and closed-source models, revealing critical performance gaps within this field.

### Ad-hoc Concept Forming in the Game Codenames as a Means for Evaluating Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.11707)] [[cool](https://papers.cool/arxiv/2502.11707)] [[pdf](https://arxiv.org/pdf/2502.11707)]
> **Authors**: Sherzod Hakimov,Lara Pfennigschmidt,David Schlangen
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: This study utilizes the game Codenames as a benchmarking tool to evaluate large language models (LLMs) with respect to specific linguistic and cognitive skills. LLMs play each side of the game, where one side generates a clue word covering several target words and the other guesses those target words. We designed various experiments by controlling the choice of words (abstract vs. concrete words, ambiguous vs. monosemic) or the opponent (programmed to be faster or slower in revealing words). Recent commercial and open-weight models were compared side-by-side to find out factors affecting their performance. The evaluation reveals details about their strategies, challenging cases, and limitations of LLMs.

### LLM Agents Making Agent Tools 
[[arxiv](https://arxiv.org/abs/2502.11705)] [[cool](https://papers.cool/arxiv/2502.11705)] [[pdf](https://arxiv.org/pdf/2502.11705)]
> **Authors**: Georg Wölflein,Dyke Ferber,Daniel Truhn,Ognjen Arandjelović,Jakob Nikolas Kather
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习,多代理系统
- **Abstract**: Tool use has turned large language models (LLMs) into powerful agents that can perform complex multi-step tasks by dynamically utilising external software components. However, these tools must be implemented in advance by human developers, hindering the applicability of LLM agents in domains which demand large numbers of highly specialised tools, like in life sciences and medicine. Motivated by the growing trend of scientific studies accompanied by public code repositories, we propose ToolMaker, a novel agentic framework that autonomously transforms papers with code into LLM-compatible tools. Given a short task description and a repository URL, ToolMaker autonomously installs required dependencies and generates code to perform the task, using a closed-loop self-correction mechanism to iteratively diagnose and rectify errors. To evaluate our approach, we introduce a benchmark comprising 15 diverse and complex computational tasks spanning both medical and non-medical domains with over 100 unit tests to objectively assess tool correctness and robustness. ToolMaker correctly implements 80% of the tasks, substantially outperforming current state-of-the-art software engineering agents. ToolMaker therefore is a step towards fully autonomous agent-based scientific workflows.

### CMQCIC-Bench: A Chinese Benchmark for Evaluating Large Language Models in Medical Quality Control Indicator Calculation 
[[arxiv](https://arxiv.org/abs/2502.11703)] [[cool](https://papers.cool/arxiv/2502.11703)] [[pdf](https://arxiv.org/pdf/2502.11703)]
> **Authors**: Guangya Yu,Yanhao Li,Zongying Jiang,Yuxiong Jin,Li Dai,Yupian Lin,Ruihui Hou,Weiyan Zhang,Yongqi Fan,Qi Ye,Jingping Liu,Tong Ruan
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 16 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Medical quality control indicators are essential to assess the qualifications of healthcare institutions for medical services. With the impressive performance of large language models (LLMs) like GPT-4 in the medical field, leveraging these technologies for the Medical Quality Control Indicator Calculation (MQCIC) presents a promising approach. In this work, (1) we introduce a real-world task MQCIC and propose an open-source Chinese electronic medical records (EMRs)-based dataset (CMQCIC-Bench) comprising 785 instances and 76 indicators. (2) We propose a semi-automatic method to enhance the rule representation. Then we propose the Clinical Facts-based Inferential Rule (CF-IR) method that disentangles the clinical fact verification and inferential rule reasoning actions. (3) We conduct comprehensive experiments on 20 representative LLMs, covering general and medical models. Our findings reveal that CF-IR outperforms Chain-of-Thought methods in MQCIC tasks. (4) We conduct an error analysis and investigate the capabilities of clinical fact verification and inferential rule reasoning, providing insights to improve performance in the MQCIC further. The dataset and code is available in this repo https://anonymous.4open.science/r/C-MQCIC-1151.

### Improve LLM-as-a-Judge Ability as a General Ability 
[[arxiv](https://arxiv.org/abs/2502.11689)] [[cool](https://papers.cool/arxiv/2502.11689)] [[pdf](https://arxiv.org/pdf/2502.11689)]
> **Authors**: Jiachen Yu,Shaoning Sun,Xiaohui Hu,Jiaxu Yan,Kaidong Yu,Xuelong Li
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: LLM-as-a-Judge leverages the generative and reasoning capabilities of large language models (LLMs) to evaluate LLM responses across diverse scenarios, providing accurate preference signals. This approach plays a vital role in aligning LLMs with human values, ensuring ethical and reliable AI outputs that align with societal norms. Recent studies have raised many methods to train LLM as generative judges, but most of them are data consuming or lack accuracy, and only focus on LLM's judge ability. In this work, we regard judge ability as a general ability of LLM and implement a two-stage training approach, comprising supervised fine-tuning (SFT) warm-up and direct preference optimization (DPO) enhancement, to achieve judge style adaptation and improve judgment accuracy. Additionally, we introduce an efficient data synthesis method to generate judgmental content. Experimental results demonstrate that our approach, utilizing only about 2% to 40% of the data required by other methods, achieves SOTA performance on RewardBench. Furthermore, our training method enhances the general capabilities of the model by constructing complicated judge task, and the judge signals provided by our model have significantly enhanced the downstream DPO training performance of our internal models in our test to optimize policy model with Judge Model. We also open-source our model weights and training data to facilitate further research.

### From Isolates to Families: Using Neural Networks for Automated Language Affiliation 
[[arxiv](https://arxiv.org/abs/2502.11688)] [[cool](https://papers.cool/arxiv/2502.11688)] [[pdf](https://arxiv.org/pdf/2502.11688)]
> **Authors**: Frederic Blum,Steffen Herbold,Johann-Mattis List
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Submitted to the 63rd Annual Meeting of the Association for Computational Linguistics, Vienna, Austria
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: In historical linguistics, the affiliation of languages to a common language family is traditionally carried out using a complex workflow that relies on manually comparing individual languages. Large-scale standardized collections of multilingual wordlists and grammatical language structures might help to improve this and open new avenues for developing automated language affiliation workflows. Here, we present neural network models that use lexical and grammatical data from a worldwide sample of more than 1,000 languages with known affiliations to classify individual languages into families. In line with the traditional assumption of most linguists, our results show that models trained on lexical data alone outperform models solely based on grammatical data, whereas combining both types of data yields even better performance. In additional experiments, we show how our models can identify long-ranging relations between entire subgroups, how they can be employed to investigate potential relatives of linguistic isolates, and how they can help us to obtain first hints on the affiliation of so far unaffiliated languages. We conclude that models for automated language affiliation trained on lexical and grammatical data provide comparative linguists with a valuable tool for evaluating hypotheses about deep and unknown language relations.

### MathFimer: Enhancing Mathematical Reasoning by Expanding Reasoning Steps through Fill-in-the-Middle Task 
[[arxiv](https://arxiv.org/abs/2502.11684)] [[cool](https://papers.cool/arxiv/2502.11684)] [[pdf](https://arxiv.org/pdf/2502.11684)]
> **Authors**: Yuchen Yan,Yongliang Shen,Yang Liu,Jin Jiang,Xin Xu,Mengdi Zhang,Jian Shao,Yueting Zhuang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Mathematical reasoning represents a critical frontier in advancing large language models (LLMs). While step-by-step approaches have emerged as the dominant paradigm for mathematical problem-solving in LLMs, the quality of reasoning steps in training data fundamentally constrains the performance of the models. Recent studies has demonstrated that more detailed intermediate steps can enhance model performance, yet existing methods for step expansion either require more powerful external models or incur substantial computational costs. In this paper, we introduce MathFimer, a novel framework for mathematical reasoning step expansion inspired by the "Fill-in-the-middle" task from code completion. By decomposing solution chains into prefix-suffix pairs and training models to reconstruct missing intermediate steps, we develop a specialized model, MathFimer-7B, on our carefully curated NuminaMath-FIM dataset. We then apply these models to enhance existing mathematical reasoning datasets by inserting detailed intermediate steps into their solution chains, creating MathFimer-expanded versions. Through comprehensive experiments on multiple mathematical reasoning datasets, including MathInstruct, MetaMathQA and etc., we demonstrate that models trained on MathFimer-expanded data consistently outperform their counterparts trained on original data across various benchmarks such as GSM8K and MATH. Our approach offers a practical, scalable solution for enhancing mathematical reasoning capabilities in LLMs without relying on powerful external models or expensive inference procedures.

### RIDE: Enhancing Large Language Model Alignment through Restyled In-Context Learning Demonstration Exemplars 
[[arxiv](https://arxiv.org/abs/2502.11681)] [[cool](https://papers.cool/arxiv/2502.11681)] [[pdf](https://arxiv.org/pdf/2502.11681)]
> **Authors**: Yuncheng Hua,Lizhen Qu,Zhuang Li,Hao Xue,Flora D. Salim,Gholamreza Haffari
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 38 pages, 2 figures, 20 tables; The paper is under review in ARR
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Alignment tuning is crucial for ensuring large language models (LLMs) behave ethically and helpfully. Current alignment approaches require high-quality annotations and significant training resources. This paper proposes a low-cost, tuning-free method using in-context learning (ICL) to enhance LLM alignment. Through an analysis of high-quality ICL demos, we identified style as a key factor influencing LLM alignment capabilities and explicitly restyled ICL exemplars based on this stylistic framework. Additionally, we combined the restyled demos to achieve a balance between the two conflicting aspects of LLM alignment--factuality and safety. We packaged the restyled examples as prompts to trigger few-shot learning, improving LLM alignment. Compared to the best baseline approach, with an average score of 5.00 as the maximum, our method achieves a maximum 0.10 increase on the Alpaca task (from 4.50 to 4.60), a 0.22 enhancement on the Just-eval benchmark (from 4.34 to 4.56), and a maximum improvement of 0.32 (from 3.53 to 3.85) on the MT-Bench dataset. We release the code and data at https://github.com/AnonymousCode-ComputerScience/RIDE.

### Towards Fully Exploiting LLM Internal States to Enhance Knowledge Boundary Perception 
[[arxiv](https://arxiv.org/abs/2502.11677)] [[cool](https://papers.cool/arxiv/2502.11677)] [[pdf](https://arxiv.org/pdf/2502.11677)]
> **Authors**: Shiyu Ni,Keping Bi,Jiafeng Guo,Lulu Yu,Baolong Bi,Xueqi Cheng
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) exhibit impressive performance across diverse tasks but often struggle to accurately gauge their knowledge boundaries, leading to confident yet incorrect responses. This paper explores leveraging LLMs' internal states to enhance their perception of knowledge boundaries from efficiency and risk perspectives. We investigate whether LLMs can estimate their confidence using internal states before response generation, potentially saving computational resources. Our experiments on datasets like Natural Questions, HotpotQA, and MMLU reveal that LLMs demonstrate significant pre-generation perception, which is further refined post-generation, with perception gaps remaining stable across varying conditions. To mitigate risks in critical domains, we introduce Consistency-based Confidence Calibration ($C^3$), which assesses confidence consistency through question reformulation. $C^3$ significantly improves LLMs' ability to recognize their knowledge gaps, enhancing the unknown perception rate by 5.6\% on NQ and 4.9\% on HotpotQA. Our findings suggest that pre-generation confidence estimation can optimize efficiency, while $C^3$ effectively controls output risks, advancing the reliability of LLMs in practical applications.

### Diversity-Oriented Data Augmentation with Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.11671)] [[cool](https://papers.cool/arxiv/2502.11671)] [[pdf](https://arxiv.org/pdf/2502.11671)]
> **Authors**: Zaitian Wang,Jinghan Zhang,Xinhao Zhang,Kunpeng Liu,Pengfei Wang,Yuanchun Zhou
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Data augmentation is an essential technique in natural language processing (NLP) for enriching training datasets by generating diverse samples. This process is crucial for improving the robustness and generalization capabilities of NLP models. However, a significant challenge remains: \textit{Insufficient Attention to Sample Distribution Diversity}. Most existing methods focus on increasing the sample numbers while neglecting the sample distribution diversity, which can lead to model overfitting. In response, we explore data augmentation's impact on dataset diversity and propose a \textbf{\underline{D}}iversity-\textbf{\underline{o}}riented data \textbf{\underline{Aug}}mentation framework (\textbf{DoAug}). % \(\mathscr{DoAug}\) Specifically, we utilize a diversity-oriented fine-tuning approach to train an LLM as a diverse paraphraser, which is capable of augmenting textual datasets by generating diversified paraphrases. Then, we apply the LLM paraphraser to a selected coreset of highly informative samples and integrate the paraphrases with the original data to create a more diverse augmented dataset. Finally, we conduct extensive experiments on 12 real-world textual datasets. The results show that our fine-tuned LLM augmenter improves diversity while preserving label consistency, thereby enhancing the robustness and performance of downstream tasks. Specifically, it achieves an average performance gain of \(10.52\%\), surpassing the runner-up baseline with more than three percentage points.

### Is Human-Like Text Liked by Humans? Multilingual Human Detection and Preference Against AI 
[[arxiv](https://arxiv.org/abs/2502.11614)] [[cool](https://papers.cool/arxiv/2502.11614)] [[pdf](https://arxiv.org/pdf/2502.11614)]
> **Authors**: Yuxia Wang,Rui Xing,Jonibek Mansurov,Giovanni Puccetti,Zhuohan Xie,Minh Ngoc Ta,Jiahui Geng,Jinyan Su,Mervat Abassy,Saad El Dine Ahmed,Kareem Elozeiri,Nurkhan Laiyk,Maiya Goloburda,Tarek Mahmoud,Raj Vardhan Tomar,Alexander Aziz,Ryuto Koike,Masahiro Kaneko,Artem Shelmanov,Ekaterina Artemova,Vladislav Mikhailov,Akim Tsvigun,Alham Fikri Aji,Nizar Habash,Iryna Gurevych, et al. (1 additional authors not shown)
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Prior studies have shown that distinguishing text generated by large language models (LLMs) from human-written one is highly challenging, and often no better than random guessing. To verify the generalizability of this finding across languages and domains, we perform an extensive case study to identify the upper bound of human detection accuracy. Across 16 datasets covering 9 languages and 9 domains, 19 annotators achieved an average detection accuracy of 87.6%, thus challenging previous conclusions. We find that major gaps between human and machine text lie in concreteness, cultural nuances, and diversity. Prompting by explicitly explaining the distinctions in the prompts can partially bridge the gaps in over 50% of the cases. However, we also find that humans do not always prefer human-written text, particularly when they cannot clearly identify its source.

### DR.GAP: Mitigating Bias in Large Language Models using Gender-Aware Prompting with Demonstration and Reasoning 
[[arxiv](https://arxiv.org/abs/2502.11603)] [[cool](https://papers.cool/arxiv/2502.11603)] [[pdf](https://arxiv.org/pdf/2502.11603)]
> **Authors**: Hongye Qiu,Yue Xu,Meikang Qiu,Wenjie Wang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large Language Models (LLMs) exhibit strong natural language processing capabilities but also inherit and amplify societal biases, including gender bias, raising fairness concerns. Existing debiasing methods face significant limitations: parameter tuning requires access to model weights, prompt-based approaches often degrade model utility, and optimization-based techniques lack generalizability. To address these challenges, we propose DR.GAP (Demonstration and Reasoning for Gender-Aware Prompting), an automated and model-agnostic approach that mitigates gender bias while preserving model performance. DR.GAP selects bias-revealing examples and generates structured reasoning to guide models toward more impartial responses. Extensive experiments on coreference resolution and QA tasks across multiple LLMs (GPT-3.5, Llama3, and Llama2-Alpaca) demonstrate its effectiveness, generalization ability, and robustness. DR.GAP can generalize to vision-language models (VLMs), achieving significant bias reduction.

### Can LLM Watermarks Robustly Prevent Unauthorized Knowledge Distillation? 
[[arxiv](https://arxiv.org/abs/2502.11598)] [[cool](https://papers.cool/arxiv/2502.11598)] [[pdf](https://arxiv.org/pdf/2502.11598)]
> **Authors**: Leyi Pan,Aiwei Liu,Shiyu Huang,Yijian Lu,Xuming Hu,Lijie Wen,Irwin King,Philip S. Yu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 22 pages, 12 figures, 13 tables
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The radioactive nature of Large Language Model (LLM) watermarking enables the detection of watermarks inherited by student models when trained on the outputs of watermarked teacher models, making it a promising tool for preventing unauthorized knowledge distillation. However, the robustness of watermark radioactivity against adversarial actors remains largely unexplored. In this paper, we investigate whether student models can acquire the capabilities of teacher models through knowledge distillation while avoiding watermark inheritance. We propose two categories of watermark removal approaches: pre-distillation removal through untargeted and targeted training data paraphrasing (UP and TP), and post-distillation removal through inference-time watermark neutralization (WN). Extensive experiments across multiple model pairs, watermarking schemes and hyper-parameter settings demonstrate that both TP and WN thoroughly eliminate inherited watermarks, with WN achieving this while maintaining knowledge transfer efficiency and low computational overhead. Given the ongoing deployment of watermarking techniques in production LLMs, these findings emphasize the urgent need for more robust defense strategies. Our code is available at https://github.com/THU-BPM/Watermark-Radioactivity-Attack.

### Language Complexity Measurement as a Noisy Zero-Shot Proxy for Evaluating LLM Performance 
[[arxiv](https://arxiv.org/abs/2502.11578)] [[cool](https://papers.cool/arxiv/2502.11578)] [[pdf](https://arxiv.org/pdf/2502.11578)]
> **Authors**: Birger Moell,Johan Boye
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Submitted to ACL 2025
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large Language Models (LLMs) have made significant strides in natural language generation but often face challenges in tasks requiring precise calculations and structural analysis. This paper investigates the performance of state-of-the-art LLMs on language complexity measurement tasks, through the computation of the LIX readability metric and Average Dependency Distance (ADD). Using Swedish high school and university-level essays, we evaluate the models' abilities to compute LIX scores and perform dependency parsing, comparing their results to established ground truths. Our findings reveal that while all models demonstrate some capacity for these tasks, ChatGPT-o1-mini performs most consistently, achieving the highest accuracy in both LIX computation and dependency parsing. Additionally, we observe a strong significant correlation -0.875 p 0.026 (N=6) between the models' accuracy in computing LIX and their overall performance on the Massive Multitask Language Understanding (MMLU) benchmark. These results suggest that language complexity measurement abilities can serve as a noisy zero-shot proxies for assessing the general capabilities of LLMs, providing a practical method for model evaluation without the need for extensive benchmarking datasets.

### InfiR : Crafting Effective Small Language Models and Multimodal Small Language Models in Reasoning 
[[arxiv](https://arxiv.org/abs/2502.11573)] [[cool](https://papers.cool/arxiv/2502.11573)] [[pdf](https://arxiv.org/pdf/2502.11573)]
> **Authors**: Congkai Xie,Shuo Cai,Wenjun Wang,Pengxiang Li,Zhijie Sang,Kejing Yang,Yiming Zhang,Zhen Li,Guanghao Zhu,Zeyu Liu,Yang Yu,Yuhang Liu,Su Lu,Baoyi He,Qi Zhou,Xiaotian Han,Jianbo Yuan,Shengyu Zhang,Fei Wu,Hongxia Yang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) have made significant advancements in reasoning capabilities. However, they still face challenges such as high computational demands and privacy concerns. This paper focuses on developing efficient Small Language Models (SLMs) and Multimodal Small Language Models (MSLMs) that retain competitive reasoning abilities. We introduce a novel training pipeline that enhances reasoning capabilities and facilitates deployment on edge devices, achieving state-of-the-art performance while minimizing development costs. \InfR~ aims to advance AI systems by improving reasoning, reducing adoption barriers, and addressing privacy concerns through smaller model sizes. Resources are available at https://github. com/Reallm-Labs/InfiR.

### FaMTEB: Massive Text Embedding Benchmark in Persian Language 
[[arxiv](https://arxiv.org/abs/2502.11571)] [[cool](https://papers.cool/arxiv/2502.11571)] [[pdf](https://arxiv.org/pdf/2502.11571)]
> **Authors**: Erfan Zinvandi,Morteza Alikhani,Mehran Sarmadi,Zahra Pourbahman,Sepehr Arvin,Reza Kazemi,Arash Amini
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: to appear in ACL 2025
- **标题**: None
- **领域**: 计算语言学,信息检索,机器学习
- **Abstract**: In this paper, we introduce a comprehensive benchmark for Persian (Farsi) text embeddings, built upon the Massive Text Embedding Benchmark (MTEB). Our benchmark includes 63 datasets spanning seven different tasks: classification, clustering, pair classification, reranking, retrieval, summary retrieval, and semantic textual similarity. The datasets are formed as a combination of existing, translated, and newly generated data, offering a diverse evaluation framework for Persian language models. Given the increasing use of text embedding models in chatbots, evaluation datasets are becoming inseparable ingredients in chatbot challenges and Retrieval-Augmented Generation systems. As a contribution, we include chatbot evaluation datasets in the MTEB benchmark for the first time. In addition, in this paper, we introduce the new task of summary retrieval which is not part of the tasks included in standard MTEB. Another contribution of this paper is the introduction of a substantial number of new Persian language NLP datasets suitable for training and evaluation, some of which have no previous counterparts in Persian. We evaluate the performance of several Persian and multilingual embedding models in a range of tasks. This work introduces an open-source benchmark with datasets, code and a public leaderboard.

### Towards Reasoning Ability of Small Language Models 
[[arxiv](https://arxiv.org/abs/2502.11569)] [[cool](https://papers.cool/arxiv/2502.11569)] [[pdf](https://arxiv.org/pdf/2502.11569)]
> **Authors**: Gaurav Srivastava,Shuxiang Cao,Xuan Wang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Reasoning has long been viewed as an emergent property of large language models (LLMs), appearing at or above a certain scale ($\sim$100B parameters). However, recent studies challenge this assumption, showing that small language models (SLMs) can also achieve competitive reasoning performance. SLMs are increasingly favored for their efficiency and deployability. However, there is a lack of systematic study on the reasoning abilities of diverse SLMs, including those trained from scratch or derived from LLMs through quantization, pruning, and distillation. This raises a critical question: Can SLMs achieve reasoning abilities comparable to LLMs? In this work, we systematically survey, benchmark, and analyze 72 SLMs from six model families across 14 reasoning benchmarks. For reliable evaluation, we examine four evaluation methods and compare four LLM judges against human evaluations on 800 data points. We repeat all experiments three times to ensure a robust performance assessment. Additionally, we analyze the impact of different prompting strategies in small models. Beyond accuracy, we also evaluate model robustness under adversarial conditions and intermediate reasoning steps. Our findings challenge the assumption that scaling is the only way to achieve strong reasoning. Instead, we foresee a future where SLMs with strong reasoning capabilities can be developed through structured training or post-training compression. They can serve as efficient alternatives to LLMs for reasoning-intensive tasks.

### Reinforced Information Retrieval 
[[arxiv](https://arxiv.org/abs/2502.11562)] [[cool](https://papers.cool/arxiv/2502.11562)] [[pdf](https://arxiv.org/pdf/2502.11562)]
> **Authors**: Chaofan Li,Zheng Liu,Jianlyv Chen,Defu Lian,Yingxia Shao
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: While retrieval techniques are widely used in practice, they still face significant challenges in cross-domain scenarios. Recently, generation-augmented methods have emerged as a promising solution to this problem. These methods enhance raw queries by incorporating additional information from an LLM-based generator, facilitating more direct retrieval of relevant documents. However, existing methods struggle with highly specialized situations that require extensive domain expertise. To address this problem, we present \textbf{Reinforced-IR}, a novel approach that jointly adapts a pre-trained retriever and generator for precise cross-domain retrieval. A key innovation of Reinforced-IR is its \textbf{Self-Boosting} framework, which enables retriever and generator to learn from each other's feedback. Specifically, the generator is reinforced to generate query augmentations that enhance the retriever's performance, while the retriever is trained to better discriminate the relevant documents identified by the generator. This iterative process allows the end-to-end retrieval performance to be progressively optimized using an unlabeled corpus from the target domain. In our experiment, Reinforced-IR outperforms existing domain adaptation methods by a large margin, leading to substantial improvements in retrieval quality across a wide range of application scenarios.

### Auto-Search and Refinement: An Automated Framework for Gender Bias Mitigation in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.11559)] [[cool](https://papers.cool/arxiv/2502.11559)] [[pdf](https://arxiv.org/pdf/2502.11559)]
> **Authors**: Yue Xu,Chengyan Fu,Li Xiong,Sibei Yang,Wenjie Wang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Pre-training large language models (LLMs) on vast text corpora enhances natural language processing capabilities but risks encoding social biases, particularly gender bias. While parameter-modification methods like fine-tuning mitigate bias, they are resource-intensive, unsuitable for closed-source models, and lack adaptability to evolving societal norms. Instruction-based approaches offer flexibility but often compromise task performance. To address these limitations, we propose $\textit{FaIRMaker}$, an automated and model-independent framework that employs an $\textbf{auto-search and refinement}$ paradigm to adaptively generate Fairwords, which act as instructions integrated into input queries to reduce gender bias and enhance response quality. Extensive experiments demonstrate that $\textit{FaIRMaker}$ automatically searches for and dynamically refines Fairwords, effectively mitigating gender bias while preserving task integrity and ensuring compatibility with both API-based and open-source LLMs.

### DCAD-2000: A Multilingual Dataset across 2000+ Languages with Data Cleaning as Anomaly Detection 
[[arxiv](https://arxiv.org/abs/2502.11546)] [[cool](https://papers.cool/arxiv/2502.11546)] [[pdf](https://arxiv.org/pdf/2502.11546)]
> **Authors**: Yingli Shen,Wen Lai,Shuo Wang,Xueren Zhang,Kangyang Luo,Alexander Fraser,Maosong Sun
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The rapid development of multilingual large language models (LLMs) highlights the need for high-quality, diverse, and clean multilingual datasets. In this paper, we introduce DCAD-2000 (Data Cleaning as Anomaly Detection), a large-scale multilingual corpus built using newly extracted Common Crawl data and existing multilingual datasets. DCAD-2000 includes over 2,282 languages, 46.72TB of data, and 8.63 billion documents, spanning 155 high- and medium-resource languages and 159 writing scripts. To overcome the limitations of current data cleaning methods, which rely on manual heuristic thresholds, we propose reframing data cleaning as an anomaly detection task. This dynamic filtering approach significantly enhances data quality by identifying and removing noisy or anomalous content. We evaluate the quality of DCAD-2000 on the FineTask benchmark, demonstrating substantial improvements in multilingual dataset quality and task performance.

### Evaluating o1-Like LLMs: Unlocking Reasoning for Translation through Comprehensive Analysis 
[[arxiv](https://arxiv.org/abs/2502.11544)] [[cool](https://papers.cool/arxiv/2502.11544)] [[pdf](https://arxiv.org/pdf/2502.11544)]
> **Authors**: Andong Chen,Yuchen Song,Wenxin Zhu,Kehai Chen,Muyun Yang,Tiejun Zhao,Min zhang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The o1-Like LLMs are transforming AI by simulating human cognitive processes, but their performance in multilingual machine translation (MMT) remains underexplored. This study examines: (1) how o1-Like LLMs perform in MMT tasks and (2) what factors influence their translation quality. We evaluate multiple o1-Like LLMs and compare them with traditional models like ChatGPT and GPT-4o. Results show that o1-Like LLMs establish new multilingual translation benchmarks, with DeepSeek-R1 surpassing GPT-4o in contextless tasks. They demonstrate strengths in historical and cultural translation but exhibit a tendency for rambling issues in Chinese-centric outputs. Further analysis reveals three key insights: (1) High inference costs and slower processing speeds make complex translation tasks more resource-intensive. (2) Translation quality improves with model size, enhancing commonsense reasoning and cultural translation. (3) The temperature parameter significantly impacts output quality-lower temperatures yield more stable and accurate translations, while higher temperatures reduce coherence and precision.

### MuSC: Improving Complex Instruction Following with Multi-granularity Self-Contrastive Training 
[[arxiv](https://arxiv.org/abs/2502.11541)] [[cool](https://papers.cool/arxiv/2502.11541)] [[pdf](https://arxiv.org/pdf/2502.11541)]
> **Authors**: Hui Huang,Jiaheng Liu,Yancheng He,Shilong Li,Bing Xu,Conghui Zhu,Muyun Yang,Tiejun Zhao
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Complex instruction-following with elaborate constraints is imperative for Large Language Models (LLMs). While existing methods have constructed data for complex instruction alignment, they all rely on a more advanced model, especially GPT-4, limiting their application. In this paper, we propose a Multi-granularity Self-Contrastive Training (MuSC) framework, to improve the complex instruction alignment without relying on a stronger model. Our method is conducted on both coarse and fine granularity. On coarse-granularity, we construct constraint-aware preference data based on instruction decomposition and recombination. On fine-granularity, we perform token-aware preference optimization with dynamic token-level supervision. Our method is evaluated on open-sourced models, and experiment results show our method achieves significant improvement on both complex and general instruction-following benchmarks, surpassing previous self-alignment methods.

### Be Cautious When Merging Unfamiliar LLMs: A Phishing Model Capable of Stealing Privacy 
[[arxiv](https://arxiv.org/abs/2502.11533)] [[cool](https://papers.cool/arxiv/2502.11533)] [[pdf](https://arxiv.org/pdf/2502.11533)]
> **Authors**: Zhenyuan Guo,Yi Shi,Wenlong Meng,Chen Gong,Chengkun Wei,Wenzhi Chen
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Model merging is a widespread technology in large language models (LLMs) that integrates multiple task-specific LLMs into a unified one, enabling the merged model to inherit the specialized capabilities of these LLMs. Most task-specific LLMs are sourced from open-source communities and have not undergone rigorous auditing, potentially imposing risks in model merging. This paper highlights an overlooked privacy risk: \textit{an unsafe model could compromise the privacy of other LLMs involved in the model merging.} Specifically, we propose PhiMM, a privacy attack approach that trains a phishing model capable of stealing privacy using a crafted privacy phishing instruction dataset. Furthermore, we introduce a novel model cloaking method that mimics a specialized capability to conceal attack intent, luring users into merging the phishing model. Once victims merge the phishing model, the attacker can extract personally identifiable information (PII) or infer membership information (MI) by querying the merged model with the phishing instruction. Experimental results show that merging a phishing model increases the risk of privacy breaches. Compared to the results before merging, PII leakage increased by 3.9\% and MI leakage increased by 17.4\% on average. We release the code of PhiMM through a link.

### Training Large Language Models to be Better Rule Followers 
[[arxiv](https://arxiv.org/abs/2502.11525)] [[cool](https://papers.cool/arxiv/2502.11525)] [[pdf](https://arxiv.org/pdf/2502.11525)]
> **Authors**: Yi Hu,Shijia Kang,Haotong Yang,Haotian Xu,Muhan Zhang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) have shown impressive performance across a wide range of tasks. However, they often exhibit unexpected failures in seemingly straightforward tasks, suggesting a reliance on case-based reasoning rather than rule-based reasoning. While the vast training corpus of LLMs contains numerous textual "rules", current training methods fail to leverage these rules effectively. Crucially, the relationships between these "rules" and their corresponding "instances" are not explicitly modeled. As a result, while LLMs can often recall rules with ease, they fail to apply these rules strictly and consistently in relevant reasoning scenarios. In this paper, we investigate the rule-following capabilities of LLMs and propose Meta Rule-Following Fine-Tuning (Meta-RFFT) to enhance the cross-task transferability of rule-following abilities. We first construct a dataset of 88 tasks requiring following rules, encompassing diverse reasoning domains. We demonstrate through extensive experiments that models trained on large-scale rule-following tasks are better rule followers, outperforming the baselines in both downstream fine-tuning and few-shot prompting scenarios. This highlights the cross-task transferability of models with the aid of Meta-RFFT. Furthermore, we examine the influence of factors such as dataset size, rule formulation, and in-context learning.

### AURORA:Automated Training Framework of Universal Process Reward Models via Ensemble Prompting and Reverse Verification 
[[arxiv](https://arxiv.org/abs/2502.11520)] [[cool](https://papers.cool/arxiv/2502.11520)] [[pdf](https://arxiv.org/pdf/2502.11520)]
> **Authors**: Xiaoyu Tan,Tianchu Yao,Chao Qu,Bin Li,Minghao Yang,Dakuan Lu,Haozhe Wang,Xihe Qiu,Wei Chu,Yinghui Xu,Yuan Qi
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Under Review
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The reasoning capabilities of advanced large language models (LLMs) like o1 have revolutionized artificial intelligence applications. Nevertheless, evaluating and optimizing complex reasoning processes remain significant challenges due to diverse policy distributions and the inherent limitations of human effort and accuracy. In this paper, we present AURORA, a novel automated framework for training universal process reward models (PRMs) using ensemble prompting and reverse verification. The framework employs a two-phase approach: First, it uses diverse prompting strategies and ensemble methods to perform automated annotation and evaluation of processes, ensuring robust assessments for reward learning. Second, it leverages practical reference answers for reverse verification, enhancing the model's ability to validate outputs and improving training accuracy. To assess the framework's performance, we extend beyond the existing ProcessBench benchmark by introducing UniversalBench, which evaluates reward predictions across full trajectories under diverse policy distribtion with long Chain-of-Thought (CoT) outputs. Experimental results demonstrate that AURORA enhances process evaluation accuracy, improves PRMs' accuracy for diverse policy distributions and long-CoT responses. The project will be open-sourced at https://auroraprm.github.io/. The Universal-PRM-7B is available at https://huggingface.co/infly/Universal-PRM-7B.

### Learning to Keep a Promise: Scaling Language Model Decoding Parallelism with Learned Asynchronous Decoding 
[[arxiv](https://arxiv.org/abs/2502.11517)] [[cool](https://papers.cool/arxiv/2502.11517)] [[pdf](https://arxiv.org/pdf/2502.11517)]
> **Authors**: Tian Jin,Ellie Y. Cheng,Zack Ankner,Nikunj Saunshi,Blake M. Elias,Amir Yazdanbakhsh,Jonathan Ragan-Kelley,Suvinay Subramanian,Michael Carbin
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 15 pages
- **标题**: None
- **领域**: 计算语言学,分布式、并行和集群计算,机器学习
- **Abstract**: Decoding with autoregressive large language models (LLMs) traditionally occurs sequentially, generating one token after another. An emerging line of work explored parallel decoding by identifying and simultaneously generating semantically independent chunks of LLM responses. However, these techniques rely on hand-crafted heuristics tied to syntactic structures like lists and paragraphs, making them rigid and imprecise. We present PASTA, a learning-based system that teaches LLMs to identify semantic independence and express parallel decoding opportunities in their own responses. At its core are PASTA-LANG and its interpreter: PASTA-LANG is an annotation language that enables LLMs to express semantic independence in their own responses; the language interpreter acts on these annotations to orchestrate parallel decoding on-the-fly at inference time. Through a two-stage finetuning process, we train LLMs to generate PASTA-LANG annotations that optimize both response quality and decoding speed. Evaluation on AlpacaEval, an instruction following benchmark, shows that our approach Pareto-dominates existing methods in terms of decoding speed and response quality; our results demonstrate geometric mean speedups ranging from 1.21x to 1.93x with corresponding quality changes of +2.2% to -7.1%, measured by length-controlled win rates against sequential decoding baseline.

### Chinese Spelling Correction: A Comprehensive Survey of Progress, Challenges, and Opportunities 
[[arxiv](https://arxiv.org/abs/2502.11508)] [[cool](https://papers.cool/arxiv/2502.11508)] [[pdf](https://arxiv.org/pdf/2502.11508)]
> **Authors**: Changchun Liu,Kai Zhang,Junzhe Jiang,Zixiao Kong,Qi Liu,Enhong Chen
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Chinese Spelling Correction (CSC) is a critical task in natural language processing, aimed at detecting and correcting spelling errors in Chinese text. This survey provides a comprehensive overview of CSC, tracing its evolution from pre-trained language models to large language models, and critically analyzing their respective strengths and weaknesses in this domain. Moreover, we further present a detailed examination of existing benchmark datasets, highlighting their inherent challenges and limitations. Finally, we propose promising future research directions, particularly focusing on leveraging the potential of LLMs and their reasoning capabilities for improved CSC performance. To the best of our knowledge, this is the first comprehensive survey dedicated to the field of CSC. We believe this work will serve as a valuable resource for researchers, fostering a deeper understanding of the field and inspiring future advancements.

### Token Pruning in Multimodal Large Language Models: Are We Solving the Right Problem? 
[[arxiv](https://arxiv.org/abs/2502.11501)] [[cool](https://papers.cool/arxiv/2502.11501)] [[pdf](https://arxiv.org/pdf/2502.11501)]
> **Authors**: Zichen Wen,Yifeng Gao,Weijia Li,Conghui He,Linfeng Zhang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 12 pages, 3 figures
- **标题**: None
- **领域**: 计算语言学,计算机视觉和模式识别
- **Abstract**: Multimodal large language models (MLLMs) have shown remarkable performance for cross-modal understanding and generation, yet still suffer from severe inference costs. Recently, abundant works have been proposed to solve this problem with token pruning, which identifies the redundant tokens in MLLMs and then prunes them to reduce the computation and KV storage costs, leading to significant acceleration without training. While these methods claim efficiency gains, critical questions about their fundamental design and evaluation remain unanswered: Why do many existing approaches underperform even compared to naive random token selection? Are attention-based scoring sufficient for reliably identifying redundant tokens? Is language information really helpful during token pruning? What makes a good trade-off between token importance and duplication? Are current evaluation protocols comprehensive and unbiased? The ignorance of previous research on these problems hinders the long-term development of token pruning. In this paper, we answer these questions one by one, providing insights into the design of future token pruning methods.

### Balanced Multi-Factor In-Context Learning for Multilingual Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.11495)] [[cool](https://papers.cool/arxiv/2502.11495)] [[pdf](https://arxiv.org/pdf/2502.11495)]
> **Authors**: Masahiro Kaneko,Alham Fikri Aji,Timothy Baldwin
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Multilingual large language models (MLLMs) are able to leverage in-context learning (ICL) to achieve high performance by leveraging cross-lingual knowledge transfer without parameter updates. However, their effectiveness is highly sensitive to example selection, particularly in multilingual settings. Based on the findings of existing work, three key factors influence multilingual ICL: (1) semantic similarity, (2) linguistic alignment, and (3) language-specific performance. However, existing approaches address these factors independently, without explicitly disentangling their combined impact, leaving optimal example selection underexplored. To address this gap, we propose balanced multi-factor ICL (\textbf{BMF-ICL}), a method that quantifies and optimally balances these factors for improved example selection. Experiments on mCSQA and TYDI across four MLLMs demonstrate that BMF-ICL outperforms existing methods. Further analysis highlights the importance of incorporating all three factors and the importance of selecting examples from multiple languages.

### Stop Looking for Important Tokens in Multimodal Language Models: Duplication Matters More 
[[arxiv](https://arxiv.org/abs/2502.11494)] [[cool](https://papers.cool/arxiv/2502.11494)] [[pdf](https://arxiv.org/pdf/2502.11494)]
> **Authors**: Zichen Wen,Yifeng Gao,Shaobo Wang,Junyuan Zhang,Qintong Zhang,Weijia Li,Conghui He,Linfeng Zhang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 15 pages, 8 figures
- **标题**: None
- **领域**: 计算语言学,计算机视觉和模式识别
- **Abstract**: Vision tokens in multimodal large language models often dominate huge computational overhead due to their excessive length compared to linguistic modality. Abundant recent methods aim to solve this problem with token pruning, which first defines an importance criterion for tokens and then prunes the unimportant vision tokens during inference. However, in this paper, we show that the importance is not an ideal indicator to decide whether a token should be pruned. Surprisingly, it usually results in inferior performance than random token pruning and leading to incompatibility to efficient attention computation operators.Instead, we propose DART (Duplication-Aware Reduction of Tokens), which prunes tokens based on its duplication with other tokens, leading to significant and training-free acceleration. Concretely, DART selects a small subset of pivot tokens and then retains the tokens with low duplication to the pivots, ensuring minimal information loss during token pruning. Experiments demonstrate that DART can prune 88.9% vision tokens while maintaining comparable performance, leading to a 1.99$\times$ and 2.99$\times$ speed-up in total time and prefilling stage, respectively, with good compatibility to efficient attention operators. Our codes are available at https://github.com/ZichenWen1/DART.

### DAST: Context-Aware Compression in LLMs via Dynamic Allocation of Soft Tokens 
[[arxiv](https://arxiv.org/abs/2502.11493)] [[cool](https://papers.cool/arxiv/2502.11493)] [[pdf](https://arxiv.org/pdf/2502.11493)]
> **Authors**: Shaoshen Chen,Yangning Li,Zishan Xu,Yinghui Li,Xin Su,Zifei Shan,Hai-tao Zheng
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models (LLMs) face computational inefficiencies and redundant processing when handling long context inputs, prompting a focus on compression techniques. While existing semantic vector-based compression methods achieve promising performance, these methods fail to account for the intrinsic information density variations between context chunks, instead allocating soft tokens uniformly across context chunks. This uniform distribution inevitably diminishes allocation to information-critical regions. To address this, we propose Dynamic Allocation of Soft Tokens (DAST), a simple yet effective method that leverages the LLM's intrinsic understanding of contextual relevance to guide compression. DAST combines perplexity-based local information with attention-driven global information to dynamically allocate soft tokens to the informative-rich chunks, enabling effective, context-aware compression. Experimental results across multiple benchmarks demonstrate that DAST surpasses state-of-the-art methods.

### Ontology-Guided Reverse Thinking Makes Large Language Models Stronger on Knowledge Graph Question Answering 
[[arxiv](https://arxiv.org/abs/2502.11491)] [[cool](https://papers.cool/arxiv/2502.11491)] [[pdf](https://arxiv.org/pdf/2502.11491)]
> **Authors**: Runxuan Liu,Bei Luo,Jiaqi Li,Baoxin Wang,Ming Liu,Dayong Wu,Shijin Wang,Bing Qin
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large language models (LLMs) have shown remarkable capabilities in natural language processing. However, in knowledge graph question answering tasks (KGQA), there remains the issue of answering questions that require multi-hop reasoning. Existing methods rely on entity vector matching, but the purpose of the question is abstract and difficult to match with specific entities. As a result, it is difficult to establish reasoning paths to the purpose, which leads to information loss and redundancy. To address this issue, inspired by human reverse thinking, we propose Ontology-Guided Reverse Thinking (ORT), a novel framework that constructs reasoning paths from purposes back to conditions. ORT operates in three key phases: (1) using LLM to extract purpose labels and condition labels, (2) constructing label reasoning paths based on the KG ontology, and (3) using the label reasoning paths to guide knowledge retrieval. Experiments on the WebQSP and CWQ datasets show that ORT achieves state-of-the-art performance and significantly enhances the capability of LLMs for KGQA.

### FastMCTS: A Simple Sampling Strategy for Data Synthesis 
[[arxiv](https://arxiv.org/abs/2502.11476)] [[cool](https://papers.cool/arxiv/2502.11476)] [[pdf](https://arxiv.org/pdf/2502.11476)]
> **Authors**: Peiji Li,Kai Lv,Yunfan Shao,Yichuan Ma,Linyang Li,Xiaoqing Zheng,Xipeng Qiu,Qipeng Guo
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: work in progress
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Synthetic high-quality multi-step reasoning data can significantly enhance the performance of large language models on various tasks. However, most existing methods rely on rejection sampling, which generates trajectories independently and suffers from inefficiency and imbalanced sampling across problems of varying difficulty. In this work, we introduce FastMCTS, an innovative data synthesis strategy inspired by Monte Carlo Tree Search. FastMCTS provides a more efficient sampling method for multi-step reasoning data, offering step-level evaluation signals and promoting balanced sampling across problems of different difficulty levels. Experiments on both English and Chinese reasoning datasets demonstrate that FastMCTS generates over 30\% more correct reasoning paths compared to rejection sampling as the number of generated tokens scales up. Furthermore, under comparable synthetic data budgets, models trained on FastMCTS-generated data outperform those trained on rejection sampling data by 3.9\% across multiple benchmarks. As a lightweight sampling strategy, FastMCTS offers a practical and efficient alternative for synthesizing high-quality reasoning data. Our code will be released soon.

### GLTW: Joint Improved Graph Transformer and LLM via Three-Word Language for Knowledge Graph Completion 
[[arxiv](https://arxiv.org/abs/2502.11471)] [[cool](https://papers.cool/arxiv/2502.11471)] [[pdf](https://arxiv.org/pdf/2502.11471)]
> **Authors**: Kangyang Luo,Yuzhuo Bai,Cheng Gao,Shuzheng Si,Yingli Shen,Zhu Liu,Zhitong Wang,Cunliang Kong,Wenhao Li,Yufei Huang,Ye Tian,Xuantang Xiong,Lei Han,Maosong Sun
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,信息检索
- **Abstract**: Knowledge Graph Completion (KGC), which aims to infer missing or incomplete facts, is a crucial task for KGs. However, integrating the vital structural information of KGs into Large Language Models (LLMs) and outputting predictions deterministically remains challenging. To address this, we propose a new method called GLTW, which encodes the structural information of KGs and merges it with LLMs to enhance KGC performance. Specifically, we introduce an improved Graph Transformer (iGT) that effectively encodes subgraphs with both local and global structural information and inherits the characteristics of language model, bypassing training from scratch. Also, we develop a subgraph-based multi-classification training objective, using all entities within KG as classification objects, to boost learning efficiency.Importantly, we combine iGT with an LLM that takes KG language prompts as input.Our extensive experiments on various KG datasets show that GLTW achieves significant performance gains compared to SOTA baselines.

### If Attention Serves as a Cognitive Model of Human Memory Retrieval, What is the Plausible Memory Representation? 
[[arxiv](https://arxiv.org/abs/2502.11469)] [[cool](https://papers.cool/arxiv/2502.11469)] [[pdf](https://arxiv.org/pdf/2502.11469)]
> **Authors**: Ryo Yoshida,Shinnosuke Isono,Kohei Kajikawa,Taiga Someya,Yushi Sugimito,Yohei Oseki
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 16 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Recent work in computational psycholinguistics has revealed intriguing parallels between attention mechanisms and human memory retrieval, focusing primarily on Transformer architectures that operate on token-level representations. However, computational psycholinguistic research has also established that syntactic structures provide compelling explanations for human sentence processing that word-level factors alone cannot fully account for. In this study, we investigate whether the attention mechanism of Transformer Grammar (TG), which uniquely operates on syntactic structures as representational units, can serve as a cognitive model of human memory retrieval, using Normalized Attention Entropy (NAE) as a linking hypothesis between model behavior and human processing difficulty. Our experiments demonstrate that TG's attention achieves superior predictive power for self-paced reading times compared to vanilla Transformer's, with further analyses revealing independent contributions from both models. These findings suggest that human sentence processing involves dual memory representations -- one based on syntactic structures and another on token sequences -- with attention serving as the general retrieval algorithm, while highlighting the importance of incorporating syntactic structures as representational units.

### UnitCoder: Scalable Iterative Code Synthesis with Unit Test Guidance 
[[arxiv](https://arxiv.org/abs/2502.11460)] [[cool](https://papers.cool/arxiv/2502.11460)] [[pdf](https://arxiv.org/pdf/2502.11460)]
> **Authors**: Yichuan Ma,Yunfan Shao,Peiji Li,Demin Song,Qipeng Guo,Linyang Li,Xipeng Qiu,Kai Chen
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: work in progress
- **标题**: None
- **领域**: 计算语言学,软件工程
- **Abstract**: Large Language Models (LLMs) have demonstrated remarkable capabilities in various tasks, yet code generation remains a major challenge. Current approaches for obtaining high-quality code data primarily focus on (i) collecting large-scale pre-training data and (ii) synthesizing instruction data through prompt engineering with powerful models. While pre-training data faces quality consistency issues, instruction-based synthesis suffers from limited instruction diversity and inherent biases of LLMs. To address this gap, we introduce UnitCoder, a systematic pipeline leveraging model-generated unit tests to both guide and validate the code generation process. Combined with large-scale package-based retrieval from pre-training corpus, we generate a dataset of 500K+ verifiable programs containing diverse API calls. Evaluations on multiple Python benchmarks (BigCodeBench, HumanEval, MBPP) demonstrate that models fine-tuned on our synthetic data exhibit consistent performance improvements. Notably, Llama3.1-8B and InternLM2.5-7B improve from 31\% and 28\% to 40\% and 39\% success rates on BigCodeBench, respectively. Our work presents a scalable approach that leverages model-generated unit tests to guide the synthesis of high-quality code data from pre-training corpora, demonstrating the potential for producing diverse and high-quality post-training data at scale. All code and data will be released (https://github.com).

### Aligning Sentence Simplification with ESL Learner's Proficiency for Language Acquisition 
[[arxiv](https://arxiv.org/abs/2502.11457)] [[cool](https://papers.cool/arxiv/2502.11457)] [[pdf](https://arxiv.org/pdf/2502.11457)]
> **Authors**: Guanlin Li,Yuki Arase,Noel Crespi
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: NAACL2025 main
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Text simplification is crucial for improving accessibility and comprehension for English as a Second Language (ESL) learners. This study goes a step further and aims to facilitate ESL learners' language acquisition by simplification. Specifically, we propose simplifying complex sentences to appropriate levels for learners while also increasing vocabulary coverage of the target level in the simplifications. We achieve this without a parallel corpus by conducting reinforcement learning on a large language model. Our method employs token-level and sentence-level rewards, and iteratively trains the model on its self-generated outputs to guide the model to search for simplification hypotheses that satisfy the target attributes. Experiment results on CEFR-SP and TurkCorpus datasets show that the proposed method can effectively increase the frequency and diversity of vocabulary of the target level by more than $20\%$ compared to baseline models, while maintaining high simplification quality.

### UniCBE: An Uniformity-driven Comparing Based Evaluation Framework with Unified Multi-Objective Optimization 
[[arxiv](https://arxiv.org/abs/2502.11454)] [[cool](https://papers.cool/arxiv/2502.11454)] [[pdf](https://arxiv.org/pdf/2502.11454)]
> **Authors**: Peiwen Yuan,Shaoxiong Feng,Yiwei Li,Xinglin Wang,Yueqi Zhang,Jiayi Shi,Chuyi Tan,Boyuan Pan,Yao Hu,Kan Li
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: ICLR 2025 spotlight
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Human preference plays a significant role in measuring large language models and guiding them to align with human values. Unfortunately, current comparing-based evaluation (CBE) methods typically focus on a single optimization objective, failing to effectively utilize scarce yet valuable preference signals. To address this, we delve into key factors that can enhance the accuracy, convergence, and scalability of CBE: suppressing sampling bias, balancing descending process of uncertainty, and mitigating updating uncertainty. Following the derived guidelines, we propose UniCBE, a unified uniformity-driven CBE framework which simultaneously optimize these core objectives by constructing and integrating three decoupled sampling probability matrices, each designed to ensure uniformity in specific aspects. We further ablate the optimal tuple sampling and preference aggregation strategies to achieve efficient CBE. On the AlpacaEval benchmark, UniCBE saves over 17% of evaluation budgets while achieving a Pearson correlation with ground truth exceeding 0.995, demonstrating excellent accuracy and convergence. In scenarios where new models are continuously introduced, UniCBE can even save over 50% of evaluation costs, highlighting its improved scalability.

### From Personas to Talks: Revisiting the Impact of Personas on LLM-Synthesized Emotional Support Conversations 
[[arxiv](https://arxiv.org/abs/2502.11451)] [[cool](https://papers.cool/arxiv/2502.11451)] [[pdf](https://arxiv.org/pdf/2502.11451)]
> **Authors**: Shenghan Wu,Yang Deng,Yimo Zhu,Wynne Hsu,Mong Li Lee
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The rapid advancement of Large Language Models (LLMs) has revolutionized the generation of emotional support conversations (ESC), offering scalable solutions with reduced costs and enhanced data privacy. This paper explores the role of personas in the creation of ESC by LLMs. Our research utilizes established psychological frameworks to measure and infuse persona traits into LLMs, which then generate dialogues in the emotional support scenario. We conduct extensive evaluations to understand the stability of persona traits in dialogues, examining shifts in traits post-generation and their impact on dialogue quality and strategy distribution. Experimental results reveal several notable findings: 1) LLMs can infer core persona traits, 2) subtle shifts in emotionality and extraversion occur, influencing the dialogue dynamics, and 3) the application of persona traits modifies the distribution of emotional support strategies, enhancing the relevance and empathetic quality of the responses. These findings highlight the potential of persona-driven LLMs in crafting more personalized, empathetic, and effective emotional support dialogues, which has significant implications for the future design of AI-driven emotional support systems.

### Does RAG Really Perform Bad For Long-Context Processing? 
[[arxiv](https://arxiv.org/abs/2502.11444)] [[cool](https://papers.cool/arxiv/2502.11444)] [[pdf](https://arxiv.org/pdf/2502.11444)]
> **Authors**: Kun Luo,Zheng Liu,Peitian Zhang,Hongjin Qian,Jun Zhao,Kang Liu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The efficient processing of long context poses a serious challenge for large language models (LLMs). Recently, retrieval-augmented generation (RAG) has emerged as a promising strategy for this problem, as it enables LLMs to make selective use of the long context for efficient computation. However, existing RAG approaches lag behind other long-context processing methods due to inherent limitations on inaccurate retrieval and fragmented contexts. To address these challenges, we introduce RetroLM, a novel RAG framework for long-context processing. Unlike traditional methods, RetroLM employs KV-level retrieval augmentation, where it partitions the LLM's KV cache into contiguous pages and retrieves the most crucial ones for efficient computation. This approach enhances robustness to retrieval inaccuracy, facilitates effective utilization of fragmented contexts, and saves the cost from repeated computation. Building on this framework, we further develop a specialized retriever for precise retrieval of critical pages and conduct unsupervised post-training to optimize the model's ability to leverage retrieved information. We conduct comprehensive evaluations with a variety of benchmarks, including LongBench, InfiniteBench, and RULER, where RetroLM significantly outperforms existing long-context LLMs and efficient long-context processing methods, particularly in tasks requiring intensive reasoning or extremely long-context comprehension.

## 密码学和安全(cs.CR:Cryptography and Security)

### LMN: A Tool for Generating Machine Enforceable Policies from Natural Language Access Control Rules using LLMs 
[[arxiv](https://arxiv.org/abs/2502.12460)] [[cool](https://papers.cool/arxiv/2502.12460)] [[pdf](https://arxiv.org/pdf/2502.12460)]
> **Authors**: Pratik Sonune,Ritwik Rai,Shamik Sural,Vijayalakshmi Atluri,Ashish Kundu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,机器学习
- **Abstract**: Organizations often lay down rules or guidelines called Natural Language Access Control Policies (NLACPs) for specifying who gets access to which information and when. However, these cannot be directly used in a target access control model like Attribute-based Access Control (ABAC). Manually translating the NLACP rules into Machine Enforceable Security Policies (MESPs) is both time consuming and resource intensive, rendering it infeasible especially for large organizations. Automated machine translation workflows, on the other hand, require information security officers to be adept at using such processes. To effectively address this problem, we have developed a free web-based publicly accessible tool called LMN (LLMs for generating MESPs from NLACPs) that takes an NLACP as input and converts it into a corresponding MESP. Internally, LMN uses the GPT 3.5 API calls and an appropriately chosen prompt. Extensive experiments with different prompts and performance metrics firmly establish the usefulness of LMN.

### Hybrid Machine Learning Models for Intrusion Detection in IoT: Leveraging a Real-World IoT Dataset 
[[arxiv](https://arxiv.org/abs/2502.12382)] [[cool](https://papers.cool/arxiv/2502.12382)] [[pdf](https://arxiv.org/pdf/2502.12382)]
> **Authors**: Md Ahnaf Akif,Ismail Butun,Andre Williams,Imadeldin Mahgoub
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 9 pages, 8 figures, 2 tables, journal submission
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: The rapid growth of the Internet of Things (IoT) has revolutionized industries, enabling unprecedented connectivity and functionality. However, this expansion also increases vulnerabilities, exposing IoT networks to increasingly sophisticated cyberattacks. Intrusion Detection Systems (IDS) are crucial for mitigating these threats, and recent advancements in Machine Learning (ML) offer promising avenues for improvement. This research explores a hybrid approach, combining several standalone ML models such as Random Forest (RF), XGBoost, K-Nearest Neighbors (KNN), and AdaBoost, in a voting-based hybrid classifier for effective IoT intrusion detection. This ensemble method leverages the strengths of individual algorithms to enhance accuracy and address challenges related to data complexity and scalability. Using the widely-cited IoT-23 dataset, a prominent benchmark in IoT cybersecurity research, we evaluate our hybrid classifiers for both binary and multi-class intrusion detection problems, ensuring a fair comparison with existing literature. Results demonstrate that our proposed hybrid models, designed for robustness and scalability, outperform standalone approaches in IoT environments. This work contributes to the development of advanced, intelligent IDS frameworks capable of addressing evolving cyber threats.

### Enhanced Anomaly Detection in IoMT Networks using Ensemble AI Models on the CICIoMT2024 Dataset 
[[arxiv](https://arxiv.org/abs/2502.11854)] [[cool](https://papers.cool/arxiv/2502.11854)] [[pdf](https://arxiv.org/pdf/2502.11854)]
> **Authors**: Prathamesh Chandekar,Mansi Mehta,Swet Chandan
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,机器学习
- **Abstract**: The rapid proliferation of Internet of Medical Things (IoMT) devices in healthcare has introduced unique cybersecurity challenges, primarily due to the diverse communication protocols and critical nature of these devices This research aims to develop an advanced, real-time anomaly detection framework tailored for IoMT network traffic, leveraging AI/ML models and the CICIoMT2024 dataset By integrating multi-protocol (MQTT, WiFi), attack-specific (DoS, DDoS), time-series (active/idle states), and device-specific (Bluetooth) data, our study captures a comprehensive range of IoMT interactions As part of our data analysis, various machine learning techniques are employed which include an ensemble model using XGBoost for improved performance against specific attack types, sequential models comprised of LSTM and CNN-LSTM that leverage time dependencies, and unsupervised models such as Autoencoders and Isolation Forest that are good in general anomaly detection The results of the experiment prove with an ensemble model lowers false positive rates and reduced detections.

### BaxBench: Can LLMs Generate Correct and Secure Backends? 
[[arxiv](https://arxiv.org/abs/2502.11844)] [[cool](https://papers.cool/arxiv/2502.11844)] [[pdf](https://arxiv.org/pdf/2502.11844)]
> **Authors**: Mark Vero,Niels Mündler,Victor Chibotaru,Veselin Raychev,Maximilian Baader,Nikola Jovanović,Jingxuan He,Martin Vechev
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能,机器学习,编程语言
- **Abstract**: The automatic generation of programs has long been a fundamental challenge in computer science. Recent benchmarks have shown that large language models (LLMs) can effectively generate code at the function level, make code edits, and solve algorithmic coding tasks. However, to achieve full automation, LLMs should be able to generate production-quality, self-contained application modules. To evaluate the capabilities of LLMs in solving this challenge, we introduce BaxBench, a novel evaluation benchmark consisting of 392 tasks for the generation of backend applications. We focus on backends for three critical reasons: (i) they are practically relevant, building the core components of most modern web and cloud software, (ii) they are difficult to get right, requiring multiple functions and files to achieve the desired functionality, and (iii) they are security-critical, as they are exposed to untrusted third-parties, making secure solutions that prevent deployment-time attacks an imperative. BaxBench validates the functionality of the generated applications with comprehensive test cases, and assesses their security exposure by executing end-to-end exploits. Our experiments reveal key limitations of current LLMs in both functionality and security: (i) even the best model, OpenAI o1, achieves a mere 60% on code correctness; (ii) on average, we could successfully execute security exploits on more than half of the correct programs generated by each LLM; and (iii) in less popular backend frameworks, models further struggle to generate correct and secure applications. Progress on BaxBench signifies important steps towards autonomous and secure software development with LLMs.

### ReVeil: Unconstrained Concealed Backdoor Attack on Deep Neural Networks using Machine Unlearning 
[[arxiv](https://arxiv.org/abs/2502.11687)] [[cool](https://papers.cool/arxiv/2502.11687)] [[pdf](https://arxiv.org/pdf/2502.11687)]
> **Authors**: Manaar Alam,Hithem Lamri,Michail Maniatakos
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: This paper is accepted at 62nd Design Automation Conference (DAC) 2025
- **标题**: None
- **领域**: 密码学和安全,人工智能,机器学习
- **Abstract**: Backdoor attacks embed hidden functionalities in deep neural networks (DNN), triggering malicious behavior with specific inputs. Advanced defenses monitor anomalous DNN inferences to detect such attacks. However, concealed backdoors evade detection by maintaining a low pre-deployment attack success rate (ASR) and restoring high ASR post-deployment via machine unlearning. Existing concealed backdoors are often constrained by requiring white-box or black-box access or auxiliary data, limiting their practicality when such access or data is unavailable. This paper introduces ReVeil, a concealed backdoor attack targeting the data collection phase of the DNN training pipeline, requiring no model access or auxiliary data. ReVeil maintains low pre-deployment ASR across four datasets and four trigger patterns, successfully evades three popular backdoor detection methods, and restores high ASR post-deployment through machine unlearning.

### DELMAN: Dynamic Defense Against Large Language Model Jailbreaking with Model Editing 
[[arxiv](https://arxiv.org/abs/2502.11647)] [[cool](https://papers.cool/arxiv/2502.11647)] [[pdf](https://arxiv.org/pdf/2502.11647)]
> **Authors**: Yi Wang,Fenghua Weng,Sibei Yang,Zhan Qin,Minlie Huang,Wenjie Wang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Large Language Models (LLMs) are widely applied in decision making, but their deployment is threatened by jailbreak attacks, where adversarial users manipulate model behavior to bypass safety measures. Existing defense mechanisms, such as safety fine-tuning and model editing, either require extensive parameter modifications or lack precision, leading to performance degradation on general tasks, which is unsuitable to post-deployment safety alignment. To address these challenges, we propose DELMAN (Dynamic Editing for LLMs JAilbreak DefeNse), a novel approach leveraging direct model editing for precise, dynamic protection against jailbreak attacks. DELMAN directly updates a minimal set of relevant parameters to neutralize harmful behaviors while preserving the model's utility. To avoid triggering a safe response in benign context, we incorporate KL-divergence regularization to ensure the updated model remains consistent with the original model when processing benign queries. Experimental results demonstrate that DELMAN outperforms baseline methods in mitigating jailbreak attacks while preserving the model's utility, and adapts seamlessly to new attack instances, providing a practical and efficient solution for post-deployment model protection.

### DeFiScope: Detecting Various DeFi Price Manipulations with LLM Reasoning 
[[arxiv](https://arxiv.org/abs/2502.11521)] [[cool](https://papers.cool/arxiv/2502.11521)] [[pdf](https://arxiv.org/pdf/2502.11521)]
> **Authors**: Juantao Zhong,Daoyuan Wu,Ye Liu,Maoyi Xie,Yang Liu,Yi Li,Ning Liu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: DeFi (Decentralized Finance) is one of the most important applications of today's cryptocurrencies and smart contracts. It manages hundreds of billions in Total Value Locked (TVL) on-chain, yet it remains susceptible to common DeFi price manipulation attacks. Despite state-of-the-art (SOTA) systems like DeFiRanger and DeFort, we found that they are less effective to non-standard price models in custom DeFi protocols, which account for 44.2% of the 95 DeFi price manipulation attacks reported over the past three years. In this paper, we introduce the first LLM-based approach, DeFiScope, for detecting DeFi price manipulation attacks in both standard and custom price models. Our insight is that large language models (LLMs) have certain intelligence to abstract price calculation from code and infer the trend of token price changes based on the extracted price models. To further strengthen LLMs in this aspect, we leverage Foundry to synthesize on-chain data and use it to fine-tune a DeFi price-specific LLM. Together with the high-level DeFi operations recovered from low-level transaction data, DeFiScope detects various DeFi price manipulations according to systematically mined patterns. Experimental results show that DeFiScope achieves a high precision of 96% and a recall rate of 80%, significantly outperforming SOTA approaches. Moreover, we evaluate DeFiScope's cost-effectiveness and demonstrate its practicality by helping our industry partner confirm 147 real-world price manipulation attacks, including discovering 81 previously unknown historical incidents.

### Optimized detection of cyber-attacks on IoT networks via hybrid deep learning models 
[[arxiv](https://arxiv.org/abs/2502.11470)] [[cool](https://papers.cool/arxiv/2502.11470)] [[pdf](https://arxiv.org/pdf/2502.11470)]
> **Authors**: Ahmed Bensaoud,Jugal Kalita
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: ef:deeplearningmodels." Ad Hoc Networks 170 (2025): 103770
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: The rapid expansion of Internet of Things (IoT) devices has increased the risk of cyber-attacks, making effective detection essential for securing IoT networks. This work introduces a novel approach combining Self-Organizing Maps (SOMs), Deep Belief Networks (DBNs), and Autoencoders to detect known and previously unseen attack patterns. A comprehensive evaluation using simulated and real-world traffic data is conducted, with models optimized via Particle Swarm Optimization (PSO). The system achieves an accuracy of up to 99.99% and Matthews Correlation Coefficient (MCC) values exceeding 99.50%. Experiments on NSL-KDD, UNSW-NB15, and CICIoT2023 confirm the model's strong performance across diverse attack types. These findings suggest that the proposed method enhances IoT security by identifying emerging threats and adapting to evolving attack strategies.

## 计算机视觉和模式识别(cs.CV:Computer Vision and Pattern Recognition)

### Learning Transformation-Isomorphic Latent Space for Accurate Hand Pose Estimation 
[[arxiv](https://arxiv.org/abs/2502.12535)] [[cool](https://papers.cool/arxiv/2502.12535)] [[pdf](https://arxiv.org/pdf/2502.12535)]
> **Authors**: Kaiwen Ren,Lei Hu,Zhiheng Zhang,Yongjing Ye,Shihong Xia
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Vision-based regression tasks, such as hand pose estimation, have achieved higher accuracy and faster convergence through representation learning. However, existing representation learning methods often encounter the following issues: the high semantic level of features extracted from images is inadequate for regressing low-level information, and the extracted features include task-irrelevant information, reducing their compactness and interfering with regression tasks. To address these challenges, we propose TI-Net, a highly versatile visual Network backbone designed to construct a Transformation Isomorphic latent space. Specifically, we employ linear transformations to model geometric transformations in the latent space and ensure that {\rm TI-Net} aligns them with those in the image space. This ensures that the latent features capture compact, low-level information beneficial for pose estimation tasks. We evaluated TI-Net on the hand pose estimation task to demonstrate the network's superiority. On the DexYCB dataset, TI-Net achieved a 10% improvement in the PA-MPJPE metric compared to specialized state-of-the-art (SOTA) hand pose estimation methods. Our code will be released in the future.

### NoKSR: Kernel-Free Neural Surface Reconstruction via Point Cloud Serialization 
[[arxiv](https://arxiv.org/abs/2502.12534)] [[cool](https://papers.cool/arxiv/2502.12534)] [[pdf](https://arxiv.org/pdf/2502.12534)]
> **Authors**: Zhen Li,Weiwei Sun,Shrisudhan Govindarajan,Shaobo Xia,Daniel Rebain,Kwang Moo Yi,Andrea Tagliasacchi
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Project page: see https://theialab.github.io/noksr/
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: We present a novel approach to large-scale point cloud surface reconstruction by developing an efficient framework that converts an irregular point cloud into a signed distance field (SDF). Our backbone builds upon recent transformer-based architectures (i.e., PointTransformerV3), that serializes the point cloud into a locality-preserving sequence of tokens. We efficiently predict the SDF value at a point by aggregating nearby tokens, where fast approximate neighbors can be retrieved thanks to the serialization. We serialize the point cloud at different levels/scales, and non-linearly aggregate a feature to predict the SDF value. We show that aggregating across multiple scales is critical to overcome the approximations introduced by the serialization (i.e. false negatives in the neighborhood). Our frameworks sets the new state-of-the-art in terms of accuracy and efficiency (better or similar performance with half the latency of the best prior method, coupled with a simpler implementation), particularly on outdoor datasets where sparse-grid methods have shown limited performance.

### SafeEraser: Enhancing Safety in Multimodal Large Language Models through Multimodal Machine Unlearning 
[[arxiv](https://arxiv.org/abs/2502.12520)] [[cool](https://papers.cool/arxiv/2502.12520)] [[pdf](https://arxiv.org/pdf/2502.12520)]
> **Authors**: Junkai Chen,Zhijie Deng,Kening Zheng,Yibo Yan,Shuliang Liu,PeiJun Wu,Peijie Jiang,Jia Liu,Xuming Hu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: As Multimodal Large Language Models (MLLMs) develop, their potential security issues have become increasingly prominent. Machine Unlearning (MU), as an effective strategy for forgetting specific knowledge in training data, has been widely used in privacy protection. However, MU for safety in MLLM has yet to be fully explored. To address this issue, we propose SAFEERASER, a safety unlearning benchmark for MLLMs, consisting of 3,000 images and 28.8K VQA pairs. We comprehensively evaluate unlearning methods from two perspectives: forget quality and model utility. Our findings show that existing MU methods struggle to maintain model performance while implementing the forget operation and often suffer from over-forgetting. Hence, we introduce Prompt Decouple (PD) Loss to alleviate over-forgetting through decouple prompt during unlearning process. To quantitatively measure over-forgetting mitigated by PD Loss, we propose a new metric called Safe Answer Refusal Rate (SARR). Experimental results demonstrate that combining PD Loss with existing unlearning methods can effectively prevent over-forgetting and achieve a decrease of 79.5% in the SARR metric of LLaVA-7B and LLaVA-13B, while maintaining forget quality and model utility. Our code and dataset will be released upon acceptance. Warning: This paper contains examples of harmful language and images, and reader discretion is recommended.

### RealSyn: An Effective and Scalable Multimodal Interleaved Document Transformation Paradigm 
[[arxiv](https://arxiv.org/abs/2502.12513)] [[cool](https://papers.cool/arxiv/2502.12513)] [[pdf](https://arxiv.org/pdf/2502.12513)]
> **Authors**: Tiancheng Gu,Kaicheng Yang,Chaoyi Zhang,Yin Xie,Xiang An,Ziyong Feng,Dongnan Liu,Weidong Cai,Jiankang Deng
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 16 pages, 12 figures, Webpage: https://garygutc.github.io/RealSyn
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: After pre-training on extensive image-text pairs, Contrastive Language-Image Pre-training (CLIP) demonstrates promising performance on a wide variety of benchmarks. However, a substantial volume of non-paired data, such as multimodal interleaved documents, remains underutilized for vision-language representation learning. To fully leverage these unpaired documents, we initially establish a Real-World Data Extraction pipeline to extract high-quality images and texts. Then we design a hierarchical retrieval method to efficiently associate each image with multiple semantically relevant realistic texts. To further enhance fine-grained visual information, we propose an image semantic augmented generation module for synthetic text production. Furthermore, we employ a semantic balance sampling strategy to improve dataset diversity, enabling better learning of long-tail concepts. Based on these innovations, we construct RealSyn, a dataset combining realistic and synthetic texts, available in three scales: 15M, 30M, and 100M. Extensive experiments demonstrate that RealSyn effectively advances vision-language representation learning and exhibits strong scalability. Models pre-trained on RealSyn achieve state-of-the-art performance on multiple downstream tasks. To facilitate future research, the RealSyn dataset and pre-trained model weights are released at https://github.com/deepglint/RealSyn.

### Enhancing Audio-Visual Spiking Neural Networks through Semantic-Alignment and Cross-Modal Residual Learning 
[[arxiv](https://arxiv.org/abs/2502.12488)] [[cool](https://papers.cool/arxiv/2502.12488)] [[pdf](https://arxiv.org/pdf/2502.12488)]
> **Authors**: Xiang He,Dongcheng Zhao,Yiting Dong,Guobin Shen,Xin Yang,Yi Zeng
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: The manuscript is under review and the code is available https://github.com/Brain-Cog-Lab/S-CMRL
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Humans interpret and perceive the world by integrating sensory information from multiple modalities, such as vision and hearing. Spiking Neural Networks (SNNs), as brain-inspired computational models, exhibit unique advantages in emulating the brain's information processing mechanisms. However, existing SNN models primarily focus on unimodal processing and lack efficient cross-modal information fusion, thereby limiting their effectiveness in real-world multimodal scenarios. To address this challenge, we propose a semantic-alignment cross-modal residual learning (S-CMRL) framework, a Transformer-based multimodal SNN architecture designed for effective audio-visual integration. S-CMRL leverages a spatiotemporal spiking attention mechanism to extract complementary features across modalities, and incorporates a cross-modal residual learning strategy to enhance feature integration. Additionally, a semantic alignment optimization mechanism is introduced to align cross-modal features within a shared semantic space, improving their consistency and complementarity. Extensive experiments on three benchmark datasets CREMA-D, UrbanSound8K-AV, and MNISTDVS-NTIDIGITS demonstrate that S-CMRL significantly outperforms existing multimodal SNN methods, achieving the state-of-the-art performance. The code is publicly available at https://github.com/Brain-Cog-Lab/S-CMRL.

### Predicate Hierarchies Improve Few-Shot State Classification 
[[arxiv](https://arxiv.org/abs/2502.12481)] [[cool](https://papers.cool/arxiv/2502.12481)] [[pdf](https://arxiv.org/pdf/2502.12481)]
> **Authors**: Emily Jin,Joy Hsu,Jiajun Wu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: ICLR 2025. First two authors contributed equally. Project page: https://emilyzjin.github.io/projects/phier.html
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习,机器人技术
- **Abstract**: State classification of objects and their relations is core to many long-horizon tasks, particularly in robot planning and manipulation. However, the combinatorial explosion of possible object-predicate combinations, coupled with the need to adapt to novel real-world environments, makes it a desideratum for state classification models to generalize to novel queries with few examples. To this end, we propose PHIER, which leverages predicate hierarchies to generalize effectively in few-shot scenarios. PHIER uses an object-centric scene encoder, self-supervised losses that infer semantic relations between predicates, and a hyperbolic distance metric that captures hierarchical structure; it learns a structured latent space of image-predicate pairs that guides reasoning over state classification queries. We evaluate PHIER in the CALVIN and BEHAVIOR robotic environments and show that PHIER significantly outperforms existing methods in few-shot, out-of-distribution state classification, and demonstrates strong zero- and few-shot generalization from simulated to real-world tasks. Our results demonstrate that leveraging predicate hierarchies improves performance on state classification tasks with limited data.

### Benchmarking Zero-Shot Facial Emotion Annotation with Large Language Models: A Multi-Class and Multi-Frame Approach in DailyLife 
[[arxiv](https://arxiv.org/abs/2502.12454)] [[cool](https://papers.cool/arxiv/2502.12454)] [[pdf](https://arxiv.org/pdf/2502.12454)]
> **Authors**: He Zhang,Xinyi Fu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 10 pages
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: This study investigates the feasibility and performance of using large language models (LLMs) to automatically annotate human emotions in everyday scenarios. We conducted experiments on the DailyLife subset of the publicly available FERV39k dataset, employing the GPT-4o-mini model for rapid, zero-shot labeling of key frames extracted from video segments. Under a seven-class emotion taxonomy ("Angry," "Disgust," "Fear," "Happy," "Neutral," "Sad," "Surprise"), the LLM achieved an average precision of approximately 50%. In contrast, when limited to ternary emotion classification (negative/neutral/positive), the average precision increased to approximately 64%. Additionally, we explored a strategy that integrates multiple frames within 1-2 second video clips to enhance labeling performance and reduce costs. The results indicate that this approach can slightly improve annotation accuracy. Overall, our preliminary findings highlight the potential application of zero-shot LLMs in human facial emotion annotation tasks, offering new avenues for reducing labeling costs and broadening the applicability of LLMs in complex multimodal environments.

### Multi Image Super Resolution Modeling for Earth System Models 
[[arxiv](https://arxiv.org/abs/2502.12427)] [[cool](https://papers.cool/arxiv/2502.12427)] [[pdf](https://arxiv.org/pdf/2502.12427)]
> **Authors**: Ehsan Zeraatkar,Salah A Faroughi,Jelena Tešić
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Super-resolution (SR) techniques are essential for improving Earth System Model (ESM) data's spatial resolution, which helps better understand complex environmental processes. This paper presents a new algorithm, ViFOR, which combines Vision Transformers (ViT) and Implicit Neural Representation Networks (INRs) to generate High-Resolution (HR) images from Low-Resolution (LR) inputs. ViFOR introduces a novel integration of Fourier-based activation functions within the Vision Transformer architecture, enabling it to effectively capture global context and high-frequency details critical for accurate SR reconstruction. The results show that ViFOR outperforms state-of-the-art methods such as ViT, Sinusoidal Representation Networks (SIREN), and SR Generative Adversarial Networks (SRGANs) based on metrics like Peak Signal-to-Noise Ratio (PSNR) and Mean Squared Error (MSE) both for global as well as the local imagery. ViFOR improves PSNR of up to 4.18 dB, 1.56 dB, and 1.73 dB over ViT for full images in the Source Temperature, Shortwave, and Longwave Flux.

### Boosting Illuminant Estimation in Deep Color Constancy through Enhancing Brightness Robustness 
[[arxiv](https://arxiv.org/abs/2502.12418)] [[cool](https://papers.cool/arxiv/2502.12418)] [[pdf](https://arxiv.org/pdf/2502.12418)]
> **Authors**: Mengda Xie,Chengzhi Zhong,Yiling He,Zhan Qin,Meie Fang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Color constancy estimates illuminant chromaticity to correct color-biased images. Recently, Deep Neural Network-driven Color Constancy (DNNCC) models have made substantial advancements. Nevertheless, the potential risks in DNNCC due to the vulnerability of deep neural networks have not yet been explored. In this paper, we conduct the first investigation into the impact of a key factor in color constancy-brightness-on DNNCC from a robustness perspective. Our evaluation reveals that several mainstream DNNCC models exhibit high sensitivity to brightness despite their focus on chromaticity estimation. This sheds light on a potential limitation of existing DNNCC models: their sensitivity to brightness may hinder performance given the widespread brightness variations in real-world datasets. From the insights of our analysis, we propose a simple yet effective brightness robustness enhancement strategy for DNNCC models, termed BRE. The core of BRE is built upon the adaptive step-size adversarial brightness augmentation technique, which identifies high-risk brightness variation and generates augmented images via explicit brightness adjustment. Subsequently, BRE develops a brightness-robustness-aware model optimization strategy that integrates adversarial brightness training and brightness contrastive loss, significantly bolstering the brightness robustness of DNNCC models. BRE is hyperparameter-free and can be integrated into existing DNNCC models, without incurring additional overhead during the testing phase. Experiments on two public color constancy datasets-ColorChecker and Cube+-demonstrate that the proposed BRE consistently enhances the illuminant estimation performance of existing DNNCC models, reducing the estimation error by an average of 5.04% across six mainstream DNNCC models, underscoring the critical role of enhancing brightness robustness in these models.

### Gaseous Object Detection 
[[arxiv](https://arxiv.org/abs/2502.12415)] [[cool](https://papers.cool/arxiv/2502.12415)] [[pdf](https://arxiv.org/pdf/2502.12415)]
> **Authors**: Kailai Zhou,Yibo Wang,Tao Lv,Qiu Shen,Xun Cao
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: IEEE Transactions on Pattern Analysis andMachineIntelligence (2024)
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Object detection, a fundamental and challenging problem in computer vision, has experienced rapid development due to the effectiveness of deep learning. The current objects to be detected are mostly rigid solid substances with apparent and distinct visual characteristics. In this paper, we endeavor on a scarcely explored task named Gaseous Object Detection (GOD), which is undertaken to explore whether the object detection techniques can be extended from solid substances to gaseous substances. Nevertheless, the gas exhibits significantly different visual characteristics: 1) saliency deficiency, 2) arbitrary and ever-changing shapes, 3) lack of distinct boundaries. To facilitate the study on this challenging task, we construct a GOD-Video dataset comprising 600 videos (141,017 frames) that cover various attributes with multiple types of gases. A comprehensive benchmark is established based on this dataset, allowing for a rigorous evaluation of frame-level and video-level detectors. Deduced from the Gaussian dispersion model, the physics-inspired Voxel Shift Field (VSF) is designed to model geometric irregularities and ever-changing shapes in potential 3D space. By integrating VSF into Faster RCNN, the VSF RCNN serves as a simple but strong baseline for gaseous object detection. Our work aims to attract further research into this valuable albeit challenging area.

### OCT Data is All You Need: How Vision Transformers with and without Pre-training Benefit Imaging 
[[arxiv](https://arxiv.org/abs/2502.12379)] [[cool](https://papers.cool/arxiv/2502.12379)] [[pdf](https://arxiv.org/pdf/2502.12379)]
> **Authors**: Zihao Han,Philippe De Wilde
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Optical Coherence Tomography (OCT) provides high-resolution cross-sectional images useful for diagnosing various diseases, but their distinct characteristics from natural images raise questions about whether large-scale pre-training on datasets like ImageNet is always beneficial. In this paper, we investigate the impact of ImageNet-based pre-training on Vision Transformer (ViT) performance for OCT image classification across different dataset sizes. Our experiments cover four-category retinal pathologies (CNV, DME, Drusen, Normal). Results suggest that while pre-training can accelerate convergence and potentially offer better performance in smaller datasets, training from scratch may achieve comparable or even superior accuracy when sufficient OCT data is available. Our findings highlight the importance of matching domain characteristics in pre-training and call for further study on large-scale OCT-specific pre-training.

### Detecting Systematic Weaknesses in Vision Models along Predefined Human-Understandable Dimensions 
[[arxiv](https://arxiv.org/abs/2502.12360)] [[cool](https://papers.cool/arxiv/2502.12360)] [[pdf](https://arxiv.org/pdf/2502.12360)]
> **Authors**: Sujan Sai Gannamaneni,Rohil Prakash Rao,Michael Mock,Maram Akila,Stefan Wrobel
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: Studying systematic weaknesses of DNNs has gained prominence in the last few years with the rising focus on building safe AI systems. Slice discovery methods (SDMs) are prominent algorithmic approaches for finding such systematic weaknesses. They identify top-k semantically coherent slices/subsets of data where a DNN-under-test has low performance. For being directly useful, e.g., as evidences in a safety argumentation, slices should be aligned with human-understandable (safety-relevant) dimensions, which, for example, are defined by safety and domain experts as parts of the operational design domain (ODD). While straightforward for structured data, the lack of semantic metadata makes these investigations challenging for unstructured data. Therefore, we propose a complete workflow which combines contemporary foundation models with algorithms for combinatorial search that consider structured data and DNN errors for finding systematic weaknesses in images. In contrast to existing approaches, ours identifies weak slices that are in line with predefined human-understandable dimensions. As the workflow includes foundation models, its intermediate and final results may not always be exact. Therefore, we build into our workflow an approach to address the impact of noisy metadata. We evaluate our approach w.r.t. its quality on four popular computer vision datasets, including autonomous driving datasets like Cityscapes, BDD100k, and RailSem19, while using multiple state-of-the-art models as DNNs-under-test.

### LanP: Rethinking the Impact of Language Priors in Large Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2502.12359)] [[cool](https://papers.cool/arxiv/2502.12359)] [[pdf](https://arxiv.org/pdf/2502.12359)]
> **Authors**: Zongyu Wu,Yuwei Niu,Hongcheng Gao,Minhua Lin,Zhiwei Zhang,Zhifang Zhang,Qi Shi,Yilong Wang,Sike Fu,Junjie Xu,Junjie Ao,Enyan Dai,Lei Feng,Xiang Zhang,Suhang Wang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Preprint
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Large Vision-Language Models (LVLMs) have shown impressive performance in various tasks. However, LVLMs suffer from hallucination, which hinders their adoption in the real world. Existing studies emphasized that the strong language priors of LVLMs can overpower visual information, causing hallucinations. However, the positive role of language priors is the key to a powerful LVLM. If the language priors are too weak, LVLMs will struggle to leverage rich parameter knowledge and instruction understanding abilities to complete tasks in challenging visual scenarios where visual information alone is insufficient. Therefore, we propose a benchmark called LanP to rethink the impact of Language Priors in LVLMs. It is designed to investigate how strong language priors are in current LVLMs. LanP consists of 170 images and 340 corresponding well-designed questions. Extensive experiments on 25 popular LVLMs reveal that many LVLMs' language priors are not strong enough to effectively aid question answering when objects are partially hidden. Many models, including GPT-4 Turbo, exhibit an accuracy below 0.5 in such a scenario.

### Data-Efficient Limited-Angle CT Using Deep Priors and Regularization 
[[arxiv](https://arxiv.org/abs/2502.12293)] [[cool](https://papers.cool/arxiv/2502.12293)] [[pdf](https://arxiv.org/pdf/2502.12293)]
> **Authors**: Ilmari Vahteristo,Zhi-Song Liu,Andreas Rupp
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 12 pages, 2 reference pages, 5 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Reconstructing an image from its Radon transform is a fundamental computed tomography (CT) task arising in applications such as X-ray scans. In many practical scenarios, a full 180-degree scan is not feasible, or there is a desire to reduce radiation exposure. In these limited-angle settings, the problem becomes ill-posed, and methods designed for full-view data often leave significant artifacts. We propose a very low-data approach to reconstruct the original image from its Radon transform under severe angle limitations. Because the inverse problem is ill-posed, we combine multiple regularization methods, including Total Variation, a sinogram filter, Deep Image Prior, and a patch-level autoencoder. We use a differentiable implementation of the Radon transform, which allows us to use gradient-based techniques to solve the inverse problem. Our method is evaluated on a dataset from the Helsinki Tomography Challenge 2022, where the goal is to reconstruct a binary disk from its limited-angle sinogram. We only use a total of 12 data points--eight for learning a prior and four for hyperparameter selection--and achieve results comparable to the best synthetic data-driven approaches.

### SmokeNet: Efficient Smoke Segmentation Leveraging Multiscale Convolutions and Multiview Attention Mechanisms 
[[arxiv](https://arxiv.org/abs/2502.12258)] [[cool](https://papers.cool/arxiv/2502.12258)] [[pdf](https://arxiv.org/pdf/2502.12258)]
> **Authors**: Xuesong Liu,Emmett J. Ientilucci
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Efficient segmentation of smoke plumes is crucial for environmental monitoring and industrial safety, enabling the detection and mitigation of harmful emissions from activities like quarry blasts and wildfires. Accurate segmentation facilitates environmental impact assessments, timely interventions, and compliance with safety standards. However, existing models often face high computational demands and limited adaptability to diverse smoke appearances, restricting their deployment in resource-constrained environments. To address these issues, we introduce SmokeNet, a novel deep learning architecture that leverages multiscale convolutions and multiview linear attention mechanisms combined with layer-specific loss functions to handle the complex dynamics of diverse smoke plumes, ensuring efficient and accurate segmentation across varied environments. Additionally, we evaluate SmokeNet's performance and versatility using four datasets, including our quarry blast smoke dataset made available to the community. The results demonstrate that SmokeNet maintains a favorable balance between computational efficiency and segmentation accuracy, making it suitable for deployment in environmental monitoring and safety management systems. By contributing a new dataset and offering an efficient segmentation model, SmokeNet advances smoke segmentation capabilities in diverse and challenging environments.

### Diffusion Models without Classifier-free Guidance 
[[arxiv](https://arxiv.org/abs/2502.12154)] [[cool](https://papers.cool/arxiv/2502.12154)] [[pdf](https://arxiv.org/pdf/2502.12154)]
> **Authors**: Zhicong Tang,Jianmin Bao,Dong Chen,Baining Guo
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: This paper presents Model-guidance (MG), a novel objective for training diffusion model that addresses and removes of the commonly used Classifier-free guidance (CFG). Our innovative approach transcends the standard modeling of solely data distribution to incorporating the posterior probability of conditions. The proposed technique originates from the idea of CFG and is easy yet effective, making it a plug-and-play module for existing models. Our method significantly accelerates the training process, doubles the inference speed, and achieve exceptional quality that parallel and even surpass concurrent diffusion models with CFG. Extensive experiments demonstrate the effectiveness, efficiency, scalability on different models and datasets. Finally, we establish state-of-the-art performance on ImageNet 256 benchmarks with an FID of 1.34. Our code is available at https://github.com/tzco/Diffusion-wo-CFG.

### HermesFlow: Seamlessly Closing the Gap in Multimodal Understanding and Generation 
[[arxiv](https://arxiv.org/abs/2502.12148)] [[cool](https://papers.cool/arxiv/2502.12148)] [[pdf](https://arxiv.org/pdf/2502.12148)]
> **Authors**: Ling Yang,Xinchen Zhang,Ye Tian,Chenming Shang,Minghao Xu,Wentao Zhang,Bin Cui
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Code: https://github.com/Gen-Verse/HermesFlow
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: The remarkable success of the autoregressive paradigm has made significant advancement in Multimodal Large Language Models (MLLMs), with powerful models like Show-o, Transfusion and Emu3 achieving notable progress in unified image understanding and generation. For the first time, we uncover a common phenomenon: the understanding capabilities of MLLMs are typically stronger than their generative capabilities, with a significant gap between the two. Building on this insight, we propose HermesFlow, a simple yet general framework designed to seamlessly bridge the gap between understanding and generation in MLLMs. Specifically, we take the homologous data as input to curate homologous preference data of both understanding and generation. Through Pair-DPO and self-play iterative optimization, HermesFlow effectively aligns multimodal understanding and generation using homologous preference data. Extensive experiments demonstrate the significant superiority of our approach over prior methods, particularly in narrowing the gap between multimodal understanding and generation. These findings highlight the potential of HermesFlow as a general alignment framework for next-generation multimodal foundation models. Code: https://github.com/Gen-Verse/HermesFlow

### MagicArticulate: Make Your 3D Models Articulation-Ready 
[[arxiv](https://arxiv.org/abs/2502.12135)] [[cool](https://papers.cool/arxiv/2502.12135)] [[pdf](https://arxiv.org/pdf/2502.12135)]
> **Authors**: Chaoyue Song,Jianfeng Zhang,Xiu Li,Fan Yang,Yiwen Chen,Zhongcong Xu,Jun Hao Liew,Xiaoyang Guo,Fayao Liu,Jiashi Feng,Guosheng Lin
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Project: https://chaoyuesong.github.io/MagicArticulate
- **标题**: None
- **领域**: 计算机视觉和模式识别,图形
- **Abstract**: With the explosive growth of 3D content creation, there is an increasing demand for automatically converting static 3D models into articulation-ready versions that support realistic animation. Traditional approaches rely heavily on manual annotation, which is both time-consuming and labor-intensive. Moreover, the lack of large-scale benchmarks has hindered the development of learning-based solutions. In this work, we present MagicArticulate, an effective framework that automatically transforms static 3D models into articulation-ready assets. Our key contributions are threefold. First, we introduce Articulation-XL, a large-scale benchmark containing over 33k 3D models with high-quality articulation annotations, carefully curated from Objaverse-XL. Second, we propose a novel skeleton generation method that formulates the task as a sequence modeling problem, leveraging an auto-regressive transformer to naturally handle varying numbers of bones or joints within skeletons and their inherent dependencies across different 3D models. Third, we predict skinning weights using a functional diffusion process that incorporates volumetric geodesic distance priors between vertices and joints. Extensive experiments demonstrate that MagicArticulate significantly outperforms existing methods across diverse object categories, achieving high-quality articulation that enables realistic animation. Project page: https://chaoyuesong.github.io/MagicArticulate.

### PRISM: Self-Pruning Intrinsic Selection Method for Training-Free Multimodal Data Selection 
[[arxiv](https://arxiv.org/abs/2502.12119)] [[cool](https://papers.cool/arxiv/2502.12119)] [[pdf](https://arxiv.org/pdf/2502.12119)]
> **Authors**: Jinhe Bi,Yifan Wang,Danqi Yan,Xun Xiao,Artur Hecker,Volker Tresp,Yunpu Ma
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **Abstract**: Visual instruction tuning refines pre-trained Multimodal Large Language Models (MLLMs) to enhance their real-world task performance. However, the rapid expansion of visual instruction datasets introduces significant data redundancy, leading to excessive computational costs. Existing data selection methods predominantly rely on proxy models or loss-based metrics, both of which impose substantial computational overheads due to the necessity of model inference and backpropagation. To address this challenge, we propose PRISM, a novel training-free approach for efficient multimodal data selection. Unlike existing methods, PRISM eliminates the reliance on proxy models, warm-up pretraining, and gradient-based optimization. Instead, it leverages Pearson correlation analysis to quantify the intrinsic visual encoding properties of MLLMs, computing a task-specific correlation score to identify high-value instances. This not only enbles data-efficient selection,but maintains the original performance. Empirical evaluations across multiple MLLMs demonstrate that PRISM reduces the overall time required for visual instruction tuning and data selection to just 30% of conventional methods, while surpassing fully fine-tuned models across eight multimodal and three language understanding benchmarks, achieving a 101.7% relative improvement in final performance.

### Descriminative-Generative Custom Tokens for Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2502.12095)] [[cool](https://papers.cool/arxiv/2502.12095)] [[pdf](https://arxiv.org/pdf/2502.12095)]
> **Authors**: Pramuditha Perera,Matthew Trager,Luca Zancato,Alessandro Achille,Stefano Soatto
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: This paper explores the possibility of learning custom tokens for representing new concepts in Vision-Language Models (VLMs). Our aim is to learn tokens that can be effective for both discriminative and generative tasks while composing well with words to form new input queries. The targeted concept is specified in terms of a small set of images and a parent concept described using text. We operate on CLIP text features and propose to use a combination of a textual inversion loss and a classification loss to ensure that text features of the learned token are aligned with image features of the concept in the CLIP embedding space. We restrict the learned token to a low-dimensional subspace spanned by tokens for attributes that are appropriate for the given super-class. These modifications improve the quality of compositions of the learned token with natural language for generating new scenes. Further, we show that learned custom tokens can be used to form queries for text-to-image retrieval task, and also have the important benefit that composite queries can be visualized to ensure that the desired concept is faithfully encoded. Based on this, we introduce the method of Generation Aided Image Retrieval, where the query is modified at inference time to better suit the search intent. On the DeepFashion2 dataset, our method improves Mean Reciprocal Retrieval (MRR) over relevant baselines by 7%.

### Unhackable Temporal Rewarding for Scalable Video MLLMs 
[[arxiv](https://arxiv.org/abs/2502.12081)] [[cool](https://papers.cool/arxiv/2502.12081)] [[pdf](https://arxiv.org/pdf/2502.12081)]
> **Authors**: En Yu,Kangheng Lin,Liang Zhao,Yana Wei,Zining Zhu,Haoran Wei,Jianjian Sun,Zheng Ge,Xiangyu Zhang,Jingyu Wang,Wenbing Tao
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Accepted by ICLR2025. Project Page: https://ahnsun.github.io/UTR/
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算语言学
- **Abstract**: In the pursuit of superior video-processing MLLMs, we have encountered a perplexing paradox: the "anti-scaling law", where more data and larger models lead to worse performance. This study unmasks the culprit: "temporal hacking", a phenomenon where models shortcut by fixating on select frames, missing the full video narrative. In this work, we systematically establish a comprehensive theory of temporal hacking, defining it from a reinforcement learning perspective, introducing the Temporal Perplexity (TPL) score to assess this misalignment, and proposing the Unhackable Temporal Rewarding (UTR) framework to mitigate the temporal hacking. Both theoretically and empirically, TPL proves to be a reliable indicator of temporal modeling quality, correlating strongly with frame activation patterns. Extensive experiments reveal that UTR not only counters temporal hacking but significantly elevates video comprehension capabilities. This work not only advances video-AI systems but also illuminates the critical importance of aligning proxy rewards with true objectives in MLLM development.

### HumanGif: Single-View Human Diffusion with Generative Prior 
[[arxiv](https://arxiv.org/abs/2502.12080)] [[cool](https://papers.cool/arxiv/2502.12080)] [[pdf](https://arxiv.org/pdf/2502.12080)]
> **Authors**: Shoukang Hu,Takuya Narihira,Kazumi Fukuda,Ryosuke Sawata,Takashi Shibuya,Yuki Mitsufuji
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Project page: https://skhu101.github.io/HumanGif/
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Previous 3D human creation methods have made significant progress in synthesizing view-consistent and temporally aligned results from sparse-view images or monocular videos. However, it remains challenging to produce perpetually realistic, view-consistent, and temporally coherent human avatars from a single image, as limited information is available in the single-view input setting. Motivated by the success of 2D character animation, we propose HumanGif, a single-view human diffusion model with generative prior. Specifically, we formulate the single-view-based 3D human novel view and pose synthesis as a single-view-conditioned human diffusion process, utilizing generative priors from foundational diffusion models to complement the missing information. To ensure fine-grained and consistent novel view and pose synthesis, we introduce a Human NeRF module in HumanGif to learn spatially aligned features from the input image, implicitly capturing the relative camera and human pose transformation. Furthermore, we introduce an image-level loss during optimization to bridge the gap between latent and image spaces in diffusion models. Extensive experiments on RenderPeople and DNA-Rendering datasets demonstrate that HumanGif achieves the best perceptual performance, with better generalizability for novel view and pose synthesis.

### Enhancing Transparent Object Pose Estimation: A Fusion of GDR-Net and Edge Detection 
[[arxiv](https://arxiv.org/abs/2502.12027)] [[cool](https://papers.cool/arxiv/2502.12027)] [[pdf](https://arxiv.org/pdf/2502.12027)]
> **Authors**: Tessa Pulli,Peter Hönig,Stefan Thalhammer,Matthias Hirschmanner,Markus Vincze
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: accepted at First Austrian Symposium onAI, Robotics, and Vision (AIROV 2024)
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Object pose estimation of transparent objects remains a challenging task in the field of robot vision due to the immense influence of lighting, background, and reflections. However, the edges of clear objects have the highest contrast, which leads to stable and prominent features. We propose a novel approach by incorporating edge detection in a pre-processing step for the tasks of object detection and object pose estimation. We conducted experiments to investigate the effect of edge detectors on transparent objects. We examine the performance of the state-of-the-art 6D object pose estimation pipeline GDR-Net and the object detector YOLOX when applying different edge detectors as pre-processing steps (i.e., Canny edge detection with and without color information, and holistically-nested edges (HED)). We evaluate the physically-based rendered dataset Trans6D-32 K of transparent objects with parameters proposed by the BOP Challenge. Our results indicate that applying edge detection as a pre-processing enhances performance for certain objects.

### Predicting Next-Day Wildfire Spread with Time Series and Attention 
[[arxiv](https://arxiv.org/abs/2502.12003)] [[cool](https://papers.cool/arxiv/2502.12003)] [[pdf](https://arxiv.org/pdf/2502.12003)]
> **Authors**: Saad Lahrichi,Jesse Johnson,Jordan Malof
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Recent research has demonstrated the potential of deep neural networks (DNNs) to accurately predict next-day wildfire spread, based upon the current extent of a fire and geospatial rasters of influential environmental covariates e.g., vegetation, topography, climate, and weather. In this work, we investigate a recent transformer-based model, termed the SwinUnet, for next-day wildfire prediction. We benchmark Swin-based models against several current state-of-the-art models on WildfireSpreadTS (WFTS), a large public benchmark dataset of historical wildfire events. We consider two next-day fire prediction scenarios: when the model is given input of (i) a single previous day of data, or (ii) five previous days of data. We find that, with the proper modifications, SwinUnet achieves state-of-the-art accuracy on next-day prediction for both the single-day and multi-day scenarios. SwinUnet's success depends heavily upon utilizing pre-trained weights from ImageNet. Consistent with prior work, we also found that models with multi-day-input always outperformed models with single-day input.

### MultiFlow: A unified deep learning framework for multi-vessel classification, segmentation and clustering of phase-contrast MRI validated on a multi-site single ventricle patient cohort 
[[arxiv](https://arxiv.org/abs/2502.11993)] [[cool](https://papers.cool/arxiv/2502.11993)] [[pdf](https://arxiv.org/pdf/2502.11993)]
> **Authors**: Tina Yao,Nicole St. Clair,Gabriel F. Miller,FORCE Investigators,Jennifer A. Steeden,Rahul H. Rathod,Vivek Muthurangu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 6 Figures, 1 Table
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: This study presents a unified deep learning (DL) framework, MultiFlowSeg, for classification and segmentation of velocity-encoded phase-contrast magnetic resonance imaging data, and MultiFlowDTC for temporal clustering of flow phenotypes. Applied to the FORCE registry of Fontan procedure patients, MultiFlowSeg achieved 100% classification accuracy for the aorta, SVC, and IVC, and 94% for the LPA and RPA. It demonstrated robust segmentation with a median Dice score of 0.91 (IQR: 0.86-0.93). The automated pipeline processed registry data, achieving high segmentation success despite challenges like poor image quality and dextrocardia. Temporal clustering identified five distinct patient subgroups, with significant differences in clinical outcomes, including ejection fraction, exercise tolerance, liver disease, and mortality. These results demonstrate the potential of combining DL and time-varying flow data for improved CHD prognosis and personalized care.

### DLFR-VAE: Dynamic Latent Frame Rate VAE for Video Generation 
[[arxiv](https://arxiv.org/abs/2502.11897)] [[cool](https://papers.cool/arxiv/2502.11897)] [[pdf](https://arxiv.org/pdf/2502.11897)]
> **Authors**: Zhihang Yuan,Siyuan Wang,Rui Xie,Hanling Zhang,Tongcheng Fang,Yuzhang Shang,Shengen Yan,Guohao Dai,Yu Wang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: In this paper, we propose the Dynamic Latent Frame Rate VAE (DLFR-VAE), a training-free paradigm that can make use of adaptive temporal compression in latent space. While existing video generative models apply fixed compression rates via pretrained VAE, we observe that real-world video content exhibits substantial temporal non-uniformity, with high-motion segments containing more information than static scenes. Based on this insight, DLFR-VAE dynamically adjusts the latent frame rate according to the content complexity. Specifically, DLFR-VAE comprises two core innovations: (1) A Dynamic Latent Frame Rate Scheduler that partitions videos into temporal chunks and adaptively determines optimal frame rates based on information-theoretic content complexity, and (2) A training-free adaptation mechanism that transforms pretrained VAE architectures into a dynamic VAE that can process features with variable frame rates. Our simple but effective DLFR-VAE can function as a plug-and-play module, seamlessly integrating with existing video generation models and accelerating the video generation process.

### From Open-Vocabulary to Vocabulary-Free Semantic Segmentation 
[[arxiv](https://arxiv.org/abs/2502.11891)] [[cool](https://papers.cool/arxiv/2502.11891)] [[pdf](https://arxiv.org/pdf/2502.11891)]
> **Authors**: Klara Reichard,Giulia Rizzoli,Stefano Gasperini,Lukas Hoyer,Pietro Zanuttigh,Nassir Navab,Federico Tombari
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Submitted to: Pattern Recognition Letters, Klara Reichard and Giulia Rizzoli equally contributed to this work
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Open-vocabulary semantic segmentation enables models to identify novel object categories beyond their training data. While this flexibility represents a significant advancement, current approaches still rely on manually specified class names as input, creating an inherent bottleneck in real-world applications. This work proposes a Vocabulary-Free Semantic Segmentation pipeline, eliminating the need for predefined class vocabularies. Specifically, we address the chicken-and-egg problem where users need knowledge of all potential objects within a scene to identify them, yet the purpose of segmentation is often to discover these objects. The proposed approach leverages Vision-Language Models to automatically recognize objects and generate appropriate class names, aiming to solve the challenge of class specification and naming quality. Through extensive experiments on several public datasets, we highlight the crucial role of the text encoder in model performance, particularly when the image text classes are paired with generated descriptions. Despite the challenges introduced by the sensitivity of the segmentation text encoder to false negatives within the class tagging process, which adds complexity to the task, we demonstrate that our fully automated pipeline significantly enhances vocabulary-free segmentation accuracy across diverse real-world scenarios.

### Defining and Evaluating Visual Language Models' Basic Spatial Abilities: A Perspective from Psychometrics 
[[arxiv](https://arxiv.org/abs/2502.11859)] [[cool](https://papers.cool/arxiv/2502.11859)] [[pdf](https://arxiv.org/pdf/2502.11859)]
> **Authors**: Wenrui Xu,Dalin Lyu,Weihang Wang,Jie Feng,Chen Gao,Yong Li
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算语言学
- **Abstract**: The Theory of Multiple Intelligences underscores the hierarchical nature of cognitive capabilities. To advance Spatial Artificial Intelligence, we pioneer a psychometric framework defining five Basic Spatial Abilities (BSAs) in Visual Language Models (VLMs): Spatial Perception, Spatial Relation, Spatial Orientation, Mental Rotation, and Spatial Visualization. Benchmarking 13 mainstream VLMs through nine validated psychometric experiments reveals significant gaps versus humans (average score 24.95 vs. 68.38), with three key findings: 1) VLMs mirror human hierarchies (strongest in 2D orientation, weakest in 3D rotation) with independent BSAs (Pearson's r<0.4); 2) Smaller models such as Qwen2-VL-7B surpass larger counterparts, with Qwen leading (30.82) and InternVL2 lagging (19.6); 3) Interventions like chain-of-thought (0.100 accuracy gain) and 5-shot training (0.259 improvement) show limits from architectural constraints. Identified barriers include weak geometry encoding and missing dynamic simulation. By linking psychometric BSAs to VLM capabilities, we provide a diagnostic toolkit for spatial intelligence evaluation, methodological foundations for embodied AI development, and a cognitive science-informed roadmap for achieving human-like spatial intelligence.

### Intuitive physics understanding emerges from self-supervised pretraining on natural videos 
[[arxiv](https://arxiv.org/abs/2502.11831)] [[cool](https://papers.cool/arxiv/2502.11831)] [[pdf](https://arxiv.org/pdf/2502.11831)]
> **Authors**: Quentin Garrido,Nicolas Ballas,Mahmoud Assran,Adrien Bardes,Laurent Najman,Michael Rabbat,Emmanuel Dupoux,Yann LeCun
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 24 pages,14 figures, 5 tables
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: We investigate the emergence of intuitive physics understanding in general-purpose deep neural network models trained to predict masked regions in natural videos. Leveraging the violation-of-expectation framework, we find that video prediction models trained to predict outcomes in a learned representation space demonstrate an understanding of various intuitive physics properties, such as object permanence and shape consistency. In contrast, video prediction in pixel space and multimodal large language models, which reason through text, achieve performance closer to chance. Our comparisons of these architectures reveal that jointly learning an abstract representation space while predicting missing parts of sensory input, akin to predictive coding, is sufficient to acquire an understanding of intuitive physics, and that even models trained on one week of unique video achieve above chance performance. This challenges the idea that core knowledge -- a set of innate systems to help understand the world -- needs to be hardwired to develop an understanding of intuitive physics.

### Revealing Bias Formation in Deep Neural Networks Through the Geometric Mechanisms of Human Visual Decoupling 
[[arxiv](https://arxiv.org/abs/2502.11809)] [[cool](https://papers.cool/arxiv/2502.11809)] [[pdf](https://arxiv.org/pdf/2502.11809)]
> **Authors**: Yanbiao Ma,Bowei Liu,Wei Dai,Jiayi Chen,Shuo Li
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Deep neural networks (DNNs) often exhibit biases toward certain categories during object recognition, even under balanced training data conditions. The intrinsic mechanisms underlying these biases remain unclear. Inspired by the human visual system, which decouples object manifolds through hierarchical processing to achieve object recognition, we propose a geometric analysis framework linking the geometric complexity of class-specific perceptual manifolds in DNNs to model bias. Our findings reveal that differences in geometric complexity can lead to varying recognition capabilities across categories, introducing biases. To support this analysis, we present the Perceptual-Manifold-Geometry library, designed for calculating the geometric properties of perceptual manifolds.

### 3D Gaussian Inpainting with Depth-Guided Cross-View Consistency 
[[arxiv](https://arxiv.org/abs/2502.11801)] [[cool](https://papers.cool/arxiv/2502.11801)] [[pdf](https://arxiv.org/pdf/2502.11801)]
> **Authors**: Sheng-Yu Huang,Zi-Ting Chou,Yu-Chiang Frank Wang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: When performing 3D inpainting using novel-view rendering methods like Neural Radiance Field (NeRF) or 3D Gaussian Splatting (3DGS), how to achieve texture and geometry consistency across camera views has been a challenge. In this paper, we propose a framework of 3D Gaussian Inpainting with Depth-Guided Cross-View Consistency (3DGIC) for cross-view consistent 3D inpainting. Guided by the rendered depth information from each training view, our 3DGIC exploits background pixels visible across different views for updating the inpainting mask, allowing us to refine the 3DGS for inpainting purposes.Through extensive experiments on benchmark datasets, we confirm that our 3DGIC outperforms current state-of-the-art 3D inpainting methods quantitatively and qualitatively.

### Deep Neural Networks for Accurate Depth Estimation with Latent Space Features 
[[arxiv](https://arxiv.org/abs/2502.11777)] [[cool](https://papers.cool/arxiv/2502.11777)] [[pdf](https://arxiv.org/pdf/2502.11777)]
> **Authors**: Siddiqui Muhammad Yasir,Hyunsik Ahn
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: ef:Yasir, S.M.; Ahn, H.DeepNeuralNetworks for Accurate Depth Estimation with Latent Space Features. Biomimetics 2024, 9, 747
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Depth estimation plays a pivotal role in advancing human-robot interactions, especially in indoor environments where accurate 3D scene reconstruction is essential for tasks like navigation and object handling. Monocular depth estimation, which relies on a single RGB camera, offers a more affordable solution compared to traditional methods that use stereo cameras or LiDAR. However, despite recent progress, many monocular approaches struggle with accurately defining depth boundaries, leading to less precise reconstructions. In response to these challenges, this study introduces a novel depth estimation framework that leverages latent space features within a deep convolutional neural network to enhance the precision of monocular depth maps. The proposed model features dual encoder-decoder architecture, enabling both color-to-depth and depth-to-depth transformations. This structure allows for refined depth estimation through latent space encoding. To further improve the accuracy of depth boundaries and local features, a new loss function is introduced. This function combines latent loss with gradient loss, helping the model maintain the integrity of depth boundaries. The framework is thoroughly tested using the NYU Depth V2 dataset, where it sets a new benchmark, particularly excelling in complex indoor scenarios. The results clearly show that this approach effectively reduces depth ambiguities and blurring, making it a promising solution for applications in human-robot interaction and 3D scene reconstruction.

### video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model 
[[arxiv](https://arxiv.org/abs/2502.11775)] [[cool](https://papers.cool/arxiv/2502.11775)] [[pdf](https://arxiv.org/pdf/2502.11775)]
> **Authors**: Guangzhi Sun,Yudong Yang,Jimin Zhuang,Changli Tang,Yixuan Li,Wei Li,Zejun MA,Chao Zhang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: While recent advancements in reasoning optimization have significantly enhanced the capabilities of large language models (LLMs), existing efforts to improve reasoning have been limited to solving mathematical problems and focusing on visual graphical inputs, neglecting broader applications in general video understanding.This paper proposes video-SALMONN-o1, the first open-source reasoning-enhanced audio-visual LLM designed for general video understanding tasks. To enhance its reasoning abilities, we develop a reasoning-intensive dataset featuring challenging audio-visual questions with step-by-step solutions. We also propose process direct preference optimization (pDPO), which leverages contrastive step selection to achieve efficient step-level reward modelling tailored for multimodal inputs. Additionally, we introduce RivaBench, the first reasoning-intensive video understanding benchmark, featuring over 4,000 high-quality, expert-curated question-answer pairs across scenarios such as standup comedy, academic presentations, and synthetic video detection. video-SALMONN-o1 achieves 3-8% accuracy improvements over the LLaVA-OneVision baseline across different video reasoning benchmarks. Besides, pDPO achieves 6-8% improvements compared to the supervised fine-tuning model on RivaBench. Enhanced reasoning enables video-SALMONN-o1 zero-shot synthetic video detection capabilities.

### Lightweight Deepfake Detection Based on Multi-Feature Fusion 
[[arxiv](https://arxiv.org/abs/2502.11763)] [[cool](https://papers.cool/arxiv/2502.11763)] [[pdf](https://arxiv.org/pdf/2502.11763)]
> **Authors**: Siddiqui Muhammad Yasir,Hyun Kim
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: ef:Yasir, S.M.; Kim, H. Lightweight Deepfake Detection Based on Multi-Feature Fusion. Appl. Sci. 2025, 15, 1954
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Deepfake technology utilizes deep learning based face manipulation techniques to seamlessly replace faces in videos creating highly realistic but artificially generated content. Although this technology has beneficial applications in media and entertainment misuse of its capabilities may lead to serious risks including identity theft cyberbullying and false information. The integration of DL with visual cognition has resulted in important technological improvements particularly in addressing privacy risks caused by artificially generated deepfake images on digital media platforms. In this study we propose an efficient and lightweight method for detecting deepfake images and videos making it suitable for devices with limited computational resources. In order to reduce the computational burden usually associated with DL models our method integrates machine learning classifiers in combination with keyframing approaches and texture analysis. Moreover the features extracted with a histogram of oriented gradients (HOG) local binary pattern (LBP) and KAZE bands were integrated to evaluate using random forest extreme gradient boosting extra trees and support vector classifier algorithms. Our findings show a feature-level fusion of HOG LBP and KAZE features improves accuracy to 92% and 96% on FaceForensics++ and Celeb-DFv2 respectively.

### Language Models Can See Better: Visual Contrastive Decoding For LLM Multimodal Reasoning 
[[arxiv](https://arxiv.org/abs/2502.11751)] [[cool](https://papers.cool/arxiv/2502.11751)] [[pdf](https://arxiv.org/pdf/2502.11751)]
> **Authors**: Yuqi Pang,Bowen Yang,Haoqin Tu,Yun Cao,Zeyu Zhang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Accepted to ICASSP 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Although Large Language Models (LLMs) excel in reasoning and generation for language tasks, they are not specifically designed for multimodal challenges. Training Multimodal Large Language Models (MLLMs), however, is resource-intensive and constrained by various training limitations. In this paper, we propose the Modular-based Visual Contrastive Decoding (MVCD) framework to move this obstacle. Our framework leverages LLMs' In-Context Learning (ICL) capability and the proposed visual contrastive-example decoding (CED), specifically tailored for this framework, without requiring any additional training. By converting visual signals into text and focusing on contrastive output distributions during decoding, we can highlight the new information introduced by contextual examples, explore their connections, and avoid over-reliance on prior encoded knowledge. MVCD enhances LLMs' visual perception to make it see and reason over the input visuals. To demonstrate MVCD's effectiveness, we conduct experiments with four LLMs across five question answering datasets. Our results not only show consistent improvement in model accuracy but well explain the effective components inside our decoding strategy. Our code will be available at https://github.com/Pbhgit/MVCD.

### JotlasNet: Joint Tensor Low-Rank and Attention-based Sparse Unrolling Network for Accelerating Dynamic MRI 
[[arxiv](https://arxiv.org/abs/2502.11749)] [[cool](https://papers.cool/arxiv/2502.11749)] [[pdf](https://arxiv.org/pdf/2502.11749)]
> **Authors**: Yinghao Zhang,Haiyan Gui,Ningdi Yang,Yue Hu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 13 pages, 7 figures, accepted by Magnetic Resonance Imaging
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Joint low-rank and sparse unrolling networks have shown superior performance in dynamic MRI reconstruction. However, existing works mainly utilized matrix low-rank priors, neglecting the tensor characteristics of dynamic MRI images, and only a global threshold is applied for the sparse constraint to the multi-channel data, limiting the flexibility of the network. Additionally, most of them have inherently complex network structure, with intricate interactions among variables. In this paper, we propose a novel deep unrolling network, JotlasNet, for dynamic MRI reconstruction by jointly utilizing tensor low-rank and attention-based sparse priors. Specifically, we utilize tensor low-rank prior to exploit the structural correlations in high-dimensional data. Convolutional neural networks are used to adaptively learn the low-rank and sparse transform domains. A novel attention-based soft thresholding operator is proposed to assign a unique learnable threshold to each channel of the data in the CNN-learned sparse domain. The network is unrolled from the elaborately designed composite splitting algorithm and thus features a simple yet efficient parallel structure. Extensive experiments on two datasets (OCMR, CMRxRecon) demonstrate the superior performance of JotlasNet in dynamic MRI reconstruction.

### ILIAS: Instance-Level Image retrieval At Scale 
[[arxiv](https://arxiv.org/abs/2502.11748)] [[cool](https://papers.cool/arxiv/2502.11748)] [[pdf](https://arxiv.org/pdf/2502.11748)]
> **Authors**: Giorgos Kordopatis-Zilos,Vladan Stojnić,Anna Manko,Pavel Šuma,Nikolaos-Antonios Ypsilantis,Nikos Efthymiadis,Zakaria Laskar,Jiří Matas,Ondřej Chum,Giorgos Tolias
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: This work introduces ILIAS, a new test dataset for Instance-Level Image retrieval At Scale. It is designed to evaluate the ability of current and future foundation models and retrieval techniques to recognize particular objects. The key benefits over existing datasets include large scale, domain diversity, accurate ground truth, and a performance that is far from saturated. ILIAS includes query and positive images for 1,000 object instances, manually collected to capture challenging conditions and diverse domains. Large-scale retrieval is conducted against 100 million distractor images from YFCC100M. To avoid false negatives without extra annotation effort, we include only query objects confirmed to have emerged after 2014, i.e. the compilation date of YFCC100M. An extensive benchmarking is performed with the following observations: i) models fine-tuned on specific domains, such as landmarks or products, excel in that domain but fail on ILIAS ii) learning a linear adaptation layer using multi-domain class supervision results in performance improvements, especially for vision-language models iii) local descriptors in retrieval re-ranking are still a key ingredient, especially in the presence of severe background clutter iv) the text-to-image performance of the vision-language foundation models is surprisingly close to the corresponding image-to-image case. website: https://vrg.fel.cvut.cz/ilias/

### Adversarially Robust CLIP Models Can Induce Better (Robust) Perceptual Metrics 
[[arxiv](https://arxiv.org/abs/2502.11725)] [[cool](https://papers.cool/arxiv/2502.11725)] [[pdf](https://arxiv.org/pdf/2502.11725)]
> **Authors**: Francesco Croce,Christian Schlarmann,Naman Deep Singh,Matthias Hein
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: This work has been accepted for publication in the IEEE Conference on Secure and TrustworthyMachineLearning(SaTML). The final version will be available on IEEE Xplore
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Measuring perceptual similarity is a key tool in computer vision. In recent years perceptual metrics based on features extracted from neural networks with large and diverse training sets, e.g. CLIP, have become popular. At the same time, the metrics extracted from features of neural networks are not adversarially robust. In this paper we show that adversarially robust CLIP models, called R-CLIP$_\textrm{F}$, obtained by unsupervised adversarial fine-tuning induce a better and adversarially robust perceptual metric that outperforms existing metrics in a zero-shot setting, and further matches the performance of state-of-the-art metrics while being robust after fine-tuning. Moreover, our perceptual metric achieves strong performance on related tasks such as robust image-to-image retrieval, which becomes especially relevant when applied to "Not Safe for Work" (NSFW) content detection and dataset filtering. While standard perceptual metrics can be easily attacked by a small perturbation completely degrading NSFW detection, our robust perceptual metric maintains high accuracy under an attack while having similar performance for unperturbed images. Finally, perceptual metrics induced by robust CLIP models have higher interpretability: feature inversion can show which images are considered similar, while text inversion can find what images are associated to a given prompt. This also allows us to visualize the very rich visual concepts learned by a CLIP model, including memorized persons, paintings and complex queries.

### Incomplete Modality Disentangled Representation for Ophthalmic Disease Grading and Diagnosis 
[[arxiv](https://arxiv.org/abs/2502.11724)] [[cool](https://papers.cool/arxiv/2502.11724)] [[pdf](https://arxiv.org/pdf/2502.11724)]
> **Authors**: Chengzhi Liu,Zile Huang,Zhe Chen,Feilong Tang,Yu Tian,Zhongxing Xu,Zihong Luo,Yalin Zheng,Yanda Meng
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 7 Pages, 6 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Ophthalmologists typically require multimodal data sources to improve diagnostic accuracy in clinical decisions. However, due to medical device shortages, low-quality data and data privacy concerns, missing data modalities are common in real-world scenarios. Existing deep learning methods tend to address it by learning an implicit latent subspace representation for different modality combinations. We identify two significant limitations of these methods: (1) implicit representation constraints that hinder the model's ability to capture modality-specific information and (2) modality heterogeneity, causing distribution gaps and redundancy in feature representations. To address these, we propose an Incomplete Modality Disentangled Representation (IMDR) strategy, which disentangles features into explicit independent modal-common and modal-specific features by guidance of mutual information, distilling informative knowledge and enabling it to reconstruct valuable missing semantics and produce robust multimodal representations. Furthermore, we introduce a joint proxy learning module that assists IMDR in eliminating intra-modality redundancy by exploiting the extracted proxies from each class. Experiments on four ophthalmology multimodal datasets demonstrate that the proposed IMDR outperforms the state-of-the-art methods significantly.

### MaskGWM: A Generalizable Driving World Model with Video Mask Reconstruction 
[[arxiv](https://arxiv.org/abs/2502.11663)] [[cool](https://papers.cool/arxiv/2502.11663)] [[pdf](https://arxiv.org/pdf/2502.11663)]
> **Authors**: Jingcheng Ni,Yuxin Guo,Yichen Liu,Rui Chen,Lewei Lu,Zehuan Wu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: World models that forecast environmental changes from actions are vital for autonomous driving models with strong generalization. The prevailing driving world model mainly build on video prediction model. Although these models can produce high-fidelity video sequences with advanced diffusion-based generator, they are constrained by their predictive duration and overall generalization capabilities. In this paper, we explore to solve this problem by combining generation loss with MAE-style feature-level context learning. In particular, we instantiate this target with three key design: (1) A more scalable Diffusion Transformer (DiT) structure trained with extra mask construction task. (2) we devise diffusion-related mask tokens to deal with the fuzzy relations between mask reconstruction and generative diffusion process. (3) we extend mask construction task to spatial-temporal domain by utilizing row-wise mask for shifted self-attention rather than masked self-attention in MAE. Then, we adopt a row-wise cross-view module to align with this mask design. Based on above improvement, we propose MaskGWM: a Generalizable driving World Model embodied with Video Mask reconstruction. Our model contains two variants: MaskGWM-long, focusing on long-horizon prediction, and MaskGWM-mview, dedicated to multi-view generation. Comprehensive experiments on standard benchmarks validate the effectiveness of the proposed method, which contain normal validation of Nuscene dataset, long-horizon rollout of OpenDV-2K dataset and zero-shot validation of Waymo dataset. Quantitative metrics on these datasets show our method notably improving state-of-the-art driving world model.

### Object-Centric Image to Video Generation with Language Guidance 
[[arxiv](https://arxiv.org/abs/2502.11655)] [[cool](https://papers.cool/arxiv/2502.11655)] [[pdf](https://arxiv.org/pdf/2502.11655)]
> **Authors**: Angel Villar-Corrales,Gjergj Plepi,Sven Behnke
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Accurate and flexible world models are crucial for autonomous systems to understand their environment and predict future events. Object-centric models, with structured latent spaces, have shown promise in modeling object dynamics and interactions, but often face challenges in scaling to complex datasets and incorporating external guidance, limiting their applicability in robotics. To address these limitations, we propose TextOCVP, an object-centric model for image-to-video generation guided by textual descriptions. TextOCVP parses an observed scene into object representations, called slots, and utilizes a text-conditioned transformer predictor to forecast future object states and video frames. Our approach jointly models object dynamics and interactions while incorporating textual guidance, thus leading to accurate and controllable predictions. Our method's structured latent space offers enhanced control over the prediction process, outperforming several image-to-video generative baselines. Additionally, we demonstrate that structured object-centric representations provide superior controllability and interpretability, facilitating the modeling of object dynamics and enabling more precise and understandable predictions. Videos and code are available at https://play-slot.github.io/TextOCVP/.

### MMXU: A Multi-Modal and Multi-X-ray Understanding Dataset for Disease Progression 
[[arxiv](https://arxiv.org/abs/2502.11651)] [[cool](https://papers.cool/arxiv/2502.11651)] [[pdf](https://arxiv.org/pdf/2502.11651)]
> **Authors**: Linjie Mu,Zhongzhen Huang,Shengqian Qin,Yakun Zhu,Shaoting Zhang,Xiaofan Zhang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Large vision-language models (LVLMs) have shown great promise in medical applications, particularly in visual question answering (MedVQA) and diagnosis from medical images. However, existing datasets and models often fail to consider critical aspects of medical diagnostics, such as the integration of historical records and the analysis of disease progression over time. In this paper, we introduce MMXU (Multimodal and MultiX-ray Understanding), a novel dataset for MedVQA that focuses on identifying changes in specific regions between two patient visits. Unlike previous datasets that primarily address single-image questions, MMXU enables multi-image questions, incorporating both current and historical patient data. We demonstrate the limitations of current LVLMs in identifying disease progression on MMXU-\textit{test}, even those that perform well on traditional benchmarks. To address this, we propose a MedRecord-Augmented Generation (MAG) approach, incorporating both global and regional historical records. Our experiments show that integrating historical records significantly enhances diagnostic accuracy by at least 20\%, bridging the gap between current LVLMs and human expert performance. Additionally, we fine-tune models with MAG on MMXU-\textit{dev}, which demonstrates notable improvements. We hope this work could illuminate the avenue of advancing the use of LVLMs in medical diagnostics by emphasizing the importance of historical context in interpreting medical images. Our dataset is released at \href{https://github.com/linjiemu/MMXU}{https://github.com/linjiemu/MMXU}.

### Enhancing Out-of-Distribution Detection in Medical Imaging with Normalizing Flows 
[[arxiv](https://arxiv.org/abs/2502.11638)] [[cool](https://papers.cool/arxiv/2502.11638)] [[pdf](https://arxiv.org/pdf/2502.11638)]
> **Authors**: Dariush Lotfi,Mohammad-Ali Nikouei Mahani,Mohamad Koohi-Moghadam,Kyongtae Ty Bae
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Out-of-distribution (OOD) detection is crucial in AI-driven medical imaging to ensure reliability and safety by identifying inputs outside a model's training distribution. Existing methods often require retraining or modifications to pre-trained models, which is impractical for clinical applications. This study introduces a post-hoc normalizing flow-based approach that seamlessly integrates with pre-trained models. By leveraging normalizing flows, it estimates the likelihood of feature vectors extracted from pre-trained models, capturing semantically meaningful representations without relying on pixel-level statistics. The method was evaluated using the MedMNIST benchmark and a newly curated MedOOD dataset simulating clinically relevant distributional shifts. Performance was measured using standard OOD detection metrics (e.g., AUROC, FPR@95, AUPR_IN, AUPR_OUT), with statistical analyses comparing it against ten baseline methods. On MedMNIST, the proposed model achieved an AUROC of 93.80%, outperforming state-of-the-art methods. On MedOOD, it achieved an AUROC of 84.61%, demonstrating superior performance against other methods. Its post-hoc nature ensures compatibility with existing clinical workflows, addressing the limitations of previous approaches. The model and code to build OOD datasets are available at https://github.com/dlotfi/MedOODFlow.

### Real-time Neural Rendering of LiDAR Point Clouds 
[[arxiv](https://arxiv.org/abs/2502.11618)] [[cool](https://papers.cool/arxiv/2502.11618)] [[pdf](https://arxiv.org/pdf/2502.11618)]
> **Authors**: Joni Vanherck,Brent Zoomers,Tom Mertens,Lode Jorissen,Nick Michiels
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 6 pages, 3 figures, 1 table,
- **标题**: None
- **领域**: 计算机视觉和模式识别,图形
- **Abstract**: Static LiDAR scanners produce accurate, dense, colored point clouds, but often contain obtrusive artifacts which makes them ill-suited for direct display. We propose an efficient method to render photorealistic images of such scans without any expensive preprocessing or training of a scene-specific model. A naive projection of the point cloud to the output view using 1x1 pixels is fast and retains the available detail, but also results in unintelligible renderings as background points leak in between the foreground pixels. The key insight is that these projections can be transformed into a realistic result using a deep convolutional model in the form of a U-Net, and a depth-based heuristic that prefilters the data. The U-Net also handles LiDAR-specific problems such as missing parts due to occlusion, color inconsistencies and varying point densities. We also describe a method to generate synthetic training data to deal with imperfectly-aligned ground truth images. Our method achieves real-time rendering rates using an off-the-shelf GPU and outperforms the state-of-the-art in both speed and quality.

### iMOVE: Instance-Motion-Aware Video Understanding 
[[arxiv](https://arxiv.org/abs/2502.11594)] [[cool](https://papers.cool/arxiv/2502.11594)] [[pdf](https://arxiv.org/pdf/2502.11594)]
> **Authors**: Jiaze Li,Yaya Shi,Zongyang Ma,Haoran Xu,Feng Cheng,Huihui Xiao,Ruiwen Kang,Fan Yang,Tingting Gao,Di Zhang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Enhancing the fine-grained instance spatiotemporal motion perception capabilities of Video Large Language Models is crucial for improving their temporal and general video understanding. However, current models struggle to perceive detailed and complex instance motions. To address these challenges, we have made improvements from both data and model perspectives. In terms of data, we have meticulously curated iMOVE-IT, the first large-scale instance-motion-aware video instruction-tuning dataset. This dataset is enriched with comprehensive instance motion annotations and spatiotemporal mutual-supervision tasks, providing extensive training for the model's instance-motion-awareness. Building on this foundation, we introduce iMOVE, an instance-motion-aware video foundation model that utilizes Event-aware Spatiotemporal Efficient Modeling to retain informative instance spatiotemporal motion details while maintaining computational efficiency. It also incorporates Relative Spatiotemporal Position Tokens to ensure awareness of instance spatiotemporal positions. Evaluations indicate that iMOVE excels not only in video temporal understanding and general video understanding but also demonstrates significant advantages in long-term video understanding.

### Syllables to Scenes: Literary-Guided Free-Viewpoint 3D Scene Synthesis from Japanese Haiku 
[[arxiv](https://arxiv.org/abs/2502.11586)] [[cool](https://papers.cool/arxiv/2502.11586)] [[pdf](https://arxiv.org/pdf/2502.11586)]
> **Authors**: Chunan Yu,Yidong Han,Chaotao Ding,Ying Zang,Lanyun Zhu,Xinhao Chen,Zejian Li,Renjun Xu,Tianrun Chen
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 16 pages, 11 figures, submitted to IJCAI
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: In the era of the metaverse, where immersive technologies redefine human experiences, translating abstract literary concepts into navigable 3D environments presents a fundamental challenge in preserving semantic and emotional fidelity. This research introduces HaikuVerse, a novel framework for transforming poetic abstraction into spatial representation, with Japanese Haiku serving as an ideal test case due to its sophisticated encapsulation of profound emotions and imagery within minimal text. While existing text-to-3D methods struggle with nuanced interpretations, we present a literary-guided approach that synergizes traditional poetry analysis with advanced generative technologies. Our framework centers on two key innovations: (1) Hierarchical Literary-Criticism Theory Grounded Parsing (H-LCTGP), which captures both explicit imagery and implicit emotional resonance through structured semantic decomposition, and (2) Progressive Dimensional Synthesis (PDS), a multi-stage pipeline that systematically transforms poetic elements into coherent 3D scenes through sequential diffusion processes, geometric optimization, and real-time enhancement. Extensive experiments demonstrate that HaikuVerse significantly outperforms conventional text-to-3D approaches in both literary fidelity and visual quality, establishing a new paradigm for preserving cultural heritage in immersive digital spaces. Project website at: https://syllables-to-scenes.github.io/

### Variable-frame CNNLSTM for Breast Nodule Classification using Ultrasound Videos 
[[arxiv](https://arxiv.org/abs/2502.11481)] [[cool](https://papers.cool/arxiv/2502.11481)] [[pdf](https://arxiv.org/pdf/2502.11481)]
> **Authors**: Xiangxiang Cui,Zhongyu Li,Xiayue Fan,Peng Huang,Ying Wang,Meng Yang,Shi Chang,Jihua Zhu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: The intersection of medical imaging and artificial intelligence has become an important research direction in intelligent medical treatment, particularly in the analysis of medical images using deep learning for clinical diagnosis. Despite the advances, existing keyframe classification methods lack extraction of time series features, while ultrasonic video classification based on three-dimensional convolution requires uniform frame numbers across patients, resulting in poor feature extraction efficiency and model classification performance. This study proposes a novel video classification method based on CNN and LSTM, introducing NLP's long and short sentence processing scheme into video classification for the first time. The method reduces CNN-extracted image features to 1x512 dimension, followed by sorting and compressing feature vectors for LSTM training. Specifically, feature vectors are sorted by patient video frame numbers and populated with padding value 0 to form variable batches, with invalid padding values compressed before LSTM training to conserve computing resources. Experimental results demonstrate that our variable-frame CNNLSTM method outperforms other approaches across all metrics, showing improvements of 3-6% in F1 score and 1.5% in specificity compared to keyframe methods. The variable-frame CNNLSTM also achieves better accuracy and precision than equal-frame CNNLSTM. These findings validate the effectiveness of our approach in classifying variable-frame ultrasound videos and suggest potential applications in other medical imaging modalities.

### Learning to Sample Effective and Diverse Prompts for Text-to-Image Generation 
[[arxiv](https://arxiv.org/abs/2502.11477)] [[cool](https://papers.cool/arxiv/2502.11477)] [[pdf](https://arxiv.org/pdf/2502.11477)]
> **Authors**: Taeyoung Yun,Dinghuai Zhang,Jinkyoo Park,Ling Pan
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 18 pages, 14 figures, 6 tables
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Recent advances in text-to-image diffusion models have achieved impressive image generation capabilities. However, it remains challenging to control the generation process with desired properties (e.g., aesthetic quality, user intention), which can be expressed as black-box reward functions. In this paper, we focus on prompt adaptation, which refines the original prompt into model-preferred prompts to generate desired images. While prior work uses reinforcement learning (RL) to optimize prompts, we observe that applying RL often results in generating similar postfixes and deterministic behaviors. To this end, we introduce \textbf{P}rompt \textbf{A}daptation with \textbf{G}FlowNets (\textbf{PAG}), a novel approach that frames prompt adaptation as a probabilistic inference problem. Our key insight is that leveraging Generative Flow Networks (GFlowNets) allows us to shift from reward maximization to sampling from an unnormalized density function, enabling both high-quality and diverse prompt generation. However, we identify that a naive application of GFlowNets suffers from mode collapse and uncovers a previously overlooked phenomenon: the progressive loss of neural plasticity in the model, which is compounded by inefficient credit assignment in sequential prompt generation. To address this critical challenge, we develop a systematic approach in PAG with flow reactivation, reward-prioritized sampling, and reward decomposition for prompt adaptation. Extensive experiments validate that PAG successfully learns to sample effective and diverse prompts for text-to-image generation. We also show that PAG exhibits strong robustness across various reward functions and transferability to different text-to-image models.

## 计算机与社会(cs.CY:Computers and Society)

### Could AI Leapfrog the Web? Evidence from Teachers in Sierra Leone 
[[arxiv](https://arxiv.org/abs/2502.12397)] [[cool](https://papers.cool/arxiv/2502.12397)] [[pdf](https://arxiv.org/pdf/2502.12397)]
> **Authors**: Daniel Björkegren,Jun Ho Choi,Divya Budihal,Dominic Sobhani,Oliver Garrod,Paul Atherton
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,人工智能,人机交互,普通经济学
- **Abstract**: Access to digital information is a driver of economic development. But although 85% of sub-Saharan Africa's population is covered by mobile broadband signal, only 37% use the internet, and those who do seldom use the web. We investigate whether AI can bridge this gap by analyzing how 469 teachers use an AI chatbot in Sierra Leone. The chatbot, accessible via a common messaging app, is compared against traditional web search. Teachers use AI more frequently than web search for teaching assistance. Data cost is the most frequently cited reason for low internet usage across Africa. The average web search result consumes 3,107 times more data than an AI response, making AI 87% less expensive than web search. Additionally, only 2% of results for corresponding web searches contain content from Sierra Leone. In blinded evaluations, an independent sample of teachers rate AI responses as more relevant, helpful, and correct than web search results. These findings suggest that AI-driven solutions can cost-effectively bridge information gaps in low-connectivity regions.

### Human-centered explanation does not fit all: The interplay of sociotechnical, cognitive, and individual factors in the effect AI explanations in algorithmic decision-making 
[[arxiv](https://arxiv.org/abs/2502.12354)] [[cool](https://papers.cool/arxiv/2502.12354)] [[pdf](https://arxiv.org/pdf/2502.12354)]
> **Authors**: Yongsu Ahn,Yu-Run Lin,Malihe Alikhani,Eunjeong Cheon
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,人工智能,人机交互
- **Abstract**: Recent XAI studies have investigated what constitutes a \textit{good} explanation in AI-assisted decision-making. Despite the widely accepted human-friendly properties of explanations, such as contrastive and selective, existing studies have yielded inconsistent findings. To address these gaps, our study focuses on the cognitive dimensions of explanation evaluation, by evaluating six explanations with different contrastive strategies and information selectivity and scrutinizing factors behind their valuation process. Our analysis results find that contrastive explanations are not the most preferable or understandable in general; Rather, different contrastive and selective explanations were appreciated to a different extent based on who they are, when, how, and what to explain -- with different level of cognitive load and engagement and sociotechnical contexts. Given these findings, we call for a nuanced view of explanation strategies, with implications for designing AI interfaces to accommodate individual and contextual differences in AI-assisted decision-making.

### Exploring LLM-based Student Simulation for Metacognitive Cultivation 
[[arxiv](https://arxiv.org/abs/2502.11678)] [[cool](https://papers.cool/arxiv/2502.11678)] [[pdf](https://arxiv.org/pdf/2502.11678)]
> **Authors**: Haoxuan Li,Jifan Yu,Xin Cong,Yang Dang,Yisi Zhan,Huiqin Liu,Zhiyuan Liu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 12 pages, 8 figures
- **标题**: None
- **领域**: 计算机与社会,计算语言学
- **Abstract**: Metacognitive education plays a crucial role in cultivating students' self-regulation and reflective thinking, providing essential support for those with learning difficulties through academic advising. Simulating students with insufficient learning capabilities using large language models offers a promising approach to refining pedagogical methods without ethical concerns. However, existing simulations often fail to authentically represent students' learning struggles and face challenges in evaluation due to the lack of reliable metrics and ethical constraints in data collection. To address these issues, we propose a pipeline for automatically generating and filtering high-quality simulated student agents. Our approach leverages a two-round automated scoring system validated by human experts and employs a score propagation module to obtain more consistent scores across the student graph. Experimental results demonstrate that our pipeline efficiently identifies high-quality student agents, and we discuss the traits that influence the simulation's effectiveness. By simulating students with varying degrees of learning difficulties, our work paves the way for broader applications in personalized learning and educational assessment.

## 数据库(cs.DB:Databases)

### SQL-o1: A Self-Reward Heuristic Dynamic Search Method for Text-to-SQL 
[[arxiv](https://arxiv.org/abs/2502.11741)] [[cool](https://papers.cool/arxiv/2502.11741)] [[pdf](https://arxiv.org/pdf/2502.11741)]
> **Authors**: Shuai Lyu,Haoran Luo,Zhonghong Ou,Yifan Zhu,Xiaoran Shang,Yang Qin,Meina Song
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 10 pages,4 figures
- **标题**: None
- **领域**: 数据库,人工智能
- **Abstract**: The Text-to-SQL(Text2SQL) task aims to convert natural language queries into executable SQL queries. Thanks to the application of large language models (LLMs), significant progress has been made in this field. However, challenges such as model scalability, limited generation space, and coherence issues in SQL generation still persist. To address these issues, we propose SQL-o1, a Self-Reward-based heuristic search method designed to enhance the reasoning ability of LLMs in SQL query generation. SQL-o1 combines Monte Carlo Tree Search (MCTS) for heuristic process-level search and constructs a Schema-Aware dataset to help the model better understand database schemas. Extensive experiments on the Bird and Spider datasets demonstrate that SQL-o1 improves execution accuracy by 10.8\% on the complex Bird dataset compared to the latest baseline methods, even outperforming GPT-4-based approaches. Additionally, SQL-o1 excels in few-shot learning scenarios and shows strong cross-model transferability. Our code is publicly available at:https://github.com/ShuaiLyu0110/SQL-o1.

## 分布式、并行和集群计算(cs.DC:Distributed, Parallel, and Cluster Computing)

### Connecting Large Language Model Agent to High Performance Computing Resource 
[[arxiv](https://arxiv.org/abs/2502.12280)] [[cool](https://papers.cool/arxiv/2502.12280)] [[pdf](https://arxiv.org/pdf/2502.12280)]
> **Authors**: Heng Ma,Alexander Brace,Carlo Siebenschuh,Greg Pauloski,Ian Foster,Arvind Ramanathan
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 7 pages, 4 figures
- **标题**: None
- **领域**: 分布式、并行和集群计算,人工智能
- **Abstract**: The Large Language Model agent workflow enables the LLM to invoke tool functions to increase the performance on specific scientific domain questions. To tackle large scale of scientific research, it requires access to computing resource and parallel computing setup. In this work, we implemented Parsl to the LangChain/LangGraph tool call setup, to bridge the gap between the LLM agent to the computing resource. Two tool call implementations were set up and tested on both local workstation and HPC environment on Polaris/ALCF. The first implementation with Parsl-enabled LangChain tool node queues the tool functions concurrently to the Parsl workers for parallel execution. The second configuration is implemented by converting the tool functions into Parsl ensemble functions, and is more suitable for large task on super computer environment. The LLM agent workflow was prompted to run molecular dynamics simulations, with different protein structure and simulation conditions. These results showed the LLM agent tools were managed and executed concurrently by Parsl on the available computing resource.

### Scalable and Cost-Efficient ML Inference: Parallel Batch Processing with Serverless Functions 
[[arxiv](https://arxiv.org/abs/2502.12017)] [[cool](https://papers.cool/arxiv/2502.12017)] [[pdf](https://arxiv.org/pdf/2502.12017)]
> **Authors**: Amine Barrak,Emna Ksontini
> **First submission**: 2025-01-30
> **First announcement**: 2025-02-18
> **comment**: ef:ICSOC 2024, 22nd International Conference on Service-Oriented Computing
- **标题**: None
- **领域**: 分布式、并行和集群计算,机器学习
- **Abstract**: As data-intensive applications grow, batch processing in limited-resource environments faces scalability and resource management challenges. Serverless computing offers a flexible alternative, enabling dynamic resource allocation and automatic scaling. This paper explores how serverless architectures can make large-scale ML inference tasks faster and cost-effective by decomposing monolithic processes into parallel functions. Through a case study on sentiment analysis using the DistilBERT model and the IMDb dataset, we demonstrate that serverless parallel processing can reduce execution time by over 95% compared to monolithic approaches, at the same cost.

### InTec: integrated things-edge computing: a framework for distributing machine learning pipelines in edge AI systems 
[[arxiv](https://arxiv.org/abs/2502.11644)] [[cool](https://papers.cool/arxiv/2502.11644)] [[pdf](https://arxiv.org/pdf/2502.11644)]
> **Authors**: Habib Larian,Faramarz Safi-Esfahani
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: For InTec framework implementation, see GitHub repository https://github.com/IDASLab/InTec_Framework
- **标题**: None
- **领域**: 分布式、并行和集群计算,人工智能
- **Abstract**: With the rapid expansion of the Internet of Things (IoT), sensors, smartphones, and wearables have become integral to daily life, powering smart applications in home automation, healthcare, and intelligent transportation. However, these advancements face significant challenges due to latency and bandwidth constraints imposed by traditional cloud based machine learning (ML) frameworks. The need for innovative solutions is evident as cloud computing struggles with increased latency and network congestion. Previous attempts to offload parts of the ML pipeline to edge and cloud layers have yet to fully resolve these issues, often worsening system response times and network congestion due to the computational limitations of edge devices. In response to these challenges, this study introduces the InTec (Integrated Things Edge Computing) framework, a groundbreaking innovation in IoT architecture. Unlike existing methods, InTec fully leverages the potential of a three tier architecture by strategically distributing ML tasks across the Things, Edge, and Cloud layers. This comprehensive approach enables real time data processing at the point of data generation, significantly reducing latency, optimizing network traffic, and enhancing system reliability. InTec effectiveness is validated through empirical evaluation using the MHEALTH dataset for human motion detection in smart homes, demonstrating notable improvements in key metrics: an 81.56 percent reduction in response time, a 10.92 percent decrease in network traffic, a 9.82 percent improvement in throughput, a 21.86 percent reduction in edge energy consumption, and a 25.83 percent reduction in cloud energy consumption. These advancements establish InTec as a new benchmark for scalable, responsive, and energy efficient IoT applications, demonstrating its potential to revolutionize how the ML pipeline is integrated into Edge AI (EI) systems.

## 计算机科学与博弈论(cs.GT:Computer Science and Game Theory)

### Deviation Ratings: A General, Clone-Invariant Rating Method 
[[arxiv](https://arxiv.org/abs/2502.11645)] [[cool](https://papers.cool/arxiv/2502.11645)] [[pdf](https://arxiv.org/pdf/2502.11645)]
> **Authors**: Luke Marris,Siqi Liu,Ian Gemp,Georgios Piliouras,Marc Lanctot
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 计算机科学与博弈论,计算语言学,多代理系统,其他统计数据
- **Abstract**: Many real-world multi-agent or multi-task evaluation scenarios can be naturally modelled as normal-form games due to inherent strategic (adversarial, cooperative, and mixed motive) interactions. These strategic interactions may be agentic (e.g. players trying to win), fundamental (e.g. cost vs quality), or complementary (e.g. niche finding and specialization). In such a formulation, it is the strategies (actions, policies, agents, models, tasks, prompts, etc.) that are rated. However, the rating problem is complicated by redundancy and complexity of N-player strategic interactions. Repeated or similar strategies can distort ratings for those that counter or complement them. Previous work proposed ``clone invariant'' ratings to handle such redundancies, but this was limited to two-player zero-sum (i.e. strictly competitive) interactions. This work introduces the first N-player general-sum clone invariant rating, called deviation ratings, based on coarse correlated equilibria. The rating is explored on several domains including LLMs evaluation.

## 人机交互(cs.HC:Human-Computer Interaction)

### Characterizing Photorealism and Artifacts in Diffusion Model-Generated Images 
[[arxiv](https://arxiv.org/abs/2502.11989)] [[cool](https://papers.cool/arxiv/2502.11989)] [[pdf](https://arxiv.org/pdf/2502.11989)]
> **Authors**: Negar Kamali,Karyn Nakamura,Aakriti Kumar,Angelos Chatzimparmpas,Jessica Hullman,Matthew Groh
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 26 pages, 24 Figures, Accepted by ACM CHI 2025
- **标题**: None
- **领域**: 人机交互,人工智能,计算机视觉和模式识别
- **Abstract**: Diffusion model-generated images can appear indistinguishable from authentic photographs, but these images often contain artifacts and implausibilities that reveal their AI-generated provenance. Given the challenge to public trust in media posed by photorealistic AI-generated images, we conducted a large-scale experiment measuring human detection accuracy on 450 diffusion-model generated images and 149 real images. Based on collecting 749,828 observations and 34,675 comments from 50,444 participants, we find that scene complexity of an image, artifact types within an image, display time of an image, and human curation of AI-generated images all play significant roles in how accurately people distinguish real from AI-generated images. Additionally, we propose a taxonomy characterizing artifacts often appearing in images generated by diffusion models. Our empirical observations and taxonomy offer nuanced insights into the capabilities and limitations of diffusion models to generate photorealistic images in 2024.

### From Text to Trust: Empowering AI-assisted Decision Making with Adaptive LLM-powered Analysis 
[[arxiv](https://arxiv.org/abs/2502.11919)] [[cool](https://papers.cool/arxiv/2502.11919)] [[pdf](https://arxiv.org/pdf/2502.11919)]
> **Authors**: Zhuoyan Li,Hangxiao Zhu,Zhuoran Lu,Ziang Xiao,Ming Yin
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: CHI 2025
- **标题**: None
- **领域**: 人机交互,计算语言学
- **Abstract**: AI-assisted decision making becomes increasingly prevalent, yet individuals often fail to utilize AI-based decision aids appropriately especially when the AI explanations are absent, potentially as they do not %understand reflect on AI's decision recommendations critically. Large language models (LLMs), with their exceptional conversational and analytical capabilities, present great opportunities to enhance AI-assisted decision making in the absence of AI explanations by providing natural-language-based analysis of AI's decision recommendation, e.g., how each feature of a decision making task might contribute to the AI recommendation. In this paper, via a randomized experiment, we first show that presenting LLM-powered analysis of each task feature, either sequentially or concurrently, does not significantly improve people's AI-assisted decision performance. To enable decision makers to better leverage LLM-powered analysis, we then propose an algorithmic framework to characterize the effects of LLM-powered analysis on human decisions and dynamically decide which analysis to present. Our evaluation with human subjects shows that this approach effectively improves decision makers' appropriate reliance on AI in AI-assisted decision making.

### Toward Metaphor-Fluid Conversation Design for Voice User Interfaces 
[[arxiv](https://arxiv.org/abs/2502.11554)] [[cool](https://papers.cool/arxiv/2502.11554)] [[pdf](https://arxiv.org/pdf/2502.11554)]
> **Authors**: Smit Desai,Jessie Chin,Dakuo Wang,Benjamin Cowan,Michael Twidale
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 人机交互,人工智能,计算语言学,计算机与社会,新兴技术
- **Abstract**: Metaphors play a critical role in shaping user experiences with Voice User Interfaces (VUIs), yet existing designs often rely on static, human-centric metaphors that fail to adapt to diverse contexts and user needs. This paper introduces Metaphor-Fluid Design, a novel approach that dynamically adjusts metaphorical representations based on conversational use-contexts. We compare this approach to a Default VUI, which characterizes the present implementation of commercial VUIs commonly designed around the persona of an assistant, offering a uniform interaction style across contexts. In Study 1 (N=130), metaphors were mapped to four key use-contexts-commands, information seeking, sociality, and error recovery-along the dimensions of formality and hierarchy, revealing distinct preferences for task-specific metaphorical designs. Study 2 (N=91) evaluates a Metaphor-Fluid VUI against a Default VUI, showing that the Metaphor-Fluid VUI enhances perceived intention to adopt, enjoyment, and likability by aligning better with user expectations for different contexts. However, individual differences in metaphor preferences highlight the need for personalization. These findings challenge the one-size-fits-all paradigm of VUI design and demonstrate the potential of Metaphor-Fluid Design to create more adaptive and engaging human-AI interactions.

## 信息检索(cs.IR:Information Retrieval)

### HopRAG: Multi-Hop Reasoning for Logic-Aware Retrieval-Augmented Generation 
[[arxiv](https://arxiv.org/abs/2502.12442)] [[cool](https://papers.cool/arxiv/2502.12442)] [[pdf](https://arxiv.org/pdf/2502.12442)]
> **Authors**: Hao Liu,Zhengren Wang,Xi Chen,Zhiyu Li,Feiyu Xiong,Qinhan Yu,Wentao Zhang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,计算语言学
- **Abstract**: Retrieval-Augmented Generation (RAG) systems often struggle with imperfect retrieval, as traditional retrievers focus on lexical or semantic similarity rather than logical relevance. To address this, we propose HopRAG, a novel RAG framework that augments retrieval with logical reasoning through graph-structured knowledge exploration. During indexing, HopRAG constructs a passage graph, with text chunks as vertices and logical connections established via LLM-generated pseudo-queries as edges. During retrieval, it employs a retrieve-reason-prune mechanism: starting with lexically or semantically similar passages, the system explores multi-hop neighbors guided by pseudo-queries and LLM reasoning to identify truly relevant ones. Extensive experiments demonstrate HopRAG's superiority, achieving 76.78\% higher answer accuracy and 65.07\% improved retrieval F1 score compared to conventional methods. The repository is available at https://github.com/LIU-Hao-2002/HopRAG.

### Solving the Cold Start Problem on One's Own as an End User via Preference Transfer 
[[arxiv](https://arxiv.org/abs/2502.12398)] [[cool](https://papers.cool/arxiv/2502.12398)] [[pdf](https://arxiv.org/pdf/2502.12398)]
> **Authors**: Ryoma Sato
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 25 pages
- **标题**: None
- **领域**: 信息检索,人工智能,机器学习
- **Abstract**: We propose a new approach that enables end users to directly solve the cold start problem by themselves. The cold start problem is a common issue in recommender systems, and many methods have been proposed to address the problem on the service provider's side. However, when the service provider does not take action, users are left with poor recommendations and no means to improve their experience. We propose an algorithm, Pretender, that allows end users to proactively solve the cold start problem on their own. Pretender does not require any special support from the service provider and can be deployed independently by users. We formulate the problem as minimizing the distance between the source and target distributions and optimize item selection from the target service accordingly. Furthermore, we establish theoretical guarantees for Pretender based on a discrete quadrature problem. We conduct experiments on real-world datasets to demonstrate the effectiveness of Pretender.

### Fast or Better? Balancing Accuracy and Cost in Retrieval-Augmented Generation with Flexible User Control 
[[arxiv](https://arxiv.org/abs/2502.12145)] [[cool](https://papers.cool/arxiv/2502.12145)] [[pdf](https://arxiv.org/pdf/2502.12145)]
> **Authors**: Jinyan Su,Jennifer Healey,Preslav Nakov,Claire Cardie
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,人工智能
- **Abstract**: Retrieval-Augmented Generation (RAG) has emerged as a powerful approach to mitigate large language model (LLM) hallucinations by incorporating external knowledge retrieval. However, existing RAG frameworks often apply retrieval indiscriminately,leading to inefficiencies-over-retrieving when unnecessary or failing to retrieve iteratively when required for complex reasoning. Recent adaptive retrieval strategies, though adaptively navigates these retrieval strategies, predict only based on query complexity and lacks user-driven flexibility, making them infeasible for diverse user application needs. In this paper, we introduce a novel user-controllable RAG framework that enables dynamic adjustment of the accuracy-cost trade-off. Our approach leverages two classifiers: one trained to prioritize accuracy and another to prioritize retrieval efficiency. Via an interpretable control parameter $α$, users can seamlessly navigate between minimal-cost retrieval and high-accuracy retrieval based on their specific requirements. We empirically demonstrate that our approach effectively balances accuracy, retrieval cost, and user controllability, making it a practical and adaptable solution for real-world applications.

## 信息论(cs.IT:Information Theory)

### Token Communications: A Unified Framework for Cross-modal Context-aware Semantic Communications 
[[arxiv](https://arxiv.org/abs/2502.12096)] [[cool](https://papers.cool/arxiv/2502.12096)] [[pdf](https://arxiv.org/pdf/2502.12096)]
> **Authors**: Li Qiao,Mahdi Boloursaz Mashhadi,Zhen Gao,Rahim Tafazolli,Mehdi Bennis,Dusit Niyato
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 信息论,计算机视觉和模式识别,多媒体,信号处理
- **Abstract**: In this paper, we introduce token communications (TokCom), a unified framework to leverage cross-modal context information in generative semantic communications (GenSC). TokCom is a new paradigm, motivated by the recent success of generative foundation models and multimodal large language models (GFM/MLLMs), where the communication units are tokens, enabling efficient transformer-based token processing at the transmitter and receiver. In this paper, we introduce the potential opportunities and challenges of leveraging context in GenSC, explore how to integrate GFM/MLLMs-based token processing into semantic communication systems to leverage cross-modal context effectively, present the key principles for efficient TokCom at various layers in future wireless networks. We demonstrate the corresponding TokCom benefits in a GenSC setup for image, leveraging cross-modal context information, which increases the bandwidth efficiency by 70.8% with negligible loss of semantic/perceptual quality. Finally, the potential research directions are identified to facilitate adoption of TokCom in future wireless networks.

### Reconfigurable Intelligent Surfaces-Assisted Integrated Access and Backhaul 
[[arxiv](https://arxiv.org/abs/2502.12011)] [[cool](https://papers.cool/arxiv/2502.12011)] [[pdf](https://arxiv.org/pdf/2502.12011)]
> **Authors**: Charitha Madapatha,Behrooz Makki,Hao Guo,Tommy Svensson
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Submitted to 2025 European Conference on Networks and Communications (EuCNC) & 6G Summit, 2025, Poznan, Poland
- **标题**: None
- **领域**: 信息论,机器学习,网络和互联网架构
- **Abstract**: In this paper, we study the impact of reconfigurable intelligent surfaces (RISs) on the coverage extension of integrated access and backhaul (IAB) networks. Particularly, using a finite stochastic geometry model, with random distributions of user equipments (UEs) in a finite region, and planned hierachical architecture for IAB, we study the service coverage probability defined as the probability of the event that the UEs' minimum rate requirements are satisfied. We present comparisons between different cases including IAB-only, IAB assisted with RIS for backhaul as well as IAB assisted by network controlled repeaters (NCRs). Our investigations focus on wide-area IAB assisted with RIS through the lens of different design architectures and deployments, revealing both conflicts and synergies for minimizing the effect of tree foliage over seasonal changes. Our simulation results reveal both opportunities and challenges towards the implementation of RIS in IAB.

## 机器学习(cs.LG:Machine Learning)

### Finding Optimal Trading History in Reinforcement Learning for Stock Market Trading 
[[arxiv](https://arxiv.org/abs/2502.12537)] [[cool](https://papers.cool/arxiv/2502.12537)] [[pdf](https://arxiv.org/pdf/2502.12537)]
> **Authors**: Sina Montazeri,Haseebullah Jumakhanb,Amir Mirzaeinia
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: This paper investigates the optimization of temporal windows in Financial Deep Reinforcement Learning (DRL) models using 2D Convolutional Neural Networks (CNNs). We introduce a novel approach to treating the temporal field as a hyperparameter and examine its impact on model performance across various datasets and feature arrangements. We introduce a new hyperparameter for the CNN policy, proposing that this temporal field can and should be treated as a hyperparameter for these models. We examine the significance of this temporal field by iteratively expanding the window of observations presented to the CNN policy during the deep reinforcement learning process. Our iterative process involves progressively increasing the observation period from two weeks to twelve weeks, allowing us to examine the effects of different temporal windows on the model's performance. This window expansion is implemented in two settings. In one setting, we rearrange the features in the dataset to group them by company, allowing the model to have a full view of company data in its observation window and CNN kernel. In the second setting, we do not group the features by company, and features are arranged by category. Our study reveals that shorter temporal windows are most effective when no feature rearrangement to group per company is in effect. However, the model will utilize longer temporal windows and yield better performance once we introduce the feature rearrangement. To examine the consistency of our findings, we repeated our experiment on two datasets containing the same thirty companies from the Dow Jones Index but with different features in each dataset and consistently observed the above-mentioned patterns. The result is a trading model significantly outperforming global financial services firms such as the Global X Guru by the established Mirae Asset.

### Alternating Regret for Online Convex Optimization 
[[arxiv](https://arxiv.org/abs/2502.12529)] [[cool](https://papers.cool/arxiv/2502.12529)] [[pdf](https://arxiv.org/pdf/2502.12529)]
> **Authors**: Soumita Hait,Ping Li,Haipeng Luo,Mengxiao Zhang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Motivated by alternating learning dynamics in two-player games, a recent work by Cevher et al.(2024) shows that $o(\sqrt{T})$ alternating regret is possible for any $T$-round adversarial Online Linear Optimization (OLO) problem, and left as an open question whether the same is true for general Online Convex Optimization (OCO). We answer this question in the affirmative by showing that the continuous Hedge algorithm achieves $\tilde{\mathcal{O}}(d^{\frac{2}{3}}T^{\frac{1}{3}})$ alternating regret for any adversarial $d$-dimensional OCO problems. We show that this implies an alternating learning dynamic that finds a Nash equilibrium for any convex-concave zero-sum games or a coarse correlated equilibrium for any convex two-player general-sum games at a rate of $\tilde{\mathcal{O}}(d^{\frac{2}{3}}/T^{\frac{2}{3}})$. To further improve the time complexity and/or the dimension dependence, we propose another simple algorithm, Follow-the-Regularized-Leader with a regularizer whose convex conjugate is 3rd-order smooth, for OCO with smooth and self-concordant loss functions (such as linear or quadratic losses). We instantiate our algorithm with different regularizers and show that, for example, when the decision set is the $\ell_2$ ball, our algorithm achieves $\tilde{\mathcal{O}}(T^{\frac{2}{5}})$ alternating regret with no dimension dependence (and a better $\tilde{\mathcal{O}}(T^{\frac{1}{3}})$ bound for quadratic losses). We complement our results by showing some algorithm-specific alternating regret lower bounds, including a somewhat surprising $Ω(\sqrt{T})$ lower bound for a Regret Matching variant that is widely used in alternating learning dynamics.

### Contextual Linear Bandits with Delay as Payoff 
[[arxiv](https://arxiv.org/abs/2502.12528)] [[cool](https://papers.cool/arxiv/2502.12528)] [[pdf](https://arxiv.org/pdf/2502.12528)]
> **Authors**: Mengxiao Zhang,Yingfei Wang,Haipeng Luo
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: A recent work by Schlisselberg et al. (2024) studies a delay-as-payoff model for stochastic multi-armed bandits, where the payoff (either loss or reward) is delayed for a period that is proportional to the payoff itself. While this captures many real-world applications, the simple multi-armed bandit setting limits the practicality of their results. In this paper, we address this limitation by studying the delay-as-payoff model for contextual linear bandits. Specifically, we start from the case with a fixed action set and propose an efficient algorithm whose regret overhead compared to the standard no-delay case is at most $DΔ_{\max}\log T$, where $T$ is the total horizon, $D$ is the maximum delay, and $Δ_{\max}$ is the maximum suboptimality gap. When payoff is loss, we also show further improvement of the bound, demonstrating a separation between reward and loss similar to Schlisselberg et al. (2024). Contrary to standard linear bandit algorithms that construct least squares estimator and confidence ellipsoid, the main novelty of our algorithm is to apply a phased arm elimination procedure by only picking actions in a volumetric spanner of the action set, which addresses challenges arising from both payoff-dependent delays and large action sets. We further extend our results to the case with varying action sets by adopting the reduction from Hanna et al. (2023). Finally, we implement our algorithm and showcase its effectiveness and superior performance in experiments.

### From Abstract to Actionable: Pairwise Shapley Values for Explainable AI 
[[arxiv](https://arxiv.org/abs/2502.12525)] [[cool](https://papers.cool/arxiv/2502.12525)] [[pdf](https://arxiv.org/pdf/2502.12525)]
> **Authors**: Jiaxin Xu,Hung Chau,Angela Burden
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Explainable AI (XAI) is critical for ensuring transparency, accountability, and trust in machine learning systems as black-box models are increasingly deployed within high-stakes domains. Among XAI methods, Shapley values are widely used for their fairness and consistency axioms. However, prevalent Shapley value approximation methods commonly rely on abstract baselines or computationally intensive calculations, which can limit their interpretability and scalability. To address such challenges, we propose Pairwise Shapley Values, a novel framework that grounds feature attributions in explicit, human-relatable comparisons between pairs of data instances proximal in feature space. Our method introduces pairwise reference selection combined with single-value imputation to deliver intuitive, model-agnostic explanations while significantly reducing computational overhead. Here, we demonstrate that Pairwise Shapley Values enhance interpretability across diverse regression and classification scenarios--including real estate pricing, polymer property prediction, and drug discovery datasets. We conclude that the proposed methods enable more transparent AI systems and advance the real-world applicability of XAI.

### Understanding Generalization in Transformers: Error Bounds and Training Dynamics Under Benign and Harmful Overfitting 
[[arxiv](https://arxiv.org/abs/2502.12508)] [[cool](https://papers.cool/arxiv/2502.12508)] [[pdf](https://arxiv.org/pdf/2502.12508)]
> **Authors**: Yingying Zhang,Zhenyu Wu,Jian Li,Yong Liu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Transformers serve as the foundational architecture for many successful large-scale models, demonstrating the ability to overfit the training data while maintaining strong generalization on unseen data, a phenomenon known as benign overfitting. However, research on how the training dynamics influence error bounds within the context of benign overfitting has been limited. This paper addresses this gap by developing a generalization theory for a two-layer transformer with labeled flip noise. Specifically, we present generalization error bounds for both benign and harmful overfitting under varying signal-to-noise ratios (SNR), where the training dynamics are categorized into three distinct stages, each with its corresponding error bounds. Additionally, we conduct extensive experiments to identify key factors that influence test errors in transformers. Our experimental results align closely with the theoretical predictions, validating our findings.

### Mixture of Attention Yields Accurate Results for Tabular Data 
[[arxiv](https://arxiv.org/abs/2502.12507)] [[cool](https://papers.cool/arxiv/2502.12507)] [[pdf](https://arxiv.org/pdf/2502.12507)]
> **Authors**: Xuechen Li,Yupeng Li,Jian Liu,Xiaolin Jin,Tian Yang,Xin Hu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 15 pages, 4 figures
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Tabular data inherently exhibits significant feature heterogeneity, but existing transformer-based methods lack specialized mechanisms to handle this property. To bridge the gap, we propose MAYA, an encoder-decoder transformer-based framework. In the encoder, we design a Mixture of Attention (MOA) that constructs multiple parallel attention branches and averages the features at each branch, effectively fusing heterogeneous features while limiting parameter growth. Additionally, we employ collaborative learning with a dynamic consistency weight constraint to produce more robust representations. In the decoder stage, cross-attention is utilized to seamlessly integrate tabular data with corresponding label features. This dual-attention mechanism effectively captures both intra-instance and inter-instance interactions. We evaluate the proposed method on a wide range of datasets and compare it with other state-of-the-art transformer-based methods. Extensive experiments demonstrate that our model achieves superior performance among transformer-based methods in both tabular classification and regression tasks.

### GPU Memory Usage Optimization for Backward Propagation in Deep Network Training 
[[arxiv](https://arxiv.org/abs/2502.12499)] [[cool](https://papers.cool/arxiv/2502.12499)] [[pdf](https://arxiv.org/pdf/2502.12499)]
> **Authors**: Ding-Yong Hong,Tzu-Hsien Tsai,Ning Wang,Pangfeng Liu,Jan-Jan Wu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: To appear in JPDC 2025
- **标题**: None
- **领域**: 机器学习,数据结构和算法
- **Abstract**: In modern Deep Learning, it has been a trend to design larger Deep Neural Networks (DNNs) for the execution of more complex tasks and better accuracy. On the other hand, Convolutional Neural Networks (CNNs) have become the standard method for most of computer vision tasks. However, the memory allocation for the intermediate data in convolution layers can cause severe memory pressure during model training. Many solutions have been proposed to resolve the problem. Besides hardware-dependent solutions, a general methodology rematerialization can reduce GPU memory usage by trading computation for memory efficiently. The idea is to select a set of intermediate results during the forward phase as checkpoints, and only save them in memory to reduce memory usage. The backward phase recomputes the intermediate data from the closest checkpoints in memory as needed. This recomputation increases execution time but saves memory by not storing all intermediate results in memory during the forward phase. In this paper, we will focus on efficiently finding the optimal checkpoint subset to achieve the least peak memory usage during the model training. We first describe the theoretical background of the training of a neural network using mathematical equations. We use these equations to identify all essential data required during both forward and backward phases to compute the gradient of weights of the model. We first identify the checkpoint selection problem and propose a dynamic programming algorithm with time complexity O(n3) to solve the problem of finding the optimal checkpoint subset. With extensive experiments, we formulate a more accurate description of the problem using our theoretical analysis and revise the objective function based on the tracing, and propose an O(n)-time algorithm for finding the optimal checkpoint subset.

### EDGE: Efficient Data Selection for LLM Agents via Guideline Effectiveness 
[[arxiv](https://arxiv.org/abs/2502.12494)] [[cool](https://papers.cool/arxiv/2502.12494)] [[pdf](https://arxiv.org/pdf/2502.12494)]
> **Authors**: Yunxiao Zhang,Guanming Xiong,Haochen Li,Wen Zhao
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Large Language Models (LLMs) have shown remarkable capabilities as AI agents. However, existing methods for enhancing LLM-agent abilities often lack a focus on data quality, leading to inefficiencies and suboptimal results in both fine-tuning and prompt engineering. To address this issue, we introduce EDGE, a novel approach for identifying informative samples without needing golden answers. We propose the Guideline Effectiveness (GE) metric, which selects challenging samples by measuring the impact of human-provided guidelines in multi-turn interaction tasks. A low GE score indicates that the human expertise required for a sample is missing from the guideline, making the sample more informative. By selecting samples with low GE scores, we can improve the efficiency and outcomes of both prompt engineering and fine-tuning processes for LLMs. Extensive experiments validate the performance of our method. Our method achieves competitive results on the HotpotQA and WebShop and datasets, requiring 75\% and 50\% less data, respectively, while outperforming existing methods. We also provide a fresh perspective on the data quality of LLM-agent fine-tuning.

### LocalEscaper: A Weakly-supervised Framework with Regional Reconstruction for Scalable Neural TSP Solvers 
[[arxiv](https://arxiv.org/abs/2502.12484)] [[cool](https://papers.cool/arxiv/2502.12484)] [[pdf](https://arxiv.org/pdf/2502.12484)]
> **Authors**: Junrui Wen,Yifei Li,Bart Selman,Kun He
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Neural solvers have shown significant potential in solving the Traveling Salesman Problem (TSP), yet current approaches face significant challenges. Supervised learning (SL)-based solvers require large amounts of high-quality labeled data, while reinforcement learning (RL)-based solvers, though less dependent on such data, often suffer from inefficiencies. To address these limitations, we propose LocalEscaper, a novel weakly-supervised learning framework for large-scale TSP. LocalEscaper effectively combines the advantages of both SL and RL, enabling effective training on datasets with low-quality labels. To further enhance solution quality, we introduce a regional reconstruction strategy, which mitigates the problem of local optima, a common issue in existing local reconstruction methods. Additionally, we propose a linear-complexity attention mechanism that reduces computational overhead, enabling the efficient solution of large-scale TSPs without sacrificing performance. Experimental results on both synthetic and real-world datasets demonstrate that LocalEscaper outperforms existing neural solvers, achieving state-of-the-art results. Notably, it sets a new benchmark for scalability and efficiency, solving TSP instances with up to 50,000 cities.

### MotifBench: A standardized protein design benchmark for motif-scaffolding problems 
[[arxiv](https://arxiv.org/abs/2502.12479)] [[cool](https://papers.cool/arxiv/2502.12479)] [[pdf](https://arxiv.org/pdf/2502.12479)]
> **Authors**: Zhuoqi Zheng,Bo Zhang,Kieran Didi,Kevin K. Yang,Jason Yim,Joseph L. Watson,Hai-Feng Chen,Brian L. Trippe
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Associated content available at github.com/blt2114/MotifBench
- **标题**: None
- **领域**: 机器学习,生物分子
- **Abstract**: The motif-scaffolding problem is a central task in computational protein design: Given the coordinates of atoms in a geometry chosen to confer a desired biochemical function (a motif), the task is to identify diverse protein structures (scaffolds) that include the motif and maintain its geometry. Significant recent progress on motif-scaffolding has been made due to computational evaluation with reliable protein structure prediction and fixed-backbone sequence design methods. However, significant variability in evaluation strategies across publications has hindered comparability of results, challenged reproducibility, and impeded robust progress. In response we introduce MotifBench, comprising (1) a precisely specified pipeline and evaluation metrics, (2) a collection of 30 benchmark problems, and (3) an implementation of this benchmark and leaderboard at github.com/blt2114/MotifBench. The MotifBench test cases are more difficult compared to earlier benchmarks, and include protein design problems for which solutions are known but on which, to the best of our knowledge, state-of-the-art methods fail to identify any solution.

### MCTS-Judge: Test-Time Scaling in LLM-as-a-Judge for Code Correctness Evaluation 
[[arxiv](https://arxiv.org/abs/2502.12468)] [[cool](https://papers.cool/arxiv/2502.12468)] [[pdf](https://arxiv.org/pdf/2502.12468)]
> **Authors**: Yutong Wang,Pengliang Ji,Chaoqun Yang,Kaixin Li,Ming Hu,Jiaoyang Li,Guillaume Sartoretti
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: The LLM-as-a-Judge paradigm shows promise for evaluating generative content but lacks reliability in reasoning-intensive scenarios, such as programming. Inspired by recent advances in reasoning models and shifts in scaling laws, we pioneer bringing test-time computation into LLM-as-a-Judge, proposing MCTS-Judge, a resource-efficient, System-2 thinking framework for code correctness evaluation. MCTS-Judge leverages Monte Carlo Tree Search (MCTS) to decompose problems into simpler, multi-perspective evaluations. Through a node-selection strategy that combines self-assessment based on historical actions in the current trajectory and the Upper Confidence Bound for Trees based on prior rollouts, MCTS-Judge balances global optimization and refinement of the current trajectory. We further designed a high-precision, unit-test-level reward mechanism to encourage the Large Language Model (LLM) to perform line-by-line analysis. Extensive experiments on three benchmarks and five LLMs demonstrate the effectiveness of MCTS-Judge, which improves the base model's accuracy from 41% to 80%, surpassing the o1-series models with 3x fewer tokens. Further evaluations validate the superiority of its reasoning trajectory in logic, analytics, thoroughness, and overall quality, while revealing the test-time scaling law of the LLM-as-a-Judge paradigm.

### EquiBench: Benchmarking Code Reasoning Capabilities of Large Language Models via Equivalence Checking 
[[arxiv](https://arxiv.org/abs/2502.12466)] [[cool](https://papers.cool/arxiv/2502.12466)] [[pdf](https://arxiv.org/pdf/2502.12466)]
> **Authors**: Anjiang Wei,Jiannan Cao,Ran Li,Hongyu Chen,Yuhui Zhang,Ziheng Wang,Yaofeng Sun,Yuan Liu,Thiago S. F. X. Teixeira,Diyi Yang,Ke Wang,Alex Aiken
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,编程语言,软件工程
- **Abstract**: Equivalence checking, i.e., determining whether two programs produce identical outputs for all possible inputs, underpins a broad range of applications, including software refactoring, testing, and optimization. We present the task of equivalence checking as a new way to evaluate the code reasoning abilities of large language models (LLMs). We introduce EquiBench, a dataset of 2400 program pairs spanning four programming languages and six equivalence categories. These pairs are systematically generated through program analysis, compiler scheduling, and superoptimization, covering nontrivial structural transformations that demand deep semantic reasoning beyond simple syntactic variations. Our evaluation of 17 state-of-the-art LLMs shows that OpenAI o3-mini achieves the highest overall accuracy of 78.0%. In the most challenging categories, the best accuracies are 62.3% and 68.8%, only modestly above the 50% random baseline for binary classification, indicating significant room for improvement in current models' code reasoning capabilities.

### Computational-Statistical Tradeoffs at the Next-Token Prediction Barrier: Autoregressive and Imitation Learning under Misspecification 
[[arxiv](https://arxiv.org/abs/2502.12465)] [[cool](https://papers.cool/arxiv/2502.12465)] [[pdf](https://arxiv.org/pdf/2502.12465)]
> **Authors**: Dhruv Rohatgi,Adam Block,Audrey Huang,Akshay Krishnamurthy,Dylan J. Foster
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 75 pages
- **标题**: None
- **领域**: 机器学习,数据结构和算法
- **Abstract**: Next-token prediction with the logarithmic loss is a cornerstone of autoregressive sequence modeling, but, in practice, suffers from error amplification, where errors in the model compound and generation quality degrades as sequence length $H$ increases. From a theoretical perspective, this phenomenon should not appear in well-specified settings, and, indeed, a growing body of empirical work hypothesizes that misspecification, where the learner is not sufficiently expressive to represent the target distribution, may be the root cause. Under misspecification -- where the goal is to learn as well as the best-in-class model up to a multiplicative approximation factor $C\geq 1$ -- we confirm that $C$ indeed grows with $H$ for next-token prediction, lending theoretical support to this empirical hypothesis. We then ask whether this mode of error amplification is avoidable algorithmically, computationally, or information-theoretically, and uncover inherent computational-statistical tradeoffs. We show: (1) Information-theoretically, one can avoid error amplification and achieve $C=O(1)$. (2) Next-token prediction can be made robust so as to achieve $C=\tilde O(H)$, representing moderate error amplification, but this is an inherent barrier: any next-token prediction-style objective must suffer $C=Ω(H)$. (3) For the natural testbed of autoregressive linear models, no computationally efficient algorithm can achieve sub-polynomial approximation factor $C=e^{(\log H)^{1-Ω(1)}}$; however, at least for binary token spaces, one can smoothly trade compute for statistical power and improve on $C=Ω(H)$ in sub-exponential time. Our results have consequences in the more general setting of imitation learning, where the widely-used behavior cloning algorithm generalizes next-token prediction.

### UniMatch: Universal Matching from Atom to Task for Few-Shot Drug Discovery 
[[arxiv](https://arxiv.org/abs/2502.12453)] [[cool](https://papers.cool/arxiv/2502.12453)] [[pdf](https://arxiv.org/pdf/2502.12453)]
> **Authors**: Ruifeng Li,Mingqian Li,Wei Liu,Yuhua Zhou,Xiangxin Zhou,Yuan Yao,Qiang Zhang,Hongyang Chen
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: accepted as ICLR 2025 Spotlight
- **标题**: None
- **领域**: 机器学习,人工智能,生物分子
- **Abstract**: Drug discovery is crucial for identifying candidate drugs for various diseases.However, its low success rate often results in a scarcity of annotations, posing a few-shot learning problem. Existing methods primarily focus on single-scale features, overlooking the hierarchical molecular structures that determine different molecular properties. To address these issues, we introduce Universal Matching Networks (UniMatch), a dual matching framework that integrates explicit hierarchical molecular matching with implicit task-level matching via meta-learning, bridging multi-level molecular representations and task-level generalization. Specifically, our approach explicitly captures structural features across multiple levels, such as atoms, substructures, and molecules, via hierarchical pooling and matching, facilitating precise molecular representation and comparison. Additionally, we employ a meta-learning strategy for implicit task-level matching, allowing the model to capture shared patterns across tasks and quickly adapt to new ones. This unified matching framework ensures effective molecular alignment while leveraging shared meta-knowledge for fast adaptation. Our experimental results demonstrate that UniMatch outperforms state-of-the-art methods on the MoleculeNet and FS-Mol benchmarks, achieving improvements of 2.87% in AUROC and 6.52% in delta AUPRC. UniMatch also shows excellent generalization ability on the Meta-MolNet benchmark.

### SparAMX: Accelerating Compressed LLMs Token Generation on AMX-powered CPUs 
[[arxiv](https://arxiv.org/abs/2502.12444)] [[cool](https://papers.cool/arxiv/2502.12444)] [[pdf](https://arxiv.org/pdf/2502.12444)]
> **Authors**: Ahmed F. AbouElhamayed,Jordan Dotzel,Yash Akhauri,Chi-Chih Chang,Sameh Gobriel,J. Pablo Muñoz,Vui Seng Chua,Nilesh Jain,Mohamed S. Abdelfattah
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,硬件架构,表现
- **Abstract**: Large language models have high compute, latency, and memory requirements. While specialized accelerators such as GPUs and TPUs typically run these workloads, CPUs are more widely available and consume less energy. Accelerating LLMs with CPUs enables broader AI access at a lower cost and power consumption. This acceleration potential for CPUs is especially relevant during the memory-bound decoding stage of LLM inference, which processes one token at a time and is becoming increasingly utilized with reasoning models. We utilize Advanced Matrix Extensions (AMX) support on the latest Intel CPUs together with unstructured sparsity to achieve a $1.42 \times$ reduction in end-to-end latency compared to the current PyTorch implementation by applying our technique in linear layers. We provide a set of open-source customized sparse kernels that can speed up any PyTorch model by automatically replacing all linear layers with our custom sparse implementation. Furthermore, we demonstrate for the first time the use of unstructured sparsity in the attention computation achieving a $1.14 \times$ speedup over the current systems without compromising accuracy. Code: https://github.com/IntelLabs/Hardware-Aware-Automated-Machine-Learning/tree/main/SparAMX

### Bridge the Gaps between Machine Unlearning and AI Regulation 
[[arxiv](https://arxiv.org/abs/2502.12430)] [[cool](https://papers.cool/arxiv/2502.12430)] [[pdf](https://arxiv.org/pdf/2502.12430)]
> **Authors**: Bill Marino,Meghdad Kurmanji,Nicholas D. Lane
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: The "right to be forgotten" and the data privacy laws that encode it have motivated machine unlearning since its earliest days. Now, an inbound wave of artificial intelligence regulations - like the European Union's Artificial Intelligence Act (AIA) - potentially offer important new use cases for machine unlearning. However, this position paper argues, this opportunity will only be realized if researchers, aided by policymakers, proactively bridge the (sometimes sizable) gaps between machine unlearning's state of the art and its potential applications to AI regulation. To demonstrate this point, we use the AIA as an example. Specifically, we deliver a "state of the union" as regards machine unlearning's current potential for aiding compliance with the AIA. This starts with a precise cataloging of the potential applications of machine unlearning to AIA compliance. For each, we flag any legal ambiguities clouding the potential application and, moreover, flag the technical gaps that exist between the potential application and the state of the art of machine unlearning. Finally, we end with a call to action: for both machine learning researchers and policymakers, to, respectively, solve the open technical and legal questions that will unlock machine unlearning's potential to assist compliance with the AIA - and other AI regulation like it.

### DivIL: Unveiling and Addressing Over-Invariance for Out-of- Distribution Generalization 
[[arxiv](https://arxiv.org/abs/2502.12413)] [[cool](https://papers.cool/arxiv/2502.12413)] [[pdf](https://arxiv.org/pdf/2502.12413)]
> **Authors**: Jiaqi Wang,Yuhang Zhou,Zhixiong Zhang,Qiguang Chen,Yongqiang Chen,James Cheng
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Out-of-distribution generalization is a common problem that expects the model to perform well in the different distributions even far from the train data. A popular approach to addressing this issue is invariant learning (IL), in which the model is compiled to focus on invariant features instead of spurious features by adding strong constraints during training. However, there are some potential pitfalls of strong invariant constraints. Due to the limited number of diverse environments and over-regularization in the feature space, it may lead to a loss of important details in the invariant features while alleviating the spurious correlations, namely the over-invariance, which can also degrade the generalization performance. We theoretically define the over-invariance and observe that this issue occurs in various classic IL methods. To alleviate this issue, we propose a simple approach Diverse Invariant Learning (DivIL) by adding the unsupervised contrastive learning and the random masking mechanism compensatory for the invariant constraints, which can be applied to various IL methods. Furthermore, we conduct experiments across multiple modalities across 12 datasets and 6 classic models, verifying our over-invariance insight and the effectiveness of our DivIL framework. Our code is available at https://github.com/kokolerk/DivIL.

### Incomplete Graph Learning: A Comprehensive Survey 
[[arxiv](https://arxiv.org/abs/2502.12412)] [[cool](https://papers.cool/arxiv/2502.12412)] [[pdf](https://arxiv.org/pdf/2502.12412)]
> **Authors**: Riting Xia,Huibo Liu,Anchen Li,Xueyan Liu,Yan Zhang,Chunxu Zhang,Bo Yang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,图像和视频处理
- **Abstract**: Graph learning is a prevalent field that operates on ubiquitous graph data. Effective graph learning methods can extract valuable information from graphs. However, these methods are non-robust and affected by missing attributes in graphs, resulting in sub-optimal outcomes. This has led to the emergence of incomplete graph learning, which aims to process and learn from incomplete graphs to achieve more accurate and representative results. In this paper, we conducted a comprehensive review of the literature on incomplete graph learning. Initially, we categorize incomplete graphs and provide precise definitions of relevant concepts, terminologies, and techniques, thereby establishing a solid understanding for readers. Subsequently, we classify incomplete graph learning methods according to the types of incompleteness: (1) attribute-incomplete graph learning methods, (2) attribute-missing graph learning methods, and (3) hybrid-absent graph learning methods. By systematically classifying and summarizing incomplete graph learning methods, we highlight the commonalities and differences among existing approaches, aiding readers in selecting methods and laying the groundwork for further advancements. In addition, we summarize the datasets, incomplete processing modes, evaluation metrics, and application domains used by the current methods. Lastly, we discuss the current challenges and propose future directions for incomplete graph learning, with the aim of stimulating further innovations in this crucial field. To our knowledge, this is the first review dedicated to incomplete graph learning, aiming to offer valuable insights for researchers in related fields.We developed an online resource to follow relevant research based on this review, available at https://github.com/cherry-a11y/Incomplete-graph-learning.git

### Efficient Neural SDE Training using Wiener-Space Cubature 
[[arxiv](https://arxiv.org/abs/2502.12395)] [[cool](https://papers.cool/arxiv/2502.12395)] [[pdf](https://arxiv.org/pdf/2502.12395)]
> **Authors**: Luke Snow,Vikram Krishnamurthy
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: A neural stochastic differential equation (SDE) is an SDE with drift and diffusion terms parametrized by neural networks. The training procedure for neural SDEs consists of optimizing the SDE vector field (neural network) parameters to minimize the expected value of an objective functional on infinite-dimensional path-space. Existing training techniques focus on methods to efficiently compute path-wise gradients of the objective functional with respect to these parameters, then pair this with Monte-Carlo simulation to estimate the expectation, and stochastic gradient descent to optimize. In this work we introduce a novel training technique which bypasses and improves upon Monte-Carlo simulation; we extend results in the theory of Wiener-space cubature to approximate the expected objective functional by a weighted sum of deterministic ODE solutions. This allows us to compute gradients by efficient ODE adjoint methods. Furthermore, we exploit a high-order recombination scheme to drastically reduce the number of ODE solutions necessary to achieve a reasonable approximation. We show that this Wiener-space cubature approach can surpass the O(1/sqrt(n)) rate of Monte-Carlo simulation, or the O(log(n)/n) rate of quasi-Monte-Carlo, to achieve a O(1/n) rate under reasonable assumptions.

### Reward-Safety Balance in Offline Safe RL via Diffusion Regularization 
[[arxiv](https://arxiv.org/abs/2502.12391)] [[cool](https://papers.cool/arxiv/2502.12391)] [[pdf](https://arxiv.org/pdf/2502.12391)]
> **Authors**: Junyu Guo,Zhi Zheng,Donghao Ying,Ming Jin,Shangding Gu,Costas Spanos,Javad Lavaei
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Constrained reinforcement learning (RL) seeks high-performance policies under safety constraints. We focus on an offline setting where the agent has only a fixed dataset -- common in realistic tasks to prevent unsafe exploration. To address this, we propose Diffusion-Regularized Constrained Offline Reinforcement Learning (DRCORL), which first uses a diffusion model to capture the behavioral policy from offline data and then extracts a simplified policy to enable efficient inference. We further apply gradient manipulation for safety adaptation, balancing the reward objective and constraint satisfaction. This approach leverages high-quality offline data while incorporating safety requirements. Empirical results show that DRCORL achieves reliable safety performance, fast inference, and strong reward outcomes across robot learning tasks. Compared to existing safe offline RL methods, it consistently meets cost limits and performs well with the same hyperparameters, indicating practical applicability in real-world scenarios.

### Achieving Upper Bound Accuracy of Joint Training in Continual Learning 
[[arxiv](https://arxiv.org/abs/2502.12388)] [[cool](https://papers.cool/arxiv/2502.12388)] [[pdf](https://arxiv.org/pdf/2502.12388)]
> **Authors**: Saleh Momeni,Bing Liu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Continual learning has been an active research area in machine learning, focusing on incrementally learning a sequence of tasks. A key challenge is catastrophic forgetting (CF), and most research efforts have been directed toward mitigating this issue. However, a significant gap remains between the accuracy achieved by state-of-the-art continual learning algorithms and the ideal or upper-bound accuracy achieved by training all tasks together jointly. This gap has hindered or even prevented the adoption of continual learning in applications, as accuracy is often of paramount importance. Recently, another challenge, termed inter-task class separation (ICS), was also identified, which spurred a theoretical study into principled approaches for solving continual learning. Further research has shown that by leveraging the theory and the power of large foundation models, it is now possible to achieve upper-bound accuracy, which has been empirically validated using both text and image classification datasets. Continual learning is now ready for real-life applications. This paper surveys the main research leading to this achievement, justifies the approach both intuitively and from neuroscience research, and discusses insights gained.

### Scalable Back-Propagation-Free Training of Optical Physics-Informed Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.12384)] [[cool](https://papers.cool/arxiv/2502.12384)] [[pdf](https://arxiv.org/pdf/2502.12384)]
> **Authors**: Yequan Zhao,Xinling Yu,Xian Xiao,Zhixiong Chen,Ziyue Liu,Geza Kurczveil,Raymond G. Beausoleil,Sijia Liu,Zheng Zhang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Physics-informed neural networks (PINNs) have shown promise in solving partial differential equations (PDEs), with growing interest in their energy-efficient, real-time training on edge devices. Photonic computing offers a potential solution to achieve this goal because of its ultra-high operation speed. However, the lack of photonic memory and the large device sizes prevent training real-size PINNs on photonic chips. This paper proposes a completely back-propagation-free (BP-free) and highly salable framework for training real-size PINNs on silicon photonic platforms. Our approach involves three key innovations: (1) a sparse-grid Stein derivative estimator to avoid the BP in the loss evaluation of a PINN, (2) a dimension-reduced zeroth-order optimization via tensor-train decomposition to achieve better scalability and convergence in BP-free training, and (3) a scalable on-chip photonic PINN training accelerator design using photonic tensor cores. We validate our numerical methods on both low- and high-dimensional PDE benchmarks. Through circuit simulation based on real device parameters, we further demonstrate the significant performance benefit (e.g., real-time training, huge chip area reduction) of our photonic accelerator.

### Locally-Deployed Chain-of-Thought (CoT) Reasoning Model in Chemical Engineering: Starting from 30 Experimental Data 
[[arxiv](https://arxiv.org/abs/2502.12383)] [[cool](https://papers.cool/arxiv/2502.12383)] [[pdf](https://arxiv.org/pdf/2502.12383)]
> **Authors**: Tianhang Zhou,Yingchun Niu,Xingying Lan,Chunming Xu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Code is avaliable upon request
- **标题**: None
- **领域**: 机器学习,应用领域
- **Abstract**: In the field of chemical engineering, traditional data-processing and prediction methods face significant challenges. Machine-learning and large-language models (LLMs) also have their respective limitations. This paper explores the application of the Chain-of-Thought (CoT) reasoning model in chemical engineering, starting from 30 experimental data points. By integrating traditional surrogate models like Gaussian processes and random forests with powerful LLMs such as DeepSeek-R1, a hierarchical architecture is proposed. Two CoT-building methods, Large Language Model-Chain of Thought (LLM-CoT) and Machine Learning-Large Language Model-Chain of Thought (ML-LLM-CoT), are studied. The LLM-CoT combines local models DeepSeek-r1:14b and Qwen2:7b with Ollama. The ML-LLM-CoT integrates a pre-trained Gaussian ML model with the LLM-based CoT framework. Our results show that during construction, ML-LLM-CoT is more efficient. It only has 2 points that require rethink and a total of 4 rethink times, while LLM-CoT has 5 points that need to be re-thought and 34 total rethink times. In predicting the solubility of 20 molecules with dissimilar structures, the number of molecules with a prediction deviation higher than 100\% for the Gaussian model, LLM-CoT, and ML-LLM-CoT is 7, 6, and 4 respectively. These results indicate that ML-LLM-CoT performs better in controlling the number of high-deviation molecules, optimizing the average deviation, and achieving a higher success rate in solubility judgment, providing a more reliable method for chemical engineering and molecular property prediction. This study breaks through the limitations of traditional methods and offers new solutions for rapid property prediction and process optimization in chemical engineering.

### Linear Diffusion Networks: Harnessing Diffusion Processes for Global Interactions 
[[arxiv](https://arxiv.org/abs/2502.12381)] [[cool](https://papers.cool/arxiv/2502.12381)] [[pdf](https://arxiv.org/pdf/2502.12381)]
> **Authors**: Jacob Fein-Ashley
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Diffusion kernels capture global dependencies. We present Linear Diffusion Networks (LDNs), a novel architecture that reinterprets sequential data processing as a unified diffusion process. Our model integrates adaptive diffusion modules with localized nonlinear updates and a diffusion-inspired attention mechanism. This design enables efficient global information propagation while preserving fine-grained temporal details. LDN overcomes the limitations of conventional recurrent and transformer models by allowing full parallelization across time steps and supporting robust multi-scale temporal representations. Experiments on benchmark sequence modeling tasks demonstrate that LDN delivers superior performance and scalability, setting a new standard for global interaction in sequential data.

### Positional Encoding in Transformer-Based Time Series Models: A Survey 
[[arxiv](https://arxiv.org/abs/2502.12370)] [[cool](https://papers.cool/arxiv/2502.12370)] [[pdf](https://arxiv.org/pdf/2502.12370)]
> **Authors**: Habib Irani,Vangelis Metsis
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 15 pages, 6 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Recent advancements in transformer-based models have greatly improved time series analysis, providing robust solutions for tasks such as forecasting, anomaly detection, and classification. A crucial element of these models is positional encoding, which allows transformers to capture the intrinsic sequential nature of time series data. This survey systematically examines existing techniques for positional encoding in transformer-based time series models. We investigate a variety of methods, including fixed, learnable, relative, and hybrid approaches, and evaluate their effectiveness in different time series classification tasks. Furthermore, we outline key challenges and suggest potential research directions to enhance positional encoding strategies. By delivering a comprehensive overview and quantitative benchmarking, this survey intends to assist researchers and practitioners in selecting and designing effective positional encoding methods for transformer-based time series models.

### ScriptoriumWS: A Code Generation Assistant for Weak Supervision 
[[arxiv](https://arxiv.org/abs/2502.12366)] [[cool](https://papers.cool/arxiv/2502.12366)] [[pdf](https://arxiv.org/pdf/2502.12366)]
> **Authors**: Tzu-Heng Huang,Catherine Cao,Spencer Schoenberg,Harit Vishwakarma,Nicholas Roberts,Frederic Sala
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Appeared in ICLR'23DeepLearningfor Code (DL4C) Workshop & 2023 MidwestMachineLearningSymposium
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Weak supervision is a popular framework for overcoming the labeled data bottleneck: the need to obtain labels for training data. In weak supervision, multiple noisy-but-cheap sources are used to provide guesses of the label and are aggregated to produce high-quality pseudolabels. These sources are often expressed as small programs written by domain experts -- and so are expensive to obtain. Instead, we argue for using code-generation models to act as coding assistants for crafting weak supervision sources. We study prompting strategies to maximize the quality of the generated sources, settling on a multi-tier strategy that incorporates multiple types of information. We explore how to best combine hand-written and generated sources. Using these insights, we introduce ScriptoriumWS, a weak supervision system that, when compared to hand-crafted sources, maintains accuracy and greatly improves coverage.

### Stability-based Generalization Bounds for Variational Inference 
[[arxiv](https://arxiv.org/abs/2502.12353)] [[cool](https://papers.cool/arxiv/2502.12353)] [[pdf](https://arxiv.org/pdf/2502.12353)]
> **Authors**: Yadi Wei,Roni Khardon
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 20 pages, 3 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Variational inference (VI) is widely used for approximate inference in Bayesian machine learning. In addition to this practical success, generalization bounds for variational inference and related algorithms have been developed, mostly through the connection to PAC-Bayes analysis. A second line of work has provided algorithm-specific generalization bounds through stability arguments or using mutual information bounds, and has shown that the bounds are tight in practice, but unfortunately these bounds do not directly apply to approximate Bayesian algorithms. This paper fills this gap by developing algorithm-specific stability based generalization bounds for a class of approximate Bayesian algorithms that includes VI, specifically when using stochastic gradient descent to optimize their objective. As in the non-Bayesian case, the generalization error is bounded by by expected parameter differences on a perturbed dataset. The new approach complements PAC-Bayes analysis and can provide tighter bounds in some cases. An experimental illustration shows that the new approach yields non-vacuous bounds on modern neural network architectures and datasets and that it can shed light on performance differences between variant approximate Bayesian algorithms.

### Towards Mechanistic Interpretability of Graph Transformers via Attention Graphs 
[[arxiv](https://arxiv.org/abs/2502.12352)] [[cool](https://papers.cool/arxiv/2502.12352)] [[pdf](https://arxiv.org/pdf/2502.12352)]
> **Authors**: Batu El,Deepro Choudhury,Pietro Liò,Chaitanya K. Joshi
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: We introduce Attention Graphs, a new tool for mechanistic interpretability of Graph Neural Networks (GNNs) and Graph Transformers based on the mathematical equivalence between message passing in GNNs and the self-attention mechanism in Transformers. Attention Graphs aggregate attention matrices across Transformer layers and heads to describe how information flows among input nodes. Through experiments on homophilous and heterophilous node classification tasks, we analyze Attention Graphs from a network science perspective and find that: (1) When Graph Transformers are allowed to learn the optimal graph structure using all-to-all attention among input nodes, the Attention Graphs learned by the model do not tend to correlate with the input/original graph structure; and (2) For heterophilous graphs, different Graph Transformer variants can achieve similar performance while utilising distinct information flow patterns. Open source code: https://github.com/batu-el/understanding-inductive-biases-of-gnns

### QuZO: Quantized Zeroth-Order Fine-Tuning for Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.12346)] [[cool](https://papers.cool/arxiv/2502.12346)] [[pdf](https://arxiv.org/pdf/2502.12346)]
> **Authors**: Jiajun Zhou,Yifan Yang,Kai Zhen,Ziyue Liu,Yequan Zhao,Ershad Banijamali,Athanasios Mouchtaris,Ngai Wong,Zheng Zhang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Language Models (LLMs) are often quantized to lower precision to reduce the memory cost and latency in inference. However, quantization often degrades model performance, thus fine-tuning is required for various down-stream tasks. Traditional fine-tuning methods such as stochastic gradient descent and Adam optimization require backpropagation, which are error-prone in the low-precision settings. To overcome these limitations, we propose the Quantized Zeroth-Order (QuZO) framework, specifically designed for fine-tuning LLMs through low-precision (e.g., 4- or 8-bit) forward passes. Our method can avoid the error-prone low-precision straight-through estimator, and utilizes optimized stochastic rounding to mitigate the increased bias. QuZO simplifies the training process, while achieving results comparable to first-order methods in ${\rm FP}8$ and superior accuracy in ${\rm INT}8$ and ${\rm INT}4$ training. Experiments demonstrate that low-bit training QuZO achieves performance comparable to MeZO optimization on GLUE, Multi-Choice, and Generation tasks, while reducing memory cost by $2.94 \times$ in LLaMA2-7B fine-tuning compared to quantized first-order methods.

### Understanding Silent Data Corruption in LLM Training 
[[arxiv](https://arxiv.org/abs/2502.12340)] [[cool](https://papers.cool/arxiv/2502.12340)] [[pdf](https://arxiv.org/pdf/2502.12340)]
> **Authors**: Jeffrey Ma,Hengzhi Pei,Leonard Lausen,George Karypis
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,分布式、并行和集群计算
- **Abstract**: As the scale of training large language models (LLMs) increases, one emergent failure is silent data corruption (SDC), where hardware produces incorrect computations without explicit failure signals. In this work, we are the first to investigate the impact of real-world SDCs on LLM training by comparing model training between healthy production nodes and unhealthy nodes exhibiting SDCs. With the help from a cloud computing platform, we access the unhealthy nodes that were swept out from production by automated fleet management. Using deterministic execution via XLA compiler and our proposed synchronization mechanisms, we isolate and analyze the impact of SDC errors on these nodes at three levels: at each submodule computation, at a single optimizer step, and at a training period. Our results reveal that the impact of SDCs on computation varies on different unhealthy nodes. Although in most cases the perturbations from SDCs on submodule computation and gradients are relatively small, SDCs can lead models to converge to different optima with different weights and even cause spikes in the training loss. Our analysis sheds light on further understanding and mitigating the impact of SDCs.

### A Novel Unified Parametric Assumption for Nonconvex Optimization 
[[arxiv](https://arxiv.org/abs/2502.12329)] [[cool](https://papers.cool/arxiv/2502.12329)] [[pdf](https://arxiv.org/pdf/2502.12329)]
> **Authors**: Artem Riabinin,Ahmed Khaled,Peter Richtárik
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,优化与控制,机器学习
- **Abstract**: Nonconvex optimization is central to modern machine learning, but the general framework of nonconvex optimization yields weak convergence guarantees that are too pessimistic compared to practice. On the other hand, while convexity enables efficient optimization, it is of limited applicability to many practical problems. To bridge this gap and better understand the practical success of optimization algorithms in nonconvex settings, we introduce a novel unified parametric assumption. Our assumption is general enough to encompass a broad class of nonconvex functions while also being specific enough to enable the derivation of a unified convergence theorem for gradient-based methods. Notably, by tuning the parameters of our assumption, we demonstrate its versatility in recovering several existing function classes as special cases and in identifying functions amenable to efficient optimization. We derive our convergence theorem for both deterministic and stochastic optimization, and conduct experiments to verify that our assumption can hold practically over optimization trajectories.

### Adversarial Debiasing for Unbiased Parameter Recovery 
[[arxiv](https://arxiv.org/abs/2502.12323)] [[cool](https://papers.cool/arxiv/2502.12323)] [[pdf](https://arxiv.org/pdf/2502.12323)]
> **Authors**: Luke C Sanford,Megan Ayers,Matthew Gordon,Eliana Stone
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 12 pages, 8 figures
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Advances in machine learning and the increasing availability of high-dimensional data have led to the proliferation of social science research that uses the predictions of machine learning models as proxies for measures of human activity or environmental outcomes. However, prediction errors from machine learning models can lead to bias in the estimates of regression coefficients. In this paper, we show how this bias can arise, propose a test for detecting bias, and demonstrate the use of an adversarial machine learning algorithm in order to de-bias predictions. These methods are applicable to any setting where machine-learned predictions are the dependent variable in a regression. We conduct simulations and empirical exercises using ground truth and satellite data on forest cover in Africa. Using the predictions from a naive machine learning model leads to biased parameter estimates, while the predictions from the adversarial model recover the true coefficients.

### Mean-Field Bayesian Optimisation 
[[arxiv](https://arxiv.org/abs/2502.12315)] [[cool](https://papers.cool/arxiv/2502.12315)] [[pdf](https://arxiv.org/pdf/2502.12315)]
> **Authors**: Petar Steinberg,Juliusz Ziomek,Matej Jusup,Ilija Bogunovic
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 16 pages, 5 figures, 2 tables
- **标题**: None
- **领域**: 机器学习,多代理系统
- **Abstract**: We address the problem of optimising the average payoff for a large number of cooperating agents, where the payoff function is unknown and treated as a black box. While standard Bayesian Optimisation (BO) methods struggle with the scalability required for high-dimensional input spaces, we demonstrate how leveraging the mean-field assumption on the black-box function can transform BO into an efficient and scalable solution. Specifically, we introduce MF-GP-UCB, a novel efficient algorithm designed to optimise agent payoffs in this setting. Our theoretical analysis establishes a regret bound for MF-GP-UCB that is independent of the number of agents, contrasting sharply with the exponential dependence observed when naive BO methods are applied. We evaluate our algorithm on a diverse set of tasks, including real-world problems, such as optimising the location of public bikes for a bike-sharing programme, distributing taxi fleets, and selecting refuelling ports for maritime vessels. Empirical results demonstrate that MF-GP-UCB significantly outperforms existing benchmarks, offering substantial improvements in performance and scalability, constituting a promising solution for mean-field, black-box optimisation. The code is available at https://github.com/petarsteinberg/MF-BO.

### Chaotic Map based Compression Approach to Classification 
[[arxiv](https://arxiv.org/abs/2502.12302)] [[cool](https://papers.cool/arxiv/2502.12302)] [[pdf](https://arxiv.org/pdf/2502.12302)]
> **Authors**: Harikrishnan N B,Anuja Vats,Nithin Nagaraj,Marius Pedersen
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 8 pages, 4 figures, 2 tables, 4 algorithms
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Modern machine learning approaches often prioritize performance at the cost of increased complexity, computational demands, and reduced interpretability. This paper introduces a novel framework that challenges this trend by reinterpreting learning from an information-theoretic perspective, viewing it as a search for encoding schemes that capture intrinsic data structures through compact representations. Rather than following the conventional approach of fitting data to complex models, we propose a fundamentally different method that maps data to intervals of initial conditions in a dynamical system. Our GLS (Generalized Lüroth Series) coding compression classifier employs skew tent maps - a class of chaotic maps - both for encoding data into initial conditions and for subsequent recovery. The effectiveness of this simple framework is noteworthy, with performance closely approaching that of well-established machine learning methods. On the breast cancer dataset, our approach achieves 92.98\% accuracy, comparable to Naive Bayes at 94.74\%. While these results do not exceed state-of-the-art performance, the significance of our contribution lies not in outperforming existing methods but in demonstrating that a fundamentally simpler, more interpretable approach can achieve competitive results.

### Per-channel autoregressive linear prediction padding in tiled CNN processing of 2D spatial data 
[[arxiv](https://arxiv.org/abs/2502.12300)] [[cool](https://papers.cool/arxiv/2502.12300)] [[pdf](https://arxiv.org/pdf/2502.12300)]
> **Authors**: Olli Niemitalo,Otto Rosenberg,Nathaniel Narra,Olli Koskela,Iivari Kunttu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 18 pages, 20 figures including appendix; to be submitted for review; for source code, see https://doi.org/10.5281/zenodo.14871260
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: We present linear prediction as a differentiable padding method. For each channel, a stochastic autoregressive linear model is fitted to the padding input by minimizing its noise terms in the least-squares sense. The padding is formed from the expected values of the autoregressive model given the known pixels. We trained the convolutional RVSR super-resolution model from scratch on satellite image data, using different padding methods. Linear prediction padding slightly reduced the mean square super-resolution error compared to zero and replication padding, with a moderate increase in time cost. Linear prediction padding better approximated satellite image data and RVSR feature map data. With zero padding, RVSR appeared to use more of its capacity to compensate for the high approximation error. Cropping the network output by a few pixels reduced the super-resolution error and the effect of the choice of padding method on the error, favoring output cropping with the faster replication and zero padding methods, for the studied workload.

### On the Computational Tractability of the (Many) Shapley Values 
[[arxiv](https://arxiv.org/abs/2502.12295)] [[cool](https://papers.cool/arxiv/2502.12295)] [[pdf](https://arxiv.org/pdf/2502.12295)]
> **Authors**: Reda Marzouk,Shahaf Bassan,Guy Katz,Colin de la Higuera
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: To appear in AISTATS 2025
- **标题**: None
- **领域**: 机器学习,计算复杂度,计算机科学中的逻辑
- **Abstract**: Recent studies have examined the computational complexity of computing Shapley additive explanations (also known as SHAP) across various models and distributions, revealing their tractability or intractability in different settings. However, these studies primarily focused on a specific variant called Conditional SHAP, though many other variants exist and address different limitations. In this work, we analyze the complexity of computing a much broader range of such variants, including Conditional, Interventional, and Baseline SHAP, while exploring both local and global computations. We show that both local and global Interventional and Baseline SHAP can be computed in polynomial time for various ML models under Hidden Markov Model distributions, extending popular algorithms such as TreeSHAP beyond empirical distributions. On the downside, we prove intractability results for these variants over a wide range of neural networks and tree ensembles. We believe that our results emphasize the intricate diversity of computing Shapley values, demonstrating how their complexity is substantially shaped by both the specific SHAP variant, the model type, and the distribution.

### Independence Tests for Language Models 
[[arxiv](https://arxiv.org/abs/2502.12292)] [[cool](https://papers.cool/arxiv/2502.12292)] [[pdf](https://arxiv.org/pdf/2502.12292)]
> **Authors**: Sally Zhu,Ahmed Ahmed,Rohith Kuditipudi,Percy Liang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: We consider the following problem: given the weights of two models, can we test whether they were trained independently -- i.e., from independent random initializations? We consider two settings: constrained and unconstrained. In the constrained setting, we make assumptions about model architecture and training and propose a family of statistical tests that yield exact p-values with respect to the null hypothesis that the models are trained from independent random initializations. These p-values are valid regardless of the composition of either model's training data; we compute them by simulating exchangeable copies of each model under our assumptions and comparing various similarity measures of weights and activations between the original two models versus these copies. We report the p-values from these tests on pairs of 21 open-weight models (210 total pairs) and correctly identify all pairs of non-independent models. Our tests remain effective even if one model was fine-tuned for many tokens. In the unconstrained setting, where we make no assumptions about training procedures, can change model architecture, and allow for adversarial evasion attacks, the previous tests no longer work. Instead, we propose a new test which matches hidden activations between two models, and which is robust to adversarial transformations and to changes in model architecture. The test can also do localized testing: identifying specific non-independent components of models. Though we no longer obtain exact p-values from this, empirically we find it behaves as one and reliably identifies non-independent models. Notably, we can use the test to identify specific parts of one model that are derived from another (e.g., how Llama 3.1-8B was pruned to initialize Llama 3.2-3B, or shared layers between Mistral-7B and StripedHyena-7B), and it is even robust to retraining individual layers of either model from scratch.

### Healthcare cost prediction for heterogeneous patient profiles using deep learning models with administrative claims data 
[[arxiv](https://arxiv.org/abs/2502.12277)] [[cool](https://papers.cool/arxiv/2502.12277)] [[pdf](https://arxiv.org/pdf/2502.12277)]
> **Authors**: Mohammad Amin Morid,Olivia R. Liu Sheng
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: ef:Information Systems Research (forthcoming 2025)
- **标题**: None
- **领域**: 机器学习,计算机与社会
- **Abstract**: Problem: How can we design patient cost prediction models that effectively address the challenges of heterogeneity in administrative claims (AC) data to ensure accurate, fair, and generalizable predictions, especially for high-need (HN) patients with complex chronic conditions? Relevance: Accurate and equitable patient cost predictions are vital for developing health management policies and optimizing resource allocation, which can lead to significant cost savings for healthcare payers, including government agencies and private insurers. Addressing disparities in prediction outcomes for HN patients ensures better economic and clinical decision-making, benefiting both patients and payers. Methodology: This study is grounded in socio-technical considerations that emphasize the interplay between technical systems (e.g., deep learning models) and humanistic outcomes (e.g., fairness in healthcare decisions). It incorporates representation learning and entropy measurement to address heterogeneity and complexity in data and patient profiles, particularly for HN patients. We propose a channel-wise deep learning framework that mitigates data heterogeneity by segmenting AC data into separate channels based on types of codes (e.g., diagnosis, procedures) and costs. This approach is paired with a flexible evaluation design that uses multi-channel entropy measurement to assess patient heterogeneity. Results: The proposed channel-wise models reduce prediction errors by 23% compared to single-channel models, leading to 16.4% and 19.3% reductions in overpayments and underpayments, respectively. Notably, the reduction in prediction bias is significantly higher for HN patients, demonstrating effectiveness in handling heterogeneity and complexity in data and patient profiles. This demonstrates the potential for applying channel-wise modeling to domains with similar heterogeneity challenges.

### Learning to Reason at the Frontier of Learnability 
[[arxiv](https://arxiv.org/abs/2502.12272)] [[cool](https://papers.cool/arxiv/2502.12272)] [[pdf](https://arxiv.org/pdf/2502.12272)]
> **Authors**: Thomas Foster,Jakob Foerster
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Reinforcement learning is now widely adopted as the final stage of large language model training, especially for reasoning-style tasks such as maths problems. Typically, models attempt each question many times during a single training step and attempt to learn from their successes and failures. However, we demonstrate that throughout training with two popular algorithms (PPO and VinePPO) on two widely used datasets, many questions are either solved by all attempts - meaning they are already learned - or by none - providing no meaningful training signal. To address this, we adapt a method from the reinforcement learning literature - sampling for learnability - and apply it to the reinforcement learning stage of LLM training. Our curriculum prioritises questions with high variance of success, i.e. those where the agent sometimes succeeds, but not always. Our findings demonstrate that this curriculum consistently boosts training performance across multiple algorithms and datasets, paving the way for more efficient and effective reinforcement learning with LLMs.

### Identifying the Best Transition Law 
[[arxiv](https://arxiv.org/abs/2502.12227)] [[cool](https://papers.cool/arxiv/2502.12227)] [[pdf](https://arxiv.org/pdf/2502.12227)]
> **Authors**: Mehrasa Ahmadipour,élise Crepon,Aurélien Garivier
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Motivated by recursive learning in Markov Decision Processes, this paper studies best-arm identification in bandit problems where each arm's reward is drawn from a multinomial distribution with a known support. We compare the performance { reached by strategies including notably LUCB without and with use of this knowledge. } In the first case, we use classical non-parametric approaches for the confidence intervals. In the second case, where a probability distribution is to be estimated, we first use classical deviation bounds (Hoeffding and Bernstein) on each dimension independently, and then the Empirical Likelihood method (EL-LUCB) on the joint probability vector. The effectiveness of these methods is demonstrated through simulations on scenarios with varying levels of structural complexity.

### On Creating a Causally Grounded Usable Rating Method for Assessing the Robustness of Foundation Models Supporting Time Series 
[[arxiv](https://arxiv.org/abs/2502.12226)] [[cool](https://papers.cool/arxiv/2502.12226)] [[pdf](https://arxiv.org/pdf/2502.12226)]
> **Authors**: Kausik Lakkaraju,Rachneet Kaur,Parisa Zehtabi,Sunandita Patra,Siva Likitha Valluru,Zhen Zeng,Biplav Srivastava,Marco Valtorta
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Foundation Models (FMs) have improved time series forecasting in various sectors, such as finance, but their vulnerability to input disturbances can hinder their adoption by stakeholders, such as investors and analysts. To address this, we propose a causally grounded rating framework to study the robustness of Foundational Models for Time Series (FMTS) with respect to input perturbations. We evaluate our approach to the stock price prediction problem, a well-studied problem with easily accessible public data, evaluating six state-of-the-art (some multi-modal) FMTS across six prominent stocks spanning three industries. The ratings proposed by our framework effectively assess the robustness of FMTS and also offer actionable insights for model selection and deployment. Within the scope of our study, we find that (1) multi-modal FMTS exhibit better robustness and accuracy compared to their uni-modal versions and, (2) FMTS pre-trained on time series forecasting task exhibit better robustness and forecasting accuracy compared to general-purpose FMTS pre-trained across diverse settings. Further, to validate our framework's usability, we conduct a user study showcasing FMTS prediction errors along with our computed ratings. The study confirmed that our ratings reduced the difficulty for users in comparing the robustness of different systems.

### Subjective Logic Encodings 
[[arxiv](https://arxiv.org/abs/2502.12225)] [[cool](https://papers.cool/arxiv/2502.12225)] [[pdf](https://arxiv.org/pdf/2502.12225)]
> **Authors**: Jake Vasilakes
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: We make our code publicly available at https://github.com/jvasilakes/SLEncodings
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Many existing approaches for learning from labeled data assume the existence of gold-standard labels. According to these approaches, inter-annotator disagreement is seen as noise to be removed, either through refinement of annotation guidelines, label adjudication, or label filtering. However, annotator disagreement can rarely be totally eradicated, especially on more subjective tasks such as sentiment analysis or hate speech detection where disagreement is natural. Therefore, a new approach to learning from labeled data, called data perspectivism, seeks to leverage inter-annotator disagreement to learn models that stay true to the inherent uncertainty of the task by treating annotations as opinions of the annotators, rather than gold-standard facts. Despite this conceptual grounding, existing methods under data perspectivism are limited to using disagreement as the sole source of annotation uncertainty. To expand the possibilities of data perspectivism, we introduce Subjective Logic Encodings (SLEs), a flexible framework for constructing classification targets that explicitly encodes annotations as opinions of the annotators. Based on Subjective Logic Theory, SLEs encode labels as Dirichlet distributions and provide principled methods for encoding and aggregating various types of annotation uncertainty -- annotator confidence, reliability, and disagreement -- into the targets. We show that SLEs are a generalization of other types of label encodings as well as how to estimate models to predict SLEs using a distribution matching objective.

### IMPACTX: Improving Model Performance by Appropriately predicting CorrecT eXplanations 
[[arxiv](https://arxiv.org/abs/2502.12222)] [[cool](https://papers.cool/arxiv/2502.12222)] [[pdf](https://arxiv.org/pdf/2502.12222)]
> **Authors**: Andrea Apicella,Salvatore Giugliano,Francesco Isgrò,Roberto Prevete
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: in peer review
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: The eXplainable Artificial Intelligence (XAI) research predominantly concentrates to provide explainations about AI model decisions, especially Deep Learning (DL) models. However, there is a growing interest in using XAI techniques to automatically improve the performance of the AI systems themselves. This paper proposes IMPACTX, a novel approach that leverages XAI as a fully automated attention mechanism, without requiring external knowledge or human feedback. Experimental results show that IMPACTX has improved performance respect to the standalone ML model by integrating an attention mechanism based an XAI method outputs during the model training. Furthermore, IMPACTX directly provides proper feature attribution maps for the model's decisions, without relying on external XAI methods during the inference process. Our proposal is evaluated using three widely recognized DL models (EfficientNet-B2, MobileNet, and LeNet-5) along with three standard image datasets: CIFAR-10, CIFAR-100, and STL-10. The results show that IMPACTX consistently improves the performance of all the inspected DL models across all evaluated datasets, and it directly provides appropriate explanations for its responses.

### Optimal Brain Iterative Merging: Mitigating Interference in LLM Merging 
[[arxiv](https://arxiv.org/abs/2502.12217)] [[cool](https://papers.cool/arxiv/2502.12217)] [[pdf](https://arxiv.org/pdf/2502.12217)]
> **Authors**: Zhixiang Wang,Zhenyu Mao,Yixuan Qiao,Yunfang Wu,Biye Li
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Large Language Models (LLMs) have demonstrated impressive capabilities, but their high computational costs pose challenges for customization. Model merging offers a cost-effective alternative, yet existing methods suffer from interference among parameters, leading to performance degradation. In this work, we propose Optimal Brain Iterative Merging (OBIM), a novel method designed to mitigate both intra-model and inter-model interference. OBIM consists of two key components: (1) A saliency measurement mechanism that evaluates parameter importance based on loss changes induced by individual weight alterations, reducing intra-model interference by preserving only high-saliency parameters. (2) A mutually exclusive iterative merging framework, which incrementally integrates models using a binary mask to avoid direct parameter averaging, thereby mitigating inter-model interference. We validate OBIM through experiments on both Supervised Fine-Tuned (SFT) models and post-pretrained checkpoints. The results show that OBIM significantly outperforms existing merging techniques. Overall, OBIM provides an effective and practical solution for enhancing LLM merging.

### Tactic: Adaptive Sparse Attention with Clustering and Distribution Fitting for Long-Context LLMs 
[[arxiv](https://arxiv.org/abs/2502.12216)] [[cool](https://papers.cool/arxiv/2502.12216)] [[pdf](https://arxiv.org/pdf/2502.12216)]
> **Authors**: Kan Zhu,Tian Tang,Qinyu Xu,Yile Gu,Zhichen Zeng,Rohan Kadekodi,Liangyu Zhao,Ang Li,Arvind Krishnamurthy,Baris Kasikci
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Long-context models are essential for many applications but face inefficiencies in loading large KV caches during decoding. Prior methods enforce fixed token budgets for sparse attention, assuming a set number of tokens can approximate full attention. However, these methods overlook variations in the importance of attention across heads, layers, and contexts. To address these limitations, we propose Tactic, a sparsity-adaptive and calibration-free sparse attention mechanism that dynamically selects tokens based on their cumulative attention scores rather than a fixed token budget. By setting a target fraction of total attention scores, Tactic ensures that token selection naturally adapts to variations in attention sparsity. To efficiently approximate this selection, Tactic leverages clustering-based sorting and distribution fitting, allowing it to accurately estimate token importance with minimal computational overhead. We show that Tactic outperforms existing sparse attention algorithms, achieving superior accuracy and up to 7.29x decode attention speedup. This improvement translates to an overall 1.58x end-to-end inference speedup, making Tactic a practical and effective solution for long-context LLM inference in accuracy-sensitive applications.

### Revisiting the Test-Time Scaling of o1-like Models: Do they Truly Possess Test-Time Scaling Capabilities? 
[[arxiv](https://arxiv.org/abs/2502.12215)] [[cool](https://papers.cool/arxiv/2502.12215)] [[pdf](https://arxiv.org/pdf/2502.12215)]
> **Authors**: Zhiyuan Zeng,Qinyuan Cheng,Zhangyue Yin,Yunhua Zhou,Xipeng Qiu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: The advent of test-time scaling in large language models (LLMs), exemplified by OpenAI's o1 series, has advanced reasoning capabilities by scaling computational resource allocation during inference. While successors like QwQ, Deepseek-R1 (R1) and LIMO replicate these advancements, whether these models truly possess test-time scaling capabilities remains underexplored. This study found that longer CoTs of these o1-like models do not consistently enhance accuracy; in fact, correct solutions are often shorter than incorrect ones for the same questions. Further investigation shows this phenomenon is closely related to models' self-revision capabilities - longer CoTs contain more self-revisions, which often lead to performance degradation. We then compare sequential and parallel scaling strategies on QwQ, R1 and LIMO, finding that parallel scaling achieves better coverage and scalability. Based on these insights, we propose Shortest Majority Vote, a method that combines parallel scaling strategies with CoT length characteristics, significantly improving models' test-time scalability compared to conventional majority voting approaches.

### Spatiotemporal-aware Trend-Seasonality Decomposition Network for Traffic Flow Forecasting 
[[arxiv](https://arxiv.org/abs/2502.12213)] [[cool](https://papers.cool/arxiv/2502.12213)] [[pdf](https://arxiv.org/pdf/2502.12213)]
> **Authors**: Lingxiao Cao,Bin Wang,Guiyuan Jiang,Yanwei Yu,Junyu Dong
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Traffic prediction is critical for optimizing travel scheduling and enhancing public safety, yet the complex spatial and temporal dynamics within traffic data present significant challenges for accurate forecasting. In this paper, we introduce a novel model, the Spatiotemporal-aware Trend-Seasonality Decomposition Network (STDN). This model begins by constructing a dynamic graph structure to represent traffic flow and incorporates novel spatio-temporal embeddings to jointly capture global traffic dynamics. The representations learned are further refined by a specially designed trend-seasonality decomposition module, which disentangles the trend-cyclical component and seasonal component for each traffic node at different times within the graph. These components are subsequently processed through an encoder-decoder network to generate the final predictions. Extensive experiments conducted on real-world traffic datasets demonstrate that STDN achieves superior performance with remarkable computation cost. Furthermore, we have released a new traffic dataset named JiNan, which features unique inner-city dynamics, thereby enriching the scenario comprehensiveness in traffic prediction evaluation.

### PAR-AdvGAN: Improving Adversarial Attack Capability with Progressive Auto-Regression AdvGAN 
[[arxiv](https://arxiv.org/abs/2502.12207)] [[cool](https://papers.cool/arxiv/2502.12207)] [[pdf](https://arxiv.org/pdf/2502.12207)]
> **Authors**: Jiayu Zhang,Zhiyu Zhu,Xinyi Wang,Silin Liao,Zhibo Jin,Flora D. Salim,Huaming Chen
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Deep neural networks have demonstrated remarkable performance across various domains. However, they are vulnerable to adversarial examples, which can lead to erroneous predictions. Generative Adversarial Networks (GANs) can leverage the generators and discriminators model to quickly produce high-quality adversarial examples. Since both modules train in a competitive and simultaneous manner, GAN-based algorithms like AdvGAN can generate adversarial examples with better transferability compared to traditional methods. However, the generation of perturbations is usually limited to a single iteration, preventing these examples from fully exploiting the potential of the methods. To tackle this issue, we introduce a novel approach named Progressive Auto-Regression AdvGAN (PAR-AdvGAN). It incorporates an auto-regressive iteration mechanism within a progressive generation network to craft adversarial examples with enhanced attack capability. We thoroughly evaluate our PAR-AdvGAN method with a large-scale experiment, demonstrating its superior performance over various state-of-the-art black-box adversarial attacks, as well as the original AdvGAN.Moreover, PAR-AdvGAN significantly accelerates the adversarial example generation, i.e., achieving the speeds of up to 335.5 frames per second on Inception-v3 model, outperforming the gradient-based transferable attack algorithms. Our code is available at: https://anonymous.4open.science/r/PAR-01BF/

### An Interpretable Automated Mechanism Design Framework with Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.12203)] [[cool](https://papers.cool/arxiv/2502.12203)] [[pdf](https://arxiv.org/pdf/2502.12203)]
> **Authors**: Jiayuan Liu,Mingyu Guo,Vincent Conitzer
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机科学与博弈论,神经和进化计算
- **Abstract**: Mechanism design has long been a cornerstone of economic theory, with traditional approaches relying on mathematical derivations. Recently, automated approaches, including differentiable economics with neural networks, have emerged for designing payments and allocations. While both analytical and automated methods have advanced the field, they each face significant weaknesses: mathematical derivations are not automated and often struggle to scale to complex problems, while automated and especially neural-network-based approaches suffer from limited interpretability. To address these challenges, we introduce a novel framework that reformulates mechanism design as a code generation task. Using large language models (LLMs), we generate heuristic mechanisms described in code and evolve them to optimize over some evaluation metrics while ensuring key design criteria (e.g., strategy-proofness) through a problem-specific fixing process. This fixing process ensures any mechanism violating the design criteria is adjusted to satisfy them, albeit with some trade-offs in performance metrics. These trade-offs are factored in during the LLM-based evolution process. The code generation capabilities of LLMs enable the discovery of novel and interpretable solutions, bridging the symbolic logic of mechanism design and the generative power of modern AI. Through rigorous experimentation, we demonstrate that LLM-generated mechanisms achieve competitive performance while offering greater interpretability compared to previous approaches. Notably, our framework can rediscover existing manually designed mechanisms and provide insights into neural-network based solutions through Programming-by-Example. These results highlight the potential of LLMs to not only automate but also enhance the transparency and scalability of mechanism design, ensuring safe deployment of the mechanisms in society.

### Maximize Your Diffusion: A Study into Reward Maximization and Alignment for Diffusion-based Control 
[[arxiv](https://arxiv.org/abs/2502.12198)] [[cool](https://papers.cool/arxiv/2502.12198)] [[pdf](https://arxiv.org/pdf/2502.12198)]
> **Authors**: Dom Huh,Prasant Mohapatra
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Diffusion-based planning, learning, and control methods present a promising branch of powerful and expressive decision-making solutions. Given the growing interest, such methods have undergone numerous refinements over the past years. However, despite these advancements, existing methods are limited in their investigations regarding general methods for reward maximization within the decision-making process. In this work, we study extensions of fine-tuning approaches for control applications. Specifically, we explore extensions and various design choices for four fine-tuning approaches: reward alignment through reinforcement learning, direct preference optimization, supervised fine-tuning, and cascading diffusion. We optimize their usage to merge these independent efforts into one unified paradigm. We show the utility of such propositions in offline RL settings and demonstrate empirical improvements over a rich array of control tasks.

### GeneralizeFormer: Layer-Adaptive Model Generation across Test-Time Distribution Shifts 
[[arxiv](https://arxiv.org/abs/2502.12195)] [[cool](https://papers.cool/arxiv/2502.12195)] [[pdf](https://arxiv.org/pdf/2502.12195)]
> **Authors**: Sameer Ambekar,Zehao Xiao,Xiantong Zhen,Cees G. M. Snoek
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-18
> **comment**: WACV 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We consider the problem of test-time domain generalization, where a model is trained on several source domains and adjusted on target domains never seen during training. Different from the common methods that fine-tune the model or adjust the classifier parameters online, we propose to generate multiple layer parameters on the fly during inference by a lightweight meta-learned transformer, which we call \textit{GeneralizeFormer}. The layer-wise parameters are generated per target batch without fine-tuning or online adjustment. By doing so, our method is more effective in dynamic scenarios with multiple target distributions and also avoids forgetting valuable source distribution characteristics. Moreover, by considering layer-wise gradients, the proposed method adapts itself to various distribution shifts. To reduce the computational and time cost, we fix the convolutional parameters while only generating parameters of the Batch Normalization layers and the linear classifier. Experiments on six widely used domain generalization datasets demonstrate the benefits and abilities of the proposed method to efficiently handle various distribution shifts, generalize in dynamic scenarios, and avoid forgetting.

### AnyTouch: Learning Unified Static-Dynamic Representation across Multiple Visuo-tactile Sensors 
[[arxiv](https://arxiv.org/abs/2502.12191)] [[cool](https://papers.cool/arxiv/2502.12191)] [[pdf](https://arxiv.org/pdf/2502.12191)]
> **Authors**: Ruoxuan Feng,Jiangyu Hu,Wenke Xia,Tianci Gao,Ao Shen,Yuhao Sun,Bin Fang,Di Hu
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-18
> **comment**: Accepted by ICLR 2025
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别,机器人技术
- **Abstract**: Visuo-tactile sensors aim to emulate human tactile perception, enabling robots to precisely understand and manipulate objects. Over time, numerous meticulously designed visuo-tactile sensors have been integrated into robotic systems, aiding in completing various tasks. However, the distinct data characteristics of these low-standardized visuo-tactile sensors hinder the establishment of a powerful tactile perception system. We consider that the key to addressing this issue lies in learning unified multi-sensor representations, thereby integrating the sensors and promoting tactile knowledge transfer between them. To achieve unified representation of this nature, we introduce TacQuad, an aligned multi-modal multi-sensor tactile dataset from four different visuo-tactile sensors, which enables the explicit integration of various sensors. Recognizing that humans perceive the physical environment by acquiring diverse tactile information such as texture and pressure changes, we further propose to learn unified multi-sensor representations from both static and dynamic perspectives. By integrating tactile images and videos, we present AnyTouch, a unified static-dynamic multi-sensor representation learning framework with a multi-level structure, aimed at both enhancing comprehensive perceptual abilities and enabling effective cross-sensor transfer. This multi-level architecture captures pixel-level details from tactile data via masked modeling and enhances perception and transferability by learning semantic-level sensor-agnostic features through multi-modal alignment and cross-sensor matching. We provide a comprehensive analysis of multi-sensor transferability, and validate our method on various datasets and in the real-world pouring task. Experimental results show that our method outperforms existing methods, exhibits outstanding static and dynamic perception capabilities across various sensors.

### Boosting Generalization in Diffusion-Based Neural Combinatorial Solver via Energy-guided Sampling 
[[arxiv](https://arxiv.org/abs/2502.12188)] [[cool](https://papers.cool/arxiv/2502.12188)] [[pdf](https://arxiv.org/pdf/2502.12188)]
> **Authors**: Haoyu Lei,Kaiwen Zhou,Yinchuan Li,Zhitang Chen,Farzan Farnia
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Diffusion-based Neural Combinatorial Optimization (NCO) has demonstrated effectiveness in solving NP-complete (NPC) problems by learning discrete diffusion models for solution generation, eliminating hand-crafted domain knowledge. Despite their success, existing NCO methods face significant challenges in both cross-scale and cross-problem generalization, and high training costs compared to traditional solvers. While recent studies have introduced training-free guidance approaches that leverage pre-defined guidance functions for zero-shot conditional generation, such methodologies have not been extensively explored in combinatorial optimization. To bridge this gap, we propose a general energy-guided sampling framework during inference time that enhances both the cross-scale and cross-problem generalization capabilities of diffusion-based NCO solvers without requiring additional training. We provide theoretical analysis that helps understanding the cross-problem transfer capability. Our experimental results demonstrate that a diffusion solver, trained exclusively on the Traveling Salesman Problem (TSP), can achieve competitive zero-shot solution generation on TSP variants, such as Prize Collecting TSP (PCTSP) and the Orienteering Problem (OP), through energy-guided sampling across different problem scales.

### E2CB2former: Effecitve and Explainable Transformer for CB2 Receptor Ligand Activity Prediction 
[[arxiv](https://arxiv.org/abs/2502.12186)] [[cool](https://papers.cool/arxiv/2502.12186)] [[pdf](https://arxiv.org/pdf/2502.12186)]
> **Authors**: Jiacheng Xie,Yingrui Ji,Linghuan Zeng,Xi Xiao,Gaofei Chen,Lijing Zhu,Joyanta Jyoti Mondal,Jiansheng Chen
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,定量方法
- **Abstract**: Accurate prediction of CB2 receptor ligand activity is pivotal for advancing drug discovery targeting this receptor, which is implicated in inflammation, pain management, and neurodegenerative conditions. Although conventional machine learning and deep learning techniques have shown promise, their limited interpretability remains a significant barrier to rational drug design. In this work, we introduce CB2former, a framework that combines a Graph Convolutional Network with a Transformer architecture to predict CB2 receptor ligand activity. By leveraging the Transformer's self attention mechanism alongside the GCN's structural learning capability, CB2former not only enhances predictive performance but also offers insights into the molecular features underlying receptor activity. We benchmark CB2former against diverse baseline models including Random Forest, Support Vector Machine, K Nearest Neighbors, Gradient Boosting, Extreme Gradient Boosting, Multilayer Perceptron, Convolutional Neural Network, and Recurrent Neural Network and demonstrate its superior performance with an R squared of 0.685, an RMSE of 0.675, and an AUC of 0.940. Moreover, attention weight analysis reveals key molecular substructures influencing CB2 receptor activity, underscoring the model's potential as an interpretable AI tool for drug discovery. This ability to pinpoint critical molecular motifs can streamline virtual screening, guide lead optimization, and expedite therapeutic development. Overall, our results showcase the transformative potential of advanced AI approaches exemplified by CB2former in delivering both accurate predictions and actionable molecular insights, thus fostering interdisciplinary collaboration and innovation in drug discovery.

### Identifiable Steering via Sparse Autoencoding of Multi-Concept Shifts 
[[arxiv](https://arxiv.org/abs/2502.12179)] [[cool](https://papers.cool/arxiv/2502.12179)] [[pdf](https://arxiv.org/pdf/2502.12179)]
> **Authors**: Shruti Joshi,Andrea Dittadi,Sébastien Lachapelle,Dhanya Sridhar
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-18
> **comment**: 27 pages, 9 figures
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Steering methods manipulate the representations of large language models (LLMs) to induce responses that have desired properties, e.g., truthfulness, offering a promising approach for LLM alignment without the need for fine-tuning. Traditionally, steering has relied on supervision, such as from contrastive pairs of prompts that vary in a single target concept, which is costly to obtain and limits the speed of steering research. An appealing alternative is to use unsupervised approaches such as sparse autoencoders (SAEs) to map LLM embeddings to sparse representations that capture human-interpretable concepts. However, without further assumptions, SAEs may not be identifiable: they could learn latent dimensions that entangle multiple concepts, leading to unintentional steering of unrelated properties. We introduce Sparse Shift Autoencoders (SSAEs) that instead map the differences between embeddings to sparse representations. Crucially, we show that SSAEs are identifiable from paired observations that vary in \textit{multiple unknown concepts}, leading to accurate steering of single concepts without the need for supervision. We empirically demonstrate accurate steering across semi-synthetic and real-world language datasets using Llama-3.1 embeddings.

### Direct Preference Optimization-Enhanced Multi-Guided Diffusion Model for Traffic Scenario Generation 
[[arxiv](https://arxiv.org/abs/2502.12178)] [[cool](https://papers.cool/arxiv/2502.12178)] [[pdf](https://arxiv.org/pdf/2502.12178)]
> **Authors**: Seungjun Yu,Kisung Kim,Daejung Kim,Haewook Han,Jinhan Lee
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,多代理系统
- **Abstract**: Diffusion-based models are recognized for their effectiveness in using real-world driving data to generate realistic and diverse traffic scenarios. These models employ guided sampling to incorporate specific traffic preferences and enhance scenario realism. However, guiding the sampling process to conform to traffic rules and preferences can result in deviations from real-world traffic priors and potentially leading to unrealistic behaviors. To address this challenge, we introduce a multi-guided diffusion model that utilizes a novel training strategy to closely adhere to traffic priors, even when employing various combinations of guides. This model adopts a multi-task learning framework, enabling a single diffusion model to process various guide inputs. For increased guided sampling precision, our model is fine-tuned using the Direct Preference Optimization (DPO) algorithm. This algorithm optimizes preferences based on guide scores, effectively navigating the complexities and challenges associated with the expensive and often non-differentiable gradient calculations during the guided sampling fine-tuning process. Evaluated using the nuScenes dataset our model provides a strong baseline for balancing realism, diversity and controllability in the traffic scenario generation.

### Recent Advances of NeuroDiffEq -- An Open-Source Library for Physics-Informed Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.12177)] [[cool](https://papers.cool/arxiv/2502.12177)] [[pdf](https://arxiv.org/pdf/2502.12177)]
> **Authors**: Shuheng Liu,Pavlos Protopapas,David Sondak,Feiyu Chen
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-18
> **comment**: 13 pages, 6 figures, submitted to Journal of Open Research Software
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Solving differential equations is a critical challenge across a host of domains. While many software packages efficiently solve these equations using classical numerical approaches, there has been less effort in developing a library for researchers interested in solving such systems using neural networks. With PyTorch as its backend, NeuroDiffEq is a software library that exploits neural networks to solve differential equations. In this paper, we highlight the latest features of the NeuroDiffEq library since its debut. We show that NeuroDiffEq can solve complex boundary value problems in arbitrary dimensions, tackle boundary conditions at infinity, and maintain flexibility for dynamic injection at runtime.

### Ten Challenging Problems in Federated Foundation Models 
[[arxiv](https://arxiv.org/abs/2502.12176)] [[cool](https://papers.cool/arxiv/2502.12176)] [[pdf](https://arxiv.org/pdf/2502.12176)]
> **Authors**: Tao Fan,Hanlin Gu,Xuemei Cao,Chee Seng Chan,Qian Chen,Yiqiang Chen,Yihui Feng,Yang Gu,Jiaxiang Geng,Bing Luo,Shuoling Liu,Win Kent Ong,Chao Ren,Jiaqi Shao,Chuan Sun,Xiaoli Tang,Hong Xi Tae,Yongxin Tong,Shuyue Wei,Fan Wu,Wei Xi,Mingcong Xu,He Yang,Xin Yang,Jiangpeng Yan, et al. (8 additional authors not shown)
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Federated Foundation Models (FedFMs) represent a distributed learning paradigm that fuses general competences of foundation models as well as privacy-preserving capabilities of federated learning. This combination allows the large foundation models and the small local domain models at the remote clients to learn from each other in a teacher-student learning setting. This paper provides a comprehensive summary of the ten challenging problems inherent in FedFMs, encompassing foundational theory, utilization of private data, continual learning, unlearning, Non-IID and graph data, bidirectional knowledge transfer, incentive mechanism design, game mechanism design, model watermarking, and efficiency. The ten challenging problems manifest in five pivotal aspects: ``Foundational Theory," which aims to establish a coherent and unifying theoretical framework for FedFMs. ``Data," addressing the difficulties in leveraging domain-specific knowledge from private data while maintaining privacy; ``Heterogeneity," examining variations in data, model, and computational resources across clients; ``Security and Privacy," focusing on defenses against malicious attacks and model theft; and ``Efficiency," highlighting the need for improvements in training, communication, and parameter efficiency. For each problem, we offer a clear mathematical definition on the objective function, analyze existing methods, and discuss the key challenges and potential solutions. This in-depth exploration aims to advance the theoretical foundations of FedFMs, guide practical implementations, and inspire future research to overcome these obstacles, thereby enabling the robust, efficient, and privacy-preserving FedFMs in various real-world applications.

### Spatiotemporal Graph Neural Networks in short term load forecasting: Does adding Graph Structure in Consumption Data Improve Predictions? 
[[arxiv](https://arxiv.org/abs/2502.12175)] [[cool](https://papers.cool/arxiv/2502.12175)] [[pdf](https://arxiv.org/pdf/2502.12175)]
> **Authors**: Quoc Viet Nguyen,Joaquin Delgado Fernandez,Sergio Potenciano Menci
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-18
> **comment**: 13 pages, conference
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Short term Load Forecasting (STLF) plays an important role in traditional and modern power systems. Most STLF models predominantly exploit temporal dependencies from historical data to predict future consumption. Nowadays, with the widespread deployment of smart meters, their data can contain spatiotemporal dependencies. In particular, their consumption data is not only correlated to historical values but also to the values of neighboring smart meters. This new characteristic motivates researchers to explore and experiment with new models that can effectively integrate spatiotemporal interrelations to increase forecasting performance. Spatiotemporal Graph Neural Networks (STGNNs) can leverage such interrelations by modeling relationships between smart meters as a graph and using these relationships as additional features to predict future energy consumption. While extensively studied in other spatiotemporal forecasting domains such as traffic, environments, or renewable energy generation, their application to load forecasting remains relatively unexplored, particularly in scenarios where the graph structure is not inherently available. This paper overviews the current literature focusing on STGNNs with application in STLF. Additionally, from a technical perspective, it also benchmarks selected STGNN models for STLF at the residential and aggregate levels. The results indicate that incorporating graph features can improve forecasting accuracy at the residential level; however, this effect is not reflected at the aggregate level

### nanoML for Human Activity Recognition 
[[arxiv](https://arxiv.org/abs/2502.12173)] [[cool](https://papers.cool/arxiv/2502.12173)] [[pdf](https://arxiv.org/pdf/2502.12173)]
> **Authors**: Alan T. L. Bacellar,Mugdha P. Jadhao,Shashank Nag,Priscila M. V. Lima,Felipe M. G. Franca,Lizy K. John
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-18
> **comment**: Accepted as a full paper by the 2025 EDGEAIFOUNDATION Austin
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Human Activity Recognition (HAR) is critical for applications in healthcare, fitness, and IoT, but deploying accurate models on resource-constrained devices remains challenging due to high energy and memory demands. This paper demonstrates the application of Differentiable Weightless Neural Networks (DWNs) to HAR, achieving competitive accuracies of 96.34% and 96.67% while consuming only 56nJ and 104nJ per sample, with an inference time of just 5ns per sample. The DWNs were implemented and evaluated on an FPGA, showcasing their practical feasibility for energy-efficient hardware deployment. DWNs achieve up to 926,000x energy savings and 260x memory reduction compared to state-of-the-art deep learning methods. These results position DWNs as a nano-machine learning nanoML model for HAR, setting a new benchmark in energy efficiency and compactness for edge and wearable devices, paving the way for ultra-efficient edge AI.

### GoRA: Gradient-driven Adaptive Low Rank Adaptation 
[[arxiv](https://arxiv.org/abs/2502.12171)] [[cool](https://papers.cool/arxiv/2502.12171)] [[pdf](https://arxiv.org/pdf/2502.12171)]
> **Authors**: Haonan He,Peng Ye,Yuchen Ren,Yuan Yuan,Lei Chen
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Low-Rank Adaptation (LoRA) is a crucial method for efficiently fine-tuning pretrained large language models (LLMs), with its performance largely influenced by two key factors: rank and initialization strategy. Numerous LoRA variants have been proposed to enhance its performance by addressing these factors. However, these variants often compromise LoRA's usability or efficiency. In this paper, we analyze the fundamental limitations of existing methods and introduce a novel approach, GoRA (Gradient-driven Adaptive Low Rank Adaptation), which adaptively assigns ranks and initializes weights for low-rank adapters simultaneously based on gradient information. Extensive experimental results demonstrate that GoRA significantly improves performance while preserving the high usability and efficiency of LoRA. On the T5 model fine-tuned for the GLUE benchmark, GoRA achieves a 5.88-point improvement over LoRA and slightly surpasses full fine-tuning. Similarly, on the Llama3.1-8B-Base model fine-tuned for GSM8k tasks, GoRA outperforms LoRA with a 5.13-point improvement and exceeds full fine-tuning in high-rank settings by a margin of 2.05 points.

### MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway Dynamic Dense Connections 
[[arxiv](https://arxiv.org/abs/2502.12170)] [[cool](https://papers.cool/arxiv/2502.12170)] [[pdf](https://arxiv.org/pdf/2502.12170)]
> **Authors**: Da Xiao,Qingye Meng,Shengping Li,Xingyuan Yuan
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: We propose MUltiway Dynamic Dense (MUDD) connections, a simple yet effective method to address the limitations of residual connections and enhance cross-layer information flow in Transformers. Unlike existing dense connection approaches with static and shared connection weights, MUDD generates connection weights dynamically depending on hidden states at each sequence position and for each decoupled input stream (the query, key, value or residual) of a Transformer block. MUDD connections can be seamlessly integrated into any Transformer architecture to create MUDDFormer. Extensive experiments show that MUDDFormer significantly outperforms Transformers across various model architectures and scales in language modeling, achieving the performance of Transformers trained with 1.8X-2.4X compute. Notably, MUDDPythia-2.8B matches Pythia-6.9B in pretraining ppl and downstream tasks and even rivals Pythia-12B in five-shot settings, while adding only 0.23% parameters and 0.4% computation. Code in JAX and PyTorch and pre-trained models are available at https://github.com/Caiyun-AI/MUDDFormer .

### CFIRSTNET: Comprehensive Features for Static IR Drop Estimation with Neural Network 
[[arxiv](https://arxiv.org/abs/2502.12168)] [[cool](https://papers.cool/arxiv/2502.12168)] [[pdf](https://arxiv.org/pdf/2502.12168)]
> **Authors**: Yu-Tung Liu,Yu-Hao Cheng,Shao-Yu Wu,Hung-Ming Chen
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-18
> **comment**: IEEE/ACM International Conference on Computer-Aided Design (ICCAD '24), October 27--31, 2024
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: IR drop estimation is now considered a first-order metric due to the concern about reliability and performance in modern electronic products. Since traditional solution involves lengthy iteration and simulation flow, how to achieve fast yet accurate estimation has become an essential demand. In this work, with the help of modern AI acceleration techniques, we propose a comprehensive solution to combine both the advantages of image-based and netlist-based features in neural network framework and obtain high-quality IR drop prediction very effectively in modern designs. A customized convolutional neural network (CNN) is developed to extract PDN features and make static IR drop estimations. Trained and evaluated with the open-source dataset, experiment results show that we have obtained the best quality in the benchmark on the problem of IR drop estimation in ICCAD CAD Contest 2023, proving the effectiveness of this important design topic.

### TastepepAI, An artificial intelligence platform for taste peptide de novo design 
[[arxiv](https://arxiv.org/abs/2502.12167)] [[cool](https://papers.cool/arxiv/2502.12167)] [[pdf](https://arxiv.org/pdf/2502.12167)]
> **Authors**: Jianda Yue,Tingting Li,Jian Ouyang,Jiawei Xu,Hua Tan,Zihui Chen,Changsheng Han,Huanyu Li,Songping Liang,Zhonghua Liu,Zhonghua Liu,Ying Wang
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-18
> **comment**: 40 pages, 6 figures, research article
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Taste peptides have emerged as promising natural flavoring agents attributed to their unique organoleptic properties, high safety profile, and potential health benefits. However, the de novo identification of taste peptides derived from animal, plant, or microbial sources remains a time-consuming and resource-intensive process, significantly impeding their widespread application in the food industry. Here, we present TastePepAI, a comprehensive artificial intelligence framework for customized taste peptide design and safety assessment. As the key element of this framework, a loss-supervised adaptive variational autoencoder (LA-VAE) is implemented to efficiently optimizes the latent representation of sequences during training and facilitates the generation of target peptides with desired taste profiles. Notably, our model incorporates a novel taste-avoidance mechanism, allowing for selective flavor exclusion. Subsequently, our in-house developed toxicity prediction algorithm (SpepToxPred) is integrated in the framework to undergo rigorous safety evaluation of generated peptides. Using this integrated platform, we successfully identified 73 peptides exhibiting sweet, salty, and umami, significantly expanding the current repertoire of taste peptides. This work demonstrates the potential of TastePepAI in accelerating taste peptide discovery for food applications and provides a versatile framework adaptable to broader peptide engineering challenges.

### Mining Social Determinants of Health for Heart Failure Patient 30-Day Readmission via Large Language Model 
[[arxiv](https://arxiv.org/abs/2502.12158)] [[cool](https://papers.cool/arxiv/2502.12158)] [[pdf](https://arxiv.org/pdf/2502.12158)]
> **Authors**: Mingchen Shao,Youjeong Kang,Xiao Hu,Hyunjung Gloria Kwak,Carl Yang,Jiaying Lu
> **First submission**: 2025-01-23
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,计算机与社会
- **Abstract**: Heart Failure (HF) affects millions of Americans and leads to high readmission rates, posing significant healthcare challenges. While Social Determinants of Health (SDOH) such as socioeconomic status and housing stability play critical roles in health outcomes, they are often underrepresented in structured EHRs and hidden in unstructured clinical notes. This study leverages advanced large language models (LLMs) to extract SDOHs from clinical text and uses logistic regression to analyze their association with HF readmissions. By identifying key SDOHs (e.g. tobacco usage, limited transportation) linked to readmission risk, this work also offers actionable insights for reducing readmissions and improving patient care.

### LaM-SLidE: Latent Space Modeling of Spatial Dynamical Systems via Linked Entities 
[[arxiv](https://arxiv.org/abs/2502.12128)] [[cool](https://papers.cool/arxiv/2502.12128)] [[pdf](https://arxiv.org/pdf/2502.12128)]
> **Authors**: Florian Sestak,Artur Toshev,Andreas Fürst,Günter Klambauer,Andreas Mayr,Johannes Brandstetter
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Project page: https://ml-jku.github.io/LaM-SLidE/
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Generative models are spearheading recent progress in deep learning, showing strong promise for trajectory sampling in dynamical systems as well. However, while latent space modeling paradigms have transformed image and video generation, similar approaches are more difficult for most dynamical systems. Such systems -- from chemical molecule structures to collective human behavior -- are described by interactions of entities, making them inherently linked to connectivity patterns and the traceability of entities over time. Our approach, LaM-SLidE (Latent Space Modeling of Spatial Dynamical Systems via Linked Entities), combines the advantages of graph neural networks, i.e., the traceability of entities across time-steps, with the efficiency and scalability of recent advances in image and video generation, where pre-trained encoder and decoder are frozen to enable generative modeling in the latent space. The core idea of LaM-SLidE is to introduce identifier representations (IDs) to allow for retrieval of entity properties, e.g., entity coordinates, from latent system representations and thus enables traceability. Experimentally, across different domains, we show that LaM-SLidE performs favorably in terms of speed, accuracy, and generalizability. Code is available at https://github.com/ml-jku/LaM-SLidE .

### Minimal Ranks, Maximum Confidence: Parameter-efficient Uncertainty Quantification for LoRA 
[[arxiv](https://arxiv.org/abs/2502.12122)] [[cool](https://papers.cool/arxiv/2502.12122)] [[pdf](https://arxiv.org/pdf/2502.12122)]
> **Authors**: Patryk Marszałek,Klaudia Bałazy,Jacek Tabor,Tomasz Kuśmierczyk
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning of large language models by decomposing weight updates into low-rank matrices, significantly reducing storage and computational overhead. While effective, standard LoRA lacks mechanisms for uncertainty quantification, leading to overconfident and poorly calibrated models. Bayesian variants of LoRA address this limitation, but at the cost of a significantly increased number of trainable parameters, partially offsetting the original efficiency gains. Additionally, these models are harder to train and may suffer from unstable convergence. In this work, we propose a novel parameter-efficient Bayesian LoRA, demonstrating that effective uncertainty quantification can be achieved in very low-dimensional parameter spaces. The proposed method achieves strong performance with improved calibration and generalization while maintaining computational efficiency. Our empirical findings show that, with the appropriate projection of the weight space: (1) uncertainty can be effectively modeled in a low-dimensional space, and (2) weight covariances exhibit low ranks.

### LLMs on the Line: Data Determines Loss-to-Loss Scaling Laws 
[[arxiv](https://arxiv.org/abs/2502.12120)] [[cool](https://papers.cool/arxiv/2502.12120)] [[pdf](https://arxiv.org/pdf/2502.12120)]
> **Authors**: Prasanna Mayilvahanan,Thaddäus Wiedemer,Sayak Mallick,Matthias Bethge,Wieland Brendel
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Scaling laws guide the development of large language models (LLMs) by offering estimates for the optimal balance of model size, tokens, and compute. More recently, loss-to-loss scaling laws that relate losses across pretraining datasets and downstream tasks have emerged as a powerful tool for understanding and improving LLM performance. In this work, we investigate which factors most strongly influence loss-to-loss scaling. Our experiments reveal that the pretraining data and tokenizer determine the scaling trend. In contrast, model size, optimization hyperparameters, and even significant architectural differences, such as between transformer-based models like Llama and state-space models like Mamba, have limited impact. Consequently, practitioners should carefully curate suitable pretraining datasets for optimal downstream performance, while architectures and other settings can be freely optimized for training efficiency.

### Scaling Test-Time Compute Without Verification or RL is Suboptimal 
[[arxiv](https://arxiv.org/abs/2502.12118)] [[cool](https://papers.cool/arxiv/2502.12118)] [[pdf](https://arxiv.org/pdf/2502.12118)]
> **Authors**: Amrith Setlur,Nived Rajaraman,Sergey Levine,Aviral Kumar
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Despite substantial advances in scaling test-time compute, an ongoing debate in the community is how it should be scaled up to enable continued and efficient improvements with scaling. There are largely two approaches: first, distilling successful search or thinking traces; and second, using verification (e.g., 0/1 outcome rewards, reward models, or verifiers) to guide reinforcement learning (RL) and search algorithms. In this paper, we prove that finetuning LLMs with verifier-based (VB) methods based on RL or search is far superior to verifier-free (VF) approaches based on distilling or cloning search traces, given a fixed amount of compute/data budget. Further, we show that as we scale test-time compute (measured as the output token length) and training data, suboptimality of VF methods scales poorly compared to VB when the base pre-trained LLM presents a heterogeneous distribution over correct solution traces (e.g., different lengths, styles, etc.) and admits a non-sharp distribution over rewards on traces sampled from it. We formalize this condition using anti-concentration [Erdős, 1945]. This implies a stronger result that VB methods scale better asymptotically, with the performance gap between VB and VF methods widening as test-time budget grows. We corroborate our theory empirically on both didactic and math reasoning problems with 3/8/32B-sized pre-trained LLMs, where we find verification is crucial for scaling test-time compute.

### SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering? 
[[arxiv](https://arxiv.org/abs/2502.12115)] [[cool](https://papers.cool/arxiv/2502.12115)] [[pdf](https://arxiv.org/pdf/2502.12115)]
> **Authors**: Samuel Miserendino,Michele Wang,Tejal Patwardhan,Johannes Heidecke
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 9 pages, 24 pages appendix
- **标题**: None
- **领域**: 机器学习,软件工程
- **Abstract**: We introduce SWE-Lancer, a benchmark of over 1,400 freelance software engineering tasks from Upwork, valued at \$1 million USD total in real-world payouts. SWE-Lancer encompasses both independent engineering tasks--ranging from \$50 bug fixes to \$32,000 feature implementations--and managerial tasks, where models choose between technical implementation proposals. Independent tasks are graded with end-to-end tests triple-verified by experienced software engineers, while managerial decisions are assessed against the choices of the original hired engineering managers. We evaluate model performance and find that frontier models are still unable to solve the majority of tasks. To facilitate future research, we open-source a unified Docker image and a public evaluation split, SWE-Lancer Diamond (https://github.com/openai/SWELancer-Benchmark). By mapping model performance to monetary value, we hope SWE-Lancer enables greater research into the economic impact of AI model development.

### Using the Path of Least Resistance to Explain Deep Networks 
[[arxiv](https://arxiv.org/abs/2502.12108)] [[cool](https://papers.cool/arxiv/2502.12108)] [[pdf](https://arxiv.org/pdf/2502.12108)]
> **Authors**: Sina Salek,Joseph Enguehard
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Integrated Gradients (IG), a widely used axiomatic path-based attribution method, assigns importance scores to input features by integrating model gradients along a straight path from a baseline to the input. While effective in some cases, we show that straight paths can lead to flawed attributions. In this paper, we identify the cause of these misattributions and propose an alternative approach that treats the input space as a Riemannian manifold, computing attributions by integrating gradients along geodesics. We call this method Geodesic Integrated Gradients (GIG). To approximate geodesic paths, we introduce two techniques: a k-Nearest Neighbours-based approach for smaller models and a Stochastic Variational Inference-based method for larger ones. Additionally, we propose a new axiom, Strong Completeness, extending the axioms satisfied by IG. We show that this property is desirable for attribution methods and that GIG is the only method that satisfies it. Through experiments on both synthetic and real-world data, we demonstrate that GIG outperforms existing explainability methods, including IG.

### Meta-Statistical Learning: Supervised Learning of Statistical Inference 
[[arxiv](https://arxiv.org/abs/2502.12088)] [[cool](https://papers.cool/arxiv/2502.12088)] [[pdf](https://arxiv.org/pdf/2502.12088)]
> **Authors**: Maxime Peyrard,Kyunghyun Cho
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: This work demonstrates that the tools and principles driving the success of large language models (LLMs) can be repurposed to tackle distribution-level tasks, where the goal is to predict properties of the data-generating distribution rather than labels for individual datapoints. These tasks encompass statistical inference problems such as parameter estimation, hypothesis testing, or mutual information estimation. Framing these tasks within traditional machine learning pipelines is challenging, as supervision is typically tied to individual datapoint. We propose meta-statistical learning, a framework inspired by multi-instance learning that reformulates statistical inference tasks as supervised learning problems. In this approach, entire datasets are treated as single inputs to neural networks, which predict distribution-level parameters. Transformer-based architectures, without positional encoding, provide a natural fit due to their permutation-invariance properties. By training on large-scale synthetic datasets, meta-statistical models can leverage the scalability and optimization infrastructure of Transformer-based LLMs. We demonstrate the framework's versatility with applications in hypothesis testing and mutual information estimation, showing strong performance, particularly for small datasets where traditional neural methods struggle.

### Unifying Explainable Anomaly Detection and Root Cause Analysis in Dynamical Systems 
[[arxiv](https://arxiv.org/abs/2502.12086)] [[cool](https://papers.cool/arxiv/2502.12086)] [[pdf](https://arxiv.org/pdf/2502.12086)]
> **Authors**: Yue Sun,Rick S. Blum,Parv Venkitasubramaniam
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Accepted by the AAAI-25 Workshop on Artificial Intelligence for Cyber Security (AICS)
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Dynamical systems, prevalent in various scientific and engineering domains, are susceptible to anomalies that can significantly impact their performance and reliability. This paper addresses the critical challenges of anomaly detection, root cause localization, and anomaly type classification in dynamical systems governed by ordinary differential equations (ODEs). We define two categories of anomalies: cyber anomalies, which propagate through interconnected variables, and measurement anomalies, which remain localized to individual variables. To address these challenges, we propose the Interpretable Causality Ordinary Differential Equation (ICODE) Networks, a model-intrinsic explainable learning framework. ICODE leverages Neural ODEs for anomaly detection while employing causality inference through an explanation channel to perform root cause analysis (RCA), elucidating why specific time periods are flagged as anomalous. ICODE is designed to simultaneously perform anomaly detection, RCA, and anomaly type classification within a single, interpretable framework. Our approach is grounded in the hypothesis that anomalies alter the underlying ODEs of the system, manifesting as changes in causal relationships between variables. We provide a theoretical analysis of how perturbations in learned model parameters can be utilized to identify anomalies and their root causes in time series data. Comprehensive experimental evaluations demonstrate the efficacy of ICODE across various dynamical systems, showcasing its ability to accurately detect anomalies, classify their types, and pinpoint their origins.

### APB: Accelerating Distributed Long-Context Inference by Passing Compressed Context Blocks across GPUs 
[[arxiv](https://arxiv.org/abs/2502.12085)] [[cool](https://papers.cool/arxiv/2502.12085)] [[pdf](https://arxiv.org/pdf/2502.12085)]
> **Authors**: Yuxiang Huang,Mingye Li,Xu Han,Chaojun Xiao,Weilin Zhao,Sun Ao,Hao Zhou,Jie Zhou,Zhiyuan Liu,Maosong Sun
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Preprint
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: While long-context inference is crucial for advancing large language model (LLM) applications, its prefill speed remains a significant bottleneck. Current approaches, including sequence parallelism strategies and compute reduction through approximate attention mechanisms, still fall short of delivering optimal inference efficiency. This hinders scaling the inputs to longer sequences and processing long-context queries in a timely manner. To address this, we introduce APB, an efficient long-context inference framework that leverages multi-host approximate attention to enhance prefill speed by reducing compute and enhancing parallelism simultaneously. APB introduces a communication mechanism for essential key-value pairs within a sequence parallelism framework, enabling a faster inference speed while maintaining task performance. We implement APB by incorporating a tailored FlashAttn kernel alongside optimized distribution strategies, supporting diverse models and parallelism configurations. APB achieves speedups of up to 9.2x, 4.2x, and 1.6x compared with FlashAttn, RingAttn, and StarAttn, respectively, without any observable task performance degradation. We provide the implementation and experiment code of APB in https://github.com/thunlp/APB.

### Classifying the Stoichiometry of Virus-like Particles with Interpretable Machine Learning 
[[arxiv](https://arxiv.org/abs/2502.12049)] [[cool](https://papers.cool/arxiv/2502.12049)] [[pdf](https://arxiv.org/pdf/2502.12049)]
> **Authors**: Jiayang Zhang,Xianyuan Liu,Wei Wu,Sina Tabakhi,Wenrui Fan,Shuo Zhou,Kang Lan Tee,Tuck Seng Wong,Haiping Lu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,生物分子,定量方法
- **Abstract**: Virus-like particles (VLPs) are valuable for vaccine development due to their immune-triggering properties. Understanding their stoichiometry, the number of protein subunits to form a VLP, is critical for vaccine optimisation. However, current experimental methods to determine stoichiometry are time-consuming and require highly purified proteins. To efficiently classify stoichiometry classes in proteins, we curate a new dataset and propose an interpretable, data-driven pipeline leveraging linear machine learning models. We also explore the impact of feature encoding on model performance and interpretability, as well as methods to identify key protein sequence features influencing classification. The evaluation of our pipeline demonstrates that it can classify stoichiometry while revealing protein features that possibly influence VLP assembly. The data and code used in this work are publicly available at https://github.com/Shef-AIRE/StoicIML.

### The geometry of BERT 
[[arxiv](https://arxiv.org/abs/2502.12033)] [[cool](https://papers.cool/arxiv/2502.12033)] [[pdf](https://arxiv.org/pdf/2502.12033)]
> **Authors**: Matteo Bonino,Giorgia Ghione,Giansalvo Cirrincione
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 28 pages, 13 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Transformer neural networks, particularly Bidirectional Encoder Representations from Transformers (BERT), have shown remarkable performance across various tasks such as classification, text summarization, and question answering. However, their internal mechanisms remain mathematically obscure, highlighting the need for greater explainability and interpretability. In this direction, this paper investigates the internal mechanisms of BERT proposing a novel perspective on the attention mechanism of BERT from a theoretical perspective. The analysis encompasses both local and global network behavior. At the local level, the concept of directionality of subspace selection as well as a comprehensive study of the patterns emerging from the self-attention matrix are presented. Additionally, this work explores the semantic content of the information stream through data distribution analysis and global statistical measures including the novel concept of cone index. A case study on the classification of SARS-CoV-2 variants using RNA which resulted in a very high accuracy has been selected in order to observe these concepts in an application. The insights gained from this analysis contribute to a deeper understanding of BERT's classification process, offering potential avenues for future architectural improvements in Transformer models and further analysis in the training process.

### Unsupervised Structural-Counterfactual Generation under Domain Shift 
[[arxiv](https://arxiv.org/abs/2502.12013)] [[cool](https://papers.cool/arxiv/2502.12013)] [[pdf](https://arxiv.org/pdf/2502.12013)]
> **Authors**: Krishn Vishwas Kher,Lokesh Venkata Siva Maruthi Badisa,Kusampudi Venkata Datta Sri Harsha,Chitneedi Geetha Sowmya,SakethaNath Jagarlapudi
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 13 pages, 1 figure
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Motivated by the burgeoning interest in cross-domain learning, we present a novel generative modeling challenge: generating counterfactual samples in a target domain based on factual observations from a source domain. Our approach operates within an unsupervised paradigm devoid of parallel or joint datasets, relying exclusively on distinct observational samples and causal graphs for each domain. This setting presents challenges that surpass those of conventional counterfactual generation. Central to our methodology is the disambiguation of exogenous causes into effect-intrinsic and domain-intrinsic categories. This differentiation facilitates the integration of domain-specific causal graphs into a unified joint causal graph via shared effect-intrinsic exogenous variables. We propose leveraging Neural Causal models within this joint framework to enable accurate counterfactual generation under standard identifiability assumptions. Furthermore, we introduce a novel loss function that effectively segregates effect-intrinsic from domain-intrinsic variables during model training. Given a factual observation, our framework combines the posterior distribution of effect-intrinsic variables from the source domain with the prior distribution of domain-intrinsic variables from the target domain to synthesize the desired counterfactuals, adhering to Pearl's causal hierarchy. Intriguingly, when domain shifts are restricted to alterations in causal mechanisms without accompanying covariate shifts, our training regimen parallels the resolution of a conditional optimal transport problem. Empirical evaluations on a synthetic dataset show that our framework generates counterfactuals in the target domain that very closely resemble the ground truth.

### Selective Task Group Updates for Multi-Task Optimization 
[[arxiv](https://arxiv.org/abs/2502.11986)] [[cool](https://papers.cool/arxiv/2502.11986)] [[pdf](https://arxiv.org/pdf/2502.11986)]
> **Authors**: Wooseong Jeong,Kuk-Jin Yoon
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Accepted at ICLR 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Multi-task learning enables the acquisition of task-generic knowledge by training multiple tasks within a unified architecture. However, training all tasks together in a single architecture can lead to performance degradation, known as negative transfer, which is a main concern in multi-task learning. Previous works have addressed this issue by optimizing the multi-task network through gradient manipulation or weighted loss adjustments. However, their optimization strategy focuses on addressing task imbalance in shared parameters, neglecting the learning of task-specific parameters. As a result, they show limitations in mitigating negative transfer, since the learning of shared space and task-specific information influences each other during optimization. To address this, we propose a different approach to enhance multi-task performance by selectively grouping tasks and updating them for each batch during optimization. We introduce an algorithm that adaptively determines how to effectively group tasks and update them during the learning process. To track inter-task relations and optimize multi-task networks simultaneously, we propose proximal inter-task affinity, which can be measured during the optimization process. We provide a theoretical analysis on how dividing tasks into multiple groups and updating them sequentially significantly affects multi-task performance by enhancing the learning of task-specific parameters. Our methods substantially outperform previous multi-task optimization approaches and are scalable to different architectures and various numbers of tasks.

### Machine Learning Should Maximize Welfare, Not (Only) Accuracy 
[[arxiv](https://arxiv.org/abs/2502.11981)] [[cool](https://papers.cool/arxiv/2502.11981)] [[pdf](https://arxiv.org/pdf/2502.11981)]
> **Authors**: Nir Rosenfeld,Haifeng Xu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机与社会
- **Abstract**: Decades of research in machine learning have given us powerful tools for making accurate predictions. But when used in social settings and on human inputs, better accuracy does not immediately translate to better social outcomes. This may not be surprising given that conventional learning frameworks are not designed to express societal preferences -- let alone promote them. This position paper argues that machine learning is currently missing, and can gain much from incorporating, a proper notion of social welfare. The field of welfare economics asks: how should we allocate limited resources to self-interested agents in a way that maximizes social benefit? We argue that this perspective applies to many modern applications of machine learning in social contexts, and advocate for its adoption. Rather than disposing of prediction, we aim to leverage this forte of machine learning for promoting social welfare. We demonstrate this idea by proposing a conceptual framework that gradually transitions from accuracy maximization (with awareness to welfare) to welfare maximization (via accurate prediction). We detail applications and use-cases for which our framework can be effective, identify technical challenges and practical opportunities, and highlight future avenues worth pursuing.

### Theoretical Barriers in Bellman-Based Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.11968)] [[cool](https://papers.cool/arxiv/2502.11968)] [[pdf](https://arxiv.org/pdf/2502.11968)]
> **Authors**: Brieuc Pinon,Raphaël Jungers,Jean-Charles Delvenne
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Reinforcement Learning algorithms designed for high-dimensional spaces often enforce the Bellman equation on a sampled subset of states, relying on generalization to propagate knowledge across the state space. In this paper, we identify and formalize a fundamental limitation of this common approach. Specifically, we construct counterexample problems with a simple structure that this approach fails to exploit. Our findings reveal that such algorithms can neglect critical information about the problems, leading to inefficiencies. Furthermore, we extend this negative result to another approach from the literature: Hindsight Experience Replay learning state-to-state reachability.

### Massively Scaling Explicit Policy-conditioned Value Functions 
[[arxiv](https://arxiv.org/abs/2502.11949)] [[cool](https://papers.cool/arxiv/2502.11949)] [[pdf](https://arxiv.org/pdf/2502.11949)]
> **Authors**: Nico Bohlinger,Jan Peters
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: We introduce a scaling strategy for Explicit Policy-Conditioned Value Functions (EPVFs) that significantly improves performance on challenging continuous-control tasks. EPVFs learn a value function V(θ) that is explicitly conditioned on the policy parameters, enabling direct gradient-based updates to the parameters of any policy. However, EPVFs at scale struggle with unrestricted parameter growth and efficient exploration in the policy parameter space. To address these issues, we utilize massive parallelization with GPU-based simulators, big batch sizes, weight clipping and scaled peturbations. Our results show that EPVFs can be scaled to solve complex tasks, such as a custom Ant environment, and can compete with state-of-the-art Deep Reinforcement Learning (DRL) baselines like Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC). We further explore action-based policy parameter representations from previous work and specialized neural network architectures to efficiently handle weight-space features, which have not been used in the context of DRL before.

### Sharp-PINNs: staggered hard-constrained physics-informed neural networks for phase field modelling of corrosion 
[[arxiv](https://arxiv.org/abs/2502.11942)] [[cool](https://papers.cool/arxiv/2502.11942)] [[pdf](https://arxiv.org/pdf/2502.11942)]
> **Authors**: Nanxi Chen,Chuanjie Cui,Rujin Ma,Airong Chen,Sifan Wang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算物理
- **Abstract**: Physics-informed neural networks have shown significant potential in solving partial differential equations (PDEs) across diverse scientific fields. However, their performance often deteriorates when addressing PDEs with intricate and strongly coupled solutions. In this work, we present a novel Sharp-PINN framework to tackle complex phase field corrosion problems. Instead of minimizing all governing PDE residuals simultaneously, the Sharp-PINNs introduce a staggered training scheme that alternately minimizes the residuals of Allen-Cahn and Cahn-Hilliard equations, which govern the corrosion system. To further enhance its efficiency and accuracy, we design an advanced neural network architecture that integrates random Fourier features as coordinate embeddings, employs a modified multi-layer perceptron as the primary backbone, and enforces hard constraints in the output layer. This framework is benchmarked through simulations of corrosion problems with multiple pits, where the staggered training scheme and network architecture significantly improve both the efficiency and accuracy of PINNs. Moreover, in three-dimensional cases, our approach is 5-10 times faster than traditional finite element methods while maintaining competitive accuracy, demonstrating its potential for real-world engineering applications in corrosion prediction.

### Deep Spatio-Temporal Neural Network for Air Quality Reanalysis 
[[arxiv](https://arxiv.org/abs/2502.11941)] [[cool](https://papers.cool/arxiv/2502.11941)] [[pdf](https://arxiv.org/pdf/2502.11941)]
> **Authors**: Ammar Kheder,Benjamin Foreback,Lili Wang,Zhi-Song Liu,Michael Boy
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Air quality prediction is key to mitigating health impacts and guiding decisions, yet existing models tend to focus on temporal trends while overlooking spatial generalization. We propose AQ-Net, a spatiotemporal reanalysis model for both observed and unobserved stations in the near future. AQ-Net utilizes the LSTM and multi-head attention for the temporal regression. We also propose a cyclic encoding technique to ensure continuous time representation. To learn fine-grained spatial air quality estimation, we incorporate AQ-Net with the neural kNN to explore feature-based interpolation, such that we can fill the spatial gaps given coarse observation stations. To demonstrate the efficiency of our model for spatiotemporal reanalysis, we use data from 2013-2017 collected in northern China for PM2.5 analysis. Extensive experiments show that AQ-Net excels in air quality reanalysis, highlighting the potential of hybrid spatio-temporal models to better capture environmental dynamics, especially in urban areas where both spatial and temporal variability are critical.

### FitLight: Federated Imitation Learning for Plug-and-Play Autonomous Traffic Signal Control 
[[arxiv](https://arxiv.org/abs/2502.11937)] [[cool](https://papers.cool/arxiv/2502.11937)] [[pdf](https://arxiv.org/pdf/2502.11937)]
> **Authors**: Yutong Ye,Yingbo Zhou,Zhusen Liu,Xiao Du,Hao Zhou,Xiang Lian,Mingsong Chen
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Although Reinforcement Learning (RL)-based Traffic Signal Control (TSC) methods have been extensively studied, their practical applications still raise some serious issues such as high learning cost and poor generalizability. This is because the ``trial-and-error'' training style makes RL agents extremely dependent on the specific traffic environment, which also requires a long convergence time. To address these issues, we propose a novel Federated Imitation Learning (FIL)-based framework for multi-intersection TSC, named FitLight, which allows RL agents to plug-and-play for any traffic environment without additional pre-training cost. Unlike existing imitation learning approaches that rely on pre-training RL agents with demonstrations, FitLight allows real-time imitation learning and seamless transition to reinforcement learning. Due to our proposed knowledge-sharing mechanism and novel hybrid pressure-based agent design, RL agents can quickly find a best control policy with only a few episodes. Moreover, for resource-constrained TSC scenarios, FitLight supports model pruning and heterogeneous model aggregation, such that RL agents can work on a micro-controller with merely 16{\it KB} RAM and 32{\it KB} ROM. Extensive experiments demonstrate that, compared to state-of-the-art methods, FitLight not only provides a superior starting point but also converges to a better final solution on both real-world and synthetic datasets, even under extreme resource limitations.

### Continual Learning Should Move Beyond Incremental Classification 
[[arxiv](https://arxiv.org/abs/2502.11927)] [[cool](https://papers.cool/arxiv/2502.11927)] [[pdf](https://arxiv.org/pdf/2502.11927)]
> **Authors**: Rupert Mitchell,Antonio Alliegro,Raffaello Camoriano,Dustin Carrión-Ojeda,Antonio Carta,Georgia Chalvatzaki,Nikhil Churamani,Carlo D'Eramo,Samin Hamidi,Robin Hesse,Fabian Hinder,Roshni Ramanna Kamath,Vincenzo Lomonaco,Subarnaduti Paul,Francesca Pistilli,Tinne Tuytelaars,Gido M van de Ven,Kristian Kersting,Simone Schaub-Meyer,Martin Mundt
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Continual learning (CL) is the sub-field of machine learning concerned with accumulating knowledge in dynamic environments. So far, CL research has mainly focused on incremental classification tasks, where models learn to classify new categories while retaining knowledge of previously learned ones. Here, we argue that maintaining such a focus limits both theoretical development and practical applicability of CL methods. Through a detailed analysis of concrete examples - including multi-target classification, robotics with constrained output spaces, learning in continuous task domains, and higher-level concept memorization - we demonstrate how current CL approaches often fail when applied beyond standard classification. We identify three fundamental challenges: (C1) the nature of continuity in learning problems, (C2) the choice of appropriate spaces and metrics for measuring similarity, and (C3) the role of learning objectives beyond classification. For each challenge, we provide specific recommendations to help move the field forward, including formalizing temporal dynamics through distribution processes, developing principled approaches for continuous task spaces, and incorporating density estimation and generative objectives. In so doing, this position paper aims to broaden the scope of CL research while strengthening its theoretical foundations, making it more applicable to real-world problems.

### VLP: Vision-Language Preference Learning for Embodied Manipulation 
[[arxiv](https://arxiv.org/abs/2502.11918)] [[cool](https://papers.cool/arxiv/2502.11918)] [[pdf](https://arxiv.org/pdf/2502.11918)]
> **Authors**: Runze Liu,Chenjia Bai,Jiafei Lyu,Shengjie Sun,Yali Du,Xiu Li
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器人技术
- **Abstract**: Reward engineering is one of the key challenges in Reinforcement Learning (RL). Preference-based RL effectively addresses this issue by learning from human feedback. However, it is both time-consuming and expensive to collect human preference labels. In this paper, we propose a novel \textbf{V}ision-\textbf{L}anguage \textbf{P}reference learning framework, named \textbf{VLP}, which learns a vision-language preference model to provide preference feedback for embodied manipulation tasks. To achieve this, we define three types of language-conditioned preferences and construct a vision-language preference dataset, which contains versatile implicit preference orders without human annotations. The preference model learns to extract language-related features, and then serves as a preference annotator in various downstream tasks. The policy can be learned according to the annotated preferences via reward learning or direct policy optimization. Extensive empirical results on simulated embodied manipulation tasks demonstrate that our method provides accurate preferences and generalizes to unseen tasks and unseen language instructions, outperforming the baselines by a large margin.

### Adversarial Alignment for LLMs Requires Simpler, Reproducible, and More Measurable Objectives 
[[arxiv](https://arxiv.org/abs/2502.11910)] [[cool](https://papers.cool/arxiv/2502.11910)] [[pdf](https://arxiv.org/pdf/2502.11910)]
> **Authors**: Leo Schwinn,Yan Scholten,Tom Wollschläger,Sophie Xhonneux,Stephen Casper,Stephan Günnemann,Gauthier Gidel
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Misaligned research objectives have considerably hindered progress in adversarial robustness research over the past decade. For instance, an extensive focus on optimizing target metrics, while neglecting rigorous standardized evaluation, has led researchers to pursue ad-hoc heuristic defenses that were seemingly effective. Yet, most of these were exposed as flawed by subsequent evaluations, ultimately contributing little measurable progress to the field. In this position paper, we illustrate that current research on the robustness of large language models (LLMs) risks repeating past patterns with potentially worsened real-world implications. To address this, we argue that realigned objectives are necessary for meaningful progress in adversarial alignment. To this end, we build on established cybersecurity taxonomy to formally define differences between past and emerging threat models that apply to LLMs. Using this framework, we illustrate that progress requires disentangling adversarial alignment into addressable sub-problems and returning to core academic principles, such as measureability, reproducibility, and comparability. Although the field presents significant challenges, the fresh start on adversarial robustness offers the unique opportunity to build on past experience while avoiding previous mistakes.

### CAMEL: Continuous Action Masking Enabled by Large Language Models for Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.11896)] [[cool](https://papers.cool/arxiv/2502.11896)] [[pdf](https://arxiv.org/pdf/2502.11896)]
> **Authors**: Yanxiao Zhao,Yangge Qian,Jingyang Shan,Xiaolin Qin
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Accepted at RLDM 2025
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Reinforcement learning (RL) in continuous action spaces encounters persistent challenges, such as inefficient exploration and convergence to suboptimal solutions. To address these limitations, we propose CAMEL, a novel framework integrating LLM-generated suboptimal policies into the RL training pipeline. CAMEL leverages dynamic action masking and an adaptive epsilon-masking mechanism to guide exploration during early training stages while gradually enabling agents to optimize policies independently. At the core of CAMEL lies the integration of Python-executable suboptimal policies generated by LLMs based on environment descriptions and task objectives. Although simplistic and hard-coded, these policies offer valuable initial guidance for RL agents. To effectively utilize these priors, CAMEL employs masking-aware optimization to dynamically constrain the action space based on LLM outputs. Additionally, epsilon-masking gradually reduces reliance on LLM-generated guidance, enabling agents to transition from constrained exploration to autonomous policy refinement. Experimental validation on Gymnasium MuJoCo environments demonstrates the effectiveness of CAMEL. In Hopper-v4 and Ant-v4, LLM-generated policies significantly improve sample efficiency, achieving performance comparable to or surpassing expert masking baselines. For Walker2d-v4, where LLMs struggle to accurately model bipedal gait dynamics, CAMEL maintains robust RL performance without notable degradation, highlighting the framework's adaptability across diverse tasks. While CAMEL shows promise in enhancing sample efficiency and mitigating convergence challenges, these issues remain open for further research. Future work aims to generalize CAMEL to multimodal LLMs for broader observation-action spaces and automate policy evaluation, reducing human intervention and enhancing scalability in RL training pipelines.

### Continual Quantization-Aware Pre-Training: When to transition from 16-bit to 1.58-bit pre-training for BitNet language models? 
[[arxiv](https://arxiv.org/abs/2502.11895)] [[cool](https://papers.cool/arxiv/2502.11895)] [[pdf](https://arxiv.org/pdf/2502.11895)]
> **Authors**: Jacob Nielsen,Peter Schneider-Kamp,Lukas Galke
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Large language models (LLMs) require immense resources for training and inference. Quantization, a technique that reduces the precision of model parameters, offers a promising solution for improving LLM efficiency and sustainability. While post-training quantization methods typically achieve 4-8 bits per parameter, recent research suggests that training LLMs with 1.58 bits per weight parameter from scratch can maintain model accuracy while greatly reducing memory requirements and energy consumption at inference time. Here, we investigate a training strategy for quantization-aware pre-training, where the models are first trained with 16-bit precision and then transition into 1.58-bit quantization-aware training. Our results on 11 downstream tasks show that this 16-to-1.58-bit training strategy is preferable over full 1.58-bit training and leaves models closer to those which have undergone 16-bit training. We further investigate the effects of retaining the optimizer state at the transition point and gradually phasing in quantization strength -- finding that both techniques alleviate the magnitude of loss spikes, but also that these effects can be compensated through further training.

### Rethinking Benign Overfitting in Two-Layer Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.11893)] [[cool](https://papers.cool/arxiv/2502.11893)] [[pdf](https://arxiv.org/pdf/2502.11893)]
> **Authors**: Ruichen Xu,Kexin Chen
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Recent theoretical studies (Kou et al., 2023; Cao et al., 2022) have revealed a sharp phase transition from benign to harmful overfitting when the noise-to-feature ratio exceeds a threshold-a situation common in long-tailed data distributions where atypical data is prevalent. However, harmful overfitting rarely happens in overparameterized neural networks. Further experimental results suggested that memorization is necessary for achieving near-optimal generalization error in long-tailed data distributions (Feldman & Zhang, 2020). We argue that this discrepancy between theoretical predictions and empirical observations arises because previous feature-noise data models overlook the heterogeneous nature of noise across different data classes. In this paper, we refine the feature-noise data model by incorporating class-dependent heterogeneous noise and re-examine the overfitting phenomenon in neural networks. Through a comprehensive analysis of the training dynamics, we establish test loss bounds for the refined model. Our findings reveal that neural networks can leverage "data noise", previously deemed harmful, to learn implicit features that improve the classification accuracy for long-tailed data. Experimental validation on both synthetic and real-world datasets supports our theoretical results.

### LIMR: Less is More for RL Scaling 
[[arxiv](https://arxiv.org/abs/2502.11886)] [[cool](https://papers.cool/arxiv/2502.11886)] [[pdf](https://arxiv.org/pdf/2502.11886)]
> **Authors**: Xuefeng Li,Haoyang Zou,Pengfei Liu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 6pages
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: In this paper, we ask: what truly determines the effectiveness of RL training data for enhancing language models' reasoning capabilities? While recent advances like o1, Deepseek R1, and Kimi1.5 demonstrate RL's potential, the lack of transparency about training data requirements has hindered systematic progress. Starting directly from base models without distillation, we challenge the assumption that scaling up RL training data inherently improves performance. we demonstrate that a strategically selected subset of just 1,389 samples can outperform the full 8,523-sample dataset. We introduce Learning Impact Measurement (LIM), an automated method to evaluate and prioritize training samples based on their alignment with model learning trajectories, enabling efficient resource utilization and scalable implementation. Our method achieves comparable or even superior performance using only 1,389 samples versus the full 8,523 samples dataset. Notably, while recent data-efficient approaches (e.g., LIMO and s1) show promise with 32B-scale models, we find it significantly underperforms at 7B-scale through supervised fine-tuning (SFT). In contrast, our RL-based LIMR achieves 16.7% higher accuracy on AIME24 and outperforms LIMO and s1 by 13.0% and 22.2% on MATH500. These results fundamentally reshape our understanding of RL scaling in LLMs, demonstrating that precise sample selection, rather than data scale, may be the key to unlocking enhanced reasoning capabilities. For reproducible research and future innovation, we are open-sourcing LIMR, including implementation of LIM, training and evaluation code, curated datasets, and trained models at https://github.com/GAIR-NLP/LIMR.

### Bitnet.cpp: Efficient Edge Inference for Ternary LLMs 
[[arxiv](https://arxiv.org/abs/2502.11880)] [[cool](https://papers.cool/arxiv/2502.11880)] [[pdf](https://arxiv.org/pdf/2502.11880)]
> **Authors**: Jinheng Wang,Hansong Zhou,Ting Song,Shijie Cao,Yan Xia,Ting Cao,Jianyu Wei,Shuming Ma,Hongyu Wang,Furu Wei
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 18 pages, 11 figures
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,分布式、并行和集群计算
- **Abstract**: The advent of 1-bit large language models (LLMs), led by BitNet b1.58, has spurred interest in ternary LLMs. Despite this, research and practical applications focusing on efficient edge inference for ternary LLMs remain scarce. To bridge this gap, we introduce Bitnet.cpp, an inference system optimized for BitNet b1.58 and ternary LLMs. Given that mixed-precision matrix multiplication (mpGEMM) constitutes the bulk of inference time in ternary LLMs, Bitnet.cpp incorporates a novel mpGEMM library to facilitate sub-2-bits-per-weight, efficient and lossless inference. The library features two core solutions: Ternary Lookup Table (TL), which addresses spatial inefficiencies of previous bit-wise methods, and Int2 with a Scale (I2_S), which ensures lossless edge inference, both enabling high-speed inference. Our experiments show that Bitnet.cpp achieves up to a 6.25x increase in speed over full-precision baselines and up to 2.32x over low-bit baselines, setting new benchmarks in the field. Additionally, we expand TL to element-wise lookup table (ELUT) for low-bit LLMs in the appendix, presenting both theoretical and empirical evidence of its considerable potential. Bitnet.cpp is publicly available at https://github.com/microsoft/BitNet/tree/paper , offering a sophisticated solution for the efficient and practical deployment of edge LLMs.

### FedEAT: A Robustness Optimization Framework for Federated LLMs 
[[arxiv](https://arxiv.org/abs/2502.11863)] [[cool](https://papers.cool/arxiv/2502.11863)] [[pdf](https://arxiv.org/pdf/2502.11863)]
> **Authors**: Yahao Pang,Xingyuan Wu,Xiaojin Zhang,Wei Chen,Hai Jin
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Significant advancements have been made by Large Language Models (LLMs) in the domains of natural language understanding and automated content creation. However, they still face persistent problems, including substantial computational costs and inadequate availability of training data. The combination of Federated Learning (FL) and LLMs (federated LLMs) offers a solution by leveraging distributed data while protecting privacy, which positions it as an ideal choice for sensitive domains. However, Federated LLMs still suffer from robustness challenges, including data heterogeneity, malicious clients, and adversarial attacks, which greatly hinder their applications. We first introduce the robustness problems in federated LLMs, to address these challenges, we propose FedEAT (Federated Embedding space Adversarial Training), a novel framework that applies adversarial training in the embedding space of client LLM and employs a robust aggregation approach, specifically geometric median aggregation, to enhance the robustness of Federated LLMs. Our experiments demonstrate that FedEAT effectively improves the robustness of Federated LLMs with minimal performance loss.

### StructTransform: A Scalable Attack Surface for Safety-Aligned Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.11853)] [[cool](https://papers.cool/arxiv/2502.11853)] [[pdf](https://arxiv.org/pdf/2502.11853)]
> **Authors**: Shehel Yoosuf,Temoor Ali,Ahmed Lekssays,Mashael AlSabah,Issa Khalil
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: In this work, we present a series of structure transformation attacks on LLM alignment, where we encode natural language intent using diverse syntax spaces, ranging from simple structure formats and basic query languages (e.g. SQL) to new novel spaces and syntaxes created entirely by LLMs. Our extensive evaluation shows that our simplest attacks can achieve close to 90% success rate, even on strict LLMs (such as Claude 3.5 Sonnet) using SOTA alignment mechanisms. We improve the attack performance further by using an adaptive scheme that combines structure transformations along with existing \textit{content transformations}, resulting in over 96% ASR with 0% refusals. To generalize our attacks, we explore numerous structure formats, including syntaxes purely generated by LLMs. Our results indicate that such novel syntaxes are easy to generate and result in a high ASR, suggesting that defending against our attacks is not a straightforward process. Finally, we develop a benchmark and evaluate existing safety-alignment defenses against it, showing that most of them fail with 100% ASR. Our results show that existing safety alignment mostly relies on token-level patterns without recognizing harmful concepts, highlighting and motivating the need for serious research efforts in this direction. As a case study, we demonstrate how attackers can use our attack to easily generate a sample malware, and a corpus of fraudulent SMS messages, which perform well in bypassing detection.

### Steering the LoCoMotif: Using Domain Knowledge in Time Series Motif Discovery 
[[arxiv](https://arxiv.org/abs/2502.11850)] [[cool](https://papers.cool/arxiv/2502.11850)] [[pdf](https://arxiv.org/pdf/2502.11850)]
> **Authors**: Aras Yurtman,Daan Van Wesenbeeck,Wannes Meert,Hendrik Blockeel
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: Time Series Motif Discovery (TSMD) identifies repeating patterns in time series data, but its unsupervised nature might result in motifs that are not interesting to the user. To address this, we propose a framework that allows the user to impose constraints on the motifs to be discovered, where constraints can easily be defined according to the properties of the desired motifs in the application domain. We also propose an efficient implementation of the framework, the LoCoMotif-DoK algorithm. We demonstrate that LoCoMotif-DoK can effectively leverage domain knowledge in real and synthetic data, outperforming other TSMD techniques which only support a limited form of domain knowledge.

### Model Generalization on Text Attribute Graphs: Principles with Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.11836)] [[cool](https://papers.cool/arxiv/2502.11836)] [[pdf](https://arxiv.org/pdf/2502.11836)]
> **Authors**: Haoyu Wang,Shikun Liu,Rongzhe Wei,Pan Li
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Large language models (LLMs) have recently been introduced to graph learning, aiming to extend their zero-shot generalization success to tasks where labeled graph data is scarce. Among these applications, inference over text-attributed graphs (TAGs) presents unique challenges: existing methods struggle with LLMs' limited context length for processing large node neighborhoods and the misalignment between node embeddings and the LLM token space. To address these issues, we establish two key principles for ensuring generalization and derive the framework LLM-BP accordingly: (1) Unifying the attribute space with task-adaptive embeddings, where we leverage LLM-based encoders and task-aware prompting to enhance generalization of the text attribute embeddings; (2) Developing a generalizable graph information aggregation mechanism, for which we adopt belief propagation with LLM-estimated parameters that adapt across graphs. Evaluations on 11 real-world TAG benchmarks demonstrate that LLM-BP significantly outperforms existing approaches, achieving 8.10% improvement with task-conditional embeddings and an additional 1.71% gain from adaptive aggregation.

### Intersectional Fairness in Reinforcement Learning with Large State and Constraint Spaces 
[[arxiv](https://arxiv.org/abs/2502.11828)] [[cool](https://papers.cool/arxiv/2502.11828)] [[pdf](https://arxiv.org/pdf/2502.11828)]
> **Authors**: Eric Eaton,Marcel Hussing,Michael Kearns,Aaron Roth,Sikata Bela Sengupta,Jessica Sorrell
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机科学与博弈论
- **Abstract**: In traditional reinforcement learning (RL), the learner aims to solve a single objective optimization problem: find the policy that maximizes expected reward. However, in many real-world settings, it is important to optimize over multiple objectives simultaneously. For example, when we are interested in fairness, states might have feature annotations corresponding to multiple (intersecting) demographic groups to whom reward accrues, and our goal might be to maximize the reward of the group receiving the minimal reward. In this work, we consider a multi-objective optimization problem in which each objective is defined by a state-based reweighting of a single scalar reward function. This generalizes the problem of maximizing the reward of the minimum reward group. We provide oracle-efficient algorithms to solve these multi-objective RL problems even when the number of objectives is exponentially large-for tabular MDPs, as well as for large MDPs when the group functions have additional structure. Finally, we experimentally validate our theoretical results and demonstrate applications on a preferential attachment graph MDP.

### IMTS-Mixer: Mixer-Networks for Irregular Multivariate Time Series Forecasting 
[[arxiv](https://arxiv.org/abs/2502.11816)] [[cool](https://papers.cool/arxiv/2502.11816)] [[pdf](https://arxiv.org/pdf/2502.11816)]
> **Authors**: Christian Klötergens,Tim Dernedde,Lars Schmidt-Thieme
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: :I.5
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Forecasting Irregular Multivariate Time Series (IMTS) has recently emerged as a distinct research field, necessitating specialized models to address its unique challenges. While most forecasting literature assumes regularly spaced observations without missing values, many real-world datasets - particularly in healthcare, climate research, and biomechanics - violate these assumptions. Time Series (TS)-mixer models have achieved remarkable success in regular multivariate time series forecasting. However, they remain unexplored for IMTS due to their requirement for complete and evenly spaced observations. To bridge this gap, we introduce IMTS-Mixer, a novel forecasting architecture designed specifically for IMTS. Our approach retains the core principles of TS mixer models while introducing innovative methods to transform IMTS into fixed-size matrix representations, enabling their seamless integration with mixer modules. We evaluate IMTS-Mixer on a benchmark of four real-world datasets from various domains. Our results demonstrate that IMTS-Mixer establishes a new state-of-the-art in forecasting accuracy while also improving computational efficiency.

### Interpretable Machine Learning for Kronecker Coefficients 
[[arxiv](https://arxiv.org/abs/2502.11774)] [[cool](https://papers.cool/arxiv/2502.11774)] [[pdf](https://arxiv.org/pdf/2502.11774)]
> **Authors**: Giorgi Butbaia,Kyu-Hwan Lee,Fabian Ruehle
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,组合学,表征论,机器学习
- **Abstract**: We analyze the saliency of neural networks and employ interpretable machine learning models to predict whether the Kronecker coefficients of the symmetric group are zero or not. Our models use triples of partitions as input features, as well as b-loadings derived from the principal component of an embedding that captures the differences between partitions. Across all approaches, we achieve an accuracy of approximately 83% and derive explicit formulas for a decision function in terms of b-loadings. Additionally, we develop transformer-based models for prediction, achieving the highest reported accuracy of over 99%.

### From Selection to Generation: A Survey of LLM-based Active Learning 
[[arxiv](https://arxiv.org/abs/2502.11767)] [[cool](https://papers.cool/arxiv/2502.11767)] [[pdf](https://arxiv.org/pdf/2502.11767)]
> **Authors**: Yu Xia,Subhojyoti Mukherjee,Zhouhang Xie,Junda Wu,Xintong Li,Ryan Aponte,Hanjia Lyu,Joe Barrow,Hongjie Chen,Franck Dernoncourt,Branislav Kveton,Tong Yu,Ruiyi Zhang,Jiuxiang Gu,Nesreen K. Ahmed,Yu Wang,Xiang Chen,Hanieh Deilamsalehy,Sungchul Kim,Zhengmian Hu,Yue Zhao,Nedim Lipka,Seunghyun Yoon,Ting-Hao Kenneth Huang,Zichao Wang, et al. (9 additional authors not shown)
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Active Learning (AL) has been a powerful paradigm for improving model efficiency and performance by selecting the most informative data points for labeling and training. In recent active learning frameworks, Large Language Models (LLMs) have been employed not only for selection but also for generating entirely new data instances and providing more cost-effective annotations. Motivated by the increasing importance of high-quality data and efficient model training in the era of LLMs, we present a comprehensive survey on LLM-based Active Learning. We introduce an intuitive taxonomy that categorizes these techniques and discuss the transformative roles LLMs can play in the active learning loop. We further examine the impact of AL on LLM learning paradigms and its applications across various domains. Finally, we identify open challenges and propose future research directions. This survey aims to serve as an up-to-date resource for researchers and practitioners seeking to gain an intuitive understanding of LLM-based AL techniques and deploy them to new applications.

### On the Computation of the Fisher Information in Continual Learning 
[[arxiv](https://arxiv.org/abs/2502.11756)] [[cool](https://papers.cool/arxiv/2502.11756)] [[pdf](https://arxiv.org/pdf/2502.11756)]
> **Authors**: Gido M. van de Ven
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: To appear in the blogpost track at ICLR 2025
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别,机器学习
- **Abstract**: One of the most popular methods for continual learning with deep neural networks is Elastic Weight Consolidation (EWC), which involves computing the Fisher Information. The exact way in which the Fisher Information is computed is however rarely described, and multiple different implementations for it can be found online. This blog post discusses and empirically compares several often-used implementations, which highlights that many currently reported results for EWC could likely be improved by changing the way the Fisher Information is computed.

### Robust Partial-Label Learning by Leveraging Class Activation Values 
[[arxiv](https://arxiv.org/abs/2502.11743)] [[cool](https://papers.cool/arxiv/2502.11743)] [[pdf](https://arxiv.org/pdf/2502.11743)]
> **Authors**: Tobias Fuchs,Florian Kalinke
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Real-world training data is often noisy; for example, human annotators assign conflicting class labels to the same instances. Partial-label learning (PLL) is a weakly supervised learning paradigm that allows training classifiers in this context without manual data cleaning. While state-of-the-art methods have good predictive performance, their predictions are sensitive to high noise levels, out-of-distribution data, and adversarial perturbations. We propose a novel PLL method based on subjective logic, which explicitly represents uncertainty by leveraging the magnitudes of the underlying neural network's class activation values. Thereby, we effectively incorporate prior knowledge about the class labels by using a novel label weight re-distribution strategy that we prove to be optimal. We empirically show that our method yields more robust predictions in terms of predictive performance under high PLL noise levels, handling out-of-distribution examples, and handling adversarial perturbations on the test instances.

### Mitigating Visual Knowledge Forgetting in MLLM Instruction-tuning via Modality-decoupled Gradient Descent 
[[arxiv](https://arxiv.org/abs/2502.11740)] [[cool](https://papers.cool/arxiv/2502.11740)] [[pdf](https://arxiv.org/pdf/2502.11740)]
> **Authors**: Junda Wu,Yuxin Xiong,Xintong Li,Yu Xia,Ruoyu Wang,Yu Wang,Tong Yu,Sungchul Kim,Ryan A. Rossi,Lina Yao,Jingbo Shang,Julian McAuley
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 9 pages
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: Recent MLLMs have shown emerging visual understanding and reasoning abilities after being pre-trained on large-scale multimodal datasets. Unlike pre-training, where MLLMs receive rich visual-text alignment, instruction-tuning is often text-driven with weaker visual supervision, leading to the degradation of pre-trained visual understanding and causing visual forgetting. Existing approaches, such as direct fine-tuning and continual learning methods, fail to explicitly address this issue, often compressing visual representations and prioritizing task alignment over visual retention, which further worsens visual forgetting. To overcome this limitation, we introduce a novel perspective leveraging effective rank to quantify the degradation of visual representation richness, interpreting this degradation through the information bottleneck principle as excessive compression that leads to the degradation of crucial pre-trained visual knowledge. Building on this view, we propose a modality-decoupled gradient descent (MDGD) method that regulates gradient updates to maintain the effective rank of visual representations while mitigating the over-compression effects described by the information bottleneck. By explicitly disentangling the optimization of visual understanding from task-specific alignment, MDGD preserves pre-trained visual knowledge while enabling efficient task adaptation. To enable lightweight instruction-tuning, we further develop a memory-efficient fine-tuning approach using gradient masking, which selectively updates a subset of model parameters to enable parameter-efficient fine-tuning (PEFT), reducing computational overhead while preserving rich visual representations. Extensive experiments across various downstream tasks and backbone MLLMs demonstrate that MDGD effectively mitigates visual forgetting from pre-trained tasks while enabling strong adaptation to new tasks.

### Proactive Depot Discovery: A Generative Framework for Flexible Location-Routing 
[[arxiv](https://arxiv.org/abs/2502.11715)] [[cool](https://papers.cool/arxiv/2502.11715)] [[pdf](https://arxiv.org/pdf/2502.11715)]
> **Authors**: Site Qu,Guoqiang Hu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: The Location-Routing Problem (LRP), which combines the challenges of facility (depot) locating and vehicle route planning, is critically constrained by the reliance on predefined depot candidates, limiting the solution space and potentially leading to suboptimal outcomes. Previous research on LRP without predefined depots is scant and predominantly relies on heuristic algorithms that iteratively attempt depot placements across a planar area. Such approaches lack the ability to proactively generate depot locations that meet specific geographic requirements, revealing a notable gap in current research landscape. To bridge this gap, we propose a data-driven generative DRL framework, designed to proactively generate depots for LRP without predefined depot candidates, solely based on customer requests data which include geographic and demand information. It can operate in two distinct modes: direct generation of exact depot locations, and the creation of a multivariate Gaussian distribution for flexible depots sampling. By extracting depots' geographic pattern from customer requests data, our approach can dynamically respond to logistical needs, identifying high-quality depot locations that further reduce total routing costs compared to traditional methods. Extensive experiments demonstrate that, for a same group of customer requests, compared with those depots identified through random attempts, our framework can proactively generate depots that lead to superior solution routes with lower routing cost. The implications of our framework potentially extend into real-world applications, particularly in emergency medical rescue and disaster relief logistics, where rapid establishment and adjustment of depot locations are paramount, showcasing its potential in addressing LRP for dynamic and unpredictable environments.

### Knowledge-aware contrastive heterogeneous molecular graph learning 
[[arxiv](https://arxiv.org/abs/2502.11711)] [[cool](https://papers.cool/arxiv/2502.11711)] [[pdf](https://arxiv.org/pdf/2502.11711)]
> **Authors**: Mukun Chen,Jia Wu,Shirui Pan,Fu Lin,Bo Du,Xiuwen Gong,Wenbin Hu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Molecular representation learning is pivotal in predicting molecular properties and advancing drug design. Traditional methodologies, which predominantly rely on homogeneous graph encoding, are limited by their inability to integrate external knowledge and represent molecular structures across different levels of granularity. To address these limitations, we propose a paradigm shift by encoding molecular graphs into heterogeneous structures, introducing a novel framework: Knowledge-aware Contrastive Heterogeneous Molecular Graph Learning (KCHML). This approach leverages contrastive learning to enrich molecular representations with embedded external knowledge. KCHML conceptualizes molecules through three distinct graph views-molecular, elemental, and pharmacological-enhanced by heterogeneous molecular graphs and a dual message-passing mechanism. This design offers a comprehensive representation for property prediction, as well as for downstream tasks such as drug-drug interaction (DDI) prediction. Extensive benchmarking demonstrates KCHML's superiority over state-of-the-art molecular property prediction models, underscoring its ability to capture intricate molecular features.

### Double Momentum and Error Feedback for Clipping with Fast Rates and Differential Privacy 
[[arxiv](https://arxiv.org/abs/2502.11682)] [[cool](https://papers.cool/arxiv/2502.11682)] [[pdf](https://arxiv.org/pdf/2502.11682)]
> **Authors**: Rustem Islamov,Samuel Horvath,Aurelien Lucchi,Peter Richtarik,Eduard Gorbunov
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,优化与控制,机器学习
- **Abstract**: Strong Differential Privacy (DP) and Optimization guarantees are two desirable properties for a method in Federated Learning (FL). However, existing algorithms do not achieve both properties at once: they either have optimal DP guarantees but rely on restrictive assumptions such as bounded gradients/bounded data heterogeneity, or they ensure strong optimization performance but lack DP guarantees. To address this gap in the literature, we propose and analyze a new method called Clip21-SGD2M based on a novel combination of clipping, heavy-ball momentum, and Error Feedback. In particular, for non-convex smooth distributed problems with clients having arbitrarily heterogeneous data, we prove that Clip21-SGD2M has optimal convergence rate and also near optimal (local-)DP neighborhood. Our numerical experiments on non-convex logistic regression and training of neural networks highlight the superiority of Clip21-SGD2M over baselines in terms of the optimization performance for a given DP-budget.

### Spectral structure learning for clinical time series 
[[arxiv](https://arxiv.org/abs/2502.11680)] [[cool](https://papers.cool/arxiv/2502.11680)] [[pdf](https://arxiv.org/pdf/2502.11680)]
> **Authors**: Ivan Lerner,Anita Burgun,Francis Bach
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We develop and evaluate a structure learning algorithm for clinical time series. Clinical time series are multivariate time series observed in multiple patients and irregularly sampled, challenging existing structure learning algorithms. We assume that our times series are realizations of StructGP, a k-dimensional multi-output or multi-task stationary Gaussian process (GP), with independent patients sharing the same covariance function. StructGP encodes ordered conditional relations between time series, represented in a directed acyclic graph. We implement an adapted NOTEARS algorithm, which based on a differentiable definition of acyclicity, recovers the graph by solving a series of continuous optimization problems. Simulation results show that up to mean degree 3 and 20 tasks, we reach a median recall of 0.93% [IQR, 0.86, 0.97] while keeping a median precision of 0.71% [0.57-0.84], for recovering directed edges. We further show that the regularization path is key to identifying the graph. With StructGP, we proposed a model of time series dependencies, that flexibly adapt to different time series regularity, while enabling us to learn these dependencies from observations.

### Best of Both Worlds: Regret Minimization versus Minimax Play 
[[arxiv](https://arxiv.org/abs/2502.11673)] [[cool](https://papers.cool/arxiv/2502.11673)] [[pdf](https://arxiv.org/pdf/2502.11673)]
> **Authors**: Adrian Müller,Jon Schneider,Stratis Skoulakis,Luca Viano,Volkan Cevher
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: In this paper, we investigate the existence of online learning algorithms with bandit feedback that simultaneously guarantee $O(1)$ regret compared to a given comparator strategy, and $O(\sqrt{T})$ regret compared to the best strategy in hindsight, where $T$ is the number of rounds. We provide the first affirmative answer to this question. In the context of symmetric zero-sum games, both in normal- and extensive form, we show that our results allow us to guarantee to risk at most $O(1)$ loss while being able to gain $Ω(T)$ from exploitable opponents, thereby combining the benefits of both no-regret algorithms and minimax play.

### Exact Upper and Lower Bounds for the Output Distribution of Neural Networks with Random Inputs 
[[arxiv](https://arxiv.org/abs/2502.11672)] [[cool](https://papers.cool/arxiv/2502.11672)] [[pdf](https://arxiv.org/pdf/2502.11672)]
> **Authors**: Andrey Kofnov,Daniel Kapla,Ezio Bartocci,Efstathia Bura
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 16 pages
- **标题**: None
- **领域**: 机器学习,方法论,机器学习
- **Abstract**: We derive exact upper and lower bounds for the cumulative distribution function (cdf) of the output of a neural network over its entire support subject to noisy (stochastic) inputs. The upper and lower bounds converge to the true cdf over its domain as the resolution increases. Our method applies to any feedforward NN using continuous monotonic piecewise differentiable activation functions (e.g., ReLU, tanh and softmax) and convolutional NNs, which were beyond the scope of competing approaches. The novelty and an instrumental tool of our approach is to bound general NNs with ReLU NNs. The ReLU NN based bounds are then used to derive upper and lower bounds of the cdf of the NN output. Experiments demonstrate that our method delivers guaranteed bounds of the predictive output distribution over its support, thus providing exact error guarantees, in contrast to competing approaches.

### Hyperspherical Energy Transformer with Recurrent Depth 
[[arxiv](https://arxiv.org/abs/2502.11646)] [[cool](https://papers.cool/arxiv/2502.11646)] [[pdf](https://arxiv.org/pdf/2502.11646)]
> **Authors**: Yunzhe Hu,Difan Zou,Dong Xu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 20 pages, 13 figures, 12 tables
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Transformer-based foundation models have achieved unprecedented success with a gigantic amount of parameters and computational resources. Yet, the core building blocks of these models, the Transformer layers, and how they are arranged and configured are primarily engineered from the bottom up and driven by heuristics. For advancing next-generation architectures, it demands exploring a prototypical model that is amenable to high interpretability and of practical competence. To this end, we take a step from the top-down view and design neural networks from an energy minimization perspective. Specifically, to promote isotropic token distribution on the sphere, we formulate a modified Hopfield energy function on the subspace-embedded hypersphere, based on which Transformer layers with symmetric structures are designed as the iterative optimization for the energy function. By integrating layers with the same parameters, we propose \textit{Hyper-Spherical Energy Transformer} (Hyper-SET), an alternative to the vanilla Transformer with recurrent depth. This design inherently provides greater interpretability and allows for scaling to deeper layers without a significant increase in the number of parameters. We also empirically demonstrate that Hyper-SET achieves comparable or even superior performance on both synthetic and real-world tasks, such as solving Sudoku and masked image modeling, while utilizing fewer parameters.

### Neural Interpretable Reasoning 
[[arxiv](https://arxiv.org/abs/2502.11639)] [[cool](https://papers.cool/arxiv/2502.11639)] [[pdf](https://arxiv.org/pdf/2502.11639)]
> **Authors**: Pietro Barbiero,Giuseppe Marra,Gabriele Ciravegna,David Debot,Francesco De Santis,Michelangelo Diligenti,Mateo Espinosa Zarlenga,Francesco Giannini
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,神经和进化计算
- **Abstract**: We formalize a novel modeling framework for achieving interpretability in deep learning, anchored in the principle of inference equivariance. While the direct verification of interpretability scales exponentially with the number of variables of the system, we show that this complexity can be mitigated by treating interpretability as a Markovian property and employing neural re-parametrization techniques. Building on these insights, we propose a new modeling paradigm -- neural generation and interpretable execution -- that enables scalable verification of equivariance. This paradigm provides a general approach for designing Neural Interpretable Reasoners that are not only expressive but also transparent.

### In-Context Parametric Inference: Point or Distribution Estimators? 
[[arxiv](https://arxiv.org/abs/2502.11617)] [[cool](https://papers.cool/arxiv/2502.11617)] [[pdf](https://arxiv.org/pdf/2502.11617)]
> **Authors**: Sarthak Mittal,Yoshua Bengio,Nikolay Malkin,Guillaume Lajoie
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Bayesian and frequentist inference are two fundamental paradigms in statistical estimation. Bayesian methods treat hypotheses as random variables, incorporating priors and updating beliefs via Bayes' theorem, whereas frequentist methods assume fixed but unknown hypotheses, relying on estimators like maximum likelihood. While extensive research has compared these approaches, the frequentist paradigm of obtaining point estimates has become predominant in deep learning, as Bayesian inference is challenging due to the computational complexity and the approximation gap of posterior estimation methods. However, a good understanding of trade-offs between the two approaches is lacking in the regime of amortized estimators, where in-context learners are trained to estimate either point values via maximum likelihood or maximum a posteriori estimation, or full posteriors using normalizing flows, score-based diffusion samplers, or diagonal Gaussian approximations, conditioned on observations. To help resolve this, we conduct a rigorous comparative analysis spanning diverse problem settings, from linear models to shallow neural networks, with a robust evaluation framework assessing both in-distribution and out-of-distribution generalization on tractable tasks. Our experiments indicate that amortized point estimators generally outperform posterior inference, though the latter remain competitive in some low-dimensional problems, and we further discuss why this might be the case.

### Maximum Entropy Reinforcement Learning with Diffusion Policy 
[[arxiv](https://arxiv.org/abs/2502.11612)] [[cool](https://papers.cool/arxiv/2502.11612)] [[pdf](https://arxiv.org/pdf/2502.11612)]
> **Authors**: Xiaoyi Dong,Jian Cheng,Xi Sheryl Zhang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 21 pages, 7 figures
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: The Soft Actor-Critic (SAC) algorithm with a Gaussian policy has become a mainstream implementation for realizing the Maximum Entropy Reinforcement Learning (MaxEnt RL) objective, which incorporates entropy maximization to encourage exploration and enhance policy robustness. While the Gaussian policy performs well on simpler tasks, its exploration capacity and potential performance in complex multi-goal RL environments are limited by its inherent unimodality. In this paper, we employ the diffusion model, a powerful generative model capable of capturing complex multimodal distributions, as the policy representation to fulfill the MaxEnt RL objective, developing a method named MaxEnt RL with Diffusion Policy (MaxEntDP). Our method enables efficient exploration and brings the policy closer to the optimal MaxEnt policy. Experimental results on Mujoco benchmarks show that MaxEntDP outperforms the Gaussian policy and other generative models within the MaxEnt RL framework, and performs comparably to other state-of-the-art diffusion-based online RL algorithms. Our code is available at https://github.com/diffusionyes/MaxEntDP.

### Exploiting Task Relationships for Continual Learning Using Transferability-Aware Task Embeddings 
[[arxiv](https://arxiv.org/abs/2502.11609)] [[cool](https://papers.cool/arxiv/2502.11609)] [[pdf](https://arxiv.org/pdf/2502.11609)]
> **Authors**: Yanru Wu,Xiangyu Chen,Jianning Wang,Enming Zhang,Hanbing Liu,Yang Li
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Continual learning (CL) has been an essential topic in the contemporary application of deep neural networks, where catastrophic forgetting (CF) can impede a model's ability to acquire knowledge progressively. Existing CL strategies primarily address CF by regularizing model updates or separating task-specific and shared components. However, these methods focus on task model elements while overlooking the potential of leveraging inter-task relationships for learning enhancement. To address this, we propose a transferability-aware task embedding named H-embedding and train a hypernet under its guidance to learn task-conditioned model weights for CL tasks. Particularly, H-embedding is introduced based on an information theoretical transferability measure and is designed to be online and easy to compute. The framework is also characterized by notable practicality, which only requires storing a low-dimensional task embedding for each task, and can be efficiently trained in an end-to-end way. Extensive evaluations and experimental analyses on datasets including Permuted MNIST, Cifar10/100, and ImageNet-R demonstrate that our framework performs prominently compared to various baseline methods, displaying great potential in exploiting intrinsic task relationships.

### GraphThought: Graph Combinatorial Optimization with Thought Generation 
[[arxiv](https://arxiv.org/abs/2502.11607)] [[cool](https://papers.cool/arxiv/2502.11607)] [[pdf](https://arxiv.org/pdf/2502.11607)]
> **Authors**: Zixiao Huang,Lifeng Guo,Junjie Sheng,Haosheng Chen,Wenhao Li,Bo Jin,Changhong Lu,Xiangfeng Wang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 41 pages, 5 figures, 13 tables
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Large language models (LLMs) have demonstrated remarkable capabilities across various domains, especially in text processing and generative tasks. Recent advancements in the reasoning capabilities of state-of-the-art LLMs, such as OpenAI-o1, have significantly broadened their applicability, particularly in complex problem-solving and logical inference. However, most existing LLMs struggle with notable limitations in handling graph combinatorial optimization (GCO) problems. To bridge this gap, we formally define the Optimal Thoughts Design (OTD) problem, including its state and action thought space. We then introduce a novel framework, GraphThought, designed to generate high-quality thought datasets for GCO problems. Leveraging these datasets, we fine-tune the Llama-3-8B-Instruct model to develop Llama-GT. Notably, despite its compact 8B-parameter architecture, Llama-GT matches the performance of state-of-the-art LLMs on the GraphArena benchmark. Experimental results show that our approach outperforms both proprietary and open-source models, even rivaling specialized models like o1-mini. This work sets a new state-of-the-art benchmark while challenging the prevailing notion that model scale is the primary driver of reasoning capability.

### An Actor-Critic Algorithm with Function Approximation for Risk Sensitive Cost Markov Decision Processes 
[[arxiv](https://arxiv.org/abs/2502.11604)] [[cool](https://papers.cool/arxiv/2502.11604)] [[pdf](https://arxiv.org/pdf/2502.11604)]
> **Authors**: Soumyajit Guin,Vivek S. Borkar,Shalabh Bhatnagar
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: In this paper, we consider the risk-sensitive cost criterion with exponentiated costs for Markov decision processes and develop a model-free policy gradient algorithm in this setting. Unlike additive cost criteria such as average or discounted cost, the risk-sensitive cost criterion is less studied due to the complexity resulting from the multiplicative structure of the resulting Bellman equation. We develop an actor-critic algorithm with function approximation in this setting and provide its asymptotic convergence analysis. We also show the results of numerical experiments that demonstrate the superiority in performance of our algorithm over other recent algorithms in the literature.

### LLM Embeddings for Deep Learning on Tabular Data 
[[arxiv](https://arxiv.org/abs/2502.11596)] [[cool](https://papers.cool/arxiv/2502.11596)] [[pdf](https://arxiv.org/pdf/2502.11596)]
> **Authors**: Boshko Koloski,Andrei Margeloiu,Xiangjian Jiang,Blaž Škrlj,Nikola Simidjievski,Mateja Jamnik
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Tabular deep-learning methods require embedding numerical and categorical input features into high-dimensional spaces before processing them. Existing methods deal with this heterogeneous nature of tabular data by employing separate type-specific encoding approaches. This limits the cross-table transfer potential and the exploitation of pre-trained knowledge. We propose a novel approach that first transforms tabular data into text, and then leverages pre-trained representations from LLMs to encode this data, resulting in a plug-and-play solution to improv ing deep-learning tabular methods. We demonstrate that our approach improves accuracy over competitive models, such as MLP, ResNet and FT-Transformer, by validating on seven classification datasets.

### Towards a Trustworthy Anomaly Detection for Critical Applications through Approximated Partial AUC Loss 
[[arxiv](https://arxiv.org/abs/2502.11570)] [[cool](https://papers.cool/arxiv/2502.11570)] [[pdf](https://arxiv.org/pdf/2502.11570)]
> **Authors**: Arnaud Bougaham,Benoît Frénay
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: Anomaly Detection is a crucial step for critical applications such in the industrial, medical or cybersecurity domains. These sectors share the same requirement of handling differently the different types of classification errors. Indeed, even if false positives are acceptable, false negatives are not, because it would reflect a missed detection of a quality issue, a disease or a cyber threat. To fulfill this requirement, we propose a method that dynamically applies a trustworthy approximated partial AUC ROC loss (tapAUC). A binary classifier is trained to optimize the specific range of the AUC ROC curve that prevents the True Positive Rate (TPR) to reach 100% while minimizing the False Positive Rate (FPR). The optimal threshold that does not trigger any false negative is then kept and used at the test step. The results show a TPR of 92.52% at a 20.43% FPR for an average across 6 datasets, representing a TPR improvement of 4.3% for a FPR cost of 12.2% against other state-of-the-art methods. The code is available at https://github.com/ArnaudBougaham/tapAUC.

### Continuous Diffusion Model for Language Modeling 
[[arxiv](https://arxiv.org/abs/2502.11564)] [[cool](https://papers.cool/arxiv/2502.11564)] [[pdf](https://arxiv.org/pdf/2502.11564)]
> **Authors**: Jaehyeong Jo,Sung Ju Hwang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Diffusion models have emerged as a promising alternative to autoregressive models in modeling discrete categorical data. Yet diffusion models that directly work on discrete data space do not fully exploit the power of iterative refinement, as the signals are lost during the transition between discrete states. Existing continuous diffusion models for discrete data have limited performance compared to discrete approaches, and the unclear link between them restricts the development of diffusion models for discrete data. In this work, we propose a continuous diffusion model for language modeling that incorporates the geometry of the underlying categorical distribution. We establish a connection between the discrete diffusion and continuous flow on the statistical manifold, and building on the analogy, we introduce a simple design for the diffusion process that generalizes previous discrete diffusion models. We further propose a simulation-free training framework based on radial symmetry and a simple technique to address the high dimensionality of the manifold. Comprehensive experiments on language modeling benchmarks and other modalities show that our method outperforms existing discrete diffusion models and approaches the performance of autoregressive models. Codes available at \href{https://github.com/harryjo97/RDLM}{https://github.com/harryjo97/RDLM}.

### : A Modular World Model over Streams of Tokens 
[[arxiv](https://arxiv.org/abs/2502.11537)] [[cool](https://papers.cool/arxiv/2502.11537)] [[pdf](https://arxiv.org/pdf/2502.11537)]
> **Authors**: Lior Cohen,Kaixin Wang,Bingyi Kang,Uri Gadot,Shie Mannor
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Token-based world models emerged as a promising modular framework, modeling dynamics over token streams while optimizing tokenization separately. While successful in visual environments with discrete actions (e.g., Atari games), their broader applicability remains uncertain. In this paper, we introduce $\text{M}^{\text{3}}$, a $\textbf{m}$odular $\textbf{w}$orld $\textbf{m}$odel that extends this framework, enabling flexible combinations of observation and action modalities through independent modality-specific components. $\text{M}^{\text{3}}$ integrates several improvements from existing literature to enhance agent performance. Through extensive empirical evaluation across diverse benchmarks, $\text{M}^{\text{3}}$ achieves state-of-the-art sample efficiency for planning-free world models. Notably, among these methods, it is the first to reach a human-level median score on Atari 100K, with superhuman performance on 13 games. Our code and model weights are publicly available at https://github.com/leor-c/M3.

### MaZO: Masked Zeroth-Order Optimization for Multi-Task Fine-Tuning of Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.11513)] [[cool](https://papers.cool/arxiv/2502.11513)] [[pdf](https://arxiv.org/pdf/2502.11513)]
> **Authors**: Zhen Zhang,Yifan Yang,Kai Zhen,Nathan Susanj,Athanasios Mouchtaris,Siegfried Kunzmann,Zheng Zhang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 17 pages
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Large language models have demonstrated exceptional capabilities across diverse tasks, but their fine-tuning demands significant memory, posing challenges for resource-constrained environments. Zeroth-order (ZO) optimization provides a memory-efficient alternative by eliminating the need for backpropagation. However, ZO optimization suffers from high gradient variance, and prior research has largely focused on single-task learning, leaving its application to multi-task learning unexplored. Multi-task learning is crucial for leveraging shared knowledge across tasks to improve generalization, yet it introduces unique challenges under ZO settings, such as amplified gradient variance and collinearity. In this paper, we present MaZO, the first framework specifically designed for multi-task LLM fine-tuning under ZO optimization. MaZO tackles these challenges at the parameter level through two key innovations: a weight importance metric to identify critical parameters and a multi-task weight update mask to selectively update these parameters, reducing the dimensionality of the parameter space and mitigating task conflicts. Experiments demonstrate that MaZO achieves state-of-the-art performance, surpassing even multi-task learning methods designed for first-order optimization.

### DifCluE: Generating Counterfactual Explanations with Diffusion Autoencoders and modal clustering 
[[arxiv](https://arxiv.org/abs/2502.11509)] [[cool](https://papers.cool/arxiv/2502.11509)] [[pdf](https://arxiv.org/pdf/2502.11509)]
> **Authors**: Suparshva Jain,Amit Sangroya,Lovekesh Vig
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Generating multiple counterfactual explanations for different modes within a class presents a significant challenge, as these modes are distinct yet converge under the same classification. Diffusion probabilistic models (DPMs) have demonstrated a strong ability to capture the underlying modes of data distributions. In this paper, we harness the power of a Diffusion Autoencoder to generate multiple distinct counterfactual explanations. By clustering in the latent space, we uncover the directions corresponding to the different modes within a class, enabling the generation of diverse and meaningful counterfactuals. We introduce a novel methodology, DifCluE, which consistently identifies these modes and produces more reliable counterfactual explanations. Our experimental results demonstrate that DifCluE outperforms the current state-of-the-art in generating multiple counterfactual explanations, offering a significant advancement in model interpretability.

### Learning Surrogate Potential Mean Field Games via Gaussian Processes: A Data-Driven Approach to Ill-Posed Inverse Problems 
[[arxiv](https://arxiv.org/abs/2502.11506)] [[cool](https://papers.cool/arxiv/2502.11506)] [[pdf](https://arxiv.org/pdf/2502.11506)]
> **Authors**: Jingguo Zhang,Xianjin Yang,Chenchen Mou,Chao Zhou
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 36 pages
- **标题**: None
- **领域**: 机器学习,优化与控制,机器学习
- **Abstract**: Mean field games (MFGs) describe the collective behavior of large populations of interacting agents. In this work, we tackle ill-posed inverse problems in potential MFGs, aiming to recover the agents' population, momentum, and environmental setup from limited, noisy measurements and partial observations. These problems are ill-posed because multiple MFG configurations can explain the same data, or different parameters can yield nearly identical observations. Nonetheless, they remain crucial in practice for real-world scenarios where data are inherently sparse or noisy, or where the MFG structure is not fully determined. Our focus is on finding surrogate MFGs that accurately reproduce the observed data despite these challenges. We propose two Gaussian process (GP)-based frameworks: an inf-sup formulation and a bilevel approach. The choice between them depends on whether the unknown parameters introduce concavity in the objective. In the inf-sup framework, we use the linearity of GPs and their parameterization structure to maintain convex-concave properties, allowing us to apply standard convex optimization algorithms. In the bilevel framework, we employ a gradient-descent-based algorithm and introduce two methods for computing the outer gradient. The first method leverages an existing solver for the inner potential MFG and applies automatic differentiation, while the second adopts an adjoint-based strategy that computes the outer gradient independently of the inner solver. Our numerical experiments show that when sufficient prior information is available, the unknown parameters can be accurately recovered. Otherwise, if prior information is limited, the inverse problem is ill-posed, but our frameworks can still produce surrogate MFG models that closely match observed data.

### A GNN-based Spectral Filtering Mechanism for Imbalance Classification in Network Digital Twin 
[[arxiv](https://arxiv.org/abs/2502.11505)] [[cool](https://papers.cool/arxiv/2502.11505)] [[pdf](https://arxiv.org/pdf/2502.11505)]
> **Authors**: Abubakar Isah,Ibrahim Aliyu,Sulaiman Muhammad Rashid,Jaehyung Park,Minsoo Hahn,Jinsul Kim
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: arXiv admin note: substantial text overlap with arXiv:2406.06595
- **标题**: None
- **领域**: 机器学习,网络和互联网架构
- **Abstract**: Graph Neural Networks are gaining attention in Fifth-Generation (5G) core network digital twins, which are data-driven complex systems with numerous components. Analyzing these data can be challenging due to rare failure types, leading to imbalanced classification in multiclass settings. Digital twins of 5G networks increasingly employ graph classification as the main method for identifying failure types. However, the skewed distribution of failure occurrences is a major class imbalance issue that prevents effective graph data mining. Previous studies have not sufficiently tackled this complex problem. In this paper, we propose Class-Fourier Graph Neural Network (CF-GNN) introduces a class-oriented spectral filtering mechanism that ensures precise classification by estimating a unique spectral filter for each class. We employ eigenvalue and eigenvector spectral filtering to capture and adapt to variations in the minority classes, ensuring accurate class-specific feature discrimination, and adept at graph representation learning for complex local structures among neighbors in an end-to-end setting. Extensive experiments have demonstrated that the proposed CF-GNN could help with both the creation of new techniques for enhancing classifiers and the investigation of the characteristics of the multi-class imbalanced data in a network digital twin system.

### Accelerated Gradient-based Design Optimization Via Differentiable Physics-Informed Neural Operator: A Composites Autoclave Processing Case Study 
[[arxiv](https://arxiv.org/abs/2502.11504)] [[cool](https://papers.cool/arxiv/2502.11504)] [[pdf](https://arxiv.org/pdf/2502.11504)]
> **Authors**: Janak M. Patel,Milad Ramezankhani,Anirudh Deodhar,Dagnachew Birru
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 15 pages, 7 figures
- **标题**: None
- **领域**: 机器学习,人工智能,数值分析
- **Abstract**: Simulation and optimization are crucial for advancing the engineering design of complex systems and processes. Traditional optimization methods require substantial computational time and effort due to their reliance on resource-intensive simulations, such as finite element analysis, and the complexity of rigorous optimization algorithms. Data-agnostic AI-based surrogate models, such as Physics-Informed Neural Operators (PINOs), offer a promising alternative to these conventional simulations, providing drastically reduced inference time, unparalleled data efficiency, and zero-shot super-resolution capability. However, the predictive accuracy of these models is often constrained to small, low-dimensional design spaces or systems with relatively simple dynamics. To address this, we introduce a novel Physics-Informed DeepONet (PIDON) architecture, which extends the capabilities of conventional neural operators to effectively model the nonlinear behavior of complex engineering systems across high-dimensional design spaces and a wide range of dynamic design configurations. This new architecture outperforms existing SOTA models, enabling better predictions across broader design spaces. Leveraging PIDON's differentiability, we integrate a gradient-based optimization approach using the Adam optimizer to efficiently determine optimal design variables. This forms an end-to-end gradient-based optimization framework that accelerates the design process while enhancing scalability and efficiency. We demonstrate the effectiveness of this framework in the optimization of aerospace-grade composites curing processes achieving a 3x speedup in obtaining optimal design variables compared to gradient-free methods. Beyond composites processing, the proposed model has the potential to be used as a scalable and efficient optimization tool for broader applications in advanced engineering and digital twin systems.

### GPU-accelerated Multi-relational Parallel Graph Retrieval for Web-scale Recommendations 
[[arxiv](https://arxiv.org/abs/2502.11490)] [[cool](https://papers.cool/arxiv/2502.11490)] [[pdf](https://arxiv.org/pdf/2502.11490)]
> **Authors**: Zhuoning Guo,Guangxing Chen,Qian Gao,Xiaochao Liao,Jianjia Zheng,Lu Shen,Hao Liu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,分布式、并行和集群计算,信息检索
- **Abstract**: Web recommendations provide personalized items from massive catalogs for users, which rely heavily on retrieval stages to trade off the effectiveness and efficiency of selecting a small relevant set from billion-scale candidates in online digital platforms. As one of the largest Chinese search engine and news feed providers, Baidu resorts to Deep Neural Network (DNN) and graph-based Approximate Nearest Neighbor Search (ANNS) algorithms for accurate relevance estimation and efficient search for relevant items. However, current retrieval at Baidu fails in comprehensive user-item relational understanding due to dissected interaction modeling, and performs inefficiently in large-scale graph-based ANNS because of suboptimal traversal navigation and the GPU computational bottleneck under high concurrency. To this end, we propose a GPU-accelerated Multi-relational Parallel Graph Retrieval (GMP-GR) framework to achieve effective yet efficient retrieval in web-scale recommendations. First, we propose a multi-relational user-item relevance metric learning method that unifies diverse user behaviors through multi-objective optimization and employs a self-covariant loss to enhance pathfinding performance. Second, we develop a hierarchical parallel graph-based ANNS to boost graph retrieval throughput, which conducts breadth-depth-balanced searches on a large-scale item graph and cost-effectively handles irregular neural computation via adaptive aggregation on GPUs. In addition, we integrate system optimization strategies in the deployment of GMP-GR in Baidu. Extensive experiments demonstrate the superiority of GMP-GR in retrieval accuracy and efficiency. Deployed across more than twenty applications at Baidu, GMP-GR serves hundreds of millions of users with a throughput exceeding one hundred million requests per second.

### Dictionary-Learning-Based Data Pruning for System Identification 
[[arxiv](https://arxiv.org/abs/2502.11484)] [[cool](https://papers.cool/arxiv/2502.11484)] [[pdf](https://arxiv.org/pdf/2502.11484)]
> **Authors**: Tingna Wang,Sikai Zhang,Limin Sun
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,系统与控制
- **Abstract**: System identification is normally involved in augmenting time series data by time shifting and nonlinearisation (via polynomial basis), which introduce redundancy both feature-wise and sample-wise. Many research works focus on reducing redundancy feature-wise, while less attention is paid to sample-wise redundancy. This paper proposes a novel data pruning method, called (mini-batch) FastCan, to reduce sample-wise redundancy based on dictionary learning. Time series data is represented by some representative samples, called atoms, via dictionary learning. The useful samples are selected based on their correlation with the atoms. The method is tested on one simulated dataset and two benchmark datasets. The R-squared between the coefficients of models trained on the full and the coefficients of models trained on pruned datasets is adopted to evaluate the performance of data pruning methods. It is found that the proposed method significantly outperforms the random pruning method.

### No-regret incentive-compatible online learning under exact truthfulness with non-myopic experts 
[[arxiv](https://arxiv.org/abs/2502.11483)] [[cool](https://papers.cool/arxiv/2502.11483)] [[pdf](https://arxiv.org/pdf/2502.11483)]
> **Authors**: Junpei Komiyama,Nishant A. Mehta,Ali Mortazavi
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 44 pages
- **标题**: None
- **领域**: 机器学习,计算机科学与博弈论,机器学习
- **Abstract**: We study an online forecasting setting in which, over $T$ rounds, $N$ strategic experts each report a forecast to a mechanism, the mechanism selects one forecast, and then the outcome is revealed. In any given round, each expert has a belief about the outcome, but the expert wishes to select its report so as to maximize the total number of times it is selected. The goal of the mechanism is to obtain low belief regret: the difference between its cumulative loss (based on its selected forecasts) and the cumulative loss of the best expert in hindsight (as measured by the experts' beliefs). We consider exactly truthful mechanisms for non-myopic experts, meaning that truthfully reporting its belief strictly maximizes the expert's subjective probability of being selected in any future round. Even in the full-information setting, it is an open problem to obtain the first no-regret exactly truthful mechanism in this setting. We develop the first no-regret mechanism for this setting via an online extension of the Independent-Event Lotteries Forecasting Competition Mechanism (I-ELF). By viewing this online I-ELF as a novel instance of Follow the Perturbed Leader (FPL) with noise based on random walks with loss-dependent perturbations, we obtain $\tilde{O}(\sqrt{T N})$ regret. Our results are fueled by new tail bounds for Poisson binomial random variables that we develop. We extend our results to the bandit setting, where we give an exactly truthful mechanism obtaining $\tilde{O}(T^{2/3} N^{1/3})$ regret; this is the first no-regret result even among approximately truthful mechanisms.

### DATA: Decomposed Attention-based Task Adaptation for Rehearsal-Free Continual Learning 
[[arxiv](https://arxiv.org/abs/2502.11482)] [[cool](https://papers.cool/arxiv/2502.11482)] [[pdf](https://arxiv.org/pdf/2502.11482)]
> **Authors**: Huanxuan Liao,Shizhu He,Yupu Hao,Jun Zhao,Kang Liu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Continual learning (CL) is essential for Large Language Models (LLMs) to adapt to evolving real-world demands, yet they are susceptible to catastrophic forgetting (CF). While traditional CF solutions rely on expensive data rehearsal, recent rehearsal-free methods employ model-based and regularization-based strategies to address this issue. However, these approaches often neglect the model's plasticity, which is crucial to achieving optimal performance on newly learned tasks. Consequently, a key challenge in CL is striking a balance between preserving plasticity and mitigating CF. To tackle this challenge, we propose the $\textbf{D}$ecomposed $\textbf{A}$ttention-based $\textbf{T}$ask $\textbf{A}$daptation (DATA), which explicitly decouples and learns both task-specific and task-shared knowledge using high-rank and low-rank task adapters (e.g., LoRAs). For new tasks, DATA dynamically adjusts the weights of adapters of different ranks based on their relevance and distinction from previous tasks, allowing the model to acquire new task-specific skills while effectively retaining previously learned knowledge. Specifically, we implement a decomposed component weighting strategy comprising learnable components that collectively generate attention-based weights, allowing the model to integrate and utilize diverse knowledge from each DATA. Extensive experiments on three widely used benchmarks demonstrate that our proposed method achieves state-of-the-art performance. Notably, our approach significantly enhances model plasticity and mitigates CF by extending learnable components and employing stochastic restoration during training iterations.

### Enhancing Offline Model-Based RL via Active Model Selection: A Bayesian Optimization Perspective 
[[arxiv](https://arxiv.org/abs/2502.11480)] [[cool](https://papers.cool/arxiv/2502.11480)] [[pdf](https://arxiv.org/pdf/2502.11480)]
> **Authors**: Yu-Wei Yang,Yun-Ming Chan,Wei Hung,Xi Liu,Ping-Chun Hsieh
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 18 pages, 6 figures
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Offline model-based reinforcement learning (MBRL) serves as a competitive framework that can learn well-performing policies solely from pre-collected data with the help of learned dynamics models. To fully unleash the power of offline MBRL, model selection plays a pivotal role in determining the dynamics model utilized for downstream policy learning. However, offline MBRL conventionally relies on validation or off-policy evaluation, which are rather inaccurate due to the inherent distribution shift in offline RL. To tackle this, we propose BOMS, an active model selection framework that enhances model selection in offline MBRL with only a small online interaction budget, through the lens of Bayesian optimization (BO). Specifically, we recast model selection as BO and enable probabilistic inference in BOMS by proposing a novel model-induced kernel, which is theoretically grounded and computationally efficient. Through extensive experiments, we show that BOMS improves over the baseline methods with a small amount of online interaction comparable to only $1\%$-$2.5\%$ of offline training data on various RL tasks.

### Approximation of Permutation Invariant Polynomials by Transformers: Efficient Construction in Column-Size 
[[arxiv](https://arxiv.org/abs/2502.11467)] [[cool](https://papers.cool/arxiv/2502.11467)] [[pdf](https://arxiv.org/pdf/2502.11467)]
> **Authors**: Naoki Takeshita,Masaaki Imaizumi
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 29 pages
- **标题**: None
- **领域**: 机器学习,泛函分析
- **Abstract**: Transformers are a type of neural network that have demonstrated remarkable performance across various domains, particularly in natural language processing tasks. Motivated by this success, research on the theoretical understanding of transformers has garnered significant attention. A notable example is the mathematical analysis of their approximation power, which validates the empirical expressive capability of transformers. In this study, we investigate the ability of transformers to approximate column-symmetric polynomials, an extension of symmetric polynomials that take matrices as input. Consequently, we establish an explicit relationship between the size of the transformer network and its approximation capability, leveraging the parameter efficiency of transformers and their compatibility with symmetry by focusing on the algebraic properties of symmetric polynomials.

### GiFT: Gibbs Fine-Tuning for Code Generation 
[[arxiv](https://arxiv.org/abs/2502.11466)] [[cool](https://papers.cool/arxiv/2502.11466)] [[pdf](https://arxiv.org/pdf/2502.11466)]
> **Authors**: Haochen Li,Wanjin Feng,Xin Zhou,Zhiqi Shen
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学,软件工程
- **Abstract**: Training Large Language Models (LLMs) with synthetic data is a prevalent practice in code generation. A key approach is self-training, where LLMs are iteratively trained on self-generated correct code snippets. In this case, the self-generated codes are drawn from a conditional distribution, conditioned on a specific seed description. However, the seed description is not the only valid representation that aligns with its intended meaning. With all valid descriptions and codes forming a joint space, codes drawn from the conditional distribution would lead to an underrepresentation of the full description-code space. As such, we propose Gibbs Fine-Tuning (GiFT), a novel self-training method inspired by Gibbs sampling. GiFT allows self-generated data to be drawn from the marginal distribution of the joint space, thereby mitigating the biases inherent in conditional sampling. We provide a theoretical analysis demonstrating the potential benefits of fine-tuning LLMs with code derived from the marginal distribution. Furthermore, we propose a perplexity-based code selection method to mitigate the imbalanced long-tail distribution of the self-generated codes. Empirical evaluation of two LLMs across four datasets demonstrates that GiFT achieves superior performance, particularly on more challenging benchmarks.

### Towards Efficient Pre-training: Exploring FP4 Precision in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.11458)] [[cool](https://papers.cool/arxiv/2502.11458)] [[pdf](https://arxiv.org/pdf/2502.11458)]
> **Authors**: Jiecheng Zhou,Ding Tang,Rong Fu,Boni Hu,Haoran Xu,Yi Wang,Zhilin Pei,Zhongling Su,Liang Liu,Xingcheng Zhang,Weiming Zhang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 8 pages, 2 figure
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: The burgeoning computational demands for training large language models (LLMs) necessitate efficient methods, including quantized training, which leverages low-bit arithmetic operations to reduce costs. While FP8 precision has shown potential, leveraging FP4 remains challenging due to inherent quantization errors and limited representation capability. Based on the Transformer architecture, we present an FP4 training scheme for LLMs, overcoming these obstacles through mixed-precision quantization strategies tailed for different modules and training stages. This allows us to apply the precision level suitable to distinct components within the model, ensuring that multi-head attention and linear layers are handled appropriately. Our pretraining recipe ensures stability in backpropagation by incorporating fine-grained quantization methods with a target precision training schedule. Experimental results demonstrate that our FP4 training scheme achieves accuracy comparable to BF16 and FP8, with smaller theoretical computational cost. With the advent of next-generation hardware supporting FP4, our method sets the foundation for efficient ultra-low precision training.

### Connector-S: A Survey of Connectors in Multi-modal Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.11453)] [[cool](https://papers.cool/arxiv/2502.11453)] [[pdf](https://arxiv.org/pdf/2502.11453)]
> **Authors**: Xun Zhu,Zheng Zhang,Xi Chen,Yiming Shi,Miao Li,Ji Wu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: With the rapid advancements in multi-modal large language models (MLLMs), connectors play a pivotal role in bridging diverse modalities and enhancing model performance. However, the design and evolution of connectors have not been comprehensively analyzed, leaving gaps in understanding how these components function and hindering the development of more powerful connectors. In this survey, we systematically review the current progress of connectors in MLLMs and present a structured taxonomy that categorizes connectors into atomic operations (mapping, compression, mixture of experts) and holistic designs (multi-layer, multi-encoder, multi-modal scenarios), highlighting their technical contributions and advancements. Furthermore, we discuss several promising research frontiers and challenges, including high-resolution input, dynamic compression, guide information selection, combination strategy, and interpretability. This survey is intended to serve as a foundational reference and a clear roadmap for researchers, providing valuable insights into the design and optimization of next-generation connectors to enhance the performance and adaptability of MLLMs.

### Fishing For Cheap And Efficient Pruners At Initialization 
[[arxiv](https://arxiv.org/abs/2502.11450)] [[cool](https://papers.cool/arxiv/2502.11450)] [[pdf](https://arxiv.org/pdf/2502.11450)]
> **Authors**: Ivo Gollini Navarrete,Nicolas Mauricio Cuadrado,Jose Renato Restom,Martin Takáč,Samuel Horváth
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 8 pages of main content (excluding references), 2 figures, 2 tables, 1 algorithm, and 11 pages of appendix. Code available at https://github.com/Gollini/Fisher_Taylor_Sensitivity
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Pruning offers a promising solution to mitigate the associated costs and environmental impact of deploying large deep neural networks (DNNs). Traditional approaches rely on computationally expensive trained models or time-consuming iterative prune-retrain cycles, undermining their utility in resource-constrained settings. To address this issue, we build upon the established principles of saliency (LeCun et al., 1989) and connection sensitivity (Lee et al., 2018) to tackle the challenging problem of one-shot pruning neural networks (NNs) before training (PBT) at initialization. We introduce Fisher-Taylor Sensitivity (FTS), a computationally cheap and efficient pruning criterion based on the empirical Fisher Information Matrix (FIM) diagonal, offering a viable alternative for integrating first- and second-order information to identify a model's structurally important parameters. Although the FIM-Hessian equivalency only holds for convergent models that maximize the likelihood, recent studies (Karakida et al., 2019) suggest that, even at initialization, the FIM captures essential geometric information of parameters in overparameterized NNs, providing the basis for our method. Finally, we demonstrate empirically that layer collapse, a critical limitation of data-dependent pruning methodologies, is easily overcome by pruning within a single training epoch after initialization. We perform experiments on ResNet18 and VGG19 with CIFAR-10 and CIFAR-100, widely used benchmarks in pruning research. Our method achieves competitive performance against state-of-the-art techniques for one-shot PBT, even under extreme sparsity conditions. Our code is made available to the public.

### Does Editing Provide Evidence for Localization? 
[[arxiv](https://arxiv.org/abs/2502.11447)] [[cool](https://papers.cool/arxiv/2502.11447)] [[pdf](https://arxiv.org/pdf/2502.11447)]
> **Authors**: Zihao Wang,Victor Veitch
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: :68T50ACM Class:I.2.7; I.2.6; F.1.1
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: A basic aspiration for interpretability research in large language models is to "localize" semantically meaningful behaviors to particular components within the LLM. There are various heuristics for finding candidate locations within the LLM. Once a candidate localization is found, it can be assessed by editing the internal representations at the corresponding localization and checking whether this induces model behavior that is consistent with the semantic interpretation of the localization. The question we address here is: how strong is the evidence provided by such edits? To evaluate the localization claim, we want to assess the effect of the optimal intervention at a particular location. The key new technical tool is a way of adapting LLM alignment techniques to find such optimal localized edits. With this tool in hand, we give an example where the edit-based evidence for localization appears strong, but where localization clearly fails. Indeed, we find that optimal edits at random localizations can be as effective as aligning the full model. In aggregate, our results suggest that merely observing that localized edits induce targeted changes in behavior provides little to no evidence that these locations actually encode the target behavior.

## 计算机科学中的逻辑(cs.LO:Logic in Computer Science)

### Towards Practical First-Order Model Counting 
[[arxiv](https://arxiv.org/abs/2502.12278)] [[cool](https://papers.cool/arxiv/2502.12278)] [[pdf](https://arxiv.org/pdf/2502.12278)]
> **Authors**: Ananth K. Kidambi,Guramrit Singh,Paulius Dilkas,Kuldeep S. Meel
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 18 pages, 1 figure
- **标题**: None
- **领域**: 计算机科学中的逻辑,人工智能
- **Abstract**: First-order model counting (FOMC) is the problem of counting the number of models of a sentence in first-order logic. Since lifted inference techniques rely on reductions to variants of FOMC, the design of scalable methods for FOMC has attracted attention from both theoreticians and practitioners over the past decade. Recently, a new approach based on first-order knowledge compilation was proposed. This approach, called Crane, instead of simply providing the final count, generates definitions of (possibly recursive) functions that can be evaluated with different arguments to compute the model count for any domain size. However, this approach is not fully automated, as it requires manual evaluation of the constructed functions. The primary contribution of this work is a fully automated compilation algorithm, called Gantry, which transforms the function definitions into C++ code equipped with arbitrary-precision arithmetic. These additions allow the new FOMC algorithm to scale to domain sizes over 500,000 times larger than the current state of the art, as demonstrated through experimental results.

## 多代理系统(cs.MA:Multiagent Systems)

### HARBOR: Exploring Persona Dynamics in Multi-Agent Competition 
[[arxiv](https://arxiv.org/abs/2502.12149)] [[cool](https://papers.cool/arxiv/2502.12149)] [[pdf](https://arxiv.org/pdf/2502.12149)]
> **Authors**: Kenan Jiang,Li Xiong,Fei Liu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 多代理系统,人工智能,计算语言学
- **Abstract**: We investigate factors contributing to LLM agents' success in competitive multi-agent environments, using auctions as a testbed where agents bid to maximize profit. The agents are equipped with bidding domain knowledge, distinct personas that reflect item preferences, and a memory of auction history. Our work extends the classic auction scenario by creating a realistic environment where multiple agents bid on houses, weighing aspects such as size, location, and budget to secure the most desirable homes at the lowest prices. Particularly, we investigate three key questions: (a) How does a persona influence an agent's behavior in a competitive setting? (b) Can an agent effectively profile its competitors' behavior during auctions? (c) How can persona profiling be leveraged to create an advantage using strategies such as theory of mind? Through a series of experiments, we analyze the behaviors of LLM agents and shed light on new findings. Our testbed, called HARBOR, offers a valuable platform for deepening our understanding of multi-agent workflows in competitive environments.

### Generative Multi-Agent Collaboration in Embodied AI: A Systematic Review 
[[arxiv](https://arxiv.org/abs/2502.11518)] [[cool](https://papers.cool/arxiv/2502.11518)] [[pdf](https://arxiv.org/pdf/2502.11518)]
> **Authors**: Di Wu,Xian Wei,Guang Chen,Hao Shen,Xiangfeng Wang,Wenhao Li,Bo Jin
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 18 pages
- **标题**: None
- **领域**: 多代理系统,人工智能,机器学习
- **Abstract**: Embodied multi-agent systems (EMAS) have attracted growing attention for their potential to address complex, real-world challenges in areas such as logistics and robotics. Recent advances in foundation models pave the way for generative agents capable of richer communication and adaptive problem-solving. This survey provides a systematic examination of how EMAS can benefit from these generative capabilities. We propose a taxonomy that categorizes EMAS by system architectures and embodiment modalities, emphasizing how collaboration spans both physical and virtual contexts. Central building blocks, perception, planning, communication, and feedback, are then analyzed to illustrate how generative techniques bolster system robustness and flexibility. Through concrete examples, we demonstrate the transformative effects of integrating foundation models into embodied, multi-agent frameworks. Finally, we discuss challenges and future directions, underlining the significant promise of EMAS to reshape the landscape of AI-driven collaboration.

## 神经和进化计算(cs.NE:Neural and Evolutionary Computing)

### An Algorithm Board in Neural Decoding 
[[arxiv](https://arxiv.org/abs/2502.12536)] [[cool](https://papers.cool/arxiv/2502.12536)] [[pdf](https://arxiv.org/pdf/2502.12536)]
> **Authors**: Jingyi Feng,Kai Yang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 16 pages, 10 figures, 2 tables
- **标题**: None
- **领域**: 神经和进化计算,人工智能
- **Abstract**: Understanding the mechanisms of neural encoding and decoding has always been a highly interesting research topic in fields such as neuroscience and cognitive intelligence. In prior studies, some researchers identified a symmetry in neural data decoded by unsupervised methods in motor scenarios and constructed a cognitive learning system based on this pattern (i.e., symmetry). Nevertheless, the distribution state of the data flow that significantly influences neural decoding positions still remains a mystery within the system, which further restricts the enhancement of the system's interpretability. Based on this, this paper mainly explores changes in the distribution state within the system from the machine learning and mathematical statistics perspectives. In the experiment, we assessed the correctness of this symmetry using various tools and indicators commonly utilized in mathematics and statistics. According to the experimental results, the normal distribution (or Gaussian distribution) plays a crucial role in the decoding of prediction positions within the system. Eventually, an algorithm board similar to the Galton board was built to serve as the mathematical foundation of the discovered symmetry.

### Application-oriented automatic hyperparameter optimization for spiking neural network prototyping 
[[arxiv](https://arxiv.org/abs/2502.12172)] [[cool](https://papers.cool/arxiv/2502.12172)] [[pdf](https://arxiv.org/pdf/2502.12172)]
> **Authors**: Vittorio Fra
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 神经和进化计算,机器学习
- **Abstract**: Hyperparameter optimization (HPO) is of paramount importance in the development of high-performance, specialized artificial intelligence (AI) models, ranging from well-established machine learning (ML) solutions to the deep learning (DL) domain and the field of spiking neural networks (SNNs). The latter introduce further complexity due to the neuronal computational units and their additional hyperparameters, whose inadequate setting can dramatically impact the final model performance. At the cost of possible reduced generalization capabilities, the most suitable strategy to fully disclose the power of SNNs is to adopt an application-oriented approach and perform extensive HPO experiments. To facilitate these operations, automatic pipelines are fundamental, and their configuration is crucial. In this document, the Neural Network Intelligence (NNI) toolkit is used as reference framework to present one such solution, with a use case example providing evidence of the corresponding results. In addition, a summary of published works employing the presented pipeline is reported as possible source of insights into application-oriented HPO experiments for SNN prototyping.

### Scalable and Robust Physics-Informed Graph Neural Networks for Water Distribution Systems 
[[arxiv](https://arxiv.org/abs/2502.12164)] [[cool](https://papers.cool/arxiv/2502.12164)] [[pdf](https://arxiv.org/pdf/2502.12164)]
> **Authors**: Inaam Ashraf,André Artelt,Barbara Hammer
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 神经和进化计算,机器学习,系统与控制
- **Abstract**: Water distribution systems (WDSs) are an important part of critical infrastructure becoming increasingly significant in the face of climate change and urban population growth. We propose a robust and scalable surrogate deep learning (DL) model to enable efficient planning, expansion, and rehabilitation of WDSs. Our approach incorporates an improved graph neural network architecture, an adapted physics-informed algorithm, an innovative training scheme, and a physics-preserving data normalization method. Evaluation results on a number of WDSs demonstrate that our model outperforms the current state-of-the-art DL model. Moreover, our method allows us to scale the model to bigger and more realistic WDSs. Furthermore, our approach makes the model more robust to out-of-distribution input features (demands, pipe diameters). Hence, our proposed method constitutes a significant step towards bridging the simulation-to-real gap in the use of artificial intelligence for WDSs.

## 机器人技术(cs.RO:Robotics)

### GSCE: A Prompt Framework with Enhanced Reasoning for Reliable LLM-driven Drone Control 
[[arxiv](https://arxiv.org/abs/2502.12531)] [[cool](https://papers.cool/arxiv/2502.12531)] [[pdf](https://arxiv.org/pdf/2502.12531)]
> **Authors**: Wenhao Wang,Yanyan Li,Long Jiao,Jiawei Yuan
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 8 pages
- **标题**: None
- **领域**: 机器人技术,人工智能
- **Abstract**: The integration of Large Language Models (LLMs) into robotic control, including drones, has the potential to revolutionize autonomous systems. Research studies have demonstrated that LLMs can be leveraged to support robotic operations. However, when facing tasks with complex reasoning, concerns and challenges are raised about the reliability of solutions produced by LLMs. In this paper, we propose a prompt framework with enhanced reasoning to enable reliable LLM-driven control for drones. Our framework consists of novel technical components designed using Guidelines, Skill APIs, Constraints, and Examples, namely GSCE. GSCE is featured by its reliable and constraint-compliant code generation. We performed thorough experiments using GSCE for the control of drones with a wide level of task complexities. Our experiment results demonstrate that GSCE can significantly improve task success rates and completeness compared to baseline approaches, highlighting its potential for reliable LLM-driven autonomous drone systems.

### Soft Robotics for Search and Rescue: Advancements, Challenges, and Future Directions 
[[arxiv](https://arxiv.org/abs/2502.12373)] [[cool](https://papers.cool/arxiv/2502.12373)] [[pdf](https://arxiv.org/pdf/2502.12373)]
> **Authors**: Abhishek Sebastian
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,人工智能
- **Abstract**: Soft robotics has emerged as a transformative technology in Search and Rescue (SAR) operations, addressing challenges in navigating complex, hazardous environments that often limit traditional rigid robots. This paper critically examines advancements in soft robotic technologies tailored for SAR applications, focusing on their unique capabilities in adaptability, safety, and efficiency. By leveraging bio-inspired designs, flexible materials, and advanced locomotion mechanisms, such as crawling, rolling, and shape morphing, soft robots demonstrate exceptional potential in disaster scenarios. However, significant barriers persist, including material durability, power inefficiency, sensor integration, and control complexity. This comprehensive review highlights the current state of soft robotics in SAR, discusses simulation methodologies and hardware validations, and introduces performance metrics essential for their evaluation. By bridging the gap between theoretical advancements and practical deployment, this study underscores the potential of soft robotic systems to revolutionize SAR missions and advocates for continued interdisciplinary innovation to overcome existing limitations.

### IMLE Policy: Fast and Sample Efficient Visuomotor Policy Learning via Implicit Maximum Likelihood Estimation 
[[arxiv](https://arxiv.org/abs/2502.12371)] [[cool](https://papers.cool/arxiv/2502.12371)] [[pdf](https://arxiv.org/pdf/2502.12371)]
> **Authors**: Krishan Rana,Robert Lee,David Pershouse,Niko Suenderhauf
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Videos and code are available at https://imle-policy.github.io/
- **标题**: None
- **领域**: 机器人技术,人工智能,机器学习
- **Abstract**: Recent advances in imitation learning, particularly using generative modelling techniques like diffusion, have enabled policies to capture complex multi-modal action distributions. However, these methods often require large datasets and multiple inference steps for action generation, posing challenges in robotics where the cost for data collection is high and computation resources are limited. To address this, we introduce IMLE Policy, a novel behaviour cloning approach based on Implicit Maximum Likelihood Estimation (IMLE). IMLE Policy excels in low-data regimes, effectively learning from minimal demonstrations and requiring 38\% less data on average to match the performance of baseline methods in learning complex multi-modal behaviours. Its simple generator-based architecture enables single-step action generation, improving inference speed by 97.3\% compared to Diffusion Policy, while outperforming single-step Flow Matching. We validate our approach across diverse manipulation tasks in simulated and real-world environments, showcasing its ability to capture complex behaviours under data constraints. Videos and code are provided on our project page: https://imle-policy.github.io/.

### Hovering Flight of Soft-Actuated Insect-Scale Micro Aerial Vehicles using Deep Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.12355)] [[cool](https://papers.cool/arxiv/2502.12355)] [[pdf](https://arxiv.org/pdf/2502.12355)]
> **Authors**: Yi-Hsuan Hsiao,Wei-Tung Chen,Yun-Sheng Chang,Pulkit Agrawal,YuFeng Chen
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 7 pages, 7 figures, accepted to 2025 IEEE International Conference on Soft Robotics (RoboSoft)
- **标题**: None
- **领域**: 机器人技术,机器学习,系统与控制
- **Abstract**: Soft-actuated insect-scale micro aerial vehicles (IMAVs) pose unique challenges for designing robust and computationally efficient controllers. At the millimeter scale, fast robot dynamics ($\sim$ms), together with system delay, model uncertainty, and external disturbances significantly affect flight performances. Here, we design a deep reinforcement learning (RL) controller that addresses system delay and uncertainties. To initialize this neural network (NN) controller, we propose a modified behavior cloning (BC) approach with state-action re-matching to account for delay and domain-randomized expert demonstration to tackle uncertainty. Then we apply proximal policy optimization (PPO) to fine-tune the policy during RL, enhancing performance and smoothing commands. In simulations, our modified BC substantially increases the mean reward compared to baseline BC; and RL with PPO improves flight quality and reduces command fluctuations. We deploy this controller on two different insect-scale aerial robots that weigh 720 mg and 850 mg, respectively. The robots demonstrate multiple successful zero-shot hovering flights, with the longest lasting 50 seconds and root-mean-square errors of 1.34 cm in lateral direction and 0.05 cm in altitude, marking the first end-to-end deep RL-based flight on soft-driven IMAVs.

### X-IL: Exploring the Design Space of Imitation Learning Policies 
[[arxiv](https://arxiv.org/abs/2502.12330)] [[cool](https://papers.cool/arxiv/2502.12330)] [[pdf](https://arxiv.org/pdf/2502.12330)]
> **Authors**: Xiaogang Jia,Atalay Donat,Xi Huang,Xuan Zhao,Denis Blessing,Hongyi Zhou,Han A. Wang,Hanyi Zhang,Qian Wang,Rudolf Lioutikov,Gerhard Neumann
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,机器学习
- **Abstract**: Designing modern imitation learning (IL) policies requires making numerous decisions, including the selection of feature encoding, architecture, policy representation, and more. As the field rapidly advances, the range of available options continues to grow, creating a vast and largely unexplored design space for IL policies. In this work, we present X-IL, an accessible open-source framework designed to systematically explore this design space. The framework's modular design enables seamless swapping of policy components, such as backbones (e.g., Transformer, Mamba, xLSTM) and policy optimization techniques (e.g., Score-matching, Flow-matching). This flexibility facilitates comprehensive experimentation and has led to the discovery of novel policy configurations that outperform existing methods on recent robot learning benchmarks. Our experiments demonstrate not only significant performance gains but also provide valuable insights into the strengths and weaknesses of various design choices. This study serves as both a practical reference for practitioners and a foundation for guiding future research in imitation learning.

### Learning Getting-Up Policies for Real-World Humanoid Robots 
[[arxiv](https://arxiv.org/abs/2502.12152)] [[cool](https://papers.cool/arxiv/2502.12152)] [[pdf](https://arxiv.org/pdf/2502.12152)]
> **Authors**: Xialin He,Runpei Dong,Zixuan Chen,Saurabh Gupta
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Project page: https://humanoid-getup.github.io/
- **标题**: None
- **领域**: 机器人技术,机器学习
- **Abstract**: Automatic fall recovery is a crucial prerequisite before humanoid robots can be reliably deployed. Hand-designing controllers for getting up is difficult because of the varied configurations a humanoid can end up in after a fall and the challenging terrains humanoid robots are expected to operate on. This paper develops a learning framework to produce controllers that enable humanoid robots to get up from varying configurations on varying terrains. Unlike previous successful applications of humanoid locomotion learning, the getting-up task involves complex contact patterns, which necessitates accurately modeling the collision geometry and sparser rewards. We address these challenges through a two-phase approach that follows a curriculum. The first stage focuses on discovering a good getting-up trajectory under minimal constraints on smoothness or speed / torque limits. The second stage then refines the discovered motions into deployable (i.e. smooth and slow) motions that are robust to variations in initial configuration and terrains. We find these innovations enable a real-world G1 humanoid robot to get up from two main situations that we considered: a) lying face up and b) lying face down, both tested on flat, deformable, slippery surfaces and slopes (e.g., sloppy grass and snowfield). To the best of our knowledge, this is the first successful demonstration of learned getting-up policies for human-sized humanoid robots in the real world. Project page: https://humanoid-getup.github.io/

### Stonefish: Supporting Machine Learning Research in Marine Robotics 
[[arxiv](https://arxiv.org/abs/2502.11887)] [[cool](https://papers.cool/arxiv/2502.11887)] [[pdf](https://arxiv.org/pdf/2502.11887)]
> **Authors**: Michele Grimaldi,Patryk Cieslak,Eduardo Ochoa,Vibhav Bharti,Hayat Rajani,Ignacio Carlucho,Maria Koskinopoulou,Yvan R. Petillot,Nuno Gracias
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Accepted as full paper at ICRA 2025
- **标题**: None
- **领域**: 机器人技术,人工智能,系统与控制
- **Abstract**: Simulations are highly valuable in marine robotics, offering a cost-effective and controlled environment for testing in the challenging conditions of underwater and surface operations. Given the high costs and logistical difficulties of real-world trials, simulators capable of capturing the operational conditions of subsea environments have become key in developing and refining algorithms for remotely-operated and autonomous underwater vehicles. This paper highlights recent enhancements to the Stonefish simulator, an advanced open-source platform supporting development and testing of marine robotics solutions. Key updates include a suite of additional sensors, such as an event-based camera, a thermal camera, and an optical flow camera, as well as, visual light communication, support for tethered operations, improved thruster modelling, more flexible hydrodynamics, and enhanced sonar accuracy. These developments and an automated annotation tool significantly bolster Stonefish's role in marine robotics research, especially in the field of machine learning, where training data with a known ground truth is hard or impossible to collect.

## 声音(cs.SD:Sound)

### Myna: Masking-Based Contrastive Learning of Musical Representations 
[[arxiv](https://arxiv.org/abs/2502.12511)] [[cool](https://papers.cool/arxiv/2502.12511)] [[pdf](https://arxiv.org/pdf/2502.12511)]
> **Authors**: Ori Yonay,Tracy Hammond,Tianbao Yang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 声音,人工智能,机器学习
- **Abstract**: We present Myna, a simple yet effective approach for self-supervised musical representation learning. Built on a contrastive learning framework, Myna introduces two key innovations: (1) the use of a Vision Transformer (ViT) on mel-spectrograms as the backbone and (2) a novel data augmentation strategy, token masking, that masks 90 percent of spectrogram tokens. These innovations deliver both effectiveness and efficiency: (i) Token masking enables a significant increase in per-GPU batch size, from 48 or 120 in prior methods (CLMR, MULE) to 4096. (ii) By avoiding traditional augmentations, Myna retains pitch sensitivity, enhancing performance in tasks like key detection. (iii) The use of vertical patches allows the model to better capture critical features for key detection. Our hybrid model, Myna-22M-Hybrid, processes both 16x16 and 128x2 patches, achieving state-of-the-art results. Trained on a single GPU, it outperforms MULE (62M) on average and rivals MERT-95M, which was trained on 16 and 64 GPUs, respectively. Additionally, it surpasses MERT-95M-public, establishing itself as the best-performing model trained on publicly available data. We release our code and models to promote reproducibility and facilitate future research.

### Masked Latent Prediction and Classification for Self-Supervised Audio Representation Learning 
[[arxiv](https://arxiv.org/abs/2502.12031)] [[cool](https://papers.cool/arxiv/2502.12031)] [[pdf](https://arxiv.org/pdf/2502.12031)]
> **Authors**: Aurian Quelennec,Pierre Chouteau,Geoffroy Peeters,Slim Essid
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Copyright 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works
- **标题**: None
- **领域**: 声音,人工智能
- **Abstract**: Recently, self-supervised learning methods based on masked latent prediction have proven to encode input data into powerful representations. However, during training, the learned latent space can be further transformed to extract higher-level information that could be more suited for downstream classification tasks. Therefore, we propose a new method: MAsked latenT Prediction And Classification (MATPAC), which is trained with two pretext tasks solved jointly. As in previous work, the first pretext task is a masked latent prediction task, ensuring a robust input representation in the latent space. The second one is unsupervised classification, which utilises the latent representations of the first pretext task to match probability distributions between a teacher and a student. We validate the MATPAC method by comparing it to other state-of-the-art proposals and conducting ablations studies. MATPAC reaches state-of-the-art self-supervised learning results on reference audio classification datasets such as OpenMIC, GTZAN, ESC-50 and US8K and outperforms comparable supervised methods results for musical auto-tagging on Magna-tag-a-tune.

### ChordFormer: A Conformer-Based Architecture for Large-Vocabulary Audio Chord Recognition 
[[arxiv](https://arxiv.org/abs/2502.11840)] [[cool](https://papers.cool/arxiv/2502.11840)] [[pdf](https://arxiv.org/pdf/2502.11840)]
> **Authors**: Muhammad Waseem Akram,Stefano Dettori,Valentina Colla,Giorgio Carlo Buttazzo
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 13 pages, 4 figures
- **标题**: None
- **领域**: 声音,人工智能,计算机视觉和模式识别,信息检索,机器学习
- **Abstract**: Chord recognition serves as a critical task in music information retrieval due to the abstract and descriptive nature of chords in music analysis. While audio chord recognition systems have achieved significant accuracy for small vocabularies (e.g., major/minor chords), large-vocabulary chord recognition remains a challenging problem. This complexity also arises from the inherent long-tail distribution of chords, where rare chord types are underrepresented in most datasets, leading to insufficient training samples. Effective chord recognition requires leveraging contextual information from audio sequences, yet existing models, such as combinations of convolutional neural networks, bidirectional long short-term memory networks, and bidirectional transformers, face limitations in capturing long-term dependencies and exhibit suboptimal performance on large-vocabulary chord recognition tasks. This work proposes ChordFormer, a novel conformer-based architecture designed to tackle structural chord recognition (e.g., triads, bass, sevenths) for large vocabularies. ChordFormer leverages conformer blocks that integrate convolutional neural networks with transformers, thus enabling the model to capture both local patterns and global dependencies effectively. By addressing challenges such as class imbalance through a reweighted loss function and structured chord representations, ChordFormer outperforms state-of-the-art models, achieving a 2% improvement in frame-wise accuracy and a 6% increase in class-wise accuracy on large-vocabulary chord datasets. Furthermore, ChordFormer excels in handling class imbalance, providing robust and balanced recognition across chord types. This approach bridges the gap between theoretical music knowledge and practical applications, advancing the field of large-vocabulary chord recognition.

### TAPS: Throat and Acoustic Paired Speech Dataset for Deep Learning-Based Speech Enhancement 
[[arxiv](https://arxiv.org/abs/2502.11478)] [[cool](https://papers.cool/arxiv/2502.11478)] [[pdf](https://arxiv.org/pdf/2502.11478)]
> **Authors**: Yunsik Kim,Yonghun Song,Yoonyoung Chung
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 声音,机器学习,音频和语音处理
- **Abstract**: In high-noise environments such as factories, subways, and busy streets, capturing clear speech is challenging due to background noise. Throat microphones provide a solution with their noise-suppressing properties, reducing the noise while recording speech. However, a significant limitation remains: high-frequency information is attenuated as sound waves pass through skin and tissue, reducing speech clarity. Recent deep learning approaches have shown promise in enhancing throat microphone recordings, but further progress is constrained by the absence of standardized dataset. We introduce a throat and acoustic paired speech dataset (TAPS), a collection of paired utterances recorded from 60 native Korean speakers using throat and acoustic microphones. To demonstrate the TAPS's utility, we tested three baseline deep learning models and identified the mapping-based approach as superior in improving speech quality and restoring content. Additionally, we propose an optimal method to mitigate the signal mismatch between throat and acoustic microphones, ensuring model performance. These results highlight the potential of TAPS to serve as a standardized dataset and advance research in throat microphone-based speech enhancement.

## 软件工程(cs.SE:Software Engineering)

### NeuroStrata: Harnessing Neurosymbolic Paradigms for Improved Design, Testability, and Verifiability of Autonomous CPS 
[[arxiv](https://arxiv.org/abs/2502.12267)] [[cool](https://papers.cool/arxiv/2502.12267)] [[pdf](https://arxiv.org/pdf/2502.12267)]
> **Authors**: Xi Zheng,Ziyang Li,Ivan Ruchkin,Ruzica Piskac,Miroslav Pajic
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: Autonomous cyber-physical systems (CPSs) leverage AI for perception, planning, and control but face trust and safety certification challenges due to inherent uncertainties. The neurosymbolic paradigm replaces stochastic layers with interpretable symbolic AI, enabling determinism. While promising, challenges like multisensor fusion, adaptability, and verification remain. This paper introduces NeuroStrata, a neurosymbolic framework to enhance the testing and verification of autonomous CPS. We outline its key components, present early results, and detail future plans.

## 社交和信息网络(cs.SI:Social and Information Networks)

### UniGO: A Unified Graph Neural Network for Modeling Opinion Dynamics on Graphs 
[[arxiv](https://arxiv.org/abs/2502.11519)] [[cool](https://papers.cool/arxiv/2502.11519)] [[pdf](https://arxiv.org/pdf/2502.11519)]
> **Authors**: Hao Li,Hao Jiang,Yuke Zheng,Hao Sun,Wenying Gong
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: WWW2025
- **标题**: None
- **领域**: 社交和信息网络,人工智能
- **Abstract**: Polarization and fragmentation in social media amplify user biases, making it increasingly important to understand the evolution of opinions. Opinion dynamics provide interpretability for studying opinion evolution, yet incorporating these insights into predictive models remains challenging. This challenge arises due to the inherent complexity of the diversity of opinion fusion rules and the difficulty in capturing equilibrium states while avoiding over-smoothing. This paper constructs a unified opinion dynamics model to integrate different opinion fusion rules and generates corresponding synthetic datasets. To fully leverage the advantages of unified opinion dynamics, we introduces UniGO, a framework for modeling opinion evolution on graphs. Using a coarsen-refine mechanism, UniGO efficiently models opinion dynamics through a graph neural network, mitigating over-smoothing while preserving equilibrium phenomena. UniGO leverages pretraining on synthetic datasets, which enhances its ability to generalize to real-world scenarios, providing a viable paradigm for applications of opinion dynamics. Experimental results on both synthetic and real-world datasets demonstrate UniGO's effectiveness in capturing complex opinion formation processes and predicting future evolution. The pretrained model also shows strong generalization capability, validating the benefits of using synthetic data to boost real-world performance.

## 理论经济学(econ.TH:Theoretical Economics)

### Multi-dimensional Test Design 
[[arxiv](https://arxiv.org/abs/2502.12264)] [[cool](https://papers.cool/arxiv/2502.12264)] [[pdf](https://arxiv.org/pdf/2502.12264)]
> **Authors**: Xiaoyun Qiu,Liren Shan
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 理论经济学,计算机与社会,计算机科学与博弈论,机器学习
- **Abstract**: How should one jointly design tests and the arrangement of agencies to administer these tests (testing procedure)? To answer this question, we analyze a model where a principal must use multiple tests to screen an agent with a multi-dimensional type, knowing that the agent can change his type at a cost. We identify a new tradeoff between setting difficult tests and using a difficult testing procedure. We compare two settings: (1) the agent only misrepresents his type (manipulation) and (2) the agent improves his actual type (investment). Examples include interviews, regulations, and data classification. We show that in the manipulation setting, stringent tests combined with an easy procedure, i.e., offering tests sequentially in a fixed order, is optimal. In contrast, in the investment setting, non-stringent tests with a difficult procedure, i.e., offering tests simultaneously, is optimal; however, under mild conditions offering them sequentially in a random order may be as good. Our results suggest that whether the agent manipulates or invests in his type determines which arrangement of agencies is optimal.

## 音频和语音处理(eess.AS:Audio and Speech Processing)

### A Comprehensive Survey on Generative AI for Video-to-Music Generation 
[[arxiv](https://arxiv.org/abs/2502.12489)] [[cool](https://papers.cool/arxiv/2502.12489)] [[pdf](https://arxiv.org/pdf/2502.12489)]
> **Authors**: Shulei Ji,Songruoyao Wu,Zihao Wang,Shuyu Li,Kejun Zhang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 音频和语音处理,人工智能,多媒体
- **Abstract**: The burgeoning growth of video-to-music generation can be attributed to the ascendancy of multimodal generative models. However, there is a lack of literature that comprehensively combs through the work in this field. To fill this gap, this paper presents a comprehensive review of video-to-music generation using deep generative AI techniques, focusing on three key components: visual feature extraction, music generation frameworks, and conditioning mechanisms. We categorize existing approaches based on their designs for each component, clarifying the roles of different strategies. Preceding this, we provide a fine-grained classification of video and music modalities, illustrating how different categories influence the design of components within the generation pipelines. Furthermore, we summarize available multimodal datasets and evaluation metrics while highlighting ongoing challenges in the field.

### LMFCA-Net: A Lightweight Model for Multi-Channel Speech Enhancement with Efficient Narrow-Band and Cross-Band Attention 
[[arxiv](https://arxiv.org/abs/2502.11462)] [[cool](https://papers.cool/arxiv/2502.11462)] [[pdf](https://arxiv.org/pdf/2502.11462)]
> **Authors**: Yaokai Zhang,Hanchen Pei,Wanqi Wang,Gongping Huang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Accepted at ICASSP 2025
- **标题**: None
- **领域**: 音频和语音处理,机器学习,声音
- **Abstract**: Deep learning based end-to-end multi-channel speech enhancement methods have achieved impressive performance by leveraging sub-band, cross-band, and spatial information. However, these methods often demand substantial computational resources, limiting their practicality on terminal devices. This paper presents a lightweight multi-channel speech enhancement network with decoupled fully connected attention (LMFCA-Net). The proposed LMFCA-Net introduces time-axis decoupled fully-connected attention (T-FCA) and frequency-axis decoupled fully-connected attention (F-FCA) mechanisms to effectively capture long-range narrow-band and cross-band information without recurrent units. Experimental results show that LMFCA-Net performs comparably to state-of-the-art methods while significantly reducing computational complexity and latency, making it a promising solution for practical applications.

## 图像和视频处理(eess.IV:Image and Video Processing)

### 3D ReX: Causal Explanations in 3D Neuroimaging Classification 
[[arxiv](https://arxiv.org/abs/2502.12181)] [[cool](https://papers.cool/arxiv/2502.12181)] [[pdf](https://arxiv.org/pdf/2502.12181)]
> **Authors**: Melane Navaratnarajah,Sophie A. Martin,David A. Kelly,Nathan Blake,Hana Chockler
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别,机器学习
- **Abstract**: Explainability remains a significant problem for AI models in medical imaging, making it challenging for clinicians to trust AI-driven predictions. We introduce 3D ReX, the first causality-based post-hoc explainability tool for 3D models. 3D ReX uses the theory of actual causality to generate responsibility maps which highlight the regions most crucial to the model's decision. We test 3D ReX on a stroke detection model, providing insight into the spatial distribution of features relevant to stroke.

### ClusMFL: A Cluster-Enhanced Framework for Modality-Incomplete Multimodal Federated Learning in Brain Imaging Analysis 
[[arxiv](https://arxiv.org/abs/2502.12180)] [[cool](https://papers.cool/arxiv/2502.12180)] [[pdf](https://arxiv.org/pdf/2502.12180)]
> **Authors**: Xinpeng Wang,Rong Zhou,Han Xie,Xiaoying Tang,Lifang He,Carl Yang
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别,机器学习
- **Abstract**: Multimodal Federated Learning (MFL) has emerged as a promising approach for collaboratively training multimodal models across distributed clients, particularly in healthcare domains. In the context of brain imaging analysis, modality incompleteness presents a significant challenge, where some institutions may lack specific imaging modalities (e.g., PET, MRI, or CT) due to privacy concerns, device limitations, or data availability issues. While existing work typically assumes modality completeness or oversimplifies missing-modality scenarios, we simulate a more realistic setting by considering both client-level and instance-level modality incompleteness in this study. Building on this realistic simulation, we propose ClusMFL, a novel MFL framework that leverages feature clustering for cross-institutional brain imaging analysis under modality incompleteness. Specifically, ClusMFL utilizes the FINCH algorithm to construct a pool of cluster centers for the feature embeddings of each modality-label pair, effectively capturing fine-grained data distributions. These cluster centers are then used for feature alignment within each modality through supervised contrastive learning, while also acting as proxies for missing modalities, allowing cross-modal knowledge transfer. Furthermore, ClusMFL employs a modality-aware aggregation strategy, further enhancing the model's performance in scenarios with severe modality incompleteness. We evaluate the proposed framework on the ADNI dataset, utilizing structural MRI and PET scans. Extensive experimental results demonstrate that ClusMFL achieves state-of-the-art performance compared to various baseline methods across varying levels of modality incompleteness, providing a scalable solution for cross-institutional brain imaging analysis.

## 几何拓扑(math.GT:Geometric Topology)

### On the Learnability of Knot Invariants: Representation, Predictability, and Neural Similarity 
[[arxiv](https://arxiv.org/abs/2502.12243)] [[cool](https://papers.cool/arxiv/2502.12243)] [[pdf](https://arxiv.org/pdf/2502.12243)]
> **Authors**: Audrey Lindsay,Fabian Ruehle
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 14 pages, 3 figures, 1 table
- **标题**: None
- **领域**: 几何拓扑,机器学习
- **Abstract**: We analyze different aspects of neural network predictions of knot invariants. First, we investigate the impact of different knot representations on the prediction of invariants and find that braid representations work in general the best. Second, we study which knot invariants are easy to learn, with invariants derived from hyperbolic geometry and knot diagrams being very easy to learn, while invariants derived from topological or homological data are harder. Predicting the Arf invariant could not be learned for any representation. Third, we propose a cosine similarity score based on gradient saliency vectors, and a joint misclassification score to uncover similarities in neural networks trained to predict related topological invariants.

## 优化与控制(math.OC:Optimization and Control)

### Symmetric Rank-One Quasi-Newton Methods for Deep Learning Using Cubic Regularization 
[[arxiv](https://arxiv.org/abs/2502.12298)] [[cool](https://papers.cool/arxiv/2502.12298)] [[pdf](https://arxiv.org/pdf/2502.12298)]
> **Authors**: Aditya Ranganath,Mukesh Singhal,Roummel Marcia
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: submitted to TMLR
- **标题**: None
- **领域**: 优化与控制,信息论,机器学习,数值分析,机器学习
- **Abstract**: Stochastic gradient descent and other first-order variants, such as Adam and AdaGrad, are commonly used in the field of deep learning due to their computational efficiency and low-storage memory requirements. However, these methods do not exploit curvature information. Consequently, iterates can converge to saddle points or poor local minima. On the other hand, Quasi-Newton methods compute Hessian approximations which exploit this information with a comparable computational budget. Quasi-Newton methods re-use previously computed iterates and gradients to compute a low-rank structured update. The most widely used quasi-Newton update is the L-BFGS, which guarantees a positive semi-definite Hessian approximation, making it suitable in a line search setting. However, the loss functions in DNNs are non-convex, where the Hessian is potentially non-positive definite. In this paper, we propose using a limited-memory symmetric rank-one quasi-Newton approach which allows for indefinite Hessian approximations, enabling directions of negative curvature to be exploited. Furthermore, we use a modified adaptive regularized cubics approach, which generates a sequence of cubic subproblems that have closed-form solutions with suitable regularization choices. We investigate the performance of our proposed method on autoencoders and feed-forward neural network models and compare our approach to state-of-the-art first-order adaptive stochastic methods as well as other quasi-Newton methods.x

## 统计理论(math.ST:Statistics Theory)

### Stability Bounds for Smooth Optimal Transport Maps and their Statistical Implications 
[[arxiv](https://arxiv.org/abs/2502.12326)] [[cool](https://papers.cool/arxiv/2502.12326)] [[pdf](https://arxiv.org/pdf/2502.12326)]
> **Authors**: Sivaraman Balakrishnan,Tudor Manole
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 26 pages, 1 figure
- **标题**: None
- **领域**: 统计理论,机器学习,方法论,机器学习
- **Abstract**: We study estimators of the optimal transport (OT) map between two probability distributions. We focus on plugin estimators derived from the OT map between estimates of the underlying distributions. We develop novel stability bounds for OT maps which generalize those in past work, and allow us to reduce the problem of optimally estimating the transport map to that of optimally estimating densities in the Wasserstein distance. In contrast, past work provided a partial connection between these problems and relied on regularity theory for the Monge-Ampere equation to bridge the gap, a step which required unnatural assumptions to obtain sharp guarantees. We also provide some new insights into the connections between stability bounds which arise in the analysis of plugin estimators and growth bounds for the semi-dual functional which arise in the analysis of Brenier potential-based estimators of the transport map. We illustrate the applicability of our new stability bounds by revisiting the smooth setting studied by Manole et al., analyzing two of their estimators under more general conditions. Critically, our bounds do not require smoothness or boundedness assumptions on the underlying measures. As an illustrative application, we develop and analyze a novel tuning parameter-free estimator for the OT map between two strongly log-concave distributions.

## 计算物理(physics.comp-ph:Computational Physics)

### Learning Smooth and Expressive Interatomic Potentials for Physical Property Prediction 
[[arxiv](https://arxiv.org/abs/2502.12147)] [[cool](https://papers.cool/arxiv/2502.12147)] [[pdf](https://arxiv.org/pdf/2502.12147)]
> **Authors**: Xiang Fu,Brandon M. Wood,Luis Barroso-Luque,Daniel S. Levine,Meng Gao,Misko Dzamba,C. Lawrence Zitnick
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 19 pages, 14 figures, 5 tables
- **标题**: None
- **领域**: 计算物理,机器学习
- **Abstract**: Machine learning interatomic potentials (MLIPs) have become increasingly effective at approximating quantum mechanical calculations at a fraction of the computational cost. However, lower errors on held out test sets do not always translate to improved results on downstream physical property prediction tasks. In this paper, we propose testing MLIPs on their practical ability to conserve energy during molecular dynamic simulations. If passed, improved correlations are found between test errors and their performance on physical property prediction tasks. We identify choices which may lead to models failing this test, and use these observations to improve upon highly-expressive models. The resulting model, eSEN, provides state-of-the-art results on a range of physical property prediction tasks, including materials stability prediction, thermal conductivity prediction, and phonon calculations.

## 流体动力学(physics.flu-dyn:Fluid Dynamics)

### Scientific Machine Learning of Flow Resistance Using Universal Shallow Water Equations with Differentiable Programming 
[[arxiv](https://arxiv.org/abs/2502.12396)] [[cool](https://papers.cool/arxiv/2502.12396)] [[pdf](https://arxiv.org/pdf/2502.12396)]
> **Authors**: Xiaofeng Liu,Yalan Song
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 流体动力学,计算工程、金融和科学,机器学习
- **Abstract**: Shallow water equations (SWEs) are the backbone of most hydrodynamics models for flood prediction, river engineering, and many other water resources applications. The estimation of flow resistance, i.e., the Manning's roughness coefficient $n$, is crucial for ensuring model accuracy, and has been previously determined using empirical formulas or tables. To better account for temporal and spatial variability in channel roughness, inverse modeling of $n$ using observed flow data is more reliable and adaptable; however, it is challenging when using traditional SWE solvers. Based on the concept of universal differential equation (UDE), which combines physics-based differential equations with neural networks (NNs), we developed a universal SWEs (USWEs) solver, Hydrograd, for hybrid hydrodynamics modeling. It can do accurate forward simulations, support automatic differentiation (AD) for gradient-based sensitivity analysis and parameter inversion, and perform scientific machine learning for physics discovery. In this work, we first validated the accuracy of its forward modeling, then applied a real-world case to demonstrate the ability of USWEs to capture model sensitivity (gradients) and perform inverse modeling of Manning's $n$. Furthermore, we used a NN to learn a universal relationship between $n$, hydraulic parameters, and flow in a real river channel. Unlike inverse modeling using surrogate models, Hydrograd uses a two-dimensional SWEs solver as its physics backbone, which eliminates the need for data-intensive pretraining and resolves the generalization problem when applied to out-of-sample scenarios. This differentiable modeling approach, with seamless integration with NNs, provides a new pathway for solving complex inverse problems and discovering new physics in hydrodynamics.

## 地球物理学(physics.geo-ph:Geophysics)

### Integrating Artificial Intelligence and Geophysical Insights for Earthquake Forecasting: A Cross-Disciplinary Review 
[[arxiv](https://arxiv.org/abs/2502.12161)] [[cool](https://papers.cool/arxiv/2502.12161)] [[pdf](https://arxiv.org/pdf/2502.12161)]
> **Authors**: Zhang Ying,Wen Congcong,Sornette Didier,Zhan Chengxiang
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 地球物理学,人工智能,机器学习
- **Abstract**: Earthquake forecasting remains a significant scientific challenge, with current methods falling short of achieving the performance necessary for meaningful societal benefits. Traditional models, primarily based on past seismicity and geomechanical data, struggle to capture the complexity of seismic patterns and often overlook valuable non-seismic precursors such as geophysical, geochemical, and atmospheric anomalies. The integration of such diverse data sources into forecasting models, combined with advancements in AI technologies, offers a promising path forward. AI methods, particularly deep learning, excel at processing complex, large-scale datasets, identifying subtle patterns, and handling multidimensional relationships, making them well-suited for overcoming the limitations of conventional approaches. This review highlights the importance of combining AI with geophysical knowledge to create robust, physics-informed forecasting models. It explores current AI methods, input data types, loss functions, and practical considerations for model development, offering guidance to both geophysicists and AI researchers. While many AI-based studies oversimplify earthquake prediction, neglecting critical features such as data imbalance and spatio-temporal clustering, the integration of specialized geophysical insights into AI models can address these shortcomings. We emphasize the importance of interdisciplinary collaboration, urging geophysicists to experiment with AI architectures thoughtfully and encouraging AI experts to deepen their understanding of seismology. By bridging these disciplines, we can develop more accurate, reliable, and societally impactful earthquake forecasting tools.

### PreAdaptFWI: Pretrained-Based Adaptive Residual Learning for Full-Waveform Inversion Without Dataset Dependency 
[[arxiv](https://arxiv.org/abs/2502.11913)] [[cool](https://papers.cool/arxiv/2502.11913)] [[pdf](https://arxiv.org/pdf/2502.11913)]
> **Authors**: Xintong Dong,Zhengyi Yuan,Jun Lin,Shiqi Dong,Xunqian Tong,Yue Li
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 地球物理学,机器学习
- **Abstract**: Full-waveform inversion (FWI) is a method that utilizes seismic data to invert the physical parameters of subsurface media by minimizing the difference between simulated and observed waveforms. Due to its ill-posed nature, FWI is susceptible to getting trapped in local minima. Consequently, various research efforts have attempted to combine neural networks with FWI to stabilize the inversion process. This study presents a simple yet effective training framework that is independent of dataset reliance and requires only moderate pre-training on a simple initial model to stabilize network outputs. During the transfer learning phase, the conventional FWI gradients will simultaneously update both the neural network and the proposed adaptive residual learning module, which learns the residual mapping of large-scale distribution features in the network's output, rather than directly fitting the target mapping. Through this synergistic training paradigm, the proposed algorithm effectively infers the physically-informed prior knowledge into a global representation of stratigraphic distribution, as well as capturing subtle variations in inter-layer velocities within local details, thereby escaping local optima. Evaluating the method on two benchmark models under various conditions, including absent low-frequency data, noise interference, and differing initial models, along with corresponding ablation experiments, consistently demonstrates the superiority of the proposed approach.

## 仪器仪表和探测器(physics.ins-det:Instrumentation and Detectors)

### Antimatter Annihilation Vertex Reconstruction with Deep Learning for ALPHA-g Radial Time Projection Chamber 
[[arxiv](https://arxiv.org/abs/2502.12169)] [[cool](https://papers.cool/arxiv/2502.12169)] [[pdf](https://arxiv.org/pdf/2502.12169)]
> **Authors**: Ashley Ferreira,Mahip Singh,Yukiya Saito,Andrea Capra,Ina Carli,Daniel Duque Quiceno,Wojciech T. Fedorko,Makoto C. Fujiwara,Muyan Li,Lars Martin,Gareth Smith,Anqui Xu
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 仪器仪表和探测器,机器学习,高能物理-实验
- **Abstract**: The ALPHA-g experiment at CERN aims to precisely measure the terrestrial gravitational acceleration of antihydrogen atoms. A radial Time Projection Chamber (rTPC), that surrounds the ALPHA-g magnetic trap, is employed to determine the annihilation location, called the vertex. The standard approach requires identifying the trajectories of the ionizing particles in the rTPC from the location of their interaction in the gas (spacepoints), and inferring the vertex positions by finding the point where those trajectories (helices) pass closest to one another. In this work, we present a novel approach to vertex reconstruction using an ensemble of models based on the PointNet deep learning architecture. The newly developed model, PointNet Ensemble for Annihilation Reconstruction (PEAR), directly learns the relation between the location of the vertices and the rTPC spacepoints, thus eliminating the need to identify and fit the particle tracks. PEAR shows strong performance in reconstructing vertical vertex positions from simulated data, that is superior to the standard approach for all metrics considered. Furthermore, the deep learning approach can reconstruct the vertical vertex position when the standard approach fails.

## 等离子体物理(physics.plasm-ph:Plasma Physics)

### Learning Plasma Dynamics and Robust Rampdown Trajectories with Predict-First Experiments at TCV 
[[arxiv](https://arxiv.org/abs/2502.12327)] [[cool](https://papers.cool/arxiv/2502.12327)] [[pdf](https://arxiv.org/pdf/2502.12327)]
> **Authors**: Allen M. Wang,Alessandro Pau,Cristina Rea,Oswin So,Charles Dawson,Olivier Sauter,Mark D. Boyer,Anna Vu,Cristian Galperti,Chuchu Fan,Antoine Merle,Yoeri Poels,Cristina Venturini,Stefano Marchioni,the TCV Team
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 等离子体物理,人工智能,机器学习,系统与控制
- **Abstract**: The rampdown in tokamak operations is a difficult to simulate phase during which the plasma is often pushed towards multiple instability limits. To address this challenge, and reduce the risk of disrupting operations, we leverage recent advances in Scientific Machine Learning (SciML) to develop a neural state-space model (NSSM) that predicts plasma dynamics during Tokamak à Configuration Variable (TCV) rampdowns. By integrating simple physics structure and data-driven models, the NSSM efficiently learns plasma dynamics during the rampdown from a modest dataset of 311 pulses with only five pulses in the reactor relevant high performance regime. The NSSM is parallelized across uncertainties, and reinforcement learning (RL) is applied to design trajectories that avoid multiple instability limits with high probability. Experiments at TCV ramping down high performance plasmas show statistically significant improvements in current and energy at plasma termination, with improvements in speed through continuous re-training. A predict-first experiment, increasing plasma current by 20\% from baseline, demonstrates the NSSM's ability to make small extrapolations with sufficient accuracy to design trajectories that successfully terminate the pulse. The developed approach paves the way for designing tokamak controls with robustness to considerable uncertainty, and demonstrates the relevance of the SciML approach to learning plasma dynamics for rapidly developing robust trajectories and controls during the incremental campaigns of upcoming burning plasma tokamaks.

### Towards Transparent and Accurate Plasma State Monitoring at JET 
[[arxiv](https://arxiv.org/abs/2502.12182)] [[cool](https://papers.cool/arxiv/2502.12182)] [[pdf](https://arxiv.org/pdf/2502.12182)]
> **Authors**: Andrin Bürli,Alessandro Pau,Thomas Koller,Olivier Sauter,JET Contributors
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 等离子体物理,人工智能,机器学习
- **Abstract**: Controlling and monitoring plasma within a tokamak device is complex and challenging. Plasma off-normal events, such as disruptions, are hindering steady-state operation. For large devices, they can even endanger the machine's integrity and it represents in general one of the most serious concerns for the exploitation of the tokamak concept for future power plants. Effective plasma state monitoring carries the potential to enable an understanding of such phenomena and their evolution which is crucial for the successful operation of tokamaks. This paper presents the application of a transparent and data-driven methodology to monitor the plasma state in a tokamak. Compared to previous studies in the field, supervised and unsupervised learning techniques are combined. The dataset consisted of 520 expert-validated discharges from JET. The goal was to provide an interpretable plasma state representation for the JET operational space by leveraging multi-task learning for the first time in the context of plasma state monitoring. When evaluated as disruption predictors, a sequence-based approach showed significant improvements compared to the state-based models. The best resulting network achieved a promising cross-validated success rate when combined with a physical indicator and accounting for nearby instabilities. Qualitative evaluations of the learned latent space uncovered operational and disruptive regions as well as patterns related to learned dynamics and global feature importance. The applied methodology provides novel possibilities for the definition of triggers to switch between different control scenarios, data analysis, and learning as well as exploring latent dynamics for plasma state monitoring. It also showed promising quantitative and qualitative results with warning times suitable for avoidance purposes and distributions that are consistent with known physical mechanisms.

### How does ion temperature gradient turbulence depend on magnetic geometry? Insights from data and machine learning 
[[arxiv](https://arxiv.org/abs/2502.11657)] [[cool](https://papers.cool/arxiv/2502.11657)] [[pdf](https://arxiv.org/pdf/2502.11657)]
> **Authors**: Matt Landreman,Jong Youl Choi,Caio Alves,Prasanna Balaprakash,R. Michael Churchill,Rory Conlin,Gareth Roberg-Clark
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 等离子体物理,机器学习
- **Abstract**: Magnetic geometry has a significant effect on the level of turbulent transport in fusion plasmas. Here, we model and analyze this dependence using multiple machine learning methods and a dataset of > 200,000 nonlinear simulations of ion-temperature-gradient turbulence in diverse non-axisymmetric geometries. The dataset is generated using a large collection of both optimized and randomly generated stellarator equilibria. At fixed gradients, the turbulent heat flux varies between geometries by several orders of magnitude. Trends are apparent among the configurations with particularly high or low heat flux. Regression and classification techniques from machine learning are then applied to extract patterns in the dataset. Due to a symmetry of the gyrokinetic equation, the heat flux and regressions thereof should be invariant to translations of the raw features in the parallel coordinate, similar to translation invariance in computer vision applications. Multiple regression models including convolutional neural networks (CNNs) and decision trees can achieve reasonable predictive power for the heat flux in held-out test configurations, with highest accuracy for the CNNs. Using Spearman correlation, sequential feature selection, and Shapley values to measure feature importance, it is consistently found that the most important geometric lever on the heat flux is the flux surface compression in regions of bad curvature. The second most important feature relates to the magnitude of geodesic curvature. These two features align remarkably with surrogates that have been proposed based on theory, while the methods here allow a natural extension to more features for increased accuracy. The dataset, released with this publication, may also be used to test other proposed surrogates, and we find many previously published proxies do correlate well with both the heat flux and stability boundary.

## 生物分子(q-bio.BM:Biomolecules)

### Towards Efficient Molecular Property Optimization with Graph Energy Based Models 
[[arxiv](https://arxiv.org/abs/2502.12219)] [[cool](https://papers.cool/arxiv/2502.12219)] [[pdf](https://arxiv.org/pdf/2502.12219)]
> **Authors**: Luca Miglior,Lorenzo Simone,Marco Podda,Davide Bacciu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: Accepted at ESANN 2025
- **标题**: None
- **领域**: 生物分子,机器学习
- **Abstract**: Optimizing chemical properties is a challenging task due to the vastness and complexity of chemical space. Here, we present a generative energy-based architecture for implicit chemical property optimization, designed to efficiently generate molecules that satisfy target properties without explicit conditional generation. We use Graph Energy Based Models and a training approach that does not require property labels. We validated our approach on well-established chemical benchmarks, showing superior results to state-of-the-art methods and demonstrating robustness and efficiency towards de novo drug design.

## 量子物理学(quant-ph:Quantum Physics)

### Exploring Quantum Control Landscape and Solution Space Complexity through Dimensionality Reduction & Optimization Algorithms 
[[arxiv](https://arxiv.org/abs/2502.11905)] [[cool](https://papers.cool/arxiv/2502.11905)] [[pdf](https://arxiv.org/pdf/2502.11905)]
> **Authors**: Haftu W. Fentaw,Steve Campbell,Simon Caton
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 量子物理学,新兴技术,机器学习
- **Abstract**: Understanding the quantum control landscape (QCL) is important for designing effective quantum control strategies. In this study, we analyze the QCL for a single two-level quantum system (qubit) using various control strategies. We employ Principal Component Analysis (PCA), to visualize and analyze the QCL for higher dimensional control parameters. Our results indicate that dimensionality reduction techniques such as PCA, can play an important role in understanding the complex nature of quantum control in higher dimensions. Evaluations of traditional control techniques and machine learning algorithms reveal that Genetic Algorithms (GA) outperform Stochastic Gradient Descent (SGD), while Q-learning (QL) shows great promise compared to Deep Q-Networks (DQN) and Proximal Policy Optimization (PPO). Additionally, our experiments highlight the importance of reward function design in DQN and PPO demonstrating that using immediate reward results in improved performance rather than delayed rewards for systems with short time steps. A study of solution space complexity was conducted by using Cluster Density Index (CDI) as a key metric for analyzing the density of optimal solutions in the landscape. The CDI reflects cluster quality and helps determine whether a given algorithm generates regions of high fidelity or not. Our results provide insights into effective quantum control strategies, emphasizing the significance of parameter selection and algorithm optimization.

### Ansatz-free Hamiltonian learning with Heisenberg-limited scaling 
[[arxiv](https://arxiv.org/abs/2502.11900)] [[cool](https://papers.cool/arxiv/2502.11900)] [[pdf](https://arxiv.org/pdf/2502.11900)]
> **Authors**: Hong-Ye Hu,Muzhou Ma,Weiyuan Gong,Qi Ye,Yu Tong,Steven T. Flammia,Susanne F. Yelin
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 5 pages, 1 figure with Supplementary Materials (17 pages, 1 figure). HYH and MM contributed equally
- **标题**: None
- **领域**: 量子物理学,信息论,机器学习
- **Abstract**: Learning the unknown interactions that govern a quantum system is crucial for quantum information processing, device benchmarking, and quantum sensing. The problem, known as Hamiltonian learning, is well understood under the assumption that interactions are local, but this assumption may not hold for arbitrary Hamiltonians. Previous methods all require high-order inverse polynomial dependency with precision, unable to surpass the standard quantum limit and reach the gold standard Heisenberg-limited scaling. Whether Heisenberg-limited Hamiltonian learning is possible without prior assumptions about the interaction structures, a challenge we term \emph{ansatz-free Hamiltonian learning}, remains an open question. In this work, we present a quantum algorithm to learn arbitrary sparse Hamiltonians without any structure constraints using only black-box queries of the system's real-time evolution and minimal digital controls to attain Heisenberg-limited scaling in estimation error. Our method is also resilient to state-preparation-and-measurement errors, enhancing its practical feasibility. Moreover, we establish a fundamental trade-off between total evolution time and quantum control on learning arbitrary interactions, revealing the intrinsic interplay between controllability and total evolution time complexity for any learning algorithm. These results pave the way for further exploration into Heisenberg-limited Hamiltonian learning in complex quantum systems under minimal assumptions, potentially enabling new benchmarking and verification protocols.

## 应用领域(stat.AP:Applications)

### Bridging the Data Gap in AI Reliability Research and Establishing DR-AIR, a Comprehensive Data Repository for AI Reliability 
[[arxiv](https://arxiv.org/abs/2502.12386)] [[cool](https://papers.cool/arxiv/2502.12386)] [[pdf](https://arxiv.org/pdf/2502.12386)]
> **Authors**: Simin Zheng,Jared M. Clark,Fatemeh Salboukh,Priscila Silva,Karen da Mata,Fenglian Pan,Jie Min,Jiayi Lian,Caleb B. King,Lance Fiondella,Jian Liu,Xinwei Deng,Yili Hong
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 34 pages, 12 figures
- **标题**: None
- **领域**: 应用领域,人工智能
- **Abstract**: Artificial intelligence (AI) technology and systems have been advancing rapidly. However, ensuring the reliability of these systems is crucial for fostering public confidence in their use. This necessitates the modeling and analysis of reliability data specific to AI systems. A major challenge in AI reliability research, particularly for those in academia, is the lack of readily available AI reliability data. To address this gap, this paper focuses on conducting a comprehensive review of available AI reliability data and establishing DR-AIR: a data repository for AI reliability. Specifically, we introduce key measurements and data types for assessing AI reliability, along with the methodologies used to collect these data. We also provide a detailed description of the currently available datasets with illustrative examples. Furthermore, we outline the setup of the DR-AIR repository and demonstrate its practical applications. This repository provides easy access to datasets specifically curated for AI reliability research. We believe these efforts will significantly benefit the AI research community by facilitating access to valuable reliability data and promoting collaboration across various academic domains within AI. We conclude our paper with a call to action, encouraging the research community to contribute and share AI reliability data to further advance this critical field of study.

## 方法论(stat.ME:Methodology)

### Time Series Treatment Effects Analysis with Always-Missing Controls 
[[arxiv](https://arxiv.org/abs/2502.12393)] [[cool](https://papers.cool/arxiv/2502.12393)] [[pdf](https://arxiv.org/pdf/2502.12393)]
> **Authors**: Juan Shu,Qiyu Han,George Chen,Xihao Cao,Kangming Luo,Dan Pallotta,Shivam Agrawal,Yuping Lu,Xiaoyu Zhang,Jawad Mansoor,Jyoti Anand
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 方法论,人工智能,机器学习,机器学习
- **Abstract**: Estimating treatment effects in time series data presents a significant challenge, especially when the control group is always unobservable. For example, in analyzing the effects of Christmas on retail sales, we lack direct observation of what would have occurred in late December without the Christmas impact. To address this, we try to recover the control group in the event period while accounting for confounders and temporal dependencies. Experimental results on the M5 Walmart retail sales data demonstrate robust estimation of the potential outcome of the control group as well as accurate predicted holiday effect. Furthermore, we provided theoretical guarantees for the estimated treatment effect, proving its consistency and asymptotic normality. The proposed methodology is applicable not only to this always-missing control scenario but also in other conventional time series causal inference settings.

## 机器学习(stat.ML:Machine Learning)

### Suboptimal Shapley Value Explanations 
[[arxiv](https://arxiv.org/abs/2502.12209)] [[cool](https://papers.cool/arxiv/2502.12209)] [[pdf](https://arxiv.org/pdf/2502.12209)]
> **Authors**: Xiaolei Lu
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Deep Neural Networks (DNNs) have demonstrated strong capacity in supporting a wide variety of applications. Shapley value has emerged as a prominent tool to analyze feature importance to help people understand the inference process of deep neural models. Computing Shapley value function requires choosing a baseline to represent feature's missingness. However, existing random and conditional baselines could negatively influence the explanation. In this paper, by analyzing the suboptimality of different baselines, we identify the problematic baseline where the asymmetric interaction between $\bm{x}'_i$ (the replacement of the faithful influential feature) and other features has significant directional bias toward the model's output, and conclude that $p(y|\bm{x}'_i) = p(y)$ potentially minimizes the asymmetric interaction involving $\bm{x}'_i$. We further generalize the uninformativeness of $\bm{x}'_i$ toward the label space $L$ to avoid estimating $p(y)$ and design a simple uncertainty-based reweighting mechanism to accelerate the computation process. We conduct experiments on various NLP tasks and our quantitative analysis demonstrates the effectiveness of the proposed uncertainty-based reweighting mechanism. Furthermore, by measuring the consistency of explanations generated by explainable methods and human, we highlight the disparity between model inference and human understanding.

### How compositional generalization and creativity improve as diffusion models are trained 
[[arxiv](https://arxiv.org/abs/2502.12089)] [[cool](https://papers.cool/arxiv/2502.12089)] [[pdf](https://arxiv.org/pdf/2502.12089)]
> **Authors**: Alessandro Favero,Antonio Sclocchi,Francesco Cagnetta,Pascal Frossard,Matthieu Wyart
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Natural data is often organized as a hierarchical composition of features. How many samples do generative models need to learn the composition rules, so as to produce a combinatorial number of novel data? What signal in the data is exploited to learn? We investigate these questions both theoretically and empirically. Theoretically, we consider diffusion models trained on simple probabilistic context-free grammars - tree-like graphical models used to represent the structure of data such as language and images. We demonstrate that diffusion models learn compositional rules with the sample complexity required for clustering features with statistically similar context, a process similar to the word2vec algorithm. However, this clustering emerges hierarchically: higher-level, more abstract features associated with longer contexts require more data to be identified. This mechanism leads to a sample complexity that scales polynomially with the said context size. As a result, diffusion models trained on intermediate dataset size generate data coherent up to a certain scale, but that lacks global coherence. We test these predictions in different domains, and find remarkable agreement: both generated texts and images achieve progressively larger coherence lengths as the training time or dataset size grows. We discuss connections between the hierarchical clustering mechanism we introduce here and the renormalization group in physics.

### Low-Rank Thinning 
[[arxiv](https://arxiv.org/abs/2502.12063)] [[cool](https://papers.cool/arxiv/2502.12063)] [[pdf](https://arxiv.org/pdf/2502.12063)]
> **Authors**: Annabelle Michael Carrell,Albert Gong,Abhishek Shetty,Raaz Dwivedi,Lester Mackey
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习,优化与控制,统计理论,方法论
- **Abstract**: The goal in thinning is to summarize a dataset using a small set of representative points. Remarkably, sub-Gaussian thinning algorithms like Kernel Halving and Compress can match the quality of uniform subsampling while substantially reducing the number of summary points. However, existing guarantees cover only a restricted range of distributions and kernel-based quality measures and suffer from pessimistic dimension dependence. To address these deficiencies, we introduce a new low-rank analysis of sub-Gaussian thinning that applies to any distribution and any kernel, guaranteeing high-quality compression whenever the kernel or data matrix is approximately low-rank. To demonstrate the broad applicability of the techniques, we design practical sub-Gaussian thinning approaches that improve upon the best known guarantees for approximating attention in transformers, accelerating stochastic gradient training through reordering, and distinguishing distributions in near-linear time.

### Refined PAC-Bayes Bounds for Offline Bandits 
[[arxiv](https://arxiv.org/abs/2502.11953)] [[cool](https://papers.cool/arxiv/2502.11953)] [[pdf](https://arxiv.org/pdf/2502.11953)]
> **Authors**: Amaury Gouverneur,Tobias J. Oechtering,Mikael Skoglund
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 6 pages
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: In this paper, we present refined probabilistic bounds on empirical reward estimates for off-policy learning in bandit problems. We build on the PAC-Bayesian bounds from Seldin et al. (2010) and improve on their results using a new parameter optimization approach introduced by Rodríguez et al. (2024). This technique is based on a discretization of the space of possible events to optimize the "in probability" parameter. We provide two parameter-free PAC-Bayes bounds, one based on Hoeffding-Azuma's inequality and the other based on Bernstein's inequality. We prove that our bounds are almost optimal as they recover the same rate as would be obtained by setting the "in probability" parameter after the realization of the data.

### Neural Guided Diffusion Bridges 
[[arxiv](https://arxiv.org/abs/2502.11909)] [[cool](https://papers.cool/arxiv/2502.11909)] [[pdf](https://arxiv.org/pdf/2502.11909)]
> **Authors**: Gefan Yang,Frank van der Meulen,Stefan Sommer
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: We propose a novel method for simulating conditioned diffusion processes (diffusion bridges) in Euclidean spaces. By training a neural network to approximate bridge dynamics, our approach eliminates the need for computationally intensive Markov Chain Monte Carlo (MCMC) methods or reverse-process modeling. Compared to existing methods, it offers greater robustness across various diffusion specifications and conditioning scenarios. This applies in particular to rare events and multimodal distributions, which pose challenges for score-learning- and MCMC-based approaches. We propose a flexible variational family for approximating the diffusion bridge path measure which is partially specified by a neural network. Once trained, it enables efficient independent sampling at a cost comparable to sampling the unconditioned (forward) process.

### JoLT: Joint Probabilistic Predictions on Tabular Data Using LLMs 
[[arxiv](https://arxiv.org/abs/2502.11877)] [[cool](https://papers.cool/arxiv/2502.11877)] [[pdf](https://arxiv.org/pdf/2502.11877)]
> **Authors**: Aliaksandra Shysheya,John Bronskill,James Requeima,Shoaib Ahmed Siddiqui,Javier Gonzalez,David Duvenaud,Richard E. Turner
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: We introduce a simple method for probabilistic predictions on tabular data based on Large Language Models (LLMs) called JoLT (Joint LLM Process for Tabular data). JoLT uses the in-context learning capabilities of LLMs to define joint distributions over tabular data conditioned on user-specified side information about the problem, exploiting the vast repository of latent problem-relevant knowledge encoded in LLMs. JoLT defines joint distributions for multiple target variables with potentially heterogeneous data types without any data conversion, data preprocessing, special handling of missing data, or model training, making it accessible and efficient for practitioners. Our experiments show that JoLT outperforms competitive methods on low-shot single-target and multi-target tabular classification and regression tasks. Furthermore, we show that JoLT can automatically handle missing data and perform data imputation by leveraging textual side information. We argue that due to its simplicity and generality, JoLT is an effective approach for a wide variety of real prediction problems.

### Private Synthetic Graph Generation and Fused Gromov-Wasserstein Distance 
[[arxiv](https://arxiv.org/abs/2502.11778)] [[cool](https://papers.cool/arxiv/2502.11778)] [[pdf](https://arxiv.org/pdf/2502.11778)]
> **Authors**: Leoni Carla Wirth,Gholamali Aminian,Gesine Reinert
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,数据结构和算法,机器学习,可能性
- **Abstract**: Networks are popular for representing complex data. In particular, differentially private synthetic networks are much in demand for method and algorithm development. The network generator should be easy to implement and should come with theoretical guarantees. Here we start with complex data as input and jointly provide a network representation as well as a synthetic network generator. Using a random connection model, we devise an effective algorithmic approach for generating attributed synthetic graphs which is $ε$-differentially private at the vertex level, while preserving utility under an appropriate notion of distance which we develop. We provide theoretical guarantees for the accuracy of the private synthetic graphs using the fused Gromov-Wasserstein distance, which extends the Wasserstein metric to structured data. Our method draws inspiration from the PSMM method of \citet{he2023}.

### Deep Subspace Learning for Surface Anomaly Classification Based on 3D Point Cloud Data 
[[arxiv](https://arxiv.org/abs/2502.11669)] [[cool](https://papers.cool/arxiv/2502.11669)] [[pdf](https://arxiv.org/pdf/2502.11669)]
> **Authors**: Xuanming Cao,Chengyu Tao,Juan Du
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Surface anomaly classification is critical for manufacturing system fault diagnosis and quality control. However, the following challenges always hinder accurate anomaly classification in practice: (i) Anomaly patterns exhibit intra-class variation and inter-class similarity, presenting challenges in the accurate classification of each sample. (ii) Despite the predefined classes, new types of anomalies can occur during production that require to be detected accurately. (iii) Anomalous data is rare in manufacturing processes, leading to limited data for model learning. To tackle the above challenges simultaneously, this paper proposes a novel deep subspace learning-based 3D anomaly classification model. Specifically, starting from a lightweight encoder to extract the latent representations, we model each class as a subspace to account for the intra-class variation, while promoting distinct subspaces of different classes to tackle the inter-class similarity. Moreover, the explicit modeling of subspaces offers the capability to detect out-of-distribution samples, i.e., new types of anomalies, and the regularization effect with much fewer learnable parameters of our proposed subspace classifier, compared to the popular Multi-Layer Perceptions (MLPs). Extensive numerical experiments demonstrate our method achieves better anomaly classification results than benchmark methods, and can effectively identify the new types of anomalies.

### On the kernel learning problem 
[[arxiv](https://arxiv.org/abs/2502.11665)] [[cool](https://papers.cool/arxiv/2502.11665)] [[pdf](https://arxiv.org/pdf/2502.11665)]
> **Authors**: Yang Li,Feng Ruan
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 61 pages
- **标题**: None
- **领域**: 机器学习,机器学习,经典分析和常微分方程,泛函分析,优化与控制
- **Abstract**: The classical kernel ridge regression problem aims to find the best fit for the output $Y$ as a function of the input data $X\in \mathbb{R}^d$, with a fixed choice of regularization term imposed by a given choice of a reproducing kernel Hilbert space, such as a Sobolev space. Here we consider a generalization of the kernel ridge regression problem, by introducing an extra matrix parameter $U$, which aims to detect the scale parameters and the feature variables in the data, and thereby improve the efficiency of kernel ridge regression. This naturally leads to a nonlinear variational problem to optimize the choice of $U$. We study various foundational mathematical aspects of this variational problem, and in particular how this behaves in the presence of multiscale structures in the data.

### Distributional autoencoders know the score 
[[arxiv](https://arxiv.org/abs/2502.11583)] [[cool](https://papers.cool/arxiv/2502.11583)] [[pdf](https://arxiv.org/pdf/2502.11583)]
> **Authors**: Andrej Leban
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: 24 pages, 6 figures
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: This work presents novel and desirable properties of a recently introduced class of autoencoders -- the Distributional Principal Autoencoder (DPA) -- that combines distributionally correct reconstruction with principal components-like interpretability of the encodings. First, we show that the level sets of the encoder orient themselves exactly with regard to the score of the data distribution. This both explains the method's often remarkable performance in disentangling the the factors of variation of the data, as well as opens up possibilities of recovering its distribution while having access to samples only. In settings where the score itself has physical meaning -- such as when the data obey the Boltzmann distribution -- we demonstrate that the method can recover scientifically important quantities such as the \textit{minimum free energy path}. Second, we show that if the data lie on a manifold that can be approximated by the encoder, the optimal encoder's components beyond the dimension of the manifold will carry absolutely no additional information about the data distribution. This promises new ways of determining the number of relevant dimensions of the data beyond common heuristics such as the scree plot. Finally, the fact that the method is learning the score means that it could have promise as a generative model, potentially rivaling approaches such as diffusion, which similarly attempts to approximate the score of the data distribution.

### All Models Are Miscalibrated, But Some Less So: Comparing Calibration with Conditional Mean Operators 
[[arxiv](https://arxiv.org/abs/2502.11465)] [[cool](https://papers.cool/arxiv/2502.11465)] [[pdf](https://arxiv.org/pdf/2502.11465)]
> **Authors**: Peter Moskvichev,Dino Sejdinovic
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-18
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: When working in a high-risk setting, having well calibrated probabilistic predictive models is a crucial requirement. However, estimators for calibration error are not always able to correctly distinguish which model is better calibrated. We propose the \emph{conditional kernel calibration error} (CKCE) which is based on the Hilbert-Schmidt norm of the difference between conditional mean operators. By working directly with the definition of strong calibration as the distance between conditional distributions, which we represent by their embeddings in reproducing kernel Hilbert spaces, the CKCE is less sensitive to the marginal distribution of predictive models. This makes it more effective for relative comparisons than previously proposed calibration metrics. Our experiments, using both synthetic and real data, show that CKCE provides a more consistent ranking of models by their calibration error and is more robust against distribution shift.

## 其他论文

- [AnimAlte:Designing AI-Infused Cartoon Videos to Improve Preschoolers' Language Learning with Family Engagement at Home](https://arxiv.org/abs/2502.12526)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Mentoring Software in Education and Its Impact on Teacher Development: An Integrative Literature Review](https://arxiv.org/abs/2502.12515)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [Simulating Cooperative Prosocial Behavior with Multi-Agent LLMs: Evidence and Mechanisms for AI Agents to Inform Policy Decisions](https://arxiv.org/abs/2502.12504)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [USPilot: An Embodied Robotic Assistant Ultrasound System with Large Language Model Enhanced Graph Planner](https://arxiv.org/abs/2502.12498)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [SoK: Understanding Vulnerabilities in the Large Language Model Supply Chain](https://arxiv.org/abs/2502.12497)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [Explainable AI-Driven Neural Activity Analysis in Parkinsonian Rats under Electrical Stimulation](https://arxiv.org/abs/2502.12471)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC,q-bio.NC in whitelist
- [From Principles to Applications: A Comprehensive Survey of Discrete Tokenizers in Generation, Comprehension, Recommendation, and Information Retrieval](https://arxiv.org/abs/2502.12448)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [Protecting Human Cognition in the Age of AI](https://arxiv.org/abs/2502.12447)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC,cs.CY in whitelist
- [TherAIssist: Assisting Art Therapy Homework and Client-Practitioner Collaboration through Human-AI Interaction](https://arxiv.org/abs/2502.12443)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Note-Level Singing Melody Transcription for Time-Aligned Musical Score Generation](https://arxiv.org/abs/2502.12438)
  - **标题**: None
  - **Filtered Reason**: none of cs.SD,eess.SP,eess.AS in whitelist
- [Sensing-based Robustness Challenges in Agricultural Robotic Harvesting](https://arxiv.org/abs/2502.12403)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO,eess.SY in whitelist
- [Hardware-Software Co-Design for Accelerating Transformer Inference Leveraging Compute-in-Memory](https://arxiv.org/abs/2502.12344)
  - **标题**: None
  - **Filtered Reason**: none of cs.AR in whitelist
- [Rational Capability in Concurrent Games](https://arxiv.org/abs/2502.12286)
  - **标题**: None
  - **Filtered Reason**: none of cs.MA,cs.LO in whitelist
- [Gem5-AcceSys: Enabling System-Level Exploration of Standard Interconnects for Novel Accelerators](https://arxiv.org/abs/2502.12273)
  - **标题**: None
  - **Filtered Reason**: none of cs.PF,cs.AR in whitelist
- [ReF Decompile: Relabeling and Function Call Enhanced Decompile](https://arxiv.org/abs/2502.12221)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [AI-Augmented Metamorphic Testing for Comprehensive Validation of Autonomous Vehicles](https://arxiv.org/abs/2502.12208)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO,cs.SE in whitelist
- [Robust blue-green urban flood risk management optimised with a genetic algorithm for multiple rainstorm return periods](https://arxiv.org/abs/2502.12174)
  - **标题**: None
  - **Filtered Reason**: none of cs.CE,cs.CY,cs.NE in whitelist
- [Learning in a Multifield Coherent Ising Machine](https://arxiv.org/abs/2502.12020)
  - **标题**: None
  - **Filtered Reason**: none of cond-mat.dis-nn,cond-mat.mes-hall,cs.ET,nlin.AO,cs.NE in whitelist
- [Beyond Sentiment: Examining the Role of Moral Foundations in User Engagement with News on Twitter](https://arxiv.org/abs/2502.12009)
  - **标题**: None
  - **Filtered Reason**: none of cs.SI in whitelist
- [Learning Automata with Name Allocation](https://arxiv.org/abs/2502.11947)
  - **标题**: None
  - **Filtered Reason**: none of cs.FL in whitelist
- [A formal implementation of Behavior Trees to act in robotics](https://arxiv.org/abs/2502.11904)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [MQG4AI Towards Responsible High-risk AI -- Illustrated for Transparency Focusing on Explainability Techniques](https://arxiv.org/abs/2502.11889)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [Neural Chaos: A Spectral Stochastic Neural Operator](https://arxiv.org/abs/2502.11835)
  - **标题**: None
  - **Filtered Reason**: none of cs.CE,stat.ML,physics.comp-ph in whitelist
- [HAAN: A Holistic Approach for Accelerating Normalization Operations in Large Language Models](https://arxiv.org/abs/2502.11832)
  - **标题**: None
  - **Filtered Reason**: none of cs.AR in whitelist
- [Assessing the impacts of tradable credit schemes through agent-based simulation](https://arxiv.org/abs/2502.11822)
  - **标题**: None
  - **Filtered Reason**: none of stat.ML,cs.SE,cs.GT in whitelist
- [Residual Learning towards High-fidelity Vehicle Dynamics Modeling with Transformer](https://arxiv.org/abs/2502.11800)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [BackdoorDM: A Comprehensive Benchmark for Backdoor Learning in Diffusion Model](https://arxiv.org/abs/2502.11798)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [Exploring the Versal AI Engine for 3D Gaussian Splatting](https://arxiv.org/abs/2502.11782)
  - **标题**: None
  - **Filtered Reason**: none of cs.AR in whitelist
- [Open-Ended and Knowledge-Intensive Video Question Answering](https://arxiv.org/abs/2502.11747)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [Enhancing Recommendation Explanations through User-Centric Refinement](https://arxiv.org/abs/2502.11721)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [Can you pass that tool?: Implications of Indirect Speech in Physical Human-Robot Collaboration](https://arxiv.org/abs/2502.11720)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC,cs.RO in whitelist
- [Parallel-in-Time Kalman Smoothing Using Orthogonal Transformations](https://arxiv.org/abs/2502.11686)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC,eess.SP in whitelist
- [Accelerating Elliptic Curve Point Additions on Versal AI Engine for Multi-scalar Multiplication](https://arxiv.org/abs/2502.11660)
  - **标题**: None
  - **Filtered Reason**: none of cs.AR in whitelist
- [An Innovative Brain-Computer Interface Interaction System Based on the Large Language Model](https://arxiv.org/abs/2502.11659)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Causal Models in Requirement Specifications for Machine Learning: A vision](https://arxiv.org/abs/2502.11629)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Assessing Correctness in LLM-Based Code Generation via Uncertainty Estimation](https://arxiv.org/abs/2502.11620)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Accuracy Assessment of OpenAlex and Clarivate Scholar ID with an LLM-Assisted Benchmark](https://arxiv.org/abs/2502.11610)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [Improving Rare-Word Recognition of Whisper in Zero-Shot Settings](https://arxiv.org/abs/2502.11572)
  - **标题**: None
  - **Filtered Reason**: none of cs.SD,eess.AS in whitelist
- [Anti-Degeneracy Scheme for Lidar SLAM based on Particle Filter in Geometry Feature-Less Environments](https://arxiv.org/abs/2502.11486)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [BagChain: A Dual-functional Blockchain Leveraging Bagging-based Distributed Learning](https://arxiv.org/abs/2502.11464)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC in whitelist
- [Adversary-Aware DPO: Enhancing Safety Alignment in Vision Language Models via Adversarial Training](https://arxiv.org/abs/2502.11455)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
