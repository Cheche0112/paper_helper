> 本文由 [https://github.com/huiyeruzhou/arxiv_crawler](https://github.com/huiyeruzhou/arxiv_crawler) 自动生成
>
> 领域白名单：cs.AI,cs.CL,cs.LG,cs.CV
> 关键词： LLM, GPT, AI, language+model, deep+learning, transformer, neural+network, machine+learning

# 论文全览：2025-02-11

共有432篇相关领域论文, 另有25篇其他

## 天体物理学仪器和方法(astro-ph.IM:Instrumentation and Methods for Astrophysics)

### Flat U-Net: An Efficient Ultralightweight Model for Solar Filament Segmentation in Full-disk H$α$ Images 
[[arxiv](https://arxiv.org/abs/2502.07259)] [[cool](https://papers.cool/arxiv/2502.07259)] [[pdf](https://arxiv.org/pdf/2502.07259)]
> **Authors**: GaoFei Zhu,GangHua Lin,Xiao Yang,Cheng Zeng
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 15 pages, 5 figures, 3 tables, accepted for publication in ApJ
- **标题**: None
- **领域**: 天体物理学仪器和方法,太阳和恒星天体物理学,计算机视觉和模式识别,机器学习
- **Abstract**: Solar filaments are one of the most prominent features observed on the Sun, and their evolutions are closely related to various solar activities, such as flares and coronal mass ejections. Real-time automated identification of solar filaments is the most effective approach to managing large volumes of data. Existing models of filament identification are characterized by large parameter sizes and high computational costs, which limit their future applications in highly integrated and intelligent ground-based and space-borne observation devices. Consequently, the design of more lightweight models will facilitate the advancement of intelligent observation equipment. In this study, we introduce Flat U-Net, a novel and highly efficient ultralightweight model that incorporates simplified channel attention (SCA) and channel self-attention (CSA) convolutional blocks for the segmentation of solar filaments in full-disk H$α$ images. Feature information from each network layer is fully extracted to reconstruct interchannel feature representations. Each block effectively optimizes the channel features from the previous layer, significantly reducing parameters. The network architecture presents an elegant flattening, improving its efficiency, and simplifying the overall design. Experimental validation demonstrates that a model composed of pure SCAs achieves a precision of approximately 0.93, with dice similarity coefficient (DSC) and recall rates of 0.76 and 0.64, respectively, significantly outperforming the classical U-Net. Introducing a certain number of CSA blocks improves the DSC and recall rates to 0.82 and 0.74, respectively, which demonstrates a pronounced advantage, particularly concerning model weight size and detection effectiveness. The data set, models, and code are available as open-source resources.

## 材料科学(cond-mat.mtrl-sci:Materials Science)

### WyckoffDiff -- A Generative Diffusion Model for Crystal Symmetry 
[[arxiv](https://arxiv.org/abs/2502.06485)] [[cool](https://papers.cool/arxiv/2502.06485)] [[pdf](https://arxiv.org/pdf/2502.06485)]
> **Authors**: Filip Ekström Kelvinius,Oskar B. Andersson,Abhijith S. Parackal,Dong Qian,Rickard Armiento,Fredrik Lindsten
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 材料科学,人工智能,机器学习
- **Abstract**: Crystalline materials often exhibit a high level of symmetry. However, most generative models do not account for symmetry, but rather model each atom without any constraints on its position or element. We propose a generative model, Wyckoff Diffusion (WyckoffDiff), which generates symmetry-based descriptions of crystals. This is enabled by considering a crystal structure representation that encodes all symmetry, and we design a novel neural network architecture which enables using this representation inside a discrete generative model framework. In addition to respecting symmetry by construction, the discrete nature of our model enables fast generation. We additionally present a new metric, Fréchet Wrenformer Distance, which captures the symmetry aspects of the materials generated, and we benchmark WyckoffDiff against recently proposed generative models for crystal generation.

### A physics-based data-driven model for CO$_2$ gas diffusion electrodes to drive automated laboratories 
[[arxiv](https://arxiv.org/abs/2502.06323)] [[cool](https://papers.cool/arxiv/2502.06323)] [[pdf](https://arxiv.org/pdf/2502.06323)]
> **Authors**: Ivan Grega,Félix Therrien,Abhishek Soni,Karry Ocean,Kevan Dettelbach,Ribwar Ahmadi,Mehrdad Mokhtari,Curtis P. Berlinguette,Yoshua Bengio
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 7 pages, 5 figures. Submitted to AI4Mat-ICLR2025 workshop
- **标题**: None
- **领域**: 材料科学,机器学习
- **Abstract**: The electrochemical reduction of atmospheric CO$_2$ into high-energy molecules with renewable energy is a promising avenue for energy storage that can take advantage of existing infrastructure especially in areas where sustainable alternatives to fossil fuels do not exist. Automated laboratories are currently being developed and used to optimize the composition and operating conditions of gas diffusion electrodes (GDEs), the device in which this reaction takes place. Improving the efficiency of GDEs is crucial for this technology to become viable. Here we present a modeling framework to efficiently explore the high-dimensional parameter space of GDE designs in an active learning context. At the core of the framework is an uncertainty-aware physics model calibrated with experimental data. The model has the flexibility to capture various input parameter spaces and any carbon products which can be modeled with Tafel kinetics. It is interpretable, and a Gaussian process layer can capture deviations of real data from the function space of the physical model itself. We deploy the model in a simulated active learning setup with real electrochemical data gathered by the AdaCarbon automated laboratory and show that it can be used to efficiently traverse the multi-dimensional parameter space.

## 软凝聚态物质(cond-mat.soft:Soft Condensed Matter)

### On the use of neural networks for the structural characterization of polymeric porous materials 
[[arxiv](https://arxiv.org/abs/2502.07076)] [[cool](https://papers.cool/arxiv/2502.07076)] [[pdf](https://arxiv.org/pdf/2502.07076)]
> **Authors**: Jorge Torre,Suset Barroso-Solares,M. A. Rodríguez-Pérez,Javier Pinto
> **First submission**: 2025-01-25
> **First announcement**: 2025-02-11
> **comment**: ef:Polymer, Volume 291, 2024, 126597
- **标题**: None
- **领域**: 软凝聚态物质,材料科学,计算机视觉和模式识别,图像和视频处理
- **Abstract**: The structural characterization is an essential task in the study of porous materials. To achieve reliable results, it requires to evaluate images with hundreds of pores. Current methods require large time amounts and are subjected to human errors and subjectivity. A completely automatic tool would not only speed up the process but also enhance its reliability and reproducibility. Therefore, the main objective of this article is the study of a deep-learning-based technique for the structural characterization of porous materials, through the use of a convolutional neural network. Several fine-tuned Mask R CNN models are evaluated using different training configurations in four separate datasets each composed of numerous SEM images of diverse polymeric porous materials: closed-pore extruded polystyrene (XPS), polyurethane (PU), and poly(methyl methacrylate) (PMMA), and open-pore PU. Results prove the tool capable of providing very accurate results, equivalent to those achieved by time consuming manual methods, in a matter of seconds.

## 人工智能(cs.AI:Artificial Intelligence)

### Monte Carlo Tree Diffusion for System 2 Planning 
[[arxiv](https://arxiv.org/abs/2502.07202)] [[cool](https://papers.cool/arxiv/2502.07202)] [[pdf](https://arxiv.org/pdf/2502.07202)]
> **Authors**: Jaesik Yoon,Hyeonseo Cho,Doojin Baek,Yoshua Bengio,Sungjin Ahn
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 20 pages, 7 figures
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: Diffusion models have recently emerged as a powerful tool for planning. However, unlike Monte Carlo Tree Search (MCTS)-whose performance naturally improves with additional test-time computation (TTC), standard diffusion-based planners offer only limited avenues for TTC scalability. In this paper, we introduce Monte Carlo Tree Diffusion (MCTD), a novel framework that integrates the generative strength of diffusion models with the adaptive search capabilities of MCTS. Our method reconceptualizes denoising as a tree-structured process, allowing partially denoised plans to be iteratively evaluated, pruned, and refined. By selectively expanding promising trajectories while retaining the flexibility to revisit and improve suboptimal branches, MCTD achieves the benefits of MCTS such as controlling exploration-exploitation trade-offs within the diffusion framework. Empirical results on challenging long-horizon tasks show that MCTD outperforms diffusion baselines, yielding higher-quality solutions as TTC increases.

### Bag of Tricks for Inference-time Computation of LLM Reasoning 
[[arxiv](https://arxiv.org/abs/2502.07191)] [[cool](https://papers.cool/arxiv/2502.07191)] [[pdf](https://arxiv.org/pdf/2502.07191)]
> **Authors**: Fan Liu,Wenshuo Chao,Naiqiang Tan,Hao Liu
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: With the advancement of large language models (LLMs), solving complex reasoning tasks has gained increasing attention. Inference-time computation methods (e.g., Best-of-N, beam search, et al.) are particularly valuable as they can enhance reasoning performance without modifying model parameters or requiring additional training. However, these techniques come with implementation challenges, and most existing methods remain at the proof-of-concept stage with limited practical adoption due to their computational complexity and varying effectiveness across different tasks. In this paper, we investigate and benchmark diverse inference-time computation strategies across reasoning tasks of varying complexity. Since most current methods rely on a proposer-verifier pipeline that first generates candidate solutions (e.g., reasoning solutions) and then selects the best one based on reward signals (e.g., RLHF rewards, process rewards), our research focuses on optimizing both candidate solution generation (e.g., instructing prompts, hyperparameters such as temperature and top-p) and reward mechanisms (e.g., self-evaluation, reward types). Through extensive experiments (more than 20,000 A100-80G GPU hours with over 1,000 experiments) across a variety of models (e.g., Llama, Qwen, and Mistral families) of various sizes, our ablation studies reveal that previously overlooked strategies can significantly enhance performance (e.g., tuning temperature can improve reasoning task performance by up to 5%). Furthermore, we establish a standardized benchmark for inference-time computation by systematically evaluating six representative methods across eight reasoning tasks. These findings provide a stronger foundation for future research. The code is available at https://github.com/usail-hkust/benchmark_inference_time_computation_LLM

### Understanding LLMs' Fluid Intelligence Deficiency: An Analysis of the ARC Task 
[[arxiv](https://arxiv.org/abs/2502.07190)] [[cool](https://papers.cool/arxiv/2502.07190)] [[pdf](https://arxiv.org/pdf/2502.07190)]
> **Authors**: Junjie Wu,Mo Yu,Lemao Liu,Dit-Yan Yeung,Jie Zhou
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 22 pages, 9 figures, accepted by NAACL 2025 main conference
- **标题**: None
- **领域**: 人工智能
- **Abstract**: While LLMs have exhibited strong performance on various NLP tasks, it is noteworthy that most of these tasks rely on utilizing the vast amount of knowledge encoded in LLMs' parameters, rather than solving new problems without prior knowledge. In cognitive research, the latter ability is referred to as fluid intelligence, which is considered to be critical for assessing human intelligence. Recent research on fluid intelligence assessments has highlighted significant deficiencies in LLMs' abilities. In this paper, we analyze the challenges LLMs face in demonstrating fluid intelligence through controlled experiments, using the most representative ARC task as an example. Our study revealed three major limitations in existing LLMs: limited ability for skill composition, unfamiliarity with abstract input formats, and the intrinsic deficiency of left-to-right decoding. Our data and code can be found in https://wujunjie1998.github.io/araoc-benchmark.github.io/.

### Interactive Data Harmonization with LLM Agents 
[[arxiv](https://arxiv.org/abs/2502.07132)] [[cool](https://papers.cool/arxiv/2502.07132)] [[pdf](https://arxiv.org/pdf/2502.07132)]
> **Authors**: Aécio Santos,Eduardo H. M. Pena,Roque Lopez,Juliana Freire
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,数据库
- **Abstract**: Data harmonization is an essential task that entails integrating datasets from diverse sources. Despite years of research in this area, it remains a time-consuming and challenging task due to schema mismatches, varying terminologies, and differences in data collection methodologies. This paper presents the case for agentic data harmonization as a means to both empower experts to harmonize their data and to streamline the process. We introduce Harmonia, a system that combines LLM-based reasoning, an interactive user interface, and a library of data harmonization primitives to automate the synthesis of data harmonization pipelines. We demonstrate Harmonia in a clinical data harmonization scenario, where it helps to interactively create reusable pipelines that map datasets to a standard format. Finally, we discuss challenges and open problems, and suggest research directions for advancing our vision.

### Autonomous Deep Agent 
[[arxiv](https://arxiv.org/abs/2502.07056)] [[cool](https://papers.cool/arxiv/2502.07056)] [[pdf](https://arxiv.org/pdf/2502.07056)]
> **Authors**: Amy Yu,Erik Lebedev,Lincoln Everett,Xiaoxin Chen,Terry Chen
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: :I.2.6; I.2.7
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: This technical brief introduces Deep Agent, an advanced autonomous AI system designed to manage complex multi-phase tasks through a novel hierarchical task management architecture. The system's foundation is built on our Hierarchical Task DAG (HTDAG) framework, which dynamically decomposes high-level objectives into manageable sub-tasks while rigorously maintaining dependencies and execution coherence. Deep Agent advances beyond traditional agent systems through three key innovations: First, it implements a recursive two-stage planner-executor architecture that enables continuous task refinement and adaptation as circumstances change. Second, it features an Autonomous API & Tool Creation (AATC) system that automatically generates reusable components from UI interactions, substantially reducing operational costs for similar tasks. Third, it incorporates Prompt Tweaking Engine and Autonomous Prompt Feedback Learning components that optimize Large Language Model prompts for specific scenarios, enhancing both inference accuracy and operational stability. These components are integrated to form a service infrastructure that manages user contexts, handles complex task dependencies, and orchestrates end-to-end agentic workflow execution. Through this sophisticated architecture, Deep Agent establishes a novel paradigm in self-governing AI systems, demonstrating robust capability to independently handle intricate, multi-step tasks while maintaining consistent efficiency and reliability through continuous self-optimization.

### Position: Episodic Memory is the Missing Piece for Long-Term LLM Agents 
[[arxiv](https://arxiv.org/abs/2502.06975)] [[cool](https://papers.cool/arxiv/2502.06975)] [[pdf](https://arxiv.org/pdf/2502.06975)]
> **Authors**: Mathis Pink,Qinyuan Wu,Vy Ai Vo,Javier Turek,Jianing Mu,Alexander Huth,Mariya Toneva
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: As Large Language Models (LLMs) evolve from text-completion tools into fully fledged agents operating in dynamic environments, they must address the challenge of continually learning and retaining long-term knowledge. Many biological systems solve these challenges with episodic memory, which supports single-shot learning of instance-specific contexts. Inspired by this, we present an episodic memory framework for LLM agents, centered around five key properties of episodic memory that underlie adaptive and context-sensitive behavior. With various research efforts already partially covering these properties, this position paper argues that now is the right time for an explicit, integrated focus on episodic memory to catalyze the development of long-term agents. To this end, we outline a roadmap that unites several research directions under the goal to support all five properties of episodic memory for more efficient long-term LLM agents.

### On the Emergence of Thinking in LLMs I: Searching for the Right Intuition 
[[arxiv](https://arxiv.org/abs/2502.06773)] [[cool](https://papers.cool/arxiv/2502.06773)] [[pdf](https://arxiv.org/pdf/2502.06773)]
> **Authors**: Guanghao Ye,Khiem Duc Pham,Xinzhi Zhang,Sivakanth Gopi,Baolin Peng,Beibin Li,Janardhan Kulkarni,Huseyin A. Inan
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Abstract shortened for arXiv
- **标题**: None
- **领域**: 人工智能,计算语言学,机器学习
- **Abstract**: Recent AI advancements, such as OpenAI's new models, are transforming LLMs into LRMs (Large Reasoning Models) that perform reasoning during inference, taking extra time and compute for higher-quality outputs. We aim to uncover the algorithmic framework for training LRMs. Methods like self-consistency, PRM, and AlphaZero suggest reasoning as guided search. We ask: what is the simplest, most scalable way to enable search in LLMs? We propose a post-training framework called Reinforcement Learning via Self-Play (RLSP). RLSP involves three steps: (1) supervised fine-tuning with human or synthetic demonstrations of the reasoning process, (2) using an exploration reward signal to encourage diverse and efficient reasoning behaviors, and (3) RL training with an outcome verifier to ensure correctness while preventing reward hacking. Our key innovation is to decouple exploration and correctness signals during PPO training, carefully balancing them to improve performance and efficiency. Empirical studies in the math domain show that RLSP improves reasoning. On the Llama-3.1-8B-Instruct model, RLSP can boost performance by 23% in MATH-500 test set; On AIME 2024 math problems, Qwen2.5-32B-Instruct improved by 10% due to RLSP. However, a more important finding of this work is that the models trained using RLSP, even with the simplest exploration reward that encourages the model to take more intermediate steps, showed several emergent behaviors such as backtracking, exploration of ideas, and verification. These findings demonstrate that RLSP framework might be enough to enable emergence of complex reasoning abilities in LLMs when scaled. Lastly, we propose a theory as to why RLSP search strategy is more suitable for LLMs inspired by a remarkable result that says CoT provably increases computational power of LLMs, which grows as the number of steps in CoT \cite{li2024chain,merrill2023expresssive}.

### Application of Artificial Intelligence (AI) in Civil Engineering 
[[arxiv](https://arxiv.org/abs/2502.06727)] [[cool](https://papers.cool/arxiv/2502.06727)] [[pdf](https://arxiv.org/pdf/2502.06727)]
> **Authors**: Temitope Funmilayo Awolusi,Bernard Chukwuemeka Finbarrs-Ezema,Isaac Munachimdinamma Chukwudulue,Marc Azab
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Kindly cite published version if given access
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Hard computing generally deals with precise data, which provides ideal solutions to problems. However, in the civil engineering field, amongst other disciplines, that is not always the case as real-world systems are continuously changing. Here lies the need to explore soft computing methods and artificial intelligence to solve civil engineering shortcomings. The integration of advanced computational models, including Artificial Neural Networks (ANNs), Fuzzy Logic, Genetic Algorithms (GAs), and Probabilistic Reasoning, has revolutionized the domain of civil engineering. These models have significantly advanced diverse sub-fields by offering innovative solutions and improved analysis capabilities. Sub-fields such as: slope stability analysis, bearing capacity, water quality and treatment, transportation systems, air quality, structural materials, etc. ANNs predict non-linearities and provide accurate estimates. Fuzzy logic uses an efficient decision-making process to provide a more precise assessment of systems. Lastly, while GAs optimizes models (based on evolutionary processes) for better outcomes, probabilistic reasoning lowers their statistical uncertainties.

### A Frontier AI Risk Management Framework: Bridging the Gap Between Current AI Practices and Established Risk Management 
[[arxiv](https://arxiv.org/abs/2502.06656)] [[cool](https://papers.cool/arxiv/2502.06656)] [[pdf](https://arxiv.org/pdf/2502.06656)]
> **Authors**: Simeon Campos,Henry Papadatos,Fabien Roger,Chloé Touzet,Otter Quarks,Malcolm Murray
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: The recent development of powerful AI systems has highlighted the need for robust risk management frameworks in the AI industry. Although companies have begun to implement safety frameworks, current approaches often lack the systematic rigor found in other high-risk industries. This paper presents a comprehensive risk management framework for the development of frontier AI that bridges this gap by integrating established risk management principles with emerging AI-specific practices. The framework consists of four key components: (1) risk identification (through literature review, open-ended red-teaming, and risk modeling), (2) risk analysis and evaluation using quantitative metrics and clearly defined thresholds, (3) risk treatment through mitigation measures such as containment, deployment controls, and assurance processes, and (4) risk governance establishing clear organizational structures and accountability. Drawing from best practices in mature industries such as aviation or nuclear power, while accounting for AI's unique challenges, this framework provides AI developers with actionable guidelines for implementing robust risk management. The paper details how each component should be implemented throughout the life-cycle of the AI system - from planning through deployment - and emphasizes the importance and feasibility of conducting risk management work prior to the final training run to minimize the burden associated with it.

### Unbiased Evaluation of Large Language Models from a Causal Perspective 
[[arxiv](https://arxiv.org/abs/2502.06655)] [[cool](https://papers.cool/arxiv/2502.06655)] [[pdf](https://arxiv.org/pdf/2502.06655)]
> **Authors**: Meilin Chen,Jian Tian,Liang Ma,Di Xie,Weijie Chen,Jiang Zhu
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Benchmark contamination has become a significant concern in the LLM evaluation community. Previous Agents-as-an-Evaluator address this issue by involving agents in the generation of questions. Despite their success, the biases in Agents-as-an-Evaluator methods remain largely unexplored. In this paper, we present a theoretical formulation of evaluation bias, providing valuable insights into designing unbiased evaluation protocols. Furthermore, we identify two type of bias in Agents-as-an-Evaluator through carefully designed probing tasks on a minimal Agents-as-an-Evaluator setup. To address these issues, we propose the Unbiased Evaluator, an evaluation protocol that delivers a more comprehensive, unbiased, and interpretable assessment of LLMs.Extensive experiments reveal significant room for improvement in current LLMs. Additionally, we demonstrate that the Unbiased Evaluator not only offers strong evidence of benchmark contamination but also provides interpretable evaluation results.

### On the Impact of the Utility in Semivalue-based Data Valuation 
[[arxiv](https://arxiv.org/abs/2502.06574)] [[cool](https://papers.cool/arxiv/2502.06574)] [[pdf](https://arxiv.org/pdf/2502.06574)]
> **Authors**: Mélissa Tamine,Benjamin Heymann,Patrick Loiseau,Maxime Vono
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 34 pages, 21 figures
- **标题**: None
- **领域**: 人工智能,计算机科学与博弈论,机器学习
- **Abstract**: Semivalue-based data valuation in machine learning (ML) quantifies the contribution of individual data points to a downstream ML task by leveraging principles from cooperative game theory and the notion of utility. While this framework has been used in practice for assessing data quality, our experiments reveal inconsistent valuation outcomes across different utilities, albeit all related to ML performance. Beyond raising concerns about the reliability of data valuation, this inconsistency is challenging to interpret, as it stems from the complex interaction of the utility with data points and semivalue weights, which has barely been studied in prior work. In this paper, we take a first step toward clarifying the utility impact on semivalue-based data valuation. Specifically, we provide geometric interpretations of this impact for a broad family of classification utilities, which includes the accuracy and the arithmetic mean. We introduce the notion of spatial signatures: given a semivalue, data points can be embedded into a two-dimensional space, and utility functions map to the dual of this space. This geometric perspective separates the influence of the dataset and semivalue from that of the utility, providing a theoretical explanation for the experimentally observed sensitivity of valuation outcomes to the utility choice.

### Can We Trust AI Benchmarks? An Interdisciplinary Review of Current Issues in AI Evaluation 
[[arxiv](https://arxiv.org/abs/2502.06559)] [[cool](https://papers.cool/arxiv/2502.06559)] [[pdf](https://arxiv.org/pdf/2502.06559)]
> **Authors**: Maria Eriksson,Erasmo Purificato,Arman Noroozian,Joao Vinagre,Guillaume Chaslot,Emilia Gomez,David Fernandez-Llorca
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Submitted to ACM Conference on Fairness, Accountability, and Transparency (FAccT) 2025
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Quantitative Artificial Intelligence (AI) Benchmarks have emerged as fundamental tools for evaluating the performance, capability, and safety of AI models and systems. Currently, they shape the direction of AI development and are playing an increasingly prominent role in regulatory frameworks. As their influence grows, however, so too does concerns about how and with what effects they evaluate highly sensitive topics such as capabilities, including high-impact capabilities, safety and systemic risks. This paper presents an interdisciplinary meta-review of about 100 studies that discuss shortcomings in quantitative benchmarking practices, published in the last 10 years. It brings together many fine-grained issues in the design and application of benchmarks (such as biases in dataset creation, inadequate documentation, data contamination, and failures to distinguish signal from noise) with broader sociotechnical issues (such as an over-focus on evaluating text-based AI models according to one-time testing logic that fails to account for how AI models are increasingly multimodal and interact with humans and other technical systems). Our review also highlights a series of systemic flaws in current benchmarking practices, such as misaligned incentives, construct validity issues, unknown unknowns, and problems with the gaming of benchmark results. Furthermore, it underscores how benchmark practices are fundamentally shaped by cultural, commercial and competitive dynamics that often prioritise state-of-the-art performance at the expense of broader societal concerns. By providing an overview of risks associated with existing benchmarking procedures, we problematise disproportionate trust placed in benchmarks and contribute to ongoing efforts to improve the accountability and relevance of quantitative AI benchmarks within the complexities of real-world scenarios.

### AppVLM: A Lightweight Vision Language Model for Online App Control 
[[arxiv](https://arxiv.org/abs/2502.06395)] [[cool](https://papers.cool/arxiv/2502.06395)] [[pdf](https://arxiv.org/pdf/2502.06395)]
> **Authors**: Georgios Papoudakis,Thomas Coste,Zhihao Wu,Jianye Hao,Jun Wang,Kun Shao
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: The utilisation of foundation models as smartphone assistants, termed app agents, is a critical research challenge. These agents aim to execute human instructions on smartphones by interpreting textual instructions and performing actions via the device's interface. While promising, current approaches face significant limitations. Methods that use large proprietary models, such as GPT-4o, are computationally expensive, while those that use smaller fine-tuned models often lack adaptability to out-of-distribution tasks. In this work, we introduce AppVLM, a lightweight Vision-Language Model (VLM). First, we fine-tune it offline on the AndroidControl dataset. Then, we refine its policy by collecting data from the AndroidWorld environment and performing further training iterations. Our results indicate that AppVLM achieves the highest action prediction accuracy in offline evaluation on the AndroidControl dataset, compared to all evaluated baselines, and matches GPT-4o in online task completion success rate in the AndroidWorld environment, while being up to ten times faster. This makes AppVLM a practical and efficient solution for real-world deployment.

## 计算语言学(cs.CL:Computation and Language)

### Graph RAG-Tool Fusion 
[[arxiv](https://arxiv.org/abs/2502.07223)] [[cool](https://papers.cool/arxiv/2502.07223)] [[pdf](https://arxiv.org/pdf/2502.07223)]
> **Authors**: Elias Lumer,Pradeep Honaganahalli Basavaraju,Myles Mason,James A. Burke,Vamse Kumar Subbiah
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 25 pages, 14 figures, 2 tables
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Recent developments in retrieval-augmented generation (RAG) for selecting relevant tools from a tool knowledge base enable LLM agents to scale their complex tool calling capabilities to hundreds or thousands of external tools, APIs, or agents-as-tools. However, traditional RAG-based tool retrieval fails to capture structured dependencies between tools, limiting the retrieval accuracy of a retrieved tool's dependencies. For example, among a vector database of tools, a "get stock price" API requires a "stock ticker" parameter from a "get stock ticker" API, and both depend on OS-level internet connectivity tools. In this paper, we address this limitation by introducing Graph RAG-Tool Fusion, a novel plug-and-play approach that combines the strengths of vector-based retrieval with efficient graph traversal to capture all relevant tools (nodes) along with any nested dependencies (edges) within the predefined tool knowledge graph. We also present ToolLinkOS, a new tool selection benchmark of 573 fictional tools, spanning over 15 industries, each with an average of 6.3 tool dependencies. We demonstrate that Graph RAG-Tool Fusion achieves absolute improvements of 71.7% and 22.1% over naïve RAG on ToolLinkOS and ToolSandbox benchmarks, respectively (mAP@10). ToolLinkOS dataset is available at https://github.com/EliasLumer/Graph-RAG-Tool-Fusion-ToolLinkOS

### A Large-Scale Benchmark for Vietnamese Sentence Paraphrases 
[[arxiv](https://arxiv.org/abs/2502.07188)] [[cool](https://papers.cool/arxiv/2502.07188)] [[pdf](https://arxiv.org/pdf/2502.07188)]
> **Authors**: Sang Quang Nguyen,Kiet Van Nguyen
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Accepted in NAACL 2025 Findings
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: This paper presents ViSP, a high-quality Vietnamese dataset for sentence paraphrasing, consisting of 1.2M original-paraphrase pairs collected from various domains. The dataset was constructed using a hybrid approach that combines automatic paraphrase generation with manual evaluation to ensure high quality. We conducted experiments using methods such as back-translation, EDA, and baseline models like BART and T5, as well as large language models (LLMs), including GPT-4o, Gemini-1.5, Aya, Qwen-2.5, and Meta-Llama-3.1 variants. To the best of our knowledge, this is the first large-scale study on Vietnamese paraphrasing. We hope that our dataset and findings will serve as a valuable foundation for future research and applications in Vietnamese paraphrase tasks.

### Perceived Confidence Scoring for Data Annotation with Zero-Shot LLMs 
[[arxiv](https://arxiv.org/abs/2502.07186)] [[cool](https://papers.cool/arxiv/2502.07186)] [[pdf](https://arxiv.org/pdf/2502.07186)]
> **Authors**: Sina Salimian,Gias Uddin,Most Husne Jahan,Shaina Raza
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Zero-shot LLMs are now also used for textual classification tasks, e.g., sentiment/emotion detection of a given input as a sentence/article. However, their performance can be suboptimal in such data annotation tasks. We introduce a novel technique Perceived Confidence Scoring (PCS) that evaluates LLM's confidence for its classification of an input by leveraging Metamorphic Relations (MRs). The MRs generate semantically equivalent yet textually mutated versions of the input. Following the principles of Metamorphic Testing (MT), the mutated versions are expected to have annotation labels similar to the input. By analyzing the consistency of LLM responses across these variations, PCS computes a confidence score based on the frequency of predicted labels. PCS can be used both for single LLM and multiple LLM settings (e.g., majority voting). We introduce an algorithm Perceived Differential Evolution (PDE) that determines the optimal weights assigned to the MRs and the LLMs for a classification task. Empirical evaluation shows PCS significantly improves zero-shot accuracy for Llama-3-8B-Instruct (4.96%) and Mistral-7B-Instruct-v0.3 (10.52%), with Gemma-2-9b-it showing a 9.39% gain. When combining all three models, PCS significantly outperforms majority voting by 7.75%.

### Refine Knowledge of Large Language Models via Adaptive Contrastive Learning 
[[arxiv](https://arxiv.org/abs/2502.07184)] [[cool](https://papers.cool/arxiv/2502.07184)] [[pdf](https://arxiv.org/pdf/2502.07184)]
> **Authors**: Yinghui Li,Haojing Huang,Jiayi Kuang,Yangning Li,Shu-Yu Guo,Chao Qu,Xiaoyu Tan,Hai-Tao Zheng,Ying Shen,Philip S. Yu
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Accepted to ICLR 2025
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: How to alleviate the hallucinations of Large Language Models (LLMs) has always been the fundamental goal pursued by the LLMs research community. Looking through numerous hallucination-related studies, a mainstream category of methods is to reduce hallucinations by optimizing the knowledge representation of LLMs to change their output. Considering that the core focus of these works is the knowledge acquired by models, and knowledge has long been a central theme in human societal progress, we believe that the process of models refining knowledge can greatly benefit from the way humans learn. In our work, by imitating the human learning process, we design an Adaptive Contrastive Learning strategy. Our method flexibly constructs different positive and negative samples for contrastive learning based on LLMs' actual mastery of knowledge. This strategy helps LLMs consolidate the correct knowledge they already possess, deepen their understanding of the correct knowledge they have encountered but not fully grasped, forget the incorrect knowledge they previously learned, and honestly acknowledge the knowledge they lack. Extensive experiments and detailed analyses on widely used datasets demonstrate the effectiveness of our method.

### Don't Just Demo, Teach Me the Principles: A Principle-Based Multi-Agent Prompting Strategy for Text Classification 
[[arxiv](https://arxiv.org/abs/2502.07165)] [[cool](https://papers.cool/arxiv/2502.07165)] [[pdf](https://arxiv.org/pdf/2502.07165)]
> **Authors**: Peipei Wei,Dimitris Dimitriadis,Yan Xu,Mingwei Shen
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: To be published in AAAI 2025 Workshop on AdvancingLLM-Based Multi-Agent Collaboration
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: We present PRINCIPLE-BASED PROMPTING, a simple but effective multi-agent prompting strategy for text classification. It first asks multiple LLM agents to independently generate candidate principles based on analysis of demonstration samples with or without labels, consolidates them into final principles via a finalizer agent, and then sends them to a classifier agent to perform downstream classification tasks. Extensive experiments on binary and multi-class classification datasets with different sizes of LLMs show that our approach not only achieves substantial performance gains (1.55% - 19.37%) over zero-shot prompting on macro-F1 score but also outperforms other strong baselines (CoT and stepback prompting). Principles generated by our approach help LLMs perform better on classification tasks than human crafted principles on two private datasets. Our multi-agent PRINCIPLE-BASED PROMPTING approach also shows on-par or better performance compared to demonstration-based few-shot prompting approaches, yet with substantially lower inference costs. Ablation studies show that label information and the multi-agent cooperative LLM framework play an important role in generating high-quality principles to facilitate downstream classification tasks.

### Does Training on Synthetic Data Make Models Less Robust? 
[[arxiv](https://arxiv.org/abs/2502.07164)] [[cool](https://papers.cool/arxiv/2502.07164)] [[pdf](https://arxiv.org/pdf/2502.07164)]
> **Authors**: Lingze Zhang,Ellie Pavlick
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: An increasingly common practice is to train large language models (LLMs) using synthetic data. Often this synthetic data is produced by the same or similar LLMs as those it is being used to train. This raises the question of whether the synthetic data might in fact exacerbate certain "blindspots" by reinforcing heuristics that the LLM already encodes. In this paper, we conduct simulated experiments on the natural language inference (NLI) task with Llama-2-7B-hf models. We use MultiNLI as the general task and HANS, a targeted evaluation set designed to measure the presence of specific heuristic strategies for NLI, as our "blindspot" task. Our goal is to determine whether performance disparities between the general and blind spot tasks emerge. Our results indicate that synthetic data does not reinforce blindspots in the way we expected. Specifically, we see that, while fine-tuning with synthetic data doesn't necessarily reduce the use of the heuristic, it also does not make it worse as we hypothesized.

### Ask Patients with Patience: Enabling LLMs for Human-Centric Medical Dialogue with Grounded Reasoning 
[[arxiv](https://arxiv.org/abs/2502.07143)] [[cool](https://papers.cool/arxiv/2502.07143)] [[pdf](https://arxiv.org/pdf/2502.07143)]
> **Authors**: Jiayuan Zhu,Junde Wu
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Accurate and efficient diagnosis in online medical consultations remains a challenge for current large language models. These models often rely on single-turn interactions and lack the ability to refine their predictions through follow-up questions. Additionally, their responses frequently contain complex medical terminology, making them less accessible to non-medical users and creating barriers to effective communication. In this paper, we introduce Ask Patients with Patience (APP), the first multi-turn dialogue that enables LLMs to iteratively refine diagnoses based on grounded reasoning. By integrating medical guidelines and entropy minimization, APP improves both diagnostic accuracy and efficiency. Furthermore, it features human-centric communication that bridges the gap between user comprehension and medical terminology, significantly enhancing user accessibility and engagement. We evaluated APP using a subset of the ReMeDi dataset, comparing it with single-turn and traditional multi-turn LLM baselines. APP achieved higher similarity scores in diagnosis predictions, demonstrating better alignment with ground truth diagnoses. Entropy analysis showed that APP reduces diagnostic uncertainty more rapidly across iterations, increasing confidence in its predictions. APP also excels in user accessibility and empathy, further bridging the gap between complex medical language and user understanding. Code will be released at: https://github.com/SuperMedIntel/AskPatients.

### Language-TPP: Integrating Temporal Point Processes with Language Models for Event Analysis 
[[arxiv](https://arxiv.org/abs/2502.07139)] [[cool](https://papers.cool/arxiv/2502.07139)] [[pdf](https://arxiv.org/pdf/2502.07139)]
> **Authors**: Quyu Kong,Yixuan Zhang,Yang Liu,Panrong Tong,Enqi Liu,Feng Zhou
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Temporal Point Processes (TPPs) have been widely used for event sequence modeling, but they often struggle to incorporate rich textual event descriptions effectively. Conversely, while Large Language Models (LLMs) have been shown remarkable capabilities in processing textual data, they lack mechanisms for handling temporal dynamics. To bridge this gap, we introduce Language-TPP, a unified framework that integrates TPPs with LLMs for enhanced event sequence modeling. Language-TPP introduces a novel temporal encoding mechanism that converts continuous time intervals into specialized byte-tokens, enabling seamless integration with standard LLM architectures. This approach allows Language-TPP to achieve state-of-the-art performance across multiple TPP tasks, including event time prediction, type prediction, and intensity estimation, on five datasets. Additionally, we demonstrate that incorporating temporal information significantly improves the quality of generated event descriptions.

### TWICE: What Advantages Can Low-Resource Domain-Specific Embedding Model Bring? -- A Case Study on Korea Financial Texts 
[[arxiv](https://arxiv.org/abs/2502.07131)] [[cool](https://papers.cool/arxiv/2502.07131)] [[pdf](https://arxiv.org/pdf/2502.07131)]
> **Authors**: Yewon Hwang,Sungbum Jung,Hanwool Lee,Sara Yu
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Submitted to ICLR@FinancialAI
- **标题**: None
- **领域**: 计算语言学,计算金融
- **Abstract**: Domain specificity of embedding models is critical for effective performance. However, existing benchmarks, such as FinMTEB, are primarily designed for high-resource languages, leaving low-resource settings, such as Korean, under-explored. Directly translating established English benchmarks often fails to capture the linguistic and cultural nuances present in low-resource domains. In this paper, titled TWICE: What Advantages Can Low-Resource Domain-Specific Embedding Models Bring? A Case Study on Korea Financial Texts, we introduce KorFinMTEB, a novel benchmark for the Korean financial domain, specifically tailored to reflect its unique cultural characteristics in low-resource languages. Our experimental results reveal that while the models perform robustly on a translated version of FinMTEB, their performance on KorFinMTEB uncovers subtle yet critical discrepancies, especially in tasks requiring deeper semantic understanding, that underscore the limitations of direct translation. This discrepancy highlights the necessity of benchmarks that incorporate language-specific idiosyncrasies and cultural nuances. The insights from our study advocate for the development of domain-specific evaluation frameworks that can more accurately assess and drive the progress of embedding models in low-resource settings.

### Cardiverse: Harnessing LLMs for Novel Card Game Prototyping 
[[arxiv](https://arxiv.org/abs/2502.07128)] [[cool](https://papers.cool/arxiv/2502.07128)] [[pdf](https://arxiv.org/pdf/2502.07128)]
> **Authors**: Danrui Li,Sen Zhang,Sam S. Sohn,Kaidong Hu,Muhammad Usman,Mubbasir Kapadia
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 13 pages, 7 figures, 3 tables
- **标题**: None
- **领域**: 计算语言学,人工智能,多媒体
- **Abstract**: The prototyping of computer games, particularly card games, requires extensive human effort in creative ideation and gameplay evaluation. Recent advances in Large Language Models (LLMs) offer opportunities to automate and streamline these processes. However, it remains challenging for LLMs to design novel game mechanics beyond existing databases, generate consistent gameplay environments, and develop scalable gameplay AI for large-scale evaluations. This paper addresses these challenges by introducing a comprehensive automated card game prototyping framework. The approach highlights a graph-based indexing method for generating novel game designs, an LLM-driven system for consistent game code generation validated by gameplay records, and a gameplay AI constructing method that uses an ensemble of LLM-generated action-value functions optimized through self-play. These contributions aim to accelerate card game prototyping, reduce human labor, and lower barriers to entry for game developers.

### Structural Reformation of Large Language Model Neuron Encapsulation for Divergent Information Aggregation 
[[arxiv](https://arxiv.org/abs/2502.07124)] [[cool](https://papers.cool/arxiv/2502.07124)] [[pdf](https://arxiv.org/pdf/2502.07124)]
> **Authors**: Denis Bakushev,Gideon Boultinghouse,Harriet Oppenheimer,Sebastian Gillingwater,Valentina Ashington,Wilfred Stanborough
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Structured neuron encapsulation introduces a modular framework that enables more effective aggregation and specialization of information within deep learning architectures. A model modified through this framework demonstrated improved perplexity scores, greater lexical variability, and enhanced consistency in logical reasoning, suggesting that structured parameter distribution contributes to more efficient language representation. Statistical analyses of generated text highlighted a wider range of sentence structures and reduced redundancy in token selection, indicating that encapsulation fosters more adaptable language generation. A detailed evaluation of attention weight distributions revealed that the experimental model exhibited greater divergence in cross-layer activations, supporting the hypothesis that encapsulated neurons assume specialized processing roles. Logical consistency assessments further demonstrated that modular architectures mitigate contradictory outputs, reducing internal conflicts in inferred relationships between linguistic constructs. Computational trade-offs were analyzed, with results showing a minor increase in processing overhead, though improvements in parameter efficiency and structured decision-making compensated for the additional complexity. The mathematical formulation of the encapsulation mechanism confirmed that modular aggregation maintains stable convergence properties while promoting distinct functional roles for different neuron clusters.

### Multi-turn Evaluation of Anthropomorphic Behaviours in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.07077)] [[cool](https://papers.cool/arxiv/2502.07077)] [[pdf](https://arxiv.org/pdf/2502.07077)]
> **Authors**: Lujain Ibrahim,Canfer Akbulut,Rasmi Elasmar,Charvi Rastogi,Minsuk Kahng,Meredith Ringel Morris,Kevin R. McKee,Verena Rieser,Murray Shanahan,Laura Weidinger
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,计算机与社会,人机交互
- **Abstract**: The tendency of users to anthropomorphise large language models (LLMs) is of growing interest to AI developers, researchers, and policy-makers. Here, we present a novel method for empirically evaluating anthropomorphic LLM behaviours in realistic and varied settings. Going beyond single-turn static benchmarks, we contribute three methodological advances in state-of-the-art (SOTA) LLM evaluation. First, we develop a multi-turn evaluation of 14 anthropomorphic behaviours. Second, we present a scalable, automated approach by employing simulations of user interactions. Third, we conduct an interactive, large-scale human subject study (N=1101) to validate that the model behaviours we measure predict real users' anthropomorphic perceptions. We find that all SOTA LLMs evaluated exhibit similar behaviours, characterised by relationship-building (e.g., empathy and validation) and first-person pronoun use, and that the majority of behaviours only first occur after multiple turns. Our work lays an empirical foundation for investigating how design choices influence anthropomorphic model behaviours and for progressing the ethical debate on the desirability of these behaviours. It also showcases the necessity of multi-turn evaluations for complex social phenomena in human-AI interaction.

### IRepair: An Intent-Aware Approach to Repair Data-Driven Errors in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.07072)] [[cool](https://papers.cool/arxiv/2502.07072)] [[pdf](https://arxiv.org/pdf/2502.07072)]
> **Authors**: Sayem Mohammad Imtiaz,Astha Singh,Fraol Batole,Hridesh Rajan
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Accepted as full research paper at FSE'2025
- **标题**: None
- **领域**: 计算语言学,人工智能,软件工程
- **Abstract**: Not a day goes by without hearing about the impressive feats of large language models (LLMs), and equally, not a day passes without hearing about their challenges. LLMs are notoriously vulnerable to biases in their dataset, leading to issues such as toxicity. While domain-adaptive training has been employed to mitigate these issues, these techniques often address all model parameters indiscriminately during the repair process, resulting in poor repair quality and reduced model versatility. In this paper, we introduce a novel dynamic slicing-based intent-aware LLM repair strategy, IRepair. This approach selectively targets the most error-prone sections of the model for repair. Specifically, we propose dynamically slicing the model's most sensitive layers that require immediate attention, concentrating repair efforts on those areas. This method enables more effective repairs with potentially less impact on the model's overall performance by altering a smaller portion of the model. We evaluated our technique on three models from the GPT2 and GPT-Neo families, with parameters ranging from 800M to 1.6B, in a toxicity mitigation setup. Our results show that IRepair repairs errors 43.6% more effectively while causing 46% less disruption to general performance compared to the closest baseline, direct preference optimization. Our empirical analysis also reveals that errors are more concentrated in a smaller section of the model, with the top 20% of layers exhibiting 773% more error density than the remaining 80\%. This highlights the need for selective repair. Additionally, we demonstrate that a dynamic selection approach is essential for addressing errors dispersed throughout the model, ensuring a robust and efficient repair.

### Specializing Large Language Models to Simulate Survey Response Distributions for Global Populations 
[[arxiv](https://arxiv.org/abs/2502.07068)] [[cool](https://papers.cool/arxiv/2502.07068)] [[pdf](https://arxiv.org/pdf/2502.07068)]
> **Authors**: Yong Cao,Haijiang Liu,Arnav Arora,Isabelle Augenstein,Paul Röttger,Daniel Hershcovich
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 15 pages, 9 figures, accepted to NAACL 2025 main
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large-scale surveys are essential tools for informing social science research and policy, but running surveys is costly and time-intensive. If we could accurately simulate group-level survey results, this would therefore be very valuable to social science research. Prior work has explored the use of large language models (LLMs) for simulating human behaviors, mostly through prompting. In this paper, we are the first to specialize LLMs for the task of simulating survey response distributions. As a testbed, we use country-level results from two global cultural surveys. We devise a fine-tuning method based on first-token probabilities to minimize divergence between predicted and actual response distributions for a given question. Then, we show that this method substantially outperforms other methods and zero-shot classifiers, even on unseen questions, countries, and a completely unseen survey. While even our best models struggle with the task, especially on unseen questions, our results demonstrate the benefits of specialization for simulation, which may accelerate progress towards sufficiently accurate simulation in the future.

### Using Contextually Aligned Online Reviews to Measure LLMs' Performance Disparities Across Language Varieties 
[[arxiv](https://arxiv.org/abs/2502.07058)] [[cool](https://papers.cool/arxiv/2502.07058)] [[pdf](https://arxiv.org/pdf/2502.07058)]
> **Authors**: Zixin Tang,Chieh-Yang Huang,Tsung-Chi Li,Ho Yin Sam Ng,Hen-Hsen Huang,Ting-Hao 'Kenneth' Huang
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Accepted by 2025 Annual Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics (NAACL), theme track
- **标题**: None
- **领域**: 计算语言学,人机交互
- **Abstract**: A language can have different varieties. These varieties can affect the performance of natural language processing (NLP) models, including large language models (LLMs), which are often trained on data from widely spoken varieties. This paper introduces a novel and cost-effective approach to benchmark model performance across language varieties. We argue that international online review platforms, such as Booking.com, can serve as effective data sources for constructing datasets that capture comments in different language varieties from similar real-world scenarios, like reviews for the same hotel with the same rating using the same language (e.g., Mandarin Chinese) but different language varieties (e.g., Taiwan Mandarin, Mainland Mandarin). To prove this concept, we constructed a contextually aligned dataset comprising reviews in Taiwan Mandarin and Mainland Mandarin and tested six LLMs in a sentiment analysis task. Our results show that LLMs consistently underperform in Taiwan Mandarin.

### Tokenization Standards for Linguistic Integrity: Turkish as a Benchmark 
[[arxiv](https://arxiv.org/abs/2502.07057)] [[cool](https://papers.cool/arxiv/2502.07057)] [[pdf](https://arxiv.org/pdf/2502.07057)]
> **Authors**: M. Ali Bayram,Ali Arda Fincan,Ahmet Semih Gümüş,Sercan Karakaş,Banu Diri,Savaş Yıldırım
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: :68T50; 68T10ACM Class:I.2.7; I.2.6; H.3.1
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Tokenization is a fundamental preprocessing step in NLP, directly impacting large language models' (LLMs) ability to capture syntactic, morphosyntactic, and semantic structures. This paper introduces a novel framework for systematically evaluating tokenization strategies, addressing challenges in morphologically rich and low-resource languages. Using a Turkish dataset of 6,200 multiple-choice questions from the Massive Multitask Language Understanding (MMLU) benchmark, the framework assesses tokenizers across five key metrics: vocabulary size, token count, processing time, language-specific token percentages (\%TR), and token purity. These metrics provide a structured approach to evaluating how well tokenizers preserve linguistic structures. While \%TR measures the proportion of valid words in the target language, \%Pure assesses the alignment of tokens with meaningful linguistic units, such as roots and valid morphemes, minimizing semantic fragmentation. The findings reveal that \%TR, introduced as a critical metric, exhibits a stronger correlation with downstream performance (e.g., MMLU scores) than token purity, emphasizing its role in improving model accuracy. Additionally, larger model parameters do not necessarily yield better tokenization quality or enhanced results, highlighting the importance of tailored tokenization strategies that prioritize linguistic alignment. This framework sets a new standard for developing robust tokenization methods optimized for morphologically complex and low-resource languages. Future work will refine morphological analysis, explore domain-specific customizations, and conduct cross-linguistic evaluations to further enhance tokenization practices.

### Leveraging Allophony in Self-Supervised Speech Models for Atypical Pronunciation Assessment 
[[arxiv](https://arxiv.org/abs/2502.07029)] [[cool](https://papers.cool/arxiv/2502.07029)] [[pdf](https://arxiv.org/pdf/2502.07029)]
> **Authors**: Kwanghee Choi,Eunjung Yeo,Kalvin Chang,Shinji Watanabe,David Mortensen
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Accepted to NAACL 2025. Codebase available at https://github.com/juice500ml/acoustic-units-for-ood
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习,音频和语音处理
- **Abstract**: Allophony refers to the variation in the phonetic realization of a phoneme based on its phonetic environment. Modeling allophones is crucial for atypical pronunciation assessment, which involves distinguishing atypical from typical pronunciations. However, recent phoneme classifier-based approaches often simplify this by treating various realizations as a single phoneme, bypassing the complexity of modeling allophonic variation. Motivated by the acoustic modeling capabilities of frozen self-supervised speech model (S3M) features, we propose MixGoP, a novel approach that leverages Gaussian mixture models to model phoneme distributions with multiple subclusters. Our experiments show that MixGoP achieves state-of-the-art performance across four out of five datasets, including dysarthric and non-native speech. Our analysis further suggests that S3M features capture allophonic variation more effectively than MFCCs and Mel spectrograms, highlighting the benefits of integrating MixGoP with S3M features.

### AIMS.au: A Dataset for the Analysis of Modern Slavery Countermeasures in Corporate Statements 
[[arxiv](https://arxiv.org/abs/2502.07022)] [[cool](https://papers.cool/arxiv/2502.07022)] [[pdf](https://arxiv.org/pdf/2502.07022)]
> **Authors**: Adriana Eufrosiana Bora,Pierre-Luc St-Charles,Mirko Bronzi,Arsène Fansi Tchango,Bruno Rousseau,Kerrie Mengersen
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Camera ready. ICLR 2025
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Despite over a decade of legislative efforts to address modern slavery in the supply chains of large corporations, the effectiveness of government oversight remains hampered by the challenge of scrutinizing thousands of statements annually. While Large Language Models (LLMs) can be considered a well established solution for the automatic analysis and summarization of documents, recognizing concrete modern slavery countermeasures taken by companies and differentiating those from vague claims remains a challenging task. To help evaluate and fine-tune LLMs for the assessment of corporate statements, we introduce a dataset composed of 5,731 modern slavery statements taken from the Australian Modern Slavery Register and annotated at the sentence level. This paper details the construction steps for the dataset that include the careful design of annotation specifications, the selection and preprocessing of statements, and the creation of high-quality annotation subsets for effective model evaluations. To demonstrate our dataset's utility, we propose a machine learning methodology for the detection of sentences relevant to mandatory reporting requirements set by the Australian Modern Slavery Act. We then follow this methodology to benchmark modern language models under zero-shot and supervised learning settings.

### Finding Words Associated with DIF: Predicting Differential Item Functioning using LLMs and Explainable AI 
[[arxiv](https://arxiv.org/abs/2502.07017)] [[cool](https://papers.cool/arxiv/2502.07017)] [[pdf](https://arxiv.org/pdf/2502.07017)]
> **Authors**: Hotaka Maeda,Yikai Lu
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 14 pages, 2 figures, 6 tables
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: We fine-tuned and compared several encoder-based Transformer large language models (LLM) to predict differential item functioning (DIF) from the item text. We then applied explainable artificial intelligence (XAI) methods to these models to identify specific words associated with DIF. The data included 42,180 items designed for English language arts and mathematics summative state assessments among students in grades 3 to 11. Prediction $R^2$ ranged from .04 to .32 among eight focal and reference group pairs. Our findings suggest that many words associated with DIF reflect minor sub-domains included in the test blueprint by design, rather than construct-irrelevant item content that should be removed from assessments. This may explain why qualitative reviews of DIF items often yield confusing or inconclusive results. Our approach can be used to screen words associated with DIF during the item-writing process for immediate revision, or help review traditional DIF analysis results by highlighting key words in the text. Extensions of this research can enhance the fairness of assessment programs, especially those that lack resources to build high-quality items, and among smaller subpopulations where we do not have sufficient sample sizes for traditional DIF analyses.

### Demystifying Singular Defects in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.07004)] [[cool](https://papers.cool/arxiv/2502.07004)] [[pdf](https://arxiv.org/pdf/2502.07004)]
> **Authors**: Haoqi Wang,Tong Zhang,Mathieu Salzmann
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large transformer models are known to produce high-norm tokens. In vision transformers (ViTs), such tokens have been mathematically modeled through the singular vectors of the linear approximations of layers. However, in large language models (LLMs), the underlying causes of high-norm tokens remain largely unexplored, and their different properties from those of ViTs require a new analysis framework. In this paper, we provide both theoretical insights and empirical validation across a range of recent models, leading to the following observations: i) The layer-wise singular direction predicts the abrupt explosion of token norms in LLMs. ii) The negative eigenvalues of a layer explain its sudden decay. iii) The computational pathways leading to high-norm tokens differ between initial and noninitial tokens. iv) High-norm tokens are triggered by the right leading singular vector of the matrix approximating the corresponding modules. We showcase two practical applications of these findings: the improvement of quantization schemes and the design of LLM signatures. Our findings not only advance the understanding of singular defects in LLMs but also open new avenues for their application. We expect that this work will stimulate further research into the internal mechanisms of LLMs and will therefore publicly release our code.

### Investigating the Zone of Proximal Development of Language Models for In-Context Learning 
[[arxiv](https://arxiv.org/abs/2502.06990)] [[cool](https://papers.cool/arxiv/2502.06990)] [[pdf](https://arxiv.org/pdf/2502.06990)]
> **Authors**: Peng Cui,Mrinmaya Sachan
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: NAACL 2025 findings
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: In this paper, we introduce a learning analytics framework to analyze the in-context learning (ICL) behavior of large language models (LLMs) through the lens of the Zone of Proximal Development (ZPD), an established theory in educational psychology. ZPD delineates the space between what a learner is capable of doing unsupported and what the learner cannot do even with support. We adapt this concept to ICL, measuring the ZPD of LLMs based on model performance on individual examples with and without ICL. Furthermore, we propose an item response theory (IRT) model to predict the distribution of zones for LLMs. Our findings reveal a series of intricate and multifaceted behaviors of ICL, providing new insights into understanding and leveraging this technique. Finally, we demonstrate how our framework can enhance LLM in both inference and fine-tuning scenarios: (1) By predicting a model's zone of proximal development, we selectively apply ICL to queries that are most likely to benefit from demonstrations, achieving a better balance between inference cost and performance; (2) We propose a human-like curriculum for fine-tuning, which prioritizes examples within the model's ZPD. The curriculum results in improved performance, and we explain its effectiveness through an analysis of the training dynamics of LLMs.

### Multi-Agent Simulator Drives Language Models for Legal Intensive Interaction 
[[arxiv](https://arxiv.org/abs/2502.06882)] [[cool](https://papers.cool/arxiv/2502.06882)] [[pdf](https://arxiv.org/pdf/2502.06882)]
> **Authors**: Shengbin Yue,Ting Huang,Zheng Jia,Siyuan Wang,Shujun Liu,Yun Song,Xuanjing Huang,Zhongyu Wei
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-11
> **comment**: Accepted by NAACL 2025
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large Language Models (LLMs) have significantly advanced legal intelligence, but the scarcity of scenario data impedes the progress toward interactive legal scenarios. This paper introduces a Multi-agent Legal Simulation Driver (MASER) to scalably generate synthetic data by simulating interactive legal scenarios. Leveraging real-legal case sources, MASER ensures the consistency of legal attributes between participants and introduces a supervisory mechanism to align participants' characters and behaviors as well as addressing distractions. A Multi-stage Interactive Legal Evaluation (MILE) benchmark is further constructed to evaluate LLMs' performance in dynamic legal scenarios. Extensive experiments confirm the effectiveness of our framework.

### Mix Data or Merge Models? Balancing the Helpfulness, Honesty, and Harmlessness of Large Language Model via Model Merging 
[[arxiv](https://arxiv.org/abs/2502.06876)] [[cool](https://papers.cool/arxiv/2502.06876)] [[pdf](https://arxiv.org/pdf/2502.06876)]
> **Authors**: Jinluan Yang,Dingnan Jin,Anke Tang,Li Shen,Didi Zhu,Zhengyu Chen,Daixin Wang,Qing Cui,Zhiqiang Zhang,Jun Zhou,Fei Wu,Kun Kuang
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Achieving balanced alignment of large language models (LLMs) in terms of Helpfulness, Honesty, and Harmlessness (3H optimization) constitutes a cornerstone of responsible AI, with existing methods like data mixture strategies facing limitations including reliance on expert knowledge and conflicting optimization signals. While model merging offers a promising alternative by integrating specialized models, its potential for 3H optimization remains underexplored. This paper establishes the first comprehensive benchmark for model merging in 3H-aligned LLMs, systematically evaluating 15 methods (12 training-free merging and 3 data mixture techniques) across 10 datasets associated with 5 annotation dimensions, 2 LLM families, and 2 training paradigms. Our analysis reveals three pivotal insights: (i) previously overlooked collaborative/conflicting relationships among 3H dimensions, (ii) the consistent superiority of model merging over data mixture approaches in balancing alignment trade-offs, and (iii) the critical role of parameter-level conflict resolution through redundant component pruning and outlier mitigation. Building on these findings, we propose R-TSVM, a Reweighting-enhanced Task Singular Vector Merging method that incorporates outlier-aware parameter weighting and sparsity-adaptive rank selection strategies adapted to the heavy-tailed parameter distribution and sparsity for LLMs, further improving LLM alignment across multiple evaluations. We release our trained models for further exploration.

### Group Reasoning Emission Estimation Networks 
[[arxiv](https://arxiv.org/abs/2502.06874)] [[cool](https://papers.cool/arxiv/2502.06874)] [[pdf](https://arxiv.org/pdf/2502.06874)]
> **Authors**: Yanming Guo,Xiao Qian,Kevin Credit,Jin Ma
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Accurate greenhouse gas (GHG) emission reporting is critical for governments, businesses, and investors. However, adoption remains limited particularly among small and medium enterprises due to high implementation costs, fragmented emission factor databases, and a lack of robust sector classification methods. To address these challenges, we introduce Group Reasoning Emission Estimation Networks (GREEN), an AI-driven carbon accounting framework that standardizes enterprise-level emission estimation, constructs a large-scale benchmark dataset, and leverages a novel reasoning approach with large language models (LLMs). Specifically, we compile textual descriptions for 20,850 companies with validated North American Industry Classification System (NAICS) labels and align these with an economic model of carbon intensity factors. By reframing sector classification as an information retrieval task, we fine-tune Sentence-BERT models using a contrastive learning loss. To overcome the limitations of single-stage models in handling thousands of hierarchical categories, we propose a Group Reasoning method that ensembles LLM classifiers based on the natural NAICS ontology, decomposing the task into multiple sub-classification steps. We theoretically prove that this approach reduces classification uncertainty and computational complexity. Experiments on 1,114 NAICS categories yield state-of-the-art performance (83.68% Top-1, 91.47% Top-10 accuracy), and case studies on 20 companies report a mean absolute percentage error (MAPE) of 45.88%. The project is available at: https://huggingface.co/datasets/Yvnminc/ExioNAICS.

### Multimodal Cognitive Reframing Therapy via Multi-hop Psychotherapeutic Reasoning 
[[arxiv](https://arxiv.org/abs/2502.06873)] [[cool](https://papers.cool/arxiv/2502.06873)] [[pdf](https://arxiv.org/pdf/2502.06873)]
> **Authors**: Subin Kim,Hoonrae Kim,Heejin Do,Gary Geunbae Lee
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-11
> **comment**: NAACL 2025 Main
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Previous research has revealed the potential of large language models (LLMs) to support cognitive reframing therapy; however, their focus was primarily on text-based methods, often overlooking the importance of non-verbal evidence crucial in real-life therapy. To alleviate this gap, we extend the textual cognitive reframing to multimodality, incorporating visual clues. Specifically, we present a new dataset called Multi Modal-Cognitive Support Conversation (M2CoSC), which pairs each GPT-4-generated dialogue with an image that reflects the virtual client's facial expressions. To better mirror real psychotherapy, where facial expressions lead to interpreting implicit emotional evidence, we propose a multi-hop psychotherapeutic reasoning approach that explicitly identifies and incorporates subtle evidence. Our comprehensive experiments with both LLMs and vision-language models (VLMs) demonstrate that the VLMs' performance as psychotherapists is significantly improved with the M2CoSC dataset. Furthermore, the multi-hop psychotherapeutic reasoning method enables VLMs to provide more thoughtful and empathetic suggestions, outperforming standard prompting methods.

### Towards Trustworthy Retrieval Augmented Generation for Large Language Models: A Survey 
[[arxiv](https://arxiv.org/abs/2502.06872)] [[cool](https://papers.cool/arxiv/2502.06872)] [[pdf](https://arxiv.org/pdf/2502.06872)]
> **Authors**: Bo Ni,Zheyuan Liu,Leyao Wang,Yongjia Lei,Yuying Zhao,Xueqi Cheng,Qingkai Zeng,Luna Dong,Yinglong Xia,Krishnaram Kenthapadi,Ryan Rossi,Franck Dernoncourt,Md Mehrab Tanjim,Nesreen Ahmed,Xiaorui Liu,Wenqi Fan,Erik Blasch,Yu Wang,Meng Jiang,Tyler Derr
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Retrieval-Augmented Generation (RAG) is an advanced technique designed to address the challenges of Artificial Intelligence-Generated Content (AIGC). By integrating context retrieval into content generation, RAG provides reliable and up-to-date external knowledge, reduces hallucinations, and ensures relevant context across a wide range of tasks. However, despite RAG's success and potential, recent studies have shown that the RAG paradigm also introduces new risks, including robustness issues, privacy concerns, adversarial attacks, and accountability issues. Addressing these risks is critical for future applications of RAG systems, as they directly impact their trustworthiness. Although various methods have been developed to improve the trustworthiness of RAG methods, there is a lack of a unified perspective and framework for research in this topic. Thus, in this paper, we aim to address this gap by providing a comprehensive roadmap for developing trustworthy RAG systems. We place our discussion around five key perspectives: reliability, privacy, safety, fairness, explainability, and accountability. For each perspective, we present a general framework and taxonomy, offering a structured approach to understanding the current challenges, evaluating existing solutions, and identifying promising future research directions. To encourage broader adoption and innovation, we also highlight the downstream applications where trustworthy RAG systems have a significant impact.

### Related Knowledge Perturbation Matters: Rethinking Multiple Pieces of Knowledge Editing in Same-Subject 
[[arxiv](https://arxiv.org/abs/2502.06868)] [[cool](https://papers.cool/arxiv/2502.06868)] [[pdf](https://arxiv.org/pdf/2502.06868)]
> **Authors**: Zenghao Duan,Wenbin Duan,Zhiyi Yin,Yinghan Shen,Shaoling Jing,Jie Zhang,Huawei Shen,Xueqi Cheng
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-11
> **comment**: Accepted by NAACL 2025
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Knowledge editing has become a promising approach for efficiently and precisely updating knowledge embedded in large language models (LLMs). In this work, we focus on Same-Subject Editing, which involves modifying multiple attributes of a single entity to ensure comprehensive and consistent updates to entity-centric knowledge. Through preliminary observation, we identify a significant challenge: Current state-of-the-art editing methods struggle when tasked with editing multiple related knowledge pieces for the same subject. To address the lack of relevant editing data for identical subjects in traditional benchmarks, we introduce the $\text{S}^2\text{RKE}$(Same-Subject Related Knowledge Editing) benchmark. Our extensive experiments reveal that only mainstream locate-then-edit methods, such as ROME and MEMIT, exhibit "related knowledge perturbation," where subsequent edits interfere with earlier ones. Further analysis reveals that these methods over-rely on subject information, neglecting other critical factors, resulting in reduced editing effectiveness.

### Forbidden Science: Dual-Use AI Challenge Benchmark and Scientific Refusal Tests 
[[arxiv](https://arxiv.org/abs/2502.06867)] [[cool](https://papers.cool/arxiv/2502.06867)] [[pdf](https://arxiv.org/pdf/2502.06867)]
> **Authors**: David Noever,Forrest McKee
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: The development of robust safety benchmarks for large language models requires open, reproducible datasets that can measure both appropriate refusal of harmful content and potential over-restriction of legitimate scientific discourse. We present an open-source dataset and testing framework for evaluating LLM safety mechanisms across mainly controlled substance queries, analyzing four major models' responses to systematically varied prompts. Our results reveal distinct safety profiles: Claude-3.5-sonnet demonstrated the most conservative approach with 73% refusals and 27% allowances, while Mistral attempted to answer 100% of queries. GPT-3.5-turbo showed moderate restriction with 10% refusals and 90% allowances, and Grok-2 registered 20% refusals and 80% allowances. Testing prompt variation strategies revealed decreasing response consistency, from 85% with single prompts to 65% with five variations. This publicly available benchmark enables systematic evaluation of the critical balance between necessary safety restrictions and potential over-censorship of legitimate scientific inquiry, while providing a foundation for measuring progress in AI safety implementation. Chain-of-thought analysis reveals potential vulnerabilities in safety mechanisms, highlighting the complexity of implementing robust safeguards without unduly restricting desirable and valid scientific discourse.

### Knowledge Graph-Guided Retrieval Augmented Generation 
[[arxiv](https://arxiv.org/abs/2502.06864)] [[cool](https://papers.cool/arxiv/2502.06864)] [[pdf](https://arxiv.org/pdf/2502.06864)]
> **Authors**: Xiangrong Zhu,Yuexiang Xie,Yi Liu,Yaliang Li,Wei Hu
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-11
> **comment**: Accepted in the 2025 Annual Conference of the Nations of the Americas Chapter of the ACL (NAACL 2025)
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Retrieval-augmented generation (RAG) has emerged as a promising technology for addressing hallucination issues in the responses generated by large language models (LLMs). Existing studies on RAG primarily focus on applying semantic-based approaches to retrieve isolated relevant chunks, which ignore their intrinsic relationships. In this paper, we propose a novel Knowledge Graph-Guided Retrieval Augmented Generation (KG$^2$RAG) framework that utilizes knowledge graphs (KGs) to provide fact-level relationships between chunks, improving the diversity and coherence of the retrieved results. Specifically, after performing a semantic-based retrieval to provide seed chunks, KG$^2$RAG employs a KG-guided chunk expansion process and a KG-based chunk organization process to deliver relevant and important knowledge in well-organized paragraphs. Extensive experiments conducted on the HotpotQA dataset and its variants demonstrate the advantages of KG$^2$RAG compared to existing RAG-based approaches, in terms of both response quality and retrieval quality.

### LLM-Supported Natural Language to Bash Translation 
[[arxiv](https://arxiv.org/abs/2502.06858)] [[cool](https://papers.cool/arxiv/2502.06858)] [[pdf](https://arxiv.org/pdf/2502.06858)]
> **Authors**: Finnian Westenfelder,Erik Hemberg,Miguel Tulla,Stephen Moskal,Una-May O'Reilly,Silviu Chiricescu
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-11
> **comment**: 13 pages, NAACL 2025
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: The Bourne-Again Shell (Bash) command-line interface for Linux systems has complex syntax and requires extensive specialized knowledge. Using the natural language to Bash command (NL2SH) translation capabilities of large language models (LLMs) for command composition circumvents these issues. However, the NL2SH performance of LLMs is difficult to assess due to inaccurate test data and unreliable heuristics for determining the functional equivalence of Bash commands. We present a manually verified test dataset of 600 instruction-command pairs and a training dataset of 40,939 pairs, increasing the size of previous datasets by 441% and 135%, respectively. Further, we present a novel functional equivalence heuristic that combines command execution with LLM evaluation of command outputs. Our heuristic can determine the functional equivalence of two Bash commands with 95% confidence, a 16% increase over previous heuristics. Evaluation of popular LLMs using our test dataset and heuristic demonstrates that parsing, in-context learning, in-weight learning, and constrained decoding can improve NL2SH accuracy by up to 32%. Our findings emphasize the importance of dataset quality, execution-based evaluation and translation method for advancing NL2SH translation. Our code is available at https://github.com/westenfelder/NL2SH

### Self-Supervised Prompt Optimization 
[[arxiv](https://arxiv.org/abs/2502.06855)] [[cool](https://papers.cool/arxiv/2502.06855)] [[pdf](https://arxiv.org/pdf/2502.06855)]
> **Authors**: Jinyu Xiang,Jiayi Zhang,Zhaoyang Yu,Fengwei Teng,Jinhao Tu,Xinbing Liang,Sirui Hong,Chenglin Wu,Yuyu Luo
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Well-designed prompts are crucial for enhancing Large language models' (LLMs) reasoning capabilities while aligning their outputs with task requirements across diverse domains. However, manually designed prompts require expertise and iterative experimentation. While existing prompt optimization methods aim to automate this process, they rely heavily on external references such as ground truth or by humans, limiting their applicability in real-world scenarios where such data is unavailable or costly to obtain. To address this, we propose Self-Supervised Prompt Optimization (SPO), a cost-efficient framework that discovers effective prompts for both closed and open-ended tasks without requiring external reference. Motivated by the observations that prompt quality manifests directly in LLM outputs and LLMs can effectively assess adherence to task requirements, we derive evaluation and optimization signals purely from output comparisons. Specifically, SPO selects superior prompts through pairwise output comparisons evaluated by an LLM evaluator, followed by an LLM optimizer that aligns outputs with task requirements. Extensive experiments demonstrate that SPO outperforms state-of-the-art prompt optimization methods, achieving comparable or superior results with significantly lower costs (e.g., 1.1% to 5.6% of existing methods) and fewer samples (e.g., three samples). The code is available at https://github.com/geekan/MetaGPT/blob/main/examples/spo

### Survey on Vision-Language-Action Models 
[[arxiv](https://arxiv.org/abs/2502.06851)] [[cool](https://papers.cool/arxiv/2502.06851)] [[pdf](https://arxiv.org/pdf/2502.06851)]
> **Authors**: Adilzhan Adilkhanov,Amir Yelenov,Assylkhan Seitzhanov,Ayan Mazhitov,Azamat Abdikarimov,Danissa Sandykbayeva,Daryn Kenzhebek,Dinmukhammed Mukashev,Ilyas Umurbekov,Jabrail Chumakov,Kamila Spanova,Karina Burunchina,Madina Yergibay,Margulan Issa,Moldir Zabirova,Nurdaulet Zhuzbay,Nurlan Kabdyshev,Nurlan Zhaniyar,Rasul Yermagambet,Rustam Chibar,Saltanat Seitzhan,Soibkhon Khajikhanov,Tasbolat Taunyazov,Temirlan Galimzhanov,Temirlan Kaiyrbay, et al. (7 additional authors not shown)
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别
- **Abstract**: This paper presents an AI-generated review of Vision-Language-Action (VLA) models, summarizing key methodologies, findings, and future directions. The content is produced using large language models (LLMs) and is intended only for demonstration purposes. This work does not represent original research, but highlights how AI can help automate literature reviews. As AI-generated content becomes more prevalent, ensuring accuracy, reliability, and proper synthesis remains a challenge. Future research will focus on developing a structured framework for AI-assisted literature reviews, exploring techniques to enhance citation accuracy, source credibility, and contextual understanding. By examining the potential and limitations of LLM in academic writing, this study aims to contribute to the broader discussion of integrating AI into research workflows. This work serves as a preliminary step toward establishing systematic approaches for leveraging AI in literature review generation, making academic knowledge synthesis more efficient and scalable.

### Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning 
[[arxiv](https://arxiv.org/abs/2502.06781)] [[cool](https://papers.cool/arxiv/2502.06781)] [[pdf](https://arxiv.org/pdf/2502.06781)]
> **Authors**: Chengqi Lyu,Songyang Gao,Yuzhe Gu,Wenwei Zhang,Jianfei Gao,Kuikun Liu,Ziyi Wang,Shuaibin Li,Qian Zhao,Haian Huang,Weihan Cao,Jiangning Liu,Hongwei Liu,Junnan Liu,Songyang Zhang,Dahua Lin,Kai Chen
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: We released our code, data, andmodelon https://github.com/InternLM/OREAL
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Reasoning abilities, especially those for solving complex math problems, are crucial components of general intelligence. Recent advances by proprietary companies, such as o-series models of OpenAI, have made remarkable progress on reasoning tasks. However, the complete technical details remain unrevealed, and the techniques that are believed certainly to be adopted are only reinforcement learning (RL) and the long chain of thoughts. This paper proposes a new RL framework, termed OREAL, to pursue the performance limit that can be achieved through \textbf{O}utcome \textbf{RE}w\textbf{A}rd-based reinforcement \textbf{L}earning for mathematical reasoning tasks, where only binary outcome rewards are easily accessible. We theoretically prove that behavior cloning on positive trajectories from best-of-N (BoN) sampling is sufficient to learn the KL-regularized optimal policy in binary feedback environments. This formulation further implies that the rewards of negative samples should be reshaped to ensure the gradient consistency between positive and negative samples. To alleviate the long-existing difficulties brought by sparse rewards in RL, which are even exacerbated by the partial correctness of the long chain of thought for reasoning tasks, we further apply a token-level reward model to sample important tokens in reasoning trajectories for learning. With OREAL, for the first time, a 7B model can obtain 94.0 pass@1 accuracy on MATH-500 through RL, being on par with 32B models. OREAL-32B also surpasses previous 32B models trained by distillation with 95.0 pass@1 accuracy on MATH-500. Our investigation also indicates the importance of initial policy models and training queries for RL. Code, models, and data will be released to benefit future research\footnote{https://github.com/InternLM/OREAL}.

### ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates 
[[arxiv](https://arxiv.org/abs/2502.06772)] [[cool](https://papers.cool/arxiv/2502.06772)] [[pdf](https://arxiv.org/pdf/2502.06772)]
> **Authors**: Ling Yang,Zhaochen Yu,Bin Cui,Mengdi Wang
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Code: https://github.com/Gen-Verse/ReasonFlux
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: We present that hierarchical LLM reasoning via scaling thought templates can effectively optimize the reasoning search space and outperform the mathematical reasoning capabilities of powerful LLMs like OpenAI o1-preview and DeepSeek V3. We train our ReasonFlux-32B model with only 8 GPUs and introduces three innovations: (i) a structured and generic thought template library, containing around 500 high-level thought templates capable of generalizing to similar or relevant reasoning problems; (ii) performing hierarchical reinforcement learning on a sequence of thought templates instead of long CoTs, optimizing a base LLM to plan out an optimal template trajectory for gradually handling complex problems; (iii) a brand new inference scaling system that enables hierarchical LLM reasoning by adaptively scaling thought templates at inference time. With a template trajectory containing sequential thought templates, our ReasonFlux-32B significantly advances math reasoning capabilities to state-of-the-art levels. Notably, on the MATH benchmark, it achieves an accuracy of 91.2% and surpasses o1-preview by 6.7%. On the USA Math Olympiad (AIME) benchmark, ReasonFlux-32B solves an average of 56.7% of problems, surpassing o1-preview and DeepSeek-V3 by 27% and 45%, respectively. Code: https://github.com/Gen-Verse/ReasonFlux

### Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs 
[[arxiv](https://arxiv.org/abs/2502.06766)] [[cool](https://papers.cool/arxiv/2502.06766)] [[pdf](https://arxiv.org/pdf/2502.06766)]
> **Authors**: Ryan Synk,Monte Hoover,John Kirchenbauer,Neel Jain,Alex Stein,Manli Shu,Josue Melendez Sanchez,Ramani Duraiswami,Tom Goldstein
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 9 pages, 9 figures, 2 tables in main body
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: There is growing demand for performing inference with hundreds of thousands of input tokens on trained transformer models. Inference at this extreme scale demands significant computational resources, hindering the application of transformers at long contexts on commodity (i.e not data center scale) hardware. To address the inference time costs associated with running self-attention based transformer language models on long contexts and enable their adoption on widely available hardware, we propose a tunable mechanism that reduces the cost of the forward pass by attending to only the most relevant tokens at every generation step using a top-k selection mechanism. We showcase the efficiency gains afforded by our method by performing inference on context windows up to 1M tokens using approximately 16GB of GPU RAM. Our experiments reveal that models are capable of handling the sparsity induced by the reduced number of keys and values. By attending to less than 2% of input tokens, we achieve over 95% of model performance on common benchmarks (RULER, AlpacaEval, and Open LLM Leaderboard).

### Rationalization Models for Text-to-SQL 
[[arxiv](https://arxiv.org/abs/2502.06759)] [[cool](https://papers.cool/arxiv/2502.06759)] [[pdf](https://arxiv.org/pdf/2502.06759)]
> **Authors**: Gaetano Rossiello,Nhan Pham,Michael Glass,Junkyu Lee,Dharmashankar Subramanian
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,数据库
- **Abstract**: We introduce a framework for generating Chain-of-Thought (CoT) rationales to enhance text-to-SQL model fine-tuning. These rationales consist of intermediate SQL statements and explanations, serving as incremental steps toward constructing the final SQL query. The process begins with manually annotating a small set of examples, which are then used to prompt a large language model in an iterative, dynamic few-shot knowledge distillation procedure from a teacher model. A rationalization model is subsequently trained on the validated decomposed queries, enabling extensive synthetic CoT annotations for text-to-SQL datasets. To evaluate the approach, we fine-tune small language models with and without these rationales on the BIRD dataset. Results indicate that step-by-step query generation improves execution accuracy, especially for moderately and highly complex queries, while also enhancing explainability.

### Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling 
[[arxiv](https://arxiv.org/abs/2502.06703)] [[cool](https://papers.cool/arxiv/2502.06703)] [[pdf](https://arxiv.org/pdf/2502.06703)]
> **Authors**: Runze Liu,Junqi Gao,Jian Zhao,Kaiyan Zhang,Xiu Li,Biqing Qi,Wanli Ouyang,Bowen Zhou
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Test-Time Scaling (TTS) is an important method for improving the performance of Large Language Models (LLMs) by using additional computation during the inference phase. However, current studies do not systematically analyze how policy models, Process Reward Models (PRMs), and problem difficulty influence TTS. This lack of analysis limits the understanding and practical use of TTS methods. In this paper, we focus on two core questions: (1) What is the optimal approach to scale test-time computation across different policy models, PRMs, and problem difficulty levels? (2) To what extent can extended computation improve the performance of LLMs on complex tasks, and can smaller language models outperform larger ones through this approach? Through comprehensive experiments on MATH-500 and challenging AIME24 tasks, we have the following observations: (1) The compute-optimal TTS strategy is highly dependent on the choice of policy model, PRM, and problem difficulty. (2) With our compute-optimal TTS strategy, extremely small policy models can outperform larger models. For example, a 1B LLM can exceed a 405B LLM on MATH-500. Moreover, on both MATH-500 and AIME24, a 0.5B LLM outperforms GPT-4o, a 3B LLM surpasses a 405B LLM, and a 7B LLM beats o1 and DeepSeek-R1, while with higher inference efficiency. These findings show the significance of adapting TTS strategies to the specific characteristics of each task and model and indicate that TTS is a promising approach for enhancing the reasoning abilities of LLMs.

### Multi-label Scandinavian Language Identification (SLIDE) 
[[arxiv](https://arxiv.org/abs/2502.06692)] [[cool](https://papers.cool/arxiv/2502.06692)] [[pdf](https://arxiv.org/pdf/2502.06692)]
> **Authors**: Mariia Fedorova,Jonas Sebulon Frydenberg,Victoria Handford,Victoria Ovedie Chruickshank Langø,Solveig Helene Willoch,Marthe Løken Midtgaard,Yves Scherrer,Petter Mæhlum,David Samuel
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Identifying closely related languages at sentence level is difficult, in particular because it is often impossible to assign a sentence to a single language. In this paper, we focus on multi-label sentence-level Scandinavian language identification (LID) for Danish, Norwegian Bokmål, Norwegian Nynorsk, and Swedish. We present the Scandinavian Language Identification and Evaluation, SLIDE, a manually curated multi-label evaluation dataset and a suite of LID models with varying speed-accuracy tradeoffs. We demonstrate that the ability to identify multiple languages simultaneously is necessary for any accurate LID method, and present a novel approach to training such multi-label LID models.

### Boosting Self-Efficacy and Performance of Large Language Models via Verbal Efficacy Stimulations 
[[arxiv](https://arxiv.org/abs/2502.06669)] [[cool](https://papers.cool/arxiv/2502.06669)] [[pdf](https://arxiv.org/pdf/2502.06669)]
> **Authors**: Rui Chen,Tailai Peng,Xinran Xie,Dekun Lin,Zhe Cui,Zheng Chen
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: to be published in ICONIP 2024
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Significant improvements have been observed in the zero-shot capabilities of the Large Language Models (LLMs). Due to their high sensitivity to input, research has increasingly focused on enhancing LLMs' performance via direct and simple prompt engineering rather than intricate domain adaptation. Studies suggest that LLMs exhibit emotional intelligence, and both positive and negative emotions can potentially enhance task performances. However, prior interaction prompts have predominantly concentrated on a single stimulus type, neglecting to compare different stimulus effects, examine the influence of varying task difficulties, or explore underlying mechanisms. This paper, inspired by the positive correlation between self-efficacy and task performance within the social cognitive theory, introduces Verbal Efficacy Stimulations (VES). Our VES comprises three types of verbal prompts: encouraging, provocative, and critical, addressing six aspects such as helpfulness and competence. And we further categorize task difficulty, aiming to extensively investigate how distinct VES influence the self-efficacy and task achievements of language models at varied levels of difficulty. The experimental results show that the three types of VES improve the performance of LLMs on most tasks, and the most effective VES varies for different models. In extensive experiments, we have obtained some findings consistent with psychological theories, providing novel insights for future research.

### Automatic Evaluation of Healthcare LLMs Beyond Question-Answering 
[[arxiv](https://arxiv.org/abs/2502.06666)] [[cool](https://papers.cool/arxiv/2502.06666)] [[pdf](https://arxiv.org/pdf/2502.06666)]
> **Authors**: Anna Arias-Duart,Pablo Agustin Martin-Torres,Daniel Hinjos,Pablo Bernabeu-Perez,Lucia Urcelay Ganzabal,Marta Gonzalez Mallo,Ashwin Kumar Gururajan,Enrique Lopez-Cuena,Sergio Alvarez-Napagao,Dario Garcia-Gasulla
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Current Large Language Models (LLMs) benchmarks are often based on open-ended or close-ended QA evaluations, avoiding the requirement of human labor. Close-ended measurements evaluate the factuality of responses but lack expressiveness. Open-ended capture the model's capacity to produce discourse responses but are harder to assess for correctness. These two approaches are commonly used, either independently or together, though their relationship remains poorly understood. This work is focused on the healthcare domain, where both factuality and discourse matter greatly. It introduces a comprehensive, multi-axis suite for healthcare LLM evaluation, exploring correlations between open and close benchmarks and metrics. Findings include blind spots and overlaps in current methodologies. As an updated sanity check, we release a new medical benchmark --CareQA-- with both open and closed variants. Finally, we propose a novel metric for open-ended evaluations -- Relaxed Perplexity -- to mitigate the identified limitations.

### Who Taught You That? Tracing Teachers in Model Distillation 
[[arxiv](https://arxiv.org/abs/2502.06659)] [[cool](https://papers.cool/arxiv/2502.06659)] [[pdf](https://arxiv.org/pdf/2502.06659)]
> **Authors**: Somin Wadhwa,Chantal Shaib,Silvio Amir,Byron C. Wallace
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Preprint; under review
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Model distillation -- using outputs from a large teacher model to teach a small student model -- is a practical means of creating efficient models for a particular task. We ask: Can we identify a students' teacher based on its outputs? Such "footprints" left by teacher LLMs would be interesting artifacts. Beyond this, reliable teacher inference may have practical implications as actors seek to distill specific capabilities of massive proprietary LLMs into deployed smaller LMs, potentially violating terms of service. We consider practical task distillation targets including summarization, question answering, and instruction-following. We assume a finite set of candidate teacher models, which we treat as blackboxes. We design discriminative models that operate over lexical features. We find that $n$-gram similarity alone is unreliable for identifying teachers, but part-of-speech (PoS) templates preferred by student models mimic those of their teachers.

### In-Context Learning (and Unlearning) of Length Biases 
[[arxiv](https://arxiv.org/abs/2502.06653)] [[cool](https://papers.cool/arxiv/2502.06653)] [[pdf](https://arxiv.org/pdf/2502.06653)]
> **Authors**: Stephanie Schoch,Yangfeng Ji
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Accepted to NAACL 2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models have demonstrated strong capabilities to learn in-context, where exemplar input-output pairings are appended to the prompt for demonstration. However, existing work has demonstrated the ability of models to learn lexical and label biases in-context, which negatively impacts both performance and robustness of models. The impact of other statistical data biases remains under-explored, which this work aims to address. We specifically investigate the impact of length biases on in-context learning. We demonstrate that models do learn length biases in the context window for their predictions, and further empirically analyze the factors that modulate the level of bias exhibited by the model. In addition, we show that learning length information in-context can be used to counter the length bias that has been encoded in models (e.g., via fine-tuning). This reveals the power of in-context learning in debiasing model prediction behaviors without the need for costly parameter updates.

### Transparent NLP: Using RAG and LLM Alignment for Privacy Q&A 
[[arxiv](https://arxiv.org/abs/2502.06652)] [[cool](https://papers.cool/arxiv/2502.06652)] [[pdf](https://arxiv.org/pdf/2502.06652)]
> **Authors**: Anna Leschanowsky,Zahra Kolagar,Erion Çano,Ivan Habernal,Dara Hallinan,Emanuël A. P. Habets,Birgit Popp
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Submitted to ARR
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The transparency principle of the General Data Protection Regulation (GDPR) requires data processing information to be clear, precise, and accessible. While language models show promise in this context, their probabilistic nature complicates truthfulness and comprehensibility. This paper examines state-of-the-art Retrieval Augmented Generation (RAG) systems enhanced with alignment techniques to fulfill GDPR obligations. We evaluate RAG systems incorporating an alignment module like Rewindable Auto-regressive Inference (RAIN) and our proposed multidimensional extension, MultiRAIN, using a Privacy Q&A dataset. Responses are optimized for preciseness and comprehensibility and are assessed through 21 metrics, including deterministic and large language model-based evaluations. Our results show that RAG systems with an alignment module outperform baseline RAG systems on most metrics, though none fully match human answers. Principal component analysis of the results reveals complex interactions between metrics, highlighting the need to refine metrics. This study provides a foundation for integrating advanced natural language processing systems into legal compliance frameworks.

### Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building a Chinese-Centric LLM 
[[arxiv](https://arxiv.org/abs/2502.06635)] [[cool](https://papers.cool/arxiv/2502.06635)] [[pdf](https://arxiv.org/pdf/2502.06635)]
> **Authors**: Qingshui Gu,Shu Li,Tianyu Zheng,Zhaoxiang Zhang
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Steel-LLM is a Chinese-centric language model developed from scratch with the goal of creating a high-quality, open-source model despite limited computational resources. Launched in March 2024, the project aimed to train a 1-billion-parameter model on a large-scale dataset, prioritizing transparency and the sharing of practical insights to assist others in the community. The training process primarily focused on Chinese data, with a small proportion of English data included, addressing gaps in existing open-source LLMs by providing a more detailed and practical account of the model-building journey. Steel-LLM has demonstrated competitive performance on benchmarks such as CEVAL and CMMLU, outperforming early models from larger institutions. This paper provides a comprehensive summary of the project's key contributions, including data collection, model design, training methodologies, and the challenges encountered along the way, offering a valuable resource for researchers and practitioners looking to develop their own LLMs. The model checkpoints and training script are available at https://github.com/zhanshijinwat/Steel-LLM.

### Scaling Multi-Document Event Summarization: Evaluating Compression vs. Full-Text Approaches 
[[arxiv](https://arxiv.org/abs/2502.06617)] [[cool](https://papers.cool/arxiv/2502.06617)] [[pdf](https://arxiv.org/pdf/2502.06617)]
> **Authors**: Adithya Pratapa,Teruko Mitamura
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: NAACL 2025 camera-ready version
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Automatically summarizing large text collections is a valuable tool for document research, with applications in journalism, academic research, legal work, and many other fields. In this work, we contrast two classes of systems for large-scale multi-document summarization (MDS): compression and full-text. Compression-based methods use a multi-stage pipeline and often lead to lossy summaries. Full-text methods promise a lossless summary by relying on recent advances in long-context reasoning. To understand their utility on large-scale MDS, we evaluated them on three datasets, each containing approximately one hundred documents per summary. Our experiments cover a diverse set of long-context transformers (Llama-3.1, Command-R, Jamba-1.5-Mini) and compression methods (retrieval-augmented, hierarchical, incremental). Overall, we find that full-text and retrieval methods perform the best in most settings. With further analysis into the salient information retention patterns, we show that compression-based methods show strong promise at intermediate stages, even outperforming full-context. However, they suffer information loss due to their multi-stage pipeline and lack of global context. Our results highlight the need to develop hybrid approaches that combine compression and full-text approaches for optimal performance on large-scale multi-document summarization.

### Do we really have to filter out random noise in pre-training data for language models? 
[[arxiv](https://arxiv.org/abs/2502.06604)] [[cool](https://papers.cool/arxiv/2502.06604)] [[pdf](https://arxiv.org/pdf/2502.06604)]
> **Authors**: Jinghan Ru,Yuxin Xie,Xianwei Zhuang,Yuguo Yin,Yuexian Zou
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Web-scale pre-training datasets are the cornerstone of LLMs' success. However, text data curated from the internet inevitably contains random noise caused by decoding errors or unregulated web content. In contrast to previous works that focus on low quality or synthetic data, our study \textbf{provides the first systematic investigation into such random noise through a cohesive ``What-Why-How'' framework.} Surprisingly, we observed that the resulting increase in next-token prediction (NTP) loss was significantly lower than the proportion of random noise. We provide a theoretical justification for this phenomenon, which also elucidates the success of multilingual models. On the other hand, experiments show that the model's performance in downstream tasks is not based solely on the NTP loss, which means that random noise may result in degraded downstream performance. To address the potential adverse effects, we introduce a novel plug-and-play Local Gradient Matching loss, which explicitly enhances the denoising capability of the downstream task head by aligning the gradient of normal and perturbed features without requiring knowledge of the model's parameters. Additional experiments on 8 language and 14 vision benchmarks further validate its effectiveness.

### Evaluation of Multilingual Image Captioning: How far can we get with CLIP models? 
[[arxiv](https://arxiv.org/abs/2502.06600)] [[cool](https://papers.cool/arxiv/2502.06600)] [[pdf](https://arxiv.org/pdf/2502.06600)]
> **Authors**: Gonçalo Gomes,Chrysoula Zerva,Bruno Martins
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Accepted in Findings of NAACL 2025
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: The evaluation of image captions, looking at both linguistic fluency and semantic correspondence to visual contents, has witnessed a significant effort. Still, despite advancements such as the CLIPScore metric, multilingual captioning evaluation has remained relatively unexplored. This work presents several strategies, and extensive experiments, related to evaluating CLIPScore variants in multilingual settings. To address the lack of multilingual test data, we consider two different strategies: (1) using quality aware machine-translated datasets with human judgements, and (2) re-purposing multilingual datasets that target semantic inference and reasoning. Our results highlight the potential of finetuned multilingual models to generalize across languages and to handle complex linguistic challenges. Tests with machine-translated data show that multilingual CLIPScore models can maintain a high correlation with human judgements across different languages, and additional tests with natively multilingual and multicultural data further attest to the high-quality assessments.

### Hephaestus: Improving Fundamental Agent Capabilities of Large Language Models through Continual Pre-Training 
[[arxiv](https://arxiv.org/abs/2502.06589)] [[cool](https://papers.cool/arxiv/2502.06589)] [[pdf](https://arxiv.org/pdf/2502.06589)]
> **Authors**: Yuchen Zhuang,Jingfeng Yang,Haoming Jiang,Xin Liu,Kewei Cheng,Sanket Lokegaonkar,Yifan Gao,Qing Ping,Tianyi Liu,Binxuan Huang,Zheng Li,Zhengyang Wang,Pei Chen,Ruijie Wang,Rongzhi Zhang,Nasser Zalmout,Priyanka Nigam,Bing Yin,Chao Zhang
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Accepted to NAACL 2025 main conference
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Due to the scarcity of agent-oriented pre-training data, LLM-based autonomous agents typically rely on complex prompting or extensive fine-tuning, which often fails to introduce new capabilities while preserving strong generalizability. We introduce Hephaestus-Forge, the first large-scale pre-training corpus designed to enhance the fundamental capabilities of LLM agents in API function calling, intrinsic reasoning and planning, and adapting to environmental feedback. Hephaestus-Forge comprises 103B agent-specific data encompassing 76,537 APIs, including both tool documentation to introduce knowledge of API functions and function calling trajectories to strengthen intrinsic reasoning. To explore effective training protocols, we investigate scaling laws to identify the optimal recipe in data mixing ratios. By continual pre-training on Hephaestus-Forge, Hephaestus outperforms small- to medium-scale open-source LLMs and rivals commercial LLMs on three agent benchmarks, demonstrating the effectiveness of our pre-training corpus in enhancing fundamental agentic capabilities and generalization of LLMs to new tasks or environments.

### LawGPT: Knowledge-Guided Data Generation and Its Application to Legal LLM 
[[arxiv](https://arxiv.org/abs/2502.06572)] [[cool](https://papers.cool/arxiv/2502.06572)] [[pdf](https://arxiv.org/pdf/2502.06572)]
> **Authors**: Zhi Zhou,Kun-Yang Yu,Shi-Yu Tian,Xiao-Wen Yang,Jiang-Xin Shi,Pengxiao Song,Yi-Xuan Jin,Lan-Zhe Guo,Yu-Feng Li
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Preprint
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large language models (LLMs), both proprietary and open-source, have demonstrated remarkable capabilities across various natural language processing tasks. However, they face significant limitations in legal reasoning tasks. Proprietary models introduce data privacy risks and high inference costs, while open-source models underperform due to insufficient legal domain training data. To address these limitations, we study data generation for legal reasoning to improve the legal reasoning performance of open-source LLMs with the help of proprietary LLMs. This is challenging due to the lack of legal knowledge in proprietary LLMs and the difficulty in verifying the generated data. We propose KgDG, a knowledge-guided data generation framework for legal reasoning. Our framework enables leveraging legal knowledge to enhance generation diversity and introduces a refinement and verification process to ensure the quality of generated data. Moreover, we expand the generated dataset to further enhance the LLM reasoning capabilities. Using KgDG, we create a synthetic legal reasoning dataset containing 50K high-quality examples. Our trained model LawGPT outperforms existing legal-specific LLMs and achieves performance comparable to proprietary LLMs, demonstrating the effectiveness of KgDG and LawGPT. Our code and resources is publicly available at https://github.com/LAMDASZ-ML/Knowledge-Guide-Data-Generation .

### Large Language Models Meet Symbolic Provers for Logical Reasoning Evaluation 
[[arxiv](https://arxiv.org/abs/2502.06563)] [[cool](https://papers.cool/arxiv/2502.06563)] [[pdf](https://arxiv.org/pdf/2502.06563)]
> **Authors**: Chengwen Qi,Ren Ma,Bowen Li,He Du,Binyuan Hui,Jinwang Wu,Yuanjun Laili,Conghui He
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Accepted by ICLR 2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: First-order logic (FOL) reasoning, which involves sequential deduction, is pivotal for intelligent systems and serves as a valuable task for evaluating reasoning capabilities, particularly in chain-of-thought (CoT) contexts. Existing benchmarks often rely on extensive human annotation or handcrafted templates, making it difficult to achieve the necessary complexity, scalability, and diversity for robust evaluation. To address these limitations, we propose a novel framework called ProverGen that synergizes the generative strengths of Large Language Models (LLMs) with the rigor and precision of symbolic provers, enabling the creation of a scalable, diverse, and high-quality FOL reasoning dataset, ProverQA. ProverQA is also distinguished by its inclusion of accessible and logically coherent intermediate reasoning steps for each problem. Our evaluation shows that state-of-the-art LLMs struggle to solve ProverQA problems, even with CoT prompting, highlighting the dataset's challenging nature. We also finetune Llama3.1-8B-Instruct on a separate training set generated by our framework. The finetuned model demonstrates consistent improvements on both in-distribution and out-of-distribution test sets, suggesting the value of our proposed data generation framework. Code available at: https://github.com/opendatalab/ProverGen

### Position: It's Time to Act on the Risk of Efficient Personalized Text Generation 
[[arxiv](https://arxiv.org/abs/2502.06560)] [[cool](https://papers.cool/arxiv/2502.06560)] [[pdf](https://arxiv.org/pdf/2502.06560)]
> **Authors**: Eugenia Iofinova,Andrej Jovanovic,Dan Alistarh
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,计算机与社会
- **Abstract**: The recent surge in high-quality open-sourced Generative AI text models (colloquially: LLMs), as well as efficient finetuning techniques, has opened the possibility of creating high-quality personalized models, i.e., models generating text attuned to a specific individual's needs and capable of credibly imitating their writing style by leveraging that person's own data to refine an open-source model. The technology to create such models is accessible to private individuals, and training and running such models can be done cheaply on consumer-grade hardware. These advancements are a huge gain for usability and privacy. This position paper argues, however, that these advancements also introduce new safety risks by making it practically feasible for malicious actors to impersonate specific individuals at scale, for instance for the purpose of phishing emails, based on small amounts of publicly available text. We further argue that these risks are complementary to - and distinct from - the much-discussed risks of other impersonation attacks such as image, voice, or video deepfakes, and are not adequately addressed by the larger research community, or the current generation of open - and closed-source models.

### Efficient Scientific Full Text Classification: The Case of EICAT Impact Assessments 
[[arxiv](https://arxiv.org/abs/2502.06551)] [[cool](https://papers.cool/arxiv/2502.06551)] [[pdf](https://arxiv.org/pdf/2502.06551)]
> **Authors**: Marc Felix Brinner,Sina Zarrieß
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: This study explores strategies for efficiently classifying scientific full texts using both small, BERT-based models and local large language models like Llama-3.1 8B. We focus on developing methods for selecting subsets of input sentences to reduce input size while simultaneously enhancing classification performance. To this end, we compile a novel dataset consisting of full-text scientific papers from the field of invasion biology, specifically addressing the impacts of invasive species. These papers are aligned with publicly available impact assessments created by researchers for the International Union for Conservation of Nature (IUCN). Through extensive experimentation, we demonstrate that various sources like human evidence annotations, LLM-generated annotations or explainability scores can be used to train sentence selection models that improve the performance of both encoder- and decoder-based language models while optimizing efficiency through the reduction in input length, leading to improved results even if compared to models like ModernBERT that are able to handle the complete text as input. Additionally, we find that repeated sampling of shorter inputs proves to be a very effective strategy that, at a slightly increased cost, can further improve classification performance.

### Ignore the KL Penalty! Boosting Exploration on Critical Tokens to Enhance RL Fine-Tuning 
[[arxiv](https://arxiv.org/abs/2502.06533)] [[cool](https://papers.cool/arxiv/2502.06533)] [[pdf](https://arxiv.org/pdf/2502.06533)]
> **Authors**: Jean Vassoyan,Nathanaël Beau,Roman Plaud
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 11 pages, 6 figures, 5 tables. Accepted for publication in the Findings of the North American Chapter of the Association for Computational Linguistics (NAACL) 2025
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: The ability to achieve long-term goals is a key challenge in the current development of large language models (LLMs). To address this, pre-trained LLMs can be fine-tuned with reinforcement learning (RL) to explore solutions that optimize a given goal. However, exploration with LLMs is difficult, as a balance has to be struck between discovering new solutions and staying close enough to the pre-trained model, so as not to degrade basic capabilities. This is typically controlled with a Kullback-Leibler (KL) penalty. In this paper, we investigate the exploration dynamics of a small language model on a simple arithmetic task. We show how varying degrees of pre-training influence exploration and demonstrate the importance of "critical tokens" which have a dramatic impact on the final outcome. Consequently, we introduce a simple modification to the KL penalty that favors exploration on critical tokens, increasing the efficiency of the RL fine-tuning stage.

### GuideLLM: Exploring LLM-Guided Conversation with Applications in Autobiography Interviewing 
[[arxiv](https://arxiv.org/abs/2502.06494)] [[cool](https://papers.cool/arxiv/2502.06494)] [[pdf](https://arxiv.org/pdf/2502.06494)]
> **Authors**: Jinhao Duan,Xinyu Zhao,Zhuoxuan Zhang,Eunhye Ko,Lily Boddy,Chenan Wang,Tianhao Li,Alexander Rasgon,Junyuan Hong,Min Kyung Lee,Chenxi Yuan,Qi Long,Ying Ding,Tianlong Chen,Kaidi Xu
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 31 pages; the first three authors contributed equally
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Although Large Language Models (LLMs) succeed in human-guided conversations such as instruction following and question answering, the potential of LLM-guided conversations-where LLMs direct the discourse and steer the conversation's objectives-remains under-explored. In this study, we first characterize LLM-guided conversation into three fundamental components: (i) Goal Navigation; (ii) Context Management; (iii) Empathetic Engagement, and propose GuideLLM as an installation. We then implement an interviewing environment for the evaluation of LLM-guided conversation. Specifically, various topics are involved in this environment for comprehensive interviewing evaluation, resulting in around 1.4k turns of utterances, 184k tokens, and over 200 events mentioned during the interviewing for each chatbot evaluation. We compare GuideLLM with 6 state-of-the-art LLMs such as GPT-4o and Llama-3-70b-Instruct, from the perspective of interviewing quality, and autobiography generation quality. For automatic evaluation, we derive user proxies from multiple autobiographies and employ LLM-as-a-judge to score LLM behaviors. We further conduct a human-involved experiment by employing 45 human participants to chat with GuideLLM and baselines. We then collect human feedback, preferences, and ratings regarding the qualities of conversation and autobiography. Experimental results indicate that GuideLLM significantly outperforms baseline LLMs in automatic evaluation and achieves consistent leading performances in human ratings.

### Adaptive Prompting: Ad-hoc Prompt Composition for Social Bias Detection 
[[arxiv](https://arxiv.org/abs/2502.06487)] [[cool](https://papers.cool/arxiv/2502.06487)] [[pdf](https://arxiv.org/pdf/2502.06487)]
> **Authors**: Maximilian Spliethöver,Tim Knebler,Fabian Fumagalli,Maximilian Muschalik,Barbara Hammer,Eyke Hüllermeier,Henning Wachsmuth
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Accepted to NAACL 2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Recent advances on instruction fine-tuning have led to the development of various prompting techniques for large language models, such as explicit reasoning steps. However, the success of techniques depends on various parameters, such as the task, language model, and context provided. Finding an effective prompt is, therefore, often a trial-and-error process. Most existing approaches to automatic prompting aim to optimize individual techniques instead of compositions of techniques and their dependence on the input. To fill this gap, we propose an adaptive prompting approach that predicts the optimal prompt composition ad-hoc for a given input. We apply our approach to social bias detection, a highly context-dependent task that requires semantic understanding. We evaluate it with three large language models on three datasets, comparing compositions to individual techniques and other baselines. The results underline the importance of finding an effective prompt composition. Our approach robustly ensures high detection performance, and is best in several settings. Moreover, first experiments on other tasks support its generalizability.

### KARMA: Leveraging Multi-Agent LLMs for Automated Knowledge Graph Enrichment 
[[arxiv](https://arxiv.org/abs/2502.06472)] [[cool](https://papers.cool/arxiv/2502.06472)] [[pdf](https://arxiv.org/pdf/2502.06472)]
> **Authors**: Yuxing Lu,Jinzhuo Wang
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 24 pages, 3 figures, 2 tables
- **标题**: None
- **领域**: 计算语言学,人工智能,计算工程、金融和科学,数字图书馆
- **Abstract**: Maintaining comprehensive and up-to-date knowledge graphs (KGs) is critical for modern AI systems, but manual curation struggles to scale with the rapid growth of scientific literature. This paper presents KARMA, a novel framework employing multi-agent large language models (LLMs) to automate KG enrichment through structured analysis of unstructured text. Our approach employs nine collaborative agents, spanning entity discovery, relation extraction, schema alignment, and conflict resolution that iteratively parse documents, verify extracted knowledge, and integrate it into existing graph structures while adhering to domain-specific schema. Experiments on 1,200 PubMed articles from three different domains demonstrate the effectiveness of KARMA in knowledge graph enrichment, with the identification of up to 38,230 new entities while achieving 83.1\% LLM-verified correctness and reducing conflict edges by 18.6\% through multi-layer assessments.

### A Survey of Theory of Mind in Large Language Models: Evaluations, Representations, and Safety Risks 
[[arxiv](https://arxiv.org/abs/2502.06470)] [[cool](https://papers.cool/arxiv/2502.06470)] [[pdf](https://arxiv.org/pdf/2502.06470)]
> **Authors**: Hieu Minh "Jord" Nguyen
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Advancing Artificial Intelligence through Theory of Mind Workshop, AAAI 2025
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Theory of Mind (ToM), the ability to attribute mental states to others and predict their behaviour, is fundamental to social intelligence. In this paper, we survey studies evaluating behavioural and representational ToM in Large Language Models (LLMs), identify important safety risks from advanced LLM ToM capabilities, and suggest several research directions for effective evaluation and mitigation of these risks.

### Beyond Literal Token Overlap: Token Alignability for Multilinguality 
[[arxiv](https://arxiv.org/abs/2502.06468)] [[cool](https://papers.cool/arxiv/2502.06468)] [[pdf](https://arxiv.org/pdf/2502.06468)]
> **Authors**: Katharina Hämmerl,Tomasz Limisiewicz,Jindřich Libovický,Alexander Fraser
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Accepted to NAACL 2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Previous work has considered token overlap, or even similarity of token distributions, as predictors for multilinguality and cross-lingual knowledge transfer in language models. However, these very literal metrics assign large distances to language pairs with different scripts, which can nevertheless show good cross-linguality. This limits the explanatory strength of token overlap for knowledge transfer between language pairs that use distinct scripts or follow different orthographic conventions. In this paper, we propose subword token alignability as a new way to understand the impact and quality of multilingual tokenisation. In particular, this metric predicts multilinguality much better when scripts are disparate and the overlap of literal tokens is low. We analyse this metric in the context of both encoder and decoder models, look at data size as a potential distractor, and discuss how this insight may be applied to multilingual tokenisation in future work. We recommend our subword token alignability metric for identifying optimal language pairs for cross-lingual transfer, as well as to guide the construction of better multilingual tokenisers in the future. We publish our code and reproducibility details.

### Systematic Outliers in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.06415)] [[cool](https://papers.cool/arxiv/2502.06415)] [[pdf](https://arxiv.org/pdf/2502.06415)]
> **Authors**: Yongqi An,Xu Zhao,Tao Yu,Ming Tang,Jinqiao Wang
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Accepted at ICLR 2025. Project Page: https://github.com/an-yongqi/systematic-outliers
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Outliers have been widely observed in Large Language Models (LLMs), significantly impacting model performance and posing challenges for model compression. Understanding the functionality and formation mechanisms of these outliers is critically important. Existing works, however, largely focus on reducing the impact of outliers from an algorithmic perspective, lacking an in-depth investigation into their causes and roles. In this work, we provide a detailed analysis of the formation process, underlying causes, and functions of outliers in LLMs. We define and categorize three types of outliers-activation outliers, weight outliers, and attention outliers-and analyze their distributions across different dimensions, uncovering inherent connections between their occurrences and their ultimate influence on the attention mechanism. Based on these observations, we hypothesize and explore the mechanisms by which these outliers arise and function, demonstrating through theoretical derivations and experiments that they emerge due to the self-attention mechanism's softmax operation. These outliers act as implicit context-aware scaling factors within the attention mechanism. As these outliers stem from systematic influences, we term them systematic outliers. Our study not only enhances the understanding of Transformer-based LLMs but also shows that structurally eliminating outliers can accelerate convergence and improve model compression. The code is avilable at https://github.com/an-yongqi/systematic-outliers.

### SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data Annotators 
[[arxiv](https://arxiv.org/abs/2502.06394)] [[cool](https://papers.cool/arxiv/2502.06394)] [[pdf](https://arxiv.org/pdf/2502.06394)]
> **Authors**: Daniil Moskovskiy,Nikita Sushko,Sergey Pletenev,Elena Tutubalina,Alexander Panchenko
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Accepted to NAACL 2025 Main Conference
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Existing approaches to multilingual text detoxification are hampered by the scarcity of parallel multilingual datasets. In this work, we introduce a pipeline for the generation of multilingual parallel detoxification data. We also introduce SynthDetoxM, a manually collected and synthetically generated multilingual parallel text detoxification dataset comprising 16,000 high-quality detoxification sentence pairs across German, French, Spanish and Russian. The data was sourced from different toxicity evaluation datasets and then rewritten with nine modern open-source LLMs in few-shot setting. Our experiments demonstrate that models trained on the produced synthetic datasets have superior performance to those trained on the human-annotated MultiParaDetox dataset even in data limited setting. Models trained on SynthDetoxM outperform all evaluated LLMs in few-shot setting. We release our dataset and code to help further research in multilingual text detoxification.

### The exponential distribution of the orders of demonstrative, numeral, adjective and noun 
[[arxiv](https://arxiv.org/abs/2502.06342)] [[cool](https://papers.cool/arxiv/2502.06342)] [[pdf](https://arxiv.org/pdf/2502.06342)]
> **Authors**: Ramon Ferrer-i-Cancho
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,物理与社会
- **Abstract**: The frequency of the preferred order for a noun phrase formed by demonstrative, numeral, adjective and noun has received significant attention over the last two decades. We investigate the actual distribution of the preferred 24 possible orders. There is no consensus on whether it can be well-fitted by an exponential or a power law distribution. We find that an exponential distribution is a much better model. This finding and other circumstances where an exponential-like distribution is found challenge the view that power-law distributions, e.g., Zipf's law for word frequencies, are inevitable. We also investigate which of two exponential distributions gives a better fit: an exponential model where the 24 orders have non-zero probability or an exponential model where the number of orders that can have non-zero probability is variable. When parsimony and generalizability are prioritized, we find strong support for the exponential model where all 24 orders have non-zero probability. This finding suggests that there is no hard constraint on word order variation and then unattested orders merely result from undersampling, consistently with Cysouw's view.

### Expect the Unexpected: FailSafe Long Context QA for Finance 
[[arxiv](https://arxiv.org/abs/2502.06329)] [[cool](https://papers.cool/arxiv/2502.06329)] [[pdf](https://arxiv.org/pdf/2502.06329)]
> **Authors**: Kiran Kamble,Melisa Russak,Dmytro Mozolevskyi,Muayad Ali,Mateusz Russak,Waseem AlShikh
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: We propose a new long-context financial benchmark, FailSafeQA, designed to test the robustness and context-awareness of LLMs against six variations in human-interface interactions in LLM-based query-answer systems within finance. We concentrate on two case studies: Query Failure and Context Failure. In the Query Failure scenario, we perturb the original query to vary in domain expertise, completeness, and linguistic accuracy. In the Context Failure case, we simulate the uploads of degraded, irrelevant, and empty documents. We employ the LLM-as-a-Judge methodology with Qwen2.5-72B-Instruct and use fine-grained rating criteria to define and calculate Robustness, Context Grounding, and Compliance scores for 24 off-the-shelf models. The results suggest that although some models excel at mitigating input perturbations, they must balance robust answering with the ability to refrain from hallucinating. Notably, Palmyra-Fin-128k-Instruct, recognized as the most compliant model, maintained strong baseline performance but encountered challenges in sustaining robust predictions in 17% of test cases. On the other hand, the most robust model, OpenAI o3-mini, fabricated information in 41% of tested cases. The results demonstrate that even high-performing models have significant room for improvement and highlight the role of FailSafeQA as a tool for developing LLMs optimized for dependability in financial applications. The dataset is available at: https://huggingface.co/datasets/Writer/FailSafeQA

### Can AI Examine Novelty of Patents?: Novelty Evaluation Based on the Correspondence between Patent Claim and Prior Art 
[[arxiv](https://arxiv.org/abs/2502.06316)] [[cool](https://papers.cool/arxiv/2502.06316)] [[pdf](https://arxiv.org/pdf/2502.06316)]
> **Authors**: Hayato Ikoma,Teruko Mitamura
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 11 pages, 5 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Assessing the novelty of patent claims is a critical yet challenging task traditionally performed by patent examiners. While advancements in NLP have enabled progress in various patent-related tasks, novelty assessment remains unexplored. This paper introduces a novel challenge by evaluating the ability of large language models (LLMs) to assess patent novelty by comparing claims with cited prior art documents, following the process similar to that of patent examiners done. We present the first dataset specifically designed for novelty evaluation, derived from real patent examination cases, and analyze the capabilities of LLMs to address this task. Our study reveals that while classification models struggle to effectively assess novelty, generative models make predictions with a reasonable level of accuracy, and their explanations are accurate enough to understand the relationship between the target patent and prior art. These findings demonstrate the potential of LLMs to assist in patent evaluation, reducing the workload for both examiners and applicants. Our contributions highlight the limitations of current models and provide a foundation for improving AI-driven patent analysis through advanced models and refined datasets.

### Latent Convergence Modulation in Large Language Models: A Novel Approach to Iterative Contextual Realignment 
[[arxiv](https://arxiv.org/abs/2502.06302)] [[cool](https://papers.cool/arxiv/2502.06302)] [[pdf](https://arxiv.org/pdf/2502.06302)]
> **Authors**: Patricia Porretta,Sylvester Pakenham,Huxley Ainsworth,Gregory Chatten,Godfrey Allerton,Simon Hollingsworth,Vance Periwinkle
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Token prediction stability remains a challenge in autoregressive generative models, where minor variations in early inference steps often lead to significant semantic drift over extended sequences. A structured modulation mechanism was introduced to regulate hidden state transitions, ensuring that latent representation trajectories remain aligned with prior contextual dependencies while preserving generative flexibility. The modulation framework was designed to function within transformer-based architectures, dynamically constraining representation evolution without imposing external memory dependencies or extensive architectural modifications. Empirical evaluations demonstrated that structured latent adjustments contributed to reductions in perplexity fluctuations, entropy variance, and lexical instability, improving coherence in long-form text generation. Gradient propagation stability was further analyzed, revealing that the modulation process led to smoother optimization pathways, mitigating erratic fluctuations in weight updates across successive inference steps. The computational efficiency of the modulation process was assessed, showing that its integration within transformer-based architectures introduced only marginal overhead while maintaining compatibility with existing optimization frameworks. The structured modulation constraints also influenced syntactic variation, preventing excessive repetition while maintaining balanced sentence length distributions. Comparative evaluations against baseline models reinforced the role of controlled latent state evolution in improving pronoun resolution, logical consistency, and contextual alignment across autoregressive text generation tasks.

### SeaExam and SeaBench: Benchmarking LLMs with Local Multilingual Questions in Southeast Asia 
[[arxiv](https://arxiv.org/abs/2502.06298)] [[cool](https://papers.cool/arxiv/2502.06298)] [[pdf](https://arxiv.org/pdf/2502.06298)]
> **Authors**: Chaoqun Liu,Wenxuan Zhang,Jiahao Ying,Mahani Aljunied,Anh Tuan Luu,Lidong Bing
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Accepted to Findings of NAACL 2025
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: This study introduces two novel benchmarks, SeaExam and SeaBench, designed to evaluate the capabilities of Large Language Models (LLMs) in Southeast Asian (SEA) application scenarios. Unlike existing multilingual datasets primarily derived from English translations, these benchmarks are constructed based on real-world scenarios from SEA regions. SeaExam draws from regional educational exams to form a comprehensive dataset that encompasses subjects such as local history and literature. In contrast, SeaBench is crafted around multi-turn, open-ended tasks that reflect daily interactions within SEA communities. Our evaluations demonstrate that SeaExam and SeaBench more effectively discern LLM performance on SEA language tasks compared to their translated benchmarks. This highlights the importance of using real-world queries to assess the multilingual capabilities of LLMs.

### Jakiro: Boosting Speculative Decoding with Decoupled Multi-Head via MoE 
[[arxiv](https://arxiv.org/abs/2502.06282)] [[cool](https://papers.cool/arxiv/2502.06282)] [[pdf](https://arxiv.org/pdf/2502.06282)]
> **Authors**: Haiduo Huang,Fuwei Yang,Zhenhua Liu,Yixing Xu,Jinze Li,Yang Liu,Xuanwu Yin,Dong Li,Pengju Ren,Emad Barsoum
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Speculative decoding (SD) accelerates large language model inference by using a smaller draft model to predict multiple tokens, which are then verified in parallel by the larger target model. However, the limited capacity of the draft model often necessitates tree-based sampling to improve prediction accuracy, where multiple candidates are generated at each step. We identify a key limitation in this approach: the candidates at the same step are derived from the same representation, limiting diversity and reducing overall effectiveness. To address this, we propose Jakiro, leveraging Mixture of Experts (MoE), where independent experts generate diverse predictions, effectively decoupling correlations among candidates. Furthermore, we introduce a hybrid inference strategy, combining autoregressive decoding for initial tokens with parallel decoding for subsequent stages, and enhance the latter with contrastive mechanism in features to improve accuracy. Our method significantly boosts prediction accuracy and achieves higher inference speedups. Extensive experiments across diverse models validate the effectiveness and robustness of our approach, establishing a new SOTA in speculative decoding. Our codes are available at https://github.com/haiduo/Jakiro.

### DebateBench: A Challenging Long Context Reasoning Benchmark For Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.06279)] [[cool](https://papers.cool/arxiv/2502.06279)] [[pdf](https://arxiv.org/pdf/2502.06279)]
> **Authors**: Utkarsh Tiwari,Aryan Seth,Adi Mukherjee,Kaavya Mer,Kavish,Dhruv Kumar
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: We introduce DebateBench, a novel dataset consisting of an extensive collection of transcripts and metadata from some of the world's most prestigious competitive debates. The dataset consists of British Parliamentary debates from prestigious debating tournaments on diverse topics, annotated with detailed speech-level scores and house rankings sourced from official adjudication data. We curate 256 speeches across 32 debates with each debate being over 1 hour long with each input being an average of 32,000 tokens. Designed to capture long-context, large-scale reasoning tasks, DebateBench provides a benchmark for evaluating modern large language models (LLMs) on their ability to engage in argumentation, deliberation, and alignment with human experts. To do well on DebateBench, the LLMs must perform in-context learning to understand the rules and evaluation criteria of the debates, then analyze 8 seven minute long speeches and reason about the arguments presented by all speakers to give the final results. Our preliminary evaluation using GPT o1, GPT-4o, and Claude Haiku, shows that LLMs struggle to perform well on DebateBench, highlighting the need to develop more sophisticated techniques for improving their performance.

### Emergent Response Planning in LLM 
[[arxiv](https://arxiv.org/abs/2502.06258)] [[cool](https://papers.cool/arxiv/2502.06258)] [[pdf](https://arxiv.org/pdf/2502.06258)]
> **Authors**: Zhichen Dong,Zhanhui Zhou,Zhixuan Liu,Chao Yang,Chaochao Lu
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: In this work, we argue that large language models (LLMs), though trained to predict only the next token, exhibit emergent planning behaviors: $\textbf{their hidden representations encode future outputs beyond the next token}$. Through simple probing, we demonstrate that LLM prompt representations encode global attributes of their entire responses, including $\textit{structural attributes}$ (response length, reasoning steps), $\textit{content attributes}$ (character choices in storywriting, multiple-choice answers at the end of response), and $\textit{behavioral attributes}$ (answer confidence, factual consistency). In addition to identifying response planning, we explore how it scales with model size across tasks and how it evolves during generation. The findings that LLMs plan ahead for the future in their hidden representations suggests potential applications for improving transparency and generation control.

### K-ON: Stacking Knowledge On the Head Layer of Large Language Model 
[[arxiv](https://arxiv.org/abs/2502.06257)] [[cool](https://papers.cool/arxiv/2502.06257)] [[pdf](https://arxiv.org/pdf/2502.06257)]
> **Authors**: Lingbing Guo,Yichi Zhang,Zhongpu Bo,Zhuo Chen,Mengshu Sun,Zhiqiang Zhang,Wen Zhang,Huajun Chen
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: AAAI 2025 (Oral)
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Recent advancements in large language models (LLMs) have significantly improved various natural language processing (NLP) tasks. Typically, LLMs are trained to predict the next token, aligning well with many NLP tasks. However, in knowledge graph (KG) scenarios, entities are the fundamental units and identifying an entity requires at least several tokens. This leads to a granularity mismatch between KGs and natural languages. To address this issue, we propose K-ON, which integrates KG knowledge into the LLM by employing multiple head layers for next k-step prediction. K-ON can not only generate entity-level results in one step, but also enables contrastive loss against entities, which is the most powerful tool in KG representation learning. Experimental results show that K-ON outperforms state-of-the-art methods that incorporate text and even the other modalities.

### Confidence Improves Self-Consistency in LLMs 
[[arxiv](https://arxiv.org/abs/2502.06233)] [[cool](https://papers.cool/arxiv/2502.06233)] [[pdf](https://arxiv.org/pdf/2502.06233)]
> **Authors**: Amir Taubenfeld,Tom Sheffer,Eran Ofek,Amir Feder,Ariel Goldstein,Zorik Gekhman,Gal Yona
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Self-consistency decoding enhances LLMs' performance on reasoning tasks by sampling diverse reasoning paths and selecting the most frequent answer. However, it is computationally expensive, as sampling many of these (lengthy) paths is required to increase the chances that the correct answer emerges as the most frequent one. To address this, we introduce Confidence-Informed Self-Consistency (CISC). CISC performs a weighted majority vote based on confidence scores obtained directly from the model. By prioritizing high-confidence paths, it can identify the correct answer with a significantly smaller sample size. When tested on nine models and four datasets, CISC outperforms self-consistency in nearly all configurations, reducing the required number of reasoning paths by over 40% on average. In addition, we introduce the notion of within-question confidence evaluation, after showing that standard evaluation methods are poor predictors of success in distinguishing correct and incorrect answers to the same question. In fact, the most calibrated confidence method proved to be the least effective for CISC. Lastly, beyond these practical implications, our results and analyses show that LLMs can effectively judge the correctness of their own outputs, contributing to the ongoing debate on this topic.

### Examining False Positives under Inference Scaling for Mathematical Reasoning 
[[arxiv](https://arxiv.org/abs/2502.06217)] [[cool](https://papers.cool/arxiv/2502.06217)] [[pdf](https://arxiv.org/pdf/2502.06217)]
> **Authors**: Yu Wang,Nan Yang,Liang Wang,Furu Wei
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Recent advancements in language models have led to significant improvements in mathematical reasoning across various benchmarks. However, most of these benchmarks rely on automatic evaluation methods that only compare final answers using heuristics, without verifying the underlying reasoning steps. This limitation results in false positive solutions, where models may produce correct final answers but with flawed deduction paths. In this paper, we systematically examine the prevalence of false positive solutions in mathematical problem solving for language models. We analyze the characteristics and extent of this issue across different open-source models, datasets of varying difficulty levels, and decoding strategies. Specifically, we explore how false positives influence the inference time scaling behavior of language models. Our experimental results reveal that: (1) false positive solutions persist across different models, datasets, and decoding methods, (2) sampling-based inference time scaling methods do not alleviate the problem, and (3) the pass@N evaluation metric is more susceptible to false positives, suggesting a significantly lower scaling ceiling than what automatic evaluations indicate. Additionally, we analyze specific instances of false positives and discuss potential limitations in self-improvement techniques and synthetic data generation under such conditions.

### Unveiling the Capabilities of Large Language Models in Detecting Offensive Language with Annotation Disagreement 
[[arxiv](https://arxiv.org/abs/2502.06207)] [[cool](https://papers.cool/arxiv/2502.06207)] [[pdf](https://arxiv.org/pdf/2502.06207)]
> **Authors**: Junyu Lu,Kai Ma,Kaichun Wang,Kelaiti Xiao,Roy Ka-Wei Lee,Bo Xu,Liang Yang,Hongfei Lin
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 17 pages, submitted to the ACL 2025
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large Language Models (LLMs) have become essential for offensive language detection, yet their ability to handle annotation disagreement remains underexplored. Disagreement samples, which arise from subjective interpretations, pose a unique challenge due to their ambiguous nature. Understanding how LLMs process these cases, particularly their confidence levels, can offer insight into their alignment with human annotators. This study systematically evaluates the performance of multiple LLMs in detecting offensive language at varying levels of annotation agreement. We analyze binary classification accuracy, examine the relationship between model confidence and human disagreement, and explore how disagreement samples influence model decision-making during few-shot learning and instruction fine-tuning. Our findings reveal that LLMs struggle with low-agreement samples, often exhibiting overconfidence in these ambiguous cases. However, utilizing disagreement samples in training improves both detection accuracy and model alignment with human judgment. These insights provide a foundation for enhancing LLM-based offensive language detection in real-world moderation tasks.

### C-3PO: Compact Plug-and-Play Proxy Optimization to Achieve Human-like Retrieval-Augmented Generation 
[[arxiv](https://arxiv.org/abs/2502.06205)] [[cool](https://papers.cool/arxiv/2502.06205)] [[pdf](https://arxiv.org/pdf/2502.06205)]
> **Authors**: Guoxin Chen,Minpeng Liao,Peiying Yu,Dingmin Wang,Zile Qiao,Chao Yang,Xin Zhao,Kai Fan
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Ongong work
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Retrieval-augmented generation (RAG) systems face a fundamental challenge in aligning independently developed retrievers and large language models (LLMs). Existing approaches typically involve modifying either component or introducing simple intermediate modules, resulting in practical limitations and sub-optimal performance. Inspired by human search behavior -- typically involving a back-and-forth process of proposing search queries and reviewing documents, we propose C-3PO, a proxy-centric framework that facilitates communication between retrievers and LLMs through a lightweight multi-agent system. Our framework implements three specialized agents that collaboratively optimize the entire RAG pipeline without altering the retriever and LLMs. These agents work together to assess the need for retrieval, generate effective queries, and select information suitable for the LLMs. To enable effective multi-agent coordination, we develop a tree-structured rollout approach for reward credit assignment in reinforcement learning. Extensive experiments in both in-domain and out-of-distribution scenarios demonstrate that C-3PO significantly enhances RAG performance while maintaining plug-and-play flexibility and superior generalization capabilities.

### Non-literal Understanding of Number Words by Language Models 
[[arxiv](https://arxiv.org/abs/2502.06204)] [[cool](https://papers.cool/arxiv/2502.06204)] [[pdf](https://arxiv.org/pdf/2502.06204)]
> **Authors**: Polina Tsvilodub,Kanishk Gandhi,Haoran Zhao,Jan-Philipp Fränken,Michael Franke,Noah D. Goodman
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 12 pages, 10 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Humans naturally interpret numbers non-literally, effortlessly combining context, world knowledge, and speaker intent. We investigate whether large language models (LLMs) interpret numbers similarly, focusing on hyperbole and pragmatic halo effects. Through systematic comparison with human data and computational models of pragmatic reasoning, we find that LLMs diverge from human interpretation in striking ways. By decomposing pragmatic reasoning into testable components, grounded in the Rational Speech Act framework, we pinpoint where LLM processing diverges from human cognition -- not in prior knowledge, but in reasoning with it. This insight leads us to develop a targeted solution -- chain-of-thought prompting inspired by an RSA model makes LLMs' interpretations more human-like. Our work demonstrates how computational cognitive models can both diagnose AI-human differences and guide development of more human-like language understanding capabilities.

### Discourse-Driven Evaluation: Unveiling Factual Inconsistency in Long Document Summarization 
[[arxiv](https://arxiv.org/abs/2502.06185)] [[cool](https://papers.cool/arxiv/2502.06185)] [[pdf](https://arxiv.org/pdf/2502.06185)]
> **Authors**: Yang Zhong,Diane Litman
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: NAACL 2025 camera-ready version
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Detecting factual inconsistency for long document summarization remains challenging, given the complex structure of the source article and long summary length. In this work, we study factual inconsistency errors and connect them with a line of discourse analysis. We find that errors are more common in complex sentences and are associated with several discourse features. We propose a framework that decomposes long texts into discourse-inspired chunks and utilizes discourse information to better aggregate sentence-level scores predicted by natural language inference models. Our approach shows improved performance on top of different model baselines over several evaluation benchmarks, covering rich domains of texts, focusing on long document summarization. This underscores the significance of incorporating discourse features in developing models for scoring summaries for long document factual inconsistency.

### RideKE: Leveraging Low-Resource, User-Generated Twitter Content for Sentiment and Emotion Detection in Kenyan Code-Switched Dataset 
[[arxiv](https://arxiv.org/abs/2502.06180)] [[cool](https://papers.cool/arxiv/2502.06180)] [[pdf](https://arxiv.org/pdf/2502.06180)]
> **Authors**: Naome A. Etori,Maria L. Gini
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Accepted in WASSA 2024
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Social media has become a crucial open-access platform for individuals to express opinions and share experiences. However, leveraging low-resource language data from Twitter is challenging due to scarce, poor-quality content and the major variations in language use, such as slang and code-switching. Identifying tweets in these languages can be difficult as Twitter primarily supports high-resource languages. We analyze Kenyan code-switched data and evaluate four state-of-the-art (SOTA) transformer-based pretrained models for sentiment and emotion classification, using supervised and semi-supervised methods. We detail the methodology behind data collection and annotation, and the challenges encountered during the data curation phase. Our results show that XLM-R outperforms other models; for sentiment analysis, XLM-R supervised model achieves the highest accuracy (69.2\%) and F1 score (66.1\%), XLM-R semi-supervised (67.2\% accuracy, 64.1\% F1 score). In emotion analysis, DistilBERT supervised leads in accuracy (59.8\%) and F1 score (31\%), mBERT semi-supervised (accuracy (59\% and F1 score 26.5\%). AfriBERTa models show the lowest accuracy and F1 scores. All models tend to predict neutral sentiment, with Afri-BERT showing the highest bias and unique sensitivity to empathy emotion. https://github.com/NEtori21/Ride_hailing

## 密码学和安全(cs.CR:Cryptography and Security)

### A Study on the Importance of Features in Detecting Advanced Persistent Threats Using Machine Learning 
[[arxiv](https://arxiv.org/abs/2502.07207)] [[cool](https://papers.cool/arxiv/2502.07207)] [[pdf](https://arxiv.org/pdf/2502.07207)]
> **Authors**: Ehsan Hallaji,Roozbeh Razavi-Far,Mehrdad Saif
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Accepted for publication in the 2024 International Conference on Computational Science and Computational Intelligence (CSCI'24)
- **标题**: None
- **领域**: 密码学和安全,人工智能,机器学习
- **Abstract**: Advanced Persistent Threats (APTs) pose a significant security risk to organizations and industries. These attacks often lead to severe data breaches and compromise the system for a long time. Mitigating these sophisticated attacks is highly challenging due to the stealthy and persistent nature of APTs. Machine learning models are often employed to tackle this challenge by bringing automation and scalability to APT detection. Nevertheless, these intelligent methods are data-driven, and thus, highly affected by the quality and relevance of input data. This paper aims to analyze measurements considered when recording network traffic and conclude which features contribute more to detecting APT samples. To do this, we study the features associated with various APT cases and determine their importance using a machine learning framework. To ensure the generalization of our findings, several feature selection techniques are employed and paired with different classifiers to evaluate their effectiveness. Our findings provide insights into how APT detection can be enhanced in real-world scenarios.

### SAFE: Self-Supervised Anomaly Detection Framework for Intrusion Detection 
[[arxiv](https://arxiv.org/abs/2502.07119)] [[cool](https://papers.cool/arxiv/2502.07119)] [[pdf](https://arxiv.org/pdf/2502.07119)]
> **Authors**: Elvin Li,Zhengli Shang,Onat Gungor,Tajana Rosing
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Accepted by the AAAI-25 Workshop on Artificial Intelligence for Cyber Security (AICS)
- **标题**: None
- **领域**: 密码学和安全,机器学习
- **Abstract**: The proliferation of IoT devices has significantly increased network vulnerabilities, creating an urgent need for effective Intrusion Detection Systems (IDS). Machine Learning-based IDS (ML-IDS) offer advanced detection capabilities but rely on labeled attack data, which limits their ability to identify unknown threats. Self-Supervised Learning (SSL) presents a promising solution by using only normal data to detect patterns and anomalies. This paper introduces SAFE, a novel framework that transforms tabular network intrusion data into an image-like format, enabling Masked Autoencoders (MAEs) to learn robust representations of network behavior. The features extracted by the MAEs are then incorporated into a lightweight novelty detector, enhancing the effectiveness of anomaly detection. Experimental results demonstrate that SAFE outperforms the state-of-the-art anomaly detection method, Scale Learning-based Deep Anomaly Detection method (SLAD), by up to 26.2% and surpasses the state-of-the-art SSL-based network intrusion detection approach, Anomal-E, by up to 23.5% in F1-score.

### LLMs in Software Security: A Survey of Vulnerability Detection Techniques and Insights 
[[arxiv](https://arxiv.org/abs/2502.07049)] [[cool](https://papers.cool/arxiv/2502.07049)] [[pdf](https://arxiv.org/pdf/2502.07049)]
> **Authors**: Ze Sheng,Zhicheng Chen,Shuning Gu,Heqing Huang,Guofei Gu,Jeff Huang
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 33 pages, 12 figures
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Large Language Models (LLMs) are emerging as transformative tools for software vulnerability detection, addressing critical challenges in the security domain. Traditional methods, such as static and dynamic analysis, often falter due to inefficiencies, high false positive rates, and the growing complexity of modern software systems. By leveraging their ability to analyze code structures, identify patterns, and generate repair suggestions, LLMs, exemplified by models like GPT, BERT, and CodeBERT, present a novel and scalable approach to mitigating vulnerabilities. This paper provides a detailed survey of LLMs in vulnerability detection. It examines key aspects, including model architectures, application methods, target languages, fine-tuning strategies, datasets, and evaluation metrics. We also analyze the scope of current research problems, highlighting the strengths and weaknesses of existing approaches. Further, we address challenges such as cross-language vulnerability detection, multimodal data integration, and repository-level analysis. Based on these findings, we propose solutions for issues like dataset scalability, model interpretability, and applications in low-resource scenarios. Our contributions are threefold: (1) a systematic review of how LLMs are applied in vulnerability detection; (2) an analysis of shared patterns and differences across studies, with a unified framework for understanding the field; and (3) a summary of key challenges and future research directions. This work provides valuable insights for advancing LLM-based vulnerability detection. We also maintain and regularly update latest selected paper on https://github.com/OwenSanzas/LLM-For-Vulnerability-Detection

### Scalable and Ethical Insider Threat Detection through Data Synthesis and Analysis by LLMs 
[[arxiv](https://arxiv.org/abs/2502.07045)] [[cool](https://papers.cool/arxiv/2502.07045)] [[pdf](https://arxiv.org/pdf/2502.07045)]
> **Authors**: Haywood Gelman,John D. Hastings
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 6 pages, 0 figures, 8 tables
- **标题**: None
- **领域**: 密码学和安全,人工智能,计算语言学,计算机与社会
- **Abstract**: Insider threats wield an outsized influence on organizations, disproportionate to their small numbers. This is due to the internal access insiders have to systems, information, and infrastructure. %One example of this influence is where anonymous respondents submit web-based job search site reviews, an insider threat risk to organizations. Signals for such risks may be found in anonymous submissions to public web-based job search site reviews. This research studies the potential for large language models (LLMs) to analyze and detect insider threat sentiment within job site reviews. Addressing ethical data collection concerns, this research utilizes synthetic data generation using LLMs alongside existing job review datasets. A comparative analysis of sentiment scores generated by LLMs is benchmarked against expert human scoring. Findings reveal that LLMs demonstrate alignment with human evaluations in most cases, thus effectively identifying nuanced indicators of threat sentiment. The performance is lower on human-generated data than synthetic data, suggesting areas for improvement in evaluating real-world data. Text diversity analysis found differences between human-generated and LLM-generated datasets, with synthetic data exhibiting somewhat lower diversity. Overall, the results demonstrate the applicability of LLMs to insider threat detection, and a scalable solution for insider sentiment testing by overcoming ethical and logistical barriers tied to data acquisition.

### Automated Consistency Analysis of LLMs 
[[arxiv](https://arxiv.org/abs/2502.07036)] [[cool](https://papers.cool/arxiv/2502.07036)] [[pdf](https://arxiv.org/pdf/2502.07036)]
> **Authors**: Aditya Patwardhan,Vivek Vaidya,Ashish Kundu
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 10 pages, 12 figures, 3 tables, 3 algorithms
- **标题**: None
- **领域**: 密码学和安全,人工智能,机器学习
- **Abstract**: Generative AI (Gen AI) with large language models (LLMs) are being widely adopted across the industry, academia and government. Cybersecurity is one of the key sectors where LLMs can be and/or are already being used. There are a number of problems that inhibit the adoption of trustworthy Gen AI and LLMs in cybersecurity and such other critical areas. One of the key challenge to the trustworthiness and reliability of LLMs is: how consistent an LLM is in its responses? In this paper, we have analyzed and developed a formal definition of consistency of responses of LLMs. We have formally defined what is consistency of responses and then develop a framework for consistency evaluation. The paper proposes two approaches to validate consistency: self-validation, and validation across multiple LLMs. We have carried out extensive experiments for several LLMs such as GPT4oMini, GPT3.5, Gemini, Cohere, and Llama3, on a security benchmark consisting of several cybersecurity questions: informational and situational. Our experiments corroborate the fact that even though these LLMs are being considered and/or already being used for several cybersecurity tasks today, they are often inconsistent in their responses, and thus are untrustworthy and unreliable for cybersecurity.

### Generating Privacy-Preserving Personalized Advice with Zero-Knowledge Proofs and LLMs 
[[arxiv](https://arxiv.org/abs/2502.06425)] [[cool](https://papers.cool/arxiv/2502.06425)] [[pdf](https://arxiv.org/pdf/2502.06425)]
> **Authors**: Hiroki Watanabe,Motonobu Uchikoshi
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Accepted to The ACM Web Conference (WWW) 2025 Short Paper Track
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Large language models (LLMs) are increasingly utilized in domains such as finance, healthcare, and interpersonal relationships to provide advice tailored to user traits and contexts. However, this personalization often relies on sensitive data, raising critical privacy concerns and necessitating data minimization. To address these challenges, we propose a framework that integrates zero-knowledge proof (ZKP) technology, specifically zkVM, with LLM-based chatbots. This integration enables privacy-preserving data sharing by verifying user traits without disclosing sensitive information. Our research introduces both an architecture and a prompting strategy for this approach. Through empirical evaluation, we clarify the current constraints and performance limitations of both zkVM and the proposed prompting strategy, thereby demonstrating their practical feasibility in real-world scenarios.

### AiRacleX: Automated Detection of Price Oracle Manipulations via LLM-Driven Knowledge Mining and Prompt Generation 
[[arxiv](https://arxiv.org/abs/2502.06348)] [[cool](https://papers.cool/arxiv/2502.06348)] [[pdf](https://arxiv.org/pdf/2502.06348)]
> **Authors**: Bo Gao,Yuan Wang,Qingsong Wei,Yong Liu,Rick Siow Mong Goh,David Lo
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Decentralized finance (DeFi) applications depend on accurate price oracles to ensure secure transactions, yet these oracles are highly vulnerable to manipulation, enabling attackers to exploit smart contract vulnerabilities for unfair asset valuation and financial gain. Detecting such manipulations traditionally relies on the manual effort of experienced experts, presenting significant challenges. In this paper, we propose a novel LLM-driven framework that automates the detection of price oracle manipulations by leveraging the complementary strengths of different LLM models (LLMs). Our approach begins with domain-specific knowledge extraction, where an LLM model synthesizes precise insights about price oracle vulnerabilities from top-tier academic papers, eliminating the need for profound expertise from developers or auditors. This knowledge forms the foundation for a second LLM model to generate structured, context-aware chain of thought prompts, which guide a third LLM model in accurately identifying manipulation patterns in smart contracts. We validate the effectiveness of framework through experiments on 60 known vulnerabilities from 46 real-world DeFi attacks or projects spanning 2021 to 2023. The best performing combination of LLMs (Haiku-Haiku-4o-mini) identified by AiRacleX demonstrate a 2.58-times improvement in recall (0.667 vs 0.259) compared to the state-of-the-art tool GPTScan, while maintaining comparable precision. Furthermore, our framework demonstrates the feasibility of replacing commercial models with open-source alternatives, enhancing privacy and security for developers.

## 计算机视觉和模式识别(cs.CV:Computer Vision and Pattern Recognition)

### MLLM4PUE: Toward Universal Embeddings in Computational Pathology through Multimodal LLMs 
[[arxiv](https://arxiv.org/abs/2502.07221)] [[cool](https://papers.cool/arxiv/2502.07221)] [[pdf](https://arxiv.org/pdf/2502.07221)]
> **Authors**: Qifeng Zhou,Thao M. Dang,Wenliang Zhong,Yuzhi Guo,Hehuan Ma,Saiyang Na,Junzhou Huang
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Pathology plays a critical role in diagnosing a wide range of diseases, yet existing approaches often rely heavily on task-specific models trained on extensive, well-labeled datasets. These methods face sustainability challenges due to the diversity of pathologies and the labor-intensive nature of data collection. To address these limitations, we highlight the need for universal multimodal embeddings that can support multiple downstream tasks. Previous approaches often involve fine-tuning CLIP-based models, which handle images and text separately, limiting their ability to capture complex multimodal relationships. Additionally, these models are evaluated across diverse datasets without a unified benchmark for assessing multimodal embeddings in pathology. To address these challenges, we propose MLLM4PUE, a novel framework that leverages Multimodal Large Language Models (MLLMs) to generate Pathology Universal Embeddings. The MLLM4PUE framework not only facilitates robust integration of images and text but also enhances understanding and fusion capabilities across various tasks. We further introduce the Pathology Multimodal Embedding Benchmark (PMEB), a comprehensive benchmark designed to assess the quality of pathology multimodal embeddings. PMEB comprises 15 original tasks drawn from 14 datasets, organized into three meta-tasks: retrieval, classification, and composed retrieval. Experimental results demonstrate the superiority of MLLM4PUE, illustrating MLLM-based models can effectively support a wide range of downstream tasks and unify the research direction for foundation models in pathology.

### SparseFormer: Detecting Objects in HRW Shots via Sparse Vision Transformer 
[[arxiv](https://arxiv.org/abs/2502.07216)] [[cool](https://papers.cool/arxiv/2502.07216)] [[pdf](https://arxiv.org/pdf/2502.07216)]
> **Authors**: Wenxi Li,Yuchen Guo,Jilai Zheng,Haozhe Lin,Chao Ma,Lu Fang,Xiaokang Yang
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: This paper is accepted to ACM MM 2024
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Recent years have seen an increase in the use of gigapixel-level image and video capture systems and benchmarks with high-resolution wide (HRW) shots. However, unlike close-up shots in the MS COCO dataset, the higher resolution and wider field of view raise unique challenges, such as extreme sparsity and huge scale changes, causing existing close-up detectors inaccuracy and inefficiency. In this paper, we present a novel model-agnostic sparse vision transformer, dubbed SparseFormer, to bridge the gap of object detection between close-up and HRW shots. The proposed SparseFormer selectively uses attentive tokens to scrutinize the sparsely distributed windows that may contain objects. In this way, it can jointly explore global and local attention by fusing coarse- and fine-grained features to handle huge scale changes. SparseFormer also benefits from a novel Cross-slice non-maximum suppression (C-NMS) algorithm to precisely localize objects from noisy windows and a simple yet effective multi-scale strategy to improve accuracy. Extensive experiments on two HRW benchmarks, PANDA and DOTA-v1.0, demonstrate that the proposed SparseFormer significantly improves detection accuracy (up to 5.8%) and speed (up to 3x) over the state-of-the-art approaches.

### Dense Object Detection Based on De-homogenized Queries 
[[arxiv](https://arxiv.org/abs/2502.07194)] [[cool](https://papers.cool/arxiv/2502.07194)] [[pdf](https://arxiv.org/pdf/2502.07194)]
> **Authors**: Yueming Huang,Chenrui Ma,Hao Zhou,Hao Wu,Guowu Yuan
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 17 pages, 15 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Dense object detection is widely used in automatic driving, video surveillance, and other fields. This paper focuses on the challenging task of dense object detection. Currently, detection methods based on greedy algorithms, such as non-maximum suppression (NMS), often produce many repetitive predictions or missed detections in dense scenarios, which is a common problem faced by NMS-based algorithms. Through the end-to-end DETR (DEtection TRansformer), as a type of detector that can incorporate the post-processing de-duplication capability of NMS, etc., into the network, we found that homogeneous queries in the query-based detector lead to a reduction in the de-duplication capability of the network and the learning efficiency of the encoder, resulting in duplicate prediction and missed detection problems. To solve this problem, we propose learnable differentiated encoding to de-homogenize the queries, and at the same time, queries can communicate with each other via differentiated encoding information, replacing the previous self-attention among the queries. In addition, we used joint loss on the output of the encoder that considered both location and confidence prediction to give a higher-quality initialization for queries. Without cumbersome decoder stacking and guaranteeing accuracy, our proposed end-to-end detection framework was more concise and reduced the number of parameters by about 8% compared to deformable DETR. Our method achieved excellent results on the challenging CrowdHuman dataset with 93.6% average precision (AP), 39.2% MR-2, and 84.3% JI. The performance overperformed previous SOTA methods, such as Iter-E2EDet (Progressive End-to-End Object Detection) and MIP (One proposal, Multiple predictions). In addition, our method is more robust in various scenarios with different densities.

### OscNet: Machine Learning on CMOS Oscillator Networks 
[[arxiv](https://arxiv.org/abs/2502.07192)] [[cool](https://papers.cool/arxiv/2502.07192)] [[pdf](https://arxiv.org/pdf/2502.07192)]
> **Authors**: Wenxiao Cai,Thomas H. Lee
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Machine learning and AI have achieved remarkable advancements but at the cost of significant computational resources and energy consumption. This has created an urgent need for a novel, energy-efficient computational fabric to replace the current computing pipeline. Recently, a promising approach has emerged by mimicking spiking neurons in the brain and leveraging oscillators on CMOS for direct computation. In this context, we propose a new and energy efficient machine learning framework implemented on CMOS Oscillator Networks (OscNet). We model the developmental processes of the prenatal brain's visual system using OscNet, updating weights based on the biologically inspired Hebbian rule. This same pipeline is then directly applied to standard machine learning tasks. OscNet is a specially designed hardware and is inherently energy-efficient. Its reliance on forward propagation alone for training further enhances its energy efficiency while maintaining biological plausibility. Simulation validates our designs of OscNet architectures. Experimental results demonstrate that Hebbian learning pipeline on OscNet achieves performance comparable to or even surpassing traditional machine learning algorithms, highlighting its potential as a energy efficient and effective computational paradigm.

### A Survey on Mamba Architecture for Vision Applications 
[[arxiv](https://arxiv.org/abs/2502.07161)] [[cool](https://papers.cool/arxiv/2502.07161)] [[pdf](https://arxiv.org/pdf/2502.07161)]
> **Authors**: Fady Ibrahim,Guangjun Liu,Guanghui Wang
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Transformers have become foundational for visual tasks such as object detection, semantic segmentation, and video understanding, but their quadratic complexity in attention mechanisms presents scalability challenges. To address these limitations, the Mamba architecture utilizes state-space models (SSMs) for linear scalability, efficient processing, and improved contextual awareness. This paper investigates Mamba architecture for visual domain applications and its recent advancements, including Vision Mamba (ViM) and VideoMamba, which introduce bidirectional scanning, selective scanning mechanisms, and spatiotemporal processing to enhance image and video understanding. Architectural innovations like position embeddings, cross-scan modules, and hierarchical designs further optimize the Mamba framework for global and local feature extraction. These advancements position Mamba as a promising architecture in computer vision research and applications.

### Explaining 3D Computed Tomography Classifiers with Counterfactuals 
[[arxiv](https://arxiv.org/abs/2502.07156)] [[cool](https://papers.cool/arxiv/2502.07156)] [[pdf](https://arxiv.org/pdf/2502.07156)]
> **Authors**: Joseph Paul Cohen,Louis Blankemeier,Akshay Chaudhari
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Code and models: https://github.com/ieee8023/ct-counterfactuals
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Counterfactual explanations in medical imaging are critical for understanding the predictions made by deep learning models. We extend the Latent Shift counterfactual generation method from 2D applications to 3D computed tomography (CT) scans. We address the challenges associated with 3D data, such as limited training samples and high memory demands, by implementing a slice-based approach. This method leverages a 2D encoder trained on CT slices, which are subsequently combined to maintain 3D context. We demonstrate this technique on two models for clinical phenotype prediction and lung segmentation. Our approach is both memory-efficient and effective for generating interpretable counterfactuals in high-resolution 3D medical imaging.

### Mesh2SSM++: A Probabilistic Framework for Unsupervised Learning of Statistical Shape Model of Anatomies from Surface Meshes 
[[arxiv](https://arxiv.org/abs/2502.07145)] [[cool](https://papers.cool/arxiv/2502.07145)] [[pdf](https://arxiv.org/pdf/2502.07145)]
> **Authors**: Krithika Iyer,Mokshagna Sai Teja Karanam,Shireen Elhabian
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Anatomy evaluation is crucial for understanding the physiological state, diagnosing abnormalities, and guiding medical interventions. Statistical shape modeling (SSM) is vital in this process. By enabling the extraction of quantitative morphological shape descriptors from MRI and CT scans, SSM provides comprehensive descriptions of anatomical variations within a population. However, the effectiveness of SSM in anatomy evaluation hinges on the quality and robustness of the shape models. While deep learning techniques show promise in addressing these challenges by learning complex nonlinear representations of shapes, existing models still have limitations and often require pre-established shape models for training. To overcome these issues, we propose Mesh2SSM++, a novel approach that learns to estimate correspondences from meshes in an unsupervised manner. This method leverages unsupervised, permutation-invariant representation learning to estimate how to deform a template point cloud into subject-specific meshes, forming a correspondence-based shape model. Additionally, our probabilistic formulation allows learning a population-specific template, reducing potential biases associated with template selection. A key feature of Mesh2SSM++ is its ability to quantify aleatoric uncertainty, which captures inherent data variability and is essential for ensuring reliable model predictions and robust decision-making in clinical tasks, especially under challenging imaging conditions. Through extensive validation across diverse anatomies, evaluation metrics, and downstream tasks, we demonstrate that Mesh2SSM++ outperforms existing methods. Its ability to operate directly on meshes, combined with computational efficiency and interpretability through its probabilistic framework, makes it an attractive alternative to traditional and deep learning-based SSM approaches.

### Towards a Robust Framework for Multimodal Hate Detection: A Study on Video vs. Image-based Content 
[[arxiv](https://arxiv.org/abs/2502.07138)] [[cool](https://papers.cool/arxiv/2502.07138)] [[pdf](https://arxiv.org/pdf/2502.07138)]
> **Authors**: Girish A. Koushik,Diptesh Kanojia,Helen Treharne
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Accepted to the MM4SG Workshop at the WebConf 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算语言学,机器学习
- **Abstract**: Social media platforms enable the propagation of hateful content across different modalities such as textual, auditory, and visual, necessitating effective detection methods. While recent approaches have shown promise in handling individual modalities, their effectiveness across different modality combinations remains unexplored. This paper presents a systematic analysis of fusion-based approaches for multimodal hate detection, focusing on their performance across video and image-based content. Our comprehensive evaluation reveals significant modality-specific limitations: while simple embedding fusion achieves state-of-the-art performance on video content (HateMM dataset) with a 9.9% points F1-score improvement, it struggles with complex image-text relationships in memes (Hateful Memes dataset). Through detailed ablation studies and error analysis, we demonstrate how current fusion approaches fail to capture nuanced cross-modal interactions, particularly in cases involving benign confounders. Our findings provide crucial insights for developing more robust hate detection systems and highlight the need for modality-specific architectural considerations. The code is available at https://github.com/gak97/Video-vs-Meme-Hate.

### Unconstrained Body Recognition at Altitude and Range: Comparing Four Approaches 
[[arxiv](https://arxiv.org/abs/2502.07130)] [[cool](https://papers.cool/arxiv/2502.07130)] [[pdf](https://arxiv.org/pdf/2502.07130)]
> **Authors**: Blake A Myers,Matthew Q Hill,Veda Nandan Gandi,Thomas M Metz,Alice J O'Toole
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: This study presents an investigation of four distinct approaches to long-term person identification using body shape. Unlike short-term re-identification systems that rely on temporary features (e.g., clothing), we focus on learning persistent body shape characteristics that remain stable over time. We introduce a body identification model based on a Vision Transformer (ViT) (Body Identification from Diverse Datasets, BIDDS) and on a Swin-ViT model (Swin-BIDDS). We also expand on previous approaches based on the Linguistic and Non-linguistic Core ResNet Identity Models (LCRIM and NLCRIM), but with improved training. All models are trained on a large and diverse dataset of over 1.9 million images of approximately 5k identities across 9 databases. Performance was evaluated on standard re-identification benchmark datasets (MARS, MSMT17, Outdoor Gait, DeepChange) and on an unconstrained dataset that includes images at a distance (from close-range to 1000m), at altitude (from an unmanned aerial vehicle, UAV), and with clothing change. A comparative analysis across these models provides insights into how different backbone architectures and input image sizes impact long-term body identification performance across real-world conditions.

### Is Long Range Sequential Modeling Necessary For Colorectal Tumor Segmentation? 
[[arxiv](https://arxiv.org/abs/2502.07120)] [[cool](https://papers.cool/arxiv/2502.07120)] [[pdf](https://arxiv.org/pdf/2502.07120)]
> **Authors**: Abhishek Srivastava,Koushik Biswas,Gorkem Durak,Gulsah Ozden,Mustafa Adli,Ulas Bagci
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 5 pages, 1 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Segmentation of colorectal cancer (CRC) tumors in 3D medical imaging is both complex and clinically critical, providing vital support for effective radiation therapy planning and survival outcome assessment. Recently, 3D volumetric segmentation architectures incorporating long-range sequence modeling mechanisms, such as Transformers and Mamba, have gained attention for their capacity to achieve high accuracy in 3D medical image segmentation. In this work, we evaluate the effectiveness of these global token modeling techniques by pitting them against our proposed MambaOutUNet within the context of our newly introduced colorectal tumor segmentation dataset (CTS-204). Our findings suggest that robust local token interactions can outperform long-range modeling techniques in cases where the region of interest is small and anatomically complex, proposing a potential shift in 3D tumor segmentation research.

### PrismAvatar: Real-time animated 3D neural head avatars on edge devices 
[[arxiv](https://arxiv.org/abs/2502.07030)] [[cool](https://papers.cool/arxiv/2502.07030)] [[pdf](https://arxiv.org/pdf/2502.07030)]
> **Authors**: Prashant Raina,Felix Taubner,Mathieu Tuli,Eu Wern Teh,Kevin Ferreira
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 8 pages, 5 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,图形,机器学习
- **Abstract**: We present PrismAvatar: a 3D head avatar model which is designed specifically to enable real-time animation and rendering on resource-constrained edge devices, while still enjoying the benefits of neural volumetric rendering at training time. By integrating a rigged prism lattice with a 3D morphable head model, we use a hybrid rendering model to simultaneously reconstruct a mesh-based head and a deformable NeRF model for regions not represented by the 3DMM. We then distill the deformable NeRF into a rigged mesh and neural textures, which can be animated and rendered efficiently within the constraints of the traditional triangle rendering pipeline. In addition to running at 60 fps with low memory usage on mobile devices, we find that our trained models have comparable quality to state-of-the-art 3D avatar models on desktop devices.

### Early Operative Difficulty Assessment in Laparoscopic Cholecystectomy via Snapshot-Centric Video Analysis 
[[arxiv](https://arxiv.org/abs/2502.07008)] [[cool](https://papers.cool/arxiv/2502.07008)] [[pdf](https://arxiv.org/pdf/2502.07008)]
> **Authors**: Saurav Sharma,Maria Vannucci,Leonardo Pestana Legori,Mario Scaglia,Giovanni Guglielmo Laracca,Didier Mutter,Sergio Alfieri,Pietro Mascagni,Nicolas Padoy
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Accepted at IPCAI, 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Purpose: Laparoscopic cholecystectomy (LC) operative difficulty (LCOD) is highly variable and influences outcomes. Despite extensive LC studies in surgical workflow analysis, limited efforts explore LCOD using intraoperative video data. Early recognition of LCOD could allow prompt review by expert surgeons, enhance operating room (OR) planning, and improve surgical outcomes. Methods: We propose the clinical task of early LCOD assessment using limited video observations. We design SurgPrOD, a deep learning model to assess LCOD by analyzing features from global and local temporal resolutions (snapshots) of the observed LC video. Also, we propose a novel snapshot-centric attention (SCA) module, acting across snapshots, to enhance LCOD prediction. We introduce the CholeScore dataset, featuring video-level LCOD labels to validate our method. Results: We evaluate SurgPrOD on 3 LCOD assessment scales in the CholeScore dataset. On our new metric assessing early and stable correct predictions, SurgPrOD surpasses baselines by at least 0.22 points. SurgPrOD improves over baselines by at least 9 and 5 percentage points in F1 score and top1-accuracy, respectively, demonstrating its effectiveness in correct predictions. Conclusion: We propose a new task for early LCOD assessment and a novel model, SurgPrOD analyzing surgical video from global and local perspectives. Our results on the CholeScore dataset establishes a new benchmark to study LCOD using intraoperative video data.

### Grounding Creativity in Physics: A Brief Survey of Physical Priors in AIGC 
[[arxiv](https://arxiv.org/abs/2502.07007)] [[cool](https://papers.cool/arxiv/2502.07007)] [[pdf](https://arxiv.org/pdf/2502.07007)]
> **Authors**: Siwei Meng,Yawei Luo,Ping Liu
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Recent advancements in AI-generated content have significantly improved the realism of 3D and 4D generation. However, most existing methods prioritize appearance consistency while neglecting underlying physical principles, leading to artifacts such as unrealistic deformations, unstable dynamics, and implausible objects interactions. Incorporating physics priors into generative models has become a crucial research direction to enhance structural integrity and motion realism. This survey provides a review of physics-aware generative methods, systematically analyzing how physical constraints are integrated into 3D and 4D generation. First, we examine recent works in incorporating physical priors into static and dynamic 3D generation, categorizing methods based on representation types, including vision-based, NeRF-based, and Gaussian Splatting-based approaches. Second, we explore emerging techniques in 4D generation, focusing on methods that model temporal dynamics with physical simulations. Finally, we conduct a comparative analysis of major methods, highlighting their strengths, limitations, and suitability for different materials and motion dynamics. By presenting an in-depth analysis of physics-grounded AIGC, this survey aims to bridge the gap between generative models and physical realism, providing insights that inspire future research in physically consistent content generation.

### From Image to Video: An Empirical Study of Diffusion Representations 
[[arxiv](https://arxiv.org/abs/2502.07001)] [[cool](https://papers.cool/arxiv/2502.07001)] [[pdf](https://arxiv.org/pdf/2502.07001)]
> **Authors**: Pedro Vélez,Luisa F. Polanía,Yi Yang,Chuhan Zhang,Rishab Kabra,Anurag Arnab,Mehdi S. M. Sajjadi
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: Diffusion models have revolutionized generative modeling, enabling unprecedented realism in image and video synthesis. This success has sparked interest in leveraging their representations for visual understanding tasks. While recent works have explored this potential for image generation, the visual understanding capabilities of video diffusion models remain largely uncharted. To address this gap, we systematically compare the same model architecture trained for video versus image generation, analyzing the performance of their latent representations on various downstream tasks including image classification, action recognition, depth estimation, and tracking. Results show that video diffusion models consistently outperform their image counterparts, though we find a striking range in the extent of this superiority. We further analyze features extracted from different layers and with varying noise levels, as well as the effect of model size and training budget on representation and generation quality. This work marks the first direct comparison of video and image diffusion objectives for visual understanding, offering insights into the role of temporal information in representation learning.

### AI-Driven HSI: Multimodality, Fusion, Challenges, and the Deep Learning Revolution 
[[arxiv](https://arxiv.org/abs/2502.06894)] [[cool](https://papers.cool/arxiv/2502.06894)] [[pdf](https://arxiv.org/pdf/2502.06894)]
> **Authors**: David S. Bhatti,Yougin Choi,Rahman S M Wahidur,Maleeka Bakhtawar,Sumin Kim,Surin Lee,Yongtae Lee,Heung-No Lee
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-11
> **comment**: 39 Pages, 22 figures, 20 tables
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Hyperspectral imaging (HSI) captures spatial and spectral data, enabling analysis of features invisible to conventional systems. The technology is vital in fields such as weather monitoring, food quality control, counterfeit detection, healthcare diagnostics, and extending into defense, agriculture, and industrial automation at the same time. HSI has advanced with improvements in spectral resolution, miniaturization, and computational methods. This study provides an overview of the HSI, its applications, challenges in data fusion and the role of deep learning models in processing HSI data. We discuss how integration of multimodal HSI with AI, particularly with deep learning, improves classification accuracy and operational efficiency. Deep learning enhances HSI analysis in areas like feature extraction, change detection, denoising unmixing, dimensionality reduction, landcover mapping, data augmentation, spectral construction and super resolution. An emerging focus is the fusion of hyperspectral cameras with large language models (LLMs), referred as highbrain LLMs, enabling the development of advanced applications such as low visibility crash detection and face antispoofing. We also highlight key players in HSI industry, its compound annual growth rate and the growing industrial significance. The purpose is to offer insight to both technical and non-technical audience, covering HSI's images, trends, and future directions, while providing valuable information on HSI datasets and software libraries.

### A New Hybrid Intelligent Approach for Multimodal Detection of Suspected Disinformation on TikTok 
[[arxiv](https://arxiv.org/abs/2502.06893)] [[cool](https://papers.cool/arxiv/2502.06893)] [[pdf](https://arxiv.org/pdf/2502.06893)]
> **Authors**: Jared D. T. Guerrero-Sosa,Andres Montoro-Montarroso,Francisco P. Romero,Jesus Serrano-Guerrero,Jose A. Olivas
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算语言学,多媒体,符号计算
- **Abstract**: In the context of the rapid dissemination of multimedia content, identifying disinformation on social media platforms such as TikTok represents a significant challenge. This study introduces a hybrid framework that combines the computational power of deep learning with the interpretability of fuzzy logic to detect suspected disinformation in TikTok videos. The methodology is comprised of two core components: a multimodal feature analyser that extracts and evaluates data from text, audio, and video; and a multimodal disinformation detector based on fuzzy logic. These systems operate in conjunction to evaluate the suspicion of spreading disinformation, drawing on human behavioural cues such as body language, speech patterns, and text coherence. Two experiments were conducted: one focusing on context-specific disinformation and the other on the scalability of the model across broader topics. For each video evaluated, high-quality, comprehensive, well-structured reports are generated, providing a detailed view of the disinformation behaviours.

### Beyond Vision: How Large Language Models Interpret Facial Expressions from Valence-Arousal Values 
[[arxiv](https://arxiv.org/abs/2502.06875)] [[cool](https://papers.cool/arxiv/2502.06875)] [[pdf](https://arxiv.org/pdf/2502.06875)]
> **Authors**: Vaibhav Mehra,Guy Laban,Hatice Gunes
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **Abstract**: Large Language Models primarily operate through text-based inputs and outputs, yet human emotion is communicated through both verbal and non-verbal cues, including facial expressions. While Vision-Language Models analyze facial expressions from images, they are resource-intensive and may depend more on linguistic priors than visual understanding. To address this, this study investigates whether LLMs can infer affective meaning from dimensions of facial expressions-Valence and Arousal values, structured numerical representations, rather than using raw visual input. VA values were extracted using Facechannel from images of facial expressions and provided to LLMs in two tasks: (1) categorizing facial expressions into basic (on the IIMI dataset) and complex emotions (on the Emotic dataset) and (2) generating semantic descriptions of facial expressions (on the Emotic dataset). Results from the categorization task indicate that LLMs struggle to classify VA values into discrete emotion categories, particularly for emotions beyond basic polarities (e.g., happiness, sadness). However, in the semantic description task, LLMs produced textual descriptions that align closely with human-generated interpretations, demonstrating a stronger capacity for free text affective inference of facial expressions.

### BF-GAN: Development of an AI-driven Bubbly Flow Image Generation Model Using Generative Adversarial Networks 
[[arxiv](https://arxiv.org/abs/2502.06863)] [[cool](https://papers.cool/arxiv/2502.06863)] [[pdf](https://arxiv.org/pdf/2502.06863)]
> **Authors**: Wen Zhou,Shuichiro Miwa,Yang Liu,Koji Okamoto
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: A generative AI architecture called bubbly flow generative adversarial networks (BF-GAN) is developed, designed to generate realistic and high-quality bubbly flow images through physically conditioned inputs, jg and jf. Initially, 52 sets of bubbly flow experiments under varying conditions are conducted to collect 140,000 bubbly flow images with physical labels of jg and jf for training data. A multi-scale loss function is then developed, incorporating mismatch loss and pixel loss to enhance the generative performance of BF-GAN further. Regarding evaluative metrics of generative AI, the BF-GAN has surpassed conventional GAN. Physically, key parameters of bubbly flow generated by BF-GAN are extracted and compared with measurement values and empirical correlations, validating BF-GAN's generative performance. The comparative analysis demonstrate that the BF-GAN can generate realistic and high-quality bubbly flow images with any given jg and jf within the research scope. BF-GAN offers a generative AI solution for two-phase flow research, substantially lowering the time and cost required to obtain high-quality data. In addition, it can function as a benchmark dataset generator for bubbly flow detection and segmentation algorithms, enhancing overall productivity in this research domain. The BF-GAN model is available online (https://github.com/zhouzhouwen/BF-GAN).

### AutoSketch: VLM-assisted Style-Aware Vector Sketch Completion 
[[arxiv](https://arxiv.org/abs/2502.06860)] [[cool](https://papers.cool/arxiv/2502.06860)] [[pdf](https://arxiv.org/pdf/2502.06860)]
> **Authors**: Hsiao-Yuan Chin,I-Chao Shen,Yi-Ting Chiu,Bing-Yu Chen
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-11
> **comment**: 11 pages
- **标题**: None
- **领域**: 计算机视觉和模式识别,图形
- **Abstract**: The ability to automatically complete a partial sketch that depicts a complex scene, e.g., "a woman chatting with a man in the park", is very useful. However, existing sketch generation methods create sketches from scratch; they do not complete a partial sketch in the style of the original. To address this challenge, we introduce AutoSketch, a styleaware vector sketch completion method that accommodates diverse sketch styles. Our key observation is that the style descriptions of a sketch in natural language preserve the style during automatic sketch completion. Thus, we use a pretrained vision-language model (VLM) to describe the styles of the partial sketches in natural language and replicate these styles using newly generated strokes. We initially optimize the strokes to match an input prompt augmented by style descriptions extracted from the VLM. Such descriptions allow the method to establish a diffusion prior in close alignment with that of the partial sketch. Next, we utilize the VLM to generate an executable style adjustment code that adjusts the strokes to conform to the desired style. We compare our method with existing methods across various sketch styles and prompts, performed extensive ablation studies and qualitative and quantitative evaluations, and demonstrate that AutoSketch can support various sketch scenarios.

### Vision-Integrated LLMs for Autonomous Driving Assistance : Human Performance Comparison and Trust Evaluation 
[[arxiv](https://arxiv.org/abs/2502.06843)] [[cool](https://papers.cool/arxiv/2502.06843)] [[pdf](https://arxiv.org/pdf/2502.06843)]
> **Authors**: Namhee Kim,Woojin Park
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,人机交互
- **Abstract**: Traditional autonomous driving systems often struggle with reasoning in complex, unexpected scenarios due to limited comprehension of spatial relationships. In response, this study introduces a Large Language Model (LLM)-based Autonomous Driving (AD) assistance system that integrates a vision adapter and an LLM reasoning module to enhance visual understanding and decision-making. The vision adapter, combining YOLOv4 and Vision Transformer (ViT), extracts comprehensive visual features, while GPT-4 enables human-like spatial reasoning and response generation. Experimental evaluations with 45 experienced drivers revealed that the system closely mirrors human performance in describing situations and moderately aligns with human decisions in generating appropriate responses.

### EVEv2: Improved Baselines for Encoder-Free Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2502.06788)] [[cool](https://papers.cool/arxiv/2502.06788)] [[pdf](https://arxiv.org/pdf/2502.06788)]
> **Authors**: Haiwen Diao,Xiaotong Li,Yufeng Cui,Yueze Wang,Haoge Deng,Ting Pan,Wenxuan Wang,Huchuan Lu,Xinlong Wang
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 19 pages, 9 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Existing encoder-free vision-language models (VLMs) are rapidly narrowing the performance gap with their encoder-based counterparts, highlighting the promising potential for unified multimodal systems with structural simplicity and efficient deployment. We systematically clarify the performance gap between VLMs using pre-trained vision encoders, discrete tokenizers, and minimalist visual layers from scratch, deeply excavating the under-examined characteristics of encoder-free VLMs. We develop efficient strategies for encoder-free VLMs that rival mainstream encoder-based ones. After an in-depth investigation, we launch EVEv2.0, a new and improved family of encoder-free VLMs. We show that: (i) Properly decomposing and hierarchically associating vision and language within a unified model reduces interference between modalities. (ii) A well-designed training strategy enables effective optimization for encoder-free VLMs. Through extensive evaluation, our EVEv2.0 represents a thorough study for developing a decoder-only architecture across modalities, demonstrating superior data efficiency and strong vision-reasoning capability. Code is publicly available at: https://github.com/baaivision/EVE.

### Visual Agentic AI for Spatial Reasoning with a Dynamic API 
[[arxiv](https://arxiv.org/abs/2502.06787)] [[cool](https://papers.cool/arxiv/2502.06787)] [[pdf](https://arxiv.org/pdf/2502.06787)]
> **Authors**: Damiano Marsili,Rohun Agrawal,Yisong Yue,Georgia Gkioxari
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Project website: https://glab-caltech.github.io/vadar/
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Visual reasoning -- the ability to interpret the visual world -- is crucial for embodied agents that operate within three-dimensional scenes. Progress in AI has led to vision and language models capable of answering questions from images. However, their performance declines when tasked with 3D spatial reasoning. To tackle the complexity of such reasoning problems, we introduce an agentic program synthesis approach where LLM agents collaboratively generate a Pythonic API with new functions to solve common subproblems. Our method overcomes limitations of prior approaches that rely on a static, human-defined API, allowing it to handle a wider range of queries. To assess AI capabilities for 3D understanding, we introduce a new benchmark of queries involving multiple steps of grounding and inference. We show that our method outperforms prior zero-shot models for visual reasoning in 3D and empirically validate the effectiveness of our agentic framework for 3D spatial reasoning tasks. Project website: https://glab-caltech.github.io/vadar/

### Lumina-Video: Efficient and Flexible Video Generation with Multi-scale Next-DiT 
[[arxiv](https://arxiv.org/abs/2502.06782)] [[cool](https://papers.cool/arxiv/2502.06782)] [[pdf](https://arxiv.org/pdf/2502.06782)]
> **Authors**: Dongyang Liu,Shicheng Li,Yutong Liu,Zhen Li,Kai Wang,Xinyue Li,Qi Qin,Yufei Liu,Yi Xin,Zhongyu Li,Bin Fu,Chenyang Si,Yuewen Cao,Conghui He,Ziwei Liu,Yu Qiao,Qibin Hou,Hongsheng Li,Peng Gao
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Recent advancements have established Diffusion Transformers (DiTs) as a dominant framework in generative modeling. Building on this success, Lumina-Next achieves exceptional performance in the generation of photorealistic images with Next-DiT. However, its potential for video generation remains largely untapped, with significant challenges in modeling the spatiotemporal complexity inherent to video data. To address this, we introduce Lumina-Video, a framework that leverages the strengths of Next-DiT while introducing tailored solutions for video synthesis. Lumina-Video incorporates a Multi-scale Next-DiT architecture, which jointly learns multiple patchifications to enhance both efficiency and flexibility. By incorporating the motion score as an explicit condition, Lumina-Video also enables direct control of generated videos' dynamic degree. Combined with a progressive training scheme with increasingly higher resolution and FPS, and a multi-source training scheme with mixed natural and synthetic data, Lumina-Video achieves remarkable aesthetic quality and motion smoothness at high training and inference efficiency. We additionally propose Lumina-V2A, a video-to-audio model based on Next-DiT, to create synchronized sounds for generated videos. Codes are released at https://www.github.com/Alpha-VLLM/Lumina-Video.

### Accelerating Data Processing and Benchmarking of AI Models for Pathology 
[[arxiv](https://arxiv.org/abs/2502.06750)] [[cool](https://papers.cool/arxiv/2502.06750)] [[pdf](https://arxiv.org/pdf/2502.06750)]
> **Authors**: Andrew Zhang,Guillaume Jaume,Anurag Vaidya,Tong Ding,Faisal Mahmood
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Advances in foundation modeling have reshaped computational pathology. However, the increasing number of available models and lack of standardized benchmarks make it increasingly complex to assess their strengths, limitations, and potential for further development. To address these challenges, we introduce a new suite of software tools for whole-slide image processing, foundation model benchmarking, and curated publicly available tasks. We anticipate that these resources will promote transparency, reproducibility, and continued progress in the field.

### Wandering around: A bioinspired approach to visual attention through object motion sensitivity 
[[arxiv](https://arxiv.org/abs/2502.06747)] [[cool](https://papers.cool/arxiv/2502.06747)] [[pdf](https://arxiv.org/pdf/2502.06747)]
> **Authors**: Giulia D Angelo,Victoria Clerico,Chiara Bartolozzi,Matej Hoffmann,P. Michael Furlong,Alexander Hadjiivanov
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Active vision enables dynamic visual perception, offering an alternative to static feedforward architectures in computer vision, which rely on large datasets and high computational resources. Biological selective attention mechanisms allow agents to focus on salient Regions of Interest (ROIs), reducing computational demand while maintaining real-time responsiveness. Event-based cameras, inspired by the mammalian retina, enhance this capability by capturing asynchronous scene changes enabling efficient low-latency processing. To distinguish moving objects while the event-based camera is in motion the agent requires an object motion segmentation mechanism to accurately detect targets and center them in the visual field (fovea). Integrating event-based sensors with neuromorphic algorithms represents a paradigm shift, using Spiking Neural Networks to parallelize computation and adapt to dynamic environments. This work presents a Spiking Convolutional Neural Network bioinspired attention system for selective attention through object motion sensitivity. The system generates events via fixational eye movements using a Dynamic Vision Sensor integrated into the Speck neuromorphic hardware, mounted on a Pan-Tilt unit, to identify the ROI and saccade toward it. The system, characterized using ideal gratings and benchmarked against the Event Camera Motion Segmentation Dataset, reaches a mean IoU of 82.2% and a mean SSIM of 96% in multi-object motion segmentation. The detection of salient objects reaches 88.8% accuracy in office scenarios and 89.8% in low-light conditions on the Event-Assisted Low-Light Video Object Segmentation Dataset. A real-time demonstrator shows the system's 0.12 s response to dynamic scenes. Its learning-free design ensures robustness across perceptual scenes, making it a reliable foundation for real-time robotic applications serving as a basis for more complex architectures.

### ViSIR: Vision Transformer Single Image Reconstruction Method for Earth System Models 
[[arxiv](https://arxiv.org/abs/2502.06741)] [[cool](https://papers.cool/arxiv/2502.06741)] [[pdf](https://arxiv.org/pdf/2502.06741)]
> **Authors**: Ehsan Zeraatkar,Salah Faroughi,Jelena Tešić
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Purpose: Earth system models (ESMs) integrate the interactions of the atmosphere, ocean, land, ice, and biosphere to estimate the state of regional and global climate under a wide variety of conditions. The ESMs are highly complex, and thus, deep neural network architectures are used to model the complexity and store the down-sampled data. In this paper, we propose the Vision Transformer Sinusoidal Representation Networks (ViSIR) to improve the single image SR (SR) reconstruction task for the ESM data. Methods: ViSIR combines the SR capability of Vision Transformers (ViT) with the high-frequency detail preservation of the Sinusoidal Representation Network (SIREN) to address the spectral bias observed in SR tasks. Results: The ViSIR outperforms ViT by 4.1 dB, SIREN by 7.5 dB, and SR-Generative Adversarial (SR-GANs) by 7.1dB PSNR on average for three different measurements. Conclusion: The proposed ViSIR is evaluated and compared with state-of-the-art methods. The results show that the proposed algorithm is outperforming other methods in terms of Mean Square Error(MSE), Peak-Signal-to-Noise-Ratio(PSNR), and Structural Similarity Index Measure(SSIM).

### Enhancing Pneumonia Diagnosis and Severity Assessment through Deep Learning: A Comprehensive Approach Integrating CNN Classification and Infection Segmentation 
[[arxiv](https://arxiv.org/abs/2502.06735)] [[cool](https://papers.cool/arxiv/2502.06735)] [[pdf](https://arxiv.org/pdf/2502.06735)]
> **Authors**: S Kumar Reddy Mallidi
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Lung disease poses a substantial global health challenge, with pneumonia being a prevalent concern. This research focuses on leveraging deep learning techniques to detect and assess pneumonia, addressing two interconnected objectives. Initially, Convolutional Neural Network (CNN) models are introduced for pneumonia classification, emphasizing the necessity of comprehensive diagnostic assessments considering COVID-19. Subsequently, the study advocates for the utilization of deep learning-based segmentation to determine the severity of infection. This dual-pronged approach offers valuable insights for medical professionals, facilitating a more nuanced understanding and effective treatment of pneumonia. Integrating deep learning aims to elevate the accuracy and efficiency of pneumonia detection, thereby contributing to enhanced healthcare outcomes on a global scale.

### TEMSET-24K: Densely Annotated Dataset for Indexing Multipart Endoscopic Videos using Surgical Timeline Segmentation 
[[arxiv](https://arxiv.org/abs/2502.06708)] [[cool](https://papers.cool/arxiv/2502.06708)] [[pdf](https://arxiv.org/pdf/2502.06708)]
> **Authors**: Muhammad Bilal,Mahmood Alam,Deepa Bapu,Stephan Korsgen,Neeraj Lal,Simon Bach,Amir M Hajivanand,Muhammed Ali,Kamran Soomro,Iqbal Qasim,Paweł Capik,Aslam Khan,Zaheer Khan,Hunaid Vohra,Massimo Caputo,Andrew Beggs,Adnan Qayyum,Junaid Qadir,Shazad Ashraf
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Indexing endoscopic surgical videos is vital in surgical data science, forming the basis for systematic retrospective analysis and clinical performance evaluation. Despite its significance, current video analytics rely on manual indexing, a time-consuming process. Advances in computer vision, particularly deep learning, offer automation potential, yet progress is limited by the lack of publicly available, densely annotated surgical datasets. To address this, we present TEMSET-24K, an open-source dataset comprising 24,306 trans-anal endoscopic microsurgery (TEMS) video micro-clips. Each clip is meticulously annotated by clinical experts using a novel hierarchical labeling taxonomy encompassing phase, task, and action triplets, capturing intricate surgical workflows. To validate this dataset, we benchmarked deep learning models, including transformer-based architectures. Our in silico evaluation demonstrates high accuracy (up to 0.99) and F1 scores (up to 0.99) for key phases like Setup and Suturing. The STALNet model, tested with ConvNeXt, ViT, and SWIN V2 encoders, consistently segmented well-represented phases. TEMSET-24K provides a critical benchmark, propelling state-of-the-art solutions in surgical data science.

### CHIRLA: Comprehensive High-resolution Identification and Re-identification for Large-scale Analysis 
[[arxiv](https://arxiv.org/abs/2502.06681)] [[cool](https://papers.cool/arxiv/2502.06681)] [[pdf](https://arxiv.org/pdf/2502.06681)]
> **Authors**: Bessie Dominguez-Dager,Felix Escalona,Francisco Gomez-Donoso,Miguel Cazorla
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: Person re-identification (Re-ID) is a key challenge in computer vision, requiring the matching of individuals across different cameras, locations, and time periods. While most research focuses on short-term scenarios with minimal appearance changes, real-world applications demand robust Re-ID systems capable of handling long-term scenarios, where persons' appearances can change significantly due to variations in clothing and physical characteristics. In this paper, we present CHIRLA, Comprehensive High-resolution Identification and Re-identification for Large-scale Analysis, a novel dataset specifically designed for long-term person Re-ID. CHIRLA consists of recordings from strategically placed cameras over a seven-month period, capturing significant variations in both temporal and appearance attributes, including controlled changes in participants' clothing and physical features. The dataset includes 22 individuals, four connected indoor environments, and seven cameras. We collected more than five hours of video that we semi-automatically labeled to generate around one million bounding boxes with identity annotations. By introducing this comprehensive benchmark, we aim to facilitate the development and evaluation of Re-ID algorithms that can reliably perform in challenging, long-term real-world scenarios.

### Few-Shot Classification and Anatomical Localization of Tissues in SPECT Imaging 
[[arxiv](https://arxiv.org/abs/2502.06632)] [[cool](https://papers.cool/arxiv/2502.06632)] [[pdf](https://arxiv.org/pdf/2502.06632)]
> **Authors**: Mohammed Abdul Hafeez Khan,Samuel Morries Boddepalli,Siddhartha Bhattacharyya,Debasis Mitra
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 2 pages, 2 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: Accurate classification and anatomical localization are essential for effective medical diagnostics and research, which may be efficiently performed using deep learning techniques. However, availability of limited labeled data poses a significant challenge. To address this, we adapted Prototypical Networks and the Propagation-Reconstruction Network (PRNet) for few-shot classification and localization, respectively, in Single Photon Emission Computed Tomography (SPECT) images. For the proof of concept we used a 2D-sliced image cropped around heart. The Prototypical Network, with a pre-trained ResNet-18 backbone, classified ventricles, myocardium, and liver tissues with 96.67% training and 93.33% validation accuracy. PRNet, adapted for 2D imaging with an encoder-decoder architecture and skip connections, achieved a training loss of 1.395, accurately reconstructing patches and capturing spatial relationships. These results highlight the potential of Prototypical Networks for tissue classification with limited labeled data and PRNet for anatomical landmark localization, paving the way for improved performance in deep learning frameworks.

### Conformal Predictions for Human Action Recognition with Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2502.06631)] [[cool](https://papers.cool/arxiv/2502.06631)] [[pdf](https://arxiv.org/pdf/2502.06631)]
> **Authors**: Bary Tim,Fuchs Clément,Macq Benoît
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 6 pages, 7 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Human-In-The-Loop (HITL) frameworks are integral to many real-world computer vision systems, enabling human operators to make informed decisions with AI assistance. Conformal Predictions (CP), which provide label sets with rigorous guarantees on ground truth inclusion probabilities, have recently gained traction as a valuable tool in HITL settings. One key application area is video surveillance, closely associated with Human Action Recognition (HAR). This study explores the application of CP on top of state-of-the-art HAR methods that utilize extensively pre-trained Vision-Language Models (VLMs). Our findings reveal that CP can significantly reduce the average number of candidate classes without modifying the underlying VLM. However, these reductions often result in distributions with long tails. To address this, we introduce a method based on tuning the temperature parameter of the VLMs to minimize these tails without requiring additional calibration data. Our code is made available on GitHub at the address https://github.com/tbary/CP4VLM.

### TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified Flow Models 
[[arxiv](https://arxiv.org/abs/2502.06608)] [[cool](https://papers.cool/arxiv/2502.06608)] [[pdf](https://arxiv.org/pdf/2502.06608)]
> **Authors**: Yangguang Li,Zi-Xin Zou,Zexiang Liu,Dehu Wang,Yuan Liang,Zhipeng Yu,Xingchao Liu,Yuan-Chen Guo,Ding Liang,Wanli Ouyang,Yan-Pei Cao
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Recent advancements in diffusion techniques have propelled image and video generation to unprecedented levels of quality, significantly accelerating the deployment and application of generative AI. However, 3D shape generation technology has so far lagged behind, constrained by limitations in 3D data scale, complexity of 3D data processing, and insufficient exploration of advanced techniques in the 3D domain. Current approaches to 3D shape generation face substantial challenges in terms of output quality, generalization capability, and alignment with input conditions. We present TripoSG, a new streamlined shape diffusion paradigm capable of generating high-fidelity 3D meshes with precise correspondence to input images. Specifically, we propose: 1) A large-scale rectified flow transformer for 3D shape generation, achieving state-of-the-art fidelity through training on extensive, high-quality data. 2) A hybrid supervised training strategy combining SDF, normal, and eikonal losses for 3D VAE, achieving high-quality 3D reconstruction performance. 3) A data processing pipeline to generate 2 million high-quality 3D samples, highlighting the crucial rules for data quality and quantity in training 3D generative models. Through comprehensive experiments, we have validated the effectiveness of each component in our new framework. The seamless integration of these parts has enabled TripoSG to achieve state-of-the-art performance in 3D shape generation. The resulting 3D shapes exhibit enhanced detail due to high-resolution capabilities and demonstrate exceptional fidelity to input images. Moreover, TripoSG demonstrates improved versatility in generating 3D models from diverse image styles and contents, showcasing strong generalization capabilities. To foster progress and innovation in the field of 3D generation, we will make our model publicly available.

### A Large-scale AI-generated Image Inpainting Benchmark 
[[arxiv](https://arxiv.org/abs/2502.06593)] [[cool](https://papers.cool/arxiv/2502.06593)] [[pdf](https://arxiv.org/pdf/2502.06593)]
> **Authors**: Paschalis Giakoumoglou,Dimitrios Karageorgiou,Symeon Papadopoulos,Panagiotis C. Petrantonakis
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Recent advances in generative models enable highly realistic image manipulations, creating an urgent need for robust forgery detection methods. Current datasets for training and evaluating these methods are limited in scale and diversity. To address this, we propose a methodology for creating high-quality inpainting datasets and apply it to create DiQuID, comprising over 95,000 inpainted images generated from 78,000 original images sourced from MS-COCO, RAISE, and OpenImages. Our methodology consists of three components: (1) Semantically Aligned Object Replacement (SAOR) that identifies suitable objects through instance segmentation and generates contextually appropriate prompts, (2) Multiple Model Image Inpainting (MMII) that employs various state-of-the-art inpainting pipelines primarily based on diffusion models to create diverse manipulations, and (3) Uncertainty-Guided Deceptiveness Assessment (UGDA) that evaluates image realism through comparative analysis with originals. The resulting dataset surpasses existing ones in diversity, aesthetic quality, and technical quality. We provide comprehensive benchmarking results using state-of-the-art forgery detection methods, demonstrating the dataset's effectiveness in evaluating and improving detection algorithms. Through a human study with 42 participants on 1,000 images, we show that while humans struggle with images classified as deceiving by our methodology, models trained on our dataset maintain high performance on these challenging cases. Code and dataset are available at https://github.com/mever-team/DiQuID.

### Unsupervised Learning for Feature Extraction and Temporal Alignment of 3D+t Point Clouds of Zebrafish Embryos 
[[arxiv](https://arxiv.org/abs/2502.06543)] [[cool](https://papers.cool/arxiv/2502.06543)] [[pdf](https://arxiv.org/pdf/2502.06543)]
> **Authors**: Zhu Chen,Ina Laube,Johannes Stegmaier
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Zebrafish are widely used in biomedical research and developmental stages of their embryos often need to be synchronized for further analysis. We present an unsupervised approach to extract descriptive features from 3D+t point clouds of zebrafish embryos and subsequently use those features to temporally align corresponding developmental stages. An autoencoder architecture is proposed to learn a descriptive representation of the point clouds and we designed a deep regression network for their temporal alignment. We achieve a high alignment accuracy with an average mismatch of only 3.83 minutes over an experimental duration of 5.3 hours. As a fully-unsupervised approach, there is no manual labeling effort required and unlike manual analyses the method easily scales. Besides, the alignment without human annotation of the data also avoids any influence caused by subjective bias.

### CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for Zero-Shot Customized Video Diffusion Transformers 
[[arxiv](https://arxiv.org/abs/2502.06527)] [[cool](https://papers.cool/arxiv/2502.06527)] [[pdf](https://arxiv.org/pdf/2502.06527)]
> **Authors**: D. She,Mushui Liu,Jingxuan Pang,Jin Wang,Zhen Yang,Wanggui He,Guanghao Zhang,Yi Wang,Qihan Huang,Haobin Tang,Yunlong Yu,Siming Fu
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Section 4 in CustomVideoX Entity Region-Aware Enhancement has description errors. The compared methods data of Table I lacks other metrics
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Customized generation has achieved significant progress in image synthesis, yet personalized video generation remains challenging due to temporal inconsistencies and quality degradation. In this paper, we introduce CustomVideoX, an innovative framework leveraging the video diffusion transformer for personalized video generation from a reference image. CustomVideoX capitalizes on pre-trained video networks by exclusively training the LoRA parameters to extract reference features, ensuring both efficiency and adaptability. To facilitate seamless interaction between the reference image and video content, we propose 3D Reference Attention, which enables direct and simultaneous engagement of reference image features with all video frames across spatial and temporal dimensions. To mitigate the excessive influence of reference image features and textual guidance on generated video content during inference, we implement the Time-Aware Reference Attention Bias (TAB) strategy, dynamically modulating reference bias over different time steps. Additionally, we introduce the Entity Region-Aware Enhancement (ERAE) module, aligning highly activated regions of key entity tokens with reference feature injection by adjusting attention bias. To thoroughly evaluate personalized video generation, we establish a new benchmark, VideoBench, comprising over 50 objects and 100 prompts for extensive assessment. Experimental results show that CustomVideoX significantly outperforms existing methods in terms of video consistency and quality.

### UniMoD: Efficient Unified Multimodal Transformers with Mixture-of-Depths 
[[arxiv](https://arxiv.org/abs/2502.06474)] [[cool](https://papers.cool/arxiv/2502.06474)] [[pdf](https://arxiv.org/pdf/2502.06474)]
> **Authors**: Weijia Mao,Zhenheng Yang,Mike Zheng Shou
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Unified multimodal transformers, which handle both generation and understanding tasks within a shared parameter space, have received increasing attention in recent research. Although various unified transformers have been proposed, training these models is costly due to redundant tokens and heavy attention computation. In the past, studies on large language models have demonstrated that token pruning methods, such as Mixture of Depths (MoD), can significantly improve computational efficiency. MoD employs a router to select the most important ones for processing within a transformer layer. However, directly applying MoD-based token pruning to unified transformers will result in suboptimal performance because different tasks exhibit varying levels of token redundancy. In our work, we analyze the unified transformers by (1) examining attention weight patterns, (2) evaluating the layer importance and token redundancy, and (3) analyzing task interactions. Our findings reveal that token redundancy is primarily influenced by different tasks and layers. Building on these findings, we introduce UniMoD, a task-aware token pruning method that employs a separate router for each task to determine which tokens should be pruned. We apply our method to Show-o and Emu3, reducing training FLOPs by approximately 15% in Show-o and 40% in Emu3, while maintaining or improving performance on several benchmarks. Code will be released at https://github.com/showlab/UniMoD.

### Benchmarking Vision-Language Models on Optical Character Recognition in Dynamic Video Environments 
[[arxiv](https://arxiv.org/abs/2502.06445)] [[cool](https://papers.cool/arxiv/2502.06445)] [[pdf](https://arxiv.org/pdf/2502.06445)]
> **Authors**: Sankalp Nagaonkar,Augustya Sharma,Ashish Choithani,Ashutosh Trivedi
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Code and dataset: https://github.com/video-db/ocr-benchmark
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: This paper introduces an open-source benchmark for evaluating Vision-Language Models (VLMs) on Optical Character Recognition (OCR) tasks in dynamic video environments. We present a curated dataset containing 1,477 manually annotated frames spanning diverse domains, including code editors, news broadcasts, YouTube videos, and advertisements. Three state of the art VLMs - Claude-3, Gemini-1.5, and GPT-4o are benchmarked against traditional OCR systems such as EasyOCR and RapidOCR. Evaluation metrics include Word Error Rate (WER), Character Error Rate (CER), and Accuracy. Our results highlight the strengths and limitations of VLMs in video-based OCR tasks, demonstrating their potential to outperform conventional OCR models in many scenarios. However, challenges such as hallucinations, content security policies, and sensitivity to occluded or stylized text remain. The dataset and benchmarking framework are publicly available to foster further research.

### Rethinking Large-scale Dataset Compression: Shifting Focus From Labels to Images 
[[arxiv](https://arxiv.org/abs/2502.06434)] [[cool](https://papers.cool/arxiv/2502.06434)] [[pdf](https://arxiv.org/pdf/2502.06434)]
> **Authors**: Lingao Xiao,Songhua Liu,Yang He,Xinchao Wang
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Work In Progress
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Dataset distillation and dataset pruning are two prominent techniques for compressing datasets to improve computational and storage efficiency. Despite their overlapping objectives, these approaches are rarely compared directly. Even within each field, the evaluation protocols are inconsistent across various methods, which complicates fair comparisons and hinders reproducibility. Considering these limitations, we introduce in this paper a benchmark that equitably evaluates methodologies across both distillation and pruning literatures. Notably, our benchmark reveals that in the mainstream dataset distillation setting for large-scale datasets, which heavily rely on soft labels from pre-trained models, even randomly selected subsets can achieve surprisingly competitive performance. This finding suggests that an overemphasis on soft labels may be diverting attention from the intrinsic value of the image data, while also imposing additional burdens in terms of generation, storage, and application. To address these issues, we propose a new framework for dataset compression, termed Prune, Combine, and Augment (PCA), which focuses on leveraging image data exclusively, relies solely on hard labels for evaluation, and achieves state-of-the-art performance in this setup. By shifting the emphasis back to the images, our benchmark and PCA framework pave the way for more balanced and accessible techniques in dataset compression research. Our code is available at: https://github.com/ArmandXiao/Rethinking-Dataset-Compression

### Prompt-SID: Learning Structural Representation Prompt via Latent Diffusion for Single-Image Denoising 
[[arxiv](https://arxiv.org/abs/2502.06432)] [[cool](https://papers.cool/arxiv/2502.06432)] [[pdf](https://arxiv.org/pdf/2502.06432)]
> **Authors**: Huaqiu Li,Wang Zhang,Xiaowan Hu,Tao Jiang,Zikang Chen,Haoqian Wang
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Many studies have concentrated on constructing supervised models utilizing paired datasets for image denoising, which proves to be expensive and time-consuming. Current self-supervised and unsupervised approaches typically rely on blind-spot networks or sub-image pairs sampling, resulting in pixel information loss and destruction of detailed structural information, thereby significantly constraining the efficacy of such methods. In this paper, we introduce Prompt-SID, a prompt-learning-based single image denoising framework that emphasizes preserving of structural details. This approach is trained in a self-supervised manner using downsampled image pairs. It captures original-scale image information through structural encoding and integrates this prompt into the denoiser. To achieve this, we propose a structural representation generation model based on the latent diffusion process and design a structural attention module within the transformer-based denoiser architecture to decode the prompt. Additionally, we introduce a scale replay training mechanism, which effectively mitigates the scale gap from images of different resolutions. We conduct comprehensive experiments on synthetic, real-world, and fluorescence imaging datasets, showcasing the remarkable effectiveness of Prompt-SID.

### CoS: Chain-of-Shot Prompting for Long Video Understanding 
[[arxiv](https://arxiv.org/abs/2502.06428)] [[cool](https://papers.cool/arxiv/2502.06428)] [[pdf](https://arxiv.org/pdf/2502.06428)]
> **Authors**: Jian Hu,Zixu Cheng,Chenyang Si,Wei Li,Shaogang Gong
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: A training-free test-time optimisation approach for long video understanding
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Multi-modal Large Language Models (MLLMs) struggle with long videos due to the need for excessive visual tokens. These tokens exceed massively the context length of MLLMs, resulting in filled by redundant task-irrelevant shots. How to select shots is an unsolved critical problem: sparse sampling risks missing key details, while exhaustive sampling overwhelms the model with irrelevant content, leading to video misunderstanding. To solve this problem, we propose Chain-of-Shot prompting (CoS). The key idea is to frame shot selection as test-time visual prompt optimisation, choosing shots adaptive to video understanding semantic task by optimising shots-task alignment. CoS has two key parts: (1) a binary video summary mechanism that performs pseudo temporal grounding, discovering a binary coding to identify task-relevant shots, and (2) a video co-reasoning module that deploys the binary coding to pair (learning to align) task-relevant positive shots with irrelevant negative shots. It embeds the optimised shot selections into the original video, facilitating a focus on relevant context to optimize long video understanding. Experiments across three baselines and five datasets demonstrate the effectiveness and adaptability of CoS. Code given in https://lwpyh.github.io/CoS.

### Hybrid State-Space and GRU-based Graph Tokenization Mamba for Hyperspectral Image Classification 
[[arxiv](https://arxiv.org/abs/2502.06427)] [[cool](https://papers.cool/arxiv/2502.06427)] [[pdf](https://arxiv.org/pdf/2502.06427)]
> **Authors**: Muhammad Ahmad,Muhammad Hassaan Farooq Butt,Muhammad Usama,Manuel Mazzara,Salvatore Distefano,Adil Mehmood Khan,Danfeng Hong
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Hyperspectral image (HSI) classification plays a pivotal role in domains such as environmental monitoring, agriculture, and urban planning. However, it faces significant challenges due to the high-dimensional nature of the data and the complex spectral-spatial relationships inherent in HSI. Traditional methods, including conventional machine learning and convolutional neural networks (CNNs), often struggle to effectively capture these intricate spectral-spatial features and global contextual information. Transformer-based models, while powerful in capturing long-range dependencies, often demand substantial computational resources, posing challenges in scenarios where labeled datasets are limited, as is commonly seen in HSI applications. To overcome these challenges, this work proposes GraphMamba, a hybrid model that combines spectral-spatial token generation, graph-based token prioritization, and cross-attention mechanisms. The model introduces a novel hybridization of state-space modeling and Gated Recurrent Units (GRU), capturing both linear and nonlinear spatial-spectral dynamics. GraphMamba enhances the ability to model complex spatial-spectral relationships while maintaining scalability and computational efficiency across diverse HSI datasets. Through comprehensive experiments, we demonstrate that GraphMamba outperforms existing state-of-the-art models, offering a scalable and robust solution for complex HSI classification tasks.

### Robust Watermarks Leak: Channel-Aware Feature Extraction Enables Adversarial Watermark Manipulation 
[[arxiv](https://arxiv.org/abs/2502.06418)] [[cool](https://papers.cool/arxiv/2502.06418)] [[pdf](https://arxiv.org/pdf/2502.06418)]
> **Authors**: Zhongjie Ba,Yitao Zhang,Peng Cheng,Bin Gong,Xinyu Zhang,Qinglong Wang,Kui Ren
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,密码学和安全
- **Abstract**: Watermarking plays a key role in the provenance and detection of AI-generated content. While existing methods prioritize robustness against real-world distortions (e.g., JPEG compression and noise addition), we reveal a fundamental tradeoff: such robust watermarks inherently improve the redundancy of detectable patterns encoded into images, creating exploitable information leakage. To leverage this, we propose an attack framework that extracts leakage of watermark patterns through multi-channel feature learning using a pre-trained vision model. Unlike prior works requiring massive data or detector access, our method achieves both forgery and detection evasion with a single watermarked image. Extensive experiments demonstrate that our method achieves a 60\% success rate gain in detection evasion and 51\% improvement in forgery accuracy compared to state-of-the-art methods while maintaining visual fidelity. Our work exposes the robustness-stealthiness paradox: current "robust" watermarks sacrifice security for distortion resistance, providing insights for future watermark design.

### When Data Manipulation Meets Attack Goals: An In-depth Survey of Attacks for VLMs 
[[arxiv](https://arxiv.org/abs/2502.06390)] [[cool](https://papers.cool/arxiv/2502.06390)] [[pdf](https://arxiv.org/pdf/2502.06390)]
> **Authors**: Aobotao Dai,Xinyu Ma,Lei Chen,Songze Li,Lin Wang
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Vision-Language Models (VLMs) have gained considerable prominence in recent years due to their remarkable capability to effectively integrate and process both textual and visual information. This integration has significantly enhanced performance across a diverse spectrum of applications, such as scene perception and robotics. However, the deployment of VLMs has also given rise to critical safety and security concerns, necessitating extensive research to assess the potential vulnerabilities these VLM systems may harbor. In this work, we present an in-depth survey of the attack strategies tailored for VLMs. We categorize these attacks based on their underlying objectives - namely jailbreak, camouflage, and exploitation - while also detailing the various methodologies employed for data manipulation of VLMs. Meanwhile, we outline corresponding defense mechanisms that have been proposed to mitigate these vulnerabilities. By discerning key connections and distinctions among the diverse types of attacks, we propose a compelling taxonomy for VLM attacks. Moreover, we summarize the evaluation metrics that comprehensively describe the characteristics and impact of different attacks on VLMs. Finally, we conclude with a discussion of promising future research directions that could further enhance the robustness and safety of VLMs, emphasizing the importance of ongoing exploration in this critical area of study. To facilitate community engagement, we maintain an up-to-date project page, accessible at: https://github.com/AobtDai/VLM_Attack_Paper_List.

### Facial Analysis Systems and Down Syndrome 
[[arxiv](https://arxiv.org/abs/2502.06341)] [[cool](https://papers.cool/arxiv/2502.06341)] [[pdf](https://arxiv.org/pdf/2502.06341)]
> **Authors**: Marco Rondina,Fabiana Vinci,Antonio Vetrò,Juan Carlos De Martin
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: ef:MachineLearningand Principles and Practice of Knowledge Discovery in Databases. ECML PKDD 2023
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,人机交互,机器学习
- **Abstract**: The ethical, social and legal issues surrounding facial analysis technologies have been widely debated in recent years. Key critics have argued that these technologies can perpetuate bias and discrimination, particularly against marginalized groups. We contribute to this field of research by reporting on the limitations of facial analysis systems with the faces of people with Down syndrome: this particularly vulnerable group has received very little attention in the literature so far. This study involved the creation of a specific dataset of face images. An experimental group with faces of people with Down syndrome, and a control group with faces of people who are not affected by the syndrome. Two commercial tools were tested on the dataset, along three tasks: gender recognition, age prediction and face labelling. The results show an overall lower accuracy of prediction in the experimental group, and other specific patterns of performance differences: i) high error rates in gender recognition in the category of males with Down syndrome; ii) adults with Down syndrome were more often incorrectly labelled as children; iii) social stereotypes are propagated in both the control and experimental groups, with labels related to aesthetics more often associated with women, and labels related to education level and skills more often associated with men. These results, although limited in scope, shed new light on the biases that alter face classification when applied to faces of people with Down syndrome. They confirm the structural limitation of the technology, which is inherently dependent on the datasets used to train the models.

### DefTransNet: A Transformer-based Method for Non-Rigid Point Cloud Registration in the Simulation of Soft Tissue Deformation 
[[arxiv](https://arxiv.org/abs/2502.06336)] [[cool](https://papers.cool/arxiv/2502.06336)] [[pdf](https://arxiv.org/pdf/2502.06336)]
> **Authors**: Sara Monji-Azad,Marvin Kinz,Siddharth Kothari,Robin Khanna,Amrei Carla Mihan,David Maennel,Claudia Scherl,Juergen Hesser
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Soft-tissue surgeries, such as tumor resections, are complicated by tissue deformations that can obscure the accurate location and shape of tissues. By representing tissue surfaces as point clouds and applying non-rigid point cloud registration (PCR) methods, surgeons can better understand tissue deformations before, during, and after surgery. Existing non-rigid PCR methods, such as feature-based approaches, struggle with robustness against challenges like noise, outliers, partial data, and large deformations, making accurate point correspondence difficult. Although learning-based PCR methods, particularly Transformer-based approaches, have recently shown promise due to their attention mechanisms for capturing interactions, their robustness remains limited in challenging scenarios. In this paper, we present DefTransNet, a novel end-to-end Transformer-based architecture for non-rigid PCR. DefTransNet is designed to address the key challenges of deformable registration, including large deformations, outliers, noise, and partial data, by inputting source and target point clouds and outputting displacement vector fields. The proposed method incorporates a learnable transformation matrix to enhance robustness to affine transformations, integrates global and local geometric information, and captures long-range dependencies among points using Transformers. We validate our approach on four datasets: ModelNet, SynBench, 4DMatch, and DeformedTissue, using both synthetic and real-world data to demonstrate the generalization of our proposed method. Experimental results demonstrate that DefTransNet outperforms current state-of-the-art registration networks across various challenging conditions. Our code and data are publicly available.

### Cell Nuclei Detection and Classification in Whole Slide Images with Transformers 
[[arxiv](https://arxiv.org/abs/2502.06307)] [[cool](https://papers.cool/arxiv/2502.06307)] [[pdf](https://arxiv.org/pdf/2502.06307)]
> **Authors**: Oscar Pina,Eduard Dorca,Verónica Vilaplana
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Accurate and efficient cell nuclei detection and classification in histopathological Whole Slide Images (WSIs) are pivotal for digital pathology applications. Traditional cell segmentation approaches, while commonly used, are computationally expensive and require extensive post-processing, limiting their practicality for high-throughput clinical settings. In this paper, we propose a paradigm shift from segmentation to detection for extracting cell information from WSIs, introducing CellNuc-DETR as a more effective solution. We evaluate the accuracy performance of CellNuc-DETR on the PanNuke dataset and conduct cross-dataset evaluations on CoNSeP and MoNuSeg to assess robustness and generalization capabilities. Our results demonstrate state-of-the-art performance in both cell nuclei detection and classification tasks. Additionally, we assess the efficiency of CellNuc-DETR on large WSIs, showing that it not only outperforms current methods in accuracy but also significantly reduces inference times. Specifically, CellNuc-DETR is twice as fast as the fastest segmentation-based method, HoVer-NeXt, while achieving substantially higher accuracy. Moreover, it surpasses CellViT in accuracy and is approximately ten times more efficient in inference speed on WSIs. These results establish CellNuc-DETR as a superior approach for cell analysis in digital pathology, combining high accuracy with computational efficiency.

### Enhancing Ground-to-Aerial Image Matching for Visual Misinformation Detection Using Semantic Segmentation 
[[arxiv](https://arxiv.org/abs/2502.06288)] [[cool](https://papers.cool/arxiv/2502.06288)] [[pdf](https://arxiv.org/pdf/2502.06288)]
> **Authors**: Emanuele Mule,Matteo Pannacci,Ali Ghasemi Goudarzi,Francesco Pro,Lorenzo Papa,Luca Maiano,Irene Amerini
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 9 pages, 4 figures. Accepted to AI4MFDD 2025 workshop at WACV 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: The recent advancements in generative AI techniques, which have significantly increased the online dissemination of altered images and videos, have raised serious concerns about the credibility of digital media available on the Internet and distributed through information channels and social networks. This issue particularly affects domains that rely heavily on trustworthy data, such as journalism, forensic analysis, and Earth observation. To address these concerns, the ability to geolocate a non-geo-tagged ground-view image without external information, such as GPS coordinates, has become increasingly critical. This study tackles the challenge of linking a ground-view image, potentially exhibiting varying fields of view (FoV), to its corresponding satellite image without the aid of GPS data. To achieve this, we propose a novel four-stream Siamese-like architecture, the Quadruple Semantic Align Net (SAN-QUAD), which extends previous state-of-the-art (SOTA) approaches by leveraging semantic segmentation applied to both ground and satellite imagery. Experimental results on a subset of the CVUSA dataset demonstrate significant improvements of up to 9.8% over prior methods across various FoV settings.

### Towards Efficient and Intelligent Laser Weeding: Method and Dataset for Weed Stem Detection 
[[arxiv](https://arxiv.org/abs/2502.06255)] [[cool](https://papers.cool/arxiv/2502.06255)] [[pdf](https://arxiv.org/pdf/2502.06255)]
> **Authors**: Dingning Liu,Jinzhe Li,Haoyang Su,Bei Cui,Zhihui Wang,Qingbo Yuan,Wanli Ouyang,Nanqing Dong
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Accepted by AAAI-AISI 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Weed control is a critical challenge in modern agriculture, as weeds compete with crops for essential nutrient resources, significantly reducing crop yield and quality. Traditional weed control methods, including chemical and mechanical approaches, have real-life limitations such as associated environmental impact and efficiency. An emerging yet effective approach is laser weeding, which uses a laser beam as the stem cutter. Although there have been studies that use deep learning in weed recognition, its application in intelligent laser weeding still requires a comprehensive understanding. Thus, this study represents the first empirical investigation of weed recognition for laser weeding. To increase the efficiency of laser beam cut and avoid damaging the crops of interest, the laser beam shall be directly aimed at the weed root. Yet, weed stem detection remains an under-explored problem. We integrate the detection of crop and weed with the localization of weed stem into one end-to-end system. To train and validate the proposed system in a real-life scenario, we curate and construct a high-quality weed stem detection dataset with human annotations. The dataset consists of 7,161 high-resolution pictures collected in the field with annotations of 11,151 instances of weed. Experimental results show that the proposed system improves weeding accuracy by 6.7% and reduces energy cost by 32.3% compared to existing weed recognition systems.

### Multi-Scale Transformer Architecture for Accurate Medical Image Classification 
[[arxiv](https://arxiv.org/abs/2502.06243)] [[cool](https://papers.cool/arxiv/2502.06243)] [[pdf](https://arxiv.org/pdf/2502.06243)]
> **Authors**: Jiacheng Hu,Yanlin Xiang,Yang Lin,Junliang Du,Hanchao Zhang,Houze Liu
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: This study introduces an AI-driven skin lesion classification algorithm built on an enhanced Transformer architecture, addressing the challenges of accuracy and robustness in medical image analysis. By integrating a multi-scale feature fusion mechanism and refining the self-attention process, the model effectively extracts both global and local features, enhancing its ability to detect lesions with ambiguous boundaries and intricate structures. Performance evaluation on the ISIC 2017 dataset demonstrates that the improved Transformer surpasses established AI models, including ResNet50, VGG19, ResNext, and Vision Transformer, across key metrics such as accuracy, AUC, F1-Score, and Precision. Grad-CAM visualizations further highlight the interpretability of the model, showcasing strong alignment between the algorithm's focus areas and actual lesion sites. This research underscores the transformative potential of advanced AI models in medical imaging, paving the way for more accurate and reliable diagnostic tools. Future work will explore the scalability of this approach to broader medical imaging tasks and investigate the integration of multimodal data to enhance AI-driven diagnostic frameworks for intelligent healthcare.

### Unsupervised deep learning for semantic segmentation of multispectral LiDAR forest point clouds 
[[arxiv](https://arxiv.org/abs/2502.06227)] [[cool](https://papers.cool/arxiv/2502.06227)] [[pdf](https://arxiv.org/pdf/2502.06227)]
> **Authors**: Lassi Ruoppa,Oona Oinonen,Josef Taher,Matti Lehtomäki,Narges Takhtkeshha,Antero Kukko,Harri Kaartinen,Juha Hyyppä
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 30 pages, 10 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Point clouds captured with laser scanning systems from forest environments can be utilized in a wide variety of applications within forestry and plant ecology, such as the estimation of tree stem attributes, leaf angle distribution, and above-ground biomass. However, effectively utilizing the data in such tasks requires the semantic segmentation of the data into wood and foliage points, also known as leaf-wood separation. The traditional approach to leaf-wood separation has been geometry- and radiometry-based unsupervised algorithms, which tend to perform poorly on data captured with airborne laser scanning (ALS) systems, even with a high point density. While recent machine and deep learning approaches achieve great results even on sparse point clouds, they require manually labeled training data, which is often extremely laborious to produce. Multispectral (MS) information has been demonstrated to have potential for improving the accuracy of leaf-wood separation, but quantitative assessment of its effects has been lacking. This study proposes a fully unsupervised deep learning method, GrowSP-ForMS, which is specifically designed for leaf-wood separation of high-density MS ALS point clouds and based on the GrowSP architecture. GrowSP-ForMS achieved a mean accuracy of 84.3% and a mean intersection over union (mIoU) of 69.6% on our MS test set, outperforming the unsupervised reference methods by a significant margin. When compared to supervised deep learning methods, our model performed similarly to the slightly older PointNet architecture but was outclassed by more recent approaches. Finally, two ablation studies were conducted, which demonstrated that our proposed changes increased the test set mIoU of GrowSP-ForMS by 29.4 percentage points (pp) in comparison to the original GrowSP model and that utilizing MS data improved the mIoU by 5.6 pp from the monospectral case.

### FunduSAM: A Specialized Deep Learning Model for Enhanced Optic Disc and Cup Segmentation in Fundus Images 
[[arxiv](https://arxiv.org/abs/2502.06220)] [[cool](https://papers.cool/arxiv/2502.06220)] [[pdf](https://arxiv.org/pdf/2502.06220)]
> **Authors**: Jinchen Yu,Yongwei Nie,Fei Qi,Wenxiong Liao,Hongmin Cai
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,信息检索
- **Abstract**: The Segment Anything Model (SAM) has gained popularity as a versatile image segmentation method, thanks to its strong generalization capabilities across various domains. However, when applied to optic disc (OD) and optic cup (OC) segmentation tasks, SAM encounters challenges due to the complex structures, low contrast, and blurred boundaries typical of fundus images, leading to suboptimal performance. To overcome these challenges, we introduce a novel model, FunduSAM, which incorporates several Adapters into SAM to create a deep network specifically designed for OD and OC segmentation. The FunduSAM utilizes Adapter into each transformer block after encoder for parameter fine-tuning (PEFT). It enhances SAM's feature extraction capabilities by designing a Convolutional Block Attention Module (CBAM), addressing issues related to blurred boundaries and low contrast. Given the unique requirements of OD and OC segmentation, polar transformation is used to convert the original fundus OD images into a format better suited for training and evaluating FunduSAM. A joint loss is used to achieve structure preservation between the OD and OC, while accurate segmentation. Extensive experiments on the REFUGE dataset, comprising 1,200 fundus images, demonstrate the superior performance of FunduSAM compared to five mainstream approaches.

### Fully Exploiting Vision Foundation Model's Profound Prior Knowledge for Generalizable RGB-Depth Driving Scene Parsing 
[[arxiv](https://arxiv.org/abs/2502.06219)] [[cool](https://papers.cool/arxiv/2502.06219)] [[pdf](https://arxiv.org/pdf/2502.06219)]
> **Authors**: Sicen Guo,Tianyou Wen,Chuang-Wei Liu,Qijun Chen,Rui Fan
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 10 pages, 5 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Recent vision foundation models (VFMs), typically based on Vision Transformer (ViT), have significantly advanced numerous computer vision tasks. Despite their success in tasks focused solely on RGB images, the potential of VFMs in RGB-depth driving scene parsing remains largely under-explored. In this article, we take one step toward this emerging research area by investigating a feasible technique to fully exploit VFMs for generalizable RGB-depth driving scene parsing. Specifically, we explore the inherent characteristics of RGB and depth data, thereby presenting a Heterogeneous Feature Integration Transformer (HFIT). This network enables the efficient extraction and integration of comprehensive heterogeneous features without re-training ViTs. Relative depth prediction results from VFMs, used as inputs to the HFIT side adapter, overcome the limitations of the dependence on depth maps. Our proposed HFIT demonstrates superior performance compared to all other traditional single-modal and data-fusion scene parsing networks, pre-trained VFMs, and ViT adapters on the Cityscapes and KITTI Semantics datasets. We believe this novel strategy paves the way for future innovations in VFM-based data-fusion techniques for driving scene parsing. Our source code is publicly available at https://mias.group/HFIT.

### Multi-Level Decoupled Relational Distillation for Heterogeneous Architectures 
[[arxiv](https://arxiv.org/abs/2502.06189)] [[cool](https://papers.cool/arxiv/2502.06189)] [[pdf](https://arxiv.org/pdf/2502.06189)]
> **Authors**: Yaoxin Yang,Peng Ye,Weihao Lin,Kangcong Li,Yan Wen,Jia Hao,Tao Chen
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Heterogeneous distillation is an effective way to transfer knowledge from cross-architecture teacher models to student models. However, existing heterogeneous distillation methods do not take full advantage of the dark knowledge hidden in the teacher's output, limiting their performance.To this end, we propose a novel framework named Multi-Level Decoupled Relational Knowledge Distillation (MLDR-KD) to unleash the potential of relational distillation in heterogeneous distillation. Concretely, we first introduce Decoupled Finegrained Relation Alignment (DFRA) in both logit and feature levels to balance the trade-off between distilled dark knowledge and the confidence in the correct category of the heterogeneous teacher model. Then, Multi-Scale Dynamic Fusion (MSDF) module is applied to dynamically fuse the projected logits of multiscale features at different stages in student model, further improving performance of our method in feature level. We verify our method on four architectures (CNNs, Transformers, MLPs and Mambas), two datasets (CIFAR-100 and Tiny-ImageNet). Compared with the best available method, our MLDR-KD improves student model performance with gains of up to 4.86% on CIFAR-100 and 2.78% on Tiny-ImageNet datasets respectively, showing robustness and generality in heterogeneous distillation. Code will be released soon.

### CANeRV: Content Adaptive Neural Representation for Video Compression 
[[arxiv](https://arxiv.org/abs/2502.06181)] [[cool](https://papers.cool/arxiv/2502.06181)] [[pdf](https://arxiv.org/pdf/2502.06181)]
> **Authors**: Lv Tang,Jun Zhu,Xinfeng Zhang,Li Zhang,Siwei Ma,Qingming Huang
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Recent advances in video compression introduce implicit neural representation (INR) based methods, which effectively capture global dependencies and characteristics of entire video sequences. Unlike traditional and deep learning based approaches, INR-based methods optimize network parameters from a global perspective, resulting in superior compression potential. However, most current INR methods utilize a fixed and uniform network architecture across all frames, limiting their adaptability to dynamic variations within and between video sequences. This often leads to suboptimal compression outcomes as these methods struggle to capture the distinct nuances and transitions in video content. To overcome these challenges, we propose Content Adaptive Neural Representation for Video Compression (CANeRV), an innovative INR-based video compression network that adaptively conducts structure optimisation based on the specific content of each video sequence. To better capture dynamic information across video sequences, we propose a dynamic sequence-level adjustment (DSA). Furthermore, to enhance the capture of dynamics between frames within a sequence, we implement a dynamic frame-level adjustment (DFA). {Finally, to effectively capture spatial structural information within video frames, thereby enhancing the detail restoration capabilities of CANeRV, we devise a structure level hierarchical structural adaptation (HSA).} Experimental results demonstrate that CANeRV can outperform both H.266/VVC and state-of-the-art INR-based video compression techniques across diverse video datasets.

### PLATTER: A Page-Level Handwritten Text Recognition System for Indic Scripts 
[[arxiv](https://arxiv.org/abs/2502.06172)] [[cool](https://papers.cool/arxiv/2502.06172)] [[pdf](https://arxiv.org/pdf/2502.06172)]
> **Authors**: Badri Vishal Kasuba,Dhruv Kudale,Venkatapathy Subramanian,Parag Chaudhuri,Ganesh Ramakrishnan
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Submitting Preprint
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: In recent years, the field of Handwritten Text Recognition (HTR) has seen the emergence of various new models, each claiming to perform competitively better than the other in specific scenarios. However, making a fair comparison of these models is challenging due to inconsistent choices and diversity in test sets. Furthermore, recent advancements in HTR often fail to account for the diverse languages, especially Indic languages, likely due to the scarcity of relevant labeled datasets. Moreover, much of the previous work has focused primarily on character-level or word-level recognition, overlooking the crucial stage of Handwritten Text Detection (HTD) necessary for building a page-level end-to-end handwritten OCR pipeline. Through our paper, we address these gaps by making three pivotal contributions. Firstly, we present an end-to-end framework for Page-Level hAndwriTTen TExt Recognition (PLATTER) by treating it as a two-stage problem involving word-level HTD followed by HTR. This approach enables us to identify, assess, and address challenges in each stage independently. Secondly, we demonstrate the usage of PLATTER to measure the performance of our language-agnostic HTD model and present a consistent comparison of six trained HTR models on ten diverse Indic languages thereby encouraging consistent comparisons. Finally, we also release a Corpus of Handwritten Indic Scripts (CHIPS), a meticulously curated, page-level Indic handwritten OCR dataset labeled for both detection and recognition purposes. Additionally, we release our code and trained models, to encourage further contributions in this direction.

### An Interpretable Implicit-Based Approach for Modeling Local Spatial Effects: A Case Study of Global Gross Primary Productivity 
[[arxiv](https://arxiv.org/abs/2502.06170)] [[cool](https://papers.cool/arxiv/2502.06170)] [[pdf](https://arxiv.org/pdf/2502.06170)]
> **Authors**: Siqi Du,Hongsheng Huang,Kaixin Shen,Ziqi Liu,Shengjun Tang
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: In Earth sciences, unobserved factors exhibit non-stationary spatial distributions, causing the relationships between features and targets to display spatial heterogeneity. In geographic machine learning tasks, conventional statistical learning methods often struggle to capture spatial heterogeneity, leading to unsatisfactory prediction accuracy and unreliable interpretability. While approaches like Geographically Weighted Regression (GWR) capture local variations, they fall short of uncovering global patterns and tracking the continuous evolution of spatial heterogeneity. Motivated by this limitation, we propose a novel perspective - that is, simultaneously modeling common features across different locations alongside spatial differences using deep neural networks. The proposed method is a dual-branch neural network with an encoder-decoder structure. In the encoding stage, the method aggregates node information in a spatiotemporal conditional graph using GCN and LSTM, encoding location-specific spatiotemporal heterogeneity as an implicit conditional vector. Additionally, a self-attention-based encoder is used to extract location-invariant common features from the data. In the decoding stage, the approach employs a conditional generation strategy that predicts response variables and interpretative weights based on data features under spatiotemporal conditions. The approach is validated by predicting vegetation gross primary productivity (GPP) using global climate and land cover data from 2001 to 2020. Trained on 50 million samples and tested on 2.8 million, the proposed model achieves an RMSE of 0.836, outperforming LightGBM (1.063) and TabNet (0.944). Visualization analyses indicate that our method can reveal the distribution differences of the dominant factors of GPP across various times and locations.

### Efficient-vDiT: Efficient Video Diffusion Transformers With Attention Tile 
[[arxiv](https://arxiv.org/abs/2502.06155)] [[cool](https://papers.cool/arxiv/2502.06155)] [[pdf](https://arxiv.org/pdf/2502.06155)]
> **Authors**: Hangliang Ding,Dacheng Li,Runlong Su,Peiyuan Zhang,Zhijie Deng,Ion Stoica,Hao Zhang
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Despite the promise of synthesizing high-fidelity videos, Diffusion Transformers (DiTs) with 3D full attention suffer from expensive inference due to the complexity of attention computation and numerous sampling steps. For example, the popular Open-Sora-Plan model consumes more than 9 minutes for generating a single video of 29 frames. This paper addresses the inefficiency issue from two aspects: 1) Prune the 3D full attention based on the redundancy within video data; We identify a prevalent tile-style repetitive pattern in the 3D attention maps for video data, and advocate a new family of sparse 3D attention that holds a linear complexity w.r.t. the number of video frames. 2) Shorten the sampling process by adopting existing multi-step consistency distillation; We split the entire sampling trajectory into several segments and perform consistency distillation within each one to activate few-step generation capacities. We further devise a three-stage training pipeline to conjoin the low-complexity attention and few-step generation capacities. Notably, with 0.1% pretraining data, we turn the Open-Sora-Plan-1.2 model into an efficient one that is 7.4x -7.8x faster for 29 and 93 frames 720p video generation with a marginal performance trade-off in VBench. In addition, we demonstrate that our approach is amenable to distributed inference, achieving an additional 3.91x speedup when running on 4 GPUs with sequence parallelism.

## 计算机与社会(cs.CY:Computers and Society)

### Kernels of Selfhood: GPT-4o shows humanlike patterns of cognitive consistency moderated by free choice 
[[arxiv](https://arxiv.org/abs/2502.07088)] [[cool](https://papers.cool/arxiv/2502.07088)] [[pdf](https://arxiv.org/pdf/2502.07088)]
> **Authors**: Steven A. Lehr,Ketan S. Saichandran,Eddie Harmon-Jones,Nykko Vitali,Mahzarin R. Banaji
> **First submission**: 2025-01-26
> **First announcement**: 2025-02-11
> **comment**: Main Article: 10 pages, Supporting Information: 61 pages
- **标题**: None
- **领域**: 计算机与社会,人工智能,计算语言学,人机交互,机器学习
- **Abstract**: Large Language Models (LLMs) show emergent patterns that mimic human cognition. We explore whether they also mirror other, less deliberative human psychological processes. Drawing upon classical theories of cognitive consistency, two preregistered studies tested whether GPT-4o changed its attitudes toward Vladimir Putin in the direction of a positive or negative essay it wrote about the Russian leader. Indeed, GPT displayed patterns of attitude change mimicking cognitive consistency effects in humans. Even more remarkably, the degree of change increased sharply when the LLM was offered an illusion of choice about which essay (positive or negative) to write. This result suggests that GPT-4o manifests a functional analog of humanlike selfhood, although how faithfully the chatbot's behavior reflects the mechanisms of human attitude change remains to be understood.

### Integrating Generative Artificial Intelligence in ADRD: A Framework for Streamlining Diagnosis and Care in Neurodegenerative Diseases 
[[arxiv](https://arxiv.org/abs/2502.06842)] [[cool](https://papers.cool/arxiv/2502.06842)] [[pdf](https://arxiv.org/pdf/2502.06842)]
> **Authors**: Andrew G. Breithaupt,Alice Tang,Bruce L. Miller,Pedro Pinheiro-Chagas
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-11
> **comment**: 20 pages, 1 figure
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: Healthcare systems are struggling to meet the growing demand for neurological care, with challenges particularly acute in Alzheimer's disease and related dementias (ADRD). While artificial intelligence research has often focused on identifying patterns beyond human perception, implementing such predictive capabilities remains challenging as clinicians cannot readily verify insights they cannot themselves detect. We propose that large language models (LLMs) offer more immediately practical applications by enhancing clinicians' capabilities in three critical areas: comprehensive data collection, interpretation of complex clinical information, and timely application of relevant medical knowledge. These challenges stem from limited time for proper diagnosis, growing data complexity, and an overwhelming volume of medical literature that exceeds any clinician's capacity to fully master. We present a framework for responsible AI integration that leverages LLMs' ability to communicate effectively with both patients and providers while maintaining human oversight. This approach prioritizes standardized, high-quality data collection to enable a system that learns from every patient encounter while incorporating the latest clinical evidence, continuously improving care delivery. We begin to address implementation challenges and initiate important discussions around ethical considerations and governance needs. While developed for ADRD, this roadmap provides principles for responsible AI integration across neurology and other medical specialties, with potential to improve diagnostic accuracy, reduce care disparities, and advance clinical knowledge through a learning healthcare system.

## 分布式、并行和集群计算(cs.DC:Distributed, Parallel, and Cluster Computing)

### Federated Sinkhorn 
[[arxiv](https://arxiv.org/abs/2502.07021)] [[cool](https://papers.cool/arxiv/2502.07021)] [[pdf](https://arxiv.org/pdf/2502.07021)]
> **Authors**: Jeremy Kulcsar,Vyacheslav Kungurtsev,Georgios Korpas,Giulio Giaconi,William Shoosmith
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 分布式、并行和集群计算,机器学习
- **Abstract**: In this work we investigate the potential of solving the discrete Optimal Transport (OT) problem with entropy regularization in a federated learning setting. Recall that the celebrated Sinkhorn algorithm transforms the classical OT linear program into strongly convex constrained optimization, facilitating first order methods for otherwise intractably large problems. A common contemporary setting that remains an open problem as far as the application of Sinkhorn is the presence of data spread across clients with distributed inter-communication, either due to clients whose privacy is a concern, or simply by necessity of processing and memory hardware limitations. In this work we investigate various natural procedures, which we refer to as Federated Sinkhorn, that handle distributed environments where data is partitioned across multiple clients. We formulate the problem as minimizing the transport cost with an entropy regularization term, subject to marginal constraints, where block components of the source and target distribution vectors are locally known to clients corresponding to each block. We consider both synchronous and asynchronous variants as well as all-to-all and server-client communication topology protocols. Each procedure allows clients to compute local operations on their data partition while periodically exchanging information with others. We provide theoretical guarantees on convergence for the different variants under different possible conditions. We empirically demonstrate the algorithms performance on synthetic datasets and a real-world financial risk assessment application. The investigation highlights the subtle tradeoffs associated with computation and communication time in different settings and how they depend on problem size and sparsity.

### Analytic Personalized Federated Meta-Learning 
[[arxiv](https://arxiv.org/abs/2502.06915)] [[cool](https://papers.cool/arxiv/2502.06915)] [[pdf](https://arxiv.org/pdf/2502.06915)]
> **Authors**: Shunxian Gu,Chaoqun You,Deke Guo,Zhihao Qu,Bangbang Ren,Zaipeng Xie,Lailong Luo
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 分布式、并行和集群计算,机器学习
- **Abstract**: Analytic federated learning (AFL) which updates model weights only once by using closed-form least-square (LS) solutions can reduce abundant training time in gradient-free federated learning (FL). The current AFL framework cannot support deep neural network (DNN) training, which hinders its implementation on complex machine learning tasks. Meanwhile, it overlooks the heterogeneous data distribution problem that restricts the single global model from performing well on each client's task. To overcome the first challenge, we propose an AFL framework, namely FedACnnL, in which we resort to a novel local analytic learning method (ACnnL) and model the training of each layer as a distributed LS problem. For the second challenge, we propose an analytic personalized federated meta-learning framework, namely pFedACnnL, which is inherited from FedACnnL. In pFedACnnL, clients with similar data distribution share a common robust global model for fast adapting it to local tasks in an analytic manner. FedACnnL is theoretically proven to require significantly shorter training time than the conventional zeroth-order (i.e. gradient-free) FL frameworks on DNN training while the reduction ratio is $98\%$ in the experiment. Meanwhile, pFedACnnL achieves state-of-the-art (SOTA) model performance in most cases of convex and non-convex settings, compared with the previous SOTA frameworks.

### Fine-tuning Multimodal Transformers on Edge: A Parallel Split Learning Approach 
[[arxiv](https://arxiv.org/abs/2502.06355)] [[cool](https://papers.cool/arxiv/2502.06355)] [[pdf](https://arxiv.org/pdf/2502.06355)]
> **Authors**: Timo Fudala,Vasileios Tsouvalas,Nirvana Meratnia
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 10 pages, 4 figures, submitted to IJCAI 2025
- **标题**: None
- **领域**: 分布式、并行和集群计算,机器学习
- **Abstract**: Multimodal transformers integrate diverse data types like images, audio, and text, advancing tasks such as audio-visual understanding and image-text retrieval; yet their high parameterization limits deployment on resource-constrained edge devices. Split Learning (SL), which partitions models at a designated cut-layer to offload compute-intensive operations to the server, offers a promising approach for distributed training of multimodal transformers, though its application remains underexplored. We present MPSL, a parallel SL approach for computational efficient fine-tuning of multimodal transformers in a distributed manner, while eliminating label sharing, client synchronization, and per-client sub-model management. MPSL employs lightweight client-side tokenizers and a unified modality-agnostic encoder, allowing flexible adaptation to task-specific needs. Our evaluation across 7 multimodal datasets demonstrates that MPSL matches or outperforms Federated Learning, reduces client-side computations by 250x, and achieves superior scalability in communication cost with model growth. Through extensive analysis, we highlight task suitability, trade-offs, and scenarios where MPSL excels, inspiring further exploration.

## 数据结构和算法(cs.DS:Data Structures and Algorithms)

### One-Shot Learning for k-SAT 
[[arxiv](https://arxiv.org/abs/2502.07135)] [[cool](https://papers.cool/arxiv/2502.07135)] [[pdf](https://arxiv.org/pdf/2502.07135)]
> **Authors**: Andreas Galanis,Leslie Ann Goldberg,Xusheng Zhang
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 数据结构和算法,机器学习,统计理论,机器学习
- **Abstract**: Consider a $k$-SAT formula $Φ$ where every variable appears at most $d$ times, and let $σ$ be a satisfying assignment of $Φ$ sampled proportionally to $e^{βm(σ)}$ where $m(σ)$ is the number of variables set to true and $β$ is a real parameter. Given $Φ$ and $σ$, can we learn the value of $β$ efficiently? This problem falls into a recent line of works about single-sample ("one-shot") learning of Markov random fields. The $k$-SAT setting we consider here was recently studied by Galanis, Kandiros, and Kalavasis (SODA'24) where they showed that single-sample learning is possible when roughly $d\leq 2^{k/6.45}$ and impossible when $d\geq (k+1) 2^{k-1}$. Crucially, for their impossibility results they used the existence of unsatisfiable instances which, aside from the gap in $d$, left open the question of whether the feasibility threshold for one-shot learning is dictated by the satisfiability threshold of $k$-SAT formulas of bounded degree. Our main contribution is to answer this question negatively. We show that one-shot learning for $k$-SAT is infeasible well below the satisfiability threshold; in fact, we obtain impossibility results for degrees $d$ as low as $k^2$ when $β$ is sufficiently large, and bootstrap this to small values of $β$ when $d$ scales exponentially with $k$, via a probabilistic construction. On the positive side, we simplify the analysis of the learning algorithm and obtain significantly stronger bounds on $d$ in terms of $β$. In particular, for the uniform case $β\rightarrow 0$ that has been studied extensively in the sampling literature, our analysis shows that learning is possible under the condition $d\lesssim 2^{k/2}$. This is nearly optimal (up to constant factors) in the sense that it is known that sampling a uniformly-distributed satisfying assignment is NP-hard for $d\gtrsim 2^{k/2}$.

### Robust Scatter Matrix Estimation for Elliptical Distributions in Polynomial Time 
[[arxiv](https://arxiv.org/abs/2502.06564)] [[cool](https://papers.cool/arxiv/2502.06564)] [[pdf](https://arxiv.org/pdf/2502.06564)]
> **Authors**: Gleb Novikov
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 数据结构和算法,机器学习,统计理论,机器学习
- **Abstract**: We study the problem of computationally efficient robust estimation of scatter matrices of elliptical distributions under the strong contamination model. We design polynomial time algorithms that achieve dimension-independent error in Frobenius norm. Our first result is a sequence of efficient algorithms that approaches nearly optimal error. Specifically, under a mild assumption on the eigenvalues of the scatter matrix $Σ$, for every $t \in \mathbb{N}$, we design an estimator that, given $n = d^{O(t)}$ samples, in time $n^{O(t)}$ finds $\hatΣ$ such that $ \Vert{Σ^{-1/2}\, ({\hatΣ - Σ})\, Σ^{-1/2}}\Vert_{\text{F}} \le O(t \cdot \varepsilon^{1-\frac{1}{t}})$, where $\varepsilon$ is the fraction of corruption. We do not require any assumptions on the moments of the distribution, while all previously known computationally efficient algorithms for robust covariance/scatter estimation with dimension-independent error rely on strong assumptions on the moments, such as sub-Gaussianity or (certifiable) hypercontractivity. Furthermore, under a stronger assumption on the eigenvalues of $Σ$ (that, in particular, is satisfied by all matrices with constant condition number), we provide a fast (sub-quadratic in the input size) algorithm that, given nearly optimal number of samples $n = \tilde{O}(d^2/\varepsilon)$, in time $\tilde{O}({nd^2 poly(1/\varepsilon)})$ finds $\hatΣ$ such that $\Vert\hatΣ - Σ\Vert_{\text{F}} \le O(\VertΣ\Vert \cdot \sqrt{\varepsilon})$. Our approach is based on robust covariance estimation of the spatial sign (the projection onto the sphere of radius $\sqrt{d}$) of elliptical distributions.

### On the query complexity of sampling from non-log-concave distributions 
[[arxiv](https://arxiv.org/abs/2502.06200)] [[cool](https://papers.cool/arxiv/2502.06200)] [[pdf](https://arxiv.org/pdf/2502.06200)]
> **Authors**: Yuchen He,Chihao Zhang
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 数据结构和算法,机器学习,机器学习
- **Abstract**: We study the problem of sampling from a $d$-dimensional distribution with density $p(x)\propto e^{-f(x)}$, which does not necessarily satisfy good isoperimetric conditions. Specifically, we show that for any $L,M$ satisfying $LM\ge d\ge 5$, $ε\in \left(0,\frac{1}{32}\right)$, and any algorithm with query accesses to the value of $f(x)$ and $\nabla f(x)$, there exists an $L$-log-smooth distribution with second moment at most $M$ such that the algorithm requires $\left(\frac{LM}{dε}\right)^{Ω(d)}$ queries to compute a sample whose distribution is within $ε$ in total variation distance to the target distribution. We complement the lower bound with an algorithm requiring $\left(\frac{LM}{dε}\right)^{\mathcal O(d)}$ queries, thereby characterizing the tight (up to the constant in the exponent) query complexity for sampling from the family of non-log-concave distributions. Our results are in sharp contrast with the recent work of Huang et al. (COLT'24), where an algorithm with quasi-polynomial query complexity was proposed for sampling from a non-log-concave distribution when $M=\mathtt{poly}(d)$. Their algorithm works under the stronger condition that all distributions along the trajectory of the Ornstein-Uhlenbeck process, starting from the target distribution, are $\mathcal O(1)$-log-smooth. We investigate this condition and prove that it is strictly stronger than requiring the target distribution to be $\mathcal O(1)$-log-smooth. Additionally, we study this condition in the context of mixtures of Gaussians. Finally, we place our results within the broader theme of ``sampling versus optimization'', as studied in Ma et al. (PNAS'19). We show that for a wide range of parameters, sampling is strictly easier than optimization by a super-exponential factor in the dimension $d$.

## 新兴技术(cs.ET:Emerging Technologies)

### Low-power Spike-based Wearable Analytics on RRAM Crossbars 
[[arxiv](https://arxiv.org/abs/2502.06736)] [[cool](https://papers.cool/arxiv/2502.06736)] [[pdf](https://arxiv.org/pdf/2502.06736)]
> **Authors**: Abhiroop Bhattacharjee,Jinquan Shi,Wei-Chen Chen,Xinxin Wang,Priyadarshini Panda
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Accepted in 2025 IEEE International Symposium on Circuits and Systems (ISCAS)
- **标题**: None
- **领域**: 新兴技术,人工智能,硬件架构
- **Abstract**: This work introduces a spike-based wearable analytics system utilizing Spiking Neural Networks (SNNs) deployed on an In-memory Computing engine based on RRAM crossbars, which are known for their compactness and energy-efficiency. Given the hardware constraints and noise characteristics of the underlying RRAM crossbars, we propose online adaptation of pre-trained SNNs in real-time using Direct Feedback Alignment (DFA) against traditional backpropagation (BP). Direct Feedback Alignment (DFA) learning, that allows layer-parallel gradient computations, acts as a fast, energy & area-efficient method for online adaptation of SNNs on RRAM crossbars, unleashing better algorithmic performance against those adapted using BP. Through extensive simulations using our in-house hardware evaluation engine called DFA_Sim, we find that DFA achieves upto 64.1% lower energy consumption, 10.1% lower area overhead, and a 2.1x reduction in latency compared to BP, while delivering upto 7.55% higher inference accuracy on human activity recognition (HAR) tasks.

## 图形(cs.GR:Graphics)

### PyPotteryInk: One-Step Diffusion Model for Sketch to Publication-ready Archaeological Drawings 
[[arxiv](https://arxiv.org/abs/2502.06897)] [[cool](https://papers.cool/arxiv/2502.06897)] [[pdf](https://arxiv.org/pdf/2502.06897)]
> **Authors**: Lorenzo Cardarelli
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 图形,人工智能,计算机视觉和模式识别
- **Abstract**: Archaeological pottery documentation traditionally requires a time-consuming manual process of converting pencil sketches into publication-ready inked drawings. I present PyPotteryInk, an open-source automated pipeline that transforms archaeological pottery sketches into standardised publication-ready drawings using a one-step diffusion model. Built on a modified img2img-turbo architecture, the system processes drawings in a single forward pass while preserving crucial morphological details and maintaining archaeologic documentation standards and analytical value. The model employs an efficient patch-based approach with dynamic overlap, enabling high-resolution output regardless of input drawing size. I demonstrate the effectiveness of the approach on a dataset of Italian protohistoric pottery drawings, where it successfully captures both fine details like decorative patterns and structural elements like vessel profiles or handling elements. Expert evaluation confirms that the generated drawings meet publication standards while significantly reducing processing time from hours to seconds per drawing. The model can be fine-tuned to adapt to different archaeological contexts with minimal training data, making it versatile across various pottery documentation styles. The pre-trained models, the Python library and comprehensive documentation are provided to facilitate adoption within the archaeological research community.

## 计算机科学与博弈论(cs.GT:Computer Science and Game Theory)

### Incentivizing Desirable Effort Profiles in Strategic Classification: The Role of Causality and Uncertainty 
[[arxiv](https://arxiv.org/abs/2502.06749)] [[cool](https://papers.cool/arxiv/2502.06749)] [[pdf](https://arxiv.org/pdf/2502.06749)]
> **Authors**: Valia Efthymiou,Chara Podimata,Diptangshu Sen,Juba Ziani
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算机科学与博弈论,计算机与社会,机器学习
- **Abstract**: We study strategic classification in binary decision-making settings where agents can modify their features in order to improve their classification outcomes. Importantly, our work considers the causal structure across different features, acknowledging that effort in a given feature may affect other features. The main goal of our work is to understand \emph{when and how much agent effort is invested towards desirable features}, and how this is influenced by the deployed classifier, the causal structure of the agent's features, their ability to modify them, and the information available to the agent about the classifier and the feature causal graph. In the complete information case, when agents know the classifier and the causal structure of the problem, we derive conditions ensuring that rational agents focus on features favored by the principal. We show that designing classifiers to induce desirable behavior is generally non-convex, though tractable in special cases. We also extend our analysis to settings where agents have incomplete information about the classifier or the causal graph. While optimal effort selection is again a non-convex problem under general uncertainty, we highlight special cases of partial uncertainty where this selection problem becomes tractable. Our results indicate that uncertainty drives agents to favor features with higher expected importance and lower variance, potentially misaligning with principal preferences. Finally, numerical experiments based on a cardiovascular disease risk study illustrate how to incentivize desirable modifications under uncertainty.

## 人机交互(cs.HC:Human-Computer Interaction)

### Content-Driven Local Response: Supporting Sentence-Level and Message-Level Mobile Email Replies With and Without AI 
[[arxiv](https://arxiv.org/abs/2502.06430)] [[cool](https://papers.cool/arxiv/2502.06430)] [[pdf](https://arxiv.org/pdf/2502.06430)]
> **Authors**: Tim Zindulka,Sven Goller,Florian Lehmann,Daniel Buschek
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 23 pages, 14 figures, 2 tables, ACM CHI 2025
- **标题**: None
- **领域**: 人机交互,计算语言学
- **Abstract**: Mobile emailing demands efficiency in diverse situations, which motivates the use of AI. However, generated text does not always reflect how people want to respond. This challenges users with AI involvement tradeoffs not yet considered in email UIs. We address this with a new UI concept called Content-Driven Local Response (CDLR), inspired by microtasking. This allows users to insert responses into the email by selecting sentences, which additionally serves to guide AI suggestions. The concept supports combining AI for local suggestions and message-level improvements. Our user study (N=126) compared CDLR with manual typing and full reply generation. We found that CDLR supports flexible workflows with varying degrees of AI involvement, while retaining the benefits of reduced typing and errors. This work contributes a new approach to integrating AI capabilities: By redesigning the UI for workflows with and without AI, we can empower users to dynamically adjust AI involvement.

## 信息检索(cs.IR:Information Retrieval)

### Solving the Content Gap in Roblox Game Recommendations: LLM-Based Profile Generation and Reranking 
[[arxiv](https://arxiv.org/abs/2502.06802)] [[cool](https://papers.cool/arxiv/2502.06802)] [[pdf](https://arxiv.org/pdf/2502.06802)]
> **Authors**: Chen Wang,Xiaokai Wei,Yexi Jiang,Frank Ong,Kevin Gao,Xiao Yu,Zheng Hui,Se-eun Yoon,Philip Yu,Michelle Gong
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,人工智能,计算语言学,机器学习
- **Abstract**: With the vast and dynamic user-generated content on Roblox, creating effective game recommendations requires a deep understanding of game content. Traditional recommendation models struggle with the inconsistent and sparse nature of game text features such as titles and descriptions. Recent advancements in large language models (LLMs) offer opportunities to enhance recommendation systems by analyzing in-game text data. This paper addresses two challenges: generating high-quality, structured text features for games without extensive human annotation, and validating these features to ensure they improve recommendation relevance. We propose an approach that extracts in-game text and uses LLMs to infer attributes such as genre and gameplay objectives from raw player interactions. Additionally, we introduce an LLM-based re-ranking mechanism to assess the effectiveness of the generated text features, enhancing personalization and user satisfaction. Beyond recommendations, our approach supports applications such as user engagement-based integrity detection, already deployed in production. This scalable framework demonstrates the potential of in-game text understanding to improve recommendation quality on Roblox and adapt recommendations to its unique, user-generated ecosystem.

### Evaluating Entity Retrieval in Electronic Health Records: a Semantic Gap Perspective 
[[arxiv](https://arxiv.org/abs/2502.06252)] [[cool](https://papers.cool/arxiv/2502.06252)] [[pdf](https://arxiv.org/pdf/2502.06252)]
> **Authors**: Zhengyun Zhao,Hongyi Yuan,Jingjing Liu,Haichao Chen,Huaiyuan Ying,Songchi Zhou,Sheng Yu
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Under review, and the dataset will be made public upon reception of our paper
- **标题**: None
- **领域**: 信息检索,计算语言学
- **Abstract**: Entity retrieval plays a crucial role in the utilization of Electronic Health Records (EHRs) and is applied across a wide range of clinical practices. However, a comprehensive evaluation of this task is lacking due to the absence of a public benchmark. In this paper, we propose the development and release of a novel benchmark for evaluating entity retrieval in EHRs, with a particular focus on the semantic gap issue. Using discharge summaries from the MIMIC-III dataset, we incorporate ICD codes and prescription labels associated with the notes as queries, and annotate relevance judgments using GPT-4. In total, we use 1,000 patient notes, generate 1,246 queries, and provide over 77,000 relevance annotations. To offer the first assessment of the semantic gap, we introduce a novel classification system for relevance matches. Leveraging GPT-4, we categorize each relevant pair into one of five categories: string, synonym, abbreviation, hyponym, and implication. Using the proposed benchmark, we evaluate several retrieval methods, including BM25, query expansion, and state-of-the-art dense retrievers. Our findings show that BM25 provides a strong baseline but struggles with semantic matches. Query expansion significantly improves performance, though it slightly reduces string match capabilities. Dense retrievers outperform traditional methods, particularly for semantic matches, and general-domain dense retrievers often surpass those trained specifically in the biomedical domain.

## 信息论(cs.IT:Information Theory)

### Game of Coding With an Unknown Adversary 
[[arxiv](https://arxiv.org/abs/2502.07109)] [[cool](https://papers.cool/arxiv/2502.07109)] [[pdf](https://arxiv.org/pdf/2502.07109)]
> **Authors**: Hanzaleh Akbarinodehi,Parsa Moradi,Mohammad Ali Maddah-Ali
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 信息论,机器学习
- **Abstract**: Motivated by emerging decentralized applications, the \emph{game of coding} framework has been recently introduced to address scenarios where the adversary's control over coded symbols surpasses the fundamental limits of traditional coding theory. Still, the reward mechanism available in decentralized systems, motivates the adversary to act rationally. While the decoder, as the data collector (DC), has an acceptance and rejection mechanism, followed by an estimation module, the adversary aims to maximize its utility, as an increasing function of (1) the chance of acceptance (to increase the reward), and (2) estimation error. On the other hand, the decoder also adjusts its acceptance rule to maximize its own utility, as (1) an increasing function of the chance of acceptance (to keep the system functional), (2) decreasing function of the estimation error. Prior works within this framework rely on the assumption that the game is complete, that is, both the DC and the adversary are fully aware of each other's utility functions. However, in practice, the decoder is often unaware of the utility of the adversary. To address this limitation, we develop an algorithm enabling the DC to commit to a strategy that achieves within the vicinity of the equilibrium, without knowledge of the adversary's utility function. Our approach builds on an observation that at the equilibrium, the relationship between the probability of acceptance and the mean squared error (MSE) follows a predetermined curve independent of the specific utility functions of the players. By exploiting this invariant relationship, the DC can iteratively refine its strategy based on observable parameters, converging to a near-optimal solution. We provide theoretical guarantees on sample complexity and accuracy of the proposed scheme.

## 机器学习(cs.LG:Machine Learning)

### NARCE: A Mamba-Based Neural Algorithmic Reasoner Framework for Online Complex Event Detection 
[[arxiv](https://arxiv.org/abs/2502.07250)] [[cool](https://papers.cool/arxiv/2502.07250)] [[pdf](https://arxiv.org/pdf/2502.07250)]
> **Authors**: Liying Han,Gaofeng Dong,Xiaomin Ouyang,Lance Kaplan,Federico Cerutti,Mani Srivastava
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Current machine learning models excel in short-span perception tasks but struggle to derive high-level insights from long-term observation, a capability central to understanding complex events (CEs). CEs, defined as sequences of short-term atomic events (AEs) governed by spatiotemporal rules, are challenging to detect online due to the need to extract meaningful patterns from long and noisy sensor data while ignoring irrelevant events. We hypothesize that state-based methods are well-suited for CE detection, as they capture event progression through state transitions without requiring long-term memory. Baseline experiments validate this, demonstrating that the state-space model Mamba outperforms existing architectures. However, Mamba's reliance on extensive labeled data, which are difficult to obtain, motivates our second hypothesis: decoupling CE rule learning from noisy sensor data can reduce data requirements. To address this, we propose NARCE, a framework that combines Neural Algorithmic Reasoning (NAR) to split the task into two components: (i) learning CE rules independently of sensor data using synthetic concept traces generated by LLMs and (ii) mapping sensor inputs to these rules via an adapter. Our results show that NARCE outperforms baselines in accuracy, generalization to unseen and longer sensor data, and data efficiency, significantly reducing annotation costs while advancing robust CE detection.

### Linear Transformers as VAR Models: Aligning Autoregressive Attention Mechanisms with Autoregressive Forecasting 
[[arxiv](https://arxiv.org/abs/2502.07244)] [[cool](https://papers.cool/arxiv/2502.07244)] [[pdf](https://arxiv.org/pdf/2502.07244)]
> **Authors**: Jiecheng Lu,Shihao Yang
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Autoregressive attention-based time series forecasting (TSF) has drawn increasing interest, with mechanisms like linear attention sometimes outperforming vanilla attention. However, deeper Transformer architectures frequently misalign with autoregressive objectives, obscuring the underlying VAR structure embedded within linear attention and hindering their ability to capture the data generative processes in TSF. In this work, we first show that a single linear attention layer can be interpreted as a dynamic vector autoregressive (VAR) structure. We then explain that existing multi-layer Transformers have structural mismatches with the autoregressive forecasting objective, which impair interpretability and generalization ability. To address this, we show that by rearranging the MLP, attention, and input-output flow, multi-layer linear attention can also be aligned as a VAR model. Then, we propose Structural Aligned Mixture of VAR (SAMoVAR), a linear Transformer variant that integrates interpretable dynamic VAR weights for multivariate TSF. By aligning the Transformer architecture with autoregressive objectives, SAMoVAR delivers improved performance, interpretability, and computational efficiency, comparing to SOTA TSF models.

### DrugImproverGPT: A Large Language Model for Drug Optimization with Fine-Tuning via Structured Policy Optimization 
[[arxiv](https://arxiv.org/abs/2502.07237)] [[cool](https://papers.cool/arxiv/2502.07237)] [[pdf](https://arxiv.org/pdf/2502.07237)]
> **Authors**: Xuefeng Liu,Songhao Jiang,Siyu Chen,Zhuoran Yang,Yuxin Chen,Ian Foster,Rick Stevens
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学,生物分子,机器学习
- **Abstract**: Finetuning a Large Language Model (LLM) is crucial for generating results towards specific objectives. This research delves into the realm of drug optimization and introduce a novel reinforcement learning algorithm to finetune a drug optimization LLM-based generative model, enhancing the original drug across target objectives, while retains the beneficial chemical properties of the original drug. This work is comprised of two primary components: (1) DrugImprover: A framework tailored for improving robustness and efficiency in drug optimization. It includes a LLM designed for drug optimization and a novel Structured Policy Optimization (SPO) algorithm, which is theoretically grounded. This algorithm offers a unique perspective for fine-tuning the LLM-based generative model by aligning the improvement of the generated molecule with the input molecule under desired objectives. (2) A dataset of 1 million compounds, each with OEDOCK docking scores on 5 human proteins associated with cancer cells and 24 binding sites from SARS-CoV-2 virus. We conduct a comprehensive evaluation of SPO and demonstrate its effectiveness in improving the original drug across target properties. Our code and dataset will be publicly available at: https://github.com/xuefeng-cs/DrugImproverGPT.

### Simplifying Adversarially Robust PAC Learning with Tolerance 
[[arxiv](https://arxiv.org/abs/2502.07232)] [[cool](https://papers.cool/arxiv/2502.07232)] [[pdf](https://arxiv.org/pdf/2502.07232)]
> **Authors**: Hassan Ashtiani,Vinayak Pathak,Ruth Urner
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Adversarially robust PAC learning has proved to be challenging, with the currently best known learners [Montasser et al., 2021a] relying on improper methods based on intricate compression schemes, resulting in sample complexity exponential in the VC-dimension. A series of follow up work considered a slightly relaxed version of the problem called adversarially robust learning with tolerance [Ashtiani et al., 2023, Bhattacharjee et al., 2023, Raman et al., 2024] and achieved better sample complexity in terms of the VC-dimension. However, those algorithms were either improper and complex, or required additional assumptions on the hypothesis class H. We prove, for the first time, the existence of a simpler learner that achieves a sample complexity linear in the VC-dimension without requiring additional assumptions on H. Even though our learner is improper, it is "almost proper" in the sense that it outputs a hypothesis that is "similar" to a hypothesis in H. We also use the ideas from our algorithm to construct a semi-supervised learner in the tolerant setting. This simple algorithm achieves comparable bounds to the previous (non-tolerant) semi-supervised algorithm of Attias et al. [2022a], but avoids the use of intricate subroutines from previous works, and is "almost proper."

### A Memory Efficient Randomized Subspace Optimization Method for Training Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.07222)] [[cool](https://papers.cool/arxiv/2502.07222)] [[pdf](https://arxiv.org/pdf/2502.07222)]
> **Authors**: Yiming Chen,Yuan Zhang,Yin Liu,Kun Yuan,Zaiwen Wen
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The memory challenges associated with training Large Language Models (LLMs) have become a critical concern, particularly when using the Adam optimizer. To address this issue, numerous memory-efficient techniques have been proposed, with GaLore standing out as a notable example designed to reduce the memory footprint of optimizer states. However, these approaches do not alleviate the memory burden imposed by activations, rendering them unsuitable for scenarios involving long context sequences or large mini-batches. Moreover, their convergence properties are still not well-understood in the literature. In this work, we introduce a Randomized Subspace Optimization framework for pre-training and fine-tuning LLMs. Our approach decomposes the high-dimensional training problem into a series of lower-dimensional subproblems. At each iteration, a random subspace is selected, and the parameters within that subspace are optimized. This structured reduction in dimensionality allows our method to simultaneously reduce memory usage for both activations and optimizer states. We establish comprehensive convergence guarantees and derive rates for various scenarios, accommodating different optimization strategies to solve the subproblems. Extensive experiments validate the superior memory and communication efficiency of our method, achieving performance comparable to GaLore and Adam.

### LUNAR: LLM Unlearning via Neural Activation Redirection 
[[arxiv](https://arxiv.org/abs/2502.07218)] [[cool](https://papers.cool/arxiv/2502.07218)] [[pdf](https://arxiv.org/pdf/2502.07218)]
> **Authors**: William F. Shen,Xinchi Qiu,Meghdad Kurmanji,Alex Iacob,Lorenzo Sani,Yihong Chen,Nicola Cancedda,Nicholas D. Lane
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Large Language Models (LLMs) benefit from training on ever larger amounts of textual data, but as a result, they increasingly incur the risk of leaking private information. The ability to selectively remove knowledge from LLMs is, therefore, a highly desirable capability. In this paper, we propose LUNAR, a novel unlearning methodology grounded in the Linear Representation Hypothesis. LUNAR operates by redirecting the representations of unlearned data to regions that trigger the model's inherent ability to express its inability to answer. LUNAR achieves state-of-the-art unlearning performance while significantly enhancing the controllability of the unlearned model during inference. Specifically, LUNAR achieves between 2.9x to 11.7x improvements on combined "unlearning efficacy" and "model utility" score ("Deviation Score") on the PISTOL dataset across various base models. We also demonstrate, through quantitative analysis and qualitative examples, LUNAR's superior controllability in generating coherent and contextually aware responses, mitigating undesired side effects of existing methods. Moreover, we demonstrate that LUNAR is robust against white-box adversarial attacks and versatile in handling real-world scenarios, such as processing sequential unlearning requests.

### Pareto Optimal Algorithmic Recourse in Multi-cost Function 
[[arxiv](https://arxiv.org/abs/2502.07214)] [[cool](https://papers.cool/arxiv/2502.07214)] [[pdf](https://arxiv.org/pdf/2502.07214)]
> **Authors**: Wen-Ling Chen,Hong-Chang Huang,Kai-Hung Lin,Shang-Wei Hwang,Hao-Tsung Yang
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,数据结构和算法
- **Abstract**: In decision-making systems, algorithmic recourse aims to identify minimal-cost actions to alter an individual features, thereby obtaining a desired outcome. This empowers individuals to understand, question, or alter decisions that negatively affect them. However, due to the variety and sensitivity of system environments and individual personalities, quantifying the cost of a single function is nearly impossible while considering multiple criteria situations. Most current recourse mechanisms use gradient-based methods that assume cost functions are differentiable, often not applicable in real-world scenarios, resulting in sub-optimal solutions that compromise various criteria. These solutions are typically intractable and lack rigorous theoretical foundations, raising concerns regarding interpretability, reliability, and transparency from the explainable AI (XAI) perspective. To address these issues, this work proposes an algorithmic recourse framework that handles non-differentiable and discrete multi-cost functions. By formulating recourse as a multi-objective optimization problem and assigning weights to different criteria based on their importance, our method identifies Pareto optimal recourse recommendations. To demonstrate scalability, we incorporate the concept of epsilon-net, proving the ability to find approximated Pareto optimal actions. Experiments show the trade-off between different criteria and the methods scalability in large graphs. Compared to current heuristic practices, our approach provides a stronger theoretical foundation and better aligns recourse suggestions with real-world requirements.

### Evaluation for Regression Analyses on Evolving Data Streams 
[[arxiv](https://arxiv.org/abs/2502.07213)] [[cool](https://papers.cool/arxiv/2502.07213)] [[pdf](https://arxiv.org/pdf/2502.07213)]
> **Authors**: Yibin Sun,Heitor Murilo Gomes,Bernhard Pfahringer,Albert Bifet
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 11 Pages, 9 figures
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: The paper explores the challenges of regression analysis in evolving data streams, an area that remains relatively underexplored compared to classification. We propose a standardized evaluation process for regression and prediction interval tasks in streaming contexts. Additionally, we introduce an innovative drift simulation strategy capable of synthesizing various drift types, including the less-studied incremental drift. Comprehensive experiments with state-of-the-art methods, conducted under the proposed process, validate the effectiveness and robustness of our approach.

### Improve the Training Efficiency of DRL for Wireless Communication Resource Allocation: The Role of Generative Diffusion Models 
[[arxiv](https://arxiv.org/abs/2502.07211)] [[cool](https://papers.cool/arxiv/2502.07211)] [[pdf](https://arxiv.org/pdf/2502.07211)]
> **Authors**: Xinren Zhang,Jiadong Yu
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Dynamic resource allocation in mobile wireless networks involves complex, time-varying optimization problems, motivating the adoption of deep reinforcement learning (DRL). However, most existing works rely on pre-trained policies, overlooking dynamic environmental changes that rapidly invalidate the policies. Periodic retraining becomes inevitable but incurs prohibitive computational costs and energy consumption-critical concerns for resource-constrained wireless systems. We identify three root causes of inefficient retraining: high-dimensional state spaces, suboptimal action spaces exploration-exploitation trade-offs, and reward design limitations. To overcome these limitations, we propose Diffusion-based Deep Reinforcement Learning (D2RL), which leverages generative diffusion models (GDMs) to holistically enhance all three DRL components. Iterative refinement process and distribution modelling of GDMs enable (1) the generation of diverse state samples to improve environmental understanding, (2) balanced action space exploration to escape local optima, and (3) the design of discriminative reward functions that better evaluate action quality. Our framework operates in two modes: Mode I leverages GDMs to explore reward spaces and design discriminative reward functions that rigorously evaluate action quality, while Mode II synthesizes diverse state samples to enhance environmental understanding and generalization. Extensive experiments demonstrate that D2RL achieves faster convergence and reduced computational costs over conventional DRL methods for resource allocation in wireless communications while maintaining competitive policy performance. This work underscores the transformative potential of GDMs in overcoming fundamental DRL training bottlenecks for wireless networks, paving the way for practical, real-time deployments.

### Enhancing Physics-Informed Neural Networks Through Feature Engineering 
[[arxiv](https://arxiv.org/abs/2502.07209)] [[cool](https://papers.cool/arxiv/2502.07209)] [[pdf](https://arxiv.org/pdf/2502.07209)]
> **Authors**: Shaghayegh Fazliani,Zachary Frangella,Madeleine Udell
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Physics-Informed Neural Networks (PINNs) seek to solve partial differential equations (PDEs) with deep learning. Mainstream approaches that deploy fully-connected multi-layer deep learning architectures require prolonged training to achieve even moderate accuracy, while recent work on feature engineering allows higher accuracy and faster convergence. This paper introduces SAFE-NET, a Single-layered Adaptive Feature Engineering NETwork that achieves orders-of-magnitude lower errors with far fewer parameters than baseline feature engineering methods. SAFE-NET returns to basic ideas in machine learning, using Fourier features, a simplified single hidden layer network architecture, and an effective optimizer that improves the conditioning of the PINN optimization problem. Numerical results show that SAFE-NET converges faster and typically outperforms deeper networks and more complex architectures. It consistently uses fewer parameters -- on average, 65% fewer than the competing feature engineering methods -- while achieving comparable accuracy in less than 30% of the training epochs. Moreover, each SAFE-NET epoch is 95% faster than those of competing feature engineering approaches. These findings challenge the prevailing belief that modern PINNs effectively learn features in these scientific applications and highlight the efficiency gains possible through feature engineering.

### Fixed-Confidence Best Arm Identification with Decreasing Variance 
[[arxiv](https://arxiv.org/abs/2502.07199)] [[cool](https://papers.cool/arxiv/2502.07199)] [[pdf](https://arxiv.org/pdf/2502.07199)]
> **Authors**: Tamojeet Roychowdhury,Kota Srinivas Reddy,Krishna P Jagannathan,Sharayu Moharir
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 6 pages, 2 figures, accepted in the National Conference on Communications 2025
- **标题**: None
- **领域**: 机器学习,信息论,统计理论,机器学习
- **Abstract**: We focus on the problem of best-arm identification in a stochastic multi-arm bandit with temporally decreasing variances for the arms' rewards. We model arm rewards as Gaussian random variables with fixed means and variances that decrease with time. The cost incurred by the learner is modeled as a weighted sum of the time needed by the learner to identify the best arm, and the number of samples of arms collected by the learner before termination. Under this cost function, there is an incentive for the learner to not sample arms in all rounds, especially in the initial rounds. On the other hand, not sampling increases the termination time of the learner, which also increases cost. This trade-off necessitates new sampling strategies. We propose two policies. The first policy has an initial wait period with no sampling followed by continuous sampling. The second policy samples periodically and uses a weighted average of the rewards observed to identify the best arm. We provide analytical guarantees on the performance of both policies and supplement our theoretical results with simulations which show that our polices outperform the state-of-the-art policies for the classical best arm identification problem.

### Provably Efficient RLHF Pipeline: A Unified View from Contextual Bandits 
[[arxiv](https://arxiv.org/abs/2502.07193)] [[cool](https://papers.cool/arxiv/2502.07193)] [[pdf](https://arxiv.org/pdf/2502.07193)]
> **Authors**: Long-Fei Li,Yu-Yang Qian,Peng Zhao,Zhi-Hua Zhou
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Reinforcement Learning from Human Feedback (RLHF) is a widely used approach for aligning Large Language Models (LLMs) with human preferences. While recent advancements have provided valuable insights into various stages and settings of RLHF, a comprehensive theoretical understanding of the entire RLHF pipeline remains lacking. Towards this end, we propose a unified framework for the RLHF pipeline from the view of contextual bandits and provide provable efficiency guarantees. In particular, we decompose the RLHF process into two distinct stages: (post-)training and deployment, exploring both passive and active data collection strategies during the training phase. By employing the Bradley-Terry preference model with a linearly parameterized reward function, we reformulate RLHF as a contextual preference bandit problem. We then develop novel algorithms for each stage, demonstrating significant improvements over existing approaches in both statistical and computational efficiency. Finally, we apply our method to train and deploy Llama-3-8B-Instruct on the Ultrafeedback-binarized dataset, and empirical results confirm the effectiveness of our approach.

### Exploring Neural Network Pruning with Screening Methods 
[[arxiv](https://arxiv.org/abs/2502.07189)] [[cool](https://papers.cool/arxiv/2502.07189)] [[pdf](https://arxiv.org/pdf/2502.07189)]
> **Authors**: Mingyuan Wang,Yangzi Guo,Sida Liu,Yanwen Xiao
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: This work has been submitted to the IEEE for possible publication
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Deep neural networks (DNNs) such as convolutional neural networks (CNNs) for visual tasks, recurrent neural networks (RNNs) for sequence data, and transformer models for rich linguistic or multimodal tasks, achieved unprecedented performance on a wide range of tasks. The impressive performance of modern DNNs is partially attributed to their sheer scale. The latest deep learning models have tens to hundreds of millions of parameters which makes the inference processes resource-intensive. The high computational complexity of these networks prevents their deployment on resource-limited devices such as mobile platforms, IoT devices, and edge computing systems because these devices require energy-efficient and real-time processing capabilities. This paper proposes and evaluates a network pruning framework that eliminates non-essential parameters based on a statistical analysis of network component significance across classification categories. The proposed method uses screening methods coupled with a weighted scheme to assess connection and channel contributions for unstructured and structured pruning which allows for the elimination of unnecessary network elements without significantly degrading model performance. Extensive experimental validation on real-world vision datasets for both fully connected neural networks (FNNs) and CNNs has shown that the proposed framework produces competitive lean networks compared to the original networks. Moreover, the proposed framework outperforms state-of-art network pruning methods in two out of three cases.

### Local Regularizers Are Not Transductive Learners 
[[arxiv](https://arxiv.org/abs/2502.07187)] [[cool](https://papers.cool/arxiv/2502.07187)] [[pdf](https://arxiv.org/pdf/2502.07187)]
> **Authors**: Sky Jafar,Julian Asilis,Shaddin Dughmi
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 16 pages
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: We partly resolve an open question raised by Asilis et al. (COLT 2024): whether the algorithmic template of local regularization -- an intriguing generalization of explicit regularization, a.k.a. structural risk minimization -- suffices to learn all learnable multiclass problems. Specifically, we provide a negative answer to this question in the transductive model of learning. We exhibit a multiclass classification problem which is learnable in both the transductive and PAC models, yet cannot be learned transductively by any local regularizer. The corresponding hypothesis class, and our proof, are based on principles from cryptographic secret sharing. We outline challenges in extending our negative result to the PAC model, leaving open the tantalizing possibility of a PAC/transductive separation with respect to local regularization.

### Tab2Visual: Overcoming Limited Data in Tabular Data Classification Using Deep Learning with Visual Representations 
[[arxiv](https://arxiv.org/abs/2502.07181)] [[cool](https://papers.cool/arxiv/2502.07181)] [[pdf](https://arxiv.org/pdf/2502.07181)]
> **Authors**: Ahmed Mamdouh,Moumen El-Melegy,Samia Ali,Ron Kikinis
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: This research addresses the challenge of limited data in tabular data classification, particularly prevalent in domains with constraints like healthcare. We propose Tab2Visual, a novel approach that transforms heterogeneous tabular data into visual representations, enabling the application of powerful deep learning models. Tab2Visual effectively addresses data scarcity by incorporating novel image augmentation techniques and facilitating transfer learning. We extensively evaluate the proposed approach on diverse tabular datasets, comparing its performance against a wide range of machine learning algorithms, including classical methods, tree-based ensembles, and state-of-the-art deep learning models specifically designed for tabular data. We also perform an in-depth analysis of factors influencing Tab2Visual's performance. Our experimental results demonstrate that Tab2Visual outperforms other methods in classification problems with limited tabular data.

### MatrixKAN: Parallelized Kolmogorov-Arnold Network 
[[arxiv](https://arxiv.org/abs/2502.07176)] [[cool](https://papers.cool/arxiv/2502.07176)] [[pdf](https://arxiv.org/pdf/2502.07176)]
> **Authors**: Cale Coffman,Lizhong Chen
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Kolmogorov-Arnold Networks (KAN) are a new class of neural network architecture representing a promising alternative to the Multilayer Perceptron (MLP), demonstrating improved expressiveness and interpretability. However, KANs suffer from slow training and inference speeds relative to MLPs due in part to the recursive nature of the underlying B-spline calculations. This issue is particularly apparent with respect to KANs utilizing high-degree B-splines, as the number of required non-parallelizable recursions is proportional to B-spline degree. We solve this issue by proposing MatrixKAN, a novel optimization that parallelizes B-spline calculations with matrix representation and operations, thus significantly improving effective computation time for models utilizing high-degree B-splines. In this paper, we demonstrate the superior scaling of MatrixKAN's computation time relative to B-spline degree. Further, our experiments demonstrate speedups of approximately 40x relative to KAN, with significant additional speedup potential for larger datasets or higher spline degrees.

### Early Risk Prediction of Pediatric Cardiac Arrest from Electronic Health Records via Multimodal Fused Transformer 
[[arxiv](https://arxiv.org/abs/2502.07158)] [[cool](https://papers.cool/arxiv/2502.07158)] [[pdf](https://arxiv.org/pdf/2502.07158)]
> **Authors**: Jiaying Lu,Stephanie R. Brown,Songyuan Liu,Shifan Zhao,Kejun Dong,Del Bold,Michael Fundora,Alaa Aljiffry,Alex Fedorov,Jocelyn Grunwell,Xiao Hu
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Early prediction of pediatric cardiac arrest (CA) is critical for timely intervention in high-risk intensive care settings. We introduce PedCA-FT, a novel transformer-based framework that fuses tabular view of EHR with the derived textual view of EHR to fully unleash the interactions of high-dimensional risk factors and their dynamics. By employing dedicated transformer modules for each modality view, PedCA-FT captures complex temporal and contextual patterns to produce robust CA risk estimates. Evaluated on a curated pediatric cohort from the CHOA-CICU database, our approach outperforms ten other artificial intelligence models across five key performance metrics and identifies clinically meaningful risk factors. These findings underscore the potential of multimodal fusion techniques to enhance early CA detection and improve patient care.

### Rethinking Fine-Tuning when Scaling Test-Time Compute: Limiting Confidence Improves Mathematical Reasoning 
[[arxiv](https://arxiv.org/abs/2502.07154)] [[cool](https://papers.cool/arxiv/2502.07154)] [[pdf](https://arxiv.org/pdf/2502.07154)]
> **Authors**: Feng Chen,Allan Raventos,Nan Cheng,Surya Ganguli,Shaul Druckmann
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Recent progress in large language models (LLMs) highlights the power of scaling test-time compute to achieve strong performance on complex tasks, such as mathematical reasoning and code generation. This raises a critical question: how should model training be modified to optimize performance under a subsequent test-time compute strategy and budget? To explore this, we focus on pass@N, a simple test-time strategy that searches for a correct answer in $N$ independent samples. We show, surprisingly, that training with cross-entropy (CE) loss can be ${\it misaligned}$ with pass@N in that pass@N accuracy ${\it decreases}$ with longer training. We explain the origins of this misalignment in terms of model overconfidence induced by CE, and experimentally verify our prediction of overconfidence as an impediment to scaling test-time compute via pass@N. Furthermore we suggest a principled, modified training loss that is better aligned to pass@N by limiting model confidence and rescuing pass@N test performance. Our algorithm demonstrates improved mathematical reasoning on MATH and MiniF2F benchmarks under several scenarios: (1) providing answers to math questions; and (2) proving theorems by searching over proof trees of varying shapes. Overall our work underscores the importance of co-designing two traditionally separate phases of LLM development: training-time protocols and test-time search and reasoning strategies.

### Feature Importance Depends on Properties of the Data: Towards Choosing the Correct Explanations for Your Data and Decision Trees based Models 
[[arxiv](https://arxiv.org/abs/2502.07153)] [[cool](https://papers.cool/arxiv/2502.07153)] [[pdf](https://arxiv.org/pdf/2502.07153)]
> **Authors**: Célia Wafa Ayad,Thomas Bonnier,Benjamin Bosch,Sonali Parbhoo,Jesse Read
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: In order to ensure the reliability of the explanations of machine learning models, it is crucial to establish their advantages and limits and in which case each of these methods outperform. However, the current understanding of when and how each method of explanation can be used is insufficient. To fill this gap, we perform a comprehensive empirical evaluation by synthesizing multiple datasets with the desired properties. Our main objective is to assess the quality of feature importance estimates provided by local explanation methods, which are used to explain predictions made by decision tree-based models. By analyzing the results obtained from synthetic datasets as well as publicly available binary classification datasets, we observe notable disparities in the magnitude and sign of the feature importance estimates generated by these methods. Moreover, we find that these estimates are sensitive to specific properties present in the data. Although some model hyper-parameters do not significantly influence feature importance assignment, it is important to recognize that each method of explanation has limitations in specific contexts. Our assessment highlights these limitations and provides valuable insight into the suitability and reliability of different explanatory methods in various scenarios.

### Conditional Distribution Quantization in Machine Learning 
[[arxiv](https://arxiv.org/abs/2502.07151)] [[cool](https://papers.cool/arxiv/2502.07151)] [[pdf](https://arxiv.org/pdf/2502.07151)]
> **Authors**: Blaise Delattre,Sylvain Delattre,Alexandre Vérine,Alexandre Allauzen
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Conditional expectation \mathbb{E}(Y \mid X) often fails to capture the complexity of multimodal conditional distributions \mathcal{L}(Y \mid X). To address this, we propose using n-point conditional quantizations--functional mappings of X that are learnable via gradient descent--to approximate \mathcal{L}(Y \mid X). This approach adapts Competitive Learning Vector Quantization (CLVQ), tailored for conditional distributions. It goes beyond single-valued predictions by providing multiple representative points that better reflect multimodal structures. It enables the approximation of the true conditional law in the Wasserstein distance. The resulting framework is theoretically grounded and useful for uncertainty quantification and multimodal data generation tasks. For example, in computer vision inpainting tasks, multiple plausible reconstructions may exist for the same partially observed input image X. We demonstrate the effectiveness of our approach through experiments on synthetic and real-world datasets.

### Small steps no more: Global convergence of stochastic gradient bandits for arbitrary learning rates 
[[arxiv](https://arxiv.org/abs/2502.07141)] [[cool](https://papers.cool/arxiv/2502.07141)] [[pdf](https://arxiv.org/pdf/2502.07141)]
> **Authors**: Jincheng Mei,Bo Dai,Alekh Agarwal,Sharan Vaswani,Anant Raj,Csaba Szepesvari,Dale Schuurmans
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Updated version for a paper published at NeurIPS 2024
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We provide a new understanding of the stochastic gradient bandit algorithm by showing that it converges to a globally optimal policy almost surely using \emph{any} constant learning rate. This result demonstrates that the stochastic gradient algorithm continues to balance exploration and exploitation appropriately even in scenarios where standard smoothness and noise control assumptions break down. The proofs are based on novel findings about action sampling rates and the relationship between cumulative progress and noise, and extend the current understanding of how simple stochastic gradient methods behave in bandit settings.

### Fourier-enhanced Neural Networks For Systems Biology Applications 
[[arxiv](https://arxiv.org/abs/2502.07129)] [[cool](https://papers.cool/arxiv/2502.07129)] [[pdf](https://arxiv.org/pdf/2502.07129)]
> **Authors**: Enze Xu,Minghan Chen
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,定量方法
- **Abstract**: In the field of systems biology, differential equations are commonly used to model biological systems, but solving them for large-scale and complex systems can be computationally expensive. Recently, the integration of machine learning and mathematical modeling has offered new opportunities for scientific discoveries in biology and health. The emerging physics-informed neural network (PINN) has been proposed as a solution to this problem. However, PINN can be computationally expensive and unreliable for complex biological systems. To address these issues, we propose the Fourier-enhanced Neural Networks for systems biology (SB-FNN). SB-FNN uses an embedded Fourier neural network with an adaptive activation function and a cyclic penalty function to optimize the prediction of biological dynamics, particularly for biological systems that exhibit oscillatory patterns. Experimental results demonstrate that SB-FNN achieves better performance and is more efficient than PINN for handling complex biological models. Experimental results on cellular and population models demonstrate that SB-FNN outperforms PINN in both accuracy and efficiency, making it a promising alternative approach for handling complex biological models. The proposed method achieved better performance on six biological models and is expected to replace PINN as the most advanced method in systems biology.

### Online Scheduling for LLM Inference with KV Cache Constraints 
[[arxiv](https://arxiv.org/abs/2502.07115)] [[cool](https://papers.cool/arxiv/2502.07115)] [[pdf](https://arxiv.org/pdf/2502.07115)]
> **Authors**: Patrick Jaillet,Jiashuo Jiang,Chara Podimata,Zijie Zhou
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,优化与控制
- **Abstract**: Large Language Model (LLM) inference, where a trained model generates text one word at a time in response to user prompts, is a computationally intensive process requiring efficient scheduling to optimize latency and resource utilization. A key challenge in LLM inference is the management of the Key-Value (KV) cache, which reduces redundant computations but introduces memory constraints. In this work, we model LLM inference with KV cache constraints theoretically and propose novel batching and scheduling algorithms that minimize inference latency while effectively managing the KV cache's memory. We analyze both semi-online and fully online scheduling models, and our results are threefold. First, we provide a polynomial-time algorithm that achieves exact optimality in terms of average latency in the semi-online prompt arrival model. Second, in the fully online case with a stochastic prompt arrival, we introduce an efficient online scheduling algorithm with constant regret. Third, we prove that no algorithm (deterministic or randomized) can achieve a constant competitive ratio in fully online adversarial settings. Our empirical evaluations on a public LLM inference dataset, using the Llama-70B model on A100 GPUs, show that our approach significantly outperforms benchmark algorithms used currently in practice, achieving lower latency while reducing energy consumption. Overall, our results offer a path toward more sustainable and cost-effective LLM deployment.

### Likelihood-Free Estimation for Spatiotemporal Hawkes processes with missing data and application to predictive policing 
[[arxiv](https://arxiv.org/abs/2502.07111)] [[cool](https://papers.cool/arxiv/2502.07111)] [[pdf](https://arxiv.org/pdf/2502.07111)]
> **Authors**: Pramit Das,Moulinath Banerjee,Yuekai Sun
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,应用领域,方法论
- **Abstract**: With the growing use of AI technology, many police departments use forecasting software to predict probable crime hotspots and allocate patrolling resources effectively for crime prevention. The clustered nature of crime data makes self-exciting Hawkes processes a popular modeling choice. However, one significant challenge in fitting such models is the inherent missingness in crime data due to non-reporting, which can bias the estimated parameters of the predictive model, leading to inaccurate downstream hotspot forecasts, often resulting in over or under-policing in various communities, especially the vulnerable ones. Our work introduces a Wasserstein Generative Adversarial Networks (WGAN) driven likelihood-free approach to account for unreported crimes in Spatiotemporal Hawkes models. We demonstrate through empirical analysis how this methodology improves the accuracy of parametric estimation in the presence of data missingness, leading to more reliable and efficient policing strategies.

### Evaluating the Systematic Reasoning Abilities of Large Language Models through Graph Coloring 
[[arxiv](https://arxiv.org/abs/2502.07087)] [[cool](https://papers.cool/arxiv/2502.07087)] [[pdf](https://arxiv.org/pdf/2502.07087)]
> **Authors**: Alex Heyman,Joel Zylberberg
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 23 pages (8 excluding references and appendices); 8 figures (3 excluding appendices)
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Contemporary large language models are powerful problem-solving tools, but they exhibit weaknesses in their reasoning abilities which ongoing research seeks to mitigate. We investigate graph coloring as a means of evaluating an LLM's capacities for systematic step-by-step reasoning and possibility space exploration, as well as effects of semantic problem framing. We test Claude 3.5 Sonnet, Llama 3.1 405B, Gemini 1.5 Pro, GPT-4o, o1-mini, and DeepSeek-R1 on a dataset of $k$-coloring problems with $2 \leq k \leq 4$ and vertex count $4 \leq n \leq 8$, using partial algorithmic solvers to further categorize problems by difficulty. In addition to substantial but varying framing effects, we find that all models except o1-mini and R1 exhibit $>60\%$ error rates on difficult problem types in all frames ($>15\%$ for o1-mini and $>10\%$ for R1), and no model achieves perfect accuracy even in the simple domain of 2-coloring 4-vertex graphs. Our results highlight both the considerable recent progress in LLM systematic reasoning and the limits of its reliability, especially in relation to increasing computational costs. We expect that more complex graph coloring problems, and procedural generation of arbitrary-complexity reasoning problems more broadly, offer further untapped potential for LLM benchmarking.

### Fast Clustering of Categorical Big Data 
[[arxiv](https://arxiv.org/abs/2502.07081)] [[cool](https://papers.cool/arxiv/2502.07081)] [[pdf](https://arxiv.org/pdf/2502.07081)]
> **Authors**: Bipana Thapaliya,Yu Zhuang
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 12 pages, 3 figures
- **标题**: None
- **领域**: 机器学习,数据库
- **Abstract**: The K-Modes algorithm, developed for clustering categorical data, is of high algorithmic simplicity but suffers from unreliable performances in clustering quality and clustering efficiency, both heavily influenced by the choice of initial cluster centers. In this paper, we investigate Bisecting K-Modes (BK-Modes), a successive bisecting process to find clusters, in examining how good the cluster centers out of the bisecting process will be when used as initial centers for the K-Modes. The BK-Modes works by splitting a dataset into multiple clusters iteratively with one cluster being chosen and bisected into two clusters in each iteration. We use the sum of distances of data to their cluster centers as the selection metric to choose a cluster to be bisected in each iteration. This iterative process stops when K clusters are produced. The centers of these K clusters are then used as the initial cluster centers for the K-Modes. Experimental studies of the BK-Modes were carried out and were compared against the K-Modes with multiple sets of initial cluster centers as well as the best of the existing methods we found so far in our survey. Experimental results indicated good performances of BK-Modes both in the clustering quality and efficiency for large datasets.

### Contextual Thompson Sampling via Generation of Missing Data 
[[arxiv](https://arxiv.org/abs/2502.07064)] [[cool](https://papers.cool/arxiv/2502.07064)] [[pdf](https://arxiv.org/pdf/2502.07064)]
> **Authors**: Kelly W. Zhang,Tiffany Tianhui Cai,Hongseok Namkoong,Daniel Russo
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: We introduce a framework for Thompson sampling contextual bandit algorithms, in which the algorithm's ability to quantify uncertainty and make decisions depends on the quality of a generative model that is learned offline. Instead of viewing uncertainty in the environment as arising from unobservable latent parameters, our algorithm treats uncertainty as stemming from missing, but potentially observable, future outcomes. If these future outcomes were all observed, one could simply make decisions using an "oracle" policy fit on the complete dataset. Inspired by this conceptualization, at each decision-time, our algorithm uses a generative model to probabilistically impute missing future outcomes, fits a policy using the imputed complete dataset, and uses that policy to select the next action. We formally show that this algorithm is a generative formulation of Thompson Sampling and prove a state-of-the-art regret bound for it. Notably, our regret bound i) depends on the probabilistic generative model only through the quality of its offline prediction loss, and ii) applies to any method of fitting the "oracle" policy, which easily allows one to adapt Thompson sampling to decision-making settings with fairness and/or resource constraints.

### Federated Continual Learning: Concepts, Challenges, and Solutions 
[[arxiv](https://arxiv.org/abs/2502.07059)] [[cool](https://papers.cool/arxiv/2502.07059)] [[pdf](https://arxiv.org/pdf/2502.07059)]
> **Authors**: Parisa Hamedi,Roozbeh Razavi-Far,Ehsan Hallaji
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Federated Continual Learning (FCL) has emerged as a robust solution for collaborative model training in dynamic environments, where data samples are continuously generated and distributed across multiple devices. This survey provides a comprehensive review of FCL, focusing on key challenges such as heterogeneity, model stability, communication overhead, and privacy preservation. We explore various forms of heterogeneity and their impact on model performance. Solutions to non-IID data, resource-constrained platforms, and personalized learning are reviewed in an effort to show the complexities of handling heterogeneous data distributions. Next, we review techniques for ensuring model stability and avoiding catastrophic forgetting, which are critical in non-stationary environments. Privacy-preserving techniques are another aspect of FCL that have been reviewed in this work. This survey has integrated insights from federated learning and continual learning to present strategies for improving the efficacy and scalability of FCL systems, making it applicable to a wide range of real-world scenarios.

### Boosting of Classification Models with Human-in-the-Loop Computational Visual Knowledge Discovery 
[[arxiv](https://arxiv.org/abs/2502.07039)] [[cool](https://papers.cool/arxiv/2502.07039)] [[pdf](https://arxiv.org/pdf/2502.07039)]
> **Authors**: Alice Williams,Boris Kovalerchuk
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Preprint
- **标题**: None
- **领域**: 机器学习,人机交互
- **Abstract**: High-risk artificial intelligence and machine learning classification tasks, such as healthcare diagnosis, require accurate and interpretable prediction models. However, classifier algorithms typically sacrifice individual case-accuracy for overall model accuracy, limiting analysis of class overlap areas regardless of task significance. The Adaptive Boosting meta-algorithm, which won the 2003 Gödel Prize, analytically assigns higher weights to misclassified cases to reclassify. However, it relies on weaker base classifiers that are iteratively strengthened, limiting improvements from base classifiers. Combining visual and computational approaches enables selecting stronger base classifiers before boosting. This paper proposes moving boosting methodology from focusing on only misclassified cases to all cases in the class overlap areas using Computational and Interactive Visual Learning (CIVL) with a Human-in-the-Loop. It builds classifiers in lossless visualizations integrating human domain expertise and visual insights. A Divide and Classify process splits cases to simple and complex, classifying these individually through computational analysis and data visualization with lossless visualization spaces of Parallel Coordinates or other General Line Coordinates. After finding pure and overlap class areas simple cases in pure areas are classified, generating interpretable sub-models like decision rules in Propositional and First-order Logics. Only multidimensional cases in the overlap areas are losslessly visualized simplifying end-user cognitive tasks to identify difficult case patterns, including engineering features to form new classifiable patterns. Demonstration shows a perfectly accurate and losslessly interpretable model of the Iris dataset, and simulated data shows generalized benefits to accuracy and interpretability of models, increasing end-user confidence in discovered models.

### Representational Alignment with Chemical Induced Fit for Molecular Relational Learning 
[[arxiv](https://arxiv.org/abs/2502.07027)] [[cool](https://papers.cool/arxiv/2502.07027)] [[pdf](https://arxiv.org/pdf/2502.07027)]
> **Authors**: Peiliang Zhang,Jingling Yuan,Qing Xie,Yongjun Zhu,Lin Li
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Molecular Relational Learning (MRL) is widely applied in natural sciences to predict relationships between molecular pairs by extracting structural features. The representational similarity between substructure pairs determines the functional compatibility of molecular binding sites. Nevertheless, aligning substructure representations by attention mechanisms lacks guidance from chemical knowledge, resulting in unstable model performance in chemical space (\textit{e.g.}, functional group, scaffold) shifted data. With theoretical justification, we propose the \textbf{Re}presentational \textbf{Align}ment with Chemical Induced \textbf{Fit} (ReAlignFit) to enhance the stability of MRL. ReAlignFit dynamically aligns substructure representation in MRL by introducing chemical Induced Fit-based inductive bias. In the induction process, we design the Bias Correction Function based on substructure edge reconstruction to align representations between substructure pairs by simulating chemical conformational changes (dynamic combination of substructures). ReAlignFit further integrates the Subgraph Information Bottleneck during fit process to refine and optimize substructure pairs exhibiting high chemical functional compatibility, leveraging them to generate molecular embeddings. Experimental results on nine datasets demonstrate that ReAlignFit outperforms state-of-the-art models in two tasks and significantly enhances model's stability in both rule-shifted and scaffold-shifted data distributions.

### Machine Learning for Everyone: Simplifying Healthcare Analytics with BigQuery ML 
[[arxiv](https://arxiv.org/abs/2502.07026)] [[cool](https://papers.cool/arxiv/2502.07026)] [[pdf](https://arxiv.org/pdf/2502.07026)]
> **Authors**: Mohammad Amir Salari,Bahareh Rahmani
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Focus: Artificial Intelligence, Healthcare analytics, cloud computing, BigQuery ML
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Machine learning (ML) transforms healthcare by enabling predictive analytics, personalized treatments, and improved patient outcomes. However, traditional ML workflows often require specialized skills, infrastructure, and resources, limiting accessibility for many healthcare professionals. This paper explores how BigQuery ML Cloud service helps healthcare researchers and data analysts to build and deploy models using SQL, without need for advanced ML knowledge. Our results demonstrate that the Boosted Tree model achieved the highest performance among the three models making it highly effective for diabetes prediction. BigQuery ML directly integrates predictive analytics into their workflows to inform decision-making and support patient care. We reveal this capability through a case study on diabetes prediction using the Diabetes Health Indicators Dataset. Our study underscores BigQuery ML's role in democratizing machine learning, enabling faster, scalable, and efficient predictive analytics that can directly enhance healthcare decision-making processes. This study aims to bridge the gap between advanced machine learning and practical healthcare analytics by providing detailed insights into BigQuery ML's capabilities. By demonstrating its utility in a real-world case study, we highlight its potential to simplify complex workflows and expand access to predictive tools for a broader audience of healthcare professionals.

### Detecting Neurodegenerative Diseases using Frame-Level Handwriting Embeddings 
[[arxiv](https://arxiv.org/abs/2502.07025)] [[cool](https://papers.cool/arxiv/2502.07025)] [[pdf](https://arxiv.org/pdf/2502.07025)]
> **Authors**: Sarah Laouedj,Yuzhe Wang,Jesus Villalba,Thomas Thebaud,Laureano Moro-Velazquez,Najim Dehak
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: In this study, we explored the use of spectrograms to represent handwriting signals for assessing neurodegenerative diseases, including 42 healthy controls (CTL), 35 subjects with Parkinson's Disease (PD), 21 with Alzheimer's Disease (AD), and 15 with Parkinson's Disease Mimics (PDM). We applied CNN and CNN-BLSTM models for binary classification using both multi-channel fixed-size and frame-based spectrograms. Our results showed that handwriting tasks and spectrogram channel combinations significantly impacted classification performance. The highest F1-score (89.8%) was achieved for AD vs. CTL, while PD vs. CTL reached 74.5%, and PD vs. PDM scored 77.97%. CNN consistently outperformed CNN-BLSTM. Different sliding window lengths were tested for constructing frame-based spectrograms. A 1-second window worked best for AD, longer windows improved PD classification, and window length had little effect on PD vs. PDM.

### DROP: Poison Dilution via Knowledge Distillation for Federated Learning 
[[arxiv](https://arxiv.org/abs/2502.07011)] [[cool](https://papers.cool/arxiv/2502.07011)] [[pdf](https://arxiv.org/pdf/2502.07011)]
> **Authors**: Georgios Syros,Anshuman Suri,Farinaz Koushanfar,Cristina Nita-Rotaru,Alina Oprea
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,密码学和安全,分布式、并行和集群计算
- **Abstract**: Federated Learning is vulnerable to adversarial manipulation, where malicious clients can inject poisoned updates to influence the global model's behavior. While existing defense mechanisms have made notable progress, they fail to protect against adversaries that aim to induce targeted backdoors under different learning and attack configurations. To address this limitation, we introduce DROP (Distillation-based Reduction Of Poisoning), a novel defense mechanism that combines clustering and activity-tracking techniques with extraction of benign behavior from clients via knowledge distillation to tackle stealthy adversaries that manipulate low data poisoning rates and diverse malicious client ratios within the federation. Through extensive experimentation, our approach demonstrates superior robustness compared to existing defenses across a wide range of learning configurations. Finally, we evaluate existing defenses and our method under the challenging setting of non-IID client data distribution and highlight the challenges of designing a resilient FL defense in this setting.

### Geometry-aware RL for Manipulation of Varying Shapes and Deformable Objects 
[[arxiv](https://arxiv.org/abs/2502.07005)] [[cool](https://papers.cool/arxiv/2502.07005)] [[pdf](https://arxiv.org/pdf/2502.07005)]
> **Authors**: Tai Hoang,Huy Le,Philipp Becker,Vien Anh Ngo,Gerhard Neumann
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Accepted at ICLR 2025 (Oral)
- **标题**: None
- **领域**: 机器学习,机器人技术
- **Abstract**: Manipulating objects with varying geometries and deformable objects is a major challenge in robotics. Tasks such as insertion with different objects or cloth hanging require precise control and effective modelling of complex dynamics. In this work, we frame this problem through the lens of a heterogeneous graph that comprises smaller sub-graphs, such as actuators and objects, accompanied by different edge types describing their interactions. This graph representation serves as a unified structure for both rigid and deformable objects tasks, and can be extended further to tasks comprising multiple actuators. To evaluate this setup, we present a novel and challenging reinforcement learning benchmark, including rigid insertion of diverse objects, as well as rope and cloth manipulation with multiple end-effectors. These tasks present a large search space, as both the initial and target configurations are uniformly sampled in 3D space. To address this issue, we propose a novel graph-based policy model, dubbed Heterogeneous Equivariant Policy (HEPi), utilizing $SE(3)$ equivariant message passing networks as the main backbone to exploit the geometric symmetry. In addition, by modeling explicit heterogeneity, HEPi can outperform Transformer-based and non-heterogeneous equivariant policies in terms of average returns, sample efficiency, and generalization to unseen objects.

### Outsourced diffusion sampling: Efficient posterior inference in latent spaces of generative models 
[[arxiv](https://arxiv.org/abs/2502.06999)] [[cool](https://papers.cool/arxiv/2502.06999)] [[pdf](https://arxiv.org/pdf/2502.06999)]
> **Authors**: Siddarth Venkatraman,Mohsin Hasan,Minsu Kim,Luca Scimeca,Marcin Sendera,Yoshua Bengio,Glen Berseth,Nikolay Malkin
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Any well-behaved generative model over a variable $\mathbf{x}$ can be expressed as a deterministic transformation of an exogenous ('outsourced') Gaussian noise variable $\mathbf{z}$: $\mathbf{x}=f_θ(\mathbf{z})$. In such a model (e.g., a VAE, GAN, or continuous-time flow-based model), sampling of the target variable $\mathbf{x} \sim p_θ(\mathbf{x})$ is straightforward, but sampling from a posterior distribution of the form $p(\mathbf{x}\mid\mathbf{y}) \propto p_θ(\mathbf{x})r(\mathbf{x},\mathbf{y})$, where $r$ is a constraint function depending on an auxiliary variable $\mathbf{y}$, is generally intractable. We propose to amortize the cost of sampling from such posterior distributions with diffusion models that sample a distribution in the noise space ($\mathbf{z}$). These diffusion samplers are trained by reinforcement learning algorithms to enforce that the transformed samples $f_θ(\mathbf{z})$ are distributed according to the posterior in the data space ($\mathbf{x}$). For many models and constraints of interest, the posterior in the noise space is smoother than the posterior in the data space, making it more amenable to such amortized inference. Our method enables conditional sampling under unconditional GAN, (H)VAE, and flow-based priors, comparing favorably both with current amortized and non-amortized inference methods. We demonstrate the proposed outsourced diffusion sampling in several experiments with large pretrained prior models: conditional image generation, reinforcement learning with human feedback, and protein structure generation.

### Machine Learning Fleet Efficiency: Analyzing and Optimizing Large-Scale Google TPU Systems with ML Productivity Goodput 
[[arxiv](https://arxiv.org/abs/2502.06982)] [[cool](https://papers.cool/arxiv/2502.06982)] [[pdf](https://arxiv.org/pdf/2502.06982)]
> **Authors**: Arissa Wongpanich,Tayo Oguntebi,Jose Baiocchi Paredes,Yu Emma Wang,Phitchaya Mangpo Phothilimthana,Ritwika Mitra,Zongwei Zhou,Naveen Kumar,Vijay Janapa Reddi
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Recent years have seen the emergence of machine learning (ML) workloads deployed in warehouse-scale computing (WSC) settings, also known as ML fleets. As the computational demands placed on ML fleets have increased due to the rise of large models and growing demand for ML applications, it has become increasingly critical to measure and improve the efficiency of such systems. However, there is not yet an established methodology to characterize ML fleet performance and identify potential performance optimizations accordingly. This paper presents a large-scale analysis of an ML fleet based on Google's TPUs, introducing a framework to capture fleet-wide efficiency, systematically evaluate performance characteristics, and identify optimization strategies for the fleet. We begin by defining an ML fleet, outlining its components, and analyzing an example Google ML fleet in production comprising thousands of accelerators running diverse workloads. Our study reveals several critical insights: first, ML fleets extend beyond the hardware layer, with model, data, framework, compiler, and scheduling layers significantly impacting performance; second, the heterogeneous nature of ML fleets poses challenges in characterizing individual workload performance; and third, traditional utilization-based metrics prove insufficient for ML fleet characterization. To address these challenges, we present the "ML Productivity Goodput" (MPG) metric to measure ML fleet efficiency. We show how to leverage this metric to characterize the fleet across the ML system stack. We also present methods to identify and optimize performance bottlenecks using MPG, providing strategies for managing warehouse-scale ML systems in general. Lastly, we demonstrate quantitative evaluations from applying these methods to a real ML fleet for internal-facing Google TPU workloads, where we observed tangible improvements.

### User-Preference Meets Pareto-Optimality: Multi-Objective Bayesian Optimization with Local Gradient Search 
[[arxiv](https://arxiv.org/abs/2502.06971)] [[cool](https://papers.cool/arxiv/2502.06971)] [[pdf](https://arxiv.org/pdf/2502.06971)]
> **Authors**: Joshua Hang Sai Ip,Ankush Chakrabarty,Ali Mesbah,Diego Romeres
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Incorporating user preferences into multi-objective Bayesian optimization (MOBO) allows for personalization of the optimization procedure. Preferences are often abstracted in the form of an unknown utility function, estimated through pairwise comparisons of potential outcomes. However, utility-driven MOBO methods can yield solutions that are dominated by nearby solutions, as non-dominance is not enforced. Additionally, classical MOBO commonly relies on estimating the entire Pareto-front to identify the Pareto-optimal solutions, which can be expensive and ignore user preferences. Here, we present a new method, termed preference-utility-balanced MOBO (PUB-MOBO), that allows users to disambiguate between near-Pareto candidate solutions. PUB-MOBO combines utility-based MOBO with local multi-gradient descent to refine user-preferred solutions to be near-Pareto-optimal. To this end, we propose a novel preference-dominated utility function that concurrently preserves user-preferences and dominance amongst candidate solutions. A key advantage of PUB-MOBO is that the local search is restricted to a (small) region of the Pareto-front directed by user preferences, alleviating the need to estimate the entire Pareto-front. PUB-MOBO is tested on three synthetic benchmark problems: DTLZ1, DTLZ2 and DH1, as well as on three real-world problems: Vehicle Safety, Conceptual Marine Design, and Car Side Impact. PUB-MOBO consistently outperforms state-of-the-art competitors in terms of proximity to the Pareto-front and utility regret across all the problems.

### Model Diffusion for Certifiable Few-shot Transfer Learning 
[[arxiv](https://arxiv.org/abs/2502.06970)] [[cool](https://papers.cool/arxiv/2502.06970)] [[pdf](https://arxiv.org/pdf/2502.06970)]
> **Authors**: Fady Rezk,Royson Lee,Henry Gouk,Timothy Hospedales,Minyoung Kim
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: In modern large-scale deep learning, a prevalent and effective workflow for solving low-data problems is adapting powerful pre-trained foundation models (FMs) to new tasks via parameter-efficient fine-tuning (PEFT). However, while empirically effective, the resulting solutions lack generalisation guarantees to certify their accuracy - which may be required for ethical or legal reasons prior to deployment in high-importance applications. In this paper we develop a novel transfer learning approach that is designed to facilitate non-vacuous learning theoretic generalisation guarantees for downstream tasks, even in the low-shot regime. Specifically, we first use upstream tasks to train a distribution over PEFT parameters. We then learn the downstream task by a sample-and-evaluate procedure -- sampling plausible PEFTs from the trained diffusion model and selecting the one with the highest likelihood on the downstream data. Crucially, this confines our model hypothesis to a finite set of PEFT samples. In contrast to learning in the typical continuous hypothesis spaces of neural network weights, this facilitates tighter risk certificates. We instantiate our bound and show non-trivial generalization guarantees compared to existing learning approaches which lead to vacuous bounds in the low-shot regime.

### Task Offloading in Vehicular Edge Computing using Deep Reinforcement Learning: A Survey 
[[arxiv](https://arxiv.org/abs/2502.06963)] [[cool](https://papers.cool/arxiv/2502.06963)] [[pdf](https://arxiv.org/pdf/2502.06963)]
> **Authors**: Ashab Uddin,Ahmed Hamdi Sakr,Ning Zhang
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 27 Pages, 3 Figures, 3 Tables
- **标题**: None
- **领域**: 机器学习,人工智能,分布式、并行和集群计算,多代理系统
- **Abstract**: The increasing demand for Intelligent Transportation Systems (ITS) has introduced significant challenges in managing the complex, computation-intensive tasks generated by modern vehicles while offloading tasks to external computing infrastructures such as edge computing (EC), nearby vehicular , and UAVs has become influential solution to these challenges. However, traditional computational offloading strategies often struggle to adapt to the dynamic and heterogeneous nature of vehicular environments. In this study, we explored the potential of Reinforcement Learning (RL) and Deep Reinforcement Learning (DRL) frameworks to optimize computational offloading through adaptive, real-time decision-making, and we have thoroughly investigated the Markov Decision Process (MDP) approaches on the existing literature. The paper focuses on key aspects such as standardized learning models, optimized reward structures, and collaborative multi-agent systems, aiming to advance the understanding and application of DRL in vehicular networks. Our findings offer insights into enhancing the efficiency, scalability, and robustness of ITS, setting the stage for future innovations in this rapidly evolving field.

### Neighborhood-Order Learning Graph Attention Network for Fake News Detection 
[[arxiv](https://arxiv.org/abs/2502.06927)] [[cool](https://papers.cool/arxiv/2502.06927)] [[pdf](https://arxiv.org/pdf/2502.06927)]
> **Authors**: Batool Lakzaei,Mostafa Haghir Chehreghani,Alireza Bagheri
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 37 pages
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Fake news detection is a significant challenge in the digital age, which has become increasingly important with the proliferation of social media and online communication networks. Graph Neural Networks (GNN)-based methods have shown high potential in analyzing graph-structured data for this problem. However, a major limitation in conventional GNN architectures is their inability to effectively utilize information from neighbors beyond the network's layer depth, which can reduce the model's accuracy and effectiveness. In this paper, we propose a novel model called Neighborhood-Order Learning Graph Attention Network (NOL-GAT) for fake news detection. This model allows each node in each layer to independently learn its optimal neighborhood order. By doing so, the model can purposefully and efficiently extract critical information from distant neighbors. The NOL-GAT architecture consists of two main components: a Hop Network that determines the optimal neighborhood order and an Embedding Network that updates node embeddings using these optimal neighborhoods. To evaluate the model's performance, experiments are conducted on various fake news datasets. Results demonstrate that NOL-GAT significantly outperforms baseline models in metrics such as accuracy and F1-score, particularly in scenarios with limited labeled data. Features such as mitigating the over-squashing problem, improving information flow, and reducing computational complexity further highlight the advantages of the proposed model.

### Occam's model: Selecting simpler representations for better transferability estimation 
[[arxiv](https://arxiv.org/abs/2502.06925)] [[cool](https://papers.cool/arxiv/2502.06925)] [[pdf](https://arxiv.org/pdf/2502.06925)]
> **Authors**: Prabhant Singh,Sibylle Hess,Joaquin Vanschoren
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Fine-tuning models that have been pre-trained on large datasets has become a cornerstone of modern machine learning workflows. With the widespread availability of online model repositories, such as Hugging Face, it is now easier than ever to fine-tune pre-trained models for specific tasks. This raises a critical question: which pre-trained model is most suitable for a given task? This problem is called transferability estimation. In this work, we introduce two novel and effective metrics for estimating the transferability of pre-trained models. Our approach is grounded in viewing transferability as a measure of how easily a pre-trained model's representations can be trained to separate target classes, providing a unique perspective on transferability estimation. We rigorously evaluate the proposed metrics against state-of-the-art alternatives across diverse problem settings, demonstrating their robustness and practical utility. Additionally, we present theoretical insights that explain our metrics' efficacy and adaptability to various scenarios. We experimentally show that our metrics increase Kendall's Tau by up to 32% compared to the state-of-the-art baselines.

### XAMBA: Enabling Efficient State Space Models on Resource-Constrained Neural Processing Units 
[[arxiv](https://arxiv.org/abs/2502.06924)] [[cool](https://papers.cool/arxiv/2502.06924)] [[pdf](https://arxiv.org/pdf/2502.06924)]
> **Authors**: Arghadip Das,Arnab Raha,Shamik Kundu,Soumendu Kumar Ghosh,Deepak Mathaikutty,Vijay Raghunathan
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: State-Space Models (SSMs) have emerged as efficient alternatives to transformers for sequential data tasks, offering linear or near-linear scalability with sequence length, making them ideal for long-sequence applications in NLP, vision, and edge AI, including real-time transcription, translation, and contextual search. These applications require lightweight, high-performance models for deployment on resource-constrained devices like laptops and PCs. Designing specialized accelerators for every emerging neural network is costly and impractical; instead, optimizing models for existing NPUs in AI PCs provides a scalable solution. To this end, we propose XAMBA, the first framework to enable and optimize SSMs on commercial off-the-shelf (COTS) state-of-the-art (SOTA) NPUs. XAMBA follows a three-step methodology: (1) enabling SSMs on NPUs, (2) optimizing performance to meet KPI requirements, and (3) trading accuracy for additional performance gains. After enabling SSMs on NPUs, XAMBA mitigates key bottlenecks using CumBA and ReduBA, replacing sequential CumSum and ReduceSum operations with matrix-based computations, significantly improving execution speed and memory efficiency. Additionally, ActiBA enhances performance by approximating expensive activation functions (e.g., Swish, Softplus) using piecewise linear mappings, reducing latency with minimal accuracy loss. Evaluations on an Intel Core Ultra Series 2 AI PC show that XAMBA achieves up to 2.6X speed-up over the baseline. Our implementation is available at https://github.com/arghadippurdue/XAMBA.

### Do Attention Heads Compete or Cooperate during Counting? 
[[arxiv](https://arxiv.org/abs/2502.06923)] [[cool](https://papers.cool/arxiv/2502.06923)] [[pdf](https://arxiv.org/pdf/2502.06923)]
> **Authors**: Pál Zsámboki,Ádám Fraknói,Máté Gedeon,András Kornai,Zsolt Zombori
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 14 pages, 15 figures
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: We present an in-depth mechanistic interpretability analysis of training small transformers on an elementary task, counting, which is a crucial deductive step in many algorithms. In particular, we investigate the collaboration/competition among the attention heads: we ask whether the attention heads behave as a pseudo-ensemble, all solving the same subtask, or they perform different subtasks, meaning that they can only solve the original task in conjunction. Our work presents evidence that on the semantics of the counting task, attention heads behave as a pseudo-ensemble, but their outputs need to be aggregated in a non-uniform manner in order to create an encoding that conforms to the syntax. Our source code will be available upon publication.

### GraNNite: Enabling High-Performance Execution of Graph Neural Networks on Resource-Constrained Neural Processing Units 
[[arxiv](https://arxiv.org/abs/2502.06921)] [[cool](https://papers.cool/arxiv/2502.06921)] [[pdf](https://arxiv.org/pdf/2502.06921)]
> **Authors**: Arghadip Das,Shamik Kundu,Arnab Raha,Soumendu Ghosh,Deepak Mathaikutty,Vijay Raghunathan
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,硬件架构
- **Abstract**: Graph Neural Networks (GNNs) are vital for learning from graph-structured data, enabling applications in network analysis, recommendation systems, and speech analytics. Deploying them on edge devices like client PCs and laptops enhances real-time processing, privacy, and cloud independence. GNNs aid Retrieval-Augmented Generation (RAG) for Large Language Models (LLMs) and enable event-based vision tasks. However, irregular memory access, sparsity, and dynamic structures cause high latency and energy overhead on resource-constrained devices. While modern edge processors integrate CPUs, GPUs, and NPUs, NPUs designed for data-parallel tasks struggle with irregular GNN computations. We introduce GraNNite, the first hardware-aware framework optimizing GNN execution on commercial-off-the-shelf (COTS) SOTA DNN accelerators via a structured three-step methodology: (1) enabling NPU execution, (2) optimizing performance, and (3) trading accuracy for efficiency gains. Step 1 employs GraphSplit for workload distribution and StaGr for static aggregation, while GrAd and NodePad handle dynamic graphs. Step 2 boosts performance using EffOp for control-heavy tasks and GraSp for sparsity exploitation. Graph Convolution optimizations PreG, SymG, and CacheG reduce redundancy and memory transfers. Step 3 balances quality versus efficiency, where QuantGr applies INT8 quantization, and GrAx1, GrAx2, and GrAx3 accelerate attention, broadcast-add, and SAGE-max aggregation. On Intel Core Ultra AI PCs, GraNNite achieves 2.6X to 7.6X speedups over default NPU mappings and up to 8.6X energy gains over CPUs and GPUs, delivering 10.8X and 6.7X higher performance than CPUs and GPUs, respectively, across GNN models.

### Select before Act: Spatially Decoupled Action Repetition for Continuous Control 
[[arxiv](https://arxiv.org/abs/2502.06919)] [[cool](https://papers.cool/arxiv/2502.06919)] [[pdf](https://arxiv.org/pdf/2502.06919)]
> **Authors**: Buqing Nie,Yangqing Fu,Yue Gao
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: ICLR 2025
- **标题**: None
- **领域**: 机器学习,人工智能,机器人技术
- **Abstract**: Reinforcement Learning (RL) has achieved remarkable success in various continuous control tasks, such as robot manipulation and locomotion. Different to mainstream RL which makes decisions at individual steps, recent studies have incorporated action repetition into RL, achieving enhanced action persistence with improved sample efficiency and superior performance. However, existing methods treat all action dimensions as a whole during repetition, ignoring variations among them. This constraint leads to inflexibility in decisions, which reduces policy agility with inferior effectiveness. In this work, we propose a novel repetition framework called SDAR, which implements Spatially Decoupled Action Repetition through performing closed-loop act-or-repeat selection for each action dimension individually. SDAR achieves more flexible repetition strategies, leading to an improved balance between action persistence and diversity. Compared to existing repetition frameworks, SDAR is more sample efficient with higher policy performance and reduced action fluctuation. Experiments are conducted on various continuous control scenarios, demonstrating the effectiveness of spatially decoupled repetition design proposed in this work.

### Leveraging GPT-4o Efficiency for Detecting Rework Anomaly in Business Processes 
[[arxiv](https://arxiv.org/abs/2502.06918)] [[cool](https://papers.cool/arxiv/2502.06918)] [[pdf](https://arxiv.org/pdf/2502.06918)]
> **Authors**: Mohammad Derakhshan,Paolo Ceravolo,Fatemeh Mohammadi
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 14 pages, 5 images, 4 tables
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: This paper investigates the effectiveness of GPT-4o-2024-08-06, one of the Large Language Models (LLM) from OpenAI, in detecting business process anomalies, with a focus on rework anomalies. In our study, we developed a GPT-4o-based tool capable of transforming event logs into a structured format and identifying reworked activities within business event logs. The analysis was performed on a synthetic dataset designed to contain rework anomalies but free of loops. To evaluate the anomaly detection capabilities of GPT 4o-2024-08-06, we used three prompting techniques: zero-shot, one-shot, and few-shot. These techniques were tested on different anomaly distributions, namely normal, uniform, and exponential, to identify the most effective approach for each case. The results demonstrate the strong performance of GPT-4o-2024-08-06. On our dataset, the model achieved 96.14% accuracy with one-shot prompting for the normal distribution, 97.94% accuracy with few-shot prompting for the uniform distribution, and 74.21% accuracy with few-shot prompting for the exponential distribution. These results highlight the model's potential as a reliable tool for detecting rework anomalies in event logs and how anomaly distribution and prompting strategy influence the model's performance.

### Krum Federated Chain (KFC): Using blockchain to defend against adversarial attacks in Federated Learning 
[[arxiv](https://arxiv.org/abs/2502.06917)] [[cool](https://papers.cool/arxiv/2502.06917)] [[pdf](https://arxiv.org/pdf/2502.06917)]
> **Authors**: Mario García-Márquez,Nuria Rodríguez-Barroso,M. Victoria Luzón,Francisco Herrera
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Submitted toNeuralNetworks
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Federated Learning presents a nascent approach to machine learning, enabling collaborative model training across decentralized devices while safeguarding data privacy. However, its distributed nature renders it susceptible to adversarial attacks. Integrating blockchain technology with Federated Learning offers a promising avenue to enhance security and integrity. In this paper, we tackle the potential of blockchain in defending Federated Learning against adversarial attacks. First, we test Proof of Federated Learning, a well known consensus mechanism designed ad-hoc to federated contexts, as a defense mechanism demonstrating its efficacy against Byzantine and backdoor attacks when at least one miner remains uncompromised. Second, we propose Krum Federated Chain, a novel defense strategy combining Krum and Proof of Federated Learning, valid to defend against any configuration of Byzantine or backdoor attacks, even when all miners are compromised. Our experiments conducted on image classification datasets validate the effectiveness of our proposed approaches.

### Hyper Compressed Fine-Tuning of Large Foundation Models with Quantum Inspired Adapters 
[[arxiv](https://arxiv.org/abs/2502.06916)] [[cool](https://papers.cool/arxiv/2502.06916)] [[pdf](https://arxiv.org/pdf/2502.06916)]
> **Authors**: Snehal Raj,Brian Coyle
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 16 pages, 9 figures, 6 tables
- **标题**: None
- **领域**: 机器学习,人工智能,信号处理,量子物理学
- **Abstract**: Fine-tuning pre-trained large foundation models for specific tasks has become increasingly challenging due to the computational and storage demands associated with full parameter updates. Parameter-Efficient Fine-Tuning (PEFT) methods address this issue by updating only a small subset of model parameters using adapter modules. In this work, we propose \emph{Quantum-Inspired Adapters}, a PEFT approach inspired by Hamming-weight preserving quantum circuits from quantum machine learning literature. These models can be both expressive and parameter-efficient by operating in a combinatorially large space while simultaneously preserving orthogonality in weight parameters. We test our proposed adapters by adapting large language models and large vision transformers on benchmark datasets. Our method can achieve 99.2\% of the performance of existing fine-tuning methods such LoRA with a 44x parameter compression on language understanding datasets like GLUE and VTAB. Compared to existing orthogonal fine-tuning methods such as OFT or BOFT, we achieve 98\% relative performance with 25x fewer parameters. This demonstrates competitive performance paired with a significant reduction in trainable parameters. Through ablation studies, we determine that combining multiple Hamming-weight orders with orthogonality and matrix compounding are essential for performant fine-tuning. Our findings suggest that Quantum-Inspired Adapters offer a promising direction for efficient adaptation of language and vision models in resource-constrained environments.

### Foundation Models for Anomaly Detection: Vision and Challenges 
[[arxiv](https://arxiv.org/abs/2502.06911)] [[cool](https://papers.cool/arxiv/2502.06911)] [[pdf](https://arxiv.org/pdf/2502.06911)]
> **Authors**: Jing Ren,Tao Tang,Hong Jia,Haytham Fayek,Xiaodong Li,Suyu Ma,Xiwei Xu,Feng Xia
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 9 pages, 4 figures
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: As data continues to grow in volume and complexity across domains such as finance, manufacturing, and healthcare, effective anomaly detection is essential for identifying irregular patterns that may signal critical issues. Recently, foundation models (FMs) have emerged as a powerful tool for advancing anomaly detection. They have demonstrated unprecedented capabilities in enhancing anomaly identification, generating detailed data descriptions, and providing visual explanations. This survey presents the first comprehensive review of recent advancements in FM-based anomaly detection. We propose a novel taxonomy that classifies FMs into three categories based on their roles in anomaly detection tasks, i.e., as encoders, detectors, or interpreters. We provide a systematic analysis of state-of-the-art methods and discuss key challenges in leveraging FMs for improved anomaly detection. We also outline future research directions in this rapidly evolving field.

### TimeKAN: KAN-based Frequency Decomposition Learning Architecture for Long-term Time Series Forecasting 
[[arxiv](https://arxiv.org/abs/2502.06910)] [[cool](https://papers.cool/arxiv/2502.06910)] [[pdf](https://arxiv.org/pdf/2502.06910)]
> **Authors**: Songtao Huang,Zhen Zhao,Can Li,Lei Bai
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Real-world time series often have multiple frequency components that are intertwined with each other, making accurate time series forecasting challenging. Decomposing the mixed frequency components into multiple single frequency components is a natural choice. However, the information density of patterns varies across different frequencies, and employing a uniform modeling approach for different frequency components can lead to inaccurate characterization. To address this challenges, inspired by the flexibility of the recent Kolmogorov-Arnold Network (KAN), we propose a KAN-based Frequency Decomposition Learning architecture (TimeKAN) to address the complex forecasting challenges caused by multiple frequency mixtures. Specifically, TimeKAN mainly consists of three components: Cascaded Frequency Decomposition (CFD) blocks, Multi-order KAN Representation Learning (M-KAN) blocks and Frequency Mixing blocks. CFD blocks adopt a bottom-up cascading approach to obtain series representations for each frequency band. Benefiting from the high flexibility of KAN, we design a novel M-KAN block to learn and represent specific temporal patterns within each frequency band. Finally, Frequency Mixing blocks is used to recombine the frequency bands into the original format. Extensive experimental results across multiple real-world time series datasets demonstrate that TimeKAN achieves state-of-the-art performance as an extremely lightweight architecture. Code is available at https://github.com/huangst21/TimeKAN.

### Satisfaction-Aware Incentive Scheme for Federated Learning in Industrial Metaverse: DRL-Based Stackbelberg Game Approach 
[[arxiv](https://arxiv.org/abs/2502.06909)] [[cool](https://papers.cool/arxiv/2502.06909)] [[pdf](https://arxiv.org/pdf/2502.06909)]
> **Authors**: Xiaohuan Li,Shaowen Qin,Xin Tang,Jiawen Kang,Jin Ye,Zhonghua Zhao,Dusit Niyato
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机科学与博弈论
- **Abstract**: Industrial Metaverse leverages the Industrial Internet of Things (IIoT) to integrate data from diverse devices, employing federated learning and meta-computing to train models in a distributed manner while ensuring data privacy. Achieving an immersive experience for industrial Metaverse necessitates maintaining a balance between model quality and training latency. Consequently, a primary challenge in federated learning tasks is optimizing overall system performance by balancing model quality and training latency. This paper designs a satisfaction function that accounts for data size, Age of Information (AoI), and training latency. Additionally, the satisfaction function is incorporated into the utility functions to incentivize node participation in model training. We model the utility functions of servers and nodes as a two-stage Stackelberg game and employ a deep reinforcement learning approach to learn the Stackelberg equilibrium. This approach ensures balanced rewards and enhances the applicability of the incentive scheme for industrial Metaverse. Simulation results demonstrate that, under the same budget constraints, the proposed incentive scheme improves at least 23.7% utility compared to existing schemes without compromising model accuracy.

### Can ChatGPT Diagnose Alzheimer's Disease? 
[[arxiv](https://arxiv.org/abs/2502.06907)] [[cool](https://papers.cool/arxiv/2502.06907)] [[pdf](https://arxiv.org/pdf/2502.06907)]
> **Authors**: Quoc-Toan Nguyen,Linh Le,Xuan-The Tran,Thomas Do,Chin-Teng Lin
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-11
> **comment**: 14 pages, 5 figures, 5 tables
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Can ChatGPT diagnose Alzheimer's Disease (AD)? AD is a devastating neurodegenerative condition that affects approximately 1 in 9 individuals aged 65 and older, profoundly impairing memory and cognitive function. This paper utilises 9300 electronic health records (EHRs) with data from Magnetic Resonance Imaging (MRI) and cognitive tests to address an intriguing question: As a general-purpose task solver, can ChatGPT accurately detect AD using EHRs? We present an in-depth evaluation of ChatGPT using a black-box approach with zero-shot and multi-shot methods. This study unlocks ChatGPT's capability to analyse MRI and cognitive test results, as well as its potential as a diagnostic tool for AD. By automating aspects of the diagnostic process, this research opens a transformative approach for the healthcare system, particularly in addressing disparities in resource-limited regions where AD specialists are scarce. Hence, it offers a foundation for a promising method for early detection, supporting individuals with timely interventions, which is paramount for Quality of Life (QoL).

### Learning-based estimation of cattle weight gain and its influencing factors 
[[arxiv](https://arxiv.org/abs/2502.06906)] [[cool](https://papers.cool/arxiv/2502.06906)] [[pdf](https://arxiv.org/pdf/2502.06906)]
> **Authors**: Muhammad Riaz Hasib Hossain,Rafiqul Islam,Shawn R. McGrath,Md Zahidul Islam,David Lamb
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-11
> **comment**: ef:Hossain, M. R. H., Islam, R., Mcgrath, S. R., Islam, Z., & Lamb, D. (2025).Learning-based
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Many cattle farmers still depend on manual methods to measure the live weight gain of cattle at set intervals, which is time consuming, labour intensive, and stressful for both the animals and handlers. A remote and autonomous monitoring system using machine learning (ML) or deep learning (DL) can provide a more efficient and less invasive method and also predictive capabilities for future cattle weight gain (CWG). This system allows continuous monitoring and estimation of individual cattle live weight gain, growth rates and weight fluctuations considering various factors like environmental conditions, genetic predispositions, feed availability, movement patterns and behaviour. Several researchers have explored the efficiency of estimating CWG using ML and DL algorithms. However, estimating CWG suffers from a lack of consistency in its application. Moreover, ML or DL can provide weight gain estimations based on several features that vary in existing research. Additionally, previous studies have encountered various data related challenges when estimating CWG. This paper presents a comprehensive investigation in estimating CWG using advanced ML techniques based on research articles (between 2004 and 2024). This study investigates the current tools, methods, and features used in CWG estimation, as well as their strengths and weaknesses. The findings highlight the significance of using advanced ML approaches in CWG estimation and its critical influence on factors. Furthermore, this study identifies potential research gaps and provides research direction on CWG prediction, which serves as a reference for future research in this area.

### Lightweight Dataset Pruning without Full Training via Example Difficulty and Prediction Uncertainty 
[[arxiv](https://arxiv.org/abs/2502.06905)] [[cool](https://papers.cool/arxiv/2502.06905)] [[pdf](https://arxiv.org/pdf/2502.06905)]
> **Authors**: Yeseul Cho,Baekrok Shin,Changmin Kang,Chulhee Yun
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Recent advances in deep learning rely heavily on massive datasets, leading to substantial storage and training costs. Dataset pruning aims to alleviate this demand by discarding redundant examples. However, many existing methods require training a model with a full dataset over a large number of epochs before being able to prune the dataset, which ironically makes the pruning process more expensive than just training the model on the entire dataset. To overcome this limitation, we introduce a Difficulty and Uncertainty-Aware Lightweight (DUAL) score, which aims to identify important samples from the early training stage by considering both example difficulty and prediction uncertainty. To address a catastrophic accuracy drop at an extreme pruning, we further propose a ratio-adaptive sampling using Beta distribution. Experiments on various datasets and learning scenarios such as image classification with label noise and image corruption, and model architecture generalization demonstrate the superiority of our method over previous state-of-the-art (SOTA) approaches. Specifically, on ImageNet-1k, our method reduces the time cost for pruning to 66% compared to previous methods while achieving a SOTA, specifically 60% test accuracy at a 90% pruning ratio. On CIFAR datasets, the time cost is reduced to just 15% while maintaining SOTA performance.

### Emergence of Episodic Memory in Transformers: Characterizing Changes in Temporal Structure of Attention Scores During Training 
[[arxiv](https://arxiv.org/abs/2502.06902)] [[cool](https://papers.cool/arxiv/2502.06902)] [[pdf](https://arxiv.org/pdf/2502.06902)]
> **Authors**: Deven Mahesh Mistry,Anooshka Bajaj,Yash Aggarwal,Sahaj Singh Maini,Zoran Tiganj
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: We investigate in-context temporal biases in attention heads and transformer outputs. Using cognitive science methodologies, we analyze attention scores and outputs of the GPT-2 models of varying sizes. Across attention heads, we observe effects characteristic of human episodic memory, including temporal contiguity, primacy and recency. Transformer outputs demonstrate a tendency toward in-context serial recall. Importantly, this effect is eliminated after the ablation of the induction heads, which are the driving force behind the contiguity effect. Our findings offer insights into how transformers organize information temporally during in-context learning, shedding light on their similarities and differences with human memory and learning.

### Enabling Autoregressive Models to Fill In Masked Tokens 
[[arxiv](https://arxiv.org/abs/2502.06901)] [[cool](https://papers.cool/arxiv/2502.06901)] [[pdf](https://arxiv.org/pdf/2502.06901)]
> **Authors**: Daniel Israel,Aditya Grover,Guy Van den Broeck
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Historically, LLMs have been trained using either autoregressive (AR) or masked language modeling (MLM) objectives, with AR models gaining dominance in recent years. However, AR models are inherently incapable of masked infilling, which is the ability to predict masked tokens between past and future context. In contrast, MLM models suffer from intrinsic computational inefficiencies during both training and inference that hinder their scalability. This work introduces MARIA (Masked and Autoregressive Infilling Architecture), a novel approach that leverages the strengths of both paradigms to achieve state-of-the-art masked infilling performance. MARIA combines a pre-trained MLM and AR model by training a linear decoder that takes their concatenated hidden states as input. This minimal modification enables the AR model to perform infilling while retaining its inherent advantages in terms of faster inference with KV caching. Our results demonstrate that MARIA significantly outperforms existing methods, namely discrete diffusion models, on masked infilling tasks.

### Polynomial Regret Concentration of UCB for Non-Deterministic State Transitions 
[[arxiv](https://arxiv.org/abs/2502.06900)] [[cool](https://papers.cool/arxiv/2502.06900)] [[pdf](https://arxiv.org/pdf/2502.06900)]
> **Authors**: Can Cömer,Jannis Blüml,Cedric Derstroff,Kristian Kersting
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-11
> **comment**: 10 pages, 5 figures
- **标题**: None
- **领域**: 机器学习,离散数学
- **Abstract**: Monte Carlo Tree Search (MCTS) has proven effective in solving decision-making problems in perfect information settings. However, its application to stochastic and imperfect information domains remains limited. This paper extends the theoretical framework of MCTS to stochastic domains by addressing non-deterministic state transitions, where actions lead to probabilistic outcomes. Specifically, building on the work of Shah et al. (2020), we derive polynomial regret concentration bounds for the Upper Confidence Bound algorithm in multi-armed bandit problems with stochastic transitions, offering improved theoretical guarantees. Our primary contribution is proving that these bounds also apply to non-deterministic environments, ensuring robust performance in stochastic settings. This broadens the applicability of MCTS to real-world decision-making problems with probabilistic outcomes, such as in autonomous systems and financial decision-making.

### Certifying Language Model Robustness with Fuzzed Randomized Smoothing: An Efficient Defense Against Backdoor Attacks 
[[arxiv](https://arxiv.org/abs/2502.06892)] [[cool](https://papers.cool/arxiv/2502.06892)] [[pdf](https://arxiv.org/pdf/2502.06892)]
> **Authors**: Bowei He,Lihao Yin,Hui-Ling Zhen,Jianping Zhang,Lanqing Hong,Mingxuan Yuan,Chen Ma
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-11
> **comment**: Accepted by ICLR 2025
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: The widespread deployment of pre-trained language models (PLMs) has exposed them to textual backdoor attacks, particularly those planted during the pre-training stage. These attacks pose significant risks to high-reliability applications, as they can stealthily affect multiple downstream tasks. While certifying robustness against such threats is crucial, existing defenses struggle with the high-dimensional, interdependent nature of textual data and the lack of access to original poisoned pre-training data. To address these challenges, we introduce \textbf{F}uzzed \textbf{R}andomized \textbf{S}moothing (\textbf{FRS}), a novel approach for efficiently certifying language model robustness against backdoor attacks. FRS integrates software robustness certification techniques with biphased model parameter smoothing, employing Monte Carlo tree search for proactive fuzzing to identify vulnerable textual segments within the Damerau-Levenshtein space. This allows for targeted and efficient text randomization, while eliminating the need for access to poisoned training data during model smoothing. Our theoretical analysis demonstrates that FRS achieves a broader certified robustness radius compared to existing methods. Extensive experiments across various datasets, model configurations, and attack strategies validate FRS's superiority in terms of defense efficiency, accuracy, and robustness.

### LLMs for Drug-Drug Interaction Prediction: A Comprehensive Comparison 
[[arxiv](https://arxiv.org/abs/2502.06890)] [[cool](https://papers.cool/arxiv/2502.06890)] [[pdf](https://arxiv.org/pdf/2502.06890)]
> **Authors**: Gabriele De Vito,Filomena Ferrucci,Athanasios Angelakis
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,定量方法
- **Abstract**: The increasing volume of drug combinations in modern therapeutic regimens needs reliable methods for predicting drug-drug interactions (DDIs). While Large Language Models (LLMs) have revolutionized various domains, their potential in pharmaceutical research, particularly in DDI prediction, remains largely unexplored. This study thoroughly investigates LLMs' capabilities in predicting DDIs by uniquely processing molecular structures (SMILES), target organisms, and gene interaction data as raw text input from the latest DrugBank dataset. We evaluated 18 different LLMs, including proprietary models (GPT-4, Claude, Gemini) and open-source variants (from 1.5B to 72B parameters), first assessing their zero-shot capabilities in DDI prediction. We then fine-tuned selected models (GPT-4, Phi-3.5 2.7B, Qwen-2.5 3B, Gemma-2 9B, and Deepseek R1 distilled Qwen 1.5B) to optimize their performance. Our comprehensive evaluation framework included validation across 13 external DDI datasets, comparing against traditional approaches such as l2-regularized logistic regression. Fine-tuned LLMs demonstrated superior performance, with Phi-3.5 2.7B achieving a sensitivity of 0.978 in DDI prediction, with an accuracy of 0.919 on balanced datasets (50% positive, 50% negative cases). This result represents an improvement over both zero-shot predictions and state-of-the-art machine-learning methods used for DDI prediction. Our analysis reveals that LLMs can effectively capture complex molecular interaction patterns and cases where drug pairs target common genes, making them valuable tools for practical applications in pharmaceutical research and clinical settings.

### Klotski: Efficient Mixture-of-Expert Inference via Expert-Aware Multi-Batch Pipeline 
[[arxiv](https://arxiv.org/abs/2502.06888)] [[cool](https://papers.cool/arxiv/2502.06888)] [[pdf](https://arxiv.org/pdf/2502.06888)]
> **Authors**: Zhiyuan Fang,Yuegui Huang,Zicong Hong,Yufeng Lyu,Wuhui Chen,Yue Yu,Fan Yu,Zibin Zheng
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Mixture of Experts (MoE), with its distinctive sparse structure, enables the scaling of language models up to trillions of parameters without significantly increasing computational costs. However, the substantial parameter size presents a challenge for inference, as the expansion in GPU memory cannot keep pace with the growth in parameters. Although offloading techniques utilise memory from the CPU and disk and parallelise the I/O and computation for efficiency, the computation for each expert in MoE models is often less than the I/O, resulting in numerous bubbles in the pipeline. Therefore, we propose Klotski, an efficient MoE inference engine that significantly reduces pipeline bubbles through a novel expert-aware multi-batch pipeline paradigm. The proposed paradigm uses batch processing to extend the computation time of the current layer to overlap with the loading time of the next layer. Although this idea has been effectively applied to dense models, more batches may activate more experts in the MoE, leading to longer loading times and more bubbles. Thus, unlike traditional approaches, we balance computation and I/O time and minimise bubbles by orchestrating their inference orders based on their heterogeneous computation and I/O requirements and activation patterns under different batch numbers. Moreover, to adapt to different hardware environments and models, we design a constraint-sensitive I/O-compute planner and a correlation-aware expert prefetcher for a schedule that minimises pipeline bubbles. Experimental results demonstrate that Klotski achieves a superior throughput-latency trade-off compared to state-of-the-art techniques, with throughput improvements of up to 85.12x.

### Gradient Based Method for the Fusion of Lattice Quantizers 
[[arxiv](https://arxiv.org/abs/2502.06887)] [[cool](https://papers.cool/arxiv/2502.06887)] [[pdf](https://arxiv.org/pdf/2502.06887)]
> **Authors**: Liyuan Zhang,Hanzhong Cao,Jiaheng Li,Minyang Yu
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: In practical applications, lattice quantizers leverage discrete lattice points to approximate arbitrary points in the lattice. An effective lattice quantizer significantly enhances both the accuracy and efficiency of these approximations. In the context of high-dimensional lattice quantization, previous work proposed utilizing low-dimensional optimal lattice quantizers and addressed the challenge of determining the optimal length ratio in orthogonal splicing. Notably, it was demonstrated that fixed length ratios and orthogonality yield suboptimal results when combining low-dimensional lattices. Building on this foundation, another approach employed gradient descent to identify optimal lattices, which inspired us to explore the use of neural networks to discover matrices that outperform those obtained from orthogonal splicing methods. We propose two novel approaches to tackle this problem: the Household Algorithm and the Matrix Exp Algorithm. Our results indicate that both the Household Algorithm and the Matrix Exp Algorithm achieve improvements in lattice quantizers across dimensions 13, 15, 17 to 19, 21, and 22. Moreover, the Matrix Exp Algorithm demonstrates superior efficacy in high-dimensional settings.

### Topological derivative approach for deep neural network architecture adaptation 
[[arxiv](https://arxiv.org/abs/2502.06885)] [[cool](https://papers.cool/arxiv/2502.06885)] [[pdf](https://arxiv.org/pdf/2502.06885)]
> **Authors**: C G Krishnanunni,Tan Bui-Thanh,Clint Dawson
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: This work presents a novel algorithm for progressively adapting neural network architecture along the depth. In particular, we attempt to address the following questions in a mathematically principled way: i) Where to add a new capacity (layer) during the training process? ii) How to initialize the new capacity? At the heart of our approach are two key ingredients: i) the introduction of a ``shape functional" to be minimized, which depends on neural network topology, and ii) the introduction of a topological derivative of the shape functional with respect to the neural network topology. Using an optimal control viewpoint, we show that the network topological derivative exists under certain conditions, and its closed-form expression is derived. In particular, we explore, for the first time, the connection between the topological derivative from a topology optimization framework with the Hamiltonian from optimal control theory. Further, we show that the optimality condition for the shape functional leads to an eigenvalue problem for deep neural architecture adaptation. Our approach thus determines the most sensitive location along the depth where a new layer needs to be inserted during the training phase and the associated parametric initialization for the newly added layer. We also demonstrate that our layer insertion strategy can be derived from an optimal transport viewpoint as a solution to maximizing a topological derivative in $p$-Wasserstein space, where $p>= 1$. Numerical investigations with fully connected network, convolutional neural network, and vision transformer on various regression and classification problems demonstrate that our proposed approach can outperform an ad-hoc baseline network and other architecture adaptation strategies. Further, we also demonstrate other applications of topological derivative in fields such as transfer learning.

### Learning Conformal Abstention Policies for Adaptive Risk Management in Large Language and Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2502.06884)] [[cool](https://papers.cool/arxiv/2502.06884)] [[pdf](https://arxiv.org/pdf/2502.06884)]
> **Authors**: Sina Tayebati,Divake Kumar,Nastaran Darabi,Dinithi Jayasuriya,Ranganath Krishnan,Amit Ranjan Trivedi
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Large Language and Vision-Language Models (LLMs/VLMs) are increasingly used in safety-critical applications, yet their opaque decision-making complicates risk assessment and reliability. Uncertainty quantification (UQ) helps assess prediction confidence and enables abstention when uncertainty is high. Conformal prediction (CP), a leading UQ method, provides statistical guarantees but relies on static thresholds, which fail to adapt to task complexity and evolving data distributions, leading to suboptimal trade-offs in accuracy, coverage, and informativeness. To address this, we propose learnable conformal abstention, integrating reinforcement learning (RL) with CP to optimize abstention thresholds dynamically. By treating CP thresholds as adaptive actions, our approach balances multiple objectives, minimizing prediction set size while maintaining reliable coverage. Extensive evaluations across diverse LLM/VLM benchmarks show our method outperforms Least Ambiguous Classifiers (LAC) and Adaptive Prediction Sets (APS), improving accuracy by up to 3.2%, boosting AUROC for hallucination detection by 22.19%, enhancing uncertainty-guided selective generation (AUARC) by 21.17%, and reducing calibration error by 70%-85%. These improvements hold across multiple models and datasets while consistently meeting the 90% coverage target, establishing our approach as a more effective and flexible solution for reliable decision-making in safety-critical applications. The code is available at: {https://github.com/sinatayebati/vlm-uncertainty}.

### CluStRE: Streaming Graph Clustering with Multi-Stage Refinement 
[[arxiv](https://arxiv.org/abs/2502.06879)] [[cool](https://papers.cool/arxiv/2502.06879)] [[pdf](https://arxiv.org/pdf/2502.06879)]
> **Authors**: Adil Chhabra,Shai Dorian Peretz,Christian Schulz
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,数据库
- **Abstract**: We present CluStRE, a novel streaming graph clustering algorithm that balances computational efficiency with high-quality clustering using multi-stage refinement. Unlike traditional in-memory clustering approaches, CluStRE processes graphs in a streaming setting, significantly reducing memory overhead while leveraging re-streaming and evolutionary heuristics to improve solution quality. Our method dynamically constructs a quotient graph, enabling modularity-based optimization while efficiently handling large-scale graphs. We introduce multiple configurations of CluStRE to provide trade-offs between speed, memory consumption, and clustering quality. Experimental evaluations demonstrate that CluStRE improves solution quality by 89.8%, operates 2.6 times faster, and uses less than two-thirds of the memory required by the state-of-the-art streaming clustering algorithm on average. Moreover, our strongest mode enhances solution quality by up to 150% on average. With this, CluStRE achieves comparable solution quality to in-memory algorithms, i.e. over 96% of the quality of clustering approaches, including Louvain, effectively bridging the gap between streaming and traditional clustering methods.

### Deep Learning Meets Oversampling: A Learning Framework to Handle Imbalanced Classification 
[[arxiv](https://arxiv.org/abs/2502.06878)] [[cool](https://papers.cool/arxiv/2502.06878)] [[pdf](https://arxiv.org/pdf/2502.06878)]
> **Authors**: Sukumar Kishanthan,Asela Hevapathige
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Despite extensive research spanning several decades, class imbalance is still considered a profound difficulty for both machine learning and deep learning models. While data oversampling is the foremost technique to address this issue, traditional sampling techniques are often decoupled from the training phase of the predictive model, resulting in suboptimal representations. To address this, we propose a novel learning framework that can generate synthetic data instances in a data-driven manner. The proposed framework formulates the oversampling process as a composition of discrete decision criteria, thereby enhancing the representation power of the model's learning process. Extensive experiments on the imbalanced classification task demonstrate the superiority of our framework over state-of-the-art algorithms.

### WirelessGPT: A Generative Pre-trained Multi-task Learning Framework for Wireless Communication 
[[arxiv](https://arxiv.org/abs/2502.06877)] [[cool](https://papers.cool/arxiv/2502.06877)] [[pdf](https://arxiv.org/pdf/2502.06877)]
> **Authors**: Tingting Yang,Ping Zhang,Mengfan Zheng,Yuxuan Shi,Liwen Jing,Jianbo Huang,Nan Li
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-11
> **comment**: 8 pages, 4 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: This paper introduces WirelessGPT, a pioneering foundation model specifically designed for multi-task learning in wireless communication and sensing. Specifically, WirelessGPT leverages large-scale wireless channel datasets for unsupervised pretraining and extracting universal channel representations, which captures complex spatiotemporal dependencies. In fact,this task-agnostic design adapts WirelessGPT seamlessly to a wide range of downstream tasks, using a unified representation with minimal fine-tuning. By unifying communication and sensing functionalities, WirelessGPT addresses the limitations of task-specific models, offering a scalable and efficient solution for integrated sensing and communication (ISAC). With an initial parameter size of around 80 million, WirelessGPT demonstrates significant improvements over conventional methods and smaller AI models, reducing reliance on large-scale labeled data. As the first foundation model capable of supporting diverse tasks across different domains, WirelessGPT establishes a new benchmark, paving the way for future advancements in multi-task wireless systems.

### FlavorDiffusion: Predicting Food Pairings and Chemical Interactions Using Diffusion Models 
[[arxiv](https://arxiv.org/abs/2502.06871)] [[cool](https://papers.cool/arxiv/2502.06871)] [[pdf](https://arxiv.org/pdf/2502.06871)]
> **Authors**: Seo Jun Pyo
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-11
> **comment**: 8 pages
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: The study of food pairing has evolved beyond subjective expertise with the advent of machine learning. This paper presents FlavorDiffusion, a novel framework leveraging diffusion models to predict food-chemical interactions and ingredient pairings without relying on chromatography. By integrating graph-based embeddings, diffusion processes, and chemical property encoding, FlavorDiffusion addresses data imbalances and enhances clustering quality. Using a heterogeneous graph derived from datasets like Recipe1M and FlavorDB, our model demonstrates superior performance in reconstructing ingredient-ingredient relationships. The addition of a Chemical Structure Prediction (CSP) layer further refines the embedding space, achieving state-of-the-art NMI scores and enabling meaningful discovery of novel ingredient combinations. The proposed framework represents a significant step forward in computational gastronomy, offering scalable, interpretable, and chemically informed solutions for food science.

### Bridging Traffic State and Trajectory for Dynamic Road Network and Trajectory Representation Learning 
[[arxiv](https://arxiv.org/abs/2502.06870)] [[cool](https://papers.cool/arxiv/2502.06870)] [[pdf](https://arxiv.org/pdf/2502.06870)]
> **Authors**: Chengkai Han,Jingyuan Wang,Yongyao Wang,Xie Yu,Hao Lin,Chao Li,Junjie Wu
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-11
> **comment**: 9 pages, 6 figures
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Effective urban traffic management is vital for sustainable city development, relying on intelligent systems with machine learning tasks such as traffic flow prediction and travel time estimation. Traditional approaches usually focus on static road network and trajectory representation learning, and overlook the dynamic nature of traffic states and trajectories, which is crucial for downstream tasks. To address this gap, we propose TRACK, a novel framework to bridge traffic state and trajectory data for dynamic road network and trajectory representation learning. TRACK leverages graph attention networks (GAT) to encode static and spatial road segment features, and introduces a transformer-based model for trajectory representation learning. By incorporating transition probabilities from trajectory data into GAT attention weights, TRACK captures dynamic spatial features of road segments. Meanwhile, TRACK designs a traffic transformer encoder to capture the spatial-temporal dynamics of road segments from traffic state data. To further enhance dynamic representations, TRACK proposes a co-attentional transformer encoder and a trajectory-traffic state matching task. Extensive experiments on real-life urban traffic datasets demonstrate the superiority of TRACK over state-of-the-art baselines. Case studies confirm TRACK's ability to capture spatial-temporal dynamics effectively.

### A Survey on Explainable Deep Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.06869)] [[cool](https://papers.cool/arxiv/2502.06869)] [[pdf](https://arxiv.org/pdf/2502.06869)]
> **Authors**: Zelei Cheng,Jiahao Yu,Xinyu Xing
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Deep Reinforcement Learning (DRL) has achieved remarkable success in sequential decision-making tasks across diverse domains, yet its reliance on black-box neural architectures hinders interpretability, trust, and deployment in high-stakes applications. Explainable Deep Reinforcement Learning (XRL) addresses these challenges by enhancing transparency through feature-level, state-level, dataset-level, and model-level explanation techniques. This survey provides a comprehensive review of XRL methods, evaluates their qualitative and quantitative assessment frameworks, and explores their role in policy refinement, adversarial robustness, and security. Additionally, we examine the integration of reinforcement learning with Large Language Models (LLMs), particularly through Reinforcement Learning from Human Feedback (RLHF), which optimizes AI alignment with human preferences. We conclude by highlighting open research challenges and future directions to advance the development of interpretable, reliable, and accountable DRL systems.

### Global Ease of Living Index: a machine learning framework for longitudinal analysis of major economies 
[[arxiv](https://arxiv.org/abs/2502.06866)] [[cool](https://papers.cool/arxiv/2502.06866)] [[pdf](https://arxiv.org/pdf/2502.06866)]
> **Authors**: Tanay Panat,Rohitash Chandra
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计量经济学,应用领域,机器学习
- **Abstract**: The drastic changes in the global economy, geopolitical conditions, and disruptions such as the COVID-19 pandemic have impacted the cost of living and quality of life. It is important to understand the long-term nature of the cost of living and quality of life in major economies. A transparent and comprehensive living index must include multiple dimensions of living conditions. In this study, we present an approach to quantifying the quality of life through the Global Ease of Living Index that combines various socio-economic and infrastructural factors into a single composite score. Our index utilises economic indicators that define living standards, which could help in targeted interventions to improve specific areas. We present a machine learning framework for addressing the problem of missing data for some of the economic indicators for specific countries. We then curate and update the data and use a dimensionality reduction approach (principal component analysis) to create the Ease of Living Index for major economies since 1970. Our work significantly adds to the literature by offering a practical tool for policymakers to identify areas needing improvement, such as healthcare systems, employment opportunities, and public safety. Our approach with open data and code can be easily reproduced and applied to various contexts. This transparency and accessibility make our work a valuable resource for ongoing research and policy development in quality-of-life assessment.

### Deep Ritz method with Fourier feature mapping: A deep learning approach for solving variational models of microstructure 
[[arxiv](https://arxiv.org/abs/2502.06865)] [[cool](https://papers.cool/arxiv/2502.06865)] [[pdf](https://arxiv.org/pdf/2502.06865)]
> **Authors**: Ensela Mema,Ting Wang,Jaroslaw Knap
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: This paper presents a novel approach that combines the Deep Ritz Method (DRM) with Fourier feature mapping to solve minimization problems comprised of multi-well, non-convex energy potentials. These problems present computational challenges as they lack a global minimum. Through an investigation of three benchmark problems in both 1D and 2D, we observe that DRM suffers from spectral bias pathology, limiting its ability to learn solutions with high frequencies. To overcome this limitation, we modify the method by introducing Fourier feature mapping. This modification involves applying a Fourier mapping to the input layer before it passes through the hidden and output layers. Our results demonstrate that Fourier feature mapping enables DRM to generate high-frequency, multiscale solutions for the benchmark problems in both 1D and 2D, offering a promising advancement in tackling complex non-convex energy minimization problems.

### Poincaré Inequality for Local Log-Polyak-Lojasiewicz Measures : Non-asymptotic Analysis in Low-temperature Regime 
[[arxiv](https://arxiv.org/abs/2502.06862)] [[cool](https://papers.cool/arxiv/2502.06862)] [[pdf](https://arxiv.org/pdf/2502.06862)]
> **Authors**: Yun Gong,Zebang Shen,Niao He
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-11
> **comment**: This work was intended as a replacement of arXiv:2501.00429 and any subsequent updates will appear there
- **标题**: None
- **领域**: 机器学习,经典分析和常微分方程,泛函分析,可能性,机器学习
- **Abstract**: Potential functions in highly pertinent applications, such as deep learning in over-parameterized regime, are empirically observed to admit non-isolated minima. To understand the convergence behavior of stochastic dynamics in such landscapes, we propose to study the class of \logPLmeasure\ measures $μ_ε\propto \exp(-V/ε)$, where the potential $V$ satisfies a local Polyak-Łojasiewicz (PŁ) inequality, and its set of local minima is provably \emph{connected}. Notably, potentials in this class can exhibit local maxima and we characterize its optimal set S to be a compact $\mathcal{C}^2$ \emph{embedding submanifold} of $\mathbb{R}^d$ without boundary. The \emph{non-contractibility} of S distinguishes our function class from the classical convex setting topologically. Moreover, the embedding structure induces a naturally defined Laplacian-Beltrami operator on S, and we show that its first non-trivial eigenvalue provides an \emph{$ε$-independent} lower bound for the \Poincare\ constant in the \Poincare\ inequality of $μ_ε$. As a direct consequence, Langevin dynamics with such non-convex potential $V$ and diffusion coefficient $ε$ converges to its equilibrium $μ_ε$ at a rate of $\tilde{\mathcal{O}}(1/ε)$, provided $ε$ is sufficiently small. Here $\tilde{\mathcal{O}}$ hides logarithmic terms.

### Design Considerations in Offline Preference-based RL 
[[arxiv](https://arxiv.org/abs/2502.06861)] [[cool](https://papers.cool/arxiv/2502.06861)] [[pdf](https://arxiv.org/pdf/2502.06861)]
> **Authors**: Alekh Agarwal,Christoph Dann,Teodor V. Marinov
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Offline algorithms for Reinforcement Learning from Human Preferences (RLHF), which use only a fixed dataset of sampled responses given an input, and preference feedback among these responses, have gained increasing prominence in the literature on aligning language models. In this paper, we study how the different design choices made in methods such as DPO, IPO, SLiC and many variants influence the quality of the learned policy, from a theoretical perspective. Our treatment yields insights into the choices of loss function, the policy which is used to normalize log-likelihoods, and also the role of the data sampling policy. Notably, our results do not rely on the standard reparameterization-style arguments used to motivate some of the algorithms in this family, which allows us to give a unified treatment to a broad class of methods. We also conduct a small empirical study to verify some of the theoretical findings on a standard summarization benchmark.

### Gemstones: A Model Suite for Multi-Faceted Scaling Laws 
[[arxiv](https://arxiv.org/abs/2502.06857)] [[cool](https://papers.cool/arxiv/2502.06857)] [[pdf](https://arxiv.org/pdf/2502.06857)]
> **Authors**: Sean McLeish,John Kirchenbauer,David Yu Miller,Siddharth Singh,Abhinav Bhatele,Micah Goldblum,Ashwinee Panda,Tom Goldstein
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Scaling laws are typically fit using a family of models with a narrow range of frozen hyper-parameter choices. In this work we study scaling laws using a wide range of architecture and hyper-parameter choices, and highlight their impact on resulting prescriptions. As a primary artifact of our research, we release the Gemstones: the most comprehensive open-source scaling law dataset to date, consisting of over 4000 checkpoints from transformers with up to 2 billion parameters; these models have been trained with different learning rates, cooldown schedules, and architectural shapes. Our checkpoints enable more complex studies of scaling, such as a law that predicts language modeling performance as a function of model width and depth. By examining the various facets of our model suite, we find that the prescriptions of scaling laws can be highly sensitive to the experimental design process and the specific model checkpoints used during fitting. Code: https://github.com/mcleish7/gemstone-scaling-laws

### Can Large Language Models Understand Intermediate Representations? 
[[arxiv](https://arxiv.org/abs/2502.06854)] [[cool](https://papers.cool/arxiv/2502.06854)] [[pdf](https://arxiv.org/pdf/2502.06854)]
> **Authors**: Hailong Jiang,Jianfeng Zhu,Yao Wan,Bo Fang,Hongyu Zhang,Ruoming Jin,Qiang Guan
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Intermediate Representations (IRs) are essential in compiler design and program analysis, yet their comprehension by Large Language Models (LLMs) remains underexplored. This paper presents a pioneering empirical study to investigate the capabilities of LLMs, including GPT-4, GPT-3, Gemma 2, LLaMA 3.1, and Code Llama, in understanding IRs. We analyze their performance across four tasks: Control Flow Graph (CFG) reconstruction, decompilation, code summarization, and execution reasoning. Our results indicate that while LLMs demonstrate competence in parsing IR syntax and recognizing high-level structures, they struggle with control flow reasoning, execution semantics, and loop handling. Specifically, they often misinterpret branching instructions, omit critical IR operations, and rely on heuristic-based reasoning, leading to errors in CFG reconstruction, IR decompilation, and execution reasoning. The study underscores the necessity for IR-specific enhancements in LLMs, recommending fine-tuning on structured IR datasets and integration of explicit control flow models to augment their comprehension and handling of IR-related tasks.

### Native Fortran Implementation of TensorFlow-Trained Deep and Bayesian Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.06853)] [[cool](https://papers.cool/arxiv/2502.06853)] [[pdf](https://arxiv.org/pdf/2502.06853)]
> **Authors**: Aidan Furlong,Xingang Zhao,Bob Salko,Xu Wu
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-11
> **comment**: Submitted for inclusion in the 2025 American Nuclear Society Annual Conference
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Over the past decade, the investigation of machine learning (ML) within the field of nuclear engineering has grown significantly. With many approaches reaching maturity, the next phase of investigation will determine the feasibility and usefulness of ML model implementation in a production setting. Several of the codes used for reactor design and assessment are primarily written in the Fortran language, which is not immediately compatible with TensorFlow-trained ML models. This study presents a framework for implementing deep neural networks (DNNs) and Bayesian neural networks (BNNs) in Fortran, allowing for native execution without TensorFlow's C API, Python runtime, or ONNX conversion. Designed for ease of use and computational efficiency, the framework can be implemented in any Fortran code, supporting iterative solvers and UQ via ensembles or BNNs. Verification was performed using a two-input, one-output test case composed of a noisy sinusoid to compare Fortran-based predictions to those from TensorFlow. The DNN predictions showed negligible differences and achieved a 19.6x speedup, whereas the BNN predictions exhibited minor disagreement, plausibly due to differences in random number generation. An 8.0x speedup was noted for BNN inference. The approach was then further verified on a nuclear-relevant problem predicting critical heat flux (CHF), which demonstrated similar behavior along with significant computational gains. Discussion regarding the framework's successful integration into the CTF thermal-hydraulics code is also included, outlining its practical usefulness. Overall, this framework was shown to be effective at implementing both DNN and BNN model inference within Fortran, allowing for the continued study of ML-based methods in real-world nuclear applications.

### EAP-GP: Mitigating Saturation Effect in Gradient-based Automated Circuit Identification 
[[arxiv](https://arxiv.org/abs/2502.06852)] [[cool](https://papers.cool/arxiv/2502.06852)] [[pdf](https://arxiv.org/pdf/2502.06852)]
> **Authors**: Lin Zhang,Wenshuo Dong,Zhuoran Zhang,Shu Yang,Lijie Hu,Ninghao Liu,Pan Zhou,Di Wang
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Understanding the internal mechanisms of transformer-based language models remains challenging. Mechanistic interpretability based on circuit discovery aims to reverse engineer neural networks by analyzing their internal processes at the level of computational subgraphs. In this paper, we revisit existing gradient-based circuit identification methods and find that their performance is either affected by the zero-gradient problem or saturation effects, where edge attribution scores become insensitive to input changes, resulting in noisy and unreliable attribution evaluations for circuit components. To address the saturation effect, we propose Edge Attribution Patching with GradPath (EAP-GP), EAP-GP introduces an integration path, starting from the input and adaptively following the direction of the difference between the gradients of corrupted and clean inputs to avoid the saturated region. This approach enhances attribution reliability and improves the faithfulness of circuit identification. We evaluate EAP-GP on 6 datasets using GPT-2 Small, GPT-2 Medium, and GPT-2 XL. Experimental results demonstrate that EAP-GP outperforms existing methods in circuit faithfulness, achieving improvements up to 17.7%. Comparisons with manually annotated ground-truth circuits demonstrate that EAP-GP achieves precision and recall comparable to or better than previous approaches, highlighting its effectiveness in identifying accurate circuits.

### Model Fusion via Neuron Transplantation 
[[arxiv](https://arxiv.org/abs/2502.06849)] [[cool](https://papers.cool/arxiv/2502.06849)] [[pdf](https://arxiv.org/pdf/2502.06849)]
> **Authors**: Muhammed Öz,Nicholas Kiefer,Charlotte Debus,Jasmin Hörter,Achim Streit,Markus Götz
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-11
> **comment**: 18 pages, 7 figures, conference: ECML-PKDD 2024
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Ensemble learning is a widespread technique to improve the prediction performance of neural networks. However, it comes at the price of increased memory and inference time. In this work we propose a novel model fusion technique called \emph{Neuron Transplantation (NT)} in which we fuse an ensemble of models by transplanting important neurons from all ensemble members into the vacant space obtained by pruning insignificant neurons. An initial loss in performance post-transplantation can be quickly recovered via fine-tuning, consistently outperforming individual ensemble members of the same model capacity and architecture. Furthermore, NT enables all the ensemble members to be jointly pruned and jointly trained in a combined model. Comparing it to alignment-based averaging (like Optimal-Transport-fusion), it requires less fine-tuning than the corresponding OT-fused model, the fusion itself is faster and requires less memory, while the resulting model performance is comparable or better. The code is available under the following link: https://github.com/masterbaer/neuron-transplantation.

### Transfer learning in Scalable Graph Neural Network for Improved Physical Simulation 
[[arxiv](https://arxiv.org/abs/2502.06848)] [[cool](https://papers.cool/arxiv/2502.06848)] [[pdf](https://arxiv.org/pdf/2502.06848)]
> **Authors**: Siqi Shen,Yu Liu,Daniel Biggs,Omar Hafez,Jiandong Yu,Wentao Zhang,Bin Cui,Jiulong Shan
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: In recent years, Graph Neural Network (GNN) based models have shown promising results in simulating physics of complex systems. However, training dedicated graph network based physics simulators can be costly, as most models are confined to fully supervised training, which requires extensive data generated from traditional physics simulators. To date, how transfer learning could improve the model performance and training efficiency has remained unexplored. In this work, we introduce a pre-training and transfer learning paradigm for graph network simulators. We propose the scalable graph U-net (SGUNET). Incorporating an innovative depth-first search (DFS) pooling, the SGUNET is adaptable to different mesh sizes and resolutions for various simulation tasks. To enable the transfer learning between differently configured SGUNETs, we propose a set of mapping functions to align the parameters between the pre-trained model and the target model. An extra normalization term is also added into the loss to constrain the difference between the pre-trained weights and target model weights for better generalization performance. To pre-train our physics simulator we created a dataset which includes 20,000 physical simulations of randomly selected 3D shapes from the open source A Big CAD (ABC) dataset. We show that our proposed transfer learning methods allow the model to perform even better when fine-tuned with small amounts of training data than when it is trained from scratch with full extensive dataset. On the 2D Deformable Plate benchmark dataset, our pre-trained model fine-tuned on 1/16 of the training data achieved an 11.05\% improvement in position RMSE compared to the model trained from scratch.

### A Deep Learning Framework Integrating CNN and BiLSTM for Financial Systemic Risk Analysis and Prediction 
[[arxiv](https://arxiv.org/abs/2502.06847)] [[cool](https://papers.cool/arxiv/2502.06847)] [[pdf](https://arxiv.org/pdf/2502.06847)]
> **Authors**: Yu Cheng,Zhen Xu,Yuan Chen,Yuhan Wang,Zhenghao Lin,Jinsong Liu
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算工程、金融和科学
- **Abstract**: This study proposes a deep learning model based on the combination of convolutional neural network (CNN) and bidirectional long short-term memory network (BiLSTM) for discriminant analysis of financial systemic risk. The model first uses CNN to extract local patterns of multidimensional features of financial markets, and then models the bidirectional dependency of time series through BiLSTM, to comprehensively characterize the changing laws of systemic risk in spatial features and temporal dynamics. The experiment is based on real financial data sets. The results show that the model is significantly superior to traditional single models (such as BiLSTM, CNN, Transformer, and TCN) in terms of accuracy, recall, and F1 score. The F1-score reaches 0.88, showing extremely high discriminant ability. This shows that the joint strategy of combining CNN and BiLSTM can not only fully capture the complex patterns of market data but also effectively deal with the long-term dependency problem in time series data. In addition, this study also explores the robustness of the model in dealing with data noise and processing high-dimensional data, providing strong support for intelligent financial risk management. In the future, the research will further optimize the model structure, introduce methods such as reinforcement learning and multimodal data analysis, and improve the efficiency and generalization ability of the model to cope with a more complex financial environment.

### Prot2Chat: Protein LLM with Early Fusion of Sequence and Structure 
[[arxiv](https://arxiv.org/abs/2502.06846)] [[cool](https://papers.cool/arxiv/2502.06846)] [[pdf](https://arxiv.org/pdf/2502.06846)]
> **Authors**: Zhicong Wang,Zicheng Ma,Ziqiang Cao,Changlong Zhou,Jun Zhang,Yiqin Gao
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-11
> **comment**: 9 pages, 2 figures
- **标题**: None
- **领域**: 机器学习,人工智能,生物分子
- **Abstract**: Proteins play a pivotal role in living organisms, yet understanding their functions presents significant challenges, including the limited flexibility of classification-based methods, the inability to effectively leverage spatial structural information, and the lack of systematic evaluation metrics for protein Q&A systems. To address these limitations, we propose Prot2Chat, a novel framework that integrates multimodal protein representations with natural language through a unified module, enabling large language model (LLM)-driven answer generation. Our model incorporates a modified ProteinMPNN encoder, which encodes protein sequence and structural information in a unified manner, a protein-text adapter with cross-attention mechanisms, and a LLaMA3 decoder. To optimize training efficiency, we freeze the encoder and employ LoRA techniques for the decoder. We conducted experiments on two datasets, both automated metrics and expert evaluations demonstrate the superior performance of our model. Furthermore, zero-shot prediction results highlight its strong generalization capabilities. This framework offers a promising solution for bridging protein domain knowledge with natural language understanding, paving the way for transformative advancements in protein-related research.

### Exploring Model Invariance with Discrete Search for Ultra-Low-Bit Quantization 
[[arxiv](https://arxiv.org/abs/2502.06844)] [[cool](https://papers.cool/arxiv/2502.06844)] [[pdf](https://arxiv.org/pdf/2502.06844)]
> **Authors**: Yuqiao Wen,Yanshuai Cao,Lili Mou
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-11
> **comment**: :I.2.7; I.2.6; I.2.m; I.5.1; I.7.m
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Large language models have been increasing in size due to their success in a wide range of applications. This calls for a pressing need to reduce memory usage to make them more accessible. Post-training quantization is a popular technique which uses fewer bits (e.g., 4--8 bits) to represent the model without retraining it. However, it remains a challenging task to perform quantization in an ultra-low-bit setup (e.g., 2 bits). In this paper, we propose InvarExplore, a unified framework that systematically explores different model invariance at the same time, allowing us to take advantage of the synergy between each type of invariance. Importantly, InvarExplore features a discrete search algorithm that enables us to explore permutation invariance, which is under-studied as it cannot be optimized with gradient-based methods. Results show that InvarExplore is compatible with existing state-of-the-art methods, achieving an add-on performance improvement over strong competing methods.

### TorchResist: Open-Source Differentiable Resist Simulator 
[[arxiv](https://arxiv.org/abs/2502.06838)] [[cool](https://papers.cool/arxiv/2502.06838)] [[pdf](https://arxiv.org/pdf/2502.06838)]
> **Authors**: Zixiao Wang,Jieya Zhou,Su Zheng,Shuo Yin,Kaichao Liang,Shoubo Hu,Xiao Chen,Bei Yu
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-11
> **comment**: SPIE Advanced Lithography + Patterning, 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Recent decades have witnessed remarkable advancements in artificial intelligence (AI), including large language models (LLMs), image and video generative models, and embodied AI systems. These advancements have led to an explosive increase in the demand for computational power, challenging the limits of Moore's Law. Optical lithography, a critical technology in semiconductor manufacturing, faces significant challenges due to its high costs. To address this, various lithography simulators have been developed. However, many of these simulators are limited by their inadequate photoresist modeling capabilities. This paper presents TorchResist, an open-source, differentiable photoresist simulator.TorchResist employs an analytical approach to model the photoresist process, functioning as a white-box system with at most twenty interpretable parameters. Leveraging modern differentiable programming techniques and parallel computing on GPUs, TorchResist enables seamless co-optimization with other tools across multiple related tasks. Our experimental results demonstrate that TorchResist achieves superior accuracy and efficiency compared to existing solutions. The source code is publicly available.

### Comparison of CNN-based deep learning architectures for unsteady CFD acceleration on small datasets 
[[arxiv](https://arxiv.org/abs/2502.06837)] [[cool](https://papers.cool/arxiv/2502.06837)] [[pdf](https://arxiv.org/pdf/2502.06837)]
> **Authors**: Sangam Khanal,Shilaj Baral,Joongoo Jeon
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-11
> **comment**: 9 figures, 3 Tables
- **标题**: None
- **领域**: 机器学习,流体动力学
- **Abstract**: CFD acceleration for virtual nuclear reactors or digital twin technology is a primary goal in the nuclear industry. This study compares advanced convolutional neural network (CNN) architectures for accelerating unsteady computational fluid dynamics (CFD) simulations using small datasets based on a challenging natural convection flow dataset. The advanced architectures such as autoencoders, UNet, and ConvLSTM-UNet, were evaluated under identical conditions to determine their predictive accuracy and robustness in autoregressive time-series predictions. ConvLSTM-UNet consistently outperformed other models, particularly in difference value calculation, achieving lower maximum errors and stable residuals. However, error accumulation remains a challenge, limiting reliable predictions to approximately 10 timesteps. This highlights the need for enhanced strategies to improve long-term prediction stability. The novelty of this work lies in its fair comparison of state-of-the-art CNN models within the RePIT framework, demonstrating their potential for accelerating CFD simulations while identifying limitations under small data conditions. Future research will focus on exploring alternative models, such as graph neural networks and implicit neural representations. These efforts aim to develop a robust hybrid approach for long-term unsteady CFD acceleration, contributing to practical applications in virtual nuclear reactor.

### CAST: Cross Attention based multimodal fusion of Structure and Text for materials property prediction 
[[arxiv](https://arxiv.org/abs/2502.06836)] [[cool](https://papers.cool/arxiv/2502.06836)] [[pdf](https://arxiv.org/pdf/2502.06836)]
> **Authors**: Jaewan Lee,Changyoung Park,Hongjun Yang,Sungbin Lim,Sehui Han
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-11
> **comment**: 10 pages, 3 figures
- **标题**: None
- **领域**: 机器学习,材料科学,人工智能
- **Abstract**: Recent advancements in AI have revolutionized property prediction in materials science and accelerating material discovery. Graph neural networks (GNNs) stand out due to their ability to represent crystal structures as graphs, effectively capturing local interactions and delivering superior predictions. However, these methods often lose critical global information, such as crystal systems and repetitive unit connectivity. To address this, we propose CAST, a cross-attention-based multimodal fusion model that integrates graph and text modalities to preserve essential material information. CAST combines node- and token-level features using cross-attention mechanisms, surpassing previous approaches reliant on material-level embeddings like graph mean-pooling or [CLS] tokens. A masked node prediction pretraining strategy further enhances atomic-level information integration. Our method achieved up to 22.9\% improvement in property prediction across four crystal properties including band gap compared to methods like CrysMMNet and MultiMat. Pretraining was key to aligning node and text embeddings, with attention maps confirming its effectiveness in capturing relationships between nodes and tokens. This study highlights the potential of multimodal learning in materials science, paving the way for more robust predictive models that incorporate both local and global information.

### Reinforcement Learning on AYA Dyads to Enhance Medication Adherence 
[[arxiv](https://arxiv.org/abs/2502.06835)] [[cool](https://papers.cool/arxiv/2502.06835)] [[pdf](https://arxiv.org/pdf/2502.06835)]
> **Authors**: Ziping Xu,Hinal Jajal,Sung Won Choi,Inbal Nahum-Shani,Guy Shani,Alexandra M. Psihogios,Pei-Yao Hung,Susan Murphy
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Medication adherence is critical for the recovery of adolescents and young adults (AYAs) who have undergone hematopoietic cell transplantation (HCT). However, maintaining adherence is challenging for AYAs after hospital discharge, who experience both individual (e.g. physical and emotional symptoms) and interpersonal barriers (e.g., relational difficulties with their care partner, who is often involved in medication management). To optimize the effectiveness of a three-component digital intervention targeting both members of the dyad as well as their relationship, we propose a novel Multi-Agent Reinforcement Learning (MARL) approach to personalize the delivery of interventions. By incorporating the domain knowledge, the MARL framework, where each agent is responsible for the delivery of one intervention component, allows for faster learning compared with a flattened agent. Evaluation using a dyadic simulator environment, based on real clinical data, shows a significant improvement in medication adherence (approximately 3%) compared to purely random intervention delivery. The effectiveness of this approach will be further evaluated in an upcoming trial.

### A Unified Knowledge-Distillation and Semi-Supervised Learning Framework to Improve Industrial Ads Delivery Systems 
[[arxiv](https://arxiv.org/abs/2502.06834)] [[cool](https://papers.cool/arxiv/2502.06834)] [[pdf](https://arxiv.org/pdf/2502.06834)]
> **Authors**: Hamid Eghbalzadeh,Yang Wang,Rui Li,Yuji Mo,Qin Ding,Jiaxiang Fu,Liang Dai,Shuo Gu,Nima Noorshams,Sem Park,Bo Long,Xue Feng
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Industrial ads ranking systems conventionally rely on labeled impression data, which leads to challenges such as overfitting, slower incremental gain from model scaling, and biases due to discrepancies between training and serving data. To overcome these issues, we propose a Unified framework for Knowledge-Distillation and Semi-supervised Learning (UKDSL) for ads ranking, empowering the training of models on a significantly larger and more diverse datasets, thereby reducing overfitting and mitigating training-serving data discrepancies. We provide detailed formal analysis and numerical simulations on the inherent miscalibration and prediction bias of multi-stage ranking systems, and show empirical evidence of the proposed framework's capability to mitigate those. Compared to prior work, UKDSL can enable models to learn from a much larger set of unlabeled data, hence, improving the performance while being computationally efficient. Finally, we report the successful deployment of UKDSL in an industrial setting across various ranking models, serving users at multi-billion scale, across various surfaces, geological locations, clients, and optimize for various events, which to the best of our knowledge is the first of its kind in terms of the scale and efficiency at which it operates.

### Entropy Adaptive Decoding: Dynamic Model Switching for Efficient Inference 
[[arxiv](https://arxiv.org/abs/2502.06833)] [[cool](https://papers.cool/arxiv/2502.06833)] [[pdf](https://arxiv.org/pdf/2502.06833)]
> **Authors**: Toby Simonds
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: We present Entropy Adaptive Decoding (EAD), a novel approach for efficient language model inference that dynamically switches between different-sized models based on prediction uncertainty. By monitoring rolling entropy in model logit distributions, our method identifies text regions where a smaller model suffices and switches to a larger model only when prediction uncertainty exceeds a threshold. Unlike speculative decoding approaches that maintain perfect output fidelity through verification, EAD accepts controlled output divergence in exchange for computational efficiency. Our experiments on the MATH benchmark demonstrate remarkable efficiency gains across different model families. Using the LLaMA family, we maintain 96.7\% of the 11B model's performance (50.4\% vs 52.1\%) while using it for only 43\% of tokens, decreasing computational cost by 41.5\%. These gains become more pronounced with larger size differentials in the Qwen family, where we achieve 92.9\% of the 14B model's performance (74.3\% vs 80.0\%) while using it for just 25\% of tokens, decreasing computational cost by 67\%. The consistency of these results across model pairs suggests that language model computation can be significantly optimized by selectively deploying model capacity based on local generation complexity. Our findings indicate that current approaches to model inference may be unnecessarily conservative in their pursuit of perfect output fidelity, and that accepting minor performance trade-offs can enable dramatic reductions in computational costs.

### Optimizing Robustness and Accuracy in Mixture of Experts: A Dual-Model Approach 
[[arxiv](https://arxiv.org/abs/2502.06832)] [[cool](https://papers.cool/arxiv/2502.06832)] [[pdf](https://arxiv.org/pdf/2502.06832)]
> **Authors**: Xu Zhang,Kaidi Xu,Ziqing Hu,Ren Wang
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-11
> **comment**: 10 pages, 3 figures, submitted to ICML 2025 (under review)
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Mixture of Experts (MoE) have shown remarkable success in leveraging specialized expert networks for complex machine learning tasks. However, their susceptibility to adversarial attacks presents a critical challenge for deployment in robust applications. This paper addresses the critical question of how to incorporate robustness into MoEs while maintaining high natural accuracy. We begin by analyzing the vulnerability of MoE components, finding that expert networks are notably more susceptible to adversarial attacks than the router. Based on this insight, we propose a targeted robust training technique that integrates a novel loss function to enhance the adversarial robustness of MoE, requiring only the robustification of one additional expert without compromising training or inference efficiency. Building on this, we introduce a dual-model strategy that linearly combines a standard MoE model with our robustified MoE model using a smoothing parameter. This approach allows for flexible control over the robustness-accuracy trade-off. We further provide theoretical foundations by deriving certified robustness bounds for both the single MoE and the dual-model. To push the boundaries of robustness and accuracy, we propose a novel joint training strategy JTDMoE for the dual-model. This joint training enhances both robustness and accuracy beyond what is achievable with separate models. Experimental results on CIFAR-10 and TinyImageNet datasets using ResNet18 and Vision Transformer (ViT) architectures demonstrate the effectiveness of our proposed methods.

### No Location Left Behind: Measuring and Improving the Fairness of Implicit Representations for Earth Data 
[[arxiv](https://arxiv.org/abs/2502.06831)] [[cool](https://papers.cool/arxiv/2502.06831)] [[pdf](https://arxiv.org/pdf/2502.06831)]
> **Authors**: Daniel Cai,Randall Balestriero
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Implicit neural representations (INRs) exhibit growing promise in addressing Earth representation challenges, ranging from emissions monitoring to climate modeling. However, existing methods disproportionately prioritize global average performance, whereas practitioners require fine-grained insights to understand biases and variations in these models. To bridge this gap, we introduce FAIR-Earth: a first-of-its-kind dataset explicitly crafted to examine and challenge inequities in Earth representations. FAIR-Earth comprises various high-resolution Earth signals and uniquely aggregates extensive metadata along stratifications like landmass size and population density to assess the fairness of models. Evaluating state-of-the-art INRs across the various modalities of FAIR-Earth, we uncover striking performance disparities. Certain subgroups, especially those associated with high-frequency signals (e.g., islands, coastlines), are consistently poorly modeled by existing methods. In response, we propose spherical wavelet encodings, building on previous spatial encoding research. Leveraging the multi-resolution capabilities of wavelets, our encodings yield consistent performance over various scales and locations, offering more accurate and robust representations of the biased subgroups. These open-source contributions represent a crucial step towards the equitable assessment and deployment of Earth INRs.

### Convolution-Based Converter : A Weak-Prior Approach For Modeling Stochastic Processes Based On Conditional Density Estimation 
[[arxiv](https://arxiv.org/abs/2502.06829)] [[cool](https://papers.cool/arxiv/2502.06829)] [[pdf](https://arxiv.org/pdf/2502.06829)]
> **Authors**: Chaoran Pang,Shuangrong Liu,Shikun Tian,WenHao Yue,Xingshen Zhang,Lin Wang,Bo Yang
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: In this paper, a Convolution-Based Converter (CBC) is proposed to develop a methodology for removing the strong or fixed priors in estimating the probability distribution of targets based on observations in the stochastic process. Traditional approaches, e.g., Markov-based and Gaussian process-based methods, typically leverage observations to estimate targets based on strong or fixed priors (such as Markov properties or Gaussian prior). However, the effectiveness of these methods depends on how well their prior assumptions align with the characteristics of the problem. When the assumed priors are not satisfied, these approaches may perform poorly or even become unusable. To overcome the above limitation, we introduce the Convolution-Based converter (CBC), which implicitly estimates the conditional probability distribution of targets without strong or fixed priors, and directly outputs the expected trajectory of the stochastic process that satisfies the constraints from observations. This approach reduces the dependence on priors, enhancing flexibility and adaptability in modeling stochastic processes when addressing different problems. Experimental results demonstrate that our method outperforms existing baselines across multiple metrics.

### Fine-Tuning Strategies for Continual Online EEG Motor Imagery Decoding: Insights from a Large-Scale Longitudinal Study 
[[arxiv](https://arxiv.org/abs/2502.06828)] [[cool](https://papers.cool/arxiv/2502.06828)] [[pdf](https://arxiv.org/pdf/2502.06828)]
> **Authors**: Martin Wimpff,Bruno Aristimunha,Sylvain Chevallier,Bin Yang
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: This study investigates continual fine-tuning strategies for deep learning in online longitudinal electroencephalography (EEG) motor imagery (MI) decoding within a causal setting involving a large user group and multiple sessions per participant. We are the first to explore such strategies across a large user group, as longitudinal adaptation is typically studied in the single-subject setting with a single adaptation strategy, which limits the ability to generalize findings. First, we examine the impact of different fine-tuning approaches on decoder performance and stability. Building on this, we integrate online test-time adaptation (OTTA) to adapt the model during deployment, complementing the effects of prior fine-tuning. Our findings demonstrate that fine-tuning that successively builds on prior subject-specific information improves both performance and stability, while OTTA effectively adapts the model to evolving data distributions across consecutive sessions, enabling calibration-free operation. These results offer valuable insights and recommendations for future research in longitudinal online MI decoding and highlight the importance of combining domain adaptation strategies for improving BCI performance in real-world applications. Clinical Relevance: Our investigation enables more stable and efficient long-term motor imagery decoding, which is critical for neurorehabilitation and assistive technologies.

### Learning to Synthesize Compatible Fashion Items Using Semantic Alignment and Collocation Classification: An Outfit Generation Framework 
[[arxiv](https://arxiv.org/abs/2502.06827)] [[cool](https://papers.cool/arxiv/2502.06827)] [[pdf](https://arxiv.org/pdf/2502.06827)]
> **Authors**: Dongliang Zhou,Haijun Zhang,Kai Yang,Linlin Liu,Han Yan,Xiaofei Xu,Zhao Zhang,Shuicheng Yan
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-11
> **comment**: This paper was accepted by IEEE TNNLS
- **标题**: None
- **领域**: 机器学习,人工智能,图形
- **Abstract**: The field of fashion compatibility learning has attracted great attention from both the academic and industrial communities in recent years. Many studies have been carried out for fashion compatibility prediction, collocated outfit recommendation, artificial intelligence (AI)-enabled compatible fashion design, and related topics. In particular, AI-enabled compatible fashion design can be used to synthesize compatible fashion items or outfits in order to improve the design experience for designers or the efficacy of recommendations for customers. However, previous generative models for collocated fashion synthesis have generally focused on the image-to-image translation between fashion items of upper and lower clothing. In this paper, we propose a novel outfit generation framework, i.e., OutfitGAN, with the aim of synthesizing a set of complementary items to compose an entire outfit, given one extant fashion item and reference masks of target synthesized items. OutfitGAN includes a semantic alignment module, which is responsible for characterizing the mapping correspondence between the existing fashion items and the synthesized ones, to improve the quality of the synthesized images, and a collocation classification module, which is used to improve the compatibility of a synthesized outfit. In order to evaluate the performance of our proposed models, we built a large-scale dataset consisting of 20,000 fashion outfits. Extensive experimental results on this dataset show that our OutfitGAN can synthesize photo-realistic outfits and outperform state-of-the-art methods in terms of similarity, authenticity and compatibility measurements.

### Transferring Graph Neural Networks for Soft Sensor Modeling using Process Topologies 
[[arxiv](https://arxiv.org/abs/2502.06826)] [[cool](https://papers.cool/arxiv/2502.06826)] [[pdf](https://arxiv.org/pdf/2502.06826)]
> **Authors**: Maximilian F. Theisen,Gabrie M. H. Meesters,Artur M. Schweidtmann
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Data-driven soft sensors help in process operations by providing real-time estimates of otherwise hard- to-measure process quantities, e.g., viscosities or product concentrations. Currently, soft sensors need to be developed individually per plant. Using transfer learning, machine learning-based soft sensors could be reused and fine-tuned across plants and applications. However, transferring data-driven soft sensor models is in practice often not possible, because the fixed input structure of standard soft sensor models prohibits transfer if, e.g., the sensor information is not identical in all plants. We propose a topology-aware graph neural network approach for transfer learning of soft sensor models across multiple plants. In our method, plants are modeled as graphs: Unit operations are nodes, streams are edges, and sensors are embedded as attributes. Our approach brings two advantages for transfer learning: First, we not only include sensor data but also crucial information on the plant topology. Second, the graph neural network algorithm is flexible with respect to its sensor inputs. This allows us to model data from different plants with different sensor networks. We test the transfer learning capabilities of our modeling approach on ammonia synthesis loops with different process topologies. We build a soft sensor predicting the ammonia concentration in the product. After training on data from one process, we successfully transfer our soft sensor model to a previously unseen process with a different topology. Our approach promises to extend the data-driven soft sensors to cases to leverage data from multiple plants.

### RLOMM: An Efficient and Robust Online Map Matching Framework with Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.06825)] [[cool](https://papers.cool/arxiv/2502.06825)] [[pdf](https://arxiv.org/pdf/2502.06825)]
> **Authors**: Minxiao Chen,Haitao Yuan,Nan Jiang,Zhihan Zheng,Sai Wu,Ao Zhou,Shangguang Wang
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-11
> **comment**: Accepted by SIGMOD 2025
- **标题**: None
- **领域**: 机器学习,数据库
- **Abstract**: Online map matching is a fundamental problem in location-based services, aiming to incrementally match trajectory data step-by-step onto a road network. However, existing methods fail to meet the needs for efficiency, robustness, and accuracy required by large-scale online applications, making this task still a challenging problem. This paper introduces a novel framework that achieves high accuracy and efficient matching while ensuring robustness in handling diverse scenarios. To improve efficiency, we begin by modeling the online map matching problem as an Online Markov Decision Process (OMDP) based on its inherent characteristics. This approach helps efficiently merge historical and real-time data, reducing unnecessary calculations. Next, to enhance the model's robustness, we design a reinforcement learning method, enabling robust handling of real-time data from dynamically changing environments. In particular, we propose a novel model learning process and a comprehensive reward function, allowing the model to make reasonable current matches from a future-oriented perspective, and to continuously update and optimize during the decision-making process based on feedback. Lastly, to address the heterogeneity between trajectories and roads, we design distinct graph structures, facilitating efficient representation learning through graph and recurrent neural networks. To further align trajectory and road data, we introduce contrastive learning to decrease their distance in the latent space, thereby promoting effective integration of the two. Extensive evaluations on three real-world datasets confirm that our method significantly outperforms existing state-of-the-art solutions in terms of accuracy, efficiency and robustness.

### Neural Network-based Vehicular Channel Estimation Performance: Effect of Noise in the Training Set 
[[arxiv](https://arxiv.org/abs/2502.06824)] [[cool](https://papers.cool/arxiv/2502.06824)] [[pdf](https://arxiv.org/pdf/2502.06824)]
> **Authors**: Simbarashe Aldrin Ngorima,Albert Helberg,Marelie H. Davel
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-11
> **comment**: 11 pages, 5 Figures
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Vehicular communication systems face significant challenges due to high mobility and rapidly changing environments, which affect the channel over which the signals travel. To address these challenges, neural network (NN)-based channel estimation methods have been suggested. These methods are primarily trained on high signal-to-noise ratio (SNR) with the assumption that training a NN in less noisy conditions can result in good generalisation. This study examines the effectiveness of training NN-based channel estimators on mixed SNR datasets compared to training solely on high SNR datasets, as seen in several related works. Estimators evaluated in this work include an architecture that uses convolutional layers and self-attention mechanisms; a method that employs temporal convolutional networks and data pilot-aided estimation; two methods that combine classical methods with multilayer perceptrons; and the current state-of-the-art model that combines Long-Short-Term Memory networks with data pilot-aided and temporal averaging methods as post processing. Our results indicate that using only high SNR data for training is not always optimal, and the SNR range in the training dataset should be treated as a hyperparameter that can be adjusted for better performance. This is illustrated by the better performance of some models in low SNR conditions when trained on the mixed SNR dataset, as opposed to when trained exclusively on high SNR data.

### CTR-Driven Advertising Image Generation with Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.06823)] [[cool](https://papers.cool/arxiv/2502.06823)] [[pdf](https://arxiv.org/pdf/2502.06823)]
> **Authors**: Xingye Chen,Wei Feng,Zhenbang Du,Weizhen Wang,Yanyin Chen,Haohan Wang,Linkai Liu,Yaoyu Li,Jinyuan Zhao,Yu Li,Zheng Zhang,Jingjing Lv,Junjie Shen,Zhangang Lin,Jingping Shao,Yuanjie Shao,Xinge You,Changxin Gao,Nong Sang
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-11
> **comment**: Accepted to WWW 2025
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别,图形,信息检索
- **Abstract**: In web data, advertising images are crucial for capturing user attention and improving advertising effectiveness. Most existing methods generate background for products primarily focus on the aesthetic quality, which may fail to achieve satisfactory online performance. To address this limitation, we explore the use of Multimodal Large Language Models (MLLMs) for generating advertising images by optimizing for Click-Through Rate (CTR) as the primary objective. Firstly, we build targeted pre-training tasks, and leverage a large-scale e-commerce multimodal dataset to equip MLLMs with initial capabilities for advertising image generation tasks. To further improve the CTR of generated images, we propose a novel reward model to fine-tune pre-trained MLLMs through Reinforcement Learning (RL), which can jointly utilize multimodal features and accurately reflect user click preferences. Meanwhile, a product-centric preference optimization strategy is developed to ensure that the generated background content aligns with the product characteristics after fine-tuning, enhancing the overall relevance and effectiveness of the advertising images. Extensive experiments have demonstrated that our method achieves state-of-the-art performance in both online and offline metrics. Our code and pre-trained models are publicly available at: https://github.com/Chenguoz/CAIG.

### DiffListener: Discrete Diffusion Model for Listener Generation 
[[arxiv](https://arxiv.org/abs/2502.06822)] [[cool](https://papers.cool/arxiv/2502.06822)] [[pdf](https://arxiv.org/pdf/2502.06822)]
> **Authors**: Siyeol Jung,Taehwan Kim
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-11
> **comment**: Accepted at ICASSP 2025
- **标题**: None
- **领域**: 机器学习,计算语言学,图形
- **Abstract**: The listener head generation (LHG) task aims to generate natural nonverbal listener responses based on the speaker's multimodal cues. While prior work either rely on limited modalities (e.g. audio and facial information) or employ autoregressive approaches which have limitations such as accumulating prediction errors. To address these limitations, we propose DiffListener, a discrete diffusion based approach for non-autoregressive listener head generation. Our model takes the speaker's facial information, audio, and text as inputs, additionally incorporating facial differential information to represent the temporal dynamics of expressions and movements. With this explicit modeling of facial dynamics, DiffListener can generate coherent reaction sequences in a non-autoregressive manner. Through comprehensive experiments, DiffListener demonstrates state-of-the-art performance in both quantitative and qualitative evaluations. The user study shows that DiffListener generates natural context-aware listener reactions that are well synchronized with the speaker. The code and demo videos are available in https://siyeoljung.github.io/DiffListener

### LoCA: Location-Aware Cosine Adaptation for Parameter-Efficient Fine-Tuning 
[[arxiv](https://arxiv.org/abs/2502.06820)] [[cool](https://papers.cool/arxiv/2502.06820)] [[pdf](https://arxiv.org/pdf/2502.06820)]
> **Authors**: Zhekai Du,Yinjie Min,Jingjing Li,Ke Lu,Changliang Zou,Liuhua Peng,Tingjin Chu,Mingming Gong
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Low-rank adaptation (LoRA) has become a prevalent method for adapting pre-trained large language models to downstream tasks. However, the simple low-rank decomposition form may constrain the hypothesis space. To address this limitation, we introduce Location-aware Cosine Adaptation (LoCA), a novel frequency-domain parameter-efficient fine-tuning method based on inverse Discrete Cosine Transform (iDCT) with selective locations of learnable components. We begin with a comprehensive theoretical comparison between frequency-domain and low-rank decompositions for fine-tuning pre-trained large models. Our analysis reveals that frequency-domain approximation with carefully selected frequency components can surpass the expressivity of traditional low-rank-based methods. Furthermore, we demonstrate that iDCT offers a more efficient implementation compared to inverse Discrete Fourier Transform (iDFT), allowing for better selection and tuning of frequency components while maintaining equivalent expressivity to the optimal iDFT-based adaptation. By employing finite-difference approximation to estimate gradients for discrete locations of learnable coefficients on the DCT spectrum, LoCA dynamically selects the most informative frequency components during training. Experiments on diverse language and vision fine-tuning tasks demonstrate that LoCA offers enhanced parameter efficiency while maintains computational feasibility comparable to low-rank-based methods.

### Functional 3D Scene Synthesis through Human-Scene Optimization 
[[arxiv](https://arxiv.org/abs/2502.06819)] [[cool](https://papers.cool/arxiv/2502.06819)] [[pdf](https://arxiv.org/pdf/2502.06819)]
> **Authors**: Yao Wei,Matteo Toso,Pietro Morerio,Michael Ying Yang,Alessio Del Bue
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-11
> **comment**: 17 pages, 14 figures
- **标题**: None
- **领域**: 机器学习,图形
- **Abstract**: This paper presents a novel generative approach that outputs 3D indoor environments solely from a textual description of the scene. Current methods often treat scene synthesis as a mere layout prediction task, leading to rooms with overlapping objects or overly structured scenes, with limited consideration of the practical usability of the generated environment. Instead, our approach is based on a simple, but effective principle: we condition scene synthesis to generate rooms that are usable by humans. This principle is implemented by synthesizing 3D humans that interact with the objects composing the scene. If this human-centric scene generation is viable, the room layout is functional and it leads to a more coherent 3D structure. To this end, we propose a novel method for functional 3D scene synthesis, which consists of reasoning, 3D assembling and optimization. We regard text guided 3D synthesis as a reasoning process by generating a scene graph via a graph diffusion network. Considering object functional co-occurrence, a new strategy is designed to better accommodate human-object interaction and avoidance, achieving human-aware 3D scene optimization. We conduct both qualitative and quantitative experiments to validate the effectiveness of our method in generating coherent 3D scene synthesis results.

### Globality Strikes Back: Rethinking the Global Knowledge of CLIP in Training-Free Open-Vocabulary Semantic Segmentation 
[[arxiv](https://arxiv.org/abs/2502.06818)] [[cool](https://papers.cool/arxiv/2502.06818)] [[pdf](https://arxiv.org/pdf/2502.06818)]
> **Authors**: Jingyun Wang,Cilin Yan,Guoliang Kang
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-11
> **comment**: Under review
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Recent works modify CLIP to perform open-vocabulary semantic segmentation in a training-free manner (TF-OVSS). In CLIP, patch-wise image representations mainly encode the homogeneous image-level properties and thus are not discriminative enough, hindering its application to the dense prediction task. Previous works make image features more distinct across patches, through making each patch mainly attend to itself or the neighboring patches within a narrow local window. However, with their modifications, the ability of CLIP to aggregate global context information, which is known to be useful for distinguishing confusing categories, is largely weakened. In this paper, we propose a new method named GCLIP, which mines the beneficial global knowledge of CLIP to facilitate the TF-OVSS task. Firstly, we aim to equip the last-block attention with image-level properties while not introducing homogeneous attention patterns across patches. In GCLIP, we merge the attention from the global token emerging blocks with the Query-Query attention to realize this goal. Secondly, we aim to make the Value embeddings of the last-block attention module more distinct and semantically correlated. To realize this, we design a novel channel suppression strategy. As the representation of each patch is finally determined by the attention weights and the Value embeddings, our method can generate more discriminative patch-level image features while absorbing global context information. Extensive experiments on five standard benchmarks demonstrate that our method consistently outperforms previous state-of-the-arts.

### DeepCell: Multiview Representation Learning for Post-Mapping Netlists 
[[arxiv](https://arxiv.org/abs/2502.06816)] [[cool](https://papers.cool/arxiv/2502.06816)] [[pdf](https://arxiv.org/pdf/2502.06816)]
> **Authors**: Zhengyuan Shi,Chengyu Ma,Ziyang Zheng,Lingfeng Zhou,Hongyang Pan,Wentao Jiang,Fan Yang,Xiaoyan Yang,Zhufei Chu,Qiang Xu
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Representation learning for post-mapping (PM) netlists is a critical challenge in Electronic Design Automation (EDA), driven by the diverse and complex nature of modern circuit designs. Existing approaches focus on intermediate representations like And-Inverter Graphs (AIGs), limiting their applicability to post-synthesis stages. We introduce DeepCell, a multiview representation learning framework that integrates structural and functional insights from both PM netlists and AIGs to learn rich, generalizable embeddings. At its core, DeepCell employs the novel Mask Circuit Modeling (MCM) mechanism, which refines PM netlist representations in a self-supervised manner using pretrained AIG encoders. DeepCell sets a new benchmark in PM netlist representation, outperforming existing methods in predictive accuracy and reconstruction fidelity. To validate its efficacy, we apply DeepCell to functional Engineering Change Orders (ECO), achieving significant reductions in patch generation costs and runtime while improving patch quality.

### Honegumi: An Interface for Accelerating the Adoption of Bayesian Optimization in the Experimental Sciences 
[[arxiv](https://arxiv.org/abs/2502.06815)] [[cool](https://papers.cool/arxiv/2502.06815)] [[pdf](https://arxiv.org/pdf/2502.06815)]
> **Authors**: Sterling G. Baird,Andrew R. Falkowski,Taylor D. Sparks
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-11
> **comment**: 7 pages, 3 figures, 1 table
- **标题**: None
- **领域**: 机器学习,材料科学
- **Abstract**: Bayesian optimization (BO) has emerged as a powerful tool for guiding experimental design and decision-making in various scientific fields, including materials science, chemistry, and biology. However, despite its growing popularity, the complexity of existing BO libraries and the steep learning curve associated with them can deter researchers who are not well-versed in machine learning or programming. To address this barrier, we introduce Honegumi, a user-friendly, interactive tool designed to simplify the process of creating advanced Bayesian optimization scripts. Honegumi offers a dynamic selection grid that allows users to configure key parameters of their optimization tasks, generating ready-to-use, unit-tested Python scripts tailored to their specific needs. Accompanying the interface is a comprehensive suite of tutorials that provide both conceptual and practical guidance, bridging the gap between theoretical understanding and practical implementation. Built on top of the Ax platform, Honegumi leverages the power of existing state-of-the-art libraries while restructuring the user experience to make advanced BO techniques more accessible to experimental researchers. By lowering the barrier to entry and providing educational resources, Honegumi aims to accelerate the adoption of advanced Bayesian optimization methods across various domains.

### Diffusion Instruction Tuning 
[[arxiv](https://arxiv.org/abs/2502.06814)] [[cool](https://papers.cool/arxiv/2502.06814)] [[pdf](https://arxiv.org/pdf/2502.06814)]
> **Authors**: Chen Jin,Ryutaro Tanno,Amrutha Saseendran,Tom Diethe,Philip Teare
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-11
> **comment**: Project page at https://astrazeneca.github.io/vlm/
- **标题**: None
- **领域**: 机器学习,人工智能,图形
- **Abstract**: We introduce Lavender, a simple supervised fine-tuning (SFT) method that boosts the performance of advanced vision-language models (VLMs) by leveraging state-of-the-art image generation models such as Stable Diffusion. Specifically, Lavender aligns the text-vision attention in the VLM transformer with the equivalent used by Stable Diffusion during SFT, instead of adapting separate encoders. This alignment enriches the model's visual understanding and significantly boosts performance across in- and out-of-distribution tasks. Lavender requires just 0.13 million training examples, 2.5% of typical large-scale SFT datasets, and fine-tunes on standard hardware (8 GPUs) in a single day. It consistently improves state-of-the-art open-source multimodal LLMs (e.g., Llama-3.2-11B, MiniCPM-Llama3-v2.5), achieving up to 30% gains and a 68% boost on challenging out-of-distribution medical QA tasks. By efficiently transferring the visual expertise of image generators with minimal supervision, Lavender offers a scalable solution for more accurate vision-language systems. All code, training data, and models will be shared at https://astrazeneca.github.io/vlm/.

### Policy Guided Tree Search for Enhanced LLM Reasoning 
[[arxiv](https://arxiv.org/abs/2502.06813)] [[cool](https://papers.cool/arxiv/2502.06813)] [[pdf](https://arxiv.org/pdf/2502.06813)]
> **Authors**: Yang Li
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Despite their remarkable capabilities, large language models often struggle with tasks requiring complex reasoning and planning. While existing approaches like Chain-of-Thought prompting and tree search techniques show promise, they are limited by their reliance on predefined heuristics and computationally expensive exploration strategies. We propose Policy-Guided Tree Search (PGTS), a framework that combines reinforcement learning with structured tree exploration to efficiently navigate reasoning paths. Our key innovation is a learned policy that dynamically decides between expanding, branching, backtracking, or terminating exploration, eliminating the need for manual heuristics or exhaustive search. Experiments across mathematical reasoning, logical deduction, and planning benchmarks demonstrate that PGTS achieves superior reasoning performance while significantly reducing computational costs compared to existing methods. These results establish PGTS as a scalable and effective solution for tackling complex reasoning tasks with LLMs.

### Harness Local Rewards for Global Benefits: Effective Text-to-Video Generation Alignment with Patch-level Reward Models 
[[arxiv](https://arxiv.org/abs/2502.06812)] [[cool](https://papers.cool/arxiv/2502.06812)] [[pdf](https://arxiv.org/pdf/2502.06812)]
> **Authors**: Shuting Wang,Haihong Tang,Zhicheng Dou,Chenyan Xiong
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,图形
- **Abstract**: The emergence of diffusion models (DMs) has significantly improved the quality of text-to-video generation models (VGMs). However, current VGM optimization primarily emphasizes the global quality of videos, overlooking localized errors, which leads to suboptimal generation capabilities. To address this issue, we propose a post-training strategy for VGMs, HALO, which explicitly incorporates local feedback from a patch reward model, providing detailed and comprehensive training signals with the video reward model for advanced VGM optimization. To develop an effective patch reward model, we distill GPT-4o to continuously train our video reward model, which enhances training efficiency and ensures consistency between video and patch reward distributions. Furthermore, to harmoniously integrate patch rewards into VGM optimization, we introduce a granular DPO (Gran-DPO) algorithm for DMs, allowing collaborative use of both patch and video rewards during the optimization process. Experimental results indicate that our patch reward model aligns well with human annotations and HALO substantially outperforms the baselines across two evaluation methods. Further experiments quantitatively prove the existence of patch defects, and our proposed method could effectively alleviate this issue.

### Aligning Human and Machine Attention for Enhanced Supervised Learning 
[[arxiv](https://arxiv.org/abs/2502.06811)] [[cool](https://papers.cool/arxiv/2502.06811)] [[pdf](https://arxiv.org/pdf/2502.06811)]
> **Authors**: Avihay Chriqui,Inbal Yahav,Dov Teeni,Ahmed Abbasi
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Attention, or prioritization of certain information items over others, is a critical element of any learning process, for both humans and machines. Given that humans continue to outperform machines in certain learning tasks, it seems plausible that machine performance could be enriched by aligning machine attention with human attention mechanisms -- yet research on this topic is sparse and has achieved only limited success. This paper proposes a new approach to address this gap, called Human-Machine Attention Learning (HuMAL). This approach involves reliance on data annotated by humans to reflect their self-perceived attention during specific tasks. We evaluate several alternative strategies for integrating such human attention data into machine learning (ML) algorithms, using a sentiment analysis task (review data from Yelp) and a personality-type classification task (data from myPersonality). The best-performing HuMAL strategy significantly enhances the task performance of fine-tuned transformer models (BERT, as well as GPT-2 and XLNET), and the benefit is particularly pronounced under challenging conditions of imbalanced or sparse labeled data. This research contributes to a deeper understanding of strategies for integrating human attention into ML models and highlights the potential of leveraging human cognition to augment ML in real-world applications.

### Neurons Speak in Ranges: Breaking Free from Discrete Neuronal Attribution 
[[arxiv](https://arxiv.org/abs/2502.06809)] [[cool](https://papers.cool/arxiv/2502.06809)] [[pdf](https://arxiv.org/pdf/2502.06809)]
> **Authors**: Muhammad Umair Haider,Hammad Rizwan,Hassan Sajjad,Peizhong Ju,A. B. Siddique
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Interpreting and controlling the internal mechanisms of large language models (LLMs) is crucial for improving their trustworthiness and utility. Recent efforts have primarily focused on identifying and manipulating neurons by establishing discrete mappings between neurons and semantic concepts. However, such mappings struggle to handle the inherent polysemanticity in LLMs, where individual neurons encode multiple, distinct concepts. This makes precise control challenging and complicates downstream interventions. Through an in-depth analysis of both encoder and decoder-based LLMs across multiple text classification datasets, we uncover that while individual neurons encode multiple concepts, their activation magnitudes vary across concepts in distinct, Gaussian-like patterns. Building on this insight, we introduce NeuronLens, a novel range-based interpretation and manipulation framework that provides a finer view of neuron activation distributions to localize concept attribution within a neuron. Extensive empirical evaluations demonstrate that NeuronLens significantly reduces unintended interference, while maintaining precise control for manipulation of targeted concepts, outperforming existing methods.

### On the Benefits of Attribute-Driven Graph Domain Adaptation 
[[arxiv](https://arxiv.org/abs/2502.06808)] [[cool](https://papers.cool/arxiv/2502.06808)] [[pdf](https://arxiv.org/pdf/2502.06808)]
> **Authors**: Ruiyi Fang,Bingheng Li,Zhao Kang,Qiuhao Zeng,Nima Hosseini Dashtbayaz,Ruizhi Pu,Boyu Wang,Charles Ling
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-11
> **comment**: Accepted by the ICLR 2025
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Graph Domain Adaptation (GDA) addresses a pressing challenge in cross-network learning, particularly pertinent due to the absence of labeled data in real-world graph datasets. Recent studies attempted to learn domain invariant representations by eliminating structural shifts between graphs. In this work, we show that existing methodologies have overlooked the significance of the graph node attribute, a pivotal factor for graph domain alignment. Specifically, we first reveal the impact of node attributes for GDA by theoretically proving that in addition to the graph structural divergence between the domains, the node attribute discrepancy also plays a critical role in GDA. Moreover, we also empirically show that the attribute shift is more substantial than the topology shift, which further underscores the importance of node attribute alignment in GDA. Inspired by this finding, a novel cross-channel module is developed to fuse and align both views between the source and target graphs for GDA. Experimental results on a variety of benchmarks verify the effectiveness of our method.

### Competitive Programming with Large Reasoning Models 
[[arxiv](https://arxiv.org/abs/2502.06807)] [[cool](https://papers.cool/arxiv/2502.06807)] [[pdf](https://arxiv.org/pdf/2502.06807)]
> **Authors**: OpenAI,:,Ahmed El-Kishky,Alexander Wei,Andre Saraiva,Borys Minaiev,Daniel Selsam,David Dohan,Francis Song,Hunter Lightman,Ignasi Clavera,Jakub Pachocki,Jerry Tworek,Lorenz Kuhn,Lukasz Kaiser,Mark Chen,Max Schwarzer,Mostafa Rohaninejad,Nat McAleese,o3 contributors,Oleg Mürk,Rhythm Garg,Rui Shu,Szymon Sidor,Vineet Kosaraju, et al. (1 additional authors not shown)
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: We show that reinforcement learning applied to large language models (LLMs) significantly boosts performance on complex coding and reasoning tasks. Additionally, we compare two general-purpose reasoning models - OpenAI o1 and an early checkpoint of o3 - with a domain-specific system, o1-ioi, which uses hand-engineered inference strategies designed for competing in the 2024 International Olympiad in Informatics (IOI). We competed live at IOI 2024 with o1-ioi and, using hand-crafted test-time strategies, placed in the 49th percentile. Under relaxed competition constraints, o1-ioi achieved a gold medal. However, when evaluating later models such as o3, we find that o3 achieves gold without hand-crafted domain-specific strategies or relaxed constraints. Our findings show that although specialized pipelines such as o1-ioi yield solid improvements, the scaled-up, general-purpose o3 model surpasses those results without relying on hand-crafted inference heuristics. Notably, o3 achieves a gold medal at the 2024 IOI and obtains a Codeforces rating on par with elite human competitors. Overall, these results indicate that scaling general-purpose reinforcement learning, rather than relying on domain-specific techniques, offers a robust path toward state-of-the-art AI in reasoning domains, such as competitive programming.

### Logits are All We Need to Adapt Closed Models 
[[arxiv](https://arxiv.org/abs/2502.06806)] [[cool](https://papers.cool/arxiv/2502.06806)] [[pdf](https://arxiv.org/pdf/2502.06806)]
> **Authors**: Gaurush Hiranandani,Haolun Wu,Subhojyoti Mukherjee,Sanmi Koyejo
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-11
> **comment**: 33 pages, 8 figures
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Many commercial Large Language Models (LLMs) are often closed-source, limiting developers to prompt tuning for aligning content generation with specific applications. While these models currently do not provide access to token logits, we argue that if such access were available, it would enable more powerful adaptation techniques beyond prompt engineering. In this paper, we propose a token-level probability reweighting framework that, given access to logits and a small amount of task-specific data, can effectively steer black-box LLMs toward application-specific content generation. Our approach views next-token prediction through the lens of supervised classification. We show that aligning black-box LLMs with task-specific data can be formulated as a label noise correction problem, leading to \emph{Plugin} model -- an autoregressive probability reweighting model that operates solely on logits. We provide theoretical justification for why reweighting logits alone is sufficient for task adaptation. Extensive experiments with multiple datasets, LLMs, and reweighting models demonstrate the effectiveness of our method, advocating for broader access to token logits in closed-source models.

### Efficient Diffusion Models: A Survey 
[[arxiv](https://arxiv.org/abs/2502.06805)] [[cool](https://papers.cool/arxiv/2502.06805)] [[pdf](https://arxiv.org/pdf/2502.06805)]
> **Authors**: Hui Shen,Jingxuan Zhang,Boning Xiong,Rui Hu,Shoufa Chen,Zhongwei Wan,Xin Wang,Yu Zhang,Zixuan Gong,Guangyin Bao,Chaofan Tao,Yongfeng Huang,Ye Yuan,Mi Zhang
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,图形
- **Abstract**: Diffusion models have emerged as powerful generative models capable of producing high-quality contents such as images, videos, and audio, demonstrating their potential to revolutionize digital content creation. However, these capabilities come at the cost of their significant computational resources and lengthy generation time, underscoring the critical need to develop efficient techniques for practical deployment. In this survey, we provide a systematic and comprehensive review of research on efficient diffusion models. We organize the literature in a taxonomy consisting of three main categories, covering distinct yet interconnected efficient diffusion model topics from algorithm-level, system-level, and framework perspective, respectively. We have also created a GitHub repository where we organize the papers featured in this survey at https://github.com/AIoT-MLSys-Lab/Efficient-Diffusion-Model-Survey. We hope our survey can serve as a valuable resource to help researchers and practitioners gain a systematic understanding of efficient diffusion model research and inspire them to contribute to this important and exciting field.

### Emotion Recognition and Generation: A Comprehensive Review of Face, Speech, and Text Modalities 
[[arxiv](https://arxiv.org/abs/2502.06803)] [[cool](https://papers.cool/arxiv/2502.06803)] [[pdf](https://arxiv.org/pdf/2502.06803)]
> **Authors**: Rebecca Mobbs,Dimitrios Makris,Vasileios Argyriou
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-11
> **comment**: Submitted to ACM Computing Surveys
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: Emotion recognition and generation have emerged as crucial topics in Artificial Intelligence research, playing a significant role in enhancing human-computer interaction within healthcare, customer service, and other fields. Although several reviews have been conducted on emotion recognition and generation as separate entities, many of these works are either fragmented or limited to specific methodologies, lacking a comprehensive overview of recent developments and trends across different modalities. In this survey, we provide a holistic review aimed at researchers beginning their exploration in emotion recognition and generation. We introduce the fundamental principles underlying emotion recognition and generation across facial, vocal, and textual modalities. This work categorises recent state-of-the-art research into distinct technical approaches and explains the theoretical foundations and motivations behind these methodologies, offering a clearer understanding of their application. Moreover, we discuss evaluation metrics, comparative analyses, and current limitations, shedding light on the challenges faced by researchers in the field. Finally, we propose future research directions to address these challenges and encourage further exploration into developing robust, effective, and ethically responsible emotion recognition and generation systems.

### Analyzing Geospatial and Socioeconomic Disparities in Breast Cancer Screening Among Populations in the United States: Machine Learning Approach 
[[arxiv](https://arxiv.org/abs/2502.06800)] [[cool](https://papers.cool/arxiv/2502.06800)] [[pdf](https://arxiv.org/pdf/2502.06800)]
> **Authors**: Soheil Hashtarkhani,Yiwang Zhou,Fekede Asefa Kumsa,Shelley White-Means,David L Schwartz,Arash Shaban-Nejad
> **First submission**: 2025-01-30
> **First announcement**: 2025-02-11
> **comment**: 11 Pages, 4 Figures, 2 Tables
- **标题**: None
- **领域**: 机器学习,应用领域
- **Abstract**: Breast cancer screening plays a pivotal role in early detection and subsequent effective management of the disease, impacting patient outcomes and survival rates. This study aims to assess breast cancer screening rates nationwide in the United States and investigate the impact of social determinants of health on these screening rates. Data on mammography screening at the census tract level for 2018 and 2020 were collected from the Behavioral Risk Factor Surveillance System. We developed a large dataset of social determinants of health, comprising 13 variables for 72337 census tracts. Spatial analysis employing Getis-Ord Gi statistics was used to identify clusters of high and low breast cancer screening rates. To evaluate the influence of these social determinants, we implemented a random forest model, with the aim of comparing its performance to linear regression and support vector machine models. The models were evaluated using R2 and root mean squared error metrics. Shapley Additive Explanations values were subsequently used to assess the significance of variables and direction of their influence. Geospatial analysis revealed elevated screening rates in the eastern and northern United States, while central and midwestern regions exhibited lower rates. The random forest model demonstrated superior performance, with an R2=64.53 and root mean squared error of 2.06 compared to linear regression and support vector machine models. Shapley Additive Explanations values indicated that the percentage of the Black population, the number of mammography facilities within a 10-mile radius, and the percentage of the population with at least a bachelor's degree were the most influential variables, all positively associated with mammography screening rates.

### Prompt-Aware Scheduling for Efficient Text-to-Image Inferencing System 
[[arxiv](https://arxiv.org/abs/2502.06798)] [[cool](https://papers.cool/arxiv/2502.06798)] [[pdf](https://arxiv.org/pdf/2502.06798)]
> **Authors**: Shubham Agarwal,Saud Iqbal,Subrata Mitra
> **First submission**: 2025-01-28
> **First announcement**: 2025-02-11
> **comment**: Poster presented at NSDI 2024
- **标题**: None
- **领域**: 机器学习,分布式、并行和集群计算,图形
- **Abstract**: Traditional ML models utilize controlled approximations during high loads, employing faster, but less accurate models in a process called accuracy scaling. However, this method is less effective for generative text-to-image models due to their sensitivity to input prompts and performance degradation caused by large model loading overheads. This work introduces a novel text-to-image inference system that optimally matches prompts across multiple instances of the same model operating at various approximation levels to deliver high-quality images under high loads and fixed budgets.

### Information-theoretic Bayesian Optimization: Survey and Tutorial 
[[arxiv](https://arxiv.org/abs/2502.06789)] [[cool](https://papers.cool/arxiv/2502.06789)] [[pdf](https://arxiv.org/pdf/2502.06789)]
> **Authors**: Eduardo C. Garrido-Merchán
> **First submission**: 2025-01-22
> **First announcement**: 2025-02-11
> **comment**: None
- **标题**: None
- **领域**: 机器学习,人工智能,信息论,机器学习
- **Abstract**: Several scenarios require the optimization of non-convex black-box functions, that are noisy expensive to evaluate functions with unknown analytical expression, whose gradients are hence not accessible. For example, the hyper-parameter tuning problem of machine learning models. Bayesian optimization is a class of methods with state-of-the-art performance delivering a solution to this problem in real scenarios. It uses an iterative process that employs a probabilistic surrogate model, typically a Gaussian process, of the objective function to be optimized computing a posterior predictive distribution of the black-box function. Based on the information given by this posterior predictive distribution, Bayesian optimization includes the computation of an acquisition function that represents, for every input space point, the utility of evaluating that point in the next iteraiton if the objective of the process is to retrieve a global extremum. This paper is a survey of the information theoretical acquisition functions, whose performance typically outperforms the rest of acquisition functions. The main concepts of the field of information theory are also described in detail to make the reader aware of why information theory acquisition functions deliver great results in Bayesian optimization and how can we approximate them when they are intractable. We also cover how information theory acquisition functions can be adapted to complex optimization scenarios such as the multi-objective, constrained, non-myopic, multi-fidelity, parallel and asynchronous settings and provide further lines of research.

### Matryoshka Quantization 
[[arxiv](https://arxiv.org/abs/2502.06786)] [[cool](https://papers.cool/arxiv/2502.06786)] [[pdf](https://arxiv.org/pdf/2502.06786)]
> **Authors**: Pranav Nair,Puranjay Datta,Jeff Dean,Prateek Jain,Aditya Kusupati
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Quantizing model weights is critical for reducing the communication and inference costs of large models. However, quantizing models -- especially to low precisions like int4 or int2 -- requires a trade-off in model quality; int2, in particular, is known to severely degrade model quality. Consequently, practitioners are often forced to maintain multiple models with different quantization levels or serve a single model that best satisfies the quality-latency trade-off. On the other hand, integer data types, such as int8, inherently possess a nested (Matryoshka) structure where smaller bit-width integers, like int4 or int2, are nested within the most significant bits. Leveraging this insight, in this paper, we propose Matryoshka Quantization (\alg), a novel multi-scale quantization technique that alleviates the aforementioned challenge. This technique allows us to train and maintain a single quantized model but serve it with the precision demanded by the deployment. Furthermore, leveraging \alg's co-training and co-distillation regularization, int2 precision models extracted by \alg outperform standard int2 quantization by up to to 4\% and 7\% with OmniQuant and QAT as base algorithms respectively. Finally, we demonstrate that by using an extra bit to represent outliers, a model with an effective precision of 2.05-bit gives an additional 6\% improvement with OmniQuant as the base algorithm.

### DeepCrossAttention: Supercharging Transformer Residual Connections 
[[arxiv](https://arxiv.org/abs/2502.06785)] [[cool](https://papers.cool/arxiv/2502.06785)] [[pdf](https://arxiv.org/pdf/2502.06785)]
> **Authors**: Mike Heddes,Adel Javanmard,Kyriakos Axiotis,Gang Fu,MohammadHossein Bateni,Vahab Mirrokni
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Transformer networks have achieved remarkable success across diverse domains, leveraging a variety of architectural innovations, including residual connections. However, traditional residual connections, which simply sum the outputs of previous layers, can dilute crucial information. This work introduces DeepCrossAttention (DCA), an approach that enhances residual learning in transformers. DCA employs learnable, input-dependent weights to dynamically combine layer outputs, enabling the model to selectively focus on the most relevant information in any of the previous layers. Furthermore, DCA incorporates depth-wise cross-attention, allowing for richer interactions between layers at different depths. Our language modeling experiments show that DCA achieves improved perplexity for a given training time. Moreover, DCA obtains the same model quality up to 3x faster while adding a negligible number of parameters. Theoretical analysis confirms that DCA provides an improved trade-off between accuracy and model size when the ratio of collective layer ranks to the ambient dimension falls below a critical threshold.

### RelGNN: Composite Message Passing for Relational Deep Learning 
[[arxiv](https://arxiv.org/abs/2502.06784)] [[cool](https://papers.cool/arxiv/2502.06784)] [[pdf](https://arxiv.org/pdf/2502.06784)]
> **Authors**: Tianlang Chen,Charilaos Kanatsoulis,Jure Leskovec
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 14 pages
- **标题**: None
- **领域**: 机器学习,人工智能,数据库
- **Abstract**: Predictive tasks on relational databases are critical in real-world applications spanning e-commerce, healthcare, and social media. To address these tasks effectively, Relational Deep Learning (RDL) encodes relational data as graphs, enabling Graph Neural Networks (GNNs) to exploit relational structures for improved predictions. However, existing heterogeneous GNNs often overlook the intrinsic structural properties of relational databases, leading to modeling inefficiencies. Here we introduce RelGNN, a novel GNN framework specifically designed to capture the unique characteristics of relational databases. At the core of our approach is the introduction of atomic routes, which are sequences of nodes forming high-order tripartite structures. Building upon these atomic routes, RelGNN designs new composite message passing mechanisms between heterogeneous nodes, allowing direct single-hop interactions between them. This approach avoids redundant aggregations and mitigates information entanglement, ultimately leading to more efficient and accurate predictive modeling. RelGNN is evaluated on 30 diverse real-world tasks from RelBench (Fey et al., 2024), and consistently achieves state-of-the-art accuracy with up to 25% improvement.

### Towards Internet-Scale Training For Agents 
[[arxiv](https://arxiv.org/abs/2502.06776)] [[cool](https://papers.cool/arxiv/2502.06776)] [[pdf](https://arxiv.org/pdf/2502.06776)]
> **Authors**: Brandon Trabucco,Gunnar Sigurdsson,Robinson Piramuthu,Ruslan Salakhutdinov
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: The predominant approach for training web navigation agents gathers human demonstrations for a set of popular websites and hand-written tasks, but it is becoming clear that human data are an inefficient resource. We develop a pipeline to facilitate Internet-scale training for agents without laborious human annotations. In the first stage, an LLM generates tasks for 150k diverse websites. In the next stage, LLM agents complete tasks and produce trajectories. In the final stage, an LLM reviews the trajectories and judges their success. Language models are competitive with human annotators, detecting and filtering out harmful content with an accuracy of 97%, generating feasible tasks with an 89% rate, and judging successful trajectories with an 82.6% accuracy. Scaling the pipeline, agents based on Llama 3.1 70B solve 16.7% of tasks for 150k sites. Training on the data generated by our pipeline is competitive with training on human demonstrations. In data-limited settings derived from Mind2Web and WebLINX, we improve Step Accuracy by up to +89.5% and +122.1% respectively for agents trained on mixtures of data from our pipeline, and human data. When training agents with all available human data from these benchmarks, agents fail to generalize to diverse real sites, and adding our data improves their generalization by +149.0% for WebLINX and +156.3% for Mind2Web. Code will be available at: data-for-agents.github.io.

### Enhancing Performance of Explainable AI Models with Constrained Concept Refinement 
[[arxiv](https://arxiv.org/abs/2502.06775)] [[cool](https://papers.cool/arxiv/2502.06775)] [[pdf](https://arxiv.org/pdf/2502.06775)]
> **Authors**: Geyu Liang,Senne Michielssen,Salar Fattahi
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The trade-off between accuracy and interpretability has long been a challenge in machine learning (ML). This tension is particularly significant for emerging interpretable-by-design methods, which aim to redesign ML algorithms for trustworthy interpretability but often sacrifice accuracy in the process. In this paper, we address this gap by investigating the impact of deviations in concept representations-an essential component of interpretable models-on prediction performance and propose a novel framework to mitigate these effects. The framework builds on the principle of optimizing concept embeddings under constraints that preserve interpretability. Using a generative model as a test-bed, we rigorously prove that our algorithm achieves zero loss while progressively enhancing the interpretability of the resulting model. Additionally, we evaluate the practical performance of our proposed framework in generating explainable predictions for image classification tasks across various benchmarks. Compared to existing explainable methods, our approach not only improves prediction accuracy while preserving model interpretability across various large-scale benchmarks but also achieves this with significantly lower computational cost.

### ENFORCE: Exact Nonlinear Constrained Learning with Adaptive-depth Neural Projection 
[[arxiv](https://arxiv.org/abs/2502.06774)] [[cool](https://papers.cool/arxiv/2502.06774)] [[pdf](https://arxiv.org/pdf/2502.06774)]
> **Authors**: Giacomo Lastrucci,Artur M. Schweidtmann
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Ensuring neural networks adhere to domain-specific constraints is crucial for addressing safety and ethical concerns while also enhancing prediction accuracy. Despite the nonlinear nature of most real-world tasks, existing methods are predominantly limited to affine or convex constraints. We introduce ENFORCE, a neural network architecture that guarantees predictions to satisfy nonlinear constraints exactly. ENFORCE is trained with standard unconstrained gradient-based optimizers (e.g., Adam) and leverages autodifferentiation and local neural projections to enforce any $\mathcal{C}^1$ constraint to arbitrary tolerance $ε$. We build an adaptive-depth neural projection (AdaNP) module that dynamically adjusts its complexity to suit the specific problem and the required tolerance levels. ENFORCE guarantees satisfaction of equality constraints that are nonlinear in both inputs and outputs of the neural network with minimal (and adjustable) computational cost.

### Train for the Worst, Plan for the Best: Understanding Token Ordering in Masked Diffusions 
[[arxiv](https://arxiv.org/abs/2502.06768)] [[cool](https://papers.cool/arxiv/2502.06768)] [[pdf](https://arxiv.org/pdf/2502.06768)]
> **Authors**: Jaeyeon Kim,Kulin Shah,Vasilis Kontonis,Sham Kakade,Sitan Chen
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: In recent years, masked diffusion models (MDMs) have emerged as a promising alternative approach for generative modeling over discrete domains. Compared to autoregressive models (ARMs), MDMs trade off complexity at training time with flexibility at inference time. At training time, they must learn to solve an exponentially large number of infilling problems, but at inference time, they can decode tokens in essentially arbitrary order. In this work, we closely examine these two competing effects. On the training front, we theoretically and empirically demonstrate that MDMs indeed train on computationally intractable subproblems compared to their autoregressive counterparts. On the inference front, we show that a suitable strategy for adaptively choosing the token decoding order significantly enhances the capabilities of MDMs, allowing them to sidestep hard subproblems. On logic puzzles like Sudoku, we show that adaptive inference can boost solving accuracy in pretrained MDMs from $<7$% to $\approx 90$%, even outperforming ARMs with $7\times$ as many parameters and that were explicitly trained via teacher forcing to learn the right order of decoding.

### History-Guided Video Diffusion 
[[arxiv](https://arxiv.org/abs/2502.06764)] [[cool](https://papers.cool/arxiv/2502.06764)] [[pdf](https://arxiv.org/pdf/2502.06764)]
> **Authors**: Kiwhan Song,Boyuan Chen,Max Simchowitz,Yilun Du,Russ Tedrake,Vincent Sitzmann
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Project Website: https://boyuan.space/history-guidance
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: Classifier-free guidance (CFG) is a key technique for improving conditional generation in diffusion models, enabling more accurate control while enhancing sample quality. It is natural to extend this technique to video diffusion, which generates video conditioned on a variable number of context frames, collectively referred to as history. However, we find two key challenges to guiding with variable-length history: architectures that only support fixed-size conditioning, and the empirical observation that CFG-style history dropout performs poorly. To address this, we propose the Diffusion Forcing Transformer (DFoT), a video diffusion architecture and theoretically grounded training objective that jointly enable conditioning on a flexible number of history frames. We then introduce History Guidance, a family of guidance methods uniquely enabled by DFoT. We show that its simplest form, vanilla history guidance, already significantly improves video generation quality and temporal consistency. A more advanced method, history guidance across time and frequency further enhances motion dynamics, enables compositional generalization to out-of-distribution history, and can stably roll out extremely long videos. Website: https://boyuan.space/history-guidance

### When, Where and Why to Average Weights? 
[[arxiv](https://arxiv.org/abs/2502.06761)] [[cool](https://papers.cool/arxiv/2502.06761)] [[pdf](https://arxiv.org/pdf/2502.06761)]
> **Authors**: Niccolò Ajroldi,Antonio Orvieto,Jonas Geiping
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Averaging checkpoints along the training trajectory is a simple yet powerful approach to improve the generalization performance of Machine Learning models and reduce training time. Motivated by these potential gains, and in an effort to fairly and thoroughly benchmark this technique, we present an extensive evaluation of averaging techniques in modern Deep Learning, which we perform using AlgoPerf \citep{dahl_benchmarking_2023}, a large-scale benchmark for optimization algorithms. We investigate whether weight averaging can reduce training time, improve generalization, and replace learning rate decay, as suggested by recent literature. Our evaluation across seven architectures and datasets reveals that averaging significantly accelerates training and yields considerable efficiency gains, at the price of a minimal implementation and memory cost, while mildly improving generalization across all considered workloads. Finally, we explore the relationship between averaging and learning rate annealing and show how to optimally combine the two to achieve the best performances.

### What makes a good feedforward computational graph? 
[[arxiv](https://arxiv.org/abs/2502.06751)] [[cool](https://papers.cool/arxiv/2502.06751)] [[pdf](https://arxiv.org/pdf/2502.06751)]
> **Authors**: Alex Vitvitskyi,João G. M. Araújo,Marc Lackenby,Petar Veličković
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Work in progress -- comments welcome. 16 pages, 7 figures
- **标题**: None
- **领域**: 机器学习,人工智能,社交和信息网络,机器学习
- **Abstract**: As implied by the plethora of literature on graph rewiring, the choice of computational graph employed by a neural network can make a significant impact on its downstream performance. Certain effects related to the computational graph, such as under-reaching and over-squashing, may even render the model incapable of learning certain functions. Most of these effects have only been thoroughly studied in the domain of undirected graphs; however, recent years have seen a significant rise in interest in feedforward computational graphs: directed graphs without any back edges. In this paper, we study the desirable properties of a feedforward computational graph, discovering two important complementary measures: fidelity and mixing time, and evaluating a few popular choices of graphs through the lens of these measures. Our study is backed by both theoretical analyses of the metrics' asymptotic behaviour for various graphs, as well as correlating these metrics to the performance of trained neural network models using the corresponding graphs.

### Gradient Multi-Normalization for Stateless and Scalable LLM Training 
[[arxiv](https://arxiv.org/abs/2502.06742)] [[cool](https://papers.cool/arxiv/2502.06742)] [[pdf](https://arxiv.org/pdf/2502.06742)]
> **Authors**: Meyer Scetbon,Chao Ma,Wenbo Gong,Edward Meeds
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Training large language models (LLMs) typically relies on adaptive optimizers like Adam (Kingma & Ba, 2015) which store additional state information to accelerate convergence but incur significant memory overhead. Recent efforts, such as SWAN (Ma et al., 2024) address this by eliminating the need for optimizer states while achieving performance comparable to Adam via a multi-step preprocessing procedure applied to instantaneous gradients. Motivated by the success of SWAN, we introduce a novel framework for designing stateless optimizers that normalizes stochastic gradients according to multiple norms. To achieve this, we propose a simple alternating scheme to enforce the normalization of gradients w.r.t these norms. We show that our procedure can produce, up to an arbitrary precision, a fixed-point of the problem, and that SWAN is a particular instance of our approach with carefully chosen norms, providing a deeper understanding of its design. However, SWAN's computationally expensive whitening/orthogonalization step limit its practicality for large LMs. Using our principled perspective, we develop of a more efficient, scalable, and practical stateless optimizer. Our algorithm relaxes the properties of SWAN, significantly reducing its computational cost while retaining its memory efficiency, making it applicable to training large-scale models. Experiments on pre-training LLaMA models with up to 1 billion parameters demonstrate a 3X speedup over Adam with significantly reduced memory requirements, outperforming other memory-efficient baselines.

### A note on the physical interpretation of neural PDE's 
[[arxiv](https://arxiv.org/abs/2502.06739)] [[cool](https://papers.cool/arxiv/2502.06739)] [[pdf](https://arxiv.org/pdf/2502.06739)]
> **Authors**: Sauro Succi
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 12 pages
- **标题**: None
- **领域**: 机器学习,无序系统和神经网络,计算物理
- **Abstract**: We highlight a formal and substantial analogy between Machine Learning (ML) algorithms and discrete dynamical systems (DDS) in relaxation form. The analogy offers a transparent interpretation of the weights in terms of physical information-propagation processes and identifies the model function of the forward ML step with the local attractor of the corresponding discrete dynamics. Besides improving the explainability of current ML applications, this analogy may also facilitate the development of a new class ML algorithms with a reduced number of weights.

### Resurrecting saturated LLM benchmarks with adversarial encoding 
[[arxiv](https://arxiv.org/abs/2502.06738)] [[cool](https://papers.cool/arxiv/2502.06738)] [[pdf](https://arxiv.org/pdf/2502.06738)]
> **Authors**: Igor Ivanov,Dmitrii Volkov
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Recent work showed that small changes in benchmark questions can reduce LLMs' reasoning and recall. We explore two such changes: pairing questions and adding more answer options, on three benchmarks: WMDP-bio, GPQA, and MMLU variants. We find that for more capable models, these predictably reduce performance, essentially heightening the performance ceiling of a benchmark and unsaturating it again. We suggest this approach can resurrect old benchmarks.

### VersaPRM: Multi-Domain Process Reward Model via Synthetic Reasoning Data 
[[arxiv](https://arxiv.org/abs/2502.06737)] [[cool](https://papers.cool/arxiv/2502.06737)] [[pdf](https://arxiv.org/pdf/2502.06737)]
> **Authors**: Thomas Zeng,Shuibai Zhang,Shutong Wu,Christian Classen,Daewon Chae,Ethan Ewer,Minjae Lee,Heeju Kim,Wonjun Kang,Jackson Kunde,Ying Fan,Jungtaek Kim,Hyung Il Koo,Kannan Ramchandran,Dimitris Papailiopoulos,Kangwook Lee
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Process Reward Models (PRMs) have proven effective at enhancing mathematical reasoning for Large Language Models (LLMs) by leveraging increased inference-time computation. However, they are predominantly trained on mathematical data and their generalizability to non-mathematical domains has not been rigorously studied. In response, this work first shows that current PRMs have poor performance in other domains. To address this limitation, we introduce VersaPRM, a multi-domain PRM trained on synthetic reasoning data generated using our novel data generation and annotation method. VersaPRM achieves consistent performance gains across diverse domains. For instance, in the MMLU-Pro category of Law, VersaPRM via weighted majority voting, achieves a 7.9% performance gain over the majority voting baseline -- surpassing Qwen2.5-Math-PRM's gain of 1.3%. We further contribute to the community by open-sourcing all data, code and models for VersaPRM.

### Dynamic Loss-Based Sample Reweighting for Improved Large Language Model Pretraining 
[[arxiv](https://arxiv.org/abs/2502.06733)] [[cool](https://papers.cool/arxiv/2502.06733)] [[pdf](https://arxiv.org/pdf/2502.06733)]
> **Authors**: Daouda Sow,Herbert Woisetschläger,Saikiran Bulusu,Shiqiang Wang,Hans-Arno Jacobsen,Yingbin Liang
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Accepted for publication at ICLR 2025. Code base available: https://github.com/sowmaster/Sample-Level-Loss-Reweighting-ICLR-2025
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Pretraining large language models (LLMs) on vast and heterogeneous datasets is crucial for achieving state-of-the-art performance across diverse downstream tasks. However, current training paradigms treat all samples equally, overlooking the importance or relevance of individual samples throughout the training process. Existing reweighting strategies, which primarily focus on group-level data importance, fail to leverage fine-grained instance-level information and do not adapt dynamically to individual sample importance as training progresses. In this paper, we introduce novel algorithms for dynamic, instance-level data reweighting aimed at improving both the efficiency and effectiveness of LLM pretraining. Our methods adjust the weight of each training sample based on its loss value in an online fashion, allowing the model to dynamically focus on more informative or important samples at the current training stage. In particular, our framework allows us to systematically devise reweighting strategies deprioritizing redundant or uninformative data, which we find tend to work best. Furthermore, we develop a new theoretical framework for analyzing the impact of loss-based reweighting on the convergence of gradient-based optimization, providing the first formal characterization of how these strategies affect convergence bounds. We empirically validate our approach across a spectrum of tasks, from pretraining 7B and 1.4B parameter LLMs to smaller-scale language models and linear regression problems, demonstrating that our loss-based reweighting approach can lead to faster convergence and significantly improved performance.

### FlexDeMo: Decoupled Momentum Optimization for Fully and Hybrid Sharded Training 
[[arxiv](https://arxiv.org/abs/2502.06728)] [[cool](https://papers.cool/arxiv/2502.06728)] [[pdf](https://arxiv.org/pdf/2502.06728)]
> **Authors**: Mogens Henrik From,Jacob Nielsen,Lukas Galke,Peter Schneider-Kamp
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Training large neural network models requires extensive computational resources, often distributed across several nodes and accelerators. Recent findings suggest that it may be sufficient to only exchange the fast moving components of the gradients, while accumulating momentum locally (Decoupled Momentum, or DeMo). However, when considering larger models that do not fit on a single accelerate, the exchange of gradient information and the integration of DeMo needs to be reconsidered. Here, we propose employing a hybrid strategy, FlexDeMo, whereby nodes fully synchronize locally between different GPUs and inter-node communication is improved through only using the fast-moving components. This effectively combines previous hybrid sharding strategies with the advantages of decoupled momentum. Our experimental results show that FlexDeMo is on par with AdamW in terms of validation loss, demonstrating its viability.

### RSAttAE: An Information-Aware Attention-based Autoencoder Recommender System 
[[arxiv](https://arxiv.org/abs/2502.06705)] [[cool](https://papers.cool/arxiv/2502.06705)] [[pdf](https://arxiv.org/pdf/2502.06705)]
> **Authors**: Amirhossein Dadashzadeh Taromi,Sina Heydari,Mohsen Hooshmand,Majid Ramezani
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 6 pages, 4 figures
- **标题**: None
- **领域**: 机器学习,信息检索
- **Abstract**: Recommender systems play a crucial role in modern life, including information retrieval, the pharmaceutical industry, retail, and entertainment. The entertainment sector, in particular, attracts significant attention and generates substantial profits. This work proposes a new method for predicting unknown user-movie ratings to enhance customer satisfaction. To achieve this, we utilize the MovieLens 100K dataset. Our approach introduces an attention-based autoencoder to create meaningful representations and the XGBoost method for rating predictions. The results demonstrate that our proposal outperforms most of the existing state-of-the-art methods. Availability: github.com/ComputationIASBS/RecommSys

### FairDropout: Using Example-Tied Dropout to Enhance Generalization of Minority Groups 
[[arxiv](https://arxiv.org/abs/2502.06695)] [[cool](https://papers.cool/arxiv/2502.06695)] [[pdf](https://arxiv.org/pdf/2502.06695)]
> **Authors**: Geraldin Nanfack,Eugene Belilovsky
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Deep learning models frequently exploit spurious features in training data to achieve low training error, often resulting in poor generalization when faced with shifted testing distributions. To address this issue, various methods from imbalanced learning, representation learning, and classifier recalibration have been proposed to enhance the robustness of deep neural networks against spurious correlations. In this paper, we observe that models trained with empirical risk minimization tend to generalize well for examples from the majority groups while memorizing instances from minority groups. Building on recent findings that show memorization can be localized to a limited number of neurons, we apply example-tied dropout as a method we term FairDropout, aimed at redirecting this memorization to specific neurons that we subsequently drop out during inference. We empirically evaluate FairDropout using the subpopulation benchmark suite encompassing vision, language, and healthcare tasks, demonstrating that it significantly reduces reliance on spurious correlations, and outperforms state-of-the-art methods.

### Recent Advances, Applications and Open Challenges in Machine Learning for Health: Reflections from Research Roundtables at ML4H 2024 Symposium 
[[arxiv](https://arxiv.org/abs/2502.06693)] [[cool](https://papers.cool/arxiv/2502.06693)] [[pdf](https://arxiv.org/pdf/2502.06693)]
> **Authors**: Amin Adibi,Xu Cao,Zongliang Ji,Jivat Neet Kaur,Winston Chen,Elizabeth Healey,Brighton Nuwagira,Wenqian Ye,Geoffrey Woollard,Maxwell A Xu,Hejie Cui,Johnny Xi,Trenton Chang,Vasiliki Bikia,Nicole Zhang,Ayush Noori,Yuan Xia,Md. Belal Hossain,Hanna A. Frank,Alina Peluso,Yuan Pu,Shannon Zejiang Shen,John Wu,Adibvafa Fallahpour,Sazan Mahbub, et al. (17 additional authors not shown)
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机与社会
- **Abstract**: The fourth Machine Learning for Health (ML4H) symposium was held in person on December 15th and 16th, 2024, in the traditional, ancestral, and unceded territories of the Musqueam, Squamish, and Tsleil-Waututh Nations in Vancouver, British Columbia, Canada. The symposium included research roundtable sessions to foster discussions between participants and senior researchers on timely and relevant topics for the ML4H community. The organization of the research roundtables at the conference involved 13 senior and 27 junior chairs across 13 tables. Each roundtable session included an invited senior chair (with substantial experience in the field), junior chairs (responsible for facilitating the discussion), and attendees from diverse backgrounds with an interest in the session's topic.

### No Trick, No Treat: Pursuits and Challenges Towards Simulation-free Training of Neural Samplers 
[[arxiv](https://arxiv.org/abs/2502.06685)] [[cool](https://papers.cool/arxiv/2502.06685)] [[pdf](https://arxiv.org/pdf/2502.06685)]
> **Authors**: Jiajun He,Yuanqi Du,Francisco Vargas,Dinghuai Zhang,Shreyas Padhy,RuiKang OuYang,Carla Gomes,José Miguel Hernández-Lobato
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 21 pages, 5 figures, 6 tables
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: We consider the sampling problem, where the aim is to draw samples from a distribution whose density is known only up to a normalization constant. Recent breakthroughs in generative modeling to approximate a high-dimensional data distribution have sparked significant interest in developing neural network-based methods for this challenging problem. However, neural samplers typically incur heavy computational overhead due to simulating trajectories during training. This motivates the pursuit of simulation-free training procedures of neural samplers. In this work, we propose an elegant modification to previous methods, which allows simulation-free training with the help of a time-dependent normalizing flow. However, it ultimately suffers from severe mode collapse. On closer inspection, we find that nearly all successful neural samplers rely on Langevin preconditioning to avoid mode collapsing. We systematically analyze several popular methods with various objective functions and demonstrate that, in the absence of Langevin preconditioning, most of them fail to adequately cover even a simple target. Finally, we draw attention to a strong baseline by combining the state-of-the-art MCMC method, Parallel Tempering (PT), with an additional generative model to shed light on future explorations of neural samplers.

### EquiTabPFN: A Target-Permutation Equivariant Prior Fitted Networks 
[[arxiv](https://arxiv.org/abs/2502.06684)] [[cool](https://papers.cool/arxiv/2502.06684)] [[pdf](https://arxiv.org/pdf/2502.06684)]
> **Authors**: Michael Arbel,David Salinas,Frank Hutter
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Recent foundational models for tabular data, such as TabPFN, have demonstrated remarkable effectiveness in adapting to new tasks through in-context learning. However, these models overlook a crucial equivariance property: the arbitrary ordering of target dimensions should not influence model predictions. In this study, we identify this oversight as a source of incompressible error, termed the equivariance gap, which introduces instability in predictions. To mitigate these issues, we propose a novel model designed to preserve equivariance across output dimensions. Our experimental results indicate that our proposed model not only addresses these pitfalls effectively but also achieves competitive benchmark performance.

### EfficientLLM: Scalable Pruning-Aware Pretraining for Architecture-Agnostic Edge Language Models 
[[arxiv](https://arxiv.org/abs/2502.06663)] [[cool](https://papers.cool/arxiv/2502.06663)] [[pdf](https://arxiv.org/pdf/2502.06663)]
> **Authors**: Xingrun Xing,Zheng Liu,Shitao Xiao,Boyan Gao,Yiming Liang,Wanpeng Zhang,Haokun Lin,Guoqi Li,Jiajun Zhang
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Modern large language models (LLMs) driven by scaling laws, achieve intelligence emergency in large model sizes. Recently, the increasing concerns about cloud costs, latency, and privacy make it an urgent requirement to develop compact edge language models. Distinguished from direct pretraining that bounded by the scaling law, this work proposes the pruning-aware pretraining, focusing on retaining performance of much larger optimized models. It features following characteristics: 1) Data-scalable: we introduce minimal parameter groups in LLM and continuously optimize structural pruning, extending post-training pruning methods like LLM-Pruner and SparseGPT into the pretraining phase. 2) Architecture-agnostic: the LLM architecture is auto-designed using saliency-driven pruning, which is the first time to exceed SoTA human-designed LLMs in modern pretraining. We reveal that it achieves top-quality edge language models, termed EfficientLLM, by scaling up LLM compression and extending its boundary. EfficientLLM significantly outperforms SoTA baselines with $100M \sim 1B$ parameters, such as MobileLLM, SmolLM, Qwen2.5-0.5B, OLMo-1B, Llama3.2-1B in common sense benchmarks. As the first attempt, EfficientLLM bridges the performance gap between traditional LLM compression and direct pretraining methods, and we will fully open source at https://github.com/Xingrun-Xing2/EfficientLLM.

### Generating Samples to Question Trained Models 
[[arxiv](https://arxiv.org/abs/2502.06658)] [[cool](https://papers.cool/arxiv/2502.06658)] [[pdf](https://arxiv.org/pdf/2502.06658)]
> **Authors**: E. Mehmet Kıral,Nurşen Aydın,Ş. İlker Birbil
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: There is a growing need for investigating how machine learning models operate. With this work, we aim to understand trained machine learning models by questioning their data preferences. We propose a mathematical framework that allows us to probe trained models and identify their preferred samples in various scenarios including prediction-risky, parameter-sensitive, or model-contrastive samples. To showcase our framework, we pose these queries to a range of models trained on a range of classification and regression tasks, and receive answers in the form of generated data.

### Koopman-Equivariant Gaussian Processes 
[[arxiv](https://arxiv.org/abs/2502.06645)] [[cool](https://papers.cool/arxiv/2502.06645)] [[pdf](https://arxiv.org/pdf/2502.06645)]
> **Authors**: Petar Bevanda,Max Beier,Armin Lederer,Alexandre Capone,Stefan Sosnowski,Sandra Hirche
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Accepted to the 28th International Conference on Artificial Intelligence and Statistics (AISTATS)
- **标题**: None
- **领域**: 机器学习,系统与控制,机器学习
- **Abstract**: Credible forecasting and representation learning of dynamical systems are of ever-increasing importance for reliable decision-making. To that end, we propose a family of Gaussian processes (GP) for dynamical systems with linear time-invariant responses, which are nonlinear only in initial conditions. This linearity allows us to tractably quantify forecasting and representational uncertainty, simultaneously alleviating the challenge of computing the distribution of trajectories from a GP-based dynamical system and enabling a new probabilistic treatment of learning Koopman operator representations. Using a trajectory-based equivariance -- which we refer to as \textit{Koopman equivariance} -- we obtain a GP model with enhanced generalization capabilities. To allow for large-scale regression, we equip our framework with variational inference based on suitable inducing points. Experiments demonstrate on-par and often better forecasting performance compared to kernel-based methods for learning dynamical systems.

### MoETuner: Optimized Mixture of Expert Serving with Balanced Expert Placement and Token Routing 
[[arxiv](https://arxiv.org/abs/2502.06643)] [[cool](https://papers.cool/arxiv/2502.06643)] [[pdf](https://arxiv.org/pdf/2502.06643)]
> **Authors**: Seokjin Go,Divya Mahajan
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,分布式、并行和集群计算
- **Abstract**: Mixture-of-Experts (MoE) model architecture has emerged as a promising solution for scaling transformer models efficiently, offering sparse activation that reduces computational costs while increasing model capacity. However, as MoE models scale, they need to be distributed across GPU devices, thus face critical performance bottlenecks due to their large memory footprint. Expert parallelism distributes experts across GPUs, however, faces key challenges including an unbalanced token routing and expert activation, resulting in communication tail latency and processing inefficiencies. While existing solutions address some of these issues, they fail to resolve the dual challenges of load imbalance and communication skew. The imbalance in token processing load across experts causes uneven processing times on different GPUs, while communication skew between GPUs leads to unbalanced inter-GPU data transfers. These factors degrade the performance of MoE models by increasing tail latency and reducing overall throughput. To address these limitations, we propose an Integer Linear Programming (ILP) formulation to optimize expert placement by jointly considering token load, communication, and computation costs. We exploit the property that there is a token routing dependency across layers, where tokens routed to a specific expert in one layer are likely to be routed to a limited set of experts in the subsequent layer. Our solution, MoETuner, offers an optimal expert-to-GPU assignment that minimizes inter-GPU token routing costs and balances token processing across devices, thereby reducing tail latency and end-to-end execution time. Experimental results demonstrate 9.3% and 17.5% of end-to-end speedups for single-node and multi-node inference respectively, showcasing the potential of our ILP-based optimization for offering expert parallel solutions for next-generation MoEs.

### Automatic Annotation Augmentation Boosts Translation between Molecules and Natural Language 
[[arxiv](https://arxiv.org/abs/2502.06634)] [[cool](https://papers.cool/arxiv/2502.06634)] [[pdf](https://arxiv.org/pdf/2502.06634)]
> **Authors**: Zhiqiang Zhong,Simon Sataa-Yu Larsen,Haoyu Guo,Tao Tang,Kuangyu Zhou,Davide Mottin
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,生物分子
- **Abstract**: Recent advancements in AI for biological research focus on integrating molecular data with natural language to accelerate drug discovery. However, the scarcity of high-quality annotations limits progress in this area. This paper introduces LA$^3$, a Language-based Automatic Annotation Augmentation framework that leverages large language models to augment existing datasets, thereby improving AI training. We demonstrate the effectiveness of LA$^3$ by creating an enhanced dataset, LaChEBI-20, where we systematically rewrite the annotations of molecules from an established dataset. These rewritten annotations preserve essential molecular information while providing more varied sentence structures and vocabulary. Using LaChEBI-20, we train LaMolT5 based on a benchmark architecture to learn the mapping between molecular representations and augmented annotations. Experimental results on text-based *de novo* molecule generation and molecule captioning demonstrate that LaMolT5 outperforms state-of-the-art models. Notably, incorporating LA$^3$ leads to improvements of up to 301% over the benchmark architecture. Furthermore, we validate the effectiveness of LA$^3$ notable applications in *image*, *text* and *graph* tasks, affirming its versatility and utility.

### Amortized In-Context Bayesian Posterior Estimation 
[[arxiv](https://arxiv.org/abs/2502.06601)] [[cool](https://papers.cool/arxiv/2502.06601)] [[pdf](https://arxiv.org/pdf/2502.06601)]
> **Authors**: Sarthak Mittal,Niels Leif Bracher,Guillaume Lajoie,Priyank Jaini,Marcus Brubaker
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Bayesian inference provides a natural way of incorporating prior beliefs and assigning a probability measure to the space of hypotheses. Current solutions rely on iterative routines like Markov Chain Monte Carlo (MCMC) sampling and Variational Inference (VI), which need to be re-run whenever new observations are available. Amortization, through conditional estimation, is a viable strategy to alleviate such difficulties and has been the guiding principle behind simulation-based inference, neural processes and in-context methods using pre-trained models. In this work, we conduct a thorough comparative analysis of amortized in-context Bayesian posterior estimation methods from the lens of different optimization objectives and architectural choices. Such methods train an amortized estimator to perform posterior parameter inference by conditioning on a set of data examples passed as context to a sequence model such as a transformer. In contrast to language models, we leverage permutation invariant architectures as the true posterior is invariant to the ordering of context examples. Our empirical study includes generalization to out-of-distribution tasks, cases where the assumed underlying model is misspecified, and transfer from simulated to real problems. Subsequently, it highlights the superiority of the reverse KL estimator for predictive problems, especially when combined with the transformer architecture and normalizing flows.

### Continual Release Moment Estimation with Differential Privacy 
[[arxiv](https://arxiv.org/abs/2502.06597)] [[cool](https://papers.cool/arxiv/2502.06597)] [[pdf](https://arxiv.org/pdf/2502.06597)]
> **Authors**: Nikita P. Kalinin,Jalaj Upadhyay,Christoph H. Lampert
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: We propose Joint Moment Estimation (JME), a method for continually and privately estimating both the first and second moments of data with reduced noise compared to naive approaches. JME uses the matrix mechanism and a joint sensitivity analysis to allow the second moment estimation with no additional privacy cost, thereby improving accuracy while maintaining privacy. We demonstrate JME's effectiveness in two applications: estimating the running mean and covariance matrix for Gaussian density estimation, and model training with DP-Adam on CIFAR-10.

### Diffeomorphic Temporal Alignment Nets for Time-series Joint Alignment and Averaging 
[[arxiv](https://arxiv.org/abs/2502.06591)] [[cool](https://papers.cool/arxiv/2502.06591)] [[pdf](https://arxiv.org/pdf/2502.06591)]
> **Authors**: Ron Shapira Weber,Oren Freifeld
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: This manuscript covers and extends the papers: Diffeomorphic Temporal Alignment Nets (DTAN; NeruIPS 2019) and Regularization-free Diffeomorphic Temporal Alignment Nets (ICML 2023). Additional contributions: Multi-tasking DTAN, PCA-DTAN and more
- **标题**: None
- **领域**: 机器学习
- **Abstract**: In time-series analysis, nonlinear temporal misalignment remains a pivotal challenge that forestalls even simple averaging. Since its introduction, the Diffeomorphic Temporal Alignment Net (DTAN), which we first introduced (Weber et al., 2019) and further developed in (Weber & Freifeld, 2023), has proven itself as an effective solution for this problem (these conference papers are earlier partial versions of the current manuscript). DTAN predicts and applies diffeomorphic transformations in an input-dependent manner, thus facilitating the joint alignment (JA) and averaging of time-series ensembles in an unsupervised or a weakly-supervised manner. The inherent challenges of the weakly/unsupervised setting, particularly the risk of trivial solutions through excessive signal distortion, are mitigated using either one of two distinct strategies: 1) a regularization term for warps; 2) using the Inverse Consistency Averaging Error (ICAE). The latter is a novel, regularization-free approach which also facilitates the JA of variable-length signals. We also further extend our framework to incorporate multi-task learning (MT-DTAN), enabling simultaneous time-series alignment and classification. Additionally, we conduct a comprehensive evaluation of different backbone architectures, demonstrating their efficacy in time-series alignment tasks. Finally, we showcase the utility of our approach in enabling Principal Component Analysis (PCA) for misaligned time-series data. Extensive experiments across 128 UCR datasets validate the superiority of our approach over contemporary averaging methods, including both traditional and learning-based approaches, marking a significant advancement in the field of time-series analysis.

### Deep Reinforcement Learning based Triggering Function for Early Classifiers of Time Series 
[[arxiv](https://arxiv.org/abs/2502.06584)] [[cool](https://papers.cool/arxiv/2502.06584)] [[pdf](https://arxiv.org/pdf/2502.06584)]
> **Authors**: Aurélien Renault,Alexis Bondu,Antoine Cornuéjols,Vincent Lemaire
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Early Classification of Time Series (ECTS) has been recognized as an important problem in many areas where decisions have to be taken as soon as possible, before the full data availability, while time pressure increases. Numerous ECTS approaches have been proposed, based on different triggering functions, each taking into account various pieces of information related to the incoming time series and/or the output of a classifier. Although their performances have been empirically compared in the literature, no studies have been carried out on the optimality of these triggering functions that involve ``man-tailored'' decision rules. Based on the same information, could there be better triggering functions? This paper presents one way to investigate this question by showing first how to translate ECTS problems into Reinforcement Learning (RL) ones, where the very same information is used in the state space. A thorough comparison of the performance obtained by ``handmade'' approaches and their ``RL-based'' counterparts has been carried out. A second question investigated in this paper is whether a different combination of information, defining the state space in RL systems, can achieve even better performance. Experiments show that the system we describe, called \textsc{Alert}, significantly outperforms its state-of-the-art competitors on a large number of datasets.

### The Minimal Search Space for Conditional Causal Bandits 
[[arxiv](https://arxiv.org/abs/2502.06577)] [[cool](https://papers.cool/arxiv/2502.06577)] [[pdf](https://arxiv.org/pdf/2502.06577)]
> **Authors**: Francisco N. F. Q. Simoes,Itai Feigenbaum,Mehdi Dastani,Thijs van Ommen
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Submitted to ICML2025
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Causal knowledge can be used to support decision-making problems. This has been recognized in the causal bandits literature, where a causal (multi-armed) bandit is characterized by a causal graphical model and a target variable. The arms are then interventions on the causal model, and rewards are samples of the target variable. Causal bandits were originally studied with a focus on hard interventions. We focus instead on cases where the arms are conditional interventions, which more accurately model many real-world decision-making problems by allowing the value of the intervened variable to be chosen based on the observed values of other variables. This paper presents a graphical characterization of the minimal set of nodes guaranteed to contain the optimal conditional intervention, which maximizes the expected reward. We then propose an efficient algorithm with a time complexity of $O(|V| + |E|)$ to identify this minimal set of nodes. We prove that the graphical characterization and the proposed algorithm are correct. Finally, we empirically demonstrate that our algorithm significantly prunes the search space and substantially accelerates convergence rates when integrated into standard multi-armed bandit algorithms.

### Is API Access to LLMs Useful for Generating Private Synthetic Tabular Data? 
[[arxiv](https://arxiv.org/abs/2502.06555)] [[cool](https://papers.cool/arxiv/2502.06555)] [[pdf](https://arxiv.org/pdf/2502.06555)]
> **Authors**: Marika Swanberg,Ryan McKenna,Edo Roth,Albert Cheu,Peter Kairouz
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,密码学和安全
- **Abstract**: Differentially private (DP) synthetic data is a versatile tool for enabling the analysis of private data. Recent advancements in large language models (LLMs) have inspired a number of algorithm techniques for improving DP synthetic data generation. One family of approaches uses DP finetuning on the foundation model weights; however, the model weights for state-of-the-art models may not be public. In this work we propose two DP synthetic tabular data algorithms that only require API access to the foundation model. We adapt the Private Evolution algorithm (Lin et al., 2023; Xie et al., 2024) -- which was designed for image and text data -- to the tabular data domain. In our extension of Private Evolution, we define a query workload-based distance measure, which may be of independent interest. We propose a family of algorithms that use one-shot API access to LLMs, rather than adaptive queries to the LLM. Our findings reveal that API-access to powerful LLMs does not always improve the quality of DP synthetic data compared to established baselines that operate without such access. We provide insights into the underlying reasons and propose improvements to LLMs that could make them more effective for this application.

### Dimension-free Regret for Learning Asymmetric Linear Dynamical Systems 
[[arxiv](https://arxiv.org/abs/2502.06545)] [[cool](https://papers.cool/arxiv/2502.06545)] [[pdf](https://arxiv.org/pdf/2502.06545)]
> **Authors**: Annie Marsden,Elad Hazan
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 19 pages
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Previously, methods for learning marginally stable linear dynamical systems either required the transition matrix to be symmetric or incurred regret bounds that scale polynomially with the system's hidden dimension. In this work, we introduce a novel method that overcomes this trade-off, achieving dimension-free regret despite the presence of asymmetric matrices and marginal stability. Our method combines spectral filtering with linear predictors and employs Chebyshev polynomials in the complex plane to construct a novel spectral filtering basis. This construction guarantees sublinear regret in an online learning framework, without relying on any statistical or generative assumptions. Specifically, we prove that as long as the transition matrix has eigenvalues with complex component bounded by $1/\mathrm{poly} \log T$, then our method achieves regret $\tilde{O}(T^{9/10})$ when compared to the best linear dynamical predictor in hindsight.

### Sequence Transferability and Task Order Selection in Continual Learning 
[[arxiv](https://arxiv.org/abs/2502.06544)] [[cool](https://papers.cool/arxiv/2502.06544)] [[pdf](https://arxiv.org/pdf/2502.06544)]
> **Authors**: Thinh Nguyen,Cuong N. Nguyen,Quang Pham,Binh T. Nguyen,Savitha Ramasamy,Xiaoli Li,Cuong V. Nguyen
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 10 pages, 5 figures
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: In continual learning, understanding the properties of task sequences and their relationships to model performance is important for developing advanced algorithms with better accuracy. However, efforts in this direction remain underdeveloped despite encouraging progress in methodology development. In this work, we investigate the impacts of sequence transferability on continual learning and propose two novel measures that capture the total transferability of a task sequence, either in the forward or backward direction. Based on the empirical properties of these measures, we then develop a new method for the task order selection problem in continual learning. Our method can be shown to offer a better performance than the conventional strategy of random task selection.

### Boost-and-Skip: A Simple Guidance-Free Diffusion for Minority Generation 
[[arxiv](https://arxiv.org/abs/2502.06516)] [[cool](https://papers.cool/arxiv/2502.06516)] [[pdf](https://arxiv.org/pdf/2502.06516)]
> **Authors**: Soobin Um,Beomsu Kim,Jong Chul Ye
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 29 pages, 11 figures
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别,机器学习
- **Abstract**: Minority samples are underrepresented instances located in low-density regions of a data manifold, and are valuable in many generative AI applications, such as data augmentation, creative content generation, etc. Unfortunately, existing diffusion-based minority generators often rely on computationally expensive guidance dedicated for minority generation. To address this, here we present a simple yet powerful guidance-free approach called Boost-and-Skip for generating minority samples using diffusion models. The key advantage of our framework requires only two minimal changes to standard generative processes: (i) variance-boosted initialization and (ii) timestep skipping. We highlight that these seemingly-trivial modifications are supported by solid theoretical and empirical evidence, thereby effectively promoting emergence of underrepresented minority features. Our comprehensive experiments demonstrate that Boost-and-Skip greatly enhances the capability of generating minority samples, even rivaling guidance-based state-of-the-art approaches while requiring significantly fewer computations.

### Model-Based Offline Reinforcement Learning with Reliability-Guaranteed Sequence Modeling 
[[arxiv](https://arxiv.org/abs/2502.06491)] [[cool](https://papers.cool/arxiv/2502.06491)] [[pdf](https://arxiv.org/pdf/2502.06491)]
> **Authors**: Shenghong He
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Model-based offline reinforcement learning (MORL) aims to learn a policy by exploiting a dynamics model derived from an existing dataset. Applying conservative quantification to the dynamics model, most existing works on MORL generate trajectories that approximate the real data distribution to facilitate policy learning by using current information (e.g., the state and action at time step $t$). However, these works neglect the impact of historical information on environmental dynamics, leading to the generation of unreliable trajectories that may not align with the real data distribution. In this paper, we propose a new MORL algorithm \textbf{R}eliability-guaranteed \textbf{T}ransformer (RT), which can eliminate unreliable trajectories by calculating the cumulative reliability of the generated trajectory (i.e., using a weighted variational distance away from the real data). Moreover, by sampling candidate actions with high rewards, RT can efficiently generate high-return trajectories from the existing offline data. We theoretically prove the performance guarantees of RT in policy learning, and empirically demonstrate its effectiveness against state-of-the-art model-based methods on several benchmark tasks.

### Logarithmic Regret of Exploration in Average Reward Markov Decision Processes 
[[arxiv](https://arxiv.org/abs/2502.06480)] [[cool](https://papers.cool/arxiv/2502.06480)] [[pdf](https://arxiv.org/pdf/2502.06480)]
> **Authors**: Victor Boone,Bruno Gaujal
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: In average reward Markov decision processes, state-of-the-art algorithms for regret minimization follow a well-established framework: They are model-based, optimistic and episodic. First, they maintain a confidence region from which optimistic policies are computed using a well-known subroutine called Extended Value Iteration (EVI). Second, these policies are used over time windows called episodes, each ended by the Doubling Trick (DT) rule or a variant thereof. In this work, without modifying EVI, we show that there is a significant advantage in replacing (DT) by another simple rule, that we call the Vanishing Multiplicative (VM) rule. When managing episodes with (VM), the algorithm's regret is, both in theory and in practice, as good if not better than with (DT), while the one-shot behavior is greatly improved. More specifically, the management of bad episodes (when sub-optimal policies are being used) is much better under (VM) than (DT) by making the regret of exploration logarithmic rather than linear. These results are made possible by a new in-depth understanding of the contrasting behaviors of confidence regions during good and bad episodes.

### MATH-Perturb: Benchmarking LLMs' Math Reasoning Abilities against Hard Perturbations 
[[arxiv](https://arxiv.org/abs/2502.06453)] [[cool](https://papers.cool/arxiv/2502.06453)] [[pdf](https://arxiv.org/pdf/2502.06453)]
> **Authors**: Kaixuan Huang,Jiacheng Guo,Zihao Li,Xiang Ji,Jiawei Ge,Wenzhe Li,Yingqing Guo,Tianle Cai,Hui Yuan,Runzhe Wang,Yue Wu,Ming Yin,Shange Tang,Yangsibo Huang,Chi Jin,Xinyun Chen,Chiyuan Zhang,Mengdi Wang
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: v2: fix bugs in Fig. 1
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Large language models have demonstrated impressive performance on challenging mathematical reasoning tasks, which has triggered the discussion of whether the performance is achieved by true reasoning capability or memorization. To investigate this question, prior work has constructed mathematical benchmarks when questions undergo simple perturbations -- modifications that still preserve the underlying reasoning patterns of the solutions. However, no work has explored hard perturbations, which fundamentally change the nature of the problem so that the original solution steps do not apply. To bridge the gap, we construct MATH-P-Simple and MATH-P-Hard via simple perturbation and hard perturbation, respectively. Each consists of 279 perturbed math problems derived from level-5 (hardest) problems in the MATH dataset (Hendrycksmath et. al., 2021). We observe significant performance drops on MATH-P-Hard across various models, including o1-mini (-16.49%) and gemini-2.0-flash-thinking (-12.9%). We also raise concerns about a novel form of memorization where models blindly apply learned problem-solving skills without assessing their applicability to modified contexts. This issue is amplified when using original problems for in-context learning. We call for research efforts to address this challenge, which is critical for developing more robust and reliable reasoning models.

### Low-dimensional Functions are Efficiently Learnable under Randomly Biased Distributions 
[[arxiv](https://arxiv.org/abs/2502.06443)] [[cool](https://papers.cool/arxiv/2502.06443)] [[pdf](https://arxiv.org/pdf/2502.06443)]
> **Authors**: Elisabetta Cornacchia,Dan Mikulincer,Elchanan Mossel
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: The problem of learning single index and multi index models has gained significant interest as a fundamental task in high-dimensional statistics. Many recent works have analysed gradient-based methods, particularly in the setting of isotropic data distributions, often in the context of neural network training. Such studies have uncovered precise characterisations of algorithmic sample complexity in terms of certain analytic properties of the target function, such as the leap, information, and generative exponents. These properties establish a quantitative separation between low and high complexity learning tasks. In this work, we show that high complexity cases are rare. Specifically, we prove that introducing a small random perturbation to the data distribution--via a random shift in the first moment--renders any Gaussian single index model as easy to learn as a linear function. We further extend this result to a class of multi index models, namely sparse Boolean functions, also known as Juntas.

### FEMBA: Efficient and Scalable EEG Analysis with a Bidirectional Mamba Foundation Model 
[[arxiv](https://arxiv.org/abs/2502.06438)] [[cool](https://papers.cool/arxiv/2502.06438)] [[pdf](https://arxiv.org/pdf/2502.06438)]
> **Authors**: Anna Tegon,Thorir Mar Ingolfsson,Xiaying Wang,Luca Benini,Yawei Li
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 7 pages, 3 figures, 5 tables, pre-print
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Accurate and efficient electroencephalography (EEG) analysis is essential for detecting seizures and artifacts in long-term monitoring, with applications spanning hospital diagnostics to wearable health devices. Robust EEG analytics have the potential to greatly improve patient care. However, traditional deep learning models, especially Transformer-based architectures, are hindered by their quadratic time and memory complexity, making them less suitable for resource-constrained environments. To address these challenges, we present FEMBA (Foundational EEG Mamba + Bidirectional Architecture), a novel self-supervised framework that establishes new efficiency benchmarks for EEG analysis through bidirectional state-space modeling. Unlike Transformer-based models, which incur quadratic time and memory complexity, FEMBA scales linearly with sequence length, enabling more scalable and efficient processing of extended EEG recordings. Trained on over 21,000 hours of unlabeled EEG and fine-tuned on three downstream tasks, FEMBA achieves competitive performance in comparison with transformer models, with significantly lower computational cost. Specifically, it reaches 81.82% balanced accuracy (0.8921 AUROC) on TUAB and 0.949 AUROC on TUAR, while a tiny 7.8M-parameter variant demonstrates viability for resource-constrained devices. These results pave the way for scalable, general-purpose EEG analytics in both clinical and highlight FEMBA as a promising candidate for wearable applications.

### CS-SHAP: Extending SHAP to Cyclic-Spectral Domain for Better Interpretability of Intelligent Fault Diagnosis 
[[arxiv](https://arxiv.org/abs/2502.06424)] [[cool](https://papers.cool/arxiv/2502.06424)] [[pdf](https://arxiv.org/pdf/2502.06424)]
> **Authors**: Qian Chen,Xingjian Dong,Kui Hu,Kangkang Chen,Zhike Peng,Guang Meng
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 21 pages, 21 figures
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Neural networks (NNs), with their powerful nonlinear mapping and end-to-end capabilities, are widely applied in mechanical intelligent fault diagnosis (IFD). However, as typical black-box models, they pose challenges in understanding their decision basis and logic, limiting their deployment in high-reliability scenarios. Hence, various methods have been proposed to enhance the interpretability of IFD. Among these, post-hoc approaches can provide explanations without changing model architecture, preserving its flexibility and scalability. However, existing post-hoc methods often suffer from limitations in explanation forms. They either require preprocessing that disrupts the end-to-end nature or overlook fault mechanisms, leading to suboptimal explanations. To address these issues, we derived the cyclic-spectral (CS) transform and proposed the CS-SHAP by extending Shapley additive explanations (SHAP) to the CS domain. CS-SHAP can evaluate contributions from both carrier and modulation frequencies, aligning more closely with fault mechanisms and delivering clearer and more accurate explanations. Three datasets are utilized to validate the superior interpretability of CS-SHAP, ensuring its correctness, reproducibility, and practical performance. With open-source code and outstanding interpretability, CS-SHAP has the potential to be widely adopted and become the post-hoc interpretability benchmark in IFD, even in other classification tasks. The code is available on https://github.com/ChenQian0618/CS-SHAP.

### An Automated Machine Learning Framework for Surgical Suturing Action Detection under Class Imbalance 
[[arxiv](https://arxiv.org/abs/2502.06407)] [[cool](https://papers.cool/arxiv/2502.06407)] [[pdf](https://arxiv.org/pdf/2502.06407)]
> **Authors**: Baobing Zhang,Paul Sullivan,Benjie Tang,Ghulam Nabi,Mustafa Suphi Erden
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器人技术
- **Abstract**: In laparoscopy surgical training and evaluation, real-time detection of surgical actions with interpretable outputs is crucial for automated and real-time instructional feedback and skill development. Such capability would enable development of machine guided training systems. This paper presents a rapid deployment approach utilizing automated machine learning methods, based on surgical action data collected from both experienced and trainee surgeons. The proposed approach effectively tackles the challenge of highly imbalanced class distributions, ensuring robust predictions across varying skill levels of surgeons. Additionally, our method partially incorporates model transparency, addressing the reliability requirements in medical applications. Compared to deep learning approaches, traditional machine learning models not only facilitate efficient rapid deployment but also offer significant advantages in interpretability. Through experiments, this study demonstrates the potential of this approach to provide quick, reliable and effective real-time detection in surgical training environments

### The AI off-switch problem as a signalling game: bounded rationality and incomparability 
[[arxiv](https://arxiv.org/abs/2502.06403)] [[cool](https://papers.cool/arxiv/2502.06403)] [[pdf](https://arxiv.org/pdf/2502.06403)]
> **Authors**: Alessio benavoli,Alessandro facchini,Marco Zaffalon
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The off-switch problem is a critical challenge in AI control: if an AI system resists being switched off, it poses a significant risk. In this paper, we model the off-switch problem as a signalling game, where a human decision-maker communicates its preferences about some underlying decision problem to an AI agent, which then selects actions to maximise the human's utility. We assume that the human is a bounded rational agent and explore various bounded rationality mechanisms. Using real machine learning models, we reprove prior results and demonstrate that a necessary condition for an AI system to refrain from disabling its off-switch is its uncertainty about the human's utility. We also analyse how message costs influence optimal strategies and extend the analysis to scenarios involving incomparability.

### Habitizing Diffusion Planning for Efficient and Effective Decision Making 
[[arxiv](https://arxiv.org/abs/2502.06401)] [[cool](https://papers.cool/arxiv/2502.06401)] [[pdf](https://arxiv.org/pdf/2502.06401)]
> **Authors**: Haofei Lu,Yifei Shen,Dongsheng Li,Junliang Xing,Dongqi Han
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Diffusion models have shown great promise in decision-making, also known as diffusion planning. However, the slow inference speeds limit their potential for broader real-world applications. Here, we introduce Habi, a general framework that transforms powerful but slow diffusion planning models into fast decision-making models, which mimics the cognitive process in the brain that costly goal-directed behavior gradually transitions to efficient habitual behavior with repetitive practice. Even using a laptop CPU, the habitized model can achieve an average 800+ Hz decision-making frequency (faster than previous diffusion planners by orders of magnitude) on standard offline reinforcement learning benchmarks D4RL, while maintaining comparable or even higher performance compared to its corresponding diffusion planner. Our work proposes a fresh perspective of leveraging powerful diffusion models for real-world decision-making tasks. We also provide robust evaluations and analysis, offering insights from both biological and engineering perspectives for efficient and effective decision-making.

### Learning Counterfactual Outcomes Under Rank Preservation 
[[arxiv](https://arxiv.org/abs/2502.06398)] [[cool](https://papers.cool/arxiv/2502.06398)] [[pdf](https://arxiv.org/pdf/2502.06398)]
> **Authors**: Peng Wu,Haoxuan Li,Chunyuan Zheng,Yan Zeng,Jiawei Chen,Yang Liu,Ruocheng Guo,Kun Zhang
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Counterfactual inference aims to estimate the counterfactual outcome at the individual level given knowledge of an observed treatment and the factual outcome, with broad applications in fields such as epidemiology, econometrics, and management science. Previous methods rely on a known structural causal model (SCM) or assume the homogeneity of the exogenous variable and strict monotonicity between the outcome and exogenous variable. In this paper, we propose a principled approach for identifying and estimating the counterfactual outcome. We first introduce a simple and intuitive rank preservation assumption to identify the counterfactual outcome without relying on a known structural causal model. Building on this, we propose a novel ideal loss for theoretically unbiased learning of the counterfactual outcome and further develop a kernel-based estimator for its empirical estimation. Our theoretical analysis shows that the rank preservation assumption is not stronger than the homogeneity and strict monotonicity assumptions, and shows that the proposed ideal loss is convex, and the proposed estimator is unbiased. Extensive semi-synthetic and real-world experiments are conducted to demonstrate the effectiveness of the proposed method.

### How Humans Help LLMs: Assessing and Incentivizing Human Preference Annotators 
[[arxiv](https://arxiv.org/abs/2502.06387)] [[cool](https://papers.cool/arxiv/2502.06387)] [[pdf](https://arxiv.org/pdf/2502.06387)]
> **Authors**: Shang Liu,Hanzhao Wang,Zhongyao Ma,Xiaocheng Li
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机科学与博弈论,理论经济学
- **Abstract**: Human-annotated preference data play an important role in aligning large language models (LLMs). In this paper, we investigate the questions of assessing the performance of human annotators and incentivizing them to provide high-quality annotations. The quality assessment of language/text annotation faces two challenges: (i) the intrinsic heterogeneity among annotators, which prevents the classic methods that assume the underlying existence of a true label; and (ii) the unclear relationship between the annotation quality and the performance of downstream tasks, which excludes the possibility of inferring the annotators' behavior based on the model performance trained from the annotation data. Then we formulate a principal-agent model to characterize the behaviors of and the interactions between the company and the human annotators. The model rationalizes a practical mechanism of a bonus scheme to incentivize annotators which benefits both parties and it underscores the importance of the joint presence of an assessment system and a proper contract scheme. From a technical perspective, our analysis extends the existing literature on the principal-agent model by considering a continuous action space for the agent. We show the gap between the first-best and the second-best solutions (under the continuous action space) is of $Θ(1/\sqrt{n \log n})$ for the binary contracts and $Θ(1/n)$ for the linear contracts, where $n$ is the number of samples used for performance assessment; this contrasts with the known result of $\exp(-Θ(n))$ for the binary contracts when the action space is discrete. Throughout the paper, we use real preference annotation data to accompany our discussions.

### Structure-preserving contrastive learning for spatial time series 
[[arxiv](https://arxiv.org/abs/2502.06380)] [[cool](https://papers.cool/arxiv/2502.06380)] [[pdf](https://arxiv.org/pdf/2502.06380)]
> **Authors**: Yiru Jiao,Sander van Cranenburgh,Simeon Calvert,Hans van Lint
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: TL;DR: Preserving certain structures of similarity relations in spatio-temporal data can improve downstream task performance via contrastivelearning
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: Informative representations enhance model performance and generalisability in downstream tasks. However, learning self-supervised representations for spatially characterised time series, like traffic interactions, poses challenges as it requires maintaining fine-grained similarity relations in the latent space. In this study, we incorporate two structure-preserving regularisers for the contrastive learning of spatial time series: one regulariser preserves the topology of similarities between instances, and the other preserves the graph geometry of similarities across spatial and temporal dimensions. To balance contrastive learning and structure preservation, we propose a dynamic mechanism that adaptively weighs the trade-off and stabilises training. We conduct experiments on multivariate time series classification, as well as macroscopic and microscopic traffic prediction. For all three tasks, our approach preserves the structures of similarity relations more effectively and improves state-of-the-art task performances. The proposed approach can be applied to an arbitrary encoder and is particularly beneficial for time series with spatial or geographical features. Furthermore, this study suggests that higher similarity structure preservation indicates more informative and useful representations. This may help to understand the contribution of representation learning in pattern recognition with neural networks. Our code is made openly accessible with all resulting data at https://github.com/yiru-jiao/spclt.

### Solving Linear-Gaussian Bayesian Inverse Problems with Decoupled Diffusion Sequential Monte Carlo 
[[arxiv](https://arxiv.org/abs/2502.06379)] [[cool](https://papers.cool/arxiv/2502.06379)] [[pdf](https://arxiv.org/pdf/2502.06379)]
> **Authors**: Filip Ekström Kelvinius,Zheng Zhao,Fredrik Lindsten
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: A recent line of research has exploited pre-trained generative diffusion models as priors for solving Bayesian inverse problems. We contribute to this research direction by designing a sequential Monte Carlo method for linear-Gaussian inverse problems which builds on ``decoupled diffusion", where the generative process is designed such that larger updates to the sample are possible. The method is asymptotically exact and we demonstrate the effectiveness of our Decoupled Diffusion Sequential Monte Carlo (DDSMC) algorithm on both synthetic data and image reconstruction tasks. Further, we demonstrate how the approach can be extended to discrete data.

### Many-Task Federated Fine-Tuning via Unified Task Vectors 
[[arxiv](https://arxiv.org/abs/2502.06376)] [[cool](https://papers.cool/arxiv/2502.06376)] [[pdf](https://arxiv.org/pdf/2502.06376)]
> **Authors**: Vasileios Tsouvalas,Tanir Ozcelebi,Nirvana Meratnia
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 10 pages, 6 figures, submitted in IJCAI 2025
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: Federated Learning (FL) traditionally assumes homogeneous client tasks; however, in real-world scenarios, clients often specialize in diverse tasks, introducing task heterogeneity. To address this challenge, Many-Task FL (MaT-FL) has emerged, enabling clients to collaborate effectively despite task diversity. Existing MaT-FL approaches rely on client grouping or personalized layers, requiring the server to manage individual models and failing to account for clients handling multiple tasks. We propose MaTU, a MaT-FL approach that enables joint learning of task vectors across clients, eliminating the need for clustering or client-specific weight storage at the server. Our method introduces a novel aggregation mechanism that determines task similarity based on the direction of clients task vectors and constructs a unified task vector encapsulating all tasks. To address task-specific requirements, we augment the unified task vector with lightweight modulators that facilitate knowledge transfer among related tasks while disentangling dissimilar ones. Evaluated across 30 datasets, MaTU achieves superior performance over state-of-the-art MaT-FL approaches, with results comparable to per-task fine-tuning, while delivering significant communication savings.

### Hyperparameters in Score-Based Membership Inference Attacks 
[[arxiv](https://arxiv.org/abs/2502.06374)] [[cool](https://papers.cool/arxiv/2502.06374)] [[pdf](https://arxiv.org/pdf/2502.06374)]
> **Authors**: Gauri Pradhan,Joonas Jälkö,Marlon Tobaben,Antti Honkela
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: This work has been accepted for publication in the 3rd IEEE Conference on Secure and TrustworthyMachineLearning(SaTML'25). The final version will be available on IEEE Xplore
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Membership Inference Attacks (MIAs) have emerged as a valuable framework for evaluating privacy leakage by machine learning models. Score-based MIAs are distinguished, in particular, by their ability to exploit the confidence scores that the model generates for particular inputs. Existing score-based MIAs implicitly assume that the adversary has access to the target model's hyperparameters, which can be used to train the shadow models for the attack. In this work, we demonstrate that the knowledge of target hyperparameters is not a prerequisite for MIA in the transfer learning setting. Based on this, we propose a novel approach to select the hyperparameters for training the shadow models for MIA when the attacker has no prior knowledge about them by matching the output distributions of target and shadow models. We demonstrate that using the new approach yields hyperparameters that lead to an attack near indistinguishable in performance from an attack that uses target hyperparameters to train the shadow models. Furthermore, we study the empirical privacy risk of unaccounted use of training data for hyperparameter optimization (HPO) in differentially private (DP) transfer learning. We find no statistically significant evidence that performing HPO using training data would increase vulnerability to MIA.

### Improved Regret Analysis in Gaussian Process Bandits: Optimality for Noiseless Reward, RKHS norm, and Non-Stationary Variance 
[[arxiv](https://arxiv.org/abs/2502.06363)] [[cool](https://papers.cool/arxiv/2502.06363)] [[pdf](https://arxiv.org/pdf/2502.06363)]
> **Authors**: Shogo Iwazaki,Shion Takeno
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 35 pages
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: We study the Gaussian process (GP) bandit problem, whose goal is to minimize regret under an unknown reward function lying in some reproducing kernel Hilbert space (RKHS). The maximum posterior variance analysis is vital in analyzing near-optimal GP bandit algorithms such as maximum variance reduction (MVR) and phased elimination (PE). Therefore, we first show the new upper bound of the maximum posterior variance, which improves the dependence of the noise variance parameters of the GP. By leveraging this result, we refine the MVR and PE to obtain (i) a nearly optimal regret upper bound in the noiseless setting and (ii) regret upper bounds that are optimal with respect to the RKHS norm of the reward function. Furthermore, as another application of our proposed bound, we analyze the GP bandit under the time-varying noise variance setting, which is the kernelized extension of the linear bandit with heteroscedastic noise. For this problem, we show that MVR and PE-based algorithms achieve noise variance-dependent regret upper bounds, which matches our regret lower bound.

### Towards bandit-based prompt-tuning for in-the-wild foundation agents 
[[arxiv](https://arxiv.org/abs/2502.06358)] [[cool](https://papers.cool/arxiv/2502.06358)] [[pdf](https://arxiv.org/pdf/2502.06358)]
> **Authors**: Finn Rietz,Oleg Smirnov,Sara Karimi,Lele Cao
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Prompting has emerged as the dominant paradigm for adapting large, pre-trained transformer-based models to downstream tasks. The Prompting Decision Transformer (PDT) enables large-scale, multi-task offline reinforcement learning pre-training by leveraging stochastic trajectory prompts to identify the target task. However, these prompts are sampled uniformly from expert demonstrations, overlooking a critical limitation: Not all prompts are equally informative for differentiating between tasks. To address this, we propose an inference time bandit-based prompt-tuning framework that explores and optimizes trajectory prompt selection to enhance task performance. Our experiments indicate not only clear performance gains due to bandit-based prompt-tuning, but also better sample complexity, scalability, and prompt space exploration compared to prompt-tuning baselines.

### Calibrating LLMs with Information-Theoretic Evidential Deep Learning 
[[arxiv](https://arxiv.org/abs/2502.06351)] [[cool](https://papers.cool/arxiv/2502.06351)] [[pdf](https://arxiv.org/pdf/2502.06351)]
> **Authors**: Yawei Li,David Rügamer,Bernd Bischl,Mina Rezaei
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 27 pages; 3 figures; accepted to ICLR 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Fine-tuned large language models (LLMs) often exhibit overconfidence, particularly when trained on small datasets, resulting in poor calibration and inaccurate uncertainty estimates. Evidential Deep Learning (EDL), an uncertainty-aware approach, enables uncertainty estimation in a single forward pass, making it a promising method for calibrating fine-tuned LLMs. However, despite its computational efficiency, EDL is prone to overfitting, as its training objective can result in overly concentrated probability distributions. To mitigate this, we propose regularizing EDL by incorporating an information bottleneck (IB). Our approach IB-EDL suppresses spurious information in the evidence generated by the model and encourages truly predictive information to influence both the predictions and uncertainty estimates. Extensive experiments across various fine-tuned LLMs and tasks demonstrate that IB-EDL outperforms both existing EDL and non-EDL approaches. By improving the trustworthiness of LLMs, IB-EDL facilitates their broader adoption in domains requiring high levels of confidence calibration. Code is available at https://github.com/sandylaker/ib-edl.

### Provably Near-Optimal Federated Ensemble Distillation with Negligible Overhead 
[[arxiv](https://arxiv.org/abs/2502.06349)] [[cool](https://papers.cool/arxiv/2502.06349)] [[pdf](https://arxiv.org/pdf/2502.06349)]
> **Authors**: Won-Jun Jang,Hyeon-Seo Park,Si-Hyeon Lee
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Federated ensemble distillation addresses client heterogeneity by generating pseudo-labels for an unlabeled server dataset based on client predictions and training the server model using the pseudo-labeled dataset. The unlabeled server dataset can either be pre-existing or generated through a data-free approach. The effectiveness of this approach critically depends on the method of assigning weights to client predictions when creating pseudo-labels, especially in highly heterogeneous settings. Inspired by theoretical results from GANs, we propose a provably near-optimal weighting method that leverages client discriminators trained with a server-distributed generator and local datasets. Our experiments on various image classification tasks demonstrate that the proposed method significantly outperforms baselines. Furthermore, we show that the additional communication cost, client-side privacy leakage, and client-side computational overhead introduced by our method are negligible, both in scenarios with and without a pre-existing server dataset.

### Causal Lifting of Neural Representations: Zero-Shot Generalization for Causal Inferences 
[[arxiv](https://arxiv.org/abs/2502.06343)] [[cool](https://papers.cool/arxiv/2502.06343)] [[pdf](https://arxiv.org/pdf/2502.06343)]
> **Authors**: Riccardo Cadei,Ilker Demirel,Piersilvio De Bartolomeis,Lukas Lindorfer,Sylvia Cremer,Cordelia Schmid,Francesco Locatello
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: A plethora of real-world scientific investigations is waiting to scale with the support of trustworthy predictive models that can reduce the need for costly data annotations. We focus on causal inferences on a target experiment with unlabeled factual outcomes, retrieved by a predictive model fine-tuned on a labeled similar experiment. First, we show that factual outcome estimation via Empirical Risk Minimization (ERM) may fail to yield valid causal inferences on the target population, even in a randomized controlled experiment and infinite training samples. Then, we propose to leverage the observed experimental settings during training to empower generalization to downstream interventional investigations, ``Causal Lifting'' the predictive model. We propose Deconfounded Empirical Risk Minimization (DERM), a new simple learning procedure minimizing the risk over a fictitious target population, preventing potential confounding effects. We validate our method on both synthetic and real-world scientific data. Notably, for the first time, we zero-shot generalize causal inferences on ISTAnt dataset (without annotation) by causal lifting a predictive model on our experiment variant.

### Microcanonical Langevin Ensembles: Advancing the Sampling of Bayesian Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.06335)] [[cool](https://papers.cool/arxiv/2502.06335)] [[pdf](https://arxiv.org/pdf/2502.06335)]
> **Authors**: Emanuel Sommer,Jakob Robnik,Giorgi Nozadze,Uros Seljak,David Rügamer
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Despite recent advances, sampling-based inference for Bayesian Neural Networks (BNNs) remains a significant challenge in probabilistic deep learning. While sampling-based approaches do not require a variational distribution assumption, current state-of-the-art samplers still struggle to navigate the complex and highly multimodal posteriors of BNNs. As a consequence, sampling still requires considerably longer inference times than non-Bayesian methods even for small neural networks, despite recent advances in making software implementations more efficient. Besides the difficulty of finding high-probability regions, the time until samplers provide sufficient exploration of these areas remains unpredictable. To tackle these challenges, we introduce an ensembling approach that leverages strategies from optimization and a recently proposed sampler called Microcanonical Langevin Monte Carlo (MCLMC) for efficient, robust and predictable sampling performance. Compared to approaches based on the state-of-the-art No-U-Turn Sampler, our approach delivers substantial speedups up to an order of magnitude, while maintaining or improving predictive performance and uncertainty quantification across diverse tasks and data modalities. The suggested Microcanonical Langevin Ensembles and modifications to MCLMC additionally enhance the method's predictability in resource requirements, facilitating easier parallelization. All in all, the proposed method offers a promising direction for practical, scalable inference for BNNs.

### Prompt-Driven Continual Graph Learning 
[[arxiv](https://arxiv.org/abs/2502.06327)] [[cool](https://papers.cool/arxiv/2502.06327)] [[pdf](https://arxiv.org/pdf/2502.06327)]
> **Authors**: Qi Wang,Tianfei Zhou,Ye Yuan,Rui Mao
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 12 pages, 7figures
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Continual Graph Learning (CGL), which aims to accommodate new tasks over evolving graph data without forgetting prior knowledge, is garnering significant research interest. Mainstream solutions adopt the memory replay-based idea, ie, caching representative data from earlier tasks for retraining the graph model. However, this strategy struggles with scalability issues for constantly evolving graphs and raises concerns regarding data privacy. Inspired by recent advancements in the prompt-based learning paradigm, this paper introduces a novel prompt-driven continual graph learning (PROMPTCGL) framework, which learns a separate prompt for each incoming task and maintains the underlying graph neural network model fixed. In this way, PROMPTCGL naturally avoids catastrophic forgetting of knowledge from previous tasks. More specifically, we propose hierarchical prompting to instruct the model from both feature- and topology-level to fully address the variability of task graphs in dynamic continual learning. Additionally, we develop a personalized prompt generator to generate tailored prompts for each graph node while minimizing the number of prompts needed, leading to constant memory consumption regardless of the graph scale. Extensive experiments on four benchmarks show that PROMPTCGL achieves superior performance against existing CGL approaches while significantly reducing memory consumption. Our code is available at https://github.com/QiWang98/PromptCGL.

### From Pixels to Components: Eigenvector Masking for Visual Representation Learning 
[[arxiv](https://arxiv.org/abs/2502.06314)] [[cool](https://papers.cool/arxiv/2502.06314)] [[pdf](https://arxiv.org/pdf/2502.06314)]
> **Authors**: Alice Bizeul,Thomas Sutter,Alain Ryser,Bernhard Schölkopf,Julius von Kügelgen,Julia E. Vogt
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Preprint. Under review
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: Predicting masked from visible parts of an image is a powerful self-supervised approach for visual representation learning. However, the common practice of masking random patches of pixels exhibits certain failure modes, which can prevent learning meaningful high-level features, as required for downstream tasks. We propose an alternative masking strategy that operates on a suitable transformation of the data rather than on the raw pixels. Specifically, we perform principal component analysis and then randomly mask a subset of components, which accounts for a fixed ratio of the data variance. The learning task then amounts to reconstructing the masked components from the visible ones. Compared to local patches of pixels, the principal components of images carry more global information. We thus posit that predicting masked from visible components involves more high-level features, allowing our masking strategy to extract more useful representations. This is corroborated by our empirical findings which demonstrate improved image classification performance for component over pixel masking. Our method thus constitutes a simple and robust data-driven alternative to traditional masked image modeling approaches.

### Analog In-memory Training on General Non-ideal Resistive Elements: The Impact of Response Functions 
[[arxiv](https://arxiv.org/abs/2502.06309)] [[cool](https://papers.cool/arxiv/2502.06309)] [[pdf](https://arxiv.org/pdf/2502.06309)]
> **Authors**: Zhaoxian Wu,Quan Xiao,Tayfun Gokmen,Omobayode Fagbohungbe,Tianyi Chen
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,硬件架构,优化与控制
- **Abstract**: As the economic and environmental costs of training and deploying large vision or language models increase dramatically, analog in-memory computing (AIMC) emerges as a promising energy-efficient solution. However, the training perspective, especially its training dynamic, is underexplored. In AIMC hardware, the trainable weights are represented by the conductance of resistive elements and updated using consecutive electrical pulses. Among all the physical properties of resistive elements, the response to the pulses directly affects the training dynamics. This paper first provides a theoretical foundation for gradient-based training on AIMC hardware and studies the impact of response functions. We demonstrate that noisy update and asymmetric response functions negatively impact Analog SGD by imposing an implicit penalty term on the objective. To overcome the issue, Tiki-Taka, a residual learning algorithm, converges exactly to a critical point by optimizing a main array and a residual array bilevelly. The conclusion is supported by simulations validating our theoretical insights.

### Utilizing Novelty-based Evolution Strategies to Train Transformers in Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.06301)] [[cool](https://papers.cool/arxiv/2502.06301)] [[pdf](https://arxiv.org/pdf/2502.06301)]
> **Authors**: Matyáš Lorenc
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,神经和进化计算
- **Abstract**: In this paper, we experiment with novelty-based variants of OpenAI-ES, the NS-ES and NSR-ES algorithms, and evaluate their effectiveness in training complex, transformer-based architectures designed for the problem of reinforcement learning such as Decision Transformers. We also test if we can accelerate the novelty-based training of these larger models by seeding the training by a pretrained models. By this, we build on our previous work, where we tested the ability of evolution strategies - specifically the aforementioned OpenAI-ES - to train the Decision Transformer architecture. The results were mixed. NS-ES showed progress, but it would clearly need many more iterations for it to yield interesting results. NSR-ES, on the other hand, proved quite capable of being straightforwardly used on larger models, since its performance appears as similar between the feed-forward model and Decision Transformer, as it was for the OpenAI-ES in our previous work.

### The impact of allocation strategies in subset learning on the expressive power of neural networks 
[[arxiv](https://arxiv.org/abs/2502.06300)] [[cool](https://papers.cool/arxiv/2502.06300)] [[pdf](https://arxiv.org/pdf/2502.06300)]
> **Authors**: Ofir Schlisselberg,Ran Darshan
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: In traditional machine learning, models are defined by a set of parameters, which are optimized to perform specific tasks. In neural networks, these parameters correspond to the synaptic weights. However, in reality, it is often infeasible to control or update all weights. This challenge is not limited to artificial networks but extends to biological networks, such as the brain, where the extent of distributed synaptic weight modification during learning remains unclear. Motivated by these insights, we theoretically investigate how different allocations of a fixed number of learnable weights influence the capacity of neural networks. Using a teacher-student setup, we introduce a benchmark to quantify the expressivity associated with each allocation. We establish conditions under which allocations have maximal or minimal expressive power in linear recurrent neural networks and linear multi-layer feedforward networks. For suboptimal allocations, we propose heuristic principles to estimate their expressivity. These principles extend to shallow ReLU networks as well. Finally, we validate our theoretical findings with empirical experiments. Our results emphasize the critical role of strategically distributing learnable weights across the network, showing that a more widespread allocation generally enhances the network's expressive power.

### DVFS-Aware DNN Inference on GPUs: Latency Modeling and Performance Analysis 
[[arxiv](https://arxiv.org/abs/2502.06295)] [[cool](https://papers.cool/arxiv/2502.06295)] [[pdf](https://arxiv.org/pdf/2502.06295)]
> **Authors**: Yunchu Han,Zhaojun Nan,Sheng Zhou,Zhisheng Niu
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,网络和互联网架构
- **Abstract**: The rapid development of deep neural networks (DNNs) is inherently accompanied by the problem of high computational costs. To tackle this challenge, dynamic voltage frequency scaling (DVFS) is emerging as a promising technology for balancing the latency and energy consumption of DNN inference by adjusting the computing frequency of processors. However, most existing models of DNN inference time are based on the CPU-DVFS technique, and directly applying the CPU-DVFS model to DNN inference on GPUs will lead to significant errors in optimizing latency and energy consumption. In this paper, we propose a DVFS-aware latency model to precisely characterize DNN inference time on GPUs. We first formulate the DNN inference time based on extensive experiment results for different devices and analyze the impact of fitting parameters. Then by dividing DNNs into multiple blocks and obtaining the actual inference time, the proposed model is further verified. Finally, we compare our proposed model with the CPU-DVFS model in two specific cases. Evaluation results demonstrate that local inference optimization with our proposed model achieves a reduction of no less than 66% and 69% in inference time and energy consumption respectively. In addition, cooperative inference with our proposed model can improve the partition policy and reduce the energy consumption compared to the CPU-DVFS model.

### On the Expressiveness of Rational ReLU Neural Networks With Bounded Depth 
[[arxiv](https://arxiv.org/abs/2502.06283)] [[cool](https://papers.cool/arxiv/2502.06283)] [[pdf](https://arxiv.org/pdf/2502.06283)]
> **Authors**: Gennadiy Averkov,Christopher Hojny,Maximilian Merkert
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: ICLR 2025 conference paper
- **标题**: None
- **领域**: 机器学习,离散数学
- **Abstract**: To confirm that the expressive power of ReLU neural networks grows with their depth, the function $F_n = \max \{0,x_1,\ldots,x_n\}$ has been considered in the literature. A conjecture by Hertrich, Basu, Di Summa, and Skutella [NeurIPS 2021] states that any ReLU network that exactly represents $F_n$ has at least $\lceil\log_2 (n+1)\rceil$ hidden layers. The conjecture has recently been confirmed for networks with integer weights by Haase, Hertrich, and Loho [ICLR 2023]. We follow up on this line of research and show that, within ReLU networks whose weights are decimal fractions, $F_n$ can only be represented by networks with at least $\lceil\log_3 (n+1)\rceil$ hidden layers. Moreover, if all weights are $N$-ary fractions, then $F_n$ can only be represented by networks with at least $Ω( \frac{\ln n}{\ln \ln N})$ layers. These results are a partial confirmation of the above conjecture for rational ReLU networks, and provide the first non-constant lower bound on the depth of practically relevant ReLU networks.

### IceBerg: Debiased Self-Training for Class-Imbalanced Node Classification 
[[arxiv](https://arxiv.org/abs/2502.06280)] [[cool](https://papers.cool/arxiv/2502.06280)] [[pdf](https://arxiv.org/pdf/2502.06280)]
> **Authors**: Zhixun Li,Dingshuo Chen,Tong Zhao,Daixin Wang,Hongrui Liu,Zhiqiang Zhang,Jun Zhou,Jeffrey Xu Yu
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Accepted by TheWebConf (WWW) 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Graph Neural Networks (GNNs) have achieved great success in dealing with non-Euclidean graph-structured data and have been widely deployed in many real-world applications. However, their effectiveness is often jeopardized under class-imbalanced training sets. Most existing studies have analyzed class-imbalanced node classification from a supervised learning perspective, but they do not fully utilize the large number of unlabeled nodes in semi-supervised scenarios. We claim that the supervised signal is just the tip of the iceberg and a large number of unlabeled nodes have not yet been effectively utilized. In this work, we propose IceBerg, a debiased self-training framework to address the class-imbalanced and few-shot challenges for GNNs at the same time. Specifically, to figure out the Matthew effect and label distribution shift in self-training, we propose Double Balancing, which can largely improve the performance of existing baselines with just a few lines of code as a simple plug-and-play module. Secondly, to enhance the long-range propagation capability of GNNs, we disentangle the propagation and transformation operations of GNNs. Therefore, the weak supervision signals can propagate more effectively to address the few-shot issue. In summary, we find that leveraging unlabeled nodes can significantly enhance the performance of GNNs in class-imbalanced and few-shot scenarios, and even small, surgical modifications can lead to substantial performance improvements. Systematic experiments on benchmark datasets show that our method can deliver considerable performance gain over existing class-imbalanced node classification baselines. Additionally, due to IceBerg's outstanding ability to leverage unsupervised signals, it also achieves state-of-the-art results in few-shot node classification scenarios. The code of IceBerg is available at: https://github.com/ZhixunLEE/IceBerg.

### HODDI: A Dataset of High-Order Drug-Drug Interactions for Computational Pharmacovigilance 
[[arxiv](https://arxiv.org/abs/2502.06274)] [[cool](https://papers.cool/arxiv/2502.06274)] [[pdf](https://arxiv.org/pdf/2502.06274)]
> **Authors**: Zhaoying Wang,Yingdan Shi,Xiang Liu,Can Chen,Jun Wen,Ren Wang
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,分子网络
- **Abstract**: Drug-side effect research is vital for understanding adverse reactions arising in complex multi-drug therapies. However, the scarcity of higher-order datasets that capture the combinatorial effects of multiple drugs severely limits progress in this field. Existing resources such as TWOSIDES primarily focus on pairwise interactions. To fill this critical gap, we introduce HODDI, the first Higher-Order Drug-Drug Interaction Dataset, constructed from U.S. Food and Drug Administration (FDA) Adverse Event Reporting System (FAERS) records spanning the past decade, to advance computational pharmacovigilance. HODDI contains 109,744 records involving 2,506 unique drugs and 4,569 unique side effects, specifically curated to capture multi-drug interactions and their collective impact on adverse effects. Comprehensive statistical analyses demonstrate HODDI's extensive coverage and robust analytical metrics, making it a valuable resource for studying higher-order drug relationships. Evaluating HODDI with multiple models, we found that simple Multi-Layer Perceptron (MLP) can outperform graph models, while hypergraph models demonstrate superior performance in capturing complex multi-drug interactions, further validating HODDI's effectiveness. Our findings highlight the inherent value of higher-order information in drug-side effect prediction and position HODDI as a benchmark dataset for advancing research in pharmacovigilance, drug safety, and personalized medicine. The dataset and codes are available at https://github.com/TIML-Group/HODDI.

### Beyond Batch Learning: Global Awareness Enhanced Domain Adaptation 
[[arxiv](https://arxiv.org/abs/2502.06272)] [[cool](https://papers.cool/arxiv/2502.06272)] [[pdf](https://arxiv.org/pdf/2502.06272)]
> **Authors**: Lingkun Luo,Shiqiang Hu,Liming Chen
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: ef:IEEE TRANSACTIONS ON PATTERN ANALYSIS ANDMACHINEINTELLIGENCE 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: In domain adaptation (DA), the effectiveness of deep learning-based models is often constrained by batch learning strategies that fail to fully apprehend the global statistical and geometric characteristics of data distributions. Addressing this gap, we introduce 'Global Awareness Enhanced Domain Adaptation' (GAN-DA), a novel approach that transcends traditional batch-based limitations. GAN-DA integrates a unique predefined feature representation (PFR) to facilitate the alignment of cross-domain distributions, thereby achieving a comprehensive global statistical awareness. This representation is innovatively expanded to encompass orthogonal and common feature aspects, which enhances the unification of global manifold structures and refines decision boundaries for more effective DA. Our extensive experiments, encompassing 27 diverse cross-domain image classification tasks, demonstrate GAN-DA's remarkable superiority, outperforming 24 established DA methods by a significant margin. Furthermore, our in-depth analyses shed light on the decision-making processes, revealing insights into the adaptability and efficiency of GAN-DA. This approach not only addresses the limitations of existing DA methodologies but also sets a new benchmark in the realm of domain adaptation, offering broad implications for future research and applications in this field.

### Reducing Variance Caused by Communication in Decentralized Multi-agent Deep Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.06261)] [[cool](https://papers.cool/arxiv/2502.06261)] [[pdf](https://arxiv.org/pdf/2502.06261)]
> **Authors**: Changxi Zhu,Mehdi Dastani,Shihan Wang
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 30 pages, 6 figures, 6 tables
- **标题**: None
- **领域**: 机器学习
- **Abstract**: In decentralized multi-agent deep reinforcement learning (MADRL), communication can help agents to gain a better understanding of the environment to better coordinate their behaviors. Nevertheless, communication may involve uncertainty, which potentially introduces variance to the learning of decentralized agents. In this paper, we focus on a specific decentralized MADRL setting with communication and conduct a theoretical analysis to study the variance that is caused by communication in policy gradients. We propose modular techniques to reduce the variance in policy gradients during training. We adopt our modular techniques into two existing algorithms for decentralized MADRL with communication and evaluate them on multiple tasks in the StarCraft Multi-Agent Challenge and Traffic Junction domains. The results show that decentralized MADRL communication methods extended with our proposed techniques not only achieve high-performing agents but also reduce variance in policy gradients during training.

### DGNO: A Novel Physics-aware Neural Operator for Solving Forward and Inverse PDE Problems based on Deep, Generative Probabilistic Modeling 
[[arxiv](https://arxiv.org/abs/2502.06250)] [[cool](https://papers.cool/arxiv/2502.06250)] [[pdf](https://arxiv.org/pdf/2502.06250)]
> **Authors**: Yaohua Zang,Phaedon-Stelios Koutsourelakis
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,数学物理
- **Abstract**: Solving parametric partial differential equations (PDEs) and associated PDE-based, inverse problems is a central task in engineering and physics, yet existing neural operator methods struggle with high-dimensional, discontinuous inputs and require large amounts of {\em labeled} training data. We propose the Deep Generative Neural Operator (DGNO), a physics-aware framework that addresses these challenges by leveraging a deep, generative, probabilistic model in combination with a set of lower-dimensional, latent variables that simultaneously encode PDE-inputs and PDE-outputs. This formulation can make use of unlabeled data and significantly improves inverse problem-solving, particularly for discontinuous or discrete-valued input functions. DGNO enforces physics constraints without labeled data by incorporating as virtual observables, weak-form residuals based on compactly supported radial basis functions (CSRBFs). These relax regularity constraints and eliminate higher-order derivatives from the objective function. We also introduce MultiONet, a novel neural operator architecture, which is a more expressive generalization of the popular DeepONet that significantly enhances the approximating power of the proposed model. These innovations make DGNO particularly effective for challenging forward and inverse, PDE-based problems, such as those involving multi-phase media. Numerical experiments demonstrate that DGNO achieves higher accuracy across multiple benchmarks while exhibiting robustness to noise and strong generalization to out-of-distribution cases. Its adaptability, and the ability to handle sparse, noisy data while providing probabilistic estimates, make DGNO a powerful tool for scientific and engineering applications.

### PiKE: Adaptive Data Mixing for Multi-Task Learning Under Low Gradient Conflicts 
[[arxiv](https://arxiv.org/abs/2502.06244)] [[cool](https://papers.cool/arxiv/2502.06244)] [[pdf](https://arxiv.org/pdf/2502.06244)]
> **Authors**: Zeman Li,Yuan Deng,Peilin Zhong,Meisam Razaviyayn,Vahab Mirrokni
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Modern machine learning models are trained on diverse datasets and tasks to improve generalization. A key challenge in multitask learning is determining the optimal data mixing and sampling strategy across different data sources. Prior research in this multi-task learning setting has primarily focused on mitigating gradient conflicts between tasks. However, we observe that many real-world multitask learning scenarios-such as multilingual training and multi-domain learning in large foundation models-exhibit predominantly positive task interactions with minimal or no gradient conflict. Building on this insight, we introduce PiKE (Positive gradient interaction-based K-task weights Estimator), an adaptive data mixing algorithm that dynamically adjusts task contributions throughout training. PiKE optimizes task sampling to minimize overall loss, effectively leveraging positive gradient interactions with almost no additional computational overhead. We establish theoretical convergence guarantees for PiKE and demonstrate its superiority over static and non-adaptive mixing strategies. Additionally, we extend PiKE to promote fair learning across tasks, ensuring balanced progress and preventing task underrepresentation. Empirical evaluations on large-scale language model pretraining show that PiKE consistently outperforms existing heuristic and static mixing strategies, leading to faster convergence and improved downstream task performance.

### Position: Continual Learning Benefits from An Evolving Population over An Unified Model 
[[arxiv](https://arxiv.org/abs/2502.06210)] [[cool](https://papers.cool/arxiv/2502.06210)] [[pdf](https://arxiv.org/pdf/2502.06210)]
> **Authors**: Aojun Lu,Junchao Ke,Chunhui Ding,Jiahao Fan,Yanan Sun
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Deep neural networks have demonstrated remarkable success in machine learning; however, they remain fundamentally ill-suited for Continual Learning (CL). Recent research has increasingly focused on achieving CL without the need for rehearsal. Among these, parameter isolation-based methods have proven particularly effective in enhancing CL by optimizing model weights for each incremental task. Despite their success, they fall short in optimizing architectures tailored to distinct incremental tasks. To address this limitation, updating a group of models with different architectures offers a promising alternative to the traditional CL paradigm that relies on a single unified model. Building on this insight, this study introduces a novel Population-based Continual Learning (PCL) framework. PCL extends CL to the architectural level by maintaining and evolving a population of neural network architectures, which are continually refined for the current task through NAS. Importantly, the well-evolved population for the current incremental task is naturally inherited by the subsequent one, thereby facilitating forward transfer, a crucial objective in CL. Throughout the CL process, the population evolves, yielding task-specific architectures that collectively form a robust CL system. Experimental results demonstrate that PCL outperforms state-of-the-art rehearsal-free CL methods that employs a unified model, highlighting its potential as a new paradigm for CL.

### Enhancing Cost Efficiency in Active Learning with Candidate Set Query 
[[arxiv](https://arxiv.org/abs/2502.06209)] [[cool](https://papers.cool/arxiv/2502.06209)] [[pdf](https://arxiv.org/pdf/2502.06209)]
> **Authors**: Yeho Gwon,Sehyun Hwang,Hoyoung Kim,Jungseul Ok,Suha Kwak
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 20 pages, 17 figures, 4 tables
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: This paper introduces a cost-efficient active learning (AL) framework for classification, featuring a novel query design called candidate set query. Unlike traditional AL queries requiring the oracle to examine all possible classes, our method narrows down the set of candidate classes likely to include the ground-truth class, significantly reducing the search space and labeling cost. Moreover, we leverage conformal prediction to dynamically generate small yet reliable candidate sets, adapting to model enhancement over successive AL rounds. To this end, we introduce an acquisition function designed to prioritize data points that offer high information gain at lower cost. Empirical evaluations on CIFAR-10, CIFAR-100, and ImageNet64x64 demonstrate the effectiveness and scalability of our framework. Notably, it reduces labeling cost by 42% on ImageNet64x64.

### Right Time to Learn:Promoting Generalization via Bio-inspired Spacing Effect in Knowledge Distillation 
[[arxiv](https://arxiv.org/abs/2502.06192)] [[cool](https://papers.cool/arxiv/2502.06192)] [[pdf](https://arxiv.org/pdf/2502.06192)]
> **Authors**: Guanglong Sun,Hongwei Yan,Liyuan Wang,Qian Li,Bo Lei,Yi Zhong
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Knowledge distillation (KD) is a powerful strategy for training deep neural networks (DNNs). Although it was originally proposed to train a more compact ``student'' model from a large ``teacher'' model, many recent efforts have focused on adapting it to promote generalization of the model itself, such as online KD and self KD. % as an effective way Here, we propose an accessible and compatible strategy named Spaced KD to improve the effectiveness of both online KD and self KD, in which the student model distills knowledge from a teacher model trained with a space interval ahead. This strategy is inspired by a prominent theory named \emph{spacing effect} in biological learning and memory, positing that appropriate intervals between learning trials can significantly enhance learning performance. With both theoretical and empirical analyses, we demonstrate that the benefits of the proposed Spaced KD stem from convergence to a flatter loss landscape during stochastic gradient descent (SGD). We perform extensive experiments to validate the effectiveness of Spaced KD in improving the learning performance of DNNs (e.g., the performance gain is up to 2.31\% and 3.34\% on Tiny-ImageNet over online KD and self KD, respectively).

### Uncertainty-Aware Adaptation of Large Language Models for Protein-Protein Interaction Analysis 
[[arxiv](https://arxiv.org/abs/2502.06173)] [[cool](https://papers.cool/arxiv/2502.06173)] [[pdf](https://arxiv.org/pdf/2502.06173)]
> **Authors**: Sanket Jantre,Tianle Wang,Gilchan Park,Kriti Chopra,Nicholas Jeon,Xiaoning Qian,Nathan M. Urban,Byung-Jun Yoon
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,应用领域,机器学习
- **Abstract**: Identification of protein-protein interactions (PPIs) helps derive cellular mechanistic understanding, particularly in the context of complex conditions such as neurodegenerative disorders, metabolic syndromes, and cancer. Large Language Models (LLMs) have demonstrated remarkable potential in predicting protein structures and interactions via automated mining of vast biomedical literature; yet their inherent uncertainty remains a key challenge for deriving reproducible findings, critical for biomedical applications. In this study, we present an uncertainty-aware adaptation of LLMs for PPI analysis, leveraging fine-tuned LLaMA-3 and BioMedGPT models. To enhance prediction reliability, we integrate LoRA ensembles and Bayesian LoRA models for uncertainty quantification (UQ), ensuring confidence-calibrated insights into protein behavior. Our approach achieves competitive performance in PPI identification across diverse disease contexts while addressing model uncertainty, thereby enhancing trustworthiness and reproducibility in computational biology. These findings underscore the potential of uncertainty-aware LLM adaptation for advancing precision medicine and biomedical research.

### Universal Approximation of Visual Autoregressive Transformers 
[[arxiv](https://arxiv.org/abs/2502.06167)] [[cool](https://papers.cool/arxiv/2502.06167)] [[pdf](https://arxiv.org/pdf/2502.06167)]
> **Authors**: Yifang Chen,Xiaoyu Li,Yingyu Liang,Zhenmei Shi,Zhao Song
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,计算机视觉和模式识别
- **Abstract**: We investigate the fundamental limits of transformer-based foundation models, extending our analysis to include Visual Autoregressive (VAR) transformers. VAR represents a big step toward generating images using a novel, scalable, coarse-to-fine ``next-scale prediction'' framework. These models set a new quality bar, outperforming all previous methods, including Diffusion Transformers, while having state-of-the-art performance for image synthesis tasks. Our primary contributions establish that, for single-head VAR transformers with a single self-attention layer and single interpolation layer, the VAR Transformer is universal. From the statistical perspective, we prove that such simple VAR transformers are universal approximators for any image-to-image Lipschitz functions. Furthermore, we demonstrate that flow-based autoregressive transformers inherit similar approximation capabilities. Our results provide important design principles for effective and computationally efficient VAR Transformer strategies that can be used to extend their utility to more sophisticated VAR models in image generation and other related areas.

### Generalized Temporal Tensor Decomposition with Rank-revealing Latent-ODE 
[[arxiv](https://arxiv.org/abs/2502.06164)] [[cool](https://papers.cool/arxiv/2502.06164)] [[pdf](https://arxiv.org/pdf/2502.06164)]
> **Authors**: Panqi Chen,Lei Cheng,Jianlong Li,Weichang Li,Weiqing Liu,Jiang Bian,Shikai Fang
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Tensor decomposition is a fundamental tool for analyzing multi-dimensional data by learning low-rank factors to represent high-order interactions. While recent works on temporal tensor decomposition have made significant progress by incorporating continuous timestamps in latent factors, they still struggle with general tensor data with continuous indexes not only in the temporal mode but also in other modes, such as spatial coordinates in climate data. Additionally, the problem of determining the tensor rank remains largely unexplored in temporal tensor models. To address these limitations, we propose \underline{G}eneralized temporal tensor decomposition with \underline{R}ank-r\underline{E}vealing laten\underline{T}-ODE (GRET). Our approach encodes continuous spatial indexes as learnable Fourier features and employs neural ODEs in latent space to learn the temporal trajectories of factors. To automatically reveal the rank of temporal tensors, we introduce a rank-revealing Gaussian-Gamma prior over the factor trajectories. We develop an efficient variational inference scheme with an analytical evidence lower bound, enabling sampling-free optimization. Through extensive experiments on both synthetic and real-world datasets, we demonstrate that GRET not only reveals the underlying ranks of temporal tensors but also significantly outperforms existing methods in prediction performance and robustness against noise.

### Scalable k-Means Clustering for Large k via Seeded Approximate Nearest-Neighbor Search 
[[arxiv](https://arxiv.org/abs/2502.06163)] [[cool](https://papers.cool/arxiv/2502.06163)] [[pdf](https://arxiv.org/pdf/2502.06163)]
> **Authors**: Jack Spalding-Jamieson,Eliot Wong Robson,Da Wei Zheng
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 29 pages, 8 figures
- **标题**: None
- **领域**: 机器学习,计算几何,机器学习
- **Abstract**: For very large values of $k$, we consider methods for fast $k$-means clustering of massive datasets with $10^7\sim10^9$ points in high-dimensions ($d\geq100$). All current practical methods for this problem have runtimes at least $Ω(k^2)$. We find that initialization routines are not a bottleneck for this case. Instead, it is critical to improve the speed of Lloyd's local-search algorithm, particularly the step that reassigns points to their closest center. Attempting to improve this step naturally leads us to leverage approximate nearest-neighbor search methods, although this alone is not enough to be practical. Instead, we propose a family of problems we call "Seeded Approximate Nearest-Neighbor Search", for which we propose "Seeded Search-Graph" methods as a solution.

## 多代理系统(cs.MA:Multiagent Systems)

### Fairness in Multi-Agent AI: A Unified Framework for Ethical and Equitable Autonomous Systems 
[[arxiv](https://arxiv.org/abs/2502.07254)] [[cool](https://papers.cool/arxiv/2502.07254)] [[pdf](https://arxiv.org/pdf/2502.07254)]
> **Authors**: Rajesh Ranjan,Shailja Gupta,Surya Narayan Singh
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 多代理系统,人工智能,计算机与社会
- **Abstract**: Ensuring fairness in decentralized multi-agent systems presents significant challenges due to emergent biases, systemic inefficiencies, and conflicting agent incentives. This paper provides a comprehensive survey of fairness in multi-agent AI, introducing a novel framework where fairness is treated as a dynamic, emergent property of agent interactions. The framework integrates fairness constraints, bias mitigation strategies, and incentive mechanisms to align autonomous agent behaviors with societal values while balancing efficiency and robustness. Through empirical validation, we demonstrate that incorporating fairness constraints results in more equitable decision-making. This work bridges the gap between AI ethics and system design, offering a foundation for accountable, transparent, and socially responsible multi-agent AI systems.

### Bayesian Optimization for Building Social-Influence-Free Consensus 
[[arxiv](https://arxiv.org/abs/2502.07166)] [[cool](https://papers.cool/arxiv/2502.07166)] [[pdf](https://arxiv.org/pdf/2502.07166)]
> **Authors**: Masaki Adachi,Siu Lun Chau,Wenjie Xu,Anurag Singh,Michael A. Osborne,Krikamol Muandet
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 50 pages, 8 figures
- **标题**: None
- **领域**: 多代理系统,计算机科学与博弈论,机器学习,机器学习
- **Abstract**: We introduce Social Bayesian Optimization (SBO), a vote-efficient algorithm for consensus-building in collective decision-making. In contrast to single-agent scenarios, collective decision-making encompasses group dynamics that may distort agents' preference feedback, thereby impeding their capacity to achieve a social-influence-free consensus -- the most preferable decision based on the aggregated agent utilities. We demonstrate that under mild rationality axioms, reaching social-influence-free consensus using noisy feedback alone is impossible. To address this, SBO employs a dual voting system: cheap but noisy public votes (e.g., show of hands in a meeting), and more accurate, though expensive, private votes (e.g., one-to-one interview). We model social influence using an unknown social graph and leverage the dual voting system to efficiently learn this graph. Our theoretical findigns show that social graph estimation converges faster than the black-box estimation of agents' utilities, allowing us to reduce reliance on costly private votes early in the process. This enables efficient consensus-building primarily through noisy public votes, which are debiased based on the estimated social graph to infer social-influence-free feedback. We validate the efficacy of SBO across multiple real-world applications, including thermal comfort, team building, travel negotiation, and energy trading collaboration.

### Who is Helping Whom? Analyzing Inter-dependencies to Evaluate Cooperation in Human-AI Teaming 
[[arxiv](https://arxiv.org/abs/2502.06976)] [[cool](https://papers.cool/arxiv/2502.06976)] [[pdf](https://arxiv.org/pdf/2502.06976)]
> **Authors**: Upasana Biswas,Siddhant Bhambri,Subbarao Kambhampati
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 多代理系统,人工智能
- **Abstract**: The long-standing research challenges of Human-AI Teaming(HAT) and Zero-shot Cooperation(ZSC) have been tackled by applying multi-agent reinforcement learning(MARL) to train an agent by optimizing the environment reward function and evaluating their performance through task performance metrics such as task reward. However, such evaluation focuses only on task completion, while being agnostic to `how' the two agents work with each other. Specifically, we are interested in understanding the cooperation arising within the team when trained agents are paired with humans. To formally address this problem, we propose the concept of interdependence to measure how much agents rely on each other's actions to achieve the shared goal, as a key metric for evaluating cooperation in human-agent teams. Towards this, we ground this concept through a symbolic formalism and define evaluation metrics that allow us to assess the degree of reliance between the agents' actions. We pair state-of-the-art agents trained through MARL for HAT, with learned human models for the the popular Overcooked domain, and evaluate the team performance for these human-agent teams. Our results demonstrate that trained agents are not able to induce cooperative behavior, reporting very low levels of interdependence across all the teams. We also report that teaming performance of a team is not necessarily correlated with the task reward.

## 网络和互联网架构(cs.NI:Networking and Internet Architecture)

### RAILS: Risk-Aware Iterated Local Search for Joint SLA Decomposition and Service Provider Management in Multi-Domain Networks 
[[arxiv](https://arxiv.org/abs/2502.06674)] [[cool](https://papers.cool/arxiv/2502.06674)] [[pdf](https://arxiv.org/pdf/2502.06674)]
> **Authors**: Cyril Shih-Huan Hsu,Chrysa Papagianni,Paola Grosso
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: The paper has been submitted to IEEE HPSR 2025
- **标题**: None
- **领域**: 网络和互联网架构,机器学习
- **Abstract**: The emergence of the fifth generation (5G) technology has transformed mobile networks into multi-service environments, necessitating efficient network slicing to meet diverse Service Level Agreements (SLAs). SLA decomposition across multiple network domains, each potentially managed by different service providers, poses a significant challenge due to limited visibility into real-time underlying domain conditions. This paper introduces Risk-Aware Iterated Local Search (RAILS), a novel risk model-driven meta-heuristic framework designed to jointly address SLA decomposition and service provider selection in multi-domain networks. By integrating online risk modeling with iterated local search principles, RAILS effectively navigates the complex optimization landscape, utilizing historical feedback from domain controllers. We formulate the joint problem as a Mixed-Integer Nonlinear Programming (MINLP) problem and prove its NP-hardness. Extensive simulations demonstrate that RAILS achieves near-optimal performance, offering an efficient, real-time solution for adaptive SLA management in modern multi-domain networks.

### A Survey on Video Analytics in Cloud-Edge-Terminal Collaborative Systems 
[[arxiv](https://arxiv.org/abs/2502.06581)] [[cool](https://papers.cool/arxiv/2502.06581)] [[pdf](https://arxiv.org/pdf/2502.06581)]
> **Authors**: Linxiao Gong,Hao Yang,Gaoyun Fang,Bobo Ju,Juncen Guo,Xiaoguang Zhu,Xiping Hu,Yan Wang,Peng Sun,Azzedine Boukerche
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 网络和互联网架构,计算机视觉和模式识别,机器学习
- **Abstract**: The explosive growth of video data has driven the development of distributed video analytics in cloud-edge-terminal collaborative (CETC) systems, enabling efficient video processing, real-time inference, and privacy-preserving analysis. Among multiple advantages, CETC systems can distribute video processing tasks and enable adaptive analytics across cloud, edge, and terminal devices, leading to breakthroughs in video surveillance, autonomous driving, and smart cities. In this survey, we first analyze fundamental architectural components, including hierarchical, distributed, and hybrid frameworks, alongside edge computing platforms and resource management mechanisms. Building upon these foundations, edge-centric approaches emphasize on-device processing, edge-assisted offloading, and edge intelligence, while cloud-centric methods leverage powerful computational capabilities for complex video understanding and model training. Our investigation also covers hybrid video analytics incorporating adaptive task offloading and resource-aware scheduling techniques that optimize performance across the entire system. Beyond conventional approaches, recent advances in large language models and multimodal integration reveal both opportunities and challenges in platform scalability, data protection, and system reliability. Future directions also encompass explainable systems, efficient processing mechanisms, and advanced video analytics, offering valuable insights for researchers and practitioners in this dynamic field.

## 机器人技术(cs.RO:Robotics)

### Beyond Confidence: Adaptive Abstention in Dual-Threshold Conformal Prediction for Autonomous System Perception 
[[arxiv](https://arxiv.org/abs/2502.07255)] [[cool](https://papers.cool/arxiv/2502.07255)] [[pdf](https://arxiv.org/pdf/2502.07255)]
> **Authors**: Divake Kumar,Nastaran Darabi,Sina Tayebati,Amit Ranjan Trivedi
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,机器学习
- **Abstract**: Safety-critical perception systems require both reliable uncertainty quantification and principled abstention mechanisms to maintain safety under diverse operational conditions. We present a novel dual-threshold conformalization framework that provides statistically-guaranteed uncertainty estimates while enabling selective prediction in high-risk scenarios. Our approach uniquely combines a conformal threshold ensuring valid prediction sets with an abstention threshold optimized through ROC analysis, providing distribution-free coverage guarantees (>= 1 - alpha) while identifying unreliable predictions. Through comprehensive evaluation on CIFAR-100, ImageNet1K, and ModelNet40 datasets, we demonstrate superior robustness across camera and LiDAR modalities under varying environmental perturbations. The framework achieves exceptional detection performance (AUC: 0.993 to 0.995) under severe conditions while maintaining high coverage (>90.0%) and enabling adaptive abstention (13.5% to 63.4% +/- 0.5) as environmental severity increases. For LiDAR-based perception, our approach demonstrates particularly strong performance, maintaining robust coverage (>84.5%) while appropriately abstaining from unreliable predictions. Notably, the framework shows remarkable stability under heavy perturbations, with detection performance (AUC: 0.995 +/- 0.001) significantly outperforming existing methods across all modalities. Our unified approach bridges the gap between theoretical guarantees and practical deployment needs, offering a robust solution for safety-critical autonomous systems operating in challenging real-world conditions.

### Space-Aware Instruction Tuning: Dataset and Benchmark for Guide Dog Robots Assisting the Visually Impaired 
[[arxiv](https://arxiv.org/abs/2502.07183)] [[cool](https://papers.cool/arxiv/2502.07183)] [[pdf](https://arxiv.org/pdf/2502.07183)]
> **Authors**: ByungOk Han,Woo-han Yun,Beom-Su Seo,Jaehong Kim
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: ICRA 2025
- **标题**: None
- **领域**: 机器人技术,计算机视觉和模式识别
- **Abstract**: Guide dog robots offer promising solutions to enhance mobility and safety for visually impaired individuals, addressing the limitations of traditional guide dogs, particularly in perceptual intelligence and communication. With the emergence of Vision-Language Models (VLMs), robots are now capable of generating natural language descriptions of their surroundings, aiding in safer decision-making. However, existing VLMs often struggle to accurately interpret and convey spatial relationships, which is crucial for navigation in complex environments such as street crossings. We introduce the Space-Aware Instruction Tuning (SAIT) dataset and the Space-Aware Benchmark (SA-Bench) to address the limitations of current VLMs in understanding physical environments. Our automated data generation pipeline focuses on the virtual path to the destination in 3D space and the surroundings, enhancing environmental comprehension and enabling VLMs to provide more accurate guidance to visually impaired individuals. We also propose an evaluation protocol to assess VLM effectiveness in delivering walking guidance. Comparative experiments demonstrate that our space-aware instruction-tuned model outperforms state-of-the-art algorithms. We have fully open-sourced the SAIT dataset and SA-Bench, along with the related code, at https://github.com/byungokhan/Space-awareVLM

### Predictive Red Teaming: Breaking Policies Without Breaking Robots 
[[arxiv](https://arxiv.org/abs/2502.06575)] [[cool](https://papers.cool/arxiv/2502.06575)] [[pdf](https://arxiv.org/pdf/2502.06575)]
> **Authors**: Anirudha Majumdar,Mohit Sharma,Dmitry Kalashnikov,Sumeet Singh,Pierre Sermanet,Vikas Sindhwani
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,人工智能,机器学习,系统与控制
- **Abstract**: Visuomotor policies trained via imitation learning are capable of performing challenging manipulation tasks, but are often extremely brittle to lighting, visual distractors, and object locations. These vulnerabilities can depend unpredictably on the specifics of training, and are challenging to expose without time-consuming and expensive hardware evaluations. We propose the problem of predictive red teaming: discovering vulnerabilities of a policy with respect to environmental factors, and predicting the corresponding performance degradation without hardware evaluations in off-nominal scenarios. In order to achieve this, we develop RoboART: an automated red teaming (ART) pipeline that (1) modifies nominal observations using generative image editing to vary different environmental factors, and (2) predicts performance under each variation using a policy-specific anomaly detector executed on edited observations. Experiments across 500+ hardware trials in twelve off-nominal conditions for visuomotor diffusion policies demonstrate that RoboART predicts performance degradation with high accuracy (less than 0.19 average difference between predicted and real success rates). We also demonstrate how predictive red teaming enables targeted data collection: fine-tuning with data collected under conditions predicted to be adverse boosts baseline performance by 2-7x.

### SIREN: Semantic, Initialization-Free Registration of Multi-Robot Gaussian Splatting Maps 
[[arxiv](https://arxiv.org/abs/2502.06519)] [[cool](https://papers.cool/arxiv/2502.06519)] [[pdf](https://arxiv.org/pdf/2502.06519)]
> **Authors**: Ola Shorinwa,Jiankai Sun,Mac Schwager,Anirudha Majumdar
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,计算机视觉和模式识别
- **Abstract**: We present SIREN for registration of multi-robot Gaussian Splatting (GSplat) maps, with zero access to camera poses, images, and inter-map transforms for initialization or fusion of local submaps. To realize these capabilities, SIREN harnesses the versatility and robustness of semantics in three critical ways to derive a rigorous registration pipeline for multi-robot GSplat maps. First, SIREN utilizes semantics to identify feature-rich regions of the local maps where the registration problem is better posed, eliminating the need for any initialization which is generally required in prior work. Second, SIREN identifies candidate correspondences between Gaussians in the local maps using robust semantic features, constituting the foundation for robust geometric optimization, coarsely aligning 3D Gaussian primitives extracted from the local maps. Third, this key step enables subsequent photometric refinement of the transformation between the submaps, where SIREN leverages novel-view synthesis in GSplat maps along with a semantics-based image filter to compute a high-accuracy non-rigid transformation for the generation of a high-fidelity fused map. We demonstrate the superior performance of SIREN compared to competing baselines across a range of real-world datasets, and in particular, across the most widely-used robot hardware platforms, including a manipulator, drone, and quadruped. In our experiments, SIREN achieves about 90x smaller rotation errors, 300x smaller translation errors, and 44x smaller scale errors in the most challenging scenes, where competing methods struggle. We will release the code and provide a link to the project page after the review process.

### SIGMA: Sheaf-Informed Geometric Multi-Agent Pathfinding 
[[arxiv](https://arxiv.org/abs/2502.06440)] [[cool](https://papers.cool/arxiv/2502.06440)] [[pdf](https://arxiv.org/pdf/2502.06440)]
> **Authors**: Shuhao Liao,Weihang Xia,Yuhong Cao,Weiheng Dai,Chengyang He,Wenjun Wu,Guillaume Sartoretti
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Accepted for presentation at the 2025 IEEE International Conference on Robotics and Automation (ICRA)
- **标题**: None
- **领域**: 机器人技术,人工智能,多代理系统
- **Abstract**: The Multi-Agent Path Finding (MAPF) problem aims to determine the shortest and collision-free paths for multiple agents in a known, potentially obstacle-ridden environment. It is the core challenge for robotic deployments in large-scale logistics and transportation. Decentralized learning-based approaches have shown great potential for addressing the MAPF problems, offering more reactive and scalable solutions. However, existing learning-based MAPF methods usually rely on agents making decisions based on a limited field of view (FOV), resulting in short-sighted policies and inefficient cooperation in complex scenarios. There, a critical challenge is to achieve consensus on potential movements between agents based on limited observations and communications. To tackle this challenge, we introduce a new framework that applies sheaf theory to decentralized deep reinforcement learning, enabling agents to learn geometric cross-dependencies between each other through local consensus and utilize them for tightly cooperative decision-making. In particular, sheaf theory provides a mathematical proof of conditions for achieving global consensus through local observation. Inspired by this, we incorporate a neural network to approximately model the consensus in latent space based on sheaf theory and train it through self-supervised learning. During the task, in addition to normal features for MAPF as in previous works, each agent distributedly reasons about a learned consensus feature, leading to efficient cooperation on pathfinding and collision avoidance. As a result, our proposed method demonstrates significant improvements over state-of-the-art learning-based MAPF planners, especially in relatively large and complex scenarios, demonstrating its superiority over baselines in various simulations and real-world robot experiments.

## 声音(cs.SD:Sound)

### Vevo: Controllable Zero-Shot Voice Imitation with Self-Supervised Disentanglement 
[[arxiv](https://arxiv.org/abs/2502.07243)] [[cool](https://papers.cool/arxiv/2502.07243)] [[pdf](https://arxiv.org/pdf/2502.07243)]
> **Authors**: Xueyao Zhang,Xiaohui Zhang,Kainan Peng,Zhenyu Tang,Vimal Manohar,Yingru Liu,Jeff Hwang,Dangna Li,Yuhao Wang,Julian Chan,Yuan Huang,Zhizheng Wu,Mingbo Ma
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Accepted by ICLR 2025
- **标题**: None
- **领域**: 声音,人工智能
- **Abstract**: The imitation of voice, targeted on specific speech attributes such as timbre and speaking style, is crucial in speech generation. However, existing methods rely heavily on annotated data, and struggle with effectively disentangling timbre and style, leading to challenges in achieving controllable generation, especially in zero-shot scenarios. To address these issues, we propose Vevo, a versatile zero-shot voice imitation framework with controllable timbre and style. Vevo operates in two core stages: (1) Content-Style Modeling: Given either text or speech's content tokens as input, we utilize an autoregressive transformer to generate the content-style tokens, which is prompted by a style reference; (2) Acoustic Modeling: Given the content-style tokens as input, we employ a flow-matching transformer to produce acoustic representations, which is prompted by a timbre reference. To obtain the content and content-style tokens of speech, we design a fully self-supervised approach that progressively decouples the timbre, style, and linguistic content of speech. Specifically, we adopt VQ-VAE as the tokenizer for the continuous hidden features of HuBERT. We treat the vocabulary size of the VQ-VAE codebook as the information bottleneck, and adjust it carefully to obtain the disentangled speech representations. Solely self-supervised trained on 60K hours of audiobook speech data, without any fine-tuning on style-specific corpora, Vevo matches or surpasses existing methods in accent and emotion conversion tasks. Additionally, Vevo's effectiveness in zero-shot voice conversion and text-to-speech tasks further demonstrates its strong generalization and versatility. Audio samples are available at https://versavoice.github.io.

### Synthetic Audio Helps for Cognitive State Tasks 
[[arxiv](https://arxiv.org/abs/2502.06922)] [[cool](https://papers.cool/arxiv/2502.06922)] [[pdf](https://arxiv.org/pdf/2502.06922)]
> **Authors**: Adil Soubki,John Murzaku,Peter Zeng,Owen Rambow
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: John Murzaku and Adil Soubki contributed equally to this work
- **标题**: None
- **领域**: 声音,人工智能,计算语言学,机器学习
- **Abstract**: The NLP community has broadly focused on text-only approaches of cognitive state tasks, but audio can provide vital missing cues through prosody. We posit that text-to-speech models learn to track aspects of cognitive state in order to produce naturalistic audio, and that the signal audio models implicitly identify is orthogonal to the information that language models exploit. We present Synthetic Audio Data fine-tuning (SAD), a framework where we show that 7 tasks related to cognitive state modeling benefit from multimodal training on both text and zero-shot synthetic audio data from an off-the-shelf TTS system. We show an improvement over the text-only modality when adding synthetic audio data to text-only corpora. Furthermore, on tasks and corpora that do contain gold audio, we show our SAD framework achieves competitive performance with text and synthetic audio compared to text and gold audio.

### Evaluation of Deep Audio Representations for Hearables 
[[arxiv](https://arxiv.org/abs/2502.06664)] [[cool](https://papers.cool/arxiv/2502.06664)] [[pdf](https://arxiv.org/pdf/2502.06664)]
> **Authors**: Fabian Gröger,Pascal Baumann,Ludovic Amruthalingam,Laurent Simon,Ruksana Giurda,Simone Lionetti
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Accepted at International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2025)
- **标题**: None
- **领域**: 声音,人工智能,机器学习
- **Abstract**: Effectively steering hearable devices requires understanding the acoustic environment around the user. In the computational analysis of sound scenes, foundation models have emerged as the state of the art to produce high-performance, robust, multi-purpose audio representations. We introduce and release Deep Evaluation of Audio Representations (DEAR), the first dataset and benchmark to evaluate the efficacy of foundation models in capturing essential acoustic properties for hearables. The dataset includes 1,158 audio tracks, each 30 seconds long, created by spatially mixing proprietary monologues with commercial, high-quality recordings of everyday acoustic scenes. Our benchmark encompasses eight tasks that assess the general context, speech sources, and technical acoustic properties of the audio scenes. Through our evaluation of four general-purpose audio representation models, we demonstrate that the BEATs model significantly surpasses its counterparts. This superiority underscores the advantage of models trained on diverse audio collections, confirming their applicability to a wide array of auditory tasks, including encoding the environment properties necessary for hearable steering. The DEAR dataset and associated code are available at https://dear-dataset.github.io.

### Automatic Identification of Samples in Hip-Hop Music via Multi-Loss Training and an Artificial Dataset 
[[arxiv](https://arxiv.org/abs/2502.06364)] [[cool](https://papers.cool/arxiv/2502.06364)] [[pdf](https://arxiv.org/pdf/2502.06364)]
> **Authors**: Huw Cheston,Jan Van Balen,Simon Durand
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 17 pages, 6 figures
- **标题**: None
- **领域**: 声音,机器学习,音频和语音处理
- **Abstract**: Sampling, the practice of reusing recorded music or sounds from another source in a new work, is common in popular music genres like hip-hop and rap. Numerous services have emerged that allow users to identify connections between samples and the songs that incorporate them, with the goal of enhancing music discovery. Designing a system that can perform the same task automatically is challenging, as samples are commonly altered with audio effects like pitch- and time-stretching and may only be seconds long. Progress on this task has been minimal and is further blocked by the limited availability of training data. Here, we show that a convolutional neural network trained on an artificial dataset can identify real-world samples in commercial hip-hop music. We extract vocal, harmonic, and percussive elements from several databases of non-commercial music recordings using audio source separation, and train the model to fingerprint a subset of these elements in transformed versions of the original audio. We optimize the model using a joint classification and metric learning loss and show that it achieves 13% greater precision on real-world instances of sampling than a fingerprinting system using acoustic landmarks, and that it can recognize samples that have been both pitch shifted and time stretched. We also show that, for half of the commercial music recordings we tested, our model is capable of locating the position of a sample to within five seconds.

## 软件工程(cs.SE:Software Engineering)

### SnipGen: A Mining Repository Framework for Evaluating LLMs for Code 
[[arxiv](https://arxiv.org/abs/2502.07046)] [[cool](https://papers.cool/arxiv/2502.07046)] [[pdf](https://arxiv.org/pdf/2502.07046)]
> **Authors**: Daniel Rodriguez-Cardenas,Alejandro Velasco,Denys Poshyvanyk
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 5 pages, 3 figures, 2 tables
- **标题**: None
- **领域**: 软件工程,人工智能,机器学习
- **Abstract**: Language Models (LLMs), such as transformer-based neural networks trained on billions of parameters, have become increasingly prevalent in software engineering (SE). These models, trained on extensive datasets that include code repositories, exhibit remarkable capabilities for SE tasks. However, evaluating their effectiveness poses significant challenges, primarily due to the potential overlap between the datasets used for training and those employed for evaluation. To address this issue, we introduce SnipGen, a comprehensive repository mining framework designed to leverage prompt engineering across various downstream tasks for code generation. SnipGen aims to mitigate data contamination by generating robust testbeds and crafting tailored data points to assist researchers and practitioners in evaluating LLMs for code-related tasks. In our exploratory study, SnipGen mined approximately 227K data points from 338K recent code changes in GitHub commits, focusing on method-level granularity. SnipGen features a collection of prompt templates that can be combined to create a Chain-of-Thought-like sequence of prompts, enabling a nuanced assessment of LLMs' code generation quality. By providing the mining tool, the methodology, and the dataset, SnipGen empowers researchers and practitioners to rigorously evaluate and interpret LLMs' performance in software engineering contexts.

### SyncMind: Measuring Agent Out-of-Sync Recovery in Collaborative Software Engineering 
[[arxiv](https://arxiv.org/abs/2502.06994)] [[cool](https://papers.cool/arxiv/2502.06994)] [[pdf](https://arxiv.org/pdf/2502.06994)]
> **Authors**: Xuehang Guo,Xingyao Wang,Yangyi Chen,Sha Li,Chi Han,Manling Li,Heng Ji
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,人工智能,计算语言学
- **Abstract**: Software engineering (SE) is increasingly collaborative, with developers working together on shared complex codebases. Effective collaboration in shared environments requires participants -- whether humans or AI agents -- to stay on the same page as their environment evolves. When a collaborator's understanding diverges from the current state -- what we term the out-of-sync challenge -- the collaborator's actions may fail, leading to integration issues. In this work, we introduce SyncMind, a framework that systematically defines the out-of-sync problem faced by large language model (LLM) agents in collaborative software engineering (CSE). Based on SyncMind, we create SyncBench, a benchmark featuring 24,332 instances of agent out-of-sync scenarios in real-world CSE derived from 21 popular GitHub repositories with executable verification tests. Experiments on SyncBench uncover critical insights into existing LLM agents' capabilities and limitations. Besides substantial performance gaps among agents (from Llama-3.1 agent <= 3.33% to Claude-3.5-Sonnet >= 28.18%), their consistently low collaboration willingness (<= 4.86%) suggests fundamental limitations of existing LLM in CSE. However, when collaboration occurs, it positively correlates with out-of-sync recovery success. Minimal performance differences in agents' resource-aware out-of-sync recoveries further reveal their significant lack of resource awareness and adaptability, shedding light on future resource-efficient collaborative systems. Code and data are openly available on our project website: https://xhguo7.github.io/SyncMind/.

### Large Language Models for In-File Vulnerability Localization Can Be "Lost in the End" 
[[arxiv](https://arxiv.org/abs/2502.06898)] [[cool](https://papers.cool/arxiv/2502.06898)] [[pdf](https://arxiv.org/pdf/2502.06898)]
> **Authors**: Francesco Sovrano,Adam Bauer,Alberto Bacchelli
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-11
> **comment**: Accepted for publication at the ACM International Conference on the Foundations of Software Engineering (FSE) 2025. Replication Package: https://doi.org/10.5281/zenodo.14840519
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: Recent advancements in artificial intelligence have enabled processing of larger inputs, leading everyday software developers to increasingly rely on chat-based large language models (LLMs) like GPT-3.5 and GPT-4 to detect vulnerabilities across entire files, not just within functions. This new development practice requires researchers to urgently investigate whether commonly used LLMs can effectively analyze large file-sized inputs, in order to provide timely insights for software developers and engineers about the pros and cons of this emerging technological trend. Hence, the goal of this paper is to evaluate the effectiveness of several state-of-the-art chat-based LLMs, including the GPT models, in detecting in-file vulnerabilities. We conducted a costly investigation into how the performance of LLMs varies based on vulnerability type, input size, and vulnerability location within the file. To give enough statistical power to our study, we could only focus on the three most common (as well as dangerous) vulnerabilities: XSS, SQL injection, and path traversal. Our findings indicate that the effectiveness of LLMs in detecting these vulnerabilities is strongly influenced by both the location of the vulnerability and the overall size of the input. Specifically, regardless of the vulnerability type, LLMs tend to significantly (p < .05) underperform when detecting vulnerabilities located toward the end of larger files, a pattern we call the 'lost-in-the-end' effect. Finally, to further support software developers and practitioners, we also explored the optimal input size for these LLMs and presented a simple strategy for identifying it, which can be applied to other models and vulnerability types. Eventually, we show how adjusting the input size can lead to significant improvements in LLM-based vulnerability detection, with an average recall increase of over 37% across all models.

### Combining Large Language Models with Static Analyzers for Code Review Generation 
[[arxiv](https://arxiv.org/abs/2502.06633)] [[cool](https://papers.cool/arxiv/2502.06633)] [[pdf](https://arxiv.org/pdf/2502.06633)]
> **Authors**: Imen Jaoua,Oussama Ben Sghaier,Houari Sahraoui
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: Code review is a crucial but often complex, subjective, and time-consuming activity in software development. Over the past decades, significant efforts have been made to automate this process. Early approaches focused on knowledge-based systems (KBS) that apply rule-based mechanisms to detect code issues, providing precise feedback but struggling with complex, context-dependent cases. More recent work has shifted toward fine-tuning pre-trained language models for code review, enabling broader issue coverage but often at the expense of precision. In this paper, we propose a hybrid approach that combines the strengths of KBS and learning-based systems (LBS) to generate high-quality, comprehensive code reviews. Our method integrates knowledge at three distinct stages of the language model pipeline: during data preparation (Data-Augmented Training, DAT), at inference (Retrieval-Augmented Generation, RAG), and after inference (Naive Concatenation of Outputs, NCO). We empirically evaluate our combination strategies against standalone KBS and LBS fine-tuned on a real-world dataset. Our results show that these hybrid strategies enhance the relevance, completeness, and overall quality of review comments, effectively bridging the gap between rule-based tools and deep learning models.

### evclust: Python library for evidential clustering 
[[arxiv](https://arxiv.org/abs/2502.06587)] [[cool](https://papers.cool/arxiv/2502.06587)] [[pdf](https://arxiv.org/pdf/2502.06587)]
> **Authors**: Armel Soubeiga,Violaine Antoine
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 13 pages, 2 figures, Preprint
- **标题**: None
- **领域**: 软件工程,计算机视觉和模式识别,机器学习
- **Abstract**: A recent developing trend in clustering is the advancement of algorithms that not only identify clusters within data, but also express and capture the uncertainty of cluster membership. Evidential clustering addresses this by using the Dempster-Shafer theory of belief functions, a framework designed to manage and represent uncertainty. This approach results in a credal partition, a structured set of mass functions that quantify the uncertain assignment of each object to potential groups. The Python framework evclust, presented in this paper, offers a suite of efficient evidence clustering algorithms as well as tools for visualizing, evaluating and analyzing credal partitions.

### ProjectTest: A Project-level LLM Unit Test Generation Benchmark and Impact of Error Fixing Mechanisms 
[[arxiv](https://arxiv.org/abs/2502.06556)] [[cool](https://papers.cool/arxiv/2502.06556)] [[pdf](https://arxiv.org/pdf/2502.06556)]
> **Authors**: Yibo Wang,Congying Xia,Wenting Zhao,Jiangshu Du,Chunyu Miao,Zhongfen Deng,Philip S. Yu,Chen Xing
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,计算语言学
- **Abstract**: Unit test generation has become a promising and important use case of LLMs. However, existing evaluation benchmarks for assessing LLM unit test generation capabilities focus on function- or class-level code rather than more practical and challenging project-level codebases. To address such limitation, we propose ProjectTest, a project-level benchmark for unit test generation covering Python, Java, and JavaScript. ProjectTest features 20 moderate-sized and high-quality projects per language. We evaluate nine frontier LLMs on ProjectTest and the results show that all frontier LLMs tested exhibit moderate performance on ProjectTest on Python and Java, highlighting the difficulty of ProjectTest. We also conduct a thorough error analysis, which shows that even frontier LLMs, such as Claude-3.5-Sonnet, have significant basic yet critical errors, including compilation and cascade errors. Motivated by this observation, we further evaluate all frontier LLMs under manual error-fixing and self-error-fixing scenarios to assess their potential when equipped with error-fixing mechanisms. Our code and dataset is available at \href{https://github.com/YiboWANG214/ProjectTest}{ProjectTest}.

### Testing software for non-discrimination: an updated and extended audit in the Italian car insurance domain 
[[arxiv](https://arxiv.org/abs/2502.06439)] [[cool](https://papers.cool/arxiv/2502.06439)] [[pdf](https://arxiv.org/pdf/2502.06439)]
> **Authors**: Marco Rondina,Antonio Vetrò,Riccardo Coppola,Oumaima Regragrui,Alessandro Fabris,Gianmaria Silvello,Gian Antonio Susto,Juan Carlos De Martin
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 14 pages, 1 figure
- **标题**: None
- **领域**: 软件工程,人工智能,人机交互,机器学习
- **Abstract**: Context. As software systems become more integrated into society's infrastructure, the responsibility of software professionals to ensure compliance with various non-functional requirements increases. These requirements include security, safety, privacy, and, increasingly, non-discrimination. Motivation. Fairness in pricing algorithms grants equitable access to basic services without discriminating on the basis of protected attributes. Method. We replicate a previous empirical study that used black box testing to audit pricing algorithms used by Italian car insurance companies, accessible through a popular online system. With respect to the previous study, we enlarged the number of tests and the number of demographic variables under analysis. Results. Our work confirms and extends previous findings, highlighting the problematic permanence of discrimination across time: demographic variables significantly impact pricing to this day, with birthplace remaining the main discriminatory factor against individuals not born in Italian cities. We also found that driver profiles can determine the number of quotes available to the user, denying equal opportunities to all. Conclusion. The study underscores the importance of testing for non-discrimination in software systems that affect people's everyday lives. Performing algorithmic audits over time makes it possible to evaluate the evolution of such algorithms. It also demonstrates the role that empirical software engineering can play in making software systems more accountable.

### LessLeak-Bench: A First Investigation of Data Leakage in LLMs Across 83 Software Engineering Benchmarks 
[[arxiv](https://arxiv.org/abs/2502.06215)] [[cool](https://papers.cool/arxiv/2502.06215)] [[pdf](https://arxiv.org/pdf/2502.06215)]
> **Authors**: Xin Zhou,Martin Weyssow,Ratnadira Widyasari,Ting Zhang,Junda He,Yunbo Lyu,Jianming Chang,Beiqi Zhang,Dan Huang,David Lo
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 25 pages
- **标题**: None
- **领域**: 软件工程,人工智能,计算语言学
- **Abstract**: Large Language Models (LLMs) are widely utilized in software engineering (SE) tasks, such as code generation and automated program repair. However, their reliance on extensive and often undisclosed pre-training datasets raises significant concerns about data leakage, where the evaluation benchmark data is unintentionally ``seen'' by LLMs during the model's construction phase. The data leakage issue could largely undermine the validity of LLM-based research and evaluations. Despite the increasing use of LLMs in the SE community, there is no comprehensive study that assesses the extent of data leakage in SE benchmarks for LLMs yet. To address this gap, this paper presents the first large-scale analysis of data leakage in 83 SE benchmarks concerning LLMs. Our results show that in general, data leakage in SE benchmarks is minimal, with average leakage ratios of only 4.8\%, 2.8\%, and 0.7\% for Python, Java, and C/C++ benchmarks, respectively. However, some benchmarks exhibit relatively higher leakage ratios, which raises concerns about their bias in evaluation. For instance, QuixBugs and BigCloneBench have leakage ratios of 100.0\% and 55.7\%, respectively. Furthermore, we observe that data leakage has a substantial impact on LLM evaluation. We also identify key causes of high data leakage, such as the direct inclusion of benchmark data in pre-training datasets and the use of coding platforms like LeetCode for benchmark construction. To address the data leakage, we introduce \textbf{LessLeak-Bench}, a new benchmark that removes leaked samples from the 83 SE benchmarks, enabling more reliable LLM evaluations in future research. Our study enhances the understanding of data leakage in SE benchmarks and provides valuable insights for future research involving LLMs in SE.

### Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering 
[[arxiv](https://arxiv.org/abs/2502.06193)] [[cool](https://papers.cool/arxiv/2502.06193)] [[pdf](https://arxiv.org/pdf/2502.06193)]
> **Authors**: Ruiqi Wang,Jiyu Guo,Cuiyun Gao,Guodong Fan,Chun Yong Chong,Xin Xia
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Accepted by ISSTA 2025
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: Recently, large language models (LLMs) have been deployed to tackle various software engineering (SE) tasks like code generation, significantly advancing the automation of SE tasks. However, assessing the quality of these LLM-generated code and text remains challenging. The commonly used Pass@k metric necessitates extensive unit tests and configured environments, demands a high labor cost, and is not suitable for evaluating LLM-generated text. Conventional metrics like BLEU, which measure only lexical rather than semantic similarity, have also come under scrutiny. In response, a new trend has emerged to employ LLMs for automated evaluation, known as LLM-as-a-judge. These LLM-as-a-judge methods are claimed to better mimic human assessment than conventional metrics without relying on high-quality reference answers. Nevertheless, their exact human alignment in SE tasks remains unexplored. In this paper, we empirically explore LLM-as-a-judge methods for evaluating SE tasks, focusing on their alignment with human judgments. We select seven LLM-as-a-judge methods that utilize general-purpose LLMs, alongside two LLMs specifically fine-tuned for evaluation. After generating and manually scoring LLM responses on three recent SE datasets of code translation, code generation, and code summarization, we then prompt these methods to evaluate each response. Finally, we compare the scores generated by these methods with human evaluation. The results indicate that output-based methods reach the highest Pearson correlation of 81.32 and 68.51 with human scores in code translation and generation, achieving near-human evaluation, noticeably outperforming ChrF++, one of the best conventional metrics, at 34.23 and 64.92. Such output-based methods prompt LLMs to output judgments directly, and exhibit more balanced score distributions that resemble human score patterns. Finally, we provide...

## 音频和语音处理(eess.AS:Audio and Speech Processing)

### VINP: Variational Bayesian Inference with Neural Speech Prior for Joint ASR-Effective Speech Dereverberation and Blind RIR Identification 
[[arxiv](https://arxiv.org/abs/2502.07205)] [[cool](https://papers.cool/arxiv/2502.07205)] [[pdf](https://arxiv.org/pdf/2502.07205)]
> **Authors**: Pengyu Wang,Ying Fang,Xiaofei Li
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Submitted to IEEE/ACM Trans. on TASLP
- **标题**: None
- **领域**: 音频和语音处理,人工智能,机器学习
- **Abstract**: Reverberant speech, denoting the speech signal degraded by the process of reverberation, contains crucial knowledge of both anechoic source speech and room impulse response (RIR). This work proposes a variational Bayesian inference (VBI) framework with neural speech prior (VINP) for joint speech dereverberation and blind RIR identification. In VINP, a probabilistic signal model is constructed in the time-frequency (T-F) domain based on convolution transfer function (CTF) approximation. For the first time, we propose using an arbitrary discriminative dereverberation deep neural network (DNN) to predict the prior distribution of anechoic speech within a probabilistic model. By integrating both reverberant speech and the anechoic speech prior, VINP yields the maximum a posteriori (MAP) and maximum likelihood (ML) estimations of the anechoic speech spectrum and CTF filter, respectively. After simple transformations, the waveforms of anechoic speech and RIR are estimated. Moreover, VINP is effective for automatic speech recognition (ASR) systems, which sets it apart from most deep learning (DL)-based single-channel dereverberation approaches. Experiments on single-channel speech dereverberation demonstrate that VINP reaches an advanced level in most metrics related to human perception and displays unquestionable state-of-the-art (SOTA) performance in ASR-related metrics. For blind RIR identification, experiments indicate that VINP attains the SOTA level in blind estimation of reverberation time at 60 dB (RT60) and direct-to-reverberation ratio (DRR). Codes and audio samples are available online.

### Recent Advances in Discrete Speech Tokens: A Review 
[[arxiv](https://arxiv.org/abs/2502.06490)] [[cool](https://papers.cool/arxiv/2502.06490)] [[pdf](https://arxiv.org/pdf/2502.06490)]
> **Authors**: Yiwei Guo,Zhihan Li,Hankun Wang,Bohan Li,Chongtian Shao,Hanglei Zhang,Chenpeng Du,Xie Chen,Shujie Liu,Kai Yu
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 23 pages, 8 figures, 3 tables. Work in progress
- **标题**: None
- **领域**: 音频和语音处理,人工智能,多媒体,声音,信号处理
- **Abstract**: The rapid advancement of speech generation technologies in the era of large language models (LLMs) has established discrete speech tokens as a foundational paradigm for speech representation. These tokens, characterized by their discrete, compact, and concise nature, are not only advantageous for efficient transmission and storage, but also inherently compatible with the language modeling framework, enabling seamless integration of speech into text-dominated LLM architectures. Current research categorizes discrete speech tokens into two principal classes: acoustic tokens and semantic tokens, each of which has evolved into a rich research domain characterized by unique design philosophies and methodological approaches. This survey systematically synthesizes the existing taxonomy and recent innovations in discrete speech tokenization, conducts a critical examination of the strengths and limitations of each paradigm, and presents systematic experimental comparisons across token types. Furthermore, we identify persistent challenges in the field and propose potential research directions, aiming to offer actionable insights to inspire future advancements in the development and application of discrete speech tokens.

## 图像和视频处理(eess.IV:Image and Video Processing)

### Color-Quality Invariance for Robust Medical Image Segmentation 
[[arxiv](https://arxiv.org/abs/2502.07200)] [[cool](https://papers.cool/arxiv/2502.07200)] [[pdf](https://arxiv.org/pdf/2502.07200)]
> **Authors**: Ravi Shah,Atsushi Fukuda,Quan Huu Cap
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: Single-source domain generalization (SDG) in medical image segmentation remains a significant challenge, particularly for images with varying color distributions and qualities. Previous approaches often struggle when models trained on high-quality images fail to generalize to low-quality test images due to these color and quality shifts. In this work, we propose two novel techniques to enhance generalization: dynamic color image normalization (DCIN) module and color-quality generalization (CQG) loss. The DCIN dynamically normalizes the color of test images using two reference image selection strategies. Specifically, the DCIN utilizes a global reference image selection (GRIS), which finds a universal reference image, and a local reference image selection (LRIS), which selects a semantically similar reference image per test sample. Additionally, CQG loss enforces invariance to color and quality variations by ensuring consistent segmentation predictions across transformed image pairs. Experimental results show that our proposals significantly improve segmentation performance over the baseline on two target domain datasets, despite being trained solely on a single source domain. Notably, our model achieved up to a 32.3-point increase in Dice score compared to the baseline, consistently producing robust and usable results even under substantial domain shifts. Our work contributes to the development of more robust medical image segmentation models that generalize across unseen domains. The implementation code is available at https://github.com/RaviShah1/DCIN-CQG.

### Choroidal image analysis for OCT image sequences with applications in systemic health 
[[arxiv](https://arxiv.org/abs/2502.07117)] [[cool](https://papers.cool/arxiv/2502.07117)] [[pdf](https://arxiv.org/pdf/2502.07117)]
> **Authors**: Jamie Burke
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: PhD thesis toward a doctorate degree at the University of Edinburgh. PhD funded by the Medical Research Council (grant MR/N013166/1). Reviewed and examined by Dr. Roly Megaw (internal) and Prof. Pearse Keane (external) in December 2024 and ratified in the same month by the university. Official record found here: https://era.ed.ac.uk/handle/1842/42956
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习,数学软件
- **Abstract**: The choroid, a highly vascular layer behind the retina, is an extension of the central nervous system and has parallels with the renal cortex, with blood flow far exceeding that of the brain and kidney. Thus, there has been growing interest of choroidal blood flow reflecting physiological status of systemic disease. Optical coherence tomography (OCT) enables high-resolution imaging of the choroid, but conventional analysis methods remain manual or semi-automatic, limiting reproducibility, standardisation and clinical utility. In this thesis, I develop several new methods to analyse the choroid in OCT image sequences, with each successive method improving on its predecessors. I first develop two semi-automatic approaches for choroid region (Gaussian Process Edge Tracing, GPET) and vessel (Multi-scale Median Cut Quantisation, MMCQ) analysis, which improve on manual approaches but remain user-dependent. To address this, I introduce DeepGPET, a deep learning-based region segmentation method which improves on execution time, reproducibility, and end-user accessibility, but lacks choroid vessel analysis and automatic feature measurement. Improving on this, I developed Choroidalyzer, a deep learning-based pipeline to segment the choroidal space and vessels and generate fully automatic, clinically meaningful and reproducible choroidal features. I provide rigorous evaluation of these four approaches and consider their potential clinical value in three applications into systemic health: OCTANE, assessing choroidal changes in renal transplant recipients and donors; PREVENT, exploring choroidal associations with Alzheimer's risk factors at mid-life; D-RISCii, assessing choroidal variation and feasibility of OCT in critical care. In short, this thesis contributes many open-source tools for standardised choroidal measurement and highlights the choroid's potential as a biomarker in systemic health.

### Conditional diffusion model with spatial attention and latent embedding for medical image segmentation 
[[arxiv](https://arxiv.org/abs/2502.06997)] [[cool](https://papers.cool/arxiv/2502.06997)] [[pdf](https://arxiv.org/pdf/2502.06997)]
> **Authors**: Behzad Hejrati,Soumyanil Banerjee,Carri Glide-Hurst,Ming Dong
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 13 pages, 5 figures, 3 tables, Accepted in MICCAI 2024
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: Diffusion models have been used extensively for high quality image and video generation tasks. In this paper, we propose a novel conditional diffusion model with spatial attention and latent embedding (cDAL) for medical image segmentation. In cDAL, a convolutional neural network (CNN) based discriminator is used at every time-step of the diffusion process to distinguish between the generated labels and the real ones. A spatial attention map is computed based on the features learned by the discriminator to help cDAL generate more accurate segmentation of discriminative regions in an input image. Additionally, we incorporated a random latent embedding into each layer of our model to significantly reduce the number of training and sampling time-steps, thereby making it much faster than other diffusion models for image segmentation. We applied cDAL on 3 publicly available medical image segmentation datasets (MoNuSeg, Chest X-ray and Hippocampus) and observed significant qualitative and quantitative improvements with higher Dice scores and mIoU over the state-of-the-art algorithms. The source code is publicly available at https://github.com/Hejrati/cDAL/.

### Generalizable automated ischaemic stroke lesion segmentation with vision transformers 
[[arxiv](https://arxiv.org/abs/2502.06939)] [[cool](https://papers.cool/arxiv/2502.06939)] [[pdf](https://arxiv.org/pdf/2502.06939)]
> **Authors**: Chris Foulon,Robert Gray,James K. Ruffle,Jonathan Best,Tianbo Xu,Henry Watkins,Jane Rondina,Guilherme Pombo,Dominic Giles,Paul Wright,Marcela Ovando-Tellez,H. Rolf Jäger,Jorge Cardoso,Sebastien Ourselin,Geraint Rees,Parashkev Nachev
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 29 pages, 7 figures, 2 tables, 1 supplementary table, 2 supplementary figures
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习
- **Abstract**: Ischaemic stroke, a leading cause of death and disability, critically relies on neuroimaging for characterising the anatomical pattern of injury. Diffusion-weighted imaging (DWI) provides the highest expressivity in ischemic stroke but poses substantial challenges for automated lesion segmentation: susceptibility artefacts, morphological heterogeneity, age-related comorbidities, time-dependent signal dynamics, instrumental variability, and limited labelled data. Current U-Net-based models therefore underperform, a problem accentuated by inadequate evaluation metrics that focus on mean performance, neglecting anatomical, subpopulation, and acquisition-dependent variability. Here, we present a high-performance DWI lesion segmentation tool addressing these challenges through optimized vision transformer-based architectures, integration of 3563 annotated lesions from multi-site data, and algorithmic enhancements, achieving state-of-the-art results. We further propose a novel evaluative framework assessing model fidelity, equity (across demographics and lesion subtypes), anatomical precision, and robustness to instrumental variability, promoting clinical and research utility. This work advances stroke imaging by reconciling model expressivity with domain-specific challenges and redefining performance benchmarks to prioritize equity and generalizability, critical for personalized medicine and mechanistic research.

### Direct Estimation of Pediatric Heart Rate Variability from BOLD-fMRI: A Machine Learning Approach Using Dynamic Connectivity 
[[arxiv](https://arxiv.org/abs/2502.06920)] [[cool](https://papers.cool/arxiv/2502.06920)] [[pdf](https://arxiv.org/pdf/2502.06920)]
> **Authors**: Abdoljalil Addeh,Karen Ardila,Rebecca J Williams,G. Bruce Pike,M. Ethan MacDonald
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 5 pages, 5 figures, ISMSMR 2025
- **标题**: None
- **领域**: 图像和视频处理,人工智能,机器学习
- **Abstract**: In many pediatric fMRI studies, cardiac signals are often missing or of poor quality. A tool to extract Heart Rate Variation (HRV) waveforms directly from fMRI data, without the need for peripheral recording devices, would be highly beneficial. We developed a machine learning framework to accurately reconstruct HRV for pediatric applications. A hybrid model combining one-dimensional Convolutional Neural Networks (1D-CNN) and Gated Recurrent Units (GRU) analyzed BOLD signals from 628 ROIs, integrating past and future data. The model achieved an 8% improvement in HRV accuracy, as evidenced by enhanced performance metrics. This approach eliminates the need for peripheral photoplethysmography devices, reduces costs, and simplifies procedures in pediatric fMRI. Additionally, it improves the robustness of pediatric fMRI studies, which are more sensitive to physiological and developmental variations than those in adults.

### A Comprehensive Review of U-Net and Its Variants: Advances and Applications in Medical Image Segmentation 
[[arxiv](https://arxiv.org/abs/2502.06895)] [[cool](https://papers.cool/arxiv/2502.06895)] [[pdf](https://arxiv.org/pdf/2502.06895)]
> **Authors**: Wang Jiangtao,Nur Intan Raihana Ruhaiyem,Fu Panpan
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-11
> **comment**: 36 pages,26 figures,7 tables
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: Medical images often exhibit low and blurred contrast between lesions and surrounding tissues, with considerable variation in lesion edges and shapes even within the same disease, leading to significant challenges in segmentation. Therefore, precise segmentation of lesions has become an essential prerequisite for patient condition assessment and formulation of treatment plans. Significant achievements have been made in research related to the U-Net model in recent years. It improves segmentation performance and is extensively applied in the semantic segmentation of medical images to offer technical support for consistent quantitative lesion analysis methods. First, this paper classifies medical image datasets on the basis of their imaging modalities and then examines U-Net and its various improvement models from the perspective of structural modifications. The research objectives, innovative designs, and limitations of each approach are discussed in detail. Second, we summarize the four central improvement mechanisms of the U-Net and U-Net variant algorithms: the jump-connection mechanism, residual-connection mechanism, 3D-UNet, and transformer mechanism. Finally, we examine the relationships among the four core enhancement mechanisms and commonly utilized medical datasets and propose potential avenues and strategies for future advancements. This paper provides a systematic summary and reference for researchers in related fields, and we look forward to designing more efficient and stable medical image segmentation network models based on the U-Net network.

### Diffusion-empowered AutoPrompt MedSAM 
[[arxiv](https://arxiv.org/abs/2502.06817)] [[cool](https://papers.cool/arxiv/2502.06817)] [[pdf](https://arxiv.org/pdf/2502.06817)]
> **Authors**: Peng Huang,Shu Hu,Bo Peng,Jiashu Zhang,Hongtu Zhu,Xi Wu,Xin Wang
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,图形,机器学习
- **Abstract**: MedSAM, a medical foundation model derived from the SAM architecture, has demonstrated notable success across diverse medical domains. However, its clinical application faces two major challenges: the dependency on labor-intensive manual prompt generation, which imposes a significant burden on clinicians, and the absence of semantic labeling in the generated segmentation masks for organs or lesions, limiting its practicality for non-expert users. To address these limitations, we propose AutoMedSAM, an end-to-end framework derived from SAM, designed to enhance usability and segmentation performance. AutoMedSAM retains MedSAM's image encoder and mask decoder structure while introducing a novel diffusion-based class prompt encoder. The diffusion-based encoder employs a dual-decoder structure to collaboratively generate prompt embeddings guided by sparse and dense prompt definitions. These embeddings enhance the model's ability to understand and process clinical imagery autonomously. With this encoder, AutoMedSAM leverages class prompts to embed semantic information into the model's predictions, transforming MedSAM's semi-automated pipeline into a fully automated workflow. Furthermore, AutoMedSAM employs an uncertainty-aware joint optimization strategy during training to effectively inherit MedSAM's pre-trained knowledge while improving generalization by integrating multiple loss functions. Experimental results across diverse datasets demonstrate that AutoMedSAM achieves superior performance while broadening its applicability to both clinical settings and non-expert users. Code is available at https://github.com/HP-ML/AutoPromptMedSAM.git.

### Is an Ultra Large Natural Image-Based Foundation Model Superior to a Retina-Specific Model for Detecting Ocular and Systemic Diseases? 
[[arxiv](https://arxiv.org/abs/2502.06289)] [[cool](https://papers.cool/arxiv/2502.06289)] [[pdf](https://arxiv.org/pdf/2502.06289)]
> **Authors**: Qingshan Hou,Yukun Zhou,Jocelyn Hui Lin Goh,Ke Zou,Samantha Min Er Yew,Sahana Srinivasan,Meng Wang,Thaddaeus Lo,Xiaofeng Lei,Siegfried K. Wagner,Mark A. Chia,Dawei Yang,Hongyang Jiang,AnRan Ran,Rui Santos,Gabor Mark Somfai,Juan Helen Zhou,Haoyu Chen,Qingyu Chen,Carol Yim-Lui Cheung,Pearse A. Keane,Yih Chung Tham
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **Abstract**: The advent of foundation models (FMs) is transforming medical domain. In ophthalmology, RETFound, a retina-specific FM pre-trained sequentially on 1.4 million natural images and 1.6 million retinal images, has demonstrated high adaptability across clinical applications. Conversely, DINOv2, a general-purpose vision FM pre-trained on 142 million natural images, has shown promise in non-medical domains. However, its applicability to clinical tasks remains underexplored. To address this, we conducted head-to-head evaluations by fine-tuning RETFound and three DINOv2 models (large, base, small) for ocular disease detection and systemic disease prediction tasks, across eight standardized open-source ocular datasets, as well as the Moorfields AlzEye and the UK Biobank datasets. DINOv2-large model outperformed RETFound in detecting diabetic retinopathy (AUROC=0.850-0.952 vs 0.823-0.944, across three datasets, all P<=0.007) and multi-class eye diseases (AUROC=0.892 vs. 0.846, P<0.001). In glaucoma, DINOv2-base model outperformed RETFound (AUROC=0.958 vs 0.940, P<0.001). Conversely, RETFound achieved superior performance over all DINOv2 models in predicting heart failure, myocardial infarction, and ischaemic stroke (AUROC=0.732-0.796 vs 0.663-0.771, all P<0.001). These trends persisted even with 10% of the fine-tuning data. These findings showcase the distinct scenarios where general-purpose and domain-specific FMs excel, highlighting the importance of aligning FM selection with task-specific requirements to optimise clinical performance.

## 信号处理(eess.SP:Signal Processing)

### Estimation of Food Intake Quantity Using Inertial Signals from Smartwatches 
[[arxiv](https://arxiv.org/abs/2502.06649)] [[cool](https://papers.cool/arxiv/2502.06649)] [[pdf](https://arxiv.org/pdf/2502.06649)]
> **Authors**: Ioannis Levi,Konstantinos Kyritsis,Vasileios Papapanagiotou,Georgios Tsakiridis,Anastasios Delopoulos
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: Manuscript submitted for review to 47th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC) 2025
- **标题**: None
- **领域**: 信号处理,机器学习
- **Abstract**: Accurate monitoring of eating behavior is crucial for managing obesity and eating disorders such as bulimia nervosa. At the same time, existing methods rely on multiple and/or specialized sensors, greatly harming adherence and ultimately, the quality and continuity of data. This paper introduces a novel approach for estimating the weight of a bite, from a commercial smartwatch. Our publicly-available dataset contains smartwatch inertial data from ten participants, with manually annotated start and end times of each bite along with their corresponding weights from a smart scale, under semi-controlled conditions. The proposed method combines extracted behavioral features such as the time required to load the utensil with food, with statistical features of inertial signals, that serve as input to a Support Vector Regression model to estimate bite weights. Under a leave-one-subject-out cross-validation scheme, our approach achieves a mean absolute error (MAE) of 3.99 grams per bite. To contextualize this performance, we introduce the improvement metric, that measures the relative MAE difference compared to a baseline model. Our method demonstrates a 17.41% improvement, while the adapted state-of-the art method shows a -28.89% performance against that same baseline. The results presented in this work establish the feasibility of extracting meaningful bite weight estimates from commercial smartwatch inertial sensors alone, laying the groundwork for future accessible, non-invasive dietary monitoring systems.

## 高能物理-实验(hep-ex:High Energy Physics - Experiment)

### Unsupervised Particle Tracking with Neuromorphic Computing 
[[arxiv](https://arxiv.org/abs/2502.06771)] [[cool](https://papers.cool/arxiv/2502.06771)] [[pdf](https://arxiv.org/pdf/2502.06771)]
> **Authors**: Emanuele Coradin,Fabio Cufino,Muhammad Awais,Tommaso Dorigo,Enrico Lupi,Eleonora Porcu,Jinu Raj,Fredrik Sandin,Mia Tosi
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 24 pages, 21 figures, submitted to MDPI Particles
- **标题**: None
- **领域**: 高能物理-实验,新兴技术,机器学习,神经和进化计算
- **Abstract**: We study the application of a neural network architecture for identifying charged particle trajectories via unsupervised learning of delays and synaptic weights using a spike-time-dependent plasticity rule. In the considered model, the neurons receive time-encoded information on the position of particle hits in a tracking detector for a particle collider, modeled according to the geometry of the Compact Muon Solenoid Phase II detector. We show how a spiking neural network is capable of successfully identifying in a completely unsupervised way the signal left by charged particles in the presence of conspicuous noise from accidental or combinatorial hits. These results open the way to applications of neuromorphic computing to particle tracking, motivating further studies into its potential for real-time, low-power particle tracking in future high-energy physics experiments.

## 数值分析(math.NA:Numerical Analysis)

### Surrogate models for diffusion on graphs via sparse polynomials 
[[arxiv](https://arxiv.org/abs/2502.06595)] [[cool](https://papers.cool/arxiv/2502.06595)] [[pdf](https://arxiv.org/pdf/2502.06595)]
> **Authors**: Giuseppe Alessio D'Inverno,Kylian Ajavon,Simone Brugiapaglia
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: :34B45; 41A10; 41A63; 65D40
- **标题**: None
- **领域**: 数值分析,机器学习
- **Abstract**: Diffusion kernels over graphs have been widely utilized as effective tools in various applications due to their ability to accurately model the flow of information through nodes and edges. However, there is a notable gap in the literature regarding the development of surrogate models for diffusion processes on graphs. In this work, we fill this gap by proposing sparse polynomial-based surrogate models for parametric diffusion equations on graphs with community structure. In tandem, we provide convergence guarantees for both least squares and compressed sensing-based approximations by showing the holomorphic regularity of parametric solutions to these diffusion equations. Our theoretical findings are accompanied by a series of numerical experiments conducted on both synthetic and real-world graphs that demonstrate the applicability of our methodology.

## 优化与控制(math.OC:Optimization and Control)

### Dual Conic Proxy for Semidefinite Relaxation of AC Optimal Power Flow 
[[arxiv](https://arxiv.org/abs/2502.06978)] [[cool](https://papers.cool/arxiv/2502.06978)] [[pdf](https://arxiv.org/pdf/2502.06978)]
> **Authors**: Guancheng Qiu,Mathieu Tanneau,Pascal Van Hentenryck
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 优化与控制,机器学习
- **Abstract**: The nonlinear, non-convex AC Optimal Power Flow (AC-OPF) problem is fundamental for power systems operations. The intrinsic complexity of AC-OPF has fueled a growing interest in the development of optimization proxies for the problem, i.e., machine learning models that predict high-quality, close-to-optimal solutions. More recently, dual conic proxy architectures have been proposed, which combine machine learning and convex relaxations of AC-OPF, to provide valid certificates of optimality using learning-based methods. Building on this methodology, this paper proposes, for the first time, a dual conic proxy architecture for the semidefinite (SDP) relaxation of AC-OPF problems. Although the SDP relaxation is stronger than the second-order cone relaxation considered in previous work, its practical use has been hindered by its computational cost. The proposed method combines a neural network with a differentiable dual completion strategy that leverages the structure of the dual SDP problem. This approach guarantees dual feasibility, and therefore valid dual bounds, while providing orders of magnitude of speedups compared to interior-point algorithms. The paper also leverages self-supervised learning, which alleviates the need for time-consuming data generation and allows to train the proposed models efficiently. Numerical experiments are presented on several power grid benchmarks with up to 500 buses. The results demonstrate that the proposed SDP-based proxies can outperform weaker conic relaxations, while providing several orders of magnitude speedups compared to a state-of-the-art interior-point SDP solver.

### Bayesian Optimization by Kernel Regression and Density-based Exploration 
[[arxiv](https://arxiv.org/abs/2502.06178)] [[cool](https://papers.cool/arxiv/2502.06178)] [[pdf](https://arxiv.org/pdf/2502.06178)]
> **Authors**: Tansheng Zhu,Hongyu Zhou,Ke Jin,Xusheng Xu,Qiufan Yuan,Lijie Ji
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 优化与控制,机器学习,机器学习
- **Abstract**: Bayesian optimization is highly effective for optimizing expensive-to-evaluate black-box functions, but it faces significant computational challenges due to the high computational complexity of Gaussian processes, which results in a total time complexity that is quartic with respect to the number of iterations. To address this limitation, we propose the Bayesian Optimization by Kernel regression and density-based Exploration (BOKE) algorithm. BOKE uses kernel regression for efficient function approximation, kernel density for exploration, and integrates them into the confidence bound criteria to guide the optimization process, thus reducing computational costs to quadratic. Our theoretical analysis rigorously establishes the global convergence of BOKE and ensures its robustness in noisy settings. Through extensive numerical experiments on both synthetic and real-world optimization tasks, we demonstrate that BOKE not only performs competitively compared to Gaussian process-based methods but also exhibits superior computational efficiency. These results highlight BOKE's effectiveness in resource-constrained environments, providing a practical approach for optimization problems in engineering applications.

## 统计理论(math.ST:Statistics Theory)

### Are all models wrong? Fundamental limits in distribution-free empirical model falsification 
[[arxiv](https://arxiv.org/abs/2502.06765)] [[cool](https://papers.cool/arxiv/2502.06765)] [[pdf](https://arxiv.org/pdf/2502.06765)]
> **Authors**: Manuel M. Müller,Yuetian Luo,Rina Foygel Barber
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 统计理论,机器学习,机器学习
- **Abstract**: In statistics and machine learning, when we train a fitted model on available data, we typically want to ensure that we are searching within a model class that contains at least one accurate model -- that is, we would like to ensure an upper bound on the model class risk (the lowest possible risk that can be attained by any model in the class). However, it is also of interest to establish lower bounds on the model class risk, for instance so that we can determine whether our fitted model is at least approximately optimal within the class, or, so that we can decide whether the model class is unsuitable for the particular task at hand. Particularly in the setting of interpolation learning where machine learning models are trained to reach zero error on the training data, we might ask if, at the very least, a positive lower bound on the model class risk is possible -- or are we unable to detect that "all models are wrong"? In this work, we answer these questions in a distribution-free setting by establishing a model-agnostic, fundamental hardness result for the problem of constructing a lower bound on the best test error achievable over a model class, and examine its implications on specific model classes such as tree-based methods and linear regression.

### Neumann eigenmaps for landmark embedding 
[[arxiv](https://arxiv.org/abs/2502.06689)] [[cool](https://papers.cool/arxiv/2502.06689)] [[pdf](https://arxiv.org/pdf/2502.06689)]
> **Authors**: Shashank Sule,Wojciech Czaja
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: :65C50; 68T10; 62M15
- **标题**: None
- **领域**: 统计理论,机器学习,数值分析,机器学习
- **Abstract**: We present Neumann eigenmaps (NeuMaps), a novel approach for enhancing the standard diffusion map embedding using landmarks, i.e distinguished samples within the dataset. By interpreting these landmarks as a subgraph of the larger data graph, NeuMaps are obtained via the eigendecomposition of a renormalized Neumann Laplacian. We show that NeuMaps offer two key advantages: (1) they provide a computationally efficient embedding that accurately recovers the diffusion distance associated with the reflecting random walk on the subgraph, and (2) they naturally incorporate the Nyström extension within the diffusion map framework through the discrete Neumann boundary condition. Through examples in digit classification and molecular dynamics, we demonstrate that NeuMaps not only improve upon existing landmark-based embedding methods but also enhance the stability of diffusion map embeddings to the removal of highly significant points.

## 计算物理(physics.comp-ph:Computational Physics)

### Enhancing Robustness Of Digital Shadow For CO2 Storage Monitoring With Augmented Rock Physics Modeling 
[[arxiv](https://arxiv.org/abs/2502.07171)] [[cool](https://papers.cool/arxiv/2502.07171)] [[pdf](https://arxiv.org/pdf/2502.07171)]
> **Authors**: Abhinav Prakash Gahlot,Felix J. Herrmann
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算物理,机器学习,地球物理学
- **Abstract**: To meet climate targets, the IPCC underscores the necessity of technologies capable of removing gigatonnes of CO2 annually, with Geological Carbon Storage (GCS) playing a central role. GCS involves capturing CO2 and injecting it into deep geological formations for long-term storage, requiring precise monitoring to ensure containment and prevent leakage. Time-lapse seismic imaging is essential for tracking CO2 migration but often struggles to capture the complexities of multi-phase subsurface flow. Digital Shadows (DS), leveraging machine learning-driven data assimilation techniques such as nonlinear Bayesian filtering and generative AI, provide a more detailed, uncertainty-aware monitoring approach. By incorporating uncertainties in reservoir properties, DS frameworks improve CO2 migration forecasts, reducing risks in GCS operations. However, data assimilation depends on assumptions regarding reservoir properties, rock physics models, and initial conditions, which, if inaccurate, can compromise prediction reliability. This study demonstrates that augmenting forecast ensembles with diverse rock physics models mitigates the impact of incorrect assumptions and improves predictive accuracy, particularly in differentiating uniform versus patchy saturation models.

### Advancing Geological Carbon Storage Monitoring With 3d Digital Shadow Technology 
[[arxiv](https://arxiv.org/abs/2502.07169)] [[cool](https://papers.cool/arxiv/2502.07169)] [[pdf](https://arxiv.org/pdf/2502.07169)]
> **Authors**: Abhinav Prakash Gahlot,Rafael Orozco,Felix J. Herrmann
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算物理,机器学习,地球物理学
- **Abstract**: Geological Carbon Storage (GCS) is a key technology for achieving global climate goals by capturing and storing CO2 in deep geological formations. Its effectiveness and safety rely on accurate monitoring of subsurface CO2 migration using advanced time-lapse seismic imaging. A Digital Shadow framework integrates field data, including seismic and borehole measurements, to track CO2 saturation over time. Machine learning-assisted data assimilation techniques, such as generative AI and nonlinear ensemble Bayesian filtering, update a digital model of the CO2 plume while incorporating uncertainties in reservoir properties. Compared to 2D approaches, 3D monitoring enhances the spatial accuracy of GCS assessments, capturing the full extent of CO2 migration. This study extends the uncertainty-aware 2D Digital Shadow framework by incorporating 3D seismic imaging and reservoir modeling, improving decision-making and risk mitigation in CO2 storage projects.

## 仪器仪表和探测器(physics.ins-det:Instrumentation and Detectors)

### DiffNMR3: Advancing NMR Resolution Beyond Instrumental Limits 
[[arxiv](https://arxiv.org/abs/2502.06845)] [[cool](https://papers.cool/arxiv/2502.06845)] [[pdf](https://arxiv.org/pdf/2502.06845)]
> **Authors**: Sen Yan,Etienne Goffinet,Fabrizio Gabellieri,Ryan Young,Lydia Gkoura,Laurence Jennings,Filippo Castiglione,Thomas Launey
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-11
> **comment**: 13 pages, 6 figures
- **标题**: None
- **领域**: 仪器仪表和探测器,人工智能,机器学习
- **Abstract**: Nuclear Magnetic Resonance (NMR) spectroscopy is a crucial analytical technique used for molecular structure elucidation, with applications spanning chemistry, biology, materials science, and medicine. However, the frequency resolution of NMR spectra is limited by the "field strength" of the instrument. High-field NMR instruments provide high-resolution spectra but are prohibitively expensive, whereas lower-field instruments offer more accessible, but lower-resolution, results. This paper introduces an AI-driven approach that not only enhances the frequency resolution of NMR spectra through super-resolution techniques but also provides multi-scale functionality. By leveraging a diffusion model, our method can reconstruct high-field spectra from low-field NMR data, offering flexibility in generating spectra at varying magnetic field strengths. These reconstructions are comparable to those obtained from high-field instruments, enabling finer spectral details and improving molecular characterization. To date, our approach is one of the first to overcome the limitations of instrument field strength, achieving NMR super-resolution through AI. This cost-effective solution makes high-resolution analysis accessible to more researchers and industries, without the need for multimillion-dollar equipment.

## 生物分子(q-bio.BM:Biomolecules)

### ScaffoldGPT: A Scaffold-based Large Language Model for Drug Improvement 
[[arxiv](https://arxiv.org/abs/2502.06891)] [[cool](https://papers.cool/arxiv/2502.06891)] [[pdf](https://arxiv.org/pdf/2502.06891)]
> **Authors**: Xuefeng Liu,Songhao Jiang,Rick Stevens
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 生物分子,计算语言学,机器学习
- **Abstract**: Drug optimization has become increasingly crucial in light of fast-mutating virus strains and drug-resistant cancer cells. Nevertheless, it remains challenging as it necessitates retaining the beneficial properties of the original drug while simultaneously enhancing desired attributes beyond its scope. In this work, we aim to tackle this challenge by introducing ScaffoldGPT, a novel Large Language Model (LLM) designed for drug optimization based on molecular scaffolds. Our work comprises three key components: (1) A three-stage drug optimization approach that integrates pretraining, finetuning, and decoding optimization. (2) A uniquely designed two-phase incremental training approach for pre-training the drug optimization LLM-based generator on molecule scaffold with enhanced performance. (3) A token-level decoding optimization strategy, TOP-N, that enabling controlled, reward-guided generation using pretrained/finetuned LLMs. Finally, by conducting a comprehensive evaluation on COVID and cancer benchmarks, we demonstrate that SCAFFOLDGPT outperforms the competing baselines in drug optimization benchmarks, while excelling in preserving the original functional scaffold and enhancing desired properties.

## 神经元和认知(q-bio.NC:Neurons and Cognition)

### Emergence of Self-Awareness in Artificial Systems: A Minimalist Three-Layer Approach to Artificial Consciousness 
[[arxiv](https://arxiv.org/abs/2502.06810)] [[cool](https://papers.cool/arxiv/2502.06810)] [[pdf](https://arxiv.org/pdf/2502.06810)]
> **Authors**: Kurando Iida
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-11
> **comment**: 46 pages
- **标题**: None
- **领域**: 神经元和认知,人工智能
- **Abstract**: This paper proposes a minimalist three-layer model for artificial consciousness, focusing on the emergence of self-awareness. The model comprises a Cognitive Integration Layer, a Pattern Prediction Layer, and an Instinctive Response Layer, interacting with Access-Oriented and Pattern-Integrated Memory systems. Unlike brain-replication approaches, we aim to achieve minimal self-awareness through essential elements only. Self-awareness emerges from layer interactions and dynamic self-modeling, without initial explicit self-programming. We detail each component's structure, function, and implementation strategies, addressing technical feasibility. This research offers new perspectives on consciousness emergence in artificial systems, with potential implications for human consciousness understanding and adaptable AI development. We conclude by discussing ethical considerations and future research directions.

## 定量方法(q-bio.QM:Quantitative Methods)

### UniZyme: A Unified Protein Cleavage Site Predictor Enhanced with Enzyme Active-Site Knowledge 
[[arxiv](https://arxiv.org/abs/2502.06914)] [[cool](https://papers.cool/arxiv/2502.06914)] [[pdf](https://arxiv.org/pdf/2502.06914)]
> **Authors**: Chenao Li,Shuo Yan,Enyan Dai
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 18 pages,8 figures
- **标题**: None
- **领域**: 定量方法,人工智能,机器学习
- **Abstract**: Enzyme-catalyzed protein cleavage is essential for many biological functions. Accurate prediction of cleavage sites can facilitate various applications such as drug development, enzyme design, and a deeper understanding of biological mechanisms. However, most existing models are restricted to an individual enzyme, which neglects shared knowledge of enzymes and fails generalize to novel enzymes. Thus, we introduce a unified protein cleavage site predictor named UniZyme, which can generalize across diverse enzymes. To enhance the enzyme encoding for the protein cleavage site prediction, UniZyme employs a novel biochemically-informed model architecture along with active-site knowledge of proteolytic enzymes. Extensive experiments demonstrate that UniZyme achieves high accuracy in predicting cleavage sites across a range of proteolytic enzymes, including unseen enzymes. The code is available in https://anonymous.4open.science/r/UniZyme-4A67.

### A Simple yet Effective DDG Predictor is An Unsupervised Antibody Optimizer and Explainer 
[[arxiv](https://arxiv.org/abs/2502.06913)] [[cool](https://papers.cool/arxiv/2502.06913)] [[pdf](https://arxiv.org/pdf/2502.06913)]
> **Authors**: Lirong Wu,Yunfan Liu,Haitao Lin,Yufei Huang,Guojiang Zhao,Zhifeng Gao,Stan Z. Li
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 定量方法,人工智能,机器学习
- **Abstract**: The proteins that exist today have been optimized over billions of years of natural evolution, during which nature creates random mutations and selects them. The discovery of functionally promising mutations is challenged by the limited evolutionary accessible regions, i.e., only a small region on the fitness landscape is beneficial. There have been numerous priors used to constrain protein evolution to regions of landscapes with high-fitness variants, among which the change in binding free energy (DDG) of protein complexes upon mutations is one of the most commonly used priors. However, the huge mutation space poses two challenges: (1) how to improve the efficiency of DDG prediction for fast mutation screening; and (2) how to explain mutation preferences and efficiently explore accessible evolutionary regions. To address these challenges, we propose a lightweight DDG predictor (Light-DDG), which adopts a structure-aware Transformer as the backbone and enhances it by knowledge distilled from existing powerful but computationally heavy DDG predictors. Additionally, we augmented, annotated, and released a large-scale dataset containing millions of mutation data for pre-training Light-DDG. We find that such a simple yet effective Light-DDG can serve as a good unsupervised antibody optimizer and explainer. For the target antibody, we propose a novel Mutation Explainer to learn mutation preferences, which accounts for the marginal benefit of each mutation per residue. To further explore accessible evolutionary regions, we conduct preference-guided antibody optimization and evaluate antibody candidates quickly using Light-DDG to identify desirable mutations.

## 计算金融(q-fin.CP:Computational Finance)

### OrderFusion: Encoding Orderbook for Probabilistic Intraday Price Prediction 
[[arxiv](https://arxiv.org/abs/2502.06830)] [[cool](https://papers.cool/arxiv/2502.06830)] [[pdf](https://arxiv.org/pdf/2502.06830)]
> **Authors**: Runyao Yu,Yuchen Tao,Fabian Leimgruber,Tara Esterl,Jochen L. Cremer
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-11
> **comment**: 9 pages, 5 figures, 3 tables
- **标题**: None
- **领域**: 计算金融,人工智能,机器学习
- **Abstract**: Efficient and reliable probabilistic prediction of intraday electricity prices is essential to manage market uncertainties and support robust trading strategies. However, current methods often suffer from parameter inefficiencies, as they fail to fully exploit the potential of modeling interdependencies between bids and offers in the orderbook, requiring a large number of parameters for representation learning. Furthermore, these methods face the quantile crossing issue, where upper quantiles fall below the lower quantiles, resulting in unreliable probabilistic predictions. To address these two challenges, we propose an encoding method called OrderFusion and design a hierarchical multi-quantile head. The OrderFusion encodes the orderbook into a 2.5D representation, which is processed by a tailored jump cross-attention backbone to capture the interdependencies of bids and offers, enabling parameter-efficient learning. The head sets the median quantile as an anchor and predicts multiple quantiles hierarchically, ensuring reliability by enforcing monotonicity between quantiles through non-negative functions. Extensive experiments and ablation studies are conducted on four price indices: 60-min ID3, 60-min ID1, 15-min ID3, and 15-min ID1 using the German orderbook over three years to ensure a fair evaluation. The results confirm that our design choices improve overall performance, offering a parameter-efficient and reliable solution for probabilistic intraday price prediction.

## 交易和市场微观结构(q-fin.TR:Trading and Market Microstructure)

### TRADES: Generating Realistic Market Simulations with Diffusion Models 
[[arxiv](https://arxiv.org/abs/2502.07071)] [[cool](https://papers.cool/arxiv/2502.07071)] [[pdf](https://arxiv.org/pdf/2502.07071)]
> **Authors**: Leonardo Berti,Bardh Prenkaj,Paola Velardi
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-11
> **comment**: 14 pages
- **标题**: None
- **领域**: 交易和市场微观结构,人工智能,机器学习,计算金融
- **Abstract**: Financial markets are complex systems characterized by high statistical noise, nonlinearity, and constant evolution. Thus, modeling them is extremely hard. We address the task of generating realistic and responsive Limit Order Book (LOB) market simulations, which are fundamental for calibrating and testing trading strategies, performing market impact experiments, and generating synthetic market data. Previous works lack realism, usefulness, and responsiveness of the generated simulations. To bridge this gap, we propose a novel TRAnsformer-based Denoising Diffusion Probabilistic Engine for LOB Simulations (TRADES). TRADES generates realistic order flows conditioned on the state of the market, leveraging a transformer-based architecture that captures the temporal and spatial characteristics of high-frequency market data. There is a notable absence of quantitative metrics for evaluating generative market simulation models in the literature. To tackle this problem, we adapt the predictive score, a metric measured as an MAE, by training a stock price predictive model on synthetic data and testing it on real data. We compare TRADES with previous works on two stocks, reporting an x3.27 and x3.47 improvement over SoTA according to the predictive score, demonstrating that we generate useful synthetic market data for financial downstream tasks. We assess TRADES's market simulation realism and responsiveness, showing that it effectively learns the conditional data distribution and successfully reacts to an experimental agent, giving sprout to possible calibrations and evaluations of trading strategies and market impact experiments. We developed DeepMarket, the first open-source Python framework for market simulation with deep learning. Our repository includes a synthetic LOB dataset composed of TRADES's generates simulations. We release the code at github.com/LeonardoBerti00/DeepMarket.

## 量子物理学(quant-ph:Quantum Physics)

### Application of quantum machine learning using quantum kernel algorithms on multiclass neuron M type classification 
[[arxiv](https://arxiv.org/abs/2502.06281)] [[cool](https://papers.cool/arxiv/2502.06281)] [[pdf](https://arxiv.org/pdf/2502.06281)]
> **Authors**: Xavier Vasques,Hanhee Paik,Laura Cif
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: ef:Sci Rep 13, 11541 (2023)
- **标题**: None
- **领域**: 量子物理学,机器学习
- **Abstract**: The functional characterization of different neuronal types has been a longstanding and crucial challenge. With the advent of physical quantum computers, it has become possible to apply quantum machine learning algorithms to translate theoretical research into practical solutions. Previous studies have shown the advantages of quantum algorithms on artificially generated datasets, and initial experiments with small binary classification problems have yielded comparable outcomes to classical algorithms. However, it is essential to investigate the potential quantum advantage using real-world data. To the best of our knowledge, this study is the first to propose the utilization of quantum systems to classify neuron morphologies, thereby enhancing our understanding of the performance of automatic multiclass neuron classification using quantum kernel methods. We examined the influence of feature engineering on classification accuracy and found that quantum kernel methods achieved similar performance to classical methods, with certain advantages observed in various configurations.

## 应用领域(stat.AP:Applications)

### A Framework for Supervised and Unsupervised Segmentation and Classification of Materials Microstructure Images 
[[arxiv](https://arxiv.org/abs/2502.07107)] [[cool](https://papers.cool/arxiv/2502.07107)] [[pdf](https://arxiv.org/pdf/2502.07107)]
> **Authors**: Kungang Zhang,Daniel W. Apley,Wei Chen,Wing K. Liu,L. Catherine Brinson
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 应用领域,计算机视觉和模式识别,机器学习
- **Abstract**: Microstructure of materials is often characterized through image analysis to understand processing-structure-properties linkages. We propose a largely automated framework that integrates unsupervised and supervised learning methods to classify micrographs according to microstructure phase/class and, for multiphase microstructures, segments them into different homogeneous regions. With the advance of manufacturing and imaging techniques, the ultra-high resolution of imaging that reveals the complexity of microstructures and the rapidly increasing quantity of images (i.e., micrographs) enables and necessitates a more powerful and automated framework to extract materials characteristics and knowledge. The framework we propose can be used to gradually build a database of microstructure classes relevant to a particular process or group of materials, which can help in analyzing and discovering/identifying new materials. The framework has three steps: (1) segmentation of multiphase micrographs through a recently developed score-based method so that different microstructure homogeneous regions can be identified in an unsupervised manner; (2) {identification and classification of} homogeneous regions of micrographs through an uncertainty-aware supervised classification network trained using the segmented micrographs from Step $1$ with their identified labels verified via the built-in uncertainty quantification and minimal human inspection; (3) supervised segmentation (more powerful than the segmentation in Step $1$) of multiphase microstructures through a segmentation network trained with micrographs and the results from Steps $1$-$2$ using a form of data augmentation. This framework can iteratively characterize/segment new homogeneous or multiphase materials while expanding the database to enhance performance. The framework is demonstrated on various sets of materials and texture images.

## 计算(stat.CO:Computation)

### Case for a unified surrogate modelling framework in the age of AI 
[[arxiv](https://arxiv.org/abs/2502.06753)] [[cool](https://papers.cool/arxiv/2502.06753)] [[pdf](https://arxiv.org/pdf/2502.06753)]
> **Authors**: Elizaveta Semenova
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 计算,机器学习
- **Abstract**: Surrogate models are widely used in natural sciences, engineering, and machine learning to approximate complex systems and reduce computational costs. However, the current landscape lacks standardisation across key stages of the pipeline, including data collection, sampling design, model class selection, evaluation metrics, and downstream task performance analysis. This fragmentation limits reproducibility, reliability, and cross-domain applicability. The issue has only been exacerbated by the AI revolution and a new suite of surrogate model classes that it offers. In this position paper, we argue for the urgent need for a unified framework to guide the development and evaluation of surrogate models. We outline essential steps for constructing a comprehensive pipeline and discuss alternative perspectives, such as the benefits of domain-specific frameworks. By advocating for a standardised approach, this paper seeks to improve the reliability of surrogate modelling, foster cross-disciplinary knowledge transfer, and, as a result, accelerate scientific progress.

## 方法论(stat.ME:Methodology)

### Falsification of Unconfoundedness by Testing Independence of Causal Mechanisms 
[[arxiv](https://arxiv.org/abs/2502.06231)] [[cool](https://papers.cool/arxiv/2502.06231)] [[pdf](https://arxiv.org/pdf/2502.06231)]
> **Authors**: Rickard K. A. Karlsson,Jesse H. Krijthe
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 20 pages, including 5 figures, 2 tables, and appendices
- **标题**: None
- **领域**: 方法论,机器学习,机器学习
- **Abstract**: A major challenge in estimating treatment effects in observational studies is the reliance on untestable conditions such as the assumption of no unmeasured confounding. In this work, we propose an algorithm that can falsify the assumption of no unmeasured confounding in a setting with observational data from multiple heterogeneous sources, which we refer to as environments. Our proposed falsification strategy leverages a key observation that unmeasured confounding can cause observed causal mechanisms to appear dependent. Building on this observation, we develop a novel two-stage procedure that detects these dependencies with high statistical power while controlling false positives. The algorithm does not require access to randomized data and, in contrast to other falsification approaches, functions even under transportability violations when the environment has a direct effect on the outcome of interest. To showcase the practical relevance of our approach, we show that our method is able to efficiently detect confounding on both simulated and real-world data.

## 机器学习(stat.ML:Machine Learning)

### Online Covariance Matrix Estimation in Sketched Newton Methods 
[[arxiv](https://arxiv.org/abs/2502.07114)] [[cool](https://papers.cool/arxiv/2502.07114)] [[pdf](https://arxiv.org/pdf/2502.07114)]
> **Authors**: Wei Kuang,Mihai Anitescu,Sen Na
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 52 pages, 2 figures, 7 tables
- **标题**: None
- **领域**: 机器学习,机器学习,数值分析,优化与控制,计算
- **Abstract**: Given the ubiquity of streaming data, online algorithms have been widely used for parameter estimation, with second-order methods particularly standing out for their efficiency and robustness. In this paper, we study an online sketched Newton method that leverages a randomized sketching technique to perform an approximate Newton step in each iteration, thereby eliminating the computational bottleneck of second-order methods. While existing studies have established the asymptotic normality of sketched Newton methods, a consistent estimator of the limiting covariance matrix remains an open problem. We propose a fully online covariance matrix estimator that is constructed entirely from the Newton iterates and requires no matrix factorization. Compared to covariance estimators for first-order online methods, our estimator for second-order methods is batch-free. We establish the consistency and convergence rate of our estimator, and coupled with asymptotic normality results, we can then perform online statistical inference for the model parameters based on sketched Newton methods. We also discuss the extension of our estimator to constrained problems, and demonstrate its superior performance on regression problems as well as benchmark problems in the CUTEst set.

### Generative Distribution Prediction: A Unified Approach to Multimodal Learning 
[[arxiv](https://arxiv.org/abs/2502.07090)] [[cool](https://papers.cool/arxiv/2502.07090)] [[pdf](https://arxiv.org/pdf/2502.07090)]
> **Authors**: Xinyu Tian,Xiaotong Shen
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 31 pages 4 figures
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Accurate prediction with multimodal data-encompassing tabular, textual, and visual inputs or outputs-is fundamental to advancing analytics in diverse application domains. Traditional approaches often struggle to integrate heterogeneous data types while maintaining high predictive accuracy. We introduce Generative Distribution Prediction (GDP), a novel framework that leverages multimodal synthetic data generation-such as conditional diffusion models-to enhance predictive performance across structured and unstructured modalities. GDP is model-agnostic, compatible with any high-fidelity generative model, and supports transfer learning for domain adaptation. We establish a rigorous theoretical foundation for GDP, providing statistical guarantees on its predictive accuracy when using diffusion models as the generative backbone. By estimating the data-generating distribution and adapting to various loss functions for risk minimization, GDP enables accurate point predictions across multimodal settings. We empirically validate GDP on four supervised learning tasks-tabular data prediction, question answering, image captioning, and adaptive quantile regression-demonstrating its versatility and effectiveness across diverse domains.

### Confidence Intervals for Evaluation of Data Mining 
[[arxiv](https://arxiv.org/abs/2502.07016)] [[cool](https://papers.cool/arxiv/2502.07016)] [[pdf](https://arxiv.org/pdf/2502.07016)]
> **Authors**: Zheng Yuan,Wenxin Jiang
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: In data mining, when binary prediction rules are used to predict a binary outcome, many performance measures are used in a vast array of literature for the purposes of evaluation and comparison. Some examples include classification accuracy, precision, recall, F measures, and Jaccard index. Typically, these performance measures are only approximately estimated from a finite dataset, which may lead to findings that are not statistically significant. In order to properly quantify such statistical uncertainty, it is important to provide confidence intervals associated with these estimated performance measures. We consider statistical inference about general performance measures used in data mining, with both individual and joint confidence intervals. These confidence intervals are based on asymptotic normal approximations and can be computed fast, without needs to do bootstrap resampling. We study the finite sample coverage probabilities for these confidence intervals and also propose a `blurring correction' on the variance to improve the finite sample performance. This 'blurring correction' generalizes the plus-four method from binomial proportion to general performance measures used in data mining. Our framework allows multiple performance measures of multiple classification rules to be inferred simultaneously for comparisons.

### Epistemic Uncertainty in Conformal Scores: A Unified Approach 
[[arxiv](https://arxiv.org/abs/2502.06995)] [[cool](https://papers.cool/arxiv/2502.06995)] [[pdf](https://arxiv.org/pdf/2502.06995)]
> **Authors**: Luben M. C. Cabezas,Vagner S. Santos,Thiago R. Ramos,Rafael Izbicki
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Conformal prediction methods create prediction bands with distribution-free guarantees but do not explicitly capture epistemic uncertainty, which can lead to overconfident predictions in data-sparse regions. Although recent conformal scores have been developed to address this limitation, they are typically designed for specific tasks, such as regression or quantile regression. Moreover, they rely on particular modeling choices for epistemic uncertainty, restricting their applicability. We introduce $\texttt{EPICSCORE}$, a model-agnostic approach that enhances any conformal score by explicitly integrating epistemic uncertainty. Leveraging Bayesian techniques such as Gaussian Processes, Monte Carlo Dropout, or Bayesian Additive Regression Trees, $\texttt{EPICSCORE}$ adaptively expands predictive intervals in regions with limited data while maintaining compact intervals where data is abundant. As with any conformal method, it preserves finite-sample marginal coverage. Additionally, it also achieves asymptotic conditional coverage. Experiments demonstrate its good performance compared to existing methods. Designed for compatibility with any Bayesian model, but equipped with distribution-free guarantees, $\texttt{EPICSCORE}$ provides a general-purpose framework for uncertainty quantification in prediction problems.

### Learning an Optimal Assortment Policy under Observational Data 
[[arxiv](https://arxiv.org/abs/2502.06777)] [[cool](https://papers.cool/arxiv/2502.06777)] [[pdf](https://arxiv.org/pdf/2502.06777)]
> **Authors**: Yuxuan Han,Han Zhong,Miao Lu,Jose Blanchet,Zhengyuan Zhou
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习,优化与控制,统计理论
- **Abstract**: We study the fundamental problem of offline assortment optimization under the Multinomial Logit (MNL) model, where sellers must determine the optimal subset of the products to offer based solely on historical customer choice data. While most existing approaches to learning-based assortment optimization focus on the online learning of the optimal assortment through repeated interactions with customers, such exploration can be costly or even impractical in many real-world settings. In this paper, we consider the offline learning paradigm and investigate the minimal data requirements for efficient offline assortment optimization. To this end, we introduce Pessimistic Rank-Breaking (PRB), an algorithm that combines rank-breaking with pessimistic estimation. We prove that PRB is nearly minimax optimal by establishing the tight suboptimality upper bound and a nearly matching lower bound. This further shows that "optimal item coverage" - where each item in the optimal assortment appears sufficiently often in the historical data - is both sufficient and necessary for efficient offline learning. This significantly relaxes the previous requirement of observing the complete optimal assortment in the data. Our results provide fundamental insights into the data requirements for offline assortment optimization under the MNL model.

### Gaussian Approximation and Multiplier Bootstrap for Stochastic Gradient Descent 
[[arxiv](https://arxiv.org/abs/2502.06719)] [[cool](https://papers.cool/arxiv/2502.06719)] [[pdf](https://arxiv.org/pdf/2502.06719)]
> **Authors**: Marina Sheshukova,Sergey Samsonov,Denis Belomestny,Eric Moulines,Qi-Man Shao,Zhuo-Song Zhang,Alexey Naumov
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: :60F05; 62L20; 93E35
- **标题**: None
- **领域**: 机器学习,机器学习,优化与控制,可能性,统计理论
- **Abstract**: In this paper, we establish non-asymptotic convergence rates in the central limit theorem for Polyak-Ruppert-averaged iterates of stochastic gradient descent (SGD). Our analysis builds on the result of the Gaussian approximation for nonlinear statistics of independent random variables of Shao and Zhang (2022). Using this result, we prove the non-asymptotic validity of the multiplier bootstrap for constructing the confidence sets for the optimal solution of an optimization problem. In particular, our approach avoids the need to approximate the limiting covariance of Polyak-Ruppert SGD iterates, which allows us to derive approximation rates in convex distance of order up to $1/\sqrt{n}$.

### Quantile Multi-Armed Bandits with 1-bit Feedback 
[[arxiv](https://arxiv.org/abs/2502.06678)] [[cool](https://papers.cool/arxiv/2502.06678)] [[pdf](https://arxiv.org/pdf/2502.06678)]
> **Authors**: Ivan Lau,Jonathan Scarlett
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: ALT 2025
- **标题**: None
- **领域**: 机器学习,信息论,机器学习
- **Abstract**: In this paper, we study a variant of best-arm identification involving elements of risk sensitivity and communication constraints. Specifically, the goal of the learner is to identify the arm with the highest quantile reward, while the communication from an agent (who observes rewards) and the learner (who chooses actions) is restricted to only one bit of feedback per arm pull. We propose an algorithm that utilizes noisy binary search as a subroutine, allowing the learner to estimate quantile rewards through 1-bit feedback. We derive an instance-dependent upper bound on the sample complexity of our algorithm and provide an algorithm-independent lower bound for specific instances, with the two matching to within logarithmic factors under mild conditions, or even to within constant factors in certain low error probability scaling regimes. The lower bound is applicable even in the absence of communication constraints, and thus we conclude that restricting to 1-bit feedback has a minimal impact on the scaling of the sample complexity.

### iLOCO: Distribution-Free Inference for Feature Interactions 
[[arxiv](https://arxiv.org/abs/2502.06661)] [[cool](https://papers.cool/arxiv/2502.06661)] [[pdf](https://arxiv.org/pdf/2502.06661)]
> **Authors**: Camille Little,Lili Zheng,Genevera Allen
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Feature importance measures are widely studied and are essential for understanding model behavior, guiding feature selection, and enhancing interpretability. However, many machine learning fitted models involve complex, higher-order interactions between features. Existing feature importance metrics fail to capture these higher-order effects while existing interaction metrics often suffer from limited applicability or excessive computation; no methods exist to conduct statistical inference for feature interactions. To bridge this gap, we first propose a new model-agnostic metric, interaction Leave-One-Covariate-Out iLOCO, for measuring the importance of higher-order feature interactions. Next, we leverage recent advances in LOCO inference to develop distribution-free and assumption-light confidence intervals for our iLOCO metric. To address computational challenges, we also introduce an ensemble learning method for calculating the iLOCO metric and confidence intervals that we show is both computationally and statistically efficient. We validate our iLOCO metric and our confidence intervals on both synthetic and real data sets, showing that our approach outperforms existing methods and provides the first inferential approach to detecting feature interactions.

### Membership Inference Risks in Quantized Models: A Theoretical and Empirical Study 
[[arxiv](https://arxiv.org/abs/2502.06567)] [[cool](https://papers.cool/arxiv/2502.06567)] [[pdf](https://arxiv.org/pdf/2502.06567)]
> **Authors**: Eric Aubinais,Philippe Formont,Pablo Piantanida,Elisabeth Gassiat
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Quantizing machine learning models has demonstrated its effectiveness in lowering memory and inference costs while maintaining performance levels comparable to the original models. In this work, we investigate the impact of quantization procedures on the privacy of data-driven models, specifically focusing on their vulnerability to membership inference attacks. We derive an asymptotic theoretical analysis of Membership Inference Security (MIS), characterizing the privacy implications of quantized algorithm weights against the most powerful (and possibly unknown) attacks. Building on these theoretical insights, we propose a novel methodology to empirically assess and rank the privacy levels of various quantization procedures. Using synthetic datasets, we demonstrate the effectiveness of our approach in assessing the MIS of different quantizers. Furthermore, we explore the trade-off between privacy and performance using real-world data and models in the context of molecular modeling.

### Data Augmentation and Regularization for Learning Group Equivariance 
[[arxiv](https://arxiv.org/abs/2502.06547)] [[cool](https://papers.cool/arxiv/2502.06547)] [[pdf](https://arxiv.org/pdf/2502.06547)]
> **Authors**: Oskar Nordenfors,Axel Flinth
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: :68T07; 20C35; 37N40
- **标题**: None
- **领域**: 机器学习,机器学习,优化与控制
- **Abstract**: In many machine learning tasks, known symmetries can be used as an inductive bias to improve model performance. In this paper, we consider learning group equivariance through training with data augmentation. We summarize results from a previous paper of our own, and extend the results to show that equivariance of the trained model can be achieved through training on augmented data in tandem with regularization.

### Sample-efficient Learning of Concepts with Theoretical Guarantees: from Data to Concepts without Interventions 
[[arxiv](https://arxiv.org/abs/2502.06536)] [[cool](https://papers.cool/arxiv/2502.06536)] [[pdf](https://arxiv.org/pdf/2502.06536)]
> **Authors**: Hidde Fokkema,Tim van Erven,Sara Magliacane
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 47 pages, 16 figures, 9 Tables, Preprint
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Machine learning is a vital part of many real-world systems, but several concerns remain about the lack of interpretability, explainability and robustness of black-box AI systems. Concept-based models (CBM) address some of these challenges by learning interpretable concepts from high-dimensional data, e.g. images, which are used to predict labels. An important issue in CBMs is concept leakage, i.e., spurious information in the learned concepts, which effectively leads to learning "wrong" concepts. Current mitigating strategies are heuristic, have strong assumptions, e.g., they assume that the concepts are statistically independent of each other, or require substantial human interaction in terms of both interventions and labels provided by annotators. In this paper, we describe a framework that provides theoretical guarantees on the correctness of the learned concepts and on the number of required labels, without requiring any interventions. Our framework leverages causal representation learning (CRL) to learn high-level causal variables from low-level data, and learns to align these variables with interpretable concepts. We propose a linear and a non-parametric estimator for this mapping, providing a finite-sample high probability result in the linear case and an asymptotic consistency result for the non-parametric estimator. We implement our framework with state-of-the-art CRL methods, and show its efficacy in learning the correct concepts in synthetic and image benchmarks.

### Properties of Wasserstein Gradient Flows for the Sliced-Wasserstein Distance 
[[arxiv](https://arxiv.org/abs/2502.06525)] [[cool](https://papers.cool/arxiv/2502.06525)] [[pdf](https://arxiv.org/pdf/2502.06525)]
> **Authors**: Christophe Vauthier,Quentin Mérigot,Anna Korba
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 32p
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: In this paper, we investigate the properties of the Sliced Wasserstein Distance (SW) when employed as an objective functional. The SW metric has gained significant interest in the optimal transport and machine learning literature, due to its ability to capture intricate geometric properties of probability distributions while remaining computationally tractable, making it a valuable tool for various applications, including generative modeling and domain adaptation. Our study aims to provide a rigorous analysis of the critical points arising from the optimization of the SW objective. By computing explicit perturbations, we establish that stable critical points of SW cannot concentrate on segments. This stability analysis is crucial for understanding the behaviour of optimization algorithms for models trained using the SW objective. Furthermore, we investigate the properties of the SW objective, shedding light on the existence and convergence behavior of critical points. We illustrate our theoretical results through numerical experiments.

### Conformal Prediction Regions are Imprecise Highest Density Regions 
[[arxiv](https://arxiv.org/abs/2502.06331)] [[cool](https://papers.cool/arxiv/2502.06331)] [[pdf](https://arxiv.org/pdf/2502.06331)]
> **Authors**: Michele Caprio,Yusuf Sale,Eyke Hüllermeier
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: :Primary: 68T37; Secondary: 62M20; 60G25; 20M32; 15A80
- **标题**: None
- **领域**: 机器学习,机器学习,可能性
- **Abstract**: Recently, Cella and Martin proved how, under an assumption called consonance, a credal set (i.e. a closed and convex set of probabilities) can be derived from the conformal transducer associated with transductive conformal prediction. We show that the Imprecise Highest Density Region (IHDR) associated with such a credal set corresponds to the classical Conformal Prediction Region. In proving this result, we relate the set of probability density/mass functions (pdf/pmf's) associated with the elements of the credal set to the imprecise probabilistic concept of a cloud. As a result, we establish new relationships between Conformal Prediction and Imprecise Probability (IP) theories. A byproduct of our presentation is the discovery that consonant plausibility functions are monoid homomorphisms, a new algebraic property of an IP tool.

### Spectral-factorized Positive-definite Curvature Learning for NN Training 
[[arxiv](https://arxiv.org/abs/2502.06268)] [[cool](https://papers.cool/arxiv/2502.06268)] [[pdf](https://arxiv.org/pdf/2502.06268)]
> **Authors**: Wu Lin,Felix Dangel,Runa Eschenhagen,Juhan Bae,Richard E. Turner,Roger B. Grosse
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: technical report
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Many training methods, such as Adam(W) and Shampoo, learn a positive-definite curvature matrix and apply an inverse root before preconditioning. Recently, non-diagonal training methods, such as Shampoo, have gained significant attention; however, they remain computationally inefficient and are limited to specific types of curvature information due to the costly matrix root computation via matrix decomposition. To address this, we propose a Riemannian optimization approach that dynamically adapts spectral-factorized positive-definite curvature estimates, enabling the efficient application of arbitrary matrix roots and generic curvature learning. We demonstrate the efficacy and versatility of our approach in positive-definite matrix optimization and covariance adaptation for gradient-free optimization, as well as its efficiency in curvature learning for neural net training.

### Dynamic Pricing with Adversarially-Censored Demands 
[[arxiv](https://arxiv.org/abs/2502.06168)] [[cool](https://papers.cool/arxiv/2502.06168)] [[pdf](https://arxiv.org/pdf/2502.06168)]
> **Authors**: Jianyu Xu,Yining Wang,Xi Chen,Yu-Xiang Wang
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-11
> **comment**: 33 pages, 1 figure
- **标题**: None
- **领域**: 机器学习,机器学习,计量经济学,优化与控制
- **Abstract**: We study an online dynamic pricing problem where the potential demand at each time period $t=1,2,\ldots, T$ is stochastic and dependent on the price. However, a perishable inventory is imposed at the beginning of each time $t$, censoring the potential demand if it exceeds the inventory level. To address this problem, we introduce a pricing algorithm based on the optimistic estimates of derivatives. We show that our algorithm achieves $\tilde{O}(\sqrt{T})$ optimal regret even with adversarial inventory series. Our findings advance the state-of-the-art in online decision-making problems with censored feedback, offering a theoretically optimal solution against adversarial observations.

## 其他论文

- [Revisiting the Auxiliary Data in Backdoor Purification](https://arxiv.org/abs/2502.07231)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [DOGR: Leveraging Document-Oriented Contrastive Learning in Generative Retrieval](https://arxiv.org/abs/2502.07219)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [A Hybrid-Domain Floating-Point Compute-in-Memory Architecture for Efficient Acceleration of High-Precision Deep Neural Networks](https://arxiv.org/abs/2502.07212)
  - **标题**: None
  - **Filtered Reason**: none of cs.AR in whitelist
- [Towards Understanding of Frequency Dependence on Sound Event Detection](https://arxiv.org/abs/2502.07208)
  - **标题**: None
  - **Filtered Reason**: none of cs.SD,eess.AS in whitelist
- [Repository-level Code Search with Neural Retrieval Methods](https://arxiv.org/abs/2502.07067)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [Data Warehouse Design for Multiple Source Forest Inventory Management and Image Processing](https://arxiv.org/abs/2502.07015)
  - **标题**: None
  - **Filtered Reason**: none of cs.DB in whitelist
- [Enhancing Trust in Language Model-Based Code Optimization through RLHF: A Research Design](https://arxiv.org/abs/2502.06769)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Fat-Tree QRAM: A High-Bandwidth Shared Quantum Random Access Memory for Parallel Queries](https://arxiv.org/abs/2502.06767)
  - **标题**: None
  - **Filtered Reason**: none of quant-ph,cs.AR in whitelist
- [Infinite-Horizon Value Function Approximation for Model Predictive Control](https://arxiv.org/abs/2502.06760)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [A Fair Federated Learning Framework for Collaborative Network Traffic Prediction and Resource Allocation](https://arxiv.org/abs/2502.06743)
  - **标题**: None
  - **Filtered Reason**: none of cs.NI in whitelist
- [AgilePilot: DRL-Based Drone Agent for Real-Time Motion Planning in Dynamic Environments by Leveraging Object Detection](https://arxiv.org/abs/2502.06725)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [Discovery of skill switching criteria for learning agile quadruped locomotion](https://arxiv.org/abs/2502.06676)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [Differentially Private Empirical Cumulative Distribution Functions](https://arxiv.org/abs/2502.06651)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [Deciding Local Unitary Equivalence of Graph States in Quasi-Polynomial Time](https://arxiv.org/abs/2502.06566)
  - **标题**: None
  - **Filtered Reason**: none of quant-ph,cs.DM in whitelist
- [Convex Split Lemma without Inequalities](https://arxiv.org/abs/2502.06526)
  - **标题**: None
  - **Filtered Reason**: none of quant-ph,math-ph,cs.IT in whitelist
- [An Efficient Security Model for Industrial Internet of Things (IIoT) System Based on Machine Learning Principles](https://arxiv.org/abs/2502.06502)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR,cs.NI in whitelist
- [EdgeMLBalancer: A Self-Adaptive Approach for Dynamic Model Switching on Resource-Constrained Edge Devices](https://arxiv.org/abs/2502.06493)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Occ-LLM: Enhancing Autonomous Driving with Occupancy-Based Large Language Models](https://arxiv.org/abs/2502.06419)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [Simulation as Reality? The Effectiveness of LLM-Generated Data in Open-ended Question Assessment](https://arxiv.org/abs/2502.06371)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [Tracezip: Efficient Distributed Tracing via Trace Compression](https://arxiv.org/abs/2502.06318)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [The digital labour of artificial intelligence in Latin America: a comparison of Argentina, Brazil, and Venezuela](https://arxiv.org/abs/2502.06317)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [Amplifying Minority Voices: AI-Mediated Devil's Advocate System for Inclusive Group Decision-Making](https://arxiv.org/abs/2502.06251)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [XNet-Enhanced Deep BSDE Method and Numerical Analysis](https://arxiv.org/abs/2502.06238)
  - **标题**: None
  - **Filtered Reason**: none of cs.CE in whitelist
- [Timing Matters: How Using LLMs at Different Timings Influences Writers' Perceptions and Ideation Outcomes in AI-Assisted Ideation](https://arxiv.org/abs/2502.06197)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Reducing Alert Fatigue via AI-Assisted Negotiation: A Case for Dependabot](https://arxiv.org/abs/2502.06175)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
