> 本文由 [https://github.com/huiyeruzhou/arxiv_crawler](https://github.com/huiyeruzhou/arxiv_crawler) 自动生成
>
> 领域白名单：cs.AI,cs.CL,cs.LG,cs.CV
> 关键词： LLM, GPT, AI, language+model, deep+learning, transformer, neural+network, machine+learning

# 论文全览：2025-02-24

共有46篇相关领域论文, 另有2篇其他

## 人工智能(cs.AI:Artificial Intelligence)

### A Knowledge Distillation-Based Approach to Enhance Transparency of Classifier Models 
[[arxiv](https://arxiv.org/abs/2502.15959)] [[cool](https://papers.cool/arxiv/2502.15959)] [[pdf](https://arxiv.org/pdf/2502.15959)]
> **Authors**: Yuchen Jiang,Xinyuan Zhao,Yihang Wu,Ahmad Chaddad
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Accepted in AAAI 2025
- **标题**: None
- **领域**: 人工智能
- **Abstract**: With the rapid development of artificial intelligence (AI), especially in the medical field, the need for its explainability has grown. In medical image analysis, a high degree of transparency and model interpretability can help clinicians better understand and trust the decision-making process of AI models. In this study, we propose a Knowledge Distillation (KD)-based approach that aims to enhance the transparency of the AI model in medical image analysis. The initial step is to use traditional CNN to obtain a teacher model and then use KD to simplify the CNN architecture, retain most of the features of the data set, and reduce the number of network layers. It also uses the feature map of the student model to perform hierarchical analysis to identify key features and decision-making processes. This leads to intuitive visual explanations. We selected three public medical data sets (brain tumor, eye disease, and Alzheimer's disease) to test our method. It shows that even when the number of layers is reduced, our model provides a remarkable result in the test set and reduces the time required for the interpretability analysis.

### Practical Principles for AI Cost and Compute Accounting 
[[arxiv](https://arxiv.org/abs/2502.15873)] [[cool](https://papers.cool/arxiv/2502.15873)] [[pdf](https://arxiv.org/pdf/2502.15873)]
> **Authors**: Stephen Casper,Luke Bailey,Tim Schreier
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算机与社会
- **Abstract**: Policymakers are increasingly using development cost and compute as proxies for AI model capabilities and risks. Recent laws have introduced regulatory requirements that are contingent on specific thresholds. However, technical ambiguities in how to perform this accounting could create loopholes that undermine regulatory effectiveness. This paper proposes seven principles for designing practical AI cost and compute accounting standards that (1) reduce opportunities for strategic gaming, (2) avoid disincentivizing responsible risk mitigation, and (3) enable consistent implementation across companies and jurisdictions.

### C3AI: Crafting and Evaluating Constitutions for Constitutional AI 
[[arxiv](https://arxiv.org/abs/2502.15861)] [[cool](https://papers.cool/arxiv/2502.15861)] [[pdf](https://arxiv.org/pdf/2502.15861)]
> **Authors**: Yara Kyrychenko,Ke Zhou,Edyta Bogucka,Daniele Quercia
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: This has been accepted for the Web Conference 2025
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: Constitutional AI (CAI) guides LLM behavior using constitutions, but identifying which principles are most effective for model alignment remains an open challenge. We introduce the C3AI framework (\textit{Crafting Constitutions for CAI models}), which serves two key functions: (1) selecting and structuring principles to form effective constitutions before fine-tuning; and (2) evaluating whether fine-tuned CAI models follow these principles in practice. By analyzing principles from AI and psychology, we found that positively framed, behavior-based principles align more closely with human preferences than negatively framed or trait-based principles. In a safety alignment use case, we applied a graph-based principle selection method to refine an existing CAI constitution, improving safety measures while maintaining strong general reasoning capabilities. Interestingly, fine-tuned CAI models performed well on negatively framed principles but struggled with positively framed ones, in contrast to our human alignment results. This highlights a potential gap between principle design and model adherence. Overall, C3AI provides a structured and scalable approach to both crafting and evaluating CAI constitutions.

## 计算语言学(cs.CL:Computation and Language)

### R$^3$Mem: Bridging Memory Retention and Retrieval via Reversible Compression 
[[arxiv](https://arxiv.org/abs/2502.15957)] [[cool](https://papers.cool/arxiv/2502.15957)] [[pdf](https://arxiv.org/pdf/2502.15957)]
> **Authors**: Xiaoqiang Wang,Suyuchen Wang,Yun Zhu,Bang Liu
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Work in progress
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Memory plays a key role in enhancing LLMs' performance when deployed to real-world applications. Existing solutions face trade-offs: explicit memory designs based on external storage require complex management and incur storage overhead, while implicit memory designs that store information via parameters struggle with reliable retrieval. In this paper, we propose R$^3$Mem, a memory network that optimizes both information Retention and Retrieval through Reversible context compression. Specifically, R$^3$Mem employs virtual memory tokens to compress and encode infinitely long histories, further enhanced by a hierarchical compression strategy that refines information from document- to entity-level for improved assimilation across granularities. For retrieval, R$^3$Mem employs a reversible architecture, reconstructing raw data by invoking the model backward with compressed information. Implemented via parameter-efficient fine-tuning, it can integrate seamlessly with any Transformer-based model. Experiments demonstrate that our memory design achieves state-of-the-art performance in long-context language modeling and retrieval-augmented generation tasks. It also significantly outperforms conventional memory modules in long-horizon interaction tasks like conversational agents, showcasing its potential for next-generation retrieval systems.

### MMRAG: Multi-Mode Retrieval-Augmented Generation with Large Language Models for Biomedical In-Context Learning 
[[arxiv](https://arxiv.org/abs/2502.15954)] [[cool](https://papers.cool/arxiv/2502.15954)] [[pdf](https://arxiv.org/pdf/2502.15954)]
> **Authors**: Zaifu Zhan,Jun Wang,Shuang Zhou,Jiawen Deng,Rui Zhang
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Submitted to JAMIA
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Objective: To optimize in-context learning in biomedical natural language processing by improving example selection. Methods: We introduce a novel multi-mode retrieval-augmented generation (MMRAG) framework, which integrates four retrieval strategies: (1) Random Mode, selecting examples arbitrarily; (2) Top Mode, retrieving the most relevant examples based on similarity; (3) Diversity Mode, ensuring variation in selected examples; and (4) Class Mode, selecting category-representative examples. This study evaluates MMRAG on three core biomedical NLP tasks: Named Entity Recognition (NER), Relation Extraction (RE), and Text Classification (TC). The datasets used include BC2GM for gene and protein mention recognition (NER), DDI for drug-drug interaction extraction (RE), GIT for general biomedical information extraction (RE), and HealthAdvice for health-related text classification (TC). The framework is tested with two large language models (Llama2-7B, Llama3-8B) and three retrievers (Contriever, MedCPT, BGE-Large) to assess performance across different retrieval strategies. Results: The results from the Random mode indicate that providing more examples in the prompt improves the model's generation performance. Meanwhile, Top mode and Diversity mode significantly outperform Random mode on the RE (DDI) task, achieving an F1 score of 0.9669, a 26.4% improvement. Among the three retrievers tested, Contriever outperformed the other two in a greater number of experiments. Additionally, Llama 2 and Llama 3 demonstrated varying capabilities across different tasks, with Llama 3 showing a clear advantage in handling NER tasks. Conclusion: MMRAG effectively enhances biomedical in-context learning by refining example selection, mitigating data scarcity issues, and demonstrating superior adaptability for NLP-driven healthcare applications.

### AutoMedPrompt: A New Framework for Optimizing LLM Medical Prompts Using Textual Gradients 
[[arxiv](https://arxiv.org/abs/2502.15944)] [[cool](https://papers.cool/arxiv/2502.15944)] [[pdf](https://arxiv.org/pdf/2502.15944)]
> **Authors**: Sean Wu,Michael Koo,Fabien Scalzo,Ira Kurtz
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 14 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) have demonstrated increasingly sophisticated performance in medical and other fields of knowledge. Traditional methods of creating specialist LLMs require extensive fine-tuning and training of models on large datasets. Recently, prompt engineering, instead of fine-tuning, has shown potential to boost the performance of general foundation models. However, prompting methods such as chain-of-thought (CoT) may not be suitable for all subspecialty, and k-shot approaches may introduce irrelevant tokens into the context space. We present AutoMedPrompt, which explores the use of textual gradients to elicit medically relevant reasoning through system prompt optimization. AutoMedPrompt leverages TextGrad's automatic differentiation via text to improve the ability of general foundation LLMs. We evaluated AutoMedPrompt on Llama 3, an open-source LLM, using several QA benchmarks, including MedQA, PubMedQA, and the nephrology subspecialty-specific NephSAP. Our results show that prompting with textual gradients outperforms previous methods on open-source LLMs and surpasses proprietary models such as GPT-4, Claude 3 Opus, and Med-PaLM 2. AutoMedPrompt sets a new state-of-the-art (SOTA) performance on PubMedQA with an accuracy of 82.6$\%$, while also outperforming previous prompting strategies on open-sourced models for MedQA (77.7$\%$) and NephSAP (63.8$\%$).

### CVE-LLM : Ontology-Assisted Automatic Vulnerability Evaluation Using Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.15932)] [[cool](https://papers.cool/arxiv/2502.15932)] [[pdf](https://arxiv.org/pdf/2502.15932)]
> **Authors**: Rikhiya Ghosh,Hans-Martin von Stockhausen,Martin Schmitt,George Marica Vasile,Sanjeev Kumar Karn,Oladimeji Farri
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: arXiv admin note: substantial text overlap with arXiv:2407.14640
- **标题**: None
- **领域**: 计算语言学,密码学和安全
- **Abstract**: The National Vulnerability Database (NVD) publishes over a thousand new vulnerabilities monthly, with a projected 25 percent increase in 2024, highlighting the crucial need for rapid vulnerability identification to mitigate cybersecurity attacks and save costs and resources. In this work, we propose using large language models (LLMs) to learn vulnerability evaluation from historical assessments of medical device vulnerabilities in a single manufacturer's portfolio. We highlight the effectiveness and challenges of using LLMs for automatic vulnerability evaluation and introduce a method to enrich historical data with cybersecurity ontologies, enabling the system to understand new vulnerabilities without retraining the LLM. Our LLM system integrates with the in-house application - Cybersecurity Management System (CSMS) - to help Siemens Healthineers (SHS) product cybersecurity experts efficiently assess the vulnerabilities in our products. Also, we present guidelines for efficient integration of LLMs into the cybersecurity tool.

### Improving Consistency in Large Language Models through Chain of Guidance 
[[arxiv](https://arxiv.org/abs/2502.15924)] [[cool](https://papers.cool/arxiv/2502.15924)] [[pdf](https://arxiv.org/pdf/2502.15924)]
> **Authors**: Harsh Raj,Vipul Gupta,Domenic Rosati,Subhabrata Majumdar
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Accepted at Transactions ofMachineLearningResearch (TMLR) 2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Consistency is a fundamental dimension of trustworthiness in Large Language Models (LLMs). For humans to be able to trust LLM-based applications, their outputs should be consistent when prompted with inputs that carry the same meaning or intent. Despite this need, there is no known mechanism to control and guide LLMs to be more consistent at inference time. In this paper, we introduce a novel alignment strategy to maximize semantic consistency in LLM outputs. Our proposal is based on Chain of Guidance (CoG), a multistep prompting technique that generates highly consistent outputs from LLMs. For closed-book question-answering (Q&A) tasks, when compared to direct prompting, the outputs generated using CoG show improved consistency. While other approaches like template-based responses and majority voting may offer alternative paths to consistency, our work focuses on exploring the potential of guided prompting. We use synthetic data sets comprised of consistent input-output pairs to fine-tune LLMs to produce consistent and correct outputs. Our fine-tuned models are more than twice as consistent compared to base models and show strong generalization capabilities by producing consistent outputs over datasets not used in the fine-tuning process.

### Self-Taught Agentic Long Context Understanding 
[[arxiv](https://arxiv.org/abs/2502.15920)] [[cool](https://papers.cool/arxiv/2502.15920)] [[pdf](https://arxiv.org/pdf/2502.15920)]
> **Authors**: Yufan Zhuang,Xiaodong Yu,Jialian Wu,Ximeng Sun,Ze Wang,Jiang Liu,Yusheng Su,Jingbo Shang,Zicheng Liu,Emad Barsoum
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Answering complex, long-context questions remains a major challenge for large language models (LLMs) as it requires effective question clarifications and context retrieval. We propose Agentic Long-Context Understanding (AgenticLU), a framework designed to enhance an LLM's understanding of such queries by integrating targeted self-clarification with contextual grounding within an agentic workflow. At the core of AgenticLU is Chain-of-Clarifications (CoC), where models refine their understanding through self-generated clarification questions and corresponding contextual groundings. By scaling inference as a tree search where each node represents a CoC step, we achieve 97.8% answer recall on NarrativeQA with a search depth of up to three and a branching factor of eight. To amortize the high cost of this search process to training, we leverage the preference pairs for each step obtained by the CoC workflow and perform two-stage model finetuning: (1) supervised finetuning to learn effective decomposition strategies, and (2) direct preference optimization to enhance reasoning quality. This enables AgenticLU models to generate clarifications and retrieve relevant context effectively and efficiently in a single inference pass. Extensive experiments across seven long-context tasks demonstrate that AgenticLU significantly outperforms state-of-the-art prompting methods and specialized long-context LLMs, achieving robust multi-hop reasoning while sustaining consistent performance as context length grows.

### Mind the Gap! Static and Interactive Evaluations of Large Audio Models 
[[arxiv](https://arxiv.org/abs/2502.15919)] [[cool](https://papers.cool/arxiv/2502.15919)] [[pdf](https://arxiv.org/pdf/2502.15919)]
> **Authors**: Minzhi Li,William Barr Held,Michael J Ryan,Kunat Pipatanakul,Potsawee Manakul,Hao Zhu,Diyi Yang
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,声音,音频和语音处理
- **Abstract**: As AI chatbots become ubiquitous, voice interaction presents a compelling way to enable rapid, high-bandwidth communication for both semantic and social signals. This has driven research into Large Audio Models (LAMs) to power voice-native experiences. However, aligning LAM development with user goals requires a clear understanding of user needs and preferences to establish reliable progress metrics. This study addresses these challenges by introducing an interactive approach to evaluate LAMs and collecting 7,500 LAM interactions from 484 participants. Through topic modeling of user queries, we identify primary use cases for audio interfaces. We then analyze user preference rankings and qualitative feedback to determine which models best align with user needs. Finally, we evaluate how static benchmarks predict interactive performance - our analysis reveals no individual benchmark strongly correlates with interactive results ($τ\leq 0.33$ for all benchmarks). While combining multiple coarse-grained features yields modest predictive power ($R^2$=$0.30$), only two out of twenty datasets on spoken question answering and age prediction show significantly positive correlations. This suggests a clear need to develop LAM evaluations that better correlate with user preferences.

### Modality-Aware Neuron Pruning for Unlearning in Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.15910)] [[cool](https://papers.cool/arxiv/2502.15910)] [[pdf](https://arxiv.org/pdf/2502.15910)]
> **Authors**: Zheyuan Liu,Guangyao Dou,Xiangchi Yuan,Chunhui Zhang,Zhaoxuan Tan,Meng Jiang
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 19 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Generative models such as Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) trained on massive datasets can lead them to memorize and inadvertently reveal sensitive information, raising ethical and privacy concerns. While some prior works have explored this issue in the context of LLMs, it presents a unique challenge for MLLMs due to the entangled nature of knowledge across modalities, making comprehensive unlearning more difficult. To address this challenge, we propose Modality Aware Neuron Unlearning (MANU), a novel unlearning framework for MLLMs designed to selectively clip neurons based on their relative importance to the targeted forget data, curated for different modalities. Specifically, MANU consists of two stages: important neuron selection and selective pruning. The first stage identifies and collects the most influential neurons across modalities relative to the targeted forget knowledge, while the second stage is dedicated to pruning those selected neurons. MANU effectively isolates and removes the neurons that contribute most to the forget data within each modality, while preserving the integrity of retained knowledge. Our experiments conducted across various MLLM architectures illustrate that MANU can achieve a more balanced and comprehensive unlearning in each modality without largely affecting the overall model utility.

### A Close Look at Decomposition-based XAI-Methods for Transformer Language Models 
[[arxiv](https://arxiv.org/abs/2502.15886)] [[cool](https://papers.cool/arxiv/2502.15886)] [[pdf](https://arxiv.org/pdf/2502.15886)]
> **Authors**: Leila Arras,Bruno Puri,Patrick Kahardipraja,Sebastian Lapuschkin,Wojciech Samek
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 9 pages, 3 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Various XAI attribution methods have been recently proposed for the transformer architecture, allowing for insights into the decision-making process of large language models by assigning importance scores to input tokens and intermediate representations. One class of methods that seems very promising in this direction includes decomposition-based approaches, i.e., XAI-methods that redistribute the model's prediction logit through the network, as this value is directly related to the prediction. In the previous literature we note though that two prominent methods of this category, namely ALTI-Logit and LRP, have not yet been analyzed in juxtaposition and hence we propose to close this gap by conducting a careful quantitative evaluation w.r.t. ground truth annotations on a subject-verb agreement task, as well as various qualitative inspections, using BERT, GPT-2 and LLaMA-3 as a testbed. Along the way we compare and extend the ALTI-Logit and LRP methods, including the recently proposed AttnLRP variant, from an algorithmic and implementation perspective. We further incorporate in our benchmark two widely-used gradient-based attribution techniques. Finally, we make our carefullly constructed benchmark dataset for evaluating attributions on language models, as well as our code, publicly available in order to foster evaluation of XAI-methods on a well-defined common ground.

### MutaGReP: Execution-Free Repository-Grounded Plan Search for Code-Use 
[[arxiv](https://arxiv.org/abs/2502.15872)] [[cool](https://papers.cool/arxiv/2502.15872)] [[pdf](https://arxiv.org/pdf/2502.15872)]
> **Authors**: Zaid Khan,Ali Farhadi,Ranjay Krishna,Luca Weihs,Mohit Bansal,Tanmay Gupta
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Project page: zaidkhan.me/MutaGReP
- **标题**: None
- **领域**: 计算语言学,人工智能,软件工程
- **Abstract**: When a human requests an LLM to complete a coding task using functionality from a large code repository, how do we provide context from the repo to the LLM? One approach is to add the entire repo to the LLM's context window. However, most tasks involve only fraction of symbols from a repo, longer contexts are detrimental to the LLM's reasoning abilities, and context windows are not unlimited. Alternatively, we could emulate the human ability to navigate a large repo, pick out the right functionality, and form a plan to solve the task. We propose MutaGReP (Mutation-guided Grounded Repository Plan Search), an approach to search for plans that decompose a user request into natural language steps grounded in the codebase. MutaGReP performs neural tree search in plan space, exploring by mutating plans and using a symbol retriever for grounding. On the challenging LongCodeArena benchmark, our plans use less than 5% of the 128K context window for GPT-4o but rival the coding performance of GPT-4o with a context window filled with the repo. Plans produced by MutaGReP allow Qwen 2.5 Coder 32B and 72B to match the performance of GPT-4o with full repo context and enable progress on the hardest LongCodeArena tasks. Project page: zaidkhan.me/MutaGReP

### Synthetic vs. Gold: The Role of LLM-Generated Labels and Data in Cyberbullying Detection 
[[arxiv](https://arxiv.org/abs/2502.15860)] [[cool](https://papers.cool/arxiv/2502.15860)] [[pdf](https://arxiv.org/pdf/2502.15860)]
> **Authors**: Arefeh Kazemi,Sri Balaaji Natarajan Kalaivendan,Joachim Wagner,Hamza Qadeer,Brian Davis
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: This study investigates the role of LLM-generated synthetic data in cyberbullying detection. We conduct a series of experiments where we replace some or all of the authentic data with synthetic data, or augment the authentic data with synthetic data. We find that synthetic cyberbullying data can be the basis for training a classifier for harm detection that reaches performance close to that of a classifier trained with authentic data. Combining authentic with synthetic data shows improvements over the baseline of training on authentic data alone for the test data for all three LLMs tried. These results highlight the viability of synthetic data as a scalable, ethically viable alternative in cyberbullying detection while emphasizing the critical impact of LLM selection on performance outcomes.

### PPC-GPT: Federated Task-Specific Compression of Large Language Models via Pruning and Chain-of-Thought Distillation 
[[arxiv](https://arxiv.org/abs/2502.15857)] [[cool](https://papers.cool/arxiv/2502.15857)] [[pdf](https://arxiv.org/pdf/2502.15857)]
> **Authors**: Tao Fan,Guoqiang Ma,Yuanfeng Song,Lixin Fan,Kai Chen,Qiang Yang
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Compressing Large Language Models (LLMs) into task-specific Small Language Models (SLMs) encounters two significant challenges: safeguarding domain-specific knowledge privacy and managing limited resources. To tackle these challenges, we propose PPC-GPT, a innovative privacy-preserving federated framework specifically designed for compressing LLMs into task-specific SLMs via pruning and Chain-of-Thought (COT) distillation. PPC-GPT works on a server-client federated architecture, where the client sends differentially private (DP) perturbed task-specific data to the server's LLM. The LLM then generates synthetic data along with their corresponding rationales. This synthetic data is subsequently used for both LLM pruning and retraining processes. Additionally, we harness COT knowledge distillation, leveraging the synthetic data to further improve the retraining of structurally-pruned SLMs. Our experimental results demonstrate the effectiveness of PPC-GPT across various text generation tasks. By compressing LLMs into task-specific SLMs, PPC-GPT not only achieves competitive performance but also prioritizes data privacy protection.

## 计算机视觉和模式识别(cs.CV:Computer Vision and Pattern Recognition)

### Human Motion Prediction, Reconstruction, and Generation 
[[arxiv](https://arxiv.org/abs/2502.15956)] [[cool](https://papers.cool/arxiv/2502.15956)] [[pdf](https://arxiv.org/pdf/2502.15956)]
> **Authors**: Canxuan Gang,Yiran Wang
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Tech report
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: This report reviews recent advancements in human motion prediction, reconstruction, and generation. Human motion prediction focuses on forecasting future poses and movements from historical data, addressing challenges like nonlinear dynamics, occlusions, and motion style variations. Reconstruction aims to recover accurate 3D human body movements from visual inputs, often leveraging transformer-based architectures, diffusion models, and physical consistency losses to handle noise and complex poses. Motion generation synthesizes realistic and diverse motions from action labels, textual descriptions, or environmental constraints, with applications in robotics, gaming, and virtual avatars. Additionally, text-to-motion generation and human-object interaction modeling have gained attention, enabling fine-grained and context-aware motion synthesis for augmented reality and robotics. This review highlights key methodologies, datasets, challenges, and future research directions driving progress in these fields.

### Graph Attention Convolutional U-NET: A Semantic Segmentation Model for Identifying Flooded Areas 
[[arxiv](https://arxiv.org/abs/2502.15907)] [[cool](https://papers.cool/arxiv/2502.15907)] [[pdf](https://arxiv.org/pdf/2502.15907)]
> **Authors**: Muhammad Umair Danish,Madhushan Buwaneswaran,Tehara Fonseka,Katarina Grolinger
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: ef:IECON 2024 - 50th Annual Conference of the IEEE Industrial Electronics Society, 2024
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: The increasing impact of human-induced climate change and unplanned urban constructions has increased flooding incidents in recent years. Accurate identification of flooded areas is crucial for effective disaster management and urban planning. While few works have utilized convolutional neural networks and transformer-based semantic segmentation techniques for identifying flooded areas from aerial footage, recent developments in graph neural networks have created improvement opportunities. This paper proposes an innovative approach, the Graph Attention Convolutional U-NET (GAC-UNET) model, based on graph neural networks for automated identification of flooded areas. The model incorporates a graph attention mechanism and Chebyshev layers into the U-Net architecture. Furthermore, this paper explores the applicability of transfer learning and model reprogramming to enhance the accuracy of flood area segmentation models. Empirical results demonstrate that the proposed GAC-UNET model, outperforms other approaches with 91\% mAP, 94\% dice score, and 89\% IoU, providing valuable insights for informed decision-making and better planning of future infrastructures in flood-prone areas.

### RIFLEx: A Free Lunch for Length Extrapolation in Video Diffusion Transformers 
[[arxiv](https://arxiv.org/abs/2502.15894)] [[cool](https://papers.cool/arxiv/2502.15894)] [[pdf](https://arxiv.org/pdf/2502.15894)]
> **Authors**: Min Zhao,Guande He,Yixiao Chen,Hongzhou Zhu,Chongxuan Li,Jun Zhu
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Recent advancements in video generation have enabled models to synthesize high-quality, minute-long videos. However, generating even longer videos with temporal coherence remains a major challenge, and existing length extrapolation methods lead to temporal repetition or motion deceleration. In this work, we systematically analyze the role of frequency components in positional embeddings and identify an intrinsic frequency that primarily governs extrapolation behavior. Based on this insight, we propose RIFLEx, a minimal yet effective approach that reduces the intrinsic frequency to suppress repetition while preserving motion consistency, without requiring any additional modifications. RIFLEx offers a true free lunch--achieving high-quality $2\times$ extrapolation on state-of-the-art video diffusion transformers in a completely training-free manner. Moreover, it enhances quality and enables $3\times$ extrapolation by minimal fine-tuning without long videos. Project page and codes: \href{https://riflex-video.github.io/}{https://riflex-video.github.io/.}

### Understanding and Evaluating Hallucinations in 3D Visual Language Models 
[[arxiv](https://arxiv.org/abs/2502.15888)] [[cool](https://papers.cool/arxiv/2502.15888)] [[pdf](https://arxiv.org/pdf/2502.15888)]
> **Authors**: Ruiying Peng,Kaiyuan Li,Weichen Zhang,Chen Gao,Xinlei Chen,Yong Li
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Recently, 3D-LLMs, which combine point-cloud encoders with large models, have been proposed to tackle complex tasks in embodied intelligence and scene understanding. In addition to showing promising results on 3D tasks, we found that they are significantly affected by hallucinations. For instance, they may generate objects that do not exist in the scene or produce incorrect relationships between objects. To investigate this issue, this work presents the first systematic study of hallucinations in 3D-LLMs. We begin by quickly evaluating hallucinations in several representative 3D-LLMs and reveal that they are all significantly affected by hallucinations. We then define hallucinations in 3D scenes and, through a detailed analysis of datasets, uncover the underlying causes of these hallucinations. We find three main causes: (1) Uneven frequency distribution of objects in the dataset. (2) Strong correlations between objects. (3) Limited diversity in object attributes. Additionally, we propose new evaluation metrics for hallucinations, including Random Point Cloud Pair and Opposite Question Evaluations, to assess whether the model generates responses based on visual information and aligns it with the text's meaning.

### DOEI: Dual Optimization of Embedding Information for Attention-Enhanced Class Activation Maps 
[[arxiv](https://arxiv.org/abs/2502.15885)] [[cool](https://papers.cool/arxiv/2502.15885)] [[pdf](https://arxiv.org/pdf/2502.15885)]
> **Authors**: Hongjie Zhu,Zeyu Zhang,Guansong Pang,Xu Wang,Shimin Wen,Yu Bai,Daji Ergu,Ying Cai,Yang Zhao
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Weakly supervised semantic segmentation (WSSS) typically utilizes limited semantic annotations to obtain initial Class Activation Maps (CAMs). However, due to the inadequate coupling between class activation responses and semantic information in high-dimensional space, the CAM is prone to object co-occurrence or under-activation, resulting in inferior recognition accuracy. To tackle this issue, we propose DOEI, Dual Optimization of Embedding Information, a novel approach that reconstructs embedding representations through semantic-aware attention weight matrices to optimize the expression capability of embedding information. Specifically, DOEI amplifies tokens with high confidence and suppresses those with low confidence during the class-to-patch interaction. This alignment of activation responses with semantic information strengthens the propagation and decoupling of target features, enabling the generated embeddings to more accurately represent target features in high-level semantic space. In addition, we propose a hybrid-feature alignment module in DOEI that combines RGB values, embedding-guided features, and self-attention weights to increase the reliability of candidate tokens. Comprehensive experiments show that DOEI is an effective plug-and-play module that empowers state-of-the-art visual transformer-based WSSS models to significantly improve the quality of CAMs and segmentation performance on popular benchmarks, including PASCAL VOC (+3.6%, +1.5%, +1.2% mIoU) and MS COCO (+1.2%, +1.6% mIoU). Code will be available at https://github.com/AIGeeksGroup/DOEI.

### A Critical Assessment of Modern Generative Models' Ability to Replicate Artistic Styles 
[[arxiv](https://arxiv.org/abs/2502.15856)] [[cool](https://papers.cool/arxiv/2502.15856)] [[pdf](https://arxiv.org/pdf/2502.15856)]
> **Authors**: Andrea Asperti,Franky George,Tiberio Marras,Razvan Ciprian Stricescu,Fabio Zanotti
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: :68T05ACM Class:I.2.10; I.5.4
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: In recent years, advancements in generative artificial intelligence have led to the development of sophisticated tools capable of mimicking diverse artistic styles, opening new possibilities for digital creativity and artistic expression. This paper presents a critical assessment of the style replication capabilities of contemporary generative models, evaluating their strengths and limitations across multiple dimensions. We examine how effectively these models reproduce traditional artistic styles while maintaining structural integrity and compositional balance in the generated images. The analysis is based on a new large dataset of AI-generated works imitating artistic styles of the past, holding potential for a wide range of applications: the "AI-pastiche" dataset. The study is supported by extensive user surveys, collecting diverse opinions on the dataset and investigation both technical and aesthetic challenges, including the ability to generate outputs that are realistic and visually convincing, the versatility of models in handling a wide range of artistic styles, and the extent to which they adhere to the content and stylistic specifications outlined in prompts. This paper aims to provide a comprehensive overview of the current state of generative tools in style replication, offering insights into their technical and artistic limitations, potential advancements in model design and training methodologies, and emerging opportunities for enhancing digital artistry, human-AI collaboration, and the broader creative landscape.

## 计算机与社会(cs.CY:Computers and Society)

### A Comprehensive Survey on the Trustworthiness of Large Language Models in Healthcare 
[[arxiv](https://arxiv.org/abs/2502.15871)] [[cool](https://papers.cool/arxiv/2502.15871)] [[pdf](https://arxiv.org/pdf/2502.15871)]
> **Authors**: Manar Aljohani,Jun Hou,Sindhura Kommu,Xuan Wang
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,人工智能,计算语言学
- **Abstract**: The application of large language models (LLMs) in healthcare has the potential to revolutionize clinical decision-making, medical research, and patient care. As LLMs are increasingly integrated into healthcare systems, several critical challenges must be addressed to ensure their reliable and ethical deployment. These challenges include truthfulness, where models generate misleading information; privacy, with risks of unintentional data retention; robustness, requiring defenses against adversarial attacks; fairness, addressing biases in clinical outcomes; explainability, ensuring transparent decision-making; and safety, mitigating risks of misinformation and medical errors. Recently, researchers have begun developing benchmarks and evaluation frameworks to systematically assess the trustworthiness of LLMs. However, the trustworthiness of LLMs in healthcare remains underexplored, lacking a systematic review that provides a comprehensive understanding and future insights into this area. This survey bridges this gap by providing a comprehensive overview of the recent research of existing methodologies and solutions aimed at mitigating the above risks in healthcare. By focusing on key trustworthiness dimensions including truthfulness, privacy and safety, robustness, fairness and bias, and explainability, we present a thorough analysis of how these issues impact the reliability and ethical use of LLMs in healthcare. This paper highlights ongoing efforts and offers insights into future research directions to ensure the safe and trustworthy deployment of LLMs in healthcare.

### Making Sense of AI Limitations: How Individual Perceptions Shape Organizational Readiness for AI Adoption 
[[arxiv](https://arxiv.org/abs/2502.15870)] [[cool](https://papers.cool/arxiv/2502.15870)] [[pdf](https://arxiv.org/pdf/2502.15870)]
> **Authors**: Thomas Übellacker
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,人工智能,人机交互
- **Abstract**: This study investigates how individuals' perceptions of artificial intelligence (AI) limitations influence organizational readiness for AI adoption. Through semi-structured interviews with seven AI implementation experts, analyzed using the Gioia methodology, the research reveals that organizational readiness emerges through dynamic interactions between individual sensemaking, social learning, and formal integration processes. The findings demonstrate that hands-on experience with AI limitations leads to more realistic expectations and increased trust, mainly when supported by peer networks and champion systems. Organizations that successfully translate these individual and collective insights into formal governance structures achieve more sustainable AI adoption. The study advances theory by showing how organizational readiness for AI adoption evolves through continuous cycles of individual understanding, social learning, and organizational adaptation. These insights suggest that organizations should approach AI adoption not as a one-time implementation but as an ongoing strategic learning process that balances innovation with practical constraints. The research contributes to organizational readiness theory and practice by illuminating how micro-level perceptions and experiences shape macro-level adoption outcomes.

### AI Governance InternationaL Evaluation Index (AGILE Index) 
[[arxiv](https://arxiv.org/abs/2502.15859)] [[cool](https://papers.cool/arxiv/2502.15859)] [[pdf](https://arxiv.org/pdf/2502.15859)]
> **Authors**: Yi Zeng,Enmeng Lu,Xin Guan,Cunqing Huangfu,Zizhe Ruan,Ammar Younas,Kang Sun,Xuan Tang,Yuwei Wang,Hongjie Suo,Dongqi Liang,Zhengqiang Han,Aorigele Bao,Xiaoyang Guo,Jin Wang,Jiawei Xie,Yao Liang
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Evaluation Report. 85 pages, 30 Figures
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: The rapid advancement of Artificial Intelligence (AI) technology is profoundly transforming human society and concurrently presenting a series of ethical, legal, and social issues. The effective governance of AI has become a crucial global concern. Since 2022, the extensive deployment of generative AI, particularly large language models, marked a new phase in AI governance. Continuous efforts are being made by the international community in actively addressing the novel challenges posed by these AI developments. As consensus on international governance continues to be established and put into action, the practical importance of conducting a global assessment of the state of AI governance is progressively coming to light. In this context, we initiated the development of the AI Governance InternationaL Evaluation Index (AGILE Index). Adhering to the design principle, "the level of governance should match the level of development," the inaugural evaluation of the AGILE Index commences with an exploration of four foundational pillars: the development level of AI, the AI governance environment, the AI governance instruments, and the AI governance effectiveness. It covers 39 indicators across 18 dimensions to comprehensively assess the AI governance level of 14 representative countries globally. The index is utilized to delve into the status of AI governance to date in 14 countries for the first batch of evaluation. The aim is to depict the current state of AI governance in these countries through data scoring, assist them in identifying their governance stage and uncovering governance issues, and ultimately offer insights for the enhancement of their AI governance systems.

### Generative AI Training and Copyright Law 
[[arxiv](https://arxiv.org/abs/2502.15858)] [[cool](https://papers.cool/arxiv/2502.15858)] [[pdf](https://arxiv.org/pdf/2502.15858)]
> **Authors**: Tim W. Dornis,Sebastian Stober
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: submitted as an overview article to the Transactions of the International Society for Music Information Retrieval
- **标题**: None
- **领域**: 计算机与社会,人工智能,机器学习
- **Abstract**: Training generative AI models requires extensive amounts of data. A common practice is to collect such data through web scraping. Yet, much of what has been and is collected is copyright protected. Its use may be copyright infringement. In the USA, AI developers rely on "fair use" and in Europe, the prevailing view is that the exception for "Text and Data Mining" (TDM) applies. In a recent interdisciplinary tandem-study, we have argued in detail that this is actually not the case because generative AI training fundamentally differs from TDM. In this article, we share our main findings and the implications for both public and corporate research on generative models. We further discuss how the phenomenon of training data memorization leads to copyright issues independently from the "fair use" and TDM exceptions. Finally, we outline how the ISMIR could contribute to the ongoing discussion about fair practices with respect to generative AI that satisfy all stakeholders.

## 数据结构和算法(cs.DS:Data Structures and Algorithms)

### Compression Barriers for Autoregressive Transformers 
[[arxiv](https://arxiv.org/abs/2502.15955)] [[cool](https://papers.cool/arxiv/2502.15955)] [[pdf](https://arxiv.org/pdf/2502.15955)]
> **Authors**: Themistoklis Haris,Krzysztof Onak
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 数据结构和算法,人工智能,计算复杂度,机器学习
- **Abstract**: A key limitation of autoregressive Transformers is the large memory needed at inference-time to cache all previous key-value (KV) embeddings. Prior works address this by compressing the KV cache, but often assume specific structural properties of the embeddings. This raises the following natural question: Can truly sublinear space utilization be achieved without such assumptions? In this work, we answer this question in the negative. Any algorithm for attention-based token generation must use $Θ(nd)$ space, where $n$ is the number of tokens generated so far and $d = Ω(\log n)$ is the dimension of the KV embeddings. Our proof involves a reduction from a classic communication complexity problem and uses a randomized construction that leverages properties of projections in the spirit of the Johnson-Linderstrauss lemma. For the low-dimensional regime $d = o(\log n)$, we show that any algorithm requires $Ω(d\cdot e^d)$ space and prove, using tight bounds on covering numbers, that SubGen, proposed by Zandieh, Han, Mirrokni and Karbasi, matches this bound. Further, we investigate how sparsity assumptions enable token generation in truly sublinear space, presenting impossibility results and proposing a new KV cache compression algorithm for sliding window attention when the value cache outside the window is unmasked. Finally, we analyze token generation's time complexity, using an indistinguishability argument to prove that no non-adaptive algorithm can compute attention online in sublinear time for all tokens.

## 图形(cs.GR:Graphics)

### Generative AI Framework for 3D Object Generation in Augmented Reality 
[[arxiv](https://arxiv.org/abs/2502.15869)] [[cool](https://papers.cool/arxiv/2502.15869)] [[pdf](https://arxiv.org/pdf/2502.15869)]
> **Authors**: Majid Behravan
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 图形,人工智能,计算机视觉和模式识别,人机交互
- **Abstract**: This thesis presents a framework that integrates state-of-the-art generative AI models for real-time creation of three-dimensional (3D) objects in augmented reality (AR) environments. The primary goal is to convert diverse inputs, such as images and speech, into accurate 3D models, enhancing user interaction and immersion. Key components include advanced object detection algorithms, user-friendly interaction techniques, and robust AI models like Shap-E for 3D generation. Leveraging Vision Language Models (VLMs) and Large Language Models (LLMs), the system captures spatial details from images and processes textual information to generate comprehensive 3D objects, seamlessly integrating virtual objects into real-world environments. The framework demonstrates applications across industries such as gaming, education, retail, and interior design. It allows players to create personalized in-game assets, customers to see products in their environments before purchase, and designers to convert real-world objects into 3D models for real-time visualization. A significant contribution is democratizing 3D model creation, making advanced AI tools accessible to a broader audience, fostering creativity and innovation. The framework addresses challenges like handling multilingual inputs, diverse visual data, and complex environments, improving object detection and model generation accuracy, as well as loading 3D models in AR space in real-time. In conclusion, this thesis integrates generative AI and AR for efficient 3D model generation, enhancing accessibility and paving the way for innovative applications and improved user interactions in AR environments.

## 机器学习(cs.LG:Machine Learning)

### Towards Understanding Gradient Flow Dynamics of Homogeneous Neural Networks Beyond the Origin 
[[arxiv](https://arxiv.org/abs/2502.15952)] [[cool](https://papers.cool/arxiv/2502.15952)] [[pdf](https://arxiv.org/pdf/2502.15952)]
> **Authors**: Akshay Kumar,Jarvis Haupt
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,优化与控制,机器学习
- **Abstract**: Recent works exploring the training dynamics of homogeneous neural network weights under gradient flow with small initialization have established that in the early stages of training, the weights remain small and near the origin, but converge in direction. Building on this, the current paper studies the gradient flow dynamics of homogeneous neural networks with locally Lipschitz gradients, after they escape the origin. Insights gained from this analysis are used to characterize the first saddle point encountered by gradient flow after escaping the origin. Also, it is shown that for homogeneous feed-forward neural networks, under certain conditions, the sparsity structure emerging among the weights before the escape is preserved after escaping the origin and until reaching the next saddle point.

### Optimizing Pre-Training Data Mixtures with Mixtures of Data Expert Models 
[[arxiv](https://arxiv.org/abs/2502.15950)] [[cool](https://papers.cool/arxiv/2502.15950)] [[pdf](https://arxiv.org/pdf/2502.15950)]
> **Authors**: Lior Belenki,Alekh Agarwal,Tianze Shi,Kristina Toutanova
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: We propose a method to optimize language model pre-training data mixtures through efficient approximation of the cross-entropy loss corresponding to each candidate mixture via a Mixture of Data Experts (MDE). We use this approximation as a source of additional features in a regression model, trained from observations of model loss for a small number of mixtures. Experiments with Transformer decoder-only language models in the range of 70M to 1B parameters on the SlimPajama dataset show that our method achieves significantly better performance than approaches that train regression models using only the mixture rates as input features. Combining this improved optimization method with an objective that takes into account cross-entropy on end task data leads to superior performance on few-shot downstream evaluations. We also provide theoretical insights on why aggregation of data expert predictions can provide good approximations to model losses for data mixtures.

### Orthogonal Calibration for Asynchronous Federated Learning 
[[arxiv](https://arxiv.org/abs/2502.15940)] [[cool](https://papers.cool/arxiv/2502.15940)] [[pdf](https://arxiv.org/pdf/2502.15940)]
> **Authors**: Jiayun Zhang,Shuheng Li,Haiyu Huang,Xiaofan Yu,Rajesh K. Gupta,Jingbo Shang
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,分布式、并行和集群计算
- **Abstract**: Asynchronous federated learning mitigates the inefficiency of conventional synchronous aggregation by integrating updates as they arrive and adjusting their influence based on staleness. Due to asynchrony and data heterogeneity, learning objectives at the global and local levels are inherently inconsistent -- global optimization trajectories may conflict with ongoing local updates. Existing asynchronous methods simply distribute the latest global weights to clients, which can overwrite local progress and cause model drift. In this paper, we propose OrthoFL, an orthogonal calibration framework that decouples global and local learning progress and adjusts global shifts to minimize interference before merging them into local models. In OrthoFL, clients and the server maintain separate model weights. Upon receiving an update, the server aggregates it into the global weights via a moving average. For client weights, the server computes the global weight shift accumulated during the client's delay and removes the components aligned with the direction of the received update. The resulting parameters lie in a subspace orthogonal to the client update and preserve the maximal information from the global progress. The calibrated global shift is then merged into the client weights for further training. Extensive experiments show that OrthoFL improves accuracy by 9.6% and achieves a 12$\times$ speedup compared to synchronous methods. Moreover, it consistently outperforms state-of-the-art asynchronous baselines under various delay patterns and heterogeneity scenarios.

### Straight to Zero: Why Linearly Decaying the Learning Rate to Zero Works Best for LLMs 
[[arxiv](https://arxiv.org/abs/2502.15938)] [[cool](https://papers.cool/arxiv/2502.15938)] [[pdf](https://arxiv.org/pdf/2502.15938)]
> **Authors**: Shane Bergsma,Nolan Dey,Gurpreet Gosal,Gavia Gray,Daria Soboleva,Joel Hestness
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: ICLR 2025
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,神经和进化计算
- **Abstract**: LLMs are commonly trained with a learning rate (LR) warmup, followed by cosine decay to 10% of the maximum (10x decay). In a large-scale empirical study, we show that under an optimal peak LR, a simple linear decay-to-zero (D2Z) schedule consistently outperforms other schedules when training at compute-optimal dataset sizes. D2Z is superior across a range of model sizes, batch sizes, datasets, and vocabularies. Benefits increase as dataset size increases. Leveraging a novel interpretation of AdamW as an exponential moving average of weight updates, we show how linear D2Z optimally balances the demands of early training (moving away from initial conditions) and late training (averaging over more updates in order to mitigate gradient noise). In experiments, a 610M-parameter model trained for 80 tokens-per-parameter (TPP) using D2Z achieves lower loss than when trained for 200 TPP using 10x decay, corresponding to an astonishing 60% compute savings. Models such as Llama2-7B, trained for 286 TPP with 10x decay, could likely have saved a majority of compute by training with D2Z.

### On the Design of Safe Continual RL Methods for Control of Nonlinear Systems 
[[arxiv](https://arxiv.org/abs/2502.15922)] [[cool](https://papers.cool/arxiv/2502.15922)] [[pdf](https://arxiv.org/pdf/2502.15922)]
> **Authors**: Austin Coursey,Marcos Quinones-Grueiro,Gautam Biswas
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器人技术
- **Abstract**: Reinforcement learning (RL) algorithms have been successfully applied to control tasks associated with unmanned aerial vehicles and robotics. In recent years, safe RL has been proposed to allow the safe execution of RL algorithms in industrial and mission-critical systems that operate in closed loops. However, if the system operating conditions change, such as when an unknown fault occurs in the system, typical safe RL algorithms are unable to adapt while retaining past knowledge. Continual reinforcement learning algorithms have been proposed to address this issue. However, the impact of continual adaptation on the system's safety is an understudied problem. In this paper, we study the intersection of safe and continual RL. First, we empirically demonstrate that a popular continual RL algorithm, online elastic weight consolidation, is unable to satisfy safety constraints in non-linear systems subject to varying operating conditions. Specifically, we study the MuJoCo HalfCheetah and Ant environments with velocity constraints and sudden joint loss non-stationarity. Then, we show that an agent trained using constrained policy optimization, a safe RL algorithm, experiences catastrophic forgetting in continual learning settings. With this in mind, we explore a simple reward-shaping method to ensure that elastic weight consolidation prioritizes remembering both safety and task performance for safety-constrained, non-linear, and non-stationary dynamical systems.

### Connecting the geometry and dynamics of many-body complex systems with message passing neural operators 
[[arxiv](https://arxiv.org/abs/2502.15913)] [[cool](https://papers.cool/arxiv/2502.15913)] [[pdf](https://arxiv.org/pdf/2502.15913)]
> **Authors**: Nicholas A. Gabriel,Neil F. Johnson,George Em Karniadakis
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,物理与社会
- **Abstract**: The relationship between scale transformations and dynamics established by renormalization group techniques is a cornerstone of modern physical theories, from fluid mechanics to elementary particle physics. Integrating renormalization group methods into neural operators for many-body complex systems could provide a foundational inductive bias for learning their effective dynamics, while also uncovering multiscale organization. We introduce a scalable AI framework, ROMA (Renormalized Operators with Multiscale Attention), for learning multiscale evolution operators of many-body complex systems. In particular, we develop a renormalization procedure based on neural analogs of the geometric and laplacian renormalization groups, which can be co-learned with neural operators. An attention mechanism is used to model multiscale interactions by connecting geometric representations of local subgraphs and dynamical operators. We apply this framework in challenging conditions: large systems of more than 1M nodes, long-range interactions, and noisy input-output data for two contrasting examples: Kuramoto oscillators and Burgers-like social dynamics. We demonstrate that the ROMA framework improves scalability and positive transfer between forecasting and effective dynamics tasks compared to state-of-the-art operator learning techniques, while also giving insight into multiscale interactions. Additionally, we investigate power law scaling in the number of model parameters, and demonstrate a departure from typical power law exponents in the presence of hierarchical and multiscale interactions.

### IPAD: Inverse Prompt for AI Detection -- A Robust and Explainable LLM-Generated Text Detector 
[[arxiv](https://arxiv.org/abs/2502.15902)] [[cool](https://papers.cool/arxiv/2502.15902)] [[pdf](https://arxiv.org/pdf/2502.15902)]
> **Authors**: Zheng Chen,Yushi Feng,Changyang He,Yue Deng,Hongxi Pu,Bo Li
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Large Language Models (LLMs) have attained human-level fluency in text generation, which complicates the distinguishing between human-written and LLM-generated texts. This increases the risk of misuse and highlights the need for reliable detectors. Yet, existing detectors exhibit poor robustness on out-of-distribution (OOD) data and attacked data, which is critical for real-world scenarios. Also, they struggle to provide explainable evidence to support their decisions, thus undermining the reliability. In light of these challenges, we propose IPAD (Inverse Prompt for AI Detection), a novel framework consisting of a Prompt Inverter that identifies predicted prompts that could have generated the input text, and a Distinguisher that examines how well the input texts align with the predicted prompts. We develop and examine two versions of Distinguishers. Empirical evaluations demonstrate that both Distinguishers perform significantly better than the baseline methods, with version2 outperforming baselines by 9.73% on in-distribution data (F1-score) and 12.65% on OOD data (AUROC). Furthermore, a user study is conducted to illustrate that IPAD enhances the AI detection trustworthiness by allowing users to directly examine the decision-making evidence, which provides interpretable support for its state-of-the-art detection results.

### TS-OOD: Evaluating Time-Series Out-of-Distribution Detection and Prospective Directions for Progress 
[[arxiv](https://arxiv.org/abs/2502.15901)] [[cool](https://papers.cool/arxiv/2502.15901)] [[pdf](https://arxiv.org/pdf/2502.15901)]
> **Authors**: Onat Gungor,Amanda Sofie Rios,Nilesh Ahuja,Tajana Rosing
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Accepted for an oral presentation at AAAI-25 AI4TS
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Detecting out-of-distribution (OOD) data is a fundamental challenge in the deployment of machine learning models. From a security standpoint, this is particularly important because OOD test data can result in misleadingly confident yet erroneous predictions, which undermine the reliability of the deployed model. Although numerous models for OOD detection have been developed in computer vision and language, their adaptability to the time-series data domain remains limited and under-explored. Yet, time-series data is ubiquitous across manufacturing and security applications for which OOD is essential. This paper seeks to address this research gap by conducting a comprehensive analysis of modality-agnostic OOD detection algorithms. We evaluate over several multivariate time-series datasets, deep learning architectures, time-series specific data augmentations, and loss functions. Our results demonstrate that: 1) the majority of state-of-the-art OOD methods exhibit limited performance on time-series data, and 2) OOD methods based on deep feature modeling may offer greater advantages for time-series OOD detection, highlighting a promising direction for future time-series OOD detection algorithm development.

### Explaining the Success of Nearest Neighbor Methods in Prediction 
[[arxiv](https://arxiv.org/abs/2502.15900)] [[cool](https://papers.cool/arxiv/2502.15900)] [[pdf](https://arxiv.org/pdf/2502.15900)]
> **Authors**: George H. Chen,Devavrat Shah
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Originally published on May 31, 2018 in Foundations and Trends inMachineLearning; this revised version fixes some proof details for k-NN and fixed-radius NN regression and classification
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Many modern methods for prediction leverage nearest neighbor search to find past training examples most similar to a test example, an idea that dates back in text to at least the 11th century and has stood the test of time. This monograph aims to explain the success of these methods, both in theory, for which we cover foundational nonasymptotic statistical guarantees on nearest-neighbor-based regression and classification, and in practice, for which we gather prominent methods for approximate nearest neighbor search that have been essential to scaling prediction systems reliant on nearest neighbor analysis to handle massive datasets. Furthermore, we discuss connections to learning distances for use with nearest neighbor methods, including how random decision trees and ensemble methods learn nearest neighbor structure, as well as recent developments in crowdsourcing and graphons. In terms of theory, our focus is on nonasymptotic statistical guarantees, which we state in the form of how many training data and what algorithm parameters ensure that a nearest neighbor prediction method achieves a user-specified error tolerance. We begin with the most general of such results for nearest neighbor and related kernel regression and classification in general metric spaces. In such settings in which we assume very little structure, what enables successful prediction is smoothness in the function being estimated for regression, and a low probability of landing near the decision boundary for classification. In practice, these conditions could be difficult to verify for a real dataset. We then cover recent guarantees on nearest neighbor prediction in the three case studies of time series forecasting, recommending products to people over time, and delineating human organs in medical images by looking at image patches. In these case studies, clustering structure enables successful prediction.

### ML-Driven Approaches to Combat Medicare Fraud: Advances in Class Imbalance Solutions, Feature Engineering, Adaptive Learning, and Business Impact 
[[arxiv](https://arxiv.org/abs/2502.15898)] [[cool](https://papers.cool/arxiv/2502.15898)] [[pdf](https://arxiv.org/pdf/2502.15898)]
> **Authors**: Dorsa Farahmandazad,Kasra Danesh
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Medicare fraud poses a substantial challenge to healthcare systems, resulting in significant financial losses and undermining the quality of care provided to legitimate beneficiaries. This study investigates the use of machine learning (ML) to enhance Medicare fraud detection, addressing key challenges such as class imbalance, high-dimensional data, and evolving fraud patterns. A dataset comprising inpatient claims, outpatient claims, and beneficiary details was used to train and evaluate five ML models: Random Forest, KNN, LDA, Decision Tree, and AdaBoost. Data preprocessing techniques included resampling SMOTE method to address the class imbalance, feature selection for dimensionality reduction, and aggregation of diagnostic and procedural codes. Random Forest emerged as the best-performing model, achieving a training accuracy of 99.2% and validation accuracy of 98.8%, and F1-score (98.4%). The Decision Tree also performed well, achieving a validation accuracy of 96.3%. KNN and AdaBoost demonstrated moderate performance, with validation accuracies of 79.2% and 81.1%, respectively, while LDA struggled with a validation accuracy of 63.3% and a low recall of 16.6%. The results highlight the importance of advanced resampling techniques, feature engineering, and adaptive learning in detecting Medicare fraud effectively. This study underscores the potential of machine learning in addressing the complexities of fraud detection. Future work should explore explainable AI and hybrid models to improve interpretability and performance, ensuring scalable and reliable fraud detection systems that protect healthcare resources and beneficiaries.

### Directional Gradient Projection for Robust Fine-Tuning of Foundation Models 
[[arxiv](https://arxiv.org/abs/2502.15895)] [[cool](https://papers.cool/arxiv/2502.15895)] [[pdf](https://arxiv.org/pdf/2502.15895)]
> **Authors**: Chengyue Huang,Junjiao Tian,Brisa Maneechotesuwan,Shivang Chopra,Zsolt Kira
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Accepted to ICLR 2025
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,计算机视觉和模式识别
- **Abstract**: Robust fine-tuning aims to adapt large foundation models to downstream tasks while preserving their robustness to distribution shifts. Existing methods primarily focus on constraining and projecting current model towards the pre-trained initialization based on the magnitudes between fine-tuned and pre-trained weights, which often require extensive hyper-parameter tuning and can sometimes result in underfitting. In this work, we propose Directional Gradient Projection (DiGraP), a novel layer-wise trainable method that incorporates directional information from gradients to bridge regularization and multi-objective optimization. Besides demonstrating our method on image classification, as another contribution we generalize this area to the multi-modal evaluation settings for robust fine-tuning. Specifically, we first bridge the uni-modal and multi-modal gap by performing analysis on Image Classification reformulated Visual Question Answering (VQA) benchmarks and further categorize ten out-of-distribution (OOD) VQA datasets by distribution shift types and degree (i.e. near versus far OOD). Experimental results show that DiGraP consistently outperforms existing baselines across Image Classfication and VQA tasks with discriminative and generative backbones, improving both in-distribution (ID) generalization and OOD robustness.

### Enhancing Domain-Specific Retrieval-Augmented Generation: Synthetic Data Generation and Evaluation using Reasoning Models 
[[arxiv](https://arxiv.org/abs/2502.15854)] [[cool](https://papers.cool/arxiv/2502.15854)] [[pdf](https://arxiv.org/pdf/2502.15854)]
> **Authors**: Aryan Jadon,Avinash Patil,Shashank Kumar
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 8 Pages
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Retrieval-Augmented Generation (RAG) systems face significant performance gaps when applied to technical domains requiring precise information extraction from complex documents. Current evaluation methodologies relying on document-level metrics inadequately capture token-resolution retrieval accuracy that is critical for domain-related documents. We propose a framework combining granular evaluation metrics with synthetic data generation to optimize domain-specific RAG performance. First, we introduce token-aware metrics Precision $Ω$ and Intersection-over-Union (IoU) that quantify context preservation versus information density trade-offs inherent in technical texts. Second, we develop a reasoning model-driven pipeline using instruction-tuned LLMs (DeepSeek-R1, DeepSeek-R1 distilled variants, and Phi-4) to generate context-anchored QA pairs with discontinuous reference spans across three specialized corpora: SEC 10-K filings (finance), biomedical abstracts (PubMed), and APT threat reports (cybersecurity). Our empirical analysis reveals critical insights: smaller chunks (less than 10 tokens) improve precision by 31-42% (IoU = 0.071 vs. baseline 0.053) at recall costs (-18%), while domain-specific embedding strategies yield 22% variance in optimal chunk sizing (5-20 tokens). The DeepSeek-R1-Distill-Qwen-32B model demonstrates superior concept alignment (+14% mean IoU over alternatives), though no configuration universally dominates. Financial texts favor larger chunks for risk factor coverage (Recall = 0.81 at size = 20), whereas cybersecurity content benefits from atomic segmentation, Precision $Ω= 0.28$ at size = 5. Our code is available on https://github.com/aryan-jadon/Synthetic-Data-Generation-and-Evaluation-using-Reasoning-Model

## 网络和互联网架构(cs.NI:Networking and Internet Architecture)

### Space-O-RAN: Enabling Intelligent, Open, and Interoperable Non Terrestrial Networks in 6G 
[[arxiv](https://arxiv.org/abs/2502.15936)] [[cool](https://papers.cool/arxiv/2502.15936)] [[pdf](https://arxiv.org/pdf/2502.15936)]
> **Authors**: Eduardo Baena,Paolo Testolina,Michele Polese,Dimitrios Koutsonikolas,Josep Jornet,Tommaso Melodia
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 网络和互联网架构,人工智能,系统与控制
- **Abstract**: Non-terrestrial networks (NTNs) are essential for ubiquitous connectivity, providing coverage in remote and underserved areas. However, since NTNs are currently operated independently, they face challenges such as isolation, limited scalability, and high operational costs. Integrating satellite constellations with terrestrial networks offers a way to address these limitations while enabling adaptive and cost-efficient connectivity through the application of Artificial Intelligence (AI) models. This paper introduces Space-O-RAN, a framework that extends Open Radio Access Network (RAN) principles to NTNs. It employs hierarchical closed-loop control with distributed Space RAN Intelligent Controllers (Space-RICs) to dynamically manage and optimize operations across both domains. To enable adaptive resource allocation and network orchestration, the proposed architecture integrates real-time satellite optimization and control with AI-driven management and digital twin (DT) modeling. It incorporates distributed Space Applications (sApps) and dApps to ensure robust performance in in highly dynamic orbital environments. A core feature is dynamic link-interface mapping, which allows network functions to adapt to specific application requirements and changing link conditions using all physical links on the satellite. Simulation results evaluate its feasibility by analyzing latency constraints across different NTN link types, demonstrating that intra-cluster coordination operates within viable signaling delay bounds, while offloading non-real-time tasks to ground infrastructure enhances scalability toward sixth-generation (6G) networks.

## 软件工程(cs.SE:Software Engineering)

### LLMs in Mobile Apps: Practices, Challenges, and Opportunities 
[[arxiv](https://arxiv.org/abs/2502.15908)] [[cool](https://papers.cool/arxiv/2502.15908)] [[pdf](https://arxiv.org/pdf/2502.15908)]
> **Authors**: Kimberly Hau,Safwat Hassan,Shurui Zhou
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,计算语言学
- **Abstract**: The integration of AI techniques has become increasingly popular in software development, enhancing performance, usability, and the availability of intelligent features. With the rise of large language models (LLMs) and generative AI, developers now have access to a wealth of high-quality open-source models and APIs from closed-source providers, enabling easier experimentation and integration of LLMs into various systems. This has also opened new possibilities in mobile application (app) development, allowing for more personalized and intelligent apps. However, integrating LLM into mobile apps might present unique challenges for developers, particularly regarding mobile device constraints, API management, and code infrastructure. In this project, we constructed a comprehensive dataset of 149 LLM-enabled Android apps and conducted an exploratory analysis to understand how LLMs are deployed and used within mobile apps. This analysis highlights key characteristics of the dataset, prevalent integration strategies, and common challenges developers face. Our findings provide valuable insights for future research and tooling development aimed at enhancing LLM-enabled mobile apps.

## 社交和信息网络(cs.SI:Social and Information Networks)

### Efficient Estimation of Shortest-Path Distance Distributions to Samples in Graphs 
[[arxiv](https://arxiv.org/abs/2502.15890)] [[cool](https://papers.cool/arxiv/2502.15890)] [[pdf](https://arxiv.org/pdf/2502.15890)]
> **Authors**: Alan Zhu,Jiaqi Ma,Qiaozhu Mei
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 社交和信息网络,机器学习
- **Abstract**: As large graph datasets become increasingly common across many fields, sampling is often needed to reduce the graphs into manageable sizes. This procedure raises critical questions about representativeness as no sample can capture the properties of the original graph perfectly, and different parts of the graph are not evenly affected by the loss. Recent work has shown that the distances from the non-sampled nodes to the sampled nodes can be a quantitative indicator of bias and fairness in graph machine learning. However, to our knowledge, there is no method for evaluating how a sampling method affects the distribution of shortest-path distances without actually performing the sampling and shortest-path calculation. In this paper, we present an accurate and efficient framework for estimating the distribution of shortest-path distances to the sample, applicable to a wide range of sampling methods and graph structures. Our framework is faster than empirical methods and only requires the specification of degree distributions. We also extend our framework to handle graphs with community structures. While this introduces a decrease in accuracy, we demonstrate that our framework remains highly accurate on downstream comparison-based tasks. Code is publicly available at https://github.com/az1326/shortest_paths.

## 其他定量生物学(q-bio.OT:Other Quantitative Biology)

### Strategic priorities for transformative progress in advancing biology with proteomics and artificial intelligence 
[[arxiv](https://arxiv.org/abs/2502.15867)] [[cool](https://papers.cool/arxiv/2502.15867)] [[pdf](https://arxiv.org/pdf/2502.15867)]
> **Authors**: Yingying Sun,Jun A,Zhiwei Liu,Rui Sun,Liujia Qian,Samuel H. Payne,Wout Bittremieux,Markus Ralser,Chen Li,Yi Chen,Zhen Dong,Yasset Perez-Riverol,Asif Khan,Chris Sander,Ruedi Aebersold,Juan Antonio Vizcaíno,Jonathan R Krieger,Jianhua Yao,Han Wen,Linfeng Zhang,Yunping Zhu,Yue Xuan,Benjamin Boyang Sun,Liang Qiao,Henning Hermjakob, et al. (37 additional authors not shown)
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 28 pages, 2 figures, perspective inAIproteomics
- **标题**: None
- **领域**: 其他定量生物学,人工智能
- **Abstract**: Artificial intelligence (AI) is transforming scientific research, including proteomics. Advances in mass spectrometry (MS)-based proteomics data quality, diversity, and scale, combined with groundbreaking AI techniques, are unlocking new challenges and opportunities in biological discovery. Here, we highlight key areas where AI is driving innovation, from data analysis to new biological insights. These include developing an AI-friendly ecosystem for proteomics data generation, sharing, and analysis; improving peptide and protein identification and quantification; characterizing protein-protein interactions and protein complexes; advancing spatial and perturbation proteomics; integrating multi-omics data; and ultimately enabling AI-empowered virtual cells.

## 定量方法(q-bio.QM:Quantitative Methods)

### Non-Linear Flow Matching for Full-Atom Peptide Design 
[[arxiv](https://arxiv.org/abs/2502.15855)] [[cool](https://papers.cool/arxiv/2502.15855)] [[pdf](https://arxiv.org/pdf/2502.15855)]
> **Authors**: Dengdeng Huang,Shikui Tu
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 定量方法,人工智能,机器学习
- **Abstract**: Peptide design plays a pivotal role in therapeutic applications, yet existing AI-assisted methods often struggle to generate stable peptides with high affinity due to their inability to accurately simulate the dynamic docking process. To address this challenge, we propose NLFlow, a novel multi-manifold approach based on non-linear flow matching. Specifically, we design a polynomial-based conditional vector field to accelerate the convergence of the peptide's position towards the target pocket, effectively capturing the temporal inconsistencies across position, rotation, torsion, and amino acid type manifolds. This enables the model to better align with the true conformational changes observed in biological docking processes. Additionally, we incorporate interaction-related information, such as polarity, to enhance the understanding of peptide-protein binding. Extensive experiments demonstrate that NLFlow outperforms existing methods in generating peptides with superior stability, affinity, and diversity, offering a fast and efficient solution for peptide design and advancing the peptide-based therapeutic development.

## 一般财务(q-fin.GN:General Finance)

### Position: Standard Benchmarks Fail -- LLM Agents Present Overlooked Risks for Financial Applications 
[[arxiv](https://arxiv.org/abs/2502.15865)] [[cool](https://papers.cool/arxiv/2502.15865)] [[pdf](https://arxiv.org/pdf/2502.15865)]
> **Authors**: Zichen Chen,Jiaao Chen,Jianda Chen,Misha Sra
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 40 pages, 2 figures, 2 tables
- **标题**: None
- **领域**: 一般财务,人工智能,计算语言学
- **Abstract**: Current financial LLM agent benchmarks are inadequate. They prioritize task performance while ignoring fundamental safety risks. Threats like hallucinations, temporal misalignment, and adversarial vulnerabilities pose systemic risks in high-stakes financial environments, yet existing evaluation frameworks fail to capture these risks. We take a firm position: traditional benchmarks are insufficient to ensure the reliability of LLM agents in finance. To address this, we analyze existing financial LLM agent benchmarks, finding safety gaps and introducing ten risk-aware evaluation metrics. Through an empirical evaluation of both API-based and open-weight LLM agents, we reveal hidden vulnerabilities that remain undetected by conventional assessments. To move the field forward, we propose the Safety-Aware Evaluation Agent (SAEA), grounded in a three-level evaluation framework that assesses agents at the model level (intrinsic capabilities), workflow level (multi-step process reliability), and system level (integration robustness). Our findings highlight the urgent need to redefine LLM agent evaluation standards by shifting the focus from raw performance to safety, robustness, and real world resilience.

## 统计金融(q-fin.ST:Statistical Finance)

### Multi-Agent Stock Prediction Systems: Machine Learning Models, Simulations, and Real-Time Trading Strategies 
[[arxiv](https://arxiv.org/abs/2502.15853)] [[cool](https://papers.cool/arxiv/2502.15853)] [[pdf](https://arxiv.org/pdf/2502.15853)]
> **Authors**: Daksh Dave,Gauransh Sawhney,Vikhyat Chauhan
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 统计金融,机器学习
- **Abstract**: This paper presents a comprehensive study on stock price prediction, leveragingadvanced machine learning (ML) and deep learning (DL) techniques to improve financial forecasting accuracy. The research evaluates the performance of various recurrent neural network (RNN) architectures, including Long Short-Term Memory (LSTM) networks, Gated Recurrent Units (GRU), and attention-based models. These models are assessed for their ability to capture complex temporal dependencies inherent in stock market data. Our findings show that attention-based models outperform other architectures, achieving the highest accuracy by capturing both short and long-term dependencies. This study contributes valuable insights into AI-driven financial forecasting, offering practical guidance for developing more accurate and efficient trading systems.

## 其他论文

- ["Kya family planning after marriage hoti hai?": Integrating Cultural Sensitivity in an LLM Chatbot for Reproductive Health](https://arxiv.org/abs/2502.15939)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC,cs.CY in whitelist
- [Computation Offloading Strategies in Integrated Terrestrial and Non-Terrestrial Networks](https://arxiv.org/abs/2502.15903)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC,eess.SP in whitelist
