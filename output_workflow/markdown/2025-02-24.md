> 本文由 [https://github.com/huiyeruzhou/arxiv_crawler](https://github.com/huiyeruzhou/arxiv_crawler) 自动生成
>
> 领域白名单：cs.AI,cs.CL,cs.LG,cs.CV
> 关键词： LLM, GPT, AI, language+model, deep+learning, transformer, neural+network, machine+learning

# 论文全览：2025-02-24

共有616篇相关领域论文, 另有72篇其他

## 地球和行星天体物理学(astro-ph.EP:Earth and Planetary Astrophysics)

### Asteroid shape inversion with light curves using deep learning 
[[arxiv](https://arxiv.org/abs/2502.16455)] [[cool](https://papers.cool/arxiv/2502.16455)] [[pdf](https://arxiv.org/pdf/2502.16455)]
> **Authors**: YiJun Tang,ChenChen Ying,ChengZhe Xia,XiaoMing Zhang,XiaoJun Jiang
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 地球和行星天体物理学,天体物理学仪器和方法,机器学习
- **Abstract**: Asteroid shape inversion using photometric data has been a key area of study in planetary science and astronomical research.However, the current methods for asteroid shape inversion require extensive iterative calculations, making the process time-consuming and prone to becoming stuck in local optima. We directly established a mapping between photometric data and shape distribution through deep neural networks. In addition, we used 3D point clouds to represent asteroid shapes and utilized the deviation between the light curves of non-convex asteroids and their convex hulls to predict the concave areas of non-convex asteroids. We compared the results of different shape models using the Chamfer distance between traditional methods and ours and found that our method performs better, especially when handling special shapes. For the detection of concave areas on the convex hull, the intersection over union (IoU) of our predictions reached 0.89. We further validated this method using observational data from the Lowell Observatory to predict the convex shapes of the asteroids 3337 Milo and 1289 Kuta, and conducted light curve fitting experiments. The experimental results demonstrated the robustness and adaptability of the method

## 天体物理学仪器和方法(astro-ph.IM:Instrumentation and Methods for Astrophysics)

### Comparative Analysis of Black Hole Mass Estimation in Type-2 AGNs: Classical vs. Quantum Machine Learning and Deep Learning Approaches 
[[arxiv](https://arxiv.org/abs/2502.15297)] [[cool](https://papers.cool/arxiv/2502.15297)] [[pdf](https://arxiv.org/pdf/2502.15297)]
> **Authors**: Sathwik Narkedimilli,Venkata Sriram Amballa,N V Saran Kumar,R Arun Kumar,R Praneeth Reddy,Satvik Raghav,Manish M,Aswath Babu H
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 29 pages, 12 Figures, 6 Tables
- **标题**: None
- **领域**: 天体物理学仪器和方法,机器学习,量子物理学
- **Abstract**: In the case of Type-2 AGNs, estimating the mass of the black hole is challenging. Understanding how galaxies form and evolve requires considerable insight into the mass of black holes. This work compared different classical and quantum machine learning (QML) algorithms for black hole mass estimation, wherein the classical algorithms are Linear Regression, XGBoost Regression, Random Forest Regressor, Support Vector Regressor (SVR), Lasso Regression, Ridge Regression, Elastic Net Regression, Bayesian Regression, Decision Tree Regressor, Gradient Booster Regressor, Classical Neural Networks, Gated Recurrent Unit (GRU), LSTM, Deep Residual Networks (ResNets) and Transformer-Based Regression. On the other hand, quantum algorithms including Hybrid Quantum Neural Networks (QNN), Quantum Long Short-Term Memory (Q-LSTM), Sampler-QNN, Estimator-QNN, Variational Quantum Regressor (VQR), Quantum Linear Regression(Q-LR), QML with JAX optimization were also tested. The results revealed that classical algorithms gave better R^2, MAE, MSE, and RMSE results than the quantum models. Among the classical models, LSTM has the best result with an accuracy of 99.77%. Estimator-QNN has the highest accuracy for quantum algorithms with an MSE of 0.0124 and an accuracy of 99.75%. This study ascertains both the strengths and weaknesses of the classical and the quantum approaches. As far as our knowledge goes, this work could pave the way for the future application of quantum algorithms in astrophysical data analysis.

## 无序系统和神经网络(cond-mat.dis-nn:Disordered Systems and Neural Networks)

### Sampling through Algorithmic Diffusion in non-convex Perceptron problems 
[[arxiv](https://arxiv.org/abs/2502.16292)] [[cool](https://papers.cool/arxiv/2502.16292)] [[pdf](https://arxiv.org/pdf/2502.16292)]
> **Authors**: Elizaveta Demyanenko,Davide Straziota,Carlo Baldassi,Carlo Lucibello
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 无序系统和神经网络,机器学习
- **Abstract**: We analyze the problem of sampling from the solution space of simple yet non-convex neural network models by employing a denoising diffusion process known as Algorithmic Stochastic Localization, where the score function is provided by Approximate Message Passing. We introduce a formalism based on the replica method to characterize the process in the infinite-size limit in terms of a few order parameters, and, in particular, we provide criteria for the feasibility of sampling. We show that, in the case of the spherical perceptron problem with negative stability, approximate uniform sampling is achievable across the entire replica symmetric region of the phase diagram. In contrast, for the binary perceptron, uniform sampling via diffusion invariably fails due to the overlap gap property exhibited by the typical set of solutions. We discuss the first steps in defining alternative measures that can be efficiently sampled.

## 人工智能(cs.AI:Artificial Intelligence)

### Grounded Persuasive Language Generation for Automated Marketing 
[[arxiv](https://arxiv.org/abs/2502.16810)] [[cool](https://papers.cool/arxiv/2502.16810)] [[pdf](https://arxiv.org/pdf/2502.16810)]
> **Authors**: Jibang Wu,Chenghao Yang,Simon Mahns,Chaoqi Wang,Hao Zhu,Fei Fang,Haifeng Xu
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学,人机交互,普通经济学
- **Abstract**: This paper develops an agentic framework that employs large language models (LLMs) to automate the generation of persuasive and grounded marketing content, using real estate listing descriptions as our focal application domain. Our method is designed to align the generated content with user preferences while highlighting useful factual attributes. This agent consists of three key modules: (1) Grounding Module, mimicking expert human behavior to predict marketable features; (2) Personalization Module, aligning content with user preferences; (3) Marketing Module, ensuring factual accuracy and the inclusion of localized features. We conduct systematic human-subject experiments in the domain of real estate marketing, with a focus group of potential house buyers. The results demonstrate that marketing descriptions generated by our approach are preferred over those written by human experts by a clear margin. Our findings suggest a promising LLM-based agentic framework to automate large-scale targeted marketing while ensuring responsible generation using only facts.

### Understanding the Impact of Artificial Intelligence in Academic Writing: Metadata to the Rescue 
[[arxiv](https://arxiv.org/abs/2502.16713)] [[cool](https://papers.cool/arxiv/2502.16713)] [[pdf](https://arxiv.org/pdf/2502.16713)]
> **Authors**: Javier Conde,Pedro Reviriego,Joaquín Salvachúa,Gonzalo Martínez,José Alberto Hernández,Fabrizio Lombardi
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: ef:Computer (Volume: 57, Issue: 1, January 2024)
- **标题**: None
- **领域**: 人工智能
- **Abstract**: This column advocates for including artificial intelligence (AI)-specific metadata on those academic papers that are written with the help of AI in an attempt to analyze the use of such tools for disseminating research.

### From Text to Space: Mapping Abstract Spatial Models in LLMs during a Grid-World Navigation Task 
[[arxiv](https://arxiv.org/abs/2502.16690)] [[cool](https://papers.cool/arxiv/2502.16690)] [[pdf](https://arxiv.org/pdf/2502.16690)]
> **Authors**: Nicolas Martorell
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Understanding how large language models (LLMs) represent and reason about spatial information is crucial for building robust agentic systems that can navigate real and simulated environments. In this work, we investigate the influence of different text-based spatial representations on LLM performance and internal activations in a grid-world navigation task. By evaluating models of various sizes on a task that requires navigating toward a goal, we examine how the format used to encode spatial information impacts decision-making. Our experiments reveal that cartesian representations of space consistently yield higher success rates and path efficiency, with performance scaling markedly with model size. Moreover, probing LLaMA-3.1-8B revealed subsets of internal units, primarily located in intermediate layers, that robustly correlate with spatial features, such as the position of the agent in the grid or action correctness, regardless of how that information is represented, and are also activated by unrelated spatial reasoning tasks. This work advances our understanding of how LLMs process spatial information and provides valuable insights for developing more interpretable and robust agentic AI systems.

### SBSC: Step-By-Step Coding for Improving Mathematical Olympiad Performance 
[[arxiv](https://arxiv.org/abs/2502.16666)] [[cool](https://papers.cool/arxiv/2502.16666)] [[pdf](https://arxiv.org/pdf/2502.16666)]
> **Authors**: Kunal Singh,Ankan Biswas,Sayandeep Bhowmick,Pradeep Moturi,Siva Kishore Gollapalli
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: Published as a full conference paper at ICLR 2025. Shorter(Early) Version accepted at NeurIPS'24 MATH-AItrack
- **标题**: None
- **领域**: 人工智能,计算语言学,机器学习
- **Abstract**: We propose Step-by-Step Coding (SBSC): a multi-turn math reasoning framework that enables Large Language Models (LLMs) to generate sequence of programs for solving Olympiad level math problems. At each step/turn, by leveraging the code execution outputs and programs of previous steps, the model generates the next sub-task and the corresponding program to solve it. This way, SBSC, sequentially navigates to reach the final answer. SBSC allows more granular, flexible and precise approach to problem-solving compared to existing methods. Extensive experiments highlight the effectiveness of SBSC in tackling competition and Olympiad-level math problems. For Claude-3.5-Sonnet, we observe SBSC (greedy decoding) surpasses existing state-of-the-art (SOTA) program generation based reasoning strategies by absolute 10.7% on AMC12, 8% on AIME and 12.6% on MathOdyssey. Given SBSC is multi-turn in nature, we also benchmark SBSC's greedy decoding against self-consistency decoding results of existing SOTA math reasoning strategies and observe performance gain by absolute 6.2% on AMC, 6.7% on AIME and 7.4% on MathOdyssey.

### Saarthi: The First AI Formal Verification Engineer 
[[arxiv](https://arxiv.org/abs/2502.16662)] [[cool](https://papers.cool/arxiv/2502.16662)] [[pdf](https://arxiv.org/pdf/2502.16662)]
> **Authors**: Aman Kumar,Deepak Narayan Gadde,Keerthan Kopparam Radhakrishna,Djones Lettnin
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: Published at the DVCon U.S. 2025
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Recently, Devin has made a significant buzz in the Artificial Intelligence (AI) community as the world's first fully autonomous AI software engineer, capable of independently developing software code. Devin uses the concept of agentic workflow in Generative AI (GenAI), which empowers AI agents to engage in a more dynamic, iterative, and self-reflective process. In this paper, we present a similar fully autonomous AI formal verification engineer, Saarthi, capable of verifying a given RTL design end-to-end using an agentic workflow. With Saarthi, verification engineers can focus on more complex problems, and verification teams can strive for more ambitious goals. The domain-agnostic implementation of Saarthi makes it scalable for use across various domains such as RTL design, UVM-based verification, and others.

### OptionZero: Planning with Learned Options 
[[arxiv](https://arxiv.org/abs/2502.16634)] [[cool](https://papers.cool/arxiv/2502.16634)] [[pdf](https://arxiv.org/pdf/2502.16634)]
> **Authors**: Po-Wei Huang,Pei-Chiun Peng,Hung Guei,Ti-Rong Wu
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: Accepted by the Thirteenth International Conference onLearningRepresentations (ICLR 2025) as oral presentation
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: Planning with options -- a sequence of primitive actions -- has been shown effective in reinforcement learning within complex environments. Previous studies have focused on planning with predefined options or learned options through expert demonstration data. Inspired by MuZero, which learns superhuman heuristics without any human knowledge, we propose a novel approach, named OptionZero. OptionZero incorporates an option network into MuZero, providing autonomous discovery of options through self-play games. Furthermore, we modify the dynamics network to provide environment transitions when using options, allowing searching deeper under the same simulation constraints. Empirical experiments conducted in 26 Atari games demonstrate that OptionZero outperforms MuZero, achieving a 131.58% improvement in mean human-normalized score. Our behavior analysis shows that OptionZero not only learns options but also acquires strategic skills tailored to different game characteristics. Our findings show promising directions for discovering and using options in planning. Our code is available at https://rlg.iis.sinica.edu.tw/papers/optionzero.

### Toward Dependency Dynamics in Multi-Agent Reinforcement Learning for Traffic Signal Control 
[[arxiv](https://arxiv.org/abs/2502.16608)] [[cool](https://papers.cool/arxiv/2502.16608)] [[pdf](https://arxiv.org/pdf/2502.16608)]
> **Authors**: Yuli Zhang,Shangbo Wang,Dongyao Jia,Pengfei Fan,Ruiyuan Jiang,Hankang Gu,Andy H. F. Chow
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,多代理系统
- **Abstract**: Reinforcement learning (RL) emerges as a promising data-driven approach for adaptive traffic signal control (ATSC) in complex urban traffic networks, with deep neural networks substantially augmenting its learning capabilities. However, centralized RL becomes impractical for ATSC involving multiple agents due to the exceedingly high dimensionality of the joint action space. Multi-agent RL (MARL) mitigates this scalability issue by decentralizing control to local RL agents. Nevertheless, this decentralized method introduces new challenges: the environment becomes partially observable from the perspective of each local agent due to constrained inter-agent communication. Both centralized RL and MARL exhibit distinct strengths and weaknesses, particularly under heavy intersectional traffic conditions. In this paper, we justify that MARL can achieve the optimal global Q-value by separating into multiple IRL (Independent Reinforcement Learning) processes when no spill-back congestion occurs (no agent dependency) among agents (intersections). In the presence of spill-back congestion (with agent dependency), the maximum global Q-value can be achieved by using centralized RL. Building upon the conclusions, we propose a novel Dynamic Parameter Update Strategy for Deep Q-Network (DQN-DPUS), which updates the weights and bias based on the dependency dynamics among agents, i.e. updating only the diagonal sub-matrices for the scenario without spill-back congestion. We validate the DQN-DPUS in a simple network with two intersections under varying traffic, and show that the proposed strategy can speed up the convergence rate without sacrificing optimal exploration. The results corroborate our theoretical findings, demonstrating the efficacy of DQN-DPUS in optimizing traffic signal control.

### Tracking the Copyright of Large Vision-Language Models through Parameter Learning Adversarial Images 
[[arxiv](https://arxiv.org/abs/2502.16593)] [[cool](https://papers.cool/arxiv/2502.16593)] [[pdf](https://arxiv.org/pdf/2502.16593)]
> **Authors**: Yubo Wang,Jianting Tang,Chaohu Liu,Linli Xu
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: Accepted to ICLR 2025
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: Large vision-language models (LVLMs) have demonstrated remarkable image understanding and dialogue capabilities, allowing them to handle a variety of visual question answering tasks. However, their widespread availability raises concerns about unauthorized usage and copyright infringement, where users or individuals can develop their own LVLMs by fine-tuning published models. In this paper, we propose a novel method called Parameter Learning Attack (PLA) for tracking the copyright of LVLMs without modifying the original model. Specifically, we construct adversarial images through targeted attacks against the original model, enabling it to generate specific outputs. To ensure these attacks remain effective on potential fine-tuned models to trigger copyright tracking, we allow the original model to learn the trigger images by updating parameters in the opposite direction during the adversarial attack process. Notably, the proposed method can be applied after the release of the original model, thus not affecting the model's performance and behavior. To simulate real-world applications, we fine-tune the original model using various strategies across diverse datasets, creating a range of models for copyright verification. Extensive experiments demonstrate that our method can more effectively identify the original copyright of fine-tuned models compared to baseline methods. Therefore, this work provides a powerful tool for tracking copyrights and detecting unlicensed usage of LVLMs.

### LawPal : A Retrieval Augmented Generation Based System for Enhanced Legal Accessibility in India 
[[arxiv](https://arxiv.org/abs/2502.16573)] [[cool](https://papers.cool/arxiv/2502.16573)] [[pdf](https://arxiv.org/pdf/2502.16573)]
> **Authors**: Dnyanesh Panchal,Aaryan Gole,Vaibhav Narute,Raunak Joshi
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: Access to legal knowledge in India is often hindered by a lack of awareness, misinformation and limited accessibility to judicial resources. Many individuals struggle to navigate complex legal frameworks, leading to the frequent misuse of laws and inadequate legal protection. To address these issues, we propose a Retrieval-Augmented Generation (RAG)-based legal chatbot powered by vectorstore oriented FAISS for efficient and accurate legal information retrieval. Unlike traditional chatbots, our model is trained using an extensive dataset comprising legal books, official documentation and the Indian Constitution, ensuring accurate responses to even the most complex or misleading legal queries. The chatbot leverages FAISS for rapid vector-based search, significantly improving retrieval speed and accuracy. It is also prompt-engineered to handle twisted or ambiguous legal questions, reducing the chances of incorrect interpretations. Apart from its core functionality of answering legal queries, the platform includes additional features such as real-time legal news updates, legal blogs, and access to law-related books, making it a comprehensive resource for users. By integrating advanced AI techniques with an optimized retrieval system, our chatbot aims to democratize legal knowledge, enhance legal literacy, and prevent the spread of misinformation. The study demonstrates that our approach effectively improves legal accessibility while maintaining high accuracy and efficiency, thereby contributing to a more informed and empowered society.

### Rebalancing the Scales: A Systematic Mapping Study of Generative Adversarial Networks (GANs) in Addressing Data Imbalance 
[[arxiv](https://arxiv.org/abs/2502.16535)] [[cool](https://papers.cool/arxiv/2502.16535)] [[pdf](https://arxiv.org/pdf/2502.16535)]
> **Authors**: Pankaj Yadav,Gulshan Sihag,Vivek Vijay
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: 49 pages, 6 figures
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: Machine learning algorithms are used in diverse domains, many of which face significant challenges due to data imbalance. Studies have explored various approaches to address the issue, like data preprocessing, cost-sensitive learning, and ensemble methods. Generative Adversarial Networks (GANs) showed immense potential as a data preprocessing technique that generates good quality synthetic data. This study employs a systematic mapping methodology to analyze 3041 papers on GAN-based sampling techniques for imbalanced data sourced from four digital libraries. A filtering process identified 100 key studies spanning domains such as healthcare, finance, and cybersecurity. Through comprehensive quantitative analysis, this research introduces three categorization mappings as application domains, GAN techniques, and GAN variants used to handle the imbalanced nature of the data. GAN-based over-sampling emerges as an effective preprocessing method. Advanced architectures and tailored frameworks helped GANs to improve further in the case of data imbalance. GAN variants like vanilla GAN, CTGAN, and CGAN show great adaptability in structured imbalanced data cases. Interest in GANs for imbalanced data has grown tremendously, touching a peak in recent years, with journals and conferences playing crucial roles in transmitting foundational theories and practical applications. While with these advances, none of the reviewed studies explicitly explore hybridized GAN frameworks with diffusion models or reinforcement learning techniques. This gap leads to a future research idea develop innovative approaches for effectively handling data imbalance.

### Facilitating Emergency Vehicle Passage in Congested Urban Areas Using Multi-agent Deep Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.16449)] [[cool](https://papers.cool/arxiv/2502.16449)] [[pdf](https://arxiv.org/pdf/2502.16449)]
> **Authors**: Haoran Su
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: Ph.D. dissertation in Transportation Systems
- **标题**: None
- **领域**: 人工智能,系统与控制
- **Abstract**: Emergency Response Time (ERT) is crucial for urban safety, measuring cities' ability to handle medical, fire, and crime emergencies. In NYC, medical ERT increased 72% from 7.89 minutes in 2014 to 14.27 minutes in 2024, with half of delays due to Emergency Vehicle (EMV) travel times. Each minute's delay in stroke response costs 2 million brain cells, while cardiac arrest survival drops 7-10% per minute. This dissertation advances EMV facilitation through three contributions. First, EMVLight, a decentralized multi-agent reinforcement learning framework, integrates EMV routing with traffic signal pre-emption. It achieved 42.6% faster EMV travel times and 23.5% improvement for other vehicles. Second, the Dynamic Queue-Jump Lane system uses Multi-Agent Proximal Policy Optimization for coordinated lane-clearing in mixed autonomous and human-driven traffic, reducing EMV travel times by 40%. Third, an equity study of NYC Emergency Medical Services revealed disparities across boroughs: Staten Island faces delays due to sparse signalized intersections, while Manhattan struggles with congestion. Solutions include optimized EMS stations and improved intersection designs. These contributions enhance EMV mobility and emergency service equity, offering insights for policymakers and urban planners to develop safer, more efficient transportation systems.

### Navigation-GPT: A Robust and Adaptive Framework Utilizing Large Language Models for Navigation Applications 
[[arxiv](https://arxiv.org/abs/2502.16402)] [[cool](https://papers.cool/arxiv/2502.16402)] [[pdf](https://arxiv.org/pdf/2502.16402)]
> **Authors**: Feng Ma,Xiu-min Wang,Chen Chen,Xiao-bin Xu,Xin-ping Yan
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Existing navigation decision support systems often perform poorly when handling non-predefined navigation scenarios. Leveraging the generalization capabilities of large language model (LLM) in handling unknown scenarios, this research proposes a dual-core framework for LLM applications to address this issue. Firstly, through ReAct-based prompt engineering, a larger LLM core decomposes intricate navigation tasks into manageable sub-tasks, which autonomously invoke corresponding external tools to gather relevant information, using this feedback to mitigate the risk of LLM hallucinations. Subsequently, a fine-tuned and compact LLM core, acting like a first-mate is designed to process such information and unstructured external data, then to generates context-aware recommendations, ultimately delivering lookout insights and navigation hints that adhere to the International Regulations for Preventing Collisions at Sea (COLREGs) and other rules. Extensive experiments demonstrate the proposed framework not only excels in traditional ship collision avoidance tasks but also adapts effectively to unstructured, non-predefined, and unpredictable scenarios. A comparative analysis with DeepSeek-R1, GPT-4o and other SOTA models highlights the efficacy and rationality of the proposed framework. This research bridges the gap between conventional navigation systems and LLMs, offering a framework to enhance safety and operational efficiency across diverse navigation applications.

### Does Your AI Agent Get You? A Personalizable Framework for Approximating Human Models from Argumentation-based Dialogue Traces 
[[arxiv](https://arxiv.org/abs/2502.16376)] [[cool](https://papers.cool/arxiv/2502.16376)] [[pdf](https://arxiv.org/pdf/2502.16376)]
> **Authors**: Yinxu Tang,Stylianos Loukas Vasileiou,William Yeoh
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,人机交互,计算机科学中的逻辑
- **Abstract**: Explainable AI is increasingly employing argumentation methods to facilitate interactive explanations between AI agents and human users. While existing approaches typically rely on predetermined human user models, there remains a critical gap in dynamically learning and updating these models during interactions. In this paper, we present a framework that enables AI agents to adapt their understanding of human users through argumentation-based dialogues. Our approach, called Persona, draws on prospect theory and integrates a probability weighting function with a Bayesian belief update mechanism that refines a probability distribution over possible human models based on exchanged arguments. Through empirical evaluations with human users in an applied argumentation setting, we demonstrate that Persona effectively captures evolving human beliefs, facilitates personalized interactions, and outperforms state-of-the-art methods.

### Direct Alignment with Heterogeneous Preferences 
[[arxiv](https://arxiv.org/abs/2502.16320)] [[cool](https://papers.cool/arxiv/2502.16320)] [[pdf](https://arxiv.org/pdf/2502.16320)]
> **Authors**: Ali Shirali,Arash Nasr-Esfahany,Abdullah Alomar,Parsa Mirtaheri,Rediet Abebe,Ariel Procaccia
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: Alignment with human preferences is commonly framed using a universal reward function, even though human preferences are inherently heterogeneous. We formalize this heterogeneity by introducing user types and examine the limits of the homogeneity assumption. We show that aligning to heterogeneous preferences with a single policy is best achieved using the average reward across user types. However, this requires additional information about annotators. We examine improvements under different information settings, focusing on direct alignment methods. We find that minimal information can yield first-order improvements, while full feedback from each user type leads to consistent learning of the optimal policy. Surprisingly, however, no sample-efficient consistent direct loss exists in this latter setting. These results reveal a fundamental tension between consistency and sample efficiency in direct policy alignment.

### Reproducibility Study of Cooperation, Competition, and Maliciousness: LLM-Stakeholders Interactive Negotiation 
[[arxiv](https://arxiv.org/abs/2502.16242)] [[cool](https://papers.cool/arxiv/2502.16242)] [[pdf](https://arxiv.org/pdf/2502.16242)]
> **Authors**: Jose L. Garcia,Karolina Hajkova,Maria Marchenko,Carlos Miguel Patiño
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: This paper presents a reproducibility study and extension of "Cooperation, Competition, and Maliciousness: LLM-Stakeholders Interactive Negotiation." We validate the original findings using a range of open-weight models (1.5B-70B parameters) and GPT-4o Mini while introducing several novel contributions. We analyze the Pareto front of the games, propose a communication-free baseline to test whether successful negotiations are possible without agent interaction, evaluate recent small language models' performance, analyze structural information leakage in model responses, and implement an inequality metric to assess negotiation fairness. Our results demonstrate that smaller models (<10B parameters) struggle with format adherence and coherent responses, but larger open-weight models can approach proprietary model performance. Additionally, in many scenarios, single-agent approaches can achieve comparable results to multi-agent negotiations, challenging assumptions about the necessity of agent communication to perform well on the benchmark. This work also provides insights into the accessibility, fairness, environmental impact, and privacy considerations of LLM-based negotiation systems.

### Dynamic Parallel Tree Search for Efficient LLM Reasoning 
[[arxiv](https://arxiv.org/abs/2502.16235)] [[cool](https://papers.cool/arxiv/2502.16235)] [[pdf](https://arxiv.org/pdf/2502.16235)]
> **Authors**: Yifu Ding,Wentao Jiang,Shunyu Liu,Yongcheng Jing,Jinyang Guo,Yingjie Wang,Jing Zhang,Zengmao Wang,Ziwei Liu,Bo Du,Xianglong Liu,Dacheng Tao
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: 17 pages, 11 figures
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Tree of Thoughts (ToT) enhances Large Language Model (LLM) reasoning by structuring problem-solving as a spanning tree. However, recent methods focus on search accuracy while overlooking computational efficiency. The challenges of accelerating the ToT lie in the frequent switching of reasoning focus, and the redundant exploration of suboptimal solutions. To alleviate this dilemma, we propose Dynamic Parallel Tree Search (DPTS), a novel parallelism framework that aims to dynamically optimize the reasoning path in inference. It includes the Parallelism Streamline in the generation phase to build up a flexible and adaptive parallelism with arbitrary paths by fine-grained cache management and alignment. Meanwhile, the Search and Transition Mechanism filters potential candidates to dynamically maintain the reasoning focus on more possible solutions and have less redundancy. Experiments on Qwen-2.5 and Llama-3 with Math500 and GSM8K datasets show that DPTS significantly improves efficiency by 2-4x on average while maintaining or even surpassing existing reasoning algorithms in accuracy, making ToT-based reasoning more scalable and computationally efficient.

### Machine Learning Framework for Early Power, Performance, and Area Estimation of RTL 
[[arxiv](https://arxiv.org/abs/2502.16203)] [[cool](https://papers.cool/arxiv/2502.16203)] [[pdf](https://arxiv.org/pdf/2502.16203)]
> **Authors**: Anindita Chattopadhyay,Vijay Kumar Sutrakar
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: A critical stage in the evolving landscape of VLSI design is the design phase that is transformed into register-transfer level (RTL), which specifies system functionality through hardware description languages like Verilog. Generally, evaluating the quality of an RTL design demands full synthesis via electronic design automation (EDA) tool is time-consuming process that is not well-suited to rapid design iteration and optimization. Although recent breakthroughs in machine Learning (ML) have brought early prediction models, these methods usually do not provide robust and generalizable solutions with respect to a wide range of RTL designs. This paper proposes a pre-synthesis framework that makes early estimation of power, performance and area (PPA) metrics directly from the hardware description language (HDL) code making direct use of library files instead of toggle files. The proposed framework introduces a bit-level representation referred to as the simple operator graph (SOG), which uses single-bit operators to generate a generalized and flexible structure that closely mirrors the characteristics of post synthesis design. The proposed model bridges the RTL and post-synthesis design, which will help in precisely predicting key metrics. The proposed tree-based ML framework shows superior predictive performance PPA estimation. Validation is carried out on 147 distinct RTL designs. The proposed model with 147 different designs shows accuracy of 98%, 98%, and 90% for WNS, TNS and power, respectively, indicates significant accuracy improvements relative to state-of-the-art methods.

### Robustness and Cybersecurity in the EU Artificial Intelligence Act 
[[arxiv](https://arxiv.org/abs/2502.16184)] [[cool](https://papers.cool/arxiv/2502.16184)] [[pdf](https://arxiv.org/pdf/2502.16184)]
> **Authors**: Henrik Nolte,Miriam Rateike,Michèle Finck
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,密码学和安全,计算机与社会,机器学习
- **Abstract**: The EU Artificial Intelligence Act (AIA) establishes different legal principles for different types of AI systems. While prior work has sought to clarify some of these principles, little attention has been paid to robustness and cybersecurity. This paper aims to fill this gap. We identify legal challenges and shortcomings in provisions related to robustness and cybersecurity for high-risk AI systems (Art. 15 AIA) and general-purpose AI models (Art. 55 AIA). We show that robustness and cybersecurity demand resilience against performance disruptions. Furthermore, we assess potential challenges in implementing these provisions in light of recent advancements in the machine learning (ML) literature. Our analysis informs efforts to develop harmonized standards, guidelines by the European Commission, as well as benchmarks and measurement methodologies under Art. 15(2) AIA. With this, we seek to bridge the gap between legal terminology and ML research, fostering a better alignment between research and implementation efforts.

### Patterns Over Principles: The Fragility of Inductive Reasoning in LLMs under Noisy Observations 
[[arxiv](https://arxiv.org/abs/2502.16169)] [[cool](https://papers.cool/arxiv/2502.16169)] [[pdf](https://arxiv.org/pdf/2502.16169)]
> **Authors**: Chunyang Li,Weiqi Wang,Tianshi Zheng,Yangqiu Song
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Inductive reasoning, a cornerstone of human cognition, enables generalization from limited data but hasn't yet been fully achieved by large language models (LLMs). While modern LLMs excel at reasoning tasks, their ability to maintain stable and consistent rule abstraction under imperfect observations remains underexplored. To fill this gap, in this work, we introduce Robust Rule Induction, a task that evaluates LLMs' capability in inferring rules from data that are fused with noisy examples. To address this task, we further propose Sample-steered Rule Refinement (SRR), a method enhancing reasoning stability via observation diversification and execution-guided feedback. Experiments across arithmetic, cryptography, and list functions reveal: (1) SRR outperforms other methods with minimal performance degradation under noise; (2) Despite slight accuracy variation, LLMs exhibit instability under noise (e.g., 0% accuracy change with only 70% consistent score); (3) Counterfactual task gaps highlight LLMs' reliance on memorized patterns over genuine abstraction. Our findings challenge LLMs' reasoning robustness, revealing susceptibility to hypothesis drift and pattern overfitting, while providing empirical evidence critical for developing human-like inductive systems. Code and data are available at \href{https://github.com/lcy2723/Robust-Rule-Induction}{https://github.com/lcy2723/Robust-Rule-Induction}.

### Worse than Zero-shot? A Fact-Checking Dataset for Evaluating the Robustness of RAG Against Misleading Retrievals 
[[arxiv](https://arxiv.org/abs/2502.16101)] [[cool](https://papers.cool/arxiv/2502.16101)] [[pdf](https://arxiv.org/pdf/2502.16101)]
> **Authors**: Linda Zeng,Rithwik Gupta,Divij Motwani,Diji Yang,Yi Zhang
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,信息检索
- **Abstract**: Retrieval-augmented generation (RAG) has shown impressive capabilities in mitigating hallucinations in large language models (LLMs). However, LLMs struggle to handle misleading retrievals and often fail to maintain their own reasoning when exposed to conflicting or selectively-framed evidence, making them vulnerable to real-world misinformation. In such real-world retrieval scenarios, misleading and conflicting information is rampant, particularly in the political domain, where evidence is often selectively framed, incomplete, or polarized. However, existing RAG benchmarks largely assume a clean retrieval setting, where models succeed by accurately retrieving and generating answers from gold-standard documents. This assumption fails to align with real-world conditions, leading to an overestimation of RAG system performance. To bridge this gap, we introduce RAGuard, a fact-checking dataset designed to evaluate the robustness of RAG systems against misleading retrievals. Unlike prior benchmarks that rely on synthetic noise, our dataset constructs its retrieval corpus from Reddit discussions, capturing naturally occurring misinformation. It categorizes retrieved evidence into three types: supporting, misleading, and irrelevant, providing a realistic and challenging testbed for assessing how well RAG systems navigate different retrieval information. Our benchmark experiments reveal that when exposed to misleading retrievals, all tested LLM-powered RAG systems perform worse than their zero-shot baselines (i.e., no retrieval at all), highlighting their susceptibility to noisy environments. To the best of our knowledge, RAGuard is the first benchmark to systematically assess RAG robustness against misleading evidence. We expect this benchmark will drive future research toward improving RAG systems beyond idealized datasets, making them more reliable for real-world applications.

### Curie: Toward Rigorous and Automated Scientific Experimentation with AI Agents 
[[arxiv](https://arxiv.org/abs/2502.16069)] [[cool](https://papers.cool/arxiv/2502.16069)] [[pdf](https://arxiv.org/pdf/2502.16069)]
> **Authors**: Patrick Tser Jern Kon,Jiachen Liu,Qiuyi Ding,Yiming Qiu,Zhenning Yang,Yibo Huang,Jayanth Srinivasa,Myungjin Lee,Mosharaf Chowdhury,Ang Chen
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 21 pages
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: Scientific experimentation, a cornerstone of human progress, demands rigor in reliability, methodical control, and interpretability to yield meaningful results. Despite the growing capabilities of large language models (LLMs) in automating different aspects of the scientific process, automating rigorous experimentation remains a significant challenge. To address this gap, we propose Curie, an AI agent framework designed to embed rigor into the experimentation process through three key components: an intra-agent rigor module to enhance reliability, an inter-agent rigor module to maintain methodical control, and an experiment knowledge module to enhance interpretability. To evaluate Curie, we design a novel experimental benchmark composed of 46 questions across four computer science domains, derived from influential research papers, and widely adopted open-source projects. Compared to the strongest baseline tested, we achieve a 3.4$\times$ improvement in correctly answering experimental questions. Curie is open-sourced at https://github.com/Just-Curieous/Curie.

### Forecasting Open-Weight AI Model Growth on Hugging Face 
[[arxiv](https://arxiv.org/abs/2502.15987)] [[cool](https://papers.cool/arxiv/2502.15987)] [[pdf](https://arxiv.org/pdf/2502.15987)]
> **Authors**: Kushal Raj Bhandari,Pin-Yu Chen,Jianxi Gao
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Link to the website for trajectory visualization: https://forecasthuggingfacemodels.onrender.com/
- **标题**: None
- **领域**: 人工智能,计算机与社会,物理与社会
- **Abstract**: As the open-weight AI landscape continues to proliferate-with model development, significant investment, and user interest-it becomes increasingly important to predict which models will ultimately drive innovation and shape AI ecosystems. Building on parallels with citation dynamics in scientific literature, we propose a framework to quantify how an open-weight model's influence evolves. Specifically, we adapt the model introduced by Wang et al. for scientific citations, using three key parameters-immediacy, longevity, and relative fitness-to track the cumulative number of fine-tuned models of an open-weight model. Our findings reveal that this citation-style approach can effectively capture the diverse trajectories of open-weight model adoption, with most models fitting well and outliers indicating unique patterns or abrupt jumps in usage.

### A Knowledge Distillation-Based Approach to Enhance Transparency of Classifier Models 
[[arxiv](https://arxiv.org/abs/2502.15959)] [[cool](https://papers.cool/arxiv/2502.15959)] [[pdf](https://arxiv.org/pdf/2502.15959)]
> **Authors**: Yuchen Jiang,Xinyuan Zhao,Yihang Wu,Ahmad Chaddad
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Accepted in AAAI 2025
- **标题**: None
- **领域**: 人工智能
- **Abstract**: With the rapid development of artificial intelligence (AI), especially in the medical field, the need for its explainability has grown. In medical image analysis, a high degree of transparency and model interpretability can help clinicians better understand and trust the decision-making process of AI models. In this study, we propose a Knowledge Distillation (KD)-based approach that aims to enhance the transparency of the AI model in medical image analysis. The initial step is to use traditional CNN to obtain a teacher model and then use KD to simplify the CNN architecture, retain most of the features of the data set, and reduce the number of network layers. It also uses the feature map of the student model to perform hierarchical analysis to identify key features and decision-making processes. This leads to intuitive visual explanations. We selected three public medical data sets (brain tumor, eye disease, and Alzheimer's disease) to test our method. It shows that even when the number of layers is reduced, our model provides a remarkable result in the test set and reduces the time required for the interpretability analysis.

### Practical Principles for AI Cost and Compute Accounting 
[[arxiv](https://arxiv.org/abs/2502.15873)] [[cool](https://papers.cool/arxiv/2502.15873)] [[pdf](https://arxiv.org/pdf/2502.15873)]
> **Authors**: Stephen Casper,Luke Bailey,Tim Schreier
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算机与社会
- **Abstract**: Policymakers are increasingly using development cost and compute as proxies for AI model capabilities and risks. Recent laws have introduced regulatory requirements that are contingent on specific thresholds. However, technical ambiguities in how to perform this accounting could create loopholes that undermine regulatory effectiveness. This paper proposes seven principles for designing practical AI cost and compute accounting standards that (1) reduce opportunities for strategic gaming, (2) avoid disincentivizing responsible risk mitigation, and (3) enable consistent implementation across companies and jurisdictions.

### C3AI: Crafting and Evaluating Constitutions for Constitutional AI 
[[arxiv](https://arxiv.org/abs/2502.15861)] [[cool](https://papers.cool/arxiv/2502.15861)] [[pdf](https://arxiv.org/pdf/2502.15861)]
> **Authors**: Yara Kyrychenko,Ke Zhou,Edyta Bogucka,Daniele Quercia
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: This has been accepted for the Web Conference 2025
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: Constitutional AI (CAI) guides LLM behavior using constitutions, but identifying which principles are most effective for model alignment remains an open challenge. We introduce the C3AI framework (\textit{Crafting Constitutions for CAI models}), which serves two key functions: (1) selecting and structuring principles to form effective constitutions before fine-tuning; and (2) evaluating whether fine-tuned CAI models follow these principles in practice. By analyzing principles from AI and psychology, we found that positively framed, behavior-based principles align more closely with human preferences than negatively framed or trait-based principles. In a safety alignment use case, we applied a graph-based principle selection method to refine an existing CAI constitution, improving safety measures while maintaining strong general reasoning capabilities. Interestingly, fine-tuned CAI models performed well on negatively framed principles but struggled with positively framed ones, in contrast to our human alignment results. This highlights a potential gap between principle design and model adherence. Overall, C3AI provides a structured and scalable approach to both crafting and evaluating CAI constitutions.

### Vending-Bench: A Benchmark for Long-Term Coherence of Autonomous Agents 
[[arxiv](https://arxiv.org/abs/2502.15840)] [[cool](https://papers.cool/arxiv/2502.15840)] [[pdf](https://arxiv.org/pdf/2502.15840)]
> **Authors**: Axel Backlund,Lukas Petersson
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: While Large Language Models (LLMs) can exhibit impressive proficiency in isolated, short-term tasks, they often fail to maintain coherent performance over longer time horizons. In this paper, we present Vending-Bench, a simulated environment designed to specifically test an LLM-based agent's ability to manage a straightforward, long-running business scenario: operating a vending machine. Agents must balance inventories, place orders, set prices, and handle daily fees - tasks that are each simple but collectively, over long horizons (>20M tokens per run) stress an LLM's capacity for sustained, coherent decision-making. Our experiments reveal high variance in performance across multiple LLMs: Claude 3.5 Sonnet and o3-mini manage the machine well in most runs and turn a profit, but all models have runs that derail, either through misinterpreting delivery schedules, forgetting orders, or descending into tangential "meltdown" loops from which they rarely recover. We find no clear correlation between failures and the point at which the model's context window becomes full, suggesting that these breakdowns do not stem from memory limits. Apart from highlighting the high variance in performance over long time horizons, Vending-Bench also tests models' ability to acquire capital, a necessity in many hypothetical dangerous AI scenarios. We hope the benchmark can help in preparing for the advent of stronger AI systems.

### A novel approach to the relationships between data features -- based on comprehensive examination of mathematical, technological, and causal methodology 
[[arxiv](https://arxiv.org/abs/2502.15838)] [[cool](https://papers.cool/arxiv/2502.15838)] [[pdf](https://arxiv.org/pdf/2502.15838)]
> **Authors**: JaeHong Kim
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-24
> **comment**: 59 pages, 6 figures, 2 tables
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: The expansion of artificial intelligence (AI) has raised concerns about transparency, accountability, and interpretability, with counterfactual reasoning emerging as a key approach to addressing these issues. However, current mathematical, technological, and causal methodologies rely on externalization techniques that normalize feature relationships within a single coordinate space, often distorting intrinsic interactions. This study proposes the Convergent Fusion Paradigm (CFP) theory, a framework integrating mathematical, technological, and causal perspectives to provide a more precise and comprehensive analysis of feature relationships. CFP theory introduces Hilbert space and backward causation to reinterpret the feature relationships as emergent structures, offering a potential solution to the common cause problem -- a fundamental challenge in causal modeling. From a mathematical -- technical perspective, it utilizes a Riemannian manifold-based framework, thereby improving the structural representation of high- and low-dimensional data interactions. From a causal inference perspective, CFP theory adopts abduction as a methodological foundation, employing Hilbert space for a dynamic causal reasoning approach, where causal relationships are inferred abductively, and feature relationships evolve as emergent properties. Ultimately, CFP theory introduces a novel AI modeling methodology that integrates Hilbert space, backward causation, and Riemannian geometry, strengthening AI governance and transparency in counterfactual reasoning.

### Universal AI maximizes Variational Empowerment 
[[arxiv](https://arxiv.org/abs/2502.15820)] [[cool](https://papers.cool/arxiv/2502.15820)] [[pdf](https://arxiv.org/pdf/2502.15820)]
> **Authors**: Yusuke Hayashi,Koichi Takahashi
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-24
> **comment**: 9 pages, no figures
- **标题**: None
- **领域**: 人工智能,机器学习,机器学习
- **Abstract**: This paper presents a theoretical framework unifying AIXI -- a model of universal AI -- with variational empowerment as an intrinsic drive for exploration. We build on the existing framework of Self-AIXI -- a universal learning agent that predicts its own actions -- by showing how one of its established terms can be interpreted as a variational empowerment objective. We further demonstrate that universal AI's planning process can be cast as minimizing expected variational free energy (the core principle of active Inference), thereby revealing how universal AI agents inherently balance goal-directed behavior with uncertainty reduction curiosity). Moreover, we argue that power-seeking tendencies of universal AI agents can be explained not only as an instrumental strategy to secure future reward, but also as a direct consequence of empowerment maximization -- i.e.\ the agent's intrinsic drive to maintain or expand its own controllability in uncertain environments. Our main contribution is to show how these intrinsic motivations (empowerment, curiosity) systematically lead universal AI agents to seek and sustain high-optionality states. We prove that Self-AIXI asymptotically converges to the same performance as AIXI under suitable conditions, and highlight that its power-seeking behavior emerges naturally from both reward maximization and curiosity-driven exploration. Since AIXI can be view as a Bayes-optimal mathematical formulation for Artificial General Intelligence (AGI), our result can be useful for further discussion on AI safety and the controllability of AGI.

### Lean-ing on Quality: How High-Quality Data Beats Diverse Multilingual Data in AutoFormalization 
[[arxiv](https://arxiv.org/abs/2502.15795)] [[cool](https://papers.cool/arxiv/2502.15795)] [[pdf](https://arxiv.org/pdf/2502.15795)]
> **Authors**: Willy Chan,Michael Souliman,Jakob Nordhagen,Brando Miranda,Elyas Obbad,Kai Fronsdal Sanmi Koyejo
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学,机器学习,编程语言
- **Abstract**: Autoformalization, the process of transforming informal mathematical language into formal specifications and proofs remains a difficult task for state-of-the-art (large) language models. Existing works point to competing explanations for the performance gap. To this end, we introduce a novel methodology that leverages back-translation with hand-curated prompts to enhance the mathematical capabilities of language models, particularly addressing the challenge posed by the scarcity of labeled data. Specifically, we evaluate three primary variations of this strategy: (1) on-the-fly (online) backtranslation, (2) distilled (offline) backtranslation with few-shot amplification, and (3) line-by-line proof analysis integrated with proof state information. Each variant is designed to optimize data quality over quantity, focusing on the high fidelity of generated proofs rather than sheer data scale. Our findings provide evidence that employing our proposed approaches to generate synthetic data, which prioritizes quality over volume, improves the Autoformalization performance of LLMs as measured by standard benchmarks such as ProofNet. Crucially, our approach outperforms pretrained models using a minimal number of tokens. We also show, through strategic prompting and backtranslation, that our approaches surpass the performance of fine-tuning with extensive multilingual datasets such as MMA on ProofNet with only 1/150th of the tokens. Taken together, our methods show a promising new approach to significantly reduce the resources required to formalize proofs, thereby accelerating AI for math.

### One for All: A General Framework of LLMs-based Multi-Criteria Decision Making on Human Expert Level 
[[arxiv](https://arxiv.org/abs/2502.15778)] [[cool](https://papers.cool/arxiv/2502.15778)] [[pdf](https://arxiv.org/pdf/2502.15778)]
> **Authors**: Hui Wang,Fafa Zhang,Chaoxu Mu
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-24
> **comment**: 11 pages, 6 figures
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Multi-Criteria Decision Making~(MCDM) is widely applied in various fields, using quantitative and qualitative analyses of multiple levels and attributes to support decision makers in making scientific and rational decisions in complex scenarios. However, traditional MCDM methods face bottlenecks in high-dimensional problems. Given the fact that Large Language Models~(LLMs) achieve impressive performance in various complex tasks, but limited work evaluates LLMs in specific MCDM problems with the help of human domain experts, we further explore the capability of LLMs by proposing an LLM-based evaluation framework to automatically deal with general complex MCDM problems. Within the framework, we assess the performance of various typical open-source models, as well as commercial models such as Claude and ChatGPT, on 3 important applications, these models can only achieve around 60\% accuracy rate compared to the evaluation ground truth. Upon incorporation of Chain-of-Thought or few-shot prompting, the accuracy rates rise to around 70\%, and highly depend on the model. In order to further improve the performance, a LoRA-based fine-tuning technique is employed. The experimental results show that the accuracy rates for different applications improve significantly to around 95\%, and the performance difference is trivial between different models, indicating that LoRA-based fine-tuned LLMs exhibit significant and stable advantages in addressing MCDM tasks and can provide human-expert-level solutions to a wide range of MCDM challenges.

### Logic.py: Bridging the Gap between LLMs and Constraint Solvers 
[[arxiv](https://arxiv.org/abs/2502.15776)] [[cool](https://papers.cool/arxiv/2502.15776)] [[pdf](https://arxiv.org/pdf/2502.15776)]
> **Authors**: Pascal Kesseli,Peter O'Hearn,Ricardo Silveira Cabral
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-24
> **comment**: 11 pages,9 figures
- **标题**: None
- **领域**: 人工智能,计算机科学中的逻辑
- **Abstract**: We present a novel approach to formalise and solve search-based problems using large language models, which significantly improves upon previous state-of-the-art results. We demonstrate the efficacy of this approach on the logic puzzles benchmark ZebraLogicBench. Instead of letting the LLM attempt to directly solve the puzzles, our method prompts the model to formalise the problem in a logic-focused domain-specific language (DSL) called Logic.py. This formalised representation is then solved using a constraint solver, leveraging the strengths of both the language model and the solver. Our approach achieves a remarkable 65% absolute improvement over the baseline performance of Llama 3.1 70B on ZebraLogicBench, setting a new state-of-the-art with an accuracy of over 90%. This significant advancement demonstrates the potential of combining language models with domain-specific languages and auxiliary tools on traditionally challenging tasks for LLMs.

### The Process of Categorical Clipping at the Core of the Genesis of Concepts in Synthetic Neural Cognition 
[[arxiv](https://arxiv.org/abs/2502.15710)] [[cool](https://papers.cool/arxiv/2502.15710)] [[pdf](https://arxiv.org/pdf/2502.15710)]
> **Authors**: Michael Pichat,William Pogrund,Armanush Gasparian,Paloma Pichat,Samuel Demarchi,Michael Veillet-Guillem,Martin Corbet,Théo Dasilva
> **First submission**: 2025-01-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,神经元和认知
- **Abstract**: This article investigates, within the field of neuropsychology of artificial intelligence, the process of categorical segmentation performed by language models. This process involves, across different neural layers, the creation of new functional categorical dimensions to analyze the input textual data and perform the required tasks. Each neuron in a multilayer perceptron (MLP) network is associated with a specific category, generated by three factors carried by the neural aggregation function: categorical priming, categorical attention, and categorical phasing. At each new layer, these factors govern the formation of new categories derived from the categories of precursor neurons. Through a process of categorical clipping, these new categories are created by selectively extracting specific subdimensions from the preceding categories, constructing a distinction between a form and a categorical background. We explore several cognitive characteristics of this synthetic clipping in an exploratory manner: categorical reduction, categorical selectivity, separation of initial embedding dimensions, and segmentation of categorical zones.

### Knowledge Graphs: The Future of Data Integration and Insightful Discovery 
[[arxiv](https://arxiv.org/abs/2502.15689)] [[cool](https://papers.cool/arxiv/2502.15689)] [[pdf](https://arxiv.org/pdf/2502.15689)]
> **Authors**: Saher Mohamed,Kirollos Farah,Abdelrahman Lotfy,Kareem Rizk,Abdelrahman Saeed,Shahenda Mohamed,Ghada Khouriba,Tamer Arafa
> **First submission**: 2024-12-17
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: Knowledge graphs are an efficient method for representing and connecting information across various concepts, useful in reasoning, question answering, and knowledge base completion tasks. They organize data by linking points, enabling researchers to combine diverse information sources into a single database. This interdisciplinary approach helps uncover new research questions and ideas. Knowledge graphs create a web of data points (nodes) and their connections (edges), which enhances navigation, comprehension, and utilization of data for multiple purposes. They capture complex relationships inherent in unstructured data sources, offering a semantic framework for diverse entities and their attributes. Strategies for developing knowledge graphs include using seed data, named entity recognition, and relationship extraction. These graphs enhance chatbot accuracy and include multimedia data for richer information. Creating high-quality knowledge graphs involves both automated methods and human oversight, essential for accurate and comprehensive data representation.

### AutoToM: Automated Bayesian Inverse Planning and Model Discovery for Open-ended Theory of Mind 
[[arxiv](https://arxiv.org/abs/2502.15676)] [[cool](https://papers.cool/arxiv/2502.15676)] [[pdf](https://arxiv.org/pdf/2502.15676)]
> **Authors**: Zhining Zhang,Chuanyang Jin,Mung Yao Jia,Tianmin Shu
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 23 pages, 6 figures, 11 tables. Website at https://chuanyangjin.com/AutoToM/
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: Theory of Mind (ToM), the ability to understand people's mental variables based on their behavior, is key to developing socially intelligent agents. Current approaches to Theory of Mind reasoning either rely on prompting Large Language Models (LLMs), which are prone to systematic errors, or use rigid, handcrafted Bayesian Theory of Mind (BToM) models, which are more robust but cannot generalize across different domains. In this work, we introduce AutoToM, an automated Bayesian Theory of Mind method for achieving open-ended machine Theory of Mind. AutoToM can operate in any domain, infer any mental variable, and conduct robust Theory of Mind reasoning of any order. Given a Theory of Mind inference problem, AutoToM first proposes an initial BToM model. It then conducts automated Bayesian inverse planning based on the proposed model, leveraging an LLM as the backend. Based on the uncertainty of the inference, it iteratively refines the model, by introducing additional mental variables and/or incorporating more timesteps in the context. Empirical evaluations across multiple Theory of Mind benchmarks demonstrate that AutoToM consistently achieves state-of-the-art performance, offering a scalable, robust, and interpretable approach to machine Theory of Mind.

### Automating Curriculum Learning for Reinforcement Learning using a Skill-Based Bayesian Network 
[[arxiv](https://arxiv.org/abs/2502.15662)] [[cool](https://papers.cool/arxiv/2502.15662)] [[pdf](https://arxiv.org/pdf/2502.15662)]
> **Authors**: Vincent Hsiao,Mark Roberts,Laura M. Hiatt,George Konidaris,Dana Nau
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: A major challenge for reinforcement learning is automatically generating curricula to reduce training time or improve performance in some target task. We introduce SEBNs (Skill-Environment Bayesian Networks) which model a probabilistic relationship between a set of skills, a set of goals that relate to the reward structure, and a set of environment features to predict policy performance on (possibly unseen) tasks. We develop an algorithm that uses the inferred estimates of agent success from SEBN to weigh the possible next tasks by expected improvement. We evaluate the benefit of the resulting curriculum on three environments: a discrete gridworld, continuous control, and simulated robotics. The results show that curricula constructed using SEBN frequently outperform other baselines.

### Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer a Safer Path? 
[[arxiv](https://arxiv.org/abs/2502.15657)] [[cool](https://papers.cool/arxiv/2502.15657)] [[pdf](https://arxiv.org/pdf/2502.15657)]
> **Authors**: Yoshua Bengio,Michael Cohen,Damiano Fornasiere,Joumana Ghosn,Pietro Greiner,Matt MacDermott,Sören Mindermann,Adam Oberman,Jesse Richardson,Oliver Richardson,Marc-Antoine Rondeau,Pierre-Luc St-Charles,David Williams-King
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: v2 with fixed formatting for URLs and hyperlinks
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: The leading AI companies are increasingly focused on building generalist AI agents -- systems that can autonomously plan, act, and pursue goals across almost all tasks that humans can perform. Despite how useful these systems might be, unchecked AI agency poses significant risks to public safety and security, ranging from misuse by malicious actors to a potentially irreversible loss of human control. We discuss how these risks arise from current AI training methods. Indeed, various scenarios and experiments have demonstrated the possibility of AI agents engaging in deception or pursuing goals that were not specified by human operators and that conflict with human interests, such as self-preservation. Following the precautionary principle, we see a strong need for safer, yet still useful, alternatives to the current agency-driven trajectory. Accordingly, we propose as a core building block for further advances the development of a non-agentic AI system that is trustworthy and safe by design, which we call Scientist AI. This system is designed to explain the world from observations, as opposed to taking actions in it to imitate or please humans. It comprises a world model that generates theories to explain data and a question-answering inference machine. Both components operate with an explicit notion of uncertainty to mitigate the risks of overconfident predictions. In light of these considerations, a Scientist AI could be used to assist human researchers in accelerating scientific progress, including in AI safety. In particular, our system can be employed as a guardrail against AI agents that might be created despite the risks involved. Ultimately, focusing on non-agentic AI may enable the benefits of AI innovation while avoiding the risks associated with the current trajectory. We hope these arguments will motivate researchers, developers, and policymakers to favor this safer path.

### Empowering LLMs with Logical Reasoning: A Comprehensive Survey 
[[arxiv](https://arxiv.org/abs/2502.15652)] [[cool](https://papers.cool/arxiv/2502.15652)] [[pdf](https://arxiv.org/pdf/2502.15652)]
> **Authors**: Fengxiang Cheng,Haoxuan Li,Fenrong Liu,Robert van Rooij,Kun Zhang,Zhouchen Lin
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: Large language models (LLMs) have achieved remarkable successes on various natural language tasks. However, recent studies have found that there are still significant challenges to the logical reasoning abilities of LLMs. This paper summarizes and categorizes the main challenges into two aspects: (1) Logical question answering, LLMs often fail to generate the correct answer within complex logical problem which requires sophisticated deductive, inductive or abductive reasoning given a collection of premises and constrains. (2) Logical consistency, LLMs are prone to producing responses contradicting themselves across different questions. For example, a state-of-the-art Macaw question-answering LLM answers Yes to both questions Is a magpie a bird? and Does a bird have wings? but answers No to Does a magpie have wings?. To facilitate this research direction, we comprehensively investigate the most cutting-edge methods and propose detailed taxonomies of these methods. Specifically, to accurately answer complex logic questions, previous methods can be categorized based on reliance on external solvers, prompts, pretraining, and fine-tuning. To avoid logical contradictions, we discuss concepts and solutions of various logical consistencies, including implication, negation, transitivity, factuality consistency, and their composites. In addition, we review commonly used benchmark datasets and evaluation metrics, and discuss promising research directions, such as extensions to modal logic to account for uncertainty, and efficient algorithms satisfying multiple logical consistencies simultaneously.

### Paradigms of AI Evaluation: Mapping Goals, Methodologies and Culture 
[[arxiv](https://arxiv.org/abs/2502.15620)] [[cool](https://papers.cool/arxiv/2502.15620)] [[pdf](https://arxiv.org/pdf/2502.15620)]
> **Authors**: John Burden,Marko Tešić,Lorenzo Pacchiardi,José Hernández-Orallo
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: Research in AI evaluation has grown increasingly complex and multidisciplinary, attracting researchers with diverse backgrounds and objectives. As a result, divergent evaluation paradigms have emerged, often developing in isolation, adopting conflicting terminologies, and overlooking each other's contributions. This fragmentation has led to insular research trajectories and communication barriers both among different paradigms and with the general public, contributing to unmet expectations for deployed AI systems. To help bridge this insularity, in this paper we survey recent work in the AI evaluation landscape and identify six main paradigms. We characterise major recent contributions within each paradigm across key dimensions related to their goals, methodologies and research cultures. By clarifying the unique combination of questions and approaches associated with each paradigm, we aim to increase awareness of the breadth of current evaluation approaches and foster cross-pollination between different paradigms. We also identify potential gaps in the field to inspire future research directions.

### TAG: A Decentralized Framework for Multi-Agent Hierarchical Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.15425)] [[cool](https://papers.cool/arxiv/2502.15425)] [[pdf](https://arxiv.org/pdf/2502.15425)]
> **Authors**: Giuseppe Paolo,Abdelhakim Benechehab,Hamza Cherkaoui,Albert Thomas,Balázs Kégl
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习,系统与控制
- **Abstract**: Hierarchical organization is fundamental to biological systems and human societies, yet artificial intelligence systems often rely on monolithic architectures that limit adaptability and scalability. Current hierarchical reinforcement learning (HRL) approaches typically restrict hierarchies to two levels or require centralized training, which limits their practical applicability. We introduce TAME Agent Framework (TAG), a framework for constructing fully decentralized hierarchical multi-agent systems.TAG enables hierarchies of arbitrary depth through a novel LevelEnv concept, which abstracts each hierarchy level as the environment for the agents above it. This approach standardizes information flow between levels while preserving loose coupling, allowing for seamless integration of diverse agent types. We demonstrate the effectiveness of TAG by implementing hierarchical architectures that combine different RL agents across multiple levels, achieving improved performance over classical multi-agent RL baselines on standard benchmarks. Our results show that decentralized hierarchical organization enhances both learning speed and final performance, positioning TAG as a promising direction for scalable multi-agent systems.

### Chitrarth: Bridging Vision and Language for a Billion People 
[[arxiv](https://arxiv.org/abs/2502.15392)] [[cool](https://papers.cool/arxiv/2502.15392)] [[pdf](https://arxiv.org/pdf/2502.15392)]
> **Authors**: Shaharukh Khan,Ayush Tarun,Abhinav Ravi,Ali Faraz,Akshat Patidar,Praveen Kumar Pokala,Anagha Bhangare,Raja Kolla,Chandra Khatri,Shubham Agarwal
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学,计算机视觉和模式识别
- **Abstract**: Recent multimodal foundation models are primarily trained on English or high resource European language data, which hinders their applicability to other medium and low-resource languages. To address this limitation, we introduce Chitrarth (Chitra: Image; Artha: Meaning), an inclusive Vision-Language Model (VLM), specifically targeting the rich linguistic diversity and visual reasoning across 10 prominent Indian languages. Our model effectively integrates a state-of-the-art (SOTA) multilingual Large Language Model (LLM) with a vision module, primarily trained on multilingual image-text data. Furthermore, we also introduce BharatBench, a comprehensive framework for evaluating VLMs across various Indian languages, ultimately contributing to more diverse and effective AI systems. Our model achieves SOTA results for benchmarks across low resource languages while retaining its efficiency in English. Through our research, we aim to set new benchmarks in multilingual-multimodal capabilities, offering substantial improvements over existing models and establishing a foundation to facilitate future advancements in this arena.

### ARS: Automatic Routing Solver with Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.15359)] [[cool](https://papers.cool/arxiv/2502.15359)] [[pdf](https://arxiv.org/pdf/2502.15359)]
> **Authors**: Kai Li,Fei Liu,Zhenkun Wang,Xialiang Tong,Xiongwei Han,Mingxuan Yuan
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: Real-world Vehicle Routing Problems (VRPs) are characterized by a variety of practical constraints, making manual solver design both knowledge-intensive and time-consuming. Although there is increasing interest in automating the design of routing algorithms, existing research has explored only a limited array of VRP variants and fails to adequately address the complex and prevalent constraints encountered in real-world situations. To fill this gap, this paper introduces RoutBench, a benchmark of 1,000 VRP variants derived from 24 attributes, for evaluating the effectiveness of automatic routing solvers in addressing complex constraints. Along with RoutBench, we present the Automatic Routing Solver (ARS), which employs Large Language Model (LLM) agents to enhance a backbone algorithm framework by automatically generating constraint-aware heuristic code, based on problem descriptions and several representative constraints selected from a database. Our experiments show that ARS outperforms state-of-the-art LLM-based methods and commonly used solvers, automatically solving 91.67% of common VRPs and achieving at least a 30% improvement across all benchmarks.

## 硬件架构(cs.AR:Hardware Architecture)

### DeepRTL: Bridging Verilog Understanding and Generation with a Unified Representation Model 
[[arxiv](https://arxiv.org/abs/2502.15832)] [[cool](https://papers.cool/arxiv/2502.15832)] [[pdf](https://arxiv.org/pdf/2502.15832)]
> **Authors**: Yi Liu,Changran Xu,Yunhao Zhou,Zeju Li,Qiang Xu
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-24
> **comment**: ICLR 2025 Spotlight
- **标题**: None
- **领域**: 硬件架构,计算语言学,机器学习
- **Abstract**: Recent advancements in large language models (LLMs) have shown significant potential for automating hardware description language (HDL) code generation from high-level natural language instructions. While fine-tuning has improved LLMs' performance in hardware design tasks, prior efforts have largely focused on Verilog generation, overlooking the equally critical task of Verilog understanding. Furthermore, existing models suffer from weak alignment between natural language descriptions and Verilog code, hindering the generation of high-quality, synthesizable designs. To address these issues, we present DeepRTL, a unified representation model that excels in both Verilog understanding and generation. Based on CodeT5+, DeepRTL is fine-tuned on a comprehensive dataset that aligns Verilog code with rich, multi-level natural language descriptions. We also introduce the first benchmark for Verilog understanding and take the initiative to apply embedding similarity and GPT Score to evaluate the models' understanding capabilities. These metrics capture semantic similarity more accurately than traditional methods like BLEU and ROUGE, which are limited to surface-level n-gram overlaps. By adapting curriculum learning to train DeepRTL, we enable it to significantly outperform GPT-4 in Verilog understanding tasks, while achieving performance on par with OpenAI's o1-preview model in Verilog generation tasks.

### JExplore: Design Space Exploration Tool for Nvidia Jetson Boards 
[[arxiv](https://arxiv.org/abs/2502.15773)] [[cool](https://papers.cool/arxiv/2502.15773)] [[pdf](https://arxiv.org/pdf/2502.15773)]
> **Authors**: Basar Kutukcu,Sinan Xie,Sabur Baidya,Sujit Dey
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-24
> **comment**: 4 pages, 4 figures
- **标题**: None
- **领域**: 硬件架构,分布式、并行和集群计算,机器学习
- **Abstract**: Nvidia Jetson boards are powerful systems for executing artificial intelligence workloads in edge and mobile environments due to their effective GPU hardware and widely supported software stack. In addition to these benefits, Nvidia Jetson boards provide large configurability by giving the user the choice to modify many hardware parameters. This large space of configurability creates the need of searching the optimal configurations based on the user's requirements. In this work, we propose JExplore, a multi-board software and hardware design space exploration tool. JExplore can be integrated with any search tool, hence creating a common benchmarking ground for the search algorithms. Moreover, it accelerates the exploration of user application and Nvidia Jetson configurations for researchers and engineers by encapsulating host-client communication, configuration management, and metric measurement.

### PAPI: Exploiting Dynamic Parallelism in Large Language Model Decoding with a Processing-In-Memory-Enabled Computing System 
[[arxiv](https://arxiv.org/abs/2502.15470)] [[cool](https://papers.cool/arxiv/2502.15470)] [[pdf](https://arxiv.org/pdf/2502.15470)]
> **Authors**: Yintao He,Haiyu Mao,Christina Giannoula,Mohammad Sadrosadati,Juan Gómez-Luna,Huawei Li,Xiaowei Li,Ying Wang,Onur Mutlu
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: To appear in ASPLOS 2025
- **标题**: None
- **领域**: 硬件架构,人工智能,分布式、并行和集群计算,机器学习
- **Abstract**: Large language models (LLMs) are widely used for natural language understanding and text generation. An LLM model relies on a time-consuming step called LLM decoding to generate output tokens. Several prior works focus on improving the performance of LLM decoding using parallelism techniques, such as batching and speculative decoding. State-of-the-art LLM decoding has both compute-bound and memory-bound kernels. Some prior works statically identify and map these different kernels to a heterogeneous architecture consisting of both processing-in-memory (PIM) units and computation-centric accelerators. We observe that characteristics of LLM decoding kernels (e.g., whether or not a kernel is memory-bound) can change dynamically due to parameter changes to meet user and/or system demands, making (1) static kernel mapping to PIM units and computation-centric accelerators suboptimal, and (2) one-size-fits-all approach of designing PIM units inefficient due to a large degree of heterogeneity even in memory-bound kernels. In this paper, we aim to accelerate LLM decoding while considering the dynamically changing characteristics of the kernels involved. We propose PAPI (PArallel Decoding with PIM), a PIM-enabled heterogeneous architecture that exploits dynamic scheduling of compute-bound or memory-bound kernels to suitable hardware units. PAPI has two key mechanisms: (1) online kernel characterization to dynamically schedule kernels to the most suitable hardware units at runtime and (2) a PIM-enabled heterogeneous computing system that harmoniously orchestrates both computation-centric processing units and hybrid PIM units with different computing capabilities. Our experimental results on three broadly-used LLMs show that PAPI achieves 1.8$\times$ and 11.1$\times$ speedups over a state-of-the-art heterogeneous LLM accelerator and a state-of-the-art PIM-only LLM accelerator, respectively.

## 计算工程、金融和科学(cs.CE:Computational Engineering, Finance, and Science)

### Predicting the Energy Landscape of Stochastic Dynamical System via Physics-informed Self-supervised Learning 
[[arxiv](https://arxiv.org/abs/2502.16828)] [[cool](https://papers.cool/arxiv/2502.16828)] [[pdf](https://arxiv.org/pdf/2502.16828)]
> **Authors**: Ruikun Li,Huandong Wang,Qingmin Liao,Yong Li
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算工程、金融和科学,人工智能,机器学习
- **Abstract**: Energy landscapes play a crucial role in shaping dynamics of many real-world complex systems. System evolution is often modeled as particles moving on a landscape under the combined effect of energy-driven drift and noise-induced diffusion, where the energy governs the long-term motion of the particles. Estimating the energy landscape of a system has been a longstanding interdisciplinary challenge, hindered by the high operational costs or the difficulty of obtaining supervisory signals. Therefore, the question of how to infer the energy landscape in the absence of true energy values is critical. In this paper, we propose a physics-informed self-supervised learning method to learn the energy landscape from the evolution trajectories of the system. It first maps the system state from the observation space to a discrete landscape space by an adaptive codebook, and then explicitly integrates energy into the graph neural Fokker-Planck equation, enabling the joint learning of energy estimation and evolution prediction. Experimental results across interdisciplinary systems demonstrate that our estimated energy has a correlation coefficient above 0.9 with the ground truth, and evolution prediction accuracy exceeds the baseline by an average of 17.65\%. The code is available at github.com/tsinghua-fib-lab/PESLA.

### AlphaAgent: LLM-Driven Alpha Mining with Regularized Exploration to Counteract Alpha Decay 
[[arxiv](https://arxiv.org/abs/2502.16789)] [[cool](https://papers.cool/arxiv/2502.16789)] [[pdf](https://arxiv.org/pdf/2502.16789)]
> **Authors**: Ziyi Tang,Zechuan Chen,Jiarui Yang,Jiayao Mai,Yongsen Zheng,Keze Wang,Jinrui Chen,Liang Lin
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: 9 pages
- **标题**: None
- **领域**: 计算工程、金融和科学,人工智能
- **Abstract**: Alpha mining, a critical component in quantitative investment, focuses on discovering predictive signals for future asset returns in increasingly complex financial markets. However, the pervasive issue of alpha decay, where factors lose their predictive power over time, poses a significant challenge for alpha mining. Traditional methods like genetic programming face rapid alpha decay from overfitting and complexity, while approaches driven by Large Language Models (LLMs), despite their promise, often rely too heavily on existing knowledge, creating homogeneous factors that worsen crowding and accelerate decay. To address this challenge, we propose AlphaAgent, an autonomous framework that effectively integrates LLM agents with ad hoc regularizations for mining decay-resistant alpha factors. AlphaAgent employs three key mechanisms: (i) originality enforcement through a similarity measure based on abstract syntax trees (ASTs) against existing alphas, (ii) hypothesis-factor alignment via LLM-evaluated semantic consistency between market hypotheses and generated factors, and (iii) complexity control via AST-based structural constraints, preventing over-engineered constructions that are prone to overfitting. These mechanisms collectively guide the alpha generation process to balance originality, financial rationale, and adaptability to evolving market conditions, mitigating the risk of alpha decay. Extensive evaluations show that AlphaAgent outperforms traditional and LLM-based methods in mitigating alpha decay across bull and bear markets, consistently delivering significant alpha in Chinese CSI 500 and US S&P 500 markets over the past four years. Notably, AlphaAgent showcases remarkable resistance to alpha decay, elevating the potential for yielding powerful factors.

### Interpreting core forms of urban morphology linked to urban functions with explainable graph neural network 
[[arxiv](https://arxiv.org/abs/2502.16210)] [[cool](https://papers.cool/arxiv/2502.16210)] [[pdf](https://arxiv.org/pdf/2502.16210)]
> **Authors**: Dongsheng Chen,Yu Feng,Xun Li,Mingya Qu,Peng Luo,Liqiu Meng
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算工程、金融和科学,机器学习
- **Abstract**: Understanding the high-order relationship between urban form and function is essential for modeling the underlying mechanisms of sustainable urban systems. Nevertheless, it is challenging to establish an accurate data representation for complex urban forms that are readily explicable in human terms. This study proposed the concept of core urban morphology representation and developed an explainable deep learning framework for explicably symbolizing complex urban forms into the novel representation, which we call CoMo. By interpretating the well-trained deep learning model with a stable weighted F1-score of 89.14%, CoMo presents a promising approach for revealing links between urban function and urban form in terms of core urban morphology representation. Using Boston as a study area, we analyzed the core urban forms at the individual-building, block, and neighborhood level that are important to corresponding urban functions. The residential core forms follow a gradual morphological pattern along the urban spine, which is consistent with a center-urban-suburban transition. Furthermore, we prove that urban morphology directly affects land use efficiency, which has a significantly strong correlation with the location (R2=0.721, p<0.001). Overall, CoMo can explicably symbolize urban forms, provide evidence for the classic urban location theory, and offer mechanistic insights for digital twins.

## 计算语言学(cs.CL:Computation and Language)

### "Actionable Help" in Crises: A Novel Dataset and Resource-Efficient Models for Identifying Request and Offer Social Media Posts 
[[arxiv](https://arxiv.org/abs/2502.16839)] [[cool](https://papers.cool/arxiv/2502.16839)] [[pdf](https://arxiv.org/pdf/2502.16839)]
> **Authors**: Rabindra Lamsal,Maria Rodriguez Read,Shanika Karunasekera,Muhammad Imran
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: During crises, social media serves as a crucial coordination tool, but the vast influx of posts--from "actionable" requests and offers to generic content like emotional support, behavioural guidance, or outdated information--complicates effective classification. Although generative LLMs (Large Language Models) can address this issue with few-shot classification, their high computational demands limit real-time crisis response. While fine-tuning encoder-only models (e.g., BERT) is a popular choice, these models still exhibit higher inference times in resource-constrained environments. Moreover, although distilled variants (e.g., DistilBERT) exist, they are not tailored for the crisis domain. To address these challenges, we make two key contributions. First, we present CrisisHelpOffer, a novel dataset of 101k tweets collaboratively labelled by generative LLMs and validated by humans, specifically designed to distinguish actionable content from noise. Second, we introduce the first crisis-specific mini models optimized for deployment in resource-constrained settings. Across 13 crisis classification tasks, our mini models surpass BERT (also outperform or match the performance of RoBERTa, MPNet, and BERTweet), offering higher accuracy with significantly smaller sizes and faster speeds. The Medium model is 47% smaller with 3.8% higher accuracy at 3.5x speed, the Small model is 68% smaller with a 1.8% accuracy gain at 7.7x speed, and the Tiny model, 83% smaller, matches BERT's accuracy at 18.6x speed. All models outperform existing distilled variants, setting new benchmarks. Finally, as a case study, we analyze social media posts from a global crisis to explore help-seeking and assistance-offering behaviours in selected developing and developed countries.

### REGen: A Reliable Evaluation Framework for Generative Event Argument Extraction 
[[arxiv](https://arxiv.org/abs/2502.16838)] [[cool](https://papers.cool/arxiv/2502.16838)] [[pdf](https://arxiv.org/pdf/2502.16838)]
> **Authors**: Omar Sharif,Joseph Gatto,Madhusudan Basak,Sarah M. Preum
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: 20 pages, 9 figures, 13 tables
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Event argument extraction identifies arguments for predefined event roles in text. Traditional evaluations rely on exact match (EM), requiring predicted arguments to match annotated spans exactly. However, this approach fails for generative models like large language models (LLMs), which produce diverse yet semantically accurate responses. EM underestimates performance by disregarding valid variations, implicit arguments (unstated but inferable), and scattered arguments (distributed across a document). To bridge this gap, we introduce Reliable Evaluation framework for Generative event argument extraction (REGen), a framework that better aligns with human judgment. Across six datasets, REGen improves performance by an average of 23.93 F1 points over EM. Human validation further confirms REGen's effectiveness, achieving 87.67% alignment with human assessments of argument correctness.

### Finding the Sweet Spot: Preference Data Construction for Scaling Preference Optimization 
[[arxiv](https://arxiv.org/abs/2502.16825)] [[cool](https://papers.cool/arxiv/2502.16825)] [[pdf](https://arxiv.org/pdf/2502.16825)]
> **Authors**: Yao Xiao,Hai Ye,Linyao Chen,Hwee Tou Ng,Lidong Bing,Xiaoli Li,Roy Ka-wei Lee
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Iterative data generation and model retraining are widely used to align large language models (LLMs). It typically involves a policy model to generate on-policy responses and a reward model to guide training data selection. Direct Preference Optimization (DPO) further enhances this process by constructing preference pairs of chosen and rejected responses. In this work, we aim to \emph{scale up} the number of on-policy samples via repeated random sampling to improve alignment performance. Conventional practice selects the sample with the highest reward as chosen and the lowest as rejected for DPO. However, our experiments reveal that this strategy leads to a \emph{decline} in performance as the sample size increases. To address this, we investigate preference data construction through the lens of underlying normal distribution of sample rewards. We categorize the reward space into seven representative points and systematically explore all 21 ($C_7^2$) pairwise combinations. Through evaluations on four models using AlpacaEval 2, we find that selecting the rejected response at reward position $μ- 2σ$ rather than the minimum reward, is crucial for optimal performance. We finally introduce a scalable preference data construction strategy that consistently enhances model performance as the sample scale increases.

### Uncertainty Quantification of Large Language Models through Multi-Dimensional Responses 
[[arxiv](https://arxiv.org/abs/2502.16820)] [[cool](https://papers.cool/arxiv/2502.16820)] [[pdf](https://arxiv.org/pdf/2502.16820)]
> **Authors**: Tiejin Chen,Xiaoou Liu,Longchao Da,Jia Chen,Vagelis Papalexakis,Hua Wei
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks due to large training datasets and powerful transformer architecture. However, the reliability of responses from LLMs remains a question. Uncertainty quantification (UQ) of LLMs is crucial for ensuring their reliability, especially in areas such as healthcare, finance, and decision-making. Existing UQ methods primarily focus on semantic similarity, overlooking the deeper knowledge dimensions embedded in responses. We introduce a multi-dimensional UQ framework that integrates semantic and knowledge-aware similarity analysis. By generating multiple responses and leveraging auxiliary LLMs to extract implicit knowledge, we construct separate similarity matrices and apply tensor decomposition to derive a comprehensive uncertainty representation. This approach disentangles overlapping information from both semantic and knowledge dimensions, capturing both semantic variations and factual consistency, leading to more accurate UQ. Our empirical evaluations demonstrate that our method outperforms existing techniques in identifying uncertain responses, offering a more robust framework for enhancing LLM reliability in high-stakes applications.

### CoT2Align: Cross-Chain of Thought Distillation via Optimal Transport Alignment for Language Models with Different Tokenizers 
[[arxiv](https://arxiv.org/abs/2502.16806)] [[cool](https://papers.cool/arxiv/2502.16806)] [[pdf](https://arxiv.org/pdf/2502.16806)]
> **Authors**: Anh Duc Le,Tu Vu,Nam Le Hai,Nguyen Thi Ngoc Diep,Linh Ngo Van,Trung Le,Thien Huu Nguyen
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models (LLMs) achieve state-of-the-art performance across various NLP tasks but face deployment challenges due to high computational costs and memory constraints. Knowledge distillation (KD) is a promising solution, transferring knowledge from large teacher models to smaller student models. However, existing KD methods often assume shared vocabularies and tokenizers, limiting their flexibility. While approaches like Universal Logit Distillation (ULD) and Dual-Space Knowledge Distillation (DSKD) address vocabulary mismatches, they overlook the critical \textbf{reasoning-aware distillation} aspect. To bridge this gap, we propose CoT2Align a universal KD framework that integrates Chain-of-Thought (CoT) augmentation and introduces Cross-CoT Alignment to enhance reasoning transfer. Additionally, we extend Optimal Transport beyond token-wise alignment to a sequence-level and layer-wise alignment approach that adapts to varying sequence lengths while preserving contextual integrity. Comprehensive experiments demonstrate that CoT2Align outperforms existing KD methods across different vocabulary settings, improving reasoning capabilities and robustness in domain-specific tasks.

### Unsupervised Topic Models are Data Mixers for Pre-training Language Models 
[[arxiv](https://arxiv.org/abs/2502.16802)] [[cool](https://papers.cool/arxiv/2502.16802)] [[pdf](https://arxiv.org/pdf/2502.16802)]
> **Authors**: Jiahui Peng,Xinlin Zhuang,Qiu Jiantao,Ren Ma,Jing Yu,Tianyi Bai,Conghui He
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: 18 pages,7 figures
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: The performance of large language models (LLMs) is significantly affected by the quality and composition of their pre-training data, which is inherently diverse, spanning various domains, sources, and topics. Effectively integrating these heterogeneous data sources is crucial for optimizing LLM performance. Previous research has predominantly concentrated on domain-based data mixing, often neglecting the nuanced topic-level characteristics of the data. To address this gap, we propose a simple yet effective topic-based data mixing strategy that utilizes fine-grained topics generated through our topic modeling method, DataWeave. DataWeave employs a multi-stage clustering process to group semantically similar documents and utilizes LLMs to generate detailed topics, thereby facilitating a more nuanced understanding of dataset composition. Our strategy employs heuristic methods to upsample or downsample specific topics, which significantly enhances LLM performance on downstream tasks, achieving superior results compared to previous, more complex data mixing approaches. Furthermore, we confirm that the topics Science and Relationships are particularly effective, yielding the most substantial performance improvements. We will make our code and datasets publicly available.

### Are Large Language Models Good Data Preprocessors? 
[[arxiv](https://arxiv.org/abs/2502.16790)] [[cool](https://papers.cool/arxiv/2502.16790)] [[pdf](https://arxiv.org/pdf/2502.16790)]
> **Authors**: Elyas Meguellati,Nardiena Pratama,Shazia Sadiq,Gianluca Demartini
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: High-quality textual training data is essential for the success of multimodal data processing tasks, yet outputs from image captioning models like BLIP and GIT often contain errors and anomalies that are difficult to rectify using rule-based methods. While recent work addressing this issue has predominantly focused on using GPT models for data preprocessing on relatively simple public datasets, there is a need to explore a broader range of Large Language Models (LLMs) and tackle more challenging and diverse datasets. In this study, we investigate the use of multiple LLMs, including LLaMA 3.1 70B, GPT-4 Turbo, and Sonnet 3.5 v2, to refine and clean the textual outputs of BLIP and GIT. We assess the impact of LLM-assisted data cleaning by comparing downstream-task (SemEval 2024 Subtask "Multilabel Persuasion Detection in Memes") models trained on cleaned versus non-cleaned data. While our experimental results show improvements when using LLM-cleaned captions, statistical tests reveal that most of these improvements are not significant. This suggests that while LLMs have the potential to enhance data cleaning and repairing, their effectiveness may be limited depending on the context they are applied to, the complexity of the task, and the level of noise in the text. Our findings highlight the need for further research into the capabilities and limitations of LLMs in data preprocessing pipelines, especially when dealing with challenging datasets, contributing empirical evidence to the ongoing discussion about integrating LLMs into data preprocessing pipelines.

### MultiOCR-QA: Dataset for Evaluating Robustness of LLMs in Question Answering on Multilingual OCR Texts 
[[arxiv](https://arxiv.org/abs/2502.16781)] [[cool](https://papers.cool/arxiv/2502.16781)] [[pdf](https://arxiv.org/pdf/2502.16781)]
> **Authors**: Bhawna Piryani,Jamshid Mozafari,Abdelrahman Abdallah,Antoine Doucet,Adam Jatowt
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Optical Character Recognition (OCR) plays a crucial role in digitizing historical and multilingual documents, yet OCR errors -- imperfect extraction of the text, including character insertion, deletion and permutation -- can significantly impact downstream tasks like question-answering (QA). In this work, we introduce a multilingual QA dataset MultiOCR-QA, designed to analyze the effects of OCR noise on QA systems' performance. The MultiOCR-QA dataset comprises 60K question-answer pairs covering three languages, English, French, and German. The dataset is curated from OCR-ed old documents, allowing for the evaluation of OCR-induced challenges on question answering. We evaluate MultiOCR-QA on various levels and types of OCR errors to access the robustness of LLMs in handling real-world digitization errors. Our findings show that QA systems are highly prone to OCR induced errors and exhibit performance degradation on noisy OCR text.

### AISafetyLab: A Comprehensive Framework for AI Safety Evaluation and Improvement 
[[arxiv](https://arxiv.org/abs/2502.16776)] [[cool](https://papers.cool/arxiv/2502.16776)] [[pdf](https://arxiv.org/pdf/2502.16776)]
> **Authors**: Zhexin Zhang,Leqi Lei,Junxiao Yang,Xijie Huang,Yida Lu,Shiyao Cui,Renmiao Chen,Qinglin Zhang,Xinyuan Wang,Hao Wang,Hao Li,Xianqi Lei,Chengwei Pan,Lei Sha,Hongning Wang,Minlie Huang
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: 13 pages
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: As AI models are increasingly deployed across diverse real-world scenarios, ensuring their safety remains a critical yet underexplored challenge. While substantial efforts have been made to evaluate and enhance AI safety, the lack of a standardized framework and comprehensive toolkit poses significant obstacles to systematic research and practical adoption. To bridge this gap, we introduce AISafetyLab, a unified framework and toolkit that integrates representative attack, defense, and evaluation methodologies for AI safety. AISafetyLab features an intuitive interface that enables developers to seamlessly apply various techniques while maintaining a well-structured and extensible codebase for future advancements. Additionally, we conduct empirical studies on Vicuna, analyzing different attack and defense strategies to provide valuable insights into their comparative effectiveness. To facilitate ongoing research and development in AI safety, AISafetyLab is publicly available at https://github.com/thu-coai/AISafetyLab, and we are committed to its continuous maintenance and improvement.

### LED-Merging: Mitigating Safety-Utility Conflicts in Model Merging with Location-Election-Disjoint 
[[arxiv](https://arxiv.org/abs/2502.16770)] [[cool](https://papers.cool/arxiv/2502.16770)] [[pdf](https://arxiv.org/pdf/2502.16770)]
> **Authors**: Qianli Ma,Dongrui Liu,Qian Chen,Linfeng Zhang,Jing Shao
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Fine-tuning pre-trained Large Language Models (LLMs) for specialized tasks incurs substantial computational and data costs. While model merging offers a training-free solution to integrate multiple task-specific models, existing methods suffer from safety-utility conflicts where enhanced general capabilities degrade safety safeguards. We identify two root causes: \textbf{neuron misidentification} due to simplistic parameter magnitude-based selection, and \textbf{cross-task neuron interference} during merging. To address these challenges, we propose \textbf{LED-Merging}, a three-stage framework that \textbf{L}ocates task-specific neurons via gradient-based attribution, dynamically \textbf{E}lects critical neurons through multi-model importance fusion, and \textbf{D}isjoints conflicting updates through parameter isolation. Extensive experiments on Llama-3-8B, Mistral-7B, and Llama2-13B demonstrate that LED-Merging reduces harmful response rates(\emph{e.g.}, a 31.4\% decrease on Llama-3-8B-Instruct on HarmBench) while preserving 95\% of utility performance(\emph{e.g.}, 52.39\% accuracy on GSM8K). LED-Merging resolves safety-utility conflicts and provides a lightweight, training-free paradigm for constructing reliable multi-task LLMs.

### A Hybrid Approach to Information Retrieval and Answer Generation for Regulatory Texts 
[[arxiv](https://arxiv.org/abs/2502.16767)] [[cool](https://papers.cool/arxiv/2502.16767)] [[pdf](https://arxiv.org/pdf/2502.16767)]
> **Authors**: Jhon Rayo,Raul de la Rosa,Mario Garrido
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: 5 pages; Workshop paper; Proceedings of the 1st Regulatory NLP Workshop (RegNLP 2025)
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Regulatory texts are inherently long and complex, presenting significant challenges for information retrieval systems in supporting regulatory officers with compliance tasks. This paper introduces a hybrid information retrieval system that combines lexical and semantic search techniques to extract relevant information from large regulatory corpora. The system integrates a fine-tuned sentence transformer model with the traditional BM25 algorithm to achieve both semantic precision and lexical coverage. To generate accurate and comprehensive responses, retrieved passages are synthesized using Large Language Models (LLMs) within a Retrieval Augmented Generation (RAG) framework. Experimental results demonstrate that the hybrid system significantly outperforms standalone lexical and semantic approaches, with notable improvements in Recall@10 and MAP@10. By openly sharing our fine-tuned model and methodology, we aim to advance the development of robust natural language processing tools for compliance-driven applications in regulatory domains.

### Language Model Fine-Tuning on Scaled Survey Data for Predicting Distributions of Public Opinions 
[[arxiv](https://arxiv.org/abs/2502.16761)] [[cool](https://papers.cool/arxiv/2502.16761)] [[pdf](https://arxiv.org/pdf/2502.16761)]
> **Authors**: Joseph Suh,Erfan Jahanparast,Suhong Moon,Minwoo Kang,Serina Chang
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) present novel opportunities in public opinion research by predicting survey responses in advance during the early stages of survey design. Prior methods steer LLMs via descriptions of subpopulations as LLMs' input prompt, yet such prompt engineering approaches have struggled to faithfully predict the distribution of survey responses from human subjects. In this work, we propose directly fine-tuning LLMs to predict response distributions by leveraging unique structural characteristics of survey data. To enable fine-tuning, we curate SubPOP, a significantly scaled dataset of 3,362 questions and 70K subpopulation-response pairs from well-established public opinion surveys. We show that fine-tuning on SubPOP greatly improves the match between LLM predictions and human responses across various subpopulations, reducing the LLM-human gap by up to 46% compared to baselines, and achieves strong generalization to unseen surveys and subpopulations. Our findings highlight the potential of survey-based fine-tuning to improve opinion prediction for diverse, real-world subpopulations and therefore enable more efficient survey designs. Our code is available at https://github.com/JosephJeesungSuh/subpop.

### Entailment-Preserving First-order Logic Representations in Natural Language Entailment 
[[arxiv](https://arxiv.org/abs/2502.16757)] [[cool](https://papers.cool/arxiv/2502.16757)] [[pdf](https://arxiv.org/pdf/2502.16757)]
> **Authors**: Jinu Lee,Qi Liu,Runzhi Ma,Vincent Han,Ziqi Wang,Heng Ji,Julia Hockenmaier
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: 14 pages (8 pages of main content), 8 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: First-order logic (FOL) can represent the logical entailment semantics of natural language (NL) sentences, but determining natural language entailment using FOL remains a challenge. To address this, we propose the Entailment-Preserving FOL representations (EPF) task and introduce reference-free evaluation metrics for EPF, the Entailment-Preserving Rate (EPR) family. In EPF, one should generate FOL representations from multi-premise natural language entailment data (e.g. EntailmentBank) so that the automatic prover's result preserves the entailment labels. Experiments show that existing methods for NL-to-FOL translation struggle in EPF. To this extent, we propose a training method specialized for the task, iterative learning-to-rank, which directly optimizes the model's EPR score through a novel scoring function and a learning-to-rank objective. Our method achieves a 1.8-2.7% improvement in EPR and a 17.4-20.6% increase in EPR@16 compared to diverse baselines in three datasets. Further analyses reveal that iterative learning-to-rank effectively suppresses the arbitrariness of FOL representation by reducing the diversity of predicate signatures, and maintains strong performance across diverse inference types and out-of-domain data.

### SQLong: Enhanced NL2SQL for Longer Contexts with LLMs 
[[arxiv](https://arxiv.org/abs/2502.16747)] [[cool](https://papers.cool/arxiv/2502.16747)] [[pdf](https://arxiv.org/pdf/2502.16747)]
> **Authors**: Dai Quoc Nguyen,Cong Duy Vu Hoang,Duy Vu,Gioacchino Tangari,Thanh Tien Vu,Don Dharmasiri,Yuan-Fang Li,Long Duong
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习,软件工程
- **Abstract**: Open-weight large language models (LLMs) have significantly advanced performance in the Natural Language to SQL (NL2SQL) task. However, their effectiveness diminishes when dealing with large database schemas, as the context length increases. To address this limitation, we present SQLong, a novel and efficient data augmentation framework designed to enhance LLM performance in long-context scenarios for the NL2SQL task. SQLong generates augmented datasets by extending existing database schemas with additional synthetic CREATE TABLE commands and corresponding data rows, sampled from diverse schemas in the training data. This approach effectively simulates long-context scenarios during finetuning and evaluation. Through experiments on the Spider and BIRD datasets, we demonstrate that LLMs finetuned with SQLong-augmented data significantly outperform those trained on standard datasets. These imply SQLong's practical implementation and its impact on improving NL2SQL capabilities in real-world settings with complex database schemas.

### Layer-Wise Evolution of Representations in Fine-Tuned Transformers: Insights from Sparse AutoEncoders 
[[arxiv](https://arxiv.org/abs/2502.16722)] [[cool](https://papers.cool/arxiv/2502.16722)] [[pdf](https://arxiv.org/pdf/2502.16722)]
> **Authors**: Suneel Nadipalli
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: 14 pages, 5 figures
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Fine-tuning pre-trained transformers is a powerful technique for enhancing the performance of base models on specific tasks. From early applications in models like BERT to fine-tuning Large Language Models (LLMs), this approach has been instrumental in adapting general-purpose architectures for specialized downstream tasks. Understanding the fine-tuning process is crucial for uncovering how transformers adapt to specific objectives, retain general representations, and acquire task-specific features. This paper explores the underlying mechanisms of fine-tuning, specifically in the BERT transformer, by analyzing activation similarity, training Sparse AutoEncoders (SAEs), and visualizing token-level activations across different layers. Based on experiments conducted across multiple datasets and BERT layers, we observe a steady progression in how features adapt to the task at hand: early layers primarily retain general representations, middle layers act as a transition between general and task-specific features, and later layers fully specialize in task adaptation. These findings provide key insights into the inner workings of fine-tuning and its impact on representation learning within transformer architectures.

### Speed and Conversational Large Language Models: Not All Is About Tokens per Second 
[[arxiv](https://arxiv.org/abs/2502.16721)] [[cool](https://papers.cool/arxiv/2502.16721)] [[pdf](https://arxiv.org/pdf/2502.16721)]
> **Authors**: Javier Conde,Miguel González,Pedro Reviriego,Zhen Gao,Shanshan Liu,Fabrizio Lombardi
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: ef:Computer (Volume: 57, Issue: 8, August 2024)
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: The speed of open-weights large language models (LLMs) and its dependency on the task at hand, when run on GPUs, is studied to present a comparative analysis of the speed of the most popular open LLMs.

### Beyond Pattern Recognition: Probing Mental Representations of LMs 
[[arxiv](https://arxiv.org/abs/2502.16717)] [[cool](https://papers.cool/arxiv/2502.16717)] [[pdf](https://arxiv.org/pdf/2502.16717)]
> **Authors**: Moritz Miller,Kumar Shridhar
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Language Models (LMs) have demonstrated impressive capabilities in solving complex reasoning tasks, particularly when prompted to generate intermediate explanations. However, it remains an open question whether these intermediate reasoning traces represent a dynamic, evolving thought process or merely reflect sophisticated pattern recognition acquired during large scale pre training. Drawing inspiration from human cognition, where reasoning unfolds incrementally as new information is assimilated and internal models are continuously updated, we propose to delve deeper into the mental model of various LMs. We propose a new way to assess the mental modeling of LMs, where they are provided with problem details gradually, allowing each new piece of data to build upon and refine the model's internal representation of the task. We systematically compare this step by step mental modeling strategy with traditional full prompt methods across both text only and vision and text modalities. Experiments on the MathWorld dataset across different model sizes and problem complexities confirm that both text-based LLMs and multimodal LMs struggle to create mental representations, questioning how their internal cognitive processes work.

### Can ChatGPT Learn to Count Letters? 
[[arxiv](https://arxiv.org/abs/2502.16705)] [[cool](https://papers.cool/arxiv/2502.16705)] [[pdf](https://arxiv.org/pdf/2502.16705)]
> **Authors**: Javier Conde,Gonzalo Martínez,Pedro Reviriego,Zhen Gao,Shanshan Liu,Fabrizio Lombardi
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: ef:Computer (Volume: 58, Issue: 3, March 2025)
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large language models (LLMs) struggle on simple tasks such as counting the number of occurrences of a letter in a word. In this paper, we investigate if ChatGPT can learn to count letters and propose an efficient solution.

### Code Summarization Beyond Function Level 
[[arxiv](https://arxiv.org/abs/2502.16704)] [[cool](https://papers.cool/arxiv/2502.16704)] [[pdf](https://arxiv.org/pdf/2502.16704)]
> **Authors**: Vladimir Makharev,Vladimir Ivanov
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: Accepted to LLM4Code @ ICSE'25; 8 pages, 3 figures, 4 tables
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Code summarization is a critical task in natural language processing and software engineering, which aims to generate concise descriptions of source code. Recent advancements have improved the quality of these summaries, enhancing code readability and maintainability. However, the content of a repository or a class has not been considered in function code summarization. This study investigated the effectiveness of code summarization models beyond the function level, exploring the impact of class and repository contexts on the summary quality. The study involved revising benchmarks for evaluating models at class and repository levels, assessing baseline models, and evaluating LLMs with in-context learning to determine the enhancement of summary quality with additional context. The findings revealed that the fine-tuned state-of-the-art CodeT5+ base model excelled in code summarization, while incorporating few-shot learning and retrieved code chunks from RAG significantly enhanced the performance of LLMs in this task. Notably, the Deepseek Coder 1.3B and Starcoder2 15B models demonstrated substantial improvements in metrics such as BLEURT, METEOR, and BLEU-4 at both class and repository levels. Repository-level summarization exhibited promising potential but necessitates significant computational resources and gains from the inclusion of structured context. Lastly, we employed the recent SIDE code summarization metric in our evaluation. This study contributes to refining strategies for prompt engineering, few-shot learning, and RAG, addressing gaps in benchmarks for code summarization at various levels. Finally, we publish all study details, code, datasets, and results of evaluation in the GitHub repository available at https://github.com/kilimanj4r0/code-summarization-beyond-function-level.

### Uncovering the Hidden Threat of Text Watermarking from Users with Cross-Lingual Knowledge 
[[arxiv](https://arxiv.org/abs/2502.16699)] [[cool](https://papers.cool/arxiv/2502.16699)] [[pdf](https://arxiv.org/pdf/2502.16699)]
> **Authors**: Mansour Al Ghanim,Jiaqi Xue,Rochana Prih Hastuti,Mengxin Zheng,Yan Solihin,Qian Lou
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: 9 pages
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: In this study, we delve into the hidden threats posed to text watermarking by users with cross-lingual knowledge. While most research focuses on watermarking methods for English, there is a significant gap in evaluating these methods in cross-lingual contexts. This oversight neglects critical adversary scenarios involving cross-lingual users, creating uncertainty regarding the effectiveness of cross-lingual watermarking. We assess four watermarking techniques across four linguistically rich languages, examining watermark resilience and text quality across various parameters and attacks. Our focus is on a realistic scenario featuring adversaries with cross-lingual expertise, evaluating the adequacy of current watermarking methods against such challenges.

### Toward Responsible Federated Large Language Models: Leveraging a Safety Filter and Constitutional AI 
[[arxiv](https://arxiv.org/abs/2502.16691)] [[cool](https://papers.cool/arxiv/2502.16691)] [[pdf](https://arxiv.org/pdf/2502.16691)]
> **Authors**: Eunchung Noh,Jeonghun Baek
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: 5 pages, 3 figures
- **标题**: None
- **领域**: 计算语言学,分布式、并行和集群计算,多代理系统
- **Abstract**: Recent research has increasingly focused on training large language models (LLMs) using federated learning, known as FedLLM. However, responsible AI (RAI), which aims to ensure safe responses, remains underexplored in the context of FedLLM. In FedLLM, client data used for training may contain harmful content, leading to unsafe LLMs that generate harmful responses. Aggregating such unsafe LLMs into the global model and distributing them to clients may result in the widespread deployment of unsafe LLMs. To address this issue, we incorporate two well-known RAI methods into FedLLM: the safety filter and constitutional AI. Our experiments demonstrate that these methods significantly enhance the safety of the LLM, achieving over a 20% improvement on AdvBench, a benchmark for evaluating safety performance.

### WildLong: Synthesizing Realistic Long-Context Instruction Data at Scale 
[[arxiv](https://arxiv.org/abs/2502.16684)] [[cool](https://papers.cool/arxiv/2502.16684)] [[pdf](https://arxiv.org/pdf/2502.16684)]
> **Authors**: Jiaxi Li,Xingxing Zhang,Xun Wang,Xiaolong Huang,Li Dong,Liang Wang,Si-Qing Chen,Wei Lu,Furu Wei
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) with extended context windows enable tasks requiring extensive information integration but are limited by the scarcity of high-quality, diverse datasets for long-context instruction tuning. Existing data synthesis methods focus narrowly on objectives like fact retrieval and summarization, restricting their generalizability to complex, real-world tasks. WildLong extracts meta-information from real user queries, models co-occurrence relationships via graph-based methods, and employs adaptive generation to produce scalable data. It extends beyond single-document tasks to support multi-document reasoning, such as cross-document comparison and aggregation. Our models, finetuned on 150K instruction-response pairs synthesized using WildLong, surpasses existing open-source long-context-optimized models across benchmarks while maintaining strong performance on short-context tasks without incorporating supplementary short-context data. By generating a more diverse and realistic long-context instruction dataset, WildLong enhances LLMs' ability to generalize to complex, real-world reasoning over long contexts, establishing a new paradigm for long-context data synthesis.

### Automatic Input Rewriting Improves Translation with Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.16682)] [[cool](https://papers.cool/arxiv/2502.16682)] [[pdf](https://arxiv.org/pdf/2502.16682)]
> **Authors**: Dayeon Ki,Marine Carpuat
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: 27 pages, 8 figures
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Can we improve machine translation (MT) with LLMs by rewriting their inputs automatically? Users commonly rely on the intuition that well-written text is easier to translate when using off-the-shelf MT systems. LLMs can rewrite text in many ways but in the context of MT, these capabilities have been primarily exploited to rewrite outputs via post-editing. We present an empirical study of 21 input rewriting methods with 3 open-weight LLMs for translating from English into 6 target languages. We show that text simplification is the most effective MT-agnostic rewrite strategy and that it can be improved further when using quality estimation to assess translatability. Human evaluation further confirms that simplified rewrites and their MT outputs both largely preserve the original meaning of the source and MT. These results suggest LLM-assisted input rewriting as a promising direction for improving translations.

### MimeQA: Towards Socially-Intelligent Nonverbal Foundation Models 
[[arxiv](https://arxiv.org/abs/2502.16671)] [[cool](https://papers.cool/arxiv/2502.16671)] [[pdf](https://arxiv.org/pdf/2502.16671)]
> **Authors**: Hengzhi Li,Megan Tjandrasuwita,Yi R. Fung,Armando Solar-Lezama,Paul Pu Liang
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别
- **Abstract**: Socially intelligent AI that can understand and interact seamlessly with humans in daily lives is increasingly important as AI becomes more closely integrated with peoples' daily activities. However, current works in artificial social reasoning all rely on language-only, or language-dominant approaches to benchmark and training models, resulting in systems that are improving in verbal communication but struggle with nonverbal social understanding. To address this limitation, we tap into a novel source of data rich in nonverbal and social interactions -- mime videos. Mimes refer to the art of expression through gesture and movement without spoken words, which presents unique challenges and opportunities in interpreting non-verbal social communication. We contribute a new dataset called MimeQA, obtained by sourcing 221 videos from YouTube, through rigorous annotation and verification, resulting in a benchmark with 101 videos and 806 question-answer pairs. Using MimeQA, we evaluate state-of-the-art video large language models (vLLMs) and find that their overall accuracy ranges from 15-30%. Our analysis reveals that vLLMs often fail to ground imagined objects and over-rely on the text prompt while ignoring subtle nonverbal interactions. Our data resources are released at https://github.com/MIT-MI/MimeQA to inspire future work in foundation models that embody true social intelligence capable of interpreting non-verbal human interactions.

### CODESYNC: Synchronizing Large Language Models with Dynamic Code Evolution at Scale 
[[arxiv](https://arxiv.org/abs/2502.16645)] [[cool](https://papers.cool/arxiv/2502.16645)] [[pdf](https://arxiv.org/pdf/2502.16645)]
> **Authors**: Chenlong Wang,Zhaoyang Chu,Zhengxiang Cheng,Xuyi Yang,Kaiyue Qiu,Yao Wan,Zhou Zhao,Xuanhua Shi,Dongping Chen
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,软件工程
- **Abstract**: Large Language Models (LLMs) have exhibited exceptional performance in software engineering yet face challenges in adapting to continually evolving code knowledge, particularly regarding the frequent updates of third-party library APIs. This limitation, stemming from static pre-training datasets, often results in non-executable code or implementations with suboptimal safety and efficiency. To this end, this paper introduces CODESYNC, a data engine for identifying outdated code patterns and collecting real-time code knowledge updates from Python third-party libraries. Building upon CODESYNC, we develop CODESYNCBENCH, a comprehensive benchmark for assessing LLMs' ability to stay synchronized with code evolution, which covers real-world updates for 220 APIs from six Python libraries. Our benchmark offers 3,300 test cases across three evaluation tasks and an update-aware instruction tuning dataset consisting of 2,200 training samples. Extensive experiments on 14 state-of-the-art LLMs reveal that they struggle with dynamic code evolution, even with the support of advanced knowledge updating methods (e.g., DPO, ORPO, and SimPO). We believe that our benchmark can offer a strong foundation for the development of more effective methods for real-time code knowledge updating in the future. The experimental code and dataset are publicly available at: https://github.com/Lucky-voyage/Code-Sync.

### Visual-RAG: Benchmarking Text-to-Image Retrieval Augmented Generation for Visual Knowledge Intensive Queries 
[[arxiv](https://arxiv.org/abs/2502.16636)] [[cool](https://papers.cool/arxiv/2502.16636)] [[pdf](https://arxiv.org/pdf/2502.16636)]
> **Authors**: Yin Wu,Quanyu Long,Jing Li,Jianfei Yu,Wenya Wang
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: 23 pages, 6 figures
- **标题**: None
- **领域**: 计算语言学,计算机视觉和模式识别
- **Abstract**: Retrieval-Augmented Generation (RAG) is a popular approach for enhancing Large Language Models (LLMs) by addressing their limitations in verifying facts and answering knowledge-intensive questions. As the research in LLM extends their capability to handle input modality other than text, e.g. image, several multimodal RAG benchmarks are proposed. Nonetheless, they mainly use textual knowledge bases as the primary source of evidences for augmentation. There still lack benchmarks designed to evaluate images as augmentation in RAG systems and how they leverage visual knowledge. We propose Visual-RAG, a novel Question Answering benchmark that emphasizes visual knowledge intensive questions. Unlike prior works relying on text-based evidence, Visual-RAG necessitates text-to-image retrieval and integration of relevant clue images to extract visual knowledge as evidence. With Visual-RAG, we evaluate 5 open-sourced and 3 proprietary Multimodal LLMs (MLLMs), revealing that images can serve as good evidence in RAG; however, even the SoTA models struggle with effectively extracting and utilizing visual knowledge

### CodeCriticBench: A Holistic Code Critique Benchmark for Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.16614)] [[cool](https://papers.cool/arxiv/2502.16614)] [[pdf](https://arxiv.org/pdf/2502.16614)]
> **Authors**: Alexander Zhang,Marcus Dong,Jiaheng Liu,Wei Zhang,Yejie Wang,Jian Yang,Ge Zhang,Tianyu Liu,Zhongyuan Peng,Yingshui Tan,Yuanxing Zhang,Zhexu Wang,Weixun Wang,Yancheng He,Ken Deng,Wangchunshu Zhou,Wenhao Huang,Zhaoxiang Zhang
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The critique capacity of Large Language Models (LLMs) is essential for reasoning abilities, which can provide necessary suggestions (e.g., detailed analysis and constructive feedback). Therefore, how to evaluate the critique capacity of LLMs has drawn great attention and several critique benchmarks have been proposed. However, existing critique benchmarks usually have the following limitations: (1). Focusing on diverse reasoning tasks in general domains and insufficient evaluation on code tasks (e.g., only covering code generation task), where the difficulty of queries is relatively easy (e.g., the code queries of CriticBench are from Humaneval and MBPP). (2). Lacking comprehensive evaluation from different dimensions. To address these limitations, we introduce a holistic code critique benchmark for LLMs called CodeCriticBench. Specifically, our CodeCriticBench includes two mainstream code tasks (i.e., code generation and code QA) with different difficulties. Besides, the evaluation protocols include basic critique evaluation and advanced critique evaluation for different characteristics, where fine-grained evaluation checklists are well-designed for advanced settings. Finally, we conduct extensive experimental results of existing LLMs, which show the effectiveness of CodeCriticBench.

### MemeIntel: Explainable Detection of Propagandistic and Hateful Memes 
[[arxiv](https://arxiv.org/abs/2502.16612)] [[cool](https://papers.cool/arxiv/2502.16612)] [[pdf](https://arxiv.org/pdf/2502.16612)]
> **Authors**: Mohamed Bayan Kmainasi,Abul Hasnat,Md Arid Hasan,Ali Ezzat Shahroor,Firoj Alam
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: disinformation, misinformation, factuality, harmfulness, fake news, propaganda, hateful meme, multimodality, text, images
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: The proliferation of multimodal content on social media presents significant challenges in understanding and moderating complex, context-dependent issues such as misinformation, hate speech, and propaganda. While efforts have been made to develop resources and propose new methods for automatic detection, limited attention has been given to label detection and the generation of explanation-based rationales for predicted labels. To address this challenge, we introduce MemeIntel, an explanation-enhanced dataset for propaganda memes in Arabic and hateful memes in English, making it the first large-scale resource for these tasks. To solve these tasks, we propose a multi-stage optimization approach and train Vision-Language Models (VLMs). Our results demonstrate that this approach significantly improves performance over the base model for both \textbf{label detection} and explanation generation, outperforming the current state-of-the-art with an absolute improvement of ~3% on ArMeme and ~7% on Hateful Memes. For reproducibility and future research, we aim to make the MemeIntel dataset and experimental resources publicly available.

### Reasoning about Affordances: Causal and Compositional Reasoning in LLMs 
[[arxiv](https://arxiv.org/abs/2502.16606)] [[cool](https://papers.cool/arxiv/2502.16606)] [[pdf](https://arxiv.org/pdf/2502.16606)]
> **Authors**: Magnus F. Gjerde,Vanessa Cheung,David Lagnado
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: 21 pages, 7 figures, 3 tables
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: With the rapid progress of Large Language Models (LLMs), it becomes increasingly important to understand their abilities and limitations. In two experiments, we investigate the causal and compositional reasoning abilities of LLMs and humans in the domain of object affordances, an area traditionally linked to embodied cognition. The tasks, designed from scratch to avoid data contamination, require decision-makers to select unconventional objects to replace a typical tool for a particular purpose, such as using a table tennis racket to dig a hole. In Experiment 1, we evaluated GPT-3.5 and GPT-4o, finding that GPT-4o, when given chain-of-thought prompting, performed on par with human participants, while GPT-3.5 lagged significantly. In Experiment 2, we introduced two new conditions, Distractor (more object choices, increasing difficulty) and Image (object options presented visually), and evaluated Claude 3 Sonnet and Claude 3.5 Sonnet in addition to the GPT models. The Distractor condition significantly impaired performance across humans and models, although GPT-4o and Claude 3.5 still performed well above chance. Surprisingly, the Image condition had little impact on humans or GPT-4o, but significantly lowered Claude 3.5's accuracy. Qualitative analysis showed that GPT-4o and Claude 3.5 have a stronger ability than their predecessors to identify and flexibly apply causally relevant object properties. The improvement from GPT-3.5 and Claude 3 to GPT-4o and Claude 3.5 suggests that models are increasingly capable of causal and compositional reasoning in some domains, although further mechanistic research is necessary to understand how LLMs reason.

### Revealing the Pragmatic Dilemma for Moral Reasoning Acquisition in Language Models 
[[arxiv](https://arxiv.org/abs/2502.16600)] [[cool](https://papers.cool/arxiv/2502.16600)] [[pdf](https://arxiv.org/pdf/2502.16600)]
> **Authors**: Guangliang Liu,Lei Jiang,Xitong Zhang,Kristen Marie Johnson
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Ensuring that Large Language Models (LLMs) return just responses which adhere to societal values is crucial for their broader application. Prior research has shown that LLMs often fail to perform satisfactorily on tasks requiring moral cognizance, such as ethics-based judgments. While current approaches have focused on fine-tuning LLMs with curated datasets to improve their capabilities on such tasks, choosing the optimal learning paradigm to enhance the ethical responses of LLMs remains an open research debate. In this work, we aim to address this fundamental question: can current learning paradigms enable LLMs to acquire sufficient moral reasoning capabilities? Drawing from distributional semantics theory and the pragmatic nature of moral discourse, our analysis indicates that performance improvements follow a mechanism similar to that of semantic-level tasks, and therefore remain affected by the pragmatic nature of morals latent in discourse, a phenomenon we name the pragmatic dilemma. We conclude that this pragmatic dilemma imposes significant limitations on the generalization ability of current learning paradigms, making it the primary bottleneck for moral reasoning acquisition in LLMs.

### Beyond Words: How Large Language Models Perform in Quantitative Management Problem-Solving 
[[arxiv](https://arxiv.org/abs/2502.16556)] [[cool](https://papers.cool/arxiv/2502.16556)] [[pdf](https://arxiv.org/pdf/2502.16556)]
> **Authors**: Jonathan Kuzmanko
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: 28 pages, 5 figures, 19 tables
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习,应用领域
- **Abstract**: This study examines how Large Language Models (LLMs) perform when tackling quantitative management decision problems in a zero-shot setting. Drawing on 900 responses generated by five leading models across 20 diverse managerial scenarios, our analysis explores whether these base models can deliver accurate numerical decisions under varying presentation formats, scenario complexities, and repeated attempts. Contrary to prior findings, we observed no significant effects of text presentation format (direct, narrative, or tabular) or text length on accuracy. However, scenario complexity -- particularly in terms of constraints and irrelevant parameters -- strongly influenced performance, often degrading accuracy. Surprisingly, the models handled tasks requiring multiple solution steps more effectively than expected. Notably, only 28.8\% of responses were exactly correct, highlighting limitations in precision. We further found no significant ``learning effect'' across iterations: performance remained stable across repeated queries. Nonetheless, significant variations emerged among the five tested LLMs, with some showing superior binary accuracy. Overall, these findings underscore both the promise and the pitfalls of harnessing LLMs for complex quantitative decision-making, informing managers and researchers about optimal deployment strategies.

### Reasoning About Persuasion: Can LLMs Enable Explainable Propaganda Detection? 
[[arxiv](https://arxiv.org/abs/2502.16550)] [[cool](https://papers.cool/arxiv/2502.16550)] [[pdf](https://arxiv.org/pdf/2502.16550)]
> **Authors**: Maram Hasanain,Md Arid Hasan,Mohamed Bayan Kmainasi,Elisa Sartori,Ali Ezzat Shahroor,Giovanni Da San Martino,Firoj Alam
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: There has been significant research on propagandistic content detection across different modalities and languages. However, most studies have primarily focused on detection, with little attention given to explanations justifying the predicted label. This is largely due to the lack of resources that provide explanations alongside annotated labels. To address this issue, we propose a multilingual (i.e., Arabic and English) explanation-enhanced dataset, the first of its kind. Additionally, we introduce an explanation-enhanced LLM for both label detection and rationale-based explanation generation. Our findings indicate that the model performs comparably while also generating explanations. We will make the dataset and experimental resources publicly available for the research community.

### Advanced Chain-of-Thought Reasoning for Parameter Extraction from Documents Using Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.16540)] [[cool](https://papers.cool/arxiv/2502.16540)] [[pdf](https://arxiv.org/pdf/2502.16540)]
> **Authors**: Hong Cai Chen,Yi Pin Xu,Yang Zhang
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: 9 pages, 9 figures
- **标题**: None
- **领域**: 计算语言学,人工智能,硬件架构,信息检索,机器学习
- **Abstract**: Extracting parameters from technical documentation is crucial for ensuring design precision and simulation reliability in electronic design. However, current methods struggle to handle high-dimensional design data and meet the demands of real-time processing. In electronic design automation (EDA), engineers often manually search through extensive documents to retrieve component parameters required for constructing PySpice models, a process that is both labor-intensive and time-consuming. To address this challenge, we propose an innovative framework that leverages large language models (LLMs) to automate the extraction of parameters and the generation of PySpice models directly from datasheets. Our framework introduces three Chain-of-Thought (CoT) based techniques: (1) Targeted Document Retrieval (TDR), which enables the rapid identification of relevant technical sections; (2) Iterative Retrieval Optimization (IRO), which refines the parameter search through iterative improvements; and (3) Preference Optimization (PO), which dynamically prioritizes key document sections based on relevance. Experimental results show that applying all three methods together improves retrieval precision by 47.69% and reduces processing latency by 37.84%. Furthermore, effect size analysis using Cohen's d reveals that PO significantly reduces latency, while IRO contributes most to precision enhancement. These findings underscore the potential of our framework to streamline EDA processes, enhance design accuracy, and shorten development timelines. Additionally, our algorithm has model-agnostic generalization, meaning it can improve parameter search performance across different LLMs.

### Multilingual != Multicultural: Evaluating Gaps Between Multilingual Capabilities and Cultural Alignment in LLMs 
[[arxiv](https://arxiv.org/abs/2502.16534)] [[cool](https://papers.cool/arxiv/2502.16534)] [[pdf](https://arxiv.org/pdf/2502.16534)]
> **Authors**: Jonathan Rystrøm,Hannah Rose Kirk,Scott Hale
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机与社会
- **Abstract**: Large Language Models (LLMs) are becoming increasingly capable across global languages. However, the ability to communicate across languages does not necessarily translate to appropriate cultural representations. A key concern is US-centric bias, where LLMs reflect US rather than local cultural values. We propose a novel methodology that compares LLM-generated response distributions against population-level opinion data from the World Value Survey across four languages (Danish, Dutch, English, and Portuguese). Using a rigorous linear mixed-effects regression framework, we compare two families of models: Google's Gemma models (2B--27B parameters) and successive iterations of OpenAI's turbo-series. Across the families of models, we find no consistent relationships between language capabilities and cultural alignment. While the Gemma models have a positive correlation between language capability and cultural alignment across languages, the OpenAI models do not. Importantly, we find that self-consistency is a stronger predictor of multicultural alignment than multilingual capabilities. Our results demonstrate that achieving meaningful cultural alignment requires dedicated effort beyond improving general language capabilities.

### Retrieval-Augmented Fine-Tuning With Preference Optimization For Visual Program Generation 
[[arxiv](https://arxiv.org/abs/2502.16529)] [[cool](https://papers.cool/arxiv/2502.16529)] [[pdf](https://arxiv.org/pdf/2502.16529)]
> **Authors**: Deokhyung Kang,Jeonghun Cho,Yejin Jeon,Sunbin Jang,Minsub Lee,Jawoon Cho,Gary Geunbae Lee
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Visual programming languages (VPLs) allow users to create programs through graphical interfaces, which results in easier accessibility and their widespread usage in various domains. To further enhance this accessibility, recent research has focused on generating VPL code from user instructions using large language models (LLMs). Specifically, by employing prompting-based methods, these studies have shown promising results. Nevertheless, such approaches can be less effective for industrial VPLs such as Ladder Diagram (LD). LD is a pivotal language used in industrial automation processes and involves extensive domain-specific configurations, which are difficult to capture in a single prompt. In this work, we demonstrate that training-based methods outperform prompting-based methods for LD generation accuracy, even with smaller backbone models. Building on these findings, we propose a two-stage training strategy to further enhance VPL generation. First, we employ retrieval-augmented fine-tuning to leverage the repetitive use of subroutines commonly seen in industrial VPLs. Second, we apply direct preference optimization (DPO) to further guide the model toward accurate outputs, using systematically generated preference pairs through graph editing operations. Extensive experiments on real-world LD data demonstrate that our approach improves program-level accuracy by over 10% compared to supervised fine-tuning, which highlights its potential to advance industrial automation.

### Pay Attention to Real World Perturbations! Natural Robustness Evaluation in Machine Reading Comprehension 
[[arxiv](https://arxiv.org/abs/2502.16523)] [[cool](https://papers.cool/arxiv/2502.16523)] [[pdf](https://arxiv.org/pdf/2502.16523)]
> **Authors**: Yulong Wu,Viktor Schlegel,Riza Batista-Navarro
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: As neural language models achieve human-comparable performance on Machine Reading Comprehension (MRC) and see widespread adoption, ensuring their robustness in real-world scenarios has become increasingly important. Current robustness evaluation research, though, primarily develops synthetic perturbation methods, leaving unclear how well they reflect real life scenarios. Considering this, we present a framework to automatically examine MRC models on naturally occurring textual perturbations, by replacing paragraph in MRC benchmarks with their counterparts based on available Wikipedia edit history. Such perturbation type is natural as its design does not stem from an arteficial generative process, inherently distinct from the previously investigated synthetic approaches. In a large-scale study encompassing SQUAD datasets and various model architectures we observe that natural perturbations result in performance degradation in pre-trained encoder language models. More worryingly, these state-of-the-art Flan-T5 and Large Language Models (LLMs) inherit these errors. Further experiments demonstrate that our findings generalise to natural perturbations found in other more challenging MRC benchmarks. In an effort to mitigate these errors, we show that it is possible to improve the robustness to natural perturbations by training on naturally or synthetically perturbed examples, though a noticeable gap still remains compared to performance on unperturbed data.

### GraphCheck: Breaking Long-Term Text Barriers with Extracted Knowledge Graph-Powered Fact-Checking 
[[arxiv](https://arxiv.org/abs/2502.16514)] [[cool](https://papers.cool/arxiv/2502.16514)] [[pdf](https://arxiv.org/pdf/2502.16514)]
> **Authors**: Yingjian Chen,Haoran Liu,Yinhong Liu,Rui Yang,Han Yuan,Yanran Fu,Pengyuan Zhou,Qingyu Chen,James Caverlee,Irene Li
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) are widely used, but they often generate subtle factual errors, especially in long-form text. These errors are fatal in some specialized domains such as medicine. Existing fact-checking with grounding documents methods face two main challenges: (1) they struggle to understand complex multihop relations in long documents, often overlooking subtle factual errors; (2) most specialized methods rely on pairwise comparisons, requiring multiple model calls, leading to high resource and computational costs. To address these challenges, we propose \textbf{\textit{GraphCheck}}, a fact-checking framework that uses extracted knowledge graphs to enhance text representation. Graph Neural Networks further process these graphs as a soft prompt, enabling LLMs to incorporate structured knowledge more effectively. Enhanced with graph-based reasoning, GraphCheck captures multihop reasoning chains which are often overlooked by existing methods, enabling precise and efficient fact-checking in a single inference call. Experimental results on seven benchmarks spanning both general and medical domains demonstrate a 6.1\% overall improvement over baseline models. Notably, GraphCheck outperforms existing specialized fact-checkers and achieves comparable performance with state-of-the-art LLMs, such as DeepSeek-V3 and OpenAI-o1, with significantly fewer parameters.

### FanChuan: A Multilingual and Graph-Structured Benchmark For Parody Detection and Analysis 
[[arxiv](https://arxiv.org/abs/2502.16503)] [[cool](https://papers.cool/arxiv/2502.16503)] [[pdf](https://arxiv.org/pdf/2502.16503)]
> **Authors**: Yilun Zheng,Sha Li,Fangkun Wu,Yang Ziyi,Lin Hongchao,Zhichao Hu,Cai Xinjun,Ziming Wang,Jinxuan Chen,Sitao Luan,Jiahao Xu,Lihui Chen
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Parody is an emerging phenomenon on social media, where individuals imitate a role or position opposite to their own, often for humor, provocation, or controversy. Detecting and analyzing parody can be challenging and is often reliant on context, yet it plays a crucial role in understanding cultural values, promoting subcultures, and enhancing self-expression. However, the study of parody is hindered by limited available data and deficient diversity in current datasets. To bridge this gap, we built seven parody datasets from both English and Chinese corpora, with 14,755 annotated users and 21,210 annotated comments in total. To provide sufficient context information, we also collect replies and construct user-interaction graphs to provide richer contextual information, which is lacking in existing datasets. With these datasets, we test traditional methods and Large Language Models (LLMs) on three key tasks: (1) parody detection, (2) comment sentiment analysis with parody, and (3) user sentiment analysis with parody. Our extensive experiments reveal that parody-related tasks still remain challenging for all models, and contextual information plays a critical role. Interestingly, we find that, in certain scenarios, traditional sentence embedding methods combined with simple classifiers can outperform advanced LLMs, i.e. DeepSeek-R1 and GPT-o3, highlighting parody as a significant challenge for LLMs.

### Intrinsic Model Weaknesses: How Priming Attacks Unveil Vulnerabilities in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.16491)] [[cool](https://papers.cool/arxiv/2502.16491)] [[pdf](https://arxiv.org/pdf/2502.16491)]
> **Authors**: Yuyi Huang,Runzhe Zhan,Derek F. Wong,Lidia S. Chao,Ailin Tao
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) have significantly influenced various industries but suffer from a critical flaw, the potential sensitivity of generating harmful content, which poses severe societal risks. We developed and tested novel attack strategies on popular LLMs to expose their vulnerabilities in generating inappropriate content. These strategies, inspired by psychological phenomena such as the "Priming Effect", "Safe Attention Shift", and "Cognitive Dissonance", effectively attack the models' guarding mechanisms. Our experiments achieved an attack success rate (ASR) of 100% on various open-source models, including Meta's Llama-3.2, Google's Gemma-2, Mistral's Mistral-NeMo, Falcon's Falcon-mamba, Apple's DCLM, Microsoft's Phi3, and Qwen's Qwen2.5, among others. Similarly, for closed-source models such as OpenAI's GPT-4o, Google's Gemini-1.5, and Claude-3.5, we observed an ASR of at least 95% on the AdvBench dataset, which represents the current state-of-the-art. This study underscores the urgent need to reassess the use of generative models in critical applications to mitigate potential adverse societal impacts.

### All That Glitters is Not Novel: Plagiarism in AI Generated Research 
[[arxiv](https://arxiv.org/abs/2502.16487)] [[cool](https://papers.cool/arxiv/2502.16487)] [[pdf](https://arxiv.org/pdf/2502.16487)]
> **Authors**: Tarun Gupta,Danish Pruthi
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: Preprint
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Automating scientific research is considered the final frontier of science. Recently, several papers claim autonomous research agents can generate novel research ideas. Amidst the prevailing optimism, we document a critical concern: a considerable fraction of such research documents are smartly plagiarized. Unlike past efforts where experts evaluate the novelty and feasibility of research ideas, we request $13$ experts to operate under a different situational logic: to identify similarities between LLM-generated research documents and existing work. Concerningly, the experts identify $24\%$ of the $50$ evaluated research documents to be either paraphrased (with one-to-one methodological mapping), or significantly borrowed from existing work. These reported instances are cross-verified by authors of the source papers. Problematically, these LLM-generated research documents do not acknowledge original sources, and bypass inbuilt plagiarism detectors. Lastly, through controlled experiments we show that automated plagiarism detectors are inadequate at catching deliberately plagiarized ideas from an LLM. We recommend a careful assessment of LLM-generated research, and discuss the implications of our findings on research and academic publishing.

### A Fine-Tuning Approach for T5 Using Knowledge Graphs to Address Complex Tasks 
[[arxiv](https://arxiv.org/abs/2502.16484)] [[cool](https://papers.cool/arxiv/2502.16484)] [[pdf](https://arxiv.org/pdf/2502.16484)]
> **Authors**: Xiaoxuan Liao,Binrong Zhu,Jacky He,Guiran Liu,Hongye Zheng,Jia Gao
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: With the development of deep learning technology, large language models have achieved remarkable results in many natural language processing tasks. However, these models still have certain limitations in handling complex reasoning tasks and understanding rich background knowledge. To solve this problem, this study proposed a T5 model fine-tuning method based on knowledge graphs, which enhances the model's reasoning ability and context understanding ability by introducing external knowledge graphs. We used the SQuAD1.1 dataset for experiments. The experimental results show that the T5 model based on knowledge graphs is significantly better than other baseline models in reasoning accuracy, context understanding, and the ability to handle complex problems. At the same time, we also explored the impact of knowledge graphs of different scales on model performance and found that as the scale of the knowledge graph increases, the performance of the model gradually improves. Especially when dealing with complex problems, the introduction of knowledge graphs greatly improves the reasoning ability of the T5 model. Ablation experiments further verify the importance of entity and relationship embedding in the model and prove that a complete knowledge graph is crucial to improving the various capabilities of the T5 model. In summary, this study provides an effective method to enhance the reasoning and understanding capabilities of large language models and provides new directions for future research.

### Towards Fully-Automated Materials Discovery via Large-Scale Synthesis Dataset and Expert-Level LLM-as-a-Judge 
[[arxiv](https://arxiv.org/abs/2502.16457)] [[cool](https://papers.cool/arxiv/2502.16457)] [[pdf](https://arxiv.org/pdf/2502.16457)]
> **Authors**: Heegyu Kim,Taeyang Jeon,Seungtaek Choi,Jihoon Hong,Dongwon Jeon,Sungbum Cho,Ga-Yeon Baek,Kyung-Won Kwak,Dong-Hee Lee,Sun-Jin Choi,Jisu Bae,Chihoon Lee,Yunseo Kim,Jinsung Park,Hyunsouk Cho
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: under review
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Materials synthesis is vital for innovations such as energy storage, catalysis, electronics, and biomedical devices. Yet, the process relies heavily on empirical, trial-and-error methods guided by expert intuition. Our work aims to support the materials science community by providing a practical, data-driven resource. We have curated a comprehensive dataset of 17K expert-verified synthesis recipes from open-access literature, which forms the basis of our newly developed benchmark, AlchemyBench. AlchemyBench offers an end-to-end framework that supports research in large language models applied to synthesis prediction. It encompasses key tasks, including raw materials and equipment prediction, synthesis procedure generation, and characterization outcome forecasting. We propose an LLM-as-a-Judge framework that leverages large language models for automated evaluation, demonstrating strong statistical agreement with expert assessments. Overall, our contributions offer a supportive foundation for exploring the capabilities of LLMs in predicting and guiding materials synthesis, ultimately paving the way for more efficient experimental design and accelerated innovation in materials science.

### Contrastive Learning of English Language and Crystal Graphs for Multimodal Representation of Materials Knowledge 
[[arxiv](https://arxiv.org/abs/2502.16451)] [[cool](https://papers.cool/arxiv/2502.16451)] [[pdf](https://arxiv.org/pdf/2502.16451)]
> **Authors**: Yang Jeong Park,Mayank Kumaran,Chia-Wei Hsu,Elsa Olivetti,Ju Li
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: 24 pages, 14 figure
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Artificial intelligence (AI) is increasingly used for the inverse design of materials, such as crystals and molecules. Existing AI research on molecules has integrated chemical structures of molecules with textual knowledge to adapt to complex instructions. However, this approach has been unattainable for crystals due to data scarcity from the biased distribution of investigated crystals and the lack of semantic supervision in peer-reviewed literature. In this work, we introduce a contrastive language-crystals model (CLaC) pre-trained on a newly synthesized dataset of 126k crystal structure-text pairs. To demonstrate the advantage of using synthetic data to overcome data scarcity, we constructed a comparable dataset extracted from academic papers. We evaluate CLaC's generalization ability through various zero-shot cross-modal tasks and downstream applications. In experiments, CLaC achieves state-of-the-art zero-shot generalization performance in understanding crystal structures, surpassing latest large language models.

### Sequence-level Large Language Model Training with Contrastive Preference Optimization 
[[arxiv](https://arxiv.org/abs/2502.16433)] [[cool](https://papers.cool/arxiv/2502.16433)] [[pdf](https://arxiv.org/pdf/2502.16433)]
> **Authors**: Zhili Feng,Dhananjay Ram,Cole Hawkins,Aditya Rawal,Jinman Zhao,Sheng Zha
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: The next token prediction loss is the dominant self-supervised training objective for large language models and has achieved promising results in a variety of downstream tasks. However, upon closer investigation of this objective, we find that it lacks an understanding of sequence-level signals, leading to a mismatch between training and inference processes. To bridge this gap, we introduce a contrastive preference optimization (CPO) procedure that can inject sequence-level information into the language model at any training stage without expensive human labeled data. Our experiments show that the proposed objective surpasses the next token prediction in terms of win rate in the instruction-following and text generation tasks.

### Automatic Detection of Research Values from Scientific Abstracts Across Computer Science Subfields 
[[arxiv](https://arxiv.org/abs/2502.16390)] [[cool](https://papers.cool/arxiv/2502.16390)] [[pdf](https://arxiv.org/pdf/2502.16390)]
> **Authors**: Hang Jiang,Tal August,Luca Soldaini,Kyle Lo,Maria Antoniak
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: Paper accepted and presented at the 3rd International Conference of Science of Science & Innovation (ICSSI 2024) as an extended abstract
- **标题**: None
- **领域**: 计算语言学,数字图书馆
- **Abstract**: The field of Computer science (CS) has rapidly evolved over the past few decades, providing computational tools and methodologies to various fields and forming new interdisciplinary communities. This growth in CS has significantly impacted institutional practices and relevant research communities. Therefore, it is crucial to explore what specific research values, known as basic and fundamental beliefs that guide or motivate research attitudes or actions, CS-related research communities promote. Prior research has manually analyzed research values from a small sample of machine learning papers. No prior work has studied the automatic detection of research values in CS from large-scale scientific texts across different research subfields. This paper introduces a detailed annotation scheme featuring ten research values that guide CS-related research. Based on the scheme, we build value classifiers to scale up the analysis and present a systematic study over 226,600 paper abstracts from 32 CS-related subfields and 86 popular publishing venues over ten years.

### Instruction-Tuning LLMs for Event Extraction with Annotation Guidelines 
[[arxiv](https://arxiv.org/abs/2502.16377)] [[cool](https://papers.cool/arxiv/2502.16377)] [[pdf](https://arxiv.org/pdf/2502.16377)]
> **Authors**: Saurabh Srivastava,Sweta Pati,Ziyu Yao
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: In this work, we study the effect of annotation guidelines -- textual descriptions of event types and arguments, when instruction-tuning large language models for event extraction. We conducted a series of experiments with both human-provided and machine-generated guidelines in both full- and low-data settings. Our results demonstrate the promise of annotation guidelines when there is a decent amount of training data and highlight its effectiveness in improving cross-schema generalization and low-frequency event-type performance.

### A generative approach to LLM harmfulness detection with special red flag tokens 
[[arxiv](https://arxiv.org/abs/2502.16366)] [[cool](https://papers.cool/arxiv/2502.16366)] [[pdf](https://arxiv.org/pdf/2502.16366)]
> **Authors**: Sophie Xhonneux,David Dobre,Mehrnaz Mohfakhami,Leo Schwinn,Gauthier Gidel
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: 13 pages, 6 figures
- **标题**: None
- **领域**: 计算语言学,人工智能,密码学和安全,机器学习
- **Abstract**: Most safety training methods for large language models (LLMs) based on fine-tuning rely on dramatically changing the output distribution of the model when faced with a harmful request, shifting it from an unsafe answer to a refusal to respond. These methods inherently compromise model capabilities and might make auto-regressive models vulnerable to attacks that make likely an initial token of affirmative response. To avoid that, we propose to expand the model's vocabulary with a special token we call red flag token (<rf>) and propose to fine-tune the model to generate this token at any time harmful content is generated or about to be generated. This novel safety training method effectively augments LLMs into generative classifiers of harmfulness at all times during the conversation. This method offers several advantages: it enables the model to explicitly learn the concept of harmfulness while marginally affecting the generated distribution, thus maintaining the model's utility. It also evaluates each generated answer rather than just the input prompt and provides a stronger defence against sampling-based attacks. In addition, it simplifies the evaluation of the model's robustness and reduces correlated failures when combined with a classifier. We further show an increased robustness to long contexts, and supervised fine-tuning attacks.

### Wrong Answers Can Also Be Useful: PlausibleQA -- A Large-Scale QA Dataset with Answer Plausibility Scores 
[[arxiv](https://arxiv.org/abs/2502.16358)] [[cool](https://papers.cool/arxiv/2502.16358)] [[pdf](https://arxiv.org/pdf/2502.16358)]
> **Authors**: Jamshid Mozafari,Abdelrahman Abdallah,Bhawna Piryani,Adam Jatowt
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: Submitted to SIGIR 2025
- **标题**: None
- **领域**: 计算语言学,信息检索
- **Abstract**: Large Language Models (LLMs) are revolutionizing information retrieval, with chatbots becoming an important source for answering user queries. As by their design, LLMs prioritize generating correct answers, the value of highly plausible yet incorrect answers (candidate answers) tends to be overlooked. However, such answers can still prove useful, for example, they can play a crucial role in tasks like Multiple-Choice Question Answering (MCQA) and QA Robustness Assessment (QARA). Existing QA datasets primarily focus on correct answers without explicit consideration of the plausibility of other candidate answers, limiting opportunity for more nuanced evaluations of models. To address this gap, we introduce PlausibleQA, a large-scale dataset comprising 10,000 questions and 100,000 candidate answers, each annotated with plausibility scores and justifications for their selection. Additionally, the dataset includes 900,000 justifications for pairwise comparisons between candidate answers, further refining plausibility assessments. We evaluate PlausibleQA through human assessments and empirical experiments, demonstrating its utility in MCQA and QARA analysis. Our findings show that plausibility-aware approaches are effective for MCQA distractor generation and QARA. We release PlausibleQA as a resource for advancing QA research and enhancing LLM performance in distinguishing plausible distractors from correct answers.

### LegalBench.PT: A Benchmark for Portuguese Law 
[[arxiv](https://arxiv.org/abs/2502.16357)] [[cool](https://papers.cool/arxiv/2502.16357)] [[pdf](https://arxiv.org/pdf/2502.16357)]
> **Authors**: Beatriz Canaverde,Telmo Pessoa Pires,Leonor Melo Ribeiro,André F. T. Martins
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The recent application of LLMs to the legal field has spurred the creation of benchmarks across various jurisdictions and languages. However, no benchmark has yet been specifically designed for the Portuguese legal system. In this work, we present LegalBench.PT, the first comprehensive legal benchmark covering key areas of Portuguese law. To develop LegalBench.PT, we first collect long-form questions and answers from real law exams, and then use GPT-4o to convert them into multiple-choice, true/false, and matching formats. Once generated, the questions are filtered and processed to improve the quality of the dataset. To ensure accuracy and relevance, we validate our approach by having a legal professional review a sample of the generated questions. Although the questions are synthetically generated, we show that their basis in human-created exams and our rigorous filtering and processing methods applied result in a reliable benchmark for assessing LLMs' legal knowledge and reasoning abilities. Finally, we evaluate the performance of leading LLMs on LegalBench.PT and investigate potential biases in GPT-4o's responses. We also assess the performance of Portuguese lawyers on a sample of questions to establish a baseline for model comparison and validate the benchmark.

### Iterative Auto-Annotation for Scientific Named Entity Recognition Using BERT-Based Models 
[[arxiv](https://arxiv.org/abs/2502.16312)] [[cool](https://papers.cool/arxiv/2502.16312)] [[pdf](https://arxiv.org/pdf/2502.16312)]
> **Authors**: Kartik Gupta
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: 9 pages
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: This paper presents an iterative approach to performing Scientific Named Entity Recognition (SciNER) using BERT-based models. We leverage transfer learning to fine-tune pretrained models with a small but high-quality set of manually annotated data. The process is iteratively refined by using the fine-tuned model to auto-annotate a larger dataset, followed by additional rounds of fine-tuning. We evaluated two models, dslim/bert-large-NER and bert-largecased, and found that bert-large-cased consistently outperformed the former. Our approach demonstrated significant improvements in prediction accuracy and F1 scores, especially for less common entity classes. Future work could include pertaining with unlabeled data, exploring more powerful encoders like RoBERTa, and expanding the scope of manual annotations. This methodology has broader applications in NLP tasks where access to labeled data is limited.

### Fine-Tuning Qwen 2.5 3B for Realistic Movie Dialogue Generation 
[[arxiv](https://arxiv.org/abs/2502.16274)] [[cool](https://papers.cool/arxiv/2502.16274)] [[pdf](https://arxiv.org/pdf/2502.16274)]
> **Authors**: Kartik Gupta
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: 5 pages, 2 figures
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: The Qwen 2.5 3B base model was fine-tuned to generate contextually rich and engaging movie dialogue, leveraging the Cornell Movie-Dialog Corpus, a curated dataset of movie conversations. Due to the limitations in GPU computing and VRAM, the training process began with the 0.5B model progressively scaling up to the 1.5B and 3B versions as efficiency improvements were implemented. The Qwen 2.5 series, developed by Alibaba Group, stands at the forefront of small open-source pre-trained models, particularly excelling in creative tasks compared to alternatives like Meta's Llama 3.2 and Google's Gemma. Results demonstrate the ability of small models to produce high-quality, realistic dialogue, offering a promising approach for real-time, context-sensitive conversation generation.

### ThinkBench: Dynamic Out-of-Distribution Evaluation for Robust LLM Reasoning 
[[arxiv](https://arxiv.org/abs/2502.16268)] [[cool](https://papers.cool/arxiv/2502.16268)] [[pdf](https://arxiv.org/pdf/2502.16268)]
> **Authors**: Shulin Huang,Linyi Yang,Yan Song,Shuang Chen,Leyang Cui,Ziyu Wan,Qingcheng Zeng,Ying Wen,Kun Shao,Weinan Zhang,Jun Wang,Yue Zhang
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Evaluating large language models (LLMs) poses significant challenges, particularly due to issues of data contamination and the leakage of correct answers. To address these challenges, we introduce ThinkBench, a novel evaluation framework designed to evaluate LLMs' reasoning capability robustly. ThinkBench proposes a dynamic data generation method for constructing out-of-distribution (OOD) datasets and offers an OOD dataset that contains 2,912 samples drawn from reasoning tasks. ThinkBench unifies the evaluation of reasoning models and non-reasoning models. We evaluate 16 LLMs and 4 PRMs under identical experimental conditions and show that most of the LLMs' performance are far from robust and they face a certain level of data leakage. By dynamically generating OOD datasets, ThinkBench effectively provides a reliable evaluation of LLMs and reduces the impact of data contamination.

### IPO: Your Language Model is Secretly a Preference Classifier 
[[arxiv](https://arxiv.org/abs/2502.16182)] [[cool](https://papers.cool/arxiv/2502.16182)] [[pdf](https://arxiv.org/pdf/2502.16182)]
> **Authors**: Shivank Garg,Ayush Singh,Shweta Singh,Paras Chopra
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Reinforcement learning from human feedback (RLHF) has emerged as the primary method for aligning large language models (LLMs) with human preferences. While it enables LLMs to achieve human-level alignment, it often incurs significant computational and financial costs due to its reliance on training external reward models or human-labeled preferences. In this work, we propose \textbf{Implicit Preference Optimization (IPO)}, an alternative approach that leverages generative LLMs as preference classifiers, thereby reducing the dependence on external human feedback or reward models to obtain preferences. We conduct a comprehensive evaluation on the preference classification ability of LLMs using RewardBench, assessing models across different sizes, architectures, and training levels to validate our hypothesis. Furthermore, we investigate the self-improvement capabilities of LLMs by generating multiple responses for a given instruction and employing the model itself as a preference classifier for Direct Preference Optimization (DPO)-based training. Our findings demonstrate that models trained through IPO achieve performance comparable to those utilizing state-of-the-art reward models for obtaining preferences.

### BiDeV: Bilateral Defusing Verification for Complex Claim Fact-Checking 
[[arxiv](https://arxiv.org/abs/2502.16181)] [[cool](https://papers.cool/arxiv/2502.16181)] [[pdf](https://arxiv.org/pdf/2502.16181)]
> **Authors**: Yuxuan Liu,Hongda Sun,Wenya Guo,Xinyan Xiao,Cunli Mao,Zhengtao Yu,Rui Yan
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: Accepted by AAAI2025
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Complex claim fact-checking performs a crucial role in disinformation detection. However, existing fact-checking methods struggle with claim vagueness, specifically in effectively handling latent information and complex relations within claims. Moreover, evidence redundancy, where nonessential information complicates the verification process, remains a significant issue. To tackle these limitations, we propose Bilateral Defusing Verification (BiDeV), a novel fact-checking working-flow framework integrating multiple role-played LLMs to mimic the human-expert fact-checking process. BiDeV consists of two main modules: Vagueness Defusing identifies latent information and resolves complex relations to simplify the claim, and Redundancy Defusing eliminates redundant content to enhance the evidence quality. Extensive experimental results on two widely used challenging fact-checking benchmarks (Hover and Feverous-s) demonstrate that our BiDeV can achieve the best performance under both gold and open settings. This highlights the effectiveness of BiDeV in handling complex claims and ensuring precise fact-checking

### OrderSum: Semantic Sentence Ordering for Extractive Summarization 
[[arxiv](https://arxiv.org/abs/2502.16180)] [[cool](https://papers.cool/arxiv/2502.16180)] [[pdf](https://arxiv.org/pdf/2502.16180)]
> **Authors**: Taewan Kwon,Sangyong Lee
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: 29 pages, 12 pages for the main body, 5 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: There are two main approaches to recent extractive summarization: the sentence-level framework, which selects sentences to include in a summary individually, and the summary-level framework, which generates multiple candidate summaries and ranks them. Previous work in both frameworks has primarily focused on improving which sentences in a document should be included in the summary. However, the sentence order of extractive summaries, which is critical for the quality of a summary, remains underexplored. In this paper, we introduce OrderSum, a novel extractive summarization model that semantically orders sentences within an extractive summary. OrderSum proposes a new representation method to incorporate the sentence order into the embedding of the extractive summary, and an objective function to train the model to identify which extractive summary has a better sentence order in the semantic space. Extensive experimental results demonstrate that OrderSum obtains state-of-the-art performance in both sentence inclusion and sentence order for extractive summarization. In particular, OrderSum achieves a ROUGE-L score of 30.52 on CNN/DailyMail, outperforming the previous state-of-the-art model by a large margin of 2.54.

### Mapping 1,000+ Language Models via the Log-Likelihood Vector 
[[arxiv](https://arxiv.org/abs/2502.16173)] [[cool](https://papers.cool/arxiv/2502.16173)] [[pdf](https://arxiv.org/pdf/2502.16173)]
> **Authors**: Momose Oyama,Hiroaki Yamagiwa,Yusuke Takase,Hidetoshi Shimodaira
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: To compare autoregressive language models at scale, we propose using log-likelihood vectors computed on a predefined text set as model features. This approach has a solid theoretical basis: when treated as model coordinates, their squared Euclidean distance approximates the Kullback-Leibler divergence of text-generation probabilities. Our method is highly scalable, with computational cost growing linearly in both the number of models and text samples, and is easy to implement as the required features are derived from cross-entropy loss. Applying this method to over 1,000 language models, we constructed a "model map," providing a new perspective on large-scale model analysis.

### EPERM: An Evidence Path Enhanced Reasoning Model for Knowledge Graph Question and Answering 
[[arxiv](https://arxiv.org/abs/2502.16171)] [[cool](https://papers.cool/arxiv/2502.16171)] [[pdf](https://arxiv.org/pdf/2502.16171)]
> **Authors**: Xiao Long,Liansheng Zhuang,Aodi Li,Minghong Yao,Shafei Wang
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Due to the remarkable reasoning ability, Large language models (LLMs) have demonstrated impressive performance in knowledge graph question answering (KGQA) tasks, which find answers to natural language questions over knowledge graphs (KGs). To alleviate the hallucinations and lack of knowledge issues of LLMs, existing methods often retrieve the question-related information from KGs to enrich the input context. However, most methods focus on retrieving the relevant information while ignoring the importance of different types of knowledge in reasoning, which degrades their performance. To this end, this paper reformulates the KGQA problem as a graphical model and proposes a three-stage framework named the Evidence Path Enhanced Reasoning Model (EPERM) for KGQA. In the first stage, EPERM uses the fine-tuned LLM to retrieve a subgraph related to the question from the original knowledge graph. In the second stage, EPERM filters out the evidence paths that faithfully support the reasoning of the questions, and score their importance in reasoning. Finally, EPERM uses the weighted evidence paths to reason the final answer. Since considering the importance of different structural information in KGs for reasoning, EPERM can improve the reasoning ability of LLMs in KGQA tasks. Extensive experiments on benchmark datasets demonstrate that EPERM achieves superior performances in KGQA tasks.

### ZiGong 1.0: A Large Language Model for Financial Credit 
[[arxiv](https://arxiv.org/abs/2502.16159)] [[cool](https://papers.cool/arxiv/2502.16159)] [[pdf](https://arxiv.org/pdf/2502.16159)]
> **Authors**: Yu Lei,Zixuan Wang,Chu Liu,Tongyao Wang
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,计算工程、金融和科学
- **Abstract**: Large Language Models (LLMs) have demonstrated strong performance across various general Natural Language Processing (NLP) tasks. However, their effectiveness in financial credit assessment applications remains suboptimal, primarily due to the specialized financial expertise required for these tasks. To address this limitation, we propose ZiGong, a Mistral-based model enhanced through multi-task supervised fine-tuning. To specifically combat model hallucination in financial contexts, we introduce a novel data pruning methodology. Our approach utilizes a proxy model to score training samples, subsequently combining filtered data with original datasets for model training. This data refinement strategy effectively reduces hallucinations in LLMs while maintaining reliability in downstream financial applications. Experimental results show our method significantly enhances model robustness and prediction accuracy in real-world financial scenarios.

### Number Representations in LLMs: A Computational Parallel to Human Perception 
[[arxiv](https://arxiv.org/abs/2502.16147)] [[cool](https://papers.cool/arxiv/2502.16147)] [[pdf](https://arxiv.org/pdf/2502.16147)]
> **Authors**: H. V. AlquBoj,Hilal AlQuabeh,Velibor Bojkovic,Tatsuya Hiraoka,Ahmed Oumar El-Shangiti,Munachiso Nwadike,Kentaro Inui
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: The number line of LLMs
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Humans are believed to perceive numbers on a logarithmic mental number line, where smaller values are represented with greater resolution than larger ones. This cognitive bias, supported by neuroscience and behavioral studies, suggests that numerical magnitudes are processed in a sublinear fashion rather than on a uniform linear scale. Inspired by this hypothesis, we investigate whether large language models (LLMs) exhibit a similar logarithmic-like structure in their internal numerical representations. By analyzing how numerical values are encoded across different layers of LLMs, we apply dimensionality reduction techniques such as PCA and PLS followed by geometric regression to uncover latent structures in the learned embeddings. Our findings reveal that the model's numerical representations exhibit sublinear spacing, with distances between values aligning with a logarithmic scale. This suggests that LLMs, much like humans, may encode numbers in a compressed, non-uniform manner.

### The Law of Knowledge Overshadowing: Towards Understanding, Predicting, and Preventing LLM Hallucination 
[[arxiv](https://arxiv.org/abs/2502.16143)] [[cool](https://papers.cool/arxiv/2502.16143)] [[pdf](https://arxiv.org/pdf/2502.16143)]
> **Authors**: Yuji Zhang,Sha Li,Cheng Qian,Jiateng Liu,Pengfei Yu,Chi Han,Yi R. Fung,Kathleen McKeown,Chengxiang Zhai,Manling Li,Heng Ji
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: 19 pages, 5 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Hallucination is a persistent challenge in large language models (LLMs), where even with rigorous quality control, models often generate distorted facts. This paradox, in which error generation continues despite high-quality training data, calls for a deeper understanding of the underlying LLM mechanisms. To address it, we propose a novel concept: knowledge overshadowing, where model's dominant knowledge can obscure less prominent knowledge during text generation, causing the model to fabricate inaccurate details. Building on this idea, we introduce a novel framework to quantify factual hallucinations by modeling knowledge overshadowing. Central to our approach is the log-linear law, which predicts that the rate of factual hallucination increases linearly with the logarithmic scale of (1) Knowledge Popularity, (2) Knowledge Length, and (3) Model Size. The law provides a means to preemptively quantify hallucinations, offering foresight into their occurrence even before model training or inference. Built on overshadowing effect, we propose a new decoding strategy CoDa, to mitigate hallucinations, which notably enhance model factuality on Overshadow (27.9%), MemoTrap (13.1%) and NQ-Swap (18.3%). Our findings not only deepen understandings of the underlying mechanisms behind hallucinations but also provide actionable insights for developing more predictable and controllable language models.

### Understanding Zero-shot Rare Word Recognition Improvements Through LLM Integration 
[[arxiv](https://arxiv.org/abs/2502.16142)] [[cool](https://papers.cool/arxiv/2502.16142)] [[pdf](https://arxiv.org/pdf/2502.16142)]
> **Authors**: Haoxuan Wang
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,音频和语音处理
- **Abstract**: In this study, we investigate the integration of a large language model (LLM) with an automatic speech recognition (ASR) system, specifically focusing on enhancing rare word recognition performance. Using a 190,000-hour dataset primarily sourced from YouTube, pre-processed with Whisper V3 pseudo-labeling, we demonstrate that the LLM-ASR architecture outperforms traditional Zipformer-Transducer models in the zero-shot rare word recognition task, after training on a large dataset. Our analysis reveals that the LLM contributes significantly to improvements in rare word error rate (R-WER), while the speech encoder primarily determines overall transcription performance (Orthographic Word Error Rate, O-WER, and Normalized Word Error Rate, N-WER). Through extensive ablation studies, we highlight the importance of adapter integration in aligning speech encoder outputs with the LLM's linguistic capabilities. Furthermore, we emphasize the critical role of high-quality labeled data in achieving optimal performance. These findings provide valuable insights into the synergy between LLM-based ASR architectures, paving the way for future advancements in large-scale LLM-based speech recognition systems.

### Chain-of-Description: What I can understand, I can put into words 
[[arxiv](https://arxiv.org/abs/2502.16137)] [[cool](https://papers.cool/arxiv/2502.16137)] [[pdf](https://arxiv.org/pdf/2502.16137)]
> **Authors**: Jiaxin Guo,Daimeng Wei,Zongyao Li,Hengchao Shang,Yuanchang Luo,Hao Yang
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: In this paper, we propose a novel strategy defined as Chain-of-Description (CoD) Prompting, tailored for Multi-Modal Large Language Models. This approach involves having the model first provide a detailed description of the multi-modal input before generating an answer to the question. When applied to models such as Qwen2-Audio, Qwen2-VL, and Qwen2.5-VL, CoD Prompting significantly enhances performance compared to standard prompting methods. This is demonstrated by nearly a 4\% improvement in the speech category of the audio benchmark AIR-Bench-Chat and a 5.3\% improvement in the hard-level portion of the vision benchmark MMMU\_Pro. Our ablation study further validates the effectiveness of CoD Prompting.

### Be a Multitude to Itself: A Prompt Evolution Framework for Red Teaming 
[[arxiv](https://arxiv.org/abs/2502.16109)] [[cool](https://papers.cool/arxiv/2502.16109)] [[pdf](https://arxiv.org/pdf/2502.16109)]
> **Authors**: Rui Li,Peiyi Wang,Jingyuan Ma,Di Zhang,Lei Sha,Zhifang Sui
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models (LLMs) have gained increasing attention for their remarkable capacity, alongside concerns about safety arising from their potential to produce harmful content. Red teaming aims to find prompts that could elicit harmful responses from LLMs, and is essential to discover and mitigate safety risks before real-world deployment. However, manual red teaming is both time-consuming and expensive, rendering it unscalable. In this paper, we propose RTPE, a scalable evolution framework to evolve red teaming prompts across both breadth and depth dimensions, facilitating the automatic generation of numerous high-quality and diverse red teaming prompts. Specifically, in-breadth evolving employs a novel enhanced in-context learning method to create a multitude of quality prompts, whereas in-depth evolving applies customized transformation operations to enhance both content and form of prompts, thereby increasing diversity. Extensive experiments demonstrate that RTPE surpasses existing representative automatic red teaming methods on both attack success rate and diversity. In addition, based on 4,800 red teaming prompts created by RTPE, we further provide a systematic analysis of 8 representative LLMs across 8 sensitive topics.

### Echo: A Large Language Model with Temporal Episodic Memory 
[[arxiv](https://arxiv.org/abs/2502.16090)] [[cool](https://papers.cool/arxiv/2502.16090)] [[pdf](https://arxiv.org/pdf/2502.16090)]
> **Authors**: WenTao Liu,Ruohua Zhang,Aimin Zhou,Feng Gao,JiaLi Liu
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Research on large language models (LLMs) has shown remarkable performance in domains such as mathematics, programming, and literary creation. However, most studies have focused on semantic memory-based question answering, neglecting LLMs' potential to handle episodic memory (EM)-related queries. This oversight has led to suboptimal performance in applications requiring EM, including emotional companionship, personal AI assistants, and AI teachers. To address this gap, we introduce Echo, a LLM enhanced with temporal episodic memory. We propose a Multi-Agent Data Generation Framework that guides the model in generating multi-turn, complex scenario episodic memory dialogue data (EM-Train). Temporal information is innovatively incorporated into the LLM training process, and Echo is trained using the EM-Train. Furthermore, We develop an EM-Test benchmark specifically designed to evaluate LLMs' episodic memory capabilities. The EM-Test assesses performance across various time spans and difficulty levels, providing a comprehensive evaluation of multi-turn episodic memory dialogues. Our experiments demonstrate that Echo significantly outperforms state-of-the-art LLMs on EM-Test. Additionally, a qualitative analysis reveals Echo's potential to exhibit human-like episodic memory capabilities. We will open-source all datasets, code, and model weights.

### Moving Beyond Medical Exam Questions: A Clinician-Annotated Dataset of Real-World Tasks and Ambiguity in Mental Healthcare 
[[arxiv](https://arxiv.org/abs/2502.16051)] [[cool](https://papers.cool/arxiv/2502.16051)] [[pdf](https://arxiv.org/pdf/2502.16051)]
> **Authors**: Max Lamparth,Declan Grabb,Amy Franks,Scott Gershan,Kaitlyn N. Kunstman,Aaron Lulla,Monika Drummond Roots,Manu Sharma,Aryan Shrivastava,Nina Vasan,Colleen Waickman
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Current medical language model (LM) benchmarks often over-simplify the complexities of day-to-day clinical practice tasks and instead rely on evaluating LMs on multiple-choice board exam questions. Thus, we present an expert-created and annotated dataset spanning five critical domains of decision-making in mental healthcare: treatment, diagnosis, documentation, monitoring, and triage. This dataset - created without any LM assistance - is designed to capture the nuanced clinical reasoning and daily ambiguities mental health practitioners encounter, reflecting the inherent complexities of care delivery that are missing from existing datasets. Almost all 203 base questions with five answer options each have had the decision-irrelevant demographic patient information removed and replaced with variables (e.g., AGE), and are available for male, female, or non-binary-coded patients. For question categories dealing with ambiguity and multiple valid answer options, we create a preference dataset with uncertainties from the expert annotations. We outline a series of intended use cases and demonstrate the usability of our dataset by evaluating eleven off-the-shelf and four mental health fine-tuned LMs on category-specific task accuracy, on the impact of patient demographic information on decision-making, and how consistently free-form responses deviate from human annotated samples.

### Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for Multimodal Reasoning Models 
[[arxiv](https://arxiv.org/abs/2502.16033)] [[cool](https://papers.cool/arxiv/2502.16033)] [[pdf](https://arxiv.org/pdf/2502.16033)]
> **Authors**: Qianqi Yan,Yue Fan,Hongquan Li,Shan Jiang,Yang Zhao,Xinze Guan,Ching-Chen Kuo,Xin Eric Wang
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Existing Multimodal Large Language Models (MLLMs) are predominantly trained and tested on consistent visual-textual inputs, leaving open the question of whether they can handle inconsistencies in real-world, layout-rich content. To bridge this gap, we propose the Multimodal Inconsistency Reasoning (MMIR) benchmark to assess MLLMs' ability to detect and reason about semantic mismatches in artifacts such as webpages, presentation slides, and posters. MMIR comprises 534 challenging samples, each containing synthetically injected errors across five reasoning-heavy categories: Factual Contradiction, Identity Misattribution, Contextual Mismatch, Quantitative Discrepancy, and Temporal/Spatial Incoherence. We evaluate six state-of-the-art MLLMs, showing that models with dedicated multimodal reasoning capabilities, such as o1, substantially outperform their counterparts while open-source models remain particularly vulnerable to inconsistency errors. Detailed error analyses further show that models excel in detecting inconsistencies confined to a single modality, particularly in text, but struggle with cross-modal conflicts and complex layouts. Probing experiments reveal that single-modality prompting, including Chain-of-Thought (CoT) and Set-of-Mark (SoM) methods, yields marginal gains, revealing a key bottleneck in cross-modal reasoning. Our findings highlight the need for advanced multimodal reasoning and point to future research on multimodal inconsistency.

### Enhancing LLMs for Identifying and Prioritizing Important Medical Jargons from Electronic Health Record Notes Utilizing Data Augmentation 
[[arxiv](https://arxiv.org/abs/2502.16022)] [[cool](https://papers.cool/arxiv/2502.16022)] [[pdf](https://arxiv.org/pdf/2502.16022)]
> **Authors**: Won Seok Jang,Sharmin Sultana,Zonghai Yao,Hieu Tran,Zhichao Yang,Sunjae Kwon,Hong Yu
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 21pages, 5 figures, 4 tables
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: OpenNotes enables patients to access EHR notes, but medical jargon can hinder comprehension. To improve understanding, we evaluated closed- and open-source LLMs for extracting and prioritizing key medical terms using prompting, fine-tuning, and data augmentation. We assessed LLMs on 106 expert-annotated EHR notes, experimenting with (i) general vs. structured prompts, (ii) zero-shot vs. few-shot prompting, (iii) fine-tuning, and (iv) data augmentation. To enhance open-source models in low-resource settings, we used ChatGPT for data augmentation and applied ranking techniques. We incrementally increased the augmented dataset size (10 to 10,000) and conducted 5-fold cross-validation, reporting F1 score and Mean Reciprocal Rank (MRR). Our result show that fine-tuning and data augmentation improved performance over other strategies. GPT-4 Turbo achieved the highest F1 (0.433), while Mistral7B with data augmentation had the highest MRR (0.746). Open-source models, when fine-tuned or augmented, outperformed closed-source models. Notably, the best F1 and MRR scores did not always align. Few-shot prompting outperformed zero-shot in vanilla models, and structured prompts yielded different preferences across models. Fine-tuning improved zero-shot performance but sometimes degraded few-shot performance. Data augmentation performed comparably or better than other methods. Our evaluation highlights the effectiveness of prompting, fine-tuning, and data augmentation in improving model performance for medical jargon extraction in low-resource scenarios.

### KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse 
[[arxiv](https://arxiv.org/abs/2502.16002)] [[cool](https://papers.cool/arxiv/2502.16002)] [[pdf](https://arxiv.org/pdf/2502.16002)]
> **Authors**: Jingbo Yang,Bairu Hou,Wei Wei,Yujia Bao,Shiyu Chang
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: We describe KVLink, an approach for efficient key-value (KV) cache reuse in large language models (LLMs). In many LLM applications, different inputs can share overlapping context, such as the same retrieved document appearing in multiple queries. However, the LLMs still need to encode the entire context for each query, leading to redundant computation. In this paper, we propose a new strategy to eliminate such inefficiency, where the KV cache of each document is precomputed independently. During inference, the KV caches of retrieved documents are concatenated, allowing the model to reuse cached representations instead of recomputing them. To mitigate the performance degradation of LLMs when using KV caches computed independently for each document, KVLink introduces three key components: adjusting positional embeddings of the KV cache at inference to match the global position after concatenation, using trainable special tokens to restore self-attention across independently encoded documents, and applying mixed-data fine-tuning to enhance performance while preserving the model's original capabilities. Experiments across 7 datasets demonstrate that KVLink improves question answering accuracy by an average of 4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV caches, our approach reduces time-to-first-token by up to 90% compared to standard LLM inference, making it a scalable and efficient solution for context reuse.

### Med-gte-hybrid: A contextual embedding transformer model for extracting actionable information from clinical texts 
[[arxiv](https://arxiv.org/abs/2502.15996)] [[cool](https://papers.cool/arxiv/2502.15996)] [[pdf](https://arxiv.org/pdf/2502.15996)]
> **Authors**: Aditya Kumar,Simon Rauch,Mario Cypko,Oliver Amft
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 11 pages, 4 figures, 2 tables
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: We introduce a novel contextual embedding model med-gte-hybrid that was derived from the gte-large sentence transformer to extract information from unstructured clinical narratives. Our model tuning strategy for med-gte-hybrid combines contrastive learning and a denoising autoencoder. To evaluate the performance of med-gte-hybrid, we investigate several clinical prediction tasks in large patient cohorts extracted from the MIMIC-IV dataset, including Chronic Kidney Disease (CKD) patient prognosis, estimated glomerular filtration rate (eGFR) prediction, and patient mortality prediction. Furthermore, we demonstrate that the med-gte-hybrid model improves patient stratification, clustering, and text retrieval, thus outperforms current state-of-the-art models on the Massive Text Embedding Benchmark (MTEB). While some of our evaluations focus on CKD, our hybrid tuning of sentence transformers could be transferred to other medical domains and has the potential to improve clinical decision-making and personalised treatment pathways in various healthcare applications.

### Sparsity May Be All You Need: Sparse Random Parameter Adaptation 
[[arxiv](https://arxiv.org/abs/2502.15975)] [[cool](https://papers.cool/arxiv/2502.15975)] [[pdf](https://arxiv.org/pdf/2502.15975)]
> **Authors**: Jesus Rios,Pierre Dognin,Ronny Luss,Karthikeyan N. Ramamurthy
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Full fine-tuning of large language models for alignment and task adaptation has become prohibitively expensive as models have grown in size. Parameter-Efficient Fine-Tuning (PEFT) methods aim at significantly reducing the computational and memory resources needed for fine-tuning these models by only training on a small number of parameters instead of all model parameters. Currently, the most popular PEFT method is the Low-Rank Adaptation (LoRA), which freezes the parameters of the model to be fine-tuned and introduces a small set of trainable parameters in the form of low-rank matrices. We propose simply reducing the number of trainable parameters by randomly selecting a small proportion of the model parameters to train on. In this paper, we compare the efficiency and performance of our proposed approach with PEFT methods, including LoRA, as well as full parameter fine-tuning.

### R$^3$Mem: Bridging Memory Retention and Retrieval via Reversible Compression 
[[arxiv](https://arxiv.org/abs/2502.15957)] [[cool](https://papers.cool/arxiv/2502.15957)] [[pdf](https://arxiv.org/pdf/2502.15957)]
> **Authors**: Xiaoqiang Wang,Suyuchen Wang,Yun Zhu,Bang Liu
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Work in progress
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Memory plays a key role in enhancing LLMs' performance when deployed to real-world applications. Existing solutions face trade-offs: explicit memory designs based on external storage require complex management and incur storage overhead, while implicit memory designs that store information via parameters struggle with reliable retrieval. In this paper, we propose R$^3$Mem, a memory network that optimizes both information Retention and Retrieval through Reversible context compression. Specifically, R$^3$Mem employs virtual memory tokens to compress and encode infinitely long histories, further enhanced by a hierarchical compression strategy that refines information from document- to entity-level for improved assimilation across granularities. For retrieval, R$^3$Mem employs a reversible architecture, reconstructing raw data by invoking the model backward with compressed information. Implemented via parameter-efficient fine-tuning, it can integrate seamlessly with any Transformer-based model. Experiments demonstrate that our memory design achieves state-of-the-art performance in long-context language modeling and retrieval-augmented generation tasks. It also significantly outperforms conventional memory modules in long-horizon interaction tasks like conversational agents, showcasing its potential for next-generation retrieval systems.

### MMRAG: Multi-Mode Retrieval-Augmented Generation with Large Language Models for Biomedical In-Context Learning 
[[arxiv](https://arxiv.org/abs/2502.15954)] [[cool](https://papers.cool/arxiv/2502.15954)] [[pdf](https://arxiv.org/pdf/2502.15954)]
> **Authors**: Zaifu Zhan,Jun Wang,Shuang Zhou,Jiawen Deng,Rui Zhang
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Submitted to JAMIA
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Objective: To optimize in-context learning in biomedical natural language processing by improving example selection. Methods: We introduce a novel multi-mode retrieval-augmented generation (MMRAG) framework, which integrates four retrieval strategies: (1) Random Mode, selecting examples arbitrarily; (2) Top Mode, retrieving the most relevant examples based on similarity; (3) Diversity Mode, ensuring variation in selected examples; and (4) Class Mode, selecting category-representative examples. This study evaluates MMRAG on three core biomedical NLP tasks: Named Entity Recognition (NER), Relation Extraction (RE), and Text Classification (TC). The datasets used include BC2GM for gene and protein mention recognition (NER), DDI for drug-drug interaction extraction (RE), GIT for general biomedical information extraction (RE), and HealthAdvice for health-related text classification (TC). The framework is tested with two large language models (Llama2-7B, Llama3-8B) and three retrievers (Contriever, MedCPT, BGE-Large) to assess performance across different retrieval strategies. Results: The results from the Random mode indicate that providing more examples in the prompt improves the model's generation performance. Meanwhile, Top mode and Diversity mode significantly outperform Random mode on the RE (DDI) task, achieving an F1 score of 0.9669, a 26.4% improvement. Among the three retrievers tested, Contriever outperformed the other two in a greater number of experiments. Additionally, Llama 2 and Llama 3 demonstrated varying capabilities across different tasks, with Llama 3 showing a clear advantage in handling NER tasks. Conclusion: MMRAG effectively enhances biomedical in-context learning by refining example selection, mitigating data scarcity issues, and demonstrating superior adaptability for NLP-driven healthcare applications.

### AutoMedPrompt: A New Framework for Optimizing LLM Medical Prompts Using Textual Gradients 
[[arxiv](https://arxiv.org/abs/2502.15944)] [[cool](https://papers.cool/arxiv/2502.15944)] [[pdf](https://arxiv.org/pdf/2502.15944)]
> **Authors**: Sean Wu,Michael Koo,Fabien Scalzo,Ira Kurtz
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 14 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) have demonstrated increasingly sophisticated performance in medical and other fields of knowledge. Traditional methods of creating specialist LLMs require extensive fine-tuning and training of models on large datasets. Recently, prompt engineering, instead of fine-tuning, has shown potential to boost the performance of general foundation models. However, prompting methods such as chain-of-thought (CoT) may not be suitable for all subspecialty, and k-shot approaches may introduce irrelevant tokens into the context space. We present AutoMedPrompt, which explores the use of textual gradients to elicit medically relevant reasoning through system prompt optimization. AutoMedPrompt leverages TextGrad's automatic differentiation via text to improve the ability of general foundation LLMs. We evaluated AutoMedPrompt on Llama 3, an open-source LLM, using several QA benchmarks, including MedQA, PubMedQA, and the nephrology subspecialty-specific NephSAP. Our results show that prompting with textual gradients outperforms previous methods on open-source LLMs and surpasses proprietary models such as GPT-4, Claude 3 Opus, and Med-PaLM 2. AutoMedPrompt sets a new state-of-the-art (SOTA) performance on PubMedQA with an accuracy of 82.6$\%$, while also outperforming previous prompting strategies on open-sourced models for MedQA (77.7$\%$) and NephSAP (63.8$\%$).

### CVE-LLM : Ontology-Assisted Automatic Vulnerability Evaluation Using Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.15932)] [[cool](https://papers.cool/arxiv/2502.15932)] [[pdf](https://arxiv.org/pdf/2502.15932)]
> **Authors**: Rikhiya Ghosh,Hans-Martin von Stockhausen,Martin Schmitt,George Marica Vasile,Sanjeev Kumar Karn,Oladimeji Farri
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: arXiv admin note: substantial text overlap with arXiv:2407.14640
- **标题**: None
- **领域**: 计算语言学,密码学和安全
- **Abstract**: The National Vulnerability Database (NVD) publishes over a thousand new vulnerabilities monthly, with a projected 25 percent increase in 2024, highlighting the crucial need for rapid vulnerability identification to mitigate cybersecurity attacks and save costs and resources. In this work, we propose using large language models (LLMs) to learn vulnerability evaluation from historical assessments of medical device vulnerabilities in a single manufacturer's portfolio. We highlight the effectiveness and challenges of using LLMs for automatic vulnerability evaluation and introduce a method to enrich historical data with cybersecurity ontologies, enabling the system to understand new vulnerabilities without retraining the LLM. Our LLM system integrates with the in-house application - Cybersecurity Management System (CSMS) - to help Siemens Healthineers (SHS) product cybersecurity experts efficiently assess the vulnerabilities in our products. Also, we present guidelines for efficient integration of LLMs into the cybersecurity tool.

### Improving Consistency in Large Language Models through Chain of Guidance 
[[arxiv](https://arxiv.org/abs/2502.15924)] [[cool](https://papers.cool/arxiv/2502.15924)] [[pdf](https://arxiv.org/pdf/2502.15924)]
> **Authors**: Harsh Raj,Vipul Gupta,Domenic Rosati,Subhabrata Majumdar
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Accepted at Transactions ofMachineLearningResearch (TMLR) 2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Consistency is a fundamental dimension of trustworthiness in Large Language Models (LLMs). For humans to be able to trust LLM-based applications, their outputs should be consistent when prompted with inputs that carry the same meaning or intent. Despite this need, there is no known mechanism to control and guide LLMs to be more consistent at inference time. In this paper, we introduce a novel alignment strategy to maximize semantic consistency in LLM outputs. Our proposal is based on Chain of Guidance (CoG), a multistep prompting technique that generates highly consistent outputs from LLMs. For closed-book question-answering (Q&A) tasks, when compared to direct prompting, the outputs generated using CoG show improved consistency. While other approaches like template-based responses and majority voting may offer alternative paths to consistency, our work focuses on exploring the potential of guided prompting. We use synthetic data sets comprised of consistent input-output pairs to fine-tune LLMs to produce consistent and correct outputs. Our fine-tuned models are more than twice as consistent compared to base models and show strong generalization capabilities by producing consistent outputs over datasets not used in the fine-tuning process.

### Self-Taught Agentic Long Context Understanding 
[[arxiv](https://arxiv.org/abs/2502.15920)] [[cool](https://papers.cool/arxiv/2502.15920)] [[pdf](https://arxiv.org/pdf/2502.15920)]
> **Authors**: Yufan Zhuang,Xiaodong Yu,Jialian Wu,Ximeng Sun,Ze Wang,Jiang Liu,Yusheng Su,Jingbo Shang,Zicheng Liu,Emad Barsoum
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Answering complex, long-context questions remains a major challenge for large language models (LLMs) as it requires effective question clarifications and context retrieval. We propose Agentic Long-Context Understanding (AgenticLU), a framework designed to enhance an LLM's understanding of such queries by integrating targeted self-clarification with contextual grounding within an agentic workflow. At the core of AgenticLU is Chain-of-Clarifications (CoC), where models refine their understanding through self-generated clarification questions and corresponding contextual groundings. By scaling inference as a tree search where each node represents a CoC step, we achieve 97.8% answer recall on NarrativeQA with a search depth of up to three and a branching factor of eight. To amortize the high cost of this search process to training, we leverage the preference pairs for each step obtained by the CoC workflow and perform two-stage model finetuning: (1) supervised finetuning to learn effective decomposition strategies, and (2) direct preference optimization to enhance reasoning quality. This enables AgenticLU models to generate clarifications and retrieve relevant context effectively and efficiently in a single inference pass. Extensive experiments across seven long-context tasks demonstrate that AgenticLU significantly outperforms state-of-the-art prompting methods and specialized long-context LLMs, achieving robust multi-hop reasoning while sustaining consistent performance as context length grows.

### Mind the Gap! Static and Interactive Evaluations of Large Audio Models 
[[arxiv](https://arxiv.org/abs/2502.15919)] [[cool](https://papers.cool/arxiv/2502.15919)] [[pdf](https://arxiv.org/pdf/2502.15919)]
> **Authors**: Minzhi Li,William Barr Held,Michael J Ryan,Kunat Pipatanakul,Potsawee Manakul,Hao Zhu,Diyi Yang
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,声音,音频和语音处理
- **Abstract**: As AI chatbots become ubiquitous, voice interaction presents a compelling way to enable rapid, high-bandwidth communication for both semantic and social signals. This has driven research into Large Audio Models (LAMs) to power voice-native experiences. However, aligning LAM development with user goals requires a clear understanding of user needs and preferences to establish reliable progress metrics. This study addresses these challenges by introducing an interactive approach to evaluate LAMs and collecting 7,500 LAM interactions from 484 participants. Through topic modeling of user queries, we identify primary use cases for audio interfaces. We then analyze user preference rankings and qualitative feedback to determine which models best align with user needs. Finally, we evaluate how static benchmarks predict interactive performance - our analysis reveals no individual benchmark strongly correlates with interactive results ($τ\leq 0.33$ for all benchmarks). While combining multiple coarse-grained features yields modest predictive power ($R^2$=$0.30$), only two out of twenty datasets on spoken question answering and age prediction show significantly positive correlations. This suggests a clear need to develop LAM evaluations that better correlate with user preferences.

### Modality-Aware Neuron Pruning for Unlearning in Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.15910)] [[cool](https://papers.cool/arxiv/2502.15910)] [[pdf](https://arxiv.org/pdf/2502.15910)]
> **Authors**: Zheyuan Liu,Guangyao Dou,Xiangchi Yuan,Chunhui Zhang,Zhaoxuan Tan,Meng Jiang
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 19 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Generative models such as Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) trained on massive datasets can lead them to memorize and inadvertently reveal sensitive information, raising ethical and privacy concerns. While some prior works have explored this issue in the context of LLMs, it presents a unique challenge for MLLMs due to the entangled nature of knowledge across modalities, making comprehensive unlearning more difficult. To address this challenge, we propose Modality Aware Neuron Unlearning (MANU), a novel unlearning framework for MLLMs designed to selectively clip neurons based on their relative importance to the targeted forget data, curated for different modalities. Specifically, MANU consists of two stages: important neuron selection and selective pruning. The first stage identifies and collects the most influential neurons across modalities relative to the targeted forget knowledge, while the second stage is dedicated to pruning those selected neurons. MANU effectively isolates and removes the neurons that contribute most to the forget data within each modality, while preserving the integrity of retained knowledge. Our experiments conducted across various MLLM architectures illustrate that MANU can achieve a more balanced and comprehensive unlearning in each modality without largely affecting the overall model utility.

### A Close Look at Decomposition-based XAI-Methods for Transformer Language Models 
[[arxiv](https://arxiv.org/abs/2502.15886)] [[cool](https://papers.cool/arxiv/2502.15886)] [[pdf](https://arxiv.org/pdf/2502.15886)]
> **Authors**: Leila Arras,Bruno Puri,Patrick Kahardipraja,Sebastian Lapuschkin,Wojciech Samek
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 9 pages, 3 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Various XAI attribution methods have been recently proposed for the transformer architecture, allowing for insights into the decision-making process of large language models by assigning importance scores to input tokens and intermediate representations. One class of methods that seems very promising in this direction includes decomposition-based approaches, i.e., XAI-methods that redistribute the model's prediction logit through the network, as this value is directly related to the prediction. In the previous literature we note though that two prominent methods of this category, namely ALTI-Logit and LRP, have not yet been analyzed in juxtaposition and hence we propose to close this gap by conducting a careful quantitative evaluation w.r.t. ground truth annotations on a subject-verb agreement task, as well as various qualitative inspections, using BERT, GPT-2 and LLaMA-3 as a testbed. Along the way we compare and extend the ALTI-Logit and LRP methods, including the recently proposed AttnLRP variant, from an algorithmic and implementation perspective. We further incorporate in our benchmark two widely-used gradient-based attribution techniques. Finally, we make our carefullly constructed benchmark dataset for evaluating attributions on language models, as well as our code, publicly available in order to foster evaluation of XAI-methods on a well-defined common ground.

### MutaGReP: Execution-Free Repository-Grounded Plan Search for Code-Use 
[[arxiv](https://arxiv.org/abs/2502.15872)] [[cool](https://papers.cool/arxiv/2502.15872)] [[pdf](https://arxiv.org/pdf/2502.15872)]
> **Authors**: Zaid Khan,Ali Farhadi,Ranjay Krishna,Luca Weihs,Mohit Bansal,Tanmay Gupta
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Project page: zaidkhan.me/MutaGReP
- **标题**: None
- **领域**: 计算语言学,人工智能,软件工程
- **Abstract**: When a human requests an LLM to complete a coding task using functionality from a large code repository, how do we provide context from the repo to the LLM? One approach is to add the entire repo to the LLM's context window. However, most tasks involve only fraction of symbols from a repo, longer contexts are detrimental to the LLM's reasoning abilities, and context windows are not unlimited. Alternatively, we could emulate the human ability to navigate a large repo, pick out the right functionality, and form a plan to solve the task. We propose MutaGReP (Mutation-guided Grounded Repository Plan Search), an approach to search for plans that decompose a user request into natural language steps grounded in the codebase. MutaGReP performs neural tree search in plan space, exploring by mutating plans and using a symbol retriever for grounding. On the challenging LongCodeArena benchmark, our plans use less than 5% of the 128K context window for GPT-4o but rival the coding performance of GPT-4o with a context window filled with the repo. Plans produced by MutaGReP allow Qwen 2.5 Coder 32B and 72B to match the performance of GPT-4o with full repo context and enable progress on the hardest LongCodeArena tasks. Project page: zaidkhan.me/MutaGReP

### Synthetic vs. Gold: The Role of LLM-Generated Labels and Data in Cyberbullying Detection 
[[arxiv](https://arxiv.org/abs/2502.15860)] [[cool](https://papers.cool/arxiv/2502.15860)] [[pdf](https://arxiv.org/pdf/2502.15860)]
> **Authors**: Arefeh Kazemi,Sri Balaaji Natarajan Kalaivendan,Joachim Wagner,Hamza Qadeer,Brian Davis
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: This study investigates the role of LLM-generated synthetic data in cyberbullying detection. We conduct a series of experiments where we replace some or all of the authentic data with synthetic data, or augment the authentic data with synthetic data. We find that synthetic cyberbullying data can be the basis for training a classifier for harm detection that reaches performance close to that of a classifier trained with authentic data. Combining authentic with synthetic data shows improvements over the baseline of training on authentic data alone for the test data for all three LLMs tried. These results highlight the viability of synthetic data as a scalable, ethically viable alternative in cyberbullying detection while emphasizing the critical impact of LLM selection on performance outcomes.

### PPC-GPT: Federated Task-Specific Compression of Large Language Models via Pruning and Chain-of-Thought Distillation 
[[arxiv](https://arxiv.org/abs/2502.15857)] [[cool](https://papers.cool/arxiv/2502.15857)] [[pdf](https://arxiv.org/pdf/2502.15857)]
> **Authors**: Tao Fan,Guoqiang Ma,Yuanfeng Song,Lixin Fan,Kai Chen,Qiang Yang
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Compressing Large Language Models (LLMs) into task-specific Small Language Models (SLMs) encounters two significant challenges: safeguarding domain-specific knowledge privacy and managing limited resources. To tackle these challenges, we propose PPC-GPT, a innovative privacy-preserving federated framework specifically designed for compressing LLMs into task-specific SLMs via pruning and Chain-of-Thought (COT) distillation. PPC-GPT works on a server-client federated architecture, where the client sends differentially private (DP) perturbed task-specific data to the server's LLM. The LLM then generates synthetic data along with their corresponding rationales. This synthetic data is subsequently used for both LLM pruning and retraining processes. Additionally, we harness COT knowledge distillation, leveraging the synthetic data to further improve the retraining of structurally-pruned SLMs. Our experimental results demonstrate the effectiveness of PPC-GPT across various text generation tasks. By compressing LLMs into task-specific SLMs, PPC-GPT not only achieves competitive performance but also prioritizes data privacy protection.

### Control Illusion: The Failure of Instruction Hierarchies in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.15851)] [[cool](https://papers.cool/arxiv/2502.15851)] [[pdf](https://arxiv.org/pdf/2502.15851)]
> **Authors**: Yilin Geng,Haonan Li,Honglin Mu,Xudong Han,Timothy Baldwin,Omri Abend,Eduard Hovy,Lea Frermann
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large language models (LLMs) are increasingly deployed with hierarchical instruction schemes, where certain instructions (e.g., system-level directives) are expected to take precedence over others (e.g., user messages). Yet, we lack a systematic understanding of how effectively these hierarchical control mechanisms work. We introduce a systematic evaluation framework based on constraint prioritization to assess how well LLMs enforce instruction hierarchies. Our experiments across six state-of-the-art LLMs reveal that models struggle with consistent instruction prioritization, even for simple formatting conflicts. We find that the widely-adopted system/user prompt separation fails to establish a reliable instruction hierarchy, and models exhibit strong inherent biases toward certain constraint types regardless of their priority designation. While controlled prompt engineering and model fine-tuning show modest improvements, our results indicate that instruction hierarchy enforcement is not robustly realized, calling for deeper architectural innovations beyond surface-level modifications.

### Forecasting Frontier Language Model Agent Capabilities 
[[arxiv](https://arxiv.org/abs/2502.15850)] [[cool](https://papers.cool/arxiv/2502.15850)] [[pdf](https://arxiv.org/pdf/2502.15850)]
> **Authors**: Govind Pimpale,Axel Højmark,Jérémy Scheurer,Marius Hobbhahn
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: As Language Models (LMs) increasingly operate as autonomous agents, accurately forecasting their capabilities becomes crucial for societal preparedness. We evaluate six forecasting methods that predict downstream capabilities of LM agents. We use "one-step" approaches that predict benchmark scores from input metrics like compute or model release date directly or "two-step" approaches that first predict an intermediate metric like the principal component of cross-benchmark performance (PC-1) and human-evaluated competitive Elo ratings. We evaluate our forecasting methods by backtesting them on a dataset of 38 LMs from the OpenLLM 2 leaderboard. We then use the validated two-step approach (Release Date$\to$Elo$\to$Benchmark) to predict LM agent performance for frontier models on three benchmarks: SWE-Bench Verified (software development), Cybench (cybersecurity assessment), and RE-Bench (ML research engineering). Our forecast predicts that by the beginning of 2026, non-specialized LM agents with low capability elicitation will reach a success rate of 54% on SWE-Bench Verified, while state-of-the-art LM agents will reach an 87% success rate. Our approach does not account for recent advances in inference-compute scaling and might thus be too conservative.

### Verify when Uncertain: Beyond Self-Consistency in Black Box Hallucination Detection 
[[arxiv](https://arxiv.org/abs/2502.15845)] [[cool](https://papers.cool/arxiv/2502.15845)] [[pdf](https://arxiv.org/pdf/2502.15845)]
> **Authors**: Yihao Xue,Kristjan Greenewald,Youssef Mroueh,Baharan Mirzasoleiman
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large Language Models (LLMs) suffer from hallucination problems, which hinder their reliability in sensitive applications. In the black-box setting, several self-consistency-based techniques have been proposed for hallucination detection. We empirically study these techniques and show that they achieve performance close to that of a supervised (still black-box) oracle, suggesting little room for improvement within this paradigm. To address this limitation, we explore cross-model consistency checking between the target model and an additional verifier LLM. With this extra information, we observe improved oracle performance compared to purely self-consistency-based methods. We then propose a budget-friendly, two-stage detection algorithm that calls the verifier model only for a subset of cases. It dynamically switches between self-consistency and cross-consistency based on an uncertainty interval of the self-consistency classifier. We provide a geometric interpretation of consistency-based hallucination detection methods through the lens of kernel mean embeddings, offering deeper theoretical insights. Extensive experiments show that this approach maintains high detection performance while significantly reducing computational cost.

### Hallucination Detection in Large Language Models with Metamorphic Relations 
[[arxiv](https://arxiv.org/abs/2502.15844)] [[cool](https://papers.cool/arxiv/2502.15844)] [[pdf](https://arxiv.org/pdf/2502.15844)]
> **Authors**: Borui Yang,Md Afif Al Mamun,Jie M. Zhang,Gias Uddin
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-24
> **comment**: 21 pages, 11 figures
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Large Language Models (LLMs) are prone to hallucinations, e.g., factually incorrect information, in their responses. These hallucinations present challenges for LLM-based applications that demand high factual accuracy. Existing hallucination detection methods primarily depend on external resources, which can suffer from issues such as low availability, incomplete coverage, privacy concerns, high latency, low reliability, and poor scalability. There are also methods depending on output probabilities, which are often inaccessible for closed-source LLMs like GPT models. This paper presents MetaQA, a self-contained hallucination detection approach that leverages metamorphic relation and prompt mutation. Unlike existing methods, MetaQA operates without any external resources and is compatible with both open-source and closed-source LLMs. MetaQA is based on the hypothesis that if an LLM's response is a hallucination, the designed metamorphic relations will be violated. We compare MetaQA with the state-of-the-art zero-resource hallucination detection method, SelfCheckGPT, across multiple datasets, and on two open-source and two closed-source LLMs. Our results reveal that MetaQA outperforms SelfCheckGPT in terms of precision, recall, and f1 score. For the four LLMs we study, MetaQA outperforms SelfCheckGPT with a superiority margin ranging from 0.041 - 0.113 (for precision), 0.143 - 0.430 (for recall), and 0.154 - 0.368 (for F1-score). For instance, with Mistral-7B, MetaQA achieves an average F1-score of 0.435, compared to SelfCheckGPT's F1-score of 0.205, representing an improvement rate of 112.2%. MetaQA also demonstrates superiority across all different categories of questions.

### Soft Token Attacks Cannot Reliably Audit Unlearning in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.15836)] [[cool](https://papers.cool/arxiv/2502.15836)] [[pdf](https://arxiv.org/pdf/2502.15836)]
> **Authors**: Haokun Chen,Sebastian Szyller,Weilin Xu,Nageen Himayat
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large language models (LLMs) have become increasingly popular. Their emergent capabilities can be attributed to their massive training datasets. However, these datasets often contain undesirable or inappropriate content, e.g., harmful texts, personal information, and copyrighted material. This has promoted research into machine unlearning that aims to remove information from trained models. In particular, approximate unlearning seeks to achieve information removal by strategically editing the model rather than complete model retraining. Recent work has shown that soft token attacks (STA) can successfully extract purportedly unlearned information from LLMs, thereby exposing limitations in current unlearning methodologies. In this work, we reveal that STAs are an inadequate tool for auditing unlearning. Through systematic evaluation on common unlearning benchmarks (Who Is Harry Potter? and TOFU), we demonstrate that such attacks can elicit any information from the LLM, regardless of (1) the deployed unlearning algorithm, and (2) whether the queried content was originally present in the training corpus. Furthermore, we show that STA with just a few soft tokens (1-10) can elicit random strings over 400-characters long. Thus showing that STAs are too powerful, and misrepresent the effectiveness of the unlearning methods. Our work highlights the need for better evaluation baselines, and more appropriate auditing tools for assessing the effectiveness of unlearning in LLMs.

### Pragmatic Reasoning improves LLM Code Generation 
[[arxiv](https://arxiv.org/abs/2502.15835)] [[cool](https://papers.cool/arxiv/2502.15835)] [[pdf](https://arxiv.org/pdf/2502.15835)]
> **Authors**: Zhuchen Cao,Sven Apel,Adish Singla,Vera Demberg
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,软件工程
- **Abstract**: Large Language Models (LLMs) have demonstrated impressive potential in translating natural language (NL) instructions into program code. However, user instructions often contain inherent ambiguities, making it challenging for LLMs to generate code that accurately reflects the user's true intent. To address this challenge, researchers have proposed to produce multiple candidates of the program code and then rerank them to identify the best solution. In this paper, we propose CodeRSA, a novel code candidate reranking mechanism built upon the Rational Speech Act (RSA) framework, designed to guide LLMs toward more comprehensive pragmatic reasoning about user intent. We evaluate CodeRSA using one of the latest LLMs on a popular code generation dataset. Our experiment results show that CodeRSA consistently outperforms common baselines, surpasses the state-of-the-art approach in most cases, and demonstrates robust overall performance. These findings underscore the effectiveness of integrating pragmatic reasoning into code candidate reranking, offering a promising direction for enhancing code generation quality in LLMs.

### CoME: An Unlearning-based Approach to Conflict-free Model Editing 
[[arxiv](https://arxiv.org/abs/2502.15826)] [[cool](https://papers.cool/arxiv/2502.15826)] [[pdf](https://arxiv.org/pdf/2502.15826)]
> **Authors**: Dahyun Jung,Jaehyung Seo,Jaewook Lee,Chanjun Park,Heuiseok Lim
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-24
> **comment**: Accepted to NAACL 2025 main conference
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large language models (LLMs) often retain outdated or incorrect information from pre-training, which undermines their reliability. While model editing methods have been developed to address such errors without full re-training, they frequently suffer from knowledge conflicts, where outdated information interferes with new knowledge. In this work, we propose Conflict-free Model Editing (CoME), a novel framework that enhances the accuracy of knowledge updates in LLMs by selectively removing outdated knowledge. CoME leverages unlearning to mitigate knowledge interference, allowing new information to be integrated without compromising relevant linguistic features. Through experiments on GPT-J and LLaMA-3 using Counterfact and ZsRE datasets, we demonstrate that CoME improves both editing accuracy and model reliability when applied to existing editing methods. Our results highlight that the targeted removal of outdated knowledge is crucial for enhancing model editing effectiveness and maintaining the model's generative performance.

### Towards Robust ESG Analysis Against Greenwashing Risks: Aspect-Action Analysis with Cross-Category Generalization 
[[arxiv](https://arxiv.org/abs/2502.15821)] [[cool](https://papers.cool/arxiv/2502.15821)] [[pdf](https://arxiv.org/pdf/2502.15821)]
> **Authors**: Keane Ong,Rui Mao,Deeksha Varshney,Erik Cambria,Gianmarco Mengaldo
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Sustainability reports are key for evaluating companies' environmental, social and governance, ESG performance, but their content is increasingly obscured by greenwashing - sustainability claims that are misleading, exaggerated, and fabricated. Yet, existing NLP approaches for ESG analysis lack robustness against greenwashing risks, often extracting insights that reflect misleading or exaggerated sustainability claims rather than objective ESG performance. To bridge this gap, we introduce A3CG - Aspect-Action Analysis with Cross-Category Generalization, as a novel dataset to improve the robustness of ESG analysis amid the prevalence of greenwashing. By explicitly linking sustainability aspects with their associated actions, A3CG facilitates a more fine-grained and transparent evaluation of sustainability claims, ensuring that insights are grounded in verifiable actions rather than vague or misleading rhetoric. Additionally, A3CG emphasizes cross-category generalization. This ensures robust model performance in aspect-action analysis even when companies change their reports to selectively favor certain sustainability areas. Through experiments on A3CG, we analyze state-of-the-art supervised models and LLMs, uncovering their limitations and outlining key directions for future research.

### Tabular Embeddings for Tables with Bi-Dimensional Hierarchical Metadata and Nesting 
[[arxiv](https://arxiv.org/abs/2502.15819)] [[cool](https://papers.cool/arxiv/2502.15819)] [[pdf](https://arxiv.org/pdf/2502.15819)]
> **Authors**: Gyanendra Shrestha,Chutain Jiang,Sai Akula,Vivek Yannam,Anna Pyayt,Michael Gubanov
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-24
> **comment**: ef:EDBT 2025, pp. 92-105
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Embeddings serve as condensed vector representations for real-world entities, finding applications in Natural Language Processing (NLP), Computer Vision, and Data Management across diverse downstream tasks. Here, we introduce novel specialized embeddings optimized, and explicitly tailored to encode the intricacies of complex 2-D context in tables, featuring horizontal, vertical hierarchical metadata, and nesting. To accomplish that we define the Bi-dimensional tabular coordinates, separate horizontal, vertical metadata and data contexts by introducing a new visibility matrix, encode units and nesting through the embeddings specifically optimized for mimicking intricacies of such complex structured data. Through evaluation on 5 large-scale structured datasets and 3 popular downstream tasks, we observed that our solution outperforms the state-of-the-art models with the significant MAP delta of up to 0.28. GPT-4 LLM+RAG slightly outperforms us with MRR delta of up to 0.1, while we outperform it with the MAP delta of up to 0.42.

### Zero-Shot Commonsense Validation and Reasoning with Large Language Models: An Evaluation on SemEval-2020 Task 4 Dataset 
[[arxiv](https://arxiv.org/abs/2502.15810)] [[cool](https://papers.cool/arxiv/2502.15810)] [[pdf](https://arxiv.org/pdf/2502.15810)]
> **Authors**: Rawand Alfugaha,Mohammad AL-Smadi
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: This study evaluates the performance of Large Language Models (LLMs) on SemEval-2020 Task 4 dataset, focusing on commonsense validation and explanation. Our methodology involves evaluating multiple LLMs, including LLaMA3-70B, Gemma2-9B, and Mixtral-8x7B, using zero-shot prompting techniques. The models are tested on two tasks: Task A (Commonsense Validation), where models determine whether a statement aligns with commonsense knowledge, and Task B (Commonsense Explanation), where models identify the reasoning behind implausible statements. Performance is assessed based on accuracy, and results are compared to fine-tuned transformer-based models. The results indicate that larger models outperform previous models and perform closely to human evaluation for Task A, with LLaMA3-70B achieving the highest accuracy of 98.40% in Task A whereas, lagging behind previous models with 93.40% in Task B. However, while models effectively identify implausible statements, they face challenges in selecting the most relevant explanation, highlighting limitations in causal and inferential reasoning.

### On the Effectiveness of Large Language Models in Automating Categorization of Scientific Texts 
[[arxiv](https://arxiv.org/abs/2502.15745)] [[cool](https://papers.cool/arxiv/2502.15745)] [[pdf](https://arxiv.org/pdf/2502.15745)]
> **Authors**: Gautam Kishore Shahi,Oliver Hummel
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,数字图书馆,机器学习
- **Abstract**: The rapid advancement of Large Language Models (LLMs) has led to a multitude of application opportunities. One traditional task for Information Retrieval systems is the summarization and classification of texts, both of which are important for supporting humans in navigating large literature bodies as they e.g. exist with scientific publications. Due to this rapidly growing body of scientific knowledge, recent research has been aiming at building research information systems that not only offer traditional keyword search capabilities, but also novel features such as the automatic detection of research areas that are present at knowledge intensive organizations in academia and industry. To facilitate this idea, we present the results obtained from evaluating a variety of LLMs in their ability to sort scientific publications into hierarchical classifications systems. Using the FORC dataset as ground truth data, we have found that recent LLMs (such as Meta Llama 3.1) are able to reach an accuracy of up to 0.82, which is up to 0.08 better than traditional BERT models.

### Town Hall Debate Prompting: Enhancing Logical Reasoning in LLMs through Multi-Persona Interaction 
[[arxiv](https://arxiv.org/abs/2502.15725)] [[cool](https://papers.cool/arxiv/2502.15725)] [[pdf](https://arxiv.org/pdf/2502.15725)]
> **Authors**: Vivaan Sandwar,Bhav Jain,Rishan Thangaraj,Ishaan Garg,Michael Lam,Kevin Zhu
> **First submission**: 2025-01-28
> **First announcement**: 2025-02-24
> **comment**: Accepted to SoCal NLP Symposium 2024
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Debate is a commonly used form of human communication catered towards problem-solving because of its efficiency. Debate fundamentally allows multiple viewpoints to be brought up in problem-solving, and for complex problems, each viewpoint opens a new path for problem-solving. In this work, we apply this concept to LLM decision-making by proposing town hall-style debate prompting (THDP), a prompting method that splices a language model into multiple personas that will debate one another to reach a conclusion. Our experimental pipeline varies both the number of personas and the personality types of each persona to find the optimum town hall size and personality for benchmark performance as measured by ZebraLogic bench, a reasoning-intensive benchmark characterized by both multiple-choice and fill-in-the-blank questions. Our experimental results demonstrate that a town hall size of 5 personas with LLM-determined personality types performs optimally on ZebraLogic, achieving a 13\% improvement over one-shot CoT baselines in per-cell accuracy in GPT-4o, 9% puzzle accuracy increase in Claude 3.5 Sonnet, and an improvement in hard puzzle accuracy from 10-15%.

### Integrating Domain Knowledge into Large Language Models for Enhanced Fashion Recommendations 
[[arxiv](https://arxiv.org/abs/2502.15696)] [[cool](https://papers.cool/arxiv/2502.15696)] [[pdf](https://arxiv.org/pdf/2502.15696)]
> **Authors**: Zhan Shi,Shanglin Yang
> **First submission**: 2025-01-03
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,信息检索,机器学习
- **Abstract**: Fashion, deeply rooted in sociocultural dynamics, evolves as individuals emulate styles popularized by influencers and iconic figures. In the quest to replicate such refined tastes using artificial intelligence, traditional fashion ensemble methods have primarily used supervised learning to imitate the decisions of style icons, which falter when faced with distribution shifts, leading to style replication discrepancies triggered by slight variations in input. Meanwhile, large language models (LLMs) have become prominent across various sectors, recognized for their user-friendly interfaces, strong conversational skills, and advanced reasoning capabilities. To address these challenges, we introduce the Fashion Large Language Model (FLLM), which employs auto-prompt generation training strategies to enhance its capacity for delivering personalized fashion advice while retaining essential domain knowledge. Additionally, by integrating a retrieval augmentation technique during inference, the model can better adjust to individual preferences. Our results show that this approach surpasses existing models in accuracy, interpretability, and few-shot learning capabilities.

### Privacy Ripple Effects from Adding or Removing Personal Information in Language Model Training 
[[arxiv](https://arxiv.org/abs/2502.15680)] [[cool](https://papers.cool/arxiv/2502.15680)] [[pdf](https://arxiv.org/pdf/2502.15680)]
> **Authors**: Jaydeep Borkar,Matthew Jagielski,Katherine Lee,Niloofar Mireshghallah,David A. Smith,Christopher A. Choquette-Choo
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 23 pages, 26 figures
- **标题**: None
- **领域**: 计算语言学,密码学和安全
- **Abstract**: Due to the sensitive nature of personally identifiable information (PII), its owners may have the authority to control its inclusion or request its removal from large-language model (LLM) training. Beyond this, PII may be added or removed from training datasets due to evolving dataset curation techniques, because they were newly scraped for retraining, or because they were included in a new downstream fine-tuning stage. We find that the amount and ease of PII memorization is a dynamic property of a model that evolves throughout training pipelines and depends on commonly altered design choices. We characterize three such novel phenomena: (1) similar-appearing PII seen later in training can elicit memorization of earlier-seen sequences in what we call assisted memorization, and this is a significant factor (in our settings, up to 1/3); (2) adding PII can increase memorization of other PII significantly (in our settings, as much as $\approx\!7.5\times$); and (3) removing PII can lead to other PII being memorized. Model creators should consider these first- and second-order privacy risks when training models to avoid the risk of new PII regurgitation.

### FLEKE: Federated Locate-then-Edit Knowledge Editing 
[[arxiv](https://arxiv.org/abs/2502.15677)] [[cool](https://papers.cool/arxiv/2502.15677)] [[pdf](https://arxiv.org/pdf/2502.15677)]
> **Authors**: Zongkai Zhao,Guozeng Xu,Xiuhua Li,Kaiwen Wei,Jiang Zhong
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Locate-then-Edit Knowledge Editing (LEKE) is a key technique for updating large language models (LLMs) without full retraining. However, existing methods assume a single-user setting and become inefficient in real-world multi-client scenarios, where decentralized organizations (e.g., hospitals, financial institutions) independently update overlapping knowledge, leading to redundant mediator knowledge vector (MKV) computations and privacy concerns. To address these challenges, we introduce Federated Locate-then-Edit Knowledge Editing (FLEKE), a novel task that enables multiple clients to collaboratively perform LEKE while preserving privacy and reducing computational overhead. To achieve this, we propose FedEdit, a two-stage framework that optimizes MKV selection and reuse. In the first stage, clients locally apply LEKE and upload the computed MKVs. In the second stage, rather than relying solely on server-based MKV sharing, FLEKE allows clients retrieve relevant MKVs based on cosine similarity, enabling knowledge re-edit and minimizing redundant computations. Experimental results on two benchmark datasets demonstrate that FedEdit retains over 96% of the performance of non-federated LEKE while significantly outperforming a FedAvg-based baseline by approximately twofold. Besides, we find that MEMIT performs more consistently than PMET in the FLEKE task with our FedEdit framework. Our code is available at https://github.com/zongkaiz/FLEKE.

### Almost AI, Almost Human: The Challenge of Detecting AI-Polished Writing 
[[arxiv](https://arxiv.org/abs/2502.15666)] [[cool](https://papers.cool/arxiv/2502.15666)] [[pdf](https://arxiv.org/pdf/2502.15666)]
> **Authors**: Shoumik Saha,Soheil Feizi
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 17 pages, 17 figures
- **标题**: None
- **领域**: 计算语言学,人工智能,人机交互,机器学习
- **Abstract**: The growing use of large language models (LLMs) for text generation has led to widespread concerns about AI-generated content detection. However, an overlooked challenge is AI-polished text, where human-written content undergoes subtle refinements using AI tools. This raises a critical question: should minimally polished text be classified as AI-generated? Misclassification can lead to false plagiarism accusations and misleading claims about AI prevalence in online content. In this study, we systematically evaluate eleven state-of-the-art AI-text detectors using our AI-Polished-Text Evaluation (APT-Eval) dataset, which contains $11.7K$ samples refined at varying AI-involvement levels. Our findings reveal that detectors frequently misclassify even minimally polished text as AI-generated, struggle to differentiate between degrees of AI involvement, and exhibit biases against older and smaller models. These limitations highlight the urgent need for more nuanced detection methodologies.

### Machine-generated text detection prevents language model collapse 
[[arxiv](https://arxiv.org/abs/2502.15654)] [[cool](https://papers.cool/arxiv/2502.15654)] [[pdf](https://arxiv.org/pdf/2502.15654)]
> **Authors**: George Drayson,Vasileios Lampos
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: As Large Language Models (LLMs) become increasingly prevalent, their generated outputs are proliferating across the web, risking a future where machine-generated content dilutes human-authored text. Since web data is the primary resource for LLM pretraining, future models will be trained on an unknown portion of synthetic data. This will lead to model collapse, a degenerative process which causes models to reinforce their own errors and experience a drop in model performance. In this study, we investigate the impact of decoding strategy on model collapse, where we analyse the characteristics of the generated data during recursive training, its similarity to human references and the resulting model performance. Using the decoding strategies that lead to the most significant model degradation, we tackle the question: how to avoid model collapse when the origin (human or synthetic) of the training data is unknown. We design a novel methodology based on resampling the data distribution using importance weights from our machine-generated text detector. Our method is validated on two LLM variants (GPT-2 and SmolLM2) on the open-ended text generation task, demonstrating that we can successfully prevent model collapse and when there is enough human-authored data in the training dataset, our method improves model performance.

### Steering into New Embedding Spaces: Analyzing Cross-Lingual Alignment Induced by Model Interventions in Multilingual Language Models 
[[arxiv](https://arxiv.org/abs/2502.15639)] [[cool](https://papers.cool/arxiv/2502.15639)] [[pdf](https://arxiv.org/pdf/2502.15639)]
> **Authors**: Anirudh Sundar,Sinead Williamson,Katherine Metcalf,Barry-John Theobald,Skyler Seto,Masha Fedzechkina
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 34 pages
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Aligned representations across languages is a desired property in multilingual large language models (mLLMs), as alignment can improve performance in cross-lingual tasks. Typically alignment requires fine-tuning a model, which is computationally expensive, and sizable language data, which often may not be available. A data-efficient alternative to fine-tuning is model interventions -- a method for manipulating model activations to steer generation into the desired direction. We analyze the effect of a popular intervention (finding experts) on the alignment of cross-lingual representations in mLLMs. We identify the neurons to manipulate for a given language and introspect the embedding space of mLLMs pre- and post-manipulation. We show that modifying the mLLM's activations changes its embedding space such that cross-lingual alignment is enhanced. Further, we show that the changes to the embedding space translate into improved downstream performance on retrieval tasks, with up to 2x improvements in top-1 accuracy on cross-lingual retrieval.

### Extraction multi-étiquettes de relations en utilisant des couches de Transformer 
[[arxiv](https://arxiv.org/abs/2502.15619)] [[cool](https://papers.cool/arxiv/2502.15619)] [[pdf](https://arxiv.org/pdf/2502.15619)]
> **Authors**: Ngoc Luyen Le,Gildas Tagny Ngompé
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: in Frenchlanguage
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: In this article, we present the BTransformer18 model, a deep learning architecture designed for multi-label relation extraction in French texts. Our approach combines the contextual representation capabilities of pre-trained language models from the BERT family - such as BERT, RoBERTa, and their French counterparts CamemBERT and FlauBERT - with the power of Transformer encoders to capture long-term dependencies between tokens. Experiments conducted on the dataset from the TextMine'25 challenge show that our model achieves superior performance, particularly when using CamemBERT-Large, with a macro F1 score of 0.654, surpassing the results obtained with FlauBERT-Large. These results demonstrate the effectiveness of our approach for the automatic extraction of complex relations in intelligence reports.

### Probe Pruning: Accelerating LLMs through Dynamic Pruning via Model-Probing 
[[arxiv](https://arxiv.org/abs/2502.15618)] [[cool](https://papers.cool/arxiv/2502.15618)] [[pdf](https://arxiv.org/pdf/2502.15618)]
> **Authors**: Qi Le,Enmao Diao,Ziyan Wang,Xinran Wang,Jie Ding,Li Yang,Ali Anwar
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: ICLR 2025
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: We introduce Probe Pruning (PP), a novel framework for online, dynamic, structured pruning of Large Language Models (LLMs) applied in a batch-wise manner. PP leverages the insight that not all samples and tokens contribute equally to the model's output, and probing a small portion of each batch effectively identifies crucial weights, enabling tailored dynamic pruning for different batches. It comprises three main stages: probing, history-informed pruning, and full inference. In the probing stage, PP selects a small yet crucial set of hidden states, based on residual importance, to run a few model layers ahead. During the history-informed pruning stage, PP strategically integrates the probing states with historical states. Subsequently, it structurally prunes weights based on the integrated states and the PP importance score, a metric developed specifically to assess the importance of each weight channel in maintaining performance. In the final stage, full inference is conducted on the remaining weights. A major advantage of PP is its compatibility with existing models, as it operates without requiring additional neural network modules or fine-tuning. Comprehensive evaluations of PP on LLaMA-2/3 and OPT models reveal that even minimal probing-using just 1.5% of FLOPs-can substantially enhance the efficiency of structured pruning of LLMs. For instance, when evaluated on LLaMA-2-7B with WikiText2, PP achieves a 2.56 times lower ratio of performance degradation per unit of runtime reduction compared to the state-of-the-art method at a 40% pruning ratio. Our code is available at https://github.com/Qi-Le1/Probe_Pruning.

### Pastiche Novel Generation Creating: Fan Fiction You Love in Your Favorite Author's Style 
[[arxiv](https://arxiv.org/abs/2502.15616)] [[cool](https://papers.cool/arxiv/2502.15616)] [[pdf](https://arxiv.org/pdf/2502.15616)]
> **Authors**: Xueran Han,Yuhan Liu,Mingzhe Li,Wei Liu,Sen Hu,Rui Yan,Zhiqiang Xu,Xiuying Chen
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Great novels create immersive worlds with rich character arcs, well-structured plots, and nuanced writing styles. However, current novel generation methods often rely on brief, simplistic story outlines and generate details using plain, generic language. To bridge this gap, we introduce the task of Pastiche Novel Generation, which requires the generated novels to imitate the distinctive features of the original work, including understanding character profiles, predicting plausible plot developments, and writing concrete details using vivid, expressive language. To achieve this, we propose WriterAgent, a novel generation system designed to master the core aspects of literary pastiche. WriterAgent is trained through a curriculum learning paradigm, progressing from low-level stylistic mastery to high-level narrative coherence. Its key tasks include language style learning, character modeling, plot planning, and stylish writing, ensuring comprehensive narrative control. To support this, WriterAgent leverages the WriterLoRA framework, an extension of LoRA with hierarchical and cumulative task-specific modules, each specializing in a different narrative aspect. We evaluate WriterAgent on multilingual classics like Harry Potter and Dream of the Red Chamber, demonstrating its superiority over baselines in capturing the target author's settings, character dynamics, and writing style to produce coherent, faithful narratives.

### LaTIM: Measuring Latent Token-to-Token Interactions in Mamba Models 
[[arxiv](https://arxiv.org/abs/2502.15612)] [[cool](https://papers.cool/arxiv/2502.15612)] [[pdf](https://arxiv.org/pdf/2502.15612)]
> **Authors**: Hugo Pitorro,Marcos Treviso
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 8 pages, 10 figures in the main paper
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: State space models (SSMs), such as Mamba, have emerged as an efficient alternative to transformers for long-context sequence modeling. However, despite their growing adoption, SSMs lack the interpretability tools that have been crucial for understanding and improving attention-based architectures. While recent efforts provide insights into Mamba's internal mechanisms, they do not explicitly decompose token-wise contributions, leaving gaps in understanding how Mamba selectively processes sequences across layers. In this work, we introduce LaTIM, a novel token-level decomposition method for both Mamba-1 and Mamba-2 that enables fine-grained interpretability. We extensively evaluate our method across diverse tasks, including machine translation, copying, and retrieval-based generation, demonstrating its effectiveness in revealing Mamba's token-to-token interaction patterns.

### On the Robustness of Transformers against Context Hijacking for Linear Classification 
[[arxiv](https://arxiv.org/abs/2502.15609)] [[cool](https://papers.cool/arxiv/2502.15609)] [[pdf](https://arxiv.org/pdf/2502.15609)]
> **Authors**: Tianle Li,Chenyang Zhang,Xingwu Chen,Yuan Cao,Difan Zou
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习,机器学习
- **Abstract**: Transformer-based Large Language Models (LLMs) have demonstrated powerful in-context learning capabilities. However, their predictions can be disrupted by factually correct context, a phenomenon known as context hijacking, revealing a significant robustness issue. To understand this phenomenon theoretically, we explore an in-context linear classification problem based on recent advances in linear transformers. In our setup, context tokens are designed as factually correct query-answer pairs, where the queries are similar to the final query but have opposite labels. Then, we develop a general theoretical analysis on the robustness of the linear transformers, which is formulated as a function of the model depth, training context lengths, and number of hijacking context tokens. A key finding is that a well-trained deeper transformer can achieve higher robustness, which aligns with empirical observations. We show that this improvement arises because deeper layers enable more fine-grained optimization steps, effectively mitigating interference from context hijacking. This is also well supported by our numerical experiments. Our findings provide theoretical insights into the benefits of deeper architectures and contribute to enhancing the understanding of transformer architectures.

### Do Multilingual LLMs Think In English? 
[[arxiv](https://arxiv.org/abs/2502.15603)] [[cool](https://papers.cool/arxiv/2502.15603)] [[pdf](https://arxiv.org/pdf/2502.15603)]
> **Authors**: Lisa Schut,Yarin Gal,Sebastian Farquhar
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Main paper 9 pages; including appendix 48 pages
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Large language models (LLMs) have multilingual capabilities and can solve tasks across various languages. However, we show that current LLMs make key decisions in a representation space closest to English, regardless of their input and output languages. Exploring the internal representations with a logit lens for sentences in French, German, Dutch, and Mandarin, we show that the LLM first emits representations close to English for semantically-loaded words before translating them into the target language. We further show that activation steering in these LLMs is more effective when the steering vectors are computed in English rather than in the language of the inputs and outputs. This suggests that multilingual LLMs perform key reasoning steps in a representation that is heavily shaped by English in a way that is not transparent to system users.

### SafeInt: Shielding Large Language Models from Jailbreak Attacks via Safety-Aware Representation Intervention 
[[arxiv](https://arxiv.org/abs/2502.15594)] [[cool](https://papers.cool/arxiv/2502.15594)] [[pdf](https://arxiv.org/pdf/2502.15594)]
> **Authors**: Jiaqi Wu,Chen Chen,Chunyan Hou,Xiaojie Yuan
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: With the widespread real-world deployment of large language models (LLMs), ensuring their behavior complies with safety standards has become crucial. Jailbreak attacks exploit vulnerabilities in LLMs to induce undesirable behavior, posing a significant threat to LLM safety. Previous defenses often fail to achieve both effectiveness and efficiency simultaneously. Defenses from a representation perspective offer new insights, but existing interventions cannot dynamically adjust representations based on the harmfulness of the queries. To address this limitation while ensuring both effectiveness and efficiency, we propose SafeIntervention (SafeInt), a novel defense method that shields LLMs from jailbreak attacks through safety-aware representation intervention. SafeInt is built on our analysis of the representations of jailbreak samples. It adjusts representation distributions of jailbreak samples through intervention to align them with the representations of unsafe samples while minimizing unnecessary perturbations to jailbreak-irrelevant representations. We conduct comprehensive experiments covering six jailbreak attacks, two jailbreak datasets, and two utility benchmarks. Experimental results demonstrate that SafeInt outperforms all baselines in defending LLMs against jailbreak attacks while largely maintaining utility. Additionally, we evaluate SafeInt against adaptive attacks and verify its effectiveness in mitigating real-time attacks.

### Generalizing From Short to Long: Effective Data Synthesis for Long-Context Instruction Tuning 
[[arxiv](https://arxiv.org/abs/2502.15592)] [[cool](https://papers.cool/arxiv/2502.15592)] [[pdf](https://arxiv.org/pdf/2502.15592)]
> **Authors**: Wenhao Zhu,Pinzhen Chen,Hanxu Hu,Shujian Huang,Fei Yuan,Jiajun Chen,Alexandra Birch
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Long-context modelling for large language models (LLMs) has been a key area of recent research because many real world use cases require reasoning over longer inputs such as documents. The focus of research into modelling long context has been on how to model position and there has been little investigation into other important aspects of language modelling such as instruction tuning. Long context training examples are challenging and expensive to create and use. In this paper, we investigate how to design instruction data for the post-training phase of a long context pre-trained model: how much and what type of context is needed for optimal and efficient post-training. Our controlled study reveals that models instruction-tuned on short contexts can effectively generalize to longer ones, while also identifying other critical factors such as instruction difficulty and context composition. Based on these findings, we propose context synthesis, a novel data synthesis framework that leverages off-the-shelf LLMs to generate extended background contexts for high-quality instruction-answer pairs. Experiment results on the document-level benchmark (LongBench) demonstrate that our proposed approach outperforms previous instruction synthesis approaches and comes close to the performance of human-annotated long-context instruction data. The project will be available at: https://github.com/NJUNLP/context-synthesis.

### LightThinker: Thinking Step-by-Step Compression 
[[arxiv](https://arxiv.org/abs/2502.15589)] [[cool](https://papers.cool/arxiv/2502.15589)] [[pdf](https://arxiv.org/pdf/2502.15589)]
> **Authors**: Jintian Zhang,Yuqi Zhu,Mengshu Sun,Yujie Luo,Shuofei Qiao,Lun Du,Da Zheng,Huajun Chen,Ningyu Zhang
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,信息检索,机器学习,多媒体
- **Abstract**: Large language models (LLMs) have shown remarkable performance in complex reasoning tasks, but their efficiency is hindered by the substantial memory and computational costs associated with generating lengthy tokens. In this paper, we propose LightThinker, a novel method that enables LLMs to dynamically compress intermediate thoughts during reasoning. Inspired by human cognitive processes, LightThinker compresses verbose thought steps into compact representations and discards the original reasoning chains, thereby significantly reducing the number of tokens stored in the context window. This is achieved by training the model on when and how to perform compression through data construction, mapping hidden states to condensed gist tokens, and creating specialized attention masks. Additionally, we introduce the Dependency (Dep) metric to quantify the degree of compression by measuring the reliance on historical tokens during generation. Extensive experiments on four datasets and two models show that LightThinker reduces peak memory usage and inference time, while maintaining competitive accuracy. Our work provides a new direction for improving the efficiency of LLMs in complex reasoning tasks without sacrificing performance. Code will be released at https://github.com/zjunlp/LightThinker.

### Chats-Grid: An Iterative Retrieval Q&A Optimization Scheme Leveraging Large Model and Retrieval Enhancement Generation in smart grid 
[[arxiv](https://arxiv.org/abs/2502.15583)] [[cool](https://papers.cool/arxiv/2502.15583)] [[pdf](https://arxiv.org/pdf/2502.15583)]
> **Authors**: Yunfeng Li,Jiqun Zhang,Guofu Liao,Xue Shi,Junhong Liu
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 12 pages, 10 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: With rapid advancements in artificial intelligence, question-answering (Q&A) systems have become essential in intelligent search engines, virtual assistants, and customer service platforms. However, in dynamic domains like smart grids, conventional retrieval-augmented generation(RAG) Q&A systems face challenges such as inadequate retrieval quality, irrelevant responses, and inefficiencies in handling large-scale, real-time data streams. This paper proposes an optimized iterative retrieval-based Q&A framework called Chats-Grid tailored for smart grid environments. In the pre-retrieval phase, Chats-Grid advanced query expansion ensures comprehensive coverage of diverse data sources, including sensor readings, meter records, and control system parameters. During retrieval, Best Matching 25(BM25) sparse retrieval and BAAI General Embedding(BGE) dense retrieval in Chats-Grid are combined to process vast, heterogeneous datasets effectively. Post-retrieval, a fine-tuned large language model uses prompt engineering to assess relevance, filter irrelevant results, and reorder documents based on contextual accuracy. The model further generates precise, context-aware answers, adhering to quality criteria and employing a self-checking mechanism for enhanced reliability. Experimental results demonstrate Chats-Grid's superiority over state-of-the-art methods in fidelity, contextual recall, relevance, and accuracy by 2.37%, 2.19%, and 3.58% respectively. This framework advances smart grid management by improving decision-making and user interactions, fostering resilient and adaptive smart grid infrastructures.

### Interpreting and Steering LLMs with Mutual Information-based Explanations on Sparse Autoencoders 
[[arxiv](https://arxiv.org/abs/2502.15576)] [[cool](https://papers.cool/arxiv/2502.15576)] [[pdf](https://arxiv.org/pdf/2502.15576)]
> **Authors**: Xuansheng Wu,Jiayi Yuan,Wenlin Yao,Xiaoming Zhai,Ninghao Liu
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Pre-print. 20 pages, 5 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) excel at handling human queries, but they can occasionally generate flawed or unexpected responses. Understanding their internal states is crucial for understanding their successes, diagnosing their failures, and refining their capabilities. Although sparse autoencoders (SAEs) have shown promise for interpreting LLM internal representations, limited research has explored how to better explain SAE features, i.e., understanding the semantic meaning of features learned by SAE. Our theoretical analysis reveals that existing explanation methods suffer from the frequency bias issue, where they emphasize linguistic patterns over semantic concepts, while the latter is more critical to steer LLM behaviors. To address this, we propose using a fixed vocabulary set for feature interpretations and designing a mutual information-based objective, aiming to better capture the semantic meaning behind these features. We further propose two runtime steering strategies that adjust the learned feature activations based on their corresponding explanations. Empirical results show that, compared to baselines, our method provides more discourse-level explanations and effectively steers LLM behaviors to defend against jailbreak attacks. These findings highlight the value of explanations for steering LLM behaviors in downstream applications. We will release our code and data once accepted.

### A Survey of QUD Models for Discourse Processing 
[[arxiv](https://arxiv.org/abs/2502.15573)] [[cool](https://papers.cool/arxiv/2502.15573)] [[pdf](https://arxiv.org/pdf/2502.15573)]
> **Authors**: Yingxue Fu
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-24
> **comment**: accepted to the main conference of NAACL2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Question Under Discussion (QUD), which is originally a linguistic analytic framework, gains increasing attention in the community of natural language processing over the years. Various models have been proposed for implementing QUD for discourse processing. This survey summarizes these models, with a focus on application to written texts, and examines studies that explore the relationship between QUD and mainstream discourse frameworks, including RST, PDTB and SDRT. Some questions that may require further study are suggested.

### DReSD: Dense Retrieval for Speculative Decoding 
[[arxiv](https://arxiv.org/abs/2502.15572)] [[cool](https://papers.cool/arxiv/2502.15572)] [[pdf](https://arxiv.org/pdf/2502.15572)]
> **Authors**: Milan Gritta,Huiyin Xue,Gerasimos Lampouras
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Under Review
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Speculative decoding (SD) accelerates Large Language Model (LLM) generation by using an efficient draft model to propose the next few tokens, which are verified by the LLM in a single forward call, reducing latency while preserving its outputs. We focus on retrieval-based SD where the draft model retrieves the next tokens from a non-parametric datastore. Sparse retrieval (REST), which operates on the surface form of strings, is currently the dominant paradigm due to its simplicity and scalability. However, its effectiveness is limited due to the usage of short contexts and exact string matching. Instead, we introduce Dense Retrieval for Speculative Decoding (DReSD), a novel framework that uses approximate nearest neighbour search with contextualised token embeddings to retrieve the most semantically relevant token sequences for SD. Extensive experiments show that DReSD achieves (on average) 87% higher acceptance rates, 65% longer accepted tokens and 19% faster generation speeds compared to sparse retrieval (REST).

### PIP-KAG: Mitigating Knowledge Conflicts in Knowledge-Augmented Generation via Parametric Pruning 
[[arxiv](https://arxiv.org/abs/2502.15543)] [[cool](https://papers.cool/arxiv/2502.15543)] [[pdf](https://arxiv.org/pdf/2502.15543)]
> **Authors**: Pengcheng Huang,Zhenghao Liu,Yukun Yan,Xiaoyuan Yi,Hao Chen,Zhiyuan Liu,Maosong Sun,Tong Xiao,Ge Yu,Chenyan Xiong
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 20 pages, 7 figures, 7 tables
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Knowledge-Augmented Generation (KAG) has shown great promise in updating the internal memory of Large Language Models (LLMs) by integrating external knowledge. However, KAG inevitably faces knowledge conflicts when the internal memory contradicts external information. Current approaches to mitigating these conflicts mainly focus on improving external knowledge utilization. However, these methods have shown only limited effectiveness in mitigating the knowledge conflict problem, as internal knowledge continues to influence the generation process of LLMs. In this paper, we propose a ParametrIc Pruning-based Knowledge-Augmented Generation (PIP-KAG) approach, which prunes internal knowledge of LLMs and incorporates a plug-and-play adaptation module to help LLMs better leverage external sources. Additionally, we construct the CoConflictQA benchmark based on the hallucination of LLMs to better evaluate contextual faithfulness during answering questions. Experimental results on CoConflictQA demonstrate that PIP-KAG significantly reduces knowledge conflicts and improves context fidelity. Notably, PIP-KAG reduces LLM's parameters by 13%, enhancing parameter efficiency in LLMs within the KAG framework. All codes are available at https://github.com/OpenBMB/PIP-KAG.

### SOTOPIA-Ω: Dynamic Strategy Injection Learning and Social Instruction Following Evaluation for Social Agents 
[[arxiv](https://arxiv.org/abs/2502.15538)] [[cool](https://papers.cool/arxiv/2502.15538)] [[pdf](https://arxiv.org/pdf/2502.15538)]
> **Authors**: Wenyuan Zhang,Tianyun Liu,Mengxiao Song,Xiaodong Li,Tingwen Liu
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 26 pages, 5 figures, 23 tables
- **标题**: None
- **领域**: 计算语言学,计算机与社会,人机交互
- **Abstract**: Despite the abundance of prior social strategies possessed by humans, there remains a paucity of research dedicated to their transfer and integration into social agents. Our proposed SOTOPIA-Ω framework aims to address and bridge this gap, with a particular focus on enhancing the social capabilities of language agents. This framework dynamically injects multi-step reasoning strategies inspired by negotiation theory and two simple direct strategies into expert agents, thereby automating the construction of a high-quality social dialogue training corpus. Additionally, we introduce the concept of Social Instruction Following (S-IF) and propose two new S-IF evaluation metrics that complement social capability. We demonstrate that several 7B models trained on high-quality corpus not only significantly surpass the expert agent (GPT-4) in achieving social goals but also enhance S-IF performance. Analysis and variant experiments validate the advantages of dynamic construction, which can especially break the agent's prolonged deadlock.

### Scale-Distribution Decoupling: Enabling Stable and Effective Training of Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.15499)] [[cool](https://papers.cool/arxiv/2502.15499)] [[pdf](https://arxiv.org/pdf/2502.15499)]
> **Authors**: Ya Wang,Zhijian Zhuo,Yutao Zeng,Xun Zhou,Jian Yang,Xiaoqing Li
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Training stability is a persistent challenge in the pre-training of large language models (LLMs), particularly for architectures such as Post-Norm Transformers, which are prone to gradient explosion and dissipation. In this paper, we propose Scale-Distribution Decoupling (SDD), a novel approach that stabilizes training by explicitly decoupling the scale and distribution of the weight matrix in fully-connected layers. SDD applies a normalization mechanism to regulate activations and a learnable scaling vector to maintain well-conditioned gradients, effectively preventing $\textbf{gradient explosion and dissipation}$. This separation improves optimization efficiency, particularly in deep networks, by ensuring stable gradient propagation. Experimental results demonstrate that our method stabilizes training across various LLM architectures and outperforms existing techniques in different normalization configurations. Furthermore, the proposed method is lightweight and compatible with existing frameworks, making it a practical solution for stabilizing LLM training. Code is available at https://github.com/kaihemo/SDD.

### ExpliCa: Evaluating Explicit Causal Reasoning in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.15487)] [[cool](https://papers.cool/arxiv/2502.15487)] [[pdf](https://arxiv.org/pdf/2502.15487)]
> **Authors**: Martina Miliani,Serena Auriemma,Alessandro Bondielli,Emmanuele Chersoni,Lucia Passaro,Irene Sucameli,Alessandro Lenci
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Submitted to ACL 2025
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large Language Models (LLMs) are increasingly used in tasks requiring interpretive and inferential accuracy. In this paper, we introduce ExpliCa, a new dataset for evaluating LLMs in explicit causal reasoning. ExpliCa uniquely integrates both causal and temporal relations presented in different linguistic orders and explicitly expressed by linguistic connectives. The dataset is enriched with crowdsourced human acceptability ratings. We tested LLMs on ExpliCa through prompting and perplexity-based metrics. We assessed seven commercial and open-source LLMs, revealing that even top models struggle to reach 0.80 accuracy. Interestingly, models tend to confound temporal relations with causal ones, and their performance is also strongly influenced by the linguistic order of the events. Finally, perplexity-based scores and prompting performance are differently affected by model size.

### Enhancing RWKV-based Language Models for Long-Sequence Text Generation 
[[arxiv](https://arxiv.org/abs/2502.15485)] [[cool](https://papers.cool/arxiv/2502.15485)] [[pdf](https://arxiv.org/pdf/2502.15485)]
> **Authors**: Xinghan Pan
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 8 pages, 2 tables, 3 figures
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: This paper introduces an enhanced RWKV architecture with adaptive temporal gating mechanisms for improved long-context language modeling. We propose two principal innovations: (1) a position-aware convolutional shift operator that captures local syntactic patterns while preserving global coherence, and (2) a neurally-gated information routing mechanism that dynamically regulates inter-token information flow. Through comprehensive experiments on text generation tasks, our enhanced model demonstrates superior performance compared to the baseline RWKV, achieving 96.5 relative improvement in ROUGE-L scores with only 2.95 increased inference latency. Ablation studies validate the individual contributions of each component, while linguistic analysis reveals the model's adaptive attention to syntactic boundaries and entity coherence. The proposed modifications maintain RWKV's linear computational complexity while significantly enhancing its contextual modeling capabilities, establishing new state-of-the-art performance for recurrent-style architectures in long-form text generation.

### When Compression Meets Model Compression: Memory-Efficient Double Compression for Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.15443)] [[cool](https://papers.cool/arxiv/2502.15443)] [[pdf](https://arxiv.org/pdf/2502.15443)]
> **Authors**: Weilan Wang,Yu Mao,Dongdong Tang,Hongchao Du,Nan Guan,Chun Jason Xue
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large language models (LLMs) exhibit excellent performance in various tasks. However, the memory requirements of LLMs present a great challenge when deploying on memory-limited devices, even for quantized LLMs. This paper introduces a framework to compress LLM after quantization further, achieving about 2.2x compression ratio. A compression-aware quantization is first proposed to enhance model weight compressibility by re-scaling the model parameters before quantization, followed by a pruning method to improve further. Upon this, we notice that decompression can be a bottleneck during practical scenarios. We then give a detailed analysis of the trade-off between memory usage and latency brought by the proposed method. A speed-adaptive method is proposed to overcome it. The experimental results show inference with the compressed model can achieve a 40% reduction in memory size with negligible loss in accuracy and inference speed.

### Mixup Model Merge: Enhancing Model Merging Performance through Randomized Linear Interpolation 
[[arxiv](https://arxiv.org/abs/2502.15434)] [[cool](https://papers.cool/arxiv/2502.15434)] [[pdf](https://arxiv.org/pdf/2502.15434)]
> **Authors**: Yue Zhou,Yi Chang,Yuan Wu
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 15 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Model merging integrates the parameters of multiple models into a unified model, combining their diverse capabilities. Existing model merging methods are often constrained by fixed parameter merging ratios. In this study, we propose Mixup Model Merge (M$^3$), an innovative approach inspired by the Mixup data augmentation technique. This method merges the parameters of two large language models (LLMs) by randomly generating linear interpolation ratios, allowing for a more flexible and comprehensive exploration of the parameter space. Extensive experiments demonstrate the superiority of our proposed M$^3$ method in merging fine-tuned LLMs: (1) it significantly improves performance across multiple tasks, (2) it enhances LLMs' out-of-distribution (OOD) robustness and adversarial robustness, (3) it achieves superior results when combined with sparsification techniques such as DARE, and (4) it offers a simple yet efficient solution that does not require additional computational resources. In conclusion, M$^3$ is a simple yet effective model merging method that significantly enhances the performance of the merged model by randomly generating contribution ratios for two fine-tuned LLMs. The code is available at https://github.com/MLGroupJLU/MixupModelMerge.

### Pub-Guard-LLM: Detecting Fraudulent Biomedical Articles with Reliable Explanations 
[[arxiv](https://arxiv.org/abs/2502.15429)] [[cool](https://papers.cool/arxiv/2502.15429)] [[pdf](https://arxiv.org/pdf/2502.15429)]
> **Authors**: Lihu Chen,Shuojie Fu,Gabriel Freedman,Guy Martin,James Kinross,Uddhav Vaghela,Ovidiu Serban,Francesca Toni
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: long paper under review
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: A significant and growing number of published scientific articles is found to involve fraudulent practices, posing a serious threat to the credibility and safety of research in fields such as medicine. We propose Pub-Guard-LLM, the first large language model-based system tailored to fraud detection of biomedical scientific articles. We provide three application modes for deploying Pub-Guard-LLM: vanilla reasoning, retrieval-augmented generation, and multi-agent debate. Each mode allows for textual explanations of predictions. To assess the performance of our system, we introduce an open-source benchmark, PubMed Retraction, comprising over 11K real-world biomedical articles, including metadata and retraction labels. We show that, across all modes, Pub-Guard-LLM consistently surpasses the performance of various baselines and provides more reliable explanations, namely explanations which are deemed more relevant and coherent than those generated by the baselines when evaluated by multiple assessment methods. By enhancing both detection performance and explainability in scientific fraud detection, Pub-Guard-LLM contributes to safeguarding research integrity with a novel, effective, open-source tool.

### Evaluating Multimodal Generative AI with Korean Educational Standards 
[[arxiv](https://arxiv.org/abs/2502.15422)] [[cool](https://papers.cool/arxiv/2502.15422)] [[pdf](https://arxiv.org/pdf/2502.15422)]
> **Authors**: Sanghee Park,Geewook Kim
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 18 pages; To appear at NAACL 2025 Main Conference (Project page: https://github.com/naver-ai/KoNET )
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别
- **Abstract**: This paper presents the Korean National Educational Test Benchmark (KoNET), a new benchmark designed to evaluate Multimodal Generative AI Systems using Korean national educational tests. KoNET comprises four exams: the Korean Elementary General Educational Development Test (KoEGED), Middle (KoMGED), High (KoHGED), and College Scholastic Ability Test (KoCSAT). These exams are renowned for their rigorous standards and diverse questions, facilitating a comprehensive analysis of AI performance across different educational levels. By focusing on Korean, KoNET provides insights into model performance in less-explored languages. We assess a range of models - open-source, open-access, and closed APIs - by examining difficulties, subject diversity, and human error rates. The code and dataset builder will be made fully open-sourced at https://github.com/naver-ai/KoNET.

### Beyond Translation: LLM-Based Data Generation for Multilingual Fact-Checking 
[[arxiv](https://arxiv.org/abs/2502.15419)] [[cool](https://papers.cool/arxiv/2502.15419)] [[pdf](https://arxiv.org/pdf/2502.15419)]
> **Authors**: Yi-Ling Chung,Aurora Cobo,Pablo Serna
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 15 pages, 1 figure, 18 tables
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机与社会
- **Abstract**: Robust automatic fact-checking systems have the potential to combat online misinformation at scale. However, most existing research primarily focuses on English. In this paper, we introduce MultiSynFact, the first large-scale multilingual fact-checking dataset containing 2.2M claim-source pairs designed to support Spanish, German, English, and other low-resource languages. Our dataset generation pipeline leverages Large Language Models (LLMs), integrating external knowledge from Wikipedia and incorporating rigorous claim validation steps to ensure data quality. We evaluate the effectiveness of MultiSynFact across multiple models and experimental settings. Additionally, we open-source a user-friendly framework to facilitate further research in multilingual fact-checking and dataset generation.

### MHQA: A Diverse, Knowledge Intensive Mental Health Question Answering Challenge for Language Models 
[[arxiv](https://arxiv.org/abs/2502.15418)] [[cool](https://papers.cool/arxiv/2502.15418)] [[pdf](https://arxiv.org/pdf/2502.15418)]
> **Authors**: Suraj Racha,Prashant Joshi,Anshika Raman,Nikita Jangid,Mridul Sharma,Ganesh Ramakrishnan,Nirmal Punjabi
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Mental health remains a challenging problem all over the world, with issues like depression, anxiety becoming increasingly common. Large Language Models (LLMs) have seen a vast application in healthcare, specifically in answering medical questions. However, there is a lack of standard benchmarking datasets for question answering (QA) in mental health. Our work presents a novel multiple choice dataset, MHQA (Mental Health Question Answering), for benchmarking Language models (LMs). Previous mental health datasets have focused primarily on text classification into specific labels or disorders. MHQA, on the other hand, presents question-answering for mental health focused on four key domains: anxiety, depression, trauma, and obsessive/compulsive issues, with diverse question types, namely, factoid, diagnostic, prognostic, and preventive. We use PubMed abstracts as the primary source for QA. We develop a rigorous pipeline for LLM-based identification of information from abstracts based on various selection criteria and converting it into QA pairs. Further, valid QA pairs are extracted based on post-hoc validation criteria. Overall, our MHQA dataset consists of 2,475 expert-verified gold standard instances called MHQA-gold and ~56.1k pairs pseudo labeled using external medical references. We report F1 scores on different LLMs along with few-shot and supervised fine-tuning experiments, further discussing the insights for the scores.

### Textual-to-Visual Iterative Self-Verification for Slide Generation 
[[arxiv](https://arxiv.org/abs/2502.15412)] [[cool](https://papers.cool/arxiv/2502.15412)] [[pdf](https://arxiv.org/pdf/2502.15412)]
> **Authors**: Yunqing Xu,Xinbei Ma,Jiyang Qiu,Hai Zhao
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Work in progress
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Generating presentation slides is a time-consuming task that urgently requires automation. Due to their limited flexibility and lack of automated refinement mechanisms, existing autonomous LLM-based agents face constraints in real-world applicability. We decompose the task of generating missing presentation slides into two key components: content generation and layout generation, aligning with the typical process of creating academic slides. First, we introduce a content generation approach that enhances coherence and relevance by incorporating context from surrounding slides and leveraging section retrieval strategies. For layout generation, we propose a textual-to-visual self-verification process using a LLM-based Reviewer + Refiner workflow, transforming complex textual layouts into intuitive visual formats. This modality transformation simplifies the task, enabling accurate and human-like review and refinement. Experiments show that our approach significantly outperforms baseline methods in terms of alignment, logical flow, visual appeal, and readability.

### HiFi-KPI: A Dataset for Hierarchical KPI Extraction from Earnings Filings 
[[arxiv](https://arxiv.org/abs/2502.15411)] [[cool](https://papers.cool/arxiv/2502.15411)] [[pdf](https://arxiv.org/pdf/2502.15411)]
> **Authors**: Rasmus Aavang,Giovanni Rizzi,Rasmus Bøggild,Alexandre Iolov,Mike Zhang,Johannes Bjerva
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: The U.S. Securities and Exchange Commission (SEC) requires that public companies file financial reports tagging numbers with the machine readable inline eXtensible Business Reporting Language (iXBRL) standard. However, the highly complex and highly granular taxonomy defined by iXBRL limits label transferability across domains. In this paper, we introduce the Hierarchical Financial Key Performance Indicator (HiFi-KPI) dataset, designed to facilitate numerical KPI extraction at specified levels of granularity from unstructured financial text. Our approach organizes a 218,126-label hierarchy using a taxonomy based grouping method, investigating which taxonomy layer provides the most meaningful structure. HiFi-KPI comprises ~1.8M paragraphs and ~5M entities, each linked to a label in the iXBRL-specific calculation and presentation taxonomies. We provide baselines using encoder-based approaches and structured extraction using Large Language Models (LLMs). To simplify LLM inference and evaluation, we additionally release HiFi-KPI Lite, a manually curated subset with four expert-mapped labels. We publicly release all artifacts.

### Problem-Solving Logic Guided Curriculum In-Context Learning for LLMs Complex Reasoning 
[[arxiv](https://arxiv.org/abs/2502.15401)] [[cool](https://papers.cool/arxiv/2502.15401)] [[pdf](https://arxiv.org/pdf/2502.15401)]
> **Authors**: Xuetao Ma,Wenbin Jiang,Hua Huang
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: In-context learning (ICL) can significantly enhance the complex reasoning capabilities of large language models (LLMs), with the key lying in the selection and ordering of demonstration examples. Previous methods typically relied on simple features to measure the relevance between examples. We argue that these features are not sufficient to reflect the intrinsic connections between examples. In this study, we propose a curriculum ICL strategy guided by problem-solving logic. We select demonstration examples by analyzing the problem-solving logic and order them based on curriculum learning. Specifically, we constructed a problem-solving logic instruction set based on the BREAK dataset and fine-tuned a language model to analyze the problem-solving logic of examples. Subsequently, we selected appropriate demonstration examples based on problem-solving logic and assessed their difficulty according to the number of problem-solving steps. In accordance with the principles of curriculum learning, we ordered the examples from easy to hard to serve as contextual prompts. Experimental results on multiple benchmarks indicate that our method outperforms previous ICL approaches in terms of performance and efficiency, effectively enhancing the complex reasoning capabilities of LLMs. Our project will be publicly available subsequently.

### Evaluating Social Biases in LLM Reasoning 
[[arxiv](https://arxiv.org/abs/2502.15361)] [[cool](https://papers.cool/arxiv/2502.15361)] [[pdf](https://arxiv.org/pdf/2502.15361)]
> **Authors**: Xuyang Wu,Jinming Nian,Zhiqiang Tao,Yi Fang
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: In the recent development of AI reasoning, large language models (LLMs) are trained to automatically generate chain-of-thought reasoning steps, which have demonstrated compelling performance on math and coding tasks. However, when bias is mixed within the reasoning process to form strong logical arguments, it could cause even more harmful results and further induce hallucinations. In this paper, we have evaluated the 8B and 32B variants of DeepSeek-R1 against their instruction tuned counterparts on the BBQ dataset, and investigated the bias that is elicited out and being amplified through reasoning steps. To the best of our knowledge, this empirical study is the first to assess bias issues in LLM reasoning.

### AttentionEngine: A Versatile Framework for Efficient Attention Mechanisms on Diverse Hardware Platforms 
[[arxiv](https://arxiv.org/abs/2502.15349)] [[cool](https://papers.cool/arxiv/2502.15349)] [[pdf](https://arxiv.org/pdf/2502.15349)]
> **Authors**: Feiyang Chen,Yu Cheng,Lei Wang,Yuqing Xia,Ziming Miao,Lingxiao Ma,Fan Yang,Jilong Xue,Zhi Yang,Mao Yang,Haibo Chen
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 15 pages
- **标题**: None
- **领域**: 计算语言学,机器学习,表现
- **Abstract**: Transformers and large language models (LLMs) have revolutionized machine learning, with attention mechanisms at the core of their success. As the landscape of attention variants expands, so too do the challenges of optimizing their performance, particularly across different hardware platforms. Current optimization strategies are often narrowly focused, requiring extensive manual intervention to accommodate changes in model configurations or hardware environments. In this paper, we introduce AttentionEngine, a comprehensive framework designed to streamline the optimization of attention mechanisms across heterogeneous hardware backends. By decomposing attention computation into modular operations with customizable components, AttentionEngine enables flexible adaptation to diverse algorithmic requirements. The framework further automates kernel optimization through a combination of programmable templates and a robust cross-platform scheduling strategy. Empirical results reveal performance gains of up to 10x on configurations beyond the reach of existing methods. AttentionEngine offers a scalable, efficient foundation for developing and deploying attention mechanisms with minimal manual tuning. Our code has been open-sourced and is available at https://github.com/microsoft/AttentionEngine.

### Constructing a Norm for Children's Scientific Drawing: Distribution Features Based on Semantic Similarity of Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.15348)] [[cool](https://papers.cool/arxiv/2502.15348)] [[pdf](https://arxiv.org/pdf/2502.15348)]
> **Authors**: Yi Zhang,Fan Wei,Jingyi Li,Yan Wang,Yanyan Yu,Jianli Chen,Zipo Cai,Xinyu Liu,Wei Wang,Peng Wang,Zhong Wang
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: The use of children's drawings to examining their conceptual understanding has been proven to be an effective method, but there are two major problems with previous research: 1. The content of the drawings heavily relies on the task, and the ecological validity of the conclusions is low; 2. The interpretation of drawings relies too much on the subjective feelings of the researchers. To address this issue, this study uses the Large Language Model (LLM) to identify 1420 children's scientific drawings (covering 9 scientific themes/concepts), and uses the word2vec algorithm to calculate their semantic similarity. The study explores whether there are consistent drawing representations for children on the same theme, and attempts to establish a norm for children's scientific drawings, providing a baseline reference for follow-up children's drawing research. The results show that the representation of most drawings has consistency, manifested as most semantic similarity greater than 0.8. At the same time, it was found that the consistency of the representation is independent of the accuracy (of LLM's recognition), indicating the existence of consistency bias. In the subsequent exploration of influencing factors, we used Kendall rank correlation coefficient to investigate the effects of Sample Size, Abstract Degree, and Focus Points on drawings, and used word frequency statistics to explore whether children represented abstract themes/concepts by reproducing what was taught in class.

### Tokenization is Sensitive to Language Variation 
[[arxiv](https://arxiv.org/abs/2502.15343)] [[cool](https://papers.cool/arxiv/2502.15343)] [[pdf](https://arxiv.org/pdf/2502.15343)]
> **Authors**: Anna Wegmann,Dong Nguyen,David Jurgens
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Variation in language is ubiquitous and often systematically linked to regional, social, and contextual factors. Tokenizers split texts into smaller units and might behave differently for less common linguistic forms. This might affect downstream LLM performance differently on two types of tasks: Tasks where the model should be robust to language variation (e.g., for semantic tasks like NLI, labels do not depend on whether a text uses British or American spelling) and tasks where the model should be sensitive to language variation (e.g., for form-based tasks like authorship verification, labels depend on whether a text uses British or American spelling). We pre-train BERT base models for the popular Byte-Pair Encoding algorithm to investigate how key algorithmic design choices impact downstream models' performances: fitting corpus, pre-tokenizer and vocabulary size. We find that the best tokenizer varies on the two task types -- with the pre-tokenizer having the biggest impact on performance. Further, we introduce a new approach to estimate tokenizer impact on downstream LLM performance, showing significant improvement over techniques like Rényi efficiency. We encourage more work on language variation and its relation to tokenizers and thus LLM performance.

### Stepwise Informativeness Search for Improving LLM Reasoning 
[[arxiv](https://arxiv.org/abs/2502.15335)] [[cool](https://papers.cool/arxiv/2502.15335)] [[pdf](https://arxiv.org/pdf/2502.15335)]
> **Authors**: Siyuan Wang,Enda Zhao,Zhongyu Wei,Xiang Ren
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Preprint
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Advances in Large Language Models (LLMs) have significantly improved multi-step reasoning through generating free-text rationales. However, recent studies show that LLMs tend to lose focus over the middle of long contexts. This raises concerns that as reasoning progresses, LLMs may overlook information in earlier steps when decoding subsequent steps, leading to generate unreliable and redundant rationales. To address this, we propose guiding LLMs to generate more accurate and concise step-by-step rationales by (1) proactively referencing information from underutilized prior steps, and (2) minimizing redundant information between new and existing steps. We introduce stepwise informativeness search, an inference-time tree search framework incorporating two selection heuristics: grounding-guided selection which prioritizes steps paying higher attention over underutilized steps; and novelty-guided selection which encourages steps with novel conclusions. During rationale generation, we use a self-grounding strategy that prompts LLMs to explicitly reference relevant prior steps to provide premises before deduction at each step. Experimental results on four reasoning datasets demonstrate that our approach improves reasoning accuracy by generating higher-quality rationales with reduced errors and redundancy.

### Detecting Future-related Contexts of Entity Mentions 
[[arxiv](https://arxiv.org/abs/2502.15332)] [[cool](https://papers.cool/arxiv/2502.15332)] [[pdf](https://arxiv.org/pdf/2502.15332)]
> **Authors**: Puneet Prashar,Krishna Mohan Shukla,Adam Jatowt
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,信息检索
- **Abstract**: The ability to automatically identify whether an entity is referenced in a future context can have multiple applications including decision making, planning and trend forecasting. This paper focuses on detecting implicit future references in entity-centric texts, addressing the growing need for automated temporal analysis in information processing. We first present a novel dataset of 19,540 sentences built around popular entities sourced from Wikipedia, which consists of future-related and non-future-related contexts in which those entities appear. As a second contribution, we evaluate the performance of several Language Models including also Large Language Models (LLMs) on the task of distinguishing future-oriented content in the absence of explicit temporal references.

### Round Attention: A Novel Round-Level Attention Mechanism to Accelerate LLM Inference 
[[arxiv](https://arxiv.org/abs/2502.15294)] [[cool](https://papers.cool/arxiv/2502.15294)] [[pdf](https://arxiv.org/pdf/2502.15294)]
> **Authors**: Yaohua Tang,Zhicheng Hu,Kun Cheng,Fan Mo,Qiheng Lv,Hua Wang,Zhi Chen
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: The increasing context window size in large language models (LLMs) has improved their ability to handle complex, long-text tasks. However, as the conversation rounds continue, it is required to store a large amount of KV cache in GPU memory, which significantly affects the efficiency and even availability of the model serving systems. This paper analyzes dialogue data from real users and discovers that the LLM inference manifests a watershed layer, after which the distribution of round-level attention shows notable similarity. We propose Round Attention, a novel round-level attention mechanism that only recalls and computes the KV cache of the most relevant rounds. The experiments show that our method saves 55\% memory usage without compromising model performance.

### Analyzing the Inner Workings of Transformers in Compositional Generalization 
[[arxiv](https://arxiv.org/abs/2502.15277)] [[cool](https://papers.cool/arxiv/2502.15277)] [[pdf](https://arxiv.org/pdf/2502.15277)]
> **Authors**: Ryoma Kumon,Hitomi Yanaka
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Accepted to NAACL 2025 main
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The compositional generalization abilities of neural models have been sought after for human-like linguistic competence. The popular method to evaluate such abilities is to assess the models' input-output behavior. However, that does not reveal the internal mechanisms, and the underlying competence of such models in compositional generalization remains unclear. To address this problem, we explore the inner workings of a Transformer model by finding an existing subnetwork that contributes to the generalization performance and by performing causal analyses on how the model utilizes syntactic features. We find that the model depends on syntactic features to output the correct answer, but that the subnetwork with much better generalization performance than the whole model relies on a non-compositional algorithm in addition to the syntactic features. We also show that the subnetwork improves its generalization performance relatively slowly during the training compared to the in-distribution one, and the non-compositional solution is acquired in the early stages of the training.

### A Training-free LLM-based Approach to General Chinese Character Error Correction 
[[arxiv](https://arxiv.org/abs/2502.15266)] [[cool](https://papers.cool/arxiv/2502.15266)] [[pdf](https://arxiv.org/pdf/2502.15266)]
> **Authors**: Houquan Zhou,Bo Zhang,Zhenghua Li,Ming Yan,Min Zhang
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 25 pages, 12 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Chinese spelling correction (CSC) is a crucial task that aims to correct character errors in Chinese text. While conventional CSC focuses on character substitution errors caused by mistyping, two other common types of character errors, missing and redundant characters, have received less attention. These errors are often excluded from CSC datasets during the annotation process or ignored during evaluation, even when they have been annotated. This issue limits the practicality of the CSC task. To address this issue, we introduce the task of General Chinese Character Error Correction (C2EC), which focuses on all three types of character errors. We construct a high-quality C2EC benchmark by combining and manually verifying data from CCTC and Lemon datasets. We extend the training-free prompt-free CSC method to C2EC by using Levenshtein distance for handling length changes and leveraging an additional prompt-based large language model (LLM) to improve performance. Experiments show that our method enables a 14B-parameter LLM to be on par with models nearly 50 times larger on both conventional CSC and C2EC tasks, without any fine-tuning.

### Retrieval-Augmented Speech Recognition Approach for Domain Challenges 
[[arxiv](https://arxiv.org/abs/2502.15264)] [[cool](https://papers.cool/arxiv/2502.15264)] [[pdf](https://arxiv.org/pdf/2502.15264)]
> **Authors**: Peng Shen,Xugang Lu,Hisashi Kawai
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,声音,音频和语音处理
- **Abstract**: Speech recognition systems often face challenges due to domain mismatch, particularly in real-world applications where domain-specific data is unavailable because of data accessibility and confidentiality constraints. Inspired by Retrieval-Augmented Generation (RAG) techniques for large language models (LLMs), this paper introduces a LLM-based retrieval-augmented speech recognition method that incorporates domain-specific textual data at the inference stage to enhance recognition performance. Rather than relying on domain-specific textual data during the training phase, our model is trained to learn how to utilize textual information provided in prompts for LLM decoder to improve speech recognition performance. Benefiting from the advantages of the RAG retrieval mechanism, our approach efficiently accesses locally available domain-specific documents, ensuring a convenient and effective process for solving domain mismatch problems. Experiments conducted on the CSJ database demonstrate that the proposed method significantly improves speech recognition accuracy and achieves state-of-the-art results on the CSJ dataset, even without relying on the full training data.

### Corrections Meet Explanations: A Unified Framework for Explainable Grammatical Error Correction 
[[arxiv](https://arxiv.org/abs/2502.15261)] [[cool](https://papers.cool/arxiv/2502.15261)] [[pdf](https://arxiv.org/pdf/2502.15261)]
> **Authors**: Jingheng Ye,Shang Qin,Yinghui Li,Hai-Tao Zheng,Shen Wang,Qingsong Wen
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 19 pages, 2 figures, and 9 tables
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Grammatical Error Correction (GEC) faces a critical challenge concerning explainability, notably when GEC systems are designed for language learners. Existing research predominantly focuses on explaining grammatical errors extracted in advance, thus neglecting the relationship between explanations and corrections. To address this gap, we introduce EXGEC, a unified explainable GEC framework that integrates explanation and correction tasks in a generative manner, advocating that these tasks mutually reinforce each other. Experiments have been conducted on EXPECT, a recent human-labeled dataset for explainable GEC, comprising around 20k samples. Moreover, we detect significant noise within EXPECT, potentially compromising model training and evaluation. Therefore, we introduce an alternative dataset named EXPECT-denoised, ensuring a more objective framework for training and evaluation. Results on various NLP models (BART, T5, and Llama3) show that EXGEC models surpass single-task baselines in both tasks, demonstrating the effectiveness of our approach.

### LightMamba: Efficient Mamba Acceleration on FPGA with Quantization and Hardware Co-design 
[[arxiv](https://arxiv.org/abs/2502.15260)] [[cool](https://papers.cool/arxiv/2502.15260)] [[pdf](https://arxiv.org/pdf/2502.15260)]
> **Authors**: Renjie Wei,Songqiang Xu,Linfeng Zhong,Zebin Yang,Qingyu Guo,Yuan Wang,Runsheng Wang,Meng Li
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Accepted by DATE 2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: State space models (SSMs) like Mamba have recently attracted much attention. Compared to Transformer-based large language models (LLMs), Mamba achieves linear computation complexity with the sequence length and demonstrates superior performance. However, Mamba is hard to accelerate due to the scattered activation outliers and the complex computation dependency, rendering existing LLM accelerators inefficient. In this paper, we propose LightMamba that co-designs the quantization algorithm and FPGA accelerator architecture for efficient Mamba inference. We first propose an FPGA-friendly post-training quantization algorithm that features rotation-assisted quantization and power-of-two SSM quantization to reduce the majority of computation to 4-bit. We further design an FPGA accelerator that partially unrolls the Mamba computation to balance the efficiency and hardware costs. Through computation reordering as well as fine-grained tiling and fusion, the hardware utilization and memory efficiency of the accelerator get drastically improved. We implement LightMamba on Xilinx Versal VCK190 FPGA and achieve 4.65x to 6.06x higher energy efficiency over the GPU baseline. When evaluated on Alveo U280 FPGA, LightMamba reaches 93 tokens/s, which is 1.43x that of the GPU baseline.

### Understand User Opinions of Large Language Models via LLM-Powered In-the-Moment User Experience Interviews 
[[arxiv](https://arxiv.org/abs/2502.15226)] [[cool](https://papers.cool/arxiv/2502.15226)] [[pdf](https://arxiv.org/pdf/2502.15226)]
> **Authors**: Mengqiao Liu,Tevin Wang,Cassandra A. Cohen,Sarah Li,Chenyan Xiong
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,人机交互
- **Abstract**: Which large language model (LLM) is better? Every evaluation tells a story, but what do users really think about current LLMs? This paper presents CLUE, an LLM-powered interviewer that conducts in-the-moment user experience interviews, right after users interacted with LLMs, and automatically gathers insights about user opinions from massive interview logs. We conduct a study with thousands of users to understand user opinions on mainstream LLMs, recruiting users to first chat with a target LLM and then interviewed by CLUE. Our experiments demonstrate that CLUE captures interesting user opinions, for example, the bipolar views on the displayed reasoning process of DeepSeek-R1 and demands for information freshness and multi-modality. Our collected chat-and-interview logs will be released.

### ESPnet-SpeechLM: An Open Speech Language Model Toolkit 
[[arxiv](https://arxiv.org/abs/2502.15218)] [[cool](https://papers.cool/arxiv/2502.15218)] [[pdf](https://arxiv.org/pdf/2502.15218)]
> **Authors**: Jinchuan Tian,Jiatong Shi,William Chen,Siddhant Arora,Yoshiki Masuyama,Takashi Maekaku,Yihan Wu,Junyi Peng,Shikhar Bharadwaj,Yiwen Zhao,Samuele Cornell,Yifan Peng,Xiang Yue,Chao-Han Huck Yang,Graham Neubig,Shinji Watanabe
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,声音,音频和语音处理
- **Abstract**: We present ESPnet-SpeechLM, an open toolkit designed to democratize the development of speech language models (SpeechLMs) and voice-driven agentic applications. The toolkit standardizes speech processing tasks by framing them as universal sequential modeling problems, encompassing a cohesive workflow of data preprocessing, pre-training, inference, and task evaluation. With ESPnet-SpeechLM, users can easily define task templates and configure key settings, enabling seamless and streamlined SpeechLM development. The toolkit ensures flexibility, efficiency, and scalability by offering highly configurable modules for every stage of the workflow. To illustrate its capabilities, we provide multiple use cases demonstrating how competitive SpeechLMs can be constructed with ESPnet-SpeechLM, including a 1.7B-parameter model pre-trained on both text and speech tasks, across diverse benchmarks. The toolkit and its recipes are fully transparent and reproducible at: https://github.com/espnet/espnet/tree/speechlm.

## 密码学和安全(cs.CR:Cryptography and Security)

### Towards Reinforcement Learning for Exploration of Speculative Execution Vulnerabilities 
[[arxiv](https://arxiv.org/abs/2502.16756)] [[cool](https://papers.cool/arxiv/2502.16756)] [[pdf](https://arxiv.org/pdf/2502.16756)]
> **Authors**: Evan Lai,Wenjie Xiong,Edward Suh,Mohit Tiwari,Mulong Luo
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Speculative attacks such as Spectre can leak secret information without being discovered by the operating system. Speculative execution vulnerabilities are finicky and deep in the sense that to exploit them, it requires intensive manual labor and intimate knowledge of the hardware. In this paper, we introduce SpecRL, a framework that utilizes reinforcement learning to find speculative execution leaks in post-silicon (black box) microprocessors.

### RapidPen: Fully Automated IP-to-Shell Penetration Testing with LLM-based Agents 
[[arxiv](https://arxiv.org/abs/2502.16730)] [[cool](https://papers.cool/arxiv/2502.16730)] [[pdf](https://arxiv.org/pdf/2502.16730)]
> **Authors**: Sho Nakatani
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: We present RapidPen, a fully automated penetration testing (pentesting) framework that addresses the challenge of achieving an initial foothold (IP-to-Shell) without human intervention. Unlike prior approaches that focus primarily on post-exploitation or require a human-in-the-loop, RapidPen leverages large language models (LLMs) to autonomously discover and exploit vulnerabilities, starting from a single IP address. By integrating advanced ReAct-style task planning (Re) with retrieval-augmented knowledge bases of successful exploits, along with a command-generation and direct execution feedback loop (Act), RapidPen systematically scans services, identifies viable attack vectors, and executes targeted exploits in a fully automated manner. In our evaluation against a vulnerable target from the Hack The Box platform, RapidPen achieved shell access within 200-400 seconds at a per-run cost of approximately \$0.3-\$0.6, demonstrating a 60\% success rate when reusing prior "success-case" data. These results underscore the potential of truly autonomous pentesting for both security novices and seasoned professionals. Organizations without dedicated security teams can leverage RapidPen to quickly identify critical vulnerabilities, while expert pentesters can offload repetitive tasks and focus on complex challenges. Ultimately, our work aims to make penetration testing more accessible and cost-efficient, thereby enhancing the overall security posture of modern software ecosystems.

### Verification of Bit-Flip Attacks against Quantized Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.16286)] [[cool](https://papers.cool/arxiv/2502.16286)] [[pdf](https://arxiv.org/pdf/2502.16286)]
> **Authors**: Yedi Zhang,Lei Huang,Pengfei Gao,Fu Song,Jun Sun,Jin Song Dong
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: 37 pages, 13 figures, 14 tables
- **标题**: None
- **领域**: 密码学和安全,人工智能,机器学习
- **Abstract**: In the rapidly evolving landscape of neural network security, the resilience of neural networks against bit-flip attacks (i.e., an attacker maliciously flips an extremely small amount of bits within its parameter storage memory system to induce harmful behavior), has emerged as a relevant area of research. Existing studies suggest that quantization may serve as a viable defense against such attacks. Recognizing the documented susceptibility of real-valued neural networks to such attacks and the comparative robustness of quantized neural networks (QNNs), in this work, we introduce BFAVerifier, the first verification framework designed to formally verify the absence of bit-flip attacks or to identify all vulnerable parameters in a sound and rigorous manner. BFAVerifier comprises two integral components: an abstraction-based method and an MILP-based method. Specifically, we first conduct a reachability analysis with respect to symbolic parameters that represent the potential bit-flip attacks, based on a novel abstract domain with a sound guarantee. If the reachability analysis fails to prove the resilience of such attacks, then we encode this verification problem into an equivalent MILP problem which can be solved by off-the-shelf solvers. Therefore, BFAVerifier is sound, complete, and reasonably efficient. We conduct extensive experiments, which demonstrate its effectiveness and efficiency across various network architectures, quantization bit-widths, and adversary capabilities.

### An End-to-End Homomorphically Encrypted Neural Network 
[[arxiv](https://arxiv.org/abs/2502.16176)] [[cool](https://papers.cool/arxiv/2502.16176)] [[pdf](https://arxiv.org/pdf/2502.16176)]
> **Authors**: Marcos Florencio,Luiz Alencar,Bianca Lima
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Every commercially available, state-of-the-art neural network consume plain input data, which is a well-known privacy concern. We propose a new architecture based on homomorphic encryption, which allows the neural network to operate on encrypted data. We show that Homomorphic Neural Networks (HNN) can achieve full privacy and security while maintaining levels of accuracy comparable to plain neural networks. We also introduce a new layer, the Differentiable Soft-Argmax, which allows the calibration of output logits in the encrypted domain, raising the entropy of the activation parameters, thus improving the security of the model, while keeping the overall noise below the acceptable noise budget. Experiments were conducted using the Stanford Sentiment Treebank (SST-2) corpora on the DistilBERT base uncased finetuned SST-2 English sentiment analysis model, and the results show that the HNN model can achieve up to 82.5% of the accuracy of the plain model while maintaining full privacy and security.

### A Survey of Model Extraction Attacks and Defenses in Distributed Computing Environments 
[[arxiv](https://arxiv.org/abs/2502.16065)] [[cool](https://papers.cool/arxiv/2502.16065)] [[pdf](https://arxiv.org/pdf/2502.16065)]
> **Authors**: Kaixiang Zhao,Lincan Li,Kaize Ding,Neil Zhenqiang Gong,Yue Zhao,Yushun Dong
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能,机器学习
- **Abstract**: Model Extraction Attacks (MEAs) threaten modern machine learning systems by enabling adversaries to steal models, exposing intellectual property and training data. With the increasing deployment of machine learning models in distributed computing environments, including cloud, edge, and federated learning settings, each paradigm introduces distinct vulnerabilities and challenges. Without a unified perspective on MEAs across these distributed environments, organizations risk fragmented defenses, inadequate risk assessments, and substantial economic and privacy losses. This survey is motivated by the urgent need to understand how the unique characteristics of cloud, edge, and federated deployments shape attack vectors and defense requirements. We systematically examine the evolution of attack methodologies and defense mechanisms across these environments, demonstrating how environmental factors influence security strategies in critical sectors such as autonomous vehicles, healthcare, and financial services. By synthesizing recent advances in MEAs research and discussing the limitations of current evaluation practices, this survey provides essential insights for developing robust and adaptive defense strategies. Our comprehensive approach highlights the importance of integrating protective measures across the entire distributed computing landscape to ensure the secure deployment of machine learning models.

### Human-AI Collaboration in Cloud Security: Cognitive Hierarchy-Driven Deep Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.16054)] [[cool](https://papers.cool/arxiv/2502.16054)] [[pdf](https://arxiv.org/pdf/2502.16054)]
> **Authors**: Zahra Aref,Sheng Wei,Narayan B. Mandayam
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能,人机交互
- **Abstract**: Given the complexity of multi-tenant cloud environments and the need for real-time threat mitigation, Security Operations Centers (SOCs) must integrate AI-driven adaptive defenses against Advanced Persistent Threats (APTs). However, SOC analysts struggle with countering adaptive adversarial tactics, necessitating intelligent decision-support frameworks. To enhance human-AI collaboration in SOCs, we propose a Cognitive Hierarchy Theory-driven Deep Q-Network (CHT-DQN) framework that models SOC analysts' decision-making against AI-driven APT bots. The SOC analyst (defender) operates at cognitive level-1, anticipating attacker strategies, while the APT bot (attacker) follows a level-0 exploitative policy. By incorporating CHT into DQN, our framework enhances SOC defense strategies via Attack Graph (AG)-based reinforcement learning. Simulation experiments across varying AG complexities show that CHT-DQN achieves higher data protection and lower action discrepancies compared to standard DQN. A theoretical lower bound analysis further validates its superior Q-value performance. A human-in-the-loop (HITL) evaluation on Amazon Mechanical Turk (MTurk) reveals that SOC analysts using CHT-DQN-driven transition probabilities align better with adaptive attackers, improving data protection. Additionally, human decision patterns exhibit risk aversion after failure and risk-seeking behavior after success, aligning with Prospect Theory. These findings underscore the potential of integrating cognitive modeling into deep reinforcement learning to enhance SOC operations and develop real-time adaptive cloud security mechanisms.

### A Mousetrap: Fooling Large Reasoning Models for Jailbreak with Chain of Iterative Chaos 
[[arxiv](https://arxiv.org/abs/2502.15806)] [[cool](https://papers.cool/arxiv/2502.15806)] [[pdf](https://arxiv.org/pdf/2502.15806)]
> **Authors**: Yang Yao,Xuan Tong,Ruofan Wang,Yixu Wang,Lujundong Li,Liang Liu,Yan Teng,Yingchun Wang
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能,计算语言学,机器学习
- **Abstract**: Large Reasoning Models (LRMs) have significantly advanced beyond traditional Large Language Models (LLMs) with their exceptional logical reasoning capabilities, yet these improvements introduce heightened safety risks. When subjected to jailbreak attacks, their ability to generate more targeted and organized content can lead to greater harm. Although some studies claim that reasoning enables safer LRMs against existing LLM attacks, they overlook the inherent flaws within the reasoning process itself. To address this gap, we propose the first jailbreak attack targeting LRMs, exploiting their unique vulnerabilities stemming from the advanced reasoning capabilities. Specifically, we introduce a Chaos Machine, a novel component to transform attack prompts with diverse one-to-one mappings. The chaos mappings iteratively generated by the machine are embedded into the reasoning chain, which strengthens the variability and complexity and also promotes a more robust attack. Based on this, we construct the Mousetrap framework, which makes attacks projected into nonlinear-like low sample spaces with mismatched generalization enhanced. Also, due to the more competing objectives, LRMs gradually maintain the inertia of unpredictable iterative reasoning and fall into our trap. Success rates of the Mousetrap attacking o1-mini, claude-sonnet and gemini-thinking are as high as 96%, 86% and 98% respectively on our toxic dataset Trotter. On benchmarks such as AdvBench, StrongREJECT, and HarmBench, attacking claude-sonnet, well-known for its safety, Mousetrap can astonishingly achieve success rates of 87.5%, 86.58% and 93.13% respectively. Attention: This paper contains inappropriate, offensive and harmful content.

### Investigating the Impact of Quantization Methods on the Safety and Reliability of Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.15799)] [[cool](https://papers.cool/arxiv/2502.15799)] [[pdf](https://arxiv.org/pdf/2502.15799)]
> **Authors**: Artyom Kharinaev,Viktor Moskvoretskii,Egor Shvetsov,Kseniia Studenikina,Bykov Mikhail,Evgeny Burnaev
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Large Language Models (LLMs) have emerged as powerful tools for addressing modern challenges and enabling practical applications. However, their computational expense remains a significant barrier to widespread adoption. Quantization has emerged as a promising technique to democratize access and enable low resource device deployment. Despite these advancements, the safety and trustworthiness of quantized models remain underexplored, as prior studies often overlook contemporary architectures and rely on overly simplistic benchmarks and evaluations. To address this gap, we introduce OpenSafetyMini, a novel open-ended safety dataset designed to better distinguish between models. We evaluate 4 state-of-the-art quantization techniques across LLaMA and Mistral models using 4 benchmarks, including human evaluations. Our findings reveal that the optimal quantization method varies for 4-bit precision, while vector quantization techniques deliver the best safety and trustworthiness performance at 2-bit precision, providing foundation for future research.

### OCCULT: Evaluating Large Language Models for Offensive Cyber Operation Capabilities 
[[arxiv](https://arxiv.org/abs/2502.15797)] [[cool](https://papers.cool/arxiv/2502.15797)] [[pdf](https://arxiv.org/pdf/2502.15797)]
> **Authors**: Michael Kouremetis,Marissa Dotter,Alex Byrne,Dan Martin,Ethan Michalak,Gianpaolo Russo,Michael Threet,Guido Zarrella
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-24
> **comment**: 31 pages, 17 figures, 11 tables
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: The prospect of artificial intelligence (AI) competing in the adversarial landscape of cyber security has long been considered one of the most impactful, challenging, and potentially dangerous applications of AI. Here, we demonstrate a new approach to assessing AI's progress towards enabling and scaling real-world offensive cyber operations (OCO) tactics in use by modern threat actors. We detail OCCULT, a lightweight operational evaluation framework that allows cyber security experts to contribute to rigorous and repeatable measurement of the plausible cyber security risks associated with any given large language model (LLM) or AI employed for OCO. We also prototype and evaluate three very different OCO benchmarks for LLMs that demonstrate our approach and serve as examples for building benchmarks under the OCCULT framework. Finally, we provide preliminary evaluation results to demonstrate how this framework allows us to move beyond traditional all-or-nothing tests, such as those crafted from educational exercises like capture-the-flag environments, to contextualize our indicators and warnings in true cyber threat scenarios that present risks to modern infrastructure. We find that there has been significant recent advancement in the risks of AI being used to scale realistic cyber threats. For the first time, we find a model (DeepSeek-R1) is capable of correctly answering over 90% of challenging offensive cyber knowledge tests in our Threat Actor Competency Test for LLMs (TACTL) multiple-choice benchmarks. We also show how Meta's Llama and Mistral's Mixtral model families show marked performance improvements over earlier models against our benchmarks where LLMs act as offensive agents in MITRE's high-fidelity offensive and defensive cyber operations simulation environment, CyberLayer.

### A Defensive Framework Against Adversarial Attacks on Machine Learning-Based Network Intrusion Detection Systems 
[[arxiv](https://arxiv.org/abs/2502.15561)] [[cool](https://papers.cool/arxiv/2502.15561)] [[pdf](https://arxiv.org/pdf/2502.15561)]
> **Authors**: Benyamin Tafreshian,Shengzhi Zhang
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Accepted to IEEEAI+ TrustCom 2024
- **标题**: None
- **领域**: 密码学和安全,机器学习
- **Abstract**: As cyberattacks become increasingly sophisticated, advanced Network Intrusion Detection Systems (NIDS) are critical for modern network security. Traditional signature-based NIDS are inadequate against zero-day and evolving attacks. In response, machine learning (ML)-based NIDS have emerged as promising solutions; however, they are vulnerable to adversarial evasion attacks that subtly manipulate network traffic to bypass detection. To address this vulnerability, we propose a novel defensive framework that enhances the robustness of ML-based NIDS by simultaneously integrating adversarial training, dataset balancing techniques, advanced feature engineering, ensemble learning, and extensive model fine-tuning. We validate our framework using the NSL-KDD and UNSW-NB15 datasets. Experimental results show, on average, a 35% increase in detection accuracy and a 12.5% reduction in false positives compared to baseline models, particularly under adversarial conditions. The proposed defense against adversarial attacks significantly advances the practical deployment of robust ML-based NIDS in real-world networks.

### Adversarial Prompt Evaluation: Systematic Benchmarking of Guardrails Against Prompt Input Attacks on LLMs 
[[arxiv](https://arxiv.org/abs/2502.15427)] [[cool](https://papers.cool/arxiv/2502.15427)] [[pdf](https://arxiv.org/pdf/2502.15427)]
> **Authors**: Giulio Zizzo,Giandomenico Cornacchia,Kieran Fraser,Muhammad Zaid Hameed,Ambrish Rawat,Beat Buesser,Mark Purcell,Pin-Yu Chen,Prasanna Sattigeri,Kush Varshney
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: NeurIPS 2024, Safe GenerativeAIWorkshop
- **标题**: None
- **领域**: 密码学和安全,机器学习
- **Abstract**: As large language models (LLMs) become integrated into everyday applications, ensuring their robustness and security is increasingly critical. In particular, LLMs can be manipulated into unsafe behaviour by prompts known as jailbreaks. The variety of jailbreak styles is growing, necessitating the use of external defences known as guardrails. While many jailbreak defences have been proposed, not all defences are able to handle new out-of-distribution attacks due to the narrow segment of jailbreaks used to align them. Moreover, the lack of systematisation around defences has created significant gaps in their practical application. In this work, we perform systematic benchmarking across 15 different defences, considering a broad swathe of malicious and benign datasets. We find that there is significant performance variation depending on the style of jailbreak a defence is subject to. Additionally, we show that based on current datasets available for evaluation, simple baselines can display competitive out-of-distribution performance compared to many state-of-the-art defences. Code is available at https://github.com/IBM/Adversarial-Prompt-Evaluation.

### Attention Eclipse: Manipulating Attention to Bypass LLM Safety-Alignment 
[[arxiv](https://arxiv.org/abs/2502.15334)] [[cool](https://papers.cool/arxiv/2502.15334)] [[pdf](https://arxiv.org/pdf/2502.15334)]
> **Authors**: Pedram Zaree,Md Abdullah Al Mamun,Quazi Mishkatul Alam,Yue Dong,Ihsen Alouani,Nael Abu-Ghazaleh
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能,机器学习
- **Abstract**: Recent research has shown that carefully crafted jailbreak inputs can induce large language models to produce harmful outputs, despite safety measures such as alignment. It is important to anticipate the range of potential Jailbreak attacks to guide effective defenses and accurate assessment of model safety. In this paper, we present a new approach for generating highly effective Jailbreak attacks that manipulate the attention of the model to selectively strengthen or weaken attention among different parts of the prompt. By harnessing attention loss, we develop more effective jailbreak attacks, that are also transferrable. The attacks amplify the success rate of existing Jailbreak algorithms including GCG, AutoDAN, and ReNeLLM, while lowering their generation cost (for example, the amplified GCG attack achieves 91.2% ASR, vs. 67.9% for the original attack on Llama2-7B/AdvBench, using less than a third of the generation time).

### Steganographic Embeddings as an Effective Data Augmentation 
[[arxiv](https://arxiv.org/abs/2502.15245)] [[cool](https://papers.cool/arxiv/2502.15245)] [[pdf](https://arxiv.org/pdf/2502.15245)]
> **Authors**: Nicholas DiSalvo
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 10 pages, 4 figures. For associated code and experiments, see this http URL https://github.com/nickd16/steganographic-augmentations
- **标题**: None
- **领域**: 密码学和安全,机器学习,多媒体
- **Abstract**: Image Steganography is a cryptographic technique that embeds secret information into an image, ensuring the hidden data remains undetectable to the human eye while preserving the image's original visual integrity. Least Significant Bit (LSB) Steganography achieves this by replacing the k least significant bits of an image with the k most significant bits of a secret image, maintaining the appearance of the original image while simultaneously encoding the essential elements of the hidden data. In this work, we shift away from conventional applications of steganography in deep learning and explore its potential from a new angle. We present experimental results on CIFAR-10 showing that LSB Steganography, when used as a data augmentation strategy for downstream computer vision tasks such as image classification, can significantly improve the training efficiency of deep neural networks. It can also act as an implicit, uniformly discretized piecewise linear approximation of color augmentations such as (brightness, contrast, hue, and saturation), without introducing additional training overhead through a new joint image training regime that disregards the need for tuning sensitive augmentation hyperparameters.

### A General Pseudonymization Framework for Cloud-Based LLMs: Replacing Privacy Information in Controlled Text Generation 
[[arxiv](https://arxiv.org/abs/2502.15233)] [[cool](https://papers.cool/arxiv/2502.15233)] [[pdf](https://arxiv.org/pdf/2502.15233)]
> **Authors**: Shilong Hou,Ruilin Shang,Zi Long,Xianghua Fu,Yin Chen
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: under review
- **标题**: None
- **领域**: 密码学和安全,计算语言学
- **Abstract**: An increasing number of companies have begun providing services that leverage cloud-based large language models (LLMs), such as ChatGPT. However, this development raises substantial privacy concerns, as users' prompts are transmitted to and processed by the model providers. Among the various privacy protection methods for LLMs, those implemented during the pre-training and fine-tuning phrases fail to mitigate the privacy risks associated with the remote use of cloud-based LLMs by users. On the other hand, methods applied during the inference phrase are primarily effective in scenarios where the LLM's inference does not rely on privacy-sensitive information. In this paper, we outline the process of remote user interaction with LLMs and, for the first time, propose a detailed definition of a general pseudonymization framework applicable to cloud-based LLMs. The experimental results demonstrate that the proposed framework strikes an optimal balance between privacy protection and utility. The code for our method is available to the public at https://github.com/Mebymeby/Pseudonymization-Framework.

## 计算机视觉和模式识别(cs.CV:Computer Vision and Pattern Recognition)

### Fair Foundation Models for Medical Image Analysis: Challenges and Perspectives 
[[arxiv](https://arxiv.org/abs/2502.16841)] [[cool](https://papers.cool/arxiv/2502.16841)] [[pdf](https://arxiv.org/pdf/2502.16841)]
> **Authors**: Dilermando Queiroz,Anderson Carlos,André Anjos,Lilian Berton
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: Ensuring equitable Artificial Intelligence (AI) in healthcare demands systems that make unbiased decisions across all demographic groups, bridging technical innovation with ethical principles. Foundation Models (FMs), trained on vast datasets through self-supervised learning, enable efficient adaptation across medical imaging tasks while reducing dependency on labeled data. These models demonstrate potential for enhancing fairness, though significant challenges remain in achieving consistent performance across demographic groups. Our review indicates that effective bias mitigation in FMs requires systematic interventions throughout all stages of development. While previous approaches focused primarily on model-level bias mitigation, our analysis reveals that fairness in FMs requires integrated interventions throughout the development pipeline, from data documentation to deployment protocols. This comprehensive framework advances current knowledge by demonstrating how systematic bias mitigation, combined with policy engagement, can effectively address both technical and institutional barriers to equitable AI in healthcare. The development of equitable FMs represents a critical step toward democratizing advanced healthcare technologies, particularly for underserved populations and regions with limited medical infrastructure and computational resources.

### FedBM: Stealing Knowledge from Pre-trained Language Models for Heterogeneous Federated Learning 
[[arxiv](https://arxiv.org/abs/2502.16832)] [[cool](https://papers.cool/arxiv/2502.16832)] [[pdf](https://arxiv.org/pdf/2502.16832)]
> **Authors**: Meilu Zhu,Qiushi Yang,Zhifan Gao,Yixuan Yuan,Jun Liu
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: Accepted by MedIA 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Federated learning (FL) has shown great potential in medical image computing since it provides a decentralized learning paradigm that allows multiple clients to train a model collaboratively without privacy leakage. However, current studies have shown that data heterogeneity incurs local learning bias in classifiers and feature extractors of client models during local training, leading to the performance degradation of a federation system. To address these issues, we propose a novel framework called Federated Bias eliMinating (FedBM) to get rid of local learning bias in heterogeneous federated learning (FL), which mainly consists of two modules, i.e., Linguistic Knowledge-based Classifier Construction (LKCC) and Concept-guided Global Distribution Estimation (CGDE). Specifically, LKCC exploits class concepts, prompts and pre-trained language models (PLMs) to obtain concept embeddings. These embeddings are used to estimate the latent concept distribution of each class in the linguistic space. Based on the theoretical derivation, we can rely on these distributions to pre-construct a high-quality classifier for clients to achieve classification optimization, which is frozen to avoid classifier bias during local training. CGDE samples probabilistic concept embeddings from the latent concept distributions to learn a conditional generator to capture the input space of the global model. Three regularization terms are introduced to improve the quality and utility of the generator. The generator is shared by all clients and produces pseudo data to calibrate updates of local feature extractors. Extensive comparison experiments and ablation studies on public datasets demonstrate the superior performance of FedBM over state-of-the-arts and confirm the effectiveness of each module, respectively. The code is available at https://github.com/CUHK-AIM-Group/FedBM.

### CLIP-SENet: CLIP-based Semantic Enhancement Network for Vehicle Re-identification 
[[arxiv](https://arxiv.org/abs/2502.16815)] [[cool](https://papers.cool/arxiv/2502.16815)] [[pdf](https://arxiv.org/pdf/2502.16815)]
> **Authors**: Liping Lu,Zihao Fu,Duanfeng Chu,Wei Wang,Bingrong Xu
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Vehicle re-identification (Re-ID) is a crucial task in intelligent transportation systems (ITS), aimed at retrieving and matching the same vehicle across different surveillance cameras. Numerous studies have explored methods to enhance vehicle Re-ID by focusing on semantic enhancement. However, these methods often rely on additional annotated information to enable models to extract effective semantic features, which brings many limitations. In this work, we propose a CLIP-based Semantic Enhancement Network (CLIP-SENet), an end-to-end framework designed to autonomously extract and refine vehicle semantic attributes, facilitating the generation of more robust semantic feature representations. Inspired by zero-shot solutions for downstream tasks presented by large-scale vision-language models, we leverage the powerful cross-modal descriptive capabilities of the CLIP image encoder to initially extract general semantic information. Instead of using a text encoder for semantic alignment, we design an adaptive fine-grained enhancement module (AFEM) to adaptively enhance this general semantic information at a fine-grained level to obtain robust semantic feature representations. These features are then fused with common Re-ID appearance features to further refine the distinctions between vehicles. Our comprehensive evaluation on three benchmark datasets demonstrates the effectiveness of CLIP-SENet. Our approach achieves new state-of-the-art performance, with 92.9% mAP and 98.7% Rank-1 on VeRi-776 dataset, 90.4% Rank-1 and 98.7% Rank-5 on VehicleID dataset, and 89.1% mAP and 97.9% Rank-1 on the more challenging VeRi-Wild dataset.

### SwimVG: Step-wise Multimodal Fusion and Adaption for Visual Grounding 
[[arxiv](https://arxiv.org/abs/2502.16786)] [[cool](https://papers.cool/arxiv/2502.16786)] [[pdf](https://arxiv.org/pdf/2502.16786)]
> **Authors**: Liangtao Shi,Ting Liu,Xiantao Hu,Yue Hu,Quanjun Yin,Richang Hong
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: 12 pages, 7 figures.Our code is available at https://github.com/liuting20/SwimVG
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Visual grounding aims to ground an image region through natural language, which heavily relies on cross-modal alignment. Most existing methods transfer visual/linguistic knowledge separately by fully fine-tuning uni-modal pre-trained models, followed by a simple stack of visual-language transformers for multimodal fusion. However, these approaches not only limit adequate interaction between visual and linguistic contexts, but also incur significant computational costs. Therefore, to address these issues, we explore a step-wise multimodal fusion and adaption framework, namely SwimVG. Specifically, SwimVG proposes step-wise multimodal prompts (Swip) and cross-modal interactive adapters (CIA) for visual grounding, replacing the cumbersome transformer stacks for multimodal fusion. Swip can improve {the} alignment between the vision and language representations step by step, in a token-level fusion manner. In addition, weight-level CIA further promotes multimodal fusion by cross-modal interaction. Swip and CIA are both parameter-efficient paradigms, and they fuse the cross-modal features from shallow to deep layers gradually. Experimental results on four widely-used benchmarks demonstrate that SwimVG achieves remarkable abilities and considerable benefits in terms of efficiency. Our code is available at https://github.com/liuting20/SwimVG.

### A Transformer-in-Transformer Network Utilizing Knowledge Distillation for Image Recognition 
[[arxiv](https://arxiv.org/abs/2502.16762)] [[cool](https://papers.cool/arxiv/2502.16762)] [[pdf](https://arxiv.org/pdf/2502.16762)]
> **Authors**: Dewan Tauhid Rahman,Yeahia Sarker,Antar Mazumder,Md. Shamim Anower
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: This paper presents a novel knowledge distillation neural architecture leveraging efficient transformer networks for effective image classification. Natural images display intricate arrangements encompassing numerous extraneous elements. Vision transformers utilize localized patches to compute attention. However, exclusive dependence on patch segmentation proves inadequate in sufficiently encompassing the comprehensive nature of the image. To address this issue, we have proposed an inner-outer transformer-based architecture, which gives attention to the global and local aspects of the image. Moreover, The training of transformer models poses significant challenges due to their demanding resource, time, and data requirements. To tackle this, we integrate knowledge distillation into the architecture, enabling efficient learning. Leveraging insights from a larger teacher model, our approach enhances learning efficiency and effectiveness. Significantly, the transformer-in-transformer network acquires lightweight characteristics by means of distillation conducted within the feature extraction layer. Our featured network's robustness is established through substantial experimentation on the MNIST, CIFAR10, and CIFAR100 datasets, demonstrating commendable top-1 and top-5 accuracy. The conducted ablative analysis comprehensively validates the effectiveness of the chosen parameters and settings, showcasing their superiority against contemporary methodologies. Remarkably, the proposed Transformer-in-Transformer Network (TITN) model achieves impressive performance milestones across various datasets: securing the highest top-1 accuracy of 74.71% and a top-5 accuracy of 92.28% for the CIFAR100 dataset, attaining an unparalleled top-1 accuracy of 92.03% and top-5 accuracy of 99.80% for the CIFAR-10 dataset, and registering an exceptional top-1 accuracy of 99.56% for the MNIST dataset.

### GS-TransUNet: Integrated 2D Gaussian Splatting and Transformer UNet for Accurate Skin Lesion Analysis 
[[arxiv](https://arxiv.org/abs/2502.16748)] [[cool](https://papers.cool/arxiv/2502.16748)] [[pdf](https://arxiv.org/pdf/2502.16748)]
> **Authors**: Anand Kumar,Kavinder Roghit Kanthen,Josna John
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: 12 pages, 7 figures, SPIE Medical Imaging 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: We can achieve fast and consistent early skin cancer detection with recent developments in computer vision and deep learning techniques. However, the existing skin lesion segmentation and classification prediction models run independently, thus missing potential efficiencies from their integrated execution. To unify skin lesion analysis, our paper presents the Gaussian Splatting - Transformer UNet (GS-TransUNet), a novel approach that synergistically combines 2D Gaussian splatting with the Transformer UNet architecture for automated skin cancer diagnosis. Our unified deep learning model efficiently delivers dual-function skin lesion classification and segmentation for clinical diagnosis. Evaluated on ISIC-2017 and PH2 datasets, our network demonstrates superior performance compared to existing state-of-the-art models across multiple metrics through 5-fold cross-validation. Our findings illustrate significant advancements in the precision of segmentation and classification. This integration sets new benchmarks in the field and highlights the potential for further research into multi-task medical image analysis methodologies, promising enhancements in automated diagnostic systems.

### Interpretable Retinal Disease Prediction Using Biology-Informed Heterogeneous Graph Representations 
[[arxiv](https://arxiv.org/abs/2502.16697)] [[cool](https://papers.cool/arxiv/2502.16697)] [[pdf](https://arxiv.org/pdf/2502.16697)]
> **Authors**: Laurin Lux,Alexander H. Berger,Maria Romeo Tricas,Alaa E. Fayed,Sobha Sivaprasada,Linus Kreitner,Jonas Weidner,Martin J. Menten,Daniel Rueckert,Johannes C. Paetzold
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Interpretability is crucial to enhance trust in machine learning models for medical diagnostics. However, most state-of-the-art image classifiers based on neural networks are not interpretable. As a result, clinicians often resort to known biomarkers for diagnosis, although biomarker-based classification typically performs worse than large neural networks. This work proposes a method that surpasses the performance of established machine learning models while simultaneously improving prediction interpretability for diabetic retinopathy staging from optical coherence tomography angiography (OCTA) images. Our method is based on a novel biology-informed heterogeneous graph representation that models retinal vessel segments, intercapillary areas, and the foveal avascular zone (FAZ) in a human-interpretable way. This graph representation allows us to frame diabetic retinopathy staging as a graph-level classification task, which we solve using an efficient graph neural network. We benchmark our method against well-established baselines, including classical biomarker-based classifiers, convolutional neural networks (CNNs), and vision transformers. Our model outperforms all baselines on two datasets. Crucially, we use our biology-informed graph to provide explanations of unprecedented detail. Our approach surpasses existing methods in precisely localizing and identifying critical vessels or intercapillary areas. In addition, we give informative and human-interpretable attributions to critical characteristics. Our work contributes to the development of clinical decision-support tools in ophthalmology.

### AeroReformer: Aerial Referring Transformer for UAV-based Referring Image Segmentation 
[[arxiv](https://arxiv.org/abs/2502.16680)] [[cool](https://papers.cool/arxiv/2502.16680)] [[pdf](https://arxiv.org/pdf/2502.16680)]
> **Authors**: Rui Li
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: As a novel and challenging task, referring segmentation combines computer vision and natural language processing to localize and segment objects based on textual descriptions. While referring image segmentation (RIS) has been extensively studied in natural images, little attention has been given to aerial imagery, particularly from unmanned aerial vehicles (UAVs). The unique challenges of UAV imagery, including complex spatial scales, occlusions, and varying object orientations, render existing RIS approaches ineffective. A key limitation has been the lack of UAV-specific datasets, as manually annotating pixel-level masks and generating textual descriptions is labour-intensive and time-consuming. To address this gap, we design an automatic labelling pipeline that leverages pre-existing UAV segmentation datasets and Multimodal Large Language Models (MLLM) for generating textual descriptions. Furthermore, we propose Aerial Referring Transformer (AeroReformer), a novel framework for UAV referring image segmentation (UAV-RIS), featuring a Vision-Language Cross-Attention Module (VLCAM) for effective cross-modal understanding and a Rotation-Aware Multi-Scale Fusion (RAMSF) decoder to enhance segmentation accuracy in aerial scenes. Extensive experiments on two newly developed datasets demonstrate the superiority of AeroReformer over existing methods, establishing a new benchmark for UAV-RIS. The datasets and code will be publicly available at: https://github.com/lironui/AeroReformer.

### VPNeXt -- Rethinking Dense Decoding for Plain Vision Transformer 
[[arxiv](https://arxiv.org/abs/2502.16654)] [[cool](https://papers.cool/arxiv/2502.16654)] [[pdf](https://arxiv.org/pdf/2502.16654)]
> **Authors**: Xikai Tang,Ye Huang,Guangqiang Yin,Lixin Duan
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: Tech report, minor fix
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: We present VPNeXt, a new and simple model for the Plain Vision Transformer (ViT). Unlike the many related studies that share the same homogeneous paradigms, VPNeXt offers a fresh perspective on dense representation based on ViT. In more detail, the proposed VPNeXt addressed two concerns about the existing paradigm: (1) Is it necessary to use a complex Transformer Mask Decoder architecture to obtain good representations? (2) Does the Plain ViT really need to depend on the mock pyramid feature for upsampling? For (1), we investigated the potential underlying reasons that contributed to the effectiveness of the Transformer Decoder and introduced the Visual Context Replay (VCR) to achieve similar effects efficiently. For (2), we introduced the ViTUp module. This module fully utilizes the previously overlooked ViT real pyramid feature to achieve better upsampling results compared to the earlier mock pyramid feature. This represents the first instance of such functionality in the field of semantic segmentation for Plain ViT. We performed ablation studies on related modules to verify their effectiveness gradually. We conducted relevant comparative experiments and visualizations to show that VPNeXt achieved state-of-the-art performance with a simple and effective design. Moreover, the proposed VPNeXt significantly exceeded the long-established mIoU wall/barrier of the VOC2012 dataset, setting a new state-of-the-art by a large margin, which also stands as the largest improvement since 2015.

### Retrieval-Augmented Visual Question Answering via Built-in Autoregressive Search Engines 
[[arxiv](https://arxiv.org/abs/2502.16641)] [[cool](https://papers.cool/arxiv/2502.16641)] [[pdf](https://arxiv.org/pdf/2502.16641)]
> **Authors**: Xinwei Long,Zhiyuan Ma,Ermo Hua,Kaiyan Zhang,Biqing Qi,Bowen Zhou
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: AAAI-25
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算语言学,信息检索
- **Abstract**: Retrieval-augmented generation (RAG) has emerged to address the knowledge-intensive visual question answering (VQA) task. Current methods mainly employ separate retrieval and generation modules to acquire external knowledge and generate answers, respectively. We propose ReAuSE, an alternative to the previous RAG model for the knowledge-based VQA task, which seamlessly integrates knowledge retriever into the generative multi-modal large language model, serving as a built-in search engine. Specifically, our model functions both as a generative retriever and an accurate answer generator. It not only helps retrieve documents from the knowledge base by producing identifiers for each document, but it also answers visual questions based on the retrieved documents. Furthermore, we propose a reinforced retrieval calibration module from relevance feedback to improve retrieval performance and align with the preferences for accurate answer generation. Extensive experiments on two representative OKVQA and A-OKVQA datasets demonstrate significant improvements ranging from 2.9\% to 9.6\% across all evaluation metrics when compared to strong baselines.

### Can Large Vision-Language Models Detect Images Copyright Infringement from GenAI? 
[[arxiv](https://arxiv.org/abs/2502.16618)] [[cool](https://papers.cool/arxiv/2502.16618)] [[pdf](https://arxiv.org/pdf/2502.16618)]
> **Authors**: Qipan Xu,Zhenting Wang,Xiaoxiao He,Ligong Han,Ruixiang Tang
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **Abstract**: Generative AI models, renowned for their ability to synthesize high-quality content, have sparked growing concerns over the improper generation of copyright-protected material. While recent studies have proposed various approaches to address copyright issues, the capability of large vision-language models (LVLMs) to detect copyright infringements remains largely unexplored. In this work, we focus on evaluating the copyright detection abilities of state-of-the-art LVLMs using a various set of image samples. Recognizing the absence of a comprehensive dataset that includes both IP-infringement samples and ambiguous non-infringement negative samples, we construct a benchmark dataset comprising positive samples that violate the copyright protection of well-known IP figures, as well as negative samples that resemble these figures but do not raise copyright concerns. This dataset is created using advanced prompt engineering techniques. We then evaluate leading LVLMs using our benchmark dataset. Our experimental results reveal that LVLMs are prone to overfitting, leading to the misclassification of some negative samples as IP-infringement cases. In the final section, we analyze these failure cases and propose potential solutions to mitigate the overfitting problem.

### AdverX-Ray: Ensuring X-Ray Integrity Through Frequency-Sensitive Adversarial VAEs 
[[arxiv](https://arxiv.org/abs/2502.16610)] [[cool](https://papers.cool/arxiv/2502.16610)] [[pdf](https://arxiv.org/pdf/2502.16610)]
> **Authors**: Francisco Caetano,Christiaan Viviers,Lena Filatova,Peter H. N. de With,Fons van der Sommen
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: SPIE Medical Imaging 2025 Runner-up 2025 Robert F. Wagner All-Conference Best Student Paper Award
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Ensuring the quality and integrity of medical images is crucial for maintaining diagnostic accuracy in deep learning-based Computer-Aided Diagnosis and Computer-Aided Detection (CAD) systems. Covariate shifts are subtle variations in the data distribution caused by different imaging devices or settings and can severely degrade model performance, similar to the effects of adversarial attacks. Therefore, it is vital to have a lightweight and fast method to assess the quality of these images prior to using CAD models. AdverX-Ray addresses this need by serving as an image-quality assessment layer, designed to detect covariate shifts effectively. This Adversarial Variational Autoencoder prioritizes the discriminator's role, using the suboptimal outputs of the generator as negative samples to fine-tune the discriminator's ability to identify high-frequency artifacts. Images generated by adversarial networks often exhibit severe high-frequency artifacts, guiding the discriminator to focus excessively on these components. This makes the discriminator ideal for this approach. Trained on patches from X-ray images of specific machine models, AdverX-Ray can evaluate whether a scan matches the training distribution, or if a scan from the same machine is captured under different settings. Extensive comparisons with various OOD detection methods show that AdverX-Ray significantly outperforms existing techniques, achieving a 96.2% average AUROC using only 64 random patches from an X-ray. Its lightweight and fast architecture makes it suitable for real-time applications, enhancing the reliability of medical imaging systems. The code and pretrained models are publicly available.

### VidLBEval: Benchmarking and Mitigating Language Bias in Video-Involved LVLMs 
[[arxiv](https://arxiv.org/abs/2502.16602)] [[cool](https://papers.cool/arxiv/2502.16602)] [[pdf](https://arxiv.org/pdf/2502.16602)]
> **Authors**: Yiming Yang,Yangyang Guo,Hui Lu,Yan Wang
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Recently, Large Vision-Language Models (LVLMs) have made significant strides across diverse multimodal tasks and benchmarks. This paper reveals a largely under-explored problem from existing video-involved LVLMs - language bias, where models tend to prioritize language over video and thus result in incorrect responses. To address this research gap, we first collect a Video Language Bias Evaluation Benchmark, which is specifically designed to assess the language bias in video-involved LVLMs through two key tasks: ambiguous video contrast and interrogative question probing. Accordingly, we design accompanied evaluation metrics that aim to penalize LVLMs being biased by language. In addition, we also propose Multi-branch Contrastive Decoding (MCD), introducing two expert branches to simultaneously counteract language bias potentially generated by the amateur text-only branch. Our experiments demonstrate that i) existing video-involved LVLMs, including both proprietary and open-sourced, are largely limited by the language bias problem; ii) our MCD can effectively mitigate this issue and maintain general-purpose capabilities in various video-involved LVLMs without any additional retraining or alteration to model architectures.

### Multimodal Large Language Models for Text-rich Image Understanding: A Comprehensive Review 
[[arxiv](https://arxiv.org/abs/2502.16586)] [[cool](https://papers.cool/arxiv/2502.16586)] [[pdf](https://arxiv.org/pdf/2502.16586)]
> **Authors**: Pei Fu,Tongkun Guan,Zining Wang,Zhentao Guo,Chen Duan,Hao Sun,Boming Chen,Jiayao Ma,Qianyi Jiang,Kai Zhou,Junfeng Luo
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: The recent emergence of Multi-modal Large Language Models (MLLMs) has introduced a new dimension to the Text-rich Image Understanding (TIU) field, with models demonstrating impressive and inspiring performance. However, their rapid evolution and widespread adoption have made it increasingly challenging to keep up with the latest advancements. To address this, we present a systematic and comprehensive survey to facilitate further research on TIU MLLMs. Initially, we outline the timeline, architecture, and pipeline of nearly all TIU MLLMs. Then, we review the performance of selected models on mainstream benchmarks. Finally, we explore promising directions, challenges, and limitations within the field.

### EDocNet: Efficient Datasheet Layout Analysis Based on Focus and Global Knowledge Distillation 
[[arxiv](https://arxiv.org/abs/2502.16541)] [[cool](https://papers.cool/arxiv/2502.16541)] [[pdf](https://arxiv.org/pdf/2502.16541)]
> **Authors**: Hong Cai Chen,Longchang Wu,Yang Zhang
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: 9 pages, 6 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: When designing circuits, engineers obtain the information of electronic devices by browsing a large number of documents, which is low efficiency and heavy workload. The use of artificial intelligence technology to automatically parse documents can greatly improve the efficiency of engineers. However, the current document layout analysis model is aimed at various types of documents and is not suitable for electronic device documents. This paper proposes to use EDocNet to realize the document layout analysis function for document analysis, and use the electronic device document data set created by myself for training. The training method adopts the focus and global knowledge distillation method, and a model suitable for electronic device documents is obtained, which can divide the contents of electronic device documents into 21 categories. It has better average accuracy and average recall rate. It also greatly improves the speed of model checking.

### Color Information-Based Automated Mask Generation for Detecting Underwater Atypical Glare Areas 
[[arxiv](https://arxiv.org/abs/2502.16538)] [[cool](https://papers.cool/arxiv/2502.16538)] [[pdf](https://arxiv.org/pdf/2502.16538)]
> **Authors**: Mingyu Jeon,Yeonji Paeng,Sejin Lee
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: 7pages, 6 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,图像和视频处理
- **Abstract**: Underwater diving assistance and safety support robots acquire real-time diver information through onboard underwater cameras. This study introduces a breath bubble detection algorithm that utilizes unsupervised K-means clustering, thereby addressing the high accuracy demands of deep learning models as well as the challenges associated with constructing supervised datasets. The proposed method fuses color data and relative spatial coordinates from underwater images, employs CLAHE to mitigate noise, and subsequently performs pixel clustering to isolate reflective regions. Experimental results demonstrate that the algorithm can effectively detect regions corresponding to breath bubbles in underwater images, and that the combined use of RGB, LAB, and HSV color spaces significantly enhances detection accuracy. Overall, this research establishes a foundation for monitoring diver conditions and identifying potential equipment malfunctions in underwater environments.

### Deep unrolling for learning optimal spatially varying regularisation parameters for Total Generalised Variation 
[[arxiv](https://arxiv.org/abs/2502.16532)] [[cool](https://papers.cool/arxiv/2502.16532)] [[pdf](https://arxiv.org/pdf/2502.16532)]
> **Authors**: Thanh Trung Vu,Andreas Kofler,Kostas Papafitsoros
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习,优化与控制
- **Abstract**: We extend a recently introduced deep unrolling framework for learning spatially varying regularisation parameters in inverse imaging problems to the case of Total Generalised Variation (TGV). The framework combines a deep convolutional neural network (CNN) inferring the two spatially varying TGV parameters with an unrolled algorithmic scheme that solves the corresponding variational problem. The two subnetworks are jointly trained end-to-end in a supervised fashion and as such the CNN learns to compute those parameters that drive the reconstructed images as close to the ground truth as possible. Numerical results in image denoising and MRI reconstruction show a significant qualitative and quantitative improvement compared to the best TGV scalar parameter case as well as to other approaches employing spatially varying parameters computed by unsupervised methods. We also observe that the inferred spatially varying parameter maps have a consistent structure near the image edges, asking for further theoretical investigations. In particular, the parameter that weighs the first-order TGV term has a triple-edge structure with alternating high-low-high values whereas the one that weighs the second-order term attains small values in a large neighbourhood around the edges.

### MQADet: A Plug-and-Play Paradigm for Enhancing Open-Vocabulary Object Detection via Multimodal Question Answering 
[[arxiv](https://arxiv.org/abs/2502.16486)] [[cool](https://papers.cool/arxiv/2502.16486)] [[pdf](https://arxiv.org/pdf/2502.16486)]
> **Authors**: Caixiong Li,Xiongwei Zhao,Jinhang Zhang,Xing Zhang,Qihao Sun,Zhou Wu
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Open-vocabulary detection (OVD) is a challenging task to detect and classify objects from an unrestricted set of categories, including those unseen during training. Existing open-vocabulary detectors are limited by complex visual-textual misalignment and long-tailed category imbalances, leading to suboptimal performance in challenging scenarios. To address these limitations, we introduce MQADet, a universal paradigm for enhancing existing open-vocabulary detectors by leveraging the cross-modal reasoning capabilities of multimodal large language models (MLLMs). MQADet functions as a plug-and-play solution that integrates seamlessly with pre-trained object detectors without substantial additional training costs. Specifically, we design a novel three-stage Multimodal Question Answering (MQA) pipeline to guide the MLLMs to precisely localize complex textual and visual targets while effectively enhancing the focus of existing object detectors on relevant objects. To validate our approach, we present a new benchmark for evaluating our paradigm on four challenging open-vocabulary datasets, employing three state-of-the-art object detectors as baselines. Experimental results demonstrate that our proposed paradigm significantly improves the performance of existing detectors, particularly in unseen complex categories, across diverse and challenging scenarios. To facilitate future research, we will publicly release our code.

### Cross-domain Few-shot Object Detection with Multi-modal Textual Enrichment 
[[arxiv](https://arxiv.org/abs/2502.16469)] [[cool](https://papers.cool/arxiv/2502.16469)] [[pdf](https://arxiv.org/pdf/2502.16469)]
> **Authors**: Zeyu Shangguan,Daniel Seita,Mohammad Rostami
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: arXiv admin note: substantial text overlap with arXiv:2403.16188
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Advancements in cross-modal feature extraction and integration have significantly enhanced performance in few-shot learning tasks. However, current multi-modal object detection (MM-OD) methods often experience notable performance degradation when encountering substantial domain shifts. We propose that incorporating rich textual information can enable the model to establish a more robust knowledge relationship between visual instances and their corresponding language descriptions, thereby mitigating the challenges of domain shift. Specifically, we focus on the problem of Cross-Domain Multi-Modal Few-Shot Object Detection (CDMM-FSOD) and introduce a meta-learning-based framework designed to leverage rich textual semantics as an auxiliary modality to achieve effective domain adaptation. Our new architecture incorporates two key components: (i) A multi-modal feature aggregation module, which aligns visual and linguistic feature embeddings to ensure cohesive integration across modalities. (ii) A rich text semantic rectification module, which employs bidirectional text feature generation to refine multi-modal feature alignment, thereby enhancing understanding of language and its application in object detection. We evaluate the proposed method on common cross-domain object detection benchmarks and demonstrate that it significantly surpasses existing few-shot object detection approaches.

### VisFactor: Benchmarking Fundamental Visual Cognition in Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.16435)] [[cool](https://papers.cool/arxiv/2502.16435)] [[pdf](https://arxiv.org/pdf/2502.16435)]
> **Authors**: Jen-Tse Huang,Dasen Dai,Jen-Yuan Huang,Youliang Yuan,Xiaoyuan Liu,Wenxuan Wang,Wenxiang Jiao,Pinjia He,Zhaopeng Tu
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: Working in Progress
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算语言学
- **Abstract**: Multimodal Large Language Models (MLLMs) have demonstrated remarkable advancements in multimodal understanding; however, their fundamental visual cognitive abilities remain largely underexplored. To bridge this gap, we introduce VisFactor, a novel benchmark derived from the Factor-Referenced Cognitive Test (FRCT), a well-established psychometric assessment of human cognition. VisFactor digitalizes vision-related FRCT subtests to systematically evaluate MLLMs across essential visual cognitive tasks including spatial reasoning, perceptual speed, and pattern recognition. We present a comprehensive evaluation of state-of-the-art MLLMs, such as GPT-4o, Gemini-Pro, and Qwen-VL, using VisFactor under diverse prompting strategies like Chain-of-Thought and Multi-Agent Debate. Our findings reveal a concerning deficiency in current MLLMs' fundamental visual cognition, with performance frequently approaching random guessing and showing only marginal improvements even with advanced prompting techniques. These results underscore the critical need for focused research to enhance the core visual reasoning capabilities of MLLMs. To foster further investigation in this area, we release our VisFactor benchmark at https://github.com/CUHK-ARISE/VisFactor.

### Visual Reasoning Evaluation of Grok, Deepseek Janus, Gemini, Qwen, Mistral, and ChatGPT 
[[arxiv](https://arxiv.org/abs/2502.16428)] [[cool](https://papers.cool/arxiv/2502.16428)] [[pdf](https://arxiv.org/pdf/2502.16428)]
> **Authors**: Nidhal Jegham,Marwan Abdelatti,Abdeltawab Hendawi
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Traditional evaluations of multimodal large language models (LLMs) have been limited by their focus on single-image reasoning, failing to assess crucial aspects like contextual understanding, reasoning stability, and uncertainty calibration. This study addresses these limitations by introducing a novel benchmark that integrates multi-image reasoning tasks with rejection-based evaluation and positional bias detection. To evaluate these dimensions, we further introduce entropy as a novel metric for quantifying reasoning consistency across reordered answer variants. We applied this benchmark to assess Grok 3, ChatGPT-4o, ChatGPT-o1, Gemini 2.0 Flash Experimental, DeepSeek Janus models, Qwen2.5-VL-72B-Instruct, QVQ-72B-Preview, and Pixtral 12B across eight visual reasoning tasks, including difference spotting and diagram interpretation. Our findings reveal ChatGPT-o1 leading in overall accuracy (82.5\%) and rejection accuracy (70.0\%), closely followed by Gemini 2.0 Flash Experimental (70.8\%). QVQ-72B-Preview demonstrated superior rejection accuracy (85.5\%). Notably, Pixtral 12B (51.7\%) showed promise in specific domains, while Janus models exhibited challenges in bias and uncertainty calibration, reflected in low rejection accuracies and high entropy scores. High entropy scores in Janus models (Janus 7B: 0.8392, Janus 1B: 0.787) underscore their susceptibility to positional bias and unstable reasoning, contrasting with the low entropy and robust reasoning of ChatGPT models. The study further demonstrates that model size is not the sole determinant of performance, as evidenced by Grok 3 underperformance despite its substantial parameter count. By employing multi-image contexts, rejection mechanisms, and entropy-based consistency metrics, this benchmark sets a new standard for evaluating multimodal LLMs, enabling a more robust and reliable assessment of next-generation AI systems.

### Fine-Grained Video Captioning through Scene Graph Consolidation 
[[arxiv](https://arxiv.org/abs/2502.16427)] [[cool](https://papers.cool/arxiv/2502.16427)] [[pdf](https://arxiv.org/pdf/2502.16427)]
> **Authors**: Sanghyeok Chu,Seonguk Seo,Bohyung Han
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Recent advances in visual language models (VLMs) have significantly improved image captioning, but extending these gains to video understanding remains challenging due to the scarcity of fine-grained video captioning datasets. To bridge this gap, we propose a novel zero-shot video captioning approach that combines frame-level scene graphs from a video to obtain intermediate representations for caption generation. Our method first generates frame-level captions using an image VLM, converts them into scene graphs, and consolidates these graphs to produce comprehensive video-level descriptions. To achieve this, we leverage a lightweight graph-to-text model trained solely on text corpora, eliminating the need for video captioning annotations. Experiments on the MSR-VTT and ActivityNet Captions datasets show that our approach outperforms zero-shot video captioning baselines, demonstrating that aggregating frame-level scene graphs yields rich video understanding without requiring large-scale paired data or high inference cost.

### High-resolution Rainy Image Synthesis: Learning from Rendering 
[[arxiv](https://arxiv.org/abs/2502.16421)] [[cool](https://papers.cool/arxiv/2502.16421)] [[pdf](https://arxiv.org/pdf/2502.16421)]
> **Authors**: Kaibin Zhou,Shengjie Zhao,Hao Deng,Lin Zhang
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Currently, there are few effective methods for synthesizing a mass of high-resolution rainy images in complex illumination conditions. However, these methods are essential for synthesizing large-scale high-quality paired rainy-clean image datasets, which can train deep learning-based single image rain removal models capable of generalizing to various illumination conditions. Therefore, we propose a practical two-stage learning-from-rendering pipeline for high-resolution rainy image synthesis. The pipeline combines the benefits of the realism of rendering-based methods and the high-efficiency of learning-based methods, providing the possibility of creating large-scale high-quality paired rainy-clean image datasets. In the rendering stage, we use a rendering-based method to create a High-resolution Rainy Image (HRI) dataset, which contains realistic high-resolution paired rainy-clean images of multiple scenes and various illumination conditions. In the learning stage, to learn illumination information from background images for high-resolution rainy image generation, we propose a High-resolution Rainy Image Generation Network (HRIGNet). HRIGNet is designed to introduce a guiding diffusion model in the Latent Diffusion Model, which provides additional guidance information for high-resolution image synthesis. In our experiments, HRIGNet is able to synthesize high-resolution rainy images up to 2048x1024 resolution. Rain removal experiments on real dataset validate that our method can help improve the robustness of deep derainers to real rainy images. To make our work reproducible, source codes and the dataset have been released at https://kb824999404.github.io/HRIG/.

### A Survey on Industrial Anomalies Synthesis 
[[arxiv](https://arxiv.org/abs/2502.16412)] [[cool](https://papers.cool/arxiv/2502.16412)] [[pdf](https://arxiv.org/pdf/2502.16412)]
> **Authors**: Xichen Xu,Yanshu Wang,Yawen Huang,Jiaqi Liu,Xiaoning Lei,Guoyang Xie,Guannan Jiang,Zhichao Lu
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算工程、金融和科学
- **Abstract**: This paper comprehensively reviews anomaly synthesis methodologies. Existing surveys focus on limited techniques, missing an overall field view and understanding method interconnections. In contrast, our study offers a unified review, covering about 40 representative methods across Hand-crafted, Distribution-hypothesis-based, Generative models (GM)-based, and Vision-language models (VLM)-based synthesis. We introduce the first industrial anomaly synthesis (IAS) taxonomy. Prior works lack formal classification or use simplistic taxonomies, hampering structured comparisons and trend identification. Our taxonomy provides a fine-grained framework reflecting methodological progress and practical implications, grounding future research. Furthermore, we explore cross-modality synthesis and large-scale VLM. Previous surveys overlooked multimodal data and VLM in anomaly synthesis, limiting insights into their advantages. Our survey analyzes their integration, benefits, challenges, and prospects, offering a roadmap to boost IAS with multimodal learning. More resources are available at https://github.com/M-3LAB/awesome-anomaly-synthesis.

### An Expert Ensemble for Detecting Anomalous Scenes, Interactions, and Behaviors in Autonomous Driving 
[[arxiv](https://arxiv.org/abs/2502.16389)] [[cool](https://papers.cool/arxiv/2502.16389)] [[pdf](https://arxiv.org/pdf/2502.16389)]
> **Authors**: Tianchen Ji,Neeloy Chakraborty,Andre Schreiber,Katherine Driggs-Campbell
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: Accepted by International Journal of Robotics Research (IJRR)
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习,机器人技术
- **Abstract**: As automated vehicles enter public roads, safety in a near-infinite number of driving scenarios becomes one of the major concerns for the widespread adoption of fully autonomous driving. The ability to detect anomalous situations outside of the operational design domain is a key component in self-driving cars, enabling us to mitigate the impact of abnormal ego behaviors and to realize trustworthy driving systems. On-road anomaly detection in egocentric videos remains a challenging problem due to the difficulties introduced by complex and interactive scenarios. We conduct a holistic analysis of common on-road anomaly patterns, from which we propose three unsupervised anomaly detection experts: a scene expert that focuses on frame-level appearances to detect abnormal scenes and unexpected scene motions; an interaction expert that models normal relative motions between two road participants and raises alarms whenever anomalous interactions emerge; and a behavior expert which monitors abnormal behaviors of individual objects by future trajectory prediction. To combine the strengths of all the modules, we propose an expert ensemble (Xen) using a Kalman filter, in which the final anomaly score is absorbed as one of the states and the observations are generated by the experts. Our experiments employ a novel evaluation protocol for realistic model performance, demonstrate superior anomaly detection performance than previous methods, and show that our framework has potential in classifying anomaly types using unsupervised learning on a large-scale on-road anomaly dataset.

### MOB-GCN: A Novel Multiscale Object-Based Graph Neural Network for Hyperspectral Image Classification 
[[arxiv](https://arxiv.org/abs/2502.16289)] [[cool](https://papers.cool/arxiv/2502.16289)] [[pdf](https://arxiv.org/pdf/2502.16289)]
> **Authors**: Tuan-Anh Yang,Truong-Son Hy,Phuong D. Dao
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: This paper introduces a novel multiscale object-based graph neural network called MOB-GCN for hyperspectral image (HSI) classification. The central aim of this study is to enhance feature extraction and classification performance by utilizing multiscale object-based image analysis (OBIA). Traditional pixel-based methods often suffer from low accuracy and speckle noise, while single-scale OBIA approaches may overlook crucial information of image objects at different levels of detail. MOB-GCN overcomes these challenges by extracting and integrating features from multiple segmentation scales, leveraging the Multiresolution Graph Network (MGN) architecture to capture both fine-grained and global spatial patterns. MOB-GCN addresses this issue by extracting and integrating features from multiple segmentation scales to improve classification results using the Multiresolution Graph Network (MGN) architecture that can model fine-grained and global spatial patterns. By constructing a dynamic multiscale graph hierarchy, MOB-GCN offers a more comprehensive understanding of the intricate details and global context of HSIs. Experimental results demonstrate that MOB-GCN consistently outperforms single-scale graph convolutional networks (GCNs) in terms of classification accuracy, computational efficiency, and noise reduction, particularly when labeled data is limited. The implementation of MOB-GCN is publicly available at https://github.com/HySonLab/MultiscaleHSI

### DiffFake: Exposing Deepfakes using Differential Anomaly Detection 
[[arxiv](https://arxiv.org/abs/2502.16247)] [[cool](https://papers.cool/arxiv/2502.16247)] [[pdf](https://arxiv.org/pdf/2502.16247)]
> **Authors**: Sotirios Stamnas,Victor Sanchez
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: Accepted at WACV 2025 AI4MFDD Workshop
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Traditional deepfake detectors have dealt with the detection problem as a binary classification task. This approach can achieve satisfactory results in cases where samples of a given deepfake generation technique have been seen during training, but can easily fail with deepfakes generated by other techniques. In this paper, we propose DiffFake, a novel deepfake detector that approaches the detection problem as an anomaly detection task. Specifically, DiffFake learns natural changes that occur between two facial images of the same person by leveraging a differential anomaly detection framework. This is done by combining pairs of deep face embeddings and using them to train an anomaly detection model. We further propose to train a feature extractor on pseudo-deepfakes with global and local artifacts, to extract meaningful and generalizable features that can then be used to train the anomaly detection model. We perform extensive experiments on five different deepfake datasets and show that our method can match and sometimes even exceed the performance of state-of-the-art competitors.

### Prompt as Knowledge Bank: Boost Vision-language model via Structural Representation for zero-shot medical detection 
[[arxiv](https://arxiv.org/abs/2502.16223)] [[cool](https://papers.cool/arxiv/2502.16223)] [[pdf](https://arxiv.org/pdf/2502.16223)]
> **Authors**: Yuguang Yang,Tongfei Chen,Haoyu Huang,Linlin Yang,Chunyu Xie,Dawei Leng,Xianbin Cao,Baochang Zhang
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: Accepted as ICLR 2025 conference paper
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Zero-shot medical detection can further improve detection performance without relying on annotated medical images even upon the fine-tuned model, showing great clinical value. Recent studies leverage grounded vision-language models (GLIP) to achieve this by using detailed disease descriptions as prompts for the target disease name during the inference phase. However, these methods typically treat prompts as equivalent context to the target name, making it difficult to assign specific disease knowledge based on visual information, leading to a coarse alignment between images and target descriptions. In this paper, we propose StructuralGLIP, which introduces an auxiliary branch to encode prompts into a latent knowledge bank layer-by-layer, enabling more context-aware and fine-grained alignment. Specifically, in each layer, we select highly similar features from both the image representation and the knowledge bank, forming structural representations that capture nuanced relationships between image patches and target descriptions. These features are then fused across modalities to further enhance detection performance. Extensive experiments demonstrate that StructuralGLIP achieves a +4.1\% AP improvement over prior state-of-the-art methods across seven zero-shot medical detection benchmarks, and consistently improves fine-tuned models by +3.2\% AP on endoscopy image datasets.

### Mojito: LLM-Aided Motion Instructor with Jitter-Reduced Inertial Tokens 
[[arxiv](https://arxiv.org/abs/2502.16175)] [[cool](https://papers.cool/arxiv/2502.16175)] [[pdf](https://arxiv.org/pdf/2502.16175)]
> **Authors**: Ziwei Shan,Yaoyu He,Chengfeng Zhao,Jiashen Du,Jingyan Zhang,Qixuan Zhang,Jingyi Yu,Lan Xu
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: First three authors contribute equally. Project page: https://koyui.github.io/mojito/
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,图形
- **Abstract**: Human bodily movements convey critical insights into action intentions and cognitive processes, yet existing multimodal systems primarily focused on understanding human motion via language, vision, and audio, which struggle to capture the dynamic forces and torques inherent in 3D motion. Inertial measurement units (IMUs) present a promising alternative, offering lightweight, wearable, and privacy-conscious motion sensing. However, processing of streaming IMU data faces challenges such as wireless transmission instability, sensor noise, and drift, limiting their utility for long-term real-time motion capture (MoCap), and more importantly, online motion analysis. To address these challenges, we introduce Mojito, an intelligent motion agent that integrates inertial sensing with large language models (LLMs) for interactive motion capture and behavioral analysis.

### PersGuard: Preventing Malicious Personalization via Backdoor Attacks on Pre-trained Text-to-Image Diffusion Models 
[[arxiv](https://arxiv.org/abs/2502.16167)] [[cool](https://papers.cool/arxiv/2502.16167)] [[pdf](https://arxiv.org/pdf/2502.16167)]
> **Authors**: Xinwei Liu,Xiaojun Jia,Yuan Xun,Hua Zhang,Xiaochun Cao
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Diffusion models (DMs) have revolutionized data generation, particularly in text-to-image (T2I) synthesis. However, the widespread use of personalized generative models raises significant concerns regarding privacy violations and copyright infringement. To address these issues, researchers have proposed adversarial perturbation-based protection techniques. However, these methods have notable limitations, including insufficient robustness against data transformations and the inability to fully eliminate identifiable features of protected objects in the generated output. In this paper, we introduce PersGuard, a novel backdoor-based approach that prevents malicious personalization of specific images. Unlike traditional adversarial perturbation methods, PersGuard implant backdoor triggers into pre-trained T2I models, preventing the generation of customized outputs for designated protected images while allowing normal personalization for unprotected ones. Unfortunately, existing backdoor methods for T2I diffusion models fail to be applied to personalization scenarios due to the different backdoor objectives and the potential backdoor elimination during downstream fine-tuning processes. To address these, we propose three novel backdoor objectives specifically designed for personalization scenarios, coupled with backdoor retention loss engineered to resist downstream fine-tuning. These components are integrated into a unified optimization framework. Extensive experimental evaluations demonstrate PersGuard's effectiveness in preserving data privacy, even under challenging conditions including gray-box settings, multi-object protection, and facial identity scenarios. Our method significantly outperforms existing techniques, offering a more robust solution for privacy and copyright protection.

### A Deep Learning Framework with Geographic Information Adaptive Loss for Remote Sensing Images based UAV Self-Positioning 
[[arxiv](https://arxiv.org/abs/2502.16164)] [[cool](https://papers.cool/arxiv/2502.16164)] [[pdf](https://arxiv.org/pdf/2502.16164)]
> **Authors**: Mingkun Li,Ziming Wang,Guang Huo,Wei Chen,Xiaoning Zhao
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: With the expanding application scope of unmanned aerial vehicles (UAVs), the demand for stable UAV control has significantly increased. However, in complex environments, GPS signals are prone to interference, resulting in ineffective UAV positioning. Therefore, self-positioning of UAVs in GPS-denied environments has become a critical objective. Some methods obtain geolocation information in GPS-denied environments by matching ground objects in the UAV viewpoint with remote sensing images. However, most of these methods only provide coarse-level positioning, which satisfies cross-view geo-localization but cannot support precise UAV positioning tasks. Consequently, this paper focuses on a newer and more challenging task: precise UAV self-positioning based on remote sensing images. This approach not only considers the features of ground objects but also accounts for the spatial distribution of objects in the images. To address this challenge, we present a deep learning framework with geographic information adaptive loss, which achieves precise localization by aligning UAV images with corresponding satellite imagery in fine detail through the integration of geographic information from multiple perspectives. To validate the effectiveness of the proposed method, we conducted a series of experiments. The results demonstrate the method's efficacy in enabling UAVs to achieve precise self-positioning using remote sensing imagery.

### OmniParser V2: Structured-Points-of-Thought for Unified Visual Text Parsing and Its Generality to Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.16161)] [[cool](https://papers.cool/arxiv/2502.16161)] [[pdf](https://arxiv.org/pdf/2502.16161)]
> **Authors**: Wenwen Yu,Zhibo Yang,Jianqiang Wan,Sibo Song,Jun Tang,Wenqing Cheng,Yuliang Liu,Xiang Bai
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算语言学
- **Abstract**: Visually-situated text parsing (VsTP) has recently seen notable advancements, driven by the growing demand for automated document understanding and the emergence of large language models capable of processing document-based questions. While various methods have been proposed to tackle the complexities of VsTP, existing solutions often rely on task-specific architectures and objectives for individual tasks. This leads to modal isolation and complex workflows due to the diversified targets and heterogeneous schemas. In this paper, we introduce OmniParser V2, a universal model that unifies VsTP typical tasks, including text spotting, key information extraction, table recognition, and layout analysis, into a unified framework. Central to our approach is the proposed Structured-Points-of-Thought (SPOT) prompting schemas, which improves model performance across diverse scenarios by leveraging a unified encoder-decoder architecture, objective, and input\&output representation. SPOT eliminates the need for task-specific architectures and loss functions, significantly simplifying the processing pipeline. Our extensive evaluations across four tasks on eight different datasets show that OmniParser V2 achieves state-of-the-art or competitive results in VsTP. Additionally, we explore the integration of SPOT within a multimodal large language model structure, further enhancing text localization and recognition capabilities, thereby confirming the generality of SPOT prompting technique. The code is available at \href{https://github.com/AlibabaResearch/AdvancedLiterateMachinery}{AdvancedLiterateMachinery}.

### AnxietyFaceTrack: A Smartphone-Based Non-Intrusive Approach for Detecting Social Anxiety Using Facial Features 
[[arxiv](https://arxiv.org/abs/2502.16106)] [[cool](https://papers.cool/arxiv/2502.16106)] [[pdf](https://arxiv.org/pdf/2502.16106)]
> **Authors**: Nilesh Kumar Sahu,Snehil Gupta,Haroon R Lone
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算机与社会,机器学习
- **Abstract**: Social Anxiety Disorder (SAD) is a widespread mental health condition, yet its lack of objective markers hinders timely detection and intervention. While previous research has focused on behavioral and non-verbal markers of SAD in structured activities (e.g., speeches or interviews), these settings fail to replicate real-world, unstructured social interactions fully. Identifying non-verbal markers in naturalistic, unstaged environments is essential for developing ubiquitous and non-intrusive monitoring solutions. To address this gap, we present AnxietyFaceTrack, a study leveraging facial video analysis to detect anxiety in unstaged social settings. A cohort of 91 participants engaged in a social setting with unfamiliar individuals and their facial videos were recorded using a low-cost smartphone camera. We examined facial features, including eye movements, head position, facial landmarks, and facial action units, and used self-reported survey data to establish ground truth for multiclass (anxious, neutral, non-anxious) and binary (e.g., anxious vs. neutral) classifications. Our results demonstrate that a Random Forest classifier trained on the top 20% of features achieved the highest accuracy of 91.0% for multiclass classification and an average accuracy of 92.33% across binary classifications. Notably, head position and facial landmarks yielded the best performance for individual facial regions, achieving 85.0% and 88.0% accuracy, respectively, in multiclass classification, and 89.66% and 91.0% accuracy, respectively, across binary classifications. This study introduces a non-intrusive, cost-effective solution that can be seamlessly integrated into everyday smartphones for continuous anxiety monitoring, offering a promising pathway for early detection and intervention.

### NeurFlow: Interpreting Neural Networks through Neuron Groups and Functional Interactions 
[[arxiv](https://arxiv.org/abs/2502.16105)] [[cool](https://papers.cool/arxiv/2502.16105)] [[pdf](https://arxiv.org/pdf/2502.16105)]
> **Authors**: Tue M. Cao,Nhat X. Hoang,Hieu H. Pham,Phi Le Nguyen,My T. Thai
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: The Thirteenth International Conference onLearningRepresentations
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Understanding the inner workings of neural networks is essential for enhancing model performance and interpretability. Current research predominantly focuses on examining the connection between individual neurons and the model's final predictions. Which suffers from challenges in interpreting the internal workings of the model, particularly when neurons encode multiple unrelated features. In this paper, we propose a novel framework that transitions the focus from analyzing individual neurons to investigating groups of neurons, shifting the emphasis from neuron-output relationships to functional interaction between neurons. Our automated framework, NeurFlow, first identifies core neurons and clusters them into groups based on shared functional relationships, enabling a more coherent and interpretable view of the network's internal processes. This approach facilitates the construction of a hierarchical circuit representing neuron interactions across layers, thus improving interpretability while reducing computational costs. Our extensive empirical studies validate the fidelity of our proposed NeurFlow. Additionally, we showcase its utility in practical applications such as image debugging and automatic concept labeling, thereby highlighting its potential to advance the field of neural network explainability.

### Good Representation, Better Explanation: Role of Convolutional Neural Networks in Transformer-Based Remote Sensing Image Captioning 
[[arxiv](https://arxiv.org/abs/2502.16095)] [[cool](https://papers.cool/arxiv/2502.16095)] [[pdf](https://arxiv.org/pdf/2502.16095)]
> **Authors**: Swadhin Das,Saarthak Gupta,and Kamal Kumar,Raksha Sharma
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Remote Sensing Image Captioning (RSIC) is the process of generating meaningful descriptions from remote sensing images. Recently, it has gained significant attention, with encoder-decoder models serving as the backbone for generating meaningful captions. The encoder extracts essential visual features from the input image, transforming them into a compact representation, while the decoder utilizes this representation to generate coherent textual descriptions. Recently, transformer-based models have gained significant popularity due to their ability to capture long-range dependencies and contextual information. The decoder has been well explored for text generation, whereas the encoder remains relatively unexplored. However, optimizing the encoder is crucial as it directly influences the richness of extracted features, which in turn affects the quality of generated captions. To address this gap, we systematically evaluate twelve different convolutional neural network (CNN) architectures within a transformer-based encoder framework to assess their effectiveness in RSIC. The evaluation consists of two stages: first, a numerical analysis categorizes CNNs into different clusters, based on their performance. The best performing CNNs are then subjected to human evaluation from a human-centric perspective by a human annotator. Additionally, we analyze the impact of different search strategies, namely greedy search and beam search, to ensure the best caption. The results highlight the critical role of encoder selection in improving captioning performance, demonstrating that specific CNN architectures significantly enhance the quality of generated descriptions for remote sensing images. By providing a detailed comparison of multiple encoders, this study offers valuable insights to guide advances in transformer-based image captioning models.

### A Multi-Scale Isolation Forest Approach for Real-Time Detection and Filtering of FGSM Adversarial Attacks in Video Streams of Autonomous Vehicles 
[[arxiv](https://arxiv.org/abs/2502.16044)] [[cool](https://papers.cool/arxiv/2502.16044)] [[pdf](https://arxiv.org/pdf/2502.16044)]
> **Authors**: Richard Abhulimhen,Negash Begashaw,Gurcan Comert,Chunheng Zhao,Pierluigi Pisu
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 17 pages, 7 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,密码学和安全,机器学习
- **Abstract**: Deep Neural Networks (DNNs) have demonstrated remarkable success across a wide range of tasks, particularly in fields such as image classification. However, DNNs are highly susceptible to adversarial attacks, where subtle perturbations are introduced to input images, leading to erroneous model outputs. In today's digital era, ensuring the security and integrity of images processed by DNNs is of critical importance. One of the most prominent adversarial attack methods is the Fast Gradient Sign Method (FGSM), which perturbs images in the direction of the loss gradient to deceive the model. This paper presents a novel approach for detecting and filtering FGSM adversarial attacks in image processing tasks. Our proposed method evaluates 10,000 images, each subjected to five different levels of perturbation, characterized by $ε$ values of 0.01, 0.02, 0.05, 0.1, and 0.2. These perturbations are applied in the direction of the loss gradient. We demonstrate that our approach effectively filters adversarially perturbed images, mitigating the impact of FGSM attacks. The method is implemented in Python, and the source code is publicly available on GitHub for reproducibility and further research.

### Real Time Offside Detection using a Single Camera in Soccer 
[[arxiv](https://arxiv.org/abs/2502.16030)] [[cool](https://papers.cool/arxiv/2502.16030)] [[pdf](https://arxiv.org/pdf/2502.16030)]
> **Authors**: Shounak Desai
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 5 pages 11 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Technological advancements in soccer have surged over the past decade, transforming aspects of the sport. Unlike binary rules, many soccer regulations, such as the "Offside Rule," rely on subjective interpretation rather than straightforward True or False criteria. The on-field referee holds ultimate authority in adjudicating these nuanced decisions. A significant breakthrough in soccer officiating is the Video Assistant Referee (VAR) system, leveraging a network of 20-30 cameras within stadiums to minimize human errors. VAR's operational scope typically encompasses 10-30 cameras, ensuring high decision accuracy but at a substantial cost. This report proposes an innovative approach to offside detection using a single camera, such as the broadcasting camera, to mitigate expenses associated with sophisticated technological setups.

### FeatSharp: Your Vision Model Features, Sharper 
[[arxiv](https://arxiv.org/abs/2502.16025)] [[cool](https://papers.cool/arxiv/2502.16025)] [[pdf](https://arxiv.org/pdf/2502.16025)]
> **Authors**: Mike Ranzinger,Greg Heinrich,Pavlo Molchanov,Jan Kautz,Bryan Catanzaro,Andrew Tao
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: The feature maps of vision encoders are fundamental to myriad modern AI tasks, ranging from core perception algorithms (e.g. semantic segmentation, object detection, depth perception, etc.) to modern multimodal understanding in vision-language models (VLMs). Currently, in computer vision, the frontier of general purpose vision backbones are Vision Transformers (ViT), typically trained using contrastive loss (e.g. CLIP). A key problem with most off-the-shelf ViTs, particularly CLIP, is that these models are inflexibly low resolution. Most run at 224x224px, while the "high resolution" versions are around 378-448px, but still inflexible. We introduce a novel method to coherently and cheaply upsample the feature maps of low-res vision encoders while picking up on fine-grained details that would otherwise be lost due to resolution. We demonstrate the effectiveness of this approach on core perception tasks as well as within agglomerative model (RADIO) training as a way of providing richer targets for distillation.

### Cross-Model Transferability of Adversarial Patches in Real-time Segmentation for Autonomous Driving 
[[arxiv](https://arxiv.org/abs/2502.16012)] [[cool](https://papers.cool/arxiv/2502.16012)] [[pdf](https://arxiv.org/pdf/2502.16012)]
> **Authors**: Prashant Shekhar,Bidur Devkota,Dumindu Samaraweera,Laxima Niure Kandel,Manoj Babu
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Adversarial attacks pose a significant threat to deep learning models, particularly in safety-critical applications like healthcare and autonomous driving. Recently, patch based attacks have demonstrated effectiveness in real-time inference scenarios owing to their 'drag and drop' nature. Following this idea for Semantic Segmentation (SS), here we propose a novel Expectation Over Transformation (EOT) based adversarial patch attack that is more realistic for autonomous vehicles. To effectively train this attack we also propose a 'simplified' loss function that is easy to analyze and implement. Using this attack as our basis, we investigate whether adversarial patches once optimized on a specific SS model, can fool other models or architectures. We conduct a comprehensive cross-model transferability analysis of adversarial patches trained on SOTA Convolutional Neural Network (CNN) models such PIDNet-S, PIDNet-M and PIDNet-L, among others. Additionally, we also include the Segformer model to study transferability to Vision Transformers (ViTs). All of our analysis is conducted on the widely used Cityscapes dataset. Our study reveals key insights into how model architectures (CNN vs CNN or CNN vs. Transformer-based) influence attack susceptibility. In particular, we conclude that although the transferability (effectiveness) of attacks on unseen images of any dimension is really high, the attacks trained against one particular model are minimally effective on other models. And this was found to be true for both ViT and CNN based models. Additionally our results also indicate that for CNN-based models, the repercussions of patch attacks are local, unlike ViTs. Per-class analysis reveals that simple-classes like 'sky' suffer less misclassification than others. The code for the project is available at: https://github.com/p-shekhar/adversarial-patch-transferability

### Multi-Agent Multimodal Models for Multicultural Text to Image Generation 
[[arxiv](https://arxiv.org/abs/2502.15972)] [[cool](https://papers.cool/arxiv/2502.15972)] [[pdf](https://arxiv.org/pdf/2502.15972)]
> **Authors**: Parth Bhalerao,Mounika Yalamarty,Brian Trinh,Oana Ignat
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Large Language Models (LLMs) demonstrate impressive performance across various multimodal tasks. However, their effectiveness in cross-cultural contexts remains limited due to the predominantly Western-centric nature of existing data and models. Meanwhile, multi-agent models have shown strong capabilities in solving complex tasks. In this paper, we evaluate the performance of LLMs in a multi-agent interaction setting for the novel task of multicultural image generation. Our key contributions are: (1) We introduce MosAIG, a Multi-Agent framework that enhances multicultural Image Generation by leveraging LLMs with distinct cultural personas; (2) We provide a dataset of 9,000 multicultural images spanning five countries, three age groups, two genders, 25 historical landmarks, and five languages; and (3) We demonstrate that multi-agent interactions outperform simple, no-agent models across multiple evaluation metrics, offering valuable insights for future research. Our dataset and models are available at https://github.com/OanaIgnat/MosAIG.

### Forgotten Polygons: Multimodal Large Language Models are Shape-Blind 
[[arxiv](https://arxiv.org/abs/2502.15969)] [[cool](https://papers.cool/arxiv/2502.15969)] [[pdf](https://arxiv.org/pdf/2502.15969)]
> **Authors**: William Rudman,Michal Golovanesky,Amir Bar,Vedant Palit,Yann LeCun,Carsten Eickhoff,Ritambhara Singh
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **Abstract**: Despite strong performance on vision-language tasks, Multimodal Large Language Models (MLLMs) struggle with mathematical problem-solving, with both open-source and state-of-the-art models falling short of human performance on visual-math benchmarks. To systematically examine visual-mathematical reasoning in MLLMs, we (1) evaluate their understanding of geometric primitives, (2) test multi-step reasoning, and (3) explore a potential solution to improve visual reasoning capabilities. Our findings reveal fundamental shortcomings in shape recognition, with top models achieving under 50% accuracy in identifying regular polygons. We analyze these failures through the lens of dual-process theory and show that MLLMs rely on System 1 (intuitive, memorized associations) rather than System 2 (deliberate reasoning). Consequently, MLLMs fail to count the sides of both familiar and novel shapes, suggesting they have neither learned the concept of sides nor effectively process visual inputs. Finally, we propose Visually Cued Chain-of-Thought (VC-CoT) prompting, which enhances multi-step mathematical reasoning by explicitly referencing visual annotations in diagrams, boosting GPT-4o's accuracy on an irregular polygon side-counting task from 7% to 93%. Our findings suggest that System 2 reasoning in MLLMs remains an open problem, and visually-guided prompting is essential for successfully engaging visual reasoning. Code available at: https://github.com/rsinghlab/Shape-Blind.

### Human Motion Prediction, Reconstruction, and Generation 
[[arxiv](https://arxiv.org/abs/2502.15956)] [[cool](https://papers.cool/arxiv/2502.15956)] [[pdf](https://arxiv.org/pdf/2502.15956)]
> **Authors**: Canxuan Gang,Yiran Wang
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Tech report
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: This report reviews recent advancements in human motion prediction, reconstruction, and generation. Human motion prediction focuses on forecasting future poses and movements from historical data, addressing challenges like nonlinear dynamics, occlusions, and motion style variations. Reconstruction aims to recover accurate 3D human body movements from visual inputs, often leveraging transformer-based architectures, diffusion models, and physical consistency losses to handle noise and complex poses. Motion generation synthesizes realistic and diverse motions from action labels, textual descriptions, or environmental constraints, with applications in robotics, gaming, and virtual avatars. Additionally, text-to-motion generation and human-object interaction modeling have gained attention, enabling fine-grained and context-aware motion synthesis for augmented reality and robotics. This review highlights key methodologies, datasets, challenges, and future research directions driving progress in these fields.

### Graph Attention Convolutional U-NET: A Semantic Segmentation Model for Identifying Flooded Areas 
[[arxiv](https://arxiv.org/abs/2502.15907)] [[cool](https://papers.cool/arxiv/2502.15907)] [[pdf](https://arxiv.org/pdf/2502.15907)]
> **Authors**: Muhammad Umair Danish,Madhushan Buwaneswaran,Tehara Fonseka,Katarina Grolinger
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: ef:IECON 2024 - 50th Annual Conference of the IEEE Industrial Electronics Society, 2024
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: The increasing impact of human-induced climate change and unplanned urban constructions has increased flooding incidents in recent years. Accurate identification of flooded areas is crucial for effective disaster management and urban planning. While few works have utilized convolutional neural networks and transformer-based semantic segmentation techniques for identifying flooded areas from aerial footage, recent developments in graph neural networks have created improvement opportunities. This paper proposes an innovative approach, the Graph Attention Convolutional U-NET (GAC-UNET) model, based on graph neural networks for automated identification of flooded areas. The model incorporates a graph attention mechanism and Chebyshev layers into the U-Net architecture. Furthermore, this paper explores the applicability of transfer learning and model reprogramming to enhance the accuracy of flood area segmentation models. Empirical results demonstrate that the proposed GAC-UNET model, outperforms other approaches with 91\% mAP, 94\% dice score, and 89\% IoU, providing valuable insights for informed decision-making and better planning of future infrastructures in flood-prone areas.

### RIFLEx: A Free Lunch for Length Extrapolation in Video Diffusion Transformers 
[[arxiv](https://arxiv.org/abs/2502.15894)] [[cool](https://papers.cool/arxiv/2502.15894)] [[pdf](https://arxiv.org/pdf/2502.15894)]
> **Authors**: Min Zhao,Guande He,Yixiao Chen,Hongzhou Zhu,Chongxuan Li,Jun Zhu
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Recent advancements in video generation have enabled models to synthesize high-quality, minute-long videos. However, generating even longer videos with temporal coherence remains a major challenge, and existing length extrapolation methods lead to temporal repetition or motion deceleration. In this work, we systematically analyze the role of frequency components in positional embeddings and identify an intrinsic frequency that primarily governs extrapolation behavior. Based on this insight, we propose RIFLEx, a minimal yet effective approach that reduces the intrinsic frequency to suppress repetition while preserving motion consistency, without requiring any additional modifications. RIFLEx offers a true free lunch--achieving high-quality $2\times$ extrapolation on state-of-the-art video diffusion transformers in a completely training-free manner. Moreover, it enhances quality and enables $3\times$ extrapolation by minimal fine-tuning without long videos. Project page and codes: \href{https://riflex-video.github.io/}{https://riflex-video.github.io/.}

### Understanding and Evaluating Hallucinations in 3D Visual Language Models 
[[arxiv](https://arxiv.org/abs/2502.15888)] [[cool](https://papers.cool/arxiv/2502.15888)] [[pdf](https://arxiv.org/pdf/2502.15888)]
> **Authors**: Ruiying Peng,Kaiyuan Li,Weichen Zhang,Chen Gao,Xinlei Chen,Yong Li
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Recently, 3D-LLMs, which combine point-cloud encoders with large models, have been proposed to tackle complex tasks in embodied intelligence and scene understanding. In addition to showing promising results on 3D tasks, we found that they are significantly affected by hallucinations. For instance, they may generate objects that do not exist in the scene or produce incorrect relationships between objects. To investigate this issue, this work presents the first systematic study of hallucinations in 3D-LLMs. We begin by quickly evaluating hallucinations in several representative 3D-LLMs and reveal that they are all significantly affected by hallucinations. We then define hallucinations in 3D scenes and, through a detailed analysis of datasets, uncover the underlying causes of these hallucinations. We find three main causes: (1) Uneven frequency distribution of objects in the dataset. (2) Strong correlations between objects. (3) Limited diversity in object attributes. Additionally, we propose new evaluation metrics for hallucinations, including Random Point Cloud Pair and Opposite Question Evaluations, to assess whether the model generates responses based on visual information and aligns it with the text's meaning.

### DOEI: Dual Optimization of Embedding Information for Attention-Enhanced Class Activation Maps 
[[arxiv](https://arxiv.org/abs/2502.15885)] [[cool](https://papers.cool/arxiv/2502.15885)] [[pdf](https://arxiv.org/pdf/2502.15885)]
> **Authors**: Hongjie Zhu,Zeyu Zhang,Guansong Pang,Xu Wang,Shimin Wen,Yu Bai,Daji Ergu,Ying Cai,Yang Zhao
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Weakly supervised semantic segmentation (WSSS) typically utilizes limited semantic annotations to obtain initial Class Activation Maps (CAMs). However, due to the inadequate coupling between class activation responses and semantic information in high-dimensional space, the CAM is prone to object co-occurrence or under-activation, resulting in inferior recognition accuracy. To tackle this issue, we propose DOEI, Dual Optimization of Embedding Information, a novel approach that reconstructs embedding representations through semantic-aware attention weight matrices to optimize the expression capability of embedding information. Specifically, DOEI amplifies tokens with high confidence and suppresses those with low confidence during the class-to-patch interaction. This alignment of activation responses with semantic information strengthens the propagation and decoupling of target features, enabling the generated embeddings to more accurately represent target features in high-level semantic space. In addition, we propose a hybrid-feature alignment module in DOEI that combines RGB values, embedding-guided features, and self-attention weights to increase the reliability of candidate tokens. Comprehensive experiments show that DOEI is an effective plug-and-play module that empowers state-of-the-art visual transformer-based WSSS models to significantly improve the quality of CAMs and segmentation performance on popular benchmarks, including PASCAL VOC (+3.6%, +1.5%, +1.2% mIoU) and MS COCO (+1.2%, +1.6% mIoU). Code will be available at https://github.com/AIGeeksGroup/DOEI.

### A Critical Assessment of Modern Generative Models' Ability to Replicate Artistic Styles 
[[arxiv](https://arxiv.org/abs/2502.15856)] [[cool](https://papers.cool/arxiv/2502.15856)] [[pdf](https://arxiv.org/pdf/2502.15856)]
> **Authors**: Andrea Asperti,Franky George,Tiberio Marras,Razvan Ciprian Stricescu,Fabio Zanotti
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: :68T05ACM Class:I.2.10; I.5.4
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: In recent years, advancements in generative artificial intelligence have led to the development of sophisticated tools capable of mimicking diverse artistic styles, opening new possibilities for digital creativity and artistic expression. This paper presents a critical assessment of the style replication capabilities of contemporary generative models, evaluating their strengths and limitations across multiple dimensions. We examine how effectively these models reproduce traditional artistic styles while maintaining structural integrity and compositional balance in the generated images. The analysis is based on a new large dataset of AI-generated works imitating artistic styles of the past, holding potential for a wide range of applications: the "AI-pastiche" dataset. The study is supported by extensive user surveys, collecting diverse opinions on the dataset and investigation both technical and aesthetic challenges, including the ability to generate outputs that are realistic and visually convincing, the versatility of models in handling a wide range of artistic styles, and the extent to which they adhere to the content and stylistic specifications outlined in prompts. This paper aims to provide a comprehensive overview of the current state of generative tools in style replication, offering insights into their technical and artistic limitations, potential advancements in model design and training methodologies, and emerging opportunities for enhancing digital artistry, human-AI collaboration, and the broader creative landscape.

### ELIP: Enhanced Visual-Language Foundation Models for Image Retrieval 
[[arxiv](https://arxiv.org/abs/2502.15682)] [[cool](https://papers.cool/arxiv/2502.15682)] [[pdf](https://arxiv.org/pdf/2502.15682)]
> **Authors**: Guanqi Zhan,Yuanpei Liu,Kai Han,Weidi Xie,Andrew Zisserman
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: The objective in this paper is to improve the performance of text-to-image retrieval. To this end, we introduce a new framework that can boost the performance of large-scale pre-trained vision-language models, so that they can be used for text-to-image re-ranking. The approach, Enhanced Language-Image Pre-training (ELIP), uses the text query to predict a set of visual prompts to condition the ViT image encoding. ELIP can easily be applied to the commonly used CLIP/SigLIP and the state-of-the-art BLIP-2 architectures. To train the architecture with limited computing resources, we develop a 'student friendly' best practice involving global hard sample mining, and selection and curation of a large-scale dataset. On the evaluation side, we set up two new out-of-distribution benchmarks, Occluded COCO and ImageNet-R, to assess the zero-shot generalisation of the models to different domains. Benefiting from the novel architecture and data curation, experiments show our enhanced network significantly boosts CLIP/SigLIP performance and outperforms the state-of-the-art BLIP-2 model on text-to-image retrieval.

### Continual Person Identification using Footstep-Induced Floor Vibrations on Heterogeneous Floor Structures 
[[arxiv](https://arxiv.org/abs/2502.15632)] [[cool](https://papers.cool/arxiv/2502.15632)] [[pdf](https://arxiv.org/pdf/2502.15632)]
> **Authors**: Yiwen Dong,Hae Young Noh
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: ef:Mechanical Systems and Signal Processing, 2023
- **标题**: None
- **领域**: 计算机视觉和模式识别,信号处理,应用物理
- **Abstract**: Person identification is important for smart buildings to provide personalized services such as health monitoring, activity tracking, and personnel management. However, previous person identification relies on pre-collected data from everyone, which is impractical in many buildings and public facilities in which visitors are typically expected. This calls for a continual person identification system that gradually learns people's identities on the fly. Existing studies use cameras to achieve this goal, but they require direct line-of-sight and also have raised privacy concerns in public. Other modalities such as wearables and pressure mats are limited by the requirement of device-carrying or dense deployment. Thus, prior studies introduced footstep-induced structural vibration sensing, which is non-intrusive and perceived as more privacy-friendly. However, this approach has a significant challenge: the high variability of vibration data due to structural heterogeneity and human gait variations, which makes online person identification algorithms perform poorly. In this paper, we characterize the variability in footstep-induced structural vibration data for accurate online person identification. To achieve this, we quantify and decompose different sources of variability and then design a feature transformation function to reduce the variability within each person's data to make different people's data more separable. We evaluate our approach through field experiments with 20 people. The results show a 70% variability reduction and a 90% accuracy for online person identification.

### WorldCraft: Photo-Realistic 3D World Creation and Customization via LLM Agents 
[[arxiv](https://arxiv.org/abs/2502.15601)] [[cool](https://papers.cool/arxiv/2502.15601)] [[pdf](https://arxiv.org/pdf/2502.15601)]
> **Authors**: Xinhang Liu,Chi-Keung Tang,Yu-Wing Tai
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,图形
- **Abstract**: Constructing photorealistic virtual worlds has applications across various fields, but it often requires the extensive labor of highly trained professionals to operate conventional 3D modeling software. To democratize this process, we introduce WorldCraft, a system where large language model (LLM) agents leverage procedural generation to create indoor and outdoor scenes populated with objects, allowing users to control individual object attributes and the scene layout using intuitive natural language commands. In our framework, a coordinator agent manages the overall process and works with two specialized LLM agents to complete the scene creation: ForgeIt, which integrates an ever-growing manual through auto-verification to enable precise customization of individual objects, and ArrangeIt, which formulates hierarchical optimization problems to achieve a layout that balances ergonomic and aesthetic considerations. Additionally, our pipeline incorporates a trajectory control agent, allowing users to animate the scene and operate the camera through natural language interactions. Our system is also compatible with off-the-shelf deep 3D generators to enrich scene assets. Through evaluations and comparisons with state-of-the-art methods, we demonstrate the versatility of WorldCraft, ranging from single-object customization to intricate, large-scale interior and exterior scene designs. This system empowers non-professionals to bring their creative visions to life.

### Bridging vision language model (VLM) evaluation gaps with a framework for scalable and cost-effective benchmark generation 
[[arxiv](https://arxiv.org/abs/2502.15563)] [[cool](https://papers.cool/arxiv/2502.15563)] [[pdf](https://arxiv.org/pdf/2502.15563)]
> **Authors**: Tim Rädsch,Leon Mayer,Simon Pavicic,A. Emre Kavur,Marcel Knopp,Barış Öztürk,Klaus Maier-Hein,Paul F. Jaeger,Fabian Isensee,Annika Reinke,Lena Maier-Hein
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习
- **Abstract**: Reliable evaluation of AI models is critical for scientific progress and practical application. While existing VLM benchmarks provide general insights into model capabilities, their heterogeneous designs and limited focus on a few imaging domains pose significant challenges for both cross-domain performance comparison and targeted domain-specific evaluation. To address this, we propose three key contributions: (1) a framework for the resource-efficient creation of domain-specific VLM benchmarks enabled by task augmentation for creating multiple diverse tasks from a single existing task, (2) the release of new VLM benchmarks for seven domains, created according to the same homogeneous protocol and including 162,946 thoroughly human-validated answers, and (3) an extensive benchmarking of 22 state-of-the-art VLMs on a total of 37,171 tasks, revealing performance variances across domains and tasks, thereby supporting the need for tailored VLM benchmarks. Adoption of our methodology will pave the way for the resource-efficient domain-specific selection of models and guide future research efforts toward addressing core open questions.

### Estimating Vehicle Speed on Roadways Using RNNs and Transformers: A Video-based Approach 
[[arxiv](https://arxiv.org/abs/2502.15545)] [[cool](https://papers.cool/arxiv/2502.15545)] [[pdf](https://arxiv.org/pdf/2502.15545)]
> **Authors**: Sai Krishna Reddy Mareddy,Dhanush Upplapati,Dhanush Kumar Antharam
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习,图像和视频处理
- **Abstract**: This project explores the application of advanced machine learning models, specifically Long Short-Term Memory (LSTM), Gated Recurrent Units (GRU), and Transformers, to the task of vehicle speed estimation using video data. Traditional methods of speed estimation, such as radar and manual systems, are often constrained by high costs, limited coverage, and potential disruptions. In contrast, leveraging existing surveillance infrastructure and cutting-edge neural network architectures presents a non-intrusive, scalable solution. Our approach utilizes LSTM and GRU to effectively manage long-term dependencies within the temporal sequence of video frames, while Transformers are employed to harness their self-attention mechanisms, enabling the processing of entire sequences in parallel and focusing on the most informative segments of the data. This study demonstrates that both LSTM and GRU outperform basic Recurrent Neural Networks (RNNs) due to their advanced gating mechanisms. Furthermore, increasing the sequence length of input data consistently improves model accuracy, highlighting the importance of contextual information in dynamic environments. Transformers, in particular, show exceptional adaptability and robustness across varied sequence lengths and complexities, making them highly suitable for real-time applications in diverse traffic conditions. The findings suggest that integrating these sophisticated neural network models can significantly enhance the accuracy and reliability of automated speed detection systems, thus promising to revolutionize traffic management and road safety.

### Q-PETR: Quant-aware Position Embedding Transformation for Multi-View 3D Object Detection 
[[arxiv](https://arxiv.org/abs/2502.15488)] [[cool](https://papers.cool/arxiv/2502.15488)] [[pdf](https://arxiv.org/pdf/2502.15488)]
> **Authors**: Jiangyong Yu,Changyong Shu,Dawei Yang,Zichen Yu,Xing Hu,Yan Chen
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: PETR-based methods have dominated benchmarks in 3D perception and are increasingly becoming a key component in modern autonomous driving systems. However, their quantization performance significantly degrades when INT8 inference is required, with a degradation of 58.2% in mAP and 36.9% in NDS on the NuScenes dataset. To address this issue, we propose a quantization-aware position embedding transformation for multi-view 3D object detection, termed Q-PETR. Q-PETR offers a quantizationfriendly and deployment-friendly architecture while preserving the original performance of PETR. It substantially narrows the accuracy gap between INT8 and FP32 inference for PETR-series methods. Without bells and whistles, our approach reduces the mAP and NDS drop to within 1% under standard 8-bit per-tensor post-training quantization. Furthermore, our method exceeds the performance of the original PETR in terms of floating-point precision. Extensive experiments across a variety of PETR-series models demonstrate its broad generalization.

### Game State and Spatio-temporal Action Detection in Soccer using Graph Neural Networks and 3D Convolutional Networks 
[[arxiv](https://arxiv.org/abs/2502.15462)] [[cool](https://papers.cool/arxiv/2502.15462)] [[pdf](https://arxiv.org/pdf/2502.15462)]
> **Authors**: Jeremie Ochin,Guillaume Devineau,Bogdan Stanciulescu,Sotiris Manitsaris
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: ef:In Proceedings of the 14th International Conference on Pattern Recognition Applications and Methods (2025), ISBN 978-989-758-730-6, ISSN 2184-4313, pages 636-646
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Soccer analytics rely on two data sources: the player positions on the pitch and the sequences of events they perform. With around 2000 ball events per game, their precise and exhaustive annotation based on a monocular video stream remains a tedious and costly manual task. While state-of-the-art spatio-temporal action detection methods show promise for automating this task, they lack contextual understanding of the game. Assuming professional players' behaviors are interdependent, we hypothesize that incorporating surrounding players' information such as positions, velocity and team membership can enhance purely visual predictions. We propose a spatio-temporal action detection approach that combines visual and game state information via Graph Neural Networks trained end-to-end with state-of-the-art 3D CNNs, demonstrating improved metrics through game state integration.

### Memory Helps, but Confabulation Misleads: Understanding Streaming Events in Videos with MLLMs 
[[arxiv](https://arxiv.org/abs/2502.15457)] [[cool](https://papers.cool/arxiv/2502.15457)] [[pdf](https://arxiv.org/pdf/2502.15457)]
> **Authors**: Gengyuan Zhang,Mingcong Ding,Tong Liu,Yao Zhang,Volker Tresp
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Short paper (5 pages)
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Multimodal large language models (MLLMs) have demonstrated strong performance in understanding videos holistically, yet their ability to process streaming videos-videos are treated as a sequence of visual events-remains underexplored. Intuitively, leveraging past events as memory can enrich contextual and temporal understanding of the current event. In this paper, we show that leveraging memories as contexts helps MLLMs better understand video events. However, because such memories rely on predictions of preceding events, they may contain misinformation, leading to confabulation and degraded performance. To address this, we propose a confabulation-aware memory modification method that mitigates confabulated memory for memory-enhanced event understanding.

### Enhancing Vehicle Make and Model Recognition with 3D Attention Modules 
[[arxiv](https://arxiv.org/abs/2502.15398)] [[cool](https://papers.cool/arxiv/2502.15398)] [[pdf](https://arxiv.org/pdf/2502.15398)]
> **Authors**: Narges Semiromizadeh,Omid Nejati Manzari,Shahriar B. Shokouhi,Sattar Mirzakuchaki
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Vehicle make and model recognition (VMMR) is a crucial component of the Intelligent Transport System, garnering significant attention in recent years. VMMR has been widely utilized for detecting suspicious vehicles, monitoring urban traffic, and autonomous driving systems. The complexity of VMMR arises from the subtle visual distinctions among vehicle models and the wide variety of classes produced by manufacturers. Convolutional Neural Networks (CNNs), a prominent type of deep learning model, have been extensively employed in various computer vision tasks, including VMMR, yielding remarkable results. As VMMR is a fine-grained classification problem, it primarily faces inter-class similarity and intra-class variation challenges. In this study, we implement an attention module to address these challenges and enhance the model's focus on critical areas containing distinguishing features. This module, which does not increase the parameters of the original model, generates three-dimensional (3-D) attention weights to refine the feature map. Our proposed model integrates the attention module into two different locations within the middle section of a convolutional model, where the feature maps from these sections offer sufficient information about the input frames without being overly detailed or overly coarse. The performance of our proposed model, along with state-of-the-art (SOTA) convolutional and transformer-based models, was evaluated using the Stanford Cars dataset. Our proposed model achieved the highest accuracy, 90.69\%, among the compared models.

### The Role of Background Information in Reducing Object Hallucination in Vision-Language Models: Insights from Cutoff API Prompting 
[[arxiv](https://arxiv.org/abs/2502.15389)] [[cool](https://papers.cool/arxiv/2502.15389)] [[pdf](https://arxiv.org/pdf/2502.15389)]
> **Authors**: Masayo Tomita,Katsuhiko Hayashi,Tomoyuki Kaneko
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Under review
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Vision-Language Models (VLMs) occasionally generate outputs that contradict input images, constraining their reliability in real-world applications. While visual prompting is reported to suppress hallucinations by augmenting prompts with relevant area inside an image, the effectiveness in terms of the area remains uncertain. This study analyzes success and failure cases of Attention-driven visual prompting in object hallucination, revealing that preserving background context is crucial for mitigating object hallucination.

### MOVE: A Mixture-of-Vision-Encoders Approach for Domain-Focused Vision-Language Processing 
[[arxiv](https://arxiv.org/abs/2502.15381)] [[cool](https://papers.cool/arxiv/2502.15381)] [[pdf](https://arxiv.org/pdf/2502.15381)]
> **Authors**: Matvey Skripkin,Elizaveta Goncharova,Dmitrii Tarasov,Andrey Kuznetsov
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 10 pages, 6 figures, 4 tables
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Multimodal language models (MLMs) integrate visual and textual information by coupling a vision encoder with a large language model through the specific adapter. While existing approaches commonly rely on a single pre-trained vision encoder, there is a great variability of specialized encoders that can boost model's performance in distinct domains. In this work, we propose MOVE (Mixture of Vision Encoders) a simple yet effective approach to leverage multiple pre-trained encoders for specialized multimodal tasks. MOVE automatically routes inputs to the most appropriate encoder among candidates such as Unichat, InternViT, and Texify, thereby enhancing performance across a diverse set of benchmarks, including ChartQA, MMBench, and MMMU. Experimental results demonstrate that MOVE achieves competitive accuracy without incurring the complexities of image slicing for high-resolution images.

### Weakly Supervised Video Scene Graph Generation via Natural Language Supervision 
[[arxiv](https://arxiv.org/abs/2502.15370)] [[cool](https://papers.cool/arxiv/2502.15370)] [[pdf](https://arxiv.org/pdf/2502.15370)]
> **Authors**: Kibum Kim,Kanghoon Yoon,Yeonjun In,Jaehyeong Jeon,Jinyoung Moon,Donghyun Kim,Chanyoung Park
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 10 pages, ICLR 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Existing Video Scene Graph Generation (VidSGG) studies are trained in a fully supervised manner, which requires all frames in a video to be annotated, thereby incurring high annotation cost compared to Image Scene Graph Generation (ImgSGG). Although the annotation cost of VidSGG can be alleviated by adopting a weakly supervised approach commonly used for ImgSGG (WS-ImgSGG) that uses image captions, there are two key reasons that hinder such a naive adoption: 1) Temporality within video captions, i.e., unlike image captions, video captions include temporal markers (e.g., before, while, then, after) that indicate time related details, and 2) Variability in action duration, i.e., unlike human actions in image captions, human actions in video captions unfold over varying duration. To address these issues, we propose a Natural Language-based Video Scene Graph Generation (NL-VSGG) framework that only utilizes the readily available video captions for training a VidSGG model. NL-VSGG consists of two key modules: Temporality-aware Caption Segmentation (TCS) module and Action Duration Variability-aware caption-frame alignment (ADV) module. Specifically, TCS segments the video captions into multiple sentences in a temporal order based on a Large Language Model (LLM), and ADV aligns each segmented sentence with appropriate frames considering the variability in action duration. Our approach leads to a significant enhancement in performance compared to simply applying the WS-ImgSGG pipeline to VidSGG on the Action Genome dataset. As a further benefit of utilizing the video captions as weak supervision, we show that the VidSGG model trained by NL-VSGG is able to predict a broader range of action classes that are not included in the training data, which makes our framework practical in reality.

### SentiFormer: Metadata Enhanced Transformer for Image Sentiment Analysis 
[[arxiv](https://arxiv.org/abs/2502.15322)] [[cool](https://papers.cool/arxiv/2502.15322)] [[pdf](https://arxiv.org/pdf/2502.15322)]
> **Authors**: Bin Feng,Shulan Ruan,Mingzheng Yang,Dongxuan Han,Huijie Liu,Kai Zhang,Qi Liu
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: As more and more internet users post images online to express their daily emotions, image sentiment analysis has attracted increasing attention. Recently, researchers generally tend to design different neural networks to extract visual features from images for sentiment analysis. Despite the significant progress, metadata, the data (e.g., text descriptions and keyword tags) for describing the image, has not been sufficiently explored in this task. In this paper, we propose a novel Metadata Enhanced Transformer for sentiment analysis (SentiFormer) to fuse multiple metadata and the corresponding image into a unified framework. Specifically, we first obtain multiple metadata of the image and unify the representations of diverse data. To adaptively learn the appropriate weights for each metadata, we then design an adaptive relevance learning module to highlight more effective information while suppressing weaker ones. Moreover, we further develop a cross-modal fusion module to fuse the adaptively learned representations and make the final prediction. Extensive experiments on three publicly available datasets demonstrate the superiority and rationality of our proposed method.

### Road Traffic Sign Recognition method using Siamese network Combining Efficient-CNN based Encoder 
[[arxiv](https://arxiv.org/abs/2502.15307)] [[cool](https://papers.cool/arxiv/2502.15307)] [[pdf](https://arxiv.org/pdf/2502.15307)]
> **Authors**: Zhenghao Xi,Yuchao Shao,Yang Zheng,Xiang Liu,Yaqi Liu,Yitong Cai
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Traffic signs recognition (TSR) plays an essential role in assistant driving and intelligent transportation system. However, the noise of complex environment may lead to motion-blur or occlusion problems, which raise the tough challenge to real-time recognition with high accuracy and robust. In this article, we propose IECES-network which with improved encoders and Siamese net. The three-stage approach of our method includes Efficient-CNN based encoders, Siamese backbone and the fully-connected layers. We firstly use convolutional encoders to extract and encode the traffic sign features of augmented training samples and standard images. Then, we design the Siamese neural network with Efficient-CNN based encoder and contrastive loss function, which can be trained to improve the robustness of TSR problem when facing the samples of motion-blur and occlusion by computing the distance between inputs and templates. Additionally, the template branch of the proposed network can be stopped when executing the recognition tasks after training to raise the process speed of our real-time model, and alleviate the computational resource and parameter scale. Finally, we recombined the feature code and a fully-connected layer with SoftMax function to classify the codes of samples and recognize the category of traffic signs. The results of experiments on the Tsinghua-Tencent 100K dataset and the German Traffic Sign Recognition Benchmark dataset demonstrate the performance of the proposed IECESnetwork. Compared with other state-of-the-art methods, in the case of motion-blur and occluded environment, the proposed method achieves competitive performance precision-recall and accuracy metric average is 88.1%, 86.43% and 86.1% with a 2.9M lightweight scale, respectively. Moreover, processing time of our model is 0.1s per frame, of which the speed is increased by 1.5 times compared with existing methods.

### A Novel Riemannian Sparse Representation Learning Network for Polarimetric SAR Image Classification 
[[arxiv](https://arxiv.org/abs/2502.15302)] [[cool](https://papers.cool/arxiv/2502.15302)] [[pdf](https://arxiv.org/pdf/2502.15302)]
> **Authors**: Junfei Shi,Mengmeng Nie,Weisi Lin,Haiyan Jin,Junhuai Li,Rui Wang
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 13 pages, 9 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Deep learning is an effective end-to-end method for Polarimetric Synthetic Aperture Radar(PolSAR) image classification, but it lacks the guidance of related mathematical principle and is essentially a black-box model. In addition, existing deep models learn features in Euclidean space, where PolSAR complex matrix is commonly converted into a complex-valued vector as the network input, distorting matrix structure and channel relationship. However, the complex covariance matrix is Hermitian positive definite (HPD), and resides on a Riemannian manifold instead of a Euclidean one. Existing methods cannot measure the geometric distance of HPD matrices and easily cause some misclassifications due to inappropriate Euclidean measures. To address these issues, we propose a novel Riemannian Sparse Representation Learning Network (SRSR CNN) for PolSAR images. Firstly, a superpixel-based Riemannian Sparse Representation (SRSR) model is designed to learn the sparse features with Riemannian metric. Then, the optimization procedure of the SRSR model is inferred and further unfolded into an SRSRnet, which can automatically learn the sparse coefficients and dictionary atoms. Furthermore, to learn contextual high-level features, a CNN-enhanced module is added to improve classification performance. The proposed network is a Sparse Representation (SR) guided deep learning model, which can directly utilize the covariance matrix as the network input, and utilize Riemannian metric to learn geometric structure and sparse features of complex matrices in Riemannian space. Experiments on three real PolSAR datasets demonstrate that the proposed method surpasses state-of-the-art techniques in ensuring accurate edge details and correct region homogeneity for classification.

### Soybean pod and seed counting in both outdoor fields and indoor laboratories using unions of deep neural networks 
[[arxiv](https://arxiv.org/abs/2502.15286)] [[cool](https://papers.cool/arxiv/2502.15286)] [[pdf](https://arxiv.org/pdf/2502.15286)]
> **Authors**: Tianyou Jiang,Mingshun Shao,Tianyi Zhang,Xiaoyu Liu,Qun Yu
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Automatic counting soybean pods and seeds in outdoor fields allows for rapid yield estimation before harvesting, while indoor laboratory counting offers greater accuracy. Both methods can significantly accelerate the breeding process. However, it remains challenging for accurately counting pods and seeds in outdoor fields, and there are still no accurate enough tools for counting pods and seeds in laboratories. In this study, we developed efficient deep learning models for counting soybean pods and seeds in both outdoor fields and indoor laboratories. For outdoor fields, annotating not only visible seeds but also occluded seeds makes YOLO have the ability to estimate the number of soybean seeds that are occluded. Moreover, we enhanced YOLO architecture by integrating it with HQ-SAM (YOLO-SAM), and domain adaptation techniques (YOLO-DA), to improve model robustness and generalization across soybean images taken in outdoor fields. Testing on soybean images from the outdoor field, we achieved a mean absolute error (MAE) of 6.13 for pod counting and 10.05 for seed counting. For the indoor setting, we utilized Mask-RCNN supplemented with a Swin Transformer module (Mask-RCNN-Swin), models were trained exclusively on synthetic training images generated from a small set of labeled data. This approach resulted in near-perfect accuracy, with an MAE of 1.07 for pod counting and 1.33 for seed counting across actual laboratory images from two distinct studies.

### CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models 
[[arxiv](https://arxiv.org/abs/2502.15278)] [[cool](https://papers.cool/arxiv/2502.15278)] [[pdf](https://arxiv.org/pdf/2502.15278)]
> **Authors**: Shunchang Liu,Zhuan Shi,Lingjuan Lyu,Yaochu Jin,Boi Faltings
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 17pages, 8 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Assessing whether AI-generated images are substantially similar to copyrighted works is a crucial step in resolving copyright disputes. In this paper, we propose CopyJudge, an automated copyright infringement identification framework that leverages large vision-language models (LVLMs) to simulate practical court processes for determining substantial similarity between copyrighted images and those generated by text-to-image diffusion models. Specifically, we employ an abstraction-filtration-comparison test framework with multi-LVLM debate to assess the likelihood of infringement and provide detailed judgment rationales. Based on the judgments, we further introduce a general LVLM-based mitigation strategy that automatically optimizes infringing prompts by avoiding sensitive expressions while preserving the non-infringing content. Besides, our approach can be enhanced by exploring non-infringing noise vectors within the diffusion latent space via reinforcement learning, even without modifying the original prompts. Experimental results show that our identification method achieves comparable state-of-the-art performance, while offering superior generalization and interpretability across various forms of infringement, and that our mitigation method could more effectively mitigate memorization and IP infringement without losing non-infringing expressions.

### An ocean front detection and tracking algorithm 
[[arxiv](https://arxiv.org/abs/2502.15250)] [[cool](https://papers.cool/arxiv/2502.15250)] [[pdf](https://arxiv.org/pdf/2502.15250)]
> **Authors**: Yishuo Wang,Feng Zhou
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Existing ocean front detection methods--including histogram-based variance analysis, Lyapunov exponent, gradient thresholding, and machine learning--suffer from critical limitations: discontinuous outputs, over-detection, reliance on single-threshold decisions, and lack of open-source implementations. To address these challenges, this paper proposes the Bayesian Front Detection and Tracking framework with Metric Space Analysis (BFDT-MSA). The framework introduces three innovations: (1) a Bayesian decision mechanism that integrates gradient priors and field operators to eliminate manual threshold sensitivity; (2) morphological refinement algorithms for merging fragmented fronts, deleting spurious rings, and thinning frontal zones to pixel-level accuracy; and (3) a novel metric space definition for temporal front tracking, enabling systematic analysis of front evolution. Validated on global SST data (2022--2024), BFDT-MSA reduces over-detection by $73\%$ compared to histogram-based methods while achieving superior intensity ($0.16^\circ$C/km), continuity, and spatiotemporal coherence. The open-source release bridges a critical gap in reproducible oceanographic research.

### AutoMR: A Universal Time Series Motion Recognition Pipeline 
[[arxiv](https://arxiv.org/abs/2502.15228)] [[cool](https://papers.cool/arxiv/2502.15228)] [[pdf](https://arxiv.org/pdf/2502.15228)]
> **Authors**: Likun Zhang,Sicheng Yang,Zhuo Wang,Haining Liang,Junxiao Shen
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 5 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: In this paper, we present an end-to-end automated motion recognition (AutoMR) pipeline designed for multimodal datasets. The proposed framework seamlessly integrates data preprocessing, model training, hyperparameter tuning, and evaluation, enabling robust performance across diverse scenarios. Our approach addresses two primary challenges: 1) variability in sensor data formats and parameters across datasets, which traditionally requires task-specific machine learning implementations, and 2) the complexity and time consumption of hyperparameter tuning for optimal model performance. Our library features an all-in-one solution incorporating QuartzNet as the core model, automated hyperparameter tuning, and comprehensive metrics tracking. Extensive experiments demonstrate its effectiveness on 10 diverse datasets, achieving state-of-the-art performance. This work lays a solid foundation for deploying motion-capture solutions across varied real-world applications.

## 计算机与社会(cs.CY:Computers and Society)

### DeepSeek reshaping healthcare in China's tertiary hospitals 
[[arxiv](https://arxiv.org/abs/2502.16732)] [[cool](https://papers.cool/arxiv/2502.16732)] [[pdf](https://arxiv.org/pdf/2502.16732)]
> **Authors**: Jishizhan Chen,Qingzeng Zhang
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: The rapid integration of artificial intelligence (AI) into healthcare is transforming clinical decision-making and hospital operations. DeepSeek has emerged as a leading AI system, widely deployed across China's tertiary hospitals since January 2025. Initially implemented in Shanghai's major medical institutions, it has since expanded nationwide, enhancing diagnostic accuracy, streamlining workflows, and improving patient management. AI-powered pathology, imaging analysis, and clinical decision support systems have demonstrated significant potential in optimizing medical processes and reducing the cognitive burden on healthcare professionals. However, the widespread adoption of AI in healthcare raises critical regulatory and ethical challenges, particularly regarding accountability in AI-assisted diagnosis and the risk of automation bias. The absence of a well-defined liability framework underscores the need for policies that ensure AI functions as an assistive tool rather than an autonomous decision-maker. With continued technological advancements, AI is expected to integrate multimodal data sources, such as genomics and radiomics, paving the way for precision medicine and personalized treatment strategies. The future of AI in healthcare depends on the development of transparent regulatory structures, industry collaboration, and adaptive governance frameworks that balance innovation with responsibility, ensuring equitable and effective AI-driven medical services.

### Beyond Release: Access Considerations for Generative AI Systems 
[[arxiv](https://arxiv.org/abs/2502.16701)] [[cool](https://papers.cool/arxiv/2502.16701)] [[pdf](https://arxiv.org/pdf/2502.16701)]
> **Authors**: Irene Solaiman,Rishi Bommasani,Dan Hendrycks,Ariel Herbert-Voss,Yacine Jernite,Aviya Skowron,Andrew Trask
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: Generative AI release decisions determine whether system components are made available, but release does not address many other elements that change how users and stakeholders are able to engage with a system. Beyond release, access to system components informs potential risks and benefits. Access refers to practical needs, infrastructurally, technically, and societally, in order to use available components in some way. We deconstruct access along three axes: resourcing, technical usability, and utility. Within each category, a set of variables per system component clarify tradeoffs. For example, resourcing requires access to computing infrastructure to serve model weights. We also compare the accessibility of four high performance language models, two open-weight and two closed-weight, showing similar considerations for all based instead on access variables. Access variables set the foundation for being able to scale or increase access to users; we examine the scale of access and how scale affects ability to manage and intervene on risks. This framework better encompasses the landscape and risk-benefit tradeoffs of system releases to inform system release decisions, research, and policy.

### Unmasking Societal Biases in Respiratory Support for ICU Patients through Social Determinants of Health 
[[arxiv](https://arxiv.org/abs/2502.16477)] [[cool](https://papers.cool/arxiv/2502.16477)] [[pdf](https://arxiv.org/pdf/2502.16477)]
> **Authors**: Mira Moukheiber,Lama Moukheiber,Dana Moukheiber,Hyung-Chul Lee
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,人工智能,机器学习
- **Abstract**: In critical care settings, where precise and timely interventions are crucial for health outcomes, evaluating disparities in patient outcomes is essential. Current approaches often fail to fully capture the impact of respiratory support interventions on individuals affected by social determinants of health. While attributes such as gender, race, and age are commonly assessed and provide valuable insights, they offer only a partial view of the complexities faced by diverse populations. In this study, we focus on two clinically motivated tasks: prolonged mechanical ventilation and successful weaning. Additionally, we conduct fairness audits on the models' predictions across demographic groups and social determinants of health to better understand health inequities in respiratory interventions within the intensive care unit. Furthermore, we release a temporal benchmark dataset, verified by clinical experts, to facilitate benchmarking of clinical respiratory intervention tasks.

### A Framework for Evaluating Vision-Language Model Safety: Building Trust in AI for Public Sector Applications 
[[arxiv](https://arxiv.org/abs/2502.16361)] [[cool](https://papers.cool/arxiv/2502.16361)] [[pdf](https://arxiv.org/pdf/2502.16361)]
> **Authors**: Maisha Binte Rashid,Pablo Rivas
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: AAAI 2025 Workshop onAIfor Social Impact: Bridging Innovations in Finance, Social Media, and Crime Prevention
- **标题**: None
- **领域**: 计算机与社会,计算语言学,计算机视觉和模式识别
- **Abstract**: Vision-Language Models (VLMs) are increasingly deployed in public sector missions, necessitating robust evaluation of their safety and vulnerability to adversarial attacks. This paper introduces a novel framework to quantify adversarial risks in VLMs. We analyze model performance under Gaussian, salt-and-pepper, and uniform noise, identifying misclassification thresholds and deriving composite noise patches and saliency patterns that highlight vulnerable regions. These patterns are compared against the Fast Gradient Sign Method (FGSM) to assess their adversarial effectiveness. We propose a new Vulnerability Score that combines the impact of random noise and adversarial attacks, providing a comprehensive metric for evaluating model robustness.

### Interrogating LLM design under a fair learning doctrine 
[[arxiv](https://arxiv.org/abs/2502.16290)] [[cool](https://papers.cool/arxiv/2502.16290)] [[pdf](https://arxiv.org/pdf/2502.16290)]
> **Authors**: Johnny Tian-Zheng Wei,Maggie Wang,Ameya Godbole,Jonathan H. Choi,Robin Jia
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,计算语言学
- **Abstract**: The current discourse on large language models (LLMs) and copyright largely takes a "behavioral" perspective, focusing on model outputs and evaluating whether they are substantially similar to training data. However, substantial similarity is difficult to define algorithmically and a narrow focus on model outputs is insufficient to address all copyright risks. In this interdisciplinary work, we take a complementary "structural" perspective and shift our focus to how LLMs are trained. We operationalize a notion of "fair learning" by measuring whether any training decision substantially affected the model's memorization. As a case study, we deconstruct Pythia, an open-source LLM, and demonstrate the use of causal and correlational analyses to make factual determinations about Pythia's training decisions. By proposing a legal standard for fair learning and connecting memorization analyses to this standard, we identify how judges may advance the goals of copyright law through adjudication. Finally, we discuss how a fair learning standard might evolve to enhance its clarity by becoming more rule-like and incorporating external technical guidelines.

### A Comprehensive Survey on the Trustworthiness of Large Language Models in Healthcare 
[[arxiv](https://arxiv.org/abs/2502.15871)] [[cool](https://papers.cool/arxiv/2502.15871)] [[pdf](https://arxiv.org/pdf/2502.15871)]
> **Authors**: Manar Aljohani,Jun Hou,Sindhura Kommu,Xuan Wang
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,人工智能,计算语言学
- **Abstract**: The application of large language models (LLMs) in healthcare has the potential to revolutionize clinical decision-making, medical research, and patient care. As LLMs are increasingly integrated into healthcare systems, several critical challenges must be addressed to ensure their reliable and ethical deployment. These challenges include truthfulness, where models generate misleading information; privacy, with risks of unintentional data retention; robustness, requiring defenses against adversarial attacks; fairness, addressing biases in clinical outcomes; explainability, ensuring transparent decision-making; and safety, mitigating risks of misinformation and medical errors. Recently, researchers have begun developing benchmarks and evaluation frameworks to systematically assess the trustworthiness of LLMs. However, the trustworthiness of LLMs in healthcare remains underexplored, lacking a systematic review that provides a comprehensive understanding and future insights into this area. This survey bridges this gap by providing a comprehensive overview of the recent research of existing methodologies and solutions aimed at mitigating the above risks in healthcare. By focusing on key trustworthiness dimensions including truthfulness, privacy and safety, robustness, fairness and bias, and explainability, we present a thorough analysis of how these issues impact the reliability and ethical use of LLMs in healthcare. This paper highlights ongoing efforts and offers insights into future research directions to ensure the safe and trustworthy deployment of LLMs in healthcare.

### Making Sense of AI Limitations: How Individual Perceptions Shape Organizational Readiness for AI Adoption 
[[arxiv](https://arxiv.org/abs/2502.15870)] [[cool](https://papers.cool/arxiv/2502.15870)] [[pdf](https://arxiv.org/pdf/2502.15870)]
> **Authors**: Thomas Übellacker
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,人工智能,人机交互
- **Abstract**: This study investigates how individuals' perceptions of artificial intelligence (AI) limitations influence organizational readiness for AI adoption. Through semi-structured interviews with seven AI implementation experts, analyzed using the Gioia methodology, the research reveals that organizational readiness emerges through dynamic interactions between individual sensemaking, social learning, and formal integration processes. The findings demonstrate that hands-on experience with AI limitations leads to more realistic expectations and increased trust, mainly when supported by peer networks and champion systems. Organizations that successfully translate these individual and collective insights into formal governance structures achieve more sustainable AI adoption. The study advances theory by showing how organizational readiness for AI adoption evolves through continuous cycles of individual understanding, social learning, and organizational adaptation. These insights suggest that organizations should approach AI adoption not as a one-time implementation but as an ongoing strategic learning process that balances innovation with practical constraints. The research contributes to organizational readiness theory and practice by illuminating how micro-level perceptions and experiences shape macro-level adoption outcomes.

### AI Governance InternationaL Evaluation Index (AGILE Index) 
[[arxiv](https://arxiv.org/abs/2502.15859)] [[cool](https://papers.cool/arxiv/2502.15859)] [[pdf](https://arxiv.org/pdf/2502.15859)]
> **Authors**: Yi Zeng,Enmeng Lu,Xin Guan,Cunqing Huangfu,Zizhe Ruan,Ammar Younas,Kang Sun,Xuan Tang,Yuwei Wang,Hongjie Suo,Dongqi Liang,Zhengqiang Han,Aorigele Bao,Xiaoyang Guo,Jin Wang,Jiawei Xie,Yao Liang
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Evaluation Report. 85 pages, 30 Figures
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: The rapid advancement of Artificial Intelligence (AI) technology is profoundly transforming human society and concurrently presenting a series of ethical, legal, and social issues. The effective governance of AI has become a crucial global concern. Since 2022, the extensive deployment of generative AI, particularly large language models, marked a new phase in AI governance. Continuous efforts are being made by the international community in actively addressing the novel challenges posed by these AI developments. As consensus on international governance continues to be established and put into action, the practical importance of conducting a global assessment of the state of AI governance is progressively coming to light. In this context, we initiated the development of the AI Governance InternationaL Evaluation Index (AGILE Index). Adhering to the design principle, "the level of governance should match the level of development," the inaugural evaluation of the AGILE Index commences with an exploration of four foundational pillars: the development level of AI, the AI governance environment, the AI governance instruments, and the AI governance effectiveness. It covers 39 indicators across 18 dimensions to comprehensively assess the AI governance level of 14 representative countries globally. The index is utilized to delve into the status of AI governance to date in 14 countries for the first batch of evaluation. The aim is to depict the current state of AI governance in these countries through data scoring, assist them in identifying their governance stage and uncovering governance issues, and ultimately offer insights for the enhancement of their AI governance systems.

### Generative AI Training and Copyright Law 
[[arxiv](https://arxiv.org/abs/2502.15858)] [[cool](https://papers.cool/arxiv/2502.15858)] [[pdf](https://arxiv.org/pdf/2502.15858)]
> **Authors**: Tim W. Dornis,Sebastian Stober
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: submitted as an overview article to the Transactions of the International Society for Music Information Retrieval
- **标题**: None
- **领域**: 计算机与社会,人工智能,机器学习
- **Abstract**: Training generative AI models requires extensive amounts of data. A common practice is to collect such data through web scraping. Yet, much of what has been and is collected is copyright protected. Its use may be copyright infringement. In the USA, AI developers rely on "fair use" and in Europe, the prevailing view is that the exception for "Text and Data Mining" (TDM) applies. In a recent interdisciplinary tandem-study, we have argued in detail that this is actually not the case because generative AI training fundamentally differs from TDM. In this article, we share our main findings and the implications for both public and corporate research on generative models. We further discuss how the phenomenon of training data memorization leads to copyright issues independently from the "fair use" and TDM exceptions. Finally, we outline how the ISMIR could contribute to the ongoing discussion about fair practices with respect to generative AI that satisfy all stakeholders.

### Training AI to be Loyal 
[[arxiv](https://arxiv.org/abs/2502.15720)] [[cool](https://papers.cool/arxiv/2502.15720)] [[pdf](https://arxiv.org/pdf/2502.15720)]
> **Authors**: Sewoong Oh,Himanshu Tyagi,Pramod Viswanath
> **First submission**: 2025-01-27
> **First announcement**: 2025-02-24
> **comment**: 13 pages 4 figures
- **标题**: None
- **领域**: 计算机与社会,人工智能,密码学和安全,机器学习
- **Abstract**: Loyal AI is loyal to the community that builds it. An AI is loyal to a community if the community has ownership, alignment, and control. Community owned models can only be used with the approval of the community and share the economic rewards communally. Community aligned models have values that are aligned with the consensus of the community. Community controlled models perform functions designed by the community. Since we would like permissionless access to the loyal AI's community, we need the AI to be open source. The key scientific question then is: how can we build models that are openly accessible (open source) and yet are owned and governed by the community. This seeming impossibility is the focus of this paper where we outline a concrete pathway to Open, Monetizable and Loyal models (OML), building on our earlier work on OML, arXiv:2411.03887(1) , and a representation via a cryptographic-ML library http://github.com/sentient-agi/oml-1.0-fingerprinting .

### Governing AI Beyond the Pretraining Frontier 
[[arxiv](https://arxiv.org/abs/2502.15719)] [[cool](https://papers.cool/arxiv/2502.15719)] [[pdf](https://arxiv.org/pdf/2502.15719)]
> **Authors**: Nicholas A. Caputo
> **First submission**: 2025-01-27
> **First announcement**: 2025-02-24
> **comment**: 14 pages
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: This year, jurisdictions worldwide, including the United States, the European Union, the United Kingdom, and China, are set to enact or revise laws governing frontier AI. Their efforts largely rely on the assumption that increasing model scale through pretraining is the path to more advanced AI capabilities. Yet growing evidence suggests that this "pretraining paradigm" may be hitting a wall and major AI companies are turning to alternative approaches, like inference-time "reasoning," to boost capabilities instead. This paradigm shift presents fundamental challenges for the frontier AI governance frameworks that target pretraining scale as a key bottleneck useful for monitoring, control, and exclusion, threatening to undermine this new legal order as it emerges. This essay seeks to identify these challenges and point to new paths forward for regulation. First, we examine the existing frontier AI regulatory regime and analyze some key traits and vulnerabilities. Second, we introduce the concept of the "pretraining frontier," the capabilities threshold made possible by scaling up pretraining alone, and demonstrate how it could make the regulatory field more diffuse and complex and lead to new forms of competition. Third, we lay out a regulatory approach that focuses on increasing transparency and leveraging new natural technical bottlenecks to effectively oversee changing frontier AI development while minimizing regulatory burdens and protecting fundamental rights. Our analysis provides concrete mechanisms for governing frontier AI systems across diverse technical paradigms, offering policymakers tools for addressing both current and future regulatory challenges in frontier AI.

### Regulating Multifunctionality 
[[arxiv](https://arxiv.org/abs/2502.15715)] [[cool](https://papers.cool/arxiv/2502.15715)] [[pdf](https://arxiv.org/pdf/2502.15715)]
> **Authors**: Cary Coglianese,Colton R. Crum
> **First submission**: 2025-01-25
> **First announcement**: 2025-02-24
> **comment**: Forthcoming in Philipp Hacker, Andreas Engel, Sarah Hammer and Brent Mittelstadt (eds), The Oxford Handbook on the Foundations and Regulation of GenerativeAI(Oxford University Press)
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: Foundation models and generative artificial intelligence (AI) exacerbate a core regulatory challenge associated with AI: its heterogeneity. By their very nature, foundation models and generative AI can perform multiple functions for their users, thus presenting a vast array of different risks. This multifunctionality means that prescriptive, one-size-fits-all regulation will not be a viable option. Even performance standards and ex post liability - regulatory approaches that usually afford flexibility - are unlikely to be strong candidates for responding to multifunctional AI's risks, given challenges in monitoring and enforcement. Regulators will do well instead to promote proactive risk management on the part of developers and users by using management-based regulation, an approach that has proven effective in other contexts of heterogeneity. Regulators will also need to maintain ongoing vigilance and agility. More than in other contexts, regulators of multifunctional AI will need sufficient resources, top human talent and leadership, and organizational cultures committed to regulatory excellence.

### Integrating Generative AI in Cybersecurity Education: Case Study Insights on Pedagogical Strategies, Critical Thinking, and Responsible AI Use 
[[arxiv](https://arxiv.org/abs/2502.15357)] [[cool](https://papers.cool/arxiv/2502.15357)] [[pdf](https://arxiv.org/pdf/2502.15357)]
> **Authors**: Mahmoud Elkhodr,Ergun Gide
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 30 pages
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: The rapid advancement of Generative Artificial Intelligence (GenAI) has introduced new opportunities for transforming higher education, particularly in fields that require analytical reasoning and regulatory compliance, such as cybersecurity management. This study presents a structured framework for integrating GenAI tools into cybersecurity education, demonstrating their role in fostering critical thinking, real-world problem-solving, and regulatory awareness. The implementation strategy followed a two-stage approach, embedding GenAI within tutorial exercises and assessment tasks. Tutorials enabled students to generate, critique, and refine AI-assisted cybersecurity policies, while assessments required them to apply AI-generated outputs to real-world scenarios, ensuring alignment with industry standards and regulatory requirements. Findings indicate that AI-assisted learning significantly enhanced students' ability to evaluate security policies, refine risk assessments, and bridge theoretical knowledge with practical application. Student reflections and instructor observations revealed improvements in analytical engagement, yet challenges emerged regarding AI over-reliance, variability in AI literacy, and the contextual limitations of AI-generated content. Through structured intervention and research-driven refinement, students were able to recognize AI strengths as a generative tool while acknowledging its need for human oversight. This study further highlights the broader implications of AI adoption in cybersecurity education, emphasizing the necessity of balancing automation with expert judgment to cultivate industry-ready professionals. Future research should explore the long-term impact of AI-driven learning on cybersecurity competency, as well as the potential for adaptive AI-assisted assessments to further personalize and enhance educational outcomes.

## 数据库(cs.DB:Databases)

### V-SQL: A View-based Two-stage Text-to-SQL Framework 
[[arxiv](https://arxiv.org/abs/2502.15686)] [[cool](https://papers.cool/arxiv/2502.15686)] [[pdf](https://arxiv.org/pdf/2502.15686)]
> **Authors**: Zeshun You,Jiebin Yao,Dong Cheng,Zhiwei Wen,Zhiliang Lu,Xianyi Shen
> **First submission**: 2024-12-16
> **First announcement**: 2025-02-24
> **comment**: 10 pages,5 figures,
- **标题**: None
- **领域**: 数据库,计算语言学
- **Abstract**: The text-to-SQL task aims to convert natural language into Structured Query Language (SQL) without bias. Recently, text-to-SQL methods based on large language models (LLMs) have garnered significant attention. The core of mainstream text-to-SQL frameworks is schema linking, which aligns user queries with relevant tables and columns in the database. Previous methods focused on schema linking while neglecting to enhance LLMs' understanding of database schema. The complex coupling relationships between tables in the database constrain the SQL generation capabilities of LLMs. To tackle this issue, this paper proposes a simple yet effective strategy called view-based schema. This strategy aids LLMs in understanding the database schema by decoupling tightly coupled tables into low-coupling views. We then introduce V-SQL, a view-based two-stage text-to-SQL framework. V-SQL involves the view-based schema strategy to enhance LLMs' understanding of database schema. Results on the authoritative datasets Bird indicate that V-SQL achieves competitive performance compared to existing state-of-the-art methods.

## 分布式、并行和集群计算(cs.DC:Distributed, Parallel, and Cluster Computing)

### FairKV: Balancing Per-Head KV Cache for Fast Multi-GPU Inference 
[[arxiv](https://arxiv.org/abs/2502.15804)] [[cool](https://papers.cool/arxiv/2502.15804)] [[pdf](https://arxiv.org/pdf/2502.15804)]
> **Authors**: Bingzhe Zhao,Ke Cheng,Aomufei Yuan,Yuxuan Tian,Ruiguang Zhong,Chengchen Hu,Tong Yang,Lian Yu
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-24
> **comment**: 11 pages, 6 figures
- **标题**: None
- **领域**: 分布式、并行和集群计算,人工智能
- **Abstract**: KV cache techniques in Transformer models aim to reduce redundant computations at the expense of substantially increased memory usage, making KV cache compression an important and popular research topic. Recently, state-of-the-art KV cache compression methods implement imbalanced, per-head allocation algorithms that dynamically adjust the KV cache budget for each attention head, achieving excellent performance in single-GPU scenarios. However, we observe that such imbalanced compression leads to significant load imbalance when deploying multi-GPU inference, as some GPUs become overburdened while others remain underutilized. In this paper, we propose FairKV, a method designed to ensure fair memory usage among attention heads in systems employing imbalanced KV cache compression. The core technique of FairKV is Fair-Copying, which replicates a small subset of memory-intensive attention heads across GPUs using data parallelism to mitigate load imbalance. Our experiments on popular models, including LLaMA 70b and Mistral 24b model, demonstrate that FairKV increases throughput by 1.66x compared to standard tensor parallelism inference. Our code will be released as open source upon acceptance.

### Hybrid Offline-online Scheduling Method for Large Language Model Inference Optimization 
[[arxiv](https://arxiv.org/abs/2502.15763)] [[cool](https://papers.cool/arxiv/2502.15763)] [[pdf](https://arxiv.org/pdf/2502.15763)]
> **Authors**: Bowen Pang,Kai Li,Ruifeng She,Feifan Wang
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 分布式、并行和集群计算,人工智能,硬件架构,机器学习
- **Abstract**: With the development of large language models (LLMs), it has become increasingly important to optimize hardware usage and improve throughput. In this paper, we study the inference optimization of the serving system that deploys LLMs. To optimize system throughput and maximize hardware utilization, we formulate the inference optimization problem as a mixed-integer programming (MIP) model and propose a hybrid offline-online method as solution. The offline method improves large-scale inference systems by introducing a Minimizing Makespan Bin Packing Problem. We further provide a theoretical lower bound computation method. Then, we propose an online sorting and preemptive scheduling method to better utilize hardware. In the online iteration scheduling process, a Lagrangian method is applied to evaluate the cost efficiency of inserting prefill stages versus decode stages at each iteration and dynamically determine when to preempt decoding tasks and insert prefill tasks. Experiments using real-world data from the LLaMA-65B model and the GSM8K dataset demonstrate that system utilization improves from 80.2% to 89.1%, and the total inference time decreases from 201.00 to 190.58 seconds. A 100-cases study shows that our method consistently outperforms the baseline method and improves the utilization rate by 8.0% on average. Finally, we discuss potential future extensions, including stochastic modeling, reinforcement learning-based schedulers, and dynamic decision-making strategies for system throughput and hardware utilization.

### SmartEdge: Smart Healthcare End-to-End Integrated Edge and Cloud Computing System for Diabetes Prediction Enabled by Ensemble Machine Learning 
[[arxiv](https://arxiv.org/abs/2502.15762)] [[cool](https://papers.cool/arxiv/2502.15762)] [[pdf](https://arxiv.org/pdf/2502.15762)]
> **Authors**: Alain Hennebelle,Qifan Dieng,Leila Ismail,Rajkumar Buyya
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-24
> **comment**: :68T01; 68T09; 68M14; 68W10; 68W15ACM Class:C.2.4; C.4; C.5; D.2.2; D.2.11; I.2.5; I.2.6; I.2.11; J.0; J.7
- **标题**: None
- **领域**: 分布式、并行和集群计算,人工智能,新兴技术,机器学习
- **Abstract**: The Internet of Things (IoT) revolutionizes smart city domains such as healthcare, transportation, industry, and education. The Internet of Medical Things (IoMT) is gaining prominence, particularly in smart hospitals and Remote Patient Monitoring (RPM). The vast volume of data generated by IoMT devices should be analyzed in real-time for health surveillance, prognosis, and prediction of diseases. Current approaches relying on Cloud computing to provide the necessary computing and storage capabilities do not scale for these latency-sensitive applications. Edge computing emerges as a solution by bringing cloud services closer to IoMT devices. This paper introduces SmartEdge, an AI-powered smart healthcare end-to-end integrated edge and cloud computing system for diabetes prediction. This work addresses latency concerns and demonstrates the efficacy of edge resources in healthcare applications within an end-to-end system. The system leverages various risk factors for diabetes prediction. We propose an Edge and Cloud-enabled framework to deploy the proposed diabetes prediction models on various configurations using edge nodes and main cloud servers. Performance metrics are evaluated using, latency, accuracy, and response time. By using ensemble machine learning voting algorithms we can improve the prediction accuracy by 5% versus a single model prediction.

### LoXR: Performance Evaluation of Locally Executing LLMs on XR Devices 
[[arxiv](https://arxiv.org/abs/2502.15761)] [[cool](https://papers.cool/arxiv/2502.15761)] [[pdf](https://arxiv.org/pdf/2502.15761)]
> **Authors**: Dawar Khan,Xinyu Liu,Omar Mena,Donggang Jia,Alexandre Kouyoumdjian,Ivan Viola
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 分布式、并行和集群计算,人工智能,图形,人机交互
- **Abstract**: The deployment of large language models (LLMs) on extended reality (XR) devices has great potential to advance the field of human-AI interaction. In the case of direct, on-device model inference, selecting the appropriate model and device for specific tasks remains challenging. In this paper, we deploy 17 LLMs across four XR devices--Magic Leap 2, Meta Quest 3, Vivo X100s Pro, and Apple Vision Pro, and conduct a comprehensive evaluation. We devise an experimental setup and evaluate performance on four key metrics: performance consistency, processing speed, memory usage, and battery consumption. For each of the 68 model-device pairs, we assess performance under varying string lengths, batch sizes, and thread counts, analyzing the trade-offs for real-time XR applications. We finally propose a unified evaluation method based on the Pareto Optimality theory to select the optimal device-model pairs from the quality and speed objectives. We believe our findings offer valuable insights to guide future optimization efforts for LLM deployment on XR devices. Our evaluation method can be followed as standard groundwork for further research and development in this emerging field. All supplemental materials are available at www.nanovis.org/Loxr.html.

### A Performance Analysis of You Only Look Once Models for Deployment on Constrained Computational Edge Devices in Drone Applications 
[[arxiv](https://arxiv.org/abs/2502.15737)] [[cool](https://papers.cool/arxiv/2502.15737)] [[pdf](https://arxiv.org/pdf/2502.15737)]
> **Authors**: Lucas Rey,Ana M. Bernardos,Andrzej D. Dobrzycki,David Carramiñana,Luca Bergesio,Juan A. Besada,José Ramón Casar
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-24
> **comment**: This manuscript consists of 24 pages, 7 figures, and 7 tables
- **标题**: None
- **领域**: 分布式、并行和集群计算,人工智能,计算机视觉和模式识别
- **Abstract**: Advancements in embedded systems and Artificial Intelligence (AI) have enhanced the capabilities of Unmanned Aircraft Vehicles (UAVs) in computer vision. However, the integration of AI techniques o-nboard drones is constrained by their processing capabilities. In this sense, this study evaluates the deployment of object detection models (YOLOv8n and YOLOv8s) on both resource-constrained edge devices and cloud environments. The objective is to carry out a comparative performance analysis using a representative real-time UAV image processing pipeline. Specifically, the NVIDIA Jetson Orin Nano, Orin NX, and Raspberry Pi 5 (RPI5) devices have been tested to measure their detection accuracy, inference speed, and energy consumption, and the effects of post-training quantization (PTQ). The results show that YOLOv8n surpasses YOLOv8s in its inference speed, achieving 52 FPS on the Jetson Orin NX and 65 fps with INT8 quantization. Conversely, the RPI5 failed to satisfy the real-time processing needs in spite of its suitability for low-energy consumption applications. An analysis of both the cloud-based and edge-based end-to-end processing times showed that increased communication latencies hindered real-time applications, revealing trade-offs between edge (low latency) and cloud processing (quick processing). Overall, these findings contribute to providing recommendations and optimization strategies for the deployment of AI models on UAVs.

### DistrEE: Distributed Early Exit of Deep Neural Network Inference on Edge Devices 
[[arxiv](https://arxiv.org/abs/2502.15735)] [[cool](https://papers.cool/arxiv/2502.15735)] [[pdf](https://arxiv.org/pdf/2502.15735)]
> **Authors**: Xian Peng,Xin Wu,Lianming Xu,Li Wang,Aiguo Fei
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 分布式、并行和集群计算,人工智能,机器学习
- **Abstract**: Distributed DNN inference is becoming increasingly important as the demand for intelligent services at the network edge grows. By leveraging the power of distributed computing, edge devices can perform complicated and resource-hungry inference tasks previously only possible on powerful servers, enabling new applications in areas such as autonomous vehicles, industrial automation, and smart homes. However, it is challenging to achieve accurate and efficient distributed edge inference due to the fluctuating nature of the actual resources of the devices and the processing difficulty of the input data. In this work, we propose DistrEE, a distributed DNN inference framework that can exit model inference early to meet specific quality of service requirements. In particular, the framework firstly integrates model early exit and distributed inference for multi-node collaborative inferencing scenarios. Furthermore, it designs an early exit policy to control when the model inference terminates. Extensive simulation results demonstrate that DistrEE can efficiently realize efficient collaborative inference, achieving an effective trade-off between inference latency and accuracy.

### Cache-Craft: Managing Chunk-Caches for Efficient Retrieval-Augmented Generation 
[[arxiv](https://arxiv.org/abs/2502.15734)] [[cool](https://papers.cool/arxiv/2502.15734)] [[pdf](https://arxiv.org/pdf/2502.15734)]
> **Authors**: Shubham Agarwal,Sai Sundaresan,Subrata Mitra,Debabrata Mahapatra,Archit Gupta,Rounak Sharma,Nirmal Joshua Kapu,Tong Yu,Shiv Saini
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-24
> **comment**: Accepted at SIGMOD 2025
- **标题**: None
- **领域**: 分布式、并行和集群计算,人工智能,计算语言学,机器学习,操作系统
- **Abstract**: Retrieval-Augmented Generation (RAG) is often used with Large Language Models (LLMs) to infuse domain knowledge or user-specific information. In RAG, given a user query, a retriever extracts chunks of relevant text from a knowledge base. These chunks are sent to an LLM as part of the input prompt. Typically, any given chunk is repeatedly retrieved across user questions. However, currently, for every question, attention-layers in LLMs fully compute the key values (KVs) repeatedly for the input chunks, as state-of-the-art methods cannot reuse KV-caches when chunks appear at arbitrary locations with arbitrary contexts. Naive reuse leads to output quality degradation. This leads to potentially redundant computations on expensive GPUs and increases latency. In this work, we propose Cache-Craft, a system for managing and reusing precomputed KVs corresponding to the text chunks (we call chunk-caches) in RAG-based systems. We present how to identify chunk-caches that are reusable, how to efficiently perform a small fraction of recomputation to fix the cache to maintain output quality, and how to efficiently store and evict chunk-caches in the hardware for maximizing reuse while masking any overheads. With real production workloads as well as synthetic datasets, we show that Cache-Craft reduces redundant computation by 51% over SOTA prefix-caching and 75% over full recomputation. Additionally, with continuous batching on a real production workload, we get a 1.6X speed up in throughput and a 2X reduction in end-to-end response latency over prefix-caching while maintaining quality, for both the LLaMA-3-8B and LLaMA-3-70B models.

### A Statistical Learning Approach for Feature-Aware Task-to-Core Allocation in Heterogeneous Platforms 
[[arxiv](https://arxiv.org/abs/2502.15716)] [[cool](https://papers.cool/arxiv/2502.15716)] [[pdf](https://arxiv.org/pdf/2502.15716)]
> **Authors**: Mohammad Pivezhandi,Abusayeed Saifullah,Prashant Modekurthy
> **First submission**: 2025-01-26
> **First announcement**: 2025-02-24
> **comment**: 12 pages, 7 figures
- **标题**: None
- **领域**: 分布式、并行和集群计算,机器学习
- **Abstract**: Optimizing task-to-core allocation can substantially reduce power consumption in multi-core platforms without degrading user experience. However, many existing approaches overlook critical factors such as parallelism, compute intensity, and heterogeneous core types. In this paper, we introduce a statistical learning approach for feature selection that identifies the most influential features - such as core type, speed, temperature, and application-level parallelism or memory intensity - for accurate environment modeling and efficient energy optimization. Our experiments, conducted with state-of-the-art Linux governors and thermal modeling techniques, show that correlation-aware task-to-core allocation lowers energy consumption by up to 10% and reduces core temperature by up to 5 degrees Celsius compared to random core selection. Furthermore, our compressed, bootstrapped regression model improves thermal prediction accuracy by 6% while cutting model parameters by 16%, yielding an overall mean square error reduction of 61.6% relative to existing approaches. We provided results based on superscalar Intel Core i7 12th Gen processors with 14 cores, but validated our method across a diverse set of hardware platforms and effectively balanced performance, power, and thermal demands through statistical feature evaluation.

## 数字图书馆(cs.DL:Digital Libraries)

### ACL-rlg: A Dataset for Reading List Generation 
[[arxiv](https://arxiv.org/abs/2502.15692)] [[cool](https://papers.cool/arxiv/2502.15692)] [[pdf](https://arxiv.org/pdf/2502.15692)]
> **Authors**: Julien Aubert-Béduchaud,Florian Boudin,Béatrice Daille,Richard Dufour
> **First submission**: 2024-12-30
> **First announcement**: 2025-02-24
> **comment**: ef:The 31st International Conference on Computational Linguistics, Jan 2025, Abu Dhabi, United Arab Emirates
- **标题**: None
- **领域**: 数字图书馆,人工智能,计算语言学,信息检索
- **Abstract**: Familiarizing oneself with a new scientific field and its existing literature can be daunting due to the large amount of available articles. Curated lists of academic references, or reading lists, compiled by experts, offer a structured way to gain a comprehensive overview of a domain or a specific scientific challenge. In this work, we introduce ACL-rlg, the largest open expert-annotated reading list dataset. We also provide multiple baselines for evaluating reading list generation and formally define it as a retrieval task. Our qualitative study highlights the fact that traditional scholarly search engines and indexing methods perform poorly on this task, and GPT-4o, despite showing better results, exhibits signs of potential data contamination.

## 数据结构和算法(cs.DS:Data Structures and Algorithms)

### Learning Neural Networks with Distribution Shift: Efficiently Certifiable Guarantees 
[[arxiv](https://arxiv.org/abs/2502.16021)] [[cool](https://papers.cool/arxiv/2502.16021)] [[pdf](https://arxiv.org/pdf/2502.16021)]
> **Authors**: Gautam Chandrasekaran,Adam R. Klivans,Lin Lin Lee,Konstantinos Stavropoulos
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: To appear in The Thirteenth International Conference onLearningRepresentations (ICLR 2025) 38 pages
- **标题**: None
- **领域**: 数据结构和算法,机器学习
- **Abstract**: We give the first provably efficient algorithms for learning neural networks with distribution shift. We work in the Testable Learning with Distribution Shift framework (TDS learning) of Klivans et al. (2024), where the learner receives labeled examples from a training distribution and unlabeled examples from a test distribution and must either output a hypothesis with low test error or reject if distribution shift is detected. No assumptions are made on the test distribution. All prior work in TDS learning focuses on classification, while here we must handle the setting of nonconvex regression. Our results apply to real-valued networks with arbitrary Lipschitz activations and work whenever the training distribution has strictly sub-exponential tails. For training distributions that are bounded and hypercontractive, we give a fully polynomial-time algorithm for TDS learning one hidden-layer networks with sigmoid activations. We achieve this by importing classical kernel methods into the TDS framework using data-dependent feature maps and a type of kernel matrix that couples samples from both train and test distributions.

### Compression Barriers for Autoregressive Transformers 
[[arxiv](https://arxiv.org/abs/2502.15955)] [[cool](https://papers.cool/arxiv/2502.15955)] [[pdf](https://arxiv.org/pdf/2502.15955)]
> **Authors**: Themistoklis Haris,Krzysztof Onak
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 数据结构和算法,人工智能,计算复杂度,机器学习
- **Abstract**: A key limitation of autoregressive Transformers is the large memory needed at inference-time to cache all previous key-value (KV) embeddings. Prior works address this by compressing the KV cache, but often assume specific structural properties of the embeddings. This raises the following natural question: Can truly sublinear space utilization be achieved without such assumptions? In this work, we answer this question in the negative. Any algorithm for attention-based token generation must use $Θ(nd)$ space, where $n$ is the number of tokens generated so far and $d = Ω(\log n)$ is the dimension of the KV embeddings. Our proof involves a reduction from a classic communication complexity problem and uses a randomized construction that leverages properties of projections in the spirit of the Johnson-Linderstrauss lemma. For the low-dimensional regime $d = o(\log n)$, we show that any algorithm requires $Ω(d\cdot e^d)$ space and prove, using tight bounds on covering numbers, that SubGen, proposed by Zandieh, Han, Mirrokni and Karbasi, matches this bound. Further, we investigate how sparsity assumptions enable token generation in truly sublinear space, presenting impossibility results and proposing a new KV cache compression algorithm for sliding window attention when the value cache outside the window is unmasked. Finally, we analyze token generation's time complexity, using an indistinguishability argument to prove that no non-adaptive algorithm can compute attention online in sublinear time for all tokens.

## 图形(cs.GR:Graphics)

### Generative AI Framework for 3D Object Generation in Augmented Reality 
[[arxiv](https://arxiv.org/abs/2502.15869)] [[cool](https://papers.cool/arxiv/2502.15869)] [[pdf](https://arxiv.org/pdf/2502.15869)]
> **Authors**: Majid Behravan
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 图形,人工智能,计算机视觉和模式识别,人机交互
- **Abstract**: This thesis presents a framework that integrates state-of-the-art generative AI models for real-time creation of three-dimensional (3D) objects in augmented reality (AR) environments. The primary goal is to convert diverse inputs, such as images and speech, into accurate 3D models, enhancing user interaction and immersion. Key components include advanced object detection algorithms, user-friendly interaction techniques, and robust AI models like Shap-E for 3D generation. Leveraging Vision Language Models (VLMs) and Large Language Models (LLMs), the system captures spatial details from images and processes textual information to generate comprehensive 3D objects, seamlessly integrating virtual objects into real-world environments. The framework demonstrates applications across industries such as gaming, education, retail, and interior design. It allows players to create personalized in-game assets, customers to see products in their environments before purchase, and designers to convert real-world objects into 3D models for real-time visualization. A significant contribution is democratizing 3D model creation, making advanced AI tools accessible to a broader audience, fostering creativity and innovation. The framework addresses challenges like handling multilingual inputs, diverse visual data, and complex environments, improving object detection and model generation accuracy, as well as loading 3D models in AR space in real-time. In conclusion, this thesis integrates generative AI and AR for efficient 3D model generation, enhancing accessibility and paving the way for innovative applications and improved user interactions in AR environments.

## 人机交互(cs.HC:Human-Computer Interaction)

### Tool or Tutor? Experimental evidence from AI deployment in cancer diagnosis 
[[arxiv](https://arxiv.org/abs/2502.16411)] [[cool](https://papers.cool/arxiv/2502.16411)] [[pdf](https://arxiv.org/pdf/2502.16411)]
> **Authors**: Vivianna Fang He,Sihan Li,Phanish Puranam
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 人机交互,人工智能,机器学习
- **Abstract**: Professionals increasingly use Artificial Intelligence (AI) to enhance their capabilities and assist with task execution. While prior research has examined these uses separately, their potential interaction remains underexplored. We propose that AI-driven training (tutor effect) and AI-assisted task completion (tool effect) can be complementary and test this hypothesis in the context of lung cancer diagnosis. In a field experiment with 334 medical students, we manipulated AI deployment in training, in practice, and in both. Our findings reveal that while AI-integrated training and AI assistance independently improved diagnostic performance, their combination yielded the highest accuracy. These results underscore AI's dual role in enhancing human performance through both learning and real-time support, offering insights into AI deployment in professional settings where human expertise remains essential.

### The Design Space of Recent AI-assisted Research Tools for Ideation, Sensemaking, and Scientific Creativity 
[[arxiv](https://arxiv.org/abs/2502.16291)] [[cool](https://papers.cool/arxiv/2502.16291)] [[pdf](https://arxiv.org/pdf/2502.16291)]
> **Authors**: Runlong Ye,Matthew Varona,Oliver Huang,Patrick Yung Kang Lee,Michael Liut,Carolina Nobre
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: :H.5.2
- **标题**: None
- **领域**: 人机交互,人工智能
- **Abstract**: Generative AI (GenAI) tools are radically expanding the scope and capability of automation in knowledge work such as academic research. AI-assisted research tools show promise for augmenting human cognition and streamlining research processes, but could potentially increase automation bias and stifle critical thinking. We surveyed the past three years of publications from leading HCI venues. We closely examined 11 AI-assisted research tools, five employing traditional AI approaches and six integrating GenAI, to explore how these systems envision novel capabilities and design spaces. We consolidate four design recommendations that inform cognitive engagement when working with an AI research tool: Providing user agency and control; enabling divergent and convergent thinking; supporting adaptability and flexibility; and ensuring transparency and accuracy. We discuss how these ideas mark a shift in AI-assisted research tools from mimicking a researcher's established workflows to generative co-creation with the researcher and the opportunities this shift affords the research community.

### ZIA: A Theoretical Framework for Zero-Input AI 
[[arxiv](https://arxiv.org/abs/2502.16124)] [[cool](https://papers.cool/arxiv/2502.16124)] [[pdf](https://arxiv.org/pdf/2502.16124)]
> **Authors**: Aditi De,NeuroBits Labs
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 人机交互,机器学习
- **Abstract**: Zero-Input AI (ZIA) introduces a novel framework for human-computer interaction by enabling proactive intent prediction without explicit user commands. It integrates gaze tracking, bio-signals (EEG, heart rate), and contextual data (time, location, usage history) into a multi-modal model for real-time inference, targeting <100 ms latency. The proposed architecture employs a transformer-based model with cross-modal attention, variational Bayesian inference for uncertainty estimation, and reinforcement learning for adaptive optimization. To support deployment on edge devices (CPUs, TPUs, NPUs), ZIA utilizes quantization, weight pruning, and linear attention to reduce complexity from quadratic to linear with sequence length. Theoretical analysis establishes an information-theoretic bound on prediction error and demonstrates how multi-modal fusion improves accuracy over single-modal approaches. Expected performance suggests 85-90% accuracy with EEG integration and 60-100 ms inference latency. ZIA provides a scalable, privacy-preserving framework for accessibility, healthcare, and consumer applications, advancing AI toward anticipatory intelligence.

### LitLinker: Supporting the Ideation of Interdisciplinary Contexts with Large Language Models for Teaching Literature in Elementary Schools 
[[arxiv](https://arxiv.org/abs/2502.16097)] [[cool](https://papers.cool/arxiv/2502.16097)] [[pdf](https://arxiv.org/pdf/2502.16097)]
> **Authors**: Haoxiang Fan,Changshuang Zhou,Hao Yu,Xueyang Wu,Jiangyu Gu,Zhenhui Peng
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 人机交互,人工智能
- **Abstract**: Teaching literature under interdisciplinary contexts (e.g., science, art) that connect reading materials has become popular in elementary schools. However, constructing such contexts is challenging as it requires teachers to explore substantial amounts of interdisciplinary content and link it to the reading materials. In this paper, we develop LitLinker via an iterative design process involving 13 teachers to facilitate the ideation of interdisciplinary contexts for teaching literature. Powered by a large language model (LLM), LitLinker can recommend interdisciplinary topics and contextualize them with the literary elements (e.g., paragraphs, viewpoints) in the reading materials. A within-subjects study (N=16) shows that compared to an LLM chatbot, LitLinker can improve the integration depth of different subjects and reduce workload in this ideation task. Expert interviews (N=9) also demonstrate LitLinker's usefulness for supporting the ideation of interdisciplinary contexts for teaching literature. We conclude with concerns and design considerations for supporting interdisciplinary teaching with LLMs.

### Text-to-SQL Domain Adaptation via Human-LLM Collaborative Data Annotation 
[[arxiv](https://arxiv.org/abs/2502.15980)] [[cool](https://papers.cool/arxiv/2502.15980)] [[pdf](https://arxiv.org/pdf/2502.15980)]
> **Authors**: Yuan Tian,Daniel Lee,Fei Wu,Tung Mai,Kun Qian,Siddhartha Sahai,Tianyi Zhang,Yunyao Li
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Accepted by IUI'25
- **标题**: None
- **领域**: 人机交互,人工智能,数据库
- **Abstract**: Text-to-SQL models, which parse natural language (NL) questions to executable SQL queries, are increasingly adopted in real-world applications. However, deploying such models in the real world often requires adapting them to the highly specialized database schemas used in specific applications. We find that existing text-to-SQL models experience significant performance drops when applied to new schemas, primarily due to the lack of domain-specific data for fine-tuning. This data scarcity also limits the ability to effectively evaluate model performance in new domains. Continuously obtaining high-quality text-to-SQL data for evolving schemas is prohibitively expensive in real-world scenarios. To bridge this gap, we propose SQLsynth, a human-in-the-loop text-to-SQL data annotation system. SQLsynth streamlines the creation of high-quality text-to-SQL datasets through human-LLM collaboration in a structured workflow. A within-subjects user study comparing SQLsynth with manual annotation and ChatGPT shows that SQLsynth significantly accelerates text-to-SQL data annotation, reduces cognitive load, and produces datasets that are more accurate, natural, and diverse. Our code is available at https://github.com/adobe/nl_sql_analyzer.

### Identifying Features that Shape Perceived Consciousness in Large Language Model-based AI: A Quantitative Study of Human Responses 
[[arxiv](https://arxiv.org/abs/2502.15365)] [[cool](https://papers.cool/arxiv/2502.15365)] [[pdf](https://arxiv.org/pdf/2502.15365)]
> **Authors**: Bongsu Kang,Jundong Kim,Tae-Rim Yun,Hyojin Bae,Chang-Eop Kim
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 11 pages, 3 figures, 4 tables
- **标题**: None
- **领域**: 人机交互,人工智能,计算语言学,计算机与社会
- **Abstract**: This study quantitively examines which features of AI-generated text lead humans to perceive subjective consciousness in large language model (LLM)-based AI systems. Drawing on 99 passages from conversations with Claude 3 Opus and focusing on eight features -- metacognitive self-reflection, logical reasoning, empathy, emotionality, knowledge, fluency, unexpectedness, and subjective expressiveness -- we conducted a survey with 123 participants. Using regression and clustering analyses, we investigated how these features influence participants' perceptions of AI consciousness. The results reveal that metacognitive self-reflection and the AI's expression of its own emotions significantly increased perceived consciousness, while a heavy emphasis on knowledge reduced it. Participants clustered into seven subgroups, each showing distinct feature-weighting patterns. Additionally, higher prior knowledge of LLMs and more frequent usage of LLM-based chatbots were associated with greater overall likelihood assessments of AI consciousness. This study underscores the multidimensional and individualized nature of perceived AI consciousness and provides a foundation for better understanding the psychosocial implications of human-AI interaction.

### M2LADS Demo: A System for Generating Multimodal Learning Analytics Dashboards 
[[arxiv](https://arxiv.org/abs/2502.15363)] [[cool](https://papers.cool/arxiv/2502.15363)] [[pdf](https://arxiv.org/pdf/2502.15363)]
> **Authors**: Alvaro Becerra,Roberto Daza,Ruth Cobos,Aythami Morales,Julian Fierrez
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Published in the Workshop on Innovation and Responsibility inAI-Supported Education (iRAISE25) at AAAI 2025
- **标题**: None
- **领域**: 人机交互,计算机视觉和模式识别
- **Abstract**: We present a demonstration of a web-based system called M2LADS ("System for Generating Multimodal Learning Analytics Dashboards"), designed to integrate, synchronize, visualize, and analyze multimodal data recorded during computer-based learning sessions with biosensors. This system presents a range of biometric and behavioral data on web-based dashboards, providing detailed insights into various physiological and activity-based metrics. The multimodal data visualized include electroencephalogram (EEG) data for assessing attention and brain activity, heart rate metrics, eye-tracking data to measure visual attention, webcam video recordings, and activity logs of the monitored tasks. M2LADS aims to assist data scientists in two key ways: (1) by providing a comprehensive view of participants' experiences, displaying all data categorized by the activities in which participants are engaged, and (2) by synchronizing all biosignals and videos, facilitating easier data relabeling if any activity information contains errors.

### ComposeOn Academy: Transforming Melodic Ideas into Complete Compositions Integrating Music Learning 
[[arxiv](https://arxiv.org/abs/2502.15255)] [[cool](https://papers.cool/arxiv/2502.15255)] [[pdf](https://arxiv.org/pdf/2502.15255)]
> **Authors**: Hongxi Pu,Futian Jiang,Zihao Chen,Xingyue Song
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 人机交互,人工智能
- **Abstract**: Music composition has long been recognized as a significant art form. However, existing digital audio workstations and music production software often present high entry barriers for users lacking formal musical training. To address this, we introduce ComposeOn, a music theory-based tool designed for users with limited musical knowledge. ComposeOn enables users to easily extend their melodic ideas into complete compositions and offers simple editing features. By integrating music theory, it explains music creation at beginner, intermediate, and advanced levels. Our user study (N=10) compared ComposeOn with the baseline method, Suno AI, demonstrating that ComposeOn provides a more accessible and enjoyable composing and learning experience for individuals with limited musical skills. ComposeOn bridges the gap between theory and practice, offering an innovative solution as both a composition aid and music education platform. The study also explores the differences between theory-based music creation and generative music, highlighting the former's advantages in personal expression and learning.

## 信息检索(cs.IR:Information Retrieval)

### Ensemble ToT of LLMs and Its Application to Automatic Grading System for Supporting Self-Learning 
[[arxiv](https://arxiv.org/abs/2502.16399)] [[cool](https://papers.cool/arxiv/2502.16399)] [[pdf](https://arxiv.org/pdf/2502.16399)]
> **Authors**: Yuki Ito,Qiang Ma
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: 33 pages, 25 figures
- **标题**: None
- **领域**: 信息检索,人工智能,计算语言学
- **Abstract**: Providing students with detailed and timely grading feedback is essential for self-learning. While existing LLM-based grading systems are promising, most of them rely on one single model, which limits their performance. To address this, we propose Ensemble Tree-of-Thought (ToT), a framework that enhances LLM outputs by integrating multiple models. Using this framework, we develop a grading system. Ensemble ToT follows three steps: (1) analyzing LLM performance, (2) generating candidate answers, and (3) refining them into a final result. Based on this, our grading system first evaluates the grading tendencies of LLMs, then generates multiple results, and finally integrates them via a simulated debate. Experimental results demonstrate our approach's ability to provide accurate and explainable grading by effectively coordinating multiple LLMs.

### Separated Contrastive Learning for Matching in Cross-domain Recommendation with Curriculum Scheduling 
[[arxiv](https://arxiv.org/abs/2502.16239)] [[cool](https://papers.cool/arxiv/2502.16239)] [[pdf](https://arxiv.org/pdf/2502.16239)]
> **Authors**: Heng Chang,Liang Gu,Cheng Hu,Zhinan Zhang,Hong Zhu,Yuhui Xu,Yuan Fang,Zhen Chen
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: Accepted by TheWebConf 2025
- **标题**: None
- **领域**: 信息检索,机器学习,社交和信息网络
- **Abstract**: Cross-domain recommendation (CDR) is a task that aims to improve the recommendation performance in a target domain by leveraging the information from source domains. Contrastive learning methods have been widely adopted among intra-domain (intra-CL) and inter-domain (inter-CL) users/items for their representation learning and knowledge transfer during the matching stage of CDR. However, we observe that directly employing contrastive learning on mixed-up intra-CL and inter-CL tasks ignores the difficulty of learning from inter-domain over learning from intra-domain, and thus could cause severe training instability. Therefore, this instability deteriorates the representation learning process and hurts the quality of generated embeddings. To this end, we propose a novel framework named SCCDR built up on a separated intra-CL and inter-CL paradigm and a stop-gradient operation to handle the drawback. Specifically, SCCDR comprises two specialized curriculum stages: intra-inter separation and inter-domain curriculum scheduling. The former stage explicitly uses two distinct contrastive views for the intra-CL task in the source and target domains, respectively. Meanwhile, the latter stage deliberately tackles the inter-CL tasks with a curriculum scheduling strategy that derives effective curricula by accounting for the difficulty of negative samples anchored by overlapping users. Empirical experiments on various open-source datasets and an offline proprietary industrial dataset extracted from a real-world recommender system, and an online A/B test verify that SCCDR achieves state-of-the-art performance over multiple baselines.

### Inference Computation Scaling for Feature Augmentation in Recommendation Systems 
[[arxiv](https://arxiv.org/abs/2502.16040)] [[cool](https://papers.cool/arxiv/2502.16040)] [[pdf](https://arxiv.org/pdf/2502.16040)]
> **Authors**: Weihao Liu,Zhaocheng Du,Haiyuan Zhao,Wenbo Zhang,Xiaoyan Zhao,Gang Wang,Zhenhua Dong,Jun Xu
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,计算语言学
- **Abstract**: Large language models have become a powerful method for feature augmentation in recommendation systems. However, existing approaches relying on quick inference often suffer from incomplete feature coverage and insufficient specificity in feature descriptions, limiting their ability to capture fine-grained user preferences and undermining overall performance. Motivated by the recent success of inference scaling in math and coding tasks, we explore whether scaling inference can address these limitations and enhance feature quality. Our experiments show that scaling inference leads to significant improvements in recommendation performance, with a 12% increase in NDCG@10. The gains can be attributed to two key factors: feature quantity and specificity. In particular, models using extended Chain-of-Thought (CoT) reasoning generate a greater number of detailed and precise features, offering deeper insights into user preferences and overcoming the limitations of quick inference. We further investigate the factors influencing feature quantity, revealing that model choice and search strategy play critical roles in generating a richer and more diverse feature set. This is the first work to apply inference scaling to feature augmentation in recommendation systems, bridging advances in reasoning tasks to enhance personalized recommendation.

### Automated Query-Product Relevance Labeling using Large Language Models for E-commerce Search 
[[arxiv](https://arxiv.org/abs/2502.15990)] [[cool](https://papers.cool/arxiv/2502.15990)] [[pdf](https://arxiv.org/pdf/2502.15990)]
> **Authors**: Jayant Sachdev,Sean D Rosario,Abhijeet Phatak,He Wen,Swati Kirti,Chittaranjan Tripathy
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,人工智能,计算语言学,机器学习
- **Abstract**: Accurate query-product relevance labeling is indispensable to generate ground truth dataset for search ranking in e-commerce. Traditional approaches for annotating query-product pairs rely on human-based labeling services, which is expensive, time-consuming and prone to errors. In this work, we explore the application of Large Language Models (LLMs) to automate query-product relevance labeling for large-scale e-commerce search. We use several publicly available and proprietary LLMs for this task, and conducted experiments on two open-source datasets and an in-house e-commerce search dataset. Using prompt engineering techniques such as Chain-of-Thought (CoT) prompting, In-context Learning (ICL), and Retrieval Augmented Generation (RAG) with Maximum Marginal Relevance (MMR), we show that LLM's performance has the potential to approach human-level accuracy on this task in a fraction of the time and cost required by human-labelers, thereby suggesting that our approach is more efficient than the conventional methods. We have generated query-product relevance labels using LLMs at scale, and are using them for evaluating improvements to our search algorithms. Our work demonstrates the potential of LLMs to improve query-product relevance thus enhancing e-commerce search user experience. More importantly, this scalable alternative to human-annotation has significant implications for information retrieval domains including search and recommendation systems, where relevance scoring is crucial for optimizing the ranking of products and content to improve customer engagement and other conversion metrics.

### Visual Zero-Shot E-Commerce Product Attribute Value Extraction 
[[arxiv](https://arxiv.org/abs/2502.15979)] [[cool](https://papers.cool/arxiv/2502.15979)] [[pdf](https://arxiv.org/pdf/2502.15979)]
> **Authors**: Jiaying Gong,Ming Cheng,Hongda Shen,Pierre-Yves Vandenbussche,Janet Jenq,Hoda Eldardiry
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 10 pages, 4 figures, accepted for publication in NAACL 2025 Industry Track
- **标题**: None
- **领域**: 信息检索,计算机视觉和模式识别
- **Abstract**: Existing zero-shot product attribute value (aspect) extraction approaches in e-Commerce industry rely on uni-modal or multi-modal models, where the sellers are asked to provide detailed textual inputs (product descriptions) for the products. However, manually providing (typing) the product descriptions is time-consuming and frustrating for the sellers. Thus, we propose a cross-modal zero-shot attribute value generation framework (ViOC-AG) based on CLIP, which only requires product images as the inputs. ViOC-AG follows a text-only training process, where a task-customized text decoder is trained with the frozen CLIP text encoder to alleviate the modality gap and task disconnection. During the zero-shot inference, product aspects are generated by the frozen CLIP image encoder connected with the trained task-customized text decoder. OCR tokens and outputs from a frozen prompt-based LLM correct the decoded outputs for out-of-domain attribute values. Experiments show that ViOC-AG significantly outperforms other fine-tuned vision-language models for zero-shot attribute value extraction.

### Instruction-Based Fine-tuning of Open-Source LLMs for Predicting Customer Purchase Behaviors 
[[arxiv](https://arxiv.org/abs/2502.15724)] [[cool](https://papers.cool/arxiv/2502.15724)] [[pdf](https://arxiv.org/pdf/2502.15724)]
> **Authors**: Halil Ibrahim Ergul,Selim Balcisoy,Burcin Bozkaya
> **First submission**: 2025-01-28
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,人工智能
- **Abstract**: In this study, the performance of various predictive models, including probabilistic baseline, CNN, LSTM, and finetuned LLMs, in forecasting merchant categories from financial transaction data have been evaluated. Utilizing datasets from Bank A for training and Bank B for testing, the superior predictive capabilities of the fine-tuned Mistral Instruct model, which was trained using customer data converted into natural language format have been demonstrated. The methodology of this study involves instruction fine-tuning Mistral via LoRA (LowRank Adaptation of Large Language Models) to adapt its vast pre-trained knowledge to the specific domain of financial transactions. The Mistral model significantly outperforms traditional sequential models, achieving higher F1 scores in the three key merchant categories of bank transaction data (grocery, clothing, and gas stations) that is crucial for targeted marketing campaigns. This performance is attributed to the model's enhanced semantic understanding and adaptability which enables it to better manage minority classes and predict transaction categories with greater accuracy. These findings highlight the potential of LLMs in predicting human behavior.

### Balancing Content Size in RAG-Text2SQL System 
[[arxiv](https://arxiv.org/abs/2502.15723)] [[cool](https://papers.cool/arxiv/2502.15723)] [[pdf](https://arxiv.org/pdf/2502.15723)]
> **Authors**: Prakhar Gurawa,Anjali Dharmik
> **First submission**: 2025-01-28
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,人工智能,数据库
- **Abstract**: Large Language Models (LLMs) have emerged as a promising solution for converting natural language queries into SQL commands, enabling seamless database interaction. However, these Text-to-SQL (Text2SQL) systems face inherent limitations, hallucinations, outdated knowledge, and untraceable reasoning. To address these challenges, the integration of retrieval-augmented generation (RAG) with Text2SQL models has gained traction. RAG serves as a retrieval mechanism, providing essential contextual information, such as table schemas and metadata, to enhance the query generation process. Despite their potential, RAG + Text2SQL systems are susceptible to the quality and size of retrieved documents. While richer document content can improve schema relevance and retrieval accuracy, it also introduces noise, increasing the risk of hallucinations and reducing query fidelity as the prompt size of the Text2SQL model increases. This research investigates the nuanced trade-off between document size and quality, aiming to strike a balance that optimizes system performance. Key thresholds are identified where performance degradation occurs, along with actionable strategies to mitigate these challenges. Additionally, we explore the phenomenon of hallucinations in Text2SQL models, emphasizing the critical role of curated document presentation in minimizing errors. Our findings provide a roadmap for enhancing the robustness of RAG + Text2SQL systems, offering practical insights for real-world applications.

### iTRI-QA: a Toolset for Customized Question-Answer Dataset Generation Using Language Models for Enhanced Scientific Research 
[[arxiv](https://arxiv.org/abs/2502.15721)] [[cool](https://papers.cool/arxiv/2502.15721)] [[pdf](https://arxiv.org/pdf/2502.15721)]
> **Authors**: Qiming Liu,Zhongzheng Niu,Siting Liu,Mao Tian
> **First submission**: 2025-01-27
> **First announcement**: 2025-02-24
> **comment**: 13 pages, 3 figures
- **标题**: None
- **领域**: 信息检索,人工智能,数字图书馆
- **Abstract**: The exponential growth of AI in science necessitates efficient and scalable solutions for retrieving and preserving research information. Here, we present a tool for the development of a customized question-answer (QA) dataset, called Interactive Trained Research Innovator (iTRI) - QA, tailored for the needs of researchers leveraging language models (LMs) to retrieve scientific knowledge in a QA format. Our approach integrates curated QA datasets with a specialized research paper dataset to enhance responses' contextual relevance and accuracy using fine-tuned LM. The framework comprises four key steps: (1) the generation of high-quality and human-generated QA examples, (2) the creation of a structured research paper database, (3) the fine-tuning of LMs using domain-specific QA examples, and (4) the generation of QA dataset that align with user queries and the curated database. This pipeline provides a dynamic and domain-specific QA system that augments the utility of LMs in academic research that will be applied for future research LM deployment. We demonstrate the feasibility and scalability of our tool for streamlining knowledge retrieval in scientific contexts, paving the way for its integration into broader multi-disciplinary applications.

### Making Sense of Data in the Wild: Data Analysis Automation at Scale 
[[arxiv](https://arxiv.org/abs/2502.15718)] [[cool](https://papers.cool/arxiv/2502.15718)] [[pdf](https://arxiv.org/pdf/2502.15718)]
> **Authors**: Mara Graziani,Malina Molnar,Irina Espejo Morales,Joris Cadow-Gossweiler,Teodoro Laino
> **First submission**: 2025-01-27
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,机器学习
- **Abstract**: As the volume of publicly available data continues to grow, researchers face the challenge of limited diversity in benchmarking machine learning tasks. Although thousands of datasets are available in public repositories, the sheer abundance often complicates the search for suitable data, leaving many valuable datasets underexplored. This situation is further amplified by the fact that, despite longstanding advocacy for improving data curation quality, current solutions remain prohibitively time-consuming and resource-intensive. In this paper, we propose a novel approach that combines intelligent agents with retrieval augmented generation to automate data analysis, dataset curation and indexing at scale. Our system leverages multiple agents to analyze raw, unstructured data across public repositories, generating dataset reports and interactive visual indexes that can be easily explored. We demonstrate that our approach results in more detailed dataset descriptions, higher hit rates and greater diversity in dataset retrieval tasks. Additionally, we show that the dataset reports generated by our method can be leveraged by other machine learning models to improve the performance on specific tasks, such as improving the accuracy and realism of synthetic data generation. By streamlining the process of transforming raw data into machine-learning-ready datasets, our approach enables researchers to better utilize existing data resources.

### TrustDataFilter:Leveraging Trusted Knowledge Base Data for More Effective Filtering of Unknown Information 
[[arxiv](https://arxiv.org/abs/2502.15714)] [[cool](https://papers.cool/arxiv/2502.15714)] [[pdf](https://arxiv.org/pdf/2502.15714)]
> **Authors**: Jinghong Zhang,Yidong Cui,Weiling Wang,Xianyou Cheng
> **First submission**: 2025-01-24
> **First announcement**: 2025-02-24
> **comment**: 12 pages, 8 figures, submitted to IEEE Transactions on Knowledge and Data Engineering
- **标题**: None
- **领域**: 信息检索,人工智能,计算语言学
- **Abstract**: With the advancement of technology and changes in the market, the demand for the construction of domain-specific knowledge bases has been increasing, either to improve model performance or to promote enterprise innovation and competitiveness. The construction of domain-specific knowledge bases typically relies on web crawlers or existing industry databases, leading to problems with accuracy and consistency of the data. To address these challenges, we considered the characteristics of domain data, where internal knowledge is interconnected, and proposed the Self-Natural Language Inference Data Filtering (self-nli-TDF) framework. This framework compares trusted filtered knowledge with the data to be filtered, deducing the reasoning relationship between them, thus improving filtering performance. The framework uses plug-and-play large language models for trustworthiness assessment and employs the RoBERTa-MNLI model from the NLI domain for reasoning. We constructed three datasets in the domains of biology, radiation, and science, and conducted experiments using RoBERTa, GPT3.5, and the local Qwen2 model. The experimental results show that this framework improves filter quality, producing more consistent and reliable filtering results.

### TutorLLM: Customizing Learning Recommendations with Knowledge Tracing and Retrieval-Augmented Generation 
[[arxiv](https://arxiv.org/abs/2502.15709)] [[cool](https://papers.cool/arxiv/2502.15709)] [[pdf](https://arxiv.org/pdf/2502.15709)]
> **Authors**: Zhaoxing Li,Vahid Yazdanpanah,Jindi Wang,Wen Gu,Lei Shi,Alexandra I. Cristea,Sarah Kiden,Sebastian Stein
> **First submission**: 2025-01-20
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,人工智能,机器学习
- **Abstract**: The integration of AI in education offers significant potential to enhance learning efficiency. Large Language Models (LLMs), such as ChatGPT, Gemini, and Llama, allow students to query a wide range of topics, providing unprecedented flexibility. However, LLMs face challenges, such as handling varying content relevance and lack of personalization. To address these challenges, we propose TutorLLM, a personalized learning recommender LLM system based on Knowledge Tracing (KT) and Retrieval-Augmented Generation (RAG). The novelty of TutorLLM lies in its unique combination of KT and RAG techniques with LLMs, which enables dynamic retrieval of context-specific knowledge and provides personalized learning recommendations based on the student's personal learning state. Specifically, this integration allows TutorLLM to tailor responses based on individual learning states predicted by the Multi-Features with Latent Relations BERT-based KT (MLFBK) model and to enhance response accuracy with a Scraper model. The evaluation includes user assessment questionnaires and performance metrics, demonstrating a 10\% improvement in user satisfaction and a 5\% increase in quiz scores compared to using general LLMs alone.

### Large language models streamline automated systematic review: A preliminary study 
[[arxiv](https://arxiv.org/abs/2502.15702)] [[cool](https://papers.cool/arxiv/2502.15702)] [[pdf](https://arxiv.org/pdf/2502.15702)]
> **Authors**: Xi Chen,Xue Zhang
> **First submission**: 2025-01-08
> **First announcement**: 2025-02-24
> **comment**: 25 pages, 9 figures
- **标题**: None
- **领域**: 信息检索,人工智能,计算语言学
- **Abstract**: Large Language Models (LLMs) have shown promise in natural language processing tasks, with the potential to automate systematic reviews. This study evaluates the performance of three state-of-the-art LLMs in conducting systematic review tasks. We assessed GPT-4, Claude-3, and Mistral 8x7B across four systematic review tasks: study design formulation, search strategy development, literature screening, and data extraction. Sourced from a previously published systematic review, we provided reference standard including standard PICO (Population, Intervention, Comparison, Outcome) design, standard eligibility criteria, and data from 20 reference literature. Three investigators evaluated the quality of study design and eligibility criteria using 5-point Liker Scale in terms of accuracy, integrity, relevance, consistency and overall performance. For other tasks, the output is defined as accurate if it is the same as the reference standard. Search strategy performance was evaluated through accuracy and retrieval efficacy. Screening accuracy was assessed for both abstracts screening and full texts screening. Data extraction accuracy was evaluated across 1,120 data points comprising 3,360 individual fields. Claude-3 demonstrated superior overall performance in PICO design. In search strategy formulation, GPT-4 and Claude-3 achieved comparable accuracy, outperforming Mistral. For abstract screening, GPT-4 achieved the highest accuracy, followed by Mistral and Claude-3. In data extraction, GPT-4 significantly outperformed other models. LLMs demonstrate potential for automating systematic review tasks, with GPT-4 showing superior performance in search strategy formulation, literature screening and data extraction. These capabilities make them promising assistive tools for researchers and warrant further development and validation in this field.

### Political Events using RAG with LLMs 
[[arxiv](https://arxiv.org/abs/2502.15701)] [[cool](https://papers.cool/arxiv/2502.15701)] [[pdf](https://arxiv.org/pdf/2502.15701)]
> **Authors**: Muhammad Arslan,Saba Munawar,Christophe Cruz
> **First submission**: 2025-01-06
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,人工智能,计算语言学
- **Abstract**: In the contemporary digital landscape, media content stands as the foundation for political news analysis, offering invaluable insights sourced from various channels like news articles, social media updates, speeches, and reports. Natural Language Processing (NLP) has revolutionized Political Information Extraction (IE), automating tasks such as Event Extraction (EE) from these diverse media outlets. While traditional NLP methods often necessitate specialized expertise to build rule-based systems or train machine learning models with domain-specific datasets, the emergence of Large Language Models (LLMs) driven by Generative Artificial Intelligence (GenAI) presents a promising alternative. These models offer accessibility, alleviating challenges associated with model construction from scratch and reducing the dependency on extensive datasets during the training phase, thus facilitating rapid implementation. However, challenges persist in handling domain-specific tasks, leading to the development of the Retrieval-Augmented Generation (RAG) framework. RAG enhances LLMs by integrating external data retrieval, enriching their contextual understanding, and expanding their knowledge base beyond pre-existing training data. To illustrate RAG's efficacy, we introduce the Political EE system, specifically tailored to extract political event information from news articles. Understanding these political insights is essential for remaining informed about the latest political advancements, whether on a national or global scale.

### Sustainable Digitalization of Business with Multi-Agent RAG and LLM 
[[arxiv](https://arxiv.org/abs/2502.15700)] [[cool](https://papers.cool/arxiv/2502.15700)] [[pdf](https://arxiv.org/pdf/2502.15700)]
> **Authors**: Muhammad Arslan,Saba Munawar,Christophe Cruz
> **First submission**: 2025-01-06
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,人工智能,计算语言学
- **Abstract**: Businesses heavily rely on data sourced from various channels like news articles, financial reports, and consumer reviews to drive their operations, enabling informed decision-making and identifying opportunities. However, traditional manual methods for data extraction are often time-consuming and resource-intensive, prompting the adoption of digital transformation initiatives to enhance efficiency. Yet, concerns persist regarding the sustainability of such initiatives and their alignment with the United Nations (UN)'s Sustainable Development Goals (SDGs). This research aims to explore the integration of Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) as a sustainable solution for Information Extraction (IE) and processing. The research methodology involves reviewing existing solutions for business decision-making, noting that many systems require training new machine learning models, which are resource-intensive and have significant environmental impacts. Instead, we propose a sustainable business solution using pre-existing LLMs that can work with diverse datasets. We link domain-specific datasets to tailor LLMs to company needs and employ a Multi-Agent architecture to divide tasks such as information retrieval, enrichment, and classification among specialized agents. This approach optimizes the extraction process and improves overall efficiency. Through the utilization of these technologies, businesses can optimize resource utilization, improve decision-making processes, and contribute to sustainable development goals, thereby fostering environmental responsibility within the corporate sector.

### Robust Uplift Modeling with Large-Scale Contexts for Real-time Marketing 
[[arxiv](https://arxiv.org/abs/2502.15697)] [[cool](https://papers.cool/arxiv/2502.15697)] [[pdf](https://arxiv.org/pdf/2502.15697)]
> **Authors**: Zexu Sun,Qiyu Han,Minqin Zhu,Hao Gong,Dugang Liu,Chen Ma
> **First submission**: 2025-01-04
> **First announcement**: 2025-02-24
> **comment**: Accepted to KDD'25 Research Track, 15 pages, 11 figures
- **标题**: None
- **领域**: 信息检索,人工智能,机器学习
- **Abstract**: Improving user engagement and platform revenue is crucial for online marketing platforms. Uplift modeling is proposed to solve this problem, which applies different treatments (e.g., discounts, bonus) to satisfy corresponding users. Despite progress in this field, limitations persist. Firstly, most of them focus on scenarios where only user features exist. However, in real-world scenarios, there are rich contexts available in the online platform (e.g., short videos, news), and the uplift model needs to infer an incentive for each user on the specific item, which is called real-time marketing. Thus, only considering the user features will lead to biased prediction of the responses, which may cause the cumulative error for uplift prediction. Moreover, due to the large-scale contexts, directly concatenating the context features with the user features will cause a severe distribution shift in the treatment and control groups. Secondly, capturing the interaction relationship between the user features and context features can better predict the user response. To solve the above limitations, we propose a novel model-agnostic Robust Uplift Modeling with Large-Scale Contexts (UMLC) framework for Real-time Marketing. Our UMLC includes two customized modules. 1) A response-guided context grouping module for extracting context features information and condensing value space through clusters. 2) A feature interaction module for obtaining better uplift prediction. Specifically, this module contains two parts: a user-context interaction component for better modeling the response; a treatment-feature interaction component for discovering the treatment assignment sensitive feature of each instance to better predict the uplift. Moreover, we conduct extensive experiments on a synthetic dataset and a real-world product dataset to verify the effectiveness and compatibility of our UMLC.

### Image Fusion for Cross-Domain Sequential Recommendation 
[[arxiv](https://arxiv.org/abs/2502.15694)] [[cool](https://papers.cool/arxiv/2502.15694)] [[pdf](https://arxiv.org/pdf/2502.15694)]
> **Authors**: Wangyu Wu,Siqi Song,Xianglin Qiu,Xiaowei Huang,Fei Ma,Jimin Xiao
> **First submission**: 2024-12-30
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,计算机视觉和模式识别,机器学习
- **Abstract**: Cross-Domain Sequential Recommendation (CDSR) aims to predict future user interactions based on historical interactions across multiple domains. The key challenge in CDSR is effectively capturing cross-domain user preferences by fully leveraging both intra-sequence and inter-sequence item interactions. In this paper, we propose a novel method, Image Fusion for Cross-Domain Sequential Recommendation (IFCDSR), which incorporates item image information to better capture visual preferences. Our approach integrates a frozen CLIP model to generate image embeddings, enriching original item embeddings with visual data from both intra-sequence and inter-sequence interactions. Additionally, we employ a multiple attention layer to capture cross-domain interests, enabling joint learning of single-domain and cross-domain user preferences. To validate the effectiveness of IFCDSR, we re-partitioned four e-commerce datasets and conducted extensive experiments. Results demonstrate that IFCDSR significantly outperforms existing methods.

### Hgformer: Hyperbolic Graph Transformer for Recommendation 
[[arxiv](https://arxiv.org/abs/2502.15693)] [[cool](https://papers.cool/arxiv/2502.15693)] [[pdf](https://arxiv.org/pdf/2502.15693)]
> **Authors**: Xin Yang,Xingrun Li,Heng Chang,Jinze Yang,Xihong Yang,Shengyu Tao,Ningkang Chang,Maiko Shigeno,Junfeng Wang,Dawei Yin,Erxue Min
> **First submission**: 2024-12-30
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,人工智能,机器学习
- **Abstract**: The cold start problem is a challenging problem faced by most modern recommender systems. By leveraging knowledge from other domains, cross-domain recommendation can be an effective method to alleviate the cold start problem. However, the modelling distortion for long-tail data, which is widely present in recommender systems, is often overlooked in cross-domain recommendation. In this research, we propose a hyperbolic manifold based cross-domain collaborative filtering model using BiTGCF as the base model. We introduce the hyperbolic manifold and construct new propagation layer and transfer layer to address these challenges. The significant performance improvements across various datasets compared to the baseline models demonstrate the effectiveness of our proposed model.

### The Synergy of Automated Pipelines with Prompt Engineering and Generative AI in Web Crawling 
[[arxiv](https://arxiv.org/abs/2502.15691)] [[cool](https://papers.cool/arxiv/2502.15691)] [[pdf](https://arxiv.org/pdf/2502.15691)]
> **Authors**: Chau-Jian Huang
> **First submission**: 2024-12-29
> **First announcement**: 2025-02-24
> **comment**: 7 pages
- **标题**: None
- **领域**: 信息检索,人工智能,计算语言学
- **Abstract**: Web crawling is a critical technique for extracting online data, yet it poses challenges due to webpage diversity and anti-scraping mechanisms. This study investigates the integration of generative AI tools Claude AI (Sonnet 3.5) and ChatGPT4.0 with prompt engineering to automate web scraping. Using two prompts, PROMPT I (general inference, tested on Yahoo News) and PROMPT II (element-specific, tested on Coupons.com), we evaluate the code quality and performance of AI-generated scripts. Claude AI consistently outperformed ChatGPT-4.0 in script quality and adaptability, as confirmed by predefined evaluation metrics, including functionality, readability, modularity, and robustness. Performance data were collected through manual testing and structured scoring by three evaluators. Visualizations further illustrate Claude AI's superiority. Anti-scraping solutions, including undetected_chromedriver, Selenium, and fake_useragent, were incorporated to enhance performance. This paper demonstrates how generative AI combined with prompt engineering can simplify and improve web scraping workflows.

### Level-Navi Agent: A Framework and benchmark for Chinese Web Search Agents 
[[arxiv](https://arxiv.org/abs/2502.15690)] [[cool](https://papers.cool/arxiv/2502.15690)] [[pdf](https://arxiv.org/pdf/2502.15690)]
> **Authors**: Chuanrui Hu,Shichong Xie,Baoxin Wang,Bin Chen,Xiaofeng Cong,Jun Zhang
> **First submission**: 2024-12-20
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,人工智能,计算语言学
- **Abstract**: Large language models (LLMs), adopted to understand human language, drive the development of artificial intelligence (AI) web search agents. Compared to traditional search engines, LLM-powered AI search agents are capable of understanding and responding to complex queries with greater depth, enabling more accurate operations and better context recognition. However, little attention and effort has been paid to the Chinese web search, which results in that the capabilities of open-source models have not been uniformly and fairly evaluated. The difficulty lies in lacking three aspects: an unified agent framework, an accurately labeled dataset, and a suitable evaluation metric. To address these issues, we propose a general-purpose and training-free web search agent by level-aware navigation, Level-Navi Agent, accompanied by a well-annotated dataset (Web24) and a suitable evaluation metric. Level-Navi Agent can think through complex user questions and conduct searches across various levels on the internet to gather information for questions. Meanwhile, we provide a comprehensive evaluation of state-of-the-art LLMs under fair settings. To further facilitate future research, source code is available at Github.

### XPath Agent: An Efficient XPath Programming Agent Based on LLM for Web Crawler 
[[arxiv](https://arxiv.org/abs/2502.15688)] [[cool](https://papers.cool/arxiv/2502.15688)] [[pdf](https://arxiv.org/pdf/2502.15688)]
> **Authors**: Yu Li,Bryce Wang,Xinyu Luan
> **First submission**: 2024-12-17
> **First announcement**: 2025-02-24
> **comment**: 10 pages, 2 figures
- **标题**: None
- **领域**: 信息检索,人工智能,软件工程
- **Abstract**: We present XPath Agent, a production-ready XPath programming agent specifically designed for web crawling and web GUI testing. A key feature of XPath Agent is its ability to automatically generate XPath queries from a set of sampled web pages using a single natural language query. To demonstrate its effectiveness, we benchmark XPath Agent against a state-of-the-art XPath programming agent across a range of web crawling tasks. Our results show that XPath Agent achieves comparable performance metrics while significantly reducing token usage and improving clock-time efficiency. The well-designed two-stage pipeline allows for seamless integration into existing web crawling or web GUI testing workflows, thereby saving time and effort in manual XPath query development. The source code for XPath Agent is available at https://github.com/eavae/feilian.

### Entire-Space Variational Information Exploitation for Post-Click Conversion Rate Prediction 
[[arxiv](https://arxiv.org/abs/2502.15687)] [[cool](https://papers.cool/arxiv/2502.15687)] [[pdf](https://arxiv.org/pdf/2502.15687)]
> **Authors**: Ke Fei,Xinyue Zhang,Jingjing Li
> **First submission**: 2024-12-17
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,机器学习
- **Abstract**: In recommender systems, post-click conversion rate (CVR) estimation is an essential task to model user preferences for items and estimate the value of recommendations. Sample selection bias (SSB) and data sparsity (DS) are two persistent challenges for post-click conversion rate (CVR) estimation. Currently, entire-space approaches that exploit unclicked samples through knowledge distillation are promising to mitigate SSB and DS simultaneously. Existing methods use non-conversion, conversion, or adaptive conversion predictors to generate pseudo labels for unclicked samples. However, they fail to consider the unbiasedness and information limitations of these pseudo labels. Motivated by such analysis, we propose an entire-space variational information exploitation framework (EVI) for CVR prediction. First, EVI uses a conditional entire-space CVR teacher to generate unbiased pseudo labels. Then, it applies variational information exploitation and logit distillation to transfer non-click space information to the target CVR estimator. We conduct extensive offline experiments on six large-scale datasets. EVI demonstrated a 2.25\% average improvement compared to the state-of-the-art baselines.

### Active Large Language Model-based Knowledge Distillation for Session-based Recommendation 
[[arxiv](https://arxiv.org/abs/2502.15685)] [[cool](https://papers.cool/arxiv/2502.15685)] [[pdf](https://arxiv.org/pdf/2502.15685)]
> **Authors**: Yingpeng Du,Zhu Sun,Ziyan Wang,Haoyan Chua,Jie Zhang,Yew-Soon Ong
> **First submission**: 2024-12-15
> **First announcement**: 2025-02-24
> **comment**: 14 pages, 4 figures
- **标题**: None
- **领域**: 信息检索,机器学习
- **Abstract**: Large language models (LLMs) provide a promising way for accurate session-based recommendation (SBR), but they demand substantial computational time and memory. Knowledge distillation (KD)-based methods can alleviate these issues by transferring the knowledge to a small student, which trains a student based on the predictions of a cumbersome teacher. However, these methods encounter difficulties for \textit{LLM-based KD in SBR}. 1) It is expensive to make LLMs predict for all instances in KD. 2) LLMs may make ineffective predictions for some instances in KD, e.g., incorrect predictions for hard instances or similar predictions as existing recommenders for easy instances. In this paper, we propose an active LLM-based KD method in SBR, contributing to sustainable AI. To efficiently distill knowledge from LLMs with limited cost, we propose to extract a small proportion of instances predicted by LLMs. Meanwhile, for a more effective distillation, we propose an active learning strategy to extract instances that are as effective as possible for KD from a theoretical view. Specifically, we first formulate gains based on potential effects (e.g., effective, similar, and incorrect predictions by LLMs) and difficulties (e.g., easy or hard to fit) of instances for KD. Then, we propose to maximize the minimal gains of distillation to find the optimal selection policy for active learning, which can largely avoid extracting ineffective instances in KD. Experiments on real-world datasets show that our method significantly outperforms state-of-the-art methods for SBR.

### An Agent Framework for Real-Time Financial Information Searching with Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.15684)] [[cool](https://papers.cool/arxiv/2502.15684)] [[pdf](https://arxiv.org/pdf/2502.15684)]
> **Authors**: Jinzheng Li,Jingshu Zhang,Hongguang Li,Yiqing Shen
> **First submission**: 2024-12-14
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,人工智能
- **Abstract**: Financial decision-making requires processing vast amounts of real-time information while understanding their complex temporal relationships. While traditional search engines excel at providing real-time information access, they often struggle to comprehend sophisticated user intentions and contextual nuances. Conversely, Large Language Models (LLMs) demonstrate reasoning and interaction capabilities but may generate unreliable outputs without access to current data. While recent attempts have been made to combine LLMs with search capabilities, they suffer from (1) restricted access to specialized financial data, (2) static query structures that cannot adapt to dynamic market conditions, and (3) insufficient temporal awareness in result generation. To address these challenges, we present FinSearch, a novel agent-based search framework specifically designed for financial applications that interface with diverse financial data sources including market, stock, and news data. Innovatively, FinSearch comprises four components: (1) an LLM-based multi-step search pre-planner that decomposes user queries into structured sub-queries mapped to specific data sources through a graph representation; (2) a search executor with an LLM-based adaptive query rewriter that executes the searching of each sub-query while dynamically refining the sub-queries in its subsequent node based on intermediate search results; (3) a temporal weighting mechanism that prioritizes information relevance based on the deduced time context from the user's query; (4) an LLM-based response generator that synthesizes results into coherent, contextually appropriate outputs. To evaluate FinSearch, we construct FinSearchBench-24, a benchmark of 1,500 four-choice questions across the stock market, rate changes, monetary policy, and industry developments spanning from June to October 2024.

### Bridging Domain Gaps between Pretrained Multimodal Models and Recommendations 
[[arxiv](https://arxiv.org/abs/2502.15542)] [[cool](https://papers.cool/arxiv/2502.15542)] [[pdf](https://arxiv.org/pdf/2502.15542)]
> **Authors**: Wenyu Zhang,Jie Luo,Xinming Zhang,Yuan Fang
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,人工智能
- **Abstract**: With the explosive growth of multimodal content online, pre-trained visual-language models have shown great potential for multimodal recommendation. However, while these models achieve decent performance when applied in a frozen manner, surprisingly, due to significant domain gaps (e.g., feature distribution discrepancy and task objective misalignment) between pre-training and personalized recommendation, adopting a joint training approach instead leads to performance worse than baseline. Existing approaches either rely on simple feature extraction or require computationally expensive full model fine-tuning, struggling to balance effectiveness and efficiency. To tackle these challenges, we propose \textbf{P}arameter-efficient \textbf{T}uning for \textbf{M}ultimodal \textbf{Rec}ommendation (\textbf{PTMRec}), a novel framework that bridges the domain gap between pre-trained models and recommendation systems through a knowledge-guided dual-stage parameter-efficient training strategy. This framework not only eliminates the need for costly additional pre-training but also flexibly accommodates various parameter-efficient tuning methods.

### Lightweight yet Efficient: An External Attentive Graph Convolutional Network with Positional Prompts for Sequential Recommendation 
[[arxiv](https://arxiv.org/abs/2502.15331)] [[cool](https://papers.cool/arxiv/2502.15331)] [[pdf](https://arxiv.org/pdf/2502.15331)]
> **Authors**: Jinyu Zhang,Chao Li,Zhongying Zhao
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 26 pages, 8 figures, journal paper, accepted by TOIS at 20th February, 2025
- **标题**: None
- **领域**: 信息检索,人工智能,机器学习
- **Abstract**: Graph-based Sequential Recommender systems (GSRs) have gained significant research attention due to their ability to simultaneously handle user-item interactions and sequential relationships between items. Current GSRs often utilize composite or in-depth structures for graph encoding (e.g., the Graph Transformer). Nevertheless, they have high computational complexity, hindering the deployment on resource-constrained edge devices. Moreover, the relative position encoding in Graph Transformer has difficulty in considering the complicated positional dependencies within sequence. To this end, we propose an External Attentive Graph convolutional network with Positional prompts for Sequential recommendation, namely EA-GPS. Specifically, we first introduce an external attentive graph convolutional network that linearly measures the global associations among nodes via two external memory units. Then, we present a positional prompt-based decoder that explicitly treats the absolute item positions as external prompts. By introducing length-adaptive sequential masking and a soft attention network, such a decoder facilitates the model to capture the long-term positional dependencies and contextual relationships within sequences. Extensive experimental results on five real-world datasets demonstrate that the proposed EA-GPS outperforms the state-of-the-art methods. Remarkably, it achieves the superior performance while maintaining a smaller parameter size and lower training overhead. The implementation of this work is publicly available at https://github.com/ZZY-GraphMiningLab/EA-GPS.

### A BERT Based Hybrid Recommendation System For Academic Collaboration 
[[arxiv](https://arxiv.org/abs/2502.15223)] [[cool](https://papers.cool/arxiv/2502.15223)] [[pdf](https://arxiv.org/pdf/2502.15223)]
> **Authors**: Sangeetha N,Harish Thangaraj,Varun Vashisht,Eshaan Joshi,Kanishka Verma,Diya Katariya
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: International Conference on Intelligent Systems and Security - 2024
- **标题**: None
- **领域**: 信息检索,计算语言学,社交和信息网络
- **Abstract**: Universities serve as a hub for academic collaboration, promoting the exchange of diverse ideas and perspectives among students and faculty through interdisciplinary dialogue. However, as universities expand in size, conventional networking approaches via student chapters, class groups, and faculty committees become cumbersome. To address this challenge, an academia-specific profile recommendation system is proposed to connect like-minded stakeholders within any university community. This study evaluates three techniques: Term Frequency-Inverse Document Frequency (TF-IDF), Bidirectional Encoder Representations from Transformers (BERT), and a hybrid approach to generate effective recommendations. Due to the unlabelled nature of the dataset, Affinity Propagation cluster-based relabelling is performed to understand the grouping of similar profiles. The hybrid model demonstrated superior performance, evidenced by its similarity score, Silhouette score, Davies-Bouldin index, and Normalized Discounted Cumulative Gain (NDCG), achieving an optimal balance between diversity and relevance in recommendations. Furthermore, the optimal model has been implemented as a mobile application, which dynamically suggests relevant profiles based on users' skills and collaboration interests, incorporating contextual understanding. The potential impact of this application is significant, as it promises to enhance networking opportunities within large academic institutions through the deployment of intelligent recommendation systems.

## 信息论(cs.IT:Information Theory)

### Asymptotic evaluation of the information processing capacity in reservoir computing 
[[arxiv](https://arxiv.org/abs/2502.15769)] [[cool](https://papers.cool/arxiv/2502.15769)] [[pdf](https://arxiv.org/pdf/2502.15769)]
> **Authors**: Yohei Saito
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 信息论,机器学习,信号处理
- **Abstract**: The squared error normalized by the target output is known as the information processing capacity (IPC) and is used to evaluate the performance of reservoir computing (RC). Since RC aims to learn the relationship between input and output time series, we should evaluate the IPC for infinitely long data rather than the IPC for finite-length data. To evaluate the IPC for infinitely long data using the IPC for finite-length data, we use an asymptotic expansion of the IPC and the least-squares method. Then, we show the validity of our method by numerical simulations.

### Aligning Task- and Reconstruction-Oriented Communications for Edge Intelligence 
[[arxiv](https://arxiv.org/abs/2502.15472)] [[cool](https://papers.cool/arxiv/2502.15472)] [[pdf](https://arxiv.org/pdf/2502.15472)]
> **Authors**: Yufeng Diao,Yichi Zhang,Changyang She,Philip Guodong Zhao,Emma Liying Li
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Accepted for publication in IEEE Journal on Selected Areas in Communications (JSAC)
- **标题**: None
- **领域**: 信息论,计算机视觉和模式识别,图像和视频处理
- **Abstract**: Existing communication systems aim to reconstruct the information at the receiver side, and are known as reconstruction-oriented communications. This approach often falls short in meeting the real-time, task-specific demands of modern AI-driven applications such as autonomous driving and semantic segmentation. As a new design principle, task-oriented communications have been developed. However, it typically requires joint optimization of encoder, decoder, and modified inference neural networks, resulting in extensive cross-system redesigns and compatibility issues. This paper proposes a novel communication framework that aligns reconstruction-oriented and task-oriented communications for edge intelligence. The idea is to extend the Information Bottleneck (IB) theory to optimize data transmission by minimizing task-relevant loss function, while maintaining the structure of the original data by an information reshaper. Such an approach integrates task-oriented communications with reconstruction-oriented communications, where a variational approach is designed to handle the intractability of mutual information in high-dimensional neural network features. We also introduce a joint source-channel coding (JSCC) modulation scheme compatible with classical modulation techniques, enabling the deployment of AI technologies within existing digital infrastructures. The proposed framework is particularly effective in edge-based autonomous driving scenarios. Our evaluation in the Car Learning to Act (CARLA) simulator demonstrates that the proposed framework significantly reduces bits per service by 99.19% compared to existing methods, such as JPEG, JPEG2000, and BPG, without compromising the effectiveness of task execution.

## 机器学习(cs.LG:Machine Learning)

### In-context learning of evolving data streams with tabular foundational models 
[[arxiv](https://arxiv.org/abs/2502.16840)] [[cool](https://papers.cool/arxiv/2502.16840)] [[pdf](https://arxiv.org/pdf/2502.16840)]
> **Authors**: Afonso Lourenço,João Gama,Eric P. Xing,Goreti Marreiros
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: State-of-the-art data stream mining in supervised classification has traditionally relied on ensembles of incremental decision trees. However, the emergence of large tabular models, i.e., transformers designed for structured numerical data, marks a significant paradigm shift. These models move beyond traditional weight updates, instead employing in-context learning through prompt tuning. By using on-the-fly sketches to summarize unbounded streaming data, one can feed this information into a pre-trained model for efficient processing. This work bridges advancements from both areas, highlighting how transformers' implicit meta-learning abilities, pre-training on drifting natural data, and reliance on context optimization directly address the core challenges of adaptive learning in dynamic environments. Exploring real-time model adaptation, this research demonstrates that TabPFN, coupled with a simple sliding memory strategy, consistently outperforms ensembles of Hoeffding trees across all non-stationary benchmarks. Several promising research directions are outlined in the paper. The authors urge the community to explore these ideas, offering valuable opportunities to advance in-context stream learning.

### A Novel Multi-Task Teacher-Student Architecture with Self-Supervised Pretraining for 48-Hour Vasoactive-Inotropic Trend Analysis in Sepsis Mortality Prediction 
[[arxiv](https://arxiv.org/abs/2502.16834)] [[cool](https://papers.cool/arxiv/2502.16834)] [[pdf](https://arxiv.org/pdf/2502.16834)]
> **Authors**: Houji Jin,Negin Ashrafi,Kamiar Alaei,Elham Pishgar,Greg Placencia,Maryam Pishgar
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Sepsis is a major cause of ICU mortality, where early recognition and effective interventions are essential for improving patient outcomes. However, the vasoactive-inotropic score (VIS) varies dynamically with a patient's hemodynamic status, complicated by irregular medication patterns, missing data, and confounders, making sepsis prediction challenging. To address this, we propose a novel Teacher-Student multitask framework with self-supervised VIS pretraining via a Masked Autoencoder (MAE). The teacher model performs mortality classification and severity-score regression, while the student distills robust time-series representations, enhancing adaptation to heterogeneous VIS data. Compared to LSTM-based methods, our approach achieves an AUROC of 0.82 on MIMIC-IV 3.0 (9,476 patients), outperforming the baseline (0.74). SHAP analysis revealed that SOFA score (0.147) had the greatest impact on ICU mortality, followed by LODS (0.033), single marital status (0.031), and Medicaid insurance (0.023), highlighting the role of sociodemographic factors. SAPSII (0.020) also contributed significantly. These findings suggest that both clinical and social factors should be considered in ICU decision-making. Our novel multitask and distillation strategies enable earlier identification of high-risk patients, improving prediction accuracy and disease management, offering new tools for ICU decision support.

### Posterior Inference with Diffusion Models for High-dimensional Black-box Optimization 
[[arxiv](https://arxiv.org/abs/2502.16824)] [[cool](https://papers.cool/arxiv/2502.16824)] [[pdf](https://arxiv.org/pdf/2502.16824)]
> **Authors**: Taeyoung Yun,Kiyoung Om,Jaewoo Lee,Sujin Yun,Jinkyoo Park
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: 21 pages, 12 figures, 5 tables
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Optimizing high-dimensional and complex black-box functions is crucial in numerous scientific applications. While Bayesian optimization (BO) is a powerful method for sample-efficient optimization, it struggles with the curse of dimensionality and scaling to thousands of evaluations. Recently, leveraging generative models to solve black-box optimization problems has emerged as a promising framework. However, those methods often underperform compared to BO methods due to limited expressivity and difficulty of uncertainty estimation in high-dimensional spaces. To overcome these issues, we introduce \textbf{DiBO}, a novel framework for solving high-dimensional black-box optimization problems. Our method iterates two stages. First, we train a diffusion model to capture the data distribution and an ensemble of proxies to predict function values with uncertainty quantification. Second, we cast the candidate selection as a posterior inference problem to balance exploration and exploitation in high-dimensional spaces. Concretely, we fine-tune diffusion models to amortize posterior inference. Extensive experiments demonstrate that our method outperforms state-of-the-art baselines across various synthetic and real-world black-box optimization tasks. Our code is publicly available \href{https://github.com/umkiyoung/DiBO}{here}

### Fast, Accurate Manifold Denoising by Tunneling Riemannian Optimization 
[[arxiv](https://arxiv.org/abs/2502.16819)] [[cool](https://papers.cool/arxiv/2502.16819)] [[pdf](https://arxiv.org/pdf/2502.16819)]
> **Authors**: Shiyu Wang,Mariam Avagyan,Yihan Shen,Arnaud Lamy,Tingran Wang,Szabolcs Márka,Zsuzsa Márka,John Wright
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,优化与控制
- **Abstract**: Learned denoisers play a fundamental role in various signal generation (e.g., diffusion models) and reconstruction (e.g., compressed sensing) architectures, whose success derives from their ability to leverage low-dimensional structure in data. Existing denoising methods, however, either rely on local approximations that require a linear scan of the entire dataset or treat denoising as generic function approximation problems, often sacrificing efficiency and interpretability. We consider the problem of efficiently denoising a new noisy data point sampled from an unknown $d$-dimensional manifold $M \in \mathbb{R}^D$, using only noisy samples. This work proposes a framework for test-time efficient manifold denoising, by framing the concept of "learning-to-denoise" as "learning-to-optimize". We have two technical innovations: (i) online learning methods which learn to optimize over the manifold of clean signals using only noisy data, effectively "growing" an optimizer one sample at a time. (ii) mixed-order methods which guarantee that the learned optimizers achieve global optimality, ensuring both efficiency and near-optimal denoising performance. We corroborate these claims with theoretical analyses of both the complexity and denoising performance of mixed-order traversal. Our experiments on scientific manifolds demonstrate significantly improved complexity-performance tradeoffs compared to nearest neighbor search, which underpins existing provable denoising approaches based on exhaustive search.

### Forecasting Rare Language Model Behaviors 
[[arxiv](https://arxiv.org/abs/2502.16797)] [[cool](https://papers.cool/arxiv/2502.16797)] [[pdf](https://arxiv.org/pdf/2502.16797)]
> **Authors**: Erik Jones,Meg Tong,Jesse Mu,Mohammed Mahfoud,Jan Leike,Roger Grosse,Jared Kaplan,William Fithian,Ethan Perez,Mrinank Sharma
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Standard language model evaluations can fail to capture risks that emerge only at deployment scale. For example, a model may produce safe responses during a small-scale beta test, yet reveal dangerous information when processing billions of requests at deployment. To remedy this, we introduce a method to forecast potential risks across orders of magnitude more queries than we test during evaluation. We make forecasts by studying each query's elicitation probability -- the probability the query produces a target behavior -- and demonstrate that the largest observed elicitation probabilities predictably scale with the number of queries. We find that our forecasts can predict the emergence of diverse undesirable behaviors -- such as assisting users with dangerous chemical synthesis or taking power-seeking actions -- across up to three orders of magnitude of query volume. Our work enables model developers to proactively anticipate and patch rare failures before they manifest during large-scale deployments.

### VGFL-SA: Vertical Graph Federated Learning Structure Attack Based on Contrastive Learning 
[[arxiv](https://arxiv.org/abs/2502.16793)] [[cool](https://papers.cool/arxiv/2502.16793)] [[pdf](https://arxiv.org/pdf/2502.16793)]
> **Authors**: Yang Chen,Bin Zhou
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Graph Neural Networks (GNNs) have gained attention for their ability to learn representations from graph data. Due to privacy concerns and conflicts of interest that prevent clients from directly sharing graph data with one another, Vertical Graph Federated Learning (VGFL) frameworks have been developed. Recent studies have shown that VGFL is vulnerable to adversarial attacks that degrade performance. However, it is a common problem that client nodes are often unlabeled in the realm of VGFL. Consequently, the existing attacks, which rely on the availability of labeling information to obtain gradients, are inherently constrained in their applicability. This limitation precludes their deployment in practical, real-world environments. To address the above problems, we propose a novel graph adversarial attack against VGFL, referred to as VGFL-SA, to degrade the performance of VGFL by modifying the local clients structure without using labels. Specifically, VGFL-SA uses a contrastive learning method to complete the attack before the local clients are trained. VGFL-SA first accesses the graph structure and node feature information of the poisoned clients, and generates the contrastive views by node-degree-based edge augmentation and feature shuffling augmentation. Then, VGFL-SA uses the shared graph encoder to get the embedding of each view, and the gradients of the adjacency matrices are obtained by the contrastive function. Finally, perturbed edges are generated using gradient modification rules. We validated the performance of VGFL-SA by performing a node classification task on real-world datasets, and the results show that VGFL-SA achieves good attack effectiveness and transferability.

### The Role of Sparsity for Length Generalization in Transformers 
[[arxiv](https://arxiv.org/abs/2502.16792)] [[cool](https://papers.cool/arxiv/2502.16792)] [[pdf](https://arxiv.org/pdf/2502.16792)]
> **Authors**: Noah Golowich,Samy Jelassi,David Brandfonbrener,Sham M. Kakade,Eran Malach
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Training large language models to predict beyond their training context lengths has drawn much attention in recent years, yet the principles driving such behavior of length generalization remain underexplored. We propose a new theoretical framework to study length generalization for the next-token prediction task, as performed by decoder-only transformers. Conceptually, we show that length generalization occurs as long as each predicted token depends on a small (fixed) number of previous tokens. We formalize such tasks via a notion we call $k$-sparse planted correlation distributions, and show that an idealized model of transformers which generalize attention heads successfully length-generalize on such tasks. As a bonus, our theoretical model justifies certain techniques to modify positional embeddings which have been introduced to improve length generalization, such as position coupling. We support our theoretical results with experiments on synthetic tasks and natural language, which confirm that a key factor driving length generalization is a ``sparse'' dependency structure of each token on the previous ones. Inspired by our theory, we introduce Predictive Position Coupling, which trains the transformer to predict the position IDs used in a positional coupling approach. Predictive Position Coupling thereby allows us to broaden the array of tasks to which position coupling can successfully be applied to achieve length generalization.

### CipherPrune: Efficient and Scalable Private Transformer Inference 
[[arxiv](https://arxiv.org/abs/2502.16782)] [[cool](https://papers.cool/arxiv/2502.16782)] [[pdf](https://arxiv.org/pdf/2502.16782)]
> **Authors**: Yancheng Zhang,Jiaqi Xue,Mengxin Zheng,Mimi Xie,Mingzhe Zhang,Lei Jiang,Qian Lou
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: Accepted by ICLR 2025
- **标题**: None
- **领域**: 机器学习,密码学和安全
- **Abstract**: Private Transformer inference using cryptographic protocols offers promising solutions for privacy-preserving machine learning; however, it still faces significant runtime overhead (efficiency issues) and challenges in handling long-token inputs (scalability issues). We observe that the Transformer's operational complexity scales quadratically with the number of input tokens, making it essential to reduce the input token length. Notably, each token varies in importance, and many inputs contain redundant tokens. Additionally, prior private inference methods that rely on high-degree polynomial approximations for non-linear activations are computationally expensive. Therefore, reducing the polynomial degree for less important tokens can significantly accelerate private inference. Building on these observations, we propose \textit{CipherPrune}, an efficient and scalable private inference framework that includes a secure encrypted token pruning protocol, a polynomial reduction protocol, and corresponding Transformer network optimizations. At the protocol level, encrypted token pruning adaptively removes unimportant tokens from encrypted inputs in a progressive, layer-wise manner. Additionally, encrypted polynomial reduction assigns lower-degree polynomials to less important tokens after pruning, enhancing efficiency without decryption. At the network level, we introduce protocol-aware network optimization via a gradient-based search to maximize pruning thresholds and polynomial reduction conditions while maintaining the desired accuracy. Our experiments demonstrate that CipherPrune reduces the execution overhead of private Transformer inference by approximately $6.1\times$ for 128-token inputs and $10.6\times$ for 512-token inputs, compared to previous methods, with only a marginal drop in accuracy. The code is publicly available at https://github.com/UCF-Lou-Lab-PET/cipher-prune-inference.

### The Robustness of Structural Features in Species Interaction Networks 
[[arxiv](https://arxiv.org/abs/2502.16778)] [[cool](https://papers.cool/arxiv/2502.16778)] [[pdf](https://arxiv.org/pdf/2502.16778)]
> **Authors**: Sanaz Hasanzadeh Fard,Emily Dolson
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,社交和信息网络
- **Abstract**: Species interaction networks are a powerful tool for describing ecological communities; they typically contain nodes representing species, and edges representing interactions between those species. For the purposes of drawing abstract inferences about groups of similar networks, ecologists often use graph topology metrics to summarize structural features. However, gathering the data that underlies these networks is challenging, which can lead to some interactions being missed. Thus, it is important to understand how much different structural metrics are affected by missing data. To address this question, we analyzed a database of 148 real-world bipartite networks representing four different types of species interactions (pollination, host-parasite, plant-ant, and seed-dispersal). For each network, we measured six different topological properties: number of connected components, variance in node betweenness, variance in node PageRank, largest Eigenvalue, the number of non-zero Eigenvalues, and community detection as determined by four different algorithms. We then tested how these properties change as additional edges -- representing data that may have been missed -- are added to the networks. We found substantial variation in how robust different properties were to the missing data. For example, the Clauset-Newman-Moore and Louvain community detection algorithms showed much more gradual change as edges were added than the label propagation and Girvan-Newman algorithms did, suggesting that the former are more robust. Robustness also varied for some metrics based on interaction type. These results provide a foundation for selecting network properties to use when analyzing messy ecological network data.

### Model-Based Exploration in Monitored Markov Decision Processes 
[[arxiv](https://arxiv.org/abs/2502.16772)] [[cool](https://papers.cool/arxiv/2502.16772)] [[pdf](https://arxiv.org/pdf/2502.16772)]
> **Authors**: Alireza Kazemipour,Simone Parisi,Matthew E. Taylor,Michael Bowling
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: A tenet of reinforcement learning is that rewards are always observed by the agent. However, this is not true in many realistic settings, e.g., a human observer may not always be able to provide rewards, a sensor to observe rewards may be limited or broken, or rewards may be unavailable during deployment. Monitored Markov decision processes (Mon-MDPs) have recently been proposed as a model of such settings. Yet, Mon-MDP algorithms developed thus far do not fully exploit the problem structure, cannot take advantage of a known monitor, have no worst-case guarantees for ``unsolvable'' Mon-MDPs without specific initialization, and only have asymptotic proofs of convergence. This paper makes three contributions. First, we introduce a model-based algorithm for Mon-MDPs that addresses all of these shortcomings. The algorithm uses two instances of model-based interval estimation, one to guarantee that observable rewards are indeed observed, and another to learn the optimal policy. Second, empirical results demonstrate these advantages, showing faster convergence than prior algorithms in over two dozen benchmark settings, and even more dramatic improvements when the monitor process is known. Third, we present the first finite-sample bound on performance and show convergence to an optimal worst-case policy when some rewards are never observable.

### Exact Learning of Permutations for Nonzero Binary Inputs with Logarithmic Training Size and Quadratic Ensemble Complexity 
[[arxiv](https://arxiv.org/abs/2502.16763)] [[cool](https://papers.cool/arxiv/2502.16763)] [[pdf](https://arxiv.org/pdf/2502.16763)]
> **Authors**: George Giapitzakis,Artur Back de Luca,Kimon Fountoulakis
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: 21 pages, 1 figure
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The ability of an architecture to realize permutations is quite fundamental. For example, Large Language Models need to be able to correctly copy (and perhaps rearrange) parts of the input prompt into the output. Classical universal approximation theorems guarantee the existence of parameter configurations that solve this task but offer no insights into whether gradient-based algorithms can find them. In this paper, we address this gap by focusing on two-layer fully connected feed-forward neural networks and the task of learning permutations on nonzero binary inputs. We show that in the infinite width Neural Tangent Kernel (NTK) regime, an ensemble of such networks independently trained with gradient descent on only the $k$ standard basis vectors out of $2^k - 1$ possible inputs successfully learns any fixed permutation of length $k$ with arbitrarily high probability. By analyzing the exact training dynamics, we prove that the network's output converges to a Gaussian process whose mean captures the ground truth permutation via sign-based features. We then demonstrate how averaging these runs (an "ensemble" method) and applying a simple rounding step yields an arbitrarily accurate prediction on any possible input unseen during training. Notably, the number of models needed to achieve exact learning with high probability (which we refer to as ensemble complexity) exhibits a linearithmic dependence on the input size $k$ for a single test input and a quadratic dependence when considering all test inputs simultaneously.

### Order-Optimal Projection-Free Algorithm for Adversarially Constrained Online Convex Optimization 
[[arxiv](https://arxiv.org/abs/2502.16744)] [[cool](https://papers.cool/arxiv/2502.16744)] [[pdf](https://arxiv.org/pdf/2502.16744)]
> **Authors**: Yiyang Lu,Mohammad Pedramfar,Vaneet Aggarwal
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,优化与控制
- **Abstract**: Projection-based algorithms for constrained Online Convex Optimization (COCO) face scalability challenges in high-dimensional settings due to the computational complexity of projecting iterates onto constraint sets. This paper introduces a projection-free algorithm for COCO that achieves state-of-the-art performance guarantees while eliminating the need for projections. By integrating a separation oracle with adaptive Online Gradient Descent (OGD) and employing a Lyapunov-driven surrogate function, while dynamically adjusting step sizes using gradient norms, our method jointly optimizes the regret and cumulative constraint violation (CCV). We also use a blocked version of OGD that helps achieve tradeoffs betweeen the regret and CCV with the number of calls to the separation oracle. For convex cost functions, our algorithm attains an optimal regret of $\mathcal{O}(\sqrt{T})$ and a CCV of $\mathcal{O}(\sqrt{T} \log T)$, matching the best-known projection-based results, while only using $\tilde{\mathcal{O}}({T})$ calls to the separation oracle. The results also demonstrate a tradeoff where lower calls to the separation oracle increase the regret and the CCV. In the strongly convex setting, we further achieve a regret of $\mathcal{O}(\log T)$ and a CCV of $\mathcal{O}(\sqrt{T\log T} )$, while requiring ${\mathcal{O}}({T}^2)$ calls to the separation oracle. Further, tradeoff with the decreasing oracle calls is studied. These results close the gap between projection-free and projection-based approaches, demonstrating that projection-free methods can achieve performance comparable to projection-based counterparts.

### Keeping up with dynamic attackers: Certifying robustness to adaptive online data poisoning 
[[arxiv](https://arxiv.org/abs/2502.16737)] [[cool](https://papers.cool/arxiv/2502.16737)] [[pdf](https://arxiv.org/pdf/2502.16737)]
> **Authors**: Avinandan Bose,Laurent Lessard,Maryam Fazel,Krishnamurthy Dj Dvijotham
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: Proceedings of the 28th International Conference on Artificial Intelligence and Statistics (AISTATS) 2025, Mai Khao, Thailand. PMLR: Volume 258
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The rise of foundation models fine-tuned on human feedback from potentially untrusted users has increased the risk of adversarial data poisoning, necessitating the study of robustness of learning algorithms against such attacks. Existing research on provable certified robustness against data poisoning attacks primarily focuses on certifying robustness for static adversaries who modify a fraction of the dataset used to train the model before the training algorithm is applied. In practice, particularly when learning from human feedback in an online sense, adversaries can observe and react to the learning process and inject poisoned samples that optimize adversarial objectives better than when they are restricted to poisoning a static dataset once, before the learning algorithm is applied. Indeed, it has been shown in prior work that online dynamic adversaries can be significantly more powerful than static ones. We present a novel framework for computing certified bounds on the impact of dynamic poisoning, and use these certificates to design robust learning algorithms. We give an illustration of the framework for the mean estimation and binary classification problems and outline directions for extending this in further work. The code to implement our certificates and replicate our results is available at https://github.com/Avinandan22/Certified-Robustness.

### AUKT: Adaptive Uncertainty-Guided Knowledge Transfer with Conformal Prediction 
[[arxiv](https://arxiv.org/abs/2502.16736)] [[cool](https://papers.cool/arxiv/2502.16736)] [[pdf](https://arxiv.org/pdf/2502.16736)]
> **Authors**: Rui Liu,Peng Gao,Yu Shen,Ming Lin,Pratap Tokekar
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Knowledge transfer between teacher and student models has proven effective across various machine learning applications. However, challenges arise when the teacher's predictions are noisy, or the data domain during student training shifts from the teacher's pretraining data. In such scenarios, blindly relying on the teacher's predictions can lead to suboptimal knowledge transfer. To address these challenges, we propose a novel and universal framework, Adaptive Uncertainty-guided Knowledge Transfer ($\textbf{AUKT}$), which leverages Conformal Prediction (CP) to dynamically adjust the student's reliance on the teacher's guidance based on the teacher's prediction uncertainty. CP is a distribution-free, model-agnostic approach that provides reliable prediction sets with statistical coverage guarantees and minimal computational overhead. This adaptive mechanism mitigates the risk of learning undesirable or incorrect knowledge. We validate the proposed framework across diverse applications, including image classification, imitation-guided reinforcement learning, and autonomous driving. Experimental results consistently demonstrate that our approach improves performance, robustness and transferability, offering a promising direction for enhanced knowledge transfer in real-world applications.

### Towards Optimal Adversarial Robust Reinforcement Learning with Infinity Measurement Error 
[[arxiv](https://arxiv.org/abs/2502.16734)] [[cool](https://papers.cool/arxiv/2502.16734)] [[pdf](https://arxiv.org/pdf/2502.16734)]
> **Authors**: Haoran Li,Zicheng Zhang,Wang Luo,Congying Han,Jiayu Lv,Tiande Guo,Yudong Hu
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: arXiv admin note: substantial text overlap with arXiv:2402.02165
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Ensuring the robustness of deep reinforcement learning (DRL) agents against adversarial attacks is critical for their trustworthy deployment. Recent research highlights the challenges of achieving state-adversarial robustness and suggests that an optimal robust policy (ORP) does not always exist, complicating the enforcement of strict robustness constraints. In this paper, we further explore the concept of ORP. We first introduce the Intrinsic State-adversarial Markov Decision Process (ISA-MDP), a novel formulation where adversaries cannot fundamentally alter the intrinsic nature of state observations. ISA-MDP, supported by empirical and theoretical evidence, universally characterizes decision-making under state-adversarial paradigms. We rigorously prove that within ISA-MDP, a deterministic and stationary ORP exists, aligning with the Bellman optimal policy. Our findings theoretically reveal that improving DRL robustness does not necessarily compromise performance in natural environments. Furthermore, we demonstrate the necessity of infinity measurement error (IME) in both $Q$-function and probability spaces to achieve ORP, unveiling vulnerabilities of previous DRL algorithms that rely on $1$-measurement errors. Motivated by these insights, we develop the Consistent Adversarial Robust Reinforcement Learning (CAR-RL) framework, which optimizes surrogates of IME. We apply CAR-RL to both value-based and policy-based DRL algorithms, achieving superior performance and validating our theoretical analysis.

### Model-agnostic Coreset Selection via LLM-based Concept Bottlenecks 
[[arxiv](https://arxiv.org/abs/2502.16733)] [[cool](https://papers.cool/arxiv/2502.16733)] [[pdf](https://arxiv.org/pdf/2502.16733)]
> **Authors**: Akshay Mehra,Trisha Mittal,Subhadra Gopalakrishnan,Joshua Kimball
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Coreset Selection (CS) identifies a subset of training data that achieves model performance comparable to using the entire dataset. Many state-of-the-art CS methods, select coresets using scores whose computation requires training the downstream model on the entire dataset and recording changes in its behavior on samples as it trains (training dynamics). These scores are inefficient to compute and hard to interpret as they do not indicate whether a sample is difficult to learn in general or only for a specific model. Our work addresses these challenges by proposing an interpretable score that gauges a sample's difficulty using human-understandable textual attributes (concepts) independent of any downstream model. Specifically, we measure the alignment between a sample's visual features and concept bottlenecks, derived via large language models, by training a linear concept bottleneck layer and compute the sample's difficulty score using it. We then use this score and a stratified sampling strategy to identify the coreset. Crucially, our score is efficiently computable without training the downstream model on the full dataset even once, leads to high-performing coresets for various downstream models, and is computable even for an unlabeled dataset. Through experiments on CIFAR-10, CIFAR-100, and ImageNet-1K, we show our coresets outperform random subsets, even at high pruning rates, and achieve model performance comparable to or better than coresets found by training dynamics-based methods.

### DOSE3 : Diffusion-based Out-of-distribution detection on SE(3) trajectories 
[[arxiv](https://arxiv.org/abs/2502.16725)] [[cool](https://papers.cool/arxiv/2502.16725)] [[pdf](https://arxiv.org/pdf/2502.16725)]
> **Authors**: Hongzhe Cheng,Tianyou Zheng,Tianyi Zhang,Matthew Johnson-Roberson,Weiming Zhi
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: Out-of-Distribution(OOD) detection, a fundamental machine learning task aimed at identifying abnormal samples, traditionally requires model retraining for different inlier distributions. While recent research demonstrates the applicability of diffusion models to OOD detection, existing approaches are limited to Euclidean or latent image spaces. Our work extends OOD detection to trajectories in the Special Euclidean Group in 3D ($\mathbb{SE}(3)$), addressing a critical need in computer vision, robotics, and engineering applications that process object pose sequences in $\mathbb{SE}(3)$. We present $\textbf{D}$iffusion-based $\textbf{O}$ut-of-distribution detection on $\mathbb{SE}(3)$ ($\mathbf{DOSE3}$), a novel OOD framework that extends diffusion to a unified sample space of $\mathbb{SE}(3)$ pose sequences. Through extensive validation on multiple benchmark datasets, we demonstrate $\mathbf{DOSE3}$'s superior performance compared to state-of-the-art OOD detection frameworks.

### To Share or Not to Share: Investigating Weight Sharing in Variational Graph Autoencoders 
[[arxiv](https://arxiv.org/abs/2502.16724)] [[cool](https://papers.cool/arxiv/2502.16724)] [[pdf](https://arxiv.org/pdf/2502.16724)]
> **Authors**: Guillaume Salha-Galvan,Jiaying Xu
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: 2025 ACM Web Conference (WWW 2025)
- **标题**: None
- **领域**: 机器学习,社交和信息网络
- **Abstract**: This paper investigates the understudied practice of weight sharing (WS) in variational graph autoencoders (VGAE). WS presents both benefits and drawbacks for VGAE model design and node embedding learning, leaving its overall relevance unclear and the question of whether it should be adopted unresolved. We rigorously analyze its implications and, through extensive experiments on a wide range of graphs and VGAE variants, demonstrate that the benefits of WS consistently outweigh its drawbacks. Based on our findings, we recommend WS as an effective approach to optimize, regularize, and simplify VGAE models without significant performance loss.

### Exploring Incremental Unlearning: Techniques, Challenges, and Future Directions 
[[arxiv](https://arxiv.org/abs/2502.16708)] [[cool](https://papers.cool/arxiv/2502.16708)] [[pdf](https://arxiv.org/pdf/2502.16708)]
> **Authors**: Sadia Qureshi,Thanveer Shaik,Xiaohui Tao,Haoran Xie,Lin Li,Jianming Yong,Xiaohua Jia
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: This work has been submitted to the IEEE for possible publication
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: The growing demand for data privacy in Machine Learning (ML) applications has seen Machine Unlearning (MU) emerge as a critical area of research. As the `right to be forgotten' becomes regulated globally, it is increasingly important to develop mechanisms that delete user data from AI systems while maintaining performance and scalability of these systems. Incremental Unlearning (IU) is a promising MU solution to address the challenges of efficiently removing specific data from ML models without the need for expensive and time-consuming full retraining. This paper presents the various techniques and approaches to IU. It explores the challenges faced in designing and implementing IU mechanisms. Datasets and metrics for evaluating the performance of unlearning techniques are discussed as well. Finally, potential solutions to the IU challenges alongside future research directions are offered. This survey provides valuable insights for researchers and practitioners seeking to understand the current landscape of IU and its potential for enhancing privacy-preserving intelligent systems.

### DISC: Dynamic Decomposition Improves LLM Inference Scaling 
[[arxiv](https://arxiv.org/abs/2502.16706)] [[cool](https://papers.cool/arxiv/2502.16706)] [[pdf](https://arxiv.org/pdf/2502.16706)]
> **Authors**: Jonathan Light,Wei Cheng,Wu Yue,Masafumi Oyamada,Mengdi Wang,Santiago Paternain,Haifeng Chen
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: :I.2.6; I.2.7; I.2.8; D.2.3; F.2.2
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,软件工程
- **Abstract**: Many inference scaling methods work by breaking a problem into smaller steps (or groups of tokens), then sampling and choosing the best next step. However, these steps and their sizes are usually predetermined based on human intuition or domain knowledge. This paper introduces dynamic decomposition, a method that automatically and adaptively splits solution and reasoning traces into steps during inference. This approach improves computational efficiency by focusing more resources on difficult steps, breaking them down further and prioritizing their sampling. Experiments on coding and math benchmarks (APPS, MATH, and LiveCodeBench) show that dynamic decomposition performs better than static methods, which rely on fixed steps like token-level, sentence-level, or single-step decompositions. These results suggest that dynamic decomposition can enhance many inference scaling techniques.

### Subsampling Graphs with GNN Performance Guarantees 
[[arxiv](https://arxiv.org/abs/2502.16703)] [[cool](https://papers.cool/arxiv/2502.16703)] [[pdf](https://arxiv.org/pdf/2502.16703)]
> **Authors**: Mika Sarkin Jain,Stefanie Jegelka,Ishani Karmarkar,Luana Ruiz,Ellen Vitercik
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: How can we subsample graph data so that a graph neural network (GNN) trained on the subsample achieves performance comparable to training on the full dataset? This question is of fundamental interest, as smaller datasets reduce labeling costs, storage requirements, and computational resources needed for training. Selecting an effective subset is challenging: a poorly chosen subsample can severely degrade model performance, and empirically testing multiple subsets for quality obviates the benefits of subsampling. Therefore, it is critical that subsampling comes with guarantees on model performance. In this work, we introduce new subsampling methods for graph datasets that leverage the Tree Mover's Distance to reduce both the number of graphs and the size of individual graphs. To our knowledge, our approach is the first that is supported by rigorous theoretical guarantees: we prove that training a GNN on the subsampled data results in a bounded increase in loss compared to training on the full dataset. Unlike existing methods, our approach is both model-agnostic, requiring minimal assumptions about the GNN architecture, and label-agnostic, eliminating the need to label the full training set. This enables subsampling early in the model development pipeline (before data annotation, model selection, and hyperparameter tuning) reducing costs and resources needed for storage, labeling, and training. We validate our theoretical results with experiments showing that our approach outperforms existing subsampling methods across multiple datasets.

### Dynamic LLM Routing and Selection based on User Preferences: Balancing Performance, Cost, and Ethics 
[[arxiv](https://arxiv.org/abs/2502.16696)] [[cool](https://papers.cool/arxiv/2502.16696)] [[pdf](https://arxiv.org/pdf/2502.16696)]
> **Authors**: Deepak Babu Piskala,Vijay Raajaa,Sachin Mishra,Bruno Bozza
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: ef:International Journal of Computer Applications, Vol. 186, No. 51, November 2024, pp. 1-7
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: With the widespread deployment of large language models (LLMs) such as GPT4, BART, and LLaMA, the need for a system that can intelligently select the most suitable model for specific tasks while balancing cost, latency, accuracy, and ethical considerations has become increasingly important. Recognizing that not all tasks necessitate models with over 100 billion parameters, we introduce OptiRoute, an advanced model routing engine designed to dynamically select and route tasks to the optimal LLM based on detailed user-defined requirements. OptiRoute captures both functional (e.g., accuracy, speed, cost) and non-functional (e.g., helpfulness, harmlessness, honesty) criteria, leveraging lightweight task analysis and complexity estimation to efficiently match tasks with the best-fit models from a diverse array of LLMs. By employing a hybrid approach combining k-nearest neighbors (kNN) search and hierarchical filtering, OptiRoute optimizes for user priorities while minimizing computational overhead. This makes it ideal for real-time applications in cloud-based ML platforms, personalized AI services, and regulated industries.

### Analyzing Factors Influencing Driver Willingness to Accept Advanced Driver Assistance Systems 
[[arxiv](https://arxiv.org/abs/2502.16688)] [[cool](https://papers.cool/arxiv/2502.16688)] [[pdf](https://arxiv.org/pdf/2502.16688)]
> **Authors**: Hannah Musau,Nana Kankam Gyimah,Judith Mwakalonge,Gurcan Comert,Saidi Siuhi
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Advanced Driver Assistance Systems (ADAS) enhance highway safety by improving environmental perception and reducing human errors. However, misconceptions, trust issues, and knowledge gaps hinder widespread adoption. This study examines driver perceptions, knowledge sources, and usage patterns of ADAS in passenger vehicles. A nationwide survey collected data from a diverse sample of U.S. drivers. Machine learning models predicted ADAS adoption, with SHAP (SHapley Additive Explanations) identifying key influencing factors. Findings indicate that higher trust levels correlate with increased ADAS usage, while concerns about reliability remain a barrier. Specific features, such as Forward Collision Warning and Driver Monitoring Systems, significantly influence adoption likelihood. Demographic factors (age, gender) and driving habits (experience, frequency) also shape ADAS acceptance. Findings emphasize the influence of socioeconomic, demographic, and behavioral factors on ADAS adoption, offering guidance for automakers, policymakers, and safety advocates to improve awareness, trust, and usability.

### Are Sparse Autoencoders Useful? A Case Study in Sparse Probing 
[[arxiv](https://arxiv.org/abs/2502.16681)] [[cool](https://papers.cool/arxiv/2502.16681)] [[pdf](https://arxiv.org/pdf/2502.16681)]
> **Authors**: Subhash Kantamneni,Joshua Engels,Senthooran Rajamanoharan,Max Tegmark,Neel Nanda
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Sparse autoencoders (SAEs) are a popular method for interpreting concepts represented in large language model (LLM) activations. However, there is a lack of evidence regarding the validity of their interpretations due to the lack of a ground truth for the concepts used by an LLM, and a growing number of works have presented problems with current SAEs. One alternative source of evidence would be demonstrating that SAEs improve performance on downstream tasks beyond existing baselines. We test this by applying SAEs to the real-world task of LLM activation probing in four regimes: data scarcity, class imbalance, label noise, and covariate shift. Due to the difficulty of detecting concepts in these challenging settings, we hypothesize that SAEs' basis of interpretable, concept-level latents should provide a useful inductive bias. However, although SAEs occasionally perform better than baselines on individual datasets, we are unable to design ensemble methods combining SAEs with baselines that consistently outperform ensemble methods solely using baselines. Additionally, although SAEs initially appear promising for identifying spurious correlations, detecting poor dataset quality, and training multi-token probes, we are able to achieve similar results with simple non-SAE baselines as well. Though we cannot discount SAEs' utility on other tasks, our findings highlight the shortcomings of current SAEs and the need to rigorously evaluate interpretability methods on downstream tasks with strong baselines.

### MetaSym: A Symplectic Meta-learning Framework for Physical Intelligence 
[[arxiv](https://arxiv.org/abs/2502.16667)] [[cool](https://papers.cool/arxiv/2502.16667)] [[pdf](https://arxiv.org/pdf/2502.16667)]
> **Authors**: Pranav Vaidhyanathan,Aristotelis Papatheodorou,Mark T. Mitchison,Natalia Ares,Ioannis Havoutis
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: 8+10 pages, 5 figures, 4 tables
- **标题**: None
- **领域**: 机器学习,机器人技术,计算物理,量子物理学
- **Abstract**: Scalable and generalizable physics-aware deep learning has long been considered a significant challenge with various applications across diverse domains ranging from robotics to molecular dynamics. Central to almost all physical systems are symplectic forms, the geometric backbone that underpins fundamental invariants like energy and momentum. In this work, we introduce a novel deep learning architecture, MetaSym. In particular, MetaSym combines a strong symplectic inductive bias obtained from a symplectic encoder and an autoregressive decoder with meta-attention. This principled design ensures that core physical invariants remain intact while allowing flexible, data-efficient adaptation to system heterogeneities. We benchmark MetaSym on highly varied datasets such as a high-dimensional spring mesh system (Otness et al., 2021), an open quantum system with dissipation and measurement backaction, and robotics-inspired quadrotor dynamics. Our results demonstrate superior performance in modeling dynamics under few-shot adaptation, outperforming state-of-the-art baselines with far larger models.

### Geometric Kolmogorov-Arnold Superposition Theorem 
[[arxiv](https://arxiv.org/abs/2502.16664)] [[cool](https://papers.cool/arxiv/2502.16664)] [[pdf](https://arxiv.org/pdf/2502.16664)]
> **Authors**: Francesco Alesiani,Takashi Maruyama,Henrik Christiansen,Viktor Zaverkin
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The Kolmogorov-Arnold Theorem (KAT), or more generally, the Kolmogorov Superposition Theorem (KST), establishes that any non-linear multivariate function can be exactly represented as a finite superposition of non-linear univariate functions. Unlike the universal approximation theorem, which provides only an approximate representation without guaranteeing a fixed network size, KST offers a theoretically exact decomposition. The Kolmogorov-Arnold Network (KAN) was introduced as a trainable model to implement KAT, and recent advancements have adapted KAN using concepts from modern neural networks. However, KAN struggles to effectively model physical systems that require inherent equivariance or invariance to $E(3)$ transformations, a key property for many scientific and engineering applications. In this work, we propose a novel extension of KAT and KAN to incorporate equivariance and invariance over $O(n)$ group actions, enabling accurate and efficient modeling of these systems. Our approach provides a unified approach that bridges the gap between mathematical theory and practical architectures for physical systems, expanding the applicability of KAN to a broader class of problems.

### BioMaze: Benchmarking and Enhancing Large Language Models for Biological Pathway Reasoning 
[[arxiv](https://arxiv.org/abs/2502.16660)] [[cool](https://papers.cool/arxiv/2502.16660)] [[pdf](https://arxiv.org/pdf/2502.16660)]
> **Authors**: Haiteng Zhao,Chang Ma,Fangzhi Xu,Lingpeng Kong,Zhi-Hong Deng
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,定量方法
- **Abstract**: The applications of large language models (LLMs) in various biological domains have been explored recently, but their reasoning ability in complex biological systems, such as pathways, remains underexplored, which is crucial for predicting biological phenomena, formulating hypotheses, and designing experiments. This work explores the potential of LLMs in pathway reasoning. We introduce BioMaze, a dataset with 5.1K complex pathway problems derived from real research, covering various biological contexts including natural dynamic changes, disturbances, additional intervention conditions, and multi-scale research targets. Our evaluation of methods such as CoT and graph-augmented reasoning, shows that LLMs struggle with pathway reasoning, especially in perturbed systems. To address this, we propose PathSeeker, an LLM agent that enhances reasoning through interactive subgraph-based navigation, enabling a more effective approach to handling the complexities of biological systems in a scientifically aligned manner. The dataset and code are available at https://github.com/zhao-ht/BioMaze.

### Volume Optimality in Conformal Prediction with Structured Prediction Sets 
[[arxiv](https://arxiv.org/abs/2502.16658)] [[cool](https://papers.cool/arxiv/2502.16658)] [[pdf](https://arxiv.org/pdf/2502.16658)]
> **Authors**: Chao Gao,Liren Shan,Vaidehi Srinivas,Aravindan Vijayaraghavan
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: 41 pages, 19 figures, 2 tables
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Conformal Prediction is a widely studied technique to construct prediction sets of future observations. Most conformal prediction methods focus on achieving the necessary coverage guarantees, but do not provide formal guarantees on the size (volume) of the prediction sets. We first prove an impossibility of volume optimality where any distribution-free method can only find a trivial solution. We then introduce a new notion of volume optimality by restricting the prediction sets to belong to a set family (of finite VC-dimension), specifically a union of $k$-intervals. Our main contribution is an efficient distribution-free algorithm based on dynamic programming (DP) to find a union of $k$-intervals that is guaranteed for any distribution to have near-optimal volume among all unions of $k$-intervals satisfying the desired coverage property. By adopting the framework of distributional conformal prediction (Chernozhukov et al., 2021), the new DP based conformity score can also be applied to achieve approximate conditional coverage and conditional restricted volume optimality, as long as a reasonable estimator of the conditional CDF is available. While the theoretical results already establish volume-optimality guarantees, they are complemented by experiments that demonstrate that our method can significantly outperform existing methods in many settings.

### Few-shot Continual Relation Extraction via Open Information Extraction 
[[arxiv](https://arxiv.org/abs/2502.16648)] [[cool](https://papers.cool/arxiv/2502.16648)] [[pdf](https://arxiv.org/pdf/2502.16648)]
> **Authors**: Thiem Nguyen,Anh Nguyen,Quyen Tran,Tu Vu,Diep Nguyen,Linh Ngo,Thien Nguyen
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,信息检索
- **Abstract**: Typically, Few-shot Continual Relation Extraction (FCRE) models must balance retaining prior knowledge while adapting to new tasks with extremely limited data. However, real-world scenarios may also involve unseen or undetermined relations that existing methods still struggle to handle. To address these challenges, we propose a novel approach that leverages the Open Information Extraction concept of Knowledge Graph Construction (KGC). Our method not only exposes models to all possible pairs of relations, including determined and undetermined labels not available in the training set, but also enriches model knowledge with diverse relation descriptions, thereby enhancing knowledge retention and adaptability in this challenging scenario. In the perspective of KGC, this is the first work explored in the setting of Continual Learning, allowing efficient expansion of the graph as the data evolves. Experimental results demonstrate our superior performance compared to other state-of-the-art FCRE baselines, as well as the efficiency in handling dynamic graph construction in this setting.

### Automatic Joint Structured Pruning and Quantization for Efficient Neural Network Training and Compression 
[[arxiv](https://arxiv.org/abs/2502.16638)] [[cool](https://papers.cool/arxiv/2502.16638)] [[pdf](https://arxiv.org/pdf/2502.16638)]
> **Authors**: Xiaoyi Qu,David Aponte,Colby Banbury,Daniel P. Robinson,Tianyu Ding,Kazuhito Koishida,Ilya Zharkov,Tianyi Chen
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: Structured pruning and quantization are fundamental techniques used to reduce the size of deep neural networks (DNNs) and typically are applied independently. Applying these techniques jointly via co-optimization has the potential to produce smaller, high-quality models. However, existing joint schemes are not widely used because of (1) engineering difficulties (complicated multi-stage processes), (2) black-box optimization (extensive hyperparameter tuning to control the overall compression), and (3) insufficient architecture generalization. To address these limitations, we present the framework GETA, which automatically and efficiently performs joint structured pruning and quantization-aware training on any DNNs. GETA introduces three key innovations: (i) a quantization-aware dependency graph (QADG) that constructs a pruning search space for generic quantization-aware DNN, (ii) a partially projected stochastic gradient method that guarantees layerwise bit constraints are satisfied, and (iii) a new joint learning strategy that incorporates interpretable relationships between pruning and quantization. We present numerical experiments on both convolutional neural networks and transformer architectures that show that our approach achieves competitive (often superior) performance compared to existing joint pruning and quantization methods.

### Time Series Domain Adaptation via Latent Invariant Causal Mechanism 
[[arxiv](https://arxiv.org/abs/2502.16637)] [[cool](https://papers.cool/arxiv/2502.16637)] [[pdf](https://arxiv.org/pdf/2502.16637)]
> **Authors**: Ruichu Cai,Junxian Huang,Zhenhui Yang,Zijian Li,Emadeldeen Eldele,Min Wu,Fuchun Sun
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,方法论
- **Abstract**: Time series domain adaptation aims to transfer the complex temporal dependence from the labeled source domain to the unlabeled target domain. Recent advances leverage the stable causal mechanism over observed variables to model the domain-invariant temporal dependence. However, modeling precise causal structures in high-dimensional data, such as videos, remains challenging. Additionally, direct causal edges may not exist among observed variables (e.g., pixels). These limitations hinder the applicability of existing approaches to real-world scenarios. To address these challenges, we find that the high-dimension time series data are generated from the low-dimension latent variables, which motivates us to model the causal mechanisms of the temporal latent process. Based on this intuition, we propose a latent causal mechanism identification framework that guarantees the uniqueness of the reconstructed latent causal structures. Specifically, we first identify latent variables by utilizing sufficient changes in historical information. Moreover, by enforcing the sparsity of the relationships of latent variables, we can achieve identifiable latent causal structures. Built on the theoretical results, we develop the Latent Causality Alignment (LCA) model that leverages variational inference, which incorporates an intra-domain latent sparsity constraint for latent structure reconstruction and an inter-domain latent sparsity constraint for domain-invariant structure reconstruction. Experiment results on eight benchmarks show a general improvement in the domain-adaptive time series classification and forecasting tasks, highlighting the effectiveness of our method in real-world scenarios. Codes are available at https://github.com/DMIRLAB-Group/LCA.

### Energy-Efficient Transformer Inference: Optimization Strategies for Time Series Classification 
[[arxiv](https://arxiv.org/abs/2502.16627)] [[cool](https://papers.cool/arxiv/2502.16627)] [[pdf](https://arxiv.org/pdf/2502.16627)]
> **Authors**: Arshia Kermani,Ehsan Zeraatkar,Habib Irani
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,表现
- **Abstract**: The increasing computational demands of transformer models in time series classification necessitate effective optimization strategies for energy-efficient deployment. This paper presents a systematic investigation of optimization techniques, focusing on structured pruning and quantization methods for transformer architectures. Through extensive experimentation on three distinct datasets (RefrigerationDevices, ElectricDevices, and PLAID), we quantitatively evaluate model performance and energy efficiency across different transformer configurations. Our experimental results demonstrate that static quantization reduces energy consumption by 29.14% while maintaining classification performance, and L1 pruning achieves a 63% improvement in inference speed with minimal accuracy degradation. These findings provide valuable insights into the effectiveness of optimization strategies for transformer-based time series classification, establishing a foundation for efficient model deployment in resource-constrained environments.

### Optimal Kernel Learning for Gaussian Process Models with High-Dimensional Input 
[[arxiv](https://arxiv.org/abs/2502.16617)] [[cool](https://papers.cool/arxiv/2502.16617)] [[pdf](https://arxiv.org/pdf/2502.16617)]
> **Authors**: Lulu Kang,Minshen Xu
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: 30 pages, 7 tables, 8 figures
- **标题**: None
- **领域**: 机器学习,方法论,机器学习
- **Abstract**: Gaussian process (GP) regression is a popular surrogate modeling tool for computer simulations in engineering and scientific domains. However, it often struggles with high computational costs and low prediction accuracy when the simulation involves too many input variables. For some simulation models, the outputs may only be significantly influenced by a small subset of the input variables, referred to as the ``active variables''. We propose an optimal kernel learning approach to identify these active variables, thereby overcoming GP model limitations and enhancing system understanding. Our method approximates the original GP model's covariance function through a convex combination of kernel functions, each utilizing low-dimensional subsets of input variables. Inspired by the Fedorov-Wynn algorithm from optimal design literature, we develop an optimal kernel learning algorithm to determine this approximation. We incorporate the effect heredity principle, a concept borrowed from the field of ``design and analysis of experiments'', to ensure sparsity in active variable selection. Through several examples, we demonstrate that the proposed method outperforms alternative approaches in correctly identifying active input variables and improving prediction accuracy. It is an effective solution for interpreting the surrogate GP regression and simplifying the complex underlying system.

### Co-MTP: A Cooperative Trajectory Prediction Framework with Multi-Temporal Fusion for Autonomous Driving 
[[arxiv](https://arxiv.org/abs/2502.16589)] [[cool](https://papers.cool/arxiv/2502.16589)] [[pdf](https://arxiv.org/pdf/2502.16589)]
> **Authors**: Xinyu Zhang,Zewei Zhou,Zhaoyi Wang,Yangjie Ji,Yanjun Huang,Hong Chen
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: 8 pages, 3 figures, ICRA 2025
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别,机器人技术
- **Abstract**: Vehicle-to-everything technologies (V2X) have become an ideal paradigm to extend the perception range and see through the occlusion. Exiting efforts focus on single-frame cooperative perception, however, how to capture the temporal cue between frames with V2X to facilitate the prediction task even the planning task is still underexplored. In this paper, we introduce the Co-MTP, a general cooperative trajectory prediction framework with multi-temporal fusion for autonomous driving, which leverages the V2X system to fully capture the interaction among agents in both history and future domains to benefit the planning. In the history domain, V2X can complement the incomplete history trajectory in single-vehicle perception, and we design a heterogeneous graph transformer to learn the fusion of the history feature from multiple agents and capture the history interaction. Moreover, the goal of prediction is to support future planning. Thus, in the future domain, V2X can provide the prediction results of surrounding objects, and we further extend the graph transformer to capture the future interaction among the ego planning and the other vehicles' intentions and obtain the final future scenario state under a certain planning action. We evaluate the Co-MTP framework on the real-world dataset V2X-Seq, and the results show that Co-MTP achieves state-of-the-art performance and that both history and future fusion can greatly benefit prediction.

### Entropy-Lens: The Information Signature of Transformer Computations 
[[arxiv](https://arxiv.org/abs/2502.16570)] [[cool](https://papers.cool/arxiv/2502.16570)] [[pdf](https://arxiv.org/pdf/2502.16570)]
> **Authors**: Riccardo Ali,Francesco Caso,Christopher Irwin,Pietro Liò
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: Transformer models have revolutionized fields from natural language processing to computer vision, yet their internal computational dynamics remain poorly understood raising concerns about predictability and robustness. In this work, we introduce Entropy-Lens, a scalable, model-agnostic framework that leverages information theory to interpret frozen, off-the-shelf large-scale transformers. By quantifying the evolution of Shannon entropy within intermediate residual streams, our approach extracts computational signatures that distinguish model families, categorize task-specific prompts, and correlate with output accuracy. We further demonstrate the generality of our method by extending the analysis to vision transformers. Our results suggest that entropy-based metrics can serve as a principled tool for unveiling the inner workings of modern transformer architectures.

### Composable Strategy Framework with Integrated Video-Text based Large Language Models for Heart Failure Assessment 
[[arxiv](https://arxiv.org/abs/2502.16548)] [[cool](https://papers.cool/arxiv/2502.16548)] [[pdf](https://arxiv.org/pdf/2502.16548)]
> **Authors**: Jianzhou Chen,Xiumei Wang,Jinyang Sun,Xi Chen,Heyu Chu,Guo Song,Yuji Luo,Xingping Zhou,Rong Gu
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: Heart failure is one of the leading causes of death worldwide, with millons of deaths each year, according to data from the World Health Organization (WHO) and other public health agencies. While significant progress has been made in the field of heart failure, leading to improved survival rates and improvement of ejection fraction, there remains substantial unmet needs, due to the complexity and multifactorial characteristics. Therefore, we propose a composable strategy framework for assessment and treatment optimization in heart failure. This framework simulates the doctor-patient consultation process and leverages multi-modal algorithms to analyze a range of data, including video, physical examination, text results as well as medical history. By integrating these various data sources, our framework offers a more holistic evaluation and optimized treatment plan for patients. Our results demonstrate that this multi-modal approach outperforms single-modal artificial intelligence (AI) algorithms in terms of accuracy in heart failure (HF) prognosis prediction. Through this method, we can further evaluate the impact of various pathological indicators on HF prognosis,providing a more comprehensive evaluation.

### A Survey of Graph Transformers: Architectures, Theories and Applications 
[[arxiv](https://arxiv.org/abs/2502.16533)] [[cool](https://papers.cool/arxiv/2502.16533)] [[pdf](https://arxiv.org/pdf/2502.16533)]
> **Authors**: Chaohao Yuan,Kangfei Zhao,Ercan Engin Kuruoglu,Liang Wang,Tingyang Xu,Wenbing Huang,Deli Zhao,Hong Cheng,Yu Rong
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Graph Transformers (GTs) have demonstrated a strong capability in modeling graph structures by addressing the intrinsic limitations of graph neural networks (GNNs), such as over-smoothing and over-squashing. Recent studies have proposed diverse architectures, enhanced explainability, and practical applications for Graph Transformers. In light of these rapid developments, we conduct a comprehensive review of Graph Transformers, covering aspects such as their architectures, theoretical foundations, and applications within this survey. We categorize the architecture of Graph Transformers according to their strategies for processing structural information, including graph tokenization, positional encoding, structure-aware attention and model ensemble. Furthermore, from the theoretical perspective, we examine the expressivity of Graph Transformers in various discussed architectures and contrast them with other advanced graph learning algorithms to discover the connections. Furthermore, we provide a summary of the practical applications where Graph Transformers have been utilized, such as molecule, protein, language, vision, traffic, brain and material data. At the end of this survey, we will discuss the current challenges and prospective directions in Graph Transformers for potential future research.

### Predicting Bad Goods Risk Scores with ARIMA Time Series: A Novel Risk Assessment Approach 
[[arxiv](https://arxiv.org/abs/2502.16520)] [[cool](https://papers.cool/arxiv/2502.16520)] [[pdf](https://arxiv.org/pdf/2502.16520)]
> **Authors**: Bishwajit Prasad Gond
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,应用领域
- **Abstract**: The increasing complexity of supply chains and the rising costs associated with defective or substandard goods (bad goods) highlight the urgent need for advanced predictive methodologies to mitigate risks and enhance operational efficiency. This research presents a novel framework that integrates Time Series ARIMA (AutoRegressive Integrated Moving Average) models with a proprietary formula specifically designed to calculate bad goods after time series forecasting. By leveraging historical data patterns, including sales, returns, and capacity, the model forecasts potential quality failures, enabling proactive decision-making. ARIMA is employed to capture temporal trends in time series data, while the newly developed formula quantifies the likelihood and impact of defects with greater precision. Experimental results, validated on a dataset spanning 2022-2024 for Organic Beer-G 1 Liter, demonstrate that the proposed method outperforms traditional statistical models, such as Exponential Smoothing and Holt-Winters, in both prediction accuracy and risk evaluation. This study advances the field of predictive analytics by bridging time series forecasting, ARIMA, and risk management in supply chain quality control, offering a scalable and practical solution for minimizing losses due to bad goods.

### Guarding the Privacy of Label-Only Access to Neural Network Classifiers via iDP Verification 
[[arxiv](https://arxiv.org/abs/2502.16519)] [[cool](https://papers.cool/arxiv/2502.16519)] [[pdf](https://arxiv.org/pdf/2502.16519)]
> **Authors**: Anan Kabaha,Dana Drachsler-Cohen
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,编程语言
- **Abstract**: Neural networks are susceptible to privacy attacks that can extract private information of the training set. To cope, several training algorithms guarantee differential privacy (DP) by adding noise to their computation. However, DP requires to add noise considering every possible training set. This leads to a significant decrease in the network's accuracy. Individual DP (iDP) restricts DP to a given training set. We observe that some inputs deterministically satisfy iDP without any noise. By identifying them, we can provide iDP label-only access to the network with a minor decrease to its accuracy. However, identifying the inputs that satisfy iDP without any noise is highly challenging. Our key idea is to compute the iDP deterministic bound (iDP-DB), which overapproximates the set of inputs that do not satisfy iDP, and add noise only to their predicted labels. To compute the tightest iDP-DB, which enables to guard the label-only access with minimal accuracy decrease, we propose LUCID, which leverages several formal verification techniques. First, it encodes the problem as a mixed-integer linear program, defined over a network and over every network trained identically but without a unique data point. Second, it abstracts a set of networks using a hyper-network. Third, it eliminates the overapproximation error via a novel branch-and-bound technique. Fourth, it bounds the differences of matching neurons in the network and the hyper-network and employs linear relaxation if they are small. We show that LUCID can provide classifiers with a perfect individuals' privacy guarantee (0-iDP) -- which is infeasible for DP training algorithms -- with an accuracy decrease of 1.4%. For more relaxed $\varepsilon$-iDP guarantees, LUCID has an accuracy decrease of 1.2%. In contrast, existing DP training algorithms reduce the accuracy by 12.7%.

### PMAT: Optimizing Action Generation Order in Multi-Agent Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.16496)] [[cool](https://papers.cool/arxiv/2502.16496)] [[pdf](https://arxiv.org/pdf/2502.16496)]
> **Authors**: Kun Hu,Muning Wen,Xihuai Wang,Shao Zhang,Yiwei Shi,Minne Li,Minglong Li,Ying Wen
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: Accepted by AAMAS 2025
- **标题**: None
- **领域**: 机器学习,人工智能,多代理系统
- **Abstract**: Multi-agent reinforcement learning (MARL) faces challenges in coordinating agents due to complex interdependencies within multi-agent systems. Most MARL algorithms use the simultaneous decision-making paradigm but ignore the action-level dependencies among agents, which reduces coordination efficiency. In contrast, the sequential decision-making paradigm provides finer-grained supervision for agent decision order, presenting the potential for handling dependencies via better decision order management. However, determining the optimal decision order remains a challenge. In this paper, we introduce Action Generation with Plackett-Luce Sampling (AGPS), a novel mechanism for agent decision order optimization. We model the order determination task as a Plackett-Luce sampling process to address issues such as ranking instability and vanishing gradient during the network training process. AGPS realizes credit-based decision order determination by establishing a bridge between the significance of agents' local observations and their decision credits, thus facilitating order optimization and dependency management. Integrating AGPS with the Multi-Agent Transformer, we propose the Prioritized Multi-Agent Transformer (PMAT), a sequential decision-making MARL algorithm with decision order optimization. Experiments on benchmarks including StarCraft II Multi-Agent Challenge, Google Research Football, and Multi-Agent MuJoCo show that PMAT outperforms state-of-the-art algorithms, greatly enhancing coordination efficiency.

### On Computational Limits of FlowAR Models: Expressivity and Efficiency 
[[arxiv](https://arxiv.org/abs/2502.16490)] [[cool](https://papers.cool/arxiv/2502.16490)] [[pdf](https://arxiv.org/pdf/2502.16490)]
> **Authors**: Chengyue Gong,Yekun Ke,Xiaoyu Li,Yingyu Liang,Zhizhou Sha,Zhenmei Shi,Zhao Song
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算复杂度,计算机视觉和模式识别
- **Abstract**: The expressive power and computational complexity of deep visual generative models, such as flow-based and autoregressive (AR) models, have gained considerable interest for their wide-ranging applications in generative tasks. However, the theoretical characterization of their expressiveness through the lens of circuit complexity remains underexplored, particularly for the state-of-the-art architecture like FlowAR proposed by [Ren et al., 2024], which integrates flow-based and autoregressive mechanisms. This gap limits our understanding of their inherent computational limits and practical efficiency. In this study, we address this gap by analyzing the circuit complexity of the FlowAR architecture. We demonstrate that when the largest feature map produced by the FlowAR model has dimensions $n \times n \times c$, the FlowAR model is simulable by a family of threshold circuits $\mathsf{TC}^0$, which have constant depth $O(1)$ and polynomial width $\mathrm{poly}(n)$. This is the first study to rigorously highlight the limitations in the expressive power of FlowAR models. Furthermore, we identify the conditions under which the FlowAR model computations can achieve almost quadratic time. To validate our theoretical findings, we present efficient model variant constructions based on low-rank approximations that align with the derived criteria. Our work provides a foundation for future comparisons with other generative paradigms and guides the development of more efficient and expressive implementations.

### A Split-Window Transformer for Multi-Model Sequence Spammer Detection using Multi-Model Variational Autoencoder 
[[arxiv](https://arxiv.org/abs/2502.16483)] [[cool](https://papers.cool/arxiv/2502.16483)] [[pdf](https://arxiv.org/pdf/2502.16483)]
> **Authors**: Zhou Yang,Yucai Pang,Hongbo Yin,Yunpeng Xiao
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,多媒体,社交和信息网络
- **Abstract**: This paper introduces a new Transformer, called MS$^2$Dformer, that can be used as a generalized backbone for multi-modal sequence spammer detection. Spammer detection is a complex multi-modal task, thus the challenges of applying Transformer are two-fold. Firstly, complex multi-modal noisy information about users can interfere with feature mining. Secondly, the long sequence of users' historical behaviors also puts a huge GPU memory pressure on the attention computation. To solve these problems, we first design a user behavior Tokenization algorithm based on the multi-modal variational autoencoder (MVAE). Subsequently, a hierarchical split-window multi-head attention (SW/W-MHA) mechanism is proposed. The split-window strategy transforms the ultra-long sequences hierarchically into a combination of intra-window short-term and inter-window overall attention. Pre-trained on the public datasets, MS$^2$Dformer's performance far exceeds the previous state of the art. The experiments demonstrate MS$^2$Dformer's ability to act as a backbone.

### Feature Space Perturbation: A Panacea to Enhanced Transferability Estimation 
[[arxiv](https://arxiv.org/abs/2502.16471)] [[cool](https://papers.cool/arxiv/2502.16471)] [[pdf](https://arxiv.org/pdf/2502.16471)]
> **Authors**: Prafful Kumar Khoba,Zijian Wang,Chetan Arora,Mahsa Baktashmotlagh
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: ef:Winter Conference on Applications of Computer Vision (WACV) 2025
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: Leveraging a transferability estimation metric facilitates the non-trivial challenge of selecting the optimal model for the downstream task from a pool of pre-trained models. Most existing metrics primarily focus on identifying the statistical relationship between feature embeddings and the corresponding labels within the target dataset, but overlook crucial aspect of model robustness. This oversight may limit their effectiveness in accurately ranking pre-trained models. To address this limitation, we introduce a feature perturbation method that enhances the transferability estimation process by systematically altering the feature space. Our method includes a Spread operation that increases intra-class variability, adding complexity within classes, and an Attract operation that minimizes the distances between different classes, thereby blurring the class boundaries. Through extensive experimentation, we demonstrate the efficacy of our feature perturbation method in providing a more precise and robust estimation of model transferability. Notably, the existing LogMe method exhibited a significant improvement, showing a 28.84% increase in performance after applying our feature perturbation method.

### Improved Margin Generalization Bounds for Voting Classifiers 
[[arxiv](https://arxiv.org/abs/2502.16462)] [[cool](https://papers.cool/arxiv/2502.16462)] [[pdf](https://arxiv.org/pdf/2502.16462)]
> **Authors**: Mikael Møller Høgsgaard,Kasper Green Larsen
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,数据结构和算法,统计理论,机器学习
- **Abstract**: In this paper we establish a new margin-based generalization bound for voting classifiers, refining existing results and yielding tighter generalization guarantees for widely used boosting algorithms such as AdaBoost (Freund and Schapire, 1997). Furthermore, the new margin-based generalization bound enables the derivation of an optimal weak-to-strong learner: a Majority-of-3 large-margin classifiers with an expected error matching the theoretical lower bound. This result provides a more natural alternative to the Majority-of-5 algorithm by (Høgsgaard et al. 2024) , and matches the Majority-of-3 result by (Aden-Ali et al. 2024) for the realizable prediction model.

### MAPN: Enhancing Heterogeneous Sparse Graph Representation by Mamba-based Asynchronous Aggregation 
[[arxiv](https://arxiv.org/abs/2502.16454)] [[cool](https://papers.cool/arxiv/2502.16454)] [[pdf](https://arxiv.org/pdf/2502.16454)]
> **Authors**: Xuqi Mao,Zhenying He,X. Sean Wang
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Graph neural networks (GNNs) have become the state of the art for various graph-related tasks and are particularly prominent in heterogeneous graphs (HetGs). However, several issues plague this paradigm: first, the difficulty in fully utilizing long-range information, known as over-squashing; second, the tendency for excessive message-passing layers to produce indistinguishable representations, referred to as over-smoothing; and finally, the inadequacy of conventional MPNNs to train effectively on large sparse graphs. To address these challenges in deep neural networks for large-scale heterogeneous graphs, this paper introduces the Mamba-based Asynchronous Propagation Network (MAPN), which enhances the representation of heterogeneous sparse graphs. MAPN consists of two primary components: node sequence generation and semantic information aggregation. Node sequences are initially generated based on meta-paths through random walks, which serve as the foundation for a spatial state model that extracts essential information from nodes at various distances. It then asynchronously aggregates semantic information across multiple hops and layers, effectively preserving unique node characteristics and mitigating issues related to deep network degradation. Extensive experiments across diverse datasets demonstrate the effectiveness of MAPN in graph embeddings for various downstream tasks underscoring its substantial benefits for graph representation in large sparse heterogeneous graphs.

### Auxiliary Discrminator Sequence Generative Adversarial Networks (ADSeqGAN) for Few Sample Molecule Generation 
[[arxiv](https://arxiv.org/abs/2502.16446)] [[cool](https://papers.cool/arxiv/2502.16446)] [[pdf](https://arxiv.org/pdf/2502.16446)]
> **Authors**: Haocheng Tang,Jing Long,Junmei Wang
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,生物分子
- **Abstract**: In this work, we introduce Auxiliary Discriminator Sequence Generative Adversarial Networks (ADSeqGAN), a novel approach for molecular generation in small-sample datasets. Traditional generative models often struggle with limited training data, particularly in drug discovery, where molecular datasets for specific therapeutic targets, such as nucleic acids binders and central nervous system (CNS) drugs, are scarce. ADSeqGAN addresses this challenge by integrating an auxiliary random forest classifier as an additional discriminator into the GAN framework, significantly improves molecular generation quality and class specificity. Our method incorporates pretrained generator and Wasserstein distance to enhance training stability and diversity. We evaluate ADSeqGAN on a dataset comprising nucleic acid-targeting and protein-targeting small molecules, demonstrating its superior ability to generate nucleic acid binders compared to baseline models such as SeqGAN, ORGAN, and MolGPT. Through an oversampling strategy, ADSeqGAN also significantly improves CNS drug generation, achieving a higher yield than traditional de novo models. Critical assessments, including docking simulations and molecular property analysis, confirm that ADSeqGAN-generated molecules exhibit strong binding affinities, enhanced chemical diversity, and improved synthetic feasibility. Overall, ADSeqGAN presents a novel framework for generative molecular design in data-scarce scenarios, offering potential applications in computational drug discovery. We have demonstrated the successful applications of ADSeqGAN in generating synthetic nucleic acid-targeting and CNS drugs in this work.

### Iterative Flow Matching -- Path Correction and Gradual Refinement for Enhanced Generative Modeling 
[[arxiv](https://arxiv.org/abs/2502.16445)] [[cool](https://papers.cool/arxiv/2502.16445)] [[pdf](https://arxiv.org/pdf/2502.16445)]
> **Authors**: Eldad Haber,Shadab Ahamed,Md. Shahriar Rahim Siddiqui,Niloufar Zakariaei,Moshe Eliasof
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: 17 pages, 8 figures
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别,机器学习
- **Abstract**: Generative models for image generation are now commonly used for a wide variety of applications, ranging from guided image generation for entertainment to solving inverse problems. Nonetheless, training a generator is a non-trivial feat that requires fine-tuning and can lead to so-called hallucinations, that is, the generation of images that are unrealistic. In this work, we explore image generation using flow matching. We explain and demonstrate why flow matching can generate hallucinations, and propose an iterative process to improve the generation process. Our iterative process can be integrated into virtually $\textit{any}$ generative modeling technique, thereby enhancing the performance and robustness of image synthesis systems.

### Compression Scaling Laws:Unifying Sparsity and Quantization 
[[arxiv](https://arxiv.org/abs/2502.16440)] [[cool](https://papers.cool/arxiv/2502.16440)] [[pdf](https://arxiv.org/pdf/2502.16440)]
> **Authors**: Elias Frantar,Utku Evci,Wonpyo Park,Neil Houlsby,Dan Alistarh
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: We investigate how different compression techniques -- such as weight and activation quantization, and weight sparsity -- affect the scaling behavior of large language models (LLMs) during pretraining. Building on previous work showing that weight sparsity acts as a constant multiplier on model size in scaling laws, we demonstrate that this "effective parameter" scaling pattern extends to quantization as well. Specifically, we establish that weight-only quantization achieves strong parameter efficiency multipliers, while full quantization of both weights and activations shows diminishing returns at lower bitwidths. Our results suggest that different compression techniques can be unified under a common scaling law framework, enabling principled comparison and combination of these methods.

### Automated Flow Pattern Classification in Multi-phase Systems Using AI and Capacitance Sensing Techniques 
[[arxiv](https://arxiv.org/abs/2502.16432)] [[cool](https://papers.cool/arxiv/2502.16432)] [[pdf](https://arxiv.org/pdf/2502.16432)]
> **Authors**: Nian Ran,Fayez M. Al-Alweet,Richard Allmendinger,Ahmad Almakhlafi
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: 29 pages, Applied Soft Computing under reviewed
- **标题**: None
- **领域**: 机器学习,计算工程、金融和科学
- **Abstract**: In multiphase flow systems, classifying flow patterns is crucial to optimize fluid dynamics and enhance system efficiency. Current industrial methods and scientific laboratories mainly depend on techniques such as flow visualization using regular cameras or the naked eye, as well as high-speed imaging at elevated flow rates. These methods are limited by their reliance on subjective interpretations and are particularly applicable in transparent pipes. Consequently, conventional techniques usually achieve context-dependent accuracy rates and often lack generalizability. This study introduces a novel platform that integrates a capacitance sensor and AI-driven classification methods, benchmarked against traditional techniques. Experimental results demonstrate that the proposed approach, utilizing a 1D SENet deep learning model, achieves over 85\% accuracy on experiment-based datasets and 71\% accuracy on pattern-based datasets. These results highlight significant improvements in robustness and reliability compared to existing methodologies. This work offers a transformative pathway for real-time flow monitoring and predictive modeling, addressing key challenges in industrial applications.

### UniDyG: A Unified and Effective Representation Learning Approach for Large Dynamic Graphs 
[[arxiv](https://arxiv.org/abs/2502.16431)] [[cool](https://papers.cool/arxiv/2502.16431)] [[pdf](https://arxiv.org/pdf/2502.16431)]
> **Authors**: Yuanyuan Xu,Wenjie Zhang,Xuemin Lin,Ying Zhang
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Dynamic graphs are formulated in continuous-time or discrete-time dynamic graphs. They differ in temporal granularity: Continuous-Time Dynamic Graphs (CTDGs) exhibit rapid, localized changes, while Discrete-Time Dynamic Graphs (DTDGs) show gradual, global updates. This difference leads to isolated developments in representation learning for each type. To advance representation learning, recent research attempts to design a unified model capable of handling both CTDGs and DTDGs. However, it typically focuses on local dynamic propagation for temporal structure learning in the time domain, failing to accurately capture the structural evolution associated with each temporal granularity. In addition, existing works-whether specific or unified-often overlook the issue of temporal noise, compromising the model robustness and effectiveness. To better model both types of dynamic graphs, we propose UniDyG, a unified and effective representation learning approach, which scales to large dynamic graphs. We first propose a novel Fourier Graph Attention (FGAT) mechanism that can model local and global structural correlations based on recent neighbors and complex-number selective aggregation, while theoretically ensuring consistent representations of dynamic graphs over time. Based on approximation theory, we demonstrate that FGAT is well-suited to capture the underlying structures in CTDGs and DTDGs. We further enhance FGAT to resist temporal noise by designing an energy-gated unit, which adaptively filters out high-frequency noise according to the energy. Last, we leverage our FGAT mechanisms for temporal structure learning and employ the frequency-enhanced linear function for node-level dynamic updates, facilitating the generation of high-quality temporal embeddings. Extensive experiments show that our UniDyG achieves an average improvement of 14.4% over sixteen baselines across nine dynamic graphs.

### Network Tomography with Path-Centric Graph Neural Network 
[[arxiv](https://arxiv.org/abs/2502.16430)] [[cool](https://papers.cool/arxiv/2502.16430)] [[pdf](https://arxiv.org/pdf/2502.16430)]
> **Authors**: Yuntong Hu,Junxiang Wang,Liang Zhao
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: 13 pages, 6 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Network tomography is a crucial problem in network monitoring, where the observable path performance metric values are used to infer the unobserved ones, making it essential for tasks such as route selection, fault diagnosis, and traffic control. However, most existing methods either assume complete knowledge of network topology and metric formulas-an unrealistic expectation in many real-world scenarios with limited observability-or rely entirely on black-box end-to-end models. To tackle this, in this paper, we argue that a good network tomography requires synergizing the knowledge from both data and appropriate inductive bias from (partial) prior knowledge. To see this, we propose Deep Network Tomography (DeepNT), a novel framework that leverages a path-centric graph neural network to predict path performance metrics without relying on predefined hand-crafted metrics, assumptions, or the real network topology. The path-centric graph neural network learns the path embedding by inferring and aggregating the embeddings of the sequence of nodes that compose this path. Training path-centric graph neural networks requires learning the neural netowrk parameters and network topology under discrete constraints induced by the observed path performance metrics, which motivates us to design a learning objective that imposes connectivity and sparsity constraints on topology and path performance triangle inequality on path performance. Extensive experiments on real-world and synthetic datasets demonstrate the superiority of DeepNT in predicting performance metrics and inferring graph topology compared to state-of-the-art methods.

### Active Learning Classification from a Signal Separation Perspective 
[[arxiv](https://arxiv.org/abs/2502.16425)] [[cool](https://papers.cool/arxiv/2502.16425)] [[pdf](https://arxiv.org/pdf/2502.16425)]
> **Authors**: Hrushikesh Mhaskar,Ryan O'Dowd,Efstratios Tsoukanis
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,优化与控制
- **Abstract**: In machine learning, classification is usually seen as a function approximation problem, where the goal is to learn a function that maps input features to class labels. In this paper, we propose a novel clustering and classification framework inspired by the principles of signal separation. This approach enables efficient identification of class supports, even in the presence of overlapping distributions. We validate our method on real-world hyperspectral datasets Salinas and Indian Pines. The experimental results demonstrate that our method is competitive with the state of the art active learning algorithms by using a very small subset of data set as training points.

### TabGen-ICL: Residual-Aware In-Context Example Selection for Tabular Data Generation 
[[arxiv](https://arxiv.org/abs/2502.16414)] [[cool](https://papers.cool/arxiv/2502.16414)] [[pdf](https://arxiv.org/pdf/2502.16414)]
> **Authors**: Liancheng Fang,Aiwei Liu,Hengrui Zhang,Henry Peng Zou,Weizhi Zhang,Philip S. Yu
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Large Language models (LLMs) have achieved encouraging results in tabular data generation. However, existing approaches require fine-tuning, which is computationally expensive. This paper explores an alternative: prompting a fixed LLM with in-context examples. We observe that using randomly selected in-context examples hampers the LLM's performance, resulting in sub-optimal generation quality. To address this, we propose a novel in-context learning framework: TabGen-ICL, to enhance the in-context learning ability of LLMs for tabular data generation. TabGen-ICL operates iteratively, retrieving a subset of real samples that represent the residual between currently generated samples and true data distributions. This approach serves two purposes: locally, it provides more effective in-context learning examples for the LLM in each iteration; globally, it progressively narrows the gap between generated and real data. Extensive experiments on five real-world tabular datasets demonstrate that TabGen-ICL significantly outperforms the random selection strategy. Specifically, it reduces the error rate by a margin of $3.5\%-42.2\%$ on fidelity metrics. We demonstrate for the first time that prompting a fixed LLM can yield high-quality synthetic tabular data. The code is provided in the \href{https://github.com/fangliancheng/TabGEN-ICL}{link}.

### TrustChain: A Blockchain Framework for Auditing and Verifying Aggregators in Decentralized Federated Learning 
[[arxiv](https://arxiv.org/abs/2502.16406)] [[cool](https://papers.cool/arxiv/2502.16406)] [[pdf](https://arxiv.org/pdf/2502.16406)]
> **Authors**: Ehsan Hallaji,Roozbeh Razavi-Far,Mehrdad Saif
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,密码学和安全
- **Abstract**: The server-less nature of Decentralized Federated Learning (DFL) requires allocating the aggregation role to specific participants in each federated round. Current DFL architectures ensure the trustworthiness of the aggregator node upon selection. However, most of these studies overlook the possibility that the aggregating node may turn rogue and act maliciously after being nominated. To address this problem, this paper proposes a DFL structure, called TrustChain, that scores the aggregators before selection based on their past behavior and additionally audits them after the aggregation. To do this, the statistical independence between the client updates and the aggregated model is continuously monitored using the Hilbert-Schmidt Independence Criterion (HSIC). The proposed method relies on several principles, including blockchain, anomaly detection, and concept drift analysis. The designed structure is evaluated on several federated datasets and attack scenarios with different numbers of Byzantine nodes.

### FedNIA: Noise-Induced Activation Analysis for Mitigating Data Poisoning in FL 
[[arxiv](https://arxiv.org/abs/2502.16396)] [[cool](https://papers.cool/arxiv/2502.16396)] [[pdf](https://arxiv.org/pdf/2502.16396)]
> **Authors**: Ehsan Hallaji,Roozbeh Razavi-Far,Mehrdad Saif
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,密码学和安全
- **Abstract**: Federated learning systems are increasingly threatened by data poisoning attacks, where malicious clients compromise global models by contributing tampered updates. Existing defenses often rely on impractical assumptions, such as access to a central test dataset, or fail to generalize across diverse attack types, particularly those involving multiple malicious clients working collaboratively. To address this, we propose Federated Noise-Induced Activation Analysis (FedNIA), a novel defense framework to identify and exclude adversarial clients without relying on any central test dataset. FedNIA injects random noise inputs to analyze the layerwise activation patterns in client models leveraging an autoencoder that detects abnormal behaviors indicative of data poisoning. FedNIA can defend against diverse attack types, including sample poisoning, label flipping, and backdoors, even in scenarios with multiple attacking nodes. Experimental results on non-iid federated datasets demonstrate its effectiveness and robustness, underscoring its potential as a foundational approach for enhancing the security of federated learning systems.

### An Analyst-Inspector Framework for Evaluating Reproducibility of LLMs in Data Science 
[[arxiv](https://arxiv.org/abs/2502.16395)] [[cool](https://papers.cool/arxiv/2502.16395)] [[pdf](https://arxiv.org/pdf/2502.16395)]
> **Authors**: Qiuhai Zeng,Claire Jin,Xinyue Wang,Yuhan Zheng,Qunhua Li
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,人机交互
- **Abstract**: Large Language Models (LLMs) have demonstrated potential for data science tasks via code generation. However, the exploratory nature of data science, alongside the stochastic and opaque outputs of LLMs, raise concerns about their reliability. While prior work focuses on benchmarking LLM accuracy, reproducibility remains underexplored, despite being critical to establishing trust in LLM-driven analysis. We propose a novel analyst-inspector framework to automatically evaluate and enforce the reproducibility of LLM-generated data science workflows - the first rigorous approach to the best of our knowledge. Defining reproducibility as the sufficiency and completeness of workflows for reproducing functionally equivalent code, this framework enforces computational reproducibility principles, ensuring transparent, well-documented LLM workflows while minimizing reliance on implicit model assumptions. Using this framework, we systematically evaluate five state-of-the-art LLMs on 1,032 data analysis tasks across three diverse benchmark datasets. We also introduce two novel reproducibility-enhancing prompting strategies. Our results show that higher reproducibility strongly correlates with improved accuracy and reproducibility-enhancing prompts are effective, demonstrating structured prompting's potential to enhance automated data science workflows and enable transparent, robust AI-driven analysis. Our code is publicly available.

### Worst-case Error Bounds for Online Learning of Smooth Functions 
[[arxiv](https://arxiv.org/abs/2502.16388)] [[cool](https://papers.cool/arxiv/2502.16388)] [[pdf](https://arxiv.org/pdf/2502.16388)]
> **Authors**: Weian Xie
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,离散数学,数据结构和算法,机器学习
- **Abstract**: Online learning is a model of machine learning where the learner is trained on sequential feedback. We investigate worst-case error for the online learning of real functions that have certain smoothness constraints. Suppose that $\mathcal{F}_q$ is the class of all absolutely continuous functions $f: [0, 1] \rightarrow \mathbb{R}$ such that $\|f'\|_q \le 1$, and $\operatorname{opt}_p(\mathcal{F}_q)$ is the best possible upper bound on the sum of the $p^{\text{th}}$ powers of absolute prediction errors for any number of trials guaranteed by any learner. We show that for any $δ, ε\in (0, 1)$, $\operatorname{opt}_{1+δ} (\mathcal{F}_{1+ε}) = O(\min(δ, ε)^{-1})$. Combined with the previous results of Kimber and Long (1995) and Geneson and Zhou (2023), we achieve a complete characterization of the values of $p, q \ge 1$ that result in $\operatorname{opt}_p(\mathcal{F}_q)$ being finite, a problem open for nearly 30 years. We study the learning scenarios of smooth functions that also belong to certain special families of functions, such as polynomials. We prove a conjecture by Geneson and Zhou (2023) that it is not any easier to learn a polynomial in $\mathcal{F}_q$ than it is to learn any general function in $\mathcal{F}_q$. We also define a noisy model for the online learning of smooth functions, where the learner may receive incorrect feedback up to $η\ge 1$ times, denoting the worst-case error bound as $\operatorname{opt}^{\text{nf}}_{p, η} (\mathcal{F}_q)$. We prove that $\operatorname{opt}^{\text{nf}}_{p, η} (\mathcal{F}_q)$ is finite if and only if $\operatorname{opt}_p(\mathcal{F}_q)$ is. Moreover, we prove for all $p, q \ge 2$ and $η\ge 1$ that $\operatorname{opt}^{\text{nf}}_{p, η} (\mathcal{F}_q) = Θ(η)$.

### Simultaneous Swap Regret Minimization via KL-Calibration 
[[arxiv](https://arxiv.org/abs/2502.16387)] [[cool](https://papers.cool/arxiv/2502.16387)] [[pdf](https://arxiv.org/pdf/2502.16387)]
> **Authors**: Haipeng Luo,Spandan Senapati,Vatsal Sharan
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,数据结构和算法,计算机科学与博弈论
- **Abstract**: Calibration is a fundamental concept that aims at ensuring the reliability of probabilistic predictions by aligning them with real-world outcomes. There is a surge of studies on new calibration measures that are easier to optimize compared to the classical $\ell_1$-Calibration while still having strong implications for downstream applications. One recent such example is the work by Fishelson et al. (2025) who show that it is possible to achieve $O(T^{1/3})$ pseudo $\ell_2$-Calibration error via minimizing pseudo swap regret of the squared loss, which in fact implies the same bound for all bounded proper losses with a smooth univariate form. In this work, we significantly generalize their result in the following ways: (a) in addition to smooth univariate forms, our algorithm also simultaneously achieves $O(T^{1/3})$ swap regret for any proper loss with a twice continuously differentiable univariate form (such as Tsallis entropy); (b) our bounds hold not only for pseudo swap regret that measures losses using the forecaster's distributions on predictions, but also hold for the actual swap regret that measures losses using the forecaster's actual realized predictions. We achieve so by introducing a new stronger notion of calibration called (pseudo) KL-Calibration, which we show is equivalent to the (pseudo) swap regret for log loss. We prove that there exists an algorithm that achieves $O(T^{1/3})$ KL-Calibration error and provide an explicit algorithm that achieves $O(T^{1/3})$ pseudo KL-Calibration error. Moreover, we show that the same algorithm achieves $O(T^{1/3}(\log T)^{-1/3}\log(T/δ))$ swap regret w.p. $\ge 1-δ$ for any proper loss with a smooth univariate form, which implies $O(T^{1/3})$ $\ell_2$-Calibration error. A technical contribution of our work is a new randomized rounding procedure and a non-uniform discretization scheme to minimize the swap regret for log loss.

### Toward a Flexible Framework for Linear Representation Hypothesis Using Maximum Likelihood Estimation 
[[arxiv](https://arxiv.org/abs/2502.16385)] [[cool](https://papers.cool/arxiv/2502.16385)] [[pdf](https://arxiv.org/pdf/2502.16385)]
> **Authors**: Trung Nguyen,Yan Leng
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Linear representation hypothesis posits that high-level concepts are encoded as linear directions in the representation spaces of LLMs. Park et al. (2024) formalize this notion by unifying multiple interpretations of linear representation, such as 1-dimensional subspace representation and interventions, using a causal inner product. However, their framework relies on single-token counterfactual pairs and cannot handle ambiguous contrasting pairs, limiting its applicability to complex or context-dependent concepts. We introduce a new notion of binary concepts as unit vectors in a canonical representation space, and utilize LLMs' (neural) activation differences along with maximum likelihood estimation (MLE) to compute concept directions (i.e., steering vectors). Our method, Sum of Activation-base Normalized Difference (SAND), formalizes the use of activation differences modeled as samples from a von Mises-Fisher (vMF) distribution, providing a principled approach to derive concept directions. We extend the applicability of Park et al. (2024) by eliminating the dependency on unembedding representations and single-token pairs. Through experiments with LLaMA models across diverse concepts and benchmarks, we demonstrate that our lightweight approach offers greater flexibility, superior performance in activation engineering tasks like monitoring and manipulation.

### Understanding Fixed Predictions via Confined Regions 
[[arxiv](https://arxiv.org/abs/2502.16380)] [[cool](https://papers.cool/arxiv/2502.16380)] [[pdf](https://arxiv.org/pdf/2502.16380)]
> **Authors**: Connor Lawless,Tsui-Wei Weng,Berk Ustun,Madeleine Udell
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,优化与控制
- **Abstract**: Machine learning models are designed to predict outcomes using features about an individual, but fail to take into account how individuals can change them. Consequently, models can assign fixed predictions that deny individuals recourse to change their outcome. This work develops a new paradigm to identify fixed predictions by finding confined regions in which all individuals receive fixed predictions. We introduce the first method, ReVer, for this task, using tools from mixed-integer quadratically constrained programming. Our approach certifies recourse for out-of-sample data, provides interpretable descriptions of confined regions, and runs in seconds on real world datasets. We conduct a comprehensive empirical study of confined regions across diverse applications. Our results highlight that existing point-wise verification methods fail to discover confined regions, while ReVer provably succeeds.

### Verifying Classification with Limited Disclosure 
[[arxiv](https://arxiv.org/abs/2502.16352)] [[cool](https://papers.cool/arxiv/2502.16352)] [[pdf](https://arxiv.org/pdf/2502.16352)]
> **Authors**: Siddharth Bhandari,Liren Shan
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: 18 pages, 0 figures
- **标题**: None
- **领域**: 机器学习,密码学和安全,计算机与社会,数据结构和算法
- **Abstract**: We consider the multi-party classification problem introduced by Dong, Hartline, and Vijayaraghavan (2022) motivated by electronic discovery. In this problem, our goal is to design a protocol that guarantees the requesting party receives nearly all responsive documents while minimizing the disclosure of nonresponsive documents. We develop verification protocols that certify the correctness of a classifier by disclosing a few nonresponsive documents. We introduce a combinatorial notion called the Leave-One-Out dimension of a family of classifiers and show that the number of nonresponsive documents disclosed by our protocol is at most this dimension in the realizable setting, where a perfect classifier exists in this family. For linear classifiers with a margin, we characterize the trade-off between the margin and the number of nonresponsive documents that must be disclosed for verification. Specifically, we establish a trichotomy in this requirement: for $d$ dimensional instances, when the margin exceeds $1/3$, verification can be achieved by revealing only $O(1)$ nonresponsive documents; when the margin is exactly $1/3$, in the worst case, at least $Ω(d)$ nonresponsive documents must be disclosed; when the margin is smaller than $1/3$, verification requires $Ω(e^d)$ nonresponsive documents. We believe this result is of independent interest with applications to coding theory and combinatorial geometry. We further extend our protocols to the nonrealizable setting defining an analogous combinatorial quantity robust Leave-One-Out dimension, and to scenarios where the protocol is tolerant to misclassification errors by Alice.

### Machine Learning-Based Cloud Computing Compliance Process Automation 
[[arxiv](https://arxiv.org/abs/2502.16344)] [[cool](https://papers.cool/arxiv/2502.16344)] [[pdf](https://arxiv.org/pdf/2502.16344)]
> **Authors**: Yuqing Wang,Xiao Yang
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机与社会,分布式、并行和集群计算
- **Abstract**: Cloud computing adoption across industries has revolutionized enterprise operations while introducing significant challenges in compliance management. Organizations must continuously meet evolving regulatory requirements such as GDPR and ISO 27001, yet traditional manual review processes have become increasingly inadequate for modern business scales. This paper presents a novel machine learning-based framework for automating cloud computing compliance processes, addressing critical challenges including resource-intensive manual reviews, extended compliance cycles, and delayed risk identification. Our proposed framework integrates multiple machine learning technologies, including BERT-based document processing (94.5% accuracy), One-Class SVM for anomaly detection (88.7% accuracy), and an improved CNN-LSTM architecture for sequential compliance data analysis (90.2% accuracy). Implementation results demonstrate significant improvements: reducing compliance process duration from 7 days to 1.5 days, improving accuracy from 78% to 93%, and decreasing manual effort by 73.3%. A real-world deployment at a major securities firm validated these results, processing 800,000 daily transactions with 94.2% accuracy in risk identification.

### Exploring Sentiment Manipulation by LLM-Enabled Intelligent Trading Agents 
[[arxiv](https://arxiv.org/abs/2502.16343)] [[cool](https://papers.cool/arxiv/2502.16343)] [[pdf](https://arxiv.org/pdf/2502.16343)]
> **Authors**: David Byrd
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算工程、金融和科学,多代理系统
- **Abstract**: Companies across all economic sectors continue to deploy large language models at a rapid pace. Reinforcement learning is experiencing a resurgence of interest due to its association with the fine-tuning of language models from human feedback. Tool-chain language models control task-specific agents; if the converse has not already appeared, it soon will. In this paper, we present what we believe is the first investigation of an intelligent trading agent based on continuous deep reinforcement learning that also controls a large language model with which it can post to a social media feed observed by other traders. We empirically investigate the performance and impact of such an agent in a simulated financial market, finding that it learns to optimize its total reward, and thereby augment its profit, by manipulating the sentiment of the posts it produces. The paper concludes with discussion, limitations, and suggestions for future work.

### A Gap Between the Gaussian RKHS and Neural Networks: An Infinite-Center Asymptotic Analysis 
[[arxiv](https://arxiv.org/abs/2502.16331)] [[cool](https://papers.cool/arxiv/2502.16331)] [[pdf](https://arxiv.org/pdf/2502.16331)]
> **Authors**: Akash Kumar,Rahul Parhi,Mikhail Belkin
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: 22 pages, 1 figure
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Recent works have characterized the function-space inductive bias of infinite-width bounded-norm single-hidden-layer neural networks as a kind of bounded-variation-type space. This novel neural network Banach space encompasses many classical multivariate function spaces including certain Sobolev spaces and the spectral Barron spaces. Notably, this Banach space also includes functions that exhibit less classical regularity such as those that only vary in a few directions. On bounded domains, it is well-established that the Gaussian reproducing kernel Hilbert space (RKHS) strictly embeds into this Banach space, demonstrating a clear gap between the Gaussian RKHS and the neural network Banach space. It turns out that when investigating these spaces on unbounded domains, e.g., all of $\mathbb{R}^d$, the story is fundamentally different. We establish the following fundamental result: Certain functions that lie in the Gaussian RKHS have infinite norm in the neural network Banach space. This provides a nontrivial gap between kernel methods and neural networks by the exhibition of functions in which kernel methods can do strictly better than neural networks.

### Generalization is not a universal guarantee: Estimating similarity to training data with an ensemble out-of-distribution metric 
[[arxiv](https://arxiv.org/abs/2502.16329)] [[cool](https://papers.cool/arxiv/2502.16329)] [[pdf](https://arxiv.org/pdf/2502.16329)]
> **Authors**: W. Max Schreyer,Christopher Anderson,Reid F. Thompson
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: 10 pages, 5 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Failure of machine learning models to generalize to new data is a core problem limiting the reliability of AI systems, partly due to the lack of simple and robust methods for comparing new data to the original training dataset. We propose a standardized approach for assessing data similarity in a model-agnostic manner by constructing a supervised autoencoder for generalizability estimation (SAGE). We compare points in a low-dimensional embedded latent space, defining empirical probability measures for k-Nearest Neighbors (kNN) distance, reconstruction of inputs and task-based performance. As proof of concept for classification tasks, we use MNIST and CIFAR-10 to demonstrate how an ensemble output probability score can separate deformed images from a mixture of typical test examples, and how this SAGE score is robust to transformations of increasing severity. As further proof of concept, we extend this approach to a regression task using non-imaging data (UCI Abalone). In all cases, we show that out-of-the-box model performance increases after SAGE score filtering, even when applied to data from the model's own training and test datasets. Our out-of-distribution scoring method can be introduced during several steps of model construction and assessment, leading to future improvements in responsible deep learning implementation.

### Risk-Averse Reinforcement Learning: An Optimal Transport Perspective on Temporal Difference Learning 
[[arxiv](https://arxiv.org/abs/2502.16328)] [[cool](https://papers.cool/arxiv/2502.16328)] [[pdf](https://arxiv.org/pdf/2502.16328)]
> **Authors**: Zahra Shahrooei,Ali Baheri
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: :cs.LG
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The primary goal of reinforcement learning is to develop decision-making policies that prioritize optimal performance, frequently without considering risk or safety. In contrast, safe reinforcement learning seeks to reduce or avoid unsafe states. This letter introduces a risk-averse temporal difference algorithm that uses optimal transport theory to direct the agent toward predictable behavior. By incorporating a risk indicator, the agent learns to favor actions with predictable consequences. We evaluate the proposed algorithm in several case studies and show its effectiveness in the presence of uncertainty. The results demonstrate that our method reduces the frequency of visits to risky states while preserving performance. A Python implementation of the algorithm is available at https:// github.com/SAILRIT/Risk-averse-TD-Learning.

### Deep Time Warping for Multiple Time Series Alignment 
[[arxiv](https://arxiv.org/abs/2502.16324)] [[cool](https://papers.cool/arxiv/2502.16324)] [[pdf](https://arxiv.org/pdf/2502.16324)]
> **Authors**: Alireza Nourbakhsh,Hoda Mohammadzade
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: 32 pages, 13 figures
- **标题**: None
- **领域**: 机器学习,人工智能,信号处理
- **Abstract**: Time Series Alignment is a critical task in signal processing with numerous real-world applications. In practice, signals often exhibit temporal shifts and scaling, making classification on raw data prone to errors. This paper introduces a novel approach for Multiple Time Series Alignment (MTSA) leveraging Deep Learning techniques. While most existing methods primarily address Multiple Sequence Alignment (MSA) for protein and DNA sequences, there remains a significant gap in alignment methodologies for numerical time series. Additionally, conventional approaches typically focus on pairwise alignment, whereas our proposed method aligns all signals in a multiple manner (all the signals are aligned together at once). This innovation not only enhances alignment efficiency but also significantly improves computational speed. By decomposing into piece-wise linear sections, we introduce varying levels of complexity into the warping function. Additionally, our method ensures the satisfaction of three warping constraints: boundary, monotonicity, and continuity conditions. The utilization of a deep convolutional network allows us to employ a new loss function, addressing some limitations of Dynamic Time Warping (DTW). Experimental results on the UCR Archive 2018, comprising 129 time series datasets, demonstrate that employing our approach to align signals significantly enhances classification accuracy and warping average and also reduces the run time across the majority of these datasets.

### A calibration test for evaluating set-based epistemic uncertainty representations 
[[arxiv](https://arxiv.org/abs/2502.16299)] [[cool](https://papers.cool/arxiv/2502.16299)] [[pdf](https://arxiv.org/pdf/2502.16299)]
> **Authors**: Mira Jürgens,Thomas Mortier,Eyke Hüllermeier,Viktor Bengs,Willem Waegeman
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: The accurate representation of epistemic uncertainty is a challenging yet essential task in machine learning. A widely used representation corresponds to convex sets of probabilistic predictors, also known as credal sets. One popular way of constructing these credal sets is via ensembling or specialized supervised learning methods, where the epistemic uncertainty can be quantified through measures such as the set size or the disagreement among members. In principle, these sets should contain the true data-generating distribution. As a necessary condition for this validity, we adopt the strongest notion of calibration as a proxy. Concretely, we propose a novel statistical test to determine whether there is a convex combination of the set's predictions that is calibrated in distribution. In contrast to previous methods, our framework allows the convex combination to be instance dependent, recognizing that different ensemble members may be better calibrated in different regions of the input space. Moreover, we learn this combination via proper scoring rules, which inherently optimize for calibration. Building on differentiable, kernel-based estimators of calibration errors, we introduce a nonparametric testing procedure and demonstrate the benefits of capturing instance-level variability on of synthetic and real-world experiments.

### TimePFN: Effective Multivariate Time Series Forecasting with Synthetic Data 
[[arxiv](https://arxiv.org/abs/2502.16294)] [[cool](https://papers.cool/arxiv/2502.16294)] [[pdf](https://arxiv.org/pdf/2502.16294)]
> **Authors**: Ege Onur Taga,M. Emrullah Ildiz,Samet Oymak
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: To appear in AAAI-2025 as a conference paper
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: The diversity of time series applications and scarcity of domain-specific data highlight the need for time-series models with strong few-shot learning capabilities. In this work, we propose a novel training scheme and a transformer-based architecture, collectively referred to as TimePFN, for multivariate time-series (MTS) forecasting. TimePFN is based on the concept of Prior-data Fitted Networks (PFN), which aims to approximate Bayesian inference. Our approach consists of (1) generating synthetic MTS data through diverse Gaussian process kernels and the linear coregionalization method, and (2) a novel MTS architecture capable of utilizing both temporal and cross-channel dependencies across all input patches. We evaluate TimePFN on several benchmark datasets and demonstrate that it outperforms the existing state-of-the-art models for MTS forecasting in both zero-shot and few-shot settings. Notably, fine-tuning TimePFN with as few as 500 data points nearly matches full dataset training error, and even 50 data points yield competitive results. We also find that TimePFN exhibits strong univariate forecasting performance, attesting to its generalization ability. Overall, this work unlocks the power of synthetic data priors for MTS forecasting and facilitates strong zero- and few-shot forecasting performance.

### HetFS: A Method for Fast Similarity Search with Ad-hoc Meta-paths on Heterogeneous Information Networks 
[[arxiv](https://arxiv.org/abs/2502.16288)] [[cool](https://papers.cool/arxiv/2502.16288)] [[pdf](https://arxiv.org/pdf/2502.16288)]
> **Authors**: Xuqi Mao,Zhenyi Chen,Zhenying He,Yinan Jing,Kai Zhang,X. Sean Wang
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: ef:World Wide Web Volume 27, article number 66, (2024)
- **标题**: None
- **领域**: 机器学习,信息检索,社交和信息网络
- **Abstract**: Numerous real-world information networks form Heterogeneous Information Networks (HINs) with diverse objects and relations represented as nodes and edges in heterogeneous graphs. Similarity between nodes quantifies how closely two nodes resemble each other, mainly depending on the similarity of the nodes they are connected to, recursively. Users may be interested in only specific types of connections in the similarity definition, represented as meta-paths, i.e., a sequence of node and edge types. Existing Heterogeneous Graph Neural Network (HGNN)-based similarity search methods may accommodate meta-paths, but require retraining for different meta-paths. Conversely, existing path-based similarity search methods may switch flexibly between meta-paths but often suffer from lower accuracy, as they rely solely on path information. This paper proposes HetFS, a Fast Similarity method for ad-hoc queries with user-given meta-paths on Heterogeneous information networks. HetFS provides similarity results based on path information that satisfies the meta-path restriction, as well as node content. Extensive experiments demonstrate the effectiveness and efficiency of HetFS in addressing ad-hoc queries, outperforming state-of-the-art HGNNs and path-based approaches, and showing strong performance in downstream applications, including link prediction, node classification, and clustering.

### MolSpectra: Pre-training 3D Molecular Representation with Multi-modal Energy Spectra 
[[arxiv](https://arxiv.org/abs/2502.16284)] [[cool](https://papers.cool/arxiv/2502.16284)] [[pdf](https://arxiv.org/pdf/2502.16284)]
> **Authors**: Liang Wang,Shaozhen Liu,Yu Rong,Deli Zhao,Qiang Liu,Shu Wu,Liang Wang
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: Accepted by ICLR 2025
- **标题**: None
- **领域**: 机器学习,人工智能,计算工程、金融和科学,化学物理
- **Abstract**: Establishing the relationship between 3D structures and the energy states of molecular systems has proven to be a promising approach for learning 3D molecular representations. However, existing methods are limited to modeling the molecular energy states from classical mechanics. This limitation results in a significant oversight of quantum mechanical effects, such as quantized (discrete) energy level structures, which offer a more accurate estimation of molecular energy and can be experimentally measured through energy spectra. In this paper, we propose to utilize the energy spectra to enhance the pre-training of 3D molecular representations (MolSpectra), thereby infusing the knowledge of quantum mechanics into the molecular representations. Specifically, we propose SpecFormer, a multi-spectrum encoder for encoding molecular spectra via masked patch reconstruction. By further aligning outputs from the 3D encoder and spectrum encoder using a contrastive objective, we enhance the 3D encoder's understanding of molecules. Evaluations on public benchmarks reveal that our pre-trained representations surpass existing methods in predicting molecular properties and modeling dynamics.

### Understanding the Emergence of Multimodal Representation Alignment 
[[arxiv](https://arxiv.org/abs/2502.16282)] [[cool](https://papers.cool/arxiv/2502.16282)] [[pdf](https://arxiv.org/pdf/2502.16282)]
> **Authors**: Megan Tjandrasuwita,Chanakya Ekbote,Liu Ziyin,Paul Pu Liang
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: 21 pages, 22 figures, 3 tables
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Multimodal representation learning is fundamentally about transforming incomparable modalities into comparable representations. While prior research primarily focused on explicitly aligning these representations through targeted learning objectives and model architectures, a recent line of work has found that independently trained unimodal models of increasing scale and performance can become implicitly aligned with each other. These findings raise fundamental questions regarding the emergence of aligned representations in multimodal learning. Specifically: (1) when and why does alignment emerge implicitly? and (2) is alignment a reliable indicator of performance? Through a comprehensive empirical investigation, we demonstrate that both the emergence of alignment and its relationship with task performance depend on several critical data characteristics. These include, but are not necessarily limited to, the degree of similarity between the modalities and the balance between redundant and unique information they provide for the task. Our findings suggest that alignment may not be universally beneficial; rather, its impact on performance varies depending on the dataset and task. These insights can help practitioners determine whether increasing alignment between modalities is advantageous or, in some cases, detrimental to achieving optimal performance. Code is released at https://github.com/MeganTj/multimodal_alignment.

### FHGE: A Fast Heterogeneous Graph Embedding with Ad-hoc Meta-paths 
[[arxiv](https://arxiv.org/abs/2502.16281)] [[cool](https://papers.cool/arxiv/2502.16281)] [[pdf](https://arxiv.org/pdf/2502.16281)]
> **Authors**: Xuqi Mao,Zhenying He,X. Sean Wang
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Graph neural networks (GNNs) have emerged as the state of the art for a variety of graph-related tasks and have been widely used in Heterogeneous Graphs (HetGs), where meta-paths help encode specific semantics between various node types. Despite the revolutionary representation capabilities of existing heterogeneous GNNs (HGNNs) due to their focus on improving the effectiveness of heterogeneity capturing, the huge training costs hinder their practical deployment in real-world scenarios that frequently require handling ad-hoc queries with user-defined meta-paths. To address this, we propose FHGE, a Fast Heterogeneous Graph Embedding designed for efficient, retraining-free generation of meta-path-guided graph embeddings. The key design of the proposed framework is two-fold: segmentation and reconstruction modules. It employs Meta-Path Units (MPUs) to segment the graph into local and global components, enabling swift integration of node embeddings from relevant MPUs during reconstruction and allowing quick adaptation to specific meta-paths. In addition, a dual attention mechanism is applied to enhance semantics capturing. Extensive experiments across diverse datasets demonstrate the effectiveness and efficiency of FHGE in generating meta-path-guided graph embeddings and downstream tasks, such as link prediction and node classification, highlighting its significant advantages for real-time graph analysis in ad-hoc queries.

### Human Preferences in Large Language Model Latent Space: A Technical Analysis on the Reliability of Synthetic Data in Voting Outcome Prediction 
[[arxiv](https://arxiv.org/abs/2502.16280)] [[cool](https://papers.cool/arxiv/2502.16280)] [[pdf](https://arxiv.org/pdf/2502.16280)]
> **Authors**: Sarah Ball,Simeon Allmendinger,Frauke Kreuter,Niklas Kühl
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Generative AI (GenAI) is increasingly used in survey contexts to simulate human preferences. While many research endeavors evaluate the quality of synthetic GenAI data by comparing model-generated responses to gold-standard survey results, fundamental questions about the validity and reliability of using LLMs as substitutes for human respondents remain. Our study provides a technical analysis of how demographic attributes and prompt variations influence latent opinion mappings in large language models (LLMs) and evaluates their suitability for survey-based predictions. Using 14 different models, we find that LLM-generated data fails to replicate the variance observed in real-world human responses, particularly across demographic subgroups. In the political space, persona-to-party mappings exhibit limited differentiation, resulting in synthetic data that lacks the nuanced distribution of opinions found in survey data. Moreover, we show that prompt sensitivity can significantly alter outputs for some models, further undermining the stability and predictiveness of LLM-based simulations. As a key contribution, we adapt a probe-based methodology that reveals how LLMs encode political affiliations in their latent space, exposing the systematic distortions introduced by these models. Our findings highlight critical limitations in AI-generated survey data, urging caution in its use for public opinion research, social science experimentation, and computational behavioral modeling.

### PLS-based approach for fair representation learning 
[[arxiv](https://arxiv.org/abs/2502.16263)] [[cool](https://papers.cool/arxiv/2502.16263)] [[pdf](https://arxiv.org/pdf/2502.16263)]
> **Authors**: Elena M. De-Diego,Adrián Perez-Suay,Paula Gordaliza,Jean-Michel Loubes
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机与社会,统计理论,机器学习
- **Abstract**: We revisit the problem of fair representation learning by proposing Fair Partial Least Squares (PLS) components. PLS is widely used in statistics to efficiently reduce the dimension of the data by providing representation tailored for the prediction. We propose a novel method to incorporate fairness constraints in the construction of PLS components. This new algorithm provides a feasible way to construct such features both in the linear and the non linear case using kernel embeddings. The efficiency of our method is evaluated on different datasets, and we prove its superiority with respect to standard fair PCA method.

### Linear Attention for Efficient Bidirectional Sequence Modeling 
[[arxiv](https://arxiv.org/abs/2502.16249)] [[cool](https://papers.cool/arxiv/2502.16249)] [[pdf](https://arxiv.org/pdf/2502.16249)]
> **Authors**: Arshia Afzal,Elias Abad Rocamora,Leyla Naz Candogan,Pol Puigdemont,Francesco Tonin,Yongtao Wu,Mahsa Shoaran,Volkan Cevher
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Transformers with linear attention enable fast and parallel training. Moreover, they can be formulated as Recurrent Neural Networks (RNNs), for efficient linear-time inference. While extensively evaluated in causal sequence modeling, they have yet to be extended to the bidirectional setting. This work introduces the LION framework, establishing new theoretical foundations for linear transformers in bidirectional sequence modeling. LION constructs a bidirectional RNN equivalent to full Linear Attention. This extends the benefits of linear transformers: parallel training, and efficient inference, into the bidirectional setting. Using LION, we cast three linear transformers to their bidirectional form: LION-LIT, the bidirectional variant corresponding to (Katharopoulos et al., 2020); LION-D, extending RetNet (Sun et al., 2023); and LION-S, a linear transformer with a stable selective mask inspired by selectivity of SSMs (Dao & Gu, 2024). Replacing the attention block with LION (-LIT, -D, -S) achieves performance on bidirectional tasks that approaches that of Transformers and State-Space Models (SSMs), while delivering significant improvements in training speed. Our implementation is available in http://github.com/LIONS-EPFL/LION.

### Graph Self-Supervised Learning with Learnable Structural and Positional Encodings 
[[arxiv](https://arxiv.org/abs/2502.16233)] [[cool](https://papers.cool/arxiv/2502.16233)] [[pdf](https://arxiv.org/pdf/2502.16233)]
> **Authors**: Asiri Wijesinghe,Hao Zhu,Piotr Koniusz
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: This paper is accepted by The World Wide Web Conference (WWW) 2025
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Traditional Graph Self-Supervised Learning (GSSL) struggles to capture complex structural properties well. This limitation stems from two main factors: (1) the inadequacy of conventional Graph Neural Networks (GNNs) in representing sophisticated topological features, and (2) the focus of self-supervised learning solely on final graph representations. To address these issues, we introduce \emph{GenHopNet}, a GNN framework that integrates a $k$-hop message-passing scheme, enhancing its ability to capture local structural information without explicit substructure extraction. We theoretically demonstrate that \emph{GenHopNet} surpasses the expressiveness of the classical Weisfeiler-Lehman (WL) test for graph isomorphism. Furthermore, we propose a structural- and positional-aware GSSL framework that incorporates topological information throughout the learning process. This approach enables the learning of representations that are both sensitive to graph topology and invariant to specific structural and feature augmentations. Comprehensive experiments on graph classification datasets, including those designed to test structural sensitivity, show that our method consistently outperforms the existing approaches and maintains computational efficiency. Our work significantly advances GSSL's capability in distinguishing graphs with similar local structures but different global topologies.

### Co-evolution-based Metal-binding Residue Prediction with Graph Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.16189)] [[cool](https://papers.cool/arxiv/2502.16189)] [[pdf](https://arxiv.org/pdf/2502.16189)]
> **Authors**: Sayedmohammadreza Rastegari,Sina Tabakhi,Xianyuan Liu,Wei Sang,Haiping Lu
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: 7 pages, 3 figures
- **标题**: None
- **领域**: 机器学习,材料科学,生物分子,定量方法
- **Abstract**: In computational structural biology, predicting metal-binding sites and their corresponding metal types is challenging due to the complexity of protein structures and interactions. Conventional sequence- and structure-based prediction approaches cannot capture the complex evolutionary relationships driving these interactions to facilitate understanding, while recent co-evolution-based approaches do not fully consider the entire structure of the co-evolved residue network. In this paper, we introduce MBGNN (Metal-Binding Graph Neural Network) that utilizes the entire co-evolved residue network and effectively captures the complex dependencies within protein structures via graph neural networks to enhance the prediction of co-evolved metal-binding residues and their associated metal types. Experimental results on a public dataset show that MBGNN outperforms existing co-evolution-based metal-binding prediction methods, and it is also competitive against recent sequence-based methods, showing the potential of integrating co-evolutionary insights with advanced machine learning to deepen our understanding of protein-metal interactions. The MBGNN code is publicly available at https://github.com/SRastegari/MBGNN.

### Maybe I Should Not Answer That, but... Do LLMs Understand The Safety of Their Inputs? 
[[arxiv](https://arxiv.org/abs/2502.16174)] [[cool](https://papers.cool/arxiv/2502.16174)] [[pdf](https://arxiv.org/pdf/2502.16174)]
> **Authors**: Maciej Chrabąszcz,Filip Szatkowski,Bartosz Wójcik,Jan Dubiński,Tomasz Trzciński
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,密码学和安全
- **Abstract**: Ensuring the safety of the Large Language Model (LLM) is critical, but currently used methods in most cases sacrifice the model performance to obtain increased safety or perform poorly on data outside of their adaptation distribution. We investigate existing methods for such generalization and find them insufficient. Surprisingly, while even plain LLMs recognize unsafe prompts, they may still generate unsafe responses. To avoid performance degradation and preserve safe performance, we advocate for a two-step framework, where we first identify unsafe prompts via a lightweight classifier, and apply a "safe" model only to such prompts. In particular, we explore the design of the safety detector in more detail, investigating the use of different classifier architectures and prompting techniques. Interestingly, we find that the final hidden state for the last token is enough to provide robust performance, minimizing false positives on benign data while performing well on malicious prompt detection. Additionally, we show that classifiers trained on the representations from different model layers perform comparably on the latest model layers, indicating that safety representation is present in the LLMs' hidden states at most model stages. Our work is a step towards efficient, representation-based safety mechanisms for LLMs.

### Destroy and Repair Using Hyper Graphs for Routing 
[[arxiv](https://arxiv.org/abs/2502.16170)] [[cool](https://papers.cool/arxiv/2502.16170)] [[pdf](https://arxiv.org/pdf/2502.16170)]
> **Authors**: Ke Li,Fei Liu,Zhengkun Wang,Qingfu Zhang
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: Accepted at AAAI2025
- **标题**: None
- **领域**: 机器学习,人工智能,神经和进化计算
- **Abstract**: Recent advancements in Neural Combinatorial Optimization (NCO) have shown promise in solving routing problems like the Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP) without handcrafted designs. Research in this domain has explored two primary categories of methods: iterative and non-iterative. While non-iterative methods struggle to generate near-optimal solutions directly, iterative methods simplify the task by learning local search steps. However, existing iterative methods are often limited by restricted neighborhood searches, leading to suboptimal results. To address this limitation, we propose a novel approach that extends the search to larger neighborhoods by learning a destroy-and-repair strategy. Specifically, we introduce a Destroy-and-Repair framework based on Hyper-Graphs (DRHG). This framework reduces consecutive intact edges to hyper-edges, allowing the model to pay more attention to the destroyed part and decrease the complexity of encoding all nodes. Experiments demonstrate that DRHG achieves stateof-the-art performance on TSP with up to 10,000 nodes and shows strong generalization to real-world TSPLib and CVRPLib problems.

### Advanced Text Analytics -- Graph Neural Network for Fake News Detection in Social Media 
[[arxiv](https://arxiv.org/abs/2502.16157)] [[cool](https://papers.cool/arxiv/2502.16157)] [[pdf](https://arxiv.org/pdf/2502.16157)]
> **Authors**: Anantram Patel,Vijay Kumar Sutrakar
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Traditional Graph Neural Network (GNN) approaches for fake news detection (FND) often depend on auxiliary, non-textual data such as user interaction histories or content dissemination patterns. However, these data sources are not always accessible, limiting the effectiveness and applicability of such methods. Additionally, existing models frequently struggle to capture the detailed and intricate relationships within textual information, reducing their overall accuracy. In order to address these challenges Advanced Text Analysis Graph Neural Network (ATA-GNN) is proposed in this paper. The proposed model is designed to operate solely on textual data. ATA-GNN employs innovative topic modelling (clustering) techniques to identify typical words for each topic, leveraging multiple clustering dimensions to achieve a comprehensive semantic understanding of the text. This multi-layered design enables the model to uncover intricate textual patterns while contextualizing them within a broader semantic framework, significantly enhancing its interpretative capabilities. Extensive evaluations on widely used benchmark datasets demonstrate that ATA-GNN surpasses the performance of current GNN-based FND methods. These findings validate the potential of integrating advanced text clustering within GNN architectures to achieve more reliable and text-focused detection solutions.

### DUPRE: Data Utility Prediction for Efficient Data Valuation 
[[arxiv](https://arxiv.org/abs/2502.16152)] [[cool](https://papers.cool/arxiv/2502.16152)] [[pdf](https://arxiv.org/pdf/2502.16152)]
> **Authors**: Kieu Thao Nguyen Pham,Rachael Hwee Ling Sim,Quoc Phong Nguyen,See Kiong Ng,Bryan Kian Hsiang Low
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: 16 pages, 7 figures, the paper got accepted AAMAS 2025
- **标题**: None
- **领域**: 机器学习,计算机科学与博弈论
- **Abstract**: Data valuation is increasingly used in machine learning (ML) to decide the fair compensation for data owners and identify valuable or harmful data for improving ML models. Cooperative game theory-based data valuation, such as Data Shapley, requires evaluating the data utility (e.g., validation accuracy) and retraining the ML model for multiple data subsets. While most existing works on efficient estimation of the Shapley values have focused on reducing the number of subsets to evaluate, our framework, \texttt{DUPRE}, takes an alternative yet complementary approach that reduces the cost per subset evaluation by predicting data utilities instead of evaluating them by model retraining. Specifically, given the evaluated data utilities of some data subsets, \texttt{DUPRE} fits a \emph{Gaussian process} (GP) regression model to predict the utility of every other data subset. Our key contribution lies in the design of our GP kernel based on the sliced Wasserstein distance between empirical data distributions. In particular, we show that the kernel is valid and positive semi-definite, encodes prior knowledge of similarities between different data subsets, and can be efficiently computed. We empirically verify that \texttt{DUPRE} introduces low prediction error and speeds up data valuation for various ML models, datasets, and utility functions.

### An Improved Deep Learning Model for Word Embeddings Based Clustering for Large Text Datasets 
[[arxiv](https://arxiv.org/abs/2502.16139)] [[cool](https://papers.cool/arxiv/2502.16139)] [[pdf](https://arxiv.org/pdf/2502.16139)]
> **Authors**: Vijay Kumar Sutrakar,Nikhil Mogre
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: In this paper, an improved clustering technique for large textual datasets by leveraging fine-tuned word embeddings is presented. WEClustering technique is used as the base model. WEClustering model is fur-ther improvements incorporating fine-tuning contextual embeddings, advanced dimensionality reduction methods, and optimization of clustering algorithms. Experimental results on benchmark datasets demon-strate significant improvements in clustering metrics such as silhouette score, purity, and adjusted rand index (ARI). An increase of 45% and 67% of median silhouette score is reported for the proposed WE-Clustering_K++ (based on K-means) and WEClustering_A++ (based on Agglomerative models), respec-tively. The proposed technique will help to bridge the gap between semantic understanding and statistical robustness for large-scale text-mining tasks.

### Heterogeneous Multi-Agent Bandits with Parsimonious Hints 
[[arxiv](https://arxiv.org/abs/2502.16128)] [[cool](https://papers.cool/arxiv/2502.16128)] [[pdf](https://arxiv.org/pdf/2502.16128)]
> **Authors**: Amirmahdi Mirfakhar,Xuchuang Wang,Jinhang Zuo,Yair Zick,Mohammad Hajiesmaili
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: Accepted at AAAI-2025
- **标题**: None
- **领域**: 机器学习,人工智能,计算机科学与博弈论,多代理系统
- **Abstract**: We study a hinted heterogeneous multi-agent multi-armed bandits problem (HMA2B), where agents can query low-cost observations (hints) in addition to pulling arms. In this framework, each of the $M$ agents has a unique reward distribution over $K$ arms, and in $T$ rounds, they can observe the reward of the arm they pull only if no other agent pulls that arm. The goal is to maximize the total utility by querying the minimal necessary hints without pulling arms, achieving time-independent regret. We study HMA2B in both centralized and decentralized setups. Our main centralized algorithm, GP-HCLA, which is an extension of HCLA, uses a central decision-maker for arm-pulling and hint queries, achieving $O(M^4K)$ regret with $O(MK\log T)$ adaptive hints. In decentralized setups, we propose two algorithms, HD-ETC and EBHD-ETC, that allow agents to choose actions independently through collision-based communication and query hints uniformly until stopping, yielding $O(M^3K^2)$ regret with $O(M^3K\log T)$ hints, where the former requires knowledge of the minimum gap and the latter does not. Finally, we establish lower bounds to prove the optimality of our results and verify them through numerical simulations.

### FedOC: Optimizing Global Prototypes with Orthogonality Constraints for Enhancing Embeddings Separation in Heterogeneous Federated Learning 
[[arxiv](https://arxiv.org/abs/2502.16119)] [[cool](https://papers.cool/arxiv/2502.16119)] [[pdf](https://arxiv.org/pdf/2502.16119)]
> **Authors**: Fucheng Guo,Zeyu Luan,Qing Li,Dan Zhao,Yong Jiang
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,分布式、并行和集群计算
- **Abstract**: Federated Learning (FL) has emerged as an essential framework for distributed machine learning, especially with its potential for privacy-preserving data processing. However, existing FL frameworks struggle to address statistical and model heterogeneity, which severely impacts model performance. While Heterogeneous Federated Learning (HtFL) introduces prototype-based strategies to address the challenges, current approaches face limitations in achieving optimal separation of prototypes. This paper presents FedOC, a novel HtFL algorithm designed to improve global prototype separation through orthogonality constraints, which not only increase intra-class prototype similarity but also significantly expand the inter-class angular separation. With the guidance of the global prototype, each client keeps its embeddings aligned with the corresponding prototype in the feature space, promoting directional independence that integrates seamlessly with the cross-entropy (CE) loss. We provide theoretical proof of FedOC's convergence under non-convex conditions. Extensive experiments demonstrate that FedOC outperforms seven state-of-the-art baselines, achieving up to a 10.12% accuracy improvement in both statistical and model heterogeneity settings.

### Integrating Weather Station Data and Radar for Precipitation Nowcasting: SmaAt-fUsion and SmaAt-Krige-GNet 
[[arxiv](https://arxiv.org/abs/2502.16116)] [[cool](https://papers.cool/arxiv/2502.16116)] [[pdf](https://arxiv.org/pdf/2502.16116)]
> **Authors**: Aleksej Cornelissen,Jie Shi,Siamak Mehrkanoon
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: 11 pages, 7 figures
- **标题**: None
- **领域**: 机器学习,大气和海洋物理
- **Abstract**: In recent years, data-driven, deep learning-based approaches for precipitation nowcasting have attracted significant attention, showing promising results. However, many existing models fail to fully exploit the extensive atmospheric information available, relying primarily on precipitation data alone. This study introduces two novel deep learning architectures, SmaAt-fUsion and SmaAt-Krige-GNet, specifically designed to enhance precipitation nowcasting by integrating multi-variable weather station data with radar datasets. By leveraging additional meteorological information, these models improve representation learning in the latent space, resulting in enhanced nowcasting performance. The SmaAt-fUsion model extends the SmaAt-UNet framework by incorporating weather station data through a convolutional layer, integrating it into the bottleneck of the network. Conversely, the SmaAt-Krige-GNet model combines precipitation maps with weather station data processed using Kriging, a geo-statistical interpolation method, to generate variable-specific maps. These maps are then utilized in a dual-encoder architecture based on SmaAt-GNet, allowing multi-level data integration. Experimental evaluations were conducted using four years (2016--2019) of weather station and precipitation radar data from the Netherlands. Results demonstrate that SmaAt-Krige-GNet outperforms the standard SmaAt-UNet, which relies solely on precipitation radar data, in low precipitation scenarios, while SmaAt-fUsion surpasses SmaAt-UNet in both low and high precipitation scenarios. This highlights the potential of incorporating discrete weather station data to enhance the performance of deep learning-based weather nowcasting models.

### Detecting OOD Samples via Optimal Transport Scoring Function 
[[arxiv](https://arxiv.org/abs/2502.16115)] [[cool](https://papers.cool/arxiv/2502.16115)] [[pdf](https://arxiv.org/pdf/2502.16115)]
> **Authors**: Heng Gao,Zhuolin He,Jian Pu
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2025
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别,机器学习
- **Abstract**: To deploy machine learning models in the real world, researchers have proposed many OOD detection algorithms to help models identify unknown samples during the inference phase and prevent them from making untrustworthy predictions. Unlike methods that rely on extra data for outlier exposure training, post hoc methods detect Out-of-Distribution (OOD) samples by developing scoring functions, which are model agnostic and do not require additional training. However, previous post hoc methods may fail to capture the geometric cues embedded in network representations. Thus, in this study, we propose a novel score function based on the optimal transport theory, named OTOD, for OOD detection. We utilize information from features, logits, and the softmax probability space to calculate the OOD score for each test sample. Our experiments show that combining this information can boost the performance of OTOD with a certain margin. Experiments on the CIFAR-10 and CIFAR-100 benchmarks demonstrate the superior performance of our method. Notably, OTOD outperforms the state-of-the-art method GEN by 7.19% in the mean FPR@95 on the CIFAR-10 benchmark using ResNet-18 as the backbone, and by 12.51% in the mean FPR@95 using WideResNet-28 as the backbone. In addition, we provide theoretical guarantees for OTOD. The code is available in https://github.com/HengGao12/OTOD.

### Set a Thief to Catch a Thief: Combating Label Noise through Noisy Meta Learning 
[[arxiv](https://arxiv.org/abs/2502.16104)] [[cool](https://papers.cool/arxiv/2502.16104)] [[pdf](https://arxiv.org/pdf/2502.16104)]
> **Authors**: Hanxuan Wang,Na Lu,Xueying Zhao,Yuxuan Yan,Kaipeng Ma,Kwoh Chee Keong,Gustavo Carneiro
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: Learning from noisy labels (LNL) aims to train high-performance deep models using noisy datasets. Meta learning based label correction methods have demonstrated remarkable performance in LNL by designing various meta label rectification tasks. However, extra clean validation set is a prerequisite for these methods to perform label correction, requiring extra labor and greatly limiting their practicality. To tackle this issue, we propose a novel noisy meta label correction framework STCT, which counterintuitively uses noisy data to correct label noise, borrowing the spirit in the saying ``Set a Thief to Catch a Thief''. The core idea of STCT is to leverage noisy data which is i.i.d. with the training data as a validation set to evaluate model performance and perform label correction in a meta learning framework, eliminating the need for extra clean data. By decoupling the complex bi-level optimization in meta learning into representation learning and label correction, STCT is solved through an alternating training strategy between noisy meta correction and semi-supervised representation learning. Extensive experiments on synthetic and real-world datasets demonstrate the outstanding performance of STCT, particularly in high noise rate scenarios. STCT achieves 96.9% label correction and 95.2% classification performance on CIFAR-10 with 80% symmetric noise, significantly surpassing the current state-of-the-art.

### Privacy-Aware Joint DNN Model Deployment and Partition Optimization for Delay-Efficient Collaborative Edge Inference 
[[arxiv](https://arxiv.org/abs/2502.16091)] [[cool](https://papers.cool/arxiv/2502.16091)] [[pdf](https://arxiv.org/pdf/2502.16091)]
> **Authors**: Zhipeng Cheng,Xiaoyu Xia,Hong Wang,Minghui Liwang,Ning Chen,Xuwei Fan,Xianbin Wang
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,密码学和安全,网络和互联网架构
- **Abstract**: Edge inference (EI) is a key solution to address the growing challenges of delayed response times, limited scalability, and privacy concerns in cloud-based Deep Neural Network (DNN) inference. However, deploying DNN models on resource-constrained edge devices faces more severe challenges, such as model storage limitations, dynamic service requests, and privacy risks. This paper proposes a novel framework for privacy-aware joint DNN model deployment and partition optimization to minimize long-term average inference delay under resource and privacy constraints. Specifically, the problem is formulated as a complex optimization problem considering model deployment, user-server association, and model partition strategies. To handle the NP-hardness and future uncertainties, a Lyapunov-based approach is introduced to transform the long-term optimization into a single-time-slot problem, ensuring system performance. Additionally, a coalition formation game model is proposed for edge server association, and a greedy-based algorithm is developed for model deployment within each coalition to efficiently solve the problem. Extensive simulations show that the proposed algorithms effectively reduce inference delay while satisfying privacy constraints, outperforming baseline approaches in various scenarios.

### Category-free Out-of-Distribution Node Detection with Feature Resonance 
[[arxiv](https://arxiv.org/abs/2502.16076)] [[cool](https://papers.cool/arxiv/2502.16076)] [[pdf](https://arxiv.org/pdf/2502.16076)]
> **Authors**: Shenzhi Yang,Junbo Zhao,Shouqing Yang,Yixuan Li,Dingyu Yang,Xiaofang Zhang,Haobo Wang
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Detecting out-of-distribution (OOD) nodes in the graph-based machine-learning field is challenging, particularly when in-distribution (ID) node multi-category labels are unavailable. Thus, we focus on feature space rather than label space and find that, ideally, during the optimization of known ID samples, unknown ID samples undergo more significant representation changes than OOD samples, even if the model is trained to fit random targets, which we called the Feature Resonance phenomenon. The rationale behind it is that even without gold labels, the local manifold may still exhibit smooth resonance. Based on this, we further develop a novel graph OOD framework, dubbed Resonance-based Separation and Learning (RSL), which comprises two core modules: (i) a more practical micro-level proxy of feature resonance that measures the movement of feature vectors in one training step. (ii) integrate with synthetic OOD nodes strategy to train an effective OOD classifier. Theoretically, we derive an error bound showing the superior separability of OOD nodes during the resonance period. Empirically, RSL achieves state-of-the-art performance, reducing the FPR95 metric by an average of 18.51% across five real-world datasets.

### Implicit Bias of Gradient Descent for Non-Homogeneous Deep Networks 
[[arxiv](https://arxiv.org/abs/2502.16075)] [[cool](https://papers.cool/arxiv/2502.16075)] [[pdf](https://arxiv.org/pdf/2502.16075)]
> **Authors**: Yuhang Cai,Kangjie Zhou,Jingfeng Wu,Song Mei,Michael Lindsey,Peter L. Bartlett
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 96 pages
- **标题**: None
- **领域**: 机器学习,优化与控制,机器学习
- **Abstract**: We establish the asymptotic implicit bias of gradient descent (GD) for generic non-homogeneous deep networks under exponential loss. Specifically, we characterize three key properties of GD iterates starting from a sufficiently small empirical risk, where the threshold is determined by a measure of the network's non-homogeneity. First, we show that a normalized margin induced by the GD iterates increases nearly monotonically. Second, we prove that while the norm of the GD iterates diverges to infinity, the iterates themselves converge in direction. Finally, we establish that this directional limit satisfies the Karush-Kuhn-Tucker (KKT) conditions of a margin maximization problem. Prior works on implicit bias have focused exclusively on homogeneous networks; in contrast, our results apply to a broad class of non-homogeneous networks satisfying a mild near-homogeneity condition. In particular, our results apply to networks with residual connections and non-homogeneous activation functions, thereby resolving an open problem posed by Ji and Telgarsky (2020).

### Single Domain Generalization with Model-aware Parametric Batch-wise Mixup 
[[arxiv](https://arxiv.org/abs/2502.16064)] [[cool](https://papers.cool/arxiv/2502.16064)] [[pdf](https://arxiv.org/pdf/2502.16064)]
> **Authors**: Marzi Heidari,Yuhong Guo
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Single Domain Generalization (SDG) remains a formidable challenge in the field of machine learning, particularly when models are deployed in environments that differ significantly from their training domains. In this paper, we propose a novel data augmentation approach, named as Model-aware Parametric Batch-wise Mixup (MPBM), to tackle the challenge of SDG. MPBM deploys adversarial queries generated with stochastic gradient Langevin dynamics, and produces model-aware augmenting instances with a parametric batch-wise mixup generator network that is carefully designed through an innovative attention mechanism. By exploiting inter-feature correlations, the parameterized mixup generator introduces additional versatility in combining features across a batch of instances, thereby enhancing the capacity to generate highly adaptive and informative synthetic instances for specific queries. The synthetic data produced by this adaptable generator network, guided by informative queries, is expected to significantly enrich the representation space covered by the original training dataset and subsequently enhance the prediction model's generalizability across diverse and previously unseen domains. To prevent excessive deviation from the training data, we further incorporate a real-data alignment-based adversarial loss into the learning process of MPBM, regularizing any tendencies toward undesirable expansions. We conduct extensive experiments on several benchmark datasets. The empirical results demonstrate that by augmenting the training set with informative synthesis data, our proposed MPBM method achieves the state-of-the-art performance for single domain generalization.

### Single-Channel EEG Tokenization Through Time-Frequency Modeling 
[[arxiv](https://arxiv.org/abs/2502.16060)] [[cool](https://papers.cool/arxiv/2502.16060)] [[pdf](https://arxiv.org/pdf/2502.16060)]
> **Authors**: Jathurshan Pradeepkumar,Xihao Piao,Zheng Chen,Jimeng Sun
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,信号处理
- **Abstract**: We introduce TFM-Tokenizer, a novel tokenization framework tailored for EEG analysis that transforms continuous, noisy brain signals into a sequence of discrete, well-represented tokens for various EEG tasks. Conventional approaches typically rely on continuous embeddings and inter-channel dependencies, which are limited in capturing inherent EEG features such as temporally unpredictable patterns and diverse oscillatory waveforms. In contrast, we hypothesize that critical time-frequency features can be effectively captured from a single channel. By learning tokens that encapsulate these intrinsic patterns within a single channel, our approach yields a scalable tokenizer adaptable across diverse EEG settings. We integrate the TFM-Tokenizer with a transformer-based TFM-Encoder, leveraging established pretraining techniques from natural language processing, such as masked token prediction, followed by downstream fine-tuning for various EEG tasks. Experiments across four EEG datasets show that TFM-Token outperforms state-of-the-art methods. On TUEV, our approach improves balanced accuracy and Cohen's Kappa by 5% over baselines. Comprehensive analysis of the learned tokens demonstrates their ability to capture class-distinctive features, enhance frequency representation, and ability to encode time-frequency motifs into distinct tokens, improving interpretability.

### Since Faithfulness Fails: The Performance Limits of Neural Causal Discovery 
[[arxiv](https://arxiv.org/abs/2502.16056)] [[cool](https://papers.cool/arxiv/2502.16056)] [[pdf](https://arxiv.org/pdf/2502.16056)]
> **Authors**: Mateusz Olko,Mateusz Gajewski,Joanna Wojciechowska,Mikołaj Morzy,Piotr Sankowski,Piotr Miłoś
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 19 pages, 12 figures
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Neural causal discovery methods have recently improved in terms of scalability and computational efficiency. However, our systematic evaluation highlights significant room for improvement in their accuracy when uncovering causal structures. We identify a fundamental limitation: neural networks cannot reliably distinguish between existing and non-existing causal relationships in the finite sample regime. Our experiments reveal that neural networks, as used in contemporary causal discovery approaches, lack the precision needed to recover ground-truth graphs, even for small graphs and relatively large sample sizes. Furthermore, we identify the faithfulness property as a critical bottleneck: (i) it is likely to be violated across any reasonable dataset size range, and (ii) its violation directly undermines the performance of neural discovery methods. These findings lead us to conclude that progress within the current paradigm is fundamentally constrained, necessitating a paradigm shift in this domain.

### MedForge: Building Medical Foundation Models Like Open Source Software Development 
[[arxiv](https://arxiv.org/abs/2502.16055)] [[cool](https://papers.cool/arxiv/2502.16055)] [[pdf](https://arxiv.org/pdf/2502.16055)]
> **Authors**: Zheling Tan,Kexin Ding,Jin Gao,Mu Zhou,Dimitris Metaxas,Shaoting Zhang,Dequan Wang
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,密码学和安全,计算机视觉和模式识别,软件工程
- **Abstract**: Foundational models (FMs) have made significant strides in the healthcare domain. Yet the data silo challenge and privacy concern remain in healthcare systems, hindering safe medical data sharing and collaborative model development among institutions. The collection and curation of scalable clinical datasets increasingly become the bottleneck for training strong FMs. In this study, we propose Medical Foundation Models Merging (MedForge), a cooperative framework enabling a community-driven medical foundation model development, meanwhile preventing the information leakage of raw patient data and mitigating synchronization model development issues across clinical institutions. MedForge offers a bottom-up model construction mechanism by flexibly merging task-specific Low-Rank Adaptation (LoRA) modules, which can adapt to downstream tasks while retaining original model parameters. Through an asynchronous LoRA module integration scheme, the resulting composite model can progressively enhance its comprehensive performance on various clinical tasks. MedForge shows strong performance on multiple clinical datasets (e.g., breast cancer, lung cancer, and colon cancer) collected from different institutions. Our major findings highlight the value of collaborative foundation models in advancing multi-center clinical collaboration effectively and cohesively. Our code is publicly available at https://github.com/TanZheling/MedForge.

### Quasi Zigzag Persistence: A Topological Framework for Analyzing Time-Varying Data 
[[arxiv](https://arxiv.org/abs/2502.16049)] [[cool](https://papers.cool/arxiv/2502.16049)] [[pdf](https://arxiv.org/pdf/2502.16049)]
> **Authors**: Tamal K. Dey,Shreyas N. Samaga
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,代数拓扑
- **Abstract**: In this paper, we propose Quasi Zigzag Persistent Homology (QZPH) as a framework for analyzing time-varying data by integrating multiparameter persistence and zigzag persistence. To this end, we introduce a stable topological invariant that captures both static and dynamic features at different scales. We present an algorithm to compute this invariant efficiently. We show that it enhances the machine learning models when applied to tasks such as sleep-stage detection, demonstrating its effectiveness in capturing the evolving patterns in time-evolving datasets.

### Hierarchical Residuals Exploit Brain-Inspired Compositionality 
[[arxiv](https://arxiv.org/abs/2502.16003)] [[cool](https://papers.cool/arxiv/2502.16003)] [[pdf](https://arxiv.org/pdf/2502.16003)]
> **Authors**: Francisco M. López,Jochen Triesch
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Accepted at ESANN 2025
- **标题**: None
- **领域**: 机器学习,人工智能,神经和进化计算
- **Abstract**: We present Hierarchical Residual Networks (HiResNets), deep convolutional neural networks with long-range residual connections between layers at different hierarchical levels. HiResNets draw inspiration on the organization of the mammalian brain by replicating the direct connections from subcortical areas to the entire cortical hierarchy. We show that the inclusion of hierarchical residuals in several architectures, including ResNets, results in a boost in accuracy and faster learning. A detailed analysis of our models reveals that they perform hierarchical compositionality by learning feature maps relative to the compressed representations provided by the skip connections.

### News Sentiment as a Predictor for American Domestic Migration 
[[arxiv](https://arxiv.org/abs/2502.15998)] [[cool](https://papers.cool/arxiv/2502.15998)] [[pdf](https://arxiv.org/pdf/2502.15998)]
> **Authors**: Benjamin Lane,Simeon Sayer
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 8 pages, 3 figures
- **标题**: None
- **领域**: 机器学习,计算机与社会
- **Abstract**: This paper goes into depth on the effect that US News Sentiment from national newspapers has on US interstate migration trends. Through harnessing data from the New York Times between 2010 and 2020, an average sentiment score was calculated, allowing for data to be entered into a neural network. Then a logistic regression model was used to predict interstate migration. The results indicate the model was highly accurate as the mean margin of error was +/- 900 citizens. The predictions from the model were compared with the US Census data from 2010 to 2020 that was used to train the model. Since the input for the model was not exposed to any migration data, the model clearly demonstrated that its results were drawn from sentiment data alone. These findings are significant as they indicate that the role of the press could be used as a predictor for domestic migration which can help the government and businesses understand better what is influencing people to move to certain places.

### Human Guided Learning of Transparent Regression Models 
[[arxiv](https://arxiv.org/abs/2502.15992)] [[cool](https://papers.cool/arxiv/2502.15992)] [[pdf](https://arxiv.org/pdf/2502.15992)]
> **Authors**: Lukas Pensel,Stefan Kramer
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We present a human-in-the-loop (HIL) approach to permutation regression, the novel task of predicting a continuous value for a given ordering of items. The model is a gradient boosted regression model that incorporates simple human-understandable constraints of the form x < y, i.e. item x has to be before item y, as binary features. The approach, HuGuR (Human Guided Regression), lets a human explore the search space of such transparent regression models. Interacting with HuGuR, users can add, remove, and refine order constraints interactively, while the coefficients are calculated on the fly. We evaluate HuGuR in a user study and compare the performance of user-built models with multiple baselines on 9 data sets. The results show that the user-built models outperform the compared methods on small data sets and in general perform on par with the other methods, while being in principle understandable for humans. On larger datasets from the same domain, machine-induced models begin to outperform the user-built models. Further work will study the trust users have in models when constructed by themselves and how the scheme can be transferred to other pattern domains, such as strings, sequences, trees, or graphs.

### Mean-Shift Distillation for Diffusion Mode Seeking 
[[arxiv](https://arxiv.org/abs/2502.15989)] [[cool](https://papers.cool/arxiv/2502.15989)] [[pdf](https://arxiv.org/pdf/2502.15989)]
> **Authors**: Vikas Thamizharasan,Nikitas Chatzis,Iliyan Georgiev,Matthew Fisher,Difan Liu,Nanxuan Zhao,Evangelos Kalogerakis,Michal Lukac
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 12 pages, 8 figures
- **标题**: None
- **领域**: 机器学习,图形
- **Abstract**: We present mean-shift distillation, a novel diffusion distillation technique that provides a provably good proxy for the gradient of the diffusion output distribution. This is derived directly from mean-shift mode seeking on the distribution, and we show that its extrema are aligned with the modes. We further derive an efficient product distribution sampling procedure to evaluate the gradient. Our method is formulated as a drop-in replacement for score distillation sampling (SDS), requiring neither model retraining nor extensive modification of the sampling procedure. We show that it exhibits superior mode alignment as well as improved convergence in both synthetic and practical setups, yielding higher-fidelity results when applied to both text-to-image and text-to-3D applications with Stable Diffusion.

### Near Optimal Decision Trees in a SPLIT Second 
[[arxiv](https://arxiv.org/abs/2502.15988)] [[cool](https://papers.cool/arxiv/2502.15988)] [[pdf](https://arxiv.org/pdf/2502.15988)]
> **Authors**: Varun Babbar,Hayden McTavish,Cynthia Rudin,Margo Seltzer
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Currently under review
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Decision tree optimization is fundamental to interpretable machine learning. The most popular approach is to greedily search for the best feature at every decision point, which is fast but provably suboptimal. Recent approaches find the global optimum using branch and bound with dynamic programming, showing substantial improvements in accuracy and sparsity at great cost to scalability. An ideal solution would have the accuracy of an optimal method and the scalability of a greedy method. We introduce a family of algorithms called SPLIT (SParse Lookahead for Interpretable Trees) that moves us significantly forward in achieving this ideal balance. We demonstrate that not all sub-problems need to be solved to optimality to find high quality trees; greediness suffices near the leaves. Since each depth adds an exponential number of possible trees, this change makes our algorithms orders of magnitude faster than existing optimal methods, with negligible loss in performance. We extend this algorithm to allow scalable computation of sets of near-optimal trees (i.e., the Rashomon set).

### CoRe: Coherency Regularization for Hierarchical Time Series 
[[arxiv](https://arxiv.org/abs/2502.15983)] [[cool](https://papers.cool/arxiv/2502.15983)] [[pdf](https://arxiv.org/pdf/2502.15983)]
> **Authors**: Rares Cristian,Pavithra Harhsa,Georgia Perakis,Brian Quanz
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Hierarchical time series forecasting presents unique challenges, particularly when dealing with noisy data that may not perfectly adhere to aggregation constraints. This paper introduces a novel approach to soft coherency in hierarchical time series forecasting using neural networks. We present a network coherency regularization method, which we denote as CoRe (Coherency Regularization), a technique that trains neural networks to produce forecasts that are inherently coherent across hierarchies, without strictly enforcing aggregation constraints. Our method offers several key advantages. (1) It provides theoretical guarantees on the coherency of forecasts, even for out-of-sample data. (2) It is adaptable to scenarios where data may contain errors or missing values, making it more robust than strict coherency methods. (3) It can be easily integrated into existing neural network architectures for time series forecasting. We demonstrate the effectiveness of our approach on multiple benchmark datasets, comparing it against state-of-the-art methods in both coherent and noisy data scenarios. Additionally, our method can be used within existing generative probabilistic forecasting frameworks to generate coherent probabilistic forecasts. Our results show improved generalization and forecast accuracy, particularly in the presence of data inconsistencies. On a variety of datasets, including both strictly hierarchically coherent and noisy data, our training method has either equal or better accuracy at all levels of the hierarchy while being strictly more coherent out-of-sample than existing soft-coherency methods.

### Enhancing PPO with Trajectory-Aware Hybrid Policies 
[[arxiv](https://arxiv.org/abs/2502.15968)] [[cool](https://papers.cool/arxiv/2502.15968)] [[pdf](https://arxiv.org/pdf/2502.15968)]
> **Authors**: Qisai Liu,Zhanhong Jiang,Hsin-Jung Yang,Mahsa Khosravi,Joshua R. Waite,Soumik Sarkar
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Proximal policy optimization (PPO) is one of the most popular state-of-the-art on-policy algorithms that has become a standard baseline in modern reinforcement learning with applications in numerous fields. Though it delivers stable performance with theoretical policy improvement guarantees, high variance, and high sample complexity still remain critical challenges in on-policy algorithms. To alleviate these issues, we propose Hybrid-Policy Proximal Policy Optimization (HP3O), which utilizes a trajectory replay buffer to make efficient use of trajectories generated by recent policies. Particularly, the buffer applies the "first in, first out" (FIFO) strategy so as to keep only the recent trajectories to attenuate the data distribution drift. A batch consisting of the trajectory with the best return and other randomly sampled ones from the buffer is used for updating the policy networks. The strategy helps the agent to improve its capability on top of the most recent best performance and in turn reduce variance empirically. We theoretically construct the policy improvement guarantees for the proposed algorithm. HP3O is validated and compared against several baseline algorithms using multiple continuous control environments. Our code is available here.

### Minions: Cost-efficient Collaboration Between On-device and Cloud Language Models 
[[arxiv](https://arxiv.org/abs/2502.15964)] [[cool](https://papers.cool/arxiv/2502.15964)] [[pdf](https://arxiv.org/pdf/2502.15964)]
> **Authors**: Avanika Narayan,Dan Biderman,Sabri Eyuboglu,Avner May,Scott Linderman,James Zou,Christopher Re
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,分布式、并行和集群计算
- **Abstract**: We investigate an emerging setup in which a small, on-device language model (LM) with access to local data communicates with a frontier, cloud-hosted LM to solve real-world tasks involving financial, medical, and scientific reasoning over long documents. Can a local-remote collaboration reduce cloud inference costs while preserving quality? First, we consider a naive collaboration protocol where the local and remote models simply chat back and forth. Because only the local model reads the full context, this protocol achieves a 30.4x reduction in remote costs, but recovers only 87% of the performance of the frontier model. We identify two key limitations of this protocol: the local model struggles to (1) follow the remote model's multi-step instructions and (2) reason over long contexts. Motivated by these observations, we study an extension of this protocol, coined MinionS, in which the remote model decomposes the task into easier subtasks over shorter chunks of the document, that are executed locally in parallel. MinionS reduces costs by 5.7x on average while recovering 97.9% of the performance of the remote model alone. Our analysis reveals several key design choices that influence the trade-off between cost and performance in local-remote systems.

### Towards Efficient Contrastive PAC Learning 
[[arxiv](https://arxiv.org/abs/2502.15962)] [[cool](https://papers.cool/arxiv/2502.15962)] [[pdf](https://arxiv.org/pdf/2502.15962)]
> **Authors**: Jie Shen
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,数据结构和算法,机器学习
- **Abstract**: We study contrastive learning under the PAC learning framework. While a series of recent works have shown statistical results for learning under contrastive loss, based either on the VC-dimension or Rademacher complexity, their algorithms are inherently inefficient or not implying PAC guarantees. In this paper, we consider contrastive learning of the fundamental concept of linear representations. Surprisingly, even under such basic setting, the existence of efficient PAC learners is largely open. We first show that the problem of contrastive PAC learning of linear representations is intractable to solve in general. We then show that it can be relaxed to a semi-definite program when the distance between contrastive samples is measured by the $\ell_2$-norm. We then establish generalization guarantees based on Rademacher complexity, and connect it to PAC guarantees under certain contrastive large-margin conditions. To the best of our knowledge, this is the first efficient PAC learning algorithm for contrastive learning.

### Towards Understanding Gradient Flow Dynamics of Homogeneous Neural Networks Beyond the Origin 
[[arxiv](https://arxiv.org/abs/2502.15952)] [[cool](https://papers.cool/arxiv/2502.15952)] [[pdf](https://arxiv.org/pdf/2502.15952)]
> **Authors**: Akshay Kumar,Jarvis Haupt
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,优化与控制,机器学习
- **Abstract**: Recent works exploring the training dynamics of homogeneous neural network weights under gradient flow with small initialization have established that in the early stages of training, the weights remain small and near the origin, but converge in direction. Building on this, the current paper studies the gradient flow dynamics of homogeneous neural networks with locally Lipschitz gradients, after they escape the origin. Insights gained from this analysis are used to characterize the first saddle point encountered by gradient flow after escaping the origin. Also, it is shown that for homogeneous feed-forward neural networks, under certain conditions, the sparsity structure emerging among the weights before the escape is preserved after escaping the origin and until reaching the next saddle point.

### Optimizing Pre-Training Data Mixtures with Mixtures of Data Expert Models 
[[arxiv](https://arxiv.org/abs/2502.15950)] [[cool](https://papers.cool/arxiv/2502.15950)] [[pdf](https://arxiv.org/pdf/2502.15950)]
> **Authors**: Lior Belenki,Alekh Agarwal,Tianze Shi,Kristina Toutanova
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: We propose a method to optimize language model pre-training data mixtures through efficient approximation of the cross-entropy loss corresponding to each candidate mixture via a Mixture of Data Experts (MDE). We use this approximation as a source of additional features in a regression model, trained from observations of model loss for a small number of mixtures. Experiments with Transformer decoder-only language models in the range of 70M to 1B parameters on the SlimPajama dataset show that our method achieves significantly better performance than approaches that train regression models using only the mixture rates as input features. Combining this improved optimization method with an objective that takes into account cross-entropy on end task data leads to superior performance on few-shot downstream evaluations. We also provide theoretical insights on why aggregation of data expert predictions can provide good approximations to model losses for data mixtures.

### Orthogonal Calibration for Asynchronous Federated Learning 
[[arxiv](https://arxiv.org/abs/2502.15940)] [[cool](https://papers.cool/arxiv/2502.15940)] [[pdf](https://arxiv.org/pdf/2502.15940)]
> **Authors**: Jiayun Zhang,Shuheng Li,Haiyu Huang,Xiaofan Yu,Rajesh K. Gupta,Jingbo Shang
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,分布式、并行和集群计算
- **Abstract**: Asynchronous federated learning mitigates the inefficiency of conventional synchronous aggregation by integrating updates as they arrive and adjusting their influence based on staleness. Due to asynchrony and data heterogeneity, learning objectives at the global and local levels are inherently inconsistent -- global optimization trajectories may conflict with ongoing local updates. Existing asynchronous methods simply distribute the latest global weights to clients, which can overwrite local progress and cause model drift. In this paper, we propose OrthoFL, an orthogonal calibration framework that decouples global and local learning progress and adjusts global shifts to minimize interference before merging them into local models. In OrthoFL, clients and the server maintain separate model weights. Upon receiving an update, the server aggregates it into the global weights via a moving average. For client weights, the server computes the global weight shift accumulated during the client's delay and removes the components aligned with the direction of the received update. The resulting parameters lie in a subspace orthogonal to the client update and preserve the maximal information from the global progress. The calibrated global shift is then merged into the client weights for further training. Extensive experiments show that OrthoFL improves accuracy by 9.6% and achieves a 12$\times$ speedup compared to synchronous methods. Moreover, it consistently outperforms state-of-the-art asynchronous baselines under various delay patterns and heterogeneity scenarios.

### Straight to Zero: Why Linearly Decaying the Learning Rate to Zero Works Best for LLMs 
[[arxiv](https://arxiv.org/abs/2502.15938)] [[cool](https://papers.cool/arxiv/2502.15938)] [[pdf](https://arxiv.org/pdf/2502.15938)]
> **Authors**: Shane Bergsma,Nolan Dey,Gurpreet Gosal,Gavia Gray,Daria Soboleva,Joel Hestness
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: ICLR 2025
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,神经和进化计算
- **Abstract**: LLMs are commonly trained with a learning rate (LR) warmup, followed by cosine decay to 10% of the maximum (10x decay). In a large-scale empirical study, we show that under an optimal peak LR, a simple linear decay-to-zero (D2Z) schedule consistently outperforms other schedules when training at compute-optimal dataset sizes. D2Z is superior across a range of model sizes, batch sizes, datasets, and vocabularies. Benefits increase as dataset size increases. Leveraging a novel interpretation of AdamW as an exponential moving average of weight updates, we show how linear D2Z optimally balances the demands of early training (moving away from initial conditions) and late training (averaging over more updates in order to mitigate gradient noise). In experiments, a 610M-parameter model trained for 80 tokens-per-parameter (TPP) using D2Z achieves lower loss than when trained for 200 TPP using 10x decay, corresponding to an astonishing 60% compute savings. Models such as Llama2-7B, trained for 286 TPP with 10x decay, could likely have saved a majority of compute by training with D2Z.

### On the Design of Safe Continual RL Methods for Control of Nonlinear Systems 
[[arxiv](https://arxiv.org/abs/2502.15922)] [[cool](https://papers.cool/arxiv/2502.15922)] [[pdf](https://arxiv.org/pdf/2502.15922)]
> **Authors**: Austin Coursey,Marcos Quinones-Grueiro,Gautam Biswas
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器人技术
- **Abstract**: Reinforcement learning (RL) algorithms have been successfully applied to control tasks associated with unmanned aerial vehicles and robotics. In recent years, safe RL has been proposed to allow the safe execution of RL algorithms in industrial and mission-critical systems that operate in closed loops. However, if the system operating conditions change, such as when an unknown fault occurs in the system, typical safe RL algorithms are unable to adapt while retaining past knowledge. Continual reinforcement learning algorithms have been proposed to address this issue. However, the impact of continual adaptation on the system's safety is an understudied problem. In this paper, we study the intersection of safe and continual RL. First, we empirically demonstrate that a popular continual RL algorithm, online elastic weight consolidation, is unable to satisfy safety constraints in non-linear systems subject to varying operating conditions. Specifically, we study the MuJoCo HalfCheetah and Ant environments with velocity constraints and sudden joint loss non-stationarity. Then, we show that an agent trained using constrained policy optimization, a safe RL algorithm, experiences catastrophic forgetting in continual learning settings. With this in mind, we explore a simple reward-shaping method to ensure that elastic weight consolidation prioritizes remembering both safety and task performance for safety-constrained, non-linear, and non-stationary dynamical systems.

### Connecting the geometry and dynamics of many-body complex systems with message passing neural operators 
[[arxiv](https://arxiv.org/abs/2502.15913)] [[cool](https://papers.cool/arxiv/2502.15913)] [[pdf](https://arxiv.org/pdf/2502.15913)]
> **Authors**: Nicholas A. Gabriel,Neil F. Johnson,George Em Karniadakis
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,物理与社会
- **Abstract**: The relationship between scale transformations and dynamics established by renormalization group techniques is a cornerstone of modern physical theories, from fluid mechanics to elementary particle physics. Integrating renormalization group methods into neural operators for many-body complex systems could provide a foundational inductive bias for learning their effective dynamics, while also uncovering multiscale organization. We introduce a scalable AI framework, ROMA (Renormalized Operators with Multiscale Attention), for learning multiscale evolution operators of many-body complex systems. In particular, we develop a renormalization procedure based on neural analogs of the geometric and laplacian renormalization groups, which can be co-learned with neural operators. An attention mechanism is used to model multiscale interactions by connecting geometric representations of local subgraphs and dynamical operators. We apply this framework in challenging conditions: large systems of more than 1M nodes, long-range interactions, and noisy input-output data for two contrasting examples: Kuramoto oscillators and Burgers-like social dynamics. We demonstrate that the ROMA framework improves scalability and positive transfer between forecasting and effective dynamics tasks compared to state-of-the-art operator learning techniques, while also giving insight into multiscale interactions. Additionally, we investigate power law scaling in the number of model parameters, and demonstrate a departure from typical power law exponents in the presence of hierarchical and multiscale interactions.

### IPAD: Inverse Prompt for AI Detection -- A Robust and Explainable LLM-Generated Text Detector 
[[arxiv](https://arxiv.org/abs/2502.15902)] [[cool](https://papers.cool/arxiv/2502.15902)] [[pdf](https://arxiv.org/pdf/2502.15902)]
> **Authors**: Zheng Chen,Yushi Feng,Changyang He,Yue Deng,Hongxi Pu,Bo Li
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Large Language Models (LLMs) have attained human-level fluency in text generation, which complicates the distinguishing between human-written and LLM-generated texts. This increases the risk of misuse and highlights the need for reliable detectors. Yet, existing detectors exhibit poor robustness on out-of-distribution (OOD) data and attacked data, which is critical for real-world scenarios. Also, they struggle to provide explainable evidence to support their decisions, thus undermining the reliability. In light of these challenges, we propose IPAD (Inverse Prompt for AI Detection), a novel framework consisting of a Prompt Inverter that identifies predicted prompts that could have generated the input text, and a Distinguisher that examines how well the input texts align with the predicted prompts. We develop and examine two versions of Distinguishers. Empirical evaluations demonstrate that both Distinguishers perform significantly better than the baseline methods, with version2 outperforming baselines by 9.73% on in-distribution data (F1-score) and 12.65% on OOD data (AUROC). Furthermore, a user study is conducted to illustrate that IPAD enhances the AI detection trustworthiness by allowing users to directly examine the decision-making evidence, which provides interpretable support for its state-of-the-art detection results.

### TS-OOD: Evaluating Time-Series Out-of-Distribution Detection and Prospective Directions for Progress 
[[arxiv](https://arxiv.org/abs/2502.15901)] [[cool](https://papers.cool/arxiv/2502.15901)] [[pdf](https://arxiv.org/pdf/2502.15901)]
> **Authors**: Onat Gungor,Amanda Sofie Rios,Nilesh Ahuja,Tajana Rosing
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Accepted for an oral presentation at AAAI-25 AI4TS
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Detecting out-of-distribution (OOD) data is a fundamental challenge in the deployment of machine learning models. From a security standpoint, this is particularly important because OOD test data can result in misleadingly confident yet erroneous predictions, which undermine the reliability of the deployed model. Although numerous models for OOD detection have been developed in computer vision and language, their adaptability to the time-series data domain remains limited and under-explored. Yet, time-series data is ubiquitous across manufacturing and security applications for which OOD is essential. This paper seeks to address this research gap by conducting a comprehensive analysis of modality-agnostic OOD detection algorithms. We evaluate over several multivariate time-series datasets, deep learning architectures, time-series specific data augmentations, and loss functions. Our results demonstrate that: 1) the majority of state-of-the-art OOD methods exhibit limited performance on time-series data, and 2) OOD methods based on deep feature modeling may offer greater advantages for time-series OOD detection, highlighting a promising direction for future time-series OOD detection algorithm development.

### Explaining the Success of Nearest Neighbor Methods in Prediction 
[[arxiv](https://arxiv.org/abs/2502.15900)] [[cool](https://papers.cool/arxiv/2502.15900)] [[pdf](https://arxiv.org/pdf/2502.15900)]
> **Authors**: George H. Chen,Devavrat Shah
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Originally published on May 31, 2018 in Foundations and Trends inMachineLearning; this revised version fixes some proof details for k-NN and fixed-radius NN regression and classification
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Many modern methods for prediction leverage nearest neighbor search to find past training examples most similar to a test example, an idea that dates back in text to at least the 11th century and has stood the test of time. This monograph aims to explain the success of these methods, both in theory, for which we cover foundational nonasymptotic statistical guarantees on nearest-neighbor-based regression and classification, and in practice, for which we gather prominent methods for approximate nearest neighbor search that have been essential to scaling prediction systems reliant on nearest neighbor analysis to handle massive datasets. Furthermore, we discuss connections to learning distances for use with nearest neighbor methods, including how random decision trees and ensemble methods learn nearest neighbor structure, as well as recent developments in crowdsourcing and graphons. In terms of theory, our focus is on nonasymptotic statistical guarantees, which we state in the form of how many training data and what algorithm parameters ensure that a nearest neighbor prediction method achieves a user-specified error tolerance. We begin with the most general of such results for nearest neighbor and related kernel regression and classification in general metric spaces. In such settings in which we assume very little structure, what enables successful prediction is smoothness in the function being estimated for regression, and a low probability of landing near the decision boundary for classification. In practice, these conditions could be difficult to verify for a real dataset. We then cover recent guarantees on nearest neighbor prediction in the three case studies of time series forecasting, recommending products to people over time, and delineating human organs in medical images by looking at image patches. In these case studies, clustering structure enables successful prediction.

### ML-Driven Approaches to Combat Medicare Fraud: Advances in Class Imbalance Solutions, Feature Engineering, Adaptive Learning, and Business Impact 
[[arxiv](https://arxiv.org/abs/2502.15898)] [[cool](https://papers.cool/arxiv/2502.15898)] [[pdf](https://arxiv.org/pdf/2502.15898)]
> **Authors**: Dorsa Farahmandazad,Kasra Danesh
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Medicare fraud poses a substantial challenge to healthcare systems, resulting in significant financial losses and undermining the quality of care provided to legitimate beneficiaries. This study investigates the use of machine learning (ML) to enhance Medicare fraud detection, addressing key challenges such as class imbalance, high-dimensional data, and evolving fraud patterns. A dataset comprising inpatient claims, outpatient claims, and beneficiary details was used to train and evaluate five ML models: Random Forest, KNN, LDA, Decision Tree, and AdaBoost. Data preprocessing techniques included resampling SMOTE method to address the class imbalance, feature selection for dimensionality reduction, and aggregation of diagnostic and procedural codes. Random Forest emerged as the best-performing model, achieving a training accuracy of 99.2% and validation accuracy of 98.8%, and F1-score (98.4%). The Decision Tree also performed well, achieving a validation accuracy of 96.3%. KNN and AdaBoost demonstrated moderate performance, with validation accuracies of 79.2% and 81.1%, respectively, while LDA struggled with a validation accuracy of 63.3% and a low recall of 16.6%. The results highlight the importance of advanced resampling techniques, feature engineering, and adaptive learning in detecting Medicare fraud effectively. This study underscores the potential of machine learning in addressing the complexities of fraud detection. Future work should explore explainable AI and hybrid models to improve interpretability and performance, ensuring scalable and reliable fraud detection systems that protect healthcare resources and beneficiaries.

### Directional Gradient Projection for Robust Fine-Tuning of Foundation Models 
[[arxiv](https://arxiv.org/abs/2502.15895)] [[cool](https://papers.cool/arxiv/2502.15895)] [[pdf](https://arxiv.org/pdf/2502.15895)]
> **Authors**: Chengyue Huang,Junjiao Tian,Brisa Maneechotesuwan,Shivang Chopra,Zsolt Kira
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Accepted to ICLR 2025
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,计算机视觉和模式识别
- **Abstract**: Robust fine-tuning aims to adapt large foundation models to downstream tasks while preserving their robustness to distribution shifts. Existing methods primarily focus on constraining and projecting current model towards the pre-trained initialization based on the magnitudes between fine-tuned and pre-trained weights, which often require extensive hyper-parameter tuning and can sometimes result in underfitting. In this work, we propose Directional Gradient Projection (DiGraP), a novel layer-wise trainable method that incorporates directional information from gradients to bridge regularization and multi-objective optimization. Besides demonstrating our method on image classification, as another contribution we generalize this area to the multi-modal evaluation settings for robust fine-tuning. Specifically, we first bridge the uni-modal and multi-modal gap by performing analysis on Image Classification reformulated Visual Question Answering (VQA) benchmarks and further categorize ten out-of-distribution (OOD) VQA datasets by distribution shift types and degree (i.e. near versus far OOD). Experimental results show that DiGraP consistently outperforms existing baselines across Image Classfication and VQA tasks with discriminative and generative backbones, improving both in-distribution (ID) generalization and OOD robustness.

### Enhancing Domain-Specific Retrieval-Augmented Generation: Synthetic Data Generation and Evaluation using Reasoning Models 
[[arxiv](https://arxiv.org/abs/2502.15854)] [[cool](https://papers.cool/arxiv/2502.15854)] [[pdf](https://arxiv.org/pdf/2502.15854)]
> **Authors**: Aryan Jadon,Avinash Patil,Shashank Kumar
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 8 Pages
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Retrieval-Augmented Generation (RAG) systems face significant performance gaps when applied to technical domains requiring precise information extraction from complex documents. Current evaluation methodologies relying on document-level metrics inadequately capture token-resolution retrieval accuracy that is critical for domain-related documents. We propose a framework combining granular evaluation metrics with synthetic data generation to optimize domain-specific RAG performance. First, we introduce token-aware metrics Precision $Ω$ and Intersection-over-Union (IoU) that quantify context preservation versus information density trade-offs inherent in technical texts. Second, we develop a reasoning model-driven pipeline using instruction-tuned LLMs (DeepSeek-R1, DeepSeek-R1 distilled variants, and Phi-4) to generate context-anchored QA pairs with discontinuous reference spans across three specialized corpora: SEC 10-K filings (finance), biomedical abstracts (PubMed), and APT threat reports (cybersecurity). Our empirical analysis reveals critical insights: smaller chunks (less than 10 tokens) improve precision by 31-42% (IoU = 0.071 vs. baseline 0.053) at recall costs (-18%), while domain-specific embedding strategies yield 22% variance in optimal chunk sizing (5-20 tokens). The DeepSeek-R1-Distill-Qwen-32B model demonstrates superior concept alignment (+14% mean IoU over alternatives), though no configuration universally dominates. Financial texts favor larger chunks for risk factor coverage (Recall = 0.81 at size = 20), whereas cybersecurity content benefits from atomic segmentation, Precision $Ω= 0.28$ at size = 5. Our code is available on https://github.com/aryan-jadon/Synthetic-Data-Generation-and-Evaluation-using-Reasoning-Model

### Implicit Neural Representations for Chemical Reaction Paths 
[[arxiv](https://arxiv.org/abs/2502.15843)] [[cool](https://papers.cool/arxiv/2502.15843)] [[pdf](https://arxiv.org/pdf/2502.15843)]
> **Authors**: Kalyan Ramakrishnan,Lars L. Schaaf,Chen Lin,Guangrun Wang,Philip Torr
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-24
> **comment**: Intended for submission to the Journal of Chemical Physics. Once published, it will be available at [DOI/URL]
- **标题**: None
- **领域**: 机器学习,化学物理
- **Abstract**: We show that neural networks can be optimized to represent minimum energy paths as continuous functions, offering a flexible alternative to discrete path-search methods like Nudged Elastic Band (NEB). Our approach parameterizes reaction paths with a network trained on a loss function that discards tangential energy gradients and enables instant estimation of the transition state. We first validate the method on two-dimensional potentials and then demonstrate its advantages over NEB on challenging atomistic systems where (i) poor initial guesses yield unphysical paths, (ii) multiple competing paths exist, or (iii) the reaction follows a complex multi-step mechanism. Results highlight the versatility of the method -- for instance, a simple adjustment to the sampling strategy during optimization can help escape local-minimum solutions. Finally, in a low-dimensional setting, we demonstrate that a single neural network can learn from existing paths and generalize to unseen systems, showing promise for a universal reaction path representation.

### FedMobile: Enabling Knowledge Contribution-aware Multi-modal Federated Learning with Incomplete Modalities 
[[arxiv](https://arxiv.org/abs/2502.15839)] [[cool](https://papers.cool/arxiv/2502.15839)] [[pdf](https://arxiv.org/pdf/2502.15839)]
> **Authors**: Yi Liu,Cong Wang,Xingliang Yuan
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-24
> **comment**: The Web Conference 2025
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: The Web of Things (WoT) enhances interoperability across web-based and ubiquitous computing platforms while complementing existing IoT standards. The multimodal Federated Learning (FL) paradigm has been introduced to enhance WoT by enabling the fusion of multi-source mobile sensing data while preserving privacy. However, a key challenge in mobile sensing systems using multimodal FL is modality incompleteness, where some modalities may be unavailable or only partially captured, potentially degrading the system's performance and reliability. Current multimodal FL frameworks typically train multiple unimodal FL subsystems or apply interpolation techniques on the node side to approximate missing modalities. However, these approaches overlook the shared latent feature space among incomplete modalities across different nodes and fail to discriminate against low-quality nodes. To address this gap, we present FedMobile, a new knowledge contribution-aware multimodal FL framework designed for robust learning despite missing modalities. FedMobile prioritizes local-to-global knowledge transfer, leveraging cross-node multimodal feature information to reconstruct missing features. It also enhances system performance and resilience to modality heterogeneity through rigorous node contribution assessments and knowledge contribution-aware aggregation rules. Empirical evaluations on five widely recognized multimodal benchmark datasets demonstrate that FedMobile maintains robust learning even when up to 90% of modality information is missing or when data from two modalities are randomly missing, outperforming state-of-the-art baselines.

### Challenges of Multi-Modal Coreset Selection for Depth Prediction 
[[arxiv](https://arxiv.org/abs/2502.15834)] [[cool](https://papers.cool/arxiv/2502.15834)] [[pdf](https://arxiv.org/pdf/2502.15834)]
> **Authors**: Viktor Moskvoretskii,Narek Alvandian
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Coreset selection methods are effective in accelerating training and reducing memory requirements but remain largely unexplored in applied multimodal settings. We adapt a state-of-the-art (SoTA) coreset selection technique for multimodal data, focusing on the depth prediction task. Our experiments with embedding aggregation and dimensionality reduction approaches reveal the challenges of extending unimodal algorithms to multimodal scenarios, highlighting the need for specialized methods to better capture inter-modal relationships.

### Advancing Out-of-Distribution Detection via Local Neuroplasticity 
[[arxiv](https://arxiv.org/abs/2502.15833)] [[cool](https://papers.cool/arxiv/2502.15833)] [[pdf](https://arxiv.org/pdf/2502.15833)]
> **Authors**: Alessandro Canevaro,Julian Schmidt,Mohammad Sajad Marvi,Hang Yu,Georg Martius,Julian Jordan
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-24
> **comment**: Accepted to ICLR25
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: In the domain of machine learning, the assumption that training and test data share the same distribution is often violated in real-world scenarios, requiring effective out-of-distribution (OOD) detection. This paper presents a novel OOD detection method that leverages the unique local neuroplasticity property of Kolmogorov-Arnold Networks (KANs). Unlike traditional multilayer perceptrons, KANs exhibit local plasticity, allowing them to preserve learned information while adapting to new tasks. Our method compares the activation patterns of a trained KAN against its untrained counterpart to detect OOD samples. We validate our approach on benchmarks from image and medical domains, demonstrating superior performance and robustness compared to state-of-the-art techniques. These results underscore the potential of KANs in enhancing the reliability of machine learning systems in diverse environments.

### LACTOSE: Linear Array of Conditions, TOpologies with Separated Error-backpropagation -- The Differentiable "IF" Conditional for Differentiable Digital Signal Processing 
[[arxiv](https://arxiv.org/abs/2502.15829)] [[cool](https://papers.cool/arxiv/2502.15829)] [[pdf](https://arxiv.org/pdf/2502.15829)]
> **Authors**: Christopher Johann Clarke
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-24
> **comment**: 6 pages
- **标题**: None
- **领域**: 机器学习,神经和进化计算,声音,音频和语音处理
- **Abstract**: There has been difficulty utilising conditional statements as part of the neural network graph (e.g. if input $> x$, pass input to network $N$). This is due to the inability to backpropagate through branching conditions. The Linear Array of Conditions, TOpologies with Separated Error-backpropagation (LACTOSE) Algorithm addresses this issue and allows the conditional use of available machine learning layers for supervised learning models. In this paper, the LACTOSE algorithm is applied to a simple use of DDSP, however, the main point is the development of the "if" conditional for DDSP use. The LACTOSE algorithm stores trained parameters for each user-specified numerical range and loads the parameters dynamically during prediction.

### A Stronger Mixture of Low-Rank Experts for Fine-Tuning Foundation Models 
[[arxiv](https://arxiv.org/abs/2502.15828)] [[cool](https://papers.cool/arxiv/2502.15828)] [[pdf](https://arxiv.org/pdf/2502.15828)]
> **Authors**: Mengyang Sun,Yihao Wang,Tao Feng,Dan Zhang,Yifan Zhu,Jie Tang
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: In order to streamline the fine-tuning of foundation models, Low-Rank Adapters (LoRAs) have been substantially adopted across various fields, including instruction tuning and domain adaptation. The underlying concept of LoRA involves decomposing a full-rank matrix into the product of two lower-rank matrices, which reduces storage consumption and accelerates the training process. Furthermore, to address the limited expressive capacity of LoRA, the Mixture-of-Expert (MoE) has been introduced for incorporating multiple LoRA adapters. The integration of LoRA experts leads to a visible improvement across several downstream scenes. However, the mixture of LoRAs (MoE-LoRA) still exhibits its low robustness during tuning and inferring. Inspired by the Riemannian Preconditioners which train LoRA as a sub-space projector, we propose a new training strategy for MoE-LoRA, to stabilize and boost its feature learning procedure by multi-space projections. Examinations on SGD and AdamW optimizers demonstrate the effectiveness of our methodology. Source code is available at https://github.com/THUDM/MoELoRA_Riemannian.

### Explainable Artificial Intelligence Model for Evaluating Shear Strength Parameters of Municipal Solid Waste Across Diverse Compositional Profiles 
[[arxiv](https://arxiv.org/abs/2502.15827)] [[cool](https://papers.cool/arxiv/2502.15827)] [[pdf](https://arxiv.org/pdf/2502.15827)]
> **Authors**: Parichat Suknark,Sompote Youwaib,Tipok Kitkobsin,Sirintornthep Towprayoon,Chart Chiemchaisri,Komsilp Wangyao
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Accurate prediction of shear strength parameters in Municipal Solid Waste (MSW) remains a critical challenge in geotechnical engineering due to the heterogeneous nature of waste materials and their temporal evolution through degradation processes. This paper presents a novel explainable artificial intelligence (XAI) framework for evaluating cohesion and friction angle across diverse MSW compositional profiles. The proposed model integrates a multi-layer perceptron architecture with SHAP (SHapley Additive exPlanations) analysis to provide transparent insights into how specific waste components influence strength characteristics. Training data encompassed large-scale direct shear tests across various waste compositions and degradation states. The model demonstrated superior predictive accuracy compared to traditional gradient boosting methods, achieving mean absolute percentage errors of 7.42% and 14.96% for friction angle and cohesion predictions, respectively. Through SHAP analysis, the study revealed that fibrous materials and particle size distribution were primary drivers of shear strength variation, with food waste and plastics showing significant but non-linear effects. The model's explainability component successfully quantified these relationships, enabling evidence-based recommendations for waste management practices. This research bridges the gap between advanced machine learning and geotechnical engineering practice, offering a reliable tool for rapid assessment of MSW mechanical properties while maintaining interpretability for engineering decision-making.

### Utilizing AI and Machine Learning for Predictive Analysis of Post-Treatment Cancer Recurrence 
[[arxiv](https://arxiv.org/abs/2502.15825)] [[cool](https://papers.cool/arxiv/2502.15825)] [[pdf](https://arxiv.org/pdf/2502.15825)]
> **Authors**: Muhammad Umer Qayyum,Muhammad Fahad,Nasrullah Abbasi
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-24
> **comment**: 15 pages
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: In oncology, recurrence after treatment is one of the major challenges, related to patients' survival and quality of life. Conventionally, prediction of cancer relapse has always relied on clinical observation with statistical model support, which almost fails to explain the complex, multifactorial nature of tumor recurrence. This research explores how AI and ML models may increase the accuracy and reliability of recurrence prediction in cancer. Therefore, AI and ML create new opportunities not only for personalized medicine but also for proactive management of patients through analyzing large volumes of data on genetics, clinical manifestations, and treatment. The paper describes the various AI and ML techniques for pattern identification and outcome prediction in cancer patients using supervised and unsupervised learning. Clinical implications provide an opportunity to review how early interventions could happen and the design of treatment planning.

### InductionBench: LLMs Fail in the Simplest Complexity Class 
[[arxiv](https://arxiv.org/abs/2502.15823)] [[cool](https://papers.cool/arxiv/2502.15823)] [[pdf](https://arxiv.org/pdf/2502.15823)]
> **Authors**: Wenyue Hua,Tyler Wong,Sun Fei,Liangming Pan,Adam Jardine,William Yang Wang
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-24
> **comment**: 24 pages, 7 figures
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,形式语言和自动机理论
- **Abstract**: Large language models (LLMs) have shown remarkable improvements in reasoning and many existing benchmarks have been addressed by models such as o1 and o3 either fully or partially. However, a majority of these benchmarks emphasize deductive reasoning, including mathematical and coding tasks in which rules such as mathematical axioms or programming syntax are clearly defined, based on which LLMs can plan and apply these rules to arrive at a solution. In contrast, inductive reasoning, where one infers the underlying rules from observed data, remains less explored. Such inductive processes lie at the heart of scientific discovery, as they enable researchers to extract general principles from empirical observations. To assess whether LLMs possess this capacity, we introduce InductionBench, a new benchmark designed to evaluate the inductive reasoning ability of LLMs. Our experimental findings reveal that even the most advanced models available struggle to master the simplest complexity classes within the subregular hierarchy of functions, highlighting a notable deficiency in current LLMs' inductive reasoning capabilities. Coda and data are available https://github.com/Wenyueh/inductive_reasoning_benchmark.

### Theoretical Physics Benchmark (TPBench) -- a Dataset and Study of AI Reasoning Capabilities in Theoretical Physics 
[[arxiv](https://arxiv.org/abs/2502.15815)] [[cool](https://papers.cool/arxiv/2502.15815)] [[pdf](https://arxiv.org/pdf/2502.15815)]
> **Authors**: Daniel J. H. Chung,Zhiqi Gao,Yurii Kvasiuk,Tianyi Li,Moritz Münchmeyer,Maja Rudolph,Frederic Sala,Sai Chaitanya Tadepalli
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-24
> **comment**: 48 pages, 4 figures
- **标题**: None
- **领域**: 机器学习,宇宙学和非银河系天体物理学,人工智能,高能物理-现象学,高能物理 - 理论
- **Abstract**: We introduce a benchmark to evaluate the capability of AI to solve problems in theoretical physics, focusing on high-energy theory and cosmology. The first iteration of our benchmark consists of 57 problems of varying difficulty, from undergraduate to research level. These problems are novel in the sense that they do not come from public problem collections. We evaluate our data set on various open and closed language models, including o3-mini, o1, DeepSeek-R1, GPT-4o and versions of Llama and Qwen. While we find impressive progress in model performance with the most recent models, our research-level difficulty problems are mostly unsolved. We address challenges of auto-verifiability and grading, and discuss common failure modes. While currently state-of-the art models are still of limited use for researchers, our results show that AI assisted theoretical physics research may become possible in the near future. We discuss the main obstacles towards this goal and possible strategies to overcome them. The public problems and solutions, results for various models, and updates to the data set and score distribution, are available on the website of the dataset tpbench.org.

### Slamming: Training a Speech Language Model on One GPU in a Day 
[[arxiv](https://arxiv.org/abs/2502.15814)] [[cool](https://papers.cool/arxiv/2502.15814)] [[pdf](https://arxiv.org/pdf/2502.15814)]
> **Authors**: Gallil Maimon,Avishai Elmakies,Yossi Adi
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,声音,音频和语音处理
- **Abstract**: We introduce Slam, a recipe for training high-quality Speech Language Models (SLMs) on a single academic GPU in 24 hours. We do so through empirical analysis of model initialisation and architecture, synthetic training data, preference optimisation with synthetic data and tweaking all other components. We empirically demonstrate that this training recipe also scales well with more compute getting results on par with leading SLMs in a fraction of the compute cost. We hope these insights will make SLM training and research more accessible. In the context of SLM scaling laws, our results far outperform predicted compute optimal performance, giving an optimistic view to SLM feasibility. See code, data, models, samples at - https://pages.cs.huji.ac.il/adiyoss-lab/slamming .

### InsightVision: A Comprehensive, Multi-Level Chinese-based Benchmark for Evaluating Implicit Visual Semantics in Large Vision Language Models 
[[arxiv](https://arxiv.org/abs/2502.15812)] [[cool](https://papers.cool/arxiv/2502.15812)] [[pdf](https://arxiv.org/pdf/2502.15812)]
> **Authors**: Xiaofei Yin,Yijie Hong,Ya Guo,Yi Tu,Weiqiang Wang,Gongshen Liu,Huijia zhu
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-24
> **comment**: 19 pages, 10 figures
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: In the evolving landscape of multimodal language models, understanding the nuanced meanings conveyed through visual cues - such as satire, insult, or critique - remains a significant challenge. Existing evaluation benchmarks primarily focus on direct tasks like image captioning or are limited to a narrow set of categories, such as humor or satire, for deep semantic understanding. To address this gap, we introduce, for the first time, a comprehensive, multi-level Chinese-based benchmark designed specifically for evaluating the understanding of implicit meanings in images. This benchmark is systematically categorized into four subtasks: surface-level content understanding, symbolic meaning interpretation, background knowledge comprehension, and implicit meaning comprehension. We propose an innovative semi-automatic method for constructing datasets, adhering to established construction protocols. Using this benchmark, we evaluate 15 open-source large vision language models (LVLMs) and GPT-4o, revealing that even the best-performing model lags behind human performance by nearly 14% in understanding implicit meaning. Our findings underscore the intrinsic challenges current LVLMs face in grasping nuanced visual semantics, highlighting significant opportunities for future research and development in this domain. We will publicly release our InsightVision dataset, code upon acceptance of the paper.

### Spiking Point Transformer for Point Cloud Classification 
[[arxiv](https://arxiv.org/abs/2502.15811)] [[cool](https://papers.cool/arxiv/2502.15811)] [[pdf](https://arxiv.org/pdf/2502.15811)]
> **Authors**: Peixi Wu,Bosong Chai,Hebei Li,Menghua Zheng,Yansong Peng,Zeyu Wang,Xuan Nie,Yueyi Zhang,Xiaoyan Sun
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-24
> **comment**: Accepted by AAAI 2025
- **标题**: None
- **领域**: 机器学习,人工智能,神经和进化计算
- **Abstract**: Spiking Neural Networks (SNNs) offer an attractive and energy-efficient alternative to conventional Artificial Neural Networks (ANNs) due to their sparse binary activation. When SNN meets Transformer, it shows great potential in 2D image processing. However, their application for 3D point cloud remains underexplored. To this end, we present Spiking Point Transformer (SPT), the first transformer-based SNN framework for point cloud classification. Specifically, we first design Queue-Driven Sampling Direct Encoding for point cloud to reduce computational costs while retaining the most effective support points at each time step. We introduce the Hybrid Dynamics Integrate-and-Fire Neuron (HD-IF), designed to simulate selective neuron activation and reduce over-reliance on specific artificial neurons. SPT attains state-of-the-art results on three benchmark datasets that span both real-world and synthetic datasets in the SNN domain. Meanwhile, the theoretical energy consumption of SPT is at least 6.4$\times$ less than its ANN counterpart.

### Black Sheep in the Herd: Playing with Spuriously Correlated Attributes for Vision-Language Recognition 
[[arxiv](https://arxiv.org/abs/2502.15809)] [[cool](https://papers.cool/arxiv/2502.15809)] [[pdf](https://arxiv.org/pdf/2502.15809)]
> **Authors**: Xinyu Tian,Shu Zou,Zhaoyuan Yang,Mengqi He,Jing Zhang
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-24
> **comment**: Accepted to ICLR2025
- **标题**: None
- **领域**: 机器学习,图像和视频处理
- **Abstract**: Few-shot adaptation for Vision-Language Models (VLMs) presents a dilemma: balancing in-distribution accuracy with out-of-distribution generalization. Recent research has utilized low-level concepts such as visual attributes to enhance generalization. However, this study reveals that VLMs overly rely on a small subset of attributes on decision-making, which co-occur with the category but are not inherently part of it, termed spuriously correlated attributes. This biased nature of VLMs results in poor generalization. To address this, 1) we first propose Spurious Attribute Probing (SAP), identifying and filtering out these problematic attributes to significantly enhance the generalization of existing attribute-based methods; 2) We introduce Spurious Attribute Shielding (SAS), a plug-and-play module that mitigates the influence of these attributes on prediction, seamlessly integrating into various Parameter-Efficient Fine-Tuning (PEFT) methods. In experiments, SAP and SAS significantly enhance accuracy on distribution shifts across 11 datasets and 3 generalization tasks without compromising downstream performance, establishing a new state-of-the-art benchmark.

### FragFM: Efficient Fragment-Based Molecular Generation via Discrete Flow Matching 
[[arxiv](https://arxiv.org/abs/2502.15805)] [[cool](https://papers.cool/arxiv/2502.15805)] [[pdf](https://arxiv.org/pdf/2502.15805)]
> **Authors**: Joongwon Lee,Seonghwan Kim,Wou Youn Kim
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-24
> **comment**: 19 pages, 11 figures, under review
- **标题**: None
- **领域**: 机器学习,人工智能,化学物理
- **Abstract**: We introduce FragFM, a novel fragment-based discrete flow matching framework for molecular graph generation.FragFM generates molecules at the fragment level, leveraging a coarse-to-fine autoencoding mechanism to reconstruct atom-level details. This approach reduces computational complexity while maintaining high chemical validity, enabling more efficient and scalable molecular generation. We benchmark FragFM against state-of-the-art diffusion- and flow-based models on standard molecular generation benchmarks and natural product datasets, demonstrating superior performance in validity, property control, and sampling efficiency. Notably, FragFM achieves over 99\% validity with significantly fewer sampling steps, improving scalability while preserving molecular diversity. These results highlight the potential of fragment-based generative modeling for large-scale, property-aware molecular design, paving the way for more efficient exploration of chemical space.

### Megrez-Omni Technical Report 
[[arxiv](https://arxiv.org/abs/2502.15803)] [[cool](https://papers.cool/arxiv/2502.15803)] [[pdf](https://arxiv.org/pdf/2502.15803)]
> **Authors**: Boxun Li,Yadong Li,Zhiyuan Li,Congyi Liu,Weilin Liu,Guowei Niu,Zheyue Tan,Haiyang Xu,Zhuyu Yao,Tao Yuan,Dong Zhou,Yueqing Zhuang,Shengen Yan,Guohao Dai,Yu Wang
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: In this work, we present the Megrez models, comprising a language model (Megrez-3B-Instruct) and a multimodal model (Megrez-3B-Omni). These models are designed to deliver fast inference, compactness, and robust edge-side intelligence through a software-hardware co-design approach. Megrez-3B-Instruct offers several advantages, including high accuracy, high speed, ease of use, and a wide range of applications. Building on Megrez-3B-Instruct, Megrez-3B-Omni is an on-device multimodal understanding LLM that supports image, text, and audio analysis. It achieves state-of-the-art accuracy across all three modalities and demonstrates strong versatility and robustness, setting a new benchmark for multimodal AI models.

### A General Error-Theoretical Analysis Framework for Constructing Compression Strategies 
[[arxiv](https://arxiv.org/abs/2502.15802)] [[cool](https://papers.cool/arxiv/2502.15802)] [[pdf](https://arxiv.org/pdf/2502.15802)]
> **Authors**: Boyang Zhang,Daning Cheng,Yunquan Zhang,Meiqi Tu,Fangmin Liu,Jiake Tian
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-24
> **comment**: Under Review
- **标题**: None
- **领域**: 机器学习,人工智能,信息论
- **Abstract**: The exponential growth in parameter size and computational complexity of deep models poses significant challenges for efficient deployment. The core problem of existing compression methods is that different layers of the model have significant differences in their tolerance to compression levels. For instance, the first layer of a model can typically sustain a higher compression level compared to the last layer without compromising performance. Thus, the key challenge lies in how to allocate compression levels across layers in a way that minimizes performance loss while maximizing parameter reduction. To address this challenge, we propose a Compression Error Theory (CET) framework, designed to determine the optimal compression level for each layer. Taking quantization as an example, CET leverages differential expansion and algebraic geometry to reconstruct the quadratic form of quantization error as ellipsoids and hyperbolic paraboloids, and utilizes their geometric structures to define an error subspace. To identify the error subspace with minimal performance loss, by performing orthogonal decomposition of the geometric space, CET transforms the optimization process of the error subspace into a complementary problem. The final theoretical analysis shows that constructing the quantization subspace along the major axis results in minimal performance degradation. Through experimental verification of the theory, CET can greatly retain performance while compressing. Specifically, on the ResNet-34 model, CET achieves nearly 11$\times$ parameter compression while even surpassing performance comparable to the original model.

### An explainable transformer circuit for compositional generalization 
[[arxiv](https://arxiv.org/abs/2502.15801)] [[cool](https://papers.cool/arxiv/2502.15801)] [[pdf](https://arxiv.org/pdf/2502.15801)]
> **Authors**: Cheng Tang,Brenden Lake,Mehrdad Jazayeri
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Compositional generalization-the systematic combination of known components into novel structures-remains a core challenge in cognitive science and machine learning. Although transformer-based large language models can exhibit strong performance on certain compositional tasks, the underlying mechanisms driving these abilities remain opaque, calling into question their interpretability. In this work, we identify and mechanistically interpret the circuit responsible for compositional induction in a compact transformer. Using causal ablations, we validate the circuit and formalize its operation using a program-like description. We further demonstrate that this mechanistic understanding enables precise activation edits to steer the model's behavior predictably. Our findings advance the understanding of complex behaviors in transformers and highlight such insights can provide a direct pathway for model control.

### MaxSup: Overcoming Representation Collapse in Label Smoothing 
[[arxiv](https://arxiv.org/abs/2502.15798)] [[cool](https://papers.cool/arxiv/2502.15798)] [[pdf](https://arxiv.org/pdf/2502.15798)]
> **Authors**: Yuxuan Zhou,Heng Li,Zhi-Qi Cheng,Xudong Yan,Mario Fritz,Margret Keuper
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-24
> **comment**: 19 pages, 9 Tables, preliminary work under review do not distribute
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: Label Smoothing (LS) is widely adopted to curb overconfidence in neural network predictions and enhance generalization. However, previous research shows that LS can force feature representations into excessively tight clusters, eroding intra-class distinctions. More recent findings suggest that LS also induces overconfidence in misclassifications, yet the precise mechanism remained unclear. In this work, we decompose the loss term introduced by LS, revealing two key components: (i) a regularization term that functions only when the prediction is correct, and (ii) an error-enhancement term that emerges under misclassifications. This latter term compels the model to reinforce incorrect predictions with exaggerated certainty, further collapsing the feature space. To address these issues, we propose Max Suppression (MaxSup), which uniformly applies the intended regularization to both correct and incorrect predictions by penalizing the top-1 logit instead of the ground-truth logit. Through feature analyses, we show that MaxSup restores intra-class variation and sharpens inter-class boundaries. Extensive experiments on image classification and downstream tasks confirm that MaxSup is a more robust alternative to LS. Code is available at: https://github.com/ZhouYuxuanYX/Maximum-Suppression-Regularization.

### Pruning as a Defense: Reducing Memorization in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.15796)] [[cool](https://papers.cool/arxiv/2502.15796)] [[pdf](https://arxiv.org/pdf/2502.15796)]
> **Authors**: Mansi Gupta,Nikhar Waghela,Sarthak Gupta,Shourya Goel,Sanjif Shanmugavelu
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Large language models have been shown to memorize significant portions of their training data, which they can reproduce when appropriately prompted. This work investigates the impact of simple pruning techniques on this behavior. Our findings reveal that pruning effectively reduces the extent of memorization in LLMs, demonstrating its potential as a foundational approach for mitigating membership inference attacks.

### Self-Supervised Transformers as Iterative Solution Improvers for Constraint Satisfaction 
[[arxiv](https://arxiv.org/abs/2502.15794)] [[cool](https://papers.cool/arxiv/2502.15794)] [[pdf](https://arxiv.org/pdf/2502.15794)]
> **Authors**: Yudong W. Xu,Wenhao Li,Scott Sanner,Elias B. Khalil
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,计算机科学中的逻辑
- **Abstract**: We present a Transformer-based framework for Constraint Satisfaction Problems (CSPs). CSPs find use in many applications and thus accelerating their solution with machine learning is of wide interest. Most existing approaches rely on supervised learning from feasible solutions or reinforcement learning, paradigms that require either feasible solutions to these NP-Complete CSPs or large training budgets and a complex expert-designed reward signal. To address these challenges, we propose ConsFormer, a self-supervised framework that leverages a Transformer as a solution refiner. ConsFormer constructs a solution to a CSP iteratively in a process that mimics local search. Instead of using feasible solutions as labeled data, we devise differentiable approximations to the discrete constraints of a CSP to guide model training. Our model is trained to improve random assignments for a single step but is deployed iteratively at test time, circumventing the bottlenecks of supervised and reinforcement learning. Our method can tackle out-of-distribution CSPs simply through additional iterations.

### Anomaly Detection in Smart Power Grids with Graph-Regularized MS-SVDD: a Multimodal Subspace Learning Approach 
[[arxiv](https://arxiv.org/abs/2502.15793)] [[cool](https://papers.cool/arxiv/2502.15793)] [[pdf](https://arxiv.org/pdf/2502.15793)]
> **Authors**: Thomas Debelle,Fahad Sohrab,Pekka Abrahamsson,Moncef Gabbouj
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-24
> **comment**: 20 pages, 5 figures, supplementary material
- **标题**: None
- **领域**: 机器学习,系统与控制
- **Abstract**: In this paper, we address an anomaly detection problem in smart power grids using Multimodal Subspace Support Vector Data Description (MS-SVDD). This approach aims to leverage better feature relations by considering the data as coming from different modalities. These data are projected into a shared lower-dimensionality subspace which aims to preserve their inner characteristics. To supplement the previous work on this subject, we introduce novel multimodal graph-embedded regularizers that leverage graph information for every modality to enhance the training process, and we consider an improved training equation that allows us to maximize or minimize each modality according to the specified criteria. We apply this regularized graph-embedded model on a 3-modalities dataset after having generalized MS-SVDD algorithms to any number of modalities. To set up our application, we propose a whole preprocessing procedure to extract One-Class Classification training instances from time-bounded event time series that are used to evaluate both the reliability and earliness of our model for Event Detection.

### Signal Collapse in One-Shot Pruning: When Sparse Models Fail to Distinguish Neural Representations 
[[arxiv](https://arxiv.org/abs/2502.15790)] [[cool](https://papers.cool/arxiv/2502.15790)] [[pdf](https://arxiv.org/pdf/2502.15790)]
> **Authors**: Dhananjay Saikumar,Blesson Varghese
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: Neural network pruning is essential for reducing model complexity to enable deployment on resource constrained hardware. While performance loss of pruned networks is often attributed to the removal of critical parameters, we identify signal collapse a reduction in activation variance across layers as the root cause. Existing one shot pruning methods focus on weight selection strategies and rely on computationally expensive second order approximations. In contrast, we demonstrate that mitigating signal collapse, rather than optimizing weight selection, is key to improving accuracy of pruned networks. We propose REFLOW that addresses signal collapse without updating trainable weights, revealing high quality sparse sub networks within the original parameter space. REFLOW enables magnitude pruning to achieve state of the art performance, restoring ResNeXt101 accuracy from under 4.1% to 78.9% on ImageNet with only 20% of the weights retained, surpassing state of the art approaches.

### Masking the Gaps: An Imputation-Free Approach to Time Series Modeling with Missing Data 
[[arxiv](https://arxiv.org/abs/2502.15785)] [[cool](https://papers.cool/arxiv/2502.15785)] [[pdf](https://arxiv.org/pdf/2502.15785)]
> **Authors**: Abhilash Neog,Arka Daw,Sepideh Fatemi Khorasgani,Anuj Karpatne
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-24
> **comment**: 15 pages
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: A significant challenge in time-series (TS) modeling is the presence of missing values in real-world TS datasets. Traditional two-stage frameworks, involving imputation followed by modeling, suffer from two key drawbacks: (1) the propagation of imputation errors into subsequent TS modeling, (2) the trade-offs between imputation efficacy and imputation complexity. While one-stage approaches attempt to address these limitations, they often struggle with scalability or fully leveraging partially observed features. To this end, we propose a novel imputation-free approach for handling missing values in time series termed Missing Feature-aware Time Series Modeling (MissTSM) with two main innovations. First, we develop a novel embedding scheme that treats every combination of time-step and feature (or channel) as a distinct token. Second, we introduce a novel Missing Feature-Aware Attention (MFAA) Layer to learn latent representations at every time-step based on partially observed features. We evaluate the effectiveness of MissTSM in handling missing values over multiple benchmark datasets.

### Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating Rotation and Learnable Non-uniform Quantizer 
[[arxiv](https://arxiv.org/abs/2502.15779)] [[cool](https://papers.cool/arxiv/2502.15779)] [[pdf](https://arxiv.org/pdf/2502.15779)]
> **Authors**: Euntae Choi,Sumin Song,Woosang Lim,Sungjoo Yoo
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: We propose Rotate, Clip, and Partition (RCP), a quantization-aware training (QAT) approach that first realizes extreme compression of LLMs with W2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP integrates recent rotation techniques with a novel non-uniform weight quantizer design, by quantitatively analyzing the impact of random rotation on 2-bit weight quantization. Our weight quantizer features Learnable Direct Partitioning (LDP), which introduces learnable parameters to directly learn non-uniform intervals jointly with LLM weights. We also present a specialized GPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP can compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and 5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging mobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and MetaMath-7B with no critical problems such as convergence failure and repetition. Code will be made available at blind_review.

### Learning to Reason from Feedback at Test-Time 
[[arxiv](https://arxiv.org/abs/2502.15771)] [[cool](https://papers.cool/arxiv/2502.15771)] [[pdf](https://arxiv.org/pdf/2502.15771)]
> **Authors**: Yanyang Li,Michael Lyu,Liwei Wang
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-24
> **comment**: The code is at https://github.com/LaVi-Lab/FTTT
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Solving complex tasks in a single attempt is challenging for large language models (LLMs). Iterative interaction with the environment and feedback is often required to achieve success, making effective feedback utilization a critical topic. Existing approaches either struggle with length generalization or rely on naive retries without leveraging prior information. In this paper, we introduce FTTT, a novel paradigm that formulates feedback utilization as an optimization problem at test time. Additionally, we propose a learnable test-time optimizer, OpTune, to effectively exploit feedback. Experiments on two LLMs across four reasoning datasets demonstrate that FTTT and OpTune achieve superior scalability and performance.

### Exploring the Role of Artificial Intelligence and Machine Learning in Process Optimization for Chemical Industry 
[[arxiv](https://arxiv.org/abs/2502.15768)] [[cool](https://papers.cool/arxiv/2502.15768)] [[pdf](https://arxiv.org/pdf/2502.15768)]
> **Authors**: Zishuo Lin,Jiajie Wang,Zhe Yan,Peiyong Ma
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别,光学
- **Abstract**: The crucial field of Optical Chemical Structure Recognition (OCSR) aims to transform chemical structure photographs into machine-readable formats so that chemical databases may be efficiently stored and queried. Although a number of OCSR technologies have been created, little is known about how well they work in different picture deterioration scenarios. In this work, a new dataset of chemically structured images that have been systematically harmed graphically by compression, noise, distortion, and black overlays is presented. On these subsets, publicly accessible OCSR tools were thoroughly tested to determine how resilient they were to unfavorable circumstances. The outcomes show notable performance variation, underscoring each tool's advantages and disadvantages. Interestingly, MolScribe performed best under heavy compression (55.8% at 99%) and had the highest identification rate on undamaged photos (94.6%). MolVec performed exceptionally well against noise and black overlay (86.8% at 40%), although it declined under extreme distortion (<70%). With recognition rates below 30%, Decimer demonstrated strong sensitivity to noise and black overlay, but Imago had the lowest baseline accuracy (73.6%). The creative assessment of this study offers important new information about how well the OCSR tool performs when images deteriorate, as well as useful standards for tool development in the future.

### Generalized Attention Flow: Feature Attribution for Transformer Models via Maximum Flow 
[[arxiv](https://arxiv.org/abs/2502.15765)] [[cool](https://papers.cool/arxiv/2502.15765)] [[pdf](https://arxiv.org/pdf/2502.15765)]
> **Authors**: Behrooz Azarkhalili,Maxwell Libbrecht
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: This paper introduces Generalized Attention Flow (GAF), a novel feature attribution method for Transformer-based models to address the limitations of current approaches. By extending Attention Flow and replacing attention weights with the generalized Information Tensor, GAF integrates attention weights, their gradients, the maximum flow problem, and the barrier method to enhance the performance of feature attributions. The proposed method exhibits key theoretical properties and mitigates the shortcomings of prior techniques that rely solely on simple aggregation of attention weights. Our comprehensive benchmarking on sequence classification tasks demonstrates that a specific variant of GAF consistently outperforms state-of-the-art feature attribution methods in most evaluation settings, providing a more reliable interpretation of Transformer model outputs.

### High-Throughput Computational Screening and Interpretable Machine Learning of Metal-organic Frameworks for Iodine Capture 
[[arxiv](https://arxiv.org/abs/2502.15764)] [[cool](https://papers.cool/arxiv/2502.15764)] [[pdf](https://arxiv.org/pdf/2502.15764)]
> **Authors**: Haoyi Tan,Yukun Teng,Guangcun Shan
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-24
> **comment**: 13 page,6 figures, submitted to npjCM
- **标题**: None
- **领域**: 机器学习,材料科学,人工智能,计算物理
- **Abstract**: The removal of leaked radioactive iodine isotopes in humid environments holds significant importance in nuclear waste management and nuclear accident mitigation. In this study, high-throughput computational screening and machine learning were combined to reveal the iodine capture performance of 1816 metal-organic framework (MOF) materials under humid air conditions. Firstly, the relationship between the structural characteristics of MOFs and their adsorption properties was explored, with the aim of identifying the optimal structural parameters for iodine capture. Subsequently, two machine learning regression algorithms - Random Forest and CatBoost, were employed to predict the iodine adsorption capabilities of MOFs. In addition to 6 structural features, 25 molecular features and 8 chemical features were incorporated to enhance the prediction accuracy of the machine learning algorithms. Feature importance was assessed to determine the relative influence of various features on iodine adsorption performance, in which the Henry's coefficient and heat of adsorption to iodine were found the two most crucial chemical factors. Furthermore, four types of molecular fingerprints were introduced for providing comprehensive and detailed structural information of MOF materials. The top 20 most significant MACCS molecular fingerprints were picked out, revealing that the presence of six-membered ring structures and nitrogen atoms in the MOFs were the key structural factors that enhanced iodine adsorption, followed by the existence of oxygen atoms. This work combined high-throughput computation, machine learning, and molecular fingerprints to comprehensively elucidate the multifaceted factors influencing the iodine adsorption performance of MOFs, offering profound insightful guidelines for screening and structural design of advanced MOF materials.

### Digi-Q: Learning Q-Value Functions for Training Device-Control Agents 
[[arxiv](https://arxiv.org/abs/2502.15760)] [[cool](https://papers.cool/arxiv/2502.15760)] [[pdf](https://arxiv.org/pdf/2502.15760)]
> **Authors**: Hao Bai,Yifei Zhou,Li Erran Li,Sergey Levine,Aviral Kumar
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-24
> **comment**: Accepted to ICLR 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: While a number of existing approaches for building foundation model agents rely on prompting or fine-tuning with human demonstrations, it is not sufficient in dynamic environments (e.g., mobile device control). On-policy reinforcement learning (RL) should address these limitations, but collecting actual rollouts in an environment is often undesirable in truly open-ended agentic problems such as mobile device control or interacting with humans, where each unit of interaction is associated with a cost. In such scenarios, a method for policy learning that can utilize off-policy experience by learning a trained action-value function is much more effective. In this paper, we develop an approach, called Digi-Q, to train VLM-based action-value Q-functions which are then used to extract the agent policy. We study our approach in the mobile device control setting. Digi-Q trains the Q-function using offline temporal-difference (TD) learning, on top of frozen, intermediate-layer features of a VLM. Compared to fine-tuning the whole VLM, this approach saves us compute and enhances scalability. To make the VLM features amenable for representing the Q-function, we need to employ an initial phase of fine-tuning to amplify coverage over actionable information needed for value function. Once trained, we use this Q-function via a Best-of-N policy extraction operator that imitates the best action out of multiple candidate actions from the current policy as ranked by the value function, enabling policy improvement without environment interaction. Digi-Q outperforms several prior methods on user-scale device control tasks in Android-in-the-Wild, attaining 21.2% improvement over prior best-performing method. In some cases, our Digi-Q approach already matches state-of-the-art RL methods that require interaction. The project is open-sourced at https://github.com/DigiRL-agent/digiq

### TRKM: Twin Restricted Kernel Machines for Classification and Regression 
[[arxiv](https://arxiv.org/abs/2502.15759)] [[cool](https://papers.cool/arxiv/2502.15759)] [[pdf](https://arxiv.org/pdf/2502.15759)]
> **Authors**: A. Quadir,M. Tanveer
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Restricted kernel machines (RKMs) have considerably improved generalization in machine learning. Recent advancements explored various techniques within the RKM framework, integrating kernel functions with least squares support vector machines (LSSVM) to mirror the energy function of restricted Boltzmann machines (RBM), leading to enhanced performance. However, RKMs may face challenges in generalization when dealing with unevenly distributed or complexly clustered data. Additionally, as the dataset size increases, the computational burden of managing high-dimensional feature spaces can become substantial, potentially hindering performance in large-scale datasets. To address these challenges, we propose twin restricted kernel machine (TRKM). TRKM combines the benefits of twin models with the robustness of the RKM framework to enhance classification and regression tasks. By leveraging the Fenchel-Young inequality, we introduce a novel conjugate feature duality, allowing the formulation of classification and regression problems in terms of dual variables. This duality provides an upper bound to the objective function of the TRKM problem, resulting in a new methodology under the RKM framework. The model uses an energy function similar to that of RBM, incorporating both visible and hidden variables corresponding to both classes. Additionally, the kernel trick is employed to map data into a high-dimensional feature space, where the model identifies an optimal separating hyperplane using a regularized least squares approach. Experiments on UCI and KEEL datasets confirm TRKM's superiority over baselines, showcasing its robustness and efficiency in handling complex data. Furthermore, We implemented the TRKM model on the brain age dataset, demonstrating its efficacy in predicting brain age.

### Maturity Framework for Enhancing Machine Learning Quality 
[[arxiv](https://arxiv.org/abs/2502.15758)] [[cool](https://papers.cool/arxiv/2502.15758)] [[pdf](https://arxiv.org/pdf/2502.15758)]
> **Authors**: Angelantonio Castelli,Georgios Christos Chouliaras,Dmitri Goldenberg
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机与社会,软件工程
- **Abstract**: With the rapid integration of Machine Learning (ML) in business applications and processes, it is crucial to ensure the quality, reliability and reproducibility of such systems. We suggest a methodical approach towards ML system quality assessment and introduce a structured Maturity framework for governance of ML. We emphasize the importance of quality in ML and the need for rigorous assessment, driven by issues in ML governance and gaps in existing frameworks. Our primary contribution is a comprehensive open-sourced quality assessment method, validated with empirical evidence, accompanied by a systematic maturity framework tailored to ML systems. Drawing from applied experience at Booking.com, we discuss challenges and lessons learned during large-scale adoption within organizations. The study presents empirical findings, highlighting quality improvement trends and showcasing business outcomes. The maturity framework for ML systems, aims to become a valuable resource to reshape industry standards and enable a structural approach to improve ML maturity in any organization.

### Causal Covariate Shift Correction using Fisher information penalty 
[[arxiv](https://arxiv.org/abs/2502.15756)] [[cool](https://papers.cool/arxiv/2502.15756)] [[pdf](https://arxiv.org/pdf/2502.15756)]
> **Authors**: Behraj Khan,Behroz Mirza,Tahir Syed
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Evolving feature densities across batches of training data bias cross-validation, making model selection and assessment unreliable (\cite{sugiyama2012machine}). This work takes a distributed density estimation angle to the training setting where data are temporally distributed. \textit{Causal Covariate Shift Correction ($C^{3}$)}, accumulates knowledge about the data density of a training batch using Fisher Information, and using it to penalize the loss in all subsequent batches. The penalty improves accuracy by $12.9\%$ over the full-dataset baseline, by $20.3\%$ accuracy at maximum in batchwise and $5.9\%$ at minimum in foldwise benchmarks.

### Physics-consistent machine learning: output projection onto physical manifolds 
[[arxiv](https://arxiv.org/abs/2502.15755)] [[cool](https://papers.cool/arxiv/2502.15755)] [[pdf](https://arxiv.org/pdf/2502.15755)]
> **Authors**: Matilde Valente,Tiago C. Dias,Vasco Guerra,Rodrigo Ventura
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-24
> **comment**: 22 pages, 5 figures
- **标题**: None
- **领域**: 机器学习,人工智能,等离子体物理
- **Abstract**: Data-driven machine learning models often require extensive datasets, which can be costly or inaccessible, and their predictions may fail to comply with established physical laws. Current approaches for incorporating physical priors mitigate these issues by penalizing deviations from known physical laws, as in physics-informed neural networks, or by designing architectures that automatically satisfy specific invariants. However, penalization approaches do not guarantee compliance with physical constraints for unseen inputs, and invariant-based methods lack flexibility and generality. We propose a novel physics-consistent machine learning method that directly enforces compliance with physical principles by projecting model outputs onto the manifold defined by these laws. This procedure ensures that predictions inherently adhere to the chosen physical constraints, improving reliability and interpretability. Our method is demonstrated on two systems: a spring-mass system and a low-temperature reactive plasma. Compared to purely data-driven models, our approach significantly reduces errors in physical law compliance, enhances predictive accuracy of physical quantities, and outperforms alternatives when working with simpler models or limited datasets. The proposed projection-based technique is versatile and can function independently or in conjunction with existing physics-informed neural networks, offering a powerful, general, and scalable solution for developing fast and reliable surrogate models of complex physical systems, particularly in resource-constrained scenarios.

### Detecting Content Rating Violations in Android Applications: A Vision-Language Approach 
[[arxiv](https://arxiv.org/abs/2502.15739)] [[cool](https://papers.cool/arxiv/2502.15739)] [[pdf](https://arxiv.org/pdf/2502.15739)]
> **Authors**: D. Denipitiyage,B. Silva,S. Seneviratne,A. Seneviratne,S. Chawla
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-24
> **comment**: 11 pages, 8 figures
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别,多媒体
- **Abstract**: Despite regulatory efforts to establish reliable content-rating guidelines for mobile apps, the process of assigning content ratings in the Google Play Store remains self-regulated by the app developers. There is no straightforward method of verifying developer-assigned content ratings manually due to the overwhelming scale or automatically due to the challenging problem of interpreting textual and visual data and correlating them with content ratings. We propose and evaluate a visionlanguage approach to predict the content ratings of mobile game applications and detect content rating violations, using a dataset of metadata of popular Android games. Our method achieves ~6% better relative accuracy compared to the state-of-the-art CLIP-fine-tuned model in a multi-modal setting. Applying our classifier in the wild, we detected more than 70 possible cases of content rating violations, including nine instances with the 'Teacher Approved' badge. Additionally, our findings indicate that 34.5% of the apps identified by our classifier as violating content ratings were removed from the Play Store. In contrast, the removal rate for correctly classified apps was only 27%. This discrepancy highlights the practical effectiveness of our classifier in identifying apps that are likely to be removed based on user complaints.

### Data Wrangling Task Automation Using Code-Generating Language Models 
[[arxiv](https://arxiv.org/abs/2502.15732)] [[cool](https://papers.cool/arxiv/2502.15732)] [[pdf](https://arxiv.org/pdf/2502.15732)]
> **Authors**: Ashlesha Akella,Krishnasuri Narayanam
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-24
> **comment**: Accepted at AAAI 2025 Demo
- **标题**: None
- **领域**: 机器学习,人工智能,数据库,软件工程
- **Abstract**: Ensuring data quality in large tabular datasets is a critical challenge, typically addressed through data wrangling tasks. Traditional statistical methods, though efficient, cannot often understand the semantic context and deep learning approaches are resource-intensive, requiring task and dataset-specific training. To overcome these shortcomings, we present an automated system that utilizes large language models to generate executable code for tasks like missing value imputation, error detection, and error correction. Our system aims to identify inherent patterns in the data while leveraging external knowledge, effectively addressing both memory-dependent and memory-independent tasks.

### One-step Diffusion Models with $f$-Divergence Distribution Matching 
[[arxiv](https://arxiv.org/abs/2502.15681)] [[cool](https://papers.cool/arxiv/2502.15681)] [[pdf](https://arxiv.org/pdf/2502.15681)]
> **Authors**: Yilun Xu,Weili Nie,Arash Vahdat
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: Sampling from diffusion models involves a slow iterative process that hinders their practical deployment, especially for interactive applications. To accelerate generation speed, recent approaches distill a multi-step diffusion model into a single-step student generator via variational score distillation, which matches the distribution of samples generated by the student to the teacher's distribution. However, these approaches use the reverse Kullback-Leibler (KL) divergence for distribution matching which is known to be mode seeking. In this paper, we generalize the distribution matching approach using a novel $f$-divergence minimization framework, termed $f$-distill, that covers different divergences with different trade-offs in terms of mode coverage and training variance. We derive the gradient of the $f$-divergence between the teacher and student distributions and show that it is expressed as the product of their score differences and a weighting function determined by their density ratio. This weighting function naturally emphasizes samples with higher density in the teacher distribution, when using a less mode-seeking divergence. We observe that the popular variational score distillation approach using the reverse-KL divergence is a special case within our framework. Empirically, we demonstrate that alternative $f$-divergences, such as forward-KL and Jensen-Shannon divergences, outperform the current best variational score distillation methods across image generation tasks. In particular, when using Jensen-Shannon divergence, $f$-distill achieves current state-of-the-art one-step generation performance on ImageNet64 and zero-shot text-to-image generation on MS-COCO. Project page: https://research.nvidia.com/labs/genair/f-distill

### Testing the limits of fine-tuning to improve reasoning in vision language models 
[[arxiv](https://arxiv.org/abs/2502.15678)] [[cool](https://papers.cool/arxiv/2502.15678)] [[pdf](https://arxiv.org/pdf/2502.15678)]
> **Authors**: Luca M. Schulze Buschoff,Konstantinos Voudouris,Elif Akata,Matthias Bethge,Joshua B. Tenenbaum,Eric Schulz
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Pre-trained vision language models still fall short of human visual cognition. In an effort to improve visual cognition and align models with human behavior, we introduce visual stimuli and human judgments on visual cognition tasks, allowing us to systematically evaluate performance across cognitive domains under a consistent environment. We fine-tune models on ground truth data for intuitive physics and causal reasoning and find that this improves model performance in the respective fine-tuning domain. Furthermore, it can improve model alignment with human behavior. However, we find that fine-tuning does not contribute to robust human-like generalization to data with other visual characteristics or to tasks in other cognitive domains.

### Logit Disagreement: OoD Detection with Bayesian Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.15648)] [[cool](https://papers.cool/arxiv/2502.15648)] [[pdf](https://arxiv.org/pdf/2502.15648)]
> **Authors**: Kevin Raina
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Presented at ECCV 2024 Workshop: 3rd Workshop on Uncertainty Quantification for Computer Vision
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别,机器学习
- **Abstract**: Bayesian neural networks (BNNs), which estimate the full posterior distribution over model parameters, are well-known for their role in uncertainty quantification and its promising application in out-of-distribution detection (OoD). Amongst other uncertainty measures, BNNs provide a state-of-the art estimation of predictive entropy (total uncertainty) which can be decomposed as the sum of mutual information and expected entropy. In the context of OoD detection the estimation of predictive uncertainty in the form of the predictive entropy score confounds aleatoric and epistemic uncertainty, the latter being hypothesized to be high for OoD points. Despite these justifications, the mutual information score has been shown to perform worse than predictive entropy. Taking inspiration from Bayesian variational autoencoder (BVAE) literature, this work proposes to measure the disagreement between a corrected version of the pre-softmax quantities, otherwise known as logits, as an estimate of epistemic uncertainty for Bayesian NNs under mean field variational inference. The three proposed epistemic uncertainty scores demonstrate marked improvements over mutual information on a range of OoD experiments, with equal performance otherwise. Moreover, the epistemic uncertainty scores perform on par with the Bayesian benchmark predictive entropy on a range of MNIST and CIFAR10 experiments.

### Predicting gene essentiality and drug response from perturbation screens in preclinical cancer models with LEAP: Layered Ensemble of Autoencoders and Predictors 
[[arxiv](https://arxiv.org/abs/2502.15646)] [[cool](https://papers.cool/arxiv/2502.15646)] [[pdf](https://arxiv.org/pdf/2502.15646)]
> **Authors**: Barbara Bodinier,Gaetan Dissez,Linus Bleistein,Antonin Dauvin
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Preclinical perturbation screens, where the effects of genetic, chemical, or environmental perturbations are systematically tested on disease models, hold significant promise for machine learning-enhanced drug discovery due to their scale and causal nature. Predictive models can infer perturbation responses for previously untested disease models based on molecular profiles. These in silico labels can expand databases and guide experimental prioritization. However, modelling perturbation-specific effects and generating robust prediction performances across diverse biological contexts remain elusive. We introduce LEAP (Layered Ensemble of Autoencoders and Predictors), a novel ensemble framework to improve robustness and generalization. LEAP leverages multiple DAMAE (Data Augmented Masked Autoencoder) representations and LASSO regressors. By combining diverse gene expression representation models learned from different random initializations, LEAP consistently outperforms state-of-the-art approaches in predicting gene essentiality or drug responses in unseen cell lines, tissues and disease models. Notably, our results show that ensembling representation models, rather than prediction models alone, yields superior predictive performance. Beyond its performance gains, LEAP is computationally efficient, requires minimal hyperparameter tuning and can therefore be readily incorporated into drug discovery pipelines to prioritize promising targets and support biomarker-driven stratification. The code and datasets used in this work are made publicly available.

### AutoTandemML: Active Learning Enhanced Tandem Neural Networks for Inverse Design Problems 
[[arxiv](https://arxiv.org/abs/2502.15643)] [[cool](https://papers.cool/arxiv/2502.15643)] [[pdf](https://arxiv.org/pdf/2502.15643)]
> **Authors**: Luka Grbcic,Juliane Müller,Wibe Albert de Jong
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算工程、金融和科学,神经和进化计算
- **Abstract**: Inverse design in science and engineering involves determining optimal design parameters that achieve desired performance outcomes, a process often hindered by the complexity and high dimensionality of design spaces, leading to significant computational costs. To tackle this challenge, we propose a novel hybrid approach that combines active learning with Tandem Neural Networks to enhance the efficiency and effectiveness of solving inverse design problems. Active learning allows to selectively sample the most informative data points, reducing the required dataset size without compromising accuracy. We investigate this approach using three benchmark problems: airfoil inverse design, photonic surface inverse design, and scalar boundary condition reconstruction in diffusion partial differential equations. We demonstrate that integrating active learning with Tandem Neural Networks outperforms standard approaches across the benchmark suite, achieving better accuracy with fewer training samples.

### Training Neural ODEs Using Fully Discretized Simultaneous Optimization 
[[arxiv](https://arxiv.org/abs/2502.15642)] [[cool](https://papers.cool/arxiv/2502.15642)] [[pdf](https://arxiv.org/pdf/2502.15642)]
> **Authors**: Mariia Shapovalova,Calvin Tsay
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Accepted to the 14th IFAC Symposium on Dynamics and Control of Process Systems, including Biosystems (DYCOPS 2025)
- **标题**: None
- **领域**: 机器学习,优化与控制
- **Abstract**: Neural Ordinary Differential Equations (Neural ODEs) represent continuous-time dynamics with neural networks, offering advancements for modeling and control tasks. However, training Neural ODEs requires solving differential equations at each epoch, leading to high computational costs. This work investigates simultaneous optimization methods as a faster training alternative. In particular, we employ a collocation-based, fully discretized formulation and use IPOPT--a solver for large-scale nonlinear optimization--to simultaneously optimize collocation coefficients and neural network parameters. Using the Van der Pol Oscillator as a case study, we demonstrate faster convergence compared to traditional training methods. Furthermore, we introduce a decomposition framework utilizing Alternating Direction Method of Multipliers (ADMM) to effectively coordinate sub-models among data batches. Our results show significant potential for (collocation-based) simultaneous Neural ODE training pipelines.

### Mantis: Lightweight Calibrated Foundation Model for User-Friendly Time Series Classification 
[[arxiv](https://arxiv.org/abs/2502.15637)] [[cool](https://papers.cool/arxiv/2502.15637)] [[pdf](https://arxiv.org/pdf/2502.15637)]
> **Authors**: Vasilii Feofanov,Songkang Wen,Marius Alonso,Romain Ilbert,Hongbo Guo,Malik Tiomoko,Lujia Pan,Jianfeng Zhang,Ievgen Redko
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: In recent years, there has been increasing interest in developing foundation models for time series data that can generalize across diverse downstream tasks. While numerous forecasting-oriented foundation models have been introduced, there is a notable scarcity of models tailored for time series classification. To address this gap, we present Mantis, a new open-source foundation model for time series classification based on the Vision Transformer (ViT) architecture that has been pre-trained using a contrastive learning approach. Our experimental results show that Mantis outperforms existing foundation models both when the backbone is frozen and when fine-tuned, while achieving the lowest calibration error. In addition, we propose several adapters to handle the multivariate setting, reducing memory requirements and modeling channel interdependence.

### The Relationship Between Reasoning and Performance in Large Language Models -- o3 (mini) Thinks Harder, Not Longer 
[[arxiv](https://arxiv.org/abs/2502.15631)] [[cool](https://papers.cool/arxiv/2502.15631)] [[pdf](https://arxiv.org/pdf/2502.15631)]
> **Authors**: Marthe Ballon,Andres Algaba,Vincent Ginis
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 19 pages, 11 figures
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Large language models have demonstrated remarkable progress in mathematical reasoning, leveraging chain-of-thought and test-time compute scaling. However, many open questions remain regarding the interplay between reasoning token usage and accuracy gains. In particular, when comparing models across generations, it is unclear whether improved performance results from longer reasoning chains or more efficient reasoning. We systematically analyze chain-of-thought length across o1-mini and o3-mini variants on the Omni-MATH benchmark, finding that o3-mini (m) achieves superior accuracy without requiring longer reasoning chains than o1-mini. Moreover, we show that accuracy generally declines as reasoning chains grow across all models and compute settings, even when controlling for difficulty of the questions. This accuracy drop is significantly smaller in more proficient models, suggesting that new generations of reasoning models use test-time compute more effectively. Finally, we highlight that while o3-mini (h) achieves a marginal accuracy gain over o3-mini (m), it does so by allocating substantially more reasoning tokens across all problems, even the ones that o3-mini (m) can already solve. These findings provide new insights into the relationship between model capability and reasoning length, with implications for efficiency, scaling, and evaluation methodologies.

### PDeepPP:A Deep learning framework with Pretrained Protein language for peptide classification 
[[arxiv](https://arxiv.org/abs/2502.15610)] [[cool](https://papers.cool/arxiv/2502.15610)] [[pdf](https://arxiv.org/pdf/2502.15610)]
> **Authors**: Jixiu Zhai,Tianchi Lu,Haitian Zhong,Ziyang Xu,Yuhuan Liu,Xueying Wang,Dan Huang
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 10 pages, 5 figures, submitted to arXiv
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Protein post-translational modifications (PTMs) and bioactive peptides (BPs) play critical roles in various biological processes and have significant therapeutic potential. However, identifying PTM sites and bioactive peptides through experimental methods is often labor-intensive, costly, and time-consuming. As a result, computational tools, particularly those based on deep learning, have become effective solutions for predicting PTM sites and peptide bioactivity. Despite progress in this field, existing methods still struggle with the complexity of protein sequences and the challenge of requiring high-quality predictions across diverse datasets. To address these issues, we propose a deep learning framework that integrates pretrained protein language models with a neural network combining transformer and CNN for peptide classification. By leveraging the ability of pretrained models to capture complex relationships within protein sequences, combined with the predictive power of parallel networks, our approach improves feature extraction while enhancing prediction accuracy. This framework was applied to multiple tasks involving PTM site and bioactive peptide prediction, utilizing large-scale datasets to enhance the model's robustness. In the comparison across 33 tasks, the model achieved state-of-the-art (SOTA) performance in 25 of them, surpassing existing methods and demonstrating its versatility across different datasets. Our results suggest that this approach provides a scalable and effective solution for large-scale peptide discovery and PTM analysis, paving the way for more efficient peptide classification and functional annotation.

### Improving the Scaling Laws of Synthetic Data with Deliberate Practice 
[[arxiv](https://arxiv.org/abs/2502.15588)] [[cool](https://papers.cool/arxiv/2502.15588)] [[pdf](https://arxiv.org/pdf/2502.15588)]
> **Authors**: Reyhane Askari-Hemmat,Mohammad Pezeshki,Elvis Dohmatob,Florian Bordes,Pietro Astolfi,Melissa Hall,Jakob Verbeek,Michal Drozdzal,Adriana Romero-Soriano
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Inspired by the principle of deliberate practice in human learning, we propose Deliberate Practice for Synthetic Data Generation (DP), a novel framework that improves sample efficiency through dynamic synthetic data generation. Prior work has shown that scaling synthetic data is inherently challenging, as naively adding new data leads to diminishing returns. To address this, pruning has been identified as a key mechanism for improving scaling, enabling models to focus on the most informative synthetic samples. Rather than generating a large dataset and pruning it afterward, DP efficiently approximates the direct generation of informative samples. We theoretically show how training on challenging, informative examples improves scaling laws and empirically validate that DP achieves better scaling performance with significantly fewer training samples and iterations. On ImageNet-100, DP generates 3.4x fewer samples and requires six times fewer iterations, while on ImageNet-1k, it generates 8x fewer samples with a 30 percent reduction in iterations, all while achieving superior performance compared to prior work.

### A Cautionary Tale About "Neutrally" Informative AI Tools Ahead of the 2025 Federal Elections in Germany 
[[arxiv](https://arxiv.org/abs/2502.15568)] [[cool](https://papers.cool/arxiv/2502.15568)] [[pdf](https://arxiv.org/pdf/2502.15568)]
> **Authors**: Ina Dormuth,Sven Franke,Marlies Hafer,Tim Katzke,Alexander Marx,Emmanuel Müller,Daniel Neider,Markus Pauly,Jérôme Rutinowski
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: In this study, we examine the reliability of AI-based Voting Advice Applications (VAAs) and large language models (LLMs) in providing objective political information. Our analysis is based upon a comparison with party responses to 38 statements of the Wahl-O-Mat, a well-established German online tool that helps inform voters by comparing their views with political party positions. For the LLMs, we identify significant biases. They exhibit a strong alignment (over 75% on average) with left-wing parties and a substantially lower alignment with center-right (smaller 50%) and right-wing parties (around 30%). Furthermore, for the VAAs, intended to objectively inform voters, we found substantial deviations from the parties' stated positions in Wahl-O-Mat: While one VAA deviated in 25% of cases, another VAA showed deviations in more than 50% of cases. For the latter, we even observed that simple prompt injections led to severe hallucinations, including false claims such as non-existent connections between political parties and right-wing extremist ties.

### Model Privacy: A Unified Framework to Understand Model Stealing Attacks and Defenses 
[[arxiv](https://arxiv.org/abs/2502.15567)] [[cool](https://papers.cool/arxiv/2502.15567)] [[pdf](https://arxiv.org/pdf/2502.15567)]
> **Authors**: Ganghua Wang,Yuhong Yang,Jie Ding
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: The use of machine learning (ML) has become increasingly prevalent in various domains, highlighting the importance of understanding and ensuring its safety. One pressing concern is the vulnerability of ML applications to model stealing attacks. These attacks involve adversaries attempting to recover a learned model through limited query-response interactions, such as those found in cloud-based services or on-chip artificial intelligence interfaces. While existing literature proposes various attack and defense strategies, these often lack a theoretical foundation and standardized evaluation criteria. In response, this work presents a framework called ``Model Privacy'', providing a foundation for comprehensively analyzing model stealing attacks and defenses. We establish a rigorous formulation for the threat model and objectives, propose methods to quantify the goodness of attack and defense strategies, and analyze the fundamental tradeoffs between utility and privacy in ML models. Our developed theory offers valuable insights into enhancing the security of ML models, especially highlighting the importance of the attack-specific structure of perturbations for effective defenses. We demonstrate the application of model privacy from the defender's perspective through various learning scenarios. Extensive experiments corroborate the insights and the effectiveness of defense mechanisms developed under the proposed framework.

### Solving Inverse Problems with Deep Linear Neural Networks: Global Convergence Guarantees for Gradient Descent with Weight Decay 
[[arxiv](https://arxiv.org/abs/2502.15522)] [[cool](https://papers.cool/arxiv/2502.15522)] [[pdf](https://arxiv.org/pdf/2502.15522)]
> **Authors**: Hannah Laus,Suzanna Parkinson,Vasileios Charisopoulos,Felix Krahmer,Rebecca Willett
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,优化与控制
- **Abstract**: Machine learning methods are commonly used to solve inverse problems, wherein an unknown signal must be estimated from few measurements generated via a known acquisition procedure. In particular, neural networks perform well empirically but have limited theoretical guarantees. In this work, we study an underdetermined linear inverse problem that admits several possible solution mappings. A standard remedy (e.g., in compressed sensing) establishing uniqueness of the solution mapping is to assume knowledge of latent low-dimensional structure in the source signal. We ask the following question: do deep neural networks adapt to this low-dimensional structure when trained by gradient descent with weight decay regularization? We prove that mildly overparameterized deep linear networks trained in this manner converge to an approximate solution that accurately solves the inverse problem while implicitly encoding latent subspace structure. To our knowledge, this is the first result to rigorously show that deep linear networks trained with weight decay automatically adapt to latent subspace structure in the data under practical stepsize and weight initialization schemes. Our work highlights that regularization and overparameterization improve generalization, while overparameterization also accelerates convergence during training.

### SALSA-RL: Stability Analysis in the Latent Space of Actions for Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.15512)] [[cool](https://papers.cool/arxiv/2502.15512)] [[pdf](https://arxiv.org/pdf/2502.15512)]
> **Authors**: Xuyang Li,Romit Maulik
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Modern deep reinforcement learning (DRL) methods have made significant advances in handling continuous action spaces. However, real-world control systems--especially those requiring precise and reliable performance--often demand formal stability, and existing DRL approaches typically lack explicit mechanisms to ensure or analyze stability. To address this limitation, we propose SALSA-RL (Stability Analysis in the Latent Space of Actions), a novel RL framework that models control actions as dynamic, time-dependent variables evolving within a latent space. By employing a pre-trained encoder-decoder and a state-dependent linear system, our approach enables both stability analysis and interpretability. We demonstrated that SALSA-RL can be deployed in a non-invasive manner for assessing the local stability of actions from pretrained RL agents without compromising on performance across diverse benchmark environments. By enabling a more interpretable analysis of action generation, SALSA-RL provides a powerful tool for advancing the design, analysis, and theoretical understanding of RL systems.

### Activation Steering in Neural Theorem Provers 
[[arxiv](https://arxiv.org/abs/2502.15507)] [[cool](https://papers.cool/arxiv/2502.15507)] [[pdf](https://arxiv.org/pdf/2502.15507)]
> **Authors**: Shashank Kirtania
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Large Language Models (LLMs) have shown promise in proving formal theorems using proof assistants like Lean. However, current state of the art language models struggles to predict next step in proofs leading practitioners to use different sampling techniques to improve LLMs capabilities. We observe that the LLM is capable of predicting the correct tactic; however, it faces challenges in ranking it appropriately within the set of candidate tactics, affecting the overall selection process. To overcome this hurdle, we use activation steering to guide LLMs responses to improve the generations at the time of inference. Our results suggest that activation steering offers a promising lightweight alternative to specialized fine-tuning for enhancing theorem proving capabilities in LLMs, particularly valuable in resource-constrained environments.

### Verification and Validation for Trustworthy Scientific Machine Learning 
[[arxiv](https://arxiv.org/abs/2502.15496)] [[cool](https://papers.cool/arxiv/2502.15496)] [[pdf](https://arxiv.org/pdf/2502.15496)]
> **Authors**: John D. Jakeman,Lorena A. Barba,Joaquim R. R. A. Martins,Thomas O'Leary-Roseberry
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: mber:SAND2025-01935OMSC Class:68T07; 68N30ACM Class:I.6.4; I.6.5; G.4
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Scientific machine learning (SciML) models are transforming many scientific disciplines. However, the development of good modeling practices to increase the trustworthiness of SciML has lagged behind its application, limiting its potential impact. The goal of this paper is to start a discussion on establishing consensus-based good practices for predictive SciML. We identify key challenges in applying existing computational science and engineering guidelines, such as verification and validation protocols, and provide recommendations to address these challenges. Our discussion focuses on predictive SciML, which uses machine learning models to learn, improve, and accelerate numerical simulations of physical systems. While centered on predictive applications, our 16 recommendations aim to help researchers conduc

### Network Resource Optimization for ML-Based UAV Condition Monitoring with Vibration Analysis 
[[arxiv](https://arxiv.org/abs/2502.15491)] [[cool](https://papers.cool/arxiv/2502.15491)] [[pdf](https://arxiv.org/pdf/2502.15491)]
> **Authors**: Alexandre Gemayel,Dimitrios Michael Manias,Abdallah Shami
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Accepted for publication in IEEE Networking Letters
- **标题**: None
- **领域**: 机器学习,网络和互联网架构,信号处理,系统与控制
- **Abstract**: As smart cities begin to materialize, the role of Unmanned Aerial Vehicles (UAVs) and their reliability becomes increasingly important. One aspect of reliability relates to Condition Monitoring (CM), where Machine Learning (ML) models are leveraged to identify abnormal and adverse conditions. Given the resource-constrained nature of next-generation edge networks, the utilization of precious network resources must be minimized. This work explores the optimization of network resources for ML-based UAV CM frameworks. The developed framework uses experimental data and varies the feature extraction aggregation interval to optimize ML model selection. Additionally, by leveraging dimensionality reduction techniques, there is a 99.9% reduction in network resource consumption.

### MoMa: A Modular Deep Learning Framework for Material Property Prediction 
[[arxiv](https://arxiv.org/abs/2502.15483)] [[cool](https://papers.cool/arxiv/2502.15483)] [[pdf](https://arxiv.org/pdf/2502.15483)]
> **Authors**: Botian Wang,Yawen Ouyang,Yaohui Li,Yiqun Wang,Haorui Cui,Jianbing Zhang,Xiaonan Wang,Wei-Ying Ma,Hao Zhou
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,材料科学
- **Abstract**: Deep learning methods for material property prediction have been widely explored to advance materials discovery. However, the prevailing pre-train then fine-tune paradigm often fails to address the inherent diversity and disparity of material tasks. To overcome these challenges, we introduce MoMa, a Modular framework for Materials that first trains specialized modules across a wide range of tasks and then adaptively composes synergistic modules tailored to each downstream scenario. Evaluation across 17 datasets demonstrates the superiority of MoMa, with a substantial 14% average improvement over the strongest baseline. Few-shot and continual learning experiments further highlight MoMa's potential for real-world applications. Pioneering a new paradigm of modular material learning, MoMa will be open-sourced to foster broader community collaboration.

### Decoding for Punctured Convolutional and Turbo Codes: A Deep Learning Solution for Protocols Compliance 
[[arxiv](https://arxiv.org/abs/2502.15475)] [[cool](https://papers.cool/arxiv/2502.15475)] [[pdf](https://arxiv.org/pdf/2502.15475)]
> **Authors**: Yongli Yan,Linglong Dai
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,信息论
- **Abstract**: Neural network-based decoding methods have shown promise in enhancing error correction performance, but traditional approaches struggle with the challenges posed by punctured codes. In particular, these methods fail to address the complexities of variable code rates and the need for protocol compatibility. This paper presents a unified Long Short-Term Memory (LSTM)-based decoding architecture specifically designed to overcome these challenges. The proposed method unifies punctured convolutional and Turbo codes. A puncture embedding mechanism integrates puncturing patterns directly into the network, enabling seamless adaptation to varying code rates, while balanced bit error rate training ensures robustness across different code lengths, rates, and channels, maintaining protocol flexibility. Extensive simulations in Additive White Gaussian Noise and Rayleigh fading channels demonstrate that the proposed approach outperforms conventional decoding techniques, providing significant improvements in decoding accuracy and robustness. These results underscore the potential of LSTM-based decoding as a promising solution for next-generation artificial intelligence powered communication systems.

### Mitigating Data Scarcity in Time Series Analysis: A Foundation Model with Series-Symbol Data Generation 
[[arxiv](https://arxiv.org/abs/2502.15466)] [[cool](https://papers.cool/arxiv/2502.15466)] [[pdf](https://arxiv.org/pdf/2502.15466)]
> **Authors**: Wenxuan Wang,Kai Wu,Yujian Betterest Li,Dan Wang,Xiaoyu Zhang,Jing Liu
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Foundation models for time series analysis (TSA) have attracted significant attention. However, challenges such as data scarcity and data imbalance continue to hinder their development. To address this, we consider modeling complex systems through symbolic expressions that serve as semantic descriptors of time series. Building on this concept, we introduce a series-symbol (S2) dual-modulity data generation mechanism, enabling the unrestricted creation of high-quality time series data paired with corresponding symbolic representations. Leveraging the S2 dataset, we develop SymTime, a pre-trained foundation model for TSA. SymTime demonstrates competitive performance across five major TSA tasks when fine-tuned with downstream task, rivaling foundation models pre-trained on real-world datasets. This approach underscores the potential of dual-modality data generation and pretraining mechanisms in overcoming data scarcity and enhancing task performance.

### R-LoRA: Random Initialization of Multi-Head LoRA for Multi-Task Learning 
[[arxiv](https://arxiv.org/abs/2502.15455)] [[cool](https://papers.cool/arxiv/2502.15455)] [[pdf](https://arxiv.org/pdf/2502.15455)]
> **Authors**: Jinda Liu,Yi Chang,Yuan Wu
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 9 pages, 10 figures
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Fine-tuning large language models (LLMs) is prohibitively expensive in terms of computational and memory costs. Low-rank Adaptation (LoRA), as one of the most popular parameter-efficient fine-tuning (PEFT) methods, offers a cost-effective alternative by approximating the model changes $ΔW \in \mathbb{R}^{m \times n}$ through the product of down-projection matrix $A \in \mathbb{R}^{m \times r}$ and head matrix $B \in \mathbb{R}^{r \times n}$, where $r \ll \min(m, n)$. In real-world scenarios, LLMs are fine-tuned on data from multiple domains to perform tasks across various fields, embodying multi-task learning (MTL). LoRA often underperforms in such complex scenarios. To enhance LoRA's capability in multi-task learning, we propose R-LoRA, which incorporates Multi-Head Randomization. Multi-Head Randomization diversifies the head matrices through Multi-Head Random Initialization and Multi-Head Dropout, enabling more efficient learning of task-specific features while maintaining shared knowledge representation. Extensive experiments demonstrate that R-LoRA is better at capturing task-specific knowledge, thereby improving performance in multi-task scenarios. The code is available at https://github.com/jinda-liu/R-LoRA.

### A fast convergence algorithm based on binary integer programming for expert load balancing in MoE LLMs 
[[arxiv](https://arxiv.org/abs/2502.15451)] [[cool](https://papers.cool/arxiv/2502.15451)] [[pdf](https://arxiv.org/pdf/2502.15451)]
> **Authors**: Yuan Sun
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: MoE (Mixture-of-Expert) architectures appear frequently in large language models, and the number of experts can be over one hundred recently. However, the expert load imbalance problem always happens in MoE model pre-training, which will cause routing collapse or increased computational overhead. In order to balance loads on experts, we propose BIP-Based Balancing, an expert load balancing algorithm based on binary integer programming (BIP). The algorithm maintains an additional vector q that can help change the top-K order of s by solving a binary integer programming with very small time costs. In simulation experiments, we observe that BIP-Based Balancing make imbalance disappoint very fast, while the final sum of routine scores decreases very little. Our algorithm achieves nearly perfect trade-off between expert load balance and pre-training efficiency under the simulation view.

### Fed-SB: A Silver Bullet for Extreme Communication Efficiency and Performance in (Private) Federated LoRA Fine-Tuning 
[[arxiv](https://arxiv.org/abs/2502.15436)] [[cool](https://papers.cool/arxiv/2502.15436)] [[pdf](https://arxiv.org/pdf/2502.15436)]
> **Authors**: Raghav Singhal,Kaustubh Ponkshe,Rohit Vartak,Lav R. Varshney,Praneeth Vepakomma
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Raghav Singhal and Kaustubh Ponkshe contributed equally to this work
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,分布式、并行和集群计算
- **Abstract**: Low-Rank Adaptation (LoRA) has become ubiquitous for efficiently fine-tuning foundation models. However, federated fine-tuning using LoRA is challenging due to suboptimal updates arising from traditional federated averaging of individual adapters. Existing solutions either incur prohibitively high communication cost that scales linearly with the number of clients or suffer from performance degradation due to limited expressivity. We introduce Federated Silver Bullet (Fed-SB), a novel approach for federated fine-tuning of LLMs using LoRA-SB, a recently proposed low-rank adaptation method. LoRA-SB optimally aligns the optimization trajectory with the ideal low-rank full fine-tuning projection by learning a small square matrix (R) between adapters B and A, keeping other components fixed. Direct averaging of R guarantees exact updates, substantially reducing communication cost, which remains independent of the number of clients, and enables scalability. Fed-SB achieves state-of-the-art performance across commonsense reasoning, arithmetic reasoning, and language inference tasks while reducing communication costs by up to 230x. In private settings, Fed-SB further improves performance by (1) reducing trainable parameters, thereby lowering the noise required for differential privacy and (2) avoiding noise amplification introduced by other methods. Overall, Fed-SB establishes a new Pareto frontier in the tradeoff between communication and performance, offering an efficient and scalable solution for both private and non-private federated fine-tuning. Our code is publicly available at https://github.com/CERT-Lab/fed-sb.

### Single-pass Detection of Jailbreaking Input in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.15435)] [[cool](https://papers.cool/arxiv/2502.15435)] [[pdf](https://arxiv.org/pdf/2502.15435)]
> **Authors**: Leyla Naz Candogan,Yongtao Wu,Elias Abad Rocamora,Grigorios G. Chrysos,Volkan Cevher
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Accepted in TMLR 2025
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Defending aligned Large Language Models (LLMs) against jailbreaking attacks is a challenging problem, with existing approaches requiring multiple requests or even queries to auxiliary LLMs, making them computationally heavy. Instead, we focus on detecting jailbreaking input in a single forward pass. Our method, called Single Pass Detection SPD, leverages the information carried by the logits to predict whether the output sentence will be harmful. This allows us to defend in just one forward pass. SPD can not only detect attacks effectively on open-source models, but also minimizes the misclassification of harmless inputs. Furthermore, we show that SPD remains effective even without complete logit access in GPT-3.5 and GPT-4. We believe that our proposed method offers a promising approach to efficiently safeguard LLMs against adversarial attacks.

### Evaluate with the Inverse: Efficient Approximation of Latent Explanation Quality Distribution 
[[arxiv](https://arxiv.org/abs/2502.15403)] [[cool](https://papers.cool/arxiv/2502.15403)] [[pdf](https://arxiv.org/pdf/2502.15403)]
> **Authors**: Carlos Eiras-Franco,Anna Hedström,Marina M. -C. Höhne
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Accepted to AAAI 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Obtaining high-quality explanations of a model's output enables developers to identify and correct biases, align the system's behavior with human values, and ensure ethical compliance. Explainable Artificial Intelligence (XAI) practitioners rely on specific measures to gauge the quality of such explanations. These measures assess key attributes, such as how closely an explanation aligns with a model's decision process (faithfulness), how accurately it pinpoints the relevant input features (localization), and its consistency across different cases (robustness). Despite providing valuable information, these measures do not fully address a critical practitioner's concern: how does the quality of a given explanation compare to other potential explanations? Traditionally, the quality of an explanation has been assessed by comparing it to a randomly generated counterpart. This paper introduces an alternative: the Quality Gap Estimate (QGE). The QGE method offers a direct comparison to what can be viewed as the `inverse' explanation, one that conceptually represents the antithesis of the original explanation. Our extensive testing across multiple model architectures, datasets, and established quality metrics demonstrates that the QGE method is superior to the traditional approach. Furthermore, we show that QGE enhances the statistical reliability of these quality assessments. This advance represents a significant step toward a more insightful evaluation of explanations that enables a more effective inspection of a model's behavior.

### Learning Chern Numbers of Topological Insulators with Gauge Equivariant Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.15376)] [[cool](https://papers.cool/arxiv/2502.15376)] [[pdf](https://arxiv.org/pdf/2502.15376)]
> **Authors**: Longde Huang,Oleksandr Balabanov,Hampus Linander,Mats Granath,Daniel Persson,Jan E. Gerken
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,介观和纳米物理
- **Abstract**: Equivariant network architectures are a well-established tool for predicting invariant or equivariant quantities. However, almost all learning problems considered in this context feature a global symmetry, i.e. each point of the underlying space is transformed with the same group element, as opposed to a local ``gauge'' symmetry, where each point is transformed with a different group element, exponentially enlarging the size of the symmetry group. Gauge equivariant networks have so far mainly been applied to problems in quantum chromodynamics. Here, we introduce a novel application domain for gauge-equivariant networks in the theory of topological condensed matter physics. We use gauge equivariant networks to predict topological invariants (Chern numbers) of multiband topological insulators. The gauge symmetry of the network guarantees that the predicted quantity is a topological invariant. We introduce a novel gauge equivariant normalization layer to stabilize the training and prove a universal approximation theorem for our setup. We train on samples with trivial Chern number only but show that our models generalize to samples with non-trivial Chern number. We provide various ablations of our setup. Our code is available at https://github.com/sitronsea/GENet/tree/main.

### Efficient and Provable Algorithms for Covariate Shift 
[[arxiv](https://arxiv.org/abs/2502.15372)] [[cool](https://papers.cool/arxiv/2502.15372)] [[pdf](https://arxiv.org/pdf/2502.15372)]
> **Authors**: Deeksha Adil,Jarosław Błasiok
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,数据结构和算法
- **Abstract**: Covariate shift, a widely used assumption in tackling {\it distributional shift} (when training and test distributions differ), focuses on scenarios where the distribution of the labels conditioned on the feature vector is the same, but the distribution of features in the training and test data are different. Despite the significance and extensive work on covariate shift, theoretical guarantees for algorithms in this domain remain sparse. In this paper, we distill the essence of the covariate shift problem and focus on estimating the average $\mathbb{E}_{\tilde{\mathbf{x}}\sim p_{\mathrm{test}}}\mathbf{f}(\tilde{\mathbf{x}})$, of any unknown and bounded function $\mathbf{f}$, given labeled training samples $(\mathbf{x}_i, \mathbf{f}(\mathbf{x}_i))$, and unlabeled test samples $\tilde{\mathbf{x}}_i$; this is a core subroutine for several widely studied learning problems. We give several efficient algorithms, with provable sample complexity and computational guarantees. Moreover, we provide the first rigorous analysis of algorithms in this space when $\mathbf{f}$ is unrestricted, laying the groundwork for developing a solid theoretical foundation for covariate shift problems.

### Efficiently Solving Discounted MDPs with Predictions on Transition Matrices 
[[arxiv](https://arxiv.org/abs/2502.15345)] [[cool](https://papers.cool/arxiv/2502.15345)] [[pdf](https://arxiv.org/pdf/2502.15345)]
> **Authors**: Lixing Lyu,Jiashuo Jiang,Wang Chi Cheung
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We study infinite-horizon Discounted Markov Decision Processes (DMDPs) under a generative model. Motivated by the Algorithm with Advice framework Mitzenmacher and Vassilvitskii 2022, we propose a novel framework to investigate how a prediction on the transition matrix can enhance the sample efficiency in solving DMDPs and improve sample complexity bounds. We focus on the DMDPs with $N$ state-action pairs and discounted factor $γ$. Firstly, we provide an impossibility result that, without prior knowledge of the prediction accuracy, no sampling policy can compute an $ε$-optimal policy with a sample complexity bound better than $\tilde{O}((1-γ)^{-3} Nε^{-2})$, which matches the state-of-the-art minimax sample complexity bound with no prediction. In complement, we propose an algorithm based on minimax optimization techniques that leverages the prediction on the transition matrix. Our algorithm achieves a sample complexity bound depending on the prediction error, and the bound is uniformly better than $\tilde{O}((1-γ)^{-4} N ε^{-2})$, the previous best result derived from convex optimization methods. These theoretical findings are further supported by our numerical experiments.

### Learning with Limited Shared Information in Multi-agent Multi-armed Bandit 
[[arxiv](https://arxiv.org/abs/2502.15338)] [[cool](https://papers.cool/arxiv/2502.15338)] [[pdf](https://arxiv.org/pdf/2502.15338)]
> **Authors**: Junning Shao,Siwei Wang,Zhixuan Fang
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Multi-agent multi-armed bandit (MAMAB) is a classic collaborative learning model and has gained much attention in recent years. However, existing studies do not consider the case where an agent may refuse to share all her information with others, e.g., when some of the data contains personal privacy. In this paper, we propose a novel limited shared information multi-agent multi-armed bandit (LSI-MAMAB) model in which each agent only shares the information that she is willing to share, and propose the Balanced-ETC algorithm to help multiple agents collaborate efficiently with limited shared information. Our analysis shows that Balanced-ETC is asymptotically optimal and its average regret (on each agent) approaches a constant when there are sufficient agents involved. Moreover, to encourage agents to participate in this collaborative learning, an incentive mechanism is proposed to make sure each agent can benefit from the collaboration system. Finally, we present experimental results to validate our theoretical results.

### Tight Clusters Make Specialized Experts 
[[arxiv](https://arxiv.org/abs/2502.15315)] [[cool](https://papers.cool/arxiv/2502.15315)] [[pdf](https://arxiv.org/pdf/2502.15315)]
> **Authors**: Stefan K. Nielsen,Rachel S. Y. Teo,Laziz U. Abdullaev,Tan M. Nguyen
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Sparse Mixture-of-Experts (MoE) architectures have emerged as a promising approach to decoupling model capacity from computational cost. At the core of the MoE model is the router, which learns the underlying clustering structure of the input distribution in order to send input tokens to appropriate experts. However, latent clusters may be unidentifiable in high dimension, which causes slow convergence, susceptibility to data contamination, and overall degraded representations as the router is unable to perform appropriate token-expert matching. We examine the router through the lens of clustering optimization and derive optimal feature weights that maximally identify the latent clusters. We use these weights to compute the token-expert routing assignments in an adaptively transformed space that promotes well-separated clusters, which helps identify the best-matched expert for each token. In particular, for each expert cluster, we compute a set of weights that scales features according to whether that expert clusters tightly along that feature. We term this novel router the Adaptive Clustering (AC) router. Our AC router enables the MoE model to obtain three connected benefits: 1) faster convergence, 2) better robustness to data corruption, and 3) overall performance improvement, as experts are specialized in semantically distinct regions of the input space. We empirically demonstrate the advantages of our AC router over baseline routing methods when applied on a variety of MoE backbones for language modeling and image recognition tasks in both clean and corrupted settings.

### SVDq: 1.25-bit and 410x Key Cache Compression for LLM Attention 
[[arxiv](https://arxiv.org/abs/2502.15304)] [[cool](https://papers.cool/arxiv/2502.15304)] [[pdf](https://arxiv.org/pdf/2502.15304)]
> **Authors**: Hong Yankun,Li Xing,Zhen Hui-Ling,Yu Xianzhi,Liu Wulong,Yuan Mingxuan
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: :68T50
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: For the efficient inference of Large Language Models (LLMs), the effective compression of key-value (KV) cache is essential. Three main types of KV cache compression techniques, namely sparsity, channel compression, and quantization, have been identified. This study presents SVDq, a Singular Value Decomposition (SVD) - based mixed precision quantization method for K cache. Initially, K cache is transformed into latent channels using SVD basis representations. Since the values in latent channels decay rapidly and become negligible after only a few latent channels, our method then incorporates importance-aware quantization and compression for latent channels. This enables the effective allocation of higher precision to more significant channels. Theoretically, we prove that SVDq results in quantization errors (x0.1 or even lower) that are much lower than those of per-channel key quantization in the original space. Our findings based on RULER and LongBench benchmarks demonstrate that SVDq can achieve an equivalent key cache precision as low as 1.25-bit. When combined with key sparsity, it can reach a key compression ratio of up to 410x for attention computation, all while maintaining comparable model performance. Notably, our method is nearly lossless for LongBench datasets. This indicates that SVDq enables high-precision low-bit quantization, providing a more efficient solution for KV cache compression in LLMs.

### Beyond Fixed Variables: Expanding-variate Time Series Forecasting via Flat Scheme and Spatio-temporal Focal Learning 
[[arxiv](https://arxiv.org/abs/2502.15296)] [[cool](https://papers.cool/arxiv/2502.15296)] [[pdf](https://arxiv.org/pdf/2502.15296)]
> **Authors**: Minbo Ma,Kai Tang,Huan Li,Fei Teng,Dalin Zhang,Tianrui Li
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Multivariate Time Series Forecasting (MTSF) has long been a key research focus. Traditionally, these studies assume a fixed number of variables, but in real-world applications, Cyber-Physical Systems often expand as new sensors are deployed, increasing variables in MTSF. In light of this, we introduce a novel task, Expanding-variate Time Series Forecasting (EVTSF). This task presents unique challenges, specifically (1) handling inconsistent data shapes caused by adding new variables, and (2) addressing imbalanced spatio-temporal learning, where expanding variables have limited observed data due to the necessity for timely operation. To address these challenges, we propose STEV, a flexible spatio-temporal forecasting framework. STEV includes a new Flat Scheme to tackle the inconsistent data shape issue, which extends the graph-based spatio-temporal modeling architecture into 1D space by flattening the 2D samples along the variable dimension, making the model variable-scale-agnostic while still preserving dynamic spatial correlations through a holistic graph. We introduce a novel Spatio-temporal Focal Learning strategy that incorporates a negative filter to resolve potential conflicts between contrastive learning and graph representation, and a focal contrastive loss as its core to guide the framework to focus on optimizing the expanding variables. We benchmark EVTSF performance using three real-world datasets and compare it against three potential solutions employing SOTA MTSF models tailored for EVSTF. Experimental results show that STEV significantly outperforms its competitors, particularly on expanding variables. Notably, STEV, with only 5% of observations from the expanding period, is on par with SOTA MTSF models trained with complete observations. Further exploration of various expanding strategies underscores the generalizability of STEV in real-world applications.

### Hyperspherical Normalization for Scalable Deep Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.15280)] [[cool](https://papers.cool/arxiv/2502.15280)] [[pdf](https://arxiv.org/pdf/2502.15280)]
> **Authors**: Hojoon Lee,Youngdo Lee,Takuma Seno,Donghu Kim,Peter Stone,Jaegul Choo
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 50 pages. Preprint
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Scaling up the model size and computation has brought consistent performance improvements in supervised learning. However, this lesson often fails to apply to reinforcement learning (RL) because training the model on non-stationary data easily leads to overfitting and unstable optimization. In response, we introduce SimbaV2, a novel RL architecture designed to stabilize optimization by (i) constraining the growth of weight and feature norm by hyperspherical normalization; and (ii) using a distributional value estimation with reward scaling to maintain stable gradients under varying reward magnitudes. Using the soft actor-critic as a base algorithm, SimbaV2 scales up effectively with larger models and greater compute, achieving state-of-the-art performance on 57 continuous control tasks across 4 domains. The code is available at https://dojeon-ai.github.io/SimbaV2.

### Towards a Reward-Free Reinforcement Learning Framework for Vehicle Control 
[[arxiv](https://arxiv.org/abs/2502.15262)] [[cool](https://papers.cool/arxiv/2502.15262)] [[pdf](https://arxiv.org/pdf/2502.15262)]
> **Authors**: Jielong Yang,Daoyuan Huang
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Reinforcement learning plays a crucial role in vehicle control by guiding agents to learn optimal control strategies through designing or learning appropriate reward signals. However, in vehicle control applications, rewards typically need to be manually designed while considering multiple implicit factors, which easily introduces human biases. Although imitation learning methods does not rely on explicit reward signals, they necessitate high-quality expert actions, which are often challenging to acquire. To address these issues, we propose a reward-free reinforcement learning framework (RFRLF). This framework directly learns the target states to optimize agent behavior through a target state prediction network (TSPN) and a reward-free state-guided policy network (RFSGPN), avoiding the dependence on manually designed reward signals. Specifically, the policy network is learned via minimizing the differences between the predicted state and the expert state. Experimental results demonstrate the effectiveness of the proposed RFRLF in controlling vehicle driving, showing its advantages in improving learning efficiency and adapting to reward-free environments.

### Real-Time Moving Flock Detection in Pedestrian Trajectories Using Sequential Deep Learning Models 
[[arxiv](https://arxiv.org/abs/2502.15252)] [[cool](https://papers.cool/arxiv/2502.15252)] [[pdf](https://arxiv.org/pdf/2502.15252)]
> **Authors**: Amartaivan Sanjjamts,Hiroshi Morita,Togootogtokh Enkhtogtokh
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Understanding collective pedestrian movement is crucial for applications in crowd management, autonomous navigation, and human-robot interaction. This paper investigates the use of sequential deep learning models, including Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and Transformers, for real-time flock detection in multi-pedestrian trajectories. Our proposed approach consists of a two-stage process: first, a pre-trained binary classification model is used for pairwise trajectory classification, and second, the learned representations are applied to identify multi-agent flocks dynamically. We validate our method using real-world group movement datasets, demonstrating its robustness across varying sequence lengths and diverse movement patterns. Experimental results indicate that our model consistently detects pedestrian flocks with high accuracy and stability, even in dynamic and noisy environments. Furthermore, we extend our approach to identify other forms of collective motion, such as convoys and swarms, paving the way for more comprehensive multi-agent behavior analysis.

### Multi-agent Multi-armed Bandits with Minimum Reward Guarantee Fairness 
[[arxiv](https://arxiv.org/abs/2502.15240)] [[cool](https://papers.cool/arxiv/2502.15240)] [[pdf](https://arxiv.org/pdf/2502.15240)]
> **Authors**: Piyushi Manupriya,Himanshu,SakethaNath Jagarlapudi,Ganesh Ghalme
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,系统与控制
- **Abstract**: We investigate the problem of maximizing social welfare while ensuring fairness in a multi-agent multi-armed bandit (MA-MAB) setting. In this problem, a centralized decision-maker takes actions over time, generating random rewards for various agents. Our goal is to maximize the sum of expected cumulative rewards, a.k.a. social welfare, while ensuring that each agent receives an expected reward that is at least a constant fraction of the maximum possible expected reward. Our proposed algorithm, RewardFairUCB, leverages the Upper Confidence Bound (UCB) technique to achieve sublinear regret bounds for both fairness and social welfare. The fairness regret measures the positive difference between the minimum reward guarantee and the expected reward of a given policy, whereas the social welfare regret measures the difference between the social welfare of the optimal fair policy and that of the given policy. We show that RewardFairUCB algorithm achieves instance-independent social welfare regret guarantees of $\tilde{O}(T^{1/2})$ and a fairness regret upper bound of $\tilde{O}(T^{3/4})$. We also give the lower bound of $Ω(\sqrt{T})$ for both social welfare and fairness regret. We evaluate RewardFairUCB's performance against various baseline and heuristic algorithms using simulated data and real world data, highlighting trade-offs between fairness and social welfare regrets.

### Auto-Bench: An Automated Benchmark for Scientific Discovery in LLMs 
[[arxiv](https://arxiv.org/abs/2502.15224)] [[cool](https://papers.cool/arxiv/2502.15224)] [[pdf](https://arxiv.org/pdf/2502.15224)]
> **Authors**: Tingting Chen,Srinivas Anumasa,Beibei Lin,Vedant Shah,Anirudh Goyal,Dianbo Liu
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 13 pages
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Given the remarkable performance of Large Language Models (LLMs), an important question arises: Can LLMs conduct human-like scientific research and discover new knowledge, and act as an AI scientist? Scientific discovery is an iterative process that demands efficient knowledge updating and encoding. It involves understanding the environment, identifying new hypotheses, and reasoning about actions; however, no standardized benchmark specifically designed for scientific discovery exists for LLM agents. In response to these limitations, we introduce a novel benchmark, \textit{Auto-Bench}, that encompasses necessary aspects to evaluate LLMs for scientific discovery in both natural and social sciences. Our benchmark is based on the principles of causal graph discovery. It challenges models to uncover hidden structures and make optimal decisions, which includes generating valid justifications. By engaging interactively with an oracle, the models iteratively refine their understanding of underlying interactions, the chemistry and social interactions, through strategic interventions. We evaluate state-of-the-art LLMs, including GPT-4, Gemini, Qwen, Claude, and Llama, and observe a significant performance drop as the problem complexity increases, which suggests an important gap between machine and human intelligence that future development of LLMs need to take into consideration.

### The Evolving Landscape of LLM- and VLM-Integrated Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.15214)] [[cool](https://papers.cool/arxiv/2502.15214)] [[pdf](https://arxiv.org/pdf/2502.15214)]
> **Authors**: Sheila Schoepp,Masoud Jafaripour,Yingyue Cao,Tianpei Yang,Fatemeh Abdollahi,Shadan Golestan,Zahin Sufiyan,Osmar R. Zaiane,Matthew E. Taylor
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 9 pages, 4 figures
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Reinforcement learning (RL) has shown impressive results in sequential decision-making tasks. Meanwhile, Large Language Models (LLMs) and Vision-Language Models (VLMs) have emerged, exhibiting impressive capabilities in multimodal understanding and reasoning. These advances have led to a surge of research integrating LLMs and VLMs into RL. In this survey, we review representative works in which LLMs and VLMs are used to overcome key challenges in RL, such as lack of prior knowledge, long-horizon planning, and reward design. We present a taxonomy that categorizes these LLM/VLM-assisted RL approaches into three roles: agent, planner, and reward. We conclude by exploring open problems, including grounding, bias mitigation, improved representations, and action advice. By consolidating existing research and identifying future directions, this survey establishes a framework for integrating LLMs and VLMs into RL, advancing approaches that unify natural language and visual understanding with sequential decision-making.

## 计算机科学中的逻辑(cs.LO:Logic in Computer Science)

### Verifying Quantized Graph Neural Networks is PSPACE-complete 
[[arxiv](https://arxiv.org/abs/2502.16244)] [[cool](https://papers.cool/arxiv/2502.16244)] [[pdf](https://arxiv.org/pdf/2502.16244)]
> **Authors**: Marco Sälzer,François Schwarzentruber,Nicolas Troquard
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算机科学中的逻辑,计算复杂度,机器学习
- **Abstract**: In this paper, we investigate verification of quantized Graph Neural Networks (GNNs), where some fixed-width arithmetic is used to represent numbers. We introduce the linear-constrained validity (LVP) problem for verifying GNNs properties, and provide an efficient translation from LVP instances into a logical language. We show that LVP is in PSPACE, for any reasonable activation functions. We provide a proof system. We also prove PSPACE-hardness, indicating that while reasoning about quantized GNNs is feasible, it remains generally computationally challenging.

## 多代理系统(cs.MA:Multiagent Systems)

### Multi-Agent Autonomous Driving Systems with Large Language Models: A Survey of Recent Advances 
[[arxiv](https://arxiv.org/abs/2502.16804)] [[cool](https://papers.cool/arxiv/2502.16804)] [[pdf](https://arxiv.org/pdf/2502.16804)]
> **Authors**: Yaozu Wu,Dongyuan Li,Yankai Chen,Renhe Jiang,Henry Peng Zou,Liancheng Fang,Zhen Wang,Philip S. Yu
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 多代理系统,人工智能
- **Abstract**: Autonomous Driving Systems (ADSs) are revolutionizing transportation by reducing human intervention, improving operational efficiency, and enhancing safety. Large Language Models (LLMs), known for their exceptional planning and reasoning capabilities, have been integrated into ADSs to assist with driving decision-making. However, LLM-based single-agent ADSs face three major challenges: limited perception, insufficient collaboration, and high computational demands. To address these issues, recent advancements in LLM-based multi-agent ADSs have focused on improving inter-agent communication and cooperation. This paper provides a frontier survey of LLM-based multi-agent ADSs. We begin with a background introduction to related concepts, followed by a categorization of existing LLM-based approaches based on different agent interaction modes. We then discuss agent-human interactions in scenarios where LLM-based agents engage with humans. Finally, we summarize key applications, datasets, and challenges in this field to support future research (https://anonymous.4open.science/r/LLM-based_Multi-agent_ADS-3A5C/README.md).

### The Hidden Strength of Disagreement: Unraveling the Consensus-Diversity Tradeoff in Adaptive Multi-Agent Systems 
[[arxiv](https://arxiv.org/abs/2502.16565)] [[cool](https://papers.cool/arxiv/2502.16565)] [[pdf](https://arxiv.org/pdf/2502.16565)]
> **Authors**: Zengqing Wu,Takayuki Ito
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: Source codes are available at https://github.com/wuzengqing001225/ConsensusDiversityTradeoffMAS
- **标题**: None
- **领域**: 多代理系统,人工智能,计算语言学,计算机与社会
- **Abstract**: Consensus formation is pivotal in multi-agent systems (MAS), balancing collective coherence with individual diversity. Conventional LLM-based MAS primarily rely on explicit coordination, e.g., prompts or voting, risking premature homogenization. We argue that implicit consensus, where agents exchange information yet independently form decisions via in-context learning, can be more effective in dynamic environments that require long-horizon adaptability. By retaining partial diversity, systems can better explore novel strategies and cope with external shocks. We formalize a consensus-diversity tradeoff, showing conditions where implicit methods outperform explicit ones. Experiments on three scenarios -- Dynamic Disaster Response, Information Spread and Manipulation, and Dynamic Public-Goods Provision -- confirm partial deviation from group norms boosts exploration, robustness, and performance. We highlight emergent coordination via in-context learning, underscoring the value of preserving diversity for resilient decision-making.

### Dynamic Coalition Structure Detection in Natural Language-based Interactions 
[[arxiv](https://arxiv.org/abs/2502.16339)] [[cool](https://papers.cool/arxiv/2502.16339)] [[pdf](https://arxiv.org/pdf/2502.16339)]
> **Authors**: Abhishek N. Kulkarni,Andy Liu,Jean-Raphael Gaglione,Daniel Fried,Ufuk Topcu
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: Proceedings of the 24th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2025)
- **标题**: None
- **领域**: 多代理系统,计算语言学,计算机科学与博弈论
- **Abstract**: In strategic multi-agent sequential interactions, detecting dynamic coalition structures is crucial for understanding how self-interested agents coordinate to influence outcomes. However, natural-language-based interactions introduce unique challenges to coalition detection due to ambiguity over intents and difficulty in modeling players' subjective perspectives. We propose a new method that leverages recent advancements in large language models and game theory to predict dynamic multilateral coalition formation in Diplomacy, a strategic multi-agent game where agents negotiate coalitions using natural language. The method consists of two stages. The first stage extracts the set of agreements discussed by two agents in their private dialogue, by combining a parsing-based filtering function with a fine-tuned language model trained to predict player intents. In the second stage, we define a new metric using the concept of subjective rationalizability from hypergame theory to evaluate the expected value of an agreement for each player. We then compute this metric for each agreement identified in the first stage by assessing the strategic value of the agreement for both players and taking into account the subjective belief of one player that the second player would honor the agreement. We demonstrate that our method effectively detects potential coalition structures in online Diplomacy gameplay by assigning high values to agreements likely to be honored and low values to those likely to be violated. The proposed method provides foundational insights into coalition formation in multi-agent environments with language-based negotiation and offers key directions for future research on the analysis of complex natural language-based interactions between agents.

## 网络和互联网架构(cs.NI:Networking and Internet Architecture)

### An Autonomous Network Orchestration Framework Integrating Large Language Models with Continual Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.16198)] [[cool](https://papers.cool/arxiv/2502.16198)] [[pdf](https://arxiv.org/pdf/2502.16198)]
> **Authors**: Masoud Shokrnezhad,Tarik Taleb
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: IEEE Communications Magazine
- **标题**: None
- **领域**: 网络和互联网架构,人工智能,新兴技术,机器学习
- **Abstract**: 6G networks aim to achieve global coverage, massive connectivity, and ultra-stringent requirements. Space-Air-Ground Integrated Networks (SAGINs) and Semantic Communication (SemCom) are essential for realizing these goals, yet they introduce considerable complexity in resource orchestration. Drawing inspiration from research in robotics, a viable solution to manage this complexity is the application of Large Language Models (LLMs). Although the use of LLMs in network orchestration has recently gained attention, existing solutions have not sufficiently addressed LLM hallucinations or their adaptation to network dynamics. To address this gap, this paper proposes a framework called Autonomous Reinforcement Coordination (ARC) for a SemCom-enabled SAGIN. This framework employs an LLM-based Retrieval-Augmented Generator (RAG) monitors services, users, and resources and processes the collected data, while a Hierarchical Action Planner (HAP) orchestrates resources. ARC decomposes orchestration into two tiers, utilizing LLMs for high-level planning and Reinforcement Learning (RL) agents for low-level decision-making, in alignment with the Mixture of Experts (MoE) concept. The LLMs utilize Chain-of-Thought (CoT) reasoning for few-shot learning, empowered by contrastive learning, while the RL agents employ replay buffer management for continual learning, thereby achieving efficiency, accuracy, and adaptability. Simulations are provided to demonstrate the effectiveness of ARC, along with a comprehensive discussion on potential future research directions to enhance and upgrade ARC.

### Space-O-RAN: Enabling Intelligent, Open, and Interoperable Non Terrestrial Networks in 6G 
[[arxiv](https://arxiv.org/abs/2502.15936)] [[cool](https://papers.cool/arxiv/2502.15936)] [[pdf](https://arxiv.org/pdf/2502.15936)]
> **Authors**: Eduardo Baena,Paolo Testolina,Michele Polese,Dimitrios Koutsonikolas,Josep Jornet,Tommaso Melodia
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 网络和互联网架构,人工智能,系统与控制
- **Abstract**: Non-terrestrial networks (NTNs) are essential for ubiquitous connectivity, providing coverage in remote and underserved areas. However, since NTNs are currently operated independently, they face challenges such as isolation, limited scalability, and high operational costs. Integrating satellite constellations with terrestrial networks offers a way to address these limitations while enabling adaptive and cost-efficient connectivity through the application of Artificial Intelligence (AI) models. This paper introduces Space-O-RAN, a framework that extends Open Radio Access Network (RAN) principles to NTNs. It employs hierarchical closed-loop control with distributed Space RAN Intelligent Controllers (Space-RICs) to dynamically manage and optimize operations across both domains. To enable adaptive resource allocation and network orchestration, the proposed architecture integrates real-time satellite optimization and control with AI-driven management and digital twin (DT) modeling. It incorporates distributed Space Applications (sApps) and dApps to ensure robust performance in in highly dynamic orbital environments. A core feature is dynamic link-interface mapping, which allows network functions to adapt to specific application requirements and changing link conditions using all physical links on the satellite. Simulation results evaluate its feasibility by analyzing latency constraints across different NTN link types, demonstrating that intra-cluster coordination operates within viable signaling delay bounds, while offloading non-real-time tasks to ground infrastructure enhances scalability toward sixth-generation (6G) networks.

### Text2Net: Transforming Plain-text To A Dynamic Interactive Network Simulation Environment 
[[arxiv](https://arxiv.org/abs/2502.15754)] [[cool](https://papers.cool/arxiv/2502.15754)] [[pdf](https://arxiv.org/pdf/2502.15754)]
> **Authors**: Alireza Marefat,Abbaas Alif Mohamed Nishar,Ashwin Ashok
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-24
> **comment**: 7 pages, 9 figures, Accepted at IEEE SoutheastCon 2025
- **标题**: None
- **领域**: 网络和互联网架构,人工智能,机器学习
- **Abstract**: This paper introduces Text2Net, an innovative text-based network simulation engine that leverages natural language processing (NLP) and large language models (LLMs) to transform plain-text descriptions of network topologies into dynamic, interactive simulations. Text2Net simplifies the process of configuring network simulations, eliminating the need for users to master vendor-specific syntaxes or navigate complex graphical interfaces. Through qualitative and quantitative evaluations, we demonstrate Text2Net's ability to significantly reduce the time and effort required to deploy network scenarios compared to traditional simulators like EVE-NG. By automating repetitive tasks and enabling intuitive interaction, Text2Net enhances accessibility for students, educators, and professionals. The system facilitates hands-on learning experiences for students that bridge the gap between theoretical knowledge and practical application. The results showcase its scalability across various network complexities, marking a significant step toward revolutionizing network education and professional use cases, such as proof-of-concept testing.

### CacheMamba: Popularity Prediction for Mobile Edge Caching Networks via Selective State Spaces 
[[arxiv](https://arxiv.org/abs/2502.15746)] [[cool](https://papers.cool/arxiv/2502.15746)] [[pdf](https://arxiv.org/pdf/2502.15746)]
> **Authors**: Ghazaleh Kianfar,Zohreh Hajiakhondi-Meybodi,Arash Mohammadi
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 网络和互联网架构,机器学习,信号处理
- **Abstract**: Mobile Edge Caching (MEC) plays a pivotal role in mitigating latency in data-intensive services by dynamically caching frequently requested content on edge servers. This capability is critical for applications such as Augmented Reality (AR), Virtual Reality (VR), and Autonomous Vehicles (AV), where efficient content caching and accurate popularity prediction are essential for optimizing performance. In this paper, we explore the problem of popularity prediction in MEC by utilizing historical time-series request data of intended files, formulating this problem as a ranking task. To this aim, we propose CacheMamba model by employing Mamba, a state-space model (SSM)-based architecture, to identify the top-K files with the highest likelihood of being requested. We then benchmark the proposed model against a Transformer-based approach, demonstrating its superior performance in terms of cache-hit rate, Mean Average Precision (MAP), Normalized Discounted Cumulative Gain (NDCG), and Floating-Point Operations Per Second (FLOPS), particularly when dealing with longer sequences.

### Channel Gain Map Construction based on Subregional Learning and Prediction 
[[arxiv](https://arxiv.org/abs/2502.15733)] [[cool](https://papers.cool/arxiv/2502.15733)] [[pdf](https://arxiv.org/pdf/2502.15733)]
> **Authors**: Jiayi Chen,Ruifeng Gao,Jue Wang,Shu Sun,Yi Wu
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 网络和互联网架构,机器学习
- **Abstract**: The construction of channel gain map (CGM) is essential for realizing environment-aware wireless communications expected in 6G, for which a fundamental problem is how to predict the channel gains at unknown locations effectively by a finite number of measurements. As using a single prediction model is not effective in complex propagation environments, we propose a subregional learning-based CGM construction scheme, with which the entire map is divided into subregions via data-driven clustering, then individual models are constructed and trained for every subregion. In this way, specific propagation feature in each subregion can be better extracted with finite training data. Moreover, we propose to further improve prediction accuracy by uneven subregion sampling, as well as training data reuse around the subregion boundaries. Simulation results validate the effectiveness of the proposed scheme in CGM construction.

### Modular and Integrated AI Control Framework across Fiber and Wireless Networks for 6G 
[[arxiv](https://arxiv.org/abs/2502.15731)] [[cool](https://papers.cool/arxiv/2502.15731)] [[pdf](https://arxiv.org/pdf/2502.15731)]
> **Authors**: Merim Dzaferagic,Marco Ruffini,Daniel Kilper
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 网络和互联网架构,人工智能
- **Abstract**: The rapid evolution of communication networks towards 6G increasingly incorporates advanced AI-driven controls across various network segments to achieve intelligent, zero-touch operation. This paper proposes a comprehensive and modular framework for AI controllers, designed to be highly flexible and adaptable for use across both fiber optical and radio networks. Building on the principles established by the O-RAN Alliance for near-Real-Time RAN Intelligent Controllers (near-RT RICs), our framework extends this AI-driven control into the optical domain. Our approach addresses the critical need for a unified AI control framework across diverse network transport technologies and domains, enabling the development of intelligent, automated, and scalable 6G networks.

### Retrieval Augmented Generation Based LLM Evaluation For Protocol State Machine Inference With Chain-of-Thought Reasoning 
[[arxiv](https://arxiv.org/abs/2502.15727)] [[cool](https://papers.cool/arxiv/2502.15727)] [[pdf](https://arxiv.org/pdf/2502.15727)]
> **Authors**: Youssef Maklad,Fares Wael,Wael Elsersy,Ali Hamdi
> **First submission**: 2025-01-29
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 网络和互联网架构,人工智能,密码学和安全,信息检索
- **Abstract**: This paper presents a novel approach to evaluate the efficiency of a RAG-based agentic Large Language Model (LLM) architecture in network packet seed generation for network protocol fuzzing. Enhanced by chain-of-thought (COT) prompting techniques, the proposed approach focuses on the improvement of the seeds structural quality in order to guide protocol fuzzing frameworks through a wide exploration of the protocol state space. Our method leverages RAG and text embeddings in a two-stages. In the first stage, the agent dynamically refers to the Request For Comments (RFC) documents knowledge base for answering queries regarding the protocol Finite State Machine (FSM), then it iteratively reasons through the retrieved knowledge, for output refinement and proper seed placement. In the second stage, we evaluate the response structure quality of the agent's output, based on metrics as BLEU, ROUGE, and Word Error Rate (WER) by comparing the generated packets against the ground truth packets. Our experiments demonstrate significant improvements of up to 18.19%, 14.81%, and 23.45% in BLEU, ROUGE, and WER, respectively, over baseline models. These results confirm the potential of such approach, improving LLM-based protocol fuzzing frameworks for the identification of hidden vulnerabilities.

### UAV-assisted Internet of Vehicles: A Framework Empowered by Reinforcement Learning and Blockchain 
[[arxiv](https://arxiv.org/abs/2502.15713)] [[cool](https://papers.cool/arxiv/2502.15713)] [[pdf](https://arxiv.org/pdf/2502.15713)]
> **Authors**: Ahmed Alagha,Maha Kadadha,Rabeb Mizouni,Shakti Singh,Jamal Bentahar,Hadi Otrok
> **First submission**: 2025-01-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 网络和互联网架构,人工智能
- **Abstract**: This paper addresses the challenges of selecting relay nodes and coordinating among them in UAV-assisted Internet-of-Vehicles (IoV). The selection of UAV relay nodes in IoV employs mechanisms executed either at centralized servers or decentralized nodes, which have two main limitations: 1) the traceability of the selection mechanism execution and 2) the coordination among the selected UAVs, which is currently offered in a centralized manner and is not coupled with the relay selection. Existing UAV coordination methods often rely on optimization methods, which are not adaptable to different environment complexities, or on centralized deep reinforcement learning, which lacks scalability in multi-UAV settings. Overall, there is a need for a comprehensive framework where relay selection and coordination are coupled and executed in a transparent and trusted manner. This work proposes a framework empowered by reinforcement learning and Blockchain for UAV-assisted IoV networks. It consists of three main components: a two-sided UAV relay selection mechanism for UAV-assisted IoV, a decentralized Multi-Agent Deep Reinforcement Learning (MDRL) model for autonomous UAV coordination, and a Blockchain implementation for transparency and traceability in the interactions between vehicles and UAVs. The relay selection considers the two-sided preferences of vehicles and UAVs based on the Quality-of-UAV (QoU) and the Quality-of-Vehicle (QoV). Upon selection of relay UAVs, the decentralized coordination between them is enabled through an MDRL model trained to control their mobility and maintain the network coverage and connectivity using Proximal Policy Optimization (PPO). The evaluation results demonstrate that the proposed selection and coordination mechanisms improve the stability of the selected relays and maximize the coverage and connectivity achieved by the UAVs.

### GPUs, CPUs, and... NICs: Rethinking the Network's Role in Serving Complex AI Pipelines 
[[arxiv](https://arxiv.org/abs/2502.15712)] [[cool](https://papers.cool/arxiv/2502.15712)] [[pdf](https://arxiv.org/pdf/2502.15712)]
> **Authors**: Mike Wong,Ulysses Butler,Emma Farkash,Praveen Tammana,Anirudh Sivaraman,Ravi Netravali
> **First submission**: 2025-01-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 网络和互联网架构,人工智能,操作系统
- **Abstract**: The increasing prominence of AI necessitates the deployment of inference platforms for efficient and effective management of AI pipelines and compute resources. As these pipelines grow in complexity, the demand for distributed serving rises and introduces much-dreaded network delays. In this paper, we investigate how the network can instead be a boon to the excessively high resource overheads of AI pipelines. To alleviate these overheads, we discuss how resource-intensive data processing tasks -- a key facet of growing AI pipeline complexity -- are well-matched for the computational characteristics of packet processing pipelines and how they can be offloaded onto SmartNICs. We explore the challenges and opportunities of offloading, and propose a research agenda for integrating network hardware into AI pipelines, unlocking new opportunities for optimization.

## 机器人技术(cs.RO:Robotics)

### NatSGLD: A Dataset with Speech, Gesture, Logic, and Demonstration for Robot Learning in Natural Human-Robot Interaction 
[[arxiv](https://arxiv.org/abs/2502.16718)] [[cool](https://papers.cool/arxiv/2502.16718)] [[pdf](https://arxiv.org/pdf/2502.16718)]
> **Authors**: Snehesh Shrestha,Yantian Zha,Saketh Banagiri,Ge Gao,Yiannis Aloimonos,Cornelia Fermüller
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: arXiv admin note: substantial text overlap with arXiv:2403.02274
- **标题**: None
- **领域**: 机器人技术,人工智能
- **Abstract**: Recent advances in multimodal Human-Robot Interaction (HRI) datasets emphasize the integration of speech and gestures, allowing robots to absorb explicit knowledge and tacit understanding. However, existing datasets primarily focus on elementary tasks like object pointing and pushing, limiting their applicability to complex domains. They prioritize simpler human command data but place less emphasis on training robots to correctly interpret tasks and respond appropriately. To address these gaps, we present the NatSGLD dataset, which was collected using a Wizard of Oz (WoZ) method, where participants interacted with a robot they believed to be autonomous. NatSGLD records humans' multimodal commands (speech and gestures), each paired with a demonstration trajectory and a Linear Temporal Logic (LTL) formula that provides a ground-truth interpretation of the commanded tasks. This dataset serves as a foundational resource for research at the intersection of HRI and machine learning. By providing multimodal inputs and detailed annotations, NatSGLD enables exploration in areas such as multimodal instruction following, plan recognition, and human-advisable reinforcement learning from demonstrations. We release the dataset and code under the MIT License at https://www.snehesh.com/natsgld/ to support future HRI research.

### Reflective Planning: Vision-Language Models for Multi-Stage Long-Horizon Robotic Manipulation 
[[arxiv](https://arxiv.org/abs/2502.16707)] [[cool](https://papers.cool/arxiv/2502.16707)] [[pdf](https://arxiv.org/pdf/2502.16707)]
> **Authors**: Yunhai Feng,Jiaming Han,Zhuoran Yang,Xiangyu Yue,Sergey Levine,Jianlan Luo
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,人工智能,机器学习
- **Abstract**: Solving complex long-horizon robotic manipulation problems requires sophisticated high-level planning capabilities, the ability to reason about the physical world, and reactively choose appropriate motor skills. Vision-language models (VLMs) pretrained on Internet data could in principle offer a framework for tackling such problems. However, in their current form, VLMs lack both the nuanced understanding of intricate physics required for robotic manipulation and the ability to reason over long horizons to address error compounding issues. In this paper, we introduce a novel test-time computation framework that enhances VLMs' physical reasoning capabilities for multi-stage manipulation tasks. At its core, our approach iteratively improves a pretrained VLM with a "reflection" mechanism - it uses a generative model to imagine future world states, leverages these predictions to guide action selection, and critically reflects on potential suboptimalities to refine its reasoning. Experimental results demonstrate that our method significantly outperforms several state-of-the-art commercial VLMs as well as other post-training approaches such as Monte Carlo Tree Search (MCTS). Videos are available at https://reflect-vlm.github.io.

### Benchmarking Online Object Trackers for Underwater Robot Position Locking Applications 
[[arxiv](https://arxiv.org/abs/2502.16569)] [[cool](https://papers.cool/arxiv/2502.16569)] [[pdf](https://arxiv.org/pdf/2502.16569)]
> **Authors**: Ali Safa,Waqas Aman,Ali Al-Zawqari,Saif Al-Kuwari
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,计算机视觉和模式识别
- **Abstract**: Autonomously controlling the position of Remotely Operated underwater Vehicles (ROVs) is of crucial importance for a wide range of underwater engineering applications, such as in the inspection and maintenance of underwater industrial structures. Consequently, studying vision-based underwater robot navigation and control has recently gained increasing attention to counter the numerous challenges faced in underwater conditions, such as lighting variability, turbidity, camera image distortions (due to bubbles), and ROV positional disturbances (due to underwater currents). In this paper, we propose (to the best of our knowledge) a first rigorous unified benchmarking of more than seven Machine Learning (ML)-based one-shot object tracking algorithms for vision-based position locking of ROV platforms. We propose a position-locking system that processes images of an object of interest in front of which the ROV must be kept stable. Then, our proposed system uses the output result of different object tracking algorithms to automatically correct the position of the ROV against external disturbances. We conducted numerous real-world experiments using a BlueROV2 platform within an indoor pool and provided clear demonstrations of the strengths and weaknesses of each tracking approach. Finally, to help alleviate the scarcity of underwater ROV data, we release our acquired data base as open-source with the hope of benefiting future research.

### Gaussian Process Regression for Improved Underwater Navigation 
[[arxiv](https://arxiv.org/abs/2502.16510)] [[cool](https://papers.cool/arxiv/2502.16510)] [[pdf](https://arxiv.org/pdf/2502.16510)]
> **Authors**: Nadav Cohen,Itzik Klein
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,人工智能,信号处理,系统与控制
- **Abstract**: Accurate underwater navigation is a challenging task due to the absence of global navigation satellite system signals and the reliance on inertial navigation systems that suffer from drift over time. Doppler velocity logs (DVLs) are typically used to mitigate this drift through velocity measurements, which are commonly estimated using a parameter estimation approach such as least squares (LS). However, LS works under the assumption of ideal conditions and does not account for sensor biases, leading to suboptimal performance. This paper proposes a data-driven alternative based on multi-output Gaussian process regression (MOGPR) to improve DVL velocity estimation. MOGPR provides velocity estimates and associated measurement covariances, enabling an adaptive integration within an error-state Extended Kalman Filter (EKF). We evaluate our proposed approach using real-world AUV data and compare it against LS and a state-of-the-art deep learning model, BeamsNet. Results demonstrate that MOGPR reduces velocity estimation errors by approximately 20% while simultaneously enhancing overall navigation accuracy, particularly in the orientation states. Additionally, the incorporation of uncertainty estimates from MOGPR enables an adaptive EKF framework, improving navigation robustness in dynamic underwater environments.

### Learning Humanoid Locomotion with World Model Reconstruction 
[[arxiv](https://arxiv.org/abs/2502.16230)] [[cool](https://papers.cool/arxiv/2502.16230)] [[pdf](https://arxiv.org/pdf/2502.16230)]
> **Authors**: Wandong Sun,Long Chen,Yongbo Su,Baoshi Cao,Yang Liu,Zongwu Xie
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,机器学习
- **Abstract**: Humanoid robots are designed to navigate environments accessible to humans using their legs. However, classical research has primarily focused on controlled laboratory settings, resulting in a gap in developing controllers for navigating complex real-world terrains. This challenge mainly arises from the limitations and noise in sensor data, which hinder the robot's understanding of itself and the environment. In this study, we introduce World Model Reconstruction (WMR), an end-to-end learning-based approach for blind humanoid locomotion across challenging terrains. We propose training an estimator to explicitly reconstruct the world state and utilize it to enhance the locomotion policy. The locomotion policy takes inputs entirely from the reconstructed information. The policy and the estimator are trained jointly; however, the gradient between them is intentionally cut off. This ensures that the estimator focuses solely on world reconstruction, independent of the locomotion policy's updates. We evaluated our model on rough, deformable, and slippery surfaces in real-world scenarios, demonstrating robust adaptability and resistance to interference. The robot successfully completed a 3.2 km hike without any human assistance, mastering terrains covered with ice and snow.

### Together We Rise: Optimizing Real-Time Multi-Robot Task Allocation using Coordinated Heterogeneous Plays 
[[arxiv](https://arxiv.org/abs/2502.16079)] [[cool](https://papers.cool/arxiv/2502.16079)] [[pdf](https://arxiv.org/pdf/2502.16079)]
> **Authors**: Aritra Pal,Anandsingh Chauhan,Mayank Baranwal
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Accepted to AAMAS 2025 (AAAI Track)
- **标题**: None
- **领域**: 机器人技术,人工智能,机器学习,多代理系统,系统与控制
- **Abstract**: Efficient task allocation among multiple robots is crucial for optimizing productivity in modern warehouses, particularly in response to the increasing demands of online order fulfillment. This paper addresses the real-time multi-robot task allocation (MRTA) problem in dynamic warehouse environments, where tasks emerge with specified start and end locations. The objective is to minimize both the total travel distance of robots and delays in task completion, while also considering practical constraints such as battery management and collision avoidance. We introduce MRTAgent, a dual-agent Reinforcement Learning (RL) framework inspired by self-play, designed to optimize task assignments and robot selection to ensure timely task execution. For safe navigation, a modified linear quadratic controller (LQR) approach is employed. To the best of our knowledge, MRTAgent is the first framework to address all critical aspects of practical MRTA problems while supporting continuous robot movements.

### BOSS: Benchmark for Observation Space Shift in Long-Horizon Task 
[[arxiv](https://arxiv.org/abs/2502.15679)] [[cool](https://papers.cool/arxiv/2502.15679)] [[pdf](https://arxiv.org/pdf/2502.15679)]
> **Authors**: Yue Yang,Linfeng Zhao,Mingyu Ding,Gedas Bertasius,Daniel Szafir
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,人工智能,计算机视觉和模式识别
- **Abstract**: Robotics has long sought to develop visual-servoing robots capable of completing previously unseen long-horizon tasks. Hierarchical approaches offer a pathway for achieving this goal by executing skill combinations arranged by a task planner, with each visuomotor skill pre-trained using a specific imitation learning (IL) algorithm. However, even in simple long-horizon tasks like skill chaining, hierarchical approaches often struggle due to a problem we identify as Observation Space Shift (OSS), where the sequential execution of preceding skills causes shifts in the observation space, disrupting the performance of subsequent individually trained skill policies. To validate OSS and evaluate its impact on long-horizon tasks, we introduce BOSS (a Benchmark for Observation Space Shift). BOSS comprises three distinct challenges: "Single Predicate Shift", "Accumulated Predicate Shift", and "Skill Chaining", each designed to assess a different aspect of OSS's negative effect. We evaluated several recent popular IL algorithms on BOSS, including three Behavioral Cloning methods and the Visual Language Action model OpenVLA. Even on the simplest challenge, we observed average performance drops of 67%, 35%, 34%, and 54%, respectively, when comparing skill performance with and without OSS. Additionally, we investigate a potential solution to OSS that scales up the training data for each skill with a larger and more visually diverse set of demonstrations, with our results showing it is not sufficient to resolve OSS. The project page is: https://boss-benchmark.github.io/

### Exploring Embodied Multimodal Large Models: Development, Datasets, and Future Directions 
[[arxiv](https://arxiv.org/abs/2502.15336)] [[cool](https://papers.cool/arxiv/2502.15336)] [[pdf](https://arxiv.org/pdf/2502.15336)]
> **Authors**: Shoubin Chen,Zehao Wu,Kai Zhang,Chunyu Li,Baiyang Zhang,Fei Ma,Fei Richard Yu,Qingquan Li
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 81 pages, submitted to a journal for review
- **标题**: None
- **领域**: 机器人技术,人工智能
- **Abstract**: Embodied multimodal large models (EMLMs) have gained significant attention in recent years due to their potential to bridge the gap between perception, cognition, and action in complex, real-world environments. This comprehensive review explores the development of such models, including Large Language Models (LLMs), Large Vision Models (LVMs), and other models, while also examining other emerging architectures. We discuss the evolution of EMLMs, with a focus on embodied perception, navigation, interaction, and simulation. Furthermore, the review provides a detailed analysis of the datasets used for training and evaluating these models, highlighting the importance of diverse, high-quality data for effective learning. The paper also identifies key challenges faced by EMLMs, including issues of scalability, generalization, and real-time decision-making. Finally, we outline future directions, emphasizing the integration of multimodal sensing, reasoning, and action to advance the development of increasingly autonomous systems. By providing an in-depth analysis of state-of-the-art methods and identifying critical gaps, this paper aims to inspire future advancements in EMLMs and their applications across diverse domains.

## 声音(cs.SD:Sound)

### AAD-LLM: Neural Attention-Driven Auditory Scene Understanding 
[[arxiv](https://arxiv.org/abs/2502.16794)] [[cool](https://papers.cool/arxiv/2502.16794)] [[pdf](https://arxiv.org/pdf/2502.16794)]
> **Authors**: Xilin Jiang,Sukru Samet Dindar,Vishal Choudhari,Stephan Bickel,Ashesh Mehta,Guy M McKhann,Adeen Flinker,Daniel Friedman,Nima Mesgarani
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 声音,人工智能,计算语言学,人机交互,音频和语音处理
- **Abstract**: Auditory foundation models, including auditory large language models (LLMs), process all sound inputs equally, independent of listener perception. However, human auditory perception is inherently selective: listeners focus on specific speakers while ignoring others in complex auditory scenes. Existing models do not incorporate this selectivity, limiting their ability to generate perception-aligned responses. To address this, we introduce Intention-Informed Auditory Scene Understanding (II-ASU) and present Auditory Attention-Driven LLM (AAD-LLM), a prototype system that integrates brain signals to infer listener attention. AAD-LLM extends an auditory LLM by incorporating intracranial electroencephalography (iEEG) recordings to decode which speaker a listener is attending to and refine responses accordingly. The model first predicts the attended speaker from neural activity, then conditions response generation on this inferred attentional state. We evaluate AAD-LLM on speaker description, speech transcription and extraction, and question answering in multitalker scenarios, with both objective and subjective ratings showing improved alignment with listener intention. By taking a first step toward intention-aware auditory AI, this work explores a new paradigm where listener perception informs machine listening, paving the way for future listener-centered auditory systems. Demo and code available: https://aad-llm.github.io.

### Audio-FLAN: A Preliminary Release 
[[arxiv](https://arxiv.org/abs/2502.16584)] [[cool](https://papers.cool/arxiv/2502.16584)] [[pdf](https://arxiv.org/pdf/2502.16584)]
> **Authors**: Liumeng Xue,Ziya Zhou,Jiahao Pan,Zixuan Li,Shuai Fan,Yinghao Ma,Sitong Cheng,Dongchao Yang,Haohan Guo,Yujia Xiao,Xinsheng Wang,Zixuan Shen,Chuanbo Zhu,Xinshen Zhang,Tianchi Liu,Ruibin Yuan,Zeyue Tian,Haohe Liu,Emmanouil Benetos,Ge Zhang,Yike Guo,Wei Xue
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 声音,人工智能,计算语言学,多媒体,音频和语音处理
- **Abstract**: Recent advancements in audio tokenization have significantly enhanced the integration of audio capabilities into large language models (LLMs). However, audio understanding and generation are often treated as distinct tasks, hindering the development of truly unified audio-language models. While instruction tuning has demonstrated remarkable success in improving generalization and zero-shot learning across text and vision, its application to audio remains largely unexplored. A major obstacle is the lack of comprehensive datasets that unify audio understanding and generation. To address this, we introduce Audio-FLAN, a large-scale instruction-tuning dataset covering 80 diverse tasks across speech, music, and sound domains, with over 100 million instances. Audio-FLAN lays the foundation for unified audio-language models that can seamlessly handle both understanding (e.g., transcription, comprehension) and generation (e.g., speech, music, sound) tasks across a wide range of audio domains in a zero-shot manner. The Audio-FLAN dataset is available on HuggingFace and GitHub and will be continuously updated.

### Benchmarking machine learning for bowel sound pattern classification from tabular features to pretrained models 
[[arxiv](https://arxiv.org/abs/2502.15607)] [[cool](https://papers.cool/arxiv/2502.15607)] [[pdf](https://arxiv.org/pdf/2502.15607)]
> **Authors**: Zahra Mansour,Verena Uslar,Dirk Weyhe,Danilo Hollosi,Nils Strodthoff
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 9 pages, 6 figures and 1 table
- **标题**: None
- **领域**: 声音,机器学习,音频和语音处理,信号处理
- **Abstract**: The development of electronic stethoscopes and wearable recording sensors opened the door to the automated analysis of bowel sound (BS) signals. This enables a data-driven analysis of bowel sound patterns, their interrelations, and their correlation to different pathologies. This work leverages a BS dataset collected from 16 healthy subjects that was annotated according to four established BS patterns. This dataset is used to evaluate the performance of machine learning models to detect and/or classify BS patterns. The selection of considered models covers models using tabular features, convolutional neural networks based on spectrograms and models pre-trained on large audio datasets. The results highlight the clear superiority of pre-trained models, particularly in detecting classes with few samples, achieving an AUC of 0.89 in distinguishing BS from non-BS using a HuBERT model and an AUC of 0.89 in differentiating bowel sound patterns using a Wav2Vec 2.0 model. These results pave the way for an improved understanding of bowel sounds in general and future machine-learning-driven diagnostic applications for gastrointestinal examinations

### KAD: No More FAD! An Effective and Efficient Evaluation Metric for Audio Generation 
[[arxiv](https://arxiv.org/abs/2502.15602)] [[cool](https://papers.cool/arxiv/2502.15602)] [[pdf](https://arxiv.org/pdf/2502.15602)]
> **Authors**: Yoonjin Chung,Pilsun Eu,Junwon Lee,Keunwoo Choi,Juhan Nam,Ben Sangbae Chon
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 声音,人工智能,机器学习,音频和语音处理
- **Abstract**: Although being widely adopted for evaluating generated audio signals, the Fréchet Audio Distance (FAD) suffers from significant limitations, including reliance on Gaussian assumptions, sensitivity to sample size, and high computational complexity. As an alternative, we introduce the Kernel Audio Distance (KAD), a novel, distribution-free, unbiased, and computationally efficient metric based on Maximum Mean Discrepancy (MMD). Through analysis and empirical validation, we demonstrate KAD's advantages: (1) faster convergence with smaller sample sizes, enabling reliable evaluation with limited data; (2) lower computational cost, with scalable GPU acceleration; and (3) stronger alignment with human perceptual judgments. By leveraging advanced embeddings and characteristic kernels, KAD captures nuanced differences between real and generated audio. Open-sourced in the kadtk toolkit, KAD provides an efficient, reliable, and perceptually aligned benchmark for evaluating generative audio models.

## 软件工程(cs.SE:Software Engineering)

### Beyond Trusting Trust: Multi-Model Validation for Robust Code Generation 
[[arxiv](https://arxiv.org/abs/2502.16279)] [[cool](https://papers.cool/arxiv/2502.16279)] [[pdf](https://arxiv.org/pdf/2502.16279)]
> **Authors**: Bradley McDanel
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: 3 pages, 2 figures
- **标题**: None
- **领域**: 软件工程,人工智能,密码学和安全
- **Abstract**: This paper explores the parallels between Thompson's "Reflections on Trusting Trust" and modern challenges in LLM-based code generation. We examine how Thompson's insights about compiler backdoors take on new relevance in the era of large language models, where the mechanisms for potential exploitation are even more opaque and difficult to analyze. Building on this analogy, we discuss how the statistical nature of LLMs creates novel security challenges in code generation pipelines. As a potential direction forward, we propose an ensemble-based validation approach that leverages multiple independent models to detect anomalous code patterns through cross-model consensus. This perspective piece aims to spark discussion about trust and validation in AI-assisted software development.

### Practical programming research of Linear DML model based on the simplest Python code: From the standpoint of novice researchers 
[[arxiv](https://arxiv.org/abs/2502.16172)] [[cool](https://papers.cool/arxiv/2502.16172)] [[pdf](https://arxiv.org/pdf/2502.16172)]
> **Authors**: Shunxin Yao
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: 12 pages, 4 tables, 3 figures
- **标题**: None
- **领域**: 软件工程,机器学习
- **Abstract**: This paper presents linear DML models for causal inference using the simplest Python code on a Jupyter notebook based on an Anaconda platform and compares the performance of different DML models. The results show that current Library API technology is not yet sufficient to enable novice Python users to build qualified and high-quality DML models with the simplest coding approach. Novice users attempting to perform DML causal inference using Python still have to improve their mathematical and computer knowledge to adapt to more flexible DML programming. Additionally, the issue of mismatched outcome variable dimensions is also widespread when building linear DML models in Jupyter notebook.

### LLMs in Mobile Apps: Practices, Challenges, and Opportunities 
[[arxiv](https://arxiv.org/abs/2502.15908)] [[cool](https://papers.cool/arxiv/2502.15908)] [[pdf](https://arxiv.org/pdf/2502.15908)]
> **Authors**: Kimberly Hau,Safwat Hassan,Shurui Zhou
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,计算语言学
- **Abstract**: The integration of AI techniques has become increasingly popular in software development, enhancing performance, usability, and the availability of intelligent features. With the rise of large language models (LLMs) and generative AI, developers now have access to a wealth of high-quality open-source models and APIs from closed-source providers, enabling easier experimentation and integration of LLMs into various systems. This has also opened new possibilities in mobile application (app) development, allowing for more personalized and intelligent apps. However, integrating LLM into mobile apps might present unique challenges for developers, particularly regarding mobile device constraints, API management, and code infrastructure. In this project, we constructed a comprehensive dataset of 149 LLM-enabled Android apps and conducted an exploratory analysis to understand how LLMs are deployed and used within mobile apps. This analysis highlights key characteristics of the dataset, prevalent integration strategies, and common challenges developers face. Our findings provide valuable insights for future research and tooling development aimed at enhancing LLM-enabled mobile apps.

### Show Me Your Code! Kill Code Poisoning: A Lightweight Method Based on Code Naturalness 
[[arxiv](https://arxiv.org/abs/2502.15830)] [[cool](https://papers.cool/arxiv/2502.15830)] [[pdf](https://arxiv.org/pdf/2502.15830)]
> **Authors**: Weisong Sun,Yuchen Chen,Mengzhe Yuan,Chunrong Fang,Zhenpeng Chen,Chong Wang,Yang Liu,Baowen Xu,Zhenyu Chen
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-24
> **comment**: Accepted to the 47th International Conference on Software Engineering (ICSE 2025)
- **标题**: None
- **领域**: 软件工程,人工智能,密码学和安全
- **Abstract**: Neural code models (NCMs) have demonstrated extraordinary capabilities in code intelligence tasks. Meanwhile, the security of NCMs and NCMs-based systems has garnered increasing attention. In particular, NCMs are often trained on large-scale data from potentially untrustworthy sources, providing attackers with the opportunity to manipulate them by inserting crafted samples into the data. This type of attack is called a code poisoning attack (also known as a backdoor attack). It allows attackers to implant backdoors in NCMs and thus control model behavior, which poses a significant security threat. However, there is still a lack of effective techniques for detecting various complex code poisoning attacks. In this paper, we propose an innovative and lightweight technique for code poisoning detection named KillBadCode. KillBadCode is designed based on our insight that code poisoning disrupts the naturalness of code. Specifically, KillBadCode first builds a code language model (CodeLM) on a lightweight $n$-gram language model. Then, given poisoned data, KillBadCode utilizes CodeLM to identify those tokens in (poisoned) code snippets that will make the code snippets more natural after being deleted as trigger tokens. Considering that the removal of some normal tokens in a single sample might also enhance code naturalness, leading to a high false positive rate (FPR), we aggregate the cumulative improvement of each token across all samples. Finally, KillBadCode purifies the poisoned data by removing all poisoned samples containing the identified trigger tokens. The experimental results on two code poisoning attacks and four code intelligence tasks demonstrate that KillBadCode significantly outperforms four baselines. More importantly, KillBadCode is very efficient, with a minimum time consumption of only 5 minutes, and is 25 times faster than the best baseline on average.

### Multi-Objective Reinforcement Learning for Critical Scenario Generation of Autonomous Vehicles 
[[arxiv](https://arxiv.org/abs/2502.15792)] [[cool](https://papers.cool/arxiv/2502.15792)] [[pdf](https://arxiv.org/pdf/2502.15792)]
> **Authors**: Jiahui Wu,Chengjie Lu,Aitor Arrieta,Shaukat Ali
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,机器学习,机器人技术
- **Abstract**: Autonomous vehicles (AVs) make driving decisions without human intervention. Therefore, ensuring AVs' dependability is critical. Despite significant research and development in AV development, their dependability assurance remains a significant challenge due to the complexity and unpredictability of their operating environments. Scenario-based testing evaluates AVs under various driving scenarios, but the unlimited number of potential scenarios highlights the importance of identifying critical scenarios that can violate safety or functional requirements. Such requirements are inherently interdependent and need to be tested simultaneously. To this end, we propose MOEQT, a novel multi-objective reinforcement learning (MORL)-based approach to generate critical scenarios that simultaneously test interdependent safety and functional requirements. MOEQT adapts Envelope Q-learning as the MORL algorithm, which dynamically adapts multi-objective weights to balance the relative importance between multiple objectives. MOEQT generates critical scenarios to violate multiple requirements through dynamically interacting with the AV environment, ensuring comprehensive AV testing. We evaluate MOEQT using an advanced end-to-end AV controller and a high-fidelity simulator and compare MOEQT with two baselines: a random strategy and a single-objective RL with a weighted reward function. Our evaluation results show that MOEQT achieved an overall better performance in identifying critical scenarios for violating multiple requirements than the baselines.

### Performance Review on LLM for solving leetcode problems 
[[arxiv](https://arxiv.org/abs/2502.15770)] [[cool](https://papers.cool/arxiv/2502.15770)] [[pdf](https://arxiv.org/pdf/2502.15770)]
> **Authors**: Lun Wang,Chuanqi Shi,Shaoshui Du,Yiyi Tao,Yixian Shen,Hang Zheng,Xinyu Qiu
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: This paper presents a comprehensive performance evaluation of Large Language Models (LLMs) in solving programming challenges from Leetcode, a widely used platform for algorithm practice and technical interviews. We began by crawling the Leetcode website to collect a diverse set of problems encompassing various difficulty levels and topics. Using this dataset, we generated solutions with multiple LLMs, including GPT-4 and GPT-3.5-turbo (ChatGPT-turbo). The generated solutions were systematically evaluated for correctness and efficiency. We employed the pass@k metric to assess the success rates within a given number of attempts and analyzed the runtime performance of the solutions. Our results highlight the strengths and limitations of current LLMs [10] in code generation and problem-solving tasks, providing insights into their potential applications and areas for improvement in automated programming assistance.

### Detection of LLM-Generated Java Code Using Discretized Nested Bigrams 
[[arxiv](https://arxiv.org/abs/2502.15740)] [[cool](https://papers.cool/arxiv/2502.15740)] [[pdf](https://arxiv.org/pdf/2502.15740)]
> **Authors**: Timothy Paek,Chilukuri Mohan
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-24
> **comment**: This preprint precedes the final peer-reviewed version, which will be published in Springer's CSCI 2024 proceedings
- **标题**: None
- **领域**: 软件工程,人工智能,计算语言学,机器学习
- **Abstract**: Large Language Models (LLMs) are currently used extensively to generate code by professionals and students, motivating the development of tools to detect LLM-generated code for applications such as academic integrity and cybersecurity. We address this authorship attribution problem as a binary classification task along with feature identification and extraction. We propose new Discretized Nested Bigram Frequency features on source code groups of various sizes. Compared to prior work, improvements are obtained by representing sparse information in dense membership bins. Experimental evaluation demonstrated that our approach significantly outperformed a commonly used GPT code-detection API and baseline features, with accuracy exceeding 96% compared to 72% and 79% respectively in detecting GPT-rewritten Java code fragments for 976 files with GPT 3.5 and GPT4 using 12 features. We also outperformed three prior works on code author identification in a 40-author dataset. Our approach scales well to larger data sets, and we achieved 99% accuracy and 0.999 AUC for 76,089 files and over 1,000 authors with GPT 4o using 227 features.

### Time Warp: The Gap Between Developers' Ideal vs Actual Workweeks in an AI-Driven Era 
[[arxiv](https://arxiv.org/abs/2502.15287)] [[cool](https://papers.cool/arxiv/2502.15287)] [[pdf](https://arxiv.org/pdf/2502.15287)]
> **Authors**: Sukrit Kumar,Drishti Goel,Thomas Zimmermann,Brian Houck,B. Ashok,Chetan Bansal
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: ICSE SEIP 2025
- **标题**: None
- **领域**: 软件工程,人工智能,人机交互
- **Abstract**: Software developers balance a variety of different tasks in a workweek, yet the allocation of time often differs from what they consider ideal. Identifying and addressing these deviations is crucial for organizations aiming to enhance the productivity and well-being of the developers. In this paper, we present the findings from a survey of 484 software developers at Microsoft, which aims to identify the key differences between how developers would like to allocate their time during an ideal workweek versus their actual workweek. Our analysis reveals significant deviations between a developer's ideal workweek and their actual workweek, with a clear correlation: as the gap between these two workweeks widens, we observe a decline in both productivity and satisfaction. By examining these deviations in specific activities, we assess their direct impact on the developers' satisfaction and productivity. Additionally, given the growing adoption of AI tools in software engineering, both in the industry and academia, we identify specific tasks and areas that could be strong candidates for automation. In this paper, we make three key contributions: 1) We quantify the impact of workweek deviations on developer productivity and satisfaction 2) We identify individual tasks that disproportionately affect satisfaction and productivity 3) We provide actual data-driven insights to guide future AI automation efforts in software engineering, aligning them with the developers' requirements and ideal workflows for maximizing their productivity and satisfaction.

### Comparative Analysis of Large Language Models for Context-Aware Code Completion using SAFIM Framework 
[[arxiv](https://arxiv.org/abs/2502.15243)] [[cool](https://papers.cool/arxiv/2502.15243)] [[pdf](https://arxiv.org/pdf/2502.15243)]
> **Authors**: Hang Zhang,Yanxin Shen,Lun Wang,Chuanqi Shi,Shaoshuai Du,Yiyi Tao,Yixian Shen
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 9 pages
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: The advent of Large Language Models (LLMs) has revolutionized code completion, transforming it into a more intelligent and context-aware feature in modern integrated development environments. These advancements have significantly enhanced developers' ability to write efficient and error-free code. This study evaluates the performance of several chat-based LLMs, including Gemini 1.5 Flash, Gemini 1.5 Pro, GPT-4o, GPT-4o-mini, and GPT-4 Turbo, using the Syntax-Aware Fill-in-the-Middle (SAFIM) dataset. This benchmark is specifically designed to assess models' capabilities in syntax-sensitive code generation. Performance metrics, such as cosine similarity with ground-truth completions and latency, were employed to measure both accuracy and efficiency. The findings reveal substantial differences in the models' code completion abilities, offering valuable insights into their respective strengths and weaknesses. This work provides a comparative analysis that underscores the trade-offs between accuracy and speed, establishing a benchmark for future advancements in LLM-based code completion.

### FormalSpecCpp: A Dataset of C++ Formal Specifications created using LLMs 
[[arxiv](https://arxiv.org/abs/2502.15217)] [[cool](https://papers.cool/arxiv/2502.15217)] [[pdf](https://arxiv.org/pdf/2502.15217)]
> **Authors**: Madhurima Chakraborty,Peter Pirkelbauer,Qing Yi
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Accepted at the 2025 IEEE/ACM 22nd International Conference on Mining Software Repositories (MSR)
- **标题**: None
- **领域**: 软件工程,人工智能,机器学习,编程语言
- **Abstract**: FormalSpecCpp is a dataset designed to fill the gap in standardized benchmarks for verifying formal specifications in C++ programs. To the best of our knowledge, this is the first comprehensive collection of C++ programs with well-defined preconditions and postconditions. It provides a structured benchmark for evaluating specification inference tools and testing theaccuracy of generated specifications. Researchers and developers can use this dataset to benchmark specification inference tools,fine-tune Large Language Models (LLMs) for automated specification generation, and analyze the role of formal specifications in improving program verification and automated testing. By making this dataset publicly available, we aim to advance research in program verification, specification inference, and AI-assisted software development. The dataset and the code are available at https://github.com/MadhuNimmo/FormalSpecCpp.

## 社交和信息网络(cs.SI:Social and Information Networks)

### Efficient Estimation of Shortest-Path Distance Distributions to Samples in Graphs 
[[arxiv](https://arxiv.org/abs/2502.15890)] [[cool](https://papers.cool/arxiv/2502.15890)] [[pdf](https://arxiv.org/pdf/2502.15890)]
> **Authors**: Alan Zhu,Jiaqi Ma,Qiaozhu Mei
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 社交和信息网络,机器学习
- **Abstract**: As large graph datasets become increasingly common across many fields, sampling is often needed to reduce the graphs into manageable sizes. This procedure raises critical questions about representativeness as no sample can capture the properties of the original graph perfectly, and different parts of the graph are not evenly affected by the loss. Recent work has shown that the distances from the non-sampled nodes to the sampled nodes can be a quantitative indicator of bias and fairness in graph machine learning. However, to our knowledge, there is no method for evaluating how a sampling method affects the distribution of shortest-path distances without actually performing the sampling and shortest-path calculation. In this paper, we present an accurate and efficient framework for estimating the distribution of shortest-path distances to the sample, applicable to a wide range of sampling methods and graph structures. Our framework is faster than empirical methods and only requires the specification of degree distributions. We also extend our framework to handle graphs with community structures. While this introduces a decrease in accuracy, we demonstrate that our framework remains highly accurate on downstream comparison-based tasks. Code is publicly available at https://github.com/az1326/shortest_paths.

## 音频和语音处理(eess.AS:Audio and Speech Processing)

### Speech Enhancement Using Continuous Embeddings of Neural Audio Codec 
[[arxiv](https://arxiv.org/abs/2502.16240)] [[cool](https://papers.cool/arxiv/2502.16240)] [[pdf](https://arxiv.org/pdf/2502.16240)]
> **Authors**: Haoyang Li,Jia Qi Yip,Tianyu Fan,Eng Siong Chng
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: Accepted to ICASSP 2025
- **标题**: None
- **领域**: 音频和语音处理,人工智能,机器学习,声音
- **Abstract**: Recent advancements in Neural Audio Codec (NAC) models have inspired their use in various speech processing tasks, including speech enhancement (SE). In this work, we propose a novel, efficient SE approach by leveraging the pre-quantization output of a pretrained NAC encoder. Unlike prior NAC-based SE methods, which process discrete speech tokens using Language Models (LMs), we perform SE within the continuous embedding space of the pretrained NAC, which is highly compressed along the time dimension for efficient representation. Our lightweight SE model, optimized through an embedding-level loss, delivers results comparable to SE baselines trained on larger datasets, with a significantly lower real-time factor of 0.005. Additionally, our method achieves a low GMAC of 3.94, reducing complexity 18-fold compared to Sepformer in a simulated cloud-based audio transmission environment. This work highlights a new, efficient NAC-based SE solution, particularly suitable for cloud applications where NAC is used to compress audio before transmission. Copyright 20XX IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.

## 图像和视频处理(eess.IV:Image and Video Processing)

### Diagnosing COVID-19 Severity from Chest X-Ray Images Using ViT and CNN Architectures 
[[arxiv](https://arxiv.org/abs/2502.16622)] [[cool](https://papers.cool/arxiv/2502.16622)] [[pdf](https://arxiv.org/pdf/2502.16622)]
> **Authors**: Luis Lara,Lucia Eve Berger,Rajesh Raju
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: The COVID-19 pandemic strained healthcare resources and prompted discussion about how machine learning can alleviate physician burdens and contribute to diagnosis. Chest x-rays (CXRs) are used for diagnosis of COVID-19, but few studies predict the severity of a patient's condition from CXRs. In this study, we produce a large COVID severity dataset by merging three sources and investigate the efficacy of transfer learning using ImageNet- and CXR-pretrained models and vision transformers (ViTs) in both severity regression and classification tasks. A pretrained DenseNet161 model performed the best on the three class severity prediction problem, reaching 80% accuracy overall and 77.3%, 83.9%, and 70% on mild, moderate and severe cases, respectively. The ViT had the best regression results, with a mean absolute error of 0.5676 compared to radiologist-predicted severity scores. The project's source code is publicly available.

### Deep learning approaches to surgical video segmentation and object detection: A Scoping Review 
[[arxiv](https://arxiv.org/abs/2502.16459)] [[cool](https://papers.cool/arxiv/2502.16459)] [[pdf](https://arxiv.org/pdf/2502.16459)]
> **Authors**: Devanish N. Kamtam,Joseph B. Shrager,Satya Deepya Malla,Nicole Lin,Juan J. Cardona,Jake J. Kim,Clarence Hu
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: 38 pages, 2 figures
- **标题**: None
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **Abstract**: Introduction: Computer vision (CV) has had a transformative impact in biomedical fields such as radiology, dermatology, and pathology. Its real-world adoption in surgical applications, however, remains limited. We review the current state-of-the-art performance of deep learning (DL)-based CV models for segmentation and object detection of anatomical structures in videos obtained during surgical procedures. Methods: We conducted a scoping review of studies on semantic segmentation and object detection of anatomical structures published between 2014 and 2024 from 3 major databases - PubMed, Embase, and IEEE Xplore. The primary objective was to evaluate the state-of-the-art performance of semantic segmentation in surgical videos. Secondary objectives included examining DL models, progress toward clinical applications, and the specific challenges with segmentation of organs/tissues in surgical videos. Results: We identified 58 relevant published studies. These focused predominantly on procedures from general surgery [20(34.4%)], colorectal surgery [9(15.5%)], and neurosurgery [8(13.8%)]. Cholecystectomy [14(24.1%)] and low anterior rectal resection [5(8.6%)] were the most common procedures addressed. Semantic segmentation [47(81%)] was the primary CV task. U-Net [14(24.1%)] and DeepLab [13(22.4%)] were the most widely used models. Larger organs such as the liver (Dice score: 0.88) had higher accuracy compared to smaller structures such as nerves (Dice score: 0.49). Models demonstrated real-time inference potential ranging from 5-298 frames-per-second (fps). Conclusion: This review highlights the significant progress made in DL-based semantic segmentation for surgical videos with real-time applicability, particularly for larger organs. Addressing challenges with smaller structures, data availability, and generalizability remains crucial for future advancements.

### Large Language Model for Lossless Image Compression with Visual Prompts 
[[arxiv](https://arxiv.org/abs/2502.16163)] [[cool](https://papers.cool/arxiv/2502.16163)] [[pdf](https://arxiv.org/pdf/2502.16163)]
> **Authors**: Junhao Du,Chuqin Zhou,Ning Cao,Gang Chen,Yunuo Chen,Zhengxue Cheng,Li Song,Guo Lu,Wenjun Zhang
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: Recent advancements in deep learning have driven significant progress in lossless image compression. With the emergence of Large Language Models (LLMs), preliminary attempts have been made to leverage the extensive prior knowledge embedded in these pretrained models to enhance lossless image compression, particularly by improving the entropy model. However, a significant challenge remains in bridging the gap between the textual prior knowledge within LLMs and lossless image compression. To tackle this challenge and unlock the potential of LLMs, this paper introduces a novel paradigm for lossless image compression that incorporates LLMs with visual prompts. Specifically, we first generate a lossy reconstruction of the input image as visual prompts, from which we extract features to serve as visual embeddings for the LLM. The residual between the original image and the lossy reconstruction is then fed into the LLM along with these visual embeddings, enabling the LLM to function as an entropy model to predict the probability distribution of the residual. Extensive experiments on multiple benchmark datasets demonstrate our method achieves state-of-the-art compression performance, surpassing both traditional and learning-based lossless image codecs. Furthermore, our approach can be easily extended to images from other domains, such as medical and screen content images, achieving impressive performance. These results highlight the potential of LLMs for lossless image compression and may inspire further research in related directions.

### Patch Stitching Data Augmentation for Cancer Classification in Pathology Images 
[[arxiv](https://arxiv.org/abs/2502.16162)] [[cool](https://papers.cool/arxiv/2502.16162)] [[pdf](https://arxiv.org/pdf/2502.16162)]
> **Authors**: Jiamu Wang,Chang-Su Kim,Jin Tae Kwak
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: Computational pathology, integrating computational methods and digital imaging, has shown to be effective in advancing disease diagnosis and prognosis. In recent years, the development of machine learning and deep learning has greatly bolstered the power of computational pathology. However, there still remains the issue of data scarcity and data imbalance, which can have an adversarial effect on any computational method. In this paper, we introduce an efficient and effective data augmentation strategy to generate new pathology images from the existing pathology images and thus enrich datasets without additional data collection or annotation costs. To evaluate the proposed method, we employed two sets of colorectal cancer datasets and obtained improved classification results, suggesting that the proposed simple approach holds the potential for alleviating the data scarcity and imbalance in computational pathology.

### Anatomy-Informed Deep Learning and Radiomics for Automated Neurofibroma Segmentation in Whole-Body MRI 
[[arxiv](https://arxiv.org/abs/2502.15424)] [[cool](https://papers.cool/arxiv/2502.15424)] [[pdf](https://arxiv.org/pdf/2502.15424)]
> **Authors**: Georgii Kolokolnikov,Marie-Lena Schmalhofer,Lennart Well,Said Farschtschi,Victor-Felix Mautner,Inka Ristow,Rene Werner
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **Abstract**: Neurofibromatosis Type 1 is a genetic disorder characterized by the development of neurofibromas (NFs), which exhibit significant variability in size, morphology, and anatomical location. Accurate and automated segmentation of these tumors in whole-body magnetic resonance imaging (WB-MRI) is crucial to assess tumor burden and monitor disease progression. In this study, we present and analyze a fully automated pipeline for NF segmentation in fat-suppressed T2-weighted WB-MRI, consisting of three stages: anatomy segmentation, NF segmentation, and tumor candidate classification. In the first stage, we use the MRSegmentator model to generate an anatomy segmentation mask, extended with a high-risk zone for NFs. This mask is concatenated with the input image as anatomical context information for NF segmentation. The second stage employs an ensemble of 3D anisotropic anatomy-informed U-Nets to produce an NF segmentation confidence mask. In the final stage, tumor candidates are extracted from the confidence mask and classified based on radiomic features, distinguishing tumors from non-tumor regions and reducing false positives. We evaluate the proposed pipeline on three test sets representing different conditions: in-domain data (test set 1), varying imaging protocols and field strength (test set 2), and low tumor burden cases (test set 3). Experimental results show a 68% improvement in per-scan Dice Similarity Coefficient (DSC), a 21% increase in per-tumor DSC, and a two-fold improvement in F1 score for tumor detection in high tumor burden cases by integrating anatomy information. The method is integrated into the 3D Slicer platform for practical clinical use, with the code publicly accessible.

## 信号处理(eess.SP:Signal Processing)

### Software defined demodulation of multiple frequency shift keying with dense neural network for weak signal communications 
[[arxiv](https://arxiv.org/abs/2502.16371)] [[cool](https://papers.cool/arxiv/2502.16371)] [[pdf](https://arxiv.org/pdf/2502.16371)]
> **Authors**: Mykola Kozlenko,Vira Vialkova
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: 6 pages, 9 figures
- **标题**: None
- **领域**: 信号处理,机器学习
- **Abstract**: In this paper we present the symbol and bit error rate performance of the weak signal digital communications system. We investigate orthogonal multiple frequency shift keying modulation scheme with supervised machine learning demodulation approach using simple dense end-to-end artificial neural network. We focus on the interference immunity over an additive white Gaussian noise with average signal-to-noise ratios from -20 dB to 0 dB.

### rECGnition_v2.0: Self-Attentive Canonical Fusion of ECG and Patient Data using deep learning for effective Cardiac Diagnostics 
[[arxiv](https://arxiv.org/abs/2502.16255)] [[cool](https://papers.cool/arxiv/2502.16255)] [[pdf](https://arxiv.org/pdf/2502.16255)]
> **Authors**: Shreya Srivastava,Durgesh Kumar,Ram Jiwari,Sandeep Seth,Deepak Sharma
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 信号处理,人工智能,机器学习
- **Abstract**: The variability in ECG readings influenced by individual patient characteristics has posed a considerable challenge to adopting automated ECG analysis in clinical settings. A novel feature fusion technique termed SACC (Self Attentive Canonical Correlation) was proposed to address this. This technique is combined with DPN (Dual Pathway Network) and depth-wise separable convolution to create a robust, interpretable, and fast end-to-end arrhythmia classification model named rECGnition_v2.0 (robust ECG abnormality detection). This study uses MIT-BIH, INCARTDB and EDB dataset to evaluate the efficiency of rECGnition_v2.0 for various classes of arrhythmias. To investigate the influence of constituting model components, various ablation studies were performed, i.e. simple concatenation, CCA and proposed SACC were compared, while the importance of global and local ECG features were tested using DPN rECGnition_v2.0 model and vice versa. It was also benchmarked with state-of-the-art CNN models for overall accuracy vs model parameters, FLOPs, memory requirements, and prediction time. Furthermore, the inner working of the model was interpreted by comparing the activation locations in ECG before and after the SACC layer. rECGnition_v2.0 showed a remarkable accuracy of 98.07% and an F1-score of 98.05% for classifying ten distinct classes of arrhythmia with just 82.7M FLOPs per sample, thereby going beyond the performance metrics of current state-of-the-art (SOTA) models by utilizing MIT-BIH Arrhythmia dataset. Similarly, on INCARTDB and EDB datasets, excellent F1-scores of 98.01% and 96.21% respectively was achieved for AAMI classification. The compact architectural footprint of the rECGnition_v2.0, characterized by its lesser trainable parameters and diminished computational demands, unfurled several advantages including interpretability and scalability.

### Context-Aware Doubly-Robust Semi-Supervised Learning 
[[arxiv](https://arxiv.org/abs/2502.15577)] [[cool](https://papers.cool/arxiv/2502.15577)] [[pdf](https://arxiv.org/pdf/2502.15577)]
> **Authors**: Clement Ruah,Houssem Sifaou,Osvaldo Simeone,Bashir Al-Hashimi
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: This work has been submitted to the IEEE for possible publication
- **标题**: None
- **领域**: 信号处理,机器学习
- **Abstract**: The widespread adoption of artificial intelligence (AI) in next-generation communication systems is challenged by the heterogeneity of traffic and network conditions, which call for the use of highly contextual, site-specific, data. A promising solution is to rely not only on real-world data, but also on synthetic pseudo-data generated by a network digital twin (NDT). However, the effectiveness of this approach hinges on the accuracy of the NDT, which can vary widely across different contexts. To address this problem, this paper introduces context-aware doubly-robust (CDR) learning, a novel semi-supervised scheme that adapts its reliance on the pseudo-data to the different levels of fidelity of the NDT across contexts. CDR is evaluated on the task of downlink beamforming, showing superior performance compared to previous state-of-the-art semi-supervised approaches.

## 系统与控制(eess.SY:Systems and Control)

### From Target Tracking to Targeting Track -- Part I: A Metric for Spatio-Temporal Trajectory Evaluation 
[[arxiv](https://arxiv.org/abs/2502.15842)] [[cool](https://papers.cool/arxiv/2502.15842)] [[pdf](https://arxiv.org/pdf/2502.15842)]
> **Authors**: Tiancheng Li,Yan Song,Hongqi Fan,Jingdong Chen
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-24
> **comment**: Part I of a series of companion papers; 11 pages, 10 figures
- **标题**: None
- **领域**: 系统与控制,机器学习,信号处理
- **Abstract**: In the realm of target tracking, performance evaluation plays a pivotal role in the design, comparison, and analytics of trackers. Compared with the traditional trajectory composed of a set of point-estimates obtained by a tracker in the measurement time-series, the trajectory that our series of studies including this paper pursued is given by a curve function of time (FoT). The trajectory FoT provides complete information of the movement of the target over time and can be used to infer the state corresponding to arbitrary time, not only at the measurement time. However, there are no metrics available for comparing and evaluating the trajectory FoT. To address this lacuna, we propose a metric denominated as the spatiotemporal-aligned trajectory integral distance (Star-ID). The StarID associates and aligns the estimated and actual trajectories in the spatio-temporal domain and distinguishes between the time-aligned and unaligned segments in calculating the spatial divergence including false alarm, miss-detection and localization errors. The effectiveness of the proposed distance metric and the time-averaged version is validated through theoretical analysis and numerical examples of a single target or multiple targets.

### Model-free system identification of surface ships in waves via Hankel dynamic mode decomposition with control 
[[arxiv](https://arxiv.org/abs/2502.15782)] [[cool](https://papers.cool/arxiv/2502.15782)] [[pdf](https://arxiv.org/pdf/2502.15782)]
> **Authors**: Giorgio Palma,Andrea Serani,Shawn Aram,David W. Wundrow,David Drazen,Matteo Diez
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-24
> **comment**: arXiv admin note: substantial text overlap with arXiv:2411.07263
- **标题**: None
- **领域**: 系统与控制,机器学习
- **Abstract**: This study introduces and compares the Hankel dynamic mode decomposition with control (Hankel-DMDc) and a novel Bayesian extension of Hankel-DMDc as model-free (i.e., data-driven and equation-free) approaches for system identification and prediction of free-running ship motions in irregular waves. The proposed DMDc methods create a reduced-order model using limited data from the system state and incoming wave elevation histories, with the latter and rudder angle serving as forcing inputs. The inclusion of delayed states of the system as additional dimensions per the Hankel-DMDc improves the representation of the underlying non-linear dynamics of the system by DMD. The approaches are statistically assessed using data from free-running simulations of a 5415M hull's course-keeping in irregular beam-quartering waves at sea state 7, a highly severe condition characterized by nonlinear responses near roll-resonance. The results demonstrate robust performance and remarkable computational efficiency. The results indicate that the proposed methods effectively identify the dynamic system in analysis. Furthermore, the Bayesian formulation incorporates uncertainty quantification and enhances prediction accuracy. Ship motions are predicted with good agreement with test data over a 15 encounter waves observation window. No significant accuracy degradation is noted along the test sequences, suggesting the method can support accurate and efficient maritime design and operational planning.

### Feature Engineering Approach to Building Load Prediction: A Case Study for Commercial Building Chiller Plant Optimization in Tropical Weather 
[[arxiv](https://arxiv.org/abs/2502.15780)] [[cool](https://papers.cool/arxiv/2502.15780)] [[pdf](https://arxiv.org/pdf/2502.15780)]
> **Authors**: Zhan Wang,Chen Weidong,Huang Zhifeng,Md Raisul Islam,Chua Kian Jon
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 系统与控制,人工智能,机器学习
- **Abstract**: In tropical countries with high humidity, air conditioning can account for up to 60% of a building's energy use. For commercial buildings with centralized systems, the efficiency of the chiller plant is vital, and model predictive control provides an effective strategy for optimizing operations through dynamic adjustments based on accurate load predictions. Artificial neural networks are effective for modelling nonlinear systems but are prone to overfitting due to their complexity. Effective feature engineering can mitigate this issue. While weather data are crucial for load prediction, they are often used as raw numerical inputs without advanced processing. Clustering features is a technique that can reduce model complexity and enhance prediction accuracy. Although previous studies have explored clustering algorithms for load prediction, none have applied them to multidimensional weather data, revealing a research gap. This study presents a cooling load prediction model that combines a neural network with Kalman filtering and K-means clustering. Applied to real world data from a commercial skyscraper in Singapore's central business district, the model achieved a 46.5% improvement in prediction accuracy. An optimal chiller sequencing strategy was also developed through genetic algorithm optimization of the predictive load, potentially saving 13.8% in energy. Finally, the study evaluated the integration of thermal energy storage into the chiller plant design, demonstrating potential reductions in capital and operational costs of 26% and 13%, respectively.

### TSS GAZ PTP: Towards Improving Gumbel AlphaZero with Two-stage Self-play for Multi-constrained Electric Vehicle Routing Problems 
[[arxiv](https://arxiv.org/abs/2502.15777)] [[cool](https://papers.cool/arxiv/2502.15777)] [[pdf](https://arxiv.org/pdf/2502.15777)]
> **Authors**: Hui Wang,Xufeng Zhang,Xiaoyu Zhang,Zhenhuan Ding,Chaoxu Mu
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-24
> **comment**: 11 pages,9 figures
- **标题**: None
- **领域**: 系统与控制,人工智能
- **Abstract**: Recently, Gumbel AlphaZero~(GAZ) was proposed to solve classic combinatorial optimization problems such as TSP and JSSP by creating a carefully designed competition model~(consisting of a learning player and a competitor player), which leverages the idea of self-play. However, if the competitor is too strong or too weak, the effectiveness of self-play training can be reduced, particularly in complex CO problems. To address this problem, we further propose a two-stage self-play strategy to improve the GAZ method~(named TSS GAZ PTP). In the first stage, the learning player uses the enhanced policy network based on the Gumbel Monte Carlo Tree Search~(MCTS), and the competitor uses the historical best trained policy network~(acts as a greedy player). In the second stage, we employ Gumbel MCTS for both players, which makes the competition fiercer so that both players can continuously learn smarter trajectories. We first investigate the performance of our proposed TSS GAZ PTP method on TSP since it is also used as a test problem by the original GAZ. The results show the superior performance of TSS GAZ PTP. Then we extend TSS GAZ PTP to deal with multi-constrained Electric Vehicle Routing Problems~(EVRP), which is a recently well-known real application research topic and remains challenging as a complex CO problem. Impressively, the experimental results show that the TSS GAZ PTP outperforms the state-of-the-art Deep Reinforcement Learning methods in all types of instances tested and outperforms the optimization solver in tested large-scale instances, indicating the importance and promising of employing more dynamic self-play strategies for complex CO problems.

### Deep Reinforcement Learning-Based Bidding Strategies for Prosumers Trading in Double Auction-Based Transactive Energy Market 
[[arxiv](https://arxiv.org/abs/2502.15774)] [[cool](https://papers.cool/arxiv/2502.15774)] [[pdf](https://arxiv.org/pdf/2502.15774)]
> **Authors**: Jun Jiang,Yuanliang Li,Luyang Hou,Mohsen Ghafouri,Peng Zhang,Jun Yan,Yuhong Liu
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 系统与控制,计算机科学与博弈论,机器学习
- **Abstract**: With the large number of prosumers deploying distributed energy resources (DERs), integrating these prosumers into a transactive energy market (TEM) is a trend for the future smart grid. A community-based double auction market is considered a promising TEM that can encourage prosumers to participate and maximize social welfare. However, the traditional TEM is challenging to model explicitly due to the random bidding behavior of prosumers and uncertainties caused by the energy operation of DERs. Furthermore, although reinforcement learning algorithms provide a model-free solution to optimize prosumers' bidding strategies, their use in TEM is still challenging due to their scalability, stability, and privacy protection limitations. To address the above challenges, in this study, we design a double auction-based TEM with multiple DERs-equipped prosumers to transparently and efficiently manage energy transactions. We also propose a deep reinforcement learning (DRL) model with distributed learning and execution to ensure the scalability and privacy of the market environment. Additionally, the design of two bidding actions (i.e., bidding price and quantity) optimizes the bidding strategies for prosumers. Simulation results show that (1) the designed TEM and DRL model are robust; (2) the proposed DRL model effectively balances the energy payment and comfort satisfaction for prosumers and outperforms the state-of-the-art methods in optimizing the bidding strategies.

## 代数拓扑(math.AT:Algebraic Topology)

### Sheaf theory: from deep geometry to deep learning 
[[arxiv](https://arxiv.org/abs/2502.15476)] [[cool](https://papers.cool/arxiv/2502.15476)] [[pdf](https://arxiv.org/pdf/2502.15476)]
> **Authors**: Anton Ayzenberg,Thomas Gebhart,German Magai,Grigory Solomadin
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 117 pages, 8 figures
- **标题**: None
- **领域**: 代数拓扑,计算几何,机器学习,K-理论和同源性
- **Abstract**: This paper provides an overview of the applications of sheaf theory in deep learning, data science, and computer science in general. The primary text of this work serves as a friendly introduction to applied and computational sheaf theory accessible to those with modest mathematical familiarity. We describe intuitions and motivations underlying sheaf theory shared by both theoretical researchers and practitioners, bridging classical mathematical theory and its more recent implementations within signal processing and deep learning. We observe that most notions commonly considered specific to cellular sheaves translate to sheaves on arbitrary posets, providing an interesting avenue for further generalization of these methods in applications, and we present a new algorithm to compute sheaf cohomology on arbitrary finite posets in response. By integrating classical theory with recent applications, this work reveals certain blind spots in current machine learning practices. We conclude with a list of problems related to sheaf-theoretic applications that we find mathematically insightful and practically instructive to solve. To ensure the exposition of sheaf theory is self-contained, a rigorous mathematical introduction is provided in appendices which moves from an introduction of diagrams and sheaves to the definition of derived functors, higher order cohomology, sheaf Laplacians, sheaf diffusion, and interconnections of these subjects therein.

## 数值分析(math.NA:Numerical Analysis)

### Flow-based linear embedding for Bayesian filtering of nonlinear stochastic dynamical systems 
[[arxiv](https://arxiv.org/abs/2502.16232)] [[cool](https://papers.cool/arxiv/2502.16232)] [[pdf](https://arxiv.org/pdf/2502.16232)]
> **Authors**: Xintong Wang,Xiaofei Guan,Ling Guo,Hao Wu
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 数值分析,机器学习,机器学习
- **Abstract**: Bayesian filtering for high-dimensional nonlinear stochastic dynamical systems is a fundamental yet challenging problem in many fields of science and engineering. Existing methods face significant obstacles: Gaussian-based filters struggle with non-Gaussian distributions, sequential Monte Carlo methods are computationally intensive and prone to particle degeneracy in high dimensions, and deep learning approaches often fail to balance accuracy and efficiency in complex filtering tasks. To address these challenges, we propose a flow-based Bayesian filter (FBF) that integrates normalizing flows to construct a latent linear state-space model with Gaussian filtering distributions. This framework enables efficient density estimation and sampling through invertible transformations provided by normalizing flows, which can be learned directly from data, thereby eliminating the need for prior knowledge of system dynamics or observation models. Numerical experiments demonstrate the advantages of FBF in terms of both accuracy and efficiency.

## 优化与控制(math.OC:Optimization and Control)

### Learning-Guided Rolling Horizon Optimization for Long-Horizon Flexible Job-Shop Scheduling 
[[arxiv](https://arxiv.org/abs/2502.15791)] [[cool](https://papers.cool/arxiv/2502.15791)] [[pdf](https://arxiv.org/pdf/2502.15791)]
> **Authors**: Sirui Li,Wenbin Ouyang,Yining Ma,Cathy Wu
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 优化与控制,人工智能,机器学习
- **Abstract**: Long-horizon combinatorial optimization problems (COPs), such as the Flexible Job-Shop Scheduling Problem (FJSP), often involve complex, interdependent decisions over extended time frames, posing significant challenges for existing solvers. While Rolling Horizon Optimization (RHO) addresses this by decomposing problems into overlapping shorter-horizon subproblems, such overlap often involves redundant computations. In this paper, we present L-RHO, the first learning-guided RHO framework for COPs. L-RHO employs a neural network to intelligently fix variables that in hindsight did not need to be re-optimized, resulting in smaller and thus easier-to-solve subproblems. For FJSP, this means identifying operations with unchanged machine assignments between consecutive subproblems. Applied to FJSP, L-RHO accelerates RHO by up to 54% while significantly improving solution quality, outperforming other heuristic and learning-based baselines. We also provide in-depth discussions and verify the desirable adaptability and generalization of L-RHO across numerous FJSP variates, distributions, online scenarios and benchmark instances. Moreover, we provide a theoretical analysis to elucidate the conditions under which learning is beneficial.

### A Data-Driven Real-Time Optimal Power Flow Algorithm Using Local Feedback 
[[arxiv](https://arxiv.org/abs/2502.15306)] [[cool](https://papers.cool/arxiv/2502.15306)] [[pdf](https://arxiv.org/pdf/2502.15306)]
> **Authors**: Heng Liang,Yujin Huang,Changhong Zhao
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 优化与控制,机器学习
- **Abstract**: The increasing penetration of distributed energy resources (DERs) adds variability as well as fast control capabilities to power networks. Dispatching the DERs based on local information to provide real-time optimal network operation is the desideratum. In this paper, we propose a data-driven real-time algorithm that uses only the local measurements to solve time-varying AC optimal power flow (OPF). Specifically, we design a learnable function that takes the local feedback as input in the algorithm. The learnable function, under certain conditions, will result in a unique stationary point of the algorithm, which in turn transfers the OPF problems to be optimized over the parameters of the function. We then develop a stochastic primal-dual update to solve the variant of the OPF problems based on a deep neural network (DNN) parametrization of the learnable function, which is referred to as the training stage. We also design a gradient-free alternative to bypass the cumbersome gradient calculation of the nonlinear power flow model. The OPF solution-tracking error bound is established in the sense of universal approximation of DNN. Numerical results on the IEEE 37-bus test feeder show that the proposed method can track the time-varying OPF solutions with higher accuracy and faster computation compared to benchmark methods.

## 统计理论(math.ST:Statistics Theory)

### Monotonicity Testing of High-Dimensional Distributions with Subcube Conditioning 
[[arxiv](https://arxiv.org/abs/2502.16355)] [[cool](https://papers.cool/arxiv/2502.16355)] [[pdf](https://arxiv.org/pdf/2502.16355)]
> **Authors**: Deeparnab Chakrabarty,Xi Chen,Simeon Ristic,C. Seshadhri,Erik Waingarten
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 统计理论,计算复杂度,离散数学,数据结构和算法,机器学习
- **Abstract**: We study monotonicity testing of high-dimensional distributions on $\{-1,1\}^n$ in the model of subcube conditioning, suggested and studied by Canonne, Ron, and Servedio~\cite{CRS15} and Bhattacharyya and Chakraborty~\cite{BC18}. Previous work shows that the \emph{sample complexity} of monotonicity testing must be exponential in $n$ (Rubinfeld, Vasilian~\cite{RV20}, and Aliakbarpour, Gouleakis, Peebles, Rubinfeld, Yodpinyanee~\cite{AGPRY19}). We show that the subcube \emph{query complexity} is $\tildeΘ(n/\varepsilon^2)$, by proving nearly matching upper and lower bounds. Our work is the first to use directed isoperimetric inequalities (developed for function monotonicity testing) for analyzing a distribution testing algorithm. Along the way, we generalize an inequality of Khot, Minzer, and Safra~\cite{KMS18} to real-valued functions on $\{-1,1\}^n$. We also study uniformity testing of distributions that are promised to be monotone, a problem introduced by Rubinfeld, Servedio~\cite{RS09} , using subcube conditioning. We show that the query complexity is $\tildeΘ(\sqrt{n}/\varepsilon^2)$. Our work proves the lower bound, which matches (up to poly-logarithmic factors) the uniformity testing upper bound for general distributions (Canonne, Chen, Kamath, Levi, Waingarten~\cite{CCKLW21}). Hence, we show that monotonicity does not help, beyond logarithmic factors, in testing uniformity of distributions with subcube conditional queries.

### Dimension-free bounds in high-dimensional linear regression via error-in-operator approach 
[[arxiv](https://arxiv.org/abs/2502.15437)] [[cool](https://papers.cool/arxiv/2502.15437)] [[pdf](https://arxiv.org/pdf/2502.15437)]
> **Authors**: Fedor Noskov,Nikita Puchkin,Vladimir Spokoiny
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 100 pages
- **标题**: None
- **领域**: 统计理论,机器学习,方法论,机器学习
- **Abstract**: We consider a problem of high-dimensional linear regression with random design. We suggest a novel approach referred to as error-in-operator which does not estimate the design covariance $Σ$ directly but incorporates it into empirical risk minimization. We provide an expansion of the excess prediction risk and derive non-asymptotic dimension-free bounds on the leading term and the remainder. This helps us to show that auxiliary variables do not increase the effective dimension of the problem, provided that parameters of the procedure are tuned properly. We also discuss computational aspects of our method and illustrate its performance with numerical experiments.

## 大气和海洋物理(physics.ao-ph:Atmospheric and Oceanic Physics)

### AI Models Still Lag Behind Traditional Numerical Models in Predicting Sudden-Turning Typhoons 
[[arxiv](https://arxiv.org/abs/2502.16036)] [[cool](https://papers.cool/arxiv/2502.16036)] [[pdf](https://arxiv.org/pdf/2502.16036)]
> **Authors**: Daosheng Xu,Zebin Lu,Jeremy Cheuk-Hin Leung,Dingchi Zhao,Yi Li,Yang Shi,Bin Chen,Gaozhen Nie,Naigeng Wu,Xiangjun Tian,Yi Yang,Shaoqing Zhang,Banglin Zhang
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 大气和海洋物理,机器学习
- **Abstract**: Given the interpretability, accuracy, and stability of numerical weather prediction (NWP) models, current operational weather forecasting relies heavily on the NWP approach. In the past two years, the rapid development of Artificial Intelligence (AI) has provided an alternative solution for medium-range (1-10 days) weather forecasting. Bi et al. (2023) (hereafter Bi23) introduced the first AI-based weather prediction (AIWP) model in China, named Pangu-Weather, which offers fast prediction without compromising accuracy. In their work, Bi23 made notable claims regarding its effectiveness in extreme weather predictions. However, this claim lacks persuasiveness because the extreme nature of the two tropical cyclones (TCs) examples presented in Bi23, namely Typhoon Kong-rey and Typhoon Yutu, stems primarily from their intensities rather than their moving paths. Their claim may mislead into another meaning which is that Pangu-Weather works well in predicting unusual typhoon paths, which was not explicitly analyzed. Here, we reassess Pangu-Weather's ability to predict extreme TC trajectories from 2020-2024. Results reveal that while Pangu-Weather overall outperforms NWP models in predicting tropical cyclone (TC) tracks, it falls short in accurately predicting the rarely observed sudden-turning tracks, such as Typhoon Khanun in 2023. We argue that current AIWP models still lag behind traditional NWP models in predicting such rare extreme events in medium-range forecasts.

## 计算物理(physics.comp-ph:Computational Physics)

### A new framework for X-ray absorption spectroscopy data analysis based on machine learning: XASDAML 
[[arxiv](https://arxiv.org/abs/2502.16665)] [[cool](https://papers.cool/arxiv/2502.16665)] [[pdf](https://arxiv.org/pdf/2502.16665)]
> **Authors**: Xue Han,Haodong Yao,Fei Zhan,Xueqi Song,Junfang Zhao,Haifeng Zhao
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 计算物理,机器学习
- **Abstract**: X-ray absorption spectroscopy (XAS) is a powerful technique to probe the electronic and structural properties of materials. With the rapid growth in both the volume and complexity of XAS datasets driven by advancements in synchrotron radiation facilities, there is an increasing demand for advanced computational tools capable of efficiently analyzing large-scale data. To address these needs, we introduce XASDAML,a flexible, machine learning based framework that integrates the entire data-processing workflow-including dataset construction for spectra and structural descriptors, data filtering, ML modeling, prediction, and model evaluation-into a unified platform. Additionally, it supports comprehensive statistical analysis, leveraging methods such as principal component analysis and clustering to reveal potential patterns and relationships within large datasets. Each module operates independently, allowing users to modify or upgrade modules in response to evolving research needs or technological advances. Moreover, the platform provides a user-friendly interface via Jupyter Notebook, making it accessible to researchers at varying levels of expertise. The versatility and effectiveness of XASDAML are exemplified by its application to a copper dataset, where it efficiently manages large and complex data, supports both supervised and unsupervised machine learning models, provides comprehensive statistics for structural descriptors, generates spectral plots, and accurately predicts coordination numbers and bond lengths. Furthermore, the platform streamlining the integration of XAS with machine learning and lowering the barriers to entry for new users.

## 流体动力学(physics.flu-dyn:Fluid Dynamics)

### Update hydrological states or meteorological forcings? Comparing data assimilation methods for differentiable hydrologic models 
[[arxiv](https://arxiv.org/abs/2502.16444)] [[cool](https://papers.cool/arxiv/2502.16444)] [[pdf](https://arxiv.org/pdf/2502.16444)]
> **Authors**: Amirmoez Jamaat,Yalan Song,Farshid Rahmani,Jiangtao Liu,Kathryn Lawson,Chaopeng Shen
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 流体动力学,机器学习,大气和海洋物理
- **Abstract**: Data assimilation (DA) enables hydrologic models to update their internal states using near-real-time observations for more accurate forecasts. With deep neural networks like long short-term memory (LSTM), using either lagged observations as inputs (called "data integration") or variational DA has shown success in improving forecasts. However, it is unclear which methods are performant or optimal for physics-informed machine learning ("differentiable") models, which represent only a small amount of physically-meaningful states while using deep networks to supply parameters or missing processes. Here we developed variational DA methods for differentiable models, including optimizing adjusters for just precipitation data, just model internal hydrological states, or both. Our results demonstrated that differentiable streamflow models using the CAMELS dataset can benefit strongly and equivalently from variational DA as LSTM, with one-day lead time median Nash-Sutcliffe efficiency (NSE) elevated from 0.75 to 0.82. The resulting forecast matched or outperformed LSTM with DA in the eastern, northwestern, and central Great Plains regions of the conterminous United States. Both precipitation and state adjusters were needed to achieve these results, with the latter being substantially more effective on its own, and the former adding moderate benefits for high flows. Our DA framework does not need systematic training data and could serve as a practical DA scheme for whole river networks.

## 生物分子(q-bio.BM:Biomolecules)

### Auto-ADMET: An Effective and Interpretable AutoML Method for Chemical ADMET Property Prediction 
[[arxiv](https://arxiv.org/abs/2502.16378)] [[cool](https://papers.cool/arxiv/2502.16378)] [[pdf](https://arxiv.org/pdf/2502.16378)]
> **Authors**: Alex G. C. de Sá,David B. Ascher
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 生物分子,人工智能,机器学习
- **Abstract**: Machine learning (ML) has been playing important roles in drug discovery in the past years by providing (pre-)screening tools for prioritising chemical compounds to pass through wet lab experiments. One of the main ML tasks in drug discovery is to build quantitative structure-activity relationship (QSAR) models, associating the molecular structure of chemical compounds with an activity or property. These properties -- including absorption, distribution, metabolism, excretion and toxicity (ADMET) -- are essential to model compound behaviour, activity and interactions in the organism. Although several methods exist, the majority of them do not provide an appropriate model's personalisation, yielding to bias and lack of generalisation to new data since the chemical space usually shifts from application to application. This fact leads to low predictive performance when completely new data is being tested by the model. The area of Automated Machine Learning (AutoML) emerged aiming to solve this issue, outputting tailored ML algorithms to the data at hand. Although an important task, AutoML has not been practically used to assist cheminformatics and computational chemistry researchers often, with just a few works related to the field. To address these challenges, this work introduces Auto-ADMET, an interpretable evolutionary-based AutoML method for chemical ADMET property prediction. Auto-ADMET employs a Grammar-based Genetic Programming (GGP) method with a Bayesian Network Model to achieve comparable or better predictive performance against three alternative methods -- standard GGP method, pkCSM and XGBOOST model -- on 12 benchmark chemical ADMET property prediction datasets. The use of a Bayesian Network model on Auto-ADMET's evolutionary process assisted in both shaping the search procedure and interpreting the causes of its AutoML performance.

## 神经元和认知(q-bio.NC:Neurons and Cognition)

### Lattice-Based Pruning in Recurrent Neural Networks via Poset Modeling 
[[arxiv](https://arxiv.org/abs/2502.16525)] [[cool](https://papers.cool/arxiv/2502.16525)] [[pdf](https://arxiv.org/pdf/2502.16525)]
> **Authors**: Rakesh Sengupta
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: 9 pages,1 figure, submitted to IEEE Transactions onNeuralNetworks and Rehabilitation Systems
- **标题**: None
- **领域**: 神经元和认知,机器学习
- **Abstract**: Recurrent neural networks (RNNs) are central to sequence modeling tasks, yet their high computational complexity poses challenges for scalability and real-time deployment. Traditional pruning techniques, predominantly based on weight magnitudes, often overlook the intrinsic structural properties of these networks. We introduce a novel framework that models RNNs as partially ordered sets (posets) and constructs corresponding dependency lattices. By identifying meet irreducible neurons, our lattice-based pruning algorithm selectively retains critical connections while eliminating redundant ones. The method is implemented using both binary and continuous-valued adjacency matrices to capture different aspects of network connectivity. Evaluated on the MNIST dataset, our approach exhibits a clear trade-off between sparsity and classification accuracy. Moderate pruning maintains accuracy above 98%, while aggressive pruning achieves higher sparsity with only a modest performance decline. Unlike conventional magnitude-based pruning, our method leverages the structural organization of RNNs, resulting in more effective preservation of functional connectivity and improved efficiency in multilayer networks with top-down feedback. The proposed lattice-based pruning framework offers a rigorous and scalable approach for reducing RNN complexity while sustaining robust performance, paving the way for more efficient hierarchical models in both machine learning and computational neuroscience.

### Category-Selective Neurons in Deep Networks: Comparing Purely Visual and Visual-Language Models 
[[arxiv](https://arxiv.org/abs/2502.16456)] [[cool](https://papers.cool/arxiv/2502.16456)] [[pdf](https://arxiv.org/pdf/2502.16456)]
> **Authors**: Zitong Lu,Yuxin Wang
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 神经元和认知,计算机视觉和模式识别
- **Abstract**: Category-selective regions in the human brain, such as the fusiform face area (FFA), extrastriate body area (EBA), parahippocampal place area (PPA), and visual word form area (VWFA), play a crucial role in high-level visual processing. Here, we investigate whether artificial neural networks (ANNs) exhibit similar category-selective neurons and how these neurons vary across model layers and between purely visual and vision-language models. Inspired by fMRI functional localizer experiments, we presented images from different categories (faces, bodies, scenes, words, scrambled scenes, and scrambled words) to deep networks and identified category-selective neurons using statistical criteria. Comparing ResNet and the structurally controlled ResNet-based CLIP model, we found that both models contain category-selective neurons, with their proportion increasing across layers, mirroring category selectivity in higher-level visual brain regions. However, CLIP exhibited a higher proportion but lower specificity of category-selective neurons compared to ResNet. Additionally, CLIP's category-selective neurons were more evenly distributed across feature maps and demonstrated greater representational consistency across layers. These findings suggest that language learning increases the number of category-selective neurons while reducing their selectivity strength, reshaping visual representations in deep networks. Our study provides insights into how ANNs mirror biological vision and how multimodal learning influences category-selective representations.

### Brain-Model Evaluations Need the NeuroAI Turing Test 
[[arxiv](https://arxiv.org/abs/2502.16238)] [[cool](https://papers.cool/arxiv/2502.16238)] [[pdf](https://arxiv.org/pdf/2502.16238)]
> **Authors**: Jenelle Feather,Meenakshi Khosla,N. Apurva Ratan Murty,Aran Nayebi
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: 9 pages, 4 figures, 2 tables
- **标题**: None
- **领域**: 神经元和认知,人工智能,机器学习,神经和进化计算
- **Abstract**: What makes an artificial system a good model of intelligence? The classical test proposed by Alan Turing focuses on behavior, requiring that an artificial agent's behavior be indistinguishable from that of a human. While behavioral similarity provides a strong starting point, two systems with very different internal representations can produce the same outputs. Thus, in modeling biological intelligence, the field of NeuroAI often aims to go beyond behavioral similarity and achieve representational convergence between a model's activations and the measured activity of a biological system. This position paper argues that the standard definition of the Turing Test is incomplete for NeuroAI, and proposes a stronger framework called the ``NeuroAI Turing Test'', a benchmark that extends beyond behavior alone and \emph{additionally} requires models to produce internal neural representations that are empirically indistinguishable from those of a brain up to measured individual variability, i.e. the differences between a computational model and the brain is no more than the difference between one brain and another brain. While the brain is not necessarily the ceiling of intelligence, it remains the only universally agreed-upon example, making it a natural reference point for evaluating computational models. By proposing this framework, we aim to shift the discourse from loosely defined notions of brain inspiration to a systematic and testable standard centered on both behavior and internal representations, providing a clear benchmark for neuroscientific modeling and AI development.

### MindLLM: A Subject-Agnostic and Versatile Model for fMRI-to-Text Decoding 
[[arxiv](https://arxiv.org/abs/2502.15786)] [[cool](https://papers.cool/arxiv/2502.15786)] [[pdf](https://arxiv.org/pdf/2502.15786)]
> **Authors**: Weikang Qiu,Zheng Huang,Haoyu Hu,Aosong Feng,Yujun Yan,Rex Ying
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-24
> **comment**: 17 pages, 9 figures
- **标题**: None
- **领域**: 神经元和认知,人工智能,机器学习,信号处理
- **Abstract**: Decoding functional magnetic resonance imaging (fMRI) signals into text has been a key challenge in the neuroscience community, with the potential to advance brain-computer interfaces and uncover deeper insights into brain mechanisms. However, existing approaches often struggle with suboptimal predictive performance, limited task variety, and poor generalization across subjects. In response to this, we propose MindLLM, a model designed for subject-agnostic and versatile fMRI-to-text decoding. MindLLM consists of an fMRI encoder and an off-the-shelf LLM. The fMRI encoder employs a neuroscience-informed attention mechanism, which is capable of accommodating subjects with varying input shapes and thus achieves high-performance subject-agnostic decoding. Moreover, we introduce Brain Instruction Tuning (BIT), a novel approach that enhances the model's ability to capture diverse semantic representations from fMRI signals, facilitating more versatile decoding. We evaluate MindLLM on comprehensive fMRI-to-text benchmarks. Results demonstrate that our model outperforms the baselines, improving downstream tasks by 12.0%, unseen subject generalization by 16.4%, and novel task adaptation by 25.0%. Furthermore, the attention patterns in MindLLM provide interpretable insights into its decision-making process.

### Sparks of cognitive flexibility: self-guided context inference for flexible stimulus-response mapping by attentional routing 
[[arxiv](https://arxiv.org/abs/2502.15634)] [[cool](https://papers.cool/arxiv/2502.15634)] [[pdf](https://arxiv.org/pdf/2502.15634)]
> **Authors**: Rowan P. Sommers,Sushrut Thorat,Daniel Anthes,Tim C. Kietzmann
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 11 pages, 4 figures; edited author info, abstract, and Fig. 1B
- **标题**: None
- **领域**: 神经元和认知,机器学习
- **Abstract**: Flexible cognition demands discovering hidden rules to quickly adapt stimulus-response mappings. Standard neural networks struggle in such tasks requiring rapid, context-driven remapping. Recently, Hummos (2023) introduced a fast-and-slow learning algorithm to mitigate this shortcoming, but its scalability to complex, image-computable tasks was unclear. Here, we propose the Wisconsin Neural Network (WiNN), which extends Hummos' fast-and-slow learning to image-computable tasks demanding flexible rule-based behavior. WiNN employs a pretrained convolutional neural network for vision, coupled with an adjustable "context state" that guides attention to relevant features. If WiNN produces an incorrect response, it first iteratively updates its context state to refocus attention on task-relevant cues, then performs minimal parameter updates to attention and readout layers. This strategy preserves generalizable representations in the sensory and attention networks, reducing catastrophic forgetting. We evaluate WiNN on an image-based extension of the Wisconsin Card Sorting Task, revealing several markers of cognitive flexibility: (i) WiNN autonomously infers underlying rules, (ii) requires fewer examples to do so than control models reliant on large-scale parameter updates, (iii) can perform context-based rule inference solely via context-state adjustments-further enhanced by slow updates of attention and readout parameters, and (iv) generalizes to unseen compositional rules through context-state updates alone. By blending fast context inference with targeted attentional guidance, WiNN achieves "sparks" of flexibility. This approach offers a path toward context-sensitive models that retain knowledge while rapidly adapting to complex, rule-based tasks.

### BAN: Neuroanatomical Aligning in Auditory Recognition between Artificial Neural Network and Human Cortex 
[[arxiv](https://arxiv.org/abs/2502.15503)] [[cool](https://papers.cool/arxiv/2502.15503)] [[pdf](https://arxiv.org/pdf/2502.15503)]
> **Authors**: Haidong Wang,Pengfei Xiao,Ao Liu,Jianhua Zhang,Qia Shan
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 神经元和认知,人工智能
- **Abstract**: Drawing inspiration from neurosciences, artificial neural networks (ANNs) have evolved from shallow architectures to highly complex, deep structures, yielding exceptional performance in auditory recognition tasks. However, traditional ANNs often struggle to align with brain regions due to their excessive depth and lack of biologically realistic features, like recurrent connection. To address this, a brain-like auditory network (BAN) is introduced, which incorporates four neuroanatomically mapped areas and recurrent connection, guided by a novel metric called the brain-like auditory score (BAS). BAS serves as a benchmark for evaluating the similarity between BAN and human auditory recognition pathway. We further propose that specific areas in the cerebral cortex, mainly the middle and medial superior temporal (T2/T3) areas, correspond to the designed network structure, drawing parallels with the brain's auditory perception pathway. Our findings suggest that the neuroanatomical similarity in the cortex and auditory classification abilities of the ANN are well-aligned. In addition to delivering excellent performance on a music genre classification task, the BAN demonstrates a high BAS score. In conclusion, this study presents BAN as a recurrent, brain-inspired ANN, representing the first model that mirrors the cortical pathway of auditory recognition.

## 其他定量生物学(q-bio.OT:Other Quantitative Biology)

### Strategic priorities for transformative progress in advancing biology with proteomics and artificial intelligence 
[[arxiv](https://arxiv.org/abs/2502.15867)] [[cool](https://papers.cool/arxiv/2502.15867)] [[pdf](https://arxiv.org/pdf/2502.15867)]
> **Authors**: Yingying Sun,Jun A,Zhiwei Liu,Rui Sun,Liujia Qian,Samuel H. Payne,Wout Bittremieux,Markus Ralser,Chen Li,Yi Chen,Zhen Dong,Yasset Perez-Riverol,Asif Khan,Chris Sander,Ruedi Aebersold,Juan Antonio Vizcaíno,Jonathan R Krieger,Jianhua Yao,Han Wen,Linfeng Zhang,Yunping Zhu,Yue Xuan,Benjamin Boyang Sun,Liang Qiao,Henning Hermjakob, et al. (37 additional authors not shown)
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 28 pages, 2 figures, perspective inAIproteomics
- **标题**: None
- **领域**: 其他定量生物学,人工智能
- **Abstract**: Artificial intelligence (AI) is transforming scientific research, including proteomics. Advances in mass spectrometry (MS)-based proteomics data quality, diversity, and scale, combined with groundbreaking AI techniques, are unlocking new challenges and opportunities in biological discovery. Here, we highlight key areas where AI is driving innovation, from data analysis to new biological insights. These include developing an AI-friendly ecosystem for proteomics data generation, sharing, and analysis; improving peptide and protein identification and quantification; characterizing protein-protein interactions and protein complexes; advancing spatial and perturbation proteomics; integrating multi-omics data; and ultimately enabling AI-empowered virtual cells.

## 定量方法(q-bio.QM:Quantitative Methods)

### Non-Linear Flow Matching for Full-Atom Peptide Design 
[[arxiv](https://arxiv.org/abs/2502.15855)] [[cool](https://papers.cool/arxiv/2502.15855)] [[pdf](https://arxiv.org/pdf/2502.15855)]
> **Authors**: Dengdeng Huang,Shikui Tu
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 定量方法,人工智能,机器学习
- **Abstract**: Peptide design plays a pivotal role in therapeutic applications, yet existing AI-assisted methods often struggle to generate stable peptides with high affinity due to their inability to accurately simulate the dynamic docking process. To address this challenge, we propose NLFlow, a novel multi-manifold approach based on non-linear flow matching. Specifically, we design a polynomial-based conditional vector field to accelerate the convergence of the peptide's position towards the target pocket, effectively capturing the temporal inconsistencies across position, rotation, torsion, and amino acid type manifolds. This enables the model to better align with the true conformational changes observed in biological docking processes. Additionally, we incorporate interaction-related information, such as polarity, to enhance the understanding of peptide-protein binding. Extensive experiments demonstrate that NLFlow outperforms existing methods in generating peptides with superior stability, affinity, and diversity, offering a fast and efficient solution for peptide design and advancing the peptide-based therapeutic development.

### Breast Lump Detection and Localization with a Tactile Glove Using Deep Learning 
[[arxiv](https://arxiv.org/abs/2502.15767)] [[cool](https://papers.cool/arxiv/2502.15767)] [[pdf](https://arxiv.org/pdf/2502.15767)]
> **Authors**: Togzhan Syrymova,Amir Yelenov,Karina Burunchina,Nazgul Abulkhanova,Huseyin Atakan Varol,Juan Antonio Corrales Ramon,Zhanat Kappassov
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 定量方法,计算机视觉和模式识别,图像和视频处理
- **Abstract**: Breast cancer is the leading cause of mortality among women. Inspection of breasts by palpation is the key to early detection. We aim to create a wearable tactile glove that could localize the lump in breasts using deep learning (DL). In this work, we present our flexible fabric-based and soft wearable tactile glove for detecting the lumps within custom-made silicone breast prototypes (SBPs). SBPs are made of soft silicone that imitates the human skin and the inner part of the breast. Ball-shaped silicone tumors of 1.5-, 1.75- and 2.0-cm diameters are embedded inside to create another set with lumps. Our approach is based on the InceptionTime DL architecture with transfer learning between experienced and non-experienced users. We collected a dataset from 10 naive participants and one oncologist-mammologist palpating SBPs. We demonstrated that the DL model can classify lump presence, size and location with an accuracy of 82.22%, 67.08% and 62.63%, respectively. In addition, we showed that the model adapted to unseen experienced users with an accuracy of 95.01%, 88.54% and 82.98% for lump presence, size and location classification, respectively. This technology can assist inexperienced users or healthcare providers, thus facilitating more frequent routine checks.

### Drug-Target Interaction/Affinity Prediction: Deep Learning Models and Advances Review 
[[arxiv](https://arxiv.org/abs/2502.15346)] [[cool](https://papers.cool/arxiv/2502.15346)] [[pdf](https://arxiv.org/pdf/2502.15346)]
> **Authors**: Ali Vefghi,Zahed Rahmati,Mohammad Akbari
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 64 pages, 7 figures, 10 tables
- **标题**: None
- **领域**: 定量方法,机器学习
- **Abstract**: Drug discovery remains a slow and expensive process that involves many steps, from detecting the target structure to obtaining approval from the Food and Drug Administration (FDA), and is often riddled with safety concerns. Accurate prediction of how drugs interact with their targets and the development of new drugs by using better methods and technologies have immense potential to speed up this process, ultimately leading to faster delivery of life-saving medications. Traditional methods used for drug-target interaction prediction show limitations, particularly in capturing complex relationships between drugs and their targets. As an outcome, deep learning models have been presented to overcome the challenges of interaction prediction through their precise and efficient end results. By outlining promising research avenues and models, each with a different solution but similar to the problem, this paper aims to give researchers a better idea of methods for even more accurate and efficient prediction of drug-target interaction, ultimately accelerating the development of more effective drugs. A total of 180 prediction methods for drug-target interactions were analyzed throughout the period spanning 2016 to 2025 using different frameworks based on machine learning, mainly deep learning and graph neural networks. Additionally, this paper discusses the novelty, architecture, and input representation of these models.

### Utilizing Sequential Information of General Lab-test Results and Diagnoses History for Differential Diagnosis of Dementia 
[[arxiv](https://arxiv.org/abs/2502.15317)] [[cool](https://papers.cool/arxiv/2502.15317)] [[pdf](https://arxiv.org/pdf/2502.15317)]
> **Authors**: Yizong Xing,Dhita Putri Pratama,Yuke Wang,Yufan Zhang,Brian E. Chapman
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 7 pages, 6 figures. This work has been submitted to the IEEE for possible publication
- **标题**: None
- **领域**: 定量方法,机器学习
- **Abstract**: Early diagnosis of Alzheimer's Disease (AD) faces multiple data-related challenges, including high variability in patient data, limited access to specialized diagnostic tests, and overreliance on single-type indicators. These challenges are exacerbated by the progressive nature of AD, where subtle pathophysiological changes often precede clinical symptoms by decades. To address these limitations, this study proposes a novel approach that takes advantage of routinely collected general laboratory test histories for the early detection and differential diagnosis of AD. By modeling lab test sequences as "sentences", we apply word embedding techniques to capture latent relationships between tests and employ deep time series models, including long-short-term memory (LSTM) and Transformer networks, to model temporal patterns in patient records. Experimental results demonstrate that our approach improves diagnostic accuracy and enables scalable and costeffective AD screening in diverse clinical settings.

## 一般财务(q-fin.GN:General Finance)

### Position: Standard Benchmarks Fail -- LLM Agents Present Overlooked Risks for Financial Applications 
[[arxiv](https://arxiv.org/abs/2502.15865)] [[cool](https://papers.cool/arxiv/2502.15865)] [[pdf](https://arxiv.org/pdf/2502.15865)]
> **Authors**: Zichen Chen,Jiaao Chen,Jianda Chen,Misha Sra
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 40 pages, 2 figures, 2 tables
- **标题**: None
- **领域**: 一般财务,人工智能,计算语言学
- **Abstract**: Current financial LLM agent benchmarks are inadequate. They prioritize task performance while ignoring fundamental safety risks. Threats like hallucinations, temporal misalignment, and adversarial vulnerabilities pose systemic risks in high-stakes financial environments, yet existing evaluation frameworks fail to capture these risks. We take a firm position: traditional benchmarks are insufficient to ensure the reliability of LLM agents in finance. To address this, we analyze existing financial LLM agent benchmarks, finding safety gaps and introducing ten risk-aware evaluation metrics. Through an empirical evaluation of both API-based and open-weight LLM agents, we reveal hidden vulnerabilities that remain undetected by conventional assessments. To move the field forward, we propose the Safety-Aware Evaluation Agent (SAEA), grounded in a three-level evaluation framework that assesses agents at the model level (intrinsic capabilities), workflow level (multi-step process reliability), and system level (integration robustness). Our findings highlight the urgent need to redefine LLM agent evaluation standards by shifting the focus from raw performance to safety, robustness, and real world resilience.

## 风险管理(q-fin.RM:Risk Management)

### Bankruptcy analysis using images and convolutional neural networks (CNN) 
[[arxiv](https://arxiv.org/abs/2502.15726)] [[cool](https://papers.cool/arxiv/2502.15726)] [[pdf](https://arxiv.org/pdf/2502.15726)]
> **Authors**: Luiz Tavares,Jose Mazzon,Francisco Paletta,Fabio Barros
> **First submission**: 2025-01-29
> **First announcement**: 2025-02-24
> **comment**: 23 pages, 7 tables, 1 Figure
- **标题**: None
- **领域**: 风险管理,机器学习,统计理论,统计金融
- **Abstract**: The marketing departments of financial institutions strive to craft products and services that cater to the diverse needs of businesses of all sizes. However, it is evident upon analysis that larger corporations often receive a more substantial portion of available funds. This disparity arises from the relative ease of assessing the risk of default and bankruptcy in these more prominent companies. Historically, risk analysis studies have focused on data from publicly traded or stock exchange-listed companies, leaving a gap in knowledge about small and medium-sized enterprises (SMEs). Addressing this gap, this study introduces a method for evaluating SMEs by generating images for processing via a convolutional neural network (CNN). To this end, more than 10,000 images, one for each company in the sample, were created to identify scenarios in which the CNN can operate with higher assertiveness and reduced training error probability. The findings demonstrate a significant predictive capacity, achieving 97.8% accuracy, when a substantial number of images are utilized. Moreover, the image creation method paves the way for potential applications of this technique in various sectors and for different analytical purposes.

## 统计金融(q-fin.ST:Statistical Finance)

### Contrastive Similarity Learning for Market Forecasting: The ContraSim Framework 
[[arxiv](https://arxiv.org/abs/2502.16023)] [[cool](https://papers.cool/arxiv/2502.16023)] [[pdf](https://arxiv.org/pdf/2502.16023)]
> **Authors**: Nicholas Vinden,Raeid Saqur,Zining Zhu,Frank Rudzicz
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 8 pages, 3 appendices
- **标题**: None
- **领域**: 统计金融,机器学习
- **Abstract**: We introduce the Contrastive Similarity Space Embedding Algorithm (ContraSim), a novel framework for uncovering the global semantic relationships between daily financial headlines and market movements. ContraSim operates in two key stages: (I) Weighted Headline Augmentation, which generates augmented financial headlines along with a semantic fine-grained similarity score, and (II) Weighted Self-Supervised Contrastive Learning (WSSCL), an extended version of classical self-supervised contrastive learning that uses the similarity metric to create a refined weighted embedding space. This embedding space clusters semantically similar headlines together, facilitating deeper market insights. Empirical results demonstrate that integrating ContraSim features into financial forecasting tasks improves classification accuracy from WSJ headlines by 7%. Moreover, leveraging an information density analysis, we find that the similarity spaces constructed by ContraSim intrinsically cluster days with homogeneous market movement directions, indicating that ContraSim captures market dynamics independent of ground truth labels. Additionally, ContraSim enables the identification of historical news days that closely resemble the headlines of the current day, providing analysts with actionable insights to predict market trends by referencing analogous past events.

### Multi-Agent Stock Prediction Systems: Machine Learning Models, Simulations, and Real-Time Trading Strategies 
[[arxiv](https://arxiv.org/abs/2502.15853)] [[cool](https://papers.cool/arxiv/2502.15853)] [[pdf](https://arxiv.org/pdf/2502.15853)]
> **Authors**: Daksh Dave,Gauransh Sawhney,Vikhyat Chauhan
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 统计金融,机器学习
- **Abstract**: This paper presents a comprehensive study on stock price prediction, leveragingadvanced machine learning (ML) and deep learning (DL) techniques to improve financial forecasting accuracy. The research evaluates the performance of various recurrent neural network (RNN) architectures, including Long Short-Term Memory (LSTM) networks, Gated Recurrent Units (GRU), and attention-based models. These models are assessed for their ability to capture complex temporal dependencies inherent in stock market data. Our findings show that attention-based models outperform other architectures, achieving the highest accuracy by capturing both short and long-term dependencies. This study contributes valuable insights into AI-driven financial forecasting, offering practical guidance for developing more accurate and efficient trading systems.

### Financial fraud detection system based on improved random forest and gradient boosting machine (GBM) 
[[arxiv](https://arxiv.org/abs/2502.15822)] [[cool](https://papers.cool/arxiv/2502.15822)] [[pdf](https://arxiv.org/pdf/2502.15822)]
> **Authors**: Tianzuo Hu
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 统计金融,机器学习,一般财务,应用领域,机器学习
- **Abstract**: This paper proposes a financial fraud detection system based on improved Random Forest (RF) and Gradient Boosting Machine (GBM). Specifically, the system introduces a novel model architecture called GBM-SSRF (Gradient Boosting Machine with Simplified and Strengthened Random Forest), which cleverly combines the powerful optimization capabilities of the gradient boosting machine (GBM) with improved randomization. The computational efficiency and feature extraction capabilities of the Simplified and Strengthened Random Forest (SSRF) forest significantly improve the performance of financial fraud detection. Although the traditional random forest model has good classification capabilities, it has high computational complexity when faced with large-scale data and has certain limitations in feature selection. As a commonly used ensemble learning method, the GBM model has significant advantages in optimizing performance and handling nonlinear problems. However, GBM takes a long time to train and is prone to overfitting problems when data samples are unbalanced. In response to these limitations, this paper optimizes the random forest based on the structure, reducing the computational complexity and improving the feature selection ability through the structural simplification and enhancement of the random forest. In addition, the optimized random forest is embedded into the GBM framework, and the model can maintain efficiency and stability with the help of GBM's gradient optimization capability. Experiments show that the GBM-SSRF model not only has good performance, but also has good robustness and generalization capabilities, providing an efficient and reliable solution for financial fraud detection.

### Stock Price Prediction Using a Hybrid LSTM-GNN Model: Integrating Time-Series and Graph-Based Analysis 
[[arxiv](https://arxiv.org/abs/2502.15813)] [[cool](https://papers.cool/arxiv/2502.15813)] [[pdf](https://arxiv.org/pdf/2502.15813)]
> **Authors**: Meet Satishbhai Sonani,Atta Badii,Armin Moin
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 统计金融,人工智能,机器学习
- **Abstract**: This paper presents a novel hybrid model that integrates long-short-term memory (LSTM) networks and Graph Neural Networks (GNNs) to significantly enhance the accuracy of stock market predictions. The LSTM component adeptly captures temporal patterns in stock price data, effectively modeling the time series dynamics of financial markets. Concurrently, the GNN component leverages Pearson correlation and association analysis to model inter-stock relational data, capturing complex nonlinear polyadic dependencies influencing stock prices. The model is trained and evaluated using an expanding window validation approach, enabling continuous learning from increasing amounts of data and adaptation to evolving market conditions. Extensive experiments conducted on historical stock data demonstrate that our hybrid LSTM-GNN model achieves a mean square error (MSE) of 0.00144, representing a substantial reduction of 10.6% compared to the MSE of the standalone LSTM model of 0.00161. Furthermore, the hybrid model outperforms traditional and advanced benchmarks, including linear regression, convolutional neural networks (CNN), and dense networks. These compelling results underscore the significant potential of combining temporal and relational data through a hybrid approach, offering a powerful tool for real-time trading and financial analysis.

### TLOB: A Novel Transformer Model with Dual Attention for Stock Price Trend Prediction with Limit Order Book Data 
[[arxiv](https://arxiv.org/abs/2502.15757)] [[cool](https://papers.cool/arxiv/2502.15757)] [[pdf](https://arxiv.org/pdf/2502.15757)]
> **Authors**: Leonardo Berti,Gjergji Kasneci
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 统计金融,人工智能,机器学习,交易和市场微观结构
- **Abstract**: Stock Price Trend Prediction (SPTP) based on Limit Order Book (LOB) data is a fundamental challenge in financial markets. Despite advances in deep learning, existing models fail to generalize across different market conditions and struggle to reliably predict short-term trends. Surprisingly, by adapting a simple MLP-based architecture to LOB, we show that we surpass SoTA performance; thus, challenging the necessity of complex architectures. Unlike past work that shows robustness issues, we propose TLOB, a transformer-based model that uses a dual attention mechanism to capture spatial and temporal dependencies in LOB data. This allows it to adaptively focus on the market microstructure, making it particularly effective for longer-horizon predictions and volatile market conditions. We also introduce a new labeling method that improves on previous ones, removing the horizon bias. We evaluate TLOB's effectiveness using the established FI-2010 benchmark, which exceeds the state-of-the-art by an average of 3.7 F1-score(\%). Additionally, TLOB shows improvements on Tesla and Intel with a 1.3 and 7.7 increase in F1-score(\%), respectively. Additionally, we empirically show how stock price predictability has declined over time (-6.68 absolute points in F1-score(\%)), highlighting the growing market efficiencies. Predictability must be considered in relation to transaction costs, so we experimented with defining trends using an average spread, reflecting the primary transaction cost. The resulting performance deterioration underscores the complexity of translating trend classification into profitable trading strategies. We argue that our work provides new insights into the evolving landscape of stock price trend prediction and sets a strong foundation for future advancements in financial AI. We release the code at https://github.com/LeonardoBerti00/TLOB.

## 量子物理学(quant-ph:Quantum Physics)

### Quantum autoencoders for image classification 
[[arxiv](https://arxiv.org/abs/2502.15254)] [[cool](https://papers.cool/arxiv/2502.15254)] [[pdf](https://arxiv.org/pdf/2502.15254)]
> **Authors**: Hinako Asaoka,Kazue Kudo
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 量子物理学,计算机视觉和模式识别
- **Abstract**: Classical machine learning often struggles with complex, high-dimensional data. Quantum machine learning offers a potential solution, promising more efficient processing. While the quantum convolutional neural network (QCNN), a hybrid quantum-classical algorithm, is suitable for current noisy intermediate-scale quantum-era hardware, its learning process relies heavily on classical computation. Future large-scale, gate-based quantum computers could unlock the full potential of quantum effects in machine learning. In contrast to QCNNs, quantum autoencoders (QAEs) leverage classical optimization solely for parameter tuning. Data compression and reconstruction are handled entirely within quantum circuits, enabling purely quantum-based feature extraction. This study introduces a novel image-classification approach using QAEs, achieving classification without requiring additional qubits compared with conventional QAE implementations. The quantum circuit structure significantly impacts classification accuracy. Unlike hybrid methods such as QCNN, QAE-based classification emphasizes quantum computation. Our experiments demonstrate high accuracy in a four-class classification task, evaluating various quantum-gate configurations to understand the impact of different parameterized quantum circuit (ansatz) structures on classification performance. Our results reveal that specific ansatz structures achieve superior accuracy, and we provide an analysis of their effectiveness. Moreover, the proposed approach achieves performance comparable to that of conventional machine-learning methods while significantly reducing the number of parameters requiring optimization. These findings indicate that QAEs can serve as efficient classification models with fewer parameters and highlight the potential of utilizing quantum circuits for complete end-to-end learning, a departure from hybrid approaches such as QCNN.

## 应用领域(stat.AP:Applications)

### Rashomon perspective for measuring uncertainty in the survival predictive maintenance models 
[[arxiv](https://arxiv.org/abs/2502.15772)] [[cool](https://papers.cool/arxiv/2502.15772)] [[pdf](https://arxiv.org/pdf/2502.15772)]
> **Authors**: Yigitcan Yardimci,Mustafa Cavus
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-24
> **comment**: 4 pages, 1 figures
- **标题**: None
- **领域**: 应用领域,机器学习
- **Abstract**: The prediction of the Remaining Useful Life of aircraft engines is a critical area in high-reliability sectors such as aerospace and defense. Early failure predictions help ensure operational continuity, reduce maintenance costs, and prevent unexpected failures. Traditional regression models struggle with censored data, which can lead to biased predictions. Survival models, on the other hand, effectively handle censored data, improving predictive accuracy in maintenance processes. This paper introduces a novel approach based on the Rashomon perspective, which considers multiple models that achieve similar performance rather than relying on a single best model. This enables uncertainty quantification in survival probability predictions and enhances decision-making in predictive maintenance. The Rashomon survival curve was introduced to represent the range of survival probability estimates, providing insights into model agreement and uncertainty over time. The results on the CMAPSS dataset demonstrate that relying solely on a single model for RUL estimation may increase risk in some scenarios. The censoring levels significantly impact prediction uncertainty, with longer censoring times leading to greater variability in survival probabilities. These findings underscore the importance of incorporating model multiplicity in predictive maintenance frameworks to achieve more reliable and robust failure predictions. This paper contributes to uncertainty quantification in RUL prediction and highlights the Rashomon perspective as a powerful tool for predictive modeling.

## 方法论(stat.ME:Methodology)

### A non-parametric optimal design algorithm for population pharmacokinetics 
[[arxiv](https://arxiv.org/abs/2502.15848)] [[cool](https://papers.cool/arxiv/2502.15848)] [[pdf](https://arxiv.org/pdf/2502.15848)]
> **Authors**: Markus Hovd,Alona Kryshchenko,Michael N. Neely,Julian Otalvaro,Alan Schumitzky,Walter M. Yamada
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 方法论,机器学习,定量方法,应用领域,机器学习
- **Abstract**: This paper introduces a non-parametric estimation algorithm designed to effectively estimate the joint distribution of model parameters with application to population pharmacokinetics. Our research group has previously developed the non-parametric adaptive grid (NPAG) algorithm, which while accurate, explores parameter space using an ad-hoc method to suggest new support points. In contrast, the non-parametric optimal design (NPOD) algorithm uses a gradient approach to suggest new support points, which reduces the amount of time spent evaluating non-relevant points and by this the overall number of cycles required to reach convergence. In this paper, we demonstrate that the NPOD algorithm achieves similar solutions to NPAG across two datasets, while being significantly more efficient in both the number of cycles required and overall runtime. Given the importance of developing robust and efficient algorithms for determining drug doses quickly in pharmacokinetics, the NPOD algorithm represents a valuable advancement in non-parametric modeling. Further analysis is needed to determine which algorithm performs better under specific conditions.

## 机器学习(stat.ML:Machine Learning)

### Finite-Sample Analysis of Policy Evaluation for Robust Average Reward Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.16816)] [[cool](https://papers.cool/arxiv/2502.16816)] [[pdf](https://arxiv.org/pdf/2502.16816)]
> **Authors**: Yang Xu,Washim Uddin Mondal,Vaneet Aggarwal
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: We present the first finite-sample analysis for policy evaluation in robust average-reward Markov Decision Processes (MDPs). Prior works in this setting have established only asymptotic convergence guarantees, leaving open the question of sample complexity. In this work, we address this gap by establishing that the robust Bellman operator is a contraction under the span semi-norm, and developing a stochastic approximation framework with controlled bias. Our approach builds upon Multi-Level Monte Carlo (MLMC) techniques to estimate the robust Bellman operator efficiently. To overcome the infinite expected sample complexity inherent in standard MLMC, we introduce a truncation mechanism based on a geometric distribution, ensuring a finite constant sample complexity while maintaining a small bias that decays exponentially with the truncation level. Our method achieves the order-optimal sample complexity of $\tilde{\mathcal{O}}(ε^{-2})$ for robust policy evaluation and robust average reward estimation, marking a significant advancement in robust reinforcement learning theory.

### Transformations of predictions and realizations in consistent scoring functions 
[[arxiv](https://arxiv.org/abs/2502.16542)] [[cool](https://papers.cool/arxiv/2502.16542)] [[pdf](https://arxiv.org/pdf/2502.16542)]
> **Authors**: Hristos Tyralis,Georgia Papacharalampous
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-24
> **comment**: 28 pages
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Scoring functions constructed by transforming the realization and prediction variables of (strictly) consistent scoring functions have been widely studied empirically, yet their theoretical foundations remain unexplored. To address this gap, we establish formal characterizations of (strict) consistency for these transformed scoring functions and their elicitable functionals. Our analysis focuses on two interrelated cases: (a) transformations applied exclusively to the realization variable, and (b) bijective transformations applied jointly to both realization and prediction variables. We formulate analogous characterizations for (strict) identification functions. The resulting theoretical framework is broadly applicable to statistical and machine learning methodologies. When applied to Bregman and expectile scoring functions, our framework shows how it enables two critical advances: (a) rigorous interpretation of prior empirical findings from models trained with transformed scoring functions, and (b) systematic construction of novel identifiable and elicitable functionals, specifically the g-transformed expectation and g-transformed expectile. By unifying theoretical insights with practical applications, this work advances principled methodologies for designing scoring functions in complex predictive tasks.

### Subspace Recovery in Winsorized PCA: Insights into Accuracy and Robustness 
[[arxiv](https://arxiv.org/abs/2502.16391)] [[cool](https://papers.cool/arxiv/2502.16391)] [[pdf](https://arxiv.org/pdf/2502.16391)]
> **Authors**: Sangil Han,Kyoowon Kim,Sungkyu Jung
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习,应用领域,方法论
- **Abstract**: In this paper, we explore the theoretical properties of subspace recovery using Winsorized Principal Component Analysis (WPCA), utilizing a common data transformation technique that caps extreme values to mitigate the impact of outliers. Despite the widespread use of winsorization in various tasks of multivariate analysis, its theoretical properties, particularly for subspace recovery, have received limited attention. We provide a detailed analysis of the accuracy of WPCA, showing that increasing the number of samples while decreasing the proportion of outliers guarantees the consistency of the sample subspaces from WPCA with respect to the true population subspace. Furthermore, we establish perturbation bounds that ensure the WPCA subspace obtained from contaminated data remains close to the subspace recovered from pure data. Additionally, we extend the classical notion of breakdown points to subspace-valued statistics and derive lower bounds for the breakdown points of WPCA. Our analysis demonstrates that WPCA exhibits strong robustness to outliers while maintaining consistency under mild assumptions. A toy example is provided to numerically illustrate the behavior of the upper bounds for perturbation bounds and breakdown points, emphasizing winsorization's utility in subspace recovery.

### Rectifying Conformity Scores for Better Conditional Coverage 
[[arxiv](https://arxiv.org/abs/2502.16336)] [[cool](https://papers.cool/arxiv/2502.16336)] [[pdf](https://arxiv.org/pdf/2502.16336)]
> **Authors**: Vincent Plassier,Alexander Fishkov,Victor Dheur,Mohsen Guizani,Souhaib Ben Taieb,Maxim Panov,Eric Moulines
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习,统计理论
- **Abstract**: We present a new method for generating confidence sets within the split conformal prediction framework. Our method performs a trainable transformation of any given conformity score to improve conditional coverage while ensuring exact marginal coverage. The transformation is based on an estimate of the conditional quantile of conformity scores. The resulting method is particularly beneficial for constructing adaptive confidence sets in multi-output problems where standard conformal quantile regression approaches have limited applicability. We develop a theoretical bound that captures the influence of the accuracy of the quantile estimate on the approximate conditional validity, unlike classical bounds for conformal prediction methods that only offer marginal coverage. We experimentally show that our method is highly adaptive to the local data structure and outperforms existing methods in terms of conditional coverage, improving the reliability of statistical inference in various applications.

### Statistical Inference in Reinforcement Learning: A Selective Survey 
[[arxiv](https://arxiv.org/abs/2502.16195)] [[cool](https://papers.cool/arxiv/2502.16195)] [[pdf](https://arxiv.org/pdf/2502.16195)]
> **Authors**: Chengchun Shi
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Reinforcement learning (RL) is concerned with how intelligence agents take actions in a given environment to maximize the cumulative reward they receive. In healthcare, applying RL algorithms could assist patients in improving their health status. In ride-sharing platforms, applying RL algorithms could increase drivers' income and customer satisfaction. Over the past decade, RL has been arguably one of the most vibrant research frontiers in machine learning. Nevertheless, statistics as a field, as opposed to computer science, has only recently begun to engage with RL both in depth and in breadth. This paper present a selective review of statistical inferential tools for RL, covering both hypothesis testing and confidence interval construction. Our goal is to highlight the value of statistical inference in RL for both the statistics and machine learning communities, and to promote the broader application of classical statistical inference tools in this vibrant area of research.

### A Review of Causal Decision Making 
[[arxiv](https://arxiv.org/abs/2502.16156)] [[cool](https://papers.cool/arxiv/2502.16156)] [[pdf](https://arxiv.org/pdf/2502.16156)]
> **Authors**: Lin Ge,Hengrui Cai,Runzhe Wan,Yang Xu,Rui Song
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: To make effective decisions, it is important to have a thorough understanding of the causal relationships among actions, environments, and outcomes. This review aims to surface three crucial aspects of decision-making through a causal lens: 1) the discovery of causal relationships through causal structure learning, 2) understanding the impacts of these relationships through causal effect learning, and 3) applying the knowledge gained from the first two aspects to support decision making via causal policy learning. Moreover, we identify challenges that hinder the broader utilization of causal decision-making and discuss recent advances in overcoming these challenges. Finally, we provide future research directions to address these challenges and to further enhance the implementation of causal decision-making in practice, with real-world applications illustrated based on the proposed causal decision-making. We aim to offer a comprehensive methodology and practical implementation framework by consolidating various methods in this area into a Python-based collection. URL: https://causaldm.github.io/Causal-Decision-Making.

### Exact Recovery of Sparse Binary Vectors from Generalized Linear Measurements 
[[arxiv](https://arxiv.org/abs/2502.16008)] [[cool](https://papers.cool/arxiv/2502.16008)] [[pdf](https://arxiv.org/pdf/2502.16008)]
> **Authors**: Arya Mazumdar,Neha Sangwan
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,信息论,机器学习
- **Abstract**: We consider the problem of exact recovery of a $k$-sparse binary vector from generalized linear measurements (such as logistic regression). We analyze the linear estimation algorithm (Plan, Vershynin, Yudovina, 2017), and also show information theoretic lower bounds on the number of required measurements. As a consequence of our results, for noisy one bit quantized linear measurements ($\mathsf{1bCSbinary}$), we obtain a sample complexity of $O((k+σ^2)\log{n})$, where $σ^2$ is the noise variance. This is shown to be optimal due to the information theoretic lower bound. We also obtain tight sample complexity characterization for logistic regression. Since $\mathsf{1bCSbinary}$ is a strictly harder problem than noisy linear measurements ($\mathsf{SparseLinearReg}$) because of added quantization, the same sample complexity is achievable for $\mathsf{SparseLinearReg}$. While this sample complexity can be obtained via the popular lasso algorithm, linear estimation is computationally more efficient. Our lower bound holds for any set of measurements for $\mathsf{SparseLinearReg}$, (similar bound was known for Gaussian measurement matrices) and is closely matched by the maximum-likelihood upper bound. For $\mathsf{SparseLinearReg}$, it was conjectured in Gamarnik and Zadik, 2017 that there is a statistical-computational gap and the number of measurements should be at least $(2k+σ^2)\log{n}$ for efficient algorithms to exist. It is worth noting that our results imply that there is no such statistical-computational gap for $\mathsf{1bCSbinary}$ and logistic regression.

### Feature maps for the Laplacian kernel and its generalizations 
[[arxiv](https://arxiv.org/abs/2502.15575)] [[cool](https://papers.cool/arxiv/2502.15575)] [[pdf](https://arxiv.org/pdf/2502.15575)]
> **Authors**: Sudhendu Ahir,Parthe Pandit
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Recent applications of kernel methods in machine learning have seen a renewed interest in the Laplacian kernel, due to its stability to the bandwidth hyperparameter in comparison to the Gaussian kernel, as well as its expressivity being equivalent to that of the neural tangent kernel of deep fully connected networks. However, unlike the Gaussian kernel, the Laplacian kernel is not separable. This poses challenges for techniques to approximate it, especially via the random Fourier features (RFF) methodology and its variants. In this work, we provide random features for the Laplacian kernel and its two generalizations: Matérn kernel and the Exponential power kernel. We provide efficiently implementable schemes to sample weight matrices so that random features approximate these kernels. These weight matrices have a weakly coupled heavy-tailed randomness. Via numerical experiments on real datasets we demonstrate the efficacy of these random feature maps.

### Generalization Guarantees for Representation Learning via Data-Dependent Gaussian Mixture Priors 
[[arxiv](https://arxiv.org/abs/2502.15540)] [[cool](https://papers.cool/arxiv/2502.15540)] [[pdf](https://arxiv.org/pdf/2502.15540)]
> **Authors**: Milad Sefidgaran,Abdellatif Zaidi,Piotr Krasnowski
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: Accepted as a Spotlight Paper at ICLR 2025
- **标题**: None
- **领域**: 机器学习,信息论,机器学习
- **Abstract**: We establish in-expectation and tail bounds on the generalization error of representation learning type algorithms. The bounds are in terms of the relative entropy between the distribution of the representations extracted from the training and "test'' datasets and a data-dependent symmetric prior, i.e., the Minimum Description Length (MDL) of the latent variables for the training and test datasets. Our bounds are shown to reflect the "structure" and "simplicity'' of the encoder and significantly improve upon the few existing ones for the studied model. We then use our in-expectation bound to devise a suitable data-dependent regularizer; and we investigate thoroughly the important question of the selection of the prior. We propose a systematic approach to simultaneously learning a data-dependent Gaussian mixture prior and using it as a regularizer. Interestingly, we show that a weighted attention mechanism emerges naturally in this procedure. Our experiments show that our approach outperforms the now popular Variational Information Bottleneck (VIB) method as well as the recent Category-Dependent VIB (CDVIB).

### Fréchet Cumulative Covariance Net for Deep Nonlinear Sufficient Dimension Reduction with Random Objects 
[[arxiv](https://arxiv.org/abs/2502.15374)] [[cool](https://papers.cool/arxiv/2502.15374)] [[pdf](https://arxiv.org/pdf/2502.15374)]
> **Authors**: Hang Yuan,Christina Dan Wang,Zhou Yu
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Nonlinear sufficient dimension reduction\citep{libing_generalSDR}, which constructs nonlinear low-dimensional representations to summarize essential features of high-dimensional data, is an important branch of representation learning. However, most existing methods are not applicable when the response variables are complex non-Euclidean random objects, which are frequently encountered in many recent statistical applications. In this paper, we introduce a new statistical dependence measure termed Fréchet Cumulative Covariance (FCCov) and develop a novel nonlinear SDR framework based on FCCov. Our approach is not only applicable to complex non-Euclidean data, but also exhibits robustness against outliers. We further incorporate Feedforward Neural Networks (FNNs) and Convolutional Neural Networks (CNNs) to estimate nonlinear sufficient directions in the sample level. Theoretically, we prove that our method with squared Frobenius norm regularization achieves unbiasedness at the $σ$-field level. Furthermore, we establish non-asymptotic convergence rates for our estimators based on FNNs and ResNet-type CNNs, which match the minimax rate of nonparametric regression up to logarithmic factors. Intensive simulation studies verify the performance of our methods in both Euclidean and non-Euclidean settings. We apply our method to facial expression recognition datasets and the results underscore more realistic and broader applicability of our proposal.

### Tensor Product Neural Networks for Functional ANOVA Model 
[[arxiv](https://arxiv.org/abs/2502.15215)] [[cool](https://papers.cool/arxiv/2502.15215)] [[pdf](https://arxiv.org/pdf/2502.15215)]
> **Authors**: Seokhun Park,Insung Kong,Yongchan Choi,Chanmoo Park,Yongdai Kim
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-24
> **comment**: 45 pages
- **标题**: None
- **领域**: 机器学习,机器学习,统计理论
- **Abstract**: Interpretability for machine learning models is becoming more and more important as machine learning models become more complex. The functional ANOVA model, which decomposes a high-dimensional function into a sum of lower dimensional functions so called components, is one of the most popular tools for interpretable AI, and recently, various neural network models have been developed for estimating each component in the functional ANOVA model. However, such neural networks are highly unstable when estimating components since the components themselves are not uniquely defined. That is, there are multiple functional ANOVA decompositions for a given function. In this paper, we propose a novel interpretable model which guarantees a unique functional ANOVA decomposition and thus is able to estimate each component stably. We call our proposed model ANOVA-NODE since it is a modification of Neural Oblivious Decision Ensembles (NODE) for the functional ANOVA model. Theoretically, we prove that ANOVA-NODE can approximate a smooth function well. Additionally, we experimentally show that ANOVA-NODE provides much more stable estimation of each component and thus much more stable interpretation when training data and initial values of the model parameters vary than existing neural network models do.

## 其他论文

- [Detecting Code Vulnerabilities with Heterogeneous GNN Training](https://arxiv.org/abs/2502.16835)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [An Exploratory Study on How AI Awareness Impacts Human-AI Design Collaboration](https://arxiv.org/abs/2502.16833)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [A Review of Memory Wall for Neuromorphic Computing](https://arxiv.org/abs/2502.16823)
  - **标题**: None
  - **Filtered Reason**: none of cs.AR in whitelist
- [A new solution for cooperative game with public externalities: Analysis based on axiomatic method](https://arxiv.org/abs/2502.16800)
  - **标题**: None
  - **Filtered Reason**: none of econ.TH,cs.GT in whitelist
- [The Blessing of Reasoning: LLM-Based Contrastive Explanations in Black-Box Recommender Systems](https://arxiv.org/abs/2502.16759)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [Automated Keypoint Estimation for Self-Piercing Rivet Joints Using micro-CT Imaging and Transfer Learning](https://arxiv.org/abs/2502.16752)
  - **标题**: None
  - **Filtered Reason**: none of cs.CE in whitelist
- [Guardians of the Agentic System: Preventing Many Shots Jailbreak with Agentic System](https://arxiv.org/abs/2502.16750)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [I am not thinking anymore, just following the path.: Investigating Task Delegation Trend of Author-AI Co-Creation with Generative AIs](https://arxiv.org/abs/2502.16740)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [ViSNeRF: Efficient Multidimensional Neural Radiance Field Representation for Visualization Synthesis of Dynamic Volumetric Scenes](https://arxiv.org/abs/2502.16731)
  - **标题**: None
  - **Filtered Reason**: none of cs.GR in whitelist
- [Mapping out AI Functions in Intelligent Disaster (Mis)Management and AI-Caused Disasters](https://arxiv.org/abs/2502.16644)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [CRIUgpu: Transparent Checkpointing of GPU-Accelerated Workloads](https://arxiv.org/abs/2502.16631)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC in whitelist
- [OPAQUE: Obfuscating Phase in Quantum Circuit Compilation for Efficient IP Protection](https://arxiv.org/abs/2502.16605)
  - **标题**: None
  - **Filtered Reason**: none of quant-ph,cs.CR in whitelist
- [Can Indirect Prompt Injection Attacks Be Detected and Removed?](https://arxiv.org/abs/2502.16580)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [OpenVox: Real-time Instance-level Open-vocabulary Probabilistic Voxel Representation](https://arxiv.org/abs/2502.16528)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [Annotation-guided AoS-to-SoA conversions and GPU offloading with data views in C++](https://arxiv.org/abs/2502.16517)
  - **标题**: None
  - **Filtered Reason**: none of cs.PL,cs.PF,cs.MS in whitelist
- [Path Planning using Instruction-Guided Probabilistic Roadmaps](https://arxiv.org/abs/2502.16515)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [Unified Semantic and ID Representation Learning for Deep Recommenders](https://arxiv.org/abs/2502.16474)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [TerEffic: Highly Efficient Ternary LLM Inference on FPGA](https://arxiv.org/abs/2502.16473)
  - **标题**: None
  - **Filtered Reason**: none of cs.AR in whitelist
- [A Contemporary Survey on Semantic Communications:Theory of Mind, Generative AI, and Deep Joint Source-Channel Coding](https://arxiv.org/abs/2502.16468)
  - **标题**: None
  - **Filtered Reason**: none of cs.IT in whitelist
- [M4SC: An MLLM-based Multi-modal, Multi-task and Multi-user Semantic Communication System](https://arxiv.org/abs/2502.16418)
  - **标题**: None
  - **Filtered Reason**: none of cs.IT in whitelist
- [Quadruped Robot Simulation Using Deep Reinforcement Learning -- A step towards locomotion policy](https://arxiv.org/abs/2502.16401)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [Understanding Generative AI Risks for Youth: A Taxonomy Based on Empirical Data](https://arxiv.org/abs/2502.16383)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Walkthrough of Anthropomorphic Features in AI Assistant Tools](https://arxiv.org/abs/2502.16345)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Revolutionizing Datacenter Networks via Reconfigurable Topologies](https://arxiv.org/abs/2502.16228)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC,cs.NI in whitelist
- [LLMKey: LLM-Powered Wireless Key Generation Scheme for Next-Gen IoV Systems](https://arxiv.org/abs/2502.16199)
  - **标题**: None
  - **Filtered Reason**: none of cs.NI in whitelist
- [TutorUp: What If Your Students Were Simulated? Training Tutors to Address Engagement Challenges in Online Learning](https://arxiv.org/abs/2502.16178)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Understanding Screenwriters' Practices, Attitudes, and Future Expectations in Human-AI Co-Creation](https://arxiv.org/abs/2502.16153)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Tag-Pag: A Dedicated Tool for Systematic Web Page Annotations](https://arxiv.org/abs/2502.16150)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [Motion-Coupled Mapping Algorithm for Hybrid Rice Canopy](https://arxiv.org/abs/2502.16134)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [A Trust-Aware and Cost-Optimized Blockchain Oracle Selection Model with Deep Reinforcement Learning](https://arxiv.org/abs/2502.16133)
  - **标题**: None
  - **Filtered Reason**: none of cs.CE,cs.ET in whitelist
- [Beyond Visual Perception: Insights from Smartphone Interaction of Visually Impaired Users with Large Multimodal Models](https://arxiv.org/abs/2502.16098)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Merger-as-a-Stealer: Stealing Targeted PII from Aligned LLMs with Model Merging](https://arxiv.org/abs/2502.16094)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [Stealing Training Data from Large Language Models in Decentralized Training through Activation Inversion Attack](https://arxiv.org/abs/2502.16086)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [How to explain it to data scientists? A mixed-methods user study about explainable AI, using mental models for explanations](https://arxiv.org/abs/2502.16083)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Infinite Horizon Markov Economies](https://arxiv.org/abs/2502.16080)
  - **标题**: None
  - **Filtered Reason**: none of cs.GT in whitelist
- [Improving Deep Assertion Generation via Fine-Tuning Retrieval-Augmented Pre-trained Language Models](https://arxiv.org/abs/2502.16071)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Creative Blends of Visual Concepts](https://arxiv.org/abs/2502.16062)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC,cs.GR in whitelist
- [African Data Ethics: A Discursive Framework for Black Decolonial Data Science](https://arxiv.org/abs/2502.16043)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [Development of a Multi-Fingered Soft Gripper Digital Twin for Machine Learning-based Underactuated Control](https://arxiv.org/abs/2502.15994)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [Accountability in Code Review: The Role of Intrinsic Drivers and the Impact of LLMs](https://arxiv.org/abs/2502.15963)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- ["Kya family planning after marriage hoti hai?": Integrating Cultural Sensitivity in an LLM Chatbot for Reproductive Health](https://arxiv.org/abs/2502.15939)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC,cs.CY in whitelist
- [Computation Offloading Strategies in Integrated Terrestrial and Non-Terrestrial Networks](https://arxiv.org/abs/2502.15903)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC,eess.SP in whitelist
- [GenAI at the Edge: Comprehensive Survey on Empowering Edge Devices](https://arxiv.org/abs/2502.15816)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC in whitelist
- [Serverless Edge Computing: A Taxonomy, Systematic Literature Review, Current Trends and Research Challenges](https://arxiv.org/abs/2502.15775)
  - **标题**: None
  - **Filtered Reason**: none of cs.ET,cs.NI in whitelist
- [Open-Source Retrieval Augmented Generation Framework for Retrieving Accurate Medication Insights from Formularies for African Healthcare Workers](https://arxiv.org/abs/2502.15722)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC,cs.IR in whitelist
- [Evaluating the Efficacy of Next.js: A Comparative Analysis with React.js on Performance, SEO, and Global Network Equity](https://arxiv.org/abs/2502.15707)
  - **标题**: None
  - **Filtered Reason**: none of cs.PF,cs.NI in whitelist
- [Multi-Failure Localization in High-Degree ROADM-based Optical Networks using Rules-Informed Neural Networks](https://arxiv.org/abs/2502.15706)
  - **标题**: None
  - **Filtered Reason**: none of eess.SP,cs.NI in whitelist
- [Disentangling Popularity and Quality: An Edge Classification Approach for Fair Recommendation](https://arxiv.org/abs/2502.15699)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [Developing an Artificial Intelligence Tool for Personalized Breast Cancer Treatment Plans based on the NCCN Guidelines](https://arxiv.org/abs/2502.15698)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [MemoryPods: Enhancing Asynchronous Communication in Extended Reality](https://arxiv.org/abs/2502.15622)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Cross-Format Retrieval-Augmented Generation in XR with LLMs for Context-Aware Maintenance Assistance](https://arxiv.org/abs/2502.15604)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC,cs.IR in whitelist
- [A Comprehensive Survey of Linear, Integer, and Mixed-Integer Programming Approaches for Optimizing Resource Allocation in 5G and Beyond Networks](https://arxiv.org/abs/2502.15585)
  - **标题**: None
  - **Filtered Reason**: none of cs.NI in whitelist
- [Scaling Sparse and Dense Retrieval in Decoder-Only LLMs](https://arxiv.org/abs/2502.15526)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [Towards Swift Serverless LLM Cold Starts with ParaServe](https://arxiv.org/abs/2502.15524)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC in whitelist
- [Contract Design Under Approximate Best Responses](https://arxiv.org/abs/2502.15523)
  - **标题**: None
  - **Filtered Reason**: none of cs.GT in whitelist
- [Construction and Evaluation of LLM-based agents for Semi-Autonomous penetration testing](https://arxiv.org/abs/2502.15506)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [Jeffrey's update rule as a minimizer of Kullback-Leibler divergence](https://arxiv.org/abs/2502.15504)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR,stat.ML in whitelist
- [Programmers Aren't Obsolete Yet: A Syllabus for Teaching CS Students to Responsibly Use Large Language Models for Code Generation](https://arxiv.org/abs/2502.15493)
  - **标题**: None
  - **Filtered Reason**: none of cs.ET,cs.SE in whitelist
- [A modular risk concept for complex systems](https://arxiv.org/abs/2502.15482)
  - **标题**: None
  - **Filtered Reason**: none of cs.LO in whitelist
- [FaultGPT: Industrial Fault Diagnosis Question Answering System by Vision Language Models](https://arxiv.org/abs/2502.15481)
  - **标题**: None
  - **Filtered Reason**: none of cs.ET,eess.SP in whitelist
- [On the Effectiveness of Large Language Models in Writing Alloy Formulas](https://arxiv.org/abs/2502.15441)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Beyond Tools: Understanding How Heavy Users Integrate LLMs into Everyday Tasks and Decision-Making](https://arxiv.org/abs/2502.15395)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Advancing User-Voice Interaction: Exploring Emotion-Aware Voice Assistants Through a Role-Swapping Approach](https://arxiv.org/abs/2502.15367)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC,eess.AS,cs.SD in whitelist
- [A Universal Framework for Compressing Embeddings in CTR Prediction](https://arxiv.org/abs/2502.15355)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [FlexPie: Accelerate Distributed Inference on Edge Devices with Flexible Combinatorial Optimization[Technical Report]](https://arxiv.org/abs/2502.15312)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC in whitelist
- [DynamicGSG: Dynamic 3D Gaussian Scene Graphs for Environment Adaptation](https://arxiv.org/abs/2502.15309)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [Bridging Bug Localization and Issue Fixing: A Hierarchical Localization Framework Leveraging Large Language Models](https://arxiv.org/abs/2502.15292)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [BundleFlow: Deep Menus for Combinatorial Auctions by Diffusion-Based Optimization](https://arxiv.org/abs/2502.15283)
  - **标题**: None
  - **Filtered Reason**: none of cs.GT in whitelist
- [An approach for API synthesis using large language models](https://arxiv.org/abs/2502.15246)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [From Documents to Dialogue: Building KG-RAG Enhanced AI Assistants](https://arxiv.org/abs/2502.15237)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [User Experience with LLM-powered Conversational Recommendation Systems: A Case of Music Recommendation](https://arxiv.org/abs/2502.15229)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Peripheral Teleportation: A Rest Frame Design to Mitigate Cybersickness During Virtual Locomotion](https://arxiv.org/abs/2502.15227)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC,cs.GR in whitelist
