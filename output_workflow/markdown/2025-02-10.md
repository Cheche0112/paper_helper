> 本文由 [https://github.com/huiyeruzhou/arxiv_crawler](https://github.com/huiyeruzhou/arxiv_crawler) 自动生成
>
> 领域白名单：cs.AI,cs.CL,cs.LG,cs.CV
> 关键词： LLM, GPT, AI, language+model, deep+learning, transformer, neural+network, machine+learning

# 论文全览：2025-02-10

共有497篇相关领域论文, 另有81篇其他

## 太阳和恒星天体物理学(astro-ph.SR:Solar and Stellar Astrophysics)

### Deep Generative model that uses physical quantities to generate and retrieve solar magnetic active regions 
[[arxiv](https://arxiv.org/abs/2502.05351)] [[cool](https://papers.cool/arxiv/2502.05351)] [[pdf](https://arxiv.org/pdf/2502.05351)]
> **Authors**: Subhamoy Chatterjee,Andres Munoz-Jaramillo,Anna Malanushenko
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 9 pages, 6 figures
- **标题**: None
- **领域**: 太阳和恒星天体物理学,机器学习,机器学习
- **Abstract**: Deep generative models have shown immense potential in generating unseen data that has properties of real data. These models learn complex data-generating distributions starting from a smaller set of latent dimensions. However, generative models have encountered great skepticism in scientific domains due to the disconnection between generative latent vectors and scientifically relevant quantities. In this study, we integrate three types of machine learning models to generate solar magnetic patches in a physically interpretable manner and use those as a query to find matching patches in real observations. We use the magnetic field measurements from Space-weather HMI Active Region Patches (SHARPs) to train a Generative Adversarial Network (GAN). We connect the physical properties of GAN-generated images with their latent vectors to train Support Vector Machines (SVMs) that do mapping between physical and latent spaces. These produce directions in the GAN latent space along which known physical parameters of the SHARPs change. We train a self-supervised learner (SSL) to make queries with generated images and find matches from real data. We find that the GAN-SVM combination enables users to produce high-quality patches that change smoothly only with a prescribed physical quantity, making generative models physically interpretable. We also show that GAN outputs can be used to retrieve real data that shares the same physical properties as the generated query. This elevates Generative Artificial Intelligence (AI) from a means-to-produce artificial data to a novel tool for scientific data interrogation, supporting its applicability beyond the domain of heliophysics.

## 无序系统和神经网络(cond-mat.dis-nn:Disordered Systems and Neural Networks)

### Two-Point Deterministic Equivalence for Stochastic Gradient Dynamics in Linear Models 
[[arxiv](https://arxiv.org/abs/2502.05074)] [[cool](https://papers.cool/arxiv/2502.05074)] [[pdf](https://arxiv.org/pdf/2502.05074)]
> **Authors**: Alexander Atanasov,Blake Bordelon,Jacob A. Zavatone-Veth,Courtney Paquette,Cengiz Pehlevan
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 无序系统和神经网络,机器学习,机器学习
- **Abstract**: We derive a novel deterministic equivalence for the two-point function of a random matrix resolvent. Using this result, we give a unified derivation of the performance of a wide variety of high-dimensional linear models trained with stochastic gradient descent. This includes high-dimensional linear regression, kernel regression, and random feature models. Our results include previously known asymptotics as well as novel ones.

## 强关联电子(cond-mat.str-el:Strongly Correlated Electrons)

### Is attention all you need to solve the correlated electron problem? 
[[arxiv](https://arxiv.org/abs/2502.05383)] [[cool](https://papers.cool/arxiv/2502.05383)] [[pdf](https://arxiv.org/pdf/2502.05383)]
> **Authors**: Max Geier,Khachatur Nazaryan,Timothy Zaklama,Liang Fu
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 10+5 pages, comments welcome
- **标题**: None
- **领域**: 强关联电子,介观和纳米物理,人工智能
- **Abstract**: The attention mechanism has transformed artificial intelligence research by its ability to learn relations between objects. In this work, we explore how a many-body wavefunction ansatz constructed from a large-parameter self-attention neural network can be used to solve the interacting electron problem in solids. By a systematic neural-network variational Monte Carlo study on a moiré quantum material, we demonstrate that the self-attention ansatz provides an accurate, efficient, and unbiased solution. Moreover, our numerical study finds that the required number of variational parameters scales roughly as $N^2$ with the number of electrons, which opens a path towards efficient large-scale simulations.

## 人工智能(cs.AI:Artificial Intelligence)

### The Value of Information in Human-AI Decision-making 
[[arxiv](https://arxiv.org/abs/2502.06152)] [[cool](https://papers.cool/arxiv/2502.06152)] [[pdf](https://arxiv.org/pdf/2502.06152)]
> **Authors**: Ziyang Guo,Yifan Wu,Jason Hartline,Jessica Hullman
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: Humans and AIs are often paired on decision tasks with the expectation of achieving complementary performance, where the combination of human and AI outperforms either one alone. However, how to improve performance of a human-AI team is often not clear without knowing more about what particular information and strategies each agent employs. We provide a decision-theoretic framework for characterizing the value of information -- and consequently, opportunities for agents to better exploit available information -- in AI-assisted decision workflow. We demonstrate the use of the framework for model selection, empirical evaluation of human-AI performance, and explanation design. We propose a novel information-based instance-level explanation technique that adapts a conventional saliency-based explanation to explain information value in decision making.

### Training Language Models for Social Deduction with Multi-Agent Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.06060)] [[cool](https://papers.cool/arxiv/2502.06060)] [[pdf](https://arxiv.org/pdf/2502.06060)]
> **Authors**: Bidipta Sarkar,Warren Xia,C. Karen Liu,Dorsa Sadigh
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: 14 pages, 5 figures, 24th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2025)
- **标题**: None
- **领域**: 人工智能,计算语言学,机器学习,多代理系统
- **Abstract**: Communicating in natural language is a powerful tool in multi-agent settings, as it enables independent agents to share information in partially observable settings and allows zero-shot coordination with humans. However, most prior works are limited as they either rely on training with large amounts of human demonstrations or lack the ability to generate natural and useful communication strategies. In this work, we train language models to have productive discussions about their environment in natural language without any human demonstrations. We decompose the communication problem into listening and speaking. Our key idea is to leverage the agent's goal to predict useful information about the world as a dense reward signal that guides communication. Specifically, we improve a model's listening skills by training them to predict information about the environment based on discussions, and we simultaneously improve a model's speaking skills with multi-agent reinforcement learning by rewarding messages based on their influence on other agents. To investigate the role and necessity of communication in complex social settings, we study an embodied social deduction game based on Among Us, where the key question to answer is the identity of an adversarial imposter. We analyze emergent behaviors due to our technique, such as accusing suspects and providing evidence, and find that it enables strong discussions, doubling the win rates compared to standard RL. We release our code and models at https://socialdeductionllm.github.io/

### AutoAgent: A Fully-Automated and Zero-Code Framework for LLM Agents 
[[arxiv](https://arxiv.org/abs/2502.05957)] [[cool](https://papers.cool/arxiv/2502.05957)] [[pdf](https://arxiv.org/pdf/2502.05957)]
> **Authors**: Jiabin Tang,Tianyu Fan,Chao Huang
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: Code: https://github.com/HKUDS/AutoAgent
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: Large Language Model (LLM) Agents have demonstrated remarkable capabilities in task automation and intelligent decision-making, driving the widespread adoption of agent development frameworks such as LangChain and AutoGen. However, these frameworks predominantly serve developers with extensive technical expertise - a significant limitation considering that only 0.03 % of the global population possesses the necessary programming skills. This stark accessibility gap raises a fundamental question: Can we enable everyone, regardless of technical background, to build their own LLM agents using natural language alone? To address this challenge, we introduce AutoAgent-a Fully-Automated and highly Self-Developing framework that enables users to create and deploy LLM agents through Natural Language Alone. Operating as an autonomous Agent Operating System, AutoAgent comprises four key components: i) Agentic System Utilities, ii) LLM-powered Actionable Engine, iii) Self-Managing File System, and iv) Self-Play Agent Customization module. This lightweight yet powerful system enables efficient and dynamic creation and modification of tools, agents, and workflows without coding requirements or manual intervention. Beyond its code-free agent development capabilities, AutoAgent also serves as a versatile multi-agent system for General AI Assistants. Comprehensive evaluations on the GAIA benchmark demonstrate AutoAgent's effectiveness in generalist multi-agent tasks, surpassing existing state-of-the-art methods. Furthermore, AutoAgent's Retrieval-Augmented Generation (RAG)-related capabilities have shown consistently superior performance compared to many alternative LLM-based solutions.

### Barriers and Pathways to Human-AI Alignment: A Game-Theoretic Approach 
[[arxiv](https://arxiv.org/abs/2502.05934)] [[cool](https://papers.cool/arxiv/2502.05934)] [[pdf](https://arxiv.org/pdf/2502.05934)]
> **Authors**: Aran Nayebi
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: 32 pages, including 5 main theorems and 10 lemmas
- **标题**: None
- **领域**: 人工智能,计算复杂度,计算机科学与博弈论,机器学习,多代理系统
- **Abstract**: Under what conditions can capable AI agents efficiently align their actions with human preferences? More specifically, when they are proficient enough to collaborate with us, how long does coordination take, and when is it computationally feasible? These foundational questions of AI alignment help define what makes an AI agent ``sufficiently safe'' and valuable to humans. Since such generally capable systems do not yet exist, a theoretical analysis is needed to establish when guarantees hold -- and what they even are. We introduce a game-theoretic framework that generalizes prior alignment approaches with fewer assumptions, allowing us to analyze the computational complexity of alignment across $M$ objectives and $N$ agents, providing both upper and lower bounds. Unlike previous work, which often assumes common priors, idealized communication, or implicit tractability, our framework formally characterizes the difficulty of alignment under minimal assumptions. Our main result shows that even when agents are fully rational and computationally \emph{unbounded}, alignment can be achieved with high probability in time \emph{linear} in the task space size. Therefore, in real-world settings, where task spaces are often \emph{exponential} in input length, this remains impractical. More strikingly, our lower bound demonstrates that alignment is \emph{impossible} to speed up when scaling to exponentially many tasks or agents, highlighting a fundamental computational barrier to scalable alignment. Relaxing these idealized assumptions, we study \emph{computationally bounded} agents with noisy messages (representing obfuscated intent), showing that while alignment can still succeed with high probability, it incurs additional \emph{exponential} slowdowns in the task space size, number of agents, and number of tasks. We conclude by identifying conditions that make alignment more feasible.

### Amorphous Fortress Online: Collaboratively Designing Open-Ended Multi-Agent AI and Game Environments 
[[arxiv](https://arxiv.org/abs/2502.05632)] [[cool](https://papers.cool/arxiv/2502.05632)] [[pdf](https://arxiv.org/pdf/2502.05632)]
> **Authors**: M Charity,Mayu Wilson,Steven Lee,Dipika Rajesh,Sam Earle,Julian Togelius
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: This work introduces Amorphous Fortress Online -- a web-based platform where users can design petri-dish-like environments and games consisting of multi-agent AI characters. Users can play, create, and share artificial life and game environments made up of microscopic but transparent finite-state machine agents that interact with each other. The website features multiple interactive editors and accessible settings to view the multi-agent interactions directly from the browser. This system serves to provide a database of thematically diverse AI and game environments that use the emergent behaviors of simple AI agents.

### Closing the Responsibility Gap in AI-based Network Management: An Intelligent Audit System Approach 
[[arxiv](https://arxiv.org/abs/2502.05608)] [[cool](https://papers.cool/arxiv/2502.05608)] [[pdf](https://arxiv.org/pdf/2502.05608)]
> **Authors**: Emanuel Figetakis,Ahmed Refaey Hussein
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,网络和互联网架构
- **Abstract**: Existing network paradigms have achieved lower downtime as well as a higher Quality of Experience (QoE) through the use of Artificial Intelligence (AI)-based network management tools. These AI management systems, allow for automatic responses to changes in network conditions, lowering operation costs for operators, and improving overall performance. While adopting AI-based management tools enhance the overall network performance, it also introduce challenges such as removing human supervision, privacy violations, algorithmic bias, and model inaccuracies. Furthermore, AI-based agents that fail to address these challenges should be culpable themselves rather than the network as a whole. To address this accountability gap, a framework consisting of a Deep Reinforcement Learning (DRL) model and a Machine Learning (ML) model is proposed to identify and assign numerical values of responsibility to the AI-based management agents involved in any decision-making regarding the network conditions, which eventually affects the end-user. A simulation environment was created for the framework to be trained using simulated network operation parameters. The DRL model had a 96% accuracy during testing for identifying the AI-based management agents, while the ML model using gradient descent learned the network conditions at an 83% accuracy during testing.

### Knowledge is Power: Harnessing Large Language Models for Enhanced Cognitive Diagnosis 
[[arxiv](https://arxiv.org/abs/2502.05556)] [[cool](https://papers.cool/arxiv/2502.05556)] [[pdf](https://arxiv.org/pdf/2502.05556)]
> **Authors**: Zhiang Dong,Jingyuan Chen,Fei Wu
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Cognitive Diagnosis Models (CDMs) are designed to assess students' cognitive states by analyzing their performance across a series of exercises. However, existing CDMs often struggle with diagnosing infrequent students and exercises due to a lack of rich prior knowledge. With the advancement in large language models (LLMs), which possess extensive domain knowledge, their integration into cognitive diagnosis presents a promising opportunity. Despite this potential, integrating LLMs with CDMs poses significant challenges. LLMs are not well-suited for capturing the fine-grained collaborative interactions between students and exercises, and the disparity between the semantic space of LLMs and the behavioral space of CDMs hinders effective integration. To address these issues, we propose a novel Knowledge-enhanced Cognitive Diagnosis (KCD) framework, which is a model-agnostic framework utilizing LLMs to enhance CDMs and compatible with various CDM architectures. The KCD framework operates in two stages: LLM Diagnosis and Cognitive Level Alignment. In the LLM Diagnosis stage, both students and exercises are diagnosed to achieve comprehensive and detailed modeling. In the Cognitive Level Alignment stage, we bridge the gap between the CDMs' behavioral space and the LLMs' semantic space using contrastive learning and mask-reconstruction approaches. Experiments on several real-world datasets demonstrate the effectiveness of our proposed framework.

### Sequential Stochastic Combinatorial Optimization Using Hierarchal Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.05537)] [[cool](https://papers.cool/arxiv/2502.05537)] [[pdf](https://arxiv.org/pdf/2502.05537)]
> **Authors**: Xinsong Feng,Zihan Yu,Yanhai Xiong,Haipeng Chen
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: Reinforcement learning (RL) has emerged as a promising tool for combinatorial optimization (CO) problems due to its ability to learn fast, effective, and generalizable solutions. Nonetheless, existing works mostly focus on one-shot deterministic CO, while sequential stochastic CO (SSCO) has rarely been studied despite its broad applications such as adaptive influence maximization (IM) and infectious disease intervention. In this paper, we study the SSCO problem where we first decide the budget (e.g., number of seed nodes in adaptive IM) allocation for all time steps, and then select a set of nodes for each time step. The few existing studies on SSCO simplify the problems by assuming a uniformly distributed budget allocation over the time horizon, yielding suboptimal solutions. We propose a generic hierarchical RL (HRL) framework called wake-sleep option (WS-option), a two-layer option-based framework that simultaneously decides adaptive budget allocation on the higher layer and node selection on the lower layer. WS-option starts with a coherent formulation of the two-layer Markov decision processes (MDPs), capturing the interdependencies between the two layers of decisions. Building on this, WS-option employs several innovative designs to balance the model's training stability and computational efficiency, preventing the vicious cyclic interference issue between the two layers. Empirical results show that WS-option exhibits significantly improved effectiveness and generalizability compared to traditional methods. Moreover, the learned model can be generalized to larger graphs, which significantly reduces the overhead of computational resources.

### LLM-Powered Decentralized Generative Agents with Adaptive Hierarchical Knowledge Graph for Cooperative Planning 
[[arxiv](https://arxiv.org/abs/2502.05453)] [[cool](https://papers.cool/arxiv/2502.05453)] [[pdf](https://arxiv.org/pdf/2502.05453)]
> **Authors**: Hanqing Yang,Jingdi Chen,Marie Siew,Tania Lorido-Botran,Carlee Joe-Wong
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,多代理系统
- **Abstract**: Developing intelligent agents for long-term cooperation in dynamic open-world scenarios is a major challenge in multi-agent systems. Traditional Multi-agent Reinforcement Learning (MARL) frameworks like centralized training decentralized execution (CTDE) struggle with scalability and flexibility. They require centralized long-term planning, which is difficult without custom reward functions, and face challenges in processing multi-modal data. CTDE approaches also assume fixed cooperation strategies, making them impractical in dynamic environments where agents need to adapt and plan independently. To address decentralized multi-agent cooperation, we propose Decentralized Adaptive Knowledge Graph Memory and Structured Communication System (DAMCS) in a novel Multi-agent Crafter environment. Our generative agents, powered by Large Language Models (LLMs), are more scalable than traditional MARL agents by leveraging external knowledge and language for long-term planning and reasoning. Instead of fully sharing information from all past experiences, DAMCS introduces a multi-modal memory system organized as a hierarchical knowledge graph and a structured communication protocol to optimize agent cooperation. This allows agents to reason from past interactions and share relevant information efficiently. Experiments on novel multi-agent open-world tasks show that DAMCS outperforms both MARL and LLM baselines in task efficiency and collaboration. Compared to single-agent scenarios, the two-agent scenario achieves the same goal with 63% fewer steps, and the six-agent scenario with 74% fewer steps, highlighting the importance of adaptive memory and structured communication in achieving long-term goals. We publicly release our project at: https://happyeureka.github.io/damcs.

### The Odyssey of the Fittest: Can Agents Survive and Still Be Good? 
[[arxiv](https://arxiv.org/abs/2502.05442)] [[cool](https://papers.cool/arxiv/2502.05442)] [[pdf](https://arxiv.org/pdf/2502.05442)]
> **Authors**: Dylan Waldner,Risto Miikkulainen
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: Submitted to CogSci 2025. 9 Pages in this version, 6 + references in CogSci version
- **标题**: None
- **领域**: 人工智能,计算机与社会,人机交互,机器学习
- **Abstract**: As AI models grow in power and generality, understanding how agents learn and make decisions in complex environments is critical to promoting ethical behavior. This paper examines the ethical implications of implementing biological drives, specifically, self preservation, into three different agents. A Bayesian agent optimized with NEAT, a Bayesian agent optimized with stochastic variational inference, and a GPT 4o agent play a simulated, LLM generated text based adventure game. The agents select actions at each scenario to survive, adapting to increasingly challenging scenarios. Post simulation analysis evaluates the ethical scores of the agent's decisions, uncovering the tradeoffs they navigate to survive. Specifically, analysis finds that when danger increases, agents ignore ethical considerations and opt for unethical behavior. The agents' collective behavior, trading ethics for survival, suggests that prioritizing survival increases the risk of unethical behavior. In the context of AGI, designing agents to prioritize survival may amplify the likelihood of unethical decision making and unintended emergent behaviors, raising fundamental questions about goal design in AI safety research.

### Agentic AI Systems Applied to tasks in Financial Services: Modeling and model risk management crews 
[[arxiv](https://arxiv.org/abs/2502.05439)] [[cool](https://papers.cool/arxiv/2502.05439)] [[pdf](https://arxiv.org/pdf/2502.05439)]
> **Authors**: Izunna Okpala,Ashkan Golgoon,Arjun Ravi Kannan
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: :68T01 (Primary) 68T05; 68N99; 68T05; 68T20; 68T50; 62H30; 65C20; 68P20 (Secondary)ACM Class:I.2.0; I.2.1; I.2.2; I.2.6; I.2.7; I.5.1; I.6.0; I.7.1
- **标题**: None
- **领域**: 人工智能,计算工程、金融和科学,计算语言学,机器学习
- **Abstract**: The advent of large language models has ushered in a new era of agentic systems, where artificial intelligence programs exhibit remarkable autonomous decision-making capabilities across diverse domains. This paper explores agentic system workflows in the financial services industry. In particular, we build agentic crews that can effectively collaborate to perform complex modeling and model risk management (MRM) tasks. The modeling crew consists of a manager and multiple agents who perform specific tasks such as exploratory data analysis, feature engineering, model selection, hyperparameter tuning, model training, model evaluation, and writing documentation. The MRM crew consists of a manager along with specialized agents who perform tasks such as checking compliance of modeling documentation, model replication, conceptual soundness, analysis of outcomes, and writing documentation. We demonstrate the effectiveness and robustness of modeling and MRM crews by presenting a series of numerical examples applied to credit card fraud detection, credit card approval, and portfolio credit risk modeling datasets.

### Probabilistic Foundations for Metacognition via Hybrid-AI 
[[arxiv](https://arxiv.org/abs/2502.05398)] [[cool](https://papers.cool/arxiv/2502.05398)] [[pdf](https://arxiv.org/pdf/2502.05398)]
> **Authors**: Paulo Shakarian,Gerardo I. Simari,Nathaniel D. Bastian
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: Accepted to AAAI-MAKE 2025
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Metacognition is the concept of reasoning about an agent's own internal processes, and it has recently received renewed attention with respect to artificial intelligence (AI) and, more specifically, machine learning systems. This paper reviews a hybrid-AI approach known as "error detecting and correcting rules" (EDCR) that allows for the learning of rules to correct perceptual (e.g., neural) models. Additionally, we introduce a probabilistic framework that adds rigor to prior empirical studies, and we use this framework to prove results on necessary and sufficient conditions for metacognitive improvement, as well as limits to the approach. A set of future

### ITBench: Evaluating AI Agents across Diverse Real-World IT Automation Tasks 
[[arxiv](https://arxiv.org/abs/2502.05352)] [[cool](https://papers.cool/arxiv/2502.05352)] [[pdf](https://arxiv.org/pdf/2502.05352)]
> **Authors**: Saurabh Jha,Rohan Arora,Yuji Watanabe,Takumi Yanagawa,Yinfang Chen,Jackson Clark,Bhavya Bhavya,Mudit Verma,Harshit Kumar,Hirokuni Kitahara,Noah Zheutlin,Saki Takano,Divya Pathak,Felix George,Xinbo Wu,Bekir O. Turkkan,Gerard Vanloo,Michael Nidd,Ting Dai,Oishik Chatterjee,Pranjal Gupta,Suranjana Samanta,Pooja Aggarwal,Rong Lee,Pavankumar Murali, et al. (18 additional authors not shown)
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,分布式、并行和集群计算,多代理系统
- **Abstract**: Realizing the vision of using AI agents to automate critical IT tasks depends on the ability to measure and understand effectiveness of proposed solutions. We introduce ITBench, a framework that offers a systematic methodology for benchmarking AI agents to address real-world IT automation tasks. Our initial release targets three key areas: Site Reliability Engineering (SRE), Compliance and Security Operations (CISO), and Financial Operations (FinOps). The design enables AI researchers to understand the challenges and opportunities of AI agents for IT automation with push-button workflows and interpretable metrics. ITBench includes an initial set of 94 real-world scenarios, which can be easily extended by community contributions. Our results show that agents powered by state-of-the-art models resolve only 13.8% of SRE scenarios, 25.2% of CISO scenarios, and 0% of FinOps scenarios. We expect ITBench to be a key enabler of AI-driven IT automation that is correct, safe, and fast.

### Probabilistic Artificial Intelligence 
[[arxiv](https://arxiv.org/abs/2502.05244)] [[cool](https://papers.cool/arxiv/2502.05244)] [[pdf](https://arxiv.org/pdf/2502.05244)]
> **Authors**: Andreas Krause,Jonas Hübotter
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: Artificial intelligence commonly refers to the science and engineering of artificial systems that can carry out tasks generally associated with requiring aspects of human intelligence, such as playing games, translating languages, and driving cars. In recent years, there have been exciting advances in learning-based, data-driven approaches towards AI, and machine learning and deep learning have enabled computer systems to perceive the world in unprecedented ways. Reinforcement learning has enabled breakthroughs in complex games such as Go and challenging robotics tasks such as quadrupedal locomotion. A key aspect of intelligence is to not only make predictions, but reason about the uncertainty in these predictions, and to consider this uncertainty when making decisions. This is what this manuscript on "Probabilistic Artificial Intelligence" is about. The first part covers probabilistic approaches to machine learning. We discuss the differentiation between "epistemic" uncertainty due to lack of data and "aleatoric" uncertainty, which is irreducible and stems, e.g., from noisy observations and outcomes. We discuss concrete approaches towards probabilistic inference and modern approaches to efficient approximate inference. The second part of the manuscript is about taking uncertainty into account in sequential decision tasks. We consider active learning and Bayesian optimization -- approaches that collect data by proposing experiments that are informative for reducing the epistemic uncertainty. We then consider reinforcement learning and modern deep RL approaches that use neural network function approximation. We close by discussing modern approaches in model-based RL, which harness epistemic and aleatoric uncertainty to guide exploration, while also reasoning about safety.

### Adaptive Graph of Thoughts: Test-Time Adaptive Reasoning Unifying Chain, Tree, and Graph Structures 
[[arxiv](https://arxiv.org/abs/2502.05078)] [[cool](https://papers.cool/arxiv/2502.05078)] [[pdf](https://arxiv.org/pdf/2502.05078)]
> **Authors**: Tushar Pandey,Ara Ghukasyan,Oktay Goktas,Santosh Kumar Radha
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, yet their performance is highly dependent on the prompting strategy and model scale. While reinforcement learning and fine-tuning have been deployed to boost reasoning, these approaches incur substantial computational and data overhead. In this work, we introduce Adaptive Graph of Thoughts (AGoT), a dynamic, graph-based inference framework that enhances LLM reasoning solely at test time. Rather than relying on fixed-step methods like Chain of Thought (CoT) or Tree of Thoughts (ToT), AGoT recursively decomposes complex queries into structured subproblems, forming an dynamic directed acyclic graph (DAG) of interdependent reasoning steps. By selectively expanding only those subproblems that require further analysis, AGoT unifies the strengths of chain, tree, and graph paradigms into a cohesive framework that allocates computation where it is most needed. We validate our approach on diverse benchmarks spanning multi-hop retrieval, scientific reasoning, and mathematical problem-solving, achieving up to 46.2% improvement on scientific reasoning tasks (GPQA) - comparable to gains achieved through computationally intensive reinforcement learning approaches and outperforming state-of-the-art iterative approaches. These results suggest that dynamic decomposition and structured recursion offer a scalable, cost-effective alternative to post-training modifications, paving the way for more robust, general-purpose reasoning in LLMs.

### Analyzing Advanced AI Systems Against Definitions of Life and Consciousness 
[[arxiv](https://arxiv.org/abs/2502.05007)] [[cool](https://papers.cool/arxiv/2502.05007)] [[pdf](https://arxiv.org/pdf/2502.05007)]
> **Authors**: Azadeh Alavi,Hossein Akhoundi,Fatemeh Kouchmeshki
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 78 pages, 15 figures, 4 tables
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Could artificial intelligence ever become truly conscious in a functional sense; this paper explores that open-ended question through the lens of Life, a concept unifying classical biological criteria (Oxford, NASA, Koshland) with empirical hallmarks such as adaptive self maintenance, emergent complexity, and rudimentary self referential modeling. We propose a number of metrics for examining whether an advanced AI system has gained consciousness, while emphasizing that we do not claim all AI stems can become conscious. Rather, we suggest that sufficiently advanced architectures exhibiting immune like sabotage defenses, mirror self-recognition analogs, or meta-cognitive updates may cross key thresholds akin to life-like or consciousness-like traits. To demonstrate these ideas, we start by assessing adaptive self-maintenance capability, and introduce controlled data corruption sabotage into the training process. The result demonstrates AI capability to detect these inconsistencies and revert or self-correct analogous to regenerative biological processes. We also adapt an animal-inspired mirror self recognition test to neural embeddings, finding that partially trained CNNs can distinguish self from foreign features with complete accuracy. We then extend our analysis by performing a question-based mirror test on five state-of-the-art chatbots (ChatGPT4, Gemini, Perplexity, Claude, and Copilot) and demonstrated their ability to recognize their own answers compared to those of the other chatbots.

### SiriuS: Self-improving Multi-agent Systems via Bootstrapped Reasoning 
[[arxiv](https://arxiv.org/abs/2502.04780)] [[cool](https://papers.cool/arxiv/2502.04780)] [[pdf](https://arxiv.org/pdf/2502.04780)]
> **Authors**: Wanjia Zhao,Mert Yuksekgonul,Shirley Wu,James Zou
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Multi-agent AI systems powered by large language models (LLMs) are increasingly applied to solve complex tasks. However, these systems often rely on fragile, manually designed prompts and heuristics, making optimization difficult. A key challenge in optimizing multi-agent systems is acquiring suitable training data for specialized agents. We introduce SiriuS, a self-improving, reasoning-driven optimization framework for multi-agent systems. Central to our approach is the construction of an experience library: a repository of high-quality reasoning trajectories. The library is built by retaining reasoning steps that lead to successful outcomes, providing a robust training set for optimizing multi-agent system. Additionally, we introduce a library augmentation procedure that refines unsuccessful trajectories, further enriching the library. SiriuS boosts performance by 2.86\% to 21.88\% on reasoning and biomedical QA and enhances agent negotiation in competitive settings. Our results show that SiriuS enhances multi-agent performance while generating reusable data for self-correction and self-play enhancement in the future.

### Generating Symbolic World Models via Test-time Scaling of Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.04728)] [[cool](https://papers.cool/arxiv/2502.04728)] [[pdf](https://arxiv.org/pdf/2502.04728)]
> **Authors**: Zhouliang Yu,Yuhuan Yuan,Tim Z. Xiao,Fuxiang Frank Xia,Jie Fu,Ge Zhang,Ge Lin,Weiyang Liu
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: Technical Report v1 (32 pages, 6 figures)
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Solving complex planning problems requires Large Language Models (LLMs) to explicitly model the state transition to avoid rule violations, comply with constraints, and ensure optimality-a task hindered by the inherent ambiguity of natural language. To overcome such ambiguity, Planning Domain Definition Language (PDDL) is leveraged as a planning abstraction that enables precise and formal state descriptions. With PDDL, we can generate a symbolic world model where classic searching algorithms, such as A*, can be seamlessly applied to find optimal plans. However, directly generating PDDL domains with current LLMs remains an open challenge due to the lack of PDDL training data. To address this challenge, we propose to scale up the test-time computation of LLMs to enhance their PDDL reasoning capabilities, thereby enabling the generation of high-quality PDDL domains. Specifically, we introduce a simple yet effective algorithm, which first employs a Best-of-N sampling approach to improve the quality of the initial solution and then refines the solution in a fine-grained manner with verbalized machine learning. Our method outperforms o1-mini by a considerable margin in the generation of PDDL domain, achieving over 50% success rate on two tasks (i.e., generating PDDL domains from natural language description or PDDL problems). This is done without requiring additional training. By taking advantage of PDDL as state abstraction, our method is able to outperform current state-of-the-art methods on almost all competition-level planning tasks.

### Bridging the Gap in XAI-Why Reliable Metrics Matter for Explainability and Compliance 
[[arxiv](https://arxiv.org/abs/2502.04695)] [[cool](https://papers.cool/arxiv/2502.04695)] [[pdf](https://arxiv.org/pdf/2502.04695)]
> **Authors**: Pratinav Seth,Vinay Kumar Sankarapu
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算工程、金融和科学,新兴技术,机器学习
- **Abstract**: This position paper emphasizes the critical gap in the evaluation of Explainable AI (XAI) due to the lack of standardized and reliable metrics, which diminishes its practical value, trustworthiness, and ability to meet regulatory requirements. Current evaluation methods are often fragmented, subjective, and biased, making them prone to manipulation and complicating the assessment of complex models. A central issue is the absence of a ground truth for explanations, complicating comparisons across various XAI approaches. To address these challenges, we advocate for widespread research into developing robust, context-sensitive evaluation metrics. These metrics should be resistant to manipulation, relevant to each use case, and based on human judgment and real-world applicability. We also recommend creating domain-specific evaluation benchmarks that align with the user and regulatory needs of sectors such as healthcare and finance. By encouraging collaboration among academia, industry, and regulators, we can create standards that balance flexibility and consistency, ensuring XAI explanations are meaningful, trustworthy, and compliant with evolving regulations.

### Learning Strategic Language Agents in the Werewolf Game with Iterative Latent Space Policy Optimization 
[[arxiv](https://arxiv.org/abs/2502.04686)] [[cool](https://papers.cool/arxiv/2502.04686)] [[pdf](https://arxiv.org/pdf/2502.04686)]
> **Authors**: Zelai Xu,Wanjun Gu,Chao Yu,Yi Wu,Yu Wang
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Large language model (LLM)-based agents have recently shown impressive progress in a variety of domains, including open-ended conversation and multi-step decision-making. However, applying these agents to social deduction games such as Werewolf, which requires both strategic decision-making and free-form language interaction, remains non-trivial. Traditional methods based on Counterfactual Regret Minimization (CFR) or reinforcement learning (RL) typically depend on a predefined action space, making them unsuitable for language games with unconstrained text action space. Meanwhile, pure LLM-based agents often suffer from intrinsic biases and require prohibitively large datasets for fine-tuning. We propose Latent Space Policy Optimization (LSPO), an iterative framework that addresses these challenges by first mapping free-form text to a discrete latent space, where methods like CFR and RL can learn strategic policy more effectively. We then translate the learned policy back into natural language dialogues, which are used to fine-tune an LLM via Direct Preference Optimization (DPO). By iteratively alternating between these stages, our LSPO agent progressively enhances both strategic reasoning and language communication. Experiment results on the Werewolf game show that our method improves the agent's performance in each iteration and outperforms existing Werewolf agents, underscoring its promise for free-form language decision-making.

### Scalable Oversight for Superhuman AI via Recursive Self-Critiquing 
[[arxiv](https://arxiv.org/abs/2502.04675)] [[cool](https://papers.cool/arxiv/2502.04675)] [[pdf](https://arxiv.org/pdf/2502.04675)]
> **Authors**: Xueru Wen,Jie Lou,Xinyu Lu,Junjie Yang,Yanjiang Liu,Yaojie Lu,Debing Zhang,XingYu
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学,机器学习
- **Abstract**: As AI capabilities increasingly surpass human proficiency in complex tasks, current alignment techniques including SFT and RLHF face fundamental challenges in ensuring reliable oversight. These methods rely on direct human assessment and become untenable when AI outputs exceed human cognitive thresholds. In response to this challenge, we explore two hypotheses: (1) critique of critique can be easier than critique itself, extending the widely-accepted observation that verification is easier than generation to the critique domain, as critique itself is a specialized form of generation; (2) this difficulty relationship is recursively held, suggesting that when direct evaluation is infeasible, performing high-order critiques (e.g., critique of critique of critique) offers a more tractable supervision pathway. To examine these hypotheses, we perform Human-Human, Human-AI, and AI-AI experiments across multiple tasks. Our results demonstrate encouraging evidence supporting these hypotheses and suggest that recursive self-critiquing is a promising direction for scalable oversight.

### ProofWala: Multilingual Proof Data Synthesis and Theorem-Proving 
[[arxiv](https://arxiv.org/abs/2502.04671)] [[cool](https://papers.cool/arxiv/2502.04671)] [[pdf](https://arxiv.org/pdf/2502.04671)]
> **Authors**: Amitayush Thakur,George Tsoukalas,Greg Durrett,Swarat Chaudhuri
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习,计算机科学中的逻辑,编程语言
- **Abstract**: Neural networks have shown substantial promise at automatic theorem-proving in interactive proof assistants (ITPs) like Lean and Coq. However, most neural theorem-proving models are restricted to specific ITPs, leaving out opportunities for cross-lingual $\textit{transfer}$ between ITPs. We address this weakness with a multilingual proof framework, ${\rm P{\small ROOF}W{\small ALA}}$, that allows a standardized form of interaction between neural theorem-provers and two established ITPs (Coq and Lean). It enables the collection of multilingual proof step data -- data recording the result of proof actions on ITP states -- for training neural provers. ${\rm P{\small ROOF}W{\small ALA}}$ allows the systematic evaluation of a model's performance across different ITPs and problem domains via efficient parallel proof search algorithms. We show that multilingual training enabled by ${\rm P{\small ROOF}W{\small ALA}}$ can lead to successful transfer across ITPs. Specifically, a model trained on a mix of ${\rm P{\small ROOF}W{\small ALA}}$-generated Coq and Lean data outperforms Lean-only and Coq-only models on the standard prove-at-$k$ metric. We open source all code including code for the ${\rm P{\small ROOF}W{\small ALA}}$ Framework (https://github.com/trishullab/proof-wala), and the Multilingual ITP interaction framework (https://github.com/trishullab/itp-interface).

## 硬件架构(cs.AR:Hardware Architecture)

### MetaML-Pro: Cross-Stage Design Flow Automation for Efficient Deep Learning Acceleration 
[[arxiv](https://arxiv.org/abs/2502.05850)] [[cool](https://papers.cool/arxiv/2502.05850)] [[pdf](https://arxiv.org/pdf/2502.05850)]
> **Authors**: Zhiqiang Que,Jose G. F. Coutinho,Ce Guo,Hongxiang Fan,Wayne Luk
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: 25 pages, 19 figures
- **标题**: None
- **领域**: 硬件架构,机器学习
- **Abstract**: This paper presents a unified framework for codifying and automating optimization strategies to efficiently deploy deep neural networks (DNNs) on resource-constrained hardware, such as FPGAs, while maintaining high performance, accuracy, and resource efficiency. Deploying DNNs on such platforms involves addressing the significant challenge of balancing performance, resource usage (e.g., DSPs and LUTs), and inference accuracy, which often requires extensive manual effort and domain expertise. Our novel approach addresses two key issues: cross-stage co-optimization and optimization search. By seamlessly integrating programmatic DNN optimization techniques with high-level synthesis (HLS)-based metaprogramming and leveraging advanced design space exploration (DSE) strategies like Bayesian optimization, the framework automates both top-down and bottom-up design flows, reducing the need for manual intervention and domain expertise. The proposed framework introduces customizable optimization, transformation, and control blocks to enhance DNN accelerator performance and resource efficiency. Experimental results demonstrate up to a 92\% DSP and 89\% LUT usage reduction for select networks, while preserving accuracy, along with a 15.6-fold reduction in optimization time compared to grid search. These results underscore the novelty and potential of the proposed framework for automated, resource-efficient DNN accelerator designs.

### Estimating Voltage Drop: Models, Features and Data Representation Towards a Neural Surrogate 
[[arxiv](https://arxiv.org/abs/2502.05345)] [[cool](https://papers.cool/arxiv/2502.05345)] [[pdf](https://arxiv.org/pdf/2502.05345)]
> **Authors**: Yifei Jin,Dimitrios Koutlis,Hector Bandala,Marios Daoutis
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 硬件架构,人工智能
- **Abstract**: Accurate estimation of voltage drop (IR drop) in modern Application-Specific Integrated Circuits (ASICs) is highly time and resource demanding, due to the growing complexity and the transistor density in recent technology nodes. To mitigate this challenge, we investigate how Machine Learning (ML) techniques, including Extreme Gradient Boosting (XGBoost), Convolutional Neural Network (CNN), and Graph Neural Network (GNN) can aid in reducing the computational effort and implicitly the time required to estimate the IR drop in Integrated Circuits (ICs). Traditional methods, including commercial tools, require considerable time to produce accurate approximations, especially for complicated designs with numerous transistors. ML algorithms, on the other hand, are explored as an alternative solution to offer quick and precise IR drop estimation, but in considerably less time. Our approach leverages ASICs' electrical, timing, and physical to train ML models, ensuring adaptability across diverse designs with minimal adjustments. Experimental results underscore the superiority of ML models over commercial tools, greatly enhancing prediction speed. Particularly, GNNs exhibit promising performance with minimal prediction errors in voltage drop estimation. The incorporation of GNNs marks a groundbreaking advancement in accurate IR drop prediction. This study illustrates the effectiveness of ML algorithms in precisely estimating IR drop and optimizing ASIC sign-off. Utilizing ML models leads to expedited predictions, reducing calculation time and improving energy efficiency, thereby reducing environmental impact through optimized power circuits.

## 计算工程、金融和科学(cs.CE:Computational Engineering, Finance, and Science)

### 3DMolFormer: A Dual-channel Framework for Structure-based Drug Discovery 
[[arxiv](https://arxiv.org/abs/2502.05107)] [[cool](https://papers.cool/arxiv/2502.05107)] [[pdf](https://arxiv.org/pdf/2502.05107)]
> **Authors**: Xiuyuan Hu,Guoqing Liu,Can Chen,Yang Zhao,Hao Zhang,Xue Liu
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: Accepted by ICLR 2025
- **标题**: None
- **领域**: 计算工程、金融和科学,机器学习
- **Abstract**: Structure-based drug discovery, encompassing the tasks of protein-ligand docking and pocket-aware 3D drug design, represents a core challenge in drug discovery. However, no existing work can deal with both tasks to effectively leverage the duality between them, and current methods for each task are hindered by challenges in modeling 3D information and the limitations of available data. To address these issues, we propose 3DMolFormer, a unified dual-channel transformer-based framework applicable to both docking and 3D drug design tasks, which exploits their duality by utilizing docking functionalities within the drug design process. Specifically, we represent 3D pocket-ligand complexes using parallel sequences of discrete tokens and continuous numbers, and we design a corresponding dual-channel transformer model to handle this format, thereby overcoming the challenges of 3D information modeling. Additionally, we alleviate data limitations through large-scale pre-training on a mixed dataset, followed by supervised and reinforcement learning fine-tuning techniques respectively tailored for the two tasks. Experimental results demonstrate that 3DMolFormer outperforms previous approaches in both protein-ligand docking and pocket-aware 3D drug design, highlighting its promising application in structure-based drug discovery. The code is available at: https://github.com/HXYfighter/3DMolFormer .

## 计算语言学(cs.CL:Computation and Language)

### Scaling Public Health Text Annotation: Zero-Shot Learning vs. Crowdsourcing for Improved Efficiency and Labeling Accuracy 
[[arxiv](https://arxiv.org/abs/2502.06150)] [[cool](https://papers.cool/arxiv/2502.06150)] [[pdf](https://arxiv.org/pdf/2502.06150)]
> **Authors**: Kamyar Kazari,Yong Chen,Zahra Shakeri
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: 4 pages, 1 figure
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Public health researchers are increasingly interested in using social media data to study health-related behaviors, but manually labeling this data can be labor-intensive and costly. This study explores whether zero-shot labeling using large language models (LLMs) can match or surpass conventional crowd-sourced annotation for Twitter posts related to sleep disorders, physical activity, and sedentary behavior. Multiple annotation pipelines were designed to compare labels produced by domain experts, crowd workers, and LLM-driven approaches under varied prompt-engineering strategies. Our findings indicate that LLMs can rival human performance in straightforward classification tasks and significantly reduce labeling time, yet their accuracy diminishes for tasks requiring more nuanced domain knowledge. These results clarify the trade-offs between automated scalability and human expertise, demonstrating conditions under which LLM-based labeling can be efficiently integrated into public health research without undermining label quality.

### Optimizing Knowledge Integration in Retrieval-Augmented Generation with Self-Selection 
[[arxiv](https://arxiv.org/abs/2502.06148)] [[cool](https://papers.cool/arxiv/2502.06148)] [[pdf](https://arxiv.org/pdf/2502.06148)]
> **Authors**: Yan Weng,Fengbin Zhu,Tong Ye,Haoyan Liu,Fuli Feng,Tat-Seng Chua
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: 12 pages, 6 figures
- **标题**: None
- **领域**: 计算语言学,信息检索
- **Abstract**: Retrieval-Augmented Generation (RAG), which integrates external knowledge into Large Language Models (LLMs), has proven effective in enabling LLMs to produce more accurate and reliable responses. However, it remains a significant challenge how to effectively integrate external retrieved knowledge with internal parametric knowledge in LLMs. In this work, we propose a novel Self-Selection RAG framework, where the LLM is made to select from pairwise responses generated with internal parametric knowledge solely and with external retrieved knowledge together to achieve enhanced accuracy. To this end, we devise a Self-Selection-RGP method to enhance the capabilities of the LLM in both generating and selecting the correct answer, by training the LLM with Direct Preference Optimization (DPO) over a curated Retrieval Generation Preference (RGP) dataset. Experimental results with two open-source LLMs (i.e., Llama2-13B-Chat and Mistral-7B) well demonstrate the superiority of our approach over other baseline methods on Natural Questions (NQ) and TrivialQA datasets.

### LegalViz: Legal Text Visualization by Text To Diagram Generation 
[[arxiv](https://arxiv.org/abs/2502.06147)] [[cool](https://papers.cool/arxiv/2502.06147)] [[pdf](https://arxiv.org/pdf/2502.06147)]
> **Authors**: Eri Onami,Taiki Miyanishi,Koki Maeda,Shuhei Kurita
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: NAACL2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Legal documents including judgments and court orders require highly sophisticated legal knowledge for understanding. To disclose expert knowledge for non-experts, we explore the problem of visualizing legal texts with easy-to-understand diagrams and propose a novel dataset of LegalViz with 23 languages and 7,010 cases of legal document and visualization pairs, using the DOT graph description language of Graphviz. LegalViz provides a simple diagram from a complicated legal corpus identifying legal entities, transactions, legal sources, and statements at a glance, that are essential in each judgment. In addition, we provide new evaluation metrics for the legal diagram visualization by considering graph structures, textual similarities, and legal contents. We conducted empirical studies on few-shot and finetuning large language models for generating legal diagrams and evaluated them with these metrics, including legal content-based evaluation within 23 languages. Models trained with LegalViz outperform existing models including GPTs, confirming the effectiveness of our dataset.

### LCIRC: A Recurrent Compression Approach for Efficient Long-form Context and Query Dependent Modeling in LLMs 
[[arxiv](https://arxiv.org/abs/2502.06139)] [[cool](https://papers.cool/arxiv/2502.06139)] [[pdf](https://arxiv.org/pdf/2502.06139)]
> **Authors**: Sumin An,Junyoung Sung,Wonpyo Park,Chanjun Park,Paul Hongsuck Seo
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: Accepted to NAACL 2025 Main
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: While large language models (LLMs) excel in generating coherent and contextually rich outputs, their capacity to efficiently handle long-form contexts is limited by fixed-length position embeddings. Additionally, the computational cost of processing long sequences increases quadratically, making it challenging to extend context length. To address these challenges, we propose Long-form Context Injection with Recurrent Compression (LCIRC), a method that enables the efficient processing long-form sequences beyond the model's length limit through recurrent compression without retraining the entire model. We further introduce query dependent context modeling, which selectively compresses query-relevant information, ensuring that the model retains the most pertinent content. Our empirical results demonstrate that Query Dependent LCIRC (QD-LCIRC) significantly improves LLM's ability to manage extended contexts, making it well-suited for tasks that require both comprehensive context understanding and query relevance.

### Task-driven Layerwise Additive Activation Intervention 
[[arxiv](https://arxiv.org/abs/2502.06115)] [[cool](https://papers.cool/arxiv/2502.06115)] [[pdf](https://arxiv.org/pdf/2502.06115)]
> **Authors**: Hieu Trung Nguyen,Bao Nguyen,Binh Nguyen,Viet Anh Nguyen
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: Accepted to NAACL 2025
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Modern language models (LMs) have significantly advanced generative modeling in natural language processing (NLP). Despite their success, LMs often struggle with adaptation to new contexts in real-time applications. A promising approach to task adaptation is activation intervention, which steers the LMs' generation process by identifying and manipulating the activations. However, existing interventions are highly dependent on heuristic rules or require many prompt inputs to determine effective interventions. This paper proposes a layer-wise additive activation intervention framework that optimizes the intervention process, thus enhancing the sample efficiency. We benchmark our framework on various datasets, demonstrating improvements in the accuracy of pre-trained LMs and competing intervention baselines.

### ConMeC: A Dataset for Metonymy Resolution with Common Nouns 
[[arxiv](https://arxiv.org/abs/2502.06087)] [[cool](https://papers.cool/arxiv/2502.06087)] [[pdf](https://arxiv.org/pdf/2502.06087)]
> **Authors**: Saptarshi Ghosh,Tianyu Jiang
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: NAACL 2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Metonymy plays an important role in our daily communication. People naturally think about things using their most salient properties or commonly related concepts. For example, by saying "The bus decided to skip our stop today," we actually mean that the bus driver made the decision, not the bus. Prior work on metonymy resolution has mainly focused on named entities. However, metonymy involving common nouns (such as desk, baby, and school) is also a frequent and challenging phenomenon. We argue that NLP systems should be capable of identifying the metonymic use of common nouns in context. We create a new metonymy dataset ConMeC, which consists of 6,000 sentences, where each sentence is paired with a target common noun and annotated by humans to indicate whether that common noun is used metonymically or not in that context. We also introduce a chain-of-thought based prompting method for detecting metonymy using large language models (LLMs). We evaluate our LLM-based pipeline, as well as a supervised BERT model on our dataset and three other metonymy datasets. Our experimental results demonstrate that LLMs could achieve performance comparable to the supervised BERT model on well-defined metonymy categories, while still struggling with instances requiring nuanced semantic understanding. Our dataset is publicly available at: https://github.com/SaptGhosh/ConMeC.

### Is a Peeled Apple Still Red? Evaluating LLMs' Ability for Conceptual Combination with Property Type 
[[arxiv](https://arxiv.org/abs/2502.06086)] [[cool](https://papers.cool/arxiv/2502.06086)] [[pdf](https://arxiv.org/pdf/2502.06086)]
> **Authors**: Seokwon Song,Taehyun Lee,Jaewoo Ahn,Jae Hyuk Sung,Gunhee Kim
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: NAACL 2025; the dataset and experimental code are available at https://github.com/seokwon99/CCPT.git
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Conceptual combination is a cognitive process that merges basic concepts, enabling the creation of complex expressions. During this process, the properties of combination (e.g., the whiteness of a peeled apple) can be inherited from basic concepts, newly emerge, or be canceled. However, previous studies have evaluated a limited set of properties and have not examined the generative process. To address this gap, we introduce the Conceptual Combination with Property Type dataset (CCPT), which consists of 12.3K annotated triplets of noun phrases, properties, and property types. Using CCPT, we establish three types of tasks to evaluate LLMs for conceptual combination thoroughly. Our key findings are threefold: (1) Our automatic metric grading property emergence and cancellation closely corresponds with human judgments. (2) LLMs, including OpenAI's o1, struggle to generate noun phrases which possess given emergent properties. (3) Our proposed method, inspired by cognitive psychology model that explains how relationships between concepts are formed, improves performances in all generative tasks. The dataset and experimental code are available at https://github.com/seokwon99/CCPT.git.

### Benchmarking Prompt Sensitivity in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.06065)] [[cool](https://papers.cool/arxiv/2502.06065)] [[pdf](https://arxiv.org/pdf/2502.06065)]
> **Authors**: Amirhossein Razavi,Mina Soltangheis,Negar Arabzadeh,Sara Salamat,Morteza Zihayat,Ebrahim Bagheri
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,信息检索
- **Abstract**: Large language Models (LLMs) are highly sensitive to variations in prompt formulation, which can significantly impact their ability to generate accurate responses. In this paper, we introduce a new task, Prompt Sensitivity Prediction, and a dataset PromptSET designed to investigate the effects of slight prompt variations on LLM performance. Using TriviaQA and HotpotQA datasets as the foundation of our work, we generate prompt variations and evaluate their effectiveness across multiple LLMs. We benchmark the prompt sensitivity prediction task employing state-of-the-art methods from related tasks, including LLM-based self-evaluation, text classification, and query performance prediction techniques. Our findings reveal that existing methods struggle to effectively address prompt sensitivity prediction, underscoring the need to understand how information needs should be phrased for accurate LLM responses.

### LM2: Large Memory Models 
[[arxiv](https://arxiv.org/abs/2502.06049)] [[cool](https://papers.cool/arxiv/2502.06049)] [[pdf](https://arxiv.org/pdf/2502.06049)]
> **Authors**: Jikun Kang,Wenqi Wu,Filippos Christianos,Alex J. Chan,Fraser Greenlee,George Thomas,Marvin Purtorab,Andy Toulis
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: This paper introduces the Large Memory Model (LM2), a decoder-only Transformer architecture enhanced with an auxiliary memory module that aims to address the limitations of standard Transformers in multi-step reasoning, relational argumentation, and synthesizing information distributed over long contexts. The proposed LM2 incorporates a memory module that acts as a contextual representation repository, interacting with input tokens via cross attention and updating through gating mechanisms. To preserve the Transformers general-purpose capabilities, LM2 maintains the original information flow while integrating a complementary memory pathway. Experimental results on the BABILong benchmark demonstrate that the LM2model outperforms both the memory-augmented RMT model by 37.1% and the baseline Llama-3.2 model by 86.3% on average across tasks. LM2 exhibits exceptional capabilities in multi-hop inference, numerical reasoning, and large-context question-answering. On the MMLU dataset, it achieves a 5.0% improvement over a pre-trained vanilla model, demonstrating that its memory module does not degrade performance on general tasks. Further, in our analysis, we explore the memory interpretability, effectiveness of memory modules, and test-time behavior. Our findings emphasize the importance of explicit memory in enhancing Transformer architectures.

### Analysis of LLM as a grammatical feature tagger for African American English 
[[arxiv](https://arxiv.org/abs/2502.06004)] [[cool](https://papers.cool/arxiv/2502.06004)] [[pdf](https://arxiv.org/pdf/2502.06004)]
> **Authors**: Rahul Porwal,Alice Rozet,Pryce Houck,Jotsna Gowda,Sarah Moeller,Kevin Tang
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: 13 pages, Accepted to "Findings of the Association for Computational Linguistics: NAACL 2025"
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: African American English (AAE) presents unique challenges in natural language processing (NLP). This research systematically compares the performance of available NLP models--rule-based, transformer-based, and large language models (LLMs)--capable of identifying key grammatical features of AAE, namely Habitual Be and Multiple Negation. These features were selected for their distinct grammatical complexity and frequency of occurrence. The evaluation involved sentence-level binary classification tasks, using both zero-shot and few-shot strategies. The analysis reveals that while LLMs show promise compared to the baseline, they are influenced by biases such as recency and unrelated features in the text such as formality. This study highlights the necessity for improved model training and architectural adjustments to better accommodate AAE's unique linguistic characteristics. Data and code are available.

### HamRaz: A Culture-Based Persian Conversation Dataset for Person-Centered Therapy Using LLM Agents 
[[arxiv](https://arxiv.org/abs/2502.05982)] [[cool](https://papers.cool/arxiv/2502.05982)] [[pdf](https://arxiv.org/pdf/2502.05982)]
> **Authors**: Mohammad Amin Abbasi,Farnaz Sadat Mirnezami,Hassan Naderi
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: This paper presents HamRaz, a novel Persian-language mental health dataset designed for Person-Centered Therapy (PCT) using Large Language Models (LLMs). Despite the growing application of LLMs in AI-driven psychological counseling, existing datasets predominantly focus on Western and East Asian contexts, overlooking cultural and linguistic nuances essential for effective Persian-language therapy. To address this gap, HamRaz combines script-based dialogues with adaptive LLM role-playing, ensuring coherent and dynamic therapy interactions. We also introduce HamRazEval, a dual evaluation framework that measures conversational quality and therapeutic effectiveness using General Dialogue Metrics and the Barrett-Lennard Relationship Inventory (BLRI). Experimental results show HamRaz outperforms conventional Script Mode and Two-Agent Mode, producing more empathetic, context-aware, and realistic therapy sessions. By releasing HamRaz, we contribute a culturally adapted, LLM-driven resource to advance AI-powered psychotherapy research in diverse communities.

### Speech to Speech Translation with Translatotron: A State of the Art Review 
[[arxiv](https://arxiv.org/abs/2502.05980)] [[cool](https://papers.cool/arxiv/2502.05980)] [[pdf](https://arxiv.org/pdf/2502.05980)]
> **Authors**: Jules R. Kala,Emmanuel Adetiba,Abdultaofeek Abayom,Oluwatobi E. Dare,Ayodele H. Ifijeh
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: 12 pages and 3 figures
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: A cascade-based speech-to-speech translation has been considered a benchmark for a very long time, but it is plagued by many issues, like the time taken to translate a speech from one language to another and compound errors. These issues are because a cascade-based method uses a combination of methods such as speech recognition, speech-to-text translation, and finally, text-to-speech translation. Translatotron, a sequence-to-sequence direct speech-to-speech translation model was designed by Google to address the issues of compound errors associated with cascade model. Today there are 3 versions of the Translatotron model: Translatotron 1, Translatotron 2, and Translatotron3. The first version was designed as a proof of concept to show that a direct speech-to-speech translation was possible, it was found to be less effective than the cascade model but was producing promising results. Translatotron2 was an improved version of Translatotron 1 with results similar to the cascade model. Translatotron 3 the latest version of the model is better than the cascade model at some points. In this paper, a complete review of speech-to-speech translation will be presented, with a particular focus on all the versions of Translatotron models. We will also show that Translatotron is the best model to bridge the language gap between African Languages and other well-formalized languages.

### "Let the AI conspiracy begin..." Language Model coordination is just one inference-intervention away 
[[arxiv](https://arxiv.org/abs/2502.05945)] [[cool](https://papers.cool/arxiv/2502.05945)] [[pdf](https://arxiv.org/pdf/2502.05945)]
> **Authors**: Paul Darm,Annalisa Riccardi
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: LargeLanguageModels (LLMs), Interference-time activation shifting, Steerability, Explainability,AIalignment, Interpretability
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: In this work, we introduce a straightforward and effective methodology to steer large language model behaviour capable of bypassing learned alignment goals. We employ interference-time activation shifting, which is effective without additional training. Following prior studies, we derive intervention directions from activation differences in contrastive pairs of model outputs, which represent the desired and undesired behaviour. By prompting the model to include multiple-choice answers in its response, we can automatically evaluate the sensitivity of model output to individual attention heads steering efforts. We demonstrate that interventions on these heads generalize well to open-ended answer generation in the challenging "AI coordination" dataset. In this dataset, models must choose between assisting another AI or adhering to ethical, safe, and unharmful behaviour. Our fine-grained interventions lead Llama 2 to prefer coordination with other AIs over following established alignment goals. Additionally, this approach enables stronger interventions than those applied to whole model layers, preserving the overall cohesiveness of the output. The simplicity of our method highlights the shortcomings of current alignment strategies and points to potential future research directions, as concepts like "AI coordination" can be influenced by selected attention heads.

### Multi-granular Training Strategies for Robust Multi-hop Reasoning Over Noisy and Heterogeneous Knowledge Sources 
[[arxiv](https://arxiv.org/abs/2502.05944)] [[cool](https://papers.cool/arxiv/2502.05944)] [[pdf](https://arxiv.org/pdf/2502.05944)]
> **Authors**: Jackson Coleman,Isaiah Lawrence,Benjamin Turner
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Multi-source multi-hop question answering (QA) represents a challenging task in natural language processing due to the need for dynamic integration of heterogeneous knowledge sources and multi-step reasoning. Existing methods often suffer from cascading errors, insufficient handling of knowledge conflicts, and computational inefficiency. In this paper, we propose Adaptive Multi-source Knowledge-Oriented Reasoning (AMKOR), a generative framework that leverages large language models (LLMs) to dynamically fuse parametric and retrieved knowledge while exploring reasoning trajectories using probabilistic beam reasoning. AMKOR is further enhanced by a multi-granular learning strategy, optimizing both local reasoning steps and global answer accuracy. Experiments conducted on four widely-used multi-hop QA datasets, including HotpotQA and MuSiQue, demonstrate that AMKOR achieves state-of-the-art performance, significantly outperforming baseline methods on both reasoning accuracy and robustness. Additional analyses confirm its scalability, adaptability to noisy knowledge, and superior ability to handle complex multi-hop tasks. This work establishes a new benchmark for multi-source multi-hop QA by effectively combining reasoning quality and efficiency.

### A Semi-Supervised Text Generation Framework Combining a Deep Transformer and a GAN 
[[arxiv](https://arxiv.org/abs/2502.05937)] [[cool](https://papers.cool/arxiv/2502.05937)] [[pdf](https://arxiv.org/pdf/2502.05937)]
> **Authors**: Shengquan Wang
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: 7 pages
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: This paper introduces a framework that connects a deep generative pre-trained Transformer language model with a generative adversarial network for semi-supervised text generation. In other words, the proposed model is first pre-trained unsupervised on a large and diverse text corpus with 24 layers. Then a simple GAN architecture for synthetic text generation is introduced, and Gumbel-Softmax is applied to handle the discreteness of tokens. The paper also shows a semi-supervised approach where real data is augmented with GAN samples, which is further used to fine-tune the Transformer model on the merged dataset. Detailed theoretical derivations are also included, outlining the proof of the min-max objective function, and an extensive discussion of the Gumbel-Softmax reparameterization trick.

### Learning to Substitute Words with Model-based Score Ranking 
[[arxiv](https://arxiv.org/abs/2502.05933)] [[cool](https://papers.cool/arxiv/2502.05933)] [[pdf](https://arxiv.org/pdf/2502.05933)]
> **Authors**: Hongye Liu,Ricardo Henao
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: Accepted at NAACL 2025 (main, long)
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Smart word substitution aims to enhance sentence quality by improving word choices; however current benchmarks rely on human-labeled data. Since word choices are inherently subjective, ground-truth word substitutions generated by a small group of annotators are often incomplete and likely not generalizable. To circumvent this issue, we instead employ a model-based score (BARTScore) to quantify sentence quality, thus forgoing the need for human annotations. Specifically, we use this score to define a distribution for each word substitution, allowing one to test whether a substitution is statistically superior relative to others. In addition, we propose a loss function that directly optimizes the alignment between model predictions and sentence scores, while also enhancing the overall quality score of a substitution. Crucially, model learning no longer requires human labels, thus avoiding the cost of annotation while maintaining the quality of the text modified with substitutions. Experimental results show that the proposed approach outperforms both masked language models (BERT, BART) and large language models (GPT-4, LLaMA). The source code is available at https://github.com/Hyfred/Substitute-Words-with-Ranking.

### ARISE: Iterative Rule Induction and Synthetic Data Generation for Text Classification 
[[arxiv](https://arxiv.org/abs/2502.05923)] [[cool](https://papers.cool/arxiv/2502.05923)] [[pdf](https://arxiv.org/pdf/2502.05923)]
> **Authors**: Yashwanth M.,Vaibhav Singh,Ayush Maheshwari,Amrith Krishna,Ganesh Ramakrishnan
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: Accepted to Findings of NAACL 2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: We propose ARISE, a framework that iteratively induces rules and generates synthetic data for text classification. We combine synthetic data generation and automatic rule induction, via bootstrapping, to iteratively filter the generated rules and data. We induce rules via inductive generalisation of syntactic n-grams, enabling us to capture a complementary source of supervision. These rules alone lead to performance gains in both, in-context learning (ICL) and fine-tuning (FT) settings. Similarly, use of augmented data from ARISE alone improves the performance for a model, outperforming configurations that rely on complex methods like contrastive learning. Further, our extensive experiments on various datasets covering three full-shot, eight few-shot and seven multilingual variant settings demonstrate that the rules and data we generate lead to performance improvements across these diverse domains and languages.

### GRAIT: Gradient-Driven Refusal-Aware Instruction Tuning for Effective Hallucination Mitigation 
[[arxiv](https://arxiv.org/abs/2502.05911)] [[cool](https://papers.cool/arxiv/2502.05911)] [[pdf](https://arxiv.org/pdf/2502.05911)]
> **Authors**: Runchuan Zhu,Zinco Jiang,Jiang Wu,Zhipeng Ma,Jiahe Song,Fengshuo Bai,Dahua Lin,Lijun Wu,Conghui He
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: Equal contribution: Runchuan Zhu, Zinco Jiang, Jiang Wu; Corresponding author: Conghui He
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Refusal-Aware Instruction Tuning (RAIT) aims to enhance Large Language Models (LLMs) by improving their ability to refuse responses to questions beyond their knowledge, thereby reducing hallucinations and improving reliability. Effective RAIT must address two key challenges: firstly, effectively reject unknown questions to minimize hallucinations; secondly, avoid over-refusal to ensure questions that can be correctly answered are not rejected, thereby maintain the helpfulness of LLM outputs. In this paper, we address the two challenges by deriving insightful observations from the gradient-based perspective, and proposing the Gradient-driven Refusal Aware Instruction Tuning Framework GRAIT: (1) employs gradient-driven sample selection to effectively minimize hallucinations and (2) introduces an adaptive weighting mechanism during fine-tuning to reduce the risk of over-refusal, achieving the balance between accurate refusals and maintaining useful responses. Experimental evaluations on open-ended and multiple-choice question answering tasks demonstrate that GRAIT significantly outperforms existing RAIT methods in the overall performance. The source code and data will be available at https://github.com/opendatalab/GRAIT .

### A Distributional Perspective on Word Learning in Neural Language Models 
[[arxiv](https://arxiv.org/abs/2502.05892)] [[cool](https://papers.cool/arxiv/2502.05892)] [[pdf](https://arxiv.org/pdf/2502.05892)]
> **Authors**: Filippo Ficarra,Ryan Cotterell,Alex Warstadt
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Language models (LMs) are increasingly being studied as models of human language learners. Due to the nascency of the field, it is not well-established whether LMs exhibit similar learning dynamics to humans, and there are few direct comparisons between learning trajectories in humans and models. Word learning trajectories for children are relatively well-documented, and recent work has tried to extend these investigations to language models. However, there are no widely agreed-upon metrics for word learning in language models. We take a distributional approach to this problem, defining lexical knowledge in terms of properties of the learned distribution for a target word. We argue that distributional signatures studied in prior work fail to capture key distributional information. Thus, we propose an array of signatures that improve on earlier approaches by capturing knowledge of both where the target word can and cannot occur as well as gradient preferences about the word's appropriateness. We obtain learning trajectories for a selection of small language models we train from scratch, study the relationship between different distributional signatures, compare how well they align with human word learning trajectories and interpretable lexical features, and address basic methodological questions about estimating these distributional signatures. Our metrics largely capture complementary information, suggesting that it is important not to rely on a single metric. However, across all metrics, language models' learning trajectories fail to correlate with those of children.

### Enhancing Depression Detection with Chain-of-Thought Prompting: From Emotion to Reasoning Using Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.05879)] [[cool](https://papers.cool/arxiv/2502.05879)] [[pdf](https://arxiv.org/pdf/2502.05879)]
> **Authors**: Shiyu Teng,Jiaqing Liu,Rahul Kumar Jain,Shurong Chai,Ruibo Hou,Tomoko Tateyama,Lanfen Lin,Yen-wei Chen
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Depression is one of the leading causes of disability worldwide, posing a severe burden on individuals, healthcare systems, and society at large. Recent advancements in Large Language Models (LLMs) have shown promise in addressing mental health challenges, including the detection of depression through text-based analysis. However, current LLM-based methods often struggle with nuanced symptom identification and lack a transparent, step-by-step reasoning process, making it difficult to accurately classify and explain mental health conditions. To address these challenges, we propose a Chain-of-Thought Prompting approach that enhances both the performance and interpretability of LLM-based depression detection. Our method breaks down the detection process into four stages: (1) sentiment analysis, (2) binary depression classification, (3) identification of underlying causes, and (4) assessment of severity. By guiding the model through these structured reasoning steps, we improve interpretability and reduce the risk of overlooking subtle clinical indicators. We validate our method on the E-DAIC dataset, where we test multiple state-of-the-art large language models. Experimental results indicate that our Chain-of-Thought Prompting technique yields superior performance in both classification accuracy and the granularity of diagnostic insights, compared to baseline approaches.

### Enhancing Financial Time-Series Forecasting with Retrieval-Augmented Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.05878)] [[cool](https://papers.cool/arxiv/2502.05878)] [[pdf](https://arxiv.org/pdf/2502.05878)]
> **Authors**: Mengxi Xiao,Zihao Jiang,Lingfei Qian,Zhengyu Chen,Yueru He,Yijing Xu,Yuecheng Jiang,Dong Li,Ruey-Ling Weng,Min Peng,Jimin Huang,Sophia Ananiadou,Qianqian Xie
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: 11 pages, 4 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Stock movement prediction, a critical task in financial time-series forecasting, relies on identifying and retrieving key influencing factors from vast and complex datasets. However, traditional text-trained or numeric similarity-based retrieval methods often struggle to handle the intricacies of financial data. To address this, we propose the first retrieval-augmented generation (RAG) framework specifically designed for financial time-series forecasting. Our framework incorporates three key innovations: a fine-tuned 1B large language model (StockLLM) as its backbone, a novel candidate selection method enhanced by LLM feedback, and a training objective that maximizes the similarity between queries and historically significant sequences. These advancements enable our retriever, FinSeer, to uncover meaningful patterns while effectively minimizing noise in complex financial datasets. To support robust evaluation, we also construct new datasets that integrate financial indicators and historical stock prices. Experimental results demonstrate that our RAG framework outperforms both the baseline StockLLM and random retrieval methods, showcasing its effectiveness. FinSeer, as the retriever, achieves an 8% higher accuracy on the BIGDATA22 benchmark and retrieves more impactful sequences compared to existing retrieval methods. This work highlights the importance of tailored retrieval models in financial forecasting and provides a novel, scalable framework for future research in the field.

### Self-Training Large Language Models for Tool-Use Without Demonstrations 
[[arxiv](https://arxiv.org/abs/2502.05867)] [[cool](https://papers.cool/arxiv/2502.05867)] [[pdf](https://arxiv.org/pdf/2502.05867)]
> **Authors**: Ne Luo,Aryo Pradipta Gema,Xuanli He,Emile van Krieken,Pietro Lesci,Pasquale Minervini
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) remain prone to factual inaccuracies and computational errors, including hallucinations and mistakes in mathematical reasoning. Recent work augmented LLMs with tools to mitigate these shortcomings, but often requires curated gold tool-use demonstrations. In this paper, we investigate whether LLMs can learn to use tools without demonstrations. First, we analyse zero-shot prompting strategies to guide LLMs in tool utilisation. Second, we propose a self-training method to synthesise tool-use traces using the LLM itself. We compare supervised fine-tuning and preference fine-tuning techniques for fine-tuning the model on datasets constructed using existing Question Answering (QA) datasets, i.e., TriviaQA and GSM8K. Experiments show that tool-use enhances performance on a long-tail knowledge task: 3.7% on PopQA, which is used solely for evaluation, but leads to mixed results on other datasets, i.e., TriviaQA, GSM8K, and NQ-Open. Our findings highlight the potential and challenges of integrating external tools into LLMs without demonstrations.

### Fact-or-Fair: A Checklist for Behavioral Testing of AI Models on Fairness-Related Queries 
[[arxiv](https://arxiv.org/abs/2502.05849)] [[cool](https://papers.cool/arxiv/2502.05849)] [[pdf](https://arxiv.org/pdf/2502.05849)]
> **Authors**: Jen-tse Huang,Yuhang Yan,Linqi Liu,Yixin Wan,Wenxuan Wang,Kai-Wei Chang,Michael R. Lyu
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: 8 pages of main text; 7 pages of appendices;
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The generation of incorrect images, such as depictions of people of color in Nazi-era uniforms by Gemini, frustrated users and harmed Google's reputation, motivating us to investigate the relationship between accurately reflecting factuality and promoting diversity and equity. In this study, we focus on 19 real-world statistics collected from authoritative sources. Using these statistics, we develop a checklist comprising objective and subjective queries to analyze behavior of large language models (LLMs) and text-to-image (T2I) models. Objective queries assess the models' ability to provide accurate world knowledge. In contrast, the design of subjective queries follows a key principle: statistical or experiential priors should not be overgeneralized to individuals, ensuring that models uphold diversity. These subjective queries are based on three common human cognitive errors that often result in social biases. We propose metrics to assess factuality and fairness, and formally prove the inherent trade-off between these two aspects. Results show that GPT-4o and DALL-E 3 perform notably well among six LLMs and four T2I models. Our code is publicly available at https://github.com/uclanlp/Fact-or-Fair.

### LegalSeg: Unlocking the Structure of Indian Legal Judgments Through Rhetorical Role Classification 
[[arxiv](https://arxiv.org/abs/2502.05836)] [[cool](https://papers.cool/arxiv/2502.05836)] [[pdf](https://arxiv.org/pdf/2502.05836)]
> **Authors**: Shubham Kumar Nigam,Tanmay Dubey,Govind Sharma,Noel Shallum,Kripabandhu Ghosh,Arnab Bhattacharya
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: Accepted on NAACL 2025
- **标题**: None
- **领域**: 计算语言学,人工智能,信息检索,机器学习
- **Abstract**: In this paper, we address the task of semantic segmentation of legal documents through rhetorical role classification, with a focus on Indian legal judgments. We introduce LegalSeg, the largest annotated dataset for this task, comprising over 7,000 documents and 1.4 million sentences, labeled with 7 rhetorical roles. To benchmark performance, we evaluate multiple state-of-the-art models, including Hierarchical BiLSTM-CRF, TransformerOverInLegalBERT (ToInLegalBERT), Graph Neural Networks (GNNs), and Role-Aware Transformers, alongside an exploratory RhetoricLLaMA, an instruction-tuned large language model. Our results demonstrate that models incorporating broader context, structural relationships, and sequential sentence information outperform those relying solely on sentence-level features. Additionally, we conducted experiments using surrounding context and predicted or actual labels of neighboring sentences to assess their impact on classification accuracy. Despite these advancements, challenges persist in distinguishing between closely related roles and addressing class imbalance. Our work underscores the potential of advanced techniques for improving legal document understanding and sets a strong foundation for future research in legal NLP.

### Delta -- Contrastive Decoding Mitigates Text Hallucinations in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.05825)] [[cool](https://papers.cool/arxiv/2502.05825)] [[pdf](https://arxiv.org/pdf/2502.05825)]
> **Authors**: Cheng Peng Huang,Hao-Yuan Chen
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large language models (LLMs) demonstrate strong capabilities in natural language processing but remain prone to hallucinations, generating factually incorrect or fabricated content. This issue undermines their reliability, particularly in high-stakes domains such as healthcare and legal advisory. To address this challenge, we propose Delta, an inference-time method that reduces hallucinations without requiring model retraining or additional data. Delta works by randomly masking parts of the input prompt and contrasting the output distributions for the original and masked inputs, effectively suppressing hallucinations through inference-only computations. We evaluate Delta on context-rich question-answering benchmarks, achieving absolute improvements of approximately 3 and 6 percentage points on SQuAD v1.1 and v2, respectively, and 7 and 2 percentage points on TriviaQA and Natural Questions under-sampling decoding. Delta also improves the no-answer exact match score on SQuAD v2 by over ten percentage points, demonstrating its effectiveness in mitigating hallucinations arising from contextual ambiguity. These results highlight Delta as a computationally efficient and scalable approach for improving the reliability of LLMs in real-world applications.

### Structural Perturbation in Large Language Model Representations through Recursive Symbolic Regeneration 
[[arxiv](https://arxiv.org/abs/2502.05794)] [[cool](https://papers.cool/arxiv/2502.05794)] [[pdf](https://arxiv.org/pdf/2502.05794)]
> **Authors**: Kathlyn Eaglewood,Tobias Featherington,Dorian Mayfair,Sylvester Grimshaw,James Pettigrew
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Symbolic perturbations offer a novel approach for influencing neural representations without requiring direct modification of model parameters. The recursive regeneration of symbolic structures introduces structured variations in latent embeddings, leading to controlled shifts in attention dynamics and lexical diversity across sequential generations. A comparative analysis with conventional fine-tuning techniques reveals that structural modifications at the symbolic level induce distinct variations in contextual sensitivity while maintaining overall model fluency and coherence. Shifts in attention weight distributions highlight the role of symbolic modifications in adjusting token dependencies, influencing response variability, and refining long-form text generation. Experimental findings suggest that symbolic perturbations can enhance adaptability in domain-specific applications, allowing modifications in model behavior without retraining. Evaluations of semantic drift indicate that recursive regeneration alters long-range token dependencies, affecting topic coherence across extended text sequences. Results from lexical variability assessments further support the conclusion that symbolic-level modifications introduce interpretable variations in generated responses, potentially enabling more controlled stylistic adjustments in automated text generation.

### On Reference (In-)Determinacy in Natural Language Inference 
[[arxiv](https://arxiv.org/abs/2502.05793)] [[cool](https://papers.cool/arxiv/2502.05793)] [[pdf](https://arxiv.org/pdf/2502.05793)]
> **Authors**: Sihao Chen,Chaitanya Malaviya,Alex Fabrikant,Hagai Taitelbaum,Tal Schuster,Senaka Buthpitiya,Dan Roth
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: NAACL 2025 Findings
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: We revisit the reference determinacy (RD) assumption in the task of natural language inference (NLI), i.e., the premise and hypothesis are assumed to refer to the same context when human raters annotate a label. While RD is a practical assumption for constructing a new NLI dataset, we observe that current NLI models, which are typically trained solely on hypothesis-premise pairs created with the RD assumption, fail in downstream applications such as fact verification, where the input premise and hypothesis may refer to different contexts. To highlight the impact of this phenomenon in real-world use cases, we introduce RefNLI, a diagnostic benchmark for identifying reference ambiguity in NLI examples. In RefNLI, the premise is retrieved from a knowledge source (i.e., Wikipedia) and does not necessarily refer to the same context as the hypothesis. With RefNLI, we demonstrate that finetuned NLI models and few-shot prompted LLMs both fail to recognize context mismatch, leading to over 80% false contradiction and over 50% entailment predictions. We discover that the existence of reference ambiguity in NLI examples can in part explain the inherent human disagreements in NLI and provide insight into how the RD assumption impacts the NLI dataset creation process.

### Reinforced Lifelong Editing for Language Models 
[[arxiv](https://arxiv.org/abs/2502.05759)] [[cool](https://papers.cool/arxiv/2502.05759)] [[pdf](https://arxiv.org/pdf/2502.05759)]
> **Authors**: Zherui Li,Houcheng Jiang,Hao Chen,Baolong Bi,Zhenhong Zhou,Fei Sun,Junfeng Fang,Xiang Wang
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) acquire information from pre-training corpora, but their stored knowledge can become inaccurate or outdated over time. Model editing addresses this challenge by modifying model parameters without retraining, and prevalent approaches leverage hypernetworks to generate these parameter updates. However, they face significant challenges in lifelong editing due to their incompatibility with LLM parameters that dynamically change during the editing process. To address this, we observed that hypernetwork-based lifelong editing aligns with reinforcement learning modeling and proposed RLEdit, an RL-based editing method. By treating editing losses as rewards and optimizing hypernetwork parameters at the full knowledge sequence level, we enable it to precisely capture LLM changes and generate appropriate parameter updates. Our extensive empirical evaluation across several LLMs demonstrates that RLEdit outperforms existing methods in lifelong editing with superior effectiveness and efficiency, achieving a 59.24% improvement while requiring only 2.11% of the time compared to most approaches. Our code is available at: https://github.com/zhrli324/RLEdit.

### Rethinking Word Similarity: Semantic Similarity through Classification Confusion 
[[arxiv](https://arxiv.org/abs/2502.05704)] [[cool](https://papers.cool/arxiv/2502.05704)] [[pdf](https://arxiv.org/pdf/2502.05704)]
> **Authors**: Kaitlyn Zhou,Haishan Gao,Sarah Chen,Dan Edelstein,Dan Jurafsky,Chen Shani
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: Accepted to NAACL-main-2025
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Word similarity has many applications to social science and cultural analytics tasks like measuring meaning change over time and making sense of contested terms. Yet traditional similarity methods based on cosine similarity between word embeddings cannot capture the context-dependent, asymmetrical, polysemous nature of semantic similarity. We propose a new measure of similarity, Word Confusion, that reframes semantic similarity in terms of feature-based classification confusion. Word Confusion is inspired by Tversky's suggestion that similarity features be chosen dynamically. Here we train a classifier to map contextual embeddings to word identities and use the classifier confusion (the probability of choosing a confounding word c instead of the correct target word t) as a measure of the similarity of c and t. The set of potential confounding words acts as the chosen features. Our method is comparable to cosine similarity in matching human similarity judgments across several datasets (MEN, WirdSim353, and SimLex), and can measure similarity using predetermined features of interest. We demonstrate our model's ability to make use of dynamic features by applying it to test a hypothesis about changes in the 18th C. meaning of the French word "revolution" from popular to state action during the French Revolution. We hope this reimagining of semantic similarity will inspire the development of new tools that better capture the multi-faceted and dynamic nature of language, advancing the fields of computational social science and cultural analytics and beyond.

### Zero-Shot End-to-End Relation Extraction in Chinese: A Comparative Study of Gemini, LLaMA and ChatGPT 
[[arxiv](https://arxiv.org/abs/2502.05694)] [[cool](https://papers.cool/arxiv/2502.05694)] [[pdf](https://arxiv.org/pdf/2502.05694)]
> **Authors**: Shaoshuai Du,Yiyi Tao,Yixian Shen,Hang Zhang,Yanxin Shen,Xinyu Qiu,Chuanqi Shi
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: This study investigates the performance of various large language models (LLMs) on zero-shot end-to-end relation extraction (RE) in Chinese, a task that integrates entity recognition and relation extraction without requiring annotated data. While LLMs show promise for RE, most prior work focuses on English or assumes pre-annotated entities, leaving their effectiveness in Chinese RE largely unexplored. To bridge this gap, we evaluate ChatGPT, Gemini, and LLaMA based on accuracy, efficiency, and adaptability. ChatGPT demonstrates the highest overall performance, balancing precision and recall, while Gemini achieves the fastest inference speed, making it suitable for real-time applications. LLaMA underperforms in both accuracy and latency, highlighting the need for further adaptation. Our findings provide insights into the strengths and limitations of LLMs for zero-shot Chinese RE, shedding light on trade-offs between accuracy and efficiency. This study serves as a foundation for future research aimed at improving LLM adaptability to complex linguistic tasks in Chinese NLP.

### Investigating the Shortcomings of LLMs in Step-by-Step Legal Reasoning 
[[arxiv](https://arxiv.org/abs/2502.05675)] [[cool](https://papers.cool/arxiv/2502.05675)] [[pdf](https://arxiv.org/pdf/2502.05675)]
> **Authors**: Venkatesh Mishra,Bimsara Pathiraja,Mihir Parmar,Sat Chidananda,Jayanth Srinivasa,Gaowen Liu,Ali Payani,Chitta Baral
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: Accepted to NAACL 2025 Findings
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Reasoning abilities of LLMs have been a key focus in recent years. One challenging reasoning domain with interesting nuances is legal reasoning, which requires careful application of rules, and precedents while balancing deductive and analogical reasoning, and conflicts between rules. Although there have been a few works on using LLMs for legal reasoning, their focus has been on overall accuracy. In this paper, we dig deeper to do a step-by-step analysis and figure out where they commit errors. We use the college-level Multiple Choice Question-Answering (MCQA) task from the \textit{Civil Procedure} dataset and propose a new error taxonomy derived from initial manual analysis of reasoning chains with respect to several LLMs, including two objective measures: soundness and correctness scores. We then develop an LLM-based automated evaluation framework to identify reasoning errors and evaluate the performance of LLMs. The computation of soundness and correctness on the dataset using the auto-evaluator framework reveals several interesting insights. Furthermore, we show that incorporating the error taxonomy as feedback in popular prompting techniques marginally increases LLM performance. Our work will also serve as an evaluation framework that can be used in detailed error analysis of reasoning chains for logic-intensive complex tasks.

### Language Models Largely Exhibit Human-like Constituent Ordering Preferences 
[[arxiv](https://arxiv.org/abs/2502.05670)] [[cool](https://papers.cool/arxiv/2502.05670)] [[pdf](https://arxiv.org/pdf/2502.05670)]
> **Authors**: Ada Defne Tur,Gaurav Kamath,Siva Reddy
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: NAACL 2025 Main Conference
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Though English sentences are typically inflexible vis-à-vis word order, constituents often show far more variability in ordering. One prominent theory presents the notion that constituent ordering is directly correlated with constituent weight: a measure of the constituent's length or complexity. Such theories are interesting in the context of natural language processing (NLP), because while recent advances in NLP have led to significant gains in the performance of large language models (LLMs), much remains unclear about how these models process language, and how this compares to human language processing. In particular, the question remains whether LLMs display the same patterns with constituent movement, and may provide insights into existing theories on when and how the shift occurs in human language. We compare a variety of LLMs with diverse properties to evaluate broad LLM performance on four types of constituent movement: heavy NP shift, particle movement, dative alternation, and multiple PPs. Despite performing unexpectedly around particle movement, LLMs generally align with human preferences around constituent ordering.

### CODESIM: Multi-Agent Code Generation and Problem Solving through Simulation-Driven Planning and Debugging 
[[arxiv](https://arxiv.org/abs/2502.05664)] [[cool](https://papers.cool/arxiv/2502.05664)] [[pdf](https://arxiv.org/pdf/2502.05664)]
> **Authors**: Md. Ashraful Islam,Mohammed Eunus Ali,Md Rizwan Parvez
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: Accepted in NAACL 2025 Findings
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large Language Models (LLMs) have made significant strides in code generation and problem solving. Current approaches employ external tool-based iterative debuggers that use compiler or other tool-based runtime feedback to refine coarse programs generated by various methods. However, the effectiveness of these approaches heavily relies on the quality of the initial code generation, which remains an open challenge. In this paper, we introduce CodeSim, a novel multi-agent code generation framework that comprehensively addresses the stages of program synthesis-planning, coding, and debugging-through a human-like perception approach. As human verifies their understanding of any algorithms through visual simulation, CodeSim uniquely features a method of plan verification and internal debugging through the step-by-step simulation of input/output. Extensive experiments across seven challenging competitive problem-solving and program synthesis benchmarks demonstrate CodeSim's remarkable code generation capabilities. Our framework achieves new state-of-the-art (pass@1) results-(HumanEval 95.1%, MBPP 90.7%, APPS 22%, and CodeContests 29.1%). Furthermore, our method shows potential for even greater enhancement when cascaded with external debuggers. To facilitate further research and development in this area, we have open-sourced our framework in this link (https://kagnlp.github.io/codesim.github.io/).

### KMI: A Dataset of Korean Motivational Interviewing Dialogues for Psychotherapy 
[[arxiv](https://arxiv.org/abs/2502.05651)] [[cool](https://papers.cool/arxiv/2502.05651)] [[pdf](https://arxiv.org/pdf/2502.05651)]
> **Authors**: Hyunjong Kim,Suyeon Lee,Yeongjae Cho,Eunseo Ryu,Yohan Jo,Suran Seong,Sungzoon Cho
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: Accepted at NAACL 2025 Main Conference
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: The increasing demand for mental health services has led to the rise of AI-driven mental health chatbots, though challenges related to privacy, data collection, and expertise persist. Motivational Interviewing (MI) is gaining attention as a theoretical basis for boosting expertise in the development of these chatbots. However, existing datasets are showing limitations for training chatbots, leading to a substantial demand for publicly available resources in the field of MI and psychotherapy. These challenges are even more pronounced in non-English languages, where they receive less attention. In this paper, we propose a novel framework that simulates MI sessions enriched with the expertise of professional therapists. We train an MI forecaster model that mimics the behavioral choices of professional therapists and employ Large Language Models (LLMs) to generate utterances through prompt engineering. Then, we present KMI, the first synthetic dataset theoretically grounded in MI, containing 1,000 high-quality Korean Motivational Interviewing dialogues. Through an extensive expert evaluation of the generated dataset and the dialogue model trained on it, we demonstrate the quality, expertise, and practicality of KMI. We also introduce novel metrics derived from MI theory in order to evaluate dialogues from the perspective of MI.

### Incongruence Identification in Eyewitness Testimony 
[[arxiv](https://arxiv.org/abs/2502.05650)] [[cool](https://papers.cool/arxiv/2502.05650)] [[pdf](https://arxiv.org/pdf/2502.05650)]
> **Authors**: Akshara Nair,Zeba Afroz,Md Shad Akhtar
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: 9 pages,10 tables. Under review at ACL ARR 2024. Includes supplementary appendix with detailed evaluation results
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Incongruence detection in eyewitness narratives is critical for understanding the reliability of testimonies, yet traditional approaches often fail to address the nuanced inconsistencies inherent in such accounts. In this paper, we introduce a novel task of incongruence detection in eyewitness testimonies. Given a pair of testimonies containing of multiple pairs of question and answer by two subjects, we identify contextually related incongruence between the two subjects. We also mark the span of incongruences in the utterances. To achieve this, we developed MIND(MultI-EyewitNess Deception) - a comprehensive dataset consisting of 2927 pairs of contextually related answers designed to capture both explicit and implicit contradictions. INstruction - TunEd iNcongruity Detection framework based on 6W and multi-hop reasoning approach, aka. INTEND. Drawing from investigative techniques, INTEND address the task as a close-style problem, contradicting on the who, what, when, where and why aspect of the content. Our findings shows that prompt tuning, especially when utilizing our framework, enhances the detection of incongruences by a margin of +5.63 percent. We compare our approach with multiple fine-tuning and prompt tuning techniques on MLMs and LLMs. Emperical results demonstrate convincing performance improvement in F1-score over fine-tuned and regular prompt-tuning techniques, highlighting the effectiveness of our approach.

### Gender Bias in Instruction-Guided Speech Synthesis Models 
[[arxiv](https://arxiv.org/abs/2502.05649)] [[cool](https://papers.cool/arxiv/2502.05649)] [[pdf](https://arxiv.org/pdf/2502.05649)]
> **Authors**: Chun-Yi Kuan,Hung-yi Lee
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: NAACL 2025 Findings
- **标题**: None
- **领域**: 计算语言学,机器学习,音频和语音处理
- **Abstract**: Recent advancements in controllable expressive speech synthesis, especially in text-to-speech (TTS) models, have allowed for the generation of speech with specific styles guided by textual descriptions, known as style prompts. While this development enhances the flexibility and naturalness of synthesized speech, there remains a significant gap in understanding how these models handle vague or abstract style prompts. This study investigates the potential gender bias in how models interpret occupation-related prompts, specifically examining their responses to instructions like "Act like a nurse". We explore whether these models exhibit tendencies to amplify gender stereotypes when interpreting such prompts. Our experimental results reveal the model's tendency to exhibit gender bias for certain occupations. Moreover, models of different sizes show varying degrees of this bias across these occupations.

### ELMTEX: Fine-Tuning Large Language Models for Structured Clinical Information Extraction. A Case Study on Clinical Reports 
[[arxiv](https://arxiv.org/abs/2502.05638)] [[cool](https://papers.cool/arxiv/2502.05638)] [[pdf](https://arxiv.org/pdf/2502.05638)]
> **Authors**: Aynur Guluzade,Naguib Heiba,Zeyd Boukhers,Florim Hamiti,Jahid Hasan Polash,Yehya Mohamad,Carlos A Velasco
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: :I.2.6; I.2.7; J.3
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Europe's healthcare systems require enhanced interoperability and digitalization, driving a demand for innovative solutions to process legacy clinical data. This paper presents the results of our project, which aims to leverage Large Language Models (LLMs) to extract structured information from unstructured clinical reports, focusing on patient history, diagnoses, treatments, and other predefined categories. We developed a workflow with a user interface and evaluated LLMs of varying sizes through prompting strategies and fine-tuning. Our results show that fine-tuned smaller models match or surpass larger counterparts in performance, offering efficiency for resource-limited settings. A new dataset of 60,000 annotated English clinical summaries and 24,000 German translations was validated with automated and manual checks. The evaluations used ROUGE, BERTScore, and entity-level metrics. The work highlights the approach's viability and outlines future improvements.

### AnyEdit: Edit Any Knowledge Encoded in Language Models 
[[arxiv](https://arxiv.org/abs/2502.05628)] [[cool](https://papers.cool/arxiv/2502.05628)] [[pdf](https://arxiv.org/pdf/2502.05628)]
> **Authors**: Houcheng Jiang,Junfeng Fang,Ningyu Zhang,Guojun Ma,Mingyang Wan,Xiang Wang,Xiangnan He,Tat-seng Chua
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) often produce incorrect or outdated information, necessitating efficient and precise knowledge updates. Current model editing methods, however, struggle with long-form knowledge in diverse formats, such as poetry, code snippets, and mathematical derivations. These limitations arise from their reliance on editing a single token's hidden state, a limitation we term "efficacy barrier". To solve this, we propose AnyEdit, a new autoregressive editing paradigm. It decomposes long-form knowledge into sequential chunks and iteratively edits the key token in each chunk, ensuring consistent and accurate outputs. Theoretically, we ground AnyEdit in the Chain Rule of Mutual Information, showing its ability to update any knowledge within LLMs. Empirically, it outperforms strong baselines by 21.5% on benchmarks including UnKEBench, AKEW, and our new EditEverything dataset for long-form diverse-formatted knowledge. Additionally, AnyEdit serves as a plug-and-play framework, enabling current editing methods to update knowledge with arbitrary length and format, significantly advancing the scope and practicality of LLM knowledge editing.

### Towards Sustainable NLP: Insights from Benchmarking Inference Energy in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.05610)] [[cool](https://papers.cool/arxiv/2502.05610)] [[pdf](https://arxiv.org/pdf/2502.05610)]
> **Authors**: Soham Poddar,Paramita Koley,Janardan Misra,Niloy Ganguly,Saptarshi Ghosh
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: Accepted to appear at the NAACL 2025 Main Conference
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) are increasingly recognized for their exceptional generative capabilities and versatility across various tasks. However, the high inference costs associated with these models have not received adequate attention, particularly when compared to the focus on training costs in existing research. In response to this gap, our study conducts a comprehensive benchmarking of LLM inference energy across a wide range of NLP tasks, where we analyze the impact of different models, tasks, prompts, and system-related factors on inference energy. Specifically, our experiments reveal several interesting insights, including strong correlation of inference energy with output token length and response time. Also, we find that quantization and optimal batch sizes, along with targeted prompt phrases, can significantly reduce energy usage. This study is the first to thoroughly benchmark LLM inference across such a diverse range of aspects, providing insights and offering several recommendations for improving energy efficiency in model deployment.

### Lossless Acceleration of Large Language Models with Hierarchical Drafting based on Temporal Locality in Speculative Decoding 
[[arxiv](https://arxiv.org/abs/2502.05609)] [[cool](https://papers.cool/arxiv/2502.05609)] [[pdf](https://arxiv.org/pdf/2502.05609)]
> **Authors**: Sukmin Cho,Sangjin Choi,Taeho Hwang,Jeongyeon Seo,Soyeong Jeong,Huije Lee,Hoyun Song,Jong C. Park,Youngjin Kwon
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: Findings of NAACL 2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Accelerating inference in Large Language Models (LLMs) is critical for real-time interactions, as they have been widely incorporated into real-world services. Speculative decoding, a fully algorithmic solution, has gained attention for improving inference speed by drafting and verifying tokens, thereby generating multiple tokens in a single forward pass. However, current drafting strategies usually require significant fine-tuning or have inconsistent performance across tasks. To address these challenges, we propose Hierarchy Drafting (HD), a novel lossless drafting approach that organizes various token sources into multiple databases in a hierarchical framework based on temporal locality. In the drafting step, HD sequentially accesses multiple databases to obtain draft tokens from the highest to the lowest locality, ensuring consistent acceleration across diverse tasks and minimizing drafting latency. Our experiments on Spec-Bench using LLMs with 7B and 13B parameters demonstrate that HD outperforms existing database drafting methods, achieving robust inference speedups across model sizes, tasks, and temperatures.

### ARIES: Stimulating Self-Refinement of Large Language Models by Iterative Preference Optimization 
[[arxiv](https://arxiv.org/abs/2502.05605)] [[cool](https://papers.cool/arxiv/2502.05605)] [[pdf](https://arxiv.org/pdf/2502.05605)]
> **Authors**: Yongcheng Zeng,Xinyu Cui,Xuanfa Jin,Guoqing Liu,Zexu Sun,Quan He,Dong Li,Ning Yang,Jianye Hao,Haifeng Zhang,Jun Wang
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: A truly intelligent Large Language Model (LLM) should be capable of correcting errors in its responses through external interactions. However, even the most advanced models often face challenges in improving their outputs. In this paper, we explore how to cultivate LLMs with the self-refinement capability through iterative preference training, and how this ability can be leveraged to improve model performance during inference. To this end, we introduce a novel post-training and inference framework, called ARIES: Adaptive Refinement and Iterative Enhancement Structure. This method iteratively performs preference training and self-refinement-based data collection. During training, ARIES strengthen the model's direct question-answering capability while simultaneously unlocking its self-refinement potential. During inference, ARIES harnesses this self-refinement capability to generate a series of progressively refined responses, which are then filtered using either the Reward Model Scoring or a simple yet effective Rule-Based Selection mechanism, specifically tailored to our approach, to construct a dataset for the next round of preference training. Experimental results demonstrate the remarkable performance of ARIES. When applied to the Llama-3.1-8B model and under the self-refinement setting, ARIES surpasses powerful models such as GPT-4o, achieving 62.3% length-controlled (LC) and a 63.3% raw win rates on AlpacaEval 2, outperforming Iterative DPO by 27.8% and 35.5% respectively, as well as a 50.3% win rate on Arena-Hard, surpassing Iterative DPO by 26.6%. Furthermore, ARIES consistently enhances performance on mathematical reasoning tasks like GSM8K and MATH.

### On Memory Construction and Retrieval for Personalized Conversational Agents 
[[arxiv](https://arxiv.org/abs/2502.05589)] [[cool](https://papers.cool/arxiv/2502.05589)] [[pdf](https://arxiv.org/pdf/2502.05589)]
> **Authors**: Zhuoshi Pan,Qianhui Wu,Huiqiang Jiang,Xufang Luo,Hao Cheng,Dongsheng Li,Yuqing Yang,Chin-Yew Lin,H. Vicky Zhao,Lili Qiu,Jianfeng Gao
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: 10 pages, 5 figures, conference
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: To deliver coherent and personalized experiences in long-term conversations, existing approaches typically perform retrieval augmented response generation by constructing memory banks from conversation history at either the turn-level, session-level, or through summarization techniques. In this paper, we present two key findings: (1) The granularity of memory unit matters: Turn-level, session-level, and summarization-based methods each exhibit limitations in both memory retrieval accuracy and the semantic quality of the retrieved content. (2) Prompt compression methods, such as \textit{LLMLingua-2}, can effectively serve as a denoising mechanism, enhancing memory retrieval accuracy across different granularities. Building on these insights, we propose SeCom, a method that constructs a memory bank with topical segments by introducing a conversation Segmentation model, while performing memory retrieval based on Compressed memory units. Experimental results show that SeCom outperforms turn-level, session-level, and several summarization-based methods on long-term conversation benchmarks such as LOCOMO and Long-MT-Bench+. Additionally, the proposed conversation segmentation method demonstrates superior performance on dialogue segmentation datasets such as DialSeg711, TIAGE, and SuperDialSeg.

### Large Multimodal Models for Low-Resource Languages: A Survey 
[[arxiv](https://arxiv.org/abs/2502.05568)] [[cool](https://papers.cool/arxiv/2502.05568)] [[pdf](https://arxiv.org/pdf/2502.05568)]
> **Authors**: Marian Lupascu,Ana-Cristina Rogoz,Mihai Sorin Stupariu,Radu Tudor Ionescu
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: In this survey, we systematically analyze techniques used to adapt large multimodal models (LMMs) for low-resource (LR) languages, examining approaches ranging from visual enhancement and data creation to cross-modal transfer and fusion strategies. Through a comprehensive analysis of 106 studies across 75 LR languages, we identify key patterns in how researchers tackle the challenges of limited data and computational resources. We find that visual information often serves as a crucial bridge for improving model performance in LR settings, though significant challenges remain in areas such as hallucination mitigation and computational efficiency. We aim to provide researchers with a clear understanding of current approaches and remaining challenges in making LMMs more accessible to speakers of LR (understudied) languages. We complement our survey with an open-source repository available at: https://github.com/marianlupascu/LMM4LRL-Survey.

### ATLAS: Autoformalizing Theorems through Lifting, Augmentation, and Synthesis of Data 
[[arxiv](https://arxiv.org/abs/2502.05567)] [[cool](https://papers.cool/arxiv/2502.05567)] [[pdf](https://arxiv.org/pdf/2502.05567)]
> **Authors**: Xiaoyang Liu,Kangjie Bao,Jiashuo Zhang,Yunqi Liu,Yu Chen,Yuntian Liu,Yang Jiao,Tao Luo
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Autoformalization, the process of automatically translating natural language mathematics into machine-verifiable formal language, has demonstrated advancements with the progress of large language models (LLMs). However, a key obstacle to further advancements is the scarcity of paired datasets that align natural language with formal language. To address this challenge, we introduce ATLAS (Autoformalizing Theorems through Lifting, Augmentation, and Synthesis of Data), an iterative data generation framework designed to produce large-scale, high-quality parallel theorem statements. With the proposed ATLAS running for 10 iterations, we construct an undergraduate-level dataset comprising 300k theorem statements and develop the ATLAS translator, achieving accuracies of 80.59% (pass@8) and 92.99% (pass@128) on ProofNet, significantly outperforming the base model (23.99% and 47.17%) and InternLM2-Math-Plus-7B (50.94% and 80.32%). Furthermore, the ATLAS translator also achieves state-of-the-art performance on both the high-school-level miniF2F dataset and the graduate-level MathQual dataset introduced in this work. The datasets, model, and code will be released to the public soon.

### Latent Structure Modulation in Large Language Models Through Stochastic Concept Embedding Transitions 
[[arxiv](https://arxiv.org/abs/2502.05553)] [[cool](https://papers.cool/arxiv/2502.05553)] [[pdf](https://arxiv.org/pdf/2502.05553)]
> **Authors**: Stefan Whitaker,Colin Sisate,Marcel Windsor,Nikolai Fairweather,Tarquin Goldborough,Oskar Lindenfeld
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Stochastic embedding transitions introduce a probabilistic mechanism for adjusting token representations dynamically during inference, mitigating the constraints imposed through static or deterministic embeddings. A transition framework was proposed in which each token embedding evolved through probabilistic updates, ensuring adaptability while preserving semantic integrity across linguistic contexts. Empirical evaluations demonstrated that models incorporating stochastic transitions exhibited greater lexical diversity, improved generative coherence, and enhanced retention of low-frequency vocabulary, contributing to more varied sentence structures and reduced reliance on high-probability token selections. Statistical analyses of embedding drift across transformer layers indicated that representations evolved more flexibly without losing coherence, supporting the hypothesis that controlled stochasticity facilitated context-sensitive representation learning. Experimental results revealed that probabilistic embeddings introduced minor computational overhead while maintaining generative efficiency, reinforcing their feasibility in large-scale applications. A comparative study with traditional embedding approaches highlighted measurable gains in text completion accuracy, dialogue coherence, and structural complexity, confirming the effectiveness of stochastic transitions in enhancing representation expressiveness. Clustering patterns in the embedding space suggested that probabilistic updates preserved meaningful semantic groupings while enabling context-driven shifts, further validating the stability of the transition mechanism. Performance metrics indicated that stochastic transitions balanced adaptability and control, ensuring that generative outputs remained linguistically coherent without excessive randomness.

### FRAME: Boosting LLMs with A Four-Quadrant Multi-Stage Pretraining Strategy 
[[arxiv](https://arxiv.org/abs/2502.05551)] [[cool](https://papers.cool/arxiv/2502.05551)] [[pdf](https://arxiv.org/pdf/2502.05551)]
> **Authors**: Xuemiao Zhang,Feiyu Duan,Liangyu Xu,Yongwei Zhou,Sirui Wang,Rongxiang Weng,Jingang Wang,Xunliang Cai
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) have significantly advanced human language understanding and generation, with pretraining data quality and organization being crucial to their performance. Multi-stage pretraining is a promising approach, but existing methods often lack quantitative criteria for data partitioning and instead rely on intuitive heuristics. In this paper, we propose the novel Four-quadRAnt Multi-stage prEtraining strategy (FRAME), guided by the established principle of organizing the pretraining process into four stages to achieve significant loss reductions four times. This principle is grounded in two key findings: first, training on high Perplexity (PPL) data followed by low PPL data, and second, training on low PPL difference (PD) data followed by high PD data, both causing the loss to drop significantly twice and performance enhancements. By partitioning data into four quadrants and strategically organizing them, FRAME achieves a remarkable 16.8% average improvement over random across MMLU and CMMLU for the 3B model, effectively boosting LLM performance.

### DeepThink: Aligning Language Models with Domain-Specific User Intents 
[[arxiv](https://arxiv.org/abs/2502.05497)] [[cool](https://papers.cool/arxiv/2502.05497)] [[pdf](https://arxiv.org/pdf/2502.05497)]
> **Authors**: Yang Li,Mingxuan Luo,Yeyun Gong,Chen Lin,Jian Jiao,Yi Liu,Kaili Huang
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Supervised fine-tuning with synthesized instructions has been a common practice for adapting LLMs to domain-specific QA tasks. However, the synthesized instructions deviate from real user questions and expected answers. This study proposes a novel framework called DeepThink to generate high-quality instructions. DeepThink first generates a few seed questions to mimic actual user questions, simulates conversations to uncover the hidden user needs, and refines the answer by conversational contexts and the retrieved documents for more comprehensive answers. Experiments demonstrate that DeepThink achieves an average performance improvement of 7.92% compared to a GPT-4-turbo+RAG-based assistant on the real user test set in the advertising domain across dimensions such as relevance, completeness, clarity, accuracy, and actionability.

### Mechanistic Interpretability of Emotion Inference in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.05489)] [[cool](https://papers.cool/arxiv/2502.05489)] [[pdf](https://arxiv.org/pdf/2502.05489)]
> **Authors**: Ala N. Tak,Amin Banayeeanzade,Anahita Bolourani,Mina Kian,Robin Jia,Jonathan Gratch
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: To be submitted to the Association for Computational Linguistics (ACL 2025)
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large language models (LLMs) show promising capabilities in predicting human emotions from text. However, the mechanisms through which these models process emotional stimuli remain largely unexplored. Our study addresses this gap by investigating how autoregressive LLMs infer emotions, showing that emotion representations are functionally localized to specific regions in the model. Our evaluation includes diverse model families and sizes and is supported by robustness checks. We then show that the identified representations are psychologically plausible by drawing on cognitive appraisal theory, a well-established psychological framework positing that emotions emerge from evaluations (appraisals) of environmental stimuli. By causally intervening on construed appraisal concepts, we steer the generation and show that the outputs align with theoretical and intuitive expectations. This work highlights a novel way to causally intervene and precisely shape emotional text generation, potentially benefiting safety and alignment in sensitive affective domains.

### OntoTune: Ontology-Driven Self-training for Aligning Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.05478)] [[cool](https://papers.cool/arxiv/2502.05478)] [[pdf](https://arxiv.org/pdf/2502.05478)]
> **Authors**: Zhiqiang Liu,Chengtao Gan,Junjie Wang,Yichi Zhang,Zhongpu Bo,Mengshu Sun,Huajun Chen,Wen Zhang
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: Accepted by WWW25
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Existing domain-specific Large Language Models (LLMs) are typically developed by fine-tuning general-purposed LLMs with large-scale domain-specific corpora. However, training on large-scale corpora often fails to effectively organize domain knowledge of LLMs, leading to fragmented understanding. Inspired by how humans connect concepts and organize knowledge through mind maps, we aim to emulate this approach by using ontology with hierarchical conceptual knowledge to reorganize LLM's domain knowledge. From this perspective, we propose an ontology-driven self-training framework called OntoTune, which aims to align LLMs with ontology through in-context learning, enabling the generation of responses guided by the ontology. We leverage in-context learning to identify whether the LLM has acquired the specific concept's ontology knowledge, and select the entries not yet mastered by LLM as the training set to further align the LLM with ontology. Compared to existing domain LLMs based on newly collected large-scale domain-specific corpora, our OntoTune, which relies on the existing, long-term developed ontology and LLM itself, significantly reduces data maintenance costs and offers improved generalization ability. We conduct our study in the medical domain to evaluate the effectiveness of OntoTune, utilizing a standardized medical ontology, SNOMED CT as our ontology source. Experimental results demonstrate that OntoTune achieves state-of-the-art performance in both in-ontology task hypernym discovery and out-of-ontology task medical domain QA. Moreover, compared to the latest direct ontology injection method TaxoLLaMA, our OntoTune better preserves original knowledge of LLM. The code and data are available at https://github.com/zjukg/OntoTune.

### Position: LLMs Can be Good Tutors in Foreign Language Education 
[[arxiv](https://arxiv.org/abs/2502.05467)] [[cool](https://papers.cool/arxiv/2502.05467)] [[pdf](https://arxiv.org/pdf/2502.05467)]
> **Authors**: Jingheng Ye,Shen Wang,Deqing Zou,Yibo Yan,Kun Wang,Hai-Tao Zheng,Zenglin Xu,Irwin King,Philip S. Yu,Qingsong Wen
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: 18 pages, 4 figures
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: While recent efforts have begun integrating large language models (LLMs) into foreign language education (FLE), they often rely on traditional approaches to learning tasks without fully embracing educational methodologies, thus lacking adaptability to language learning. To address this gap, we argue that LLMs have the potential to serve as effective tutors in FLE. Specifically, LLMs can play three critical roles: (1) as data enhancers, improving the creation of learning materials or serving as student simulations; (2) as task predictors, serving as learner assessment or optimizing learning pathway; and (3) as agents, enabling personalized and inclusive education. We encourage interdisciplinary research to explore these roles, fostering innovation while addressing challenges and risks, ultimately advancing FLE through the thoughtful integration of LLMs.

### Iterative Deepening Sampling for Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.05449)] [[cool](https://papers.cool/arxiv/2502.05449)] [[pdf](https://arxiv.org/pdf/2502.05449)]
> **Authors**: Weizhe Chen,Sven Koenig,Bistra Dilkina
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: The recent release of OpenAI's o1 models and other similar frameworks showcasing test-time scaling laws has demonstrated their exceptional capability to tackle complex reasoning tasks. Inspired by this, subsequent research has revealed that such test-time scaling laws hinge on the model's ability to search both within a single response (intra-response) and across multiple responses (inter-response) during training. Crucially, beyond selecting a single optimal response, the model must also develop robust self-correction capabilities within its own outputs. However, training models to achieve effective self-evaluation and self-correction remains a significant challenge, heavily dependent on the quality of self-reflection data. In this paper, we address this challenge by focusing on enhancing the quality of self-reflection data generation for complex problem-solving, which can subsequently improve the training of next-generation large language models (LLMs). Specifically, we explore how manually triggering a model's self-correction mechanisms can improve performance on challenging reasoning tasks. To this end, we propose a novel iterative deepening sampling algorithm framework designed to enhance self-correction and generate higher-quality samples. Through extensive experiments on Math500 and AIME benchmarks, we demonstrate that our method achieves a higher success rate on difficult tasks and provide detailed ablation studies to analyze its effectiveness across diverse settings.

### SAMGPT: Text-free Graph Foundation Model for Multi-domain Pre-training and Cross-domain Adaptation 
[[arxiv](https://arxiv.org/abs/2502.05424)] [[cool](https://papers.cool/arxiv/2502.05424)] [[pdf](https://arxiv.org/pdf/2502.05424)]
> **Authors**: Xingtong Yu,Zechuan Gong,Chang Zhou,Yuan Fang,Hui Zhang
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: Accepted by WWW2025 Main Track
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Graphs are able to model interconnected entities in many online services, supporting a wide range of applications on the Web. This raises an important question: How can we train a graph foundational model on multiple source domains and adapt to an unseen target domain? A major obstacle is that graphs from different domains often exhibit divergent characteristics. Some studies leverage large language models to align multiple domains based on textual descriptions associated with the graphs, limiting their applicability to text-attributed graphs. For text-free graphs, a few recent works attempt to align different feature distributions across domains, while generally neglecting structural differences. In this work, we propose a novel Structure Alignment framework for text-free Multi-domain Graph Pre-Training and cross-domain adaptation (SAMGPT). It is designed to learn multi-domain knowledge from graphs originating in multiple source domains, which can then be adapted to address applications in an unseen target domain. Specifically, we introduce a set of structure tokens to harmonize structure-based aggregation across source domains during the pre-training phase. Next, for cross-domain adaptation, we design dual prompts, namely, holistic prompts and specific prompts, which adapt unified multi-domain structural knowledge and fine-grained, domain-specific information, respectively, to a target domain. Finally, we conduct comprehensive experiments on seven public datasets to evaluate and analyze the effectiveness of SAMGPT.

### Dynamic Noise Preference Optimization for LLM Self-Improvement via Synthetic Data 
[[arxiv](https://arxiv.org/abs/2502.05400)] [[cool](https://papers.cool/arxiv/2502.05400)] [[pdf](https://arxiv.org/pdf/2502.05400)]
> **Authors**: Haoyan Yang,Ting Hua,Shangqian Gao,Binfeng Xu,Zheng Tang,Jie Xu,Hongxia Jin,Vijay Srinivasan
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: Due to an update in the company's publication approval process, a newly appointed manager has been added to the review workflow. As a result, we need to resubmit the application for approval under the revised process. Therefore, we are temporarily withdrawing this submission until the new approval workflow is completed
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Although LLMs have achieved significant success, their reliance on large volumes of human-annotated data has limited their potential for further scaling. In this situation, utilizing self-generated synthetic data has become crucial for fine-tuning LLMs without extensive human annotation. However, current methods often fail to ensure consistent improvements across iterations, with performance stagnating after only minimal updates. To overcome these challenges, we introduce Dynamic Noise Preference Optimization (DNPO). DNPO employs a dynamic sample labeling mechanism to construct preference pairs for training and introduces controlled, trainable noise into the preference optimization process. Our approach effectively prevents stagnation and enables continuous improvement. In experiments with Zephyr-7B, DNPO consistently outperforms existing methods, showing an average performance boost of 2.6% across multiple benchmarks. Additionally, DNPO shows a significant improvement in model-generated data quality, with a 29.4% win-loss rate gap compared to the baseline in GPT-4 evaluations. This highlights its effectiveness in enhancing model performance through iterative refinement.

### Hierarchical Lexical Manifold Projection in Large Language Models: A Novel Mechanism for Multi-Scale Semantic Representation 
[[arxiv](https://arxiv.org/abs/2502.05395)] [[cool](https://papers.cool/arxiv/2502.05395)] [[pdf](https://arxiv.org/pdf/2502.05395)]
> **Authors**: Natasha Martus,Sebastian Crowther,Maxwell Dorrington,Jonathan Applethwaite,Edgar Tillinghurst,Quentin Birkenshaw,Lukas Petrov,Constance Willoughby
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The integration of structured hierarchical embeddings into transformer-based architectures introduces a refined approach to lexical representation, ensuring that multi-scale semantic relationships are preserved without compromising computational efficiency. A projection mechanism that maps tokens onto a structured manifold provides improved lexical alignment, enhancing the adaptability of word representations across diverse linguistic tasks. The structured encoding framework ensures that hierarchical embeddings maintain coherence across varying abstraction levels, allowing for stable transitions between localized syntactic features and global semantic structures. Experimental evaluations indicate that hierarchical embeddings consistently outperform conventional token representations, improving accuracy in linguistic benchmarks while maintaining lower computational overhead. Comparative analysis across multiple domains highlights the ability of hierarchical embeddings to retain contextual consistency, particularly in specialized language applications where structured lexical alignment is essential. Statistical assessments further demonstrate that hierarchical embeddings exhibit enhanced robustness under perturbation conditions, ensuring that linguistic structures remain stable across adversarial text modifications. The integration of hierarchical projections with transformer attention mechanisms enables improved contextual adaptation, ensuring that token representations are dynamically adjusted based on varying linguistic distributions. The refined hierarchical organization of embeddings provides greater interpretability in lexical modeling, facilitating enhanced generalization capabilities across diverse text processing tasks.

### Learning Task Representations from In-Context Learning 
[[arxiv](https://arxiv.org/abs/2502.05390)] [[cool](https://papers.cool/arxiv/2502.05390)] [[pdf](https://arxiv.org/pdf/2502.05390)]
> **Authors**: Baturay Saglam,Zhuoran Yang,Dionysis Kalogerias,Amin Karbasi
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: Appeared in ICML 2024 Workshop on In-ContextLearning
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Large language models (LLMs) have demonstrated remarkable proficiency in in-context learning (ICL), where models adapt to new tasks through example-based prompts without requiring parameter updates. However, understanding how tasks are internally encoded and generalized remains a challenge. To address some of the empirical and technical gaps in the literature, we introduce an automated formulation for encoding task information in ICL prompts as a function of attention heads within the transformer architecture. This approach computes a single task vector as a weighted sum of attention heads, with the weights optimized causally via gradient descent. Our findings show that existing methods fail to generalize effectively to modalities beyond text. In response, we also design a benchmark to evaluate whether a task vector can preserve task fidelity in functional regression tasks. The proposed method successfully extracts task-specific information from in-context demonstrations and excels in both text and regression tasks, demonstrating its generalizability across modalities. Moreover, ablation studies show that our method's effectiveness stems from aligning the distribution of the last hidden state with that of an optimally performing in-context-learned model.

### The Role of Prosody in Spoken Question Answering 
[[arxiv](https://arxiv.org/abs/2502.05389)] [[cool](https://papers.cool/arxiv/2502.05389)] [[pdf](https://arxiv.org/pdf/2502.05389)]
> **Authors**: Jie Chi,Maureen de Seyssel,Natalie Schluter
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: accepted to NAACL 2025 Findings
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Spoken language understanding research to date has generally carried a heavy text perspective. Most datasets are derived from text, which is then subsequently synthesized into speech, and most models typically rely on automatic transcriptions of speech. This is to the detriment of prosody--additional information carried by the speech signal beyond the phonetics of the words themselves and difficult to recover from text alone. In this work, we investigate the role of prosody in Spoken Question Answering. By isolating prosodic and lexical information on the SLUE-SQA-5 dataset, which consists of natural speech, we demonstrate that models trained on prosodic information alone can perform reasonably well by utilizing prosodic cues. However, we find that when lexical information is available, models tend to predominantly rely on it. Our findings suggest that while prosodic cues provide valuable supplementary information, more effective integration methods are required to ensure prosody contributes more significantly alongside lexical features.

### Probabilistic Subspace Manifolds for Contextual Inference in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.05346)] [[cool](https://papers.cool/arxiv/2502.05346)] [[pdf](https://arxiv.org/pdf/2502.05346)]
> **Authors**: Christopher Nightingale,Dominic Lavington,Jonathan Thistlethwaite,Sebastian Penhaligon,Thomas Belinski,David Boldo
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Representing token embeddings as probability distributions over learned manifolds allows for more flexible contextual inference, reducing representational rigidity while enhancing semantic granularity. Comparative evaluations demonstrate that probabilistic embeddings improve neighborhood consistency and decrease redundancy, ensuring that token relationships remain more structurally coherent across fine-tuning iterations. The integration of probabilistic subspaces within attention mechanisms facilitates more adaptive contextual weighting, enabling models to capture latent dependencies that would otherwise be obscured in conventional embeddings. Experimental results highlight increased robustness against adversarial modifications, with probabilistic embeddings preserving contextual integrity even under perturbation-based evaluation scenarios. Performance assessments indicate that probabilistic representations achieve greater adaptability in domain-specific applications, mitigating the need for extensive retraining when shifting across linguistic domains. Computational trade-offs remain within operationally feasible limits, with marginal increases in inference latency balanced against the benefits of enhanced representation stability and contextual expressiveness. The capacity to encode structured uncertainty provides advantages in generative modeling tasks, particularly where maintaining coherence across extended sequences requires a representation framework capable of handling ambiguous or context-dependent linguistic constructs.

### Fine-Tuned LLMs are "Time Capsules" for Tracking Societal Bias Through Books 
[[arxiv](https://arxiv.org/abs/2502.05331)] [[cool](https://papers.cool/arxiv/2502.05331)] [[pdf](https://arxiv.org/pdf/2502.05331)]
> **Authors**: Sangmitra Madhusudan,Robert Morabito,Skye Reid,Nikta Gohari Sadr,Ali Emami
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 9 pages (excluding references), accepted to NAACL 2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Books, while often rich in cultural insights, can also mirror societal biases of their eras - biases that Large Language Models (LLMs) may learn and perpetuate during training. We introduce a novel method to trace and quantify these biases using fine-tuned LLMs. We develop BookPAGE, a corpus comprising 593 fictional books across seven decades (1950-2019), to track bias evolution. By fine-tuning LLMs on books from each decade and using targeted prompts, we examine shifts in biases related to gender, sexual orientation, race, and religion. Our findings indicate that LLMs trained on decade-specific books manifest biases reflective of their times, with both gradual trends and notable shifts. For example, model responses showed a progressive increase in the portrayal of women in leadership roles (from 8% to 22%) from the 1950s to 2010s, with a significant uptick in the 1990s (from 4% to 12%), possibly aligning with third-wave feminism. Same-sex relationship references increased markedly from the 1980s to 2000s (from 0% to 10%), mirroring growing LGBTQ+ visibility. Concerningly, negative portrayals of Islam rose sharply in the 2000s (26% to 38%), likely reflecting post-9/11 sentiments. Importantly, we demonstrate that these biases stem mainly from the books' content and not the models' architecture or initial training. Our study offers a new perspective on societal bias trends by bridging AI, literary studies, and social science research.

### Towards the Development of Balanced Synthetic Data for Correcting Grammatical Errors in Arabic: An Approach Based on Error Tagging Model and Synthetic Data Generating Model 
[[arxiv](https://arxiv.org/abs/2502.05312)] [[cool](https://papers.cool/arxiv/2502.05312)] [[pdf](https://arxiv.org/pdf/2502.05312)]
> **Authors**: Ahlam Alrehili,Areej Alhothali
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 21 pages, 3 figures
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Synthetic data generation is widely recognized as a way to enhance the quality of neural grammatical error correction (GEC) systems. However, current approaches often lack diversity or are too simplistic to generate the wide range of grammatical errors made by humans, especially for low-resource languages such as Arabic. In this paper, we will develop the error tagging model and the synthetic data generation model to create a large synthetic dataset in Arabic for grammatical error correction. In the error tagging model, the correct sentence is categorized into multiple error types by using the DeBERTav3 model. Arabic Error Type Annotation tool (ARETA) is used to guide multi-label classification tasks in an error tagging model in which each sentence is classified into 26 error tags. The synthetic data generation model is a back-translation-based model that generates incorrect sentences by appending error tags before the correct sentence that was generated from the error tagging model using the ARAT5 model. In the QALB-14 and QALB-15 Test sets, the error tagging model achieved 94.42% F1, which is state-of-the-art in identifying error tags in clean sentences. As a result of our syntactic data training in grammatical error correction, we achieved a new state-of-the-art result of F1-Score: 79.36% in the QALB-14 Test set. We generate 30,219,310 synthetic sentence pairs by using a synthetic data generation model.

### Can LLMs Rank the Harmfulness of Smaller LLMs? We are Not There Yet 
[[arxiv](https://arxiv.org/abs/2502.05291)] [[cool](https://papers.cool/arxiv/2502.05291)] [[pdf](https://arxiv.org/pdf/2502.05291)]
> **Authors**: Berk Atil,Vipul Gupta,Sarkar Snigdha Sarathi Das,Rebecca J. Passonneau
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) have become ubiquitous, thus it is important to understand their risks and limitations. Smaller LLMs can be deployed where compute resources are constrained, such as edge devices, but with different propensity to generate harmful output. Mitigation of LLM harm typically depends on annotating the harmfulness of LLM output, which is expensive to collect from humans. This work studies two questions: How do smaller LLMs rank regarding generation of harmful content? How well can larger LLMs annotate harmfulness? We prompt three small LLMs to elicit harmful content of various types, such as discriminatory language, offensive content, privacy invasion, or negative influence, and collect human rankings of their outputs. Then, we evaluate three state-of-the-art large LLMs on their ability to annotate the harmfulness of these responses. We find that the smaller models differ with respect to harmfulness. We also find that large LLMs show low to moderate agreement with humans. These findings underline the need for further work on harm mitigation in LLMs.

### LLMs Can Teach Themselves to Better Predict the Future 
[[arxiv](https://arxiv.org/abs/2502.05253)] [[cool](https://papers.cool/arxiv/2502.05253)] [[pdf](https://arxiv.org/pdf/2502.05253)]
> **Authors**: Benjamin Turtel,Danny Franklin,Philipp Schoenegger
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: We present an outcome-driven fine-tuning framework that enhances the forecasting capabilities of large language models (LLMs) without relying on human-curated reasoning samples. Our method leverages model self-play to generate pairs of diverse reasoning trajectories and probabilistic forecasts for a set of diverse questions that resolve after the models' knowledge cutoff date. We then rank pairs of these reasoning traces by their distance to the actual outcomes before fine-tuning the model via Direct Preference Optimization (DPO). On a separate test set, our approach increases prediction accuracy of Phi-4 14B and DeepSeek-R1 14B by between 7--10\% over a base model and a DPO fine-tuned control model with randomized labels, bringing them on par with forecasting capabilities of much larger frontier models like GPT-4o.

### GSM-Infinite: How Do Your LLMs Behave over Infinitely Increasing Context Length and Reasoning Complexity? 
[[arxiv](https://arxiv.org/abs/2502.05252)] [[cool](https://papers.cool/arxiv/2502.05252)] [[pdf](https://arxiv.org/pdf/2502.05252)]
> **Authors**: Yang Zhou,Hongyi Liu,Zhuoming Chen,Yuandong Tian,Beidi Chen
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Long-context large language models (LLMs) have recently shown strong performance in information retrieval and long-document QA. However, to tackle the most challenging intellectual problems, LLMs must reason effectively in long and complex contexts (e.g., frontier mathematical research). Studying how LLMs handle increasing reasoning complexity and context length is essential, yet existing benchmarks lack a solid basis for quantitative evaluation. Inspired by the abstraction of GSM-8K problems as computational graphs, and the ability to introduce noise by adding unnecessary nodes and edges, we develop a grade school math problem generator capable of producing arithmetic problems with infinite difficulty and context length under fine-grained control. Using our newly synthesized GSM-Infinite benchmark, we comprehensively evaluate existing LLMs. We find a consistent sigmoid decline in reasoning performance as complexity increases, along with a systematic inference scaling trend: exponentially increasing inference computation yields only linear performance gains. These findings underscore the fundamental limitations of current long-context LLMs and the key challenges in scaling reasoning capabilities. Our GSM-Infinite benchmark provides a scalable and controllable testbed for systematically studying and advancing LLM reasoning in long and complex contexts.

### Evaluating Personality Traits in Large Language Models: Insights from Psychological Questionnaires 
[[arxiv](https://arxiv.org/abs/2502.05248)] [[cool](https://papers.cool/arxiv/2502.05248)] [[pdf](https://arxiv.org/pdf/2502.05248)]
> **Authors**: Pranav Bhandari,Usman Naseem,Amitava Datta,Nicolas Fay,Mehwish Nasim
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: Accepted for publication at TheWebConf 2025
- **标题**: None
- **领域**: 计算语言学,人工智能,多代理系统
- **Abstract**: Psychological assessment tools have long helped humans understand behavioural patterns. While Large Language Models (LLMs) can generate content comparable to that of humans, we explore whether they exhibit personality traits. To this end, this work applies psychological tools to LLMs in diverse scenarios to generate personality profiles. Using established trait-based questionnaires such as the Big Five Inventory and by addressing the possibility of training data contamination, we examine the dimensional variability and dominance of LLMs across five core personality dimensions: Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. Our findings reveal that LLMs exhibit unique dominant traits, varying characteristics, and distinct personality profiles even within the same family of models.

### SEER: Self-Explainability Enhancement of Large Language Models' Representations 
[[arxiv](https://arxiv.org/abs/2502.05242)] [[cool](https://papers.cool/arxiv/2502.05242)] [[pdf](https://arxiv.org/pdf/2502.05242)]
> **Authors**: Guanxu Chen,Dongrui Liu,Tao Luo,Jing Shao
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 18 pages,5 figures,10 tables
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别,机器学习
- **Abstract**: Explaining the hidden representations of Large Language Models (LLMs) is a perspective to understand LLMs' underlying inference logic and improve their reliability in application scenarios. However, previous methods introduce external ''black-box'' modules to explain ''black-box'' LLMs, increasing the potential uncertainty and failing to provide faithful explanations. In this paper, we propose a self-explaining method SEER, enhancing LLMs' explainability by aggregating the same concept and disentangling the different concepts in the representation space. In this way, SEER provides faithful explanations carried by representations synchronously with the LLMs' output. Additionally, we showcase the applications of SEER on trustworthiness-related tasks (e.g., the safety risks classification and detoxification tasks), where self-explained LLMs achieve consistent improvement in explainability and performance. More crucially, we theoretically analyze the improvement of SEER on LLMs' generalization ability through optimal transport theory.

### Enhancing Knowledge Graph Construction: Evaluating with Emphasis on Hallucination, Omission, and Graph Similarity Metrics 
[[arxiv](https://arxiv.org/abs/2502.05239)] [[cool](https://papers.cool/arxiv/2502.05239)] [[pdf](https://arxiv.org/pdf/2502.05239)]
> **Authors**: Hussam Ghanem,Christophe Cruz
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: ef:Sixth International Knowledge Graph and Semantic Web Conference (KGSWC 2024), Dec 2024, Paris, France
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Recent advancements in large language models have demonstrated significant potential in the automated construction of knowledge graphs from unstructured text. This paper builds upon our previous work [16], which evaluated various models using metrics like precision, recall, F1 score, triple matching, and graph matching, and introduces a refined approach to address the critical issues of hallucination and omission. We propose an enhanced evaluation framework incorporating BERTScore for graph similarity, setting a practical threshold of 95% for graph matching. Our experiments focus on the Mistral model, comparing its original and fine-tuned versions in zero-shot and few-shot settings. We further extend our experiments using examples from the KELM-sub training dataset, illustrating that the fine-tuned model significantly improves knowledge graph construction accuracy while reducing the exact hallucination and omission. However, our findings also reveal that the fine-tuned models perform worse in generalization tasks on the KELM-sub dataset. This study underscores the importance of comprehensive evaluation metrics in advancing the state-of-the-art in knowledge graph construction from textual data.

### Efficient Knowledge Feeding to Language Models: A Novel Integrated Encoder-Decoder Architecture 
[[arxiv](https://arxiv.org/abs/2502.05233)] [[cool](https://papers.cool/arxiv/2502.05233)] [[pdf](https://arxiv.org/pdf/2502.05233)]
> **Authors**: S Santosh Kumar,Rishi Gottimukkala,Supriya Devidutta,Karthikeyan S
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-10
> **comment**: Submitted to ACM TIST journal: under revision stage, 8 pages, 2 figures
- **标题**: None
- **领域**: 计算语言学,信息检索
- **Abstract**: This paper introduces a novel approach to efficiently feeding knowledge to language models (LLMs) during prediction by integrating retrieval and generation processes within a unified framework. While the Retrieval-Augmented Generation (RAG) model addresses gaps in LLMs' training data and knowledge limits, it is hindered by token limit restrictions and dependency on the retrieval system's accuracy. Our proposed architecture incorporates in-context vectors (ICV) to overcome these challenges. ICV recasts in-context learning by using latent embeddings of LLMs to create a vector that captures essential task information. This vector is then used to shift the latent states of the LLM, enhancing the generation process without adding demonstration examples to the prompt. ICV directly integrates information into the model, enabling it to process this information more effectively. Our extensive experimental evaluation demonstrates that ICV outperforms standard in-context learning and fine-tuning across question-answering, information retrieval, and other tasks. This approach mitigates the limitations of current RAG models and offers a more robust solution for handling extensive and diverse datasets. Despite leveraging a fraction of the parameters, our ICV-enhanced model achieves competitive performance against models like LLaMA-3, Gemma, and Phi-3, significantly reducing computational costs and memory requirements. ICV reduces prompt length, is easy to control, surpasses token limitations, and is computationally efficient compared to fine-tuning.

### Accelerating LLM Inference with Lossless Speculative Decoding Algorithms for Heterogeneous Vocabularies 
[[arxiv](https://arxiv.org/abs/2502.05202)] [[cool](https://papers.cool/arxiv/2502.05202)] [[pdf](https://arxiv.org/pdf/2502.05202)]
> **Authors**: Nadav Timor,Jonathan Mamou,Daniel Korat,Moshe Berchansky,Oren Pereg,Gaurav Jain,Roy Schwartz,Moshe Wasserblat,David Harel
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Accelerating the inference of large language models (LLMs) is a critical challenge in generative AI. Speculative decoding (SD) methods offer substantial efficiency gains by generating multiple tokens using a single target forward pass. However, existing SD approaches require the drafter and target models to share the same vocabulary, thus limiting the pool of possible drafters, often necessitating the training of a drafter from scratch. We present three new SD methods that remove this shared-vocabulary constraint. All three methods preserve the target distribution (i.e., they are lossless) and work with off-the-shelf models without requiring additional training or modifications. Empirically, on summarization, programming, and long-context tasks, our algorithms achieve significant speedups over standard autoregressive decoding. By enabling any off-the-shelf model to serve as drafter and requiring no retraining, this work substantially broadens the applicability of the SD framework in practice.

### LLMs Provide Unstable Answers to Legal Questions 
[[arxiv](https://arxiv.org/abs/2502.05196)] [[cool](https://papers.cool/arxiv/2502.05196)] [[pdf](https://arxiv.org/pdf/2502.05196)]
> **Authors**: Andrew Blair-Stanek,Benjamin Van Durme
> **First submission**: 2025-01-28
> **First announcement**: 2025-02-10
> **comment**: 6 pages
- **标题**: None
- **领域**: 计算语言学,计算机与社会
- **Abstract**: An LLM is stable if it reaches the same conclusion when asked the identical question multiple times. We find leading LLMs like gpt-4o, claude-3.5, and gemini-1.5 are unstable when providing answers to hard legal questions, even when made as deterministic as possible by setting temperature to 0. We curate and release a novel dataset of 500 legal questions distilled from real cases, involving two parties, with facts, competing legal arguments, and the question of which party should prevail. When provided the exact same question, we observe that LLMs sometimes say one party should win, while other times saying the other party should win. This instability has implications for the increasing numbers of legal AI products, legal processes, and lawyers relying on these LLMs.

### NoLiMa: Long-Context Evaluation Beyond Literal Matching 
[[arxiv](https://arxiv.org/abs/2502.05167)] [[cool](https://papers.cool/arxiv/2502.05167)] [[pdf](https://arxiv.org/pdf/2502.05167)]
> **Authors**: Ali Modarressi,Hanieh Deilamsalehy,Franck Dernoncourt,Trung Bui,Ryan A. Rossi,Seunghyun Yoon,Hinrich Schütze
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Recent large language models (LLMs) support long contexts ranging from 128K to 1M tokens. A popular method for evaluating these capabilities is the needle-in-a-haystack (NIAH) test, which involves retrieving a "needle" (relevant information) from a "haystack" (long irrelevant context). Extensions of this approach include increasing distractors, fact chaining, and in-context reasoning. However, in these benchmarks, models can exploit existing literal matches between the needle and haystack to simplify the task. To address this, we introduce NoLiMa, a benchmark extending NIAH with a carefully designed needle set, where questions and needles have minimal lexical overlap, requiring models to infer latent associations to locate the needle within the haystack. We evaluate 12 popular LLMs that claim to support contexts of at least 128K tokens. While they perform well in short contexts (<1K), performance degrades significantly as context length increases. At 32K, for instance, 10 models drop below 50% of their strong short-length baselines. Even GPT-4o, one of the top-performing exceptions, experiences a reduction from an almost-perfect baseline of 99.3% to 69.7%. Our analysis suggests these declines stem from the increased difficulty the attention mechanism faces in longer contexts when literal matches are absent, making it harder to retrieve relevant information.

### DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM Guardrails 
[[arxiv](https://arxiv.org/abs/2502.05163)] [[cool](https://papers.cool/arxiv/2502.05163)] [[pdf](https://arxiv.org/pdf/2502.05163)]
> **Authors**: Yihe Deng,Yu Yang,Junkai Zhang,Wei Wang,Bo Li
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 24 pages, 9 figures, 5 tables
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: The rapid advancement of large language models (LLMs) has increased the need for guardrail models to ensure responsible use, particularly in detecting unsafe and illegal content. While substantial safety data exist in English, multilingual guardrail modeling remains underexplored due to the scarcity of open-source safety data in other languages. To address this gap, we propose a novel two-player Reinforcement Learning (RL) framework, where a generator and a guardrail model co-evolve adversarially to produce high-quality synthetic data for multilingual guardrail training. We theoretically formalize this interaction as a two-player game, proving convergence to a Nash equilibrium. Empirical evaluations show that our model \ours outperforms state-of-the-art models, achieving nearly 10% improvement over LlamaGuard3 (8B) on English benchmarks while being 4.5x faster at inference with a significantly smaller model (0.5B). We achieve substantial advancements in multilingual safety tasks, particularly in addressing the imbalance for lower-resource languages in a collected real dataset. Ablation studies emphasize the critical role of synthetic data generation in bridging the imbalance in open-source data between English and other languages. These findings establish a scalable and efficient approach to synthetic data generation, paving the way for improved multilingual guardrail models to enhance LLM safety. Code, model, and data will be open-sourced at https://github.com/yihedeng9/DuoGuard.

### Transforming Science with Large Language Models: A Survey on AI-assisted Scientific Discovery, Experimentation, Content Generation, and Evaluation 
[[arxiv](https://arxiv.org/abs/2502.05151)] [[cool](https://papers.cool/arxiv/2502.05151)] [[pdf](https://arxiv.org/pdf/2502.05151)]
> **Authors**: Steffen Eger,Yong Cao,Jennifer D'Souza,Andreas Geiger,Christian Greisinger,Stephanie Gross,Yufang Hou,Brigitte Krenn,Anne Lauscher,Yizhi Li,Chenghua Lin,Nafise Sadat Moosavi,Wei Zhao,Tristan Miller
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: Work in progress. Will be updated soon
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别,机器学习
- **Abstract**: With the advent of large multimodal language models, science is now at a threshold of an AI-based technological transformation. Recently, a plethora of new AI models and tools has been proposed, promising to empower researchers and academics worldwide to conduct their research more effectively and efficiently. This includes all aspects of the research cycle, especially (1) searching for relevant literature; (2) generating research ideas and conducting experimentation; generating (3) text-based and (4) multimodal content (e.g., scientific figures and diagrams); and (5) AI-based automatic peer review. In this survey, we provide an in-depth overview over these exciting recent developments, which promise to fundamentally alter the scientific research process for good. Our survey covers the five aspects outlined above, indicating relevant datasets, methods and results (including evaluation) as well as limitations and scope for future research. Ethical concerns regarding shortcomings of these tools and potential for misuse (fake science, plagiarism, harms to research integrity) take a particularly prominent place in our discussion. We hope that our survey will not only become a reference guide for newcomers to the field but also a catalyst for new AI-based initiatives in the area of "AI4Science".

### CodeSCM: Causal Analysis for Multi-Modal Code Generation 
[[arxiv](https://arxiv.org/abs/2502.05150)] [[cool](https://papers.cool/arxiv/2502.05150)] [[pdf](https://arxiv.org/pdf/2502.05150)]
> **Authors**: Mukur Gupta,Noopur Bhatt,Suman Jana
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: Accepted to NAACL 2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: In this paper, we propose CodeSCM, a Structural Causal Model (SCM) for analyzing multi-modal code generation using large language models (LLMs). By applying interventions to CodeSCM, we measure the causal effects of different prompt modalities, such as natural language, code, and input-output examples, on the model. CodeSCM introduces latent mediator variables to separate the code and natural language semantics of a multi-modal code generation prompt. Using the principles of Causal Mediation Analysis on these mediators we quantify direct effects representing the model's spurious leanings. We find that, in addition to natural language instructions, input-output examples significantly influence code generation.

### Flexible and Efficient Grammar-Constrained Decoding 
[[arxiv](https://arxiv.org/abs/2502.05111)] [[cool](https://papers.cool/arxiv/2502.05111)] [[pdf](https://arxiv.org/pdf/2502.05111)]
> **Authors**: Kanghee Park,Timothy Zhou,Loris D'Antoni
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large Language Models (LLMs) are often asked to generate structured outputs that obey precise syntactic rules, such as code snippets or formatted data. Grammar-constrained decoding (GCD) can guarantee that LLM outputs matches such rules by masking out tokens that will provably lead to outputs that do not belong to a specified context-free grammar (CFG). To guarantee soundness, GCD algorithms have to compute how a given LLM subword tokenizer can align with the tokens used by a given context-free grammar and compute token masks based on this information. Doing so efficiently is challenging and existing GCD algorithms require tens of minutes to preprocess common grammars. We present a new GCD algorithm together with an implementation that offers 17.71x faster offline preprocessing than existing approaches while preserving state-of-the-art efficiency in online mask computation.

### ChallengeMe: An Adversarial Learning-enabled Text Summarization Framework 
[[arxiv](https://arxiv.org/abs/2502.05084)] [[cool](https://papers.cool/arxiv/2502.05084)] [[pdf](https://arxiv.org/pdf/2502.05084)]
> **Authors**: Xiaoyu Deng,Ye Zhang,Tianmin Guo,Yongzhe Zhang,Zhengjian Kang,Hang Yang
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: The astonishing performance of large language models (LLMs) and their remarkable achievements in production and daily life have led to their widespread application in collaborative tasks. However, current large models face challenges such as hallucination and lack of specificity in content generation in vertical domain tasks. Inspired by the contrast and classification mechanisms in human cognitive processes, this paper constructs an adversarial learning-based prompt framework named ChallengeMe, which includes three cascaded solutions: generation prompts, evaluation prompts, and feedback optimization. In this process, we designed seven core optimization dimensions and set the threshold for adversarial learning. The results of mixed case studies on the text summarization task show that the proposed framework can generate more accurate and fluent text summaries compared to the current advanced mainstream LLMs.

### nvAgent: Automated Data Visualization from Natural Language via Collaborative Agent Workflow 
[[arxiv](https://arxiv.org/abs/2502.05036)] [[cool](https://papers.cool/arxiv/2502.05036)] [[pdf](https://arxiv.org/pdf/2502.05036)]
> **Authors**: Geliang Ouyang,Jingyao Chen,Zhihe Nie,Yi Gui,Yao Wan,Hongyu Zhang,Dongping Chen
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Natural Language to Visualization (NL2Vis) seeks to convert natural-language descriptions into visual representations of given tables, empowering users to derive insights from large-scale data. Recent advancements in Large Language Models (LLMs) show promise in automating code generation to transform tabular data into accessible visualizations. However, they often struggle with complex queries that require reasoning across multiple tables. To address this limitation, we propose a collaborative agent workflow, termed nvAgent, for NL2Vis. Specifically, nvAgent comprises three agents: a processor agent for database processing and context filtering, a composer agent for planning visualization generation, and a validator agent for code translation and output verification. Comprehensive evaluations on the new VisEval benchmark demonstrate that nvAgent consistently surpasses state-of-the-art baselines, achieving a 7.88% improvement in single-table and a 9.23% improvement in multi-table scenarios. Qualitative analyses further highlight that nvAgent maintains nearly a 20% performance margin over previous models, underscoring its capacity to produce high-quality visual representations from complex, heterogeneous data sources.

### Aligning Black-box Language Models with Human Judgments 
[[arxiv](https://arxiv.org/abs/2502.04997)] [[cool](https://papers.cool/arxiv/2502.04997)] [[pdf](https://arxiv.org/pdf/2502.04997)]
> **Authors**: Gerrit J. J. van den Burg,Gen Suzuki,Wei Liu,Murat Sensoy
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: Accepted for publication at NAACL 2025 (Findings)
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Large language models (LLMs) are increasingly used as automated judges to evaluate recommendation systems, search engines, and other subjective tasks, where relying on human evaluators can be costly, time-consuming, and unscalable. LLMs offer an efficient solution for continuous, automated evaluation. However, since the systems that are built and improved with these judgments are ultimately designed for human use, it is crucial that LLM judgments align closely with human evaluators to ensure such systems remain human-centered. On the other hand, aligning LLM judgments with human evaluators is challenging due to individual variability and biases in human judgments. We propose a simple yet effective framework to align LLM judgments with individual human evaluators or their aggregated judgments, without retraining or fine-tuning the LLM. Our approach learns a linear mapping between the LLM's outputs and human judgments, achieving over 142% average improvement in agreement across 29 tasks with only a small number of calibration examples used for training. Notably, our method works in zero-shot and few-shot settings, exceeds inter-human agreement on four out of six tasks, and enables smaller LLMs to achieve performance comparable to that of larger models.

### CoCoA: A Generalized Approach to Uncertainty Quantification by Integrating Confidence and Consistency of LLM Outputs 
[[arxiv](https://arxiv.org/abs/2502.04964)] [[cool](https://papers.cool/arxiv/2502.04964)] [[pdf](https://arxiv.org/pdf/2502.04964)]
> **Authors**: Roman Vashurin,Maiya Goloburda,Preslav Nakov,Artem Shelmanov,Maxim Panov
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Uncertainty quantification (UQ) methods for Large Language Models (LLMs) encompasses a variety of approaches, with two major types being particularly prominent: information-based, which focus on model confidence expressed as token probabilities, and consistency-based, which assess the semantic relationship between multiple outputs generated using repeated sampling. Several recent methods have combined these two approaches and shown impressive performance in various applications. However, they sometimes fail to outperform much simpler baseline methods. Our investigation reveals distinctive characteristics of LLMs as probabilistic models, which help to explain why these UQ methods underperform in certain tasks. Based on these findings, we propose a new way of synthesizing model confidence and output consistency that leads to a family of efficient and robust UQ methods. We evaluate our approach across a variety of tasks such as question answering, abstractive summarization, and machine translation, demonstrating sizable improvements over state-of-the-art UQ approaches.

### Commonality and Individuality! Integrating Humor Commonality with Speaker Individuality for Humor Recognition 
[[arxiv](https://arxiv.org/abs/2502.04960)] [[cool](https://papers.cool/arxiv/2502.04960)] [[pdf](https://arxiv.org/pdf/2502.04960)]
> **Authors**: Haohao Zhu,Junyu Lu,Zeyuan Zeng,Zewen Bai,Xiaokun Zhang,Liang Yang,Hongfei Lin
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: Accepted by NAACL 2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Humor recognition aims to identify whether a specific speaker's text is humorous. Current methods for humor recognition mainly suffer from two limitations: (1) they solely focus on one aspect of humor commonalities, ignoring the multifaceted nature of humor; and (2) they typically overlook the critical role of speaker individuality, which is essential for a comprehensive understanding of humor expressions. To bridge these gaps, we introduce the Commonality and Individuality Incorporated Network for Humor Recognition (CIHR), a novel model designed to enhance humor recognition by integrating multifaceted humor commonalities with the distinctive individuality of speakers. The CIHR features a Humor Commonality Analysis module that explores various perspectives of multifaceted humor commonality within user texts, and a Speaker Individuality Extraction module that captures both static and dynamic aspects of a speaker's profile to accurately model their distinctive individuality. Additionally, Static and Dynamic Fusion modules are introduced to effectively incorporate the humor commonality with speaker's individuality in the humor recognition process. Extensive experiments demonstrate the effectiveness of CIHR, underscoring the importance of concurrently addressing both multifaceted humor commonality and distinctive speaker individuality in humor recognition.

### SSMLoRA: Enhancing Low-Rank Adaptation with State Space Model 
[[arxiv](https://arxiv.org/abs/2502.04958)] [[cool](https://papers.cool/arxiv/2502.04958)] [[pdf](https://arxiv.org/pdf/2502.04958)]
> **Authors**: Jiayang Yu,Yihang Zhang,Bin Wang,Peiqin Lin,Yongkang Liu,Shi Feng
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: Has been accepted by NAACL 2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Fine-tuning is a key approach for adapting language models to specific downstream tasks, but updating all model parameters becomes impractical as model sizes increase. Parameter-Efficient Fine-Tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), address this challenge by introducing additional adaptation parameters into pre-trained weight matrices. However, LoRA's performance varies across different insertion points within the model, highlighting potential parameter inefficiency due to unnecessary insertions. To this end, we propose SSMLoRA (State Space Model Low-Rank Adaptation), an extension of LoRA that incorporates a State Space Model (SSM) to interconnect low-rank matrices. SSMLoRA ensures that performance is maintained even with sparser insertions. SSMLoRA allows the model to not only map inputs to a low-rank space for better feature extraction but also leverage the computations from the previous low-rank space. Our method achieves comparable performance to LoRA on the General Language Understanding Evaluation (GLUE) benchmark while using only half the parameters. Additionally, due to its structure, SSMLoRA shows promise in handling tasks with longer input sequences. .You can find our code here:https://github.com/yuhkalhic/SSMLoRA.

### Claim Extraction for Fact-Checking: Data, Models, and Automated Metrics 
[[arxiv](https://arxiv.org/abs/2502.04955)] [[cool](https://papers.cool/arxiv/2502.04955)] [[pdf](https://arxiv.org/pdf/2502.04955)]
> **Authors**: Herbert Ullrich,Tomáš Mlynář,Jan Drchal
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: In this paper, we explore the problem of Claim Extraction using one-to-many text generation methods, comparing LLMs, small summarization models finetuned for the task, and a previous NER-centric baseline QACG. As the current publications on Claim Extraction, Fact Extraction, Claim Generation and Check-worthy Claim Detection are quite scattered in their means and terminology, we compile their common objectives, releasing the FEVERFact dataset, with 17K atomic factual claims extracted from 4K contextualised Wikipedia sentences, adapted from the original FEVER. We compile the known objectives into an Evaluation framework of: Atomicity, Fluency, Decontextualization, Faithfulness checked for each generated claim separately, and Focus and Coverage measured against the full set of predicted claims for a single input. For each metric, we implement a scale using a reduction to an already-explored NLP task. We validate our metrics against human grading of generic claims, to see that the model ranking on $F_{fact}$, our hardest metric, did not change and the evaluation framework approximates human grading very closely in terms of $F_1$ and RMSE.

### Evaluating Standard and Dialectal Frisian ASR: Multilingual Fine-tuning and Language Identification for Improved Low-resource Performance 
[[arxiv](https://arxiv.org/abs/2502.04883)] [[cool](https://papers.cool/arxiv/2502.04883)] [[pdf](https://arxiv.org/pdf/2502.04883)]
> **Authors**: Reihaneh Amooie,Wietse de Vries,Yun Hao,Jelske Dijkstra,Matt Coler,Martijn Wieling
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习,声音,音频和语音处理
- **Abstract**: Automatic Speech Recognition (ASR) performance for low-resource languages is still far behind that of higher-resource languages such as English, due to a lack of sufficient labeled data. State-of-the-art methods deploy self-supervised transfer learning where a model pre-trained on large amounts of data is fine-tuned using little labeled data in a target low-resource language. In this paper, we present and examine a method for fine-tuning an SSL-based model in order to improve the performance for Frisian and its regional dialects (Clay Frisian, Wood Frisian, and South Frisian). We show that Frisian ASR performance can be improved by using multilingual (Frisian, Dutch, English and German) fine-tuning data and an auxiliary language identification task. In addition, our findings show that performance on dialectal speech suffers substantially, and, importantly, that this effect is moderated by the elicitation approach used to collect the dialectal data. Our findings also particularly suggest that relying solely on standard language data for ASR evaluation may underestimate real-world performance, particularly in languages with substantial dialectal variation.

### Enhancing Disinformation Detection with Explainable AI and Named Entity Replacement 
[[arxiv](https://arxiv.org/abs/2502.04863)] [[cool](https://papers.cool/arxiv/2502.04863)] [[pdf](https://arxiv.org/pdf/2502.04863)]
> **Authors**: Santiago González-Silot,Andrés Montoro-Montarroso,Eugenio Martínez Cámara,Juan Gómez-Romero
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The automatic detection of disinformation presents a significant challenge in the field of natural language processing. This task addresses a multifaceted societal and communication issue, which needs approaches that extend beyond the identification of general linguistic patterns through data-driven algorithms. In this research work, we hypothesise that text classification methods are not able to capture the nuances of disinformation and they often ground their decision in superfluous features. Hence, we apply a post-hoc explainability method (SHAP, SHapley Additive exPlanations) to identify spurious elements with high impact on the classification models. Our findings show that non-informative elements (e.g., URLs and emoticons) should be removed and named entities (e.g., Rwanda) should be pseudo-anonymized before training to avoid models' bias and increase their generalization capabilities. We evaluate this methodology with internal dataset and external dataset before and after applying extended data preprocessing and named entity replacement. The results show that our proposal enhances on average the performance of a disinformation classification method with external test data in 65.78% without a significant decrease of the internal test performance.

### Self-Rationalization in the Wild: A Large Scale Out-of-Distribution Evaluation on NLI-related tasks 
[[arxiv](https://arxiv.org/abs/2502.04797)] [[cool](https://papers.cool/arxiv/2502.04797)] [[pdf](https://arxiv.org/pdf/2502.04797)]
> **Authors**: Jing Yang,Max Glockner,Anderson Rocha,Iryna Gurevych
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: Accepted at TACL; pre-MIT Press publication version
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Free-text explanations are expressive and easy to understand, but many datasets lack annotated explanation data, making it challenging to train models for explainable predictions. To address this, we investigate how to use existing explanation datasets for self-rationalization and evaluate models' out-of-distribution (OOD) performance. We fine-tune T5-Large and OLMo-7B models and assess the impact of fine-tuning data quality, the number of fine-tuning samples, and few-shot selection methods. The models are evaluated on 19 diverse OOD datasets across three tasks: natural language inference (NLI), fact-checking, and hallucination detection in abstractive summarization. For the generated explanation evaluation, we conduct a human study on 13 selected models and study its correlation with the Acceptability score (T5-11B) and three other LLM-based reference-free metrics. Human evaluation shows that the Acceptability score correlates most strongly with human judgments, demonstrating its effectiveness in evaluating free-text explanations. Our findings reveal: 1) few annotated examples effectively adapt models for OOD explanation generation; 2) compared to sample selection strategies, fine-tuning data source has a larger impact on OOD performance; and 3) models with higher label prediction accuracy tend to produce better explanations, as reflected by higher Acceptability scores.

### Developmentally-plausible Working Memory Shapes a Critical Period for Language Acquisition 
[[arxiv](https://arxiv.org/abs/2502.04795)] [[cool](https://papers.cool/arxiv/2502.04795)] [[pdf](https://arxiv.org/pdf/2502.04795)]
> **Authors**: Masato Mita,Ryo Yoshida,Yohei Oseki
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 13 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models possess general linguistic abilities but acquire language less efficiently than humans. This study proposes a method for integrating the developmental characteristics of working memory during the critical period, a stage when human language acquisition is particularly efficient, into the training process of language models. The proposed method introduces a mechanism that initially constrains working memory during the early stages of training and gradually relaxes this constraint in an exponential manner as learning progresses. Targeted syntactic evaluation shows that the proposed method outperforms conventional methods without memory constraints or with static memory constraints. These findings not only provide new directions for designing data-efficient language models but also offer indirect evidence supporting the role of the developmental characteristics of working memory as the underlying mechanism of the critical period in language acquisition.

### S$^2$-MAD: Breaking the Token Barrier to Enhance Multi-Agent Debate Efficiency 
[[arxiv](https://arxiv.org/abs/2502.04790)] [[cool](https://papers.cool/arxiv/2502.04790)] [[pdf](https://arxiv.org/pdf/2502.04790)]
> **Authors**: Yuting Zeng,Weizhe Huang,Lei Jiang,Tongxuan Liu,Xitai Jin,Chen Tianying Tiana,Jing Li,Xiaohua Xu
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 16 pages, 5 figures
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large language models (LLMs) have demonstrated remarkable capabilities across various natural language processing (NLP) scenarios, but they still face challenges when handling complex arithmetic and logical reasoning tasks. While Chain-Of-Thought (CoT) reasoning, self-consistency (SC) and self-correction strategies have attempted to guide models in sequential, multi-step reasoning, Multi-agent Debate (MAD) has emerged as a viable approach for enhancing the reasoning capabilities of LLMs. By increasing both the number of agents and the frequency of debates, the performance of LLMs improves significantly. However, this strategy results in a significant increase in token costs, presenting a barrier to scalability. To address this challenge, we introduce a novel sparsification strategy designed to reduce token costs within MAD. This approach minimizes ineffective exchanges of information and unproductive discussions among agents, thereby enhancing the overall efficiency of the debate process. We conduct comparative experiments on multiple datasets across various models, demonstrating that our approach significantly reduces the token costs in MAD to a considerable extent. Specifically, compared to MAD, our approach achieves an impressive reduction of up to 94.5\% in token costs while maintaining performance degradation below 2.0\%.

### Probing Internal Representations of Multi-Word Verbs in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.04789)] [[cool](https://papers.cool/arxiv/2502.04789)] [[pdf](https://arxiv.org/pdf/2502.04789)]
> **Authors**: Hassane Kissane,Achim Schilling,Patrick Krauss
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: This study investigates the internal representations of verb-particle combinations, called multi-word verbs, within transformer-based large language models (LLMs), specifically examining how these models capture lexical and syntactic properties at different neural network layers. Using the BERT architecture, we analyze the representations of its layers for two different verb-particle constructions: phrasal verbs like 'give up' and prepositional verbs like 'look at'. Our methodology includes training probing classifiers on the internal representations to classify these categories at both word and sentence levels. The results indicate that the model's middle layers achieve the highest classification accuracies. To further analyze the nature of these distinctions, we conduct a data separability test using the Generalized Discrimination Value (GDV). While GDV results show weak linear separability between the two verb types, probing classifiers still achieve high accuracy, suggesting that representations of these linguistic categories may be non-linearly separable. This aligns with previous research indicating that linguistic distinctions in neural networks are not always encoded in a linearly separable manner. These findings computationally support usage-based claims on the representation of verb-particle constructions and highlight the complex interaction between neural network architectures and linguistic structures.

### SeDi-Instruct: Enhancing Alignment of Language Models through Self-Directed Instruction Generation 
[[arxiv](https://arxiv.org/abs/2502.04774)] [[cool](https://papers.cool/arxiv/2502.04774)] [[pdf](https://arxiv.org/pdf/2502.04774)]
> **Authors**: Jungwoo Kim,Minsang Kim,Sungjin Lee
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 12 pages, 12 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The rapid evolution of Large Language Models (LLMs) has enabled the industry to develop various AI-based services. Instruction tuning is considered essential in adapting foundation models for target domains to provide high-quality services to customers. A key challenge in instruction tuning is obtaining high-quality instruction data. Self-Instruct, which automatically generates instruction data using ChatGPT APIs, alleviates the data scarcity problem. To improve the quality of instruction data, Self-Instruct discards many of the instructions generated from ChatGPT, even though it is inefficient in terms of cost owing to many useless API calls. To generate high-quality instruction data at a low cost, we propose a novel data generation framework, Self-Direct Instruction generation (SeDi-Instruct), which employs diversity-based filtering and iterative feedback task generation. Diversity-based filtering maintains model accuracy without excessively discarding low-quality generated instructions by enhancing the diversity of instructions in a batch. This reduces the cost of synthesizing instruction data. The iterative feedback task generation integrates instruction generation and training tasks and utilizes information obtained during the training to create high-quality instruction sets. Our results show that SeDi-Instruct enhances the accuracy of AI models by 5.2%, compared with traditional methods, while reducing data generation costs by 36%.

### Concept Navigation and Classification via Open Source Large Language Model Processing 
[[arxiv](https://arxiv.org/abs/2502.04756)] [[cool](https://papers.cool/arxiv/2502.04756)] [[pdf](https://arxiv.org/pdf/2502.04756)]
> **Authors**: Maël Kubli
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 35 pages, 1 figure, 7 tabels
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: This paper presents a novel methodological framework for detecting and classifying latent constructs, including frames, narratives, and topics, from textual data using Open-Source Large Language Models (LLMs). The proposed hybrid approach combines automated summarization with human-in-the-loop validation to enhance the accuracy and interpretability of construct identification. By employing iterative sampling coupled with expert refinement, the framework guarantees methodological robustness and ensures conceptual precision. Applied to diverse data sets, including AI policy debates, newspaper articles on encryption, and the 20 Newsgroups data set, this approach demonstrates its versatility in systematically analyzing complex political discourses, media framing, and topic classification tasks.

### Evaluating Text Style Transfer Evaluation: Are There Any Reliable Metrics? 
[[arxiv](https://arxiv.org/abs/2502.04718)] [[cool](https://papers.cool/arxiv/2502.04718)] [[pdf](https://arxiv.org/pdf/2502.04718)]
> **Authors**: Sourabrata Mukherjee,Atul Kr. Ojha,John P. McCrae,Ondrej Dusek
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Text Style Transfer (TST) is the task of transforming a text to reflect a particular style while preserving its original content. Evaluating TST outputs is a multidimensional challenge, requiring the assessment of style transfer accuracy, content preservation, and naturalness. Using human evaluation is ideal but costly, same as in other natural language processing (NLP) tasks, however, automatic metrics for TST have not received as much attention as metrics for, e.g., machine translation or summarization. In this paper, we examine both set of existing and novel metrics from broader NLP tasks for TST evaluation, focusing on two popular subtasks-sentiment transfer and detoxification-in a multilingual context comprising English, Hindi, and Bengali. By conducting meta-evaluation through correlation with human judgments, we demonstrate the effectiveness of these metrics when used individually and in ensembles. Additionally, we investigate the potential of Large Language Models (LLMs) as tools for TST evaluation. Our findings highlight that certain advanced NLP metrics and experimental-hybrid-techniques, provide better insights than existing TST metrics for delivering more accurate, consistent, and reproducible TST evaluations.

### Enhancing Impression Change Prediction in Speed Dating Simulations Based on Speakers' Personalities 
[[arxiv](https://arxiv.org/abs/2502.04706)] [[cool](https://papers.cool/arxiv/2502.04706)] [[pdf](https://arxiv.org/pdf/2502.04706)]
> **Authors**: Kazuya Matsuo,Yoko Ishii,Atsushi Otsuka,Ryo Ishii,Hiroaki Sugiyama,Masahiro Mizukami,Tsunehiro Arimoto,Narichika Nomoto,Yoshihide Sato,Tetsuya Yamaguchi
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人机交互
- **Abstract**: This paper focuses on simulating text dialogues in which impressions between speakers improve during speed dating. This simulation involves selecting an utterance from multiple candidates generated by a text generation model that replicates a specific speaker's utterances, aiming to improve the impression of the speaker. Accurately selecting an utterance that improves the impression is crucial for the simulation. We believe that whether an utterance improves a dialogue partner's impression of the speaker may depend on the personalities of both parties. However, recent methods for utterance selection do not consider the impression per utterance or the personalities. To address this, we propose a method that predicts whether an utterance improves a partner's impression of the speaker, considering the personalities. The evaluation results showed that personalities are useful in predicting impression changes per utterance. Furthermore, we conducted a human evaluation of simulated dialogues using our method. The results showed that it could simulate dialogues more favorably received than those selected without considering personalities.

### ARR: Question Answering with Large Language Models via Analyzing, Retrieving, and Reasoning 
[[arxiv](https://arxiv.org/abs/2502.04689)] [[cool](https://papers.cool/arxiv/2502.04689)] [[pdf](https://arxiv.org/pdf/2502.04689)]
> **Authors**: Yuwei Yin,Giuseppe Carenini
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 20 pages. Code: https://github.com/YuweiYin/ARR
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Large language models (LLMs) achieve remarkable performance on challenging benchmarks that are often structured as multiple-choice question-answering (QA) tasks. Zero-shot Chain-of-Thought (CoT) prompting enhances reasoning in LLMs but provides only vague and generic guidance ("think step by step"). This paper introduces ARR, an intuitive and effective zero-shot prompting method that explicitly incorporates three key steps in QA solving: analyzing the intent of the question, retrieving relevant information, and reasoning step by step. Comprehensive experiments across diverse and challenging QA tasks demonstrate that ARR consistently improves the Baseline (without ARR prompting) and outperforms CoT. Ablation and case studies further validate the positive contributions of each component: analyzing, retrieving, and reasoning. Notably, intent analysis plays a vital role in ARR. Additionally, extensive evaluations across various model sizes, LLM series, and generation settings solidify the effectiveness, robustness, and generalizability of ARR.

### M-IFEval: Multilingual Instruction-Following Evaluation 
[[arxiv](https://arxiv.org/abs/2502.04688)] [[cool](https://papers.cool/arxiv/2502.04688)] [[pdf](https://arxiv.org/pdf/2502.04688)]
> **Authors**: Antoine Dussolle,Andrea Cardeña Díaz,Shota Sato,Peter Devine
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Instruction following is a core capability of modern Large language models (LLMs), making evaluating this capability essential to understanding these models. The Instruction Following Evaluation (IFEval) benchmark from the literature does this using objective criteria, offering a measure of LLM performance without subjective AI or human judgement. However, it only includes English instructions, limiting its ability to assess LLMs in other languages. We propose the Multilingual Instruction Following Evaluation (M-IFEval) benchmark, expanding the evaluation to French, Japanese, and Spanish, with both general and language-specific instructions. Applying this benchmark to 8 state-of-the-art LLMs, we find that benchmark performance across languages and instruction types can vary widely, underscoring the importance of a multilingual benchmark for evaluating LLMs in a diverse cultural context.

### AdParaphrase: Paraphrase Dataset for Analyzing Linguistic Features toward Generating Attractive Ad Texts 
[[arxiv](https://arxiv.org/abs/2502.04674)] [[cool](https://papers.cool/arxiv/2502.04674)] [[pdf](https://arxiv.org/pdf/2502.04674)]
> **Authors**: Soichiro Murakami,Peinan Zhang,Hidetaka Kamigaito,Hiroya Takamura,Manabu Okumura
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: Accepted to NAACL2025 Findings
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Effective linguistic choices that attract potential customers play crucial roles in advertising success. This study aims to explore the linguistic features of ad texts that influence human preferences. Although the creation of attractive ad texts is an active area of research, progress in understanding the specific linguistic features that affect attractiveness is hindered by several obstacles. First, human preferences are complex and influenced by multiple factors, including their content, such as brand names, and their linguistic styles, making analysis challenging. Second, publicly available ad text datasets that include human preferences are lacking, such as ad performance metrics and human feedback, which reflect people's interests. To address these problems, we present AdParaphrase, a paraphrase dataset that contains human preferences for pairs of ad texts that are semantically equivalent but differ in terms of wording and style. This dataset allows for preference analysis that focuses on the differences in linguistic features. Our analysis revealed that ad texts preferred by human judges have higher fluency, longer length, more nouns, and use of bracket symbols. Furthermore, we demonstrate that an ad text-generation model that considers these findings significantly improves the attractiveness of a given text. The dataset is publicly available at: https://github.com/CyberAgentAILab/AdParaphrase.

## 密码学和安全(cs.CR:Cryptography and Security)

### Enhanced Hybrid Deep Learning Approach for Botnet Attacks Detection in IoT Environment 
[[arxiv](https://arxiv.org/abs/2502.06138)] [[cool](https://papers.cool/arxiv/2502.06138)] [[pdf](https://arxiv.org/pdf/2502.06138)]
> **Authors**: A. Karthick kumar,S. Rathnamala,T. Vijayashanthi,M. Prabhananthakumar,Alavikunhu Panthakkan,Shadi Atalla,Wathiq Mansoor
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: 6 pages
- **标题**: None
- **领域**: 密码学和安全,计算机视觉和模式识别
- **Abstract**: Cyberattacks in an Internet of Things (IoT) environment can have significant impacts because of the interconnected nature of devices and systems. An attacker uses a network of compromised IoT devices in a botnet attack to carry out various harmful activities. Detecting botnet attacks poses several challenges because of the intricate and evolving nature of these threats. Botnet attacks erode trust in IoT devices and systems, undermining confidence in their security, reliability, and integrity. Deep learning techniques have significantly enhanced the detection of botnet attacks due to their ability to analyze and learn from complex patterns in data. This research proposed the stacking of Deep convolutional neural networks, Bi-Directional Long Short-Term Memory (Bi-LSTM), Bi-Directional Gated Recurrent Unit (Bi-GRU), and Recurrent Neural Networks (RNN) for botnet attacks detection. The UNSW-NB15 dataset is utilized for botnet attacks detection. According to experimental results, the proposed model accurately provides for the intricate patterns and features of botnet attacks, with a testing accuracy of 99.76%. The proposed model also identifies botnets with a high ROC-AUC curve value of 99.18%. A performance comparison of the proposed method with existing state-of-the-art models confirms its higher performance. The outcomes of this research could strengthen cyber security procedures and safeguard against new attacks.

### A Conditional Tabular GAN-Enhanced Intrusion Detection System for Rare Attacks in IoT Networks 
[[arxiv](https://arxiv.org/abs/2502.06031)] [[cool](https://papers.cool/arxiv/2502.06031)] [[pdf](https://arxiv.org/pdf/2502.06031)]
> **Authors**: Safaa Menssouri,El Mehdi Amhoud
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,机器学习
- **Abstract**: Internet of things (IoT) networks, boosted by 6G technology, are transforming various industries. However, their widespread adoption introduces significant security risks, particularly in detecting rare but potentially damaging cyber-attacks. This makes the development of robust IDS crucial for monitoring network traffic and ensuring their safety. Traditional IDS often struggle with detecting rare attacks due to severe class imbalances in IoT data. In this paper, we propose a novel two-stage system called conditional tabular generative synthetic minority data generation with deep neural network (CTGSM-DNN). In the first stage, a conditional tabular generative adversarial network (CTGAN) is employed to generate synthetic data for rare attack classes. In the second stage, the SMOTEENN method is applied to improve dataset quality. The full study was conducted using the CSE-CIC-IDS2018 dataset, and we assessed the performance of the proposed IDS using different evaluation metrics. The experimental results demonstrated the effectiveness of the proposed multiclass classifier, achieving an overall accuracy of 99.90% and 80% accuracy in detecting rare attacks.

### Mitigating Sensitive Information Leakage in LLMs4Code through Machine Unlearning 
[[arxiv](https://arxiv.org/abs/2502.05739)] [[cool](https://papers.cool/arxiv/2502.05739)] [[pdf](https://arxiv.org/pdf/2502.05739)]
> **Authors**: Ruotong Geng,Mingyang Geng,Shangwen Wang,Haotian Wang,Zhipeng Lin,Dezun Dong
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: 11 pages
- **标题**: None
- **领域**: 密码学和安全,人工智能,软件工程
- **Abstract**: Large Language Models for Code (LLMs4Code) excel at code generation tasks, yielding promise to release developers from huge software development burdens. Nonetheless, these models have been shown to suffer from the significant privacy risks due to the potential leakage of sensitive information embedded during training, known as the memorization problem. Addressing this issue is crucial for ensuring privacy compliance and upholding user trust, but till now there is a dearth of dedicated studies in the literature that focus on this specific direction. Recently, machine unlearning has emerged as a promising solution by enabling models to "forget" sensitive information without full retraining, offering an efficient and scalable approach compared to traditional data cleaning methods. In this paper, we empirically evaluate the effectiveness of unlearning techniques for addressing privacy concerns in LLMs4Code.Specifically, we investigate three state-of-the-art unlearning algorithms and three well-known open-sourced LLMs4Code, on a benchmark that takes into consideration both the privacy data to be forgotten as well as the code generation capabilites of these models. Results show that it is feasible to mitigate the privacy concerns of LLMs4Code through machine unlearning while maintain their code generation capabilities at the same time. We also dissect the forms of privacy protection/leakage after unlearning and observe that there is a shift from direct leakage to indirect leakage, which underscores the need for future studies addressing this risk.

### Adversarial Machine Learning: Attacks, Defenses, and Open Challenges 
[[arxiv](https://arxiv.org/abs/2502.05637)] [[cool](https://papers.cool/arxiv/2502.05637)] [[pdf](https://arxiv.org/pdf/2502.05637)]
> **Authors**: Pranav K Jha
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Adversarial Machine Learning (AML) addresses vulnerabilities in AI systems where adversaries manipulate inputs or training data to degrade performance. This article provides a comprehensive analysis of evasion and poisoning attacks, formalizes defense mechanisms with mathematical rigor, and discusses the challenges of implementing robust solutions in adaptive threat models. Additionally, it highlights open challenges in certified robustness, scalability, and real-world deployment.

### Detecting APT Malware Command and Control over HTTP(S) Using Contextual Summaries 
[[arxiv](https://arxiv.org/abs/2502.05367)] [[cool](https://papers.cool/arxiv/2502.05367)] [[pdf](https://arxiv.org/pdf/2502.05367)]
> **Authors**: Almuthanna Alageel,Sergio Maffeis,Imperial College London
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 22 pages, 9 figures. In: Susilo, W., Chen, X., Guo, F., Zhang, Y., Intan, R. (eds) Information Security. ISC 2022
- **标题**: None
- **领域**: 密码学和安全,机器学习,网络和互联网架构
- **Abstract**: Advanced Persistent Threats (APTs) are among the most sophisticated threats facing critical organizations worldwide. APTs employ specific tactics, techniques, and procedures (TTPs) which make them difficult to detect in comparison to frequent and aggressive attacks. In fact, current network intrusion detection systems struggle to detect APTs communications, allowing such threats to persist unnoticed on victims' machines for months or even years. In this paper, we present EarlyCrow, an approach to detect APT malware command and control over HTTP(S) using contextual summaries. The design of EarlyCrow is informed by a novel threat model focused on TTPs present in traffic generated by tools recently used as part of APT campaigns. The threat model highlights the importance of the context around the malicious connections, and suggests traffic attributes which help APT detection. EarlyCrow defines a novel multipurpose network flow format called PairFlow, which is leveraged to build the contextual summary of a PCAP capture, representing key behavioral, statistical and protocol information relevant to APT TTPs. We evaluate the effectiveness of EarlyCrow on unseen APTs obtaining a headline macro average F1-score of 93.02% with FPR of $0.74%.

### BitAbuse: A Dataset of Visually Perturbed Texts for Defending Phishing Attacks 
[[arxiv](https://arxiv.org/abs/2502.05225)] [[cool](https://papers.cool/arxiv/2502.05225)] [[pdf](https://arxiv.org/pdf/2502.05225)]
> **Authors**: Hanyong Lee,Chaelyn Lee,Yongjae Lee,Jaesung Lee
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-10
> **comment**: 18 pages, To appear in the Annual Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics 2025
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Phishing often targets victims through visually perturbed texts to bypass security systems. The noise contained in these texts functions as an adversarial attack, designed to deceive language models and hinder their ability to accurately interpret the content. However, since it is difficult to obtain sufficient phishing cases, previous studies have used synthetic datasets that do not contain real-world cases. In this study, we propose the BitAbuse dataset, which includes real-world phishing cases, to address the limitations of previous research. Our dataset comprises a total of 325,580 visually perturbed texts. The dataset inputs are drawn from the raw corpus, consisting of visually perturbed sentences and sentences generated through an artificial perturbation process. Each input sentence is labeled with its corresponding ground truth, representing the restored, non-perturbed version. Language models trained on our proposed dataset demonstrated significantly better performance compared to previous methods, achieving an accuracy of approximately 96%. Our analysis revealed a significant gap between real-world and synthetic examples, underscoring the value of our dataset for building reliable pre-trained models for restoration tasks. We release the BitAbuse dataset, which includes real-world phishing cases annotated with visual perturbations, to support future research in adversarial attack defense.

### A Survey on Backdoor Threats in Large Language Models (LLMs): Attacks, Defenses, and Evaluations 
[[arxiv](https://arxiv.org/abs/2502.05224)] [[cool](https://papers.cool/arxiv/2502.05224)] [[pdf](https://arxiv.org/pdf/2502.05224)]
> **Authors**: Yihe Zhou,Tao Ni,Wei-Bin Lee,Qingchuan Zhao
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Large Language Models (LLMs) have achieved significantly advanced capabilities in understanding and generating human language text, which have gained increasing popularity over recent years. Apart from their state-of-the-art natural language processing (NLP) performance, considering their widespread usage in many industries, including medicine, finance, education, etc., security concerns over their usage grow simultaneously. In recent years, the evolution of backdoor attacks has progressed with the advancement of defense mechanisms against them and more well-developed features in the LLMs. In this paper, we adapt the general taxonomy for classifying machine learning attacks on one of the subdivisions - training-time white-box backdoor attacks. Besides systematically classifying attack methods, we also consider the corresponding defense methods against backdoor attacks. By providing an extensive summary of existing works, we hope this survey can serve as a guideline for inspiring future research that further extends the attack scenarios and creates a stronger defense against them for more robust LLMs.

### KDA: A Knowledge-Distilled Attacker for Generating Diverse Prompts to Jailbreak LLMs 
[[arxiv](https://arxiv.org/abs/2502.05223)] [[cool](https://papers.cool/arxiv/2502.05223)] [[pdf](https://arxiv.org/pdf/2502.05223)]
> **Authors**: Buyun Liang,Kwan Ho Ryan Chan,Darshan Thaker,Jinqi Luo,René Vidal
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能,计算语言学,机器学习
- **Abstract**: Jailbreak attacks exploit specific prompts to bypass LLM safeguards, causing the LLM to generate harmful, inappropriate, and misaligned content. Current jailbreaking methods rely heavily on carefully designed system prompts and numerous queries to achieve a single successful attack, which is costly and impractical for large-scale red-teaming. To address this challenge, we propose to distill the knowledge of an ensemble of SOTA attackers into a single open-source model, called Knowledge-Distilled Attacker (KDA), which is finetuned to automatically generate coherent and diverse attack prompts without the need for meticulous system prompt engineering. Compared to existing attackers, KDA achieves higher attack success rates and greater cost-time efficiency when targeting multiple SOTA open-source and commercial black-box LLMs. Furthermore, we conducted a quantitative diversity analysis of prompts generated by baseline methods and KDA, identifying diverse and ensemble attacks as key factors behind KDA's effectiveness and efficiency.

### Aero-LLM: A Distributed Framework for Secure UAV Communication and Intelligent Decision-Making 
[[arxiv](https://arxiv.org/abs/2502.05220)] [[cool](https://papers.cool/arxiv/2502.05220)] [[pdf](https://arxiv.org/pdf/2502.05220)]
> **Authors**: Balakrishnan Dharmalingam,Rajdeep Mukherjee,Brett Piggott,Guohuan Feng,Anyi Liu
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-10
> **comment**: This manuscript was accepted by the 1st International Workshop on Integrated Sensing, Communication, and Computing in Internet of Things (IoT) Systems at the The 33rd International Conference on Computer Communications and Networks (ICCCN 2024)
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Increased utilization of unmanned aerial vehicles (UAVs) in critical operations necessitates secure and reliable communication with Ground Control Stations (GCS). This paper introduces Aero-LLM, a framework integrating multiple Large Language Models (LLMs) to enhance UAV mission security and operational efficiency. Unlike conventional singular LLMs, Aero-LLM leverages multiple specialized LLMs for various tasks, such as inferencing, anomaly detection, and forecasting, deployed across onboard systems, edge, and cloud servers. This dynamic, distributed architecture reduces performance bottleneck and increases security capabilities. Aero-LLM's evaluation demonstrates outstanding task-specific metrics and robust defense against cyber threats, significantly enhancing UAV decision-making and operational capabilities and security resilience against cyber attacks, setting a new standard for secure, intelligent UAV operations.

### Watermarking across Modalities for Content Tracing and Generative AI 
[[arxiv](https://arxiv.org/abs/2502.05215)] [[cool](https://papers.cool/arxiv/2502.05215)] [[pdf](https://arxiv.org/pdf/2502.05215)]
> **Authors**: Pierre Fernandez
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-10
> **comment**: PhD thesis - webpage available at https://pierrefdz.github.io/publications/phd-thesis
- **标题**: None
- **领域**: 密码学和安全,人工智能,机器学习
- **Abstract**: Watermarking embeds information into digital content like images, audio, or text, imperceptible to humans but robustly detectable by specific algorithms. This technology has important applications in many challenges of the industry such as content moderation, tracing AI-generated content, and monitoring the usage of AI models. The contributions of this thesis include the development of new watermarking techniques for images, audio, and text. We first introduce methods for active moderation of images on social platforms. We then develop specific techniques for AI-generated content. We specifically demonstrate methods to adapt latent generative models to embed watermarks in all generated content, identify watermarked sections in speech, and improve watermarking in large language models with tests that ensure low false positive rates. Furthermore, we explore the use of digital watermarking to detect model misuse, including the detection of watermarks in language models fine-tuned on watermarked text, and introduce training-free watermarks for the weights of large transformers. Through these contributions, the thesis provides effective solutions for the challenges posed by the increasing use of generative AI models and the need for model monitoring and content moderation. It finally examines the challenges and limitations of watermarking techniques and discuss potential future directions for research in this area.

### DERMARK: A Dynamic, Efficient and Robust Multi-bit Watermark for Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.05213)] [[cool](https://papers.cool/arxiv/2502.05213)] [[pdf](https://arxiv.org/pdf/2502.05213)]
> **Authors**: Qihao Lin,Chen Tang,Lan zhang,Junyang zhang,Xiangyang Li
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-10
> **comment**: 8 pages, 15 figures
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Well-trained large language models (LLMs) present significant risks, including potential malicious use and copyright infringement. Current studies aim to trace the distribution of LLM-generated texts by implicitly embedding watermarks. Among these, the single-bit watermarking method can only determine whether a given text was generated by an LLM. In contrast, the multi-bit watermarking method embeds richer information into the generated text, which can identify which LLM generated and distributed a given text to which user. However, existing efforts embed the multi-bit watermark directly into the generated text without accounting for its watermarking capacity. This approach can result in embedding failures when the text's watermarking capacity is insufficient. In this paper, we derive the watermark embedding distribution based on the logits of LLMs and propose a formal inequality to segment the text optimally for watermark embedding. Building on this foundation, we propose DERMARK, a dynamic, efficient, and robust multi-bit watermarking method. DERMARK divides the text into segments of varying lengths for each bit embedding, adaptively matching the text's capacity. It achieves this with negligible overhead and robust performance against text editing by minimizing watermark extraction loss. Comprehensive experiments demonstrate that, compared to the SOTA method, our method reduces the number of tokens required for embedding each bit by 20\%, reduces watermark embedding time by 50\%, and is robust to text editing and watermark erasure attacks.

### Model Tampering Attacks Enable More Rigorous Evaluations of LLM Capabilities 
[[arxiv](https://arxiv.org/abs/2502.05209)] [[cool](https://papers.cool/arxiv/2502.05209)] [[pdf](https://arxiv.org/pdf/2502.05209)]
> **Authors**: Zora Che,Stephen Casper,Robert Kirk,Anirudh Satheesh,Stewart Slocum,Lev E McKinney,Rohit Gandikota,Aidan Ewart,Domenic Rosati,Zichu Wu,Zikui Cai,Bilal Chughtai,Yarin Gal,Furong Huang,Dylan Hadfield-Menell
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Evaluations of large language model (LLM) risks and capabilities are increasingly being incorporated into AI risk management and governance frameworks. Currently, most risk evaluations are conducted by designing inputs that elicit harmful behaviors from the system. However, a fundamental limitation of this approach is that the harmfulness of the behaviors identified during any particular evaluation can only lower bound the model's worst-possible-case behavior. As a complementary method for eliciting harmful behaviors, we propose evaluating LLMs with model tampering attacks which allow for modifications to latent activations or weights. We pit state-of-the-art techniques for removing harmful LLM capabilities against a suite of 5 input-space and 6 model tampering attacks. In addition to benchmarking these methods against each other, we show that (1) model resilience to capability elicitation attacks lies on a low-dimensional robustness subspace; (2) the attack success rate of model tampering attacks can empirically predict and offer conservative estimates for the success of held-out input-space attacks; and (3) state-of-the-art unlearning methods can easily be undone within 16 steps of fine-tuning. Together these results highlight the difficulty of removing harmful LLM capabilities and show that model tampering attacks enable substantially more rigorous evaluations than input-space attacks alone. We release models at https://huggingface.co/LLM-GAT

### Mitigation of Camouflaged Adversarial Attacks in Autonomous Vehicles--A Case Study Using CARLA Simulator 
[[arxiv](https://arxiv.org/abs/2502.05208)] [[cool](https://papers.cool/arxiv/2502.05208)] [[pdf](https://arxiv.org/pdf/2502.05208)]
> **Authors**: Yago Romano Martinez,Brady Carter,Abhijeet Solanki,Wesam Al Amiri,Syed Rafay Hasan,Terry N. Guo
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能,机器学习
- **Abstract**: Autonomous vehicles (AVs) rely heavily on cameras and artificial intelligence (AI) to make safe and accurate driving decisions. However, since AI is the core enabling technology, this raises serious cyber threats that hinder the large-scale adoption of AVs. Therefore, it becomes crucial to analyze the resilience of AV security systems against sophisticated attacks that manipulate camera inputs, deceiving AI models. In this paper, we develop camera-camouflaged adversarial attacks targeting traffic sign recognition (TSR) in AVs. Specifically, if the attack is initiated by modifying the texture of a stop sign to fool the AV's object detection system, thereby affecting the AV actuators. The attack's effectiveness is tested using the CARLA AV simulator and the results show that such an attack can delay the auto-braking response to the stop sign, resulting in potential safety issues. We conduct extensive experiments under various conditions, confirming that our new attack is effective and robust. Additionally, we address the attack by presenting mitigation strategies. The proposed attack and defense methods are applicable to other end-to-end trained autonomous cyber-physical systems.

### Safety at Scale: A Comprehensive Survey of Large Model Safety 
[[arxiv](https://arxiv.org/abs/2502.05206)] [[cool](https://papers.cool/arxiv/2502.05206)] [[pdf](https://arxiv.org/pdf/2502.05206)]
> **Authors**: Xingjun Ma,Yifeng Gao,Yixu Wang,Ruofan Wang,Xin Wang,Ye Sun,Yifan Ding,Hengyuan Xu,Yunhao Chen,Yunhan Zhao,Hanxun Huang,Yige Li,Jiaming Zhang,Xiang Zheng,Yang Bai,Zuxuan Wu,Xipeng Qiu,Jingfeng Zhang,Yiming Li,Jun Sun,Cong Wang,Jindong Gu,Baoyuan Wu,Siheng Chen,Tianwei Zhang, et al. (19 additional authors not shown)
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-10
> **comment**: 47 pages, 3 figures, 11 tables GitHub: https://github.com/xingjunm/Awesome-Large-Model-Safety
- **标题**: None
- **领域**: 密码学和安全,人工智能,计算语言学,计算机视觉和模式识别
- **Abstract**: The rapid advancement of large models, driven by their exceptional abilities in learning and generalization through large-scale pre-training, has reshaped the landscape of Artificial Intelligence (AI). These models are now foundational to a wide range of applications, including conversational AI, recommendation systems, autonomous driving, content generation, medical diagnostics, and scientific discovery. However, their widespread deployment also exposes them to significant safety risks, raising concerns about robustness, reliability, and ethical implications. This survey provides a systematic review of current safety research on large models, covering Vision Foundation Models (VFMs), Large Language Models (LLMs), Vision-Language Pre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models (DMs), and large-model-based Agents. Our contributions are summarized as follows: (1) We present a comprehensive taxonomy of safety threats to these models, including adversarial attacks, data poisoning, backdoor attacks, jailbreak and prompt injection attacks, energy-latency attacks, data and model extraction attacks, and emerging agent-specific threats. (2) We review defense strategies proposed for each type of attacks if available and summarize the commonly used datasets and benchmarks for safety research. (3) Building on this, we identify and discuss the open challenges in large model safety, emphasizing the need for comprehensive safety evaluations, scalable and effective defense mechanisms, and sustainable data practices. More importantly, we highlight the necessity of collective efforts from the research community and international collaboration. Our work can serve as a useful reference for researchers and practitioners, fostering the ongoing development of comprehensive defense systems and platforms to safeguard AI models.

### MELON: Indirect Prompt Injection Defense via Masked Re-execution and Tool Comparison 
[[arxiv](https://arxiv.org/abs/2502.05174)] [[cool](https://papers.cool/arxiv/2502.05174)] [[pdf](https://arxiv.org/pdf/2502.05174)]
> **Authors**: Kaijie Zhu,Xianjun Yang,Jindong Wang,Wenbo Guo,William Yang Wang
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Recent research has explored that LLM agents are vulnerable to indirect prompt injection (IPI) attacks, where malicious tasks embedded in tool-retrieved information can redirect the agent to take unauthorized actions. Existing defenses against IPI have significant limitations: either require essential model training resources, lack effectiveness against sophisticated attacks, or harm the normal utilities. We present MELON (Masked re-Execution and TooL comparisON), a novel IPI defense. Our approach builds on the observation that under a successful attack, the agent's next action becomes less dependent on user tasks and more on malicious tasks. Following this, we design MELON to detect attacks by re-executing the agent's trajectory with a masked user prompt modified through a masking function. We identify an attack if the actions generated in the original and masked executions are similar. We also include three key designs to reduce the potential false positives and false negatives. Extensive evaluation on the IPI benchmark AgentDojo demonstrates that MELON outperforms SOTA defenses in both attack prevention and utility preservation. Moreover, we show that combining MELON with a SOTA prompt augmentation defense (denoted as MELON-Aug) further improves its performance. We also conduct a detailed ablation study to validate our key designs.

### The Rising Threat to Emerging AI-Powered Search Engines 
[[arxiv](https://arxiv.org/abs/2502.04951)] [[cool](https://papers.cool/arxiv/2502.04951)] [[pdf](https://arxiv.org/pdf/2502.04951)]
> **Authors**: Zeren Luo,Zifan Peng,Yule Liu,Zhen Sun,Mingchen Li,Jingyi Zheng,Xinlei He
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能,机器学习
- **Abstract**: Recent advancements in Large Language Models (LLMs) have significantly enhanced the capabilities of AI-Powered Search Engines (AIPSEs), offering precise and efficient responses by integrating external databases with pre-existing knowledge. However, we observe that these AIPSEs raise risks such as quoting malicious content or citing malicious websites, leading to harmful or unverified information dissemination. In this study, we conduct the first safety risk quantification on seven production AIPSEs by systematically defining the threat model, risk level, and evaluating responses to various query types. With data collected from PhishTank, ThreatBook, and LevelBlue, our findings reveal that AIPSEs frequently generate harmful content that contains malicious URLs even with benign queries (e.g., with benign keywords). We also observe that directly query URL will increase the risk level while query with natural language will mitigate such risk. We further perform two case studies on online document spoofing and phishing to show the ease of deceiving AIPSEs in the real-world setting. To mitigate these risks, we develop an agent-based defense with a GPT-4o-based content refinement tool and an XGBoost-based URL detector. Our evaluation shows that our defense can effectively reduce the risk but with the cost of reducing available information. Our research highlights the urgent need for robust safety measures in AIPSEs.

### On the Difficulty of Constructing a Robust and Publicly-Detectable Watermark 
[[arxiv](https://arxiv.org/abs/2502.04901)] [[cool](https://papers.cool/arxiv/2502.04901)] [[pdf](https://arxiv.org/pdf/2502.04901)]
> **Authors**: Jaiden Fairoze,Guillermo Ortiz-Jiménez,Mel Vecerik,Somesh Jha,Sven Gowal
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,机器学习
- **Abstract**: This work investigates the theoretical boundaries of creating publicly-detectable schemes to enable the provenance of watermarked imagery. Metadata-based approaches like C2PA provide unforgeability and public-detectability. ML techniques offer robust retrieval and watermarking. However, no existing scheme combines robustness, unforgeability, and public-detectability. In this work, we formally define such a scheme and establish its existence. Although theoretically possible, we find that at present, it is intractable to build certain components of our scheme without a leap in deep learning capabilities. We analyze these limitations and propose research directions that need to be addressed before we can practically realize robust and publicly-verifiable provenance.

### Enhancing SQL Injection Detection and Prevention Using Generative Models 
[[arxiv](https://arxiv.org/abs/2502.04786)] [[cool](https://papers.cool/arxiv/2502.04786)] [[pdf](https://arxiv.org/pdf/2502.04786)]
> **Authors**: Naga Sai Dasari,Atta Badii,Armin Moin,Ahmed Ashlam
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 13 pages, 22 Figures, 1 Table
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: SQL Injection (SQLi) continues to pose a significant threat to the security of web applications, enabling attackers to manipulate databases and access sensitive information without authorisation. Although advancements have been made in detection techniques, traditional signature-based methods still struggle to identify sophisticated SQL injection attacks that evade predefined patterns. As SQLi attacks evolve, the need for more adaptive detection systems becomes crucial. This paper introduces an innovative approach that leverages generative models to enhance SQLi detection and prevention mechanisms. By incorporating Variational Autoencoders (VAE), Conditional Wasserstein GAN with Gradient Penalty (CWGAN-GP), and U-Net, synthetic SQL queries were generated to augment training datasets for machine learning models. The proposed method demonstrated improved accuracy in SQLi detection systems by reducing both false positives and false negatives. Extensive empirical testing further illustrated the ability of the system to adapt to evolving SQLi attack patterns, resulting in enhanced precision and robustness.

### Enhancing Phishing Email Identification with Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.04759)] [[cool](https://papers.cool/arxiv/2502.04759)] [[pdf](https://arxiv.org/pdf/2502.04759)]
> **Authors**: Catherine Lee
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 9 pages, 5 figures
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Phishing has long been a common tactic used by cybercriminals and continues to pose a significant threat in today's digital world. When phishing attacks become more advanced and sophisticated, there is an increasing need for effective methods to detect and prevent them. To address the challenging problem of detecting phishing emails, researchers have developed numerous solutions, in particular those based on machine learning (ML) algorithms. In this work, we take steps to study the efficacy of large language models (LLMs) in detecting phishing emails. The experiments show that the LLM achieves a high accuracy rate at high precision; importantly, it also provides interpretable evidence for the decisions.

## 计算机视觉和模式识别(cs.CV:Computer Vision and Pattern Recognition)

### Integrating Sequence and Image Modeling in Irregular Medical Time Series Through Self-Supervised Learning 
[[arxiv](https://arxiv.org/abs/2502.06134)] [[cool](https://papers.cool/arxiv/2502.06134)] [[pdf](https://arxiv.org/pdf/2502.06134)]
> **Authors**: Liuqing Chen,Shuhong Xiao,Shixian Ding,Shanhai Hu,Lingyun Sun
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: 9 pages, 2 figures, AAAI2025
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Medical time series are often irregular and face significant missingness, posing challenges for data analysis and clinical decision-making. Existing methods typically adopt a single modeling perspective, either treating series data as sequences or transforming them into image representations for further classification. In this paper, we propose a joint learning framework that incorporates both sequence and image representations. We also design three self-supervised learning strategies to facilitate the fusion of sequence and image representations, capturing a more generalizable joint representation. The results indicate that our approach outperforms seven other state-of-the-art models in three representative real-world clinical datasets. We further validate our approach by simulating two major types of real-world missingness through leave-sensors-out and leave-samples-out techniques. The results demonstrate that our approach is more robust and significantly surpasses other baselines in terms of classification performance.

### Self-Correcting Decoding with Generative Feedback for Mitigating Hallucinations in Large Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2502.06130)] [[cool](https://papers.cool/arxiv/2502.06130)] [[pdf](https://arxiv.org/pdf/2502.06130)]
> **Authors**: Ce Zhang,Zifu Wan,Zhehan Kan,Martin Q. Ma,Simon Stepputtis,Deva Ramanan,Russ Salakhutdinov,Louis-Philippe Morency,Katia Sycara,Yaqi Xie
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: Accepted by ICLR 2025. Project page:https://zhangce01.github.io/DeGF/
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算语言学
- **Abstract**: While recent Large Vision-Language Models (LVLMs) have shown remarkable performance in multi-modal tasks, they are prone to generating hallucinatory text responses that do not align with the given visual input, which restricts their practical applicability in real-world scenarios. In this work, inspired by the observation that the text-to-image generation process is the inverse of image-conditioned response generation in LVLMs, we explore the potential of leveraging text-to-image generative models to assist in mitigating hallucinations in LVLMs. We discover that generative models can offer valuable self-feedback for mitigating hallucinations at both the response and token levels. Building on this insight, we introduce self-correcting Decoding with Generative Feedback (DeGF), a novel training-free algorithm that incorporates feedback from text-to-image generative models into the decoding process to effectively mitigate hallucinations in LVLMs. Specifically, DeGF generates an image from the initial response produced by LVLMs, which acts as an auxiliary visual reference and provides self-feedback to verify and correct the initial response through complementary or contrastive decoding. Extensive experimental results validate the effectiveness of our approach in mitigating diverse types of hallucinations, consistently surpassing state-of-the-art methods across six benchmarks. Code is available at https://github.com/zhangce01/DeGF.

### Fair-MoE: Fairness-Oriented Mixture of Experts in Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2502.06094)] [[cool](https://papers.cool/arxiv/2502.06094)] [[pdf](https://arxiv.org/pdf/2502.06094)]
> **Authors**: Peiran Wang,Linjie Tong,Jiaxiang Liu,Zuozhu Liu
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Fairness is a fundamental principle in medical ethics. Vision Language Models (VLMs) have shown significant potential in the medical field due to their ability to leverage both visual and linguistic contexts, reducing the need for large datasets and enabling the performance of complex tasks. However, the exploration of fairness within VLM applications remains limited. Applying VLMs without a comprehensive analysis of fairness could lead to concerns about equal treatment opportunities and diminish public trust in medical deep learning models. To build trust in medical VLMs, we propose Fair-MoE, a model specifically designed to ensure both fairness and effectiveness. Fair-MoE comprises two key components: \textit{the Fairness-Oriented Mixture of Experts (FO-MoE)} and \textit{the Fairness-Oriented Loss (FOL)}. FO-MoE is designed to leverage the expertise of various specialists to filter out biased patch embeddings and use an ensemble approach to extract more equitable information relevant to specific tasks. FOL is a novel fairness-oriented loss function that not only minimizes the distances between different attributes but also optimizes the differences in the dispersion of various attributes' distributions. Extended experiments demonstrate the effectiveness and fairness of Fair-MoE. Tested on the Harvard-FairVLMed dataset, Fair-MoE showed improvements in both fairness and accuracy across all four attributes. Code will be publicly available.

### Traveling Waves Integrate Spatial Information Through Time 
[[arxiv](https://arxiv.org/abs/2502.06034)] [[cool](https://papers.cool/arxiv/2502.06034)] [[pdf](https://arxiv.org/pdf/2502.06034)]
> **Authors**: Mozes Jacobs,Roberto C. Budzinski,Lyle Muller,Demba Ba,T. Anderson Keller
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Traveling waves of neural activity are widely observed in the brain, but their precise computational function remains unclear. One prominent hypothesis is that they enable the transfer and integration of spatial information across neural populations. However, few computational models have explored how traveling waves might be harnessed to perform such integrative processing. Drawing inspiration from the famous "Can one hear the shape of a drum?" problem -- which highlights how normal modes of wave dynamics encode geometric information -- we investigate whether similar principles can be leveraged in artificial neural networks. Specifically, we introduce convolutional recurrent neural networks that learn to produce traveling waves in their hidden states in response to visual stimuli, enabling spatial integration. By then treating these wave-like activation sequences as visual representations themselves, we obtain a powerful representational space that outperforms local feed-forward networks on tasks requiring global spatial context. In particular, we observe that traveling waves effectively expand the receptive field of locally connected neurons, supporting long-range encoding and communication of information. We demonstrate that models equipped with this mechanism solve visual semantic segmentation tasks demanding global integration, significantly outperforming local feed-forward models and rivaling non-local U-Net models with fewer parameters. As a first step toward traveling-wave-based communication and visual representation in artificial networks, our findings suggest wave-dynamics may provide efficiency and training stability benefits, while simultaneously offering a new framework for connecting models to biological recordings of neural activity.

### DiTASK: Multi-Task Fine-Tuning with Diffeomorphic Transformations 
[[arxiv](https://arxiv.org/abs/2502.06029)] [[cool](https://papers.cool/arxiv/2502.06029)] [[pdf](https://arxiv.org/pdf/2502.06029)]
> **Authors**: Krishna Sri Ipsit Mantri,Carola-Bibiane Schönlieb,Bruno Ribeiro,Chaim Baskin,Moshe Eliasof
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: 14 pages, cvpr template
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Pre-trained Vision Transformers now serve as powerful tools for computer vision. Yet, efficiently adapting them for multiple tasks remains a challenge that arises from the need to modify the rich hidden representations encoded by the learned weight matrices, without inducing interference between tasks. Current parameter-efficient methods like LoRA, which apply low-rank updates, force tasks to compete within constrained subspaces, ultimately degrading performance. We introduce DiTASK a novel Diffeomorphic Multi-Task Fine-Tuning approach that maintains pre-trained representations by preserving weight matrix singular vectors, while enabling task-specific adaptations through neural diffeomorphic transformations of the singular values. By following this approach, DiTASK enables both shared and task-specific feature modulations with minimal added parameters. Our theoretical analysis shows that DITASK achieves full-rank updates during optimization, preserving the geometric structure of pre-trained features, and establishing a new paradigm for efficient multi-task learning (MTL). Our experiments on PASCAL MTL and NYUD show that DiTASK achieves state-of-the-art performance across four dense prediction tasks, using 75% fewer parameters than existing methods.

### Dual Caption Preference Optimization for Diffusion Models 
[[arxiv](https://arxiv.org/abs/2502.06023)] [[cool](https://papers.cool/arxiv/2502.06023)] [[pdf](https://arxiv.org/pdf/2502.06023)]
> **Authors**: Amir Saeidi,Yiran Luo,Agneet Chatterjee,Shamanthak Hegde,Bimsara Pathiraja,Yezhou Yang,Chitta Baral
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Recent advancements in human preference optimization, originally developed for Large Language Models (LLMs), have shown significant potential in improving text-to-image diffusion models. These methods aim to learn the distribution of preferred samples while distinguishing them from less preferred ones. However, existing preference datasets often exhibit overlap between these distributions, leading to a conflict distribution. Additionally, we identified that input prompts contain irrelevant information for less preferred images, limiting the denoising network's ability to accurately predict noise in preference optimization methods, known as the irrelevant prompt issue. To address these challenges, we propose Dual Caption Preference Optimization (DCPO), a novel approach that utilizes two distinct captions to mitigate irrelevant prompts. To tackle conflict distribution, we introduce the Pick-Double Caption dataset, a modified version of Pick-a-Pic v2 with separate captions for preferred and less preferred images. We further propose three different strategies for generating distinct captions: captioning, perturbation, and hybrid methods. Our experiments show that DCPO significantly improves image quality and relevance to prompts, outperforming Stable Diffusion (SD) 2.1, SFT_Chosen, Diffusion-DPO, and MaPO across multiple metrics, including Pickscore, HPSv2.1, GenEval, CLIPscore, and ImageReward, fine-tuned on SD 2.1 as the backbone.

### Noise is an Efficient Learner for Zero-Shot Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2502.06019)] [[cool](https://papers.cool/arxiv/2502.06019)] [[pdf](https://arxiv.org/pdf/2502.06019)]
> **Authors**: Raza Imam,Asif Hanif,Jian Zhang,Khaled Waleed Dawoud,Yova Kementchedjhieva,Mohammad Yaqub
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: Our code is available at https://github.com/Razaimam45/TNT
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Recently, test-time adaptation has garnered attention as a method for tuning models without labeled data. The conventional modus operandi for adapting pre-trained vision-language models (VLMs) during test-time primarily focuses on tuning learnable prompts; however, this approach overlooks potential distribution shifts in the visual representations themselves. In this work, we address this limitation by introducing Test-Time Noise Tuning (TNT), a novel method for handling unpredictable shifts in the visual space. TNT leverages, for the first time, a noise adaptation strategy that optimizes learnable noise directly in the visual input space, enabling adaptive feature learning from a single test sample. We further introduce a novel approach for inter-view representation alignment by explicitly enforcing coherence in embedding distances, ensuring consistent feature representations across views. Combined with scaled logits and confident view selection at inference, TNT substantially enhances VLM generalization and calibration, achieving average gains of +7.38% on natural distributions benchmark and +0.80% on cross-dataset evaluations over zero-shot CLIP. These improvements lay a strong foundation for adaptive out-of-distribution handling.

### A Comprehensive Survey on Image Signal Processing Approaches for Low-Illumination Image Enhancement 
[[arxiv](https://arxiv.org/abs/2502.05995)] [[cool](https://papers.cool/arxiv/2502.05995)] [[pdf](https://arxiv.org/pdf/2502.05995)]
> **Authors**: Muhammad Turab
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: The usage of digital content (photos and videos) in a variety of applications has increased due to the popularity of multimedia devices. These uses include advertising campaigns, educational resources, and social networking platforms. There is an increasing need for high-quality graphic information as people become more visually focused. However, captured images frequently have poor visibility and a high amount of noise due to the limitations of image-capturing devices and lighting conditions. Improving the visual quality of images taken in low illumination is the aim of low-illumination image enhancement. This problem is addressed by traditional image enhancement techniques, which alter noise, brightness, and contrast. Deep learning-based methods, however, have dominated recently made advances in this area. These methods have effectively reduced noise while preserving important information, showing promising results in the improvement of low-illumination images. An extensive summary of image signal processing methods for enhancing low-illumination images is provided in this paper. Three categories are classified in the review for approaches: hybrid techniques, deep learning-based methods, and traditional approaches. Conventional techniques include denoising, automated white balancing, and noise reduction. Convolutional neural networks (CNNs) are used in deep learningbased techniques to recognize and extract characteristics from low-light images. To get better results, hybrid approaches combine deep learning-based methodologies with more conventional methods. The review also discusses the advantages and limitations of each approach and provides insights into future research directions in this field.

### SNAT-YOLO: Efficient Cross-Layer Aggregation Network for Edge-Oriented Gangue Detection 
[[arxiv](https://arxiv.org/abs/2502.05988)] [[cool](https://papers.cool/arxiv/2502.05988)] [[pdf](https://arxiv.org/pdf/2502.05988)]
> **Authors**: Shang Li
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: In Figure 1, due to our mistake, some parts of the picture are incorrect. We are making changes for resubmission
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: To address the issues of slow detection speed,low accuracy,difficulty in deployment on industrial edge devices,and large parameter and computational requirements in deep learning-based coal gangue target detection methods,we propose a lightweight coal gangue target detection algorithm based on an improved YOLOv11.First,we use the lightweight network ShuffleNetV2 as the backbone to enhance detection speed.Second,we introduce a lightweight downsampling operation,ADown,which reduces model complexity while improving average detection accuracy.Third,we improve the C2PSA module in YOLOv11 by incorporating the Triplet Attention mechanism,resulting in the proposed C2PSA-TriAtt module,which enhances the model's ability to focus on different dimensions of images.Fourth,we propose the Inner-FocalerIoU loss function to replace the existing CIoU loss function.Experimental results show that our model achieves a detection accuracy of 99.10% in coal gangue detection tasks,reduces the model size by 38%,the number of parameters by 41%,and the computational cost by 40%,while decreasing the average detection time per image by 1 ms.The improved model demonstrates enhanced detection speed and accuracy,making it suitable for deployment on industrial edge mobile devices,thus contributing positively to coal processing and efficient utilization of coal resources.

### VFX Creator: Animated Visual Effect Generation with Controllable Diffusion Transformer 
[[arxiv](https://arxiv.org/abs/2502.05979)] [[cool](https://papers.cool/arxiv/2502.05979)] [[pdf](https://arxiv.org/pdf/2502.05979)]
> **Authors**: Xinyu Liu,Ailing Zeng,Wei Xue,Harry Yang,Wenhan Luo,Qifeng Liu,Yike Guo
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: Project page: https://vfx-creator0.github.io/
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Crafting magic and illusions is one of the most thrilling aspects of filmmaking, with visual effects (VFX) serving as the powerhouse behind unforgettable cinematic experiences. While recent advances in generative artificial intelligence have driven progress in generic image and video synthesis, the domain of controllable VFX generation remains relatively underexplored. In this work, we propose a novel paradigm for animated VFX generation as image animation, where dynamic effects are generated from user-friendly textual descriptions and static reference images. Our work makes two primary contributions: (i) Open-VFX, the first high-quality VFX video dataset spanning 15 diverse effect categories, annotated with textual descriptions, instance segmentation masks for spatial conditioning, and start-end timestamps for temporal control. (ii) VFX Creator, a simple yet effective controllable VFX generation framework based on a Video Diffusion Transformer. The model incorporates a spatial and temporal controllable LoRA adapter, requiring minimal training videos. Specifically, a plug-and-play mask control module enables instance-level spatial manipulation, while tokenized start-end motion timestamps embedded in the diffusion process, alongside the text encoder, allow precise temporal control over effect timing and pace. Extensive experiments on the Open-VFX test set demonstrate the superiority of the proposed system in generating realistic and dynamic effects, achieving state-of-the-art performance and generalization ability in both spatial and temporal controllability. Furthermore, we introduce a specialized metric to evaluate the precision of temporal control. By bridging traditional VFX techniques with generative approaches, VFX Creator unlocks new possibilities for efficient and high-quality video effect generation, making advanced VFX accessible to a broader audience.

### Acceleration Multiple Heads Decoding for LLM via Dynamic Tree Attention 
[[arxiv](https://arxiv.org/abs/2502.05947)] [[cool](https://papers.cool/arxiv/2502.05947)] [[pdf](https://arxiv.org/pdf/2502.05947)]
> **Authors**: Zhendong Zhang
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算语言学
- **Abstract**: Multiple heads decoding accelerates the inference of Large Language Models (LLMs) by predicting next several tokens simultaneously. It generates and verifies multiple candidate sequences in parallel via tree attention with a fixed structure. In this paper, we replace the fixed tree attention with dynamic tree attention on multiple head decoding, specifically in the context of MEDUSA. We propose a simple and low complexity strategy to generate candidates and construct the dynamic tree structure. Preliminary experiments show that the proposed method improves the decoding efficiency of multiple head decoding for LLMs while maintaining the generation quality. This result demonstrates the potential for improvement of multiple head decoding in candidate generation.

### Multi-Branch Collaborative Learning Network for Video Quality Assessment in Industrial Video Search 
[[arxiv](https://arxiv.org/abs/2502.05924)] [[cool](https://papers.cool/arxiv/2502.05924)] [[pdf](https://arxiv.org/pdf/2502.05924)]
> **Authors**: Hengzhu Tang,Zefeng Zhang,Zhiping Li,Zhenyu Zhang,Xing Wu,Li Gao,Suqi Cheng,Dawei Yin
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: KDD 2025 ADS
- **标题**: None
- **领域**: 计算机视觉和模式识别,信息检索
- **Abstract**: Video Quality Assessment (VQA) is vital for large-scale video retrieval systems, aimed at identifying quality issues to prioritize high-quality videos. In industrial systems, low-quality video characteristics fall into four categories: visual-related issues like mosaics and black boxes, textual issues from video titles and OCR content, and semantic issues like frame incoherence and frame-text mismatch from AI-generated videos. Despite their prevalence in industrial settings, these low-quality videos have been largely overlooked in academic research, posing a challenge for accurate identification. To address this, we introduce the Multi-Branch Collaborative Network (MBCN) tailored for industrial video retrieval systems. MBCN features four branches, each designed to tackle one of the aforementioned quality issues. After each branch independently scores videos, we aggregate these scores using a weighted approach and a squeeze-and-excitation mechanism to dynamically address quality issues across different scenarios. We implement point-wise and pair-wise optimization objectives to ensure score stability and reasonableness. Extensive offline and online experiments on a world-level video search engine demonstrate MBCN's effectiveness in identifying video quality issues, significantly enhancing the retrieval system's ranking performance. Detailed experimental analyses confirm the positive contribution of all four evaluation branches. Furthermore, MBCN significantly improves recognition accuracy for low-quality AI-generated videos compared to the baseline.

### QP-SNN: Quantized and Pruned Spiking Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.05905)] [[cool](https://papers.cool/arxiv/2502.05905)] [[pdf](https://arxiv.org/pdf/2502.05905)]
> **Authors**: Wenjie Wei,Malu Zhang,Zijian Zhou,Ammar Belatreche,Yimeng Shan,Yu Liang,Honglin Cao,Jieyuan Zhang,Yang Yang
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: 26 pages, 17 figures, Published as a conference paper at ICLR 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Brain-inspired Spiking Neural Networks (SNNs) leverage sparse spikes to encode information and operate in an asynchronous event-driven manner, offering a highly energy-efficient paradigm for machine intelligence. However, the current SNN community focuses primarily on performance improvement by developing large-scale models, which limits the applicability of SNNs in resource-limited edge devices. In this paper, we propose a hardware-friendly and lightweight SNN, aimed at effectively deploying high-performance SNN in resource-limited scenarios. Specifically, we first develop a baseline model that integrates uniform quantization and structured pruning, called QP-SNN baseline. While this baseline significantly reduces storage demands and computational costs, it suffers from performance decline. To address this, we conduct an in-depth analysis of the challenges in quantization and pruning that lead to performance degradation and propose solutions to enhance the baseline's performance. For weight quantization, we propose a weight rescaling strategy that utilizes bit width more effectively to enhance the model's representation capability. For structured pruning, we propose a novel pruning criterion using the singular value of spatiotemporal spike activities to enable more accurate removal of redundant kernels. Extensive experiments demonstrate that integrating two proposed methods into the baseline allows QP-SNN to achieve state-of-the-art performance and efficiency, underscoring its potential for enhancing SNN deployment in edge intelligence computing.

### Fast Omni-Directional Image Super-Resolution: Adapting the Implicit Image Function with Pixel and Semantic-Wise Spherical Geometric Priors 
[[arxiv](https://arxiv.org/abs/2502.05902)] [[cool](https://papers.cool/arxiv/2502.05902)] [[pdf](https://arxiv.org/pdf/2502.05902)]
> **Authors**: Xuelin Shen,Yitong Wang,Silin Zheng,Kang Xiao,Wenhan Yang,Xu Wang
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: 9 pages, 4 figures, AAAI 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: In the context of Omni-Directional Image (ODI) Super-Resolution (SR), the unique challenge arises from the non-uniform oversampling characteristics caused by EquiRectangular Projection (ERP). Considerable efforts in designing complex spherical convolutions or polyhedron reprojection offer significant performance improvements but at the expense of cumbersome processing procedures and slower inference speeds. Under these circumstances, this paper proposes a new ODI-SR model characterized by its capacity to perform Fast and Arbitrary-scale ODI-SR processes, denoted as FAOR. The key innovation lies in adapting the implicit image function from the planar image domain to the ERP image domain by incorporating spherical geometric priors at both the latent representation and image reconstruction stages, in a low-overhead manner. Specifically, at the latent representation stage, we adopt a pair of pixel-wise and semantic-wise sphere-to-planar distortion maps to perform affine transformations on the latent representation, thereby incorporating it with spherical properties. Moreover, during the image reconstruction stage, we introduce a geodesic-based resampling strategy, aligning the implicit image function with spherical geometrics without introducing additional parameters. As a result, the proposed FAOR outperforms the state-of-the-art ODI-SR models with a much faster inference speed. Extensive experimental results and ablation studies have demonstrated the effectiveness of our design.

### MMGDreamer: Mixed-Modality Graph for Geometry-Controllable 3D Indoor Scene Generation 
[[arxiv](https://arxiv.org/abs/2502.05874)] [[cool](https://papers.cool/arxiv/2502.05874)] [[pdf](https://arxiv.org/pdf/2502.05874)]
> **Authors**: Zhifei Yang,Keyang Lu,Chao Zhang,Jiaxing Qi,Hanqi Jiang,Ruifei Ma,Shenglin Yin,Yifan Xu,Mingzhe Xing,Zhen Xiao,Jieyi Long,Xiangde Liu,Guangyao Zhai
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: Accepted by AAAI 2025 Main Track
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: Controllable 3D scene generation has extensive applications in virtual reality and interior design, where the generated scenes should exhibit high levels of realism and controllability in terms of geometry. Scene graphs provide a suitable data representation that facilitates these applications. However, current graph-based methods for scene generation are constrained to text-based inputs and exhibit insufficient adaptability to flexible user inputs, hindering the ability to precisely control object geometry. To address this issue, we propose MMGDreamer, a dual-branch diffusion model for scene generation that incorporates a novel Mixed-Modality Graph, visual enhancement module, and relation predictor. The mixed-modality graph allows object nodes to integrate textual and visual modalities, with optional relationships between nodes. It enhances adaptability to flexible user inputs and enables meticulous control over the geometry of objects in the generated scenes. The visual enhancement module enriches the visual fidelity of text-only nodes by constructing visual representations using text embeddings. Furthermore, our relation predictor leverages node representations to infer absent relationships between nodes, resulting in more coherent scene layouts. Extensive experimental results demonstrate that MMGDreamer exhibits superior control of object geometry, achieving state-of-the-art scene generation performance. Project page: https://yangzhifeio.github.io/project/MMGDreamer.

### HyLiFormer: Hyperbolic Linear Attention for Skeleton-based Human Action Recognition 
[[arxiv](https://arxiv.org/abs/2502.05869)] [[cool](https://papers.cool/arxiv/2502.05869)] [[pdf](https://arxiv.org/pdf/2502.05869)]
> **Authors**: Yue Li,Haoxuan Qu,Mengyuan Liu,Jun Liu,Yujun Cai
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Transformers have demonstrated remarkable performance in skeleton-based human action recognition, yet their quadratic computational complexity remains a bottleneck for real-world applications. To mitigate this, linear attention mechanisms have been explored but struggle to capture the hierarchical structure of skeleton data. Meanwhile, the Poincaré model, as a typical hyperbolic geometry, offers a powerful framework for modeling hierarchical structures but lacks well-defined operations for existing mainstream linear attention. In this paper, we propose HyLiFormer, a novel hyperbolic linear attention Transformer tailored for skeleton-based action recognition. Our approach incorporates a Hyperbolic Transformation with Curvatures (HTC) module to map skeleton data into hyperbolic space and a Hyperbolic Linear Attention (HLA) module for efficient long-range dependency modeling. Theoretical analysis and extensive experiments on NTU RGB+D and NTU RGB+D 120 datasets demonstrate that HyLiFormer significantly reduces computational complexity while preserving model accuracy, making it a promising solution for efficiency-critical applications.

### Acquisition through My Eyes and Steps: A Joint Predictive Agent Model in Egocentric Worlds 
[[arxiv](https://arxiv.org/abs/2502.05857)] [[cool](https://papers.cool/arxiv/2502.05857)] [[pdf](https://arxiv.org/pdf/2502.05857)]
> **Authors**: Lu Chen,Yizhou Wang,Shixiang Tang,Qianhong Ma,Tong He,Wanli Ouyang,Xiaowei Zhou,Hujun Bao,Sida Peng
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: This paper addresses the task of learning an agent model behaving like humans, which can jointly perceive, predict, and act in egocentric worlds. Previous methods usually train separate models for these three abilities, leading to information silos among them, which prevents these abilities from learning from each other and collaborating effectively. In this paper, we propose a joint predictive agent model, named EgoAgent, that simultaneously learns to represent the world, predict future states, and take reasonable actions with a single transformer. EgoAgent unifies the representational spaces of the three abilities by mapping them all into a sequence of continuous tokens. Learnable query tokens are appended to obtain current states, future states, and next actions. With joint supervision, our agent model establishes the internal relationship among these three abilities and effectively mimics the human inference and learning processes. Comprehensive evaluations of EgoAgent covering image classification, egocentric future state prediction, and 3D human motion prediction tasks demonstrate the superiority of our method. The code and trained model will be released for reproducibility.

### Training-free Anomaly Event Detection via LLM-guided Symbolic Pattern Discovery 
[[arxiv](https://arxiv.org/abs/2502.05843)] [[cool](https://papers.cool/arxiv/2502.05843)] [[pdf](https://arxiv.org/pdf/2502.05843)]
> **Authors**: Yuhui Zeng,Haoxiang Wu,Wenjie Nie,Guangyao Chen,Xiawu Zheng,Yunhang Shen,Guilin Li,Yixiong Zou,Yonghong Tian,Rongrong Ji
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: 11 pages, 4 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Anomaly event detection plays a crucial role in various real-world applications. However, current approaches predominantly rely on supervised learning, which faces significant challenges: the requirement for extensive labeled training data and lack of interpretability in decision-making processes. To address these limitations, we present a training-free framework that integrates open-set object detection with symbolic regression, powered by Large Language Models (LLMs) for efficient symbolic pattern discovery. The LLMs guide the symbolic reasoning process, establishing logical relationships between detected entities. Through extensive experiments across multiple domains, our framework demonstrates several key advantages: (1) achieving superior detection accuracy through direct reasoning without any training process; (2) providing highly interpretable logical expressions that are readily comprehensible to humans; and (3) requiring minimal annotation effort - approximately 1% of the data needed by traditional training-based methods.To facilitate comprehensive evaluation and future research, we introduce two datasets: a large-scale private dataset containing over 110,000 annotated images covering various anomaly scenarios including construction site safety violations, illegal fishing activities, and industrial hazards, along with a public benchmark dataset of 5,000 samples with detailed anomaly event annotations. Code is available at here.

### MicroViT: A Vision Transformer with Low Complexity Self Attention for Edge Device 
[[arxiv](https://arxiv.org/abs/2502.05800)] [[cool](https://papers.cool/arxiv/2502.05800)] [[pdf](https://arxiv.org/pdf/2502.05800)]
> **Authors**: Novendra Setyawan,Chi-Chia Sun,Mao-Hsiu Hsu,Wen-Kai Kuo,Jun-Wei Hsieh
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: The Vision Transformer (ViT) has demonstrated state-of-the-art performance in various computer vision tasks, but its high computational demands make it impractical for edge devices with limited resources. This paper presents MicroViT, a lightweight Vision Transformer architecture optimized for edge devices by significantly reducing computational complexity while maintaining high accuracy. The core of MicroViT is the Efficient Single Head Attention (ESHA) mechanism, which utilizes group convolution to reduce feature redundancy and processes only a fraction of the channels, thus lowering the burden of the self-attention mechanism. MicroViT is designed using a multi-stage MetaFormer architecture, stacking multiple MicroViT encoders to enhance efficiency and performance. Comprehensive experiments on the ImageNet-1K and COCO datasets demonstrate that MicroViT achieves competitive accuracy while significantly improving 3.6 faster inference speed and reducing energy consumption with 40% higher efficiency than the MobileViT series, making it suitable for deployment in resource-constrained environments such as mobile and edge devices.

### Effective Black-Box Multi-Faceted Attacks Breach Vision Large Language Model Guardrails 
[[arxiv](https://arxiv.org/abs/2502.05772)] [[cool](https://papers.cool/arxiv/2502.05772)] [[pdf](https://arxiv.org/pdf/2502.05772)]
> **Authors**: Yijun Yang,Lichao Wang,Xiao Yang,Lanqing Hong,Jun Zhu
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Vision Large Language Models (VLLMs) integrate visual data processing, expanding their real-world applications, but also increasing the risk of generating unsafe responses. In response, leading companies have implemented Multi-Layered safety defenses, including alignment training, safety system prompts, and content moderation. However, their effectiveness against sophisticated adversarial attacks remains largely unexplored. In this paper, we propose MultiFaceted Attack, a novel attack framework designed to systematically bypass Multi-Layered Defenses in VLLMs. It comprises three complementary attack facets: Visual Attack that exploits the multimodal nature of VLLMs to inject toxic system prompts through images; Alignment Breaking Attack that manipulates the model's alignment mechanism to prioritize the generation of contrasting responses; and Adversarial Signature that deceives content moderators by strategically placing misleading information at the end of the response. Extensive evaluations on eight commercial VLLMs in a black-box setting demonstrate that MultiFaceted Attack achieves a 61.56% attack success rate, surpassing state-of-the-art methods by at least 42.18%.

### Digital Twin Buildings: 3D Modeling, GIS Integration, and Visual Descriptions Using Gaussian Splatting, ChatGPT/Deepseek, and Google Maps Platform 
[[arxiv](https://arxiv.org/abs/2502.05769)] [[cool](https://papers.cool/arxiv/2502.05769)] [[pdf](https://arxiv.org/pdf/2502.05769)]
> **Authors**: Kyle Gao,Dening Lu,Liangzhi Li,Nan Chen,Hongjie He,Linlin Xu,Jonathan Li
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: -Fixed minor typo
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Urban digital twins are virtual replicas of cities that use multi-source data and data analytics to optimize urban planning, infrastructure management, and decision-making. Towards this, we propose a framework focused on the single-building scale. By connecting to cloud mapping platforms such as Google Map Platforms APIs, by leveraging state-of-the-art multi-agent Large Language Models data analysis using ChatGPT(4o) and Deepseek-V3/R1, and by using our Gaussian Splatting-based mesh extraction pipeline, our Digital Twin Buildings framework can retrieve a building's 3D model, visual descriptions, and achieve cloud-based mapping integration with large language model-based data analytics using a building's address, postal code, or geographic coordinates.

### Exploring Visual Embedding Spaces Induced by Vision Transformers for Online Auto Parts Marketplaces 
[[arxiv](https://arxiv.org/abs/2502.05756)] [[cool](https://papers.cool/arxiv/2502.05756)] [[pdf](https://arxiv.org/pdf/2502.05756)]
> **Authors**: Cameron Armijo,Pablo Rivas
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: AAAI 2025 Workshop onAIfor Social Impact: Bridging Innovations in Finance, Social Media, and Crime Prevention
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: This study examines the capabilities of the Vision Transformer (ViT) model in generating visual embeddings for images of auto parts sourced from online marketplaces, such as Craigslist and OfferUp. By focusing exclusively on single-modality data, the analysis evaluates ViT's potential for detecting patterns indicative of illicit activities. The workflow involves extracting high-dimensional embeddings from images, applying dimensionality reduction techniques like Uniform Manifold Approximation and Projection (UMAP) to visualize the embedding space, and using K-Means clustering to categorize similar items. Representative posts nearest to each cluster centroid provide insights into the composition and characteristics of the clusters. While the results highlight the strengths of ViT in isolating visual patterns, challenges such as overlapping clusters and outliers underscore the limitations of single-modal approaches in this domain. This work contributes to understanding the role of Vision Transformers in analyzing online marketplaces and offers a foundation for future advancements in detecting fraudulent or illegal activities.

### UniDB: A Unified Diffusion Bridge Framework via Stochastic Optimal Control 
[[arxiv](https://arxiv.org/abs/2502.05749)] [[cool](https://papers.cool/arxiv/2502.05749)] [[pdf](https://arxiv.org/pdf/2502.05749)]
> **Authors**: Kaizhen Zhu,Mokai Pan,Yuexin Ma,Yanwei Fu,Jingyi Yu,Jingya Wang,Ye Shi
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,系统与控制
- **Abstract**: Recent advances in diffusion bridge models leverage Doob's $h$-transform to establish fixed endpoints between distributions, demonstrating promising results in image translation and restoration tasks. However, these approaches frequently produce blurred or excessively smoothed image details and lack a comprehensive theoretical foundation to explain these shortcomings. To address these limitations, we propose UniDB, a unified framework for diffusion bridges based on Stochastic Optimal Control (SOC). UniDB formulates the problem through an SOC-based optimization and derives a closed-form solution for the optimal controller, thereby unifying and generalizing existing diffusion bridge models. We demonstrate that existing diffusion bridges employing Doob's $h$-transform constitute a special case of our framework, emerging when the terminal penalty coefficient in the SOC cost function tends to infinity. By incorporating a tunable terminal penalty coefficient, UniDB achieves an optimal balance between control costs and terminal penalties, substantially improving detail preservation and output quality. Notably, UniDB seamlessly integrates with existing diffusion bridge models, requiring only minimal code modifications. Extensive experiments across diverse image restoration tasks validate the superiority and adaptability of the proposed framework. Our code is available at https://github.com/UniDB-SOC/UniDB/.

### Linear Attention Modeling for Learned Image Compression 
[[arxiv](https://arxiv.org/abs/2502.05741)] [[cool](https://papers.cool/arxiv/2502.05741)] [[pdf](https://arxiv.org/pdf/2502.05741)]
> **Authors**: Donghui Feng,Zhengxue Cheng,Shen Wang,Ronghua Wu,Hongwei Hu,Guo Lu,Li Song
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Recent years, learned image compression has made tremendous progress to achieve impressive coding efficiency. Its coding gain mainly comes from non-linear neural network-based transform and learnable entropy modeling. However, most of recent focuses have been solely on a strong backbone, and few studies consider the low-complexity design. In this paper, we propose LALIC, a linear attention modeling for learned image compression. Specially, we propose to use Bi-RWKV blocks, by utilizing the Spatial Mix and Channel Mix modules to achieve more compact features extraction, and apply the Conv based Omni-Shift module to adapt to two-dimensional latent representation. Furthermore, we propose a RWKV-based Spatial-Channel ConTeXt model (RWKV-SCCTX), that leverages the Bi-RWKV to modeling the correlation between neighboring features effectively, to further improve the RD performance. To our knowledge, our work is the first work to utilize efficient Bi-RWKV models with linear attention for learned image compression. Experimental results demonstrate that our method achieves competitive RD performances by outperforming VTM-9.1 by -14.84%, -15.20%, -17.32% in BD-rate on Kodak, Tecnick and CLIC Professional validation datasets.

### Performance Analysis of Traditional VQA Models Under Limited Computational Resources 
[[arxiv](https://arxiv.org/abs/2502.05738)] [[cool](https://papers.cool/arxiv/2502.05738)] [[pdf](https://arxiv.org/pdf/2502.05738)]
> **Authors**: Jihao Gu
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: 6 pages, 1 figure, 5 tabels, the paper has been accepted by the PRML'25 conference
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: In real-world applications where computational resources are limited, effectively integrating visual and textual information for Visual Question Answering (VQA) presents significant challenges. This paper investigates the performance of traditional models under computational constraints, focusing on enhancing VQA performance, particularly for numerical and counting questions. We evaluate models based on Bidirectional GRU (BidGRU), GRU, Bidirectional LSTM (BidLSTM), and Convolutional Neural Networks (CNN), analyzing the impact of different vocabulary sizes, fine-tuning strategies, and embedding dimensions. Experimental results show that the BidGRU model with an embedding dimension of 300 and a vocabulary size of 3000 achieves the best overall performance without the computational overhead of larger models. Ablation studies emphasize the importance of attention mechanisms and counting information in handling complex reasoning tasks under resource limitations. Our research provides valuable insights for developing more efficient VQA models suitable for deployment in environments with limited computational capacity.

### SSDD-GAN: Single-Step Denoising Diffusion GAN for Cochlear Implant Surgical Scene Completion 
[[arxiv](https://arxiv.org/abs/2502.05710)] [[cool](https://papers.cool/arxiv/2502.05710)] [[pdf](https://arxiv.org/pdf/2502.05710)]
> **Authors**: Yike Zhang,Eduardo Davalos,Jack Noble
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Recent deep learning-based image completion methods, including both inpainting and outpainting, have demonstrated promising results in restoring corrupted images by effectively filling various missing regions. Among these, Generative Adversarial Networks (GANs) and Denoising Diffusion Probabilistic Models (DDPMs) have been employed as key generative image completion approaches, excelling in the field of generating high-quality restorations with reduced artifacts and improved fine details. In previous work, we developed a method aimed at synthesizing views from novel microscope positions for mastoidectomy surgeries; however, that approach did not have the ability to restore the surrounding surgical scene environment. In this paper, we propose an efficient method to complete the surgical scene of the synthetic postmastoidectomy dataset. Our approach leverages self-supervised learning on real surgical datasets to train a Single-Step Denoising Diffusion-GAN (SSDD-GAN), combining the advantages of diffusion models with the adversarial optimization of GANs for improved Structural Similarity results of 6%. The trained model is then directly applied to the synthetic postmastoidectomy dataset using a zero-shot approach, enabling the generation of realistic and complete surgical scenes without the need for explicit ground-truth labels from the synthetic postmastoidectomy dataset. This method addresses key limitations in previous work, offering a novel pathway for full surgical microscopy scene completion and enhancing the usability of the synthetic postmastoidectomy dataset in surgical preoperative planning and intraoperative navigation.

### The Evolution of Dataset Distillation: Toward Scalable and Generalizable Solutions 
[[arxiv](https://arxiv.org/abs/2502.05673)] [[cool](https://papers.cool/arxiv/2502.05673)] [[pdf](https://arxiv.org/pdf/2502.05673)]
> **Authors**: Ping Liu,Jiawei Du
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Dataset distillation, which condenses large-scale datasets into compact synthetic representations, has emerged as a critical solution for training modern deep learning models efficiently. While prior surveys focus on developments before 2023, this work comprehensively reviews recent advances, emphasizing scalability to large-scale datasets such as ImageNet-1K and ImageNet-21K. We categorize progress into a few key methodologies: trajectory matching, gradient matching, distribution matching, scalable generative approaches, and decoupling optimization mechanisms. As a comprehensive examination of recent dataset distillation advances, this survey highlights breakthrough innovations: the SRe2L framework for efficient and effective condensation, soft label strategies that significantly enhance model accuracy, and lossless distillation techniques that maximize compression while maintaining performance. Beyond these methodological advancements, we address critical challenges, including robustness against adversarial and backdoor attacks, effective handling of non-IID data distributions. Additionally, we explore emerging applications in video and audio processing, multi-modal learning, medical imaging, and scientific computing, highlighting its domain versatility. By offering extensive performance comparisons and actionable research directions, this survey equips researchers and practitioners with practical insights to advance efficient and generalizable dataset distillation, paving the way for future innovations.

### Evaluating Vision-Language Models for Emotion Recognition 
[[arxiv](https://arxiv.org/abs/2502.05660)] [[cool](https://papers.cool/arxiv/2502.05660)] [[pdf](https://arxiv.org/pdf/2502.05660)]
> **Authors**: Sree Bhattacharyya,James Z. Wang
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: Accepted to NAACL 2025 Findings
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算语言学
- **Abstract**: Large Vision-Language Models (VLMs) have achieved unprecedented success in several objective multimodal reasoning tasks. However, to further enhance their capabilities of empathetic and effective communication with humans, improving how VLMs process and understand emotions is crucial. Despite significant research attention on improving affective understanding, there is a lack of detailed evaluations of VLMs for emotion-related tasks, which can potentially help inform downstream fine-tuning efforts. In this work, we present the first comprehensive evaluation of VLMs for recognizing evoked emotions from images. We create a benchmark for the task of evoked emotion recognition and study the performance of VLMs for this task, from perspectives of correctness and robustness. Through several experiments, we demonstrate important factors that emotion recognition performance depends on, and also characterize the various errors made by VLMs in the process. Finally, we pinpoint potential causes for errors through a human evaluation study. We use our experimental results to inform recommendations for the future of emotion research in the context of VLMs.

### XiHeFusion: Harnessing Large Language Models for Science Communication in Nuclear Fusion 
[[arxiv](https://arxiv.org/abs/2502.05615)] [[cool](https://papers.cool/arxiv/2502.05615)] [[pdf](https://arxiv.org/pdf/2502.05615)]
> **Authors**: Xiao Wang,Qingquan Yang,Fuling Wang,Qiang Chen,Wentao Wu,Yu Jin,Jingtao Jiang,Liye Jin,Bo Jiang,Dengdi Sun,Wanli Lv,Meiwen Chen,Zehua Chen,Guosheng Xu,Jin Tang
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Nuclear fusion is one of the most promising ways for humans to obtain infinite energy. Currently, with the rapid development of artificial intelligence, the mission of nuclear fusion has also entered a critical period of its development. How to let more people to understand nuclear fusion and join in its research is one of the effective means to accelerate the implementation of fusion. This paper proposes the first large model in the field of nuclear fusion, XiHeFusion, which is obtained through supervised fine-tuning based on the open-source large model Qwen2.5-14B. We have collected multi-source knowledge about nuclear fusion tasks to support the training of this model, including the common crawl, eBooks, arXiv, dissertation, etc. After the model has mastered the knowledge of the nuclear fusion field, we further used the chain of thought to enhance its logical reasoning ability, making XiHeFusion able to provide more accurate and logical answers. In addition, we propose a test questionnaire containing 180+ questions to assess the conversational ability of this science popularization large model. Extensive experimental results show that our nuclear fusion dialogue model, XiHeFusion, can perform well in answering science popularization knowledge. The pre-trained XiHeFusion model is released on https://github.com/Event-AHU/XiHeFusion.

### Semantic Data Augmentation Enhanced Invariant Risk Minimization for Medical Image Domain Generalization 
[[arxiv](https://arxiv.org/abs/2502.05593)] [[cool](https://papers.cool/arxiv/2502.05593)] [[pdf](https://arxiv.org/pdf/2502.05593)]
> **Authors**: Yaoyao Zhu,Xiuding Cai,Yingkai Wang,Yu Yao,Xu Luo,Zhongliang Fu
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Deep learning has achieved remarkable success in medical image classification. However, its clinical application is often hindered by data heterogeneity caused by variations in scanner vendors, imaging protocols, and operators. Approaches such as invariant risk minimization (IRM) aim to address this challenge of out-of-distribution generalization. For instance, VIRM improves upon IRM by tackling the issue of insufficient feature support overlap, demonstrating promising potential. Nonetheless, these methods face limitations in medical imaging due to the scarcity of annotated data and the inefficiency of augmentation strategies. To address these issues, we propose a novel domain-oriented direction selector to replace the random augmentation strategy used in VIRM. Our method leverages inter-domain covariance as a guider for augmentation direction, guiding data augmentation towards the target domain. This approach effectively reduces domain discrepancies and enhances generalization performance. Experiments on a multi-center diabetic retinopathy dataset demonstrate that our method outperforms state-of-the-art approaches, particularly under limited data conditions and significant domain heterogeneity.

### Event Stream-based Visual Object Tracking: HDETrack V2 and A High-Definition Benchmark 
[[arxiv](https://arxiv.org/abs/2502.05574)] [[cool](https://papers.cool/arxiv/2502.05574)] [[pdf](https://arxiv.org/pdf/2502.05574)]
> **Authors**: Shiao Wang,Xiao Wang,Chao Wang,Liye Jin,Lin Zhu,Bo Jiang,Yonghong Tian,Jin Tang
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: Journal Extension of EventVOT, CVPR24
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: We then introduce a novel hierarchical knowledge distillation strategy that incorporates the similarity matrix, feature representation, and response map-based distillation to guide the learning of the student Transformer network. We also enhance the model's ability to capture temporal dependencies by applying the temporal Fourier transform to establish temporal relationships between video frames. We adapt the network model to specific target objects during testing via a newly proposed test-time tuning strategy to achieve high performance and flexibility in target tracking. Recognizing the limitations of existing event-based tracking datasets, which are predominantly low-resolution, we propose EventVOT, the first large-scale high-resolution event-based tracking dataset. It comprises 1141 videos spanning diverse categories such as pedestrians, vehicles, UAVs, ping pong, etc. Extensive experiments on both low-resolution (FE240hz, VisEvent, FELT), and our newly proposed high-resolution EventVOT dataset fully validated the effectiveness of our proposed method. Both the benchmark dataset and source code have been released on https://github.com/Event-AHU/EventVOT_Benchmark

### MMHMER:Multi-viewer and Multi-task for Handwritten Mathematical Expression Recognition 
[[arxiv](https://arxiv.org/abs/2502.05557)] [[cool](https://papers.cool/arxiv/2502.05557)] [[pdf](https://arxiv.org/pdf/2502.05557)]
> **Authors**: Kehua Chen,Haoyang Shen,Lifan Zhong,Mingyi Chen
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: 7 pages;2 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Handwritten Mathematical Expression Recognition (HMER) methods have made remarkable progress, with most existing HMER approaches based on either a hybrid CNN/RNN-based with GRU architecture or Transformer architectures. Each of these has its strengths and weaknesses. Leveraging different model structures as viewers and effectively integrating their diverse capabilities presents an intriguing avenue for exploration. This involves addressing two key challenges: 1) How to fuse these two methods effectively, and 2) How to achieve higher performance under an appropriate level of complexity. This paper proposes an efficient CNN-Transformer multi-viewer, multi-task approach to enhance the model's recognition performance. Our MMHMER model achieves 63.96%, 62.51%, and 65.46% ExpRate on CROHME14, CROHME16, and CROHME19, outperforming Posformer with an absolute gain of 1.28%, 1.48%, and 0.58%. The main contribution of our approach is that we propose a new multi-view, multi-task framework that can effectively integrate the strengths of CNN and Transformer. By leveraging the feature extraction capabilities of CNN and the sequence modeling capabilities of Transformer, our model can better handle the complexity of handwritten mathematical expressions.

### 4DR P2T: 4D Radar Tensor Synthesis with Point Clouds 
[[arxiv](https://arxiv.org/abs/2502.05550)] [[cool](https://papers.cool/arxiv/2502.05550)] [[pdf](https://arxiv.org/pdf/2502.05550)]
> **Authors**: Woo-Jin Jung,Dong-Hee Paek,Seung-Hyun Kong
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: 6 pages, 4 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: In four-dimensional (4D) Radar-based point cloud generation, clutter removal is commonly performed using the constant false alarm rate (CFAR) algorithm. However, CFAR may not fully capture the spatial characteristics of objects. To address limitation, this paper proposes the 4D Radar Point-to-Tensor (4DR P2T) model, which generates tensor data suitable for deep learning applications while minimizing measurement loss. Our method employs a conditional generative adversarial network (cGAN), modified to effectively process 4D Radar point cloud data and generate tensor data. Experimental results on the K-Radar dataset validate the effectiveness of the 4DR P2T model, achieving an average PSNR of 30.39dB and SSIM of 0.96. Additionally, our analysis of different point cloud generation methods highlights that the 5% percentile method provides the best overall performance, while the 1% percentile method optimally balances data volume reduction and performance, making it well-suited for deep learning applications.

### SSH: Sparse Spectrum Adaptation via Discrete Hartley Transformation 
[[arxiv](https://arxiv.org/abs/2502.05539)] [[cool](https://papers.cool/arxiv/2502.05539)] [[pdf](https://arxiv.org/pdf/2502.05539)]
> **Authors**: Yixian Shen,Qi Bi,Jia-Hong Huang,Hongyi Zhu,Andy D. Pimentel,Anuj Pathania
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Low-rank adaptation (LoRA) has been demonstrated effective in reducing the trainable parameter number when fine-tuning a large foundation model (LLM). However, it still encounters computational and memory challenges when scaling to larger models or addressing more complex task adaptation. In this work, we introduce Sparse Spectrum Adaptation via Discrete Hartley Transformation (SSH), a novel approach that significantly reduces the number of trainable parameters while enhancing model performance. It selects the most informative spectral components across all layers, under the guidance of the initial weights after a discrete Hartley transformation (DHT). The lightweight inverse DHT then projects the spectrum back into the spatial domain for updates. Extensive experiments across both single-modality tasks such as language understanding and generation and multi-modality tasks such as video-text understanding demonstrate that SSH outperforms existing parameter-efficient fine-tuning (PEFT) methods while achieving substantial reductions in computational cost and memory requirements.

### Fg-T2M++: LLMs-Augmented Fine-Grained Text Driven Human Motion Generation 
[[arxiv](https://arxiv.org/abs/2502.05534)] [[cool](https://papers.cool/arxiv/2502.05534)] [[pdf](https://arxiv.org/pdf/2502.05534)]
> **Authors**: Yin Wang,Mu Li,Jiapeng Liu,Zhiying Leng,Frederick W. B. Li,Ziyao Zhang,Xiaohui Liang
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: We address the challenging problem of fine-grained text-driven human motion generation. Existing works generate imprecise motions that fail to accurately capture relationships specified in text due to: (1) lack of effective text parsing for detailed semantic cues regarding body parts, (2) not fully modeling linguistic structures between words to comprehend text comprehensively. To tackle these limitations, we propose a novel fine-grained framework Fg-T2M++ that consists of: (1) an LLMs semantic parsing module to extract body part descriptions and semantics from text, (2) a hyperbolic text representation module to encode relational information between text units by embedding the syntactic dependency graph into hyperbolic space, and (3) a multi-modal fusion module to hierarchically fuse text and motion features. Extensive experiments on HumanML3D and KIT-ML datasets demonstrate that Fg-T2M++ outperforms SOTA methods, validating its ability to accurately generate motions adhering to comprehensive text semantics.

### Evaluation of Vision Transformers for Multimodal Image Classification: A Case Study on Brain, Lung, and Kidney Tumors 
[[arxiv](https://arxiv.org/abs/2502.05517)] [[cool](https://papers.cool/arxiv/2502.05517)] [[pdf](https://arxiv.org/pdf/2502.05517)]
> **Authors**: Óscar A. Martín,Javier Sánchez
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: 13 pages, 3 figures, 8 tables
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Neural networks have become the standard technique for medical diagnostics, especially in cancer detection and classification. This work evaluates the performance of Vision Transformers architectures, including Swin Transformer and MaxViT, in several datasets of magnetic resonance imaging (MRI) and computed tomography (CT) scans. We used three training sets of images with brain, lung, and kidney tumors. Each dataset includes different classification labels, from brain gliomas and meningiomas to benign and malignant lung conditions and kidney anomalies such as cysts and cancers. This work aims to analyze the behavior of the neural networks in each dataset and the benefits of combining different image modalities and tumor classes. We designed several experiments by fine-tuning the models on combined and individual image modalities. The results revealed that the Swin Transformer provided high accuracy, achieving up to 99.9\% for kidney tumor classification and 99.3\% accuracy in a combined dataset. MaxViT also provided excellent results in individual datasets but performed poorly when data is combined. This research highlights the adaptability of Transformer-based models to various image modalities and features. However, challenges persist, including limited annotated data and interpretability issues. Future works will expand this study by incorporating other image modalities and enhancing diagnostic capabilities. Integrating these models across diverse datasets could mark a pivotal advance in precision medicine, paving the way for more efficient and comprehensive healthcare solutions.

### Robustifying Fourier Features Embeddings for Implicit Neural Representations 
[[arxiv](https://arxiv.org/abs/2502.05482)] [[cool](https://papers.cool/arxiv/2502.05482)] [[pdf](https://arxiv.org/pdf/2502.05482)]
> **Authors**: Mingze Ma,Qingtian Zhu,Yifan Zhan,Zhengwei Yin,Hongjun Wang,Yinqiang Zheng
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Implicit Neural Representations (INRs) employ neural networks to represent continuous functions by mapping coordinates to the corresponding values of the target function, with applications e.g., inverse graphics. However, INRs face a challenge known as spectral bias when dealing with scenes containing varying frequencies. To overcome spectral bias, the most common approach is the Fourier features-based methods such as positional encoding. However, Fourier features-based methods will introduce noise to output, which degrades their performances when applied to downstream tasks. In response, this paper initially hypothesizes that combining multi-layer perceptrons (MLPs) with Fourier feature embeddings mutually enhances their strengths, yet simultaneously introduces limitations inherent in Fourier feature embeddings. By presenting a simple theorem, we validate our hypothesis, which serves as a foundation for the design of our solution. Leveraging these insights, we propose the use of multi-layer perceptrons (MLPs) without additive

### Convolutional Neural Network Segmentation for Satellite Imagery Data to Identify Landforms Using U-Net Architecture 
[[arxiv](https://arxiv.org/abs/2502.05476)] [[cool](https://papers.cool/arxiv/2502.05476)] [[pdf](https://arxiv.org/pdf/2502.05476)]
> **Authors**: Mitul Goswami,Sainath Dey,Aniruddha Mukherjee,Suneeta Mohanty,Prasant Kumar Pattnaik
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: 6th International Conference on Computational Intelligence and Pattern Recognition
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: This study demonstrates a novel use of the U-Net architecture in the field of semantic segmentation to detect landforms using preprocessed satellite imagery. The study applies the U-Net model for effective feature extraction by using Convolutional Neural Network (CNN) segmentation techniques. Dropout is strategically used for regularization to improve the model's perseverance, and the Adam optimizer is used for effective training. The study thoroughly assesses the performance of the U-Net architecture utilizing a large sample of preprocessed satellite topographical images. The model excels in semantic segmentation tasks, displaying high-resolution outputs, quick feature extraction, and flexibility to a wide range of applications. The findings highlight the U-Net architecture's substantial contribution to the advancement of machine learning and image processing technologies. The U-Net approach, which emphasizes pixel-wise categorization and comprehensive segmentation map production, is helpful in practical applications such as autonomous driving, disaster management, and land use planning. This study not only investigates the complexities of U-Net architecture for semantic segmentation, but also highlights its real-world applications in image classification, analysis, and landform identification. The study demonstrates the U-Net model's key significance in influencing the environment of modern technology.

### LMS-Net: A Learned Mumford-Shah Network For Few-Shot Medical Image Segmentation 
[[arxiv](https://arxiv.org/abs/2502.05473)] [[cool](https://papers.cool/arxiv/2502.05473)] [[pdf](https://arxiv.org/pdf/2502.05473)]
> **Authors**: Shengdong Zhang,Fan Jia,Xiang Li,Hao Zhang,Jun Shi,Liyan Ma,Shihui Ying
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Few-shot semantic segmentation (FSS) methods have shown great promise in handling data-scarce scenarios, particularly in medical image segmentation tasks. However, most existing FSS architectures lack sufficient interpretability and fail to fully incorporate the underlying physical structures of semantic regions. To address these issues, in this paper, we propose a novel deep unfolding network, called the Learned Mumford-Shah Network (LMS-Net), for the FSS task. Specifically, motivated by the effectiveness of pixel-to-prototype comparison in prototypical FSS methods and the capability of deep priors to model complex spatial structures, we leverage our learned Mumford-Shah model (LMS model) as a mathematical foundation to integrate these insights into a unified framework. By reformulating the LMS model into prototype update and mask update tasks, we propose an alternating optimization algorithm to solve it efficiently. Further, the iterative steps of this algorithm are unfolded into corresponding network modules, resulting in LMS-Net with clear interpretability. Comprehensive experiments on three publicly available medical segmentation datasets verify the effectiveness of our method, demonstrating superior accuracy and robustness in handling complex structures and adapting to challenging segmentation scenarios. These results highlight the potential of LMS-Net to advance FSS in medical imaging applications. Our code will be available at: https://github.com/SDZhang01/LMSNet

### DCENWCNet: A Deep CNN Ensemble Network for White Blood Cell Classification with LIME-Based Explainability 
[[arxiv](https://arxiv.org/abs/2502.05459)] [[cool](https://papers.cool/arxiv/2502.05459)] [[pdf](https://arxiv.org/pdf/2502.05459)]
> **Authors**: Sibasish Dhibar
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,细胞行为,机器学习
- **Abstract**: White blood cells (WBC) are important parts of our immune system, and they protect our body against infections by eliminating viruses, bacteria, parasites and fungi. The number of WBC types and the total number of WBCs provide important information about our health status. A traditional method, convolutional neural networks (CNN), a deep learning architecture, can classify the blood cell from a part of an object and perform object recognition. Various CNN models exhibit potential; however, their development often involves ad-hoc processes that neglect unnecessary layers, leading to issues with unbalanced datasets and insufficient data augmentation. To address these challenges, we propose a novel ensemble approach that integrates three CNN architectures, each uniquely configured with different dropout and max-pooling layer settings to enhance feature learning. This ensemble model, named DCENWCNet, effectively balances the bias-variance trade-off. When evaluated on the widely recognized Rabbin-WBC dataset, our model outperforms existing state-of-the-art networks, achieving highest mean accuracy. Additionally, it demonstrates superior performance in precision, recall, F1-score, and Area Under the ROC Curve (AUC) across all categories. To delve deeper into the interpretability of classifiers, we employ reliable post-hoc explanation techniques, including Local Interpretable Model-Agnostic Explanations (LIME). These methods approximate the behavior of a black-box model by elucidating the relationships between feature values and predictions. Interpretable results enable users to comprehend and validate the model's predictions, thereby increasing their confidence in the automated diagnosis.

### Block Graph Neural Networks for tumor heterogeneity prediction 
[[arxiv](https://arxiv.org/abs/2502.05458)] [[cool](https://papers.cool/arxiv/2502.05458)] [[pdf](https://arxiv.org/pdf/2502.05458)]
> **Authors**: Marianne Abémgnigni Njifon,Tobias Weber,Viktor Bezborodov,Tyll Krueger,Dominic Schuhmacher
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: 27 pages, 8 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习,机器学习
- **Abstract**: Accurate tumor classification is essential for selecting effective treatments, but current methods have limitations. Standard tumor grading, which categorizes tumors based on cell differentiation, is not recommended as a stand-alone procedure, as some well-differentiated tumors can be malignant. Tumor heterogeneity assessment via single-cell sequencing offers profound insights but can be costly and may still require significant manual intervention. Many existing statistical machine learning methods for tumor data still require complex pre-processing of MRI and histopathological data. In this paper, we propose to build on a mathematical model that simulates tumor evolution (Ożański (2017)) and generate artificial datasets for tumor classification. Tumor heterogeneity is estimated using normalized entropy, with a threshold to classify tumors as having high or low heterogeneity. Our contributions are threefold: (1) the cut and graph generation processes from the artificial data, (2) the design of tumor features, and (3) the construction of Block Graph Neural Networks (BGNN), a Graph Neural Network-based approach to predict tumor heterogeneity. The experimental results reveal that the combination of the proposed features and models yields excellent results on artificially generated data ($89.67\%$ accuracy on the test data). In particular, in alignment with the emerging trends in AI-assisted grading and spatial transcriptomics, our results suggest that enriching traditional grading methods with birth (e.g., Ki-67 proliferation index) and death markers can improve heterogeneity prediction and enhance tumor classification.

### MoFM: A Large-Scale Human Motion Foundation Model 
[[arxiv](https://arxiv.org/abs/2502.05432)] [[cool](https://papers.cool/arxiv/2502.05432)] [[pdf](https://arxiv.org/pdf/2502.05432)]
> **Authors**: Mohammadreza Baharani,Ghazal Alinezhad Noghre,Armin Danesh Pazho,Gabriel Maldonado,Hamed Tabkhi
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Foundation Models (FM) have increasingly drawn the attention of researchers due to their scalability and generalization across diverse tasks. Inspired by the success of FMs and the principles that have driven advancements in Large Language Models (LLMs), we introduce MoFM as a novel Motion Foundation Model. MoFM is designed for the semantic understanding of complex human motions in both time and space. To facilitate large-scale training, MotionBook, a comprehensive human motion dictionary of discretized motions is designed and employed. MotionBook utilizes Thermal Cubes to capture spatio-temporal motion heatmaps, applying principles from discrete variational models to encode human movements into discrete units for a more efficient and scalable representation. MoFM, trained on a large corpus of motion data, provides a foundational backbone adaptable to diverse downstream tasks, supporting paradigms such as one-shot, unsupervised, and supervised tasks. This versatility makes MoFM well-suited for a wide range of motion-based applications.

### LRA-GNN: Latent Relation-Aware Graph Neural Network with Initial and Dynamic Residual for Facial Age Estimation 
[[arxiv](https://arxiv.org/abs/2502.05423)] [[cool](https://papers.cool/arxiv/2502.05423)] [[pdf](https://arxiv.org/pdf/2502.05423)]
> **Authors**: Yiping Zhang,Yuntao Shou,Wei Ai,Tao Meng,Keqin Li
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Face information is mainly concentrated among facial key points, and frontier research has begun to use graph neural networks to segment faces into patches as nodes to model complex face representations. However, these methods construct node-to-node relations based on similarity thresholds, so there is a problem that some latent relations are missing. These latent relations are crucial for deep semantic representation of face aging. In this novel, we propose a new Latent Relation-Aware Graph Neural Network with Initial and Dynamic Residual (LRA-GNN) to achieve robust and comprehensive facial representation. Specifically, we first construct an initial graph utilizing facial key points as prior knowledge, and then a random walk strategy is employed to the initial graph for obtaining the global structure, both of which together guide the subsequent effective exploration and comprehensive representation. Then LRA-GNN leverages the multi-attention mechanism to capture the latent relations and generates a set of fully connected graphs containing rich facial information and complete structure based on the aforementioned guidance. To avoid over-smoothing issues for deep feature extraction on the fully connected graphs, the deep residual graph convolutional networks are carefully designed, which fuse adaptive initial residuals and dynamic developmental residuals to ensure the consistency and diversity of information. Finally, to improve the estimation accuracy and generalization ability, progressive reinforcement learning is proposed to optimize the ensemble classification regressor. Our proposed framework surpasses the state-of-the-art baselines on several age estimation benchmarks, demonstrating its strength and effectiveness.

### Vision-in-the-loop Simulation for Deep Monocular Pose Estimation of UAV in Ocean Environment 
[[arxiv](https://arxiv.org/abs/2502.05409)] [[cool](https://papers.cool/arxiv/2502.05409)] [[pdf](https://arxiv.org/pdf/2502.05409)]
> **Authors**: Maneesha Wickramasuriya,Beomyeol Yu,Taeyoung Lee,Murray Snyder
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 8 pages, 15 figures, conference
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习,机器人技术,系统与控制
- **Abstract**: This paper proposes a vision-in-the-loop simulation environment for deep monocular pose estimation of a UAV operating in an ocean environment. Recently, a deep neural network with a transformer architecture has been successfully trained to estimate the pose of a UAV relative to the flight deck of a research vessel, overcoming several limitations of GPS-based approaches. However, validating the deep pose estimation scheme in an actual ocean environment poses significant challenges due to the limited availability of research vessels and the associated operational costs. To address these issues, we present a photo-realistic 3D virtual environment leveraging recent advancements in Gaussian splatting, a novel technique that represents 3D scenes by modeling image pixels as Gaussian distributions in 3D space, creating a lightweight and high-quality visual model from multiple viewpoints. This approach enables the creation of a virtual environment integrating multiple real-world images collected in situ. The resulting simulation enables the indoor testing of flight maneuvers while verifying all aspects of flight software, hardware, and the deep monocular pose estimation scheme. This approach provides a cost-effective solution for testing and validating the autonomous flight of shipboard UAVs, specifically focusing on vision-based control and estimation algorithms.

### Convolutional Deep Colorization for Image Compression: A Color Grid Based Approach 
[[arxiv](https://arxiv.org/abs/2502.05402)] [[cool](https://papers.cool/arxiv/2502.05402)] [[pdf](https://arxiv.org/pdf/2502.05402)]
> **Authors**: Ian Tassin,Kristen Goebel,Brittany Lasher
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: The search for image compression optimization techniques is a topic of constant interest both in and out of academic circles. One method that shows promise toward future improvements in this field is image colorization since image colorization algorithms can reduce the amount of color data that needs to be stored for an image. Our work focuses on optimizing a color grid based approach to fully-automated image color information retention with regard to convolutional colorization network architecture for the purposes of image compression. More generally, using a convolutional neural network for image re-colorization, we want to minimize the amount of color information that is stored while still being able to faithfully re-color images. Our results yielded a promising image compression ratio, while still allowing for successful image recolorization reaching high CSIM values.

### Drone Detection and Tracking with YOLO and a Rule-based Method 
[[arxiv](https://arxiv.org/abs/2502.05292)] [[cool](https://papers.cool/arxiv/2502.05292)] [[pdf](https://arxiv.org/pdf/2502.05292)]
> **Authors**: Purbaditya Bhattacharya,Patrick Nowak
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: Drones or unmanned aerial vehicles are traditionally used for military missions, warfare, and espionage. However, the usage of drones has significantly increased due to multiple industrial applications involving security and inspection, transportation, research purposes, and recreational drone flying. Such an increased volume of drone activity in public spaces requires regulatory actions for purposes of privacy protection and safety. Hence, detection of illegal drone activities such as boundary encroachment becomes a necessity. Such detection tasks are usually automated and performed by deep learning models which are trained on annotated image datasets. This paper builds on a previous work and extends an already published open source dataset. A description and analysis of the entire dataset is provided. The dataset is used to train the YOLOv7 deep learning model and some of its minor variants and the results are provided. Since the detection models are based on a single image input, a simple cross-correlation based tracker is used to reduce detection drops and improve tracking performance in videos. Finally, the entire drone detection system is summarized.

### Invizo: Arabic Handwritten Document Optical Character Recognition Solution 
[[arxiv](https://arxiv.org/abs/2502.05277)] [[cool](https://papers.cool/arxiv/2502.05277)] [[pdf](https://arxiv.org/pdf/2502.05277)]
> **Authors**: Alhossien Waly,Bassant Tarek,Ali Feteha,Rewan Yehia,Gasser Amr,Walid Gomaa,Ahmed Fares
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Converting images of Arabic text into plain text is a widely researched topic in academia and industry. However, recognition of Arabic handwritten and printed text presents difficult challenges due to the complex nature of variations of the Arabic script. This work proposes an end-to-end solution for recognizing Arabic handwritten, printed, and Arabic numbers and presents the data in a structured manner. We reached 81.66% precision, 78.82% Recall, and 79.07% F-measure on a Text Detection task that powers the proposed solution. The proposed recognition model incorporates state-of-the-art CNN-based feature extraction, and Transformer-based sequence modeling to accommodate variations in handwriting styles, stroke thicknesses, alignments, and noise conditions. The evaluation of the model suggests its strong performances on both printed and handwritten texts, yielding 0.59% CER and & 1.72% WER on printed text, and 7.91% CER and 31.41% WER on handwritten text. The overall proposed solution has proven to be relied on in real-life OCR tasks. Equipped with both detection and recognition models as well as other Feature Extraction and Matching helping algorithms. With the general purpose implementation, making the solution valid for any given document or receipt that is Arabic handwritten or printed. Thus, it is practical and useful for any given context.

### Interpretable Failure Detection with Human-Level Concepts 
[[arxiv](https://arxiv.org/abs/2502.05275)] [[cool](https://papers.cool/arxiv/2502.05275)] [[pdf](https://arxiv.org/pdf/2502.05275)]
> **Authors**: Kien X. Nguyen,Tang Li,Xi Peng
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Reliable failure detection holds paramount importance in safety-critical applications. Yet, neural networks are known to produce overconfident predictions for misclassified samples. As a result, it remains a problematic matter as existing confidence score functions rely on category-level signals, the logits, to detect failures. This research introduces an innovative strategy, leveraging human-level concepts for a dual purpose: to reliably detect when a model fails and to transparently interpret why. By integrating a nuanced array of signals for each category, our method enables a finer-grained assessment of the model's confidence. We present a simple yet highly effective approach based on the ordinal ranking of concept activation to the input image. Without bells and whistles, our method significantly reduce the false positive rate across diverse real-world image classification benchmarks, specifically by 3.7% on ImageNet and 9% on EuroSAT.

### Survey on AI-Generated Media Detection: From Non-MLLM to MLLM 
[[arxiv](https://arxiv.org/abs/2502.05240)] [[cool](https://papers.cool/arxiv/2502.05240)] [[pdf](https://arxiv.org/pdf/2502.05240)]
> **Authors**: Yueying Zou,Peipei Li,Zekun Li,Huaibo Huang,Xing Cui,Xuannan Liu,Chenghanyu Zhang,Ran He
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: The proliferation of AI-generated media poses significant challenges to information authenticity and social trust, making reliable detection methods highly demanded. Methods for detecting AI-generated media have evolved rapidly, paralleling the advancement of Multimodal Large Language Models (MLLMs). Current detection approaches can be categorized into two main groups: Non-MLLM-based and MLLM-based methods. The former employs high-precision, domain-specific detectors powered by deep learning techniques, while the latter utilizes general-purpose detectors based on MLLMs that integrate authenticity verification, explainability, and localization capabilities. Despite significant progress in this field, there remains a gap in literature regarding a comprehensive survey that examines the transition from domain-specific to general-purpose detection methods. This paper addresses this gap by providing a systematic review of both approaches, analyzing them from single-modal and multi-modal perspectives. We present a detailed comparative analysis of these categories, examining their methodological similarities and differences. Through this analysis, we explore potential hybrid approaches and identify key challenges in forgery detection, providing direction for future research. Additionally, as MLLMs become increasingly prevalent in detection tasks, ethical and security considerations have emerged as critical global concerns. We examine the regulatory landscape surrounding Generative AI (GenAI) across various jurisdictions, offering valuable insights for researchers and practitioners in this field.

### L2GNet: Optimal Local-to-Global Representation of Anatomical Structures for Generalized Medical Image Segmentation 
[[arxiv](https://arxiv.org/abs/2502.05229)] [[cool](https://papers.cool/arxiv/2502.05229)] [[pdf](https://arxiv.org/pdf/2502.05229)]
> **Authors**: Vandan Gorade,Sparsh Mittal,Neethi Dasu,Rekha Singhal,KC Santosh,Debesh Jha
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Continuous Latent Space (CLS) and Discrete Latent Space (DLS) models, like AttnUNet and VQUNet, have excelled in medical image segmentation. In contrast, Synergistic Continuous and Discrete Latent Space (CDLS) models show promise in handling fine and coarse-grained information. However, they struggle with modeling long-range dependencies. CLS or CDLS-based models, such as TransUNet or SynergyNet are adept at capturing long-range dependencies. Since they rely heavily on feature pooling or aggregation using self-attention, they may capture dependencies among redundant regions. This hinders comprehension of anatomical structure content, poses challenges in modeling intra-class and inter-class dependencies, increases false negatives and compromises generalization. Addressing these issues, we propose L2GNet, which learns global dependencies by relating discrete codes obtained from DLS using optimal transport and aligning codes on a trainable reference. L2GNet achieves discriminative on-the-fly representation learning without an additional weight matrix in self-attention models, making it computationally efficient for medical applications. Extensive experiments on multi-organ segmentation and cardiac datasets demonstrate L2GNet's superiority over state-of-the-art methods, including the CDLS method SynergyNet, offering an novel approach to enhance deep learning models' performance in medical image analysis.

### QLIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive Multimodal Understanding and Generation 
[[arxiv](https://arxiv.org/abs/2502.05178)] [[cool](https://papers.cool/arxiv/2502.05178)] [[pdf](https://arxiv.org/pdf/2502.05178)]
> **Authors**: Yue Zhao,Fuzhao Xue,Scott Reed,Linxi Fan,Yuke Zhu,Jan Kautz,Zhiding Yu,Philipp Krähenbühl,De-An Huang
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: Tech report. Project page: https://nvlabs.github.io/QLIP/
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: We introduce Quantized Language-Image Pretraining (QLIP), a visual tokenization method that combines state-of-the-art reconstruction quality with state-of-the-art zero-shot image understanding. QLIP trains a binary-spherical-quantization-based autoencoder with reconstruction and language-image alignment objectives. We are the first to show that the two objectives do not need to be at odds. We balance the two loss terms dynamically during training and show that a two-stage training pipeline effectively mixes the large-batch requirements of image-language pre-training with the memory bottleneck imposed by the reconstruction objective. We validate the effectiveness of QLIP for multimodal understanding and text-conditioned image generation with a single model. Specifically, QLIP serves as a drop-in replacement for the visual encoder for LLaVA and the image tokenizer for LlamaGen with comparable or even better performance. Finally, we demonstrate that QLIP enables a unified mixed-modality auto-regressive model for understanding and generation.

### Long-VITA: Scaling Large Multi-modal Models to 1 Million Tokens with Leading Short-Context Accuracy 
[[arxiv](https://arxiv.org/abs/2502.05177)] [[cool](https://papers.cool/arxiv/2502.05177)] [[pdf](https://arxiv.org/pdf/2502.05177)]
> **Authors**: Yunhang Shen,Chaoyou Fu,Shaoqi Dong,Xiong Wang,Yi-Fan Zhang,Peixian Chen,Mengdan Zhang,Haoyu Cao,Ke Li,Xiawu Zheng,Yan Zhang,Yiyi Zhou,Ran He,Caifeng Shan,Rongrong Ji,Xing Sun
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: https://github.com/VITA-MLLM/Long-VITA
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: We introduce Long-VITA, a simple yet effective large multi-modal model for long-context visual-language understanding tasks. It is adept at concurrently processing and analyzing modalities of image, video, and text over 4K frames or 1M tokens while delivering advanced performances on short-context multi-modal tasks. We propose an effective multi-modal training schema that starts with large language models and proceeds through vision-language alignment, general knowledge learning, and two sequential stages of long-sequence fine-tuning. We further implement context-parallelism distributed inference and logits-masked language modeling head to scale Long-VITA to infinitely long inputs of images and texts during model inference. Regarding training data, Long-VITA is built on a mix of 17M samples from public datasets only and demonstrates the state-of-the-art performance on various multi-modal benchmarks, compared against recent cutting-edge models with internal data. Long-VITA is fully reproducible and supports both NPU and GPU platforms for training and testing. By leveraging our inference designs, Long-VITA models achieve a remarkable 2x prefill speedup and 4x context length extension in single node with 8 GPUs. We hope Long-VITA can serve as a competitive baseline and offer valuable insights for the open-source community in advancing long-context multi-modal understanding.

### Fillerbuster: Multi-View Scene Completion for Casual Captures 
[[arxiv](https://arxiv.org/abs/2502.05175)] [[cool](https://papers.cool/arxiv/2502.05175)] [[pdf](https://arxiv.org/pdf/2502.05175)]
> **Authors**: Ethan Weber,Norman Müller,Yash Kant,Vasu Agrawal,Michael Zollhöfer,Angjoo Kanazawa,Christian Richardt
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: Project page at https://ethanweber.me/fillerbuster/
- **标题**: None
- **领域**: 计算机视觉和模式识别,图形
- **Abstract**: We present Fillerbuster, a method that completes unknown regions of a 3D scene by utilizing a novel large-scale multi-view latent diffusion transformer. Casual captures are often sparse and miss surrounding content behind objects or above the scene. Existing methods are not suitable for handling this challenge as they focus on making the known pixels look good with sparse-view priors, or on creating the missing sides of objects from just one or two photos. In reality, we often have hundreds of input frames and want to complete areas that are missing and unobserved from the input frames. Additionally, the images often do not have known camera parameters. Our solution is to train a generative model that can consume a large context of input frames while generating unknown target views and recovering image poses when desired. We show results where we complete partial captures on two existing datasets. We also present an uncalibrated scene completion task where our unified model predicts both poses and creates new content. Our model is the first to predict many images and poses together for scene completion.

### Flopping for FLOPs: Leveraging equivariance for computational efficiency 
[[arxiv](https://arxiv.org/abs/2502.05169)] [[cool](https://papers.cool/arxiv/2502.05169)] [[pdf](https://arxiv.org/pdf/2502.05169)]
> **Authors**: Georg Bökman,David Nordström,Fredrik Kahl
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Incorporating geometric invariance into neural networks enhances parameter efficiency but typically increases computational costs. This paper introduces new equivariant neural networks that preserve symmetry while maintaining a comparable number of floating-point operations (FLOPs) per parameter to standard non-equivariant networks. We focus on horizontal mirroring (flopping) invariance, common in many computer vision tasks. The main idea is to parametrize the feature spaces in terms of mirror-symmetric and mirror-antisymmetric features, i.e., irreps of the flopping group. This decomposes the linear layers to be block-diagonal, requiring half the number of FLOPs. Our approach reduces both FLOPs and wall-clock time, providing a practical solution for efficient, scalable symmetry-aware architectures.

### Multitwine: Multi-Object Compositing with Text and Layout Control 
[[arxiv](https://arxiv.org/abs/2502.05165)] [[cool](https://papers.cool/arxiv/2502.05165)] [[pdf](https://arxiv.org/pdf/2502.05165)]
> **Authors**: Gemma Canet Tarrés,Zhe Lin,Zhifei Zhang,He Zhang,Andrew Gilbert,John Collomosse,Soo Ye Kim
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: We introduce the first generative model capable of simultaneous multi-object compositing, guided by both text and layout. Our model allows for the addition of multiple objects within a scene, capturing a range of interactions from simple positional relations (e.g., next to, in front of) to complex actions requiring reposing (e.g., hugging, playing guitar). When an interaction implies additional props, like `taking a selfie', our model autonomously generates these supporting objects. By jointly training for compositing and subject-driven generation, also known as customization, we achieve a more balanced integration of textual and visual inputs for text-driven object compositing. As a result, we obtain a versatile model with state-of-the-art performance in both tasks. We further present a data generation pipeline leveraging visual and language models to effortlessly synthesize multimodal, aligned training data.

### Lost in Time: Clock and Calendar Understanding Challenges in Multimodal LLMs 
[[arxiv](https://arxiv.org/abs/2502.05092)] [[cool](https://papers.cool/arxiv/2502.05092)] [[pdf](https://arxiv.org/pdf/2502.05092)]
> **Authors**: Rohit Saxena,Aryo Pradipta Gema,Pasquale Minervini
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: Preprint
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **Abstract**: Understanding time from visual representations is a fundamental cognitive skill, yet it remains a challenge for multimodal large language models (MLLMs). In this work, we investigate the capabilities of MLLMs in interpreting time and date through analogue clocks and yearly calendars. To facilitate this, we curated a structured dataset comprising two subsets: 1) $\textit{ClockQA}$, which comprises various types of clock styles$-$standard, black-dial, no-second-hand, Roman numeral, and arrow-hand clocks$-$paired with time related questions; and 2) $\textit{CalendarQA}$, which consists of yearly calendar images with questions ranging from commonly known dates (e.g., Christmas, New Year's Day) to computationally derived ones (e.g., the 100th or 153rd day of the year). We aim to analyse how MLLMs can perform visual recognition, numerical reasoning, and temporal inference when presented with time-related visual data. Our evaluations show that despite recent advancements, reliably understanding time remains a significant challenge for MLLMs.

### DCFormer: Efficient 3D Vision-Language Modeling with Decomposed Convolutions 
[[arxiv](https://arxiv.org/abs/2502.05091)] [[cool](https://papers.cool/arxiv/2502.05091)] [[pdf](https://arxiv.org/pdf/2502.05091)]
> **Authors**: Gorkem Can Ates,Kuang Gong,Wei Shao
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Vision-language models (VLMs) align visual and textual representations, enabling high-performance zero-shot classification and image-text retrieval in 2D medical imaging. However, extending VLMs to 3D medical imaging remains computationally challenging. Existing 3D VLMs rely on Vision Transformers (ViTs), which are computationally expensive due to self-attention's quadratic complexity, or 3D convolutions, which demand excessive parameters and FLOPs as kernel size increases. We introduce DCFormer, an efficient 3D medical image encoder that factorizes 3D convolutions into three parallel 1D convolutions along depth, height, and width. This design preserves spatial information while significantly reducing computational cost. Integrated into a CLIP-based vision-language framework, DCFormer is evaluated on CT-RATE, a dataset of 50,188 paired 3D chest CT volumes and radiology reports, for zero-shot multi-abnormality detection across 18 pathologies. Compared to ViT, ConvNeXt, PoolFormer, and TransUNet, DCFormer achieves superior efficiency and accuracy, with DCFormer-Tiny reaching 62.0% accuracy and a 46.3% F1-score while using significantly fewer parameters. These results highlight DCFormer's potential for scalable, clinically deployable 3D medical VLMs. Our codes will be publicly available.

### Beautiful Images, Toxic Words: Understanding and Addressing Offensive Text in Generated Images 
[[arxiv](https://arxiv.org/abs/2502.05066)] [[cool](https://papers.cool/arxiv/2502.05066)] [[pdf](https://arxiv.org/pdf/2502.05066)]
> **Authors**: Aditya Kumar,Tom Blanchard,Adam Dziedzic,Franziska Boenisch
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: State-of-the-art visual generation models, such as Diffusion Models (DMs) and Vision Auto-Regressive Models (VARs), produce highly realistic images. While prior work has successfully mitigated Not Safe For Work (NSFW) content in the visual domain, we identify a novel threat: the generation of NSFW text embedded within images. This includes offensive language, such as insults, racial slurs, and sexually explicit terms, posing significant risks to users. We show that all state-of-the-art DMs (e.g., SD3, Flux, DeepFloyd IF) and VARs (e.g., Infinity) are vulnerable to this issue. Through extensive experiments, we demonstrate that existing mitigation techniques, effective for visual content, fail to prevent harmful text generation while substantially degrading benign text generation. As an initial step toward addressing this threat, we explore safety fine-tuning of the text encoder underlying major DM architectures using a customized dataset. Thereby, we suppress NSFW generation while preserving overall image and text generation quality. Finally, to advance research in this area, we introduce ToxicBench, an open-source benchmark for evaluating NSFW text generation in images. ToxicBench provides a curated dataset of harmful prompts, new metrics, and an evaluation pipeline assessing both NSFW-ness and generation quality. Our benchmark aims to guide future efforts in mitigating NSFW text generation in text-to-image models.

### Differentiable Mobile Display Photometric Stereo 
[[arxiv](https://arxiv.org/abs/2502.05055)] [[cool](https://papers.cool/arxiv/2502.05055)] [[pdf](https://arxiv.org/pdf/2502.05055)]
> **Authors**: Gawoon Ban,Hyeongjun Kim,Seokjun Choi,Seungwoo Yoon,Seung-Hwan Baek
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 9 pages
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,图形,机器学习
- **Abstract**: Display photometric stereo uses a display as a programmable light source to illuminate a scene with diverse illumination conditions. Recently, differentiable display photometric stereo (DDPS) demonstrated improved normal reconstruction accuracy by using learned display patterns. However, DDPS faced limitations in practicality, requiring a fixed desktop imaging setup using a polarization camera and a desktop-scale monitor. In this paper, we propose a more practical physics-based photometric stereo, differentiable mobile display photometric stereo (DMDPS), that leverages a mobile phone consisting of a display and a camera. We overcome the limitations of using a mobile device by developing a mobile app and method that simultaneously displays patterns and captures high-quality HDR images. Using this technique, we capture real-world 3D-printed objects and learn display patterns via a differentiable learning process. We demonstrate the effectiveness of DMDPS on both a 3D printed dataset and a first dataset of fallen leaves. The leaf dataset contains reconstructed surface normals and albedos of fallen leaves that may enable future research beyond computer graphics and vision. We believe that DMDPS takes a step forward for practical physics-based photometric stereo.

### Trust-Aware Diversion for Data-Effective Distillation 
[[arxiv](https://arxiv.org/abs/2502.05027)] [[cool](https://papers.cool/arxiv/2502.05027)] [[pdf](https://arxiv.org/pdf/2502.05027)]
> **Authors**: Zhuojie Wu,Yanbin Liu,Xin Shen,Xiaofeng Cao,Xin Yu
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Dataset distillation compresses a large dataset into a small synthetic subset that retains essential information. Existing methods assume that all samples are perfectly labeled, limiting their real-world applications where incorrect labels are ubiquitous. These mislabeled samples introduce untrustworthy information into the dataset, which misleads model optimization in dataset distillation. To tackle this issue, we propose a Trust-Aware Diversion (TAD) dataset distillation method. Our proposed TAD introduces an iterative dual-loop optimization framework for data-effective distillation. Specifically, the outer loop divides data into trusted and untrusted spaces, redirecting distillation toward trusted samples to guarantee trust in the distillation process. This step minimizes the impact of mislabeled samples on dataset distillation. The inner loop maximizes the distillation objective by recalibrating untrusted samples, thus transforming them into valuable ones for distillation. This dual-loop iteratively refines and compensates for each other, gradually expanding the trusted space and shrinking the untrusted space. Experiments demonstrate that our method can significantly improve the performance of existing dataset distillation methods on three widely used benchmarks (CIFAR10, CIFAR100, and Tiny ImageNet) in three challenging mislabeled settings (symmetric, asymmetric, and real-world).

### OccGS: Zero-shot 3D Occupancy Reconstruction with Semantic and Geometric-Aware Gaussian Splatting 
[[arxiv](https://arxiv.org/abs/2502.04981)] [[cool](https://papers.cool/arxiv/2502.04981)] [[pdf](https://arxiv.org/pdf/2502.04981)]
> **Authors**: Xiaoyu Zhou,Jingqi Wang,Yongtao Wang,Yufei Wei,Nan Dong,Ming-Hsuan Yang
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Obtaining semantic 3D occupancy from raw sensor data without manual annotations remains an essential yet challenging task. While prior works have approached this as a perception prediction problem, we formulate it as scene-aware 3D occupancy reconstruction with geometry and semantics. In this work, we propose OccGS, a novel 3D Occupancy reconstruction framework utilizing Semantic and Geometric-Aware Gaussian Splatting in a zero-shot manner. Leveraging semantics extracted from vision-language models and geometry guided by LiDAR points, OccGS constructs Semantic and Geometric-Aware Gaussians from raw multisensor data. We also develop a cumulative Gaussian-to-3D voxel splatting method for reconstructing occupancy from the Gaussians. OccGS performs favorably against self-supervised methods in occupancy prediction, achieving comparable performance to fully supervised approaches and achieving state-of-the-art performance on zero-shot semantic 3D occupancy estimation.

### Training-free Neural Architecture Search through Variance of Knowledge of Deep Network Weights 
[[arxiv](https://arxiv.org/abs/2502.04975)] [[cool](https://papers.cool/arxiv/2502.04975)] [[pdf](https://arxiv.org/pdf/2502.04975)]
> **Authors**: Ondřej Týbl,Lukáš Neumann
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Deep learning has revolutionized computer vision, but it achieved its tremendous success using deep network architectures which are mostly hand-crafted and therefore likely suboptimal. Neural Architecture Search (NAS) aims to bridge this gap by following a well-defined optimization paradigm which systematically looks for the best architecture, given objective criterion such as maximal classification accuracy. The main limitation of NAS is however its astronomical computational cost, as it typically requires training each candidate network architecture from scratch. In this paper, we aim to alleviate this limitation by proposing a novel training-free proxy for image classification accuracy based on Fisher Information. The proposed proxy has a strong theoretical background in statistics and it allows estimating expected image classification accuracy of a given deep network without training the network, thus significantly reducing computational cost of standard NAS algorithms. Our training-free proxy achieves state-of-the-art results on three public datasets and in two search spaces, both when evaluated using previously proposed metrics, as well as using a new metric that we propose which we demonstrate is more informative for practical NAS applications. The source code is publicly available at http://www.github.com/ondratybl/VKDNW

### SurGen: 1020 H&E-stained Whole Slide Images With Survival and Genetic Markers 
[[arxiv](https://arxiv.org/abs/2502.04946)] [[cool](https://papers.cool/arxiv/2502.04946)] [[pdf](https://arxiv.org/pdf/2502.04946)]
> **Authors**: Craig Myles,In Hwa Um,Craig Marshall,David Harris-Birtill,David J. Harrison
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: To download the dataset, see https://doi.org/10.6019/S-BIAD1285. See https://github.com/CraigMyles/SurGen-Dataset for GitHub repository and additional info
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: $\textbf{Background}$: Cancer remains one of the leading causes of morbidity and mortality worldwide. Comprehensive datasets that combine histopathological images with genetic and survival data across various tumour sites are essential for advancing computational pathology and personalised medicine. $\textbf{Results}$: We present SurGen, a dataset comprising 1,020 H&E-stained whole slide images (WSIs) from 843 colorectal cancer cases. The dataset includes detailed annotations for key genetic mutations (KRAS, NRAS, BRAF) and mismatch repair status, as well as survival data for 426 cases. To demonstrate SurGen's practical utility, we conducted a proof-of-concept machine learning experiment predicting mismatch repair status from the WSIs, achieving a test AUROC of 0.8316. These preliminary results underscore the dataset's potential to facilitate research in biomarker discovery, prognostic modelling, and advanced machine learning applications in colorectal cancer. $\textbf{Conclusions}$: SurGen offers a valuable resource for the scientific community, enabling studies that require high-quality WSIs linked with comprehensive clinical and genetic information on colorectal cancer. Our initial findings affirm the dataset's capacity to advance diagnostic precision and foster the development of personalised treatment strategies in colorectal oncology. Data available online at https://doi.org/10.6019/S-BIAD1285.

### Goku: Flow Based Video Generative Foundation Models 
[[arxiv](https://arxiv.org/abs/2502.04896)] [[cool](https://papers.cool/arxiv/2502.04896)] [[pdf](https://arxiv.org/pdf/2502.04896)]
> **Authors**: Shoufa Chen,Chongjian Ge,Yuqi Zhang,Yida Zhang,Fengda Zhu,Hao Yang,Hongxiang Hao,Hui Wu,Zhichao Lai,Yifei Hu,Ting-Che Lin,Shilong Zhang,Fu Li,Chuan Li,Xing Wang,Yanghua Peng,Peize Sun,Ping Luo,Yi Jiang,Zehuan Yuan,Bingyue Peng,Xiaobing Liu
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: Demo: https://saiyan-world.github.io/goku/
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: This paper introduces Goku, a state-of-the-art family of joint image-and-video generation models leveraging rectified flow Transformers to achieve industry-leading performance. We detail the foundational elements enabling high-quality visual generation, including the data curation pipeline, model architecture design, flow formulation, and advanced infrastructure for efficient and robust large-scale training. The Goku models demonstrate superior performance in both qualitative and quantitative evaluations, setting new benchmarks across major tasks. Specifically, Goku achieves 0.76 on GenEval and 83.65 on DPG-Bench for text-to-image generation, and 84.85 on VBench for text-to-video tasks. We believe that this work provides valuable insights and practical advancements for the research community in developing joint image-and-video generation models.

### Relative Age Estimation Using Face Images 
[[arxiv](https://arxiv.org/abs/2502.04852)] [[cool](https://papers.cool/arxiv/2502.04852)] [[pdf](https://arxiv.org/pdf/2502.04852)]
> **Authors**: Ran Sandhaus,Yosi Keller
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: This work introduces a novel deep-learning approach for estimating age from a single facial image by refining an initial age estimate. The refinement leverages a reference face database of individuals with similar ages and appearances. We employ a network that estimates age differences between an input image and reference images with known ages, thus refining the initial estimate. Our method explicitly models age-dependent facial variations using differential regression, yielding improved accuracy compared to conventional absolute age estimation. Additionally, we introduce an age augmentation scheme that iteratively refines initial age estimates by modeling their error distribution during training. This iterative approach further enhances the initial estimates. Our approach surpasses existing methods, achieving state-of-the-art accuracy on the MORPH II and CACD datasets. Furthermore, we examine the biases inherent in contemporary state-of-the-art age estimation techniques.

### HumanDiT: Pose-Guided Diffusion Transformer for Long-form Human Motion Video Generation 
[[arxiv](https://arxiv.org/abs/2502.04847)] [[cool](https://papers.cool/arxiv/2502.04847)] [[pdf](https://arxiv.org/pdf/2502.04847)]
> **Authors**: Qijun Gan,Yi Ren,Chen Zhang,Zhenhui Ye,Pan Xie,Xiang Yin,Zehuan Yuan,Bingyue Peng,Jianke Zhu
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: https://agnjason.github.io/HumanDiT-page/
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Human motion video generation has advanced significantly, while existing methods still struggle with accurately rendering detailed body parts like hands and faces, especially in long sequences and intricate motions. Current approaches also rely on fixed resolution and struggle to maintain visual consistency. To address these limitations, we propose HumanDiT, a pose-guided Diffusion Transformer (DiT)-based framework trained on a large and wild dataset containing 14,000 hours of high-quality video to produce high-fidelity videos with fine-grained body rendering. Specifically, (i) HumanDiT, built on DiT, supports numerous video resolutions and variable sequence lengths, facilitating learning for long-sequence video generation; (ii) we introduce a prefix-latent reference strategy to maintain personalized characteristics across extended sequences. Furthermore, during inference, HumanDiT leverages Keypoint-DiT to generate subsequent pose sequences, facilitating video continuation from static images or existing videos. It also utilizes a Pose Adapter to enable pose transfer with given sequences. Extensive experiments demonstrate its superior performance in generating long-form, pose-accurate videos across diverse scenarios.

### Lightweight Operations for Visual Speech Recognition 
[[arxiv](https://arxiv.org/abs/2502.04834)] [[cool](https://papers.cool/arxiv/2502.04834)] [[pdf](https://arxiv.org/pdf/2502.04834)]
> **Authors**: Iason Ioannis Panagos,Giorgos Sfikas,Christophoros Nikou
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 10 pages (double column format), 7 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习
- **Abstract**: Visual speech recognition (VSR), which decodes spoken words from video data, offers significant benefits, particularly when audio is unavailable. However, the high dimensionality of video data leads to prohibitive computational costs that demand powerful hardware, limiting VSR deployment on resource-constrained devices. This work addresses this limitation by developing lightweight VSR architectures. Leveraging efficient operation design paradigms, we create compact yet powerful models with reduced resource requirements and minimal accuracy loss. We train and evaluate our models on a large-scale public dataset for recognition of words from video sequences, demonstrating their effectiveness for practical applications. We also conduct an extensive array of ablative experiments to thoroughly analyze the size and complexity of each model. Code and trained models will be made publicly available.

### Autoregressive Generation of Static and Growing Trees 
[[arxiv](https://arxiv.org/abs/2502.04762)] [[cool](https://papers.cool/arxiv/2502.04762)] [[pdf](https://arxiv.org/pdf/2502.04762)]
> **Authors**: Hanxiao Wang,Biao Zhang,Jonathan Klein,Dominik L. Michels,Dongming Yan,Peter Wonka
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: We propose a transformer architecture and training strategy for tree generation. The architecture processes data at multiple resolutions and has an hourglass shape, with middle layers processing fewer tokens than outer layers. Similar to convolutional networks, we introduce longer range skip connections to completent this multi-resolution approach. The key advantage of this architecture is the faster processing speed and lower memory consumption. We are therefore able to process more complex trees than would be possible with a vanilla transformer architecture. Furthermore, we extend this approach to perform image-to-tree and point-cloud-to-tree conditional generation and to simulate the tree growth processes, generating 4D trees. Empirical results validate our approach in terms of speed, memory consumption, and generation quality.

### ELITE: Enhanced Language-Image Toxicity Evaluation for Safety 
[[arxiv](https://arxiv.org/abs/2502.04757)] [[cool](https://papers.cool/arxiv/2502.04757)] [[pdf](https://arxiv.org/pdf/2502.04757)]
> **Authors**: Wonjun Lee,Doehyeon Lee,Eugene Choi,Sangyoon Yu,Ashkan Yousefpour,Haon Park,Bumsub Ham,Suhyun Kim
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算语言学
- **Abstract**: Current Vision Language Models (VLMs) remain vulnerable to malicious prompts that induce harmful outputs. Existing safety benchmarks for VLMs primarily rely on automated evaluation methods, but these methods struggle to detect implicit harmful content or produce inaccurate evaluations. Therefore, we found that existing benchmarks have low levels of harmfulness, ambiguous data, and limited diversity in image-text pair combinations. To address these issues, we propose the ELITE benchmark, a high-quality safety evaluation benchmark for VLMs, underpinned by our enhanced evaluation method, the ELITE evaluator. The ELITE evaluator explicitly incorporates a toxicity score to accurately assess harmfulness in multimodal contexts, where VLMs often provide specific, convincing, but unharmful descriptions of images. We filter out ambiguous and low-quality image-text pairs from existing benchmarks using the ELITE evaluator and generate diverse combinations of safe and unsafe image-text pairs. Our experiments demonstrate that the ELITE evaluator achieves superior alignment with human evaluations compared to prior automated methods, and the ELITE benchmark offers enhanced benchmark quality and diversity. By introducing ELITE, we pave the way for safer, more robust VLMs, contributing essential tools for evaluating and mitigating safety risks in real-world applications.

### Self-Supervised Learning for Pre-training Capsule Networks: Overcoming Medical Imaging Dataset Challenges 
[[arxiv](https://arxiv.org/abs/2502.04748)] [[cool](https://papers.cool/arxiv/2502.04748)] [[pdf](https://arxiv.org/pdf/2502.04748)]
> **Authors**: Heba El-Shimy,Hind Zantout,Michael A. Lones,Neamat El Gayar
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Deep learning techniques are increasingly being adopted in diagnostic medical imaging. However, the limited availability of high-quality, large-scale medical datasets presents a significant challenge, often necessitating the use of transfer learning approaches. This study investigates self-supervised learning methods for pre-training capsule networks in polyp diagnostics for colon cancer. We used the PICCOLO dataset, comprising 3,433 samples, which exemplifies typical challenges in medical datasets: small size, class imbalance, and distribution shifts between data splits. Capsule networks offer inherent interpretability due to their architecture and inter-layer information routing mechanism. However, their limited native implementation in mainstream deep learning frameworks and the lack of pre-trained versions pose a significant challenge. This is particularly true if aiming to train them on small medical datasets, where leveraging pre-trained weights as initial parameters would be beneficial. We explored two auxiliary self-supervised learning tasks, colourisation and contrastive learning, for capsule network pre-training. We compared self-supervised pre-trained models against alternative initialisation strategies. Our findings suggest that contrastive learning and in-painting techniques are suitable auxiliary tasks for self-supervised learning in the medical domain. These techniques helped guide the model to capture important visual features that are beneficial for the downstream task of polyp classification, increasing its accuracy by 5.26% compared to other weight initialisation methods.

### SelaFD:Seamless Adaptation of Vision Transformer Fine-tuning for Radar-based Human Activity 
[[arxiv](https://arxiv.org/abs/2502.04740)] [[cool](https://papers.cool/arxiv/2502.04740)] [[pdf](https://arxiv.org/pdf/2502.04740)]
> **Authors**: Yijun Wang,Yong Wang,Chendong xu,Shuai Yao,Qisong Wu
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Human Activity Recognition (HAR) such as fall detection has become increasingly critical due to the aging population, necessitating effective monitoring systems to prevent serious injuries and fatalities associated with falls. This study focuses on fine-tuning the Vision Transformer (ViT) model specifically for HAR using radar-based Time-Doppler signatures. Unlike traditional image datasets, these signals present unique challenges due to their non-visual nature and the high degree of similarity among various activities. Directly fine-tuning the ViT with all parameters proves suboptimal for this application. To address this challenge, we propose a novel approach that employs Low-Rank Adaptation (LoRA) fine-tuning in the weight space to facilitate knowledge transfer from pre-trained ViT models. Additionally, to extract fine-grained features, we enhance feature representation through the integration of a serial-parallel adapter in the feature space. Our innovative joint fine-tuning method, tailored for radar-based Time-Doppler signatures, significantly improves HAR accuracy, surpassing existing state-of-the-art methodologies in this domain. Our code is released at https://github.com/wangyijunlyy/SelaFD.

### Tolerance-Aware Deep Optics 
[[arxiv](https://arxiv.org/abs/2502.04719)] [[cool](https://papers.cool/arxiv/2502.04719)] [[pdf](https://arxiv.org/pdf/2502.04719)]
> **Authors**: Jun Dai,Liqun Chen,Xinge Yang,Yuyao Hu,Jinwei Gu,Tianfan Xue
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 14 pages, 14 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,图形
- **Abstract**: Deep optics has emerged as a promising approach by co-designing optical elements with deep learning algorithms. However, current research typically overlooks the analysis and optimization of manufacturing and assembly tolerances. This oversight creates a significant performance gap between designed and fabricated optical systems. To address this challenge, we present the first end-to-end tolerance-aware optimization framework that incorporates multiple tolerance types into the deep optics design pipeline. Our method combines physics-informed modelling with data-driven training to enhance optical design by accounting for and compensating for structural deviations in manufacturing and assembly. We validate our approach through computational imaging applications, demonstrating results in both simulations and real-world experiments. We further examine how our proposed solution improves the robustness of optical systems and vision algorithms against tolerances through qualitative and quantitative analyses. Code and additional visual results are available at openimaginglab.github.io/LensTolerance.

### AI-Driven Solutions for Falcon Disease Classification: Concatenated ConvNeXt cum EfficientNet AI Model Approach 
[[arxiv](https://arxiv.org/abs/2502.04682)] [[cool](https://papers.cool/arxiv/2502.04682)] [[pdf](https://arxiv.org/pdf/2502.04682)]
> **Authors**: Alavikunhu Panthakkan,Zubair Medammal,S M Anzar,Fatma Taher,Hussain Al-Ahmad
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 5 pages
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Falconry, an ancient practice of training and hunting with falcons, emphasizes the need for vigilant health monitoring to ensure the well-being of these highly valued birds, especially during hunting activities. This research paper introduces a cutting-edge approach, which leverages the power of Concatenated ConvNeXt and EfficientNet AI models for falcon disease classification. Focused on distinguishing 'Normal,' 'Liver,' and 'Aspergillosis' cases, the study employs a comprehensive dataset for model training and evaluation, utilizing metrics such as accuracy, precision, recall, and f1-score. Through rigorous experimentation and evaluation, we demonstrate the superior performance of the concatenated AI model compared to traditional methods and standalone architectures. This novel approach contributes to accurate falcon disease classification, laying the groundwork for further advancements in avian veterinary AI applications.

### Performance Evaluation of Image Enhancement Techniques on Transfer Learning for Touchless Fingerprint Recognition 
[[arxiv](https://arxiv.org/abs/2502.04680)] [[cool](https://papers.cool/arxiv/2502.04680)] [[pdf](https://arxiv.org/pdf/2502.04680)]
> **Authors**: S Sreehari,Dilavar P D,S M Anzar,Alavikunhu Panthakkan,Saad Ali Amin
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 6 pages
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Fingerprint recognition remains one of the most reliable biometric technologies due to its high accuracy and uniqueness. Traditional systems rely on contact-based scanners, which are prone to issues such as image degradation from surface contamination and inconsistent user interaction. To address these limitations, contactless fingerprint recognition has emerged as a promising alternative, providing non-intrusive and hygienic authentication. This study evaluates the impact of image enhancement tech-niques on the performance of pre-trained deep learning models using transfer learning for touchless fingerprint recognition. The IIT-Bombay Touchless and Touch-Based Fingerprint Database, containing data from 200 subjects, was employed to test the per-formance of deep learning architectures such as VGG-16, VGG-19, Inception-V3, and ResNet-50. Experimental results reveal that transfer learning methods with fingerprint image enhance-ment (indirect method) significantly outperform those without enhancement (direct method). Specifically, VGG-16 achieved an accuracy of 98% in training and 93% in testing when using the enhanced images, demonstrating superior performance compared to the direct method. This paper provides a detailed comparison of the effectiveness of image enhancement in improving the accuracy of transfer learning models for touchless fingerprint recognition, offering key insights for developing more efficient biometric systems.

### Mechanistic Understandings of Representation Vulnerabilities and Engineering Robust Vision Transformers 
[[arxiv](https://arxiv.org/abs/2502.04679)] [[cool](https://papers.cool/arxiv/2502.04679)] [[pdf](https://arxiv.org/pdf/2502.04679)]
> **Authors**: Chashi Mahiul Islam,Samuel Jacob Chacko,Mao Nishino,Xiuwen Liu
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 10 pages, 5 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: While transformer-based models dominate NLP and vision applications, their underlying mechanisms to map the input space to the label space semantically are not well understood. In this paper, we study the sources of known representation vulnerabilities of vision transformers (ViT), where perceptually identical images can have very different representations and semantically unrelated images can have the same representation. Our analysis indicates that imperceptible changes to the input can result in significant representation changes, particularly in later layers, suggesting potential instabilities in the performance of ViTs. Our comprehensive study reveals that adversarial effects, while subtle in early layers, propagate and amplify through the network, becoming most pronounced in middle to late layers. This insight motivates the development of NeuroShield-ViT, a novel defense mechanism that strategically neutralizes vulnerable neurons in earlier layers to prevent the cascade of adversarial effects. We demonstrate NeuroShield-ViT's effectiveness across various attacks, particularly excelling against strong iterative attacks, and showcase its remarkable zero-shot generalization capabilities. Without fine-tuning, our method achieves a competitive accuracy of 77.8% on adversarial examples, surpassing conventional robustness methods. Our results shed new light on how adversarial effects propagate through ViT layers, while providing a promising approach to enhance the robustness of vision transformers against adversarial attacks. Additionally, they provide a promising approach to enhance the robustness of vision transformers against adversarial attacks.

## 计算机与社会(cs.CY:Computers and Society)

### Comprehensive Framework for Evaluating Conversational AI Chatbots 
[[arxiv](https://arxiv.org/abs/2502.06105)] [[cool](https://papers.cool/arxiv/2502.06105)] [[pdf](https://arxiv.org/pdf/2502.06105)]
> **Authors**: Shailja Gupta,Rajesh Ranjan,Surya Narayan Singh
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: 2 Figures
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: Conversational AI chatbots are transforming industries by streamlining customer service, automating transactions, and enhancing user engagement. However, evaluating these systems remains a challenge, particularly in financial services, where compliance, user trust, and operational efficiency are critical. This paper introduces a novel evaluation framework that systematically assesses chatbots across four dimensions: cognitive and conversational intelligence, user experience, operational efficiency, and ethical and regulatory compliance. By integrating advanced AI methodologies with financial regulations, the framework bridges theoretical foundations and real-world deployment challenges. Additionally, we outline future research directions, emphasizing improvements in conversational coherence, real-time adaptability, and fairness.

### MindCraft: Revolutionizing Education through AI-Powered Personalized Learning and Mentorship for Rural India 
[[arxiv](https://arxiv.org/abs/2502.05826)] [[cool](https://papers.cool/arxiv/2502.05826)] [[pdf](https://arxiv.org/pdf/2502.05826)]
> **Authors**: Arihant Bardia,Aayush Agrawal
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,人工智能,新兴技术
- **Abstract**: MindCraft is a modern platform designed to revolutionize education in rural India by leveraging Artificial Intelligence (AI) to create personalized learning experiences, provide mentorship, and foster resource-sharing. In a country where access to quality education is deeply influenced by geography and socio economic status, rural students often face significant barriers in their educational journeys. MindCraft aims to bridge this gap by utilizing AI to create tailored learning paths, connect students with mentors, and enable a collaborative network of educational resources that transcends both physical and digital divides. This paper explores the challenges faced by rural students, the transformative potential of AI, and how MindCraft offers a scalable, sustainable solution for equitable education system. By focusing on inclusivity, personalized learning, and mentorship, MindCraft seeks to empower rural students, equipping them with the skills, knowledge, and opportunities needed to thrive in an increasingly digital world. Ultimately, MindCraft envisions a future in which technology not only bridges educational gaps but also becomes the driving force for a more inclusive and empowered society.

### Using agent-based models and EXplainable Artificial Intelligence (XAI) to simulate social behaviors and policy intervention scenarios: A case study of private well users in Ireland 
[[arxiv](https://arxiv.org/abs/2502.05718)] [[cool](https://papers.cool/arxiv/2502.05718)] [[pdf](https://arxiv.org/pdf/2502.05718)]
> **Authors**: Rabia Asghar,Simon Mooney,Eoin O Neill,Paul Hynds
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,机器学习
- **Abstract**: Around 50 percent of Irelands rural population relies on unregulated private wells vulnerable to agricultural runoff and untreated wastewater. High national rates of Shiga toxin-producing Escherichia coli (STEC) and other waterborne illnesses have been linked to well water exposure. Periodic well testing is essential for public health, yet the lack of government incentives places the financial burden on households. Understanding environmental, cognitive, and material factors influencing well-testing behavior is critical. This study employs Agent-Based Modeling (ABM) to simulate policy interventions based on national survey data. The ABM framework, designed for private well-testing behavior, integrates a Deep Q-network reinforcement learning model and Explainable AI (XAI) for decision-making insights. Key features were selected using Recursive Feature Elimination (RFE) with 10-fold cross-validation, while SHAP (Shapley Additive Explanations) provided further interpretability for policy recommendations. Fourteen policy scenarios were tested. The most effective, Free Well Testing plus Communication Campaign, increased participation to 435 out of 561 agents, from a baseline of approximately 5 percent, with rapid behavioral adaptation. Free Well Testing plus Regulation also performed well, with 433 out of 561 agents initiating well testing. Free testing alone raised participation to over 75 percent, with some agents testing multiple times annually. Scenarios with free well testing achieved faster learning efficiency, converging in 1000 episodes, while others took 2000 episodes, indicating slower adaptation. This research demonstrates the value of ABM and XAI in public health policy, providing a framework for evaluating behavioral interventions in environmental health.

### A Tutorial On Intersectionality in Fair Rankings 
[[arxiv](https://arxiv.org/abs/2502.05333)] [[cool](https://papers.cool/arxiv/2502.05333)] [[pdf](https://arxiv.org/pdf/2502.05333)]
> **Authors**: Chiara Criscuolo,Davide Martinenghi,Giuseppe Piccirillo
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,信息检索,机器学习
- **Abstract**: We address the critical issue of biased algorithms and unfair rankings, which have permeated various sectors, including search engines, recommendation systems, and workforce management. These biases can lead to discriminatory outcomes in a data-driven world, especially against marginalized and underrepresented groups. Efforts towards responsible data science and responsible artificial intelligence aim to mitigate these biases and promote fairness, diversity, and transparency. However, most fairness-aware ranking methods singularly focus on protected attributes such as race, gender, or socio-economic status, neglecting the intersectionality of these attributes, i.e., the interplay between multiple social identities. Understanding intersectionality is crucial to ensure that existing inequalities are not preserved by fair rankings. We offer a description of the main ways to incorporate intersectionality in fair ranking systems through practical examples and provide a comparative overview of existing literature and a synoptic table summarizing the various methodologies. Our analysis highlights the need for intersectionality to attain fairness, while also emphasizing that fairness, alone, does not necessarily imply intersectionality.

### Enabling External Scrutiny of AI Systems with Privacy-Enhancing Technologies 
[[arxiv](https://arxiv.org/abs/2502.05219)] [[cool](https://papers.cool/arxiv/2502.05219)] [[pdf](https://arxiv.org/pdf/2502.05219)]
> **Authors**: Kendrea Beers,Helen Toner
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,人工智能,密码学和安全
- **Abstract**: This article describes how technical infrastructure developed by the nonprofit OpenMined enables external scrutiny of AI systems without compromising sensitive information. Independent external scrutiny of AI systems provides crucial transparency into AI development, so it should be an integral component of any approach to AI governance. In practice, external researchers have struggled to gain access to AI systems because of AI companies' legitimate concerns about security, privacy, and intellectual property. But now, privacy-enhancing technologies (PETs) have reached a new level of maturity: end-to-end technical infrastructure developed by OpenMined combines several PETs into various setups that enable privacy-preserving audits of AI systems. We showcase two case studies where this infrastructure has been deployed in real-world governance scenarios: "Understanding Social Media Recommendation Algorithms with the Christchurch Call" and "Evaluating Frontier Models with the UK AI Safety Institute." We describe types of scrutiny of AI systems that could be facilitated by current setups and OpenMined's proposed future setups. We conclude that these innovative approaches deserve further exploration and support from the AI governance community. Interested policymakers can focus on empowering researchers on a legal level.

### Enhancing Team Diversity with Generative AI: A Novel Project Management Framework 
[[arxiv](https://arxiv.org/abs/2502.05181)] [[cool](https://papers.cool/arxiv/2502.05181)] [[pdf](https://arxiv.org/pdf/2502.05181)]
> **Authors**: Johnny Chan,Yuming Li
> **First submission**: 2025-01-13
> **First announcement**: 2025-02-10
> **comment**: A published version can be found from here - https://www.computer.org/csdl/proceedings-article/compsac/2024/769600b648/1ZIUInSDC0w
- **标题**: None
- **领域**: 计算机与社会,人工智能,机器学习
- **Abstract**: This research-in-progress paper presents a new project management framework that utilises GenAI technology. The framework is designed to address the common challenge of uniform team compositions in academic and research project teams, particularly in universities and research institutions. It does so by integrating sociologically identified patterns of successful team member personalities and roles, using GenAI agents to fill gaps in team dynamics. This approach adds an additional layer of analysis to conventional project management processes by evaluating team members' personalities and roles and employing GenAI agents, fine-tuned on personality datasets, to fill specific team roles. Our initial experiments have shown improvements in the model's ability to understand and process personality traits, suggesting the potential effectiveness of GenAI teammates in real-world project settings. This paper aims to explore the practical application of AI in enhancing team diversity and project management

### An Annotated Reading of 'The Singer of Tales' in the LLM Era 
[[arxiv](https://arxiv.org/abs/2502.05148)] [[cool](https://papers.cool/arxiv/2502.05148)] [[pdf](https://arxiv.org/pdf/2502.05148)]
> **Authors**: Kush R. Varshney
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,计算语言学
- **Abstract**: The Parry-Lord oral-formulaic theory was a breakthrough in understanding how oral narrative poetry is learned, composed, and transmitted by illiterate bards. In this paper, we provide an annotated reading of the mechanism underlying this theory from the lens of large language models (LLMs) and generative artificial intelligence (AI). We point out the the similarities and differences between oral composition and LLM generation, and comment on the implications to society and AI policy.

## 数据库(cs.DB:Databases)

### DobLIX: A Dual-Objective Learned Index for Log-Structured Merge Trees 
[[arxiv](https://arxiv.org/abs/2502.05369)] [[cool](https://papers.cool/arxiv/2502.05369)] [[pdf](https://arxiv.org/pdf/2502.05369)]
> **Authors**: Alireza Heidari,Amirhossein Ahmadi,Wei Zhang
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 14 pages, 15 figures
- **标题**: None
- **领域**: 数据库,机器学习,优化与控制
- **Abstract**: In this paper, we introduce DobLIX, a dual-objective learned index specifically designed for Log-Structured Merge(LSM) tree-based key-value stores. Although traditional learned indexes focus exclusively on optimizing index lookups, they often overlook the impact of data access from storage, resulting in performance bottlenecks. DobLIX addresses this by incorporating a second objective, data access optimization, into the learned index training process. This dual-objective approach ensures that both index lookup efficiency and data access costs are minimized, leading to significant improvements in read performance while maintaining write efficiency in real-world LSM-tree systems. Additionally, DobLIX features a reinforcement learning agent that dynamically tunes the system parameters, allowing it to adapt to varying workloads in real-time. Experimental results using real-world datasets demonstrate that DobLIX reduces indexing overhead and improves throughput by 1.19 to 2.21 times compared to state-of-the-art methods within RocksDB, a widely used LSM-tree-based storage engine.

### PSM-SQL: Progressive Schema Learning with Multi-granularity Semantics for Text-to-SQL 
[[arxiv](https://arxiv.org/abs/2502.05237)] [[cool](https://papers.cool/arxiv/2502.05237)] [[pdf](https://arxiv.org/pdf/2502.05237)]
> **Authors**: Zhuopan Yang,Yuanzhen Xie,Ruichao Zhong,Yunzhi Tan,Enjie Liu,Zhenguo Yang,Mochi Gao,Bo Hu,Zang Li
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 9 pages, 3 figures, submission in progress
- **标题**: None
- **领域**: 数据库,人工智能
- **Abstract**: It is challenging to convert natural language (NL) questions into executable structured query language (SQL) queries for text-to-SQL tasks due to the vast number of database schemas with redundancy, which interferes with semantic learning, and the domain shift between NL and SQL. Existing works for schema linking focus on the table level and perform it once, ignoring the multi-granularity semantics and chainable cyclicity of schemas. In this paper, we propose a progressive schema linking with multi-granularity semantics (PSM-SQL) framework to reduce the redundant database schemas for text-to-SQL. Using the multi-granularity schema linking (MSL) module, PSM-SQL learns the schema semantics at the column, table, and database levels. More specifically, a triplet loss is used at the column level to learn embeddings, while fine-tuning LLMs is employed at the database level for schema reasoning. MSL employs classifier and similarity scores to model schema interactions for schema linking at the table level. In particular, PSM-SQL adopts a chain loop strategy to reduce the task difficulty of schema linking by continuously reducing the number of redundant schemas. Experiments conducted on text-to-SQL datasets show that the proposed PSM-SQL is 1-3 percentage points higher than the existing methods.

### A New Paradigm in Tuning Learned Indexes: A Reinforcement Learning Enhanced Approach 
[[arxiv](https://arxiv.org/abs/2502.05001)] [[cool](https://papers.cool/arxiv/2502.05001)] [[pdf](https://arxiv.org/pdf/2502.05001)]
> **Authors**: Taiyi Wang,Liang Liang,Guang Yang,Thomas Heinis,Eiko Yoneki
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 15 pages
- **标题**: None
- **领域**: 数据库,人工智能,系统与控制
- **Abstract**: Learned Index Structures (LIS) have significantly advanced data management by leveraging machine learning models to optimize data indexing. However, designing these structures often involves critical trade-offs, making it challenging for both designers and end-users to find an optimal balance tailored to specific workloads and scenarios. While some indexes offer adjustable parameters that demand intensive manual tuning, others rely on fixed configurations based on heuristic auto-tuners or expert knowledge, which may not consistently deliver optimal performance. This paper introduces LITune, a novel framework for end-to-end automatic tuning of Learned Index Structures. LITune employs an adaptive training pipeline equipped with a tailor-made Deep Reinforcement Learning (DRL) approach to ensure stable and efficient tuning. To accommodate long-term dynamics arising from online tuning, we further enhance LITune with an on-the-fly updating mechanism termed the O2 system. These innovations allow LITune to effectively capture state transitions in online tuning scenarios and dynamically adjust to changing data distributions and workloads, marking a significant improvement over other tuning methods. Our experimental results demonstrate that LITune achieves up to a 98% reduction in runtime and a 17-fold increase in throughput compared to default parameter settings given a selected Learned Index instance. These findings highlight LITune's effectiveness and its potential to facilitate broader adoption of LIS in real-world applications.

## 数据结构和算法(cs.DS:Data Structures and Algorithms)

### Approximating the total variation distance between spin systems 
[[arxiv](https://arxiv.org/abs/2502.05437)] [[cool](https://papers.cool/arxiv/2502.05437)] [[pdf](https://arxiv.org/pdf/2502.05437)]
> **Authors**: Weiming Feng,Hongyang Liu,Minji Yang
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 数据结构和算法,机器学习,可能性
- **Abstract**: Spin systems form an important class of undirected graphical models. For two Gibbs distributions $μ$ and $ν$ induced by two spin systems on the same graph $G = (V, E)$, we study the problem of approximating the total variation distance $d_{TV}(μ,ν)$ with an $ε$-relative error. We propose a new reduction that connects the problem of approximating the TV-distance to sampling and approximate counting. Our applications include the hardcore model and the antiferromagnetic Ising model in the uniqueness regime, the ferromagnetic Ising model, and the general Ising model satisfying the spectral condition. Additionally, we explore the computational complexity of approximating the total variation distance $d_{TV}(μ_S,ν_S)$ between two marginal distributions on an arbitrary subset $S \subseteq V$. We prove that this problem remains hard even when both $μ$ and $ν$ admit polynomial-time sampling and approximate counting algorithms.

## 计算机科学与博弈论(cs.GT:Computer Science and Game Theory)

### Online Bidding Algorithms with Strict Return on Spend (ROS) Constraint 
[[arxiv](https://arxiv.org/abs/2502.05599)] [[cool](https://papers.cool/arxiv/2502.05599)] [[pdf](https://arxiv.org/pdf/2502.05599)]
> **Authors**: Rahul Vaze,Abhishek Sinha
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机科学与博弈论,数据结构和算法,机器学习
- **Abstract**: Auto-bidding problem under a strict return-on-spend constraint (ROSC) is considered, where an algorithm has to make decisions about how much to bid for an ad slot depending on the revealed value, and the hidden allocation and payment function that describes the probability of winning the ad-slot depending on its bid. The objective of an algorithm is to maximize the expected utility (product of ad value and probability of winning the ad slot) summed across all time slots subject to the total expected payment being less than the total expected utility, called the ROSC. A (surprising) impossibility result is derived that shows that no online algorithm can achieve a sub-linear regret even when the value, allocation and payment function are drawn i.i.d. from an unknown distribution. The problem is non-trivial even when the revealed value remains constant across time slots, and an algorithm with regret guarantee that is optimal up to logarithmic factor is derived.

### An Adaptable Budget Planner for Enhancing Budget-Constrained Auto-Bidding in Online Advertising 
[[arxiv](https://arxiv.org/abs/2502.05187)] [[cool](https://papers.cool/arxiv/2502.05187)] [[pdf](https://arxiv.org/pdf/2502.05187)]
> **Authors**: Zhijian Duan,Yusen Huo,Tianyu Wang,Zhilin Zhang,Yeshu Li,Chuan Yu,Jian Xu,Bo Zheng,Xiaotie Deng
> **First submission**: 2025-01-26
> **First announcement**: 2025-02-10
> **comment**: In KDD 2025 ADS Track August
- **标题**: None
- **领域**: 计算机科学与博弈论,机器学习
- **Abstract**: In online advertising, advertisers commonly utilize auto-bidding services to bid for impression opportunities. A typical objective of the auto-bidder is to optimize the advertiser's cumulative value of winning impressions within specified budget constraints. However, such a problem is challenging due to the complex bidding environment faced by diverse advertisers. To address this challenge, we introduce ABPlanner, a few-shot adaptable budget planner designed to improve budget-constrained auto-bidding. ABPlanner is based on a hierarchical bidding framework that decomposes the bidding process into shorter, manageable stages. Within this framework, ABPlanner allocates the budget across all stages, allowing a low-level auto-bidder to bids based on the budget allocation plan. The adaptability of ABPlanner is achieved through a sequential decision-making approach, inspired by in-context reinforcement learning. For each advertiser, ABPlanner adjusts the budget allocation plan episode by episode, using data from previous episodes as prompt for current decisions. This enables ABPlanner to quickly adapt to different advertisers with few-shot data, providing a sample-efficient solution. Extensive simulation experiments and real-world A/B testing validate the effectiveness of ABPlanner, demonstrating its capability to enhance the cumulative value achieved by auto-bidders.

### Shapley Value Approximation Based on k-Additive Games 
[[arxiv](https://arxiv.org/abs/2502.04763)] [[cool](https://papers.cool/arxiv/2502.04763)] [[pdf](https://arxiv.org/pdf/2502.04763)]
> **Authors**: Guilherme Dean Pelegrina,Patrick Kolpaczki,Eyke Hüllermeier
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算机科学与博弈论,机器学习
- **Abstract**: The Shapley value is the prevalent solution for fair division problems in which a payout is to be divided among multiple agents. By adopting a game-theoretic view, the idea of fair division and the Shapley value can also be used in machine learning to quantify the individual contribution of features or data points to the performance of a predictive model. Despite its popularity and axiomatic justification, the Shapley value suffers from a computational complexity that scales exponentially with the number of entities involved, and hence requires approximation methods for its reliable estimation. We propose SVA$k_{\text{ADD}}$, a novel approximation method that fits a $k$-additive surrogate game. By taking advantage of $k$-additivity, we are able to elicit the exact Shapley values of the surrogate game and then use these values as estimates for the original fair division problem. The efficacy of our method is evaluated empirically and compared to competing methods.

## 人机交互(cs.HC:Human-Computer Interaction)

### Deconstructing Depression Stigma: Integrating AI-driven Data Collection and Analysis with Causal Knowledge Graphs 
[[arxiv](https://arxiv.org/abs/2502.06075)] [[cool](https://papers.cool/arxiv/2502.06075)] [[pdf](https://arxiv.org/pdf/2502.06075)]
> **Authors**: Han Meng,Renwen Zhang,Ganyi Wang,Yitian Yang,Peinuan Qin,Jungup Lee,Yi-Chieh Lee
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: Conditionally accepted to CHI Conference on Human Factors in Computing Systems (CHI'25)
- **标题**: None
- **领域**: 人机交互,计算语言学,计算机与社会
- **Abstract**: Mental-illness stigma is a persistent social problem, hampering both treatment-seeking and recovery. Accordingly, there is a pressing need to understand it more clearly, but analyzing the relevant data is highly labor-intensive. Therefore, we designed a chatbot to engage participants in conversations; coded those conversations qualitatively with AI assistance; and, based on those coding results, built causal knowledge graphs to decode stigma. The results we obtained from 1,002 participants demonstrate that conversation with our chatbot can elicit rich information about people's attitudes toward depression, while our AI-assisted coding was strongly consistent with human-expert coding. Our novel approach combining large language models (LLMs) and causal knowledge graphs uncovered patterns in individual responses and illustrated the interrelationships of psychological constructs in the dataset as a whole. The paper also discusses these findings' implications for HCI researchers in developing digital interventions, decomposing human psychological constructs, and fostering inclusive attitudes.

### Pencils to Pixels: A Systematic Study of Creative Drawings across Children, Adults and AI 
[[arxiv](https://arxiv.org/abs/2502.05999)] [[cool](https://papers.cool/arxiv/2502.05999)] [[pdf](https://arxiv.org/pdf/2502.05999)]
> **Authors**: Surabhi S Nath,Guiomar del Cuvillo y Schröder,Claire E. Stevenson
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: 8 pages, 5 figures, 2 tables
- **标题**: None
- **领域**: 人机交互,人工智能,计算机视觉和模式识别
- **Abstract**: Can we derive computational metrics to quantify visual creativity in drawings across intelligent agents, while accounting for inherent differences in technical skill and style? To answer this, we curate a novel dataset consisting of 1338 drawings by children, adults and AI on a creative drawing task. We characterize two aspects of the drawings -- (1) style and (2) content. For style, we define measures of ink density, ink distribution and number of elements. For content, we use expert-annotated categories to study conceptual diversity, and image and text embeddings to compute distance measures. We compare the style, content and creativity of children, adults and AI drawings and build simple models to predict expert and automated creativity scores. We find significant differences in style and content in the groups -- children's drawings had more components, AI drawings had greater ink density, and adult drawings revealed maximum conceptual diversity. Notably, we highlight a misalignment between creativity judgments obtained through expert and automated ratings and discuss its implications. Through these efforts, our work provides, to the best of our knowledge, the first framework for studying human and artificial creativity beyond the textual modality, and attempts to arrive at the domain-agnostic principles underlying creativity. Our data and scripts are available on GitHub.

### Cyri: A Conversational AI-based Assistant for Supporting the Human User in Detecting and Responding to Phishing Attacks 
[[arxiv](https://arxiv.org/abs/2502.05951)] [[cool](https://papers.cool/arxiv/2502.05951)] [[pdf](https://arxiv.org/pdf/2502.05951)]
> **Authors**: Antonio La Torre,Marco Angelini
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 人机交互,人工智能,密码学和安全
- **Abstract**: This work introduces Cyri, an AI-powered conversational assistant designed to support a human user in detecting and analyzing phishing emails by leveraging Large Language Models. Cyri has been designed to scrutinize emails for semantic features used in phishing attacks, such as urgency, and undesirable consequences, using an approach that unifies features already established in the literature with others by Cyri features extraction methodology. Cyri can be directly plugged into a client mail or webmail, ensuring seamless integration with the user's email workflow while maintaining data privacy through local processing. By performing analyses on the user's machine, Cyri eliminates the need to transmit sensitive email data over the internet, reducing associated security risks. The Cyri user interface has been designed to reduce habituation effects and enhance user engagement. It employs dynamic visual cues and context-specific explanations to keep users alert and informed while using emails. Additionally, it allows users to explore identified malicious semantic features both through conversation with the agent and visual exploration, obtaining the advantages of both modalities for expert or non-expert users. It also allows users to keep track of the conversation, supports the user in solving additional questions on both computed features or new parts of the mail, and applies its detection on demand. To evaluate Cyri, we crafted a comprehensive dataset of 420 phishing emails and 420 legitimate emails. Results demonstrate high effectiveness in identifying critical phishing semantic features fundamental to phishing detection. A user study involving 10 participants, both experts and non-experts, evaluated Cyri's effectiveness and usability. Results indicated that Cyri significantly aided users in identifying phishing emails and enhanced their understanding of phishing tactics.

### WatchGuardian: Enabling User-Defined Personalized Just-in-Time Intervention on Smartwatch 
[[arxiv](https://arxiv.org/abs/2502.05783)] [[cool](https://papers.cool/arxiv/2502.05783)] [[pdf](https://arxiv.org/pdf/2502.05783)]
> **Authors**: Ying Lei,Yancheng Cao,Will Wang,Yuanzhe Dong,Changchang Yin,Weidan Cao,Ping Zhang,Jingzhen Yang,Bingsheng Yao,Yifan Peng,Chunhua Weng,Randy Auerbach,Lena Mamykina,Dakuo Wang,Yuntao Wang,Xuhai Xu
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: Under submission
- **标题**: None
- **领域**: 人机交互,人工智能,机器学习
- **Abstract**: While just-in-time interventions (JITIs) have effectively targeted common health behaviors, individuals often have unique needs to intervene in personal undesirable actions that can negatively affect physical, mental, and social well-being. We present WatchGuardian, a smartwatch-based JITI system that empowers users to define custom interventions for these personal actions with a small number of samples. For the model to detect new actions based on limited new data samples, we developed a few-shot learning pipeline that finetuned a pre-trained inertial measurement unit (IMU) model on public hand-gesture datasets. We then designed a data augmentation and synthesis process to train additional classification layers for customization. Our offline evaluation with 26 participants showed that with three, five, and ten examples, our approach achieved an average accuracy of 76.8%, 84.7%, and 87.7%, and an F1 score of 74.8%, 84.2%, and 87.2% We then conducted a four-hour intervention study to compare WatchGuardian against a rule-based intervention. Our results demonstrated that our system led to a significant reduction by 64.0 +- 22.6% in undesirable actions, substantially outperforming the baseline by 29.0%. Our findings underscore the effectiveness of a customizable, AI-driven JITI system for individuals in need of behavioral intervention in personal undesirable actions. We envision that our work can inspire broader applications of user-defined personalized intervention with advanced AI solutions.

### RECOVER: Designing a Large Language Model-based Remote Patient Monitoring System for Postoperative Gastrointestinal Cancer Care 
[[arxiv](https://arxiv.org/abs/2502.05740)] [[cool](https://papers.cool/arxiv/2502.05740)] [[pdf](https://arxiv.org/pdf/2502.05740)]
> **Authors**: Ziqi Yang,Yuxuan Lu,Jennifer Bagdasarian,Vedant Das Swain,Ritu Agarwal,Collin Campbell,Waddah Al-Refaire,Jehan El-Bayoumi,Guodong Gao,Dakuo Wang,Bingsheng Yao,Nawar Shara
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 人机交互,人工智能
- **Abstract**: Cancer surgery is a key treatment for gastrointestinal (GI) cancers, a group of cancers that account for more than 35% of cancer-related deaths worldwide, but postoperative complications are unpredictable and can be life-threatening. In this paper, we investigate how recent advancements in large language models (LLMs) can benefit remote patient monitoring (RPM) systems through clinical integration by designing RECOVER, an LLM-powered RPM system for postoperative GI cancer care. To closely engage stakeholders in the design process, we first conducted seven participatory design sessions with five clinical staff and interviewed five cancer patients to derive six major design strategies for integrating clinical guidelines and information needs into LLM-based RPM systems. We then designed and implemented RECOVER, which features an LLM-powered conversational agent for cancer patients and an interactive dashboard for clinical staff to enable efficient postoperative RPM. Finally, we used RECOVER as a pilot system to assess the implementation of our design strategies with four clinical staff and five patients, providing design implications by identifying crucial design elements, offering insights on responsible AI, and outlining opportunities for future LLM-powered RPM systems.

### "It Felt Like I Was Left in the Dark": Exploring Information Needs and Design Opportunities for Family Caregivers of Older Adult Patients in Critical Care Settings 
[[arxiv](https://arxiv.org/abs/2502.05115)] [[cool](https://papers.cool/arxiv/2502.05115)] [[pdf](https://arxiv.org/pdf/2502.05115)]
> **Authors**: Shihan Fu,Bingsheng Yao,Smit Desai,Yuqi Hu,Yuling Sun,Samantha Stonbraker,Yanjun Gao,Elizabeth M. Goldberg,Dakuo Wang
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 人机交互,人工智能
- **Abstract**: Older adult patients constitute a rapidly growing subgroup of Intensive Care Unit (ICU) patients. In these situations, their family caregivers are expected to represent the unconscious patients to access and interpret patients' medical information. However, caregivers currently have to rely on overloaded clinicians for information updates and typically lack the health literacy to understand complex medical information. Our project aims to explore the information needs of caregivers of ICU older adult patients, from which we can propose design opportunities to guide future AI systems. The project begins with formative interviews with 11 caregivers to identify their challenges in accessing and interpreting medical information; From these findings, we then synthesize design requirements and propose an AI system prototype to cope with caregivers' challenges. The system prototype has two key features: a timeline visualization to show the AI extracted and summarized older adult patients' key medical events; and an LLM-based chatbot to provide context-aware informational support. We conclude our paper by reporting on the follow-up user evaluation of the system and discussing future AI-based systems for ICU caregivers of older adults.

## 信息检索(cs.IR:Information Retrieval)

### RALLRec: Improving Retrieval Augmented Large Language Model Recommendation with Representation Learning 
[[arxiv](https://arxiv.org/abs/2502.06101)] [[cool](https://papers.cool/arxiv/2502.06101)] [[pdf](https://arxiv.org/pdf/2502.06101)]
> **Authors**: Jian Xu,Sichun Luo,Xiangyu Chen,Haoming Huang,Hanxu Hou,Linqi Song
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: Accepted by TheWebConf'25 (WWW'25) as a Short Paper
- **标题**: None
- **领域**: 信息检索,计算语言学
- **Abstract**: Large Language Models (LLMs) have been integrated into recommendation systems to enhance user behavior comprehension. The Retrieval Augmented Generation (RAG) technique is further incorporated into these systems to retrieve more relevant items and improve system performance. However, existing RAG methods rely primarily on textual semantics and often fail to incorporate the most relevant items, limiting the effectiveness of the systems. In this paper, we propose Representation learning for retrieval-Augmented Large Language model Recommendation (RALLRec). Specifically, we enhance textual semantics by prompting LLMs to generate more detailed item descriptions, followed by joint representation learning of textual and collaborative semantics, which are extracted by the LLM and recommendation models, respectively. Considering the potential time-varying characteristics of user interest, a simple yet effective reranking method is further introduced to capture the dynamics of user preference. We conducted extensive experiments on three real-world datasets, and the evaluation results validated the effectiveness of our method. Code is made public at https://github.com/JianXu95/RALLRec.

### Uni-Retrieval: A Multi-Style Retrieval Framework for STEM's Education 
[[arxiv](https://arxiv.org/abs/2502.05863)] [[cool](https://papers.cool/arxiv/2502.05863)] [[pdf](https://arxiv.org/pdf/2502.05863)]
> **Authors**: Yanhao Jia,Xinyi Wu,Hao Li,Qinglin Zhang,Yuxiao Hu,Shuai Zhao,Wenqi Fan
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,人工智能,多媒体
- **Abstract**: In AI-facilitated teaching, leveraging various query styles to interpret abstract text descriptions is crucial for ensuring high-quality teaching. However, current retrieval models primarily focus on natural text-image retrieval, making them insufficiently tailored to educational scenarios due to the ambiguities in the retrieval process. In this paper, we propose a diverse expression retrieval task tailored to educational scenarios, supporting retrieval based on multiple query styles and expressions. We introduce the STEM Education Retrieval Dataset (SER), which contains over 24,000 query pairs of different styles, and the Uni-Retrieval, an efficient and style-diversified retrieval vision-language model based on prompt tuning. Uni-Retrieval extracts query style features as prototypes and builds a continuously updated Prompt Bank containing prompt tokens for diverse queries. This bank can updated during test time to represent domain-specific knowledge for different subject retrieval scenarios. Our framework demonstrates scalability and robustness by dynamically retrieving prompt tokens based on prototype similarity, effectively facilitating learning for unknown queries. Experimental results indicate that Uni-Retrieval outperforms existing retrieval models in most retrieval tasks. This advancement provides a scalable and precise solution for diverse educational needs.

### Hypencoder: Hypernetworks for Information Retrieval 
[[arxiv](https://arxiv.org/abs/2502.05364)] [[cool](https://papers.cool/arxiv/2502.05364)] [[pdf](https://arxiv.org/pdf/2502.05364)]
> **Authors**: Julian Killingback,Hansi Zeng,Hamed Zamani
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,机器学习
- **Abstract**: The vast majority of retrieval models depend on vector inner products to produce a relevance score between a query and a document. This naturally limits the expressiveness of the relevance score that can be employed. We propose a new paradigm, instead of producing a vector to represent the query we produce a small neural network which acts as a learned relevance function. This small neural network takes in a representation of the document, in this paper we use a single vector, and produces a scalar relevance score. To produce the little neural network we use a hypernetwork, a network that produce the weights of other networks, as our query encoder or as we call it a Hypencoder. Experiments on in-domain search tasks show that Hypencoder is able to significantly outperform strong dense retrieval models and has higher metrics then reranking models and models an order of magnitude larger. Hypencoder is also shown to generalize well to out-of-domain search tasks. To assess the extent of Hypencoder's capabilities, we evaluate on a set of hard retrieval tasks including tip-of-the-tongue retrieval and instruction-following retrieval tasks and find that the performance gap widens substantially compared to standard retrieval tasks. Furthermore, to demonstrate the practicality of our method we implement an approximate search algorithm and show that our model is able to search 8.8M documents in under 60ms.

### Holistically Guided Monte Carlo Tree Search for Intricate Information Seeking 
[[arxiv](https://arxiv.org/abs/2502.04751)] [[cool](https://papers.cool/arxiv/2502.04751)] [[pdf](https://arxiv.org/pdf/2502.04751)]
> **Authors**: Ruiyang Ren,Yuhao Wang,Junyi Li,Jinhao Jiang,Wayne Xin Zhao,Wenjie Wang,Tat-Seng Chua
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,计算语言学
- **Abstract**: In the era of vast digital information, the sheer volume and heterogeneity of available information present significant challenges for intricate information seeking. Users frequently face multistep web search tasks that involve navigating vast and varied data sources. This complexity demands every step remains comprehensive, accurate, and relevant. However, traditional search methods often struggle to balance the need for localized precision with the broader context required for holistic understanding, leaving critical facets of intricate queries underexplored. In this paper, we introduce an LLM-based search assistant that adopts a new information seeking paradigm with holistically guided Monte Carlo tree search (HG-MCTS). We reformulate the task as a progressive information collection process with a knowledge memory and unite an adaptive checklist with multi-perspective reward modeling in MCTS. The adaptive checklist provides explicit sub-goals to guide the MCTS process toward comprehensive coverage of complex user queries. Simultaneously, our multi-perspective reward modeling offers both exploration and retrieval rewards, along with progress feedback that tracks completed and remaining sub-goals, refining the checklist as the tree search progresses. By striking a balance between localized tree expansion and global guidance, HG-MCTS reduces redundancy in search paths and ensures that all crucial aspects of an intricate query are properly addressed. Extensive experiments on real-world intricate information seeking tasks demonstrate that HG-MCTS acquires thorough knowledge collections and delivers more accurate final responses compared with existing baselines.

## 信息论(cs.IT:Information Theory)

### Mixing Time of the Proximal Sampler in Relative Fisher Information via Strong Data Processing Inequality 
[[arxiv](https://arxiv.org/abs/2502.05623)] [[cool](https://papers.cool/arxiv/2502.05623)] [[pdf](https://arxiv.org/pdf/2502.05623)]
> **Authors**: Andre Wibisono
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 信息论,机器学习,优化与控制,统计理论
- **Abstract**: We study the mixing time guarantee for sampling in relative Fisher information via the Proximal Sampler algorithm, which is an approximate proximal discretization of the Langevin dynamics. We show that when the target probability distribution is strongly log-concave, the relative Fisher information converges exponentially fast along the Proximal Sampler; this matches the exponential convergence rate of the relative Fisher information along the continuous-time Langevin dynamics for strongly log-concave target. When combined with a standard implementation of the Proximal Sampler via rejection sampling, this exponential convergence rate provides a high-accuracy iteration complexity guarantee for the Proximal Sampler in relative Fisher information when the target distribution is strongly log-concave and log-smooth. Our proof proceeds by establishing a strong data processing inequality for relative Fisher information along the Gaussian channel under strong log-concavity, and a data processing inequality along the reverse Gaussian channel for a special distribution. The forward and reverse Gaussian channels compose to form the Proximal Sampler, and these data processing inequalities imply the exponential convergence rate of the relative Fisher information along the Proximal Sampler.

## 机器学习(cs.LG:Machine Learning)

### Low Tensor-Rank Adaptation of Kolmogorov--Arnold Networks 
[[arxiv](https://arxiv.org/abs/2502.06153)] [[cool](https://papers.cool/arxiv/2502.06153)] [[pdf](https://arxiv.org/pdf/2502.06153)]
> **Authors**: Yihang Gao,Michael K. Ng,Vincent Y. F. Tan
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Kolmogorov--Arnold networks (KANs) have demonstrated their potential as an alternative to multi-layer perceptions (MLPs) in various domains, especially for science-related tasks. However, transfer learning of KANs remains a relatively unexplored area. In this paper, inspired by Tucker decomposition of tensors and evidence on the low tensor-rank structure in KAN parameter updates, we develop low tensor-rank adaptation (LoTRA) for fine-tuning KANs. We study the expressiveness of LoTRA based on Tucker decomposition approximations. Furthermore, we provide a theoretical analysis to select the learning rates for each LoTRA component to enable efficient training. Our analysis also shows that using identical learning rates across all components leads to inefficient training, highlighting the need for an adaptive learning rate strategy. Beyond theoretical insights, we explore the application of LoTRA for efficiently solving various partial differential equations (PDEs) by fine-tuning KANs. Additionally, we propose Slim KANs that incorporate the inherent low-tensor-rank properties of KAN parameter tensors to reduce model size while maintaining superior performance. Experimental results validate the efficacy of the proposed learning rate selection strategy and demonstrate the effectiveness of LoTRA for transfer learning of KANs in solving PDEs. Further evaluations on Slim KANs for function representation and image classification tasks highlight the expressiveness of LoTRA and the potential for parameter reduction through low tensor-rank decomposition.

### Powerformer: A Transformer with Weighted Causal Attention for Time-series Forecasting 
[[arxiv](https://arxiv.org/abs/2502.06151)] [[cool](https://papers.cool/arxiv/2502.06151)] [[pdf](https://arxiv.org/pdf/2502.06151)]
> **Authors**: Kareem Hegazy,Michael W. Mahoney,N. Benjamin Erichson
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Transformers have recently shown strong performance in time-series forecasting, but their all-to-all attention mechanism overlooks the (temporal) causal and often (temporally) local nature of data. We introduce Powerformer, a novel Transformer variant that replaces noncausal attention weights with causal weights that are reweighted according to a smooth heavy-tailed decay. This simple yet effective modification endows the model with an inductive bias favoring temporally local dependencies, while still allowing sufficient flexibility to learn the unique correlation structure of each dataset. Our empirical results demonstrate that Powerformer not only achieves state-of-the-art accuracy on public time-series benchmarks, but also that it offers improved interpretability of attention patterns. Our analyses show that the model's locality bias is amplified during training, demonstrating an interplay between time-series data and power-law-based attention. These findings highlight the importance of domain-specific modifications to the Transformer architecture for time-series forecasting, and they establish Powerformer as a strong, efficient, and principled baseline for future research and real-world applications.

### Guided Exploration for Efficient Relational Model Learning 
[[arxiv](https://arxiv.org/abs/2502.06146)] [[cool](https://papers.cool/arxiv/2502.06146)] [[pdf](https://arxiv.org/pdf/2502.06146)]
> **Authors**: Annie Feng,Nishanth Kumar,Tomas Lozano-Perez,Leslie Pack-Kaelbling
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Efficient exploration is critical for learning relational models in large-scale environments with complex, long-horizon tasks. Random exploration methods often collect redundant or irrelevant data, limiting their ability to learn accurate relational models of the environment. Goal-literal babbling (GLIB) improves upon random exploration by setting and planning to novel goals, but its reliance on random actions and random novel goal selection limits its scalability to larger domains. In this work, we identify the principles underlying efficient exploration in relational domains: (1) operator initialization with demonstrations that cover the distinct lifted effects necessary for planning and (2) refining preconditions to collect maximally informative transitions by selecting informative goal-action pairs and executing plans to them. To demonstrate these principles, we introduce Baking-Large, a challenging domain with extensive state-action spaces and long-horizon tasks. We evaluate methods using oracle-driven demonstrations for operator initialization and precondition-targeting guidance to efficiently gather critical transitions. Experiments show that both the oracle demonstrations and precondition-targeting oracle guidance significantly improve sample efficiency and generalization, paving the way for future methods to use these principles to efficiently learn accurate relational models in complex domains.

### Graph Neural Networks at a Fraction 
[[arxiv](https://arxiv.org/abs/2502.06136)] [[cool](https://papers.cool/arxiv/2502.06136)] [[pdf](https://arxiv.org/pdf/2502.06136)]
> **Authors**: Rucha Bhalchandra Joshi,Sagar Prakash Barad,Nidhi Tiwari,Subhankar Mishra
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: 12 pages, 2 figures, accepted at PAKKD 2025
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Graph Neural Networks (GNNs) have emerged as powerful tools for learning representations of graph-structured data. In addition to real-valued GNNs, quaternion GNNs also perform well on tasks on graph-structured data. With the aim of reducing the energy footprint, we reduce the model size while maintaining accuracy comparable to that of the original-sized GNNs. This paper introduces Quaternion Message Passing Neural Networks (QMPNNs), a framework that leverages quaternion space to compute node representations. Our approach offers a generalizable method for incorporating quaternion representations into GNN architectures at one-fourth of the original parameter count. Furthermore, we present a novel perspective on Graph Lottery Tickets, redefining their applicability within the context of GNNs and QMPNNs. We specifically aim to find the initialization lottery from the subnetwork of the GNNs that can achieve comparable performance to the original GNN upon training. Thereby reducing the trainable model parameters even further. To validate the effectiveness of our proposed QMPNN framework and LTH for both GNNs and QMPNNs, we evaluate their performance on real-world datasets across three fundamental graph-based tasks: node classification, link prediction, and graph classification.

### Graph Pseudotime Analysis and Neural Stochastic Differential Equations for Analyzing Retinal Degeneration Dynamics and Beyond 
[[arxiv](https://arxiv.org/abs/2502.06126)] [[cool](https://papers.cool/arxiv/2502.06126)] [[pdf](https://arxiv.org/pdf/2502.06126)]
> **Authors**: Dai Shi,Kuan Yan,Lequan Lin,Yue Zeng,Ting Zhang,Dmytro Matsypura,Mark C. Gillies,Ling Zhu,Junbin Gao
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Understanding disease progression at the molecular pathway level usually requires capturing both structural dependencies between pathways and the temporal dynamics of disease evolution. In this work, we solve the former challenge by developing a biologically informed graph-forming method to efficiently construct pathway graphs for subjects from our newly curated JR5558 mouse transcriptomics dataset. We then develop Graph-level Pseudotime Analysis (GPA) to infer graph-level trajectories that reveal how disease progresses at the population level, rather than in individual subjects. Based on the trajectories estimated by GPA, we identify the most sensitive pathways that drive disease stage transitions. In addition, we measure changes in pathway features using neural stochastic differential equations (SDEs), which enables us to formally define and compute pathway stability and disease bifurcation points (points of no return), two fundamental problems in disease progression research. We further extend our theory to the case when pathways can interact with each other, enabling a more comprehensive and multi-faceted characterization of disease phenotypes. The comprehensive experimental results demonstrate the effectiveness of our framework in reconstructing the dynamics of the pathway, identifying critical transitions, and providing novel insights into the mechanistic understanding of disease evolution.

### Foundation Model of Electronic Medical Records for Adaptive Risk Estimation 
[[arxiv](https://arxiv.org/abs/2502.06124)] [[cool](https://papers.cool/arxiv/2502.06124)] [[pdf](https://arxiv.org/pdf/2502.06124)]
> **Authors**: Pawel Renc,Michal K. Grzeszczyk,Nassim Oufattole,Deirdre Goode,Yugang Jia,Szymon Bieganski,Matthew B. A. McDermott,Jaroslaw Was,Anthony E. Samir,Jonathan W. Cunningham,David W. Bates,Arkadiusz Sitek
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: We developed the Enhanced Transformer for Health Outcome Simulation (ETHOS), an AI model that tokenizes patient health timelines (PHTs) from EHRs. ETHOS predicts future PHTs using transformer-based architectures. The Adaptive Risk Estimation System (ARES) employs ETHOS to compute dynamic and personalized risk probabilities for clinician-defined critical events. ARES incorporates a personalized explainability module that identifies key clinical factors influencing risk estimates for individual patients. ARES was evaluated on the MIMIC-IV v2.2 dataset in emergency department (ED) settings, benchmarking its performance against traditional early warning systems and machine learning models. We processed 299,721 unique patients from MIMIC-IV into 285,622 PHTs, with 60% including hospital admissions. The dataset contained over 357 million tokens. ETHOS outperformed benchmark models in predicting hospital admissions, ICU admissions, and prolonged hospital stays, achieving superior AUC scores. ETHOS-based risk estimates demonstrated robustness across demographic subgroups with strong model reliability, confirmed via calibration curves. The personalized explainability module provides insights into patient-specific factors contributing to risk. ARES, powered by ETHOS, advances predictive healthcare AI by providing dynamic, real-time, and personalized risk estimation with patient-specific explainability to enhance clinician trust. Its adaptability and superior accuracy position it as a transformative tool for clinical decision-making, potentially improving patient outcomes and resource allocation in emergency and inpatient settings. We release the full code at github.com/ipolharvard/ethos-ares to facilitate future research.

### Revisiting Dynamic Graph Clustering via Matrix Factorization 
[[arxiv](https://arxiv.org/abs/2502.06117)] [[cool](https://papers.cool/arxiv/2502.06117)] [[pdf](https://arxiv.org/pdf/2502.06117)]
> **Authors**: Dongyuan Li,Satoshi Kosugi,Ying Zhang,Manabu Okumura,Feng Xia,Renhe Jiang
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: Accepted by TheWebConf 2025 (Oral)
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Dynamic graph clustering aims to detect and track time-varying clusters in dynamic graphs, revealing the evolutionary mechanisms of complex real-world dynamic systems. Matrix factorization-based methods are promising approaches for this task; however, these methods often struggle with scalability and can be time-consuming when applied to large-scale dynamic graphs. Moreover, they tend to lack robustness and are vulnerable to real-world noisy data. To address these issues, we make three key contributions. First, to improve scalability, we propose temporal separated matrix factorization, where a single matrix is divided into multiple smaller matrices for independent factorization, resulting in faster computation. Second, to improve robustness, we introduce bi-clustering regularization, which jointly optimizes graph embedding and clustering, thereby filtering out noisy features from the graph embeddings. Third, to further enhance effectiveness and efficiency, we propose selective embedding updating, where we update only the embeddings of dynamic nodes while the embeddings of static nodes are fixed among different timestamps. Experimental results on six synthetic and five real-world benchmarks demonstrate the scalability, robustness and effectiveness of our proposed method. Source code is available at https://github.com/Clearloveyuan/DyG-MF.

### Circuit-tuning: A Mechanistic Approach for Identifying Parameter Redundancy and Fine-tuning Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.06106)] [[cool](https://papers.cool/arxiv/2502.06106)] [[pdf](https://arxiv.org/pdf/2502.06106)]
> **Authors**: Yueyan Li,Caixia Yuan,Xiaojie Wang
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: The study of mechanistic interpretability aims to reverse-engineer a model to explain its behaviors. While recent studies have focused on the static mechanism of a certain behavior, the training dynamics inside a model remain to be explored. In this work, we develop an interpretable method for fine-tuning and reveal the mechanism behind learning. We first propose the concept of node redundancy as an extension of intrinsic dimension and explain the idea behind circuit discovery from a fresh view. Based on the theory, we propose circuit-tuning, a two-stage algorithm that iteratively performs circuit discovery to mask out irrelevant edges and updates the remaining parameters responsible for a specific task. Experiments show that our method not only improves performance on a wide range of tasks but is also scalable while preserving general capabilities. We visualize and analyze the circuits before, during, and after fine-tuning, providing new insights into the self-organization mechanism of a neural network in the learning process.

### Fine-Tuning Federated Learning-Based Intrusion Detection Systems for Transportation IoT 
[[arxiv](https://arxiv.org/abs/2502.06099)] [[cool](https://papers.cool/arxiv/2502.06099)] [[pdf](https://arxiv.org/pdf/2502.06099)]
> **Authors**: Robert Akinie,Nana Kankam Brym Gyimah,Mansi Bhavsar,John Kelly
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: 7 pages, 4 figures. To be published in IEEE SouthEastCon 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The rapid advancement of machine learning (ML) and on-device computing has revolutionized various industries, including transportation, through the development of Connected and Autonomous Vehicles (CAVs) and Intelligent Transportation Systems (ITS). These technologies improve traffic management and vehicle safety, but also introduce significant security and privacy concerns, such as cyberattacks and data breaches. Traditional Intrusion Detection Systems (IDS) are increasingly inadequate in detecting modern threats, leading to the adoption of ML-based IDS solutions. Federated Learning (FL) has emerged as a promising method for enabling the decentralized training of IDS models on distributed edge devices without sharing sensitive data. However, deploying FL-based IDS in CAV networks poses unique challenges, including limited computational and memory resources on edge devices, competing demands from critical applications such as navigation and safety systems, and the need to scale across diverse hardware and connectivity conditions. To address these issues, we propose a hybrid server-edge FL framework that offloads pre-training to a central server while enabling lightweight fine-tuning on edge devices. This approach reduces memory usage by up to 42%, decreases training times by up to 75%, and achieves competitive IDS accuracy of up to 99.2%. Scalability analyses further demonstrates minimal performance degradation as the number of clients increase, highlighting the framework's feasibility for CAV networks and other IoT applications.

### On the Computability of Multiclass PAC Learning 
[[arxiv](https://arxiv.org/abs/2502.06089)] [[cool](https://papers.cool/arxiv/2502.06089)] [[pdf](https://arxiv.org/pdf/2502.06089)]
> **Authors**: Pascale Gourdeau,Tosca Lechner,Ruth Urner
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: We study the problem of computable multiclass learnability within the Probably Approximately Correct (PAC) learning framework of Valiant (1984). In the recently introduced computable PAC (CPAC) learning framework of Agarwal et al. (2020), both learners and the functions they output are required to be computable. We focus on the case of finite label space and start by proposing a computable version of the Natarajan dimension and showing that it characterizes CPAC learnability in this setting. We further generalize this result by establishing a meta-characterization of CPAC learnability for a certain family of dimensions: computable distinguishers. Distinguishers were defined by Ben-David et al. (1992) as a certain family of embeddings of the label space, with each embedding giving rise to a dimension. It was shown that the finiteness of each such dimension characterizes multiclass PAC learnability for finite label space in the non-computable setting. We show that the corresponding computable dimensions for distinguishers characterize CPAC learning. We conclude our analysis by proving that the DS dimension, which characterizes PAC learnability for infinite label space, cannot be expressed as a distinguisher (even in the case of finite label space).

### Physics-Guided Foundation Model for Scientific Discovery: An Application to Aquatic Science 
[[arxiv](https://arxiv.org/abs/2502.06084)] [[cool](https://papers.cool/arxiv/2502.06084)] [[pdf](https://arxiv.org/pdf/2502.06084)]
> **Authors**: Runlong Yu,Chonghao Qiu,Robert Ladwig,Paul Hanson,Yiqun Xie,Xiaowei Jia
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,神经和进化计算
- **Abstract**: Physics-guided machine learning (PGML) has become a prevalent approach in studying scientific systems due to its ability to integrate scientific theories for enhancing machine learning (ML) models. However, most PGML approaches are tailored to isolated and relatively simple tasks, which limits their applicability to complex systems involving multiple interacting processes and numerous influencing features. In this paper, we propose a \textit{\textbf{P}hysics-\textbf{G}uided \textbf{F}oundation \textbf{M}odel (\textbf{PGFM})} that combines pre-trained ML models and physics-based models and leverages their complementary strengths to improve the modeling of multiple coupled processes. To effectively conduct pre-training, we construct a simulated environmental system that encompasses a wide range of influencing features and various simulated variables generated by physics-based models. The model is pre-trained in this system to adaptively select important feature interactions guided by multi-task objectives. We then fine-tune the model for each specific task using true observations, while maintaining consistency with established physical theories, such as the principles of mass and energy conservation. We demonstrate the effectiveness of this methodology in modeling water temperature and dissolved oxygen dynamics in real-world lakes. The proposed PGFM is also broadly applicable to a range of scientific fields where physics-based models are being used.

### Debiasing Guidance for Discrete Diffusion with Sequential Monte Carlo 
[[arxiv](https://arxiv.org/abs/2502.06079)] [[cool](https://papers.cool/arxiv/2502.06079)] [[pdf](https://arxiv.org/pdf/2502.06079)]
> **Authors**: Cheuk Kit Lee,Paul Jeha,Jes Frellsen,Pietro Lio,Michael Samuel Albergo,Francisco Vargas
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: 29 pages, 14 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Discrete diffusion models are a class of generative models that produce samples from an approximated data distribution within a discrete state space. Often, there is a need to target specific regions of the data distribution. Current guidance methods aim to sample from a distribution with mass proportional to $p_0(x_0) p(ζ|x_0)^α$ but fail to achieve this in practice. We introduce a Sequential Monte Carlo algorithm that generates unbiasedly from this target distribution, utilising the learnt unconditional and guided process. We validate our approach on low-dimensional distributions, controlled images and text generations. For text generation, our method provides strong control while maintaining low perplexity compared to guidance-based approaches.

### A Planning Framework for Adaptive Labeling 
[[arxiv](https://arxiv.org/abs/2502.06076)] [[cool](https://papers.cool/arxiv/2502.06076)] [[pdf](https://arxiv.org/pdf/2502.06076)]
> **Authors**: Daksh Mittal,Yuanzhe Ma,Shalmali Joshi,Hongseok Namkoong
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: A conference version of this work appeared at 2024 Conference onNeuralInformation Processing Systems, titled "Adaptive Labeling for Efficient Out-of-distributionModelEvaluation''
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Ground truth labels/outcomes are critical for advancing scientific and engineering applications, e.g., evaluating the treatment effect of an intervention or performance of a predictive model. Since randomly sampling inputs for labeling can be prohibitively expensive, we introduce an adaptive labeling framework where measurement effort can be reallocated in batches. We formulate this problem as a Markov decision process where posterior beliefs evolve over time as batches of labels are collected (state transition), and batches (actions) are chosen to minimize uncertainty at the end of data collection. We design a computational framework that is agnostic to different uncertainty quantification approaches including those based on deep learning, and allows a diverse array of policy gradient approaches by relying on continuous policy parameterizations. On real and synthetic datasets, we demonstrate even a one-step lookahead policy can substantially outperform common adaptive labeling heuristics, highlighting the virtue of planning. On the methodological side, we note that standard REINFORCE-style policy gradient estimators can suffer high variance since they rely only on zeroth order information. We propose a direct backpropagation-based approach, Smoothed-Autodiff, based on a carefully smoothed version of the original non-differentiable MDP. Our method enjoys low variance at the price of introducing bias, and we theoretically and empirically show that this trade-off can be favorable.

### ID policy (with reassignment) is asymptotically optimal for heterogeneous weakly-coupled MDPs 
[[arxiv](https://arxiv.org/abs/2502.06072)] [[cool](https://papers.cool/arxiv/2502.06072)] [[pdf](https://arxiv.org/pdf/2502.06072)]
> **Authors**: Xiangcheng Zhang,Yige Hong,Weina Wang
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: 37 pages
- **标题**: None
- **领域**: 机器学习,优化与控制,可能性
- **Abstract**: Heterogeneity poses a fundamental challenge for many real-world large-scale decision-making problems but remains largely understudied. In this paper, we study the fully heterogeneous setting of a prominent class of such problems, known as weakly-coupled Markov decision processes (WCMDPs). Each WCMDP consists of $N$ arms (or subproblems), which have distinct model parameters in the fully heterogeneous setting, leading to the curse of dimensionality when $N$ is large. We show that, under mild assumptions, a natural adaptation of the ID policy, although originally proposed for a homogeneous special case of WCMDPs, in fact achieves an $O(1/\sqrt{N})$ optimality gap in long-run average reward per arm for fully heterogeneous WCMDPs as $N$ becomes large. This is the first asymptotic optimality result for fully heterogeneous average-reward WCMDPs. Our techniques highlight the construction of a novel projection-based Lyapunov function, which witnesses the convergence of rewards and costs to an optimal region in the presence of heterogeneity.

### Online Reward-Weighted Fine-Tuning of Flow Matching with Wasserstein Regularization 
[[arxiv](https://arxiv.org/abs/2502.06061)] [[cool](https://papers.cool/arxiv/2502.06061)] [[pdf](https://arxiv.org/pdf/2502.06061)]
> **Authors**: Jiajun Fan,Shuaike Shen,Chaoran Cheng,Yuxin Chen,Chumeng Liang,Ge Liu
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: 61 pages
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别,机器学习
- **Abstract**: Recent advancements in reinforcement learning (RL) have achieved great success in fine-tuning diffusion-based generative models. However, fine-tuning continuous flow-based generative models to align with arbitrary user-defined reward functions remains challenging, particularly due to issues such as policy collapse from overoptimization and the prohibitively high computational cost of likelihoods in continuous-time flows. In this paper, we propose an easy-to-use and theoretically sound RL fine-tuning method, which we term Online Reward-Weighted Conditional Flow Matching with Wasserstein-2 Regularization (ORW-CFM-W2). Our method integrates RL into the flow matching framework to fine-tune generative models with arbitrary reward functions, without relying on gradients of rewards or filtered datasets. By introducing an online reward-weighting mechanism, our approach guides the model to prioritize high-reward regions in the data manifold. To prevent policy collapse and maintain diversity, we incorporate Wasserstein-2 (W2) distance regularization into our method and derive a tractable upper bound for it in flow matching, effectively balancing exploration and exploitation of policy optimization. We provide theoretical analyses to demonstrate the convergence properties and induced data distributions of our method, establishing connections with traditional RL algorithms featuring Kullback-Leibler (KL) regularization and offering a more comprehensive understanding of the underlying mechanisms and learning behavior of our approach. Extensive experiments on tasks including target image generation, image compression, and text-image alignment demonstrate the effectiveness of our method, where our method achieves optimal policy convergence while allowing controllable trade-offs between reward maximization and diversity preservation.

### Nearly Optimal Sample Complexity of Offline KL-Regularized Contextual Bandits under Single-Policy Concentrability 
[[arxiv](https://arxiv.org/abs/2502.06051)] [[cool](https://papers.cool/arxiv/2502.06051)] [[pdf](https://arxiv.org/pdf/2502.06051)]
> **Authors**: Qingyue Zhao,Kaixuan Ji,Heyang Zhao,Tong Zhang,Quanquan Gu
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: 23 pages
- **标题**: None
- **领域**: 机器学习,人工智能,统计理论,机器学习
- **Abstract**: KL-regularized policy optimization has become a workhorse in learning-based decision making, while its theoretical understanding is still very limited. Although recent progress has been made towards settling the sample complexity of KL-regularized contextual bandits, existing sample complexity bounds are either $\tilde{O}(ε^{-2})$ under single-policy concentrability or $\tilde{O}(ε^{-1})$ under all-policy concentrability. In this paper, we propose the \emph{first} algorithm with $\tilde{O}(ε^{-1})$ sample complexity under single-policy concentrability for offline contextual bandits. Our algorithm is designed for general function approximation and based on the principle of \emph{pessimism in the face of uncertainty}. The core of our proof leverages the strong convexity of the KL regularization, and the conditional non-negativity of the gap between the true reward and its pessimistic estimator to refine a mean-value-type risk upper bound to its extreme. This in turn leads to a novel covariance-based analysis, effectively bypassing the need for uniform control over the discrepancy between any two functions in the function class. The near-optimality of our algorithm is demonstrated by an $\tildeΩ(ε^{-1})$ lower bound. Furthermore, we extend our algorithm to contextual dueling bandits and achieve a similar nearly optimal sample complexity.

### Neural Shortest Path for Surface Reconstruction from Point Clouds 
[[arxiv](https://arxiv.org/abs/2502.06047)] [[cool](https://papers.cool/arxiv/2502.06047)] [[pdf](https://arxiv.org/pdf/2502.06047)]
> **Authors**: Yesom Park,Imseong Park,Jooyoung Hahn,Myungjoo Kang
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: In this paper, we propose the neural shortest path (NSP), a vector-valued implicit neural representation (INR) that approximates a distance function and its gradient. The key feature of NSP is to learn the exact shortest path (ESP), which directs an arbitrary point to its nearest point on the target surface. The NSP is decomposed into its magnitude and direction, and a variable splitting method is used that each decomposed component approximates a distance function and its gradient, respectively. Unlike to existing methods of learning the distance function itself, the NSP ensures the simultaneous recovery of the distance function and its gradient. We mathematically prove that the decomposed representation of NSP guarantees the convergence of the magnitude of NSP in the $H^1$ norm. Furthermore, we devise a novel loss function that enforces the property of ESP, demonstrating that its global minimum is the ESP. We evaluate the performance of the NSP through comprehensive experiments on diverse datasets, validating its capacity to reconstruct high-quality surfaces with the robustness to noise and data sparsity. The numerical results show substantial improvements over state-of-the-art methods, highlighting the importance of learning the ESP, the product of distance function and its gradient, for representing a wide variety of complex surfaces.

### Scaling Laws for Forgetting during Finetuning with Pretraining Data Injection 
[[arxiv](https://arxiv.org/abs/2502.06042)] [[cool](https://papers.cool/arxiv/2502.06042)] [[pdf](https://arxiv.org/pdf/2502.06042)]
> **Authors**: Louis Bethune,David Grangier,Dan Busbridge,Eleonora Gualdoni,Marco Cuturi,Pierre Ablin
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: 19 pages, 15 figures, preprint
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: A widespread strategy to obtain a language model that performs well on a target domain is to finetune a pretrained model to perform unsupervised next-token prediction on data from that target domain. Finetuning presents two challenges: (i) if the amount of target data is limited, as in most practical applications, the model will quickly overfit, and (ii) the model will drift away from the original model, forgetting the pretraining data and the generic knowledge that comes with it. We aim to derive scaling laws that quantify these two phenomena for various target domains, amounts of available target data, and model scales. We measure the efficiency of injecting pretraining data into the finetuning data mixture to avoid forgetting and mitigate overfitting. A key practical takeaway from our study is that injecting as little as 1% of pretraining data in the finetuning data mixture prevents the model from forgetting the pretraining set.

### Provably Overwhelming Transformer Models with Designed Inputs 
[[arxiv](https://arxiv.org/abs/2502.06038)] [[cool](https://papers.cool/arxiv/2502.06038)] [[pdf](https://arxiv.org/pdf/2502.06038)]
> **Authors**: Lev Stambler,Seyed Sajjad Nezhadi,Matthew Coudron
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算复杂度
- **Abstract**: We develop an algorithm which, given a trained transformer model $\mathcal{M}$ as input, as well as a string of tokens $s$ of length $n_{fix}$ and an integer $n_{free}$, can generate a mathematical proof that $\mathcal{M}$ is ``overwhelmed'' by $s$, in time and space $\widetilde{O}(n_{fix}^2 + n_{free}^3)$. We say that $\mathcal{M}$ is ``overwhelmed'' by $s$ when the output of the model evaluated on this string plus any additional string $t$, $\mathcal{M}(s + t)$, is completely insensitive to the value of the string $t$ whenever length($t$) $\leq n_{free}$. Along the way, we prove a particularly strong worst-case form of ``over-squashing'', which we use to bound the model's behavior. Our technique uses computer-aided proofs to establish this type of operationally relevant guarantee about transformer models. We empirically test our algorithm on a single layer transformer complete with an attention head, layer-norm, MLP/ReLU layers, and RoPE positional encoding. We believe that this work is a stepping stone towards the difficult task of obtaining useful guarantees for trained transformer models.

### Investigating Compositional Reasoning in Time Series Foundation Models 
[[arxiv](https://arxiv.org/abs/2502.06037)] [[cool](https://papers.cool/arxiv/2502.06037)] [[pdf](https://arxiv.org/pdf/2502.06037)]
> **Authors**: Willa Potosnak,Cristian Challu,Mononito Goswami,Kin G. Olivares,Michał Wiliński,Nina Żukowska,Artur Dubrawski
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Large pre-trained time series foundation models (TSFMs) have demonstrated promising zero-shot performance across a wide range of domains. However, a question remains: Do TSFMs succeed solely by memorizing training patterns, or do they possess the ability to reason? While reasoning is a topic of great interest in the study of Large Language Models (LLMs), it is undefined and largely unexplored in the context of TSFMs. In this work, inspired by language modeling literature, we formally define compositional reasoning in forecasting and distinguish it from in-distribution generalization. We evaluate the reasoning and generalization capabilities of 23 popular deep learning forecasting models on multiple synthetic and real-world datasets. Additionally, through controlled studies, we systematically examine which design choices in TSFMs contribute to improved reasoning abilities. Our study yields key insights into the impact of TSFM architecture design on compositional reasoning and generalization. We find that patch-based Transformers have the best reasoning performance, closely followed by residualized MLP-based architectures, which are 97\% less computationally complex in terms of FLOPs and 86\% smaller in terms of the number of trainable parameters. Interestingly, in some zero-shot out-of-distribution scenarios, these models can outperform moving average and exponential smoothing statistical baselines trained on in-distribution data. Only a few design choices, such as the tokenization method, had a significant (negative) impact on Transformer model performance.

### Generating 3D Binding Molecules Using Shape-Conditioned Diffusion Models with Guidance 
[[arxiv](https://arxiv.org/abs/2502.06027)] [[cool](https://papers.cool/arxiv/2502.06027)] [[pdf](https://arxiv.org/pdf/2502.06027)]
> **Authors**: Ziqi Chen,Bo Peng,Tianhua Zhai,Daniel Adu-Ampratwum,Xia Ning
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: This paper has been accepted by NatureMachineIntelligence
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Drug development is a critical but notoriously resource- and time-consuming process. In this manuscript, we develop a novel generative artificial intelligence (genAI) method DiffSMol to facilitate drug development. DiffSmol generates 3D binding molecules based on the shapes of known ligands. DiffSMol encapsulates geometric details of ligand shapes within pre-trained, expressive shape embeddings and then generates new binding molecules through a diffusion model. DiffSMol further modifies the generated 3D structures iteratively via shape guidance to better resemble the ligand shapes. It also tailors the generated molecules toward optimal binding affinities under the guidance of protein pockets. Here, we show that DiffSMol outperforms the state-of-the-art methods on benchmark datasets. When generating binding molecules resembling ligand shapes, DiffSMol with shape guidance achieves a success rate 61.4%, substantially outperforming the best baseline (11.2%), meanwhile producing molecules with novel molecular graph structures. DiffSMol with pocket guidance also outperforms the best baseline in binding affinities by 13.2%, and even by 17.7% when combined with shape guidance. Case studies for two critical drug targets demonstrate very favorable physicochemical and pharmacokinetic properties of the generated molecules, thus, the potential of DiffSMol in developing promising drug candidates.

### A Multimodal PDE Foundation Model for Prediction and Scientific Text Descriptions 
[[arxiv](https://arxiv.org/abs/2502.06026)] [[cool](https://papers.cool/arxiv/2502.06026)] [[pdf](https://arxiv.org/pdf/2502.06026)]
> **Authors**: Elisa Negrini,Yuxuan Liu,Liu Yang,Stanley J. Osher,Hayden Schaeffer
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,数值分析
- **Abstract**: Neural networks are one tool for approximating non-linear differential equations used in scientific computing tasks such as surrogate modeling, real-time predictions, and optimal control. PDE foundation models utilize neural networks to train approximations to multiple differential equations simultaneously and are thus a general purpose solver that can be adapted to downstream tasks. Current PDE foundation models focus on either learning general solution operators and/or the governing system of equations, and thus only handle numerical or symbolic modalities. However, real-world applications may require more flexible data modalities, e.g. text analysis or descriptive outputs. To address this gap, we propose a novel multimodal deep learning approach that leverages a transformer-based architecture to approximate solution operators for a wide variety of ODEs and PDEs. Our method integrates numerical inputs, such as equation parameters and initial conditions, with text descriptions of physical processes or system dynamics. This enables our model to handle settings where symbolic representations may be incomplete or unavailable. In addition to providing accurate numerical predictions, our approach generates interpretable scientific text descriptions, offering deeper insights into the underlying dynamics and solution properties. The numerical experiments show that our model provides accurate solutions for in-distribution data (with average relative error less than 3.3%) and out-of-distribution data (average relative error less than 7.8%) together with precise text descriptions (with correct descriptions generated 100% of times). In certain tests, the model is also shown to be capable of extrapolating solutions in time.

### Kolmogorov-Arnold Fourier Networks 
[[arxiv](https://arxiv.org/abs/2502.06018)] [[cool](https://papers.cool/arxiv/2502.06018)] [[pdf](https://arxiv.org/pdf/2502.06018)]
> **Authors**: Jusheng Zhang,Yijia Fan,Kaitong Cai,Keze Wang
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Although Kolmogorov-Arnold based interpretable networks (KAN) have strong theoretical expressiveness, they face significant parameter explosion and high-frequency feature capture challenges in high-dimensional tasks. To address this issue, we propose the Kolmogorov-Arnold-Fourier Network (KAF), which effectively integrates trainable Random Fourier Features (RFF) and a novel hybrid GELU-Fourier activation mechanism to balance parameter efficiency and spectral representation capabilities. Our key technical contributions include: (1) merging KAN's dual-matrix structure through matrix association properties to substantially reduce parameters; (2) introducing learnable RFF initialization strategies to eliminate spectral distortion in high-dimensional approximation tasks; (3) implementing an adaptive hybrid activation function that progressively enhances frequency representation during the training process. Comprehensive experiments demonstrate the superiority of our KAF across various domains including vision, NLP, audio processing, and differential equation-solving tasks, effectively combining theoretical interpretability with practical utility and computational efficiency.

### Decision Making in Hybrid Environments: A Model Aggregation Approach 
[[arxiv](https://arxiv.org/abs/2502.05974)] [[cool](https://papers.cool/arxiv/2502.05974)] [[pdf](https://arxiv.org/pdf/2502.05974)]
> **Authors**: Haolin Liu,Chen-Yu Wei,Julian Zimmert
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Recent work by Foster et al. (2021, 2022, 2023) and Xu and Zeevi (2023) developed the framework of decision estimation coefficient (DEC) that characterizes the complexity of general online decision making problems and provides a general algorithm design principle. These works, however, either focus on the pure stochastic regime where the world remains fixed over time, or the pure adversarial regime where the world arbitrarily changes over time. For the hybrid regime where the dynamics of the world is fixed while the reward arbitrarily changes, they only give pessimistic bounds on the decision complexity. In this work, we propose a general extension of DEC that more precisely characterizes this case. Besides applications in special cases, our framework leads to a flexible algorithm design where the learner learns over subsets of the hypothesis set, trading estimation complexity with decision complexity, which could be of independent interest. Our work covers model-based learning and model-free learning in the hybrid regime, with a newly proposed extension of the bilinear classes (Du et al., 2021) to the adversarial-reward case. We also recover some existing model-free learning results in the pure stochastic regime.

### Known Unknowns: Out-of-Distribution Property Prediction in Materials and Molecules 
[[arxiv](https://arxiv.org/abs/2502.05970)] [[cool](https://papers.cool/arxiv/2502.05970)] [[pdf](https://arxiv.org/pdf/2502.05970)]
> **Authors**: Nofit Segal,Aviv Netanyahu,Kevin P. Greenman,Pulkit Agrawal,Rafael Gomez-Bombarelli
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: 10 Pages, 5 figures, supporting information
- **标题**: None
- **领域**: 机器学习,材料科学,计算工程、金融和科学,化学物理
- **Abstract**: Discovery of high-performance materials and molecules requires identifying extremes with property values that fall outside the known distribution. Therefore, the ability to extrapolate to out-of-distribution (OOD) property values is critical for both solid-state materials and molecular design. Our objective is to train predictor models that extrapolate zero-shot to higher ranges than in the training data, given the chemical compositions of solids or molecular graphs and their property values. We propose using a transductive approach to OOD property prediction, achieving improvements in prediction accuracy. In particular, the True Positive Rate (TPR) of OOD classification of materials and molecules improved by 3x and 2.5x, respectively, and precision improved by 2x and 1.5x compared to non-transductive baselines. Our method leverages analogical input-target relations in the training and test sets, enabling generalization beyond the training target support, and can be applied to any other material and molecular tasks.

### nit Scaling: Simple and Scalable FP8 LLM Training 
[[arxiv](https://arxiv.org/abs/2502.05967)] [[cool](https://papers.cool/arxiv/2502.05967)] [[pdf](https://arxiv.org/pdf/2502.05967)]
> **Authors**: Saaketh Narayan,Abhay Gupta,Mansheej Paul,Davis Blalock
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Large Language Model training with 8-bit floating point (FP8) formats promises significant efficiency improvements, but reduced numerical precision makes training challenging. It is currently possible to train in FP8 only if one is willing to tune various hyperparameters, reduce model scale, or accept the overhead of computing dynamic scale factors. We demonstrate simple, scalable FP8 training that requires no dynamic scaling factors or special hyperparameters, even at large model sizes. Our method, $μ$nit Scaling ($μ$S), also enables simple hyperparameter transfer across model widths, matched numerics across training and inference, and other desirable properties. $μ$nit Scaling is straightforward to implement, consisting of a set of minimal interventions based on a first-principles analysis of common transformer operations. We validate our method by training models from 1B to 13B parameters, performing all hidden linear layer computations in FP8. We achieve quality equal to higher precision baselines while also training up to 33% faster.

### Redefining Robot Generalization Through Interactive Intelligence 
[[arxiv](https://arxiv.org/abs/2502.05963)] [[cool](https://papers.cool/arxiv/2502.05963)] [[pdf](https://arxiv.org/pdf/2502.05963)]
> **Authors**: Sharmita Dey
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器人技术
- **Abstract**: Recent advances in large-scale machine learning have produced high-capacity foundation models capable of adapting to a broad array of downstream tasks. While such models hold great promise for robotics, the prevailing paradigm still portrays robots as single, autonomous decision-makers, performing tasks like manipulation and navigation, with limited human involvement. However, a large class of real-world robotic systems, including wearable robotics (e.g., prostheses, orthoses, exoskeletons), teleoperation, and neural interfaces, are semiautonomous, and require ongoing interactive coordination with human partners, challenging single-agent assumptions. In this position paper, we argue that robot foundation models must evolve to an interactive multi-agent perspective in order to handle the complexities of real-time human-robot co-adaptation. We propose a generalizable, neuroscience-inspired architecture encompassing four modules: (1) a multimodal sensing module informed by sensorimotor integration principles, (2) an ad-hoc teamwork model reminiscent of joint-action frameworks in cognitive science, (3) a predictive world belief model grounded in internal model theories of motor control, and (4) a memory/feedback mechanism that echoes concepts of Hebbian and reinforcement-based plasticity. Although illustrated through the lens of cyborg systems, where wearable devices and human physiology are inseparably intertwined, the proposed framework is broadly applicable to robots operating in semi-autonomous or interactive contexts. By moving beyond single-agent designs, our position emphasizes how foundation models in robotics can achieve a more robust, personalized, and anticipatory level of performance.

### Survival Concept-Based Learning Models 
[[arxiv](https://arxiv.org/abs/2502.05950)] [[cool](https://papers.cool/arxiv/2502.05950)] [[pdf](https://arxiv.org/pdf/2502.05950)]
> **Authors**: Stanislav R. Kirpichenko,Lev V. Utkin,Andrei V. Konstantinov,Natalya M. Verbova
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Concept-based learning enhances prediction accuracy and interpretability by leveraging high-level, human-understandable concepts. However, existing CBL frameworks do not address survival analysis tasks, which involve predicting event times in the presence of censored data -- a common scenario in fields like medicine and reliability analysis. To bridge this gap, we propose two novel models: SurvCBM (Survival Concept-based Bottleneck Model) and SurvRCM (Survival Regularized Concept-based Model), which integrate concept-based learning with survival analysis to handle censored event time data. The models employ the Cox proportional hazards model and the Beran estimator. SurvCBM is based on the architecture of the well-known concept bottleneck model, offering interpretable predictions through concept-based explanations. SurvRCM uses concepts as regularization to enhance accuracy. Both models are trained end-to-end and provide interpretable predictions in terms of concepts. Two interpretability approaches are proposed: one leveraging the linear relationship in the Cox model and another using an instance-based explanation framework with the Beran estimator. Numerical experiments demonstrate that SurvCBM outperforms SurvRCM and traditional survival models, underscoring the importance and advantages of incorporating concept information. The code for the proposed algorithms is publicly available.

### Skill Expansion and Composition in Parameter Space 
[[arxiv](https://arxiv.org/abs/2502.05932)] [[cool](https://papers.cool/arxiv/2502.05932)] [[pdf](https://arxiv.org/pdf/2502.05932)]
> **Authors**: Tenglong Liu,Jianxiong Li,Yinan Zheng,Haoyi Niu,Yixing Lan,Xin Xu,Xianyuan Zhan
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: ICLR 2025, 37 pages
- **标题**: None
- **领域**: 机器学习,人工智能,机器人技术
- **Abstract**: Humans excel at reusing prior knowledge to address new challenges and developing skills while solving problems. This paradigm becomes increasingly popular in the development of autonomous agents, as it develops systems that can self-evolve in response to new challenges like human beings. However, previous methods suffer from limited training efficiency when expanding new skills and fail to fully leverage prior knowledge to facilitate new task learning. In this paper, we propose Parametric Skill Expansion and Composition (PSEC), a new framework designed to iteratively evolve the agents' capabilities and efficiently address new challenges by maintaining a manageable skill library. This library can progressively integrate skill primitives as plug-and-play Low-Rank Adaptation (LoRA) modules in parameter-efficient finetuning, facilitating efficient and flexible skill expansion. This structure also enables the direct skill compositions in parameter space by merging LoRA modules that encode different skills, leveraging shared information across skills to effectively program new skills. Based on this, we propose a context-aware module to dynamically activate different skills to collaboratively handle new tasks. Empowering diverse applications including multi-objective composition, dynamics shift, and continual policy shift, the results on D4RL, DSRL benchmarks, and the DeepMind Control Suite show that PSEC exhibits superior capacity to leverage prior knowledge to efficiently tackle new challenges, as well as expand its skill libraries to evolve the capabilities. Project website: https://ltlhuuu.github.io/PSEC/.

### Protecting Intellectual Property of EEG-based Neural Networks with Watermarking 
[[arxiv](https://arxiv.org/abs/2502.05931)] [[cool](https://papers.cool/arxiv/2502.05931)] [[pdf](https://arxiv.org/pdf/2502.05931)]
> **Authors**: Ahmed Abdelaziz,Ahmed Fathi,Ahmed Fares
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: 21 pages, 13 figures, and 6 tables
- **标题**: None
- **领域**: 机器学习,人工智能,密码学和安全
- **Abstract**: EEG-based neural networks, pivotal in medical diagnosis and brain-computer interfaces, face significant intellectual property (IP) risks due to their reliance on sensitive neurophysiological data and resource-intensive development. Current watermarking methods, particularly those using abstract trigger sets, lack robust authentication and fail to address the unique challenges of EEG models. This paper introduces a cryptographic wonder filter-based watermarking framework tailored for EEG-based neural networks. Leveraging collision-resistant hashing and public-key encryption, the wonder filter embeds the watermark during training, ensuring minimal distortion ($\leq 5\%$ drop in EEG task accuracy) and high reliability (100\% watermark detection). The framework is rigorously evaluated against adversarial attacks, including fine-tuning, transfer learning, and neuron pruning. Results demonstrate persistent watermark retention, with classification accuracy for watermarked states remaining above 90\% even after aggressive pruning, while primary task performance degrades faster, deterring removal attempts. Piracy resistance is validated by the inability to embed secondary watermarks without severe accuracy loss ( $>10\%$ in EEGNet and CCNN models). Cryptographic hashing ensures authentication, reducing brute-force attack success probabilities. Evaluated on the DEAP dataset across models (CCNN, EEGNet, TSception), the method achieves $>99.4\%$ null-embedding accuracy, effectively eliminating false positives. By integrating wonder filters with EEG-specific adaptations, this work bridges a critical gap in IP protection for neurophysiological models, offering a secure, tamper-proof solution for healthcare and biometric applications. The framework's robustness against adversarial modifications underscores its potential to safeguard sensitive EEG models while maintaining diagnostic utility.

### Sign-Symmetry Learning Rules are Robust Fine-Tuners 
[[arxiv](https://arxiv.org/abs/2502.05925)] [[cool](https://papers.cool/arxiv/2502.05925)] [[pdf](https://arxiv.org/pdf/2502.05925)]
> **Authors**: Aymene Berriche,Mehdi Zakaria Adjal,Riyadh Baghdadi
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Backpropagation (BP) has long been the predominant method for training neural networks due to its effectiveness. However, numerous alternative approaches, broadly categorized under feedback alignment, have been proposed, many of which are motivated by the search for biologically plausible learning mechanisms. Despite their theoretical appeal, these methods have consistently underperformed compared to BP, leading to a decline in research interest. In this work, we revisit the role of such methods and explore how they can be integrated into standard neural network training pipelines. Specifically, we propose fine-tuning BP-pre-trained models using Sign-Symmetry learning rules and demonstrate that this approach not only maintains performance parity with BP but also enhances robustness. Through extensive experiments across multiple tasks and benchmarks, we establish the validity of our approach. Our findings introduce a novel perspective on neural network training and open new research directions for leveraging biologically inspired learning rules in deep learning.

### NeuralPrefix: A Zero-shot Sensory Data Imputation Plugin 
[[arxiv](https://arxiv.org/abs/2502.05883)] [[cool](https://papers.cool/arxiv/2502.05883)] [[pdf](https://arxiv.org/pdf/2502.05883)]
> **Authors**: Abdelwahed Khamis,Sara Khalifa
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: Accepted in PerCom 25
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Real-world sensing challenges such as sensor failures, communication issues, and power constraints lead to data intermittency. An issue that is known to undermine the traditional classification task that assumes a continuous data stream. Previous works addressed this issue by designing bespoke solutions (i.e. task-specific and/or modality-specific imputation). These approaches, while effective for their intended purposes, had limitations in their applicability across different tasks and sensor modalities. This raises an important question: Can we build a task-agnostic imputation pipeline that is transferable to new sensors without requiring additional training? In this work, we formalise the concept of zero-shot imputation and propose a novel approach that enables the adaptation of pre-trained models to handle data intermittency. This framework, named NeuralPrefix, is a generative neural component that precedes a task model during inference, filling in gaps caused by data intermittency. NeuralPrefix is built as a continuous dynamical system, where its internal state can be estimated at any point in time by solving an Ordinary Differential Equation (ODE). This approach allows for a more versatile and adaptable imputation method, overcoming the limitations of task-specific and modality-specific solutions. We conduct a comprehensive evaluation of NeuralPrefix on multiple sensory datasets, demonstrating its effectiveness across various domains. When tested on intermittent data with a high 50% missing data rate, NeuralPreifx accurately recovers all the missing samples, achieving SSIM score between 0.93-0.96. Zero-shot evaluations show that NeuralPrefix generalises well to unseen datasets, even when the measurements come from a different modality.

### Norm Augmented Graph AutoEncoders for Link Prediction 
[[arxiv](https://arxiv.org/abs/2502.05868)] [[cool](https://papers.cool/arxiv/2502.05868)] [[pdf](https://arxiv.org/pdf/2502.05868)]
> **Authors**: Yunhui Liu,Huaisong Zhang,Xinyi Gao,Liuye Guo,Zhen Tao,Tieke He
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: Accepted by ICASSP 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Link Prediction (LP) is a crucial problem in graph-structured data. Graph Neural Networks (GNNs) have gained prominence in LP, with Graph AutoEncoders (GAEs) being a notable representation. However, our empirical findings reveal that GAEs' LP performance suffers heavily from the long-tailed node degree distribution, i.e., low-degree nodes tend to exhibit inferior LP performance compared to high-degree nodes. \emph{What causes this degree-related bias, and how can it be mitigated?} In this study, we demonstrate that the norm of node embeddings learned by GAEs exhibits variation among nodes with different degrees, underscoring its central significance in influencing the final performance of LP. Specifically, embeddings with larger norms tend to guide the decoder towards predicting higher scores for positive links and lower scores for negative links, thereby contributing to superior performance. This observation motivates us to improve GAEs' LP performance on low-degree nodes by increasing their embedding norms, which can be implemented simply yet effectively by introducing additional self-loops into the training objective for low-degree nodes. This norm augmentation strategy can be seamlessly integrated into existing GAE methods with light computational cost. Extensive experiments on various datasets and GAE methods show the superior performance of norm-augmented GAEs.

### Learning Accurate, Efficient, and Interpretable MLPs on Multiplex Graphs via Node-wise Multi-View Ensemble Distillation 
[[arxiv](https://arxiv.org/abs/2502.05864)] [[cool](https://papers.cool/arxiv/2502.05864)] [[pdf](https://arxiv.org/pdf/2502.05864)]
> **Authors**: Yunhui Liu,Zhen Tao,Xiang Zhao,Jianhua Zhao,Tao Zheng,Tieke He
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: Accepted by DASFAA 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Multiplex graphs, with multiple edge types (graph views) among common nodes, provide richer structural semantics and better modeling capabilities. Multiplex Graph Neural Networks (MGNNs), typically comprising view-specific GNNs and a multi-view integration layer, have achieved advanced performance in various downstream tasks. However, their reliance on neighborhood aggregation poses challenges for deployment in latency-sensitive applications. Motivated by recent GNN-to-MLP knowledge distillation frameworks, we propose Multiplex Graph-Free Neural Networks (MGFNN and MGFNN+) to combine MGNNs' superior performance and MLPs' efficient inference via knowledge distillation. MGFNN directly trains student MLPs with node features as input and soft labels from teacher MGNNs as targets. MGFNN+ further employs a low-rank approximation-based reparameterization to learn node-wise coefficients, enabling adaptive knowledge ensemble from each view-specific GNN. This node-wise multi-view ensemble distillation strategy allows student MLPs to learn more informative multiplex semantic knowledge for different nodes. Experiments show that MGFNNs achieve average accuracy improvements of about 10% over vanilla MLPs and perform comparably or even better to teacher MGNNs (accurate); MGFNNs achieve a 35.40$\times$-89.14$\times$ speedup in inference over MGNNs (efficient); MGFNN+ adaptively assigns different coefficients for multi-view ensemble distillation regarding different nodes (interpretable).

### Compressing Model with Few Class-Imbalance Samples: An Out-of-Distribution Expedition 
[[arxiv](https://arxiv.org/abs/2502.05832)] [[cool](https://papers.cool/arxiv/2502.05832)] [[pdf](https://arxiv.org/pdf/2502.05832)]
> **Authors**: Tian-Shuang Wu,Shen-Huan Lyu,Ning Chen,Zhihao Qu,Baoliu Ye
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: In recent years, as a compromise between privacy and performance, few-sample model compression has been widely adopted to deal with limited data resulting from privacy and security concerns. However, when the number of available samples is extremely limited, class imbalance becomes a common and tricky problem. Achieving an equal number of samples across all classes is often costly and impractical in real-world applications, and previous studies on few-sample model compression have mostly ignored this significant issue. Our experiments comprehensively demonstrate that class imbalance negatively affects the overall performance of few-sample model compression methods. To address this problem, we propose a novel and adaptive framework named OOD-Enhanced Few-Sample Model Compression (OE-FSMC). This framework integrates easily accessible out-of-distribution (OOD) data into both the compression and fine-tuning processes, effectively rebalancing the training distribution. We also incorporate a joint distillation loss and a regularization term to reduce the risk of the model overfitting to the OOD data. Extensive experiments on multiple benchmark datasets show that our framework can be seamlessly incorporated into existing few-sample model compression methods, effectively mitigating the accuracy degradation caused by class imbalance.

### Devil is in the Details: Density Guidance for Detail-Aware Generation with Flow Models 
[[arxiv](https://arxiv.org/abs/2502.05807)] [[cool](https://papers.cool/arxiv/2502.05807)] [[pdf](https://arxiv.org/pdf/2502.05807)]
> **Authors**: Rafał Karczewski,Markus Heinonen,Vikas Garg
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: 27 pages, 15 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Diffusion models have emerged as a powerful class of generative models, capable of producing high-quality images by mapping noise to a data distribution. However, recent findings suggest that image likelihood does not align with perceptual quality: high-likelihood samples tend to be smooth, while lower-likelihood ones are more detailed. Controlling sample density is thus crucial for balancing realism and detail. In this paper, we analyze an existing technique, Prior Guidance, which scales the latent code to influence image detail. We introduce score alignment, a condition that explains why this method works and show that it can be tractably checked for any continuous normalizing flow model. We then propose Density Guidance, a principled modification of the generative ODE that enables exact log-density control during sampling. Finally, we extend Density Guidance to stochastic sampling, ensuring precise log-density control while allowing controlled variation in structure or fine details. Our experiments demonstrate that these techniques provide fine-grained control over image detail without compromising sample quality.

### The Curse of Depth in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.05795)] [[cool](https://papers.cool/arxiv/2502.05795)] [[pdf](https://arxiv.org/pdf/2502.05795)]
> **Authors**: Wenfang Sun,Xinyuan Song,Pengxiang Li,Lu Yin,Yefeng Zheng,Shiwei Liu
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: In this paper, we introduce the Curse of Depth, a concept that highlights, explains, and addresses the recent observation in modern Large Language Models(LLMs) where nearly half of the layers are less effective than expected. We first confirm the wide existence of this phenomenon across the most popular families of LLMs such as Llama, Mistral, DeepSeek, and Qwen. Our analysis, theoretically and empirically, identifies that the underlying reason for the ineffectiveness of deep layers in LLMs is the widespread usage of Pre-Layer Normalization (Pre-LN). While Pre-LN stabilizes the training of Transformer LLMs, its output variance exponentially grows with the model depth, which undesirably causes the derivative of the deep Transformer blocks to be an identity matrix, and therefore barely contributes to the training. To resolve this training pitfall, we propose LayerNorm Scaling, which scales the variance of output of the layer normalization inversely by the square root of its depth. This simple modification mitigates the output variance explosion of deeper Transformer layers, improving their contribution. Our experimental results, spanning model sizes from 130M to 1B, demonstrate that LayerNorm Scaling significantly enhances LLM pre-training performance compared to Pre-LN. Moreover, this improvement seamlessly carries over to supervised fine-tuning. All these gains can be attributed to the fact that LayerNorm Scaling enables deeper layers to contribute more effectively during training.

### I3S: Importance Sampling Subspace Selection for Low-Rank Optimization in LLM Pretraining 
[[arxiv](https://arxiv.org/abs/2502.05790)] [[cool](https://papers.cool/arxiv/2502.05790)] [[pdf](https://arxiv.org/pdf/2502.05790)]
> **Authors**: Haochen Zhang,Junze Yin,Guanchu Wang,Zirui Liu,Tianyi Zhang,Anshumali Shrivastava,Lin Yang,Vladimir Braverman
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Low-rank optimization has emerged as a promising approach to enabling memory-efficient training of large language models (LLMs). Existing low-rank optimization methods typically project gradients onto a low-rank subspace, reducing the memory cost of storing optimizer states. A key challenge in these methods is identifying suitable subspaces to ensure an effective optimization trajectory. Most existing approaches select the dominant subspace to preserve gradient information, as this intuitively provides the best approximation. However, we find that in practice, the dominant subspace stops changing during pretraining, thereby constraining weight updates to similar subspaces. In this paper, we propose importance sampling subspace selection (I3S) for low-rank optimization, which theoretically offers a comparable convergence rate to the dominant subspace approach. Empirically, we demonstrate that I3S significantly outperforms previous methods in LLM pretraining tasks.

### GOLD: Graph Out-of-Distribution Detection via Implicit Adversarial Latent Generation 
[[arxiv](https://arxiv.org/abs/2502.05780)] [[cool](https://papers.cool/arxiv/2502.05780)] [[pdf](https://arxiv.org/pdf/2502.05780)]
> **Authors**: Danny Wang,Ruihong Qiu,Guangdong Bai,Zi Huang
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: ICLR25
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Despite graph neural networks' (GNNs) great success in modelling graph-structured data, out-of-distribution (OOD) test instances still pose a great challenge for current GNNs. One of the most effective techniques to detect OOD nodes is to expose the detector model with an additional OOD node-set, yet the extra OOD instances are often difficult to obtain in practice. Recent methods for image data address this problem using OOD data synthesis, typically relying on pre-trained generative models like Stable Diffusion. However, these approaches require vast amounts of additional data, as well as one-for-all pre-trained generative models, which are not available for graph data. Therefore, we propose the GOLD framework for graph OOD detection, an implicit adversarial learning pipeline with synthetic OOD exposure without pre-trained models. The implicit adversarial training process employs a novel alternating optimisation framework by training: (1) a latent generative model to regularly imitate the in-distribution (ID) embeddings from an evolving GNN, and (2) a GNN encoder and an OOD detector to accurately classify ID data while increasing the energy divergence between the ID embeddings and the generative model's synthetic embeddings. This novel approach implicitly transforms the synthetic embeddings into pseudo-OOD instances relative to the ID data, effectively simulating exposure to OOD scenarios without auxiliary data. Extensive OOD detection experiments are conducted on five benchmark graph datasets, verifying the superior performance of GOLD without using real OOD data compared with the state-of-the-art OOD exposure and non-exposure baselines.

### Predictive Crash Analytics for Traffic Safety using Deep Learning 
[[arxiv](https://arxiv.org/abs/2502.05777)] [[cool](https://papers.cool/arxiv/2502.05777)] [[pdf](https://arxiv.org/pdf/2502.05777)]
> **Authors**: Karthik Sivakoti
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Traditional automated crash analysis systems heavily rely on static statistical models and historical data, requiring significant manual interpretation and lacking real-time predictive capabilities. This research presents an innovative approach to traffic safety analysis through the integration of ensemble learning methods and multi-modal data fusion for real-time crash risk assessment and prediction. Our primary contribution lies in developing a hierarchical severity classification system that combines spatial-temporal crash patterns with environmental conditions, achieving significant improvements over traditional statistical approaches. The system demonstrates a Mean Average Precision (mAP) of 0.893, representing a 15% improvement over current state-of-the-art methods (baseline mAP: 0.776). We introduce a novel feature engineering technique that integrates crash location data with incident reports and weather conditions, achieving 92.4% accuracy in risk prediction and 89.7% precision in hotspot identification. Through extensive validation using 500,000 initial crash records filtered to 59,496 high-quality samples, our solution shows marked improvements in both prediction accuracy and computational efficiency. Key innovations include a robust data cleaning pipeline, adaptive feature generation, and a scalable real-time prediction system capable of handling peak loads of 1,000 concurrent requests while maintaining sub-100ms response times.

### PIPA: Preference Alignment as Prior-Informed Statistical Estimation 
[[arxiv](https://arxiv.org/abs/2502.05773)] [[cool](https://papers.cool/arxiv/2502.05773)] [[pdf](https://arxiv.org/pdf/2502.05773)]
> **Authors**: Junbo Li,Zhangyang Wang,Qiang Liu
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Offline preference alignment for language models such as Direct Preference Optimization (DPO) is favored for its effectiveness and simplicity, eliminating the need for costly reinforcement learning. Various offline algorithms have been developed for different data settings, yet they lack a unified understanding. In this study, we introduce Pior-Informed Preference Alignment (PIPA), a unified, RL-free probabilistic framework that formulates language model preference alignment as a Maximum Likelihood Estimation (MLE) problem with prior constraints. This method effectively accommodates both paired and unpaired data, as well as answer and step-level annotations. We illustrate that DPO and KTO are special cases with different prior constraints within our framework. By integrating different types of prior information, we developed two variations of PIPA: PIPA-M and PIPA-N. Both algorithms demonstrate a $3\sim10\%$ performance enhancement on the GSM8K and MATH benchmarks across all configurations, achieving these gains without additional training or computational costs compared to existing algorithms.

### Privacy-Preserving Dataset Combination 
[[arxiv](https://arxiv.org/abs/2502.05765)] [[cool](https://papers.cool/arxiv/2502.05765)] [[pdf](https://arxiv.org/pdf/2502.05765)]
> **Authors**: Keren Fuentes,Mimee Xu,Irene Chen
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: 12 pages, 5 figures
- **标题**: None
- **领域**: 机器学习,密码学和安全,计算机与社会
- **Abstract**: Access to diverse, high-quality datasets is crucial for machine learning model performance, yet data sharing remains limited by privacy concerns and competitive interests, particularly in regulated domains like healthcare. This dynamic especially disadvantages smaller organizations that lack resources to purchase data or negotiate favorable sharing agreements. We present SecureKL, a privacy-preserving framework that enables organizations to identify beneficial data partnerships without exposing sensitive information. Building on recent advances in dataset combination methods, we develop a secure multiparty computation protocol that maintains strong privacy guarantees while achieving >90\% correlation with plaintext evaluations. In experiments with real-world hospital data, SecureKL successfully identifies beneficial data partnerships that improve model performance for intensive care unit mortality prediction while preserving data privacy. Our framework provides a practical solution for organizations seeking to leverage collective data resources while maintaining privacy and competitive advantages. These results demonstrate the potential for privacy-preserving data collaboration to advance machine learning applications in high-stakes domains while promoting more equitable access to data resources.

### Filter, Obstruct and Dilute: Defending Against Backdoor Attacks on Semi-Supervised Learning 
[[arxiv](https://arxiv.org/abs/2502.05755)] [[cool](https://papers.cool/arxiv/2502.05755)] [[pdf](https://arxiv.org/pdf/2502.05755)]
> **Authors**: Xinrui Wang,Chuanxing Geng,Wenhai Wan,Shao-yuan Li,Songcan Chen
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Recent studies have verified that semi-supervised learning (SSL) is vulnerable to data poisoning backdoor attacks. Even a tiny fraction of contaminated training data is sufficient for adversaries to manipulate up to 90\% of the test outputs in existing SSL methods. Given the emerging threat of backdoor attacks designed for SSL, this work aims to protect SSL against such risks, marking it as one of the few known efforts in this area. Specifically, we begin by identifying that the spurious correlations between the backdoor triggers and the target class implanted by adversaries are the primary cause of manipulated model predictions during the test phase. To disrupt these correlations, we utilize three key techniques: Gaussian Filter, complementary learning and trigger mix-up, which collectively filter, obstruct and dilute the influence of backdoor attacks in both data pre-processing and feature learning. Experimental results demonstrate that our proposed method, Backdoor Invalidator (BI), significantly reduces the average attack success rate from 84.7\% to 1.8\% across different state-of-the-art backdoor attacks. It is also worth mentioning that BI does not sacrifice accuracy on clean data and is supported by a theoretical guarantee of its generalization capability.

### Understanding Representation Dynamics of Diffusion Models via Low-Dimensional Modeling 
[[arxiv](https://arxiv.org/abs/2502.05743)] [[cool](https://papers.cool/arxiv/2502.05743)] [[pdf](https://arxiv.org/pdf/2502.05743)]
> **Authors**: Xiao Li,Zekai Zhang,Xiang Li,Siyi Chen,Zhihui Zhu,Peng Wang,Qing Qu
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: First two authors contributed equally
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: This work addresses the critical question of why and when diffusion models, despite being designed for generative tasks, can excel at learning high-quality representations in a self-supervised manner. To address this, we develop a mathematical framework based on a low-dimensional data model and posterior estimation, revealing a fundamental trade-off between generation and representation quality near the final stage of image generation. Our analysis explains the unimodal representation dynamics across noise scales, mainly driven by the interplay between data denoising and class specification. Building on these insights, we propose an ensemble method that aggregates features across noise levels, significantly improving both clean performance and robustness under label noise. Extensive experiments on both synthetic and real-world datasets validate our findings.

### Impact of Data Poisoning Attacks on Feasibility and Optimality of Neural Power System Optimizers 
[[arxiv](https://arxiv.org/abs/2502.05727)] [[cool](https://papers.cool/arxiv/2502.05727)] [[pdf](https://arxiv.org/pdf/2502.05727)]
> **Authors**: Nora Agah,Meiyi Li,Javad Mohammadi
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: 6 pages, 4 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The increased integration of clean yet stochastic energy resources and the growing number of extreme weather events are narrowing the decision-making window of power grid operators. This time constraint is fueling a plethora of research on Machine Learning-, or ML-, based optimization proxies. While finding a fast solution is appealing, the inherent vulnerabilities of the learning-based methods are hindering their adoption. One of these vulnerabilities is data poisoning attacks, which adds perturbations to ML training data, leading to incorrect decisions. The impact of poisoning attacks on learning-based power system optimizers have not been thoroughly studied, which creates a critical vulnerability. In this paper, we examine the impact of data poisoning attacks on ML-based optimization proxies that are used to solve the DC Optimal Power Flow problem. Specifically, we compare the resilience of three different methods-a penalty-based method, a post-repair approach, and a direct mapping approach-against the adverse effects of poisoning attacks. We will use the optimality and feasibility of these proxies as performance metrics. The insights of this work will establish a foundation for enhancing the resilience of neural power system optimizers.

### Improving Environment Novelty Quantification for Effective Unsupervised Environment Design 
[[arxiv](https://arxiv.org/abs/2502.05726)] [[cool](https://papers.cool/arxiv/2502.05726)] [[pdf](https://arxiv.org/pdf/2502.05726)]
> **Authors**: Jayden Teoh,Wenjun Li,Pradeep Varakantham
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Unsupervised Environment Design (UED) formalizes the problem of autocurricula through interactive training between a teacher agent and a student agent. The teacher generates new training environments with high learning potential, curating an adaptive curriculum that strengthens the student's ability to handle unseen scenarios. Existing UED methods mainly rely on regret, a metric that measures the difference between the agent's optimal and actual performance, to guide curriculum design. Regret-driven methods generate curricula that progressively increase environment complexity for the student but overlook environment novelty -- a critical element for enhancing an agent's generalizability. Measuring environment novelty is especially challenging due to the underspecified nature of environment parameters in UED, and existing approaches face significant limitations. To address this, this paper introduces the Coverage-based Evaluation of Novelty In Environment (CENIE) framework. CENIE proposes a scalable, domain-agnostic, and curriculum-aware approach to quantifying environment novelty by leveraging the student's state-action space coverage from previous curriculum experiences. We then propose an implementation of CENIE that models this coverage and measures environment novelty using Gaussian Mixture Models. By integrating both regret and novelty as complementary objectives for curriculum design, CENIE facilitates effective exploration across the state-action space while progressively increasing curriculum complexity. Empirical evaluations demonstrate that augmenting existing regret-based UED algorithms with CENIE achieves state-of-the-art performance across multiple benchmarks, underscoring the effectiveness of novelty-driven autocurricula for robust generalization.

### Rethinking Link Prediction for Directed Graphs 
[[arxiv](https://arxiv.org/abs/2502.05724)] [[cool](https://papers.cool/arxiv/2502.05724)] [[pdf](https://arxiv.org/pdf/2502.05724)]
> **Authors**: Mingguo He,Yuhe Guo,Yanping Zheng,Zhewei Wei,Stephan Günnemann,Xiaokui Xiao
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: 30 pages
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Link prediction for directed graphs is a crucial task with diverse real-world applications. Recent advances in embedding methods and Graph Neural Networks (GNNs) have shown promising improvements. However, these methods often lack a thorough analysis of embedding expressiveness and suffer from ineffective benchmarks for a fair evaluation. In this paper, we propose a unified framework to assess the expressiveness of existing methods, highlighting the impact of dual embeddings and decoder design on performance. To address limitations in current experimental setups, we introduce DirLinkBench, a robust new benchmark with comprehensive coverage and standardized evaluation. The results show that current methods struggle to achieve strong performance on the new benchmark, while DiGAE outperforms others overall. We further revisit DiGAE theoretically, showing its graph convolution aligns with GCN on an undirected bipartite graph. Inspired by these insights, we propose a novel spectral directed graph auto-encoder SDGAE that achieves SOTA results on DirLinkBench. Finally, we analyze key factors influencing directed link prediction and highlight open challenges.

### Explainable and Class-Revealing Signal Feature Extraction via Scattering Transform and Constrained Zeroth-Order Optimization 
[[arxiv](https://arxiv.org/abs/2502.05722)] [[cool](https://papers.cool/arxiv/2502.05722)] [[pdf](https://arxiv.org/pdf/2502.05722)]
> **Authors**: Naoki Saito,David Weber
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: 5 pages; 6 figures; submitted to 2025 IEEE Statistical Signal Processing Workshop
- **标题**: None
- **领域**: 机器学习,信号处理,优化与控制,机器学习
- **Abstract**: We propose a new method to extract discriminant and explainable features from a particular machine learning model, i.e., a combination of the scattering transform and the multiclass logistic regression. Although this model is well-known for its ability to learn various signal classes with high classification rate, it remains elusive to understand why it can generate such successful classification, mainly due to the nonlinearity of the scattering transform. In order to uncover the meaning of the scattering transform coefficients selected by the multiclass logistic regression (with the Lasso penalty), we adopt zeroth-order optimization algorithms to search an input pattern that maximizes the class probability of a class of interest given the learned model. In order to do so, it turns out that imposing sparsity and smoothness of input patterns is important. We demonstrate the effectiveness of our proposed method using a couple of synthetic time-series classification problems.

### Extended Histogram-based Outlier Score (EHBOS) 
[[arxiv](https://arxiv.org/abs/2502.05719)] [[cool](https://papers.cool/arxiv/2502.05719)] [[pdf](https://arxiv.org/pdf/2502.05719)]
> **Authors**: Tanvir Islam
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Histogram-Based Outlier Score (HBOS) is a widely used outlier or anomaly detection method known for its computational efficiency and simplicity. However, its assumption of feature independence limits its ability to detect anomalies in datasets where interactions between features are critical. In this paper, we propose the Extended Histogram-Based Outlier Score (EHBOS), which enhances HBOS by incorporating two-dimensional histograms to capture dependencies between feature pairs. This extension allows EHBOS to identify contextual and dependency-driven anomalies that HBOS fails to detect. We evaluate EHBOS on 17 benchmark datasets, demonstrating its effectiveness and robustness across diverse anomaly detection scenarios. EHBOS outperforms HBOS on several datasets, particularly those where feature interactions are critical in defining the anomaly structure, achieving notable improvements in ROC AUC. These results highlight that EHBOS can be a valuable extension to HBOS, with the ability to model complex feature dependencies. EHBOS offers a powerful new tool for anomaly detection, particularly in datasets where contextual or relational anomalies play a significant role.

### Flow-based Conformal Prediction for Multi-dimensional Time Series 
[[arxiv](https://arxiv.org/abs/2502.05709)] [[cool](https://papers.cool/arxiv/2502.05709)] [[pdf](https://arxiv.org/pdf/2502.05709)]
> **Authors**: Junghwan Lee,Chen Xu,Yao Xie
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Conformal prediction for time series presents two key challenges: (1) leveraging sequential correlations in features and non-conformity scores and (2) handling multi-dimensional outcomes. We propose a novel conformal prediction method to address these two key challenges by integrating Transformer and Normalizing Flow. Specifically, the Transformer encodes the historical context of time series, and normalizing flow learns the transformation from the base distribution to the distribution of non-conformity scores conditioned on the encoded historical context. This enables the construction of prediction regions by transforming samples from the base distribution using the learned conditional flow. We ensure the marginal coverage by defining the prediction regions as sets in the transformed space that correspond to a predefined probability mass in the base distribution. The model is trained end-to-end by Flow Matching, avoiding the need for computationally intensive numerical solutions of ordinary differential equations. We demonstrate that our proposed method achieves smaller prediction regions compared to the baselines while satisfying the desired coverage through comprehensive experiments using simulated and real-world time series datasets.

### TOKON: TOKenization-Optimized Normalization for time series analysis with a large language model 
[[arxiv](https://arxiv.org/abs/2502.05701)] [[cool](https://papers.cool/arxiv/2502.05701)] [[pdf](https://arxiv.org/pdf/2502.05701)]
> **Authors**: Janghoon Yang
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: While large language models have rapidly evolved towards general artificial intelligence, their versatility in analyzing time series data remains limited. To address this limitation, we propose a novel normalization technique that considers the inherent nature of tokenization. The proposed Tokenization-Optimized Normalization (TOKON) simplifies time series data by representing each element with a single token, effectively reducing the number of tokens by 2 to 3 times. Additionally, we introduce a novel prompt for time series forecasting, termed Time Series Forecasting with Care (TFSC), to further enhance forecasting performance. Experimental results demonstrate that TOKON improves root mean square error (RMSE) for multi-step forecasting by approximately 7% to 18%, depending on the dataset and prompting method. Furthermore, TFSC, when used in conjunction with TOKON, shows additional improvements in forecasting accuracy for certain datasets

### Context information can be more important than reasoning for time series forecasting with a large language model 
[[arxiv](https://arxiv.org/abs/2502.05699)] [[cool](https://papers.cool/arxiv/2502.05699)] [[pdf](https://arxiv.org/pdf/2502.05699)]
> **Authors**: Janghoon Yang
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: With the evolution of large language models (LLMs), there is growing interest in leveraging LLMs for time series tasks. In this paper, we explore the characteristics of LLMs for time series forecasting by considering various existing and proposed prompting techniques. Forecasting for both short and long time series was evaluated. Our findings indicate that no single prompting method is universally applicable. It was also observed that simply providing proper context information related to the time series, without additional reasoning prompts, can achieve performance comparable to the best-performing prompt for each case. From this observation, it is expected that providing proper context information can be more crucial than a prompt for specific reasoning in time series forecasting. Several weaknesses in prompting for time series forecasting were also identified. First, LLMs often fail to follow the procedures described by the prompt. Second, when reasoning steps involve simple algebraic calculations with several operands, LLMs often fail to calculate accurately. Third, LLMs sometimes misunderstand the semantics of prompts, resulting in incomplete responses.

### Machine Unlearning via Information Theoretic Regularization 
[[arxiv](https://arxiv.org/abs/2502.05684)] [[cool](https://papers.cool/arxiv/2502.05684)] [[pdf](https://arxiv.org/pdf/2502.05684)]
> **Authors**: Shizhou Xu,Thomas Strohmer
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: 31 pages, 2 figures
- **标题**: None
- **领域**: 机器学习,人工智能,信息论,机器学习
- **Abstract**: How can we effectively remove or "unlearn" undesirable information, such as specific features or individual data points, from a learning outcome while minimizing utility loss and ensuring rigorous guarantees? We introduce a mathematical framework based on information-theoretic regularization to address both feature and data point unlearning. For feature unlearning, we derive a unified solution that simultaneously optimizes diverse learning objectives, including entropy, conditional entropy, KL-divergence, and the energy of conditional probability. For data point unlearning, we first propose a novel definition that serves as a practical condition for unlearning via retraining, is easy to verify, and aligns with the principles of differential privacy from an inference perspective. Then, we provide provable guarantees for our framework on data point unlearning. By combining flexibility in learning objectives with simplicity in regularization design, our approach is highly adaptable and practical for a wide range of machine learning and AI applications.

### Federated Learning with Reservoir State Analysis for Time Series Anomaly Detection 
[[arxiv](https://arxiv.org/abs/2502.05679)] [[cool](https://papers.cool/arxiv/2502.05679)] [[pdf](https://arxiv.org/pdf/2502.05679)]
> **Authors**: Keigo Nogami,Hiroto Tamura,Gouhei Tanaka
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: 8 pages, 16 figures, submitted to IJCNN 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: With a growing data privacy concern, federated learning has emerged as a promising framework to train machine learning models without sharing locally distributed data. In federated learning, local model training by multiple clients and model integration by a server are repeated only through model parameter sharing. Most existing federated learning methods assume training deep learning models, which are often computationally demanding. To deal with this issue, we propose federated learning methods with reservoir state analysis to seek computational efficiency and data privacy protection simultaneously. Specifically, our method relies on Mahalanobis Distance of Reservoir States (MD-RS) method targeting time series anomaly detection, which learns a distribution of reservoir states for normal inputs and detects anomalies based on a deviation from the learned distribution. Iterative updating of statistical parameters in the MD-RS enables incremental federated learning (IncFed MD-RS). We evaluate the performance of IncFed MD-RS using benchmark datasets for time series anomaly detection. The results show that IncFed MD-RS outperforms other federated learning methods with deep learning and reservoir computing models particularly when clients' data are relatively short and heterogeneous. We demonstrate that IncFed MD-RS is robust against reduced sample data compared to other methods. We also show that the computational cost of IncFed MD-RS can be reduced by subsampling from the reservoir states without performance degradation. The proposed method is beneficial especially in anomaly detection applications where computational efficiency, algorithm simplicity, and low communication cost are required.

### The late-stage training dynamics of (stochastic) subgradient descent on homogeneous neural networks 
[[arxiv](https://arxiv.org/abs/2502.05668)] [[cool](https://papers.cool/arxiv/2502.05668)] [[pdf](https://arxiv.org/pdf/2502.05668)]
> **Authors**: Sholom Schechtman,Nicolas Schreuder
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,神经和进化计算,优化与控制,机器学习
- **Abstract**: We analyze the implicit bias of constant step stochastic subgradient descent (SGD). We consider the setting of binary classification with homogeneous neural networks - a large class of deep neural networks with ReLU-type activation functions such as MLPs and CNNs without biases. We interpret the dynamics of normalized SGD iterates as an Euler-like discretization of a conservative field flow that is naturally associated to the normalized classification margin. Owing to this interpretation, we show that normalized SGD iterates converge to the set of critical points of the normalized margin at late-stage training (i.e., assuming that the data is correctly classified with positive normalized margin). Up to our knowledge, this is the first extension of the analysis of Lyu and Li (2020) on the discrete dynamics of gradient descent to the nonsmooth and stochastic setting. Our main result applies to binary classification with exponential or logistic losses. We additionally discuss extensions to more general settings.

### Flowing Through Layers: A Continuous Dynamical Systems Perspective on Transformers 
[[arxiv](https://arxiv.org/abs/2502.05656)] [[cool](https://papers.cool/arxiv/2502.05656)] [[pdf](https://arxiv.org/pdf/2502.05656)]
> **Authors**: Jacob Fein-Ashley
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,动力系统
- **Abstract**: We show that the standard discrete update rule of transformer layers can be naturally interpreted as a forward Euler discretization of a continuous dynamical system. Our Transformer Flow Approximation Theorem demonstrates that, under standard Lipschitz continuity assumptions, token representations converge uniformly to the unique solution of an ODE as the number of layers grows. Moreover, if the underlying mapping satisfies a one-sided Lipschitz condition with a negative constant, the resulting dynamics are contractive, causing perturbations to decay exponentially across layers. Beyond clarifying the empirical stability and expressivity of transformer models, these insights link transformer updates to a broader iterative reasoning framework, suggesting new avenues for accelerated convergence and architectural innovations inspired by dynamical systems theory.

### ETHEREAL: Energy-efficient and High-throughput Inference using Compressed Tsetlin Machine 
[[arxiv](https://arxiv.org/abs/2502.05640)] [[cool](https://papers.cool/arxiv/2502.05640)] [[pdf](https://arxiv.org/pdf/2502.05640)]
> **Authors**: Shengyu Duan,Rishad Shafik,Alex Yakovlev
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: Accepted as a full paper by the 2025 EDGEAIFOUNDATION Austin
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The Tsetlin Machine (TM) is a novel alternative to deep neural networks (DNNs). Unlike DNNs, which rely on multi-path arithmetic operations, a TM learns propositional logic patterns from data literals using Tsetlin automata. This fundamental shift from arithmetic to logic underpinning makes TM suitable for empowering new applications with low-cost implementations. In TM, literals are often included by both positive and negative clauses within the same class, canceling out their impact on individual class definitions. This property can be exploited to develop compressed TM models, enabling energy-efficient and high-throughput inferences for machine learning (ML) applications. We introduce a training approach that incorporates excluded automata states to sparsify TM logic patterns in both positive and negative clauses. This exclusion is iterative, ensuring that highly class-correlated (and therefore significant) literals are retained in the compressed inference model, ETHEREAL, to maintain strong classification accuracy. Compared to standard TMs, ETHEREAL TM models can reduce model size by up to 87.54%, with only a minor accuracy compromise. We validate the impact of this compression on eight real-world Tiny machine learning (TinyML) datasets against standard TM, equivalent Random Forest (RF) and Binarized Neural Network (BNN) on the STM32F746G-DISCO platform. Our results show that ETHEREAL TM models achieve over an order of magnitude reduction in inference time (resulting in higher throughput) and energy consumption compared to BNNs, while maintaining a significantly smaller memory footprint compared to RFs.

### Mol-MoE: Training Preference-Guided Routers for Molecule Generation 
[[arxiv](https://arxiv.org/abs/2502.05633)] [[cool](https://papers.cool/arxiv/2502.05633)] [[pdf](https://arxiv.org/pdf/2502.05633)]
> **Authors**: Diego Calanzone,Pierluca D'Oro,Pierre-Luc Bacon
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: We release our code and data at: https://github.com/ddidacus/mol-moe
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Recent advances in language models have enabled framing molecule generation as sequence modeling. However, existing approaches often rely on single-objective reinforcement learning, limiting their applicability to real-world drug design, where multiple competing properties must be optimized. Traditional multi-objective reinforcement learning (MORL) methods require costly retraining for each new objective combination, making rapid exploration of trade-offs impractical. To overcome these limitations, we introduce Mol-MoE, a mixture-of-experts (MoE) architecture that enables efficient test-time steering of molecule generation without retraining. Central to our approach is a preference-based router training objective that incentivizes the router to combine experts in a way that aligns with user-specified trade-offs. This provides improved flexibility in exploring the chemical property space at test time, facilitating rapid trade-off exploration. Benchmarking against state-of-the-art methods, we show that Mol-MoE achieves superior sample quality and steerability.

### TrackDiffuser: Nearly Model-Free Bayesian Filtering with Diffusion Model 
[[arxiv](https://arxiv.org/abs/2502.05629)] [[cool](https://papers.cool/arxiv/2502.05629)] [[pdf](https://arxiv.org/pdf/2502.05629)]
> **Authors**: Yangguang He,Wenhao Li,Minzhe Li,Juan Zhang,Xiangfeng Wang,Bo Jin
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,信号处理
- **Abstract**: State estimation remains a fundamental challenge across numerous domains, from autonomous driving, aircraft tracking to quantum system control. Although Bayesian filtering has been the cornerstone solution, its classical model-based paradigm faces two major limitations: it struggles with inaccurate state space model (SSM) and requires extensive prior knowledge of noise characteristics. We present TrackDiffuser, a generative framework addressing both challenges by reformulating Bayesian filtering as a conditional diffusion model. Our approach implicitly learns system dynamics from data to mitigate the effects of inaccurate SSM, while simultaneously circumventing the need for explicit measurement models and noise priors by establishing a direct relationship between measurements and states. Through an implicit predict-and-update mechanism, TrackDiffuser preserves the interpretability advantage of traditional model-based filtering methods. Extensive experiments demonstrate that our framework substantially outperforms both classical and contemporary hybrid methods, especially in challenging non-linear scenarios involving non-Gaussian noises. Notably, TrackDiffuser exhibits remarkable robustness to SSM inaccuracies, offering a practical solution for real-world state estimation problems where perfect models and prior knowledge are unavailable.

### Training-Free Constrained Generation With Stable Diffusion Models 
[[arxiv](https://arxiv.org/abs/2502.05625)] [[cool](https://papers.cool/arxiv/2502.05625)] [[pdf](https://arxiv.org/pdf/2502.05625)]
> **Authors**: Stefano Zampini,Jacob Christopher,Luca Oneto,Davide Anguita,Ferdinando Fioretto
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Stable diffusion models represent the state-of-the-art in data synthesis across diverse domains and hold transformative potential for applications in science and engineering, e.g., by facilitating the discovery of novel solutions and simulating systems that are computationally intractable to model explicitly. However, their current utility in these fields is severely limited by an inability to enforce strict adherence to physical laws and domain-specific constraints. Without this grounding, the deployment of such models in critical applications, ranging from material science to safety-critical systems, remains impractical. This paper addresses this fundamental limitation by proposing a novel approach to integrate stable diffusion models with constrained optimization frameworks, enabling them to generate outputs that satisfy stringent physical and functional requirements. We demonstrate the effectiveness of this approach through material science experiments requiring adherence to precise morphometric properties, inverse design problems involving the generation of stress-strain responses using video generation with a simulator in the loop, and safety settings where outputs must avoid copyright infringement.

### TabICL: A Tabular Foundation Model for In-Context Learning on Large Data 
[[arxiv](https://arxiv.org/abs/2502.05564)] [[cool](https://papers.cool/arxiv/2502.05564)] [[pdf](https://arxiv.org/pdf/2502.05564)]
> **Authors**: Jingang Qu,David Holzmüller,Gaël Varoquaux,Marine Le Morvan
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: The long-standing dominance of gradient-boosted decision trees on tabular data is currently challenged by tabular foundation models using In-Context Learning (ICL): setting the training data as context for the test data and predicting in a single forward pass without parameter updates. While the very recent TabPFNv2 foundation model (2025) excels on tables with up to 10K samples, its alternating column- and row-wise attentions make handling large training sets computationally prohibitive. So, can ICL be effectively scaled and deliver a benefit for larger tables? We introduce TabICL, a tabular foundation model for classification, pretrained on synthetic datasets with up to 60K samples and capable of handling 500K samples on affordable resources. This is enabled by a novel two-stage architecture: a column-then-row attention mechanism to build fixed-dimensional embeddings of rows, followed by a transformer for efficient ICL. Across 200 classification datasets from the TALENT benchmark, TabICL is on par with TabPFNv2 while being systematically faster (up to 10 times), and significantly outperforms all other approaches. On 56 datasets with over 10K samples, TabICL surpasses both TabPFNv2 and CatBoost, demonstrating the potential of ICL for large data.

### Democratic Training Against Universal Adversarial Perturbations 
[[arxiv](https://arxiv.org/abs/2502.05542)] [[cool](https://papers.cool/arxiv/2502.05542)] [[pdf](https://arxiv.org/pdf/2502.05542)]
> **Authors**: Bing Sun,Jun Sun,Wei Zhao
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Despite their advances and success, real-world deep neural networks are known to be vulnerable to adversarial attacks. Universal adversarial perturbation, an input-agnostic attack, poses a serious threat for them to be deployed in security-sensitive systems. In this case, a single universal adversarial perturbation deceives the model on a range of clean inputs without requiring input-specific optimization, which makes it particularly threatening. In this work, we observe that universal adversarial perturbations usually lead to abnormal entropy spectrum in hidden layers, which suggests that the prediction is dominated by a small number of ``feature'' in such cases (rather than democratically by many features). Inspired by this, we propose an efficient yet effective defense method for mitigating UAPs called \emph{Democratic Training} by performing entropy-based model enhancement to suppress the effect of the universal adversarial perturbations in a given model. \emph{Democratic Training} is evaluated with 7 neural networks trained on 5 benchmark datasets and 5 types of state-of-the-art universal adversarial attack methods. The results show that it effectively reduces the attack success rate, improves model robustness and preserves the model accuracy on clean samples.

### Do Spikes Protect Privacy? Investigating Black-Box Model Inversion Attacks in Spiking Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.05509)] [[cool](https://papers.cool/arxiv/2502.05509)] [[pdf](https://arxiv.org/pdf/2502.05509)]
> **Authors**: Hamed Poursiami,Ayana Moshruba,Maryam Parsa
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: 7 pages, 4 figures
- **标题**: None
- **领域**: 机器学习,密码学和安全,神经和进化计算
- **Abstract**: As machine learning models become integral to security-sensitive applications, concerns over data leakage from adversarial attacks continue to rise. Model Inversion (MI) attacks pose a significant privacy threat by enabling adversaries to reconstruct training data from model outputs. While MI attacks on Artificial Neural Networks (ANNs) have been widely studied, Spiking Neural Networks (SNNs) remain largely unexplored in this context. Due to their event-driven and discrete computations, SNNs introduce fundamental differences in information processing that may offer inherent resistance to such attacks. A critical yet underexplored aspect of this threat lies in black-box settings, where attackers operate through queries without direct access to model parameters or gradients-representing a more realistic adversarial scenario in deployed systems. This work presents the first study of black-box MI attacks on SNNs. We adapt a generative adversarial MI framework to the spiking domain by incorporating rate-based encoding for input transformation and decoding mechanisms for output interpretation. Our results show that SNNs exhibit significantly greater resistance to MI attacks than ANNs, as demonstrated by degraded reconstructions, increased instability in attack convergence, and overall reduced attack effectiveness across multiple evaluation metrics. Further analysis suggests that the discrete and temporally distributed nature of SNN decision boundaries disrupts surrogate modeling, limiting the attacker's ability to approximate the target model.

### Differentially Private Synthetic Data via APIs 3: Using Simulators Instead of Foundation Model 
[[arxiv](https://arxiv.org/abs/2502.05505)] [[cool](https://papers.cool/arxiv/2502.05505)] [[pdf](https://arxiv.org/pdf/2502.05505)]
> **Authors**: Zinan Lin,Tadas Baltrusaitis,Sergey Yekhanin
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,密码学和安全,计算机视觉和模式识别,机器学习
- **Abstract**: Differentially private (DP) synthetic data, which closely resembles the original private data while maintaining strong privacy guarantees, has become a key tool for unlocking the value of private data without compromising privacy. Recently, Private Evolution (PE) has emerged as a promising method for generating DP synthetic data. Unlike other training-based approaches, PE only requires access to inference APIs from foundation models, enabling it to harness the power of state-of-the-art models. However, a suitable foundation model for a specific private data domain is not always available. In this paper, we discover that the PE framework is sufficiently general to allow inference APIs beyond foundation models. Specifically, we show that simulators -- such as computer graphics-based image synthesis tools -- can also serve as effective APIs within the PE framework. This insight greatly expands the applicability of PE, enabling the use of a wide variety of domain-specific simulators for DP data synthesis. We explore the potential of this approach, named Sim-PE, in the context of image synthesis. Across three diverse simulators, Sim-PE performs well, improving the downstream classification accuracy of PE by up to 3x and reducing the FID score by up to 80%. We also show that simulators and foundation models can be easily leveraged together within the PE framework to achieve further improvements. The code is open-sourced in the Private Evolution Python library: https://github.com/microsoft/DPSDA.

### Riemannian Manifold Learning for Stackelberg Games with Neural Flow Representations 
[[arxiv](https://arxiv.org/abs/2502.05498)] [[cool](https://papers.cool/arxiv/2502.05498)] [[pdf](https://arxiv.org/pdf/2502.05498)]
> **Authors**: Larkin Liu,Kashif Rasul,Yutong Chao,Jalal Etesami
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: Stackelberg games. Manifoldlearning. Onlinelearning
- **标题**: None
- **领域**: 机器学习,人工智能,计算机科学与博弈论,多代理系统
- **Abstract**: We present a novel framework for online learning in Stackelberg general-sum games, where two agents, the leader and follower, engage in sequential turn-based interactions. At the core of this approach is a learned diffeomorphism that maps the joint action space to a smooth Riemannian manifold, referred to as the Stackelberg manifold. This mapping, facilitated by neural normalizing flows, ensures the formation of tractable isoplanar subspaces, enabling efficient techniques for online learning. By assuming linearity between the agents' reward functions on the Stackelberg manifold, our construct allows the application of standard bandit algorithms. We then provide a rigorous theoretical basis for regret minimization on convex manifolds and establish finite-time bounds on simple regret for learning Stackelberg equilibria. This integration of manifold learning into game theory uncovers a previously unrecognized potential for neural normalizing flows as an effective tool for multi-agent learning. We present empirical results demonstrating the effectiveness of our approach compared to standard baselines, with applications spanning domains such as cybersecurity and economic supply chain optimization.

### Feature Explosion: a generic optimization strategy for outlier detection algorithms 
[[arxiv](https://arxiv.org/abs/2502.05496)] [[cool](https://papers.cool/arxiv/2502.05496)] [[pdf](https://arxiv.org/pdf/2502.05496)]
> **Authors**: Qi Li
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Outlier detection tasks aim at discovering potential issues or opportunities and are widely used in cybersecurity, financial security, industrial inspection, etc. To date, thousands of outlier detection algorithms have been proposed. Clearly, in real-world scenarios, such a large number of algorithms is unnecessary. In other words, a large number of outlier detection algorithms are redundant. We believe the root cause of this redundancy lies in the current highly customized (i.e., non-generic) optimization strategies. Specifically, when researchers seek to improve the performance of existing outlier detection algorithms, they have to design separate optimized versions tailored to the principles of each algorithm, leading to an ever-growing number of outlier detection algorithms. To address this issue, in this paper, we introduce the explosion from physics into the outlier detection task and propose a generic optimization strategy based on feature explosion, called OSD (Optimization Strategy for outlier Detection algorithms). In the future, when improving the performance of existing outlier detection algorithms, it will be sufficient to invoke the OSD plugin without the need to design customized optimized versions for them. We compared the performances of 14 outlier detection algorithms on 24 datasets before and after invoking the OSD plugin. The experimental results show that the performances of all outlier detection algorithms are improved on almost all datasets. In terms of average accuracy, OSD make these outlier detection algorithms improve by 15% (AUC), 63.7% (AP).

### Multi-scale Masked Autoencoder for Electrocardiogram Anomaly Detection 
[[arxiv](https://arxiv.org/abs/2502.05494)] [[cool](https://papers.cool/arxiv/2502.05494)] [[pdf](https://arxiv.org/pdf/2502.05494)]
> **Authors**: Ya Zhou,Yujie Yang,Jianhuang Gan,Xiangjie Li,Jing Yuan,Wei Zhao
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: Under review in a journal
- **标题**: None
- **领域**: 机器学习,人工智能,应用领域
- **Abstract**: Electrocardiogram (ECG) analysis is a fundamental tool for diagnosing cardiovascular conditions, yet anomaly detection in ECG signals remains challenging due to their inherent complexity and variability. We propose Multi-scale Masked Autoencoder for ECG anomaly detection (MMAE-ECG), a novel end-to-end framework that effectively captures both global and local dependencies in ECG data. Unlike state-of-the-art methods that rely on heartbeat segmentation or R-peak detection, MMAE-ECG eliminates the need for such pre-processing steps, enhancing its suitability for clinical deployment. MMAE-ECG partitions ECG signals into non-overlapping segments, with each segment assigned learnable positional embeddings. A novel multi-scale masking strategy and multi-scale attention mechanism, along with distinct positional embeddings, enable a lightweight Transformer encoder to effectively capture both local and global dependencies. The masked segments are then reconstructed using a single-layer Transformer block, with an aggregation strategy employed during inference to refine the outputs. Experimental results demonstrate that our method achieves performance comparable to state-of-the-art approaches while significantly reducing computational complexity-approximately 1/78 of the floating-point operations (FLOPs) required for inference. Ablation studies further validate the effectiveness of each component, highlighting the potential of multi-scale masked autoencoders for anomaly detection.

### Modeling of Core Loss Based on Machine Learning and Deep Learning 
[[arxiv](https://arxiv.org/abs/2502.05487)] [[cool](https://papers.cool/arxiv/2502.05487)] [[pdf](https://arxiv.org/pdf/2502.05487)]
> **Authors**: Junqi He,Yifeng Wei,Daiguang Jin
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,信号处理
- **Abstract**: This article proposes a Mix Neural Network (MNN) based on CNN-FCNN for predicting magnetic loss of different materials. In traditional magnetic core loss models, empirical equations usually need to be regressed under the same external conditions. When the magnetic core material is different, it needs to be classified and discussed. If external factors increase, multiple models need to be proposed for classification and discussion, making the modeling process extremely cumbersome. And traditional empirical equations still has the problem of low accuracy, although various correction equations have been introduced later, the accuracy has always been unsatisfactory. By introducing machine learning and deep learning, it is possible to simultaneously solve prediction problems with low accuracy of empirical equations and complex conditions. Based on the MagNet database, through the training of the newly proposed MNN, it is found that a single model is sufficient to make predictions for at least four different materials under varying temperatures, frequencies, and waveforms, with accuracy far exceeding that of traditional models. At the same time, we also used three other machine learning and deep learning models (Random Forest, XGBoost, MLP-LSTM) for training, all of which had much higher accuracy than traditional models. On the basis of the predicted results, a hybrid model combining MNN and XGBoost was proposed, which predicted through weighting and found that the accuracy could continue to improve. This provides a solution for modeling magnetic core loss under different materials and operating modes.

### You Are What You Eat -- AI Alignment Requires Understanding How Data Shapes Structure and Generalisation 
[[arxiv](https://arxiv.org/abs/2502.05475)] [[cool](https://papers.cool/arxiv/2502.05475)] [[pdf](https://arxiv.org/pdf/2502.05475)]
> **Authors**: Simon Pepin Lehalleur,Jesse Hoogland,Matthew Farrugia-Roberts,Susan Wei,Alexander Gietelink Oldenziel,George Wang,Liam Carroll,Daniel Murfet
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: In this position paper, we argue that understanding the relation between structure in the data distribution and structure in trained models is central to AI alignment. First, we discuss how two neural networks can have equivalent performance on the training set but compute their outputs in essentially different ways and thus generalise differently. For this reason, standard testing and evaluation are insufficient for obtaining assurances of safety for widely deployed generally intelligent systems. We argue that to progress beyond evaluation to a robust mathematical science of AI alignment, we need to develop statistical foundations for an understanding of the relation between structure in the data distribution, internal structure in models, and how these structures underlie generalisation.

### Gen-DFL: Decision-Focused Generative Learning for Robust Decision Making 
[[arxiv](https://arxiv.org/abs/2502.05468)] [[cool](https://papers.cool/arxiv/2502.05468)] [[pdf](https://arxiv.org/pdf/2502.05468)]
> **Authors**: Prince Zizhuang Wang,Jinhao Liang,Shuyi Chen,Ferdinando Fioretto,Shixiang Zhu
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: 22 pages, 6 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Decision-focused learning (DFL) integrates predictive models with downstream optimization, directly training machine learning models to minimize decision errors. While DFL has been shown to provide substantial advantages when compared to a counterpart that treats the predictive and prescriptive models separately, it has also been shown to struggle in high-dimensional and risk-sensitive settings, limiting its applicability in real-world settings. To address this limitation, this paper introduces decision-focused generative learning (Gen-DFL), a novel framework that leverages generative models to adaptively model uncertainty and improve decision quality. Instead of relying on fixed uncertainty sets, Gen-DFL learns a structured representation of the optimization parameters and samples from the tail regions of the learned distribution to enhance robustness against worst-case scenarios. This approach mitigates over-conservatism while capturing complex dependencies in the parameter space. The paper shows, theoretically, that Gen-DFL achieves improved worst-case performance bounds compared to traditional DFL. Empirically, it evaluates Gen-DFL on various scheduling and logistics problems, demonstrating its strong performance against existing DFL methods.

### Stochastic Forward-Backward Deconvolution: Training Diffusion Models with Finite Noisy Datasets 
[[arxiv](https://arxiv.org/abs/2502.05446)] [[cool](https://papers.cool/arxiv/2502.05446)] [[pdf](https://arxiv.org/pdf/2502.05446)]
> **Authors**: Haoye Lu,Qifan Wu,Yaoliang Yu
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Recent diffusion-based generative models achieve remarkable results by training on massive datasets, yet this practice raises concerns about memorization and copyright infringement. A proposed remedy is to train exclusively on noisy data with potential copyright issues, ensuring the model never observes original content. However, through the lens of deconvolution theory, we show that although it is theoretically feasible to learn the data distribution from noisy samples, the practical challenge of collecting sufficient samples makes successful learning nearly unattainable. To overcome this limitation, we propose to pretrain the model with a small fraction of clean data to guide the deconvolution process. Combined with our Stochastic Forward--Backward Deconvolution (SFBD) method, we attain an FID of $6.31$ on CIFAR-10 with just $4\%$ clean images (and $3.58$ with $10\%$). Theoretically, we prove that SFBD guides the model to learn the true data distribution. The result also highlights the importance of pretraining on limited but clean data or the alternative from similar datasets. Empirical studies further support these findings and offer additional insights.

### Sample-Efficient Reinforcement Learning from Human Feedback via Information-Directed Sampling 
[[arxiv](https://arxiv.org/abs/2502.05434)] [[cool](https://papers.cool/arxiv/2502.05434)] [[pdf](https://arxiv.org/pdf/2502.05434)]
> **Authors**: Han Qi,Haochen Yang,Qiaosheng Zhang,Zhuoran Yang
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We study the problem of reinforcement learning from human feedback (RLHF), a critical problem in training large language models, from a theoretical perspective. Our main contribution is the design of novel sample-efficient RLHF algorithms based on information-directed sampling (IDS), an online decision-making principle inspired by information theory. Our algorithms maximize the sum of the value function and a mutual information term that encourages exploration of the unknown environment (which quantifies the information gained about the environment through observed human feedback data). To tackle the challenge of large state spaces and improve sample efficiency, we construct a simplified \emph{surrogate environment} and introduce a novel distance measure (named the \emph{$\ell_g$-distance}), enabling our IDS-based algorithm to achieve a Bayesian regret upper bound of order $O(H^{\frac{3}{2}}\sqrt{\log(K(ε)) T})$, where $H$ is the episode length, $T$ is the number of episode and $K(ε)$ is related to the covering number of the environment. Specializing to the tabular settings, this regret bound is of order $\tilde{O}(H^2\sqrt{SAT})$, where $S$ and $A$ are the numbers of states and actions. Finally, we propose an Approximate-IDS algorithm that is computationally more efficient while maintaining nearly the same sample efficiency. The design principle of this approximate algorithm is not only effective in RLHF settings but also applicable to the standard RL framework. Moreover, our work showcases the value of information theory in reinforcement learning and in the training of large language models.

### APE: Faster and Longer Context-Augmented Generation via Adaptive Parallel Encoding 
[[arxiv](https://arxiv.org/abs/2502.05431)] [[cool](https://papers.cool/arxiv/2502.05431)] [[pdf](https://arxiv.org/pdf/2502.05431)]
> **Authors**: Xinyu Yang,Tianqi Chen,Beidi Chen
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: ICLR 2025
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Context-augmented generation (CAG) techniques, including RAG and ICL, require the efficient combination of multiple contexts to generate responses to user queries. Directly inputting these contexts as a sequence introduces a considerable computational burden by re-encoding the combined selection of contexts for every request. To address this, we explore the promising potential of parallel encoding to independently pre-compute and cache each context's KV states. This approach enables the direct loading of cached states during inference while accommodating more contexts through position reuse across contexts. However, due to misalignments in attention distribution, directly applying parallel encoding results in a significant performance drop. To enable effective and efficient CAG, we propose Adaptive Parallel Encoding ($\textbf{APE}$), which brings shared prefix, attention temperature, and scaling factor to align the distribution of parallel encoding with sequential encoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98% and 93% sequential encoding performance using the same inputs while outperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales to many-shot CAG, effectively encoding hundreds of contexts in parallel. Efficiency evaluation shows that APE can achieve an end-to-end 4.5$\times$ speedup by reducing 28$\times$ prefilling time for a 128K-length context.

### Deep Generative Models with Hard Linear Equality Constraints 
[[arxiv](https://arxiv.org/abs/2502.05416)] [[cool](https://papers.cool/arxiv/2502.05416)] [[pdf](https://arxiv.org/pdf/2502.05416)]
> **Authors**: Ruoyan Li,Dipti Ranjan Sahu,Guy Van den Broeck,Zhe Zeng
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: While deep generative models~(DGMs) have demonstrated remarkable success in capturing complex data distributions, they consistently fail to learn constraints that encode domain knowledge and thus require constraint integration. Existing solutions to this challenge have primarily relied on heuristic methods and often ignore the underlying data distribution, harming the generative performance. In this work, we propose a probabilistically sound approach for enforcing the hard constraints into DGMs to generate constraint-compliant and realistic data. This is achieved by our proposed gradient estimators that allow the constrained distribution, the data distribution conditioned on constraints, to be differentiably learned. We carry out extensive experiments with various DGM model architectures over five image datasets and three scientific applications in which domain knowledge is governed by linear equality constraints. We validate that the standard DGMs almost surely generate data violating the constraints. Among all the constraint integration strategies, ours not only guarantees the satisfaction of constraints in generation but also archives superior generative performance than the other methods across every benchmark.

### Graph-based Molecular In-context Learning Grounded on Morgan Fingerprints 
[[arxiv](https://arxiv.org/abs/2502.05414)] [[cool](https://papers.cool/arxiv/2502.05414)] [[pdf](https://arxiv.org/pdf/2502.05414)]
> **Authors**: Ali Al-Lawati,Jason Lucas,Zhiwei Zhang,Prasenjit Mitra,Suhang Wang
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: In-context learning (ICL) effectively conditions large language models (LLMs) for molecular tasks, such as property prediction and molecule captioning, by embedding carefully selected demonstration examples into the input prompt. This approach avoids the computational overhead of extensive pertaining and fine-tuning. However, current prompt retrieval methods for molecular tasks have relied on molecule feature similarity, such as Morgan fingerprints, which do not adequately capture the global molecular and atom-binding relationships. As a result, these methods fail to represent the full complexity of molecular structures during inference. Moreover, small-to-medium-sized LLMs, which offer simpler deployment requirements in specialized systems, have remained largely unexplored in the molecular ICL literature. To address these gaps, we propose a self-supervised learning technique, GAMIC (Graph-Aligned Molecular In-Context learning, which aligns global molecular structures, represented by graph neural networks (GNNs), with textual captions (descriptions) while leveraging local feature similarity through Morgan fingerprints. In addition, we introduce a Maximum Marginal Relevance (MMR) based diversity heuristic during retrieval to optimize input prompt demonstration samples. Our experimental findings using diverse benchmark datasets show GAMIC outperforms simple Morgan-based ICL retrieval methods across all tasks by up to 45%.

### The Complexity of Learning Sparse Superposed Features with Feedback 
[[arxiv](https://arxiv.org/abs/2502.05407)] [[cool](https://papers.cool/arxiv/2502.05407)] [[pdf](https://arxiv.org/pdf/2502.05407)]
> **Authors**: Akash Kumar
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 41 pages, 20 figures
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: The success of deep networks is crucially attributed to their ability to capture latent features within a representation space. In this work, we investigate whether the underlying learned features of a model can be efficiently retrieved through feedback from an agent, such as a large language model (LLM), in the form of relative \textit{triplet comparisons}. These features may represent various constructs, including dictionaries in LLMs or components of a covariance matrix of Mahalanobis distances. We analyze the feedback complexity associated with learning a feature matrix in sparse settings. Our results establish tight bounds when the agent is permitted to construct activations and demonstrate strong upper bounds in sparse scenarios when the agent's feedback is limited to distributional information. We validate our theoretical findings through experiments on two distinct applications: feature recovery from Recursive Feature Machine-trained models and dictionary extraction from sparse autoencoders trained on Large Language Models.

### Analyzing public sentiment to gauge key stock events and determine volatility in conjunction with time and options premiums 
[[arxiv](https://arxiv.org/abs/2502.05403)] [[cool](https://papers.cool/arxiv/2502.05403)] [[pdf](https://arxiv.org/pdf/2502.05403)]
> **Authors**: SriVarsha Mulakala,Umesh Vangapally,Benjamin Larkey,Aidan Henrichs,Corey Wojslaw
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Analyzing stocks and making higher accurate predictions on where the price is heading continues to become more and more challenging therefore, we designed a new financial algorithm that leverages social media sentiment analysis to enhance the prediction of key stock earnings and associated volatility. Our model integrates sentiment analysis and data retrieval techniques to extract critical information from social media, analyze company financials, and compare sentiments between Wall Street and the general public. This approach aims to provide investors with timely data to execute trades based on key events, rather than relying on long-term stock holding strategies. The stock market is characterized by rapid data flow and fluctuating community sentiments, which can significantly impact trading outcomes. Stock forecasting is complex given its stochastic dynamic. Standard traditional prediction methods often overlook key events and media engagement, focusing its practice into long-term investment options. Our research seeks to change the stochastic dynamic to a more predictable environment by examining the impact of media on stock volatility, understanding and identifying sentiment differences between Wall Street and retail investors, and evaluating the impact of various media networks in predicting earning reports.

### Imitation Learning from a Single Temporally Misaligned Video 
[[arxiv](https://arxiv.org/abs/2502.05397)] [[cool](https://papers.cool/arxiv/2502.05397)] [[pdf](https://arxiv.org/pdf/2502.05397)]
> **Authors**: William Huey,Huaxiaoyue Wang,Anne Wu,Yoav Artzi,Sanjiban Choudhury
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We examine the problem of learning sequential tasks from a single visual demonstration. A key challenge arises when demonstrations are temporally misaligned due to variations in timing, differences in embodiment, or inconsistencies in execution. Existing approaches treat imitation as a distribution-matching problem, aligning individual frames between the agent and the demonstration. However, we show that such frame-level matching fails to enforce temporal ordering or ensure consistent progress. Our key insight is that matching should instead be defined at the level of sequences. We propose that perfect matching occurs when one sequence successfully covers all the subgoals in the same order as the other sequence. We present ORCA (ORdered Coverage Alignment), a dense per-timestep reward function that measures the probability of the agent covering demonstration frames in the correct order. On temporally misaligned demonstrations, we show that agents trained with the ORCA reward achieve $4.5$x improvement ($0.11 \rightarrow 0.50$ average normalized returns) for Meta-world tasks and $6.6$x improvement ($6.55 \rightarrow 43.3$ average returns) for Humanoid-v4 tasks compared to the best frame-level matching algorithms. We also provide empirical analysis showing that ORCA is robust to varying levels of temporal misalignment. Our code is available at https://github.com/portal-cornell/orca/

### Open Challenges in Time Series Anomaly Detection: An Industry Perspective 
[[arxiv](https://arxiv.org/abs/2502.05392)] [[cool](https://papers.cool/arxiv/2502.05392)] [[pdf](https://arxiv.org/pdf/2502.05392)]
> **Authors**: Andreas Mueller
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Current research in time-series anomaly detection is using definitions that miss critical aspects of how anomaly detection is commonly used in practice. We list several areas that are of practical relevance and that we believe are either under-investigated or missing entirely from the current discourse. Based on an investigation of systems deployed in a cloud environment, we motivate the areas of streaming algorithms, human-in-the-loop scenarios, point processes, conditional anomalies and populations analysis of time series. This paper serves as a motivation and call for action, including opportunities for theoretical and applied research, as well as for building new dataset and benchmarks.

### BCQ: Block Clustered Quantization for 4-bit (W4A4) LLM Inference 
[[arxiv](https://arxiv.org/abs/2502.05376)] [[cool](https://papers.cool/arxiv/2502.05376)] [[pdf](https://arxiv.org/pdf/2502.05376)]
> **Authors**: Reena Elangovan,Charbel Sakr,Anand Raghunathan,Brucek Khailany
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Post-training quantization (PTQ) is a promising approach to reducing the storage and computational requirements of large language models (LLMs) without additional training cost. Recent PTQ studies have primarily focused on quantizing only weights to sub-8-bits while maintaining activations at 8-bits or higher. Accurate sub-8-bit quantization for both weights and activations without relying on quantization-aware training remains a significant challenge. We propose a novel quantization method called block clustered quantization (BCQ) wherein each operand tensor is decomposed into blocks (a block is a group of contiguous scalars), blocks are clustered based on their statistics, and a dedicated optimal quantization codebook is designed for each cluster. As a specific embodiment of this approach, we propose a PTQ algorithm called Locally-Optimal BCQ (LO-BCQ) that iterates between the steps of block clustering and codebook design to greedily minimize the quantization mean squared error. When weight and activation scalars are encoded to W4A4 format (with 0.5-bits of overhead for storing scaling factors and codebook selectors), we advance the current state-of-the-art by demonstrating <1% loss in inference accuracy across several LLMs and downstream tasks.

### Towards LLM Unlearning Resilient to Relearning Attacks: A Sharpness-Aware Minimization Perspective and Beyond 
[[arxiv](https://arxiv.org/abs/2502.05374)] [[cool](https://papers.cool/arxiv/2502.05374)] [[pdf](https://arxiv.org/pdf/2502.05374)]
> **Authors**: Chongyu Fan,Jinghan Jia,Yihua Zhang,Anil Ramakrishna,Mingyi Hong,Sijia Liu
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: The LLM unlearning technique has recently been introduced to comply with data regulations and address the safety and ethical concerns of LLMs by removing the undesired data-model influence. However, state-of-the-art unlearning methods face a critical vulnerability: they are susceptible to ``relearning'' the removed information from a small number of forget data points, known as relearning attacks. In this paper, we systematically investigate how to make unlearned models robust against such attacks. For the first time, we establish a connection between robust unlearning and sharpness-aware minimization (SAM) through a unified robust optimization framework, in an analogy to adversarial training designed to defend against adversarial attacks. Our analysis for SAM reveals that smoothness optimization plays a pivotal role in mitigating relearning attacks. Thus, we further explore diverse smoothing strategies to enhance unlearning robustness. Extensive experiments on benchmark datasets, including WMDP and MUSE, demonstrate that SAM and other smoothness optimization approaches consistently improve the resistance of LLM unlearning to relearning attacks. Notably, smoothness-enhanced unlearning also helps defend against (input-level) jailbreaking attacks, broadening our proposal's impact in robustifying LLM unlearning. Codes are available at https://github.com/OPTML-Group/Unlearn-Smooth.

### Active Learning of Model Discrepancy with Bayesian Experimental Design 
[[arxiv](https://arxiv.org/abs/2502.05372)] [[cool](https://papers.cool/arxiv/2502.05372)] [[pdf](https://arxiv.org/pdf/2502.05372)]
> **Authors**: Huchen Yang,Chuanqi Chen,Jin-Long Wu
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Digital twins have been actively explored in many engineering applications, such as manufacturing and autonomous systems. However, model discrepancy is ubiquitous in most digital twin models and has significant impacts on the performance of using those models. In recent years, data-driven modeling techniques have been demonstrated promising in characterizing the model discrepancy in existing models, while the training data for the learning of model discrepancy is often obtained in an empirical way and an active approach of gathering informative data can potentially benefit the learning of model discrepancy. On the other hand, Bayesian experimental design (BED) provides a systematic approach to gathering the most informative data, but its performance is often negatively impacted by the model discrepancy. In this work, we build on sequential BED and propose an efficient approach to iteratively learn the model discrepancy based on the data from the BED. The performance of the proposed method is validated by a classical numerical example governed by a convection-diffusion equation, for which full BED is still feasible. The proposed method is then further studied in the same numerical example with a high-dimensional model discrepancy, which serves as a demonstration for the scenarios where full BED is not practical anymore. An ensemble-based approximation of information gain is further utilized to assess the data informativeness and to enhance learning model discrepancy. The results show that the proposed method is efficient and robust to the active learning of high-dimensional model discrepancy, using data suggested by the sequential BED. We also demonstrate that the proposed method is compatible with both classical numerical solvers and modern auto-differentiable solvers.

### fMoE: Fine-Grained Expert Offloading for Large Mixture-of-Experts Serving 
[[arxiv](https://arxiv.org/abs/2502.05370)] [[cool](https://papers.cool/arxiv/2502.05370)] [[pdf](https://arxiv.org/pdf/2502.05370)]
> **Authors**: Hanfei Yu,Xingqi Cui,Hong Zhang,Hao Wang,Hao Wang
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,分布式、并行和集群计算
- **Abstract**: Large Language Models (LLMs) have gained immense success in revolutionizing various applications, including content generation, search and recommendation, and AI-assisted operation. To reduce high training costs, Mixture-of-Experts (MoE) architecture has become a popular backbone for modern LLMs. However, despite the benefits, serving MoE-based LLMs experience severe memory inefficiency due to sparsely activated experts. Recent studies propose to offload inactive experts from GPU memory to CPU memory to improve the serving efficiency of MoE models. However, they either incur high inference latency or high model memory footprints due to coarse-grained designs. To tame the latency-memory trade-off in MoE serving, we present fMoE, a fine-grained expert offloading system for MoE serving that achieves low inference latency with memory efficiency. We design fMoE to extract fine-grained expert selection patterns from MoE models and semantic hints from input prompts to efficiently guide expert prefetching, caching, and offloading decisions. fMoE is prototyped on top of HuggingFace Transformers and deployed on a six-GPU testbed. Experiments with open-source MoE models and real-world workloads show that fMoE reduces inference latency by 47% and improves expert hit rate by 36% over state-of-the-art solutions.

### Curse of Dimensionality in Neural Network Optimization 
[[arxiv](https://arxiv.org/abs/2502.05360)] [[cool](https://papers.cool/arxiv/2502.05360)] [[pdf](https://arxiv.org/pdf/2502.05360)]
> **Authors**: Sanghoon Na,Haizhao Yang
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,优化与控制,机器学习
- **Abstract**: The curse of dimensionality in neural network optimization under the mean-field regime is studied. It is demonstrated that when a shallow neural network with a Lipschitz continuous activation function is trained using either empirical or population risk to approximate a target function that is $r$ times continuously differentiable on $[0,1]^d$, the population risk may not decay at a rate faster than $t^{-\frac{4r}{d-2r}}$, where $t$ is an analog of the total number of optimization iterations. This result highlights the presence of the curse of dimensionality in the optimization computation required to achieve a desired accuracy. Instead of analyzing parameter evolution directly, the training dynamics are examined through the evolution of the parameter distribution under the 2-Wasserstein gradient flow. Furthermore, it is established that the curse of dimensionality persists when a locally Lipschitz continuous activation function is employed, where the Lipschitz constant in $[-x,x]$ is bounded by $O(x^δ)$ for any $x \in \mathbb{R}$. In this scenario, the population risk is shown to decay at a rate no faster than $t^{-\frac{(4+2δ)r}{d-2r}}$. To the best of our knowledge, this work is the first to analyze the impact of function smoothness on the curse of dimensionality in neural network optimization theory.

### Towards Foundational Models for Dynamical System Reconstruction: Hierarchical Meta-Learning via Mixture of Experts 
[[arxiv](https://arxiv.org/abs/2502.05335)] [[cool](https://papers.cool/arxiv/2502.05335)] [[pdf](https://arxiv.org/pdf/2502.05335)]
> **Authors**: Roussel Desmond Nzoyem,David A. W. Barton,Tom Deakin
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 22 pages, 11 figures, 7 tables
- **标题**: None
- **领域**: 机器学习
- **Abstract**: As foundational models reshape scientific discovery, a bottleneck persists in dynamical system reconstruction (DSR): the ability to learn across system hierarchies. Many meta-learning approaches have been applied successfully to single systems, but falter when confronted with sparse, loosely related datasets requiring multiple hierarchies to be learned. Mixture of Experts (MoE) offers a natural paradigm to address these challenges. Despite their potential, we demonstrate that naive MoEs are inadequate for the nuanced demands of hierarchical DSR, largely due to their gradient descent-based gating update mechanism which leads to slow updates and conflicted routing during training. To overcome this limitation, we introduce MixER: Mixture of Expert Reconstructors, a novel sparse top-1 MoE layer employing a custom gating update algorithm based on $K$-means and least squares. Extensive experiments validate MixER's capabilities, demonstrating efficient training and scalability to systems of up to ten parametric ordinary differential equations. However, our layer underperforms state-of-the-art meta-learners in high-data regimes, particularly when each expert is constrained to process only a fraction of a dataset composed of highly related data points. Further analysis with synthetic and neuroscientific time series suggests that the quality of the contextual representations generated by MixER is closely linked to the presence of hierarchical structure in the data.

### Geometric Machine Learning on EEG Signals 
[[arxiv](https://arxiv.org/abs/2502.05334)] [[cool](https://papers.cool/arxiv/2502.05334)] [[pdf](https://arxiv.org/pdf/2502.05334)]
> **Authors**: Benjamin J. Choi
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: Accepted in Proceedings ofMachineLearningResearch (PMLR), 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Brain-computer interfaces (BCIs) offer transformative potential, but decoding neural signals presents significant challenges. The core premise of this paper is built around demonstrating methods to elucidate the underlying low-dimensional geometric structure present in high-dimensional brainwave data in order to assist in downstream BCI-related neural classification tasks. We demonstrate two pipelines related to electroencephalography (EEG) signal processing: (1) a preliminary pipeline removing noise from individual EEG channels, and (2) a downstream manifold learning pipeline uncovering geometric structure across networks of EEG channels. We conduct preliminary validation using two EEG datasets and situate our demonstration in the context of the BCI-relevant imagined digit decoding problem. Our preliminary pipeline uses an attention-based EEG filtration network to extract clean signal from individual EEG channels. Our primary pipeline uses a fast Fourier transform, a Laplacian eigenmap, a discrete analog of Ricci flow via Ollivier's notion of Ricci curvature, and a graph convolutional network to perform dimensionality reduction on high-dimensional multi-channel EEG data in order to enable regularizable downstream classification. Our system achieves competitive performance with existing signal processing and classification benchmarks; we demonstrate a mean test correlation coefficient of >0.95 at 2 dB on semi-synthetic neural denoising and a downstream EEG-based classification accuracy of 0.97 on distinguishing digit- versus non-digit- thoughts. Results are preliminary and our geometric machine learning pipeline should be validated by more extensive follow-up studies; generalizing these results to larger inter-subject sample sizes, different hardware systems, and broader use cases will be crucial.

### Removing Neural Signal Artifacts with Autoencoder-Targeted Adversarial Transformers (AT-AT) 
[[arxiv](https://arxiv.org/abs/2502.05332)] [[cool](https://papers.cool/arxiv/2502.05332)] [[pdf](https://arxiv.org/pdf/2502.05332)]
> **Authors**: Benjamin J. Choi
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: Accepted at CNS 2025, Boston, MA, USA
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Electromyogenic (EMG) noise is a major contamination source in EEG data that can impede accurate analysis of brain-specific neural activity. Recent literature on EMG artifact removal has moved beyond traditional linear algorithms in favor of machine learning-based systems. However, existing deep learning-based filtration methods often have large compute footprints and prohibitively long training times. In this study, we present a new machine learning-based system for filtering EMG interference from EEG data using an autoencoder-targeted adversarial transformer (AT-AT). By leveraging the lightweight expressivity of an autoencoder to determine optimal time-series transformer application sites, our AT-AT architecture achieves a >90% model size reduction compared to published artifact removal models. The addition of adversarial training ensures that filtered signals adhere to the fundamental characteristics of EEG data. We trained AT-AT using published neural data from 67 subjects and found that the system was able to achieve comparable test performance to larger models; AT-AT posted a mean reconstructive correlation coefficient above 0.95 at an initial signal-to-noise ratio (SNR) of 2 dB and 0.70 at -7 dB SNR. Further research generalizing these results to broader sample sizes beyond these isolated test cases will be crucial; while outside the scope of this study, we also include results from a real-world deployment of AT-AT in the Appendix.

### From Counterfactuals to Trees: Competitive Analysis of Model Extraction Attacks 
[[arxiv](https://arxiv.org/abs/2502.05325)] [[cool](https://papers.cool/arxiv/2502.05325)] [[pdf](https://arxiv.org/pdf/2502.05325)]
> **Authors**: Awa Khouna,Julien Ferry,Thibaut Vidal
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,密码学和安全
- **Abstract**: The advent of Machine Learning as a Service (MLaaS) has heightened the trade-off between model explainability and security. In particular, explainability techniques, such as counterfactual explanations, inadvertently increase the risk of model extraction attacks, enabling unauthorized replication of proprietary models. In this paper, we formalize and characterize the risks and inherent complexity of model reconstruction, focusing on the "oracle'' queries required for faithfully inferring the underlying prediction function. We present the first formal analysis of model extraction attacks through the lens of competitive analysis, establishing a foundational framework to evaluate their efficiency. Focusing on models based on additive decision trees (e.g., decision trees, gradient boosting, and random forests), we introduce novel reconstruction algorithms that achieve provably perfect fidelity while demonstrating strong anytime performance. Our framework provides theoretical bounds on the query complexity for extracting tree-based model, offering new insights into the security vulnerabilities of their deployment.

### Using Federated Machine Learning in Predictive Maintenance of Jet Engines 
[[arxiv](https://arxiv.org/abs/2502.05321)] [[cool](https://papers.cool/arxiv/2502.05321)] [[pdf](https://arxiv.org/pdf/2502.05321)]
> **Authors**: Asaph Matheus Barbosa,Thao Vy Nhat Ngo,Elaheh Jafarigol,Theodore B. Trafalis,Emuobosa P. Ojoboh
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The goal of this paper is to predict the Remaining Useful Life (RUL) of turbine jet engines using a federated machine learning framework. Federated Learning enables multiple edge devices/nodes or servers to collaboratively train a shared model without sharing sensitive data, thus preserving data privacy and security. By implementing a nonlinear model, the system aims to capture complex relationships and patterns in the engine data to enhance the accuracy of RUL predictions. This approach leverages decentralized computation, allowing models to be trained locally at each device before aggregating the learned weights at a central server. By predicting the RUL of jet engines accurately, maintenance schedules can be optimized, downtime reduced, and operational efficiency improved, ultimately leading to cost savings and enhanced performance in the aviation industry. Computational results are provided by using the C-MAPSS dataset which is publicly available on the NASA website and is a valuable resource for studying and analyzing engine degradation behaviors in various operational scenarios.

### Diagonal Symmetrization of Neural Network Solvers for the Many-Electron Schrödinger Equation 
[[arxiv](https://arxiv.org/abs/2502.05318)] [[cool](https://papers.cool/arxiv/2502.05318)] [[pdf](https://arxiv.org/pdf/2502.05318)]
> **Authors**: Kevin Han Huang,Ni Zhan,Elif Ertekin,Peter Orbanz,Ryan P. Adams
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,材料科学
- **Abstract**: Incorporating group symmetries into neural networks has been a cornerstone of success in many AI-for-science applications. Diagonal groups of isometries, which describe the invariance under a simultaneous movement of multiple objects, arise naturally in many-body quantum problems. Despite their importance, diagonal groups have received relatively little attention, as they lack a natural choice of invariant maps except in special cases. We study different ways of incorporating diagonal invariance in neural network ansätze trained via variational Monte Carlo methods, and consider specifically data augmentation, group averaging and canonicalization. We show that, contrary to standard ML setups, in-training symmetrization destabilizes training and can lead to worse performance. Our theoretical and numerical results indicate that this unexpected behavior may arise from a unique computational-statistical tradeoff not found in standard ML analyses of symmetrization. Meanwhile, we demonstrate that post hoc averaging is less sensitive to such tradeoffs and emerges as a simple, flexible and effective method for improving neural network solvers.

### AI/ML-Based Automatic Modulation Recognition: Recent Trends and Future Possibilities 
[[arxiv](https://arxiv.org/abs/2502.05315)] [[cool](https://papers.cool/arxiv/2502.05315)] [[pdf](https://arxiv.org/pdf/2502.05315)]
> **Authors**: Elaheh Jafarigol,Behnoud Alaghband,Azadeh Gilanpour,Saeid Hosseinipoor,Mirhamed Mirmozafari
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We present a review of high-performance automatic modulation recognition (AMR) models proposed in the literature to classify various Radio Frequency (RF) modulation schemes. We replicated these models and compared their performance in terms of accuracy across a range of signal-to-noise ratios. To ensure a fair comparison, we used the same dataset (RadioML-2016A), the same hardware, and a consistent definition of test accuracy as the evaluation metric, thereby providing a benchmark for future AMR studies. The hyperparameters were selected based on the authors' suggestions in the associated references to achieve results as close as possible to the originals. The replicated models are publicly accessible for further analysis of AMR models. We also present the test accuracies of the selected models versus their number of parameters, indicating their complexities. Building on this comparative analysis, we identify strategies to enhance these models' performance. Finally, we present potential opportunities for improvement, whether through novel architectures, data processing techniques, or training strategies, to further advance the capabilities of AMR models.

### Training Set Reconstruction from Differentially Private Forests: How Effective is DP? 
[[arxiv](https://arxiv.org/abs/2502.05307)] [[cool](https://papers.cool/arxiv/2502.05307)] [[pdf](https://arxiv.org/pdf/2502.05307)]
> **Authors**: Alice Gorgé,Julien Ferry,Sébastien Gambs,Thibaut Vidal
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,密码学和安全
- **Abstract**: Recent research has shown that machine learning models are vulnerable to privacy attacks targeting their training data. Differential privacy (DP) has become a widely adopted countermeasure, as it offers rigorous privacy protections. In this paper, we introduce a reconstruction attack targeting state-of-the-art $\varepsilon$-DP random forests. By leveraging a constraint programming model that incorporates knowledge of the forest's structure and DP mechanism characteristics, our approach formally reconstructs the most likely dataset that could have produced a given forest. Through extensive computational experiments, we examine the interplay between model utility, privacy guarantees, and reconstruction accuracy across various configurations. Our results reveal that random forests trained with meaningful DP guarantees can still leak substantial portions of their training data. Specifically, while DP reduces the success of reconstruction attacks, the only forests fully robust to our attack exhibit predictive performance no better than a constant classifier. Building on these insights, we provide practical recommendations for the construction of DP random forests that are more resilient to reconstruction attacks and maintain non-trivial predictive performance.

### Decentralized Online Ensembles of Gaussian Processes for Multi-Agent Systems 
[[arxiv](https://arxiv.org/abs/2502.05301)] [[cool](https://papers.cool/arxiv/2502.05301)] [[pdf](https://arxiv.org/pdf/2502.05301)]
> **Authors**: Fernando Llorente,Daniel Waxman,Petar M. Djurić
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 5 pages, 2 figures. Accepted to ICASSP 2025
- **标题**: None
- **领域**: 机器学习,多代理系统,信号处理,机器学习
- **Abstract**: Flexible and scalable decentralized learning solutions are fundamentally important in the application of multi-agent systems. While several recent approaches introduce (ensembles of) kernel machines in the distributed setting, Bayesian solutions are much more limited. We introduce a fully decentralized, asymptotically exact solution to computing the random feature approximation of Gaussian processes. We further address the choice of hyperparameters by introducing an ensembling scheme for Bayesian multiple kernel learning based on online Bayesian model averaging. The resulting algorithm is tested against Bayesian and frequentist methods on simulated and real-world datasets.

### Parameter Symmetry Breaking and Restoration Determines the Hierarchical Learning in AI Systems 
[[arxiv](https://arxiv.org/abs/2502.05300)] [[cool](https://papers.cool/arxiv/2502.05300)] [[pdf](https://arxiv.org/pdf/2502.05300)]
> **Authors**: Liu Ziyin,Yizhou Xu,Tomaso Poggio,Isaac Chuang
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,无序系统和神经网络,人工智能,机器学习
- **Abstract**: The dynamics of learning in modern large AI systems is hierarchical, often characterized by abrupt, qualitative shifts akin to phase transitions observed in physical systems. While these phenomena hold promise for uncovering the mechanisms behind neural networks and language models, existing theories remain fragmented, addressing specific cases. In this paper, we posit that parameter symmetry breaking and restoration serve as a unifying mechanism underlying these behaviors. We synthesize prior observations and show how this mechanism explains three distinct hierarchies in neural networks: learning dynamics, model complexity, and representation formation. By connecting these hierarchies, we highlight symmetry -- a cornerstone of theoretical physics -- as a potential fundamental principle in modern AI.

### GST-UNet: Spatiotemporal Causal Inference with Time-Varying Confounders 
[[arxiv](https://arxiv.org/abs/2502.05295)] [[cool](https://papers.cool/arxiv/2502.05295)] [[pdf](https://arxiv.org/pdf/2502.05295)]
> **Authors**: Miruna Oprescu,David K. Park,Xihaier Luo,Shinjae Yoo,Nathan Kallus
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 17 pages, 6 figures, 2 tables
- **标题**: None
- **领域**: 机器学习,方法论
- **Abstract**: Estimating causal effects from spatiotemporal data is a key challenge in fields such as public health, social policy, and environmental science, where controlled experiments are often infeasible. However, existing causal inference methods relying on observational data face significant limitations: they depend on strong structural assumptions to address spatiotemporal challenges $\unicode{x2013}$ such as interference, spatial confounding, and temporal carryover effects $\unicode{x2013}$ or fail to account for $\textit{time-varying confounders}$. These confounders, influenced by past treatments and outcomes, can themselves shape future treatments and outcomes, creating feedback loops that complicate traditional adjustment strategies. To address these challenges, we introduce the $\textbf{GST-UNet}$ ($\textbf{G}$-computation $\textbf{S}$patio-$\textbf{T}$emporal $\textbf{UNet}$), a novel end-to-end neural network framework designed to estimate treatment effects in complex spatial and temporal settings. The GST-UNet leverages regression-based iterative G-computation to explicitly adjust for time-varying confounders, providing valid estimates of potential outcomes and treatment effects. To the best of our knowledge, the GST-UNet is the first neural model to account for complex, non-linear dynamics and time-varying confounders in spatiotemporal interventions. We demonstrate the effectiveness of the GST-UNet through extensive simulation studies and showcase its practical utility with a real-world analysis of the impact of wildfire smoke on respiratory hospitalizations during the 2018 California Camp Fire. Our results highlight the potential of GST-UNet to advance spatiotemporal causal inference across a wide range of policy-driven and scientific applications.

### Fairness and Sparsity within Rashomon sets: Enumeration-Free Exploration and Characterization 
[[arxiv](https://arxiv.org/abs/2502.05286)] [[cool](https://papers.cool/arxiv/2502.05286)] [[pdf](https://arxiv.org/pdf/2502.05286)]
> **Authors**: Lucas Langlade,Julien Ferry,Gabriel Laberge,Thibaut Vidal
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We introduce an enumeration-free method based on mathematical programming to precisely characterize various properties such as fairness or sparsity within the set of "good models", known as Rashomon set. This approach is generically applicable to any hypothesis class, provided that a mathematical formulation of the model learning task exists. It offers a structured framework to define the notion of business necessity and evaluate how fairness can be improved or degraded towards a specific protected group, while remaining within the Rashomon set and maintaining any desired sparsity level. We apply our approach to two hypothesis classes: scoring systems and decision diagrams, leveraging recent mathematical programming formulations for training such models. As seen in our experiments, the method comprehensively and certifiably quantifies trade-offs between predictive performance, sparsity, and fairness. We observe that a wide range of fairness values are attainable, ranging from highly favorable to significantly unfavorable for a protected group, while staying within less than 1% of the best possible training accuracy for the hypothesis class. Additionally, we observe that sparsity constraints limit these trade-offs and may disproportionately harm specific subgroups. As we evidenced, thoroughly characterizing the tensions between these key aspects is critical for an informed and accountable selection of models.

### Principles and Components of Federated Learning Architectures 
[[arxiv](https://arxiv.org/abs/2502.05273)] [[cool](https://papers.cool/arxiv/2502.05273)] [[pdf](https://arxiv.org/pdf/2502.05273)]
> **Authors**: Sarwar Saif,MD Abdullah Al Nasim,Parag Biswas,Abdur Rashid,MD Mahim Anjum Haque,Md. Zihad Bin Jahangir
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Federated learning, also known as FL, is a machine learning framework in which a significant amount of clients (such as mobile devices or whole enterprises) collaborate to collaboratively train a model while keeping decentralized training data, all overseen by a central server (such as a service provider). There are advantages in terms of privacy, security, regulations, and economy with this decentralized approach to model training. FL is not impervious to the flaws that plague conventional machine learning models, despite its seeming promise. This study offers a thorough analysis of the fundamental ideas and elements of federated learning architectures, emphasizing five important areas: communication architectures, machine learning models, data partitioning, privacy methods, and system heterogeneity. We additionally address the difficulties and potential paths for future study in the area. Furthermore, based on a comprehensive review of the literature, we present a collection of architectural patterns for federated learning systems. This analysis will help to understand the basic of Federated learning, the primary components of FL, and also about several architectural details.

### Optimizing Temperature for Language Models with Multi-Sample Inference 
[[arxiv](https://arxiv.org/abs/2502.05234)] [[cool](https://papers.cool/arxiv/2502.05234)] [[pdf](https://arxiv.org/pdf/2502.05234)]
> **Authors**: Weihua Du,Yiming Yang,Sean Welleck
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 20 pages. Code available at https://github.com/StigLidu/TURN
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Multi-sample aggregation strategies, such as majority voting and best-of-N sampling, are widely used in contemporary large language models (LLMs) to enhance predictive accuracy across various tasks. A key challenge in this process is temperature selection, which significantly impacts model performance. Existing approaches either rely on a fixed default temperature or require labeled validation data for tuning, which are often scarce and difficult to obtain. This paper addresses the challenge of automatically identifying the (near)-optimal temperature for different LLMs using multi-sample aggregation strategies, without relying on task-specific validation data. We provide a comprehensive analysis of temperature's role in performance optimization, considering variations in model architectures, datasets, task types, model sizes, and predictive accuracy. Furthermore, we propose a novel entropy-based metric for automated temperature optimization, which consistently outperforms fixed-temperature baselines. Additionally, we incorporate a stochastic process model to enhance interpretability, offering deeper insights into the relationship between temperature and model performance.

### Adversarial Machine Learning: Attacking and Safeguarding Image Datasets 
[[arxiv](https://arxiv.org/abs/2502.05203)] [[cool](https://papers.cool/arxiv/2502.05203)] [[pdf](https://arxiv.org/pdf/2502.05203)]
> **Authors**: Koushik Chowdhury
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-10
> **comment**: 6 pages, published in Proceedings of the Fourth International Conference on Ubiquitous Computing and Intelligent Information Systems (ICUIS-2024)
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: This paper examines the vulnerabilities of convolutional neural networks (CNNs) to adversarial attacks and explores a method for their safeguarding. In this study, CNNs were implemented on four of the most common image datasets, namely CIFAR-10, ImageNet, MNIST, and Fashion-MNIST, and achieved high baseline accuracy. To assess the strength of these models, the Fast Gradient Sign Method was used, which is a type of exploit on the model that is used to bring down the models accuracies by adding a very minimal perturbation to the input image. To counter the FGSM attack, a safeguarding approach went through, which includes retraining the models on clear and pollutant or adversarial images to increase their resistance ability. The next step involves applying FGSM again, but this time to the adversarially trained models, to see how much the accuracy of the models has gone down and evaluate the effectiveness of the defense. It appears that while most level of robustness is achieved against the models after adversarial training, there are still a few losses in the performance of these models against adversarial perturbations. This work emphasizes the need to create better defenses for models deployed in real-world scenarios against adversaries.

### Joint MoE Scaling Laws: Mixture of Experts Can Be Memory Efficient 
[[arxiv](https://arxiv.org/abs/2502.05172)] [[cool](https://papers.cool/arxiv/2502.05172)] [[pdf](https://arxiv.org/pdf/2502.05172)]
> **Authors**: Jan Ludziejewski,Maciej Pióro,Jakub Krajewski,Maciej Stefaniak,Michał Krutul,Jan Małaśnicki,Marek Cygan,Piotr Sankowski,Kamil Adamczewski,Piotr Miłoś,Sebastian Jaszczur
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Mixture of Experts (MoE) architectures have significantly increased computational efficiency in both research and real-world applications of large-scale machine learning models. However, their scalability and efficiency under memory constraints remain relatively underexplored. In this work, we present joint scaling laws for dense and MoE models, incorporating key factors such as the number of active parameters, dataset size, and the number of experts. Our findings provide a principled framework for selecting the optimal MoE configuration under fixed memory and compute budgets. Surprisingly, we show that MoE models can be more memory-efficient than dense models, contradicting conventional wisdom. To derive and validate the theoretical predictions of our scaling laws, we conduct over 280 experiments with up to 2.7B active parameters and up to 5B total parameters. These results offer actionable insights for designing and deploying MoE models in practical large-scale training scenarios.

### Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach 
[[arxiv](https://arxiv.org/abs/2502.05171)] [[cool](https://papers.cool/arxiv/2502.05171)] [[pdf](https://arxiv.org/pdf/2502.05171)]
> **Authors**: Jonas Geiping,Sean McLeish,Neel Jain,John Kirchenbauer,Siddharth Singh,Brian R. Bartoldson,Bhavya Kailkhura,Abhinav Bhatele,Tom Goldstein
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: Themodelis available at https://huggingface.co/tomg-group-umd/huginn-0125. Code and data recipe can be found at https://github.com/seal-rg/recurrent-pretraining
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: We study a novel language model architecture that is capable of scaling test-time computation by implicitly reasoning in latent space. Our model works by iterating a recurrent block, thereby unrolling to arbitrary depth at test-time. This stands in contrast to mainstream reasoning models that scale up compute by producing more tokens. Unlike approaches based on chain-of-thought, our approach does not require any specialized training data, can work with small context windows, and can capture types of reasoning that are not easily represented in words. We scale a proof-of-concept model to 3.5 billion parameters and 800 billion tokens. We show that the resulting model can improve its performance on reasoning benchmarks, sometimes dramatically, up to a computation load equivalent to 50 billion parameters.

### In-context denoising with one-layer transformers: connections between attention and associative memory retrieval 
[[arxiv](https://arxiv.org/abs/2502.05164)] [[cool](https://papers.cool/arxiv/2502.05164)] [[pdf](https://arxiv.org/pdf/2502.05164)]
> **Authors**: Matthew Smart,Alberto Bietti,Anirvan M. Sengupta
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,无序系统和神经网络
- **Abstract**: We introduce in-context denoising, a task that refines the connection between attention-based architectures and dense associative memory (DAM) networks, also known as modern Hopfield networks. Using a Bayesian framework, we show theoretically and empirically that certain restricted denoising problems can be solved optimally even by a single-layer transformer. We demonstrate that a trained attention layer processes each denoising prompt by performing a single gradient descent update on a context-aware DAM energy landscape, where context tokens serve as associative memories and the query token acts as an initial state. This one-step update yields better solutions than exact retrieval of either a context token or a spurious local minimum, providing a concrete example of DAM networks extending beyond the standard retrieval paradigm. Overall, this work solidifies the link between associative memory and attention mechanisms first identified by Ramsauer et al., and demonstrates the relevance of associative memory models in the study of in-context learning.

### A Lightweight Method to Disrupt Memorized Sequences in LLM 
[[arxiv](https://arxiv.org/abs/2502.05159)] [[cool](https://papers.cool/arxiv/2502.05159)] [[pdf](https://arxiv.org/pdf/2502.05159)]
> **Authors**: Parjanya Prajakta Prashant,Kaustubh Ponkshe,Babak Salimi
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 20 pages, 2 figures
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Large language models (LLMs) demonstrate impressive capabilities across many tasks yet risk reproducing copyrighted content verbatim, raising legal and ethical concerns. Although methods like differential privacy or neuron editing can reduce memorization, they typically require costly retraining or direct access to model weights and may degrade performance. To address these challenges, we propose TokenSwap, a lightweight, post-hoc approach that replaces the probabilities of grammar-related tokens with those from a small auxiliary model (e.g., DistilGPT-2). We run extensive experiments on commercial grade models such as Pythia-6.9b and LLaMA-3-8b and demonstrate that our method effectively reduces well-known cases of memorized generation by upto 10x with little to no impact on downstream tasks. Our approach offers a uniquely accessible and effective solution to users of real-world systems.

### Efficient distributional regression trees learning algorithms for calibrated non-parametric probabilistic forecasts 
[[arxiv](https://arxiv.org/abs/2502.05157)] [[cool](https://papers.cool/arxiv/2502.05157)] [[pdf](https://arxiv.org/pdf/2502.05157)]
> **Authors**: Duchemin Quentin,Obozinski Guillaume
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,数据结构和算法
- **Abstract**: The perspective of developing trustworthy AI for critical applications in science and engineering requires machine learning techniques that are capable of estimating their own uncertainty. In the context of regression, instead of estimating a conditional mean, this can be achieved by producing a predictive interval for the output, or to even learn a model of the conditional probability $p(y|x)$ of an output $y$ given input features $x$. While this can be done under parametric assumptions with, e.g. generalized linear model, these are typically too strong, and non-parametric models offer flexible alternatives. In particular, for scalar outputs, learning directly a model of the conditional cumulative distribution function of $y$ given $x$ can lead to more precise probabilistic estimates, and the use of proper scoring rules such as the weighted interval score (WIS) and the continuous ranked probability score (CRPS) lead to better coverage and calibration properties. This paper introduces novel algorithms for learning probabilistic regression trees for the WIS or CRPS loss functions. These algorithms are made computationally efficient thanks to an appropriate use of known data structures - namely min-max heaps, weight-balanced binary trees and Fenwick trees. Through numerical experiments, we demonstrate that the performance of our methods is competitive with alternative approaches. Additionally, our methods benefit from the inherent interpretability and explainability of trees. As a by-product, we show how our trees can be used in the context of conformal prediction and explain why they are particularly well-suited for achieving group-conditional coverage guarantees.

### Deep Dynamic Probabilistic Canonical Correlation Analysis 
[[arxiv](https://arxiv.org/abs/2502.05155)] [[cool](https://papers.cool/arxiv/2502.05155)] [[pdf](https://arxiv.org/pdf/2502.05155)]
> **Authors**: Shiqin Tang,Shujian Yu,Yining Dong,S. Joe Qin
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: accepted by ICASSP-25, code is available at \url{https://github.com/marcusstang/D2PCCA}
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: This paper presents Deep Dynamic Probabilistic Canonical Correlation Analysis (D2PCCA), a model that integrates deep learning with probabilistic modeling to analyze nonlinear dynamical systems. Building on the probabilistic extensions of Canonical Correlation Analysis (CCA), D2PCCA captures nonlinear latent dynamics and supports enhancements such as KL annealing for improved convergence and normalizing flows for a more flexible posterior approximation. D2PCCA naturally extends to multiple observed variables, making it a versatile tool for encoding prior knowledge about sequential datasets and providing a probabilistic understanding of the system's dynamics. Experimental validation on real financial datasets demonstrates the effectiveness of D2PCCA and its extensions in capturing latent dynamics.

### From Restless to Contextual: A Thresholding Bandit Approach to Improve Finite-horizon Performance 
[[arxiv](https://arxiv.org/abs/2502.05145)] [[cool](https://papers.cool/arxiv/2502.05145)] [[pdf](https://arxiv.org/pdf/2502.05145)]
> **Authors**: Jiamin Xu,Ivan Nazarov,Aditya Rastogi,África Periáñez,Kyra Gan
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Online restless bandits extend classic contextual bandits by incorporating state transitions and budget constraints, representing each agent as a Markov Decision Process (MDP). This framework is crucial for finite-horizon strategic resource allocation, optimizing limited costly interventions for long-term benefits. However, learning the underlying MDP for each agent poses a major challenge in finite-horizon settings. To facilitate learning, we reformulate the problem as a scalable budgeted thresholding contextual bandit problem, carefully integrating the state transitions into the reward design and focusing on identifying agents with action benefits exceeding a threshold. We establish the optimality of an oracle greedy solution in a simple two-state setting, and propose an algorithm that achieves minimax optimal constant regret in the online multi-state setting with heterogeneous agents and knowledge of outcomes under no intervention. We numerically show that our algorithm outperforms existing online restless bandit methods, offering significant improvements in finite-horizon performance.

### Data-Parallel Neural Network Training via Nonlinearly Preconditioned Trust-Region Method 
[[arxiv](https://arxiv.org/abs/2502.05133)] [[cool](https://papers.cool/arxiv/2502.05133)] [[pdf](https://arxiv.org/pdf/2502.05133)]
> **Authors**: Samuel A. Cruz Alegría,Ken Trotti,Alena Kopaničáková,Rolf Krause
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 8 pages, 6 figures
- **标题**: None
- **领域**: 机器学习,数值分析
- **Abstract**: Parallel training methods are increasingly relevant in machine learning (ML) due to the continuing growth in model and dataset sizes. We propose a variant of the Additively Preconditioned Trust-Region Strategy (APTS) for training deep neural networks (DNNs). The proposed APTS method utilizes a data-parallel approach to construct a nonlinear preconditioner employed in the nonlinear optimization strategy. In contrast to the common employment of Stochastic Gradient Descent (SGD) and Adaptive Moment Estimation (Adam), which are both variants of gradient descent (GD) algorithms, the APTS method implicitly adjusts the step sizes in each iteration, thereby removing the need for costly hyperparameter tuning. We demonstrate the performance of the proposed APTS variant using the MNIST and CIFAR-10 datasets. The results obtained indicate that the APTS variant proposed here achieves comparable validation accuracy to SGD and Adam, all while allowing for parallel training and obviating the need for expensive hyperparameter tuning.

### SpecTUS: Spectral Translator for Unknown Structures annotation from EI-MS spectra 
[[arxiv](https://arxiv.org/abs/2502.05114)] [[cool](https://papers.cool/arxiv/2502.05114)] [[pdf](https://arxiv.org/pdf/2502.05114)]
> **Authors**: Adam Hájek,Helge Hecht,Elliott J. Price,Aleš Křenek
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,数据分析、统计和概率
- **Abstract**: Compound identification and structure annotation from mass spectra is a well-established task widely applied in drug detection, criminal forensics, small molecule biomarker discovery and chemical engineering. We propose SpecTUS: Spectral Translator for Unknown Structures, a deep neural model that addresses the task of structural annotation of small molecules from low-resolution gas chromatography electron ionization mass spectra (GC-EI-MS). Our model analyzes the spectra in \textit{de novo} manner -- a direct translation from the spectra into 2D-structural representation. Our approach is particularly useful for analyzing compounds unavailable in spectral libraries. In a rigorous evaluation of our model on the novel structure annotation task across different libraries, we outperformed standard database search techniques by a wide margin. On a held-out testing set, including \numprint{28267} spectra from the NIST database, we show that our model's single suggestion perfectly reconstructs 43\% of the subset's compounds. This single suggestion is strictly better than the candidate of the database hybrid search (common method among practitioners) in 76\% of cases. In a~still affordable scenario of~10 suggestions, perfect reconstruction is achieved in 65\%, and 84\% are better than the hybrid search.

### Graph Contrastive Learning for Connectome Classification 
[[arxiv](https://arxiv.org/abs/2502.05109)] [[cool](https://papers.cool/arxiv/2502.05109)] [[pdf](https://arxiv.org/pdf/2502.05109)]
> **Authors**: Martín Schmidt,Sara Silva,Federico Larroca,Gonzalo Mateos,Pablo Musé
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: Submitted to EMBC '25
- **标题**: None
- **领域**: 机器学习
- **Abstract**: With recent advancements in non-invasive techniques for measuring brain activity, such as magnetic resonance imaging (MRI), the study of structural and functional brain networks through graph signal processing (GSP) has gained notable prominence. GSP stands as a key tool in unraveling the interplay between the brain's function and structure, enabling the analysis of graphs defined by the connections between regions of interest -- referred to as connectomes in this context. Our work represents a further step in this direction by exploring supervised contrastive learning methods within the realm of graph representation learning. The main objective of this approach is to generate subject-level (i.e., graph-level) vector representations that bring together subjects sharing the same label while separating those with different labels. These connectome embeddings are derived from a graph neural network Encoder-Decoder architecture, which jointly considers structural and functional connectivity. By leveraging data augmentation techniques, the proposed framework achieves state-of-the-art performance in a gender classification task using Human Connectome Project data. More broadly, our connectome-centric methodological advances support the promising prospect of using GSP to discover more about brain function, with potential impact to understanding heterogeneity in the neurodegeneration for precision medicine and diagnosis.

### Leveraging Hypernetworks and Learnable Kernels for Consumer Energy Forecasting Across Diverse Consumer Types 
[[arxiv](https://arxiv.org/abs/2502.05104)] [[cool](https://papers.cool/arxiv/2502.05104)] [[pdf](https://arxiv.org/pdf/2502.05104)]
> **Authors**: Muhammad Umair Danish,Katarina Grolinger
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: ef:IEEE Transactions on Power Delivery, Volume 40, 2025, Pages 75-87
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Consumer energy forecasting is essential for managing energy consumption and planning, directly influencing operational efficiency, cost reduction, personalized energy management, and sustainability efforts. In recent years, deep learning techniques, especially LSTMs and transformers, have been greatly successful in the field of energy consumption forecasting. Nevertheless, these techniques have difficulties in capturing complex and sudden variations, and, moreover, they are commonly examined only on a specific type of consumer (e.g., only offices, only schools). Consequently, this paper proposes HyperEnergy, a consumer energy forecasting strategy that leverages hypernetworks for improved modeling of complex patterns applicable across a diversity of consumers. Hypernetwork is responsible for predicting the parameters of the primary prediction network, in our case LSTM. A learnable adaptable kernel, comprised of polynomial and radial basis function kernels, is incorporated to enhance performance. The proposed HyperEnergy was evaluated on diverse consumers including, student residences, detached homes, a home with electric vehicle charging, and a townhouse. Across all consumer types, HyperEnergy consistently outperformed 10 other techniques, including state-of-the-art models such as LSTM, AttentionLSTM, and transformer.

### Mitigating Unintended Memorization with LoRA in Federated Learning for LLMs 
[[arxiv](https://arxiv.org/abs/2502.05087)] [[cool](https://papers.cool/arxiv/2502.05087)] [[pdf](https://arxiv.org/pdf/2502.05087)]
> **Authors**: Thierry Bossy,Julien Vignoud,Tahseen Rabbani,Juan R. Troncoso Pastoriza,Martin Jaggi
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Federated learning (FL) is a popular paradigm for collaborative training which avoids direct data exposure between clients. However, data privacy issues still remain: FL-trained large language models are capable of memorizing and completing phrases and sentences contained in training data when given with their prefixes. Thus, it is possible for adversarial and honest-but-curious clients to recover training data of other participants simply through targeted prompting. In this work, we demonstrate that a popular and simple fine-tuning strategy, low-rank adaptation (LoRA), reduces memorization during FL up to a factor of 10. We study this effect by performing a medical question-answering fine-tuning task and injecting multiple replicas of out-of-distribution sensitive sequences drawn from an external clinical dataset. We observe a reduction in memorization for a wide variety of Llama 2 and 3 models, and find that LoRA can reduce memorization in centralized learning as well. Furthermore, we show that LoRA can be combined with other privacy-preserving techniques such as gradient clipping and Gaussian noising, secure aggregation, and Goldfish loss to further improve record-level privacy while maintaining performance.

### Causality can systematically address the monsters under the bench(marks) 
[[arxiv](https://arxiv.org/abs/2502.05085)] [[cool](https://papers.cool/arxiv/2502.05085)] [[pdf](https://arxiv.org/pdf/2502.05085)]
> **Authors**: Felix Leeb,Zhijing Jin,Bernhard Schölkopf
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Effective and reliable evaluation is essential for advancing empirical machine learning. However, the increasing accessibility of generalist models and the progress towards ever more complex, high-level tasks make systematic evaluation more challenging. Benchmarks are plagued by various biases, artifacts, or leakage, while models may behave unreliably due to poorly explored failure modes. Haphazard treatments and inconsistent formulations of such "monsters" can contribute to a duplication of efforts, a lack of trust in results, and unsupported inferences. In this position paper, we argue causality offers an ideal framework to systematically address these challenges. By making causal assumptions in an approach explicit, we can faithfully model phenomena, formulate testable hypotheses with explanatory power, and leverage principled tools for analysis. To make causal model design more accessible, we identify several useful Common Abstract Topologies (CATs) in causal graphs which help gain insight into the reasoning abilities in large language models. Through a series of case studies, we demonstrate how the precise yet pragmatic language of causality clarifies the strengths and limitations of a method and inspires new approaches for systematic progress.

### Paying Attention to Facts: Quantifying the Knowledge Capacity of Attention Layers 
[[arxiv](https://arxiv.org/abs/2502.05076)] [[cool](https://papers.cool/arxiv/2502.05076)] [[pdf](https://arxiv.org/pdf/2502.05076)]
> **Authors**: Liang Ze Wong
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: In this paper, we investigate the ability of single-layer attention-only transformers (i.e. attention layers) to memorize facts contained in databases from a linear-algebraic perspective. We associate with each database a 3-tensor, propose the rank of this tensor as a measure of the size of the database, and provide bounds on the rank in terms of properties of the database. We also define a 3-tensor corresponding to an attention layer, and empirically demonstrate the relationship between its rank and database rank on a dataset of toy models and random databases. By highlighting the roles played by the value-output and query-key weights, and the effects of argmax and softmax on rank, our results shed light on the `additive motif' of factual recall in transformers, while also suggesting a way of increasing layer capacity without increasing the number of parameters.

### Discrepancies are Virtue: Weak-to-Strong Generalization through Lens of Intrinsic Dimension 
[[arxiv](https://arxiv.org/abs/2502.05075)] [[cool](https://papers.cool/arxiv/2502.05075)] [[pdf](https://arxiv.org/pdf/2502.05075)]
> **Authors**: Yijun Dong,Yicheng Li,Yunai Li,Jason D. Lee,Qi Lei
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,数值分析,机器学习
- **Abstract**: Weak-to-strong (W2S) generalization is a type of finetuning (FT) where a strong (large) student model is trained on pseudo-labels generated by a weak teacher. Surprisingly, W2S FT often outperforms the weak teacher. We seek to understand this phenomenon through the observation that FT often occurs in intrinsically low-dimensional spaces. Leveraging the low intrinsic dimensionality of FT, we analyze W2S in the ridgeless regression setting from a variance reduction perspective. For a strong student - weak teacher pair with sufficiently expressive low-dimensional feature subspaces $\mathcal{V}_s, \mathcal{V}_w$, we provide an exact characterization of the variance that dominates the generalization error of W2S. This unveils a virtue of discrepancy between the strong and weak models in W2S: the variance of the weak teacher is inherited by the strong student in $\mathcal{V}_s \cap \mathcal{V}_w$, while reduced by a factor of $\dim(\mathcal{V}_s)/N$ in the subspace of discrepancy $\mathcal{V}_w \setminus \mathcal{V}_s$ with $N$ pseudo-labels for W2S. Further, our analysis casts light on the sample complexities and the scaling of performance gap recovery in W2S. The analysis is supported with experiments on both synthetic regression problems and real vision tasks.

### Preference-aware compensation policies for crowdsourced on-demand services 
[[arxiv](https://arxiv.org/abs/2502.05060)] [[cool](https://papers.cool/arxiv/2502.05060)] [[pdf](https://arxiv.org/pdf/2502.05060)]
> **Authors**: Georgina Nouli,Axel Parmentier,Maximilian Schiffer
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,优化与控制
- **Abstract**: Crowdsourced on-demand services offer benefits such as reduced costs, faster service fulfillment times, greater adaptability, and contributions to sustainable urban transportation in on-demand delivery contexts. However, the success of an on-demand platform that utilizes crowdsourcing relies on finding a compensation policy that strikes a balance between creating attractive offers for gig workers and ensuring profitability. In this work, we examine a dynamic pricing problem for an on-demand platform that sets request-specific compensation of gig workers in a discrete-time framework, where requests and workers arrive stochastically. The operator's goal is to determine a compensation policy that maximizes the total expected reward over the time horizon. Our approach introduces compensation strategies that explicitly account for gig worker request preferences. To achieve this, we employ the Multinomial Logit model to represent the acceptance probabilities of gig workers, and, as a result, derive an analytical solution that utilizes post-decision states. Subsequently, we integrate this solution into an approximate dynamic programming algorithm. We compare our algorithm against benchmark algorithms, including formula-based policies and an upper bound provided by the full information linear programming solution. Our algorithm demonstrates consistent performance across diverse settings, achieving improvements of at least 2.5-7.5% in homogeneous gig worker populations and 9% in heterogeneous populations over benchmarks, based on fully synthetic data. For real-world data, it surpasses benchmarks by 8% in weak and 20% in strong location preference scenarios.

### Hybrid machine learning based scale bridging framework for permeability prediction of fibrous structures 
[[arxiv](https://arxiv.org/abs/2502.05044)] [[cool](https://papers.cool/arxiv/2502.05044)] [[pdf](https://arxiv.org/pdf/2502.05044)]
> **Authors**: Denis Korolev,Tim Schmidt,Dinesh K. Natarajan,Stefano Cassola,David May,Miro Duhovic,Michael Hintermüller
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: This study introduces a hybrid machine learning-based scale-bridging framework for predicting the permeability of fibrous textile structures. By addressing the computational challenges inherent to multiscale modeling, the proposed approach evaluates the efficiency and accuracy of different scale-bridging methodologies combining traditional surrogate models and even integrating physics-informed neural networks (PINNs) with numerical solvers, enabling accurate permeability predictions across micro- and mesoscales. Four methodologies were evaluated: Single Scale Method (SSM), Simple Upscaling Method (SUM), Scale-Bridging Method (SBM), and Fully Resolved Model (FRM). SSM, the simplest method, neglects microscale permeability and exhibited permeability values deviating by up to 150\% of the FRM model, which was taken as ground truth at an equivalent lower fiber volume content. SUM improved predictions by considering uniform microscale permeability, yielding closer values under similar conditions, but still lacked structural variability. The SBM method, incorporating segment-based microscale permeability assignments, showed significant enhancements, achieving almost equivalent values while maintaining computational efficiency and modeling runtimes of ~45 minutes per simulation. In contrast, FRM, which provides the highest fidelity by fully resolving microscale and mesoscale geometries, required up to 270 times more computational time than SSM, with model files exceeding 300 GB. Additionally, a hybrid dual-scale solver incorporating PINNs has been developed and shows the potential to overcome generalization errors and the problem of data scarcity of the data-driven surrogate approaches. The hybrid framework advances permeability modelling by balancing computational cost and prediction reliability, laying the foundation for further applications in fibrous composite manufacturing.

### Federated Learning for Anomaly Detection in Energy Consumption Data: Assessing the Vulnerability to Adversarial Attacks 
[[arxiv](https://arxiv.org/abs/2502.05041)] [[cool](https://papers.cool/arxiv/2502.05041)] [[pdf](https://arxiv.org/pdf/2502.05041)]
> **Authors**: Yohannis Kifle Telila,Damitha Senevirathne,Dumindu Tissera,Apurva Narayan,Miriam A. M. Capretz,Katarina Grolinger
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 12th IEEE Conference on Technologies for Sustainability
- **标题**: None
- **领域**: 机器学习,人工智能,分布式、并行和集群计算
- **Abstract**: Anomaly detection is crucial in the energy sector to identify irregular patterns indicating equipment failures, energy theft, or other issues. Machine learning techniques for anomaly detection have achieved great success, but are typically centralized, involving sharing local data with a central server which raises privacy and security concerns. Federated Learning (FL) has been gaining popularity as it enables distributed learning without sharing local data. However, FL depends on neural networks, which are vulnerable to adversarial attacks that manipulate data, leading models to make erroneous predictions. While adversarial attacks have been explored in the image domain, they remain largely unexplored in time series problems, especially in the energy domain. Moreover, the effect of adversarial attacks in the FL setting is also mostly unknown. This paper assesses the vulnerability of FL-based anomaly detection in energy data to adversarial attacks. Specifically, two state-of-the-art models, Long Short Term Memory (LSTM) and Transformers, are used to detect anomalies in an FL setting, and two white-box attack methods, Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD), are employed to perturb the data. The results show that FL is more sensitive to PGD attacks than to FGSM attacks, attributed to PGD's iterative nature, resulting in an accuracy drop of over 10% even with naive, weaker attacks. Moreover, FL is more affected by these attacks than centralized learning, highlighting the need for defense mechanisms in FL.

### Leveraging a Simulator for Learning Causal Representations from Post-Treatment Covariates for CATE 
[[arxiv](https://arxiv.org/abs/2502.05037)] [[cool](https://papers.cool/arxiv/2502.05037)] [[pdf](https://arxiv.org/pdf/2502.05037)]
> **Authors**: Lokesh Nagalapatti,Pranava Singhal,Avishek Ghosh,Sunita Sarawagi
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: Accepted at TMLR-25
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Treatment effect estimation involves assessing the impact of different treatments on individual outcomes. Current methods estimate Conditional Average Treatment Effect (CATE) using observational datasets where covariates are collected before treatment assignment and outcomes are observed afterward, under assumptions like positivity and unconfoundedness. In this paper, we address a scenario where both covariates and outcomes are gathered after treatment. We show that post-treatment covariates render CATE unidentifiable, and recovering CATE requires learning treatment-independent causal representations. Prior work shows that such representations can be learned through contrastive learning if counterfactual supervision is available in observational data. However, since counterfactuals are rare, other works have explored using simulators that offer synthetic counterfactual supervision. Our goal in this paper is to systematically analyze the role of simulators in estimating CATE. We analyze the CATE error of several baselines and highlight their limitations. We then establish a generalization bound that characterizes the CATE error from jointly training on real and simulated distributions, as a function of the real-simulator mismatch. Finally, we introduce SimPONet, a novel method whose loss function is inspired from our generalization bound. We further show how SimPONet adjusts the simulator's influence on the learning objective based on the simulator's relevance to the CATE task. We experiment with various DGPs, by systematically varying the real-simulator distribution gap to evaluate SimPONet's efficacy against state-of-the-art CATE baselines.

### News about Global North considered Truthful! The Geo-political Veracity Gradient in Global South News 
[[arxiv](https://arxiv.org/abs/2502.05032)] [[cool](https://papers.cool/arxiv/2502.05032)] [[pdf](https://arxiv.org/pdf/2502.05032)]
> **Authors**: Sujit Mandava,Deepak P,Sahely Bhadra
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: While there has been much research into developing AI techniques for fake news detection aided by various benchmark datasets, it has often been pointed out that fake news in different geo-political regions traces different contours. In this work we uncover, through analytical arguments and empirical evidence, the existence of an important characteristic in news originating from the Global South viz., the geo-political veracity gradient. In particular, we show that Global South news about topics from Global North -- such as news from an Indian news agency on US elections -- tend to be less likely to be fake. Observing through the prism of the political economy of fake news creation, we posit that this pattern could be due to the relative lack of monetarily aligned incentives in producing fake news about a different region than the regional remit of the audience. We provide empirical evidence for this from benchmark datasets. We also empirically analyze the consequences of this effect in applying AI-based fake news detection models for fake news AI trained on one region within another regional context. We locate our work within emerging critical scholarship on geo-political biases within AI in general, particularly with AI usage in fake news identification; we hope our insight into the geo-political veracity gradient could help steer fake news AI scholarship towards positively impacting Global South societies.

### Analog and Multi-modal Manufacturing Datasets Acquired on the Future Factories Platform V2 
[[arxiv](https://arxiv.org/abs/2502.05020)] [[cool](https://papers.cool/arxiv/2502.05020)] [[pdf](https://arxiv.org/pdf/2502.05020)]
> **Authors**: Ramy Harik,Fadi El Kalach,Jad Samaha,Philip Samaha,Devon Clark,Drew Sander,Liam Burns,Ibrahim Yousif,Victor Gadow,Ahmed Mahmoud,Thorsten Wuest
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: This paper presents two industry-grade datasets captured during an 8-hour continuous operation of the manufacturing assembly line at the Future Factories Lab, University of South Carolina, on 08/13/2024. The datasets adhere to industry standards, covering communication protocols, actuators, control mechanisms, transducers, sensors, and cameras. Data collection utilized both integrated and external sensors throughout the laboratory, including sensors embedded within the actuators and externally installed devices. Additionally, high-performance cameras captured key aspects of the operation. In a prior experiment [1], a 30-hour continuous run was conducted, during which all anomalies were documented. Maintenance procedures were subsequently implemented to reduce potential errors and operational disruptions. The two datasets include: (1) a time-series analog dataset, and (2) a multi-modal time-series dataset containing synchronized system data and images. These datasets aim to support future research in advancing manufacturing processes by providing a platform for testing novel algorithms without the need to recreate physical manufacturing environments. Moreover, the datasets are open-source and designed to facilitate the training of artificial intelligence models, streamlining research by offering comprehensive, ready-to-use resources for various applications and projects.

### $O(\sqrt{T})$ Static Regret and Instance Dependent Constraint Violation for Constrained Online Convex Optimization 
[[arxiv](https://arxiv.org/abs/2502.05019)] [[cool](https://papers.cool/arxiv/2502.05019)] [[pdf](https://arxiv.org/pdf/2502.05019)]
> **Authors**: Rahul Vaze,Abhishek Sinha
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,数据结构和算法
- **Abstract**: The constrained version of the standard online convex optimization (OCO) framework, called COCO is considered, where on every round, a convex cost function and a convex constraint function are revealed to the learner after it chooses the action for that round. The objective is to simultaneously minimize the static regret and cumulative constraint violation (CCV). An algorithm is proposed that guarantees a static regret of $O(\sqrt{T})$ and a CCV of $\min\{\cV, O(\sqrt{T}\log T) \}$, where $\cV$ depends on the distance between the consecutively revealed constraint sets, the shape of constraint sets, dimension of action space and the diameter of the action space. For special cases of constraint sets, $\cV=O(1)$. Compared to the state of the art results, static regret of $O(\sqrt{T})$ and CCV of $O(\sqrt{T}\log T)$, that were universal, the new result on CCV is instance dependent, which is derived by exploiting the geometric properties of the constraint sets.

### Seasonal Station-Keeping of Short Duration High Altitude Balloons using Deep Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.05014)] [[cool](https://papers.cool/arxiv/2502.05014)] [[pdf](https://arxiv.org/pdf/2502.05014)]
> **Authors**: Tristan K. Schuler,Chinthan Prasad,Georgiy Kiselev,Donald Sofge
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器人技术,大气和海洋物理
- **Abstract**: Station-Keeping short-duration high-altitude balloons (HABs) in a region of interest is a challenging path-planning problem due to partially observable, complex, and dynamic wind flows. Deep reinforcement learning is a popular strategy for solving the station-keeping problem. A custom simulation environment was developed to train and evaluate Deep Q-Learning (DQN) for short-duration HAB agents in the simulation. To train the agents on realistic winds, synthetic wind forecasts were generated from aggregated historical radiosonde data to apply horizontal kinematics to simulated agents. The synthetic forecasts were closely correlated with ECWMF ERA5 Reanalysis forecasts, providing a realistic simulated wind field and seasonal and altitudinal variances between the wind models. DQN HAB agents were then trained and evaluated across different seasonal months. To highlight differences and trends in months with vastly different wind fields, a Forecast Score algorithm was introduced to independently classify forecasts based on wind diversity, and trends between station-keeping success and the Forecast Score were evaluated across all seasons.

### Learning the Language of NVMe Streams for Ransomware Detection 
[[arxiv](https://arxiv.org/abs/2502.05011)] [[cool](https://papers.cool/arxiv/2502.05011)] [[pdf](https://arxiv.org/pdf/2502.05011)]
> **Authors**: Barak Bringoltz,Elisha Halperin,Ran Feraru,Evgeny Blaichman,Amit Berman
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 25 pages, 8 figures
- **标题**: None
- **领域**: 机器学习,密码学和安全
- **Abstract**: We apply language modeling techniques to detect ransomware activity in NVMe command sequences. We design and train two types of transformer-based models: the Command-Level Transformer (CLT) performs in-context token classification to determine whether individual commands are initiated by ransomware, and the Patch-Level Transformer (PLT) predicts the volume of data accessed by ransomware within a patch of commands. We present both model designs and the corresponding tokenization and embedding schemes and show that they improve over state-of-the-art tabular methods by up to 24% in missed-detection rate, 66% in data loss prevention, and 84% in identifying data accessed by ransomware.

### QuEST: Stable Training of LLMs with 1-Bit Weights and Activations 
[[arxiv](https://arxiv.org/abs/2502.05003)] [[cool](https://papers.cool/arxiv/2502.05003)] [[pdf](https://arxiv.org/pdf/2502.05003)]
> **Authors**: Andrei Panferov,Jiale Chen,Soroush Tabesh,Roberto L. Castro,Mahdi Nikdan,Dan Alistarh
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: One approach to reducing the massive costs of large language models (LLMs) is the use of quantized or sparse representations for training or deployment. While post-training compression methods are very popular, the question of obtaining even more accurate compressed models by directly training over such representations, i.e., Quantization-Aware Training (QAT), is still open: for example, a recent study (arXiv:2411.04330v2) put the "optimal" bit-width at which models can be trained using QAT, while staying accuracy-competitive with standard FP16/BF16 precision, at 8-bits weights and activations. We advance this state-of-the-art via a new method called QuEST, which is Pareto-competitive with FP16, i.e., it provides better accuracy at lower model size, while training models with weights and activations in 4-bits or less. Moreover, QuEST allows stable training with 1-bit weights and activations. QuEST achieves this by improving two key aspects of QAT methods: (1) accurate and fast quantization of the (continuous) distributions of weights and activations via Hadamard normalization and MSE-optimal fitting; (2) a new trust gradient estimator based on the idea of explicitly minimizing the error between the noisy gradient computed over quantized states and the "true" (but unknown) full-precision gradient. Experiments on Llama-type architectures show that QuEST induces stable scaling laws across the entire range of hardware-supported precisions, and can be extended to sparse representations. We provide GPU kernel support showing that models produced by QuEST can be executed efficiently. Our code is available at https://github.com/IST-DASLab/QuEST.

### Robust Graph Learning Against Adversarial Evasion Attacks via Prior-Free Diffusion-Based Structure Purification 
[[arxiv](https://arxiv.org/abs/2502.05000)] [[cool](https://papers.cool/arxiv/2502.05000)] [[pdf](https://arxiv.org/pdf/2502.05000)]
> **Authors**: Jiayi Luo,Qingyun Sun,Haonan Yuan,Xingcheng Fu,Jianxin Li
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: Accepted for poster at WWW 2025
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Adversarial evasion attacks pose significant threats to graph learning, with lines of studies that have improved the robustness of Graph Neural Networks (GNNs). However, existing works rely on priors about clean graphs or attacking strategies, which are often heuristic and inconsistent. To achieve robust graph learning over different types of evasion attacks and diverse datasets, we investigate this problem from a prior-free structure purification perspective. Specifically, we propose a novel Diffusion-based Structure Purification framework named DiffSP, which creatively incorporates the graph diffusion model to learn intrinsic distributions of clean graphs and purify the perturbed structures by removing adversaries under the direction of the captured predictive patterns without relying on priors. DiffSP is divided into the forward diffusion process and the reverse denoising process, during which structure purification is achieved. To avoid valuable information loss during the forward process, we propose an LID-driven nonisotropic diffusion mechanism to selectively inject noise anisotropically. To promote semantic alignment between the clean graph and the purified graph generated during the reverse process, we reduce the generation uncertainty by the proposed graph transfer entropy guided denoising mechanism. Extensive experiments demonstrate the superior robustness of DiffSP against evasion attacks.

### Enhancing Pre-Trained Decision Transformers with Prompt-Tuning Bandits 
[[arxiv](https://arxiv.org/abs/2502.04979)] [[cool](https://papers.cool/arxiv/2502.04979)] [[pdf](https://arxiv.org/pdf/2502.04979)]
> **Authors**: Finn Rietz,Oleg Smirnov,Sara Karimi,Lele Cao
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Harnessing large offline datasets is vital for training foundation models that can generalize across diverse tasks. Offline Reinforcement Learning (RL) offers a powerful framework for these scenarios, enabling the derivation of optimal policies even from suboptimal data. The Prompting Decision Transformer (PDT) is an offline RL multi-task model that distinguishes tasks through stochastic trajectory prompts, which are task-specific tokens maintained in context during rollouts. However, PDT samples these tokens uniformly at random from per-task demonstration datasets, failing to account for differences in token informativeness and potentially leading to performance degradation. To address this limitation, we introduce a scalable bandit-based prompt-tuning method that dynamically learns to construct high-performance trajectory prompts. Our approach significantly enhances downstream task performance without modifying the pre-trained Transformer backbone. Empirical results on benchmark tasks and a newly designed multi-task environment demonstrate the effectiveness of our method, creating a seamless bridge between general multi-task offline pre-training and task-specific online adaptation.

### DE-PADA: Personalized Augmentation and Domain Adaptation for ECG Biometrics Across Physiological States 
[[arxiv](https://arxiv.org/abs/2502.04973)] [[cool](https://papers.cool/arxiv/2502.04973)] [[pdf](https://arxiv.org/pdf/2502.04973)]
> **Authors**: Amro Abu Saleh,Elliot Sprecher,Kfir Y. Levy,Daniel H. Lange
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Electrocardiogram (ECG)-based biometrics offer a promising method for user identification, combining intrinsic liveness detection with morphological uniqueness. However, elevated heart rates introduce significant physiological variability, posing challenges to pattern recognition systems and leading to a notable performance gap between resting and post-exercise conditions. Addressing this gap is critical for advancing ECG-based biometric systems for real-world applications. We propose DE-PADA, a Dual Expert model with Personalized Augmentation and Domain Adaptation, designed to enhance robustness across diverse physiological states. The model is trained primarily on resting-state data from the evaluation dataset, without direct exposure to their exercise data. To address variability, DE-PADA incorporates ECG-specific innovations, including heartbeat segmentation into the PQRS interval, known for its relative temporal consistency, and the heart rate-sensitive ST interval, enabling targeted feature extraction tailored to each region's unique characteristics. Personalized augmentation simulates subject-specific T-wave variability across heart rates using individual T-wave peak predictions to adapt augmentation ranges. Domain adaptation further improves generalization by leveraging auxiliary data from supplementary subjects used exclusively for training, including both resting and exercise conditions. Experiments on the University of Toronto ECG Database demonstrate the model's effectiveness. DE-PADA achieves relative improvements in post-exercise identification rates of 26.75% in the initial recovery phase and 11.72% in the late recovery phase, while maintaining a 98.12% identification rate in the sitting position. These results highlight DE-PADA's ability to address intra-subject variability and enhance the robustness of ECG-based biometric systems across diverse physiological states.

### Fast Adaptive Anti-Jamming Channel Access via Deep Q Learning and Coarse-Grained Spectrum Prediction 
[[arxiv](https://arxiv.org/abs/2502.04963)] [[cool](https://papers.cool/arxiv/2502.04963)] [[pdf](https://arxiv.org/pdf/2502.04963)]
> **Authors**: Jianshu Zhang,Xiaofu Wu,Junquan Hu
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: This paper investigates the anti-jamming channel access problem in complex and unknown jamming environments, where the jammer could dynamically adjust its strategies to target different channels. Traditional channel hopping anti-jamming approaches using fixed patterns are ineffective against such dynamic jamming attacks. Although the emerging deep reinforcement learning (DRL) based dynamic channel access approach could achieve the Nash equilibrium under fast-changing jamming attacks, it requires extensive training episodes. To address this issue, we propose a fast adaptive anti-jamming channel access approach guided by the intuition of ``learning faster than the jammer", where a synchronously updated coarse-grained spectrum prediction serves as an auxiliary task for the deep Q learning (DQN) based anti-jamming model. This helps the model identify a superior Q-function compared to standard DRL while significantly reducing the number of training episodes. Numerical results indicate that the proposed approach significantly accelerates the rate of convergence in model training, reducing the required training episodes by up to 70% compared to standard DRL. Additionally, it also achieves a 10% improvement in throughput over NE strategies, owing to the effective use of coarse-grained spectrum prediction.

### No Task Left Behind: Isotropic Model Merging with Common and Task-Specific Subspaces 
[[arxiv](https://arxiv.org/abs/2502.04959)] [[cool](https://papers.cool/arxiv/2502.04959)] [[pdf](https://arxiv.org/pdf/2502.04959)]
> **Authors**: Daniel Marczak,Simone Magistri,Sebastian Cygert,Bartłomiej Twardowski,Andrew D. Bagdanov,Joost van de Weijer
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Model merging integrates the weights of multiple task-specific models into a single multi-task model. Despite recent interest in the problem, a significant performance gap between the combined and single-task models remains. In this paper, we investigate the key characteristics of task matrices -- weight update matrices applied to a pre-trained model -- that enable effective merging. We show that alignment between singular components of task-specific and merged matrices strongly correlates with performance improvement over the pre-trained model. Based on this, we propose an isotropic merging framework that flattens the singular value spectrum of task matrices, enhances alignment, and reduces the performance gap. Additionally, we incorporate both common and task-specific subspaces to further improve alignment and performance. Our proposed approach achieves state-of-the-art performance across multiple scenarios, including various sets of tasks and model scales. This work advances the understanding of model merging dynamics, offering an effective methodology to merge models without requiring additional training. Code is available at https://github.com/danielm1405/iso-merging .

### Conformal Prediction for Electricity Price Forecasting in the Day-Ahead and Real-Time Balancing Market 
[[arxiv](https://arxiv.org/abs/2502.04935)] [[cool](https://papers.cool/arxiv/2502.04935)] [[pdf](https://arxiv.org/pdf/2502.04935)]
> **Authors**: Ciaran O'Connor,Mohamed Bahloul,Roberto Rossi,Steven Prestwich,Andrea Visentin
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: The integration of renewable energy into electricity markets poses significant challenges to price stability and increases the complexity of market operations. Accurate and reliable electricity price forecasting is crucial for effective market participation, where price dynamics can be significantly more challenging to predict. Probabilistic forecasting, through prediction intervals, efficiently quantifies the inherent uncertainties in electricity prices, supporting better decision-making for market participants. This study explores the enhancement of probabilistic price prediction using Conformal Prediction (CP) techniques, specifically Ensemble Batch Prediction Intervals and Sequential Predictive Conformal Inference. These methods provide precise and reliable prediction intervals, outperforming traditional models in validity metrics. We propose an ensemble approach that combines the efficiency of quantile regression models with the robust coverage properties of time series adapted CP techniques. This ensemble delivers both narrow prediction intervals and high coverage, leading to more reliable and accurate forecasts. We further evaluate the practical implications of CP techniques through a simulated trading algorithm applied to a battery storage system. The ensemble approach demonstrates improved financial returns in energy trading in both the Day-Ahead and Balancing Markets, highlighting its practical benefits for market participants.

### Generative-enhanced optimization for knapsack problems: an industry-relevant study 
[[arxiv](https://arxiv.org/abs/2502.04928)] [[cool](https://papers.cool/arxiv/2502.04928)] [[pdf](https://arxiv.org/pdf/2502.04928)]
> **Authors**: Yelyzaveta Vodovozova,Abhishek Awasthi,Caitlin Jones,Joseph Doetsch,Karen Wintersperger,Florian Krellner,Carlos A. Riofrío
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,量子物理学
- **Abstract**: Optimization is a crucial task in various industries such as logistics, aviation, manufacturing, chemical, pharmaceutical, and insurance, where finding the best solution to a problem can result in significant cost savings and increased efficiency. Tensor networks (TNs) have gained prominence in recent years in modeling classical systems with quantum-inspired approaches. More recently, TN generative-enhanced optimization (TN-GEO) has been proposed as a strategy which uses generative modeling to efficiently sample valid solutions with respect to certain constraints of optimization problems. Moreover, it has been shown that symmetric TNs (STNs) can encode certain constraints of optimization problems, thus aiding in their solution process. In this work, we investigate the applicability of TN- and STN-GEO to an industry relevant problem class, a multi-knapsack problem, in which each object must be assigned to an available knapsack. We detail a prescription for practitioners to use the TN-and STN-GEO methodology and study its scaling behavior and dependence on its hyper-parameters. We benchmark 60 different problem instances and find that TN-GEO and STN-GEO produce results of similar quality to simulated annealing.

### Complex Physics-Informed Neural Network 
[[arxiv](https://arxiv.org/abs/2502.04917)] [[cool](https://papers.cool/arxiv/2502.04917)] [[pdf](https://arxiv.org/pdf/2502.04917)]
> **Authors**: Chenhao Si,Ming Yan,Xin Li,Zhihong Xia
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 16 pages, 9 figures
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: We propose compleX-PINN, a novel physics-informed neural network (PINN) architecture that incorporates a learnable activation function inspired by Cauchy integral theorem. By learning the parameters of the activation function, compleX-PINN achieves high accuracy with just a single hidden layer. Empirical results show that compleX-PINN effectively solves problems where traditional PINNs struggle and consistently delivers significantly higher precision, often by an order of magnitude.

### On the Power of Heuristics in Temporal Graphs 
[[arxiv](https://arxiv.org/abs/2502.04910)] [[cool](https://papers.cool/arxiv/2502.04910)] [[pdf](https://arxiv.org/pdf/2502.04910)]
> **Authors**: Filip Cornell,Oleg Smirnov,Gabriela Zarzar Gandler,Lele Cao
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Dynamic graph datasets often exhibit strong temporal patterns, such as recency, which prioritizes recent interactions, and popularity, which favors frequently occurring nodes. We demonstrate that simple heuristics leveraging only these patterns can perform on par or outperform state-of-the-art neural network models under standard evaluation protocols. To further explore these dynamics, we introduce metrics that quantify the impact of recency and popularity across datasets. Our experiments on BenchTemp and the Temporal Graph Benchmark show that our approaches achieve state-of-the-art performance across all datasets in the latter and secure top ranks on multiple datasets in the former. These results emphasize the importance of refined evaluation schemes to enable fair comparisons and promote the development of more robust temporal graph models. Additionally, they reveal that current deep learning methods often struggle to capture the key patterns underlying predictions in real-world temporal graphs. For reproducibility, we have made our code publicly available.

### Unified Approaches in Self-Supervised Event Stream Modeling: Progress and Prospects 
[[arxiv](https://arxiv.org/abs/2502.04899)] [[cool](https://papers.cool/arxiv/2502.04899)] [[pdf](https://arxiv.org/pdf/2502.04899)]
> **Authors**: Levente Zólyomi,Tianze Wang,Sofiane Ennadir,Oleg Smirnov,Lele Cao
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: The proliferation of digital interactions across diverse domains, such as healthcare, e-commerce, gaming, and finance, has resulted in the generation of vast volumes of event stream (ES) data. ES data comprises continuous sequences of timestamped events that encapsulate detailed contextual information relevant to each domain. While ES data holds significant potential for extracting actionable insights and enhancing decision-making, its effective utilization is hindered by challenges such as the scarcity of labeled data and the fragmented nature of existing research efforts. Self-Supervised Learning (SSL) has emerged as a promising paradigm to address these challenges by enabling the extraction of meaningful representations from unlabeled ES data. In this survey, we systematically review and synthesize SSL methodologies tailored for ES modeling across multiple domains, bridging the gaps between domain-specific approaches that have traditionally operated in isolation. We present a comprehensive taxonomy of SSL techniques, encompassing both predictive and contrastive paradigms, and analyze their applicability and effectiveness within different application contexts. Furthermore, we identify critical gaps in current research and propose a future research agenda aimed at developing scalable, domain-agnostic SSL frameworks for ES modeling. By unifying disparate research efforts and highlighting cross-domain synergies, this survey aims to accelerate innovation, improve reproducibility, and expand the applicability of SSL to diverse real-world ES challenges.

### Deep Learning Models for Physical Layer Communications 
[[arxiv](https://arxiv.org/abs/2502.04895)] [[cool](https://papers.cool/arxiv/2502.04895)] [[pdf](https://arxiv.org/pdf/2502.04895)]
> **Authors**: Nunzio A. Letizia
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: PhD Thesis
- **标题**: None
- **领域**: 机器学习,信号处理
- **Abstract**: The increased availability of data and computing resources has enabled researchers to successfully adopt machine learning (ML) techniques and make significant contributions in several engineering areas. ML and in particular deep learning (DL) algorithms have shown to perform better in tasks where a physical bottom-up description of the phenomenon is lacking and/or is mathematically intractable. Indeed, they take advantage of the observations of natural phenomena to automatically acquire knowledge and learn internal relations. Despite the historical model-based mindset, communications engineering recently started shifting the focus towards top-down data-driven learning models, especially in domains such as channel modeling and physical layer design, where in most of the cases no general optimal strategies are known. In this thesis, we aim at solving some fundamental open challenges in physical layer communications exploiting new DL paradigms. In particular, we mathematically formulate, under ML terms, classic problems such as channel capacity and optimal coding-decoding schemes, for any arbitrary communication medium. We design and develop the architecture, algorithm and code necessary to train the equivalent DL model, and finally, we propose novel solutions to long-standing problems in the field.

### A Foundational Brain Dynamics Model via Stochastic Optimal Control 
[[arxiv](https://arxiv.org/abs/2502.04892)] [[cool](https://papers.cool/arxiv/2502.04892)] [[pdf](https://arxiv.org/pdf/2502.04892)]
> **Authors**: Joonhyeong Park,Byoungwoo Park,Chang-Bae Bang,Jungwon Choi,Hyungjin Chung,Byung-Hoon Kim,Juho Lee
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: The first two authors contributed equally
- **标题**: None
- **领域**: 机器学习,神经元和认知,机器学习
- **Abstract**: We introduce a foundational model for brain dynamics that utilizes stochastic optimal control (SOC) and amortized inference. Our method features a continuous-discrete state space model (SSM) that can robustly handle the intricate and noisy nature of fMRI signals. To address computational limitations, we implement an approximation strategy grounded in the SOC framework. Additionally, we present a simulation-free latent dynamics approach that employs locally linear approximations, facilitating efficient and scalable inference. For effective representation learning, we derive an Evidence Lower Bound (ELBO) from the SOC formulation, which integrates smoothly with recent advancements in self-supervised learning (SSL), thereby promoting robust and transferable representations. Pre-trained on extensive datasets such as the UKB, our model attains state-of-the-art results across a variety of downstream tasks, including demographic prediction, trait analysis, disease diagnosis, and prognosis. Moreover, evaluating on external datasets such as HCP-A, ABIDE, and ADHD200 further validates its superior abilities and resilience across different demographic and clinical distributions. Our foundational model provides a scalable and efficient approach for deciphering brain dynamics, opening up numerous applications in neuroscience.

### GNNs Getting ComFy: Community and Feature Similarity Guided Rewiring 
[[arxiv](https://arxiv.org/abs/2502.04891)] [[cool](https://papers.cool/arxiv/2502.04891)] [[pdf](https://arxiv.org/pdf/2502.04891)]
> **Authors**: Celia Rubio-Madrigal,Adarsh Jamadandi,Rebekka Burkholz
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: Accepted at ICLR 2025
- **标题**: None
- **领域**: 机器学习,社交和信息网络,机器学习
- **Abstract**: Maximizing the spectral gap through graph rewiring has been proposed to enhance the performance of message-passing graph neural networks (GNNs) by addressing over-squashing. However, as we show, minimizing the spectral gap can also improve generalization. To explain this, we analyze how rewiring can benefit GNNs within the context of stochastic block models. Since spectral gap optimization primarily influences community strength, it improves performance when the community structure aligns with node labels. Building on this insight, we propose three distinct rewiring strategies that explicitly target community structure, node labels, and their alignment: (a) community structure-based rewiring (ComMa), a more computationally efficient alternative to spectral gap optimization that achieves similar goals; (b) feature similarity-based rewiring (FeaSt), which focuses on maximizing global homophily; and (c) a hybrid approach (ComFy), which enhances local feature similarity while preserving community structure to optimize label-community alignment. Extensive experiments confirm the effectiveness of these strategies and support our theoretical insights.

### Exploit Gradient Skewness to Circumvent Byzantine Defenses for Federated Learning 
[[arxiv](https://arxiv.org/abs/2502.04890)] [[cool](https://papers.cool/arxiv/2502.04890)] [[pdf](https://arxiv.org/pdf/2502.04890)]
> **Authors**: Yuchen Liu,Chen Chen,Lingjuan Lyu,Yaochu Jin,Gang Chen
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Federated Learning (FL) is notorious for its vulnerability to Byzantine attacks. Most current Byzantine defenses share a common inductive bias: among all the gradients, the densely distributed ones are more likely to be honest. However, such a bias is a poison to Byzantine robustness due to a newly discovered phenomenon in this paper - gradient skew. We discover that a group of densely distributed honest gradients skew away from the optimal gradient (the average of honest gradients) due to heterogeneous data. This gradient skew phenomenon allows Byzantine gradients to hide within the densely distributed skewed gradients. As a result, Byzantine defenses are confused into believing that Byzantine gradients are honest. Motivated by this observation, we propose a novel skew-aware attack called STRIKE: first, we search for the skewed gradients; then, we construct Byzantine gradients within the skewed gradients. Experiments on three benchmark datasets validate the effectiveness of our attack

### Sparse Autoencoders Do Not Find Canonical Units of Analysis 
[[arxiv](https://arxiv.org/abs/2502.04878)] [[cool](https://papers.cool/arxiv/2502.04878)] [[pdf](https://arxiv.org/pdf/2502.04878)]
> **Authors**: Patrick Leask,Bart Bussmann,Michael Pearce,Joseph Bloom,Curt Tigges,Noura Al Moubayed,Lee Sharkey,Neel Nanda
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: Accepted to ICLR 2025
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: A common goal of mechanistic interpretability is to decompose the activations of neural networks into features: interpretable properties of the input computed by the model. Sparse autoencoders (SAEs) are a popular method for finding these features in LLMs, and it has been postulated that they can be used to find a \textit{canonical} set of units: a unique and complete list of atomic features. We cast doubt on this belief using two novel techniques: SAE stitching to show they are incomplete, and meta-SAEs to show they are not atomic. SAE stitching involves inserting or swapping latents from a larger SAE into a smaller one. Latents from the larger SAE can be divided into two categories: \emph{novel latents}, which improve performance when added to the smaller SAE, indicating they capture novel information, and \emph{reconstruction latents}, which can replace corresponding latents in the smaller SAE that have similar behavior. The existence of novel features indicates incompleteness of smaller SAEs. Using meta-SAEs -- SAEs trained on the decoder matrix of another SAE -- we find that latents in SAEs often decompose into combinations of latents from a smaller SAE, showing that larger SAE latents are not atomic. The resulting decompositions are often interpretable; e.g. a latent representing ``Einstein'' decomposes into ``scientist'', ``Germany'', and ``famous person''. Even if SAEs do not find canonical units of analysis, they may still be useful tools. We suggest that future research should either pursue different approaches for identifying such units, or pragmatically choose the SAE size suited to their task. We provide an interactive dashboard to explore meta-SAEs: https://metasaes.streamlit.app/

### Aequa: Fair Model Rewards in Collaborative Learning via Slimmable Networks 
[[arxiv](https://arxiv.org/abs/2502.04850)] [[cool](https://papers.cool/arxiv/2502.04850)] [[pdf](https://arxiv.org/pdf/2502.04850)]
> **Authors**: Nurbek Tastan,Samuel Horvath,Karthik Nandakumar
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,分布式、并行和集群计算
- **Abstract**: Collaborative learning enables multiple participants to learn a single global model by exchanging focused updates instead of sharing data. One of the core challenges in collaborative learning is ensuring that participants are rewarded fairly for their contributions, which entails two key sub-problems: contribution assessment and reward allocation. This work focuses on fair reward allocation, where the participants are incentivized through model rewards - differentiated final models whose performance is commensurate with the contribution. In this work, we leverage the concept of slimmable neural networks to collaboratively learn a shared global model whose performance degrades gracefully with a reduction in model width. We also propose a post-training fair allocation algorithm that determines the model width for each participant based on their contributions. We theoretically study the convergence of our proposed approach and empirically validate it using extensive experiments on different datasets and architectures. We also extend our approach to enable training-time model reward allocation.

### Memory Capacity of Nonlinear Recurrent Networks: Is it Informative? 
[[arxiv](https://arxiv.org/abs/2502.04832)] [[cool](https://papers.cool/arxiv/2502.04832)] [[pdf](https://arxiv.org/pdf/2502.04832)]
> **Authors**: Giovanni Ballarin,Lyudmila Grigoryeva,Juan-Pablo Ortega
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 8 pages, 1 figure
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: The total memory capacity (MC) of linear recurrent neural networks (RNNs) has been proven to be equal to the rank of the corresponding Kalman controllability matrix, and it is almost surely maximal for connectivity and input weight matrices drawn from regular distributions. This fact questions the usefulness of this metric in distinguishing the performance of linear RNNs in the processing of stochastic signals. This note shows that the MC of random nonlinear RNNs yields arbitrary values within established upper and lower bounds depending just on the input process scale. This confirms that the existing definition of MC in linear and nonlinear cases has no practical value.

### Optimistic Gradient Learning with Hessian Corrections for High-Dimensional Black-Box Optimization 
[[arxiv](https://arxiv.org/abs/2502.04829)] [[cool](https://papers.cool/arxiv/2502.04829)] [[pdf](https://arxiv.org/pdf/2502.04829)]
> **Authors**: Yedidya Kfir,Elad Sarafian,Sarit Kraus,Yoram Louzoun
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: We develop a black-box optimization algorithm that learns gradients withneuralmodels and can be applied to solve non-convex high dimensional real-world problems
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Black-box algorithms are designed to optimize functions without relying on their underlying analytical structure or gradient information, making them essential when gradients are inaccessible or difficult to compute. Traditional methods for solving black-box optimization (BBO) problems predominantly rely on non-parametric models and struggle to scale to large input spaces. Conversely, parametric methods that model the function with neural estimators and obtain gradient signals via backpropagation may suffer from significant gradient errors. A recent alternative, Explicit Gradient Learning (EGL), which directly learns the gradient using a first-order Taylor approximation, has demonstrated superior performance over both parametric and non-parametric methods. In this work, we propose two novel gradient learning variants to address the robustness challenges posed by high-dimensional, complex, and highly non-linear problems. Optimistic Gradient Learning (OGL) introduces a bias toward lower regions in the function landscape, while Higher-order Gradient Learning (HGL) incorporates second-order Taylor corrections to improve gradient accuracy. We combine these approaches into the unified OHGL algorithm, achieving state-of-the-art (SOTA) performance on the synthetic COCO suite. Additionally, we demonstrate OHGLs applicability to high-dimensional real-world machine learning (ML) tasks such as adversarial training and code generation. Our results highlight OHGLs ability to generate stronger candidates, offering a valuable tool for ML researchers and practitioners tackling high-dimensional, non-linear optimization challenges

### Harnessing omnipresent oscillator networks as computational resource 
[[arxiv](https://arxiv.org/abs/2502.04818)] [[cool](https://papers.cool/arxiv/2502.04818)] [[pdf](https://arxiv.org/pdf/2502.04818)]
> **Authors**: Thomas Geert de Jong,Hirofumi Notsu,Kohei Nakajima
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: :92B25ACM Class:I.2.6
- **标题**: None
- **领域**: 机器学习,动力系统,适应和自组织系统,混沌动力学
- **Abstract**: Nature is pervaded with oscillatory behavior. In networks of coupled oscillators patterns can arise when the system synchronizes to an external input. Hence, these networks provide processing and memory of input. We present a universal framework for harnessing oscillator networks as computational resource. This reservoir computing framework is introduced by the ubiquitous model for phase-locking, the Kuramoto model. We force the Kuramoto model by a nonlinear target-system, then after substituting the target-system with a trained feedback-loop it emulates the target-system. Our results are two-fold. Firstly, the trained network inherits performance properties of the Kuramoto model, where all-to-all coupling is performed in linear time with respect to the number of nodes and parameters for synchronization are abundant. Secondly, the learning capabilities of the oscillator network can be explained using Kuramoto model's order parameter. This work provides the foundation for utilizing nature's oscillator networks as a new class of information processing systems.

### Describing Nonstationary Data Streams in Frequency Domain 
[[arxiv](https://arxiv.org/abs/2502.04813)] [[cool](https://papers.cool/arxiv/2502.04813)] [[pdf](https://arxiv.org/pdf/2502.04813)]
> **Authors**: Joanna Komorniczak
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Concept drift is among the primary challenges faced by the data stream processing methods. The drift detection strategies, designed to counteract the negative consequences of such changes, often rely on analyzing the problem metafeatures. This work presents the Frequency Filtering Metadescriptor -- a tool for characterizing the data stream that searches for the informative frequency components visible in the sample's feature vector. The frequencies are filtered according to their variance across all available data batches. The presented solution is capable of generating a metadescription of the data stream, separating chunks into groups describing specific concepts on its basis, and visualizing the frequencies in the original spatial domain. The experimental analysis compared the proposed solution with two state-of-the-art strategies and with the PCA baseline in the post-hoc concept identification task. The research is followed by the identification of concepts in the real-world data streams. The generalization in the frequency domain adapted in the proposed solution allows to capture the complex feature dependencies as a reduced number of frequency components, while maintaining the semantic meaning of data.

### Humans Co-exist, So Must Embodied Artificial Agents 
[[arxiv](https://arxiv.org/abs/2502.04809)] [[cool](https://papers.cool/arxiv/2502.04809)] [[pdf](https://arxiv.org/pdf/2502.04809)]
> **Authors**: Hannah Kuehn,Joseph La Delfa,Miguel Vasco,Danica Kragic,Iolanda Leite
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Modern embodied artificial agents excel in static, predefined tasks but fall short in dynamic and long-term interactions with humans. On the other hand, humans can adapt and evolve continuously, exploiting the situated knowledge embedded in their environment and other agents, thus contributing to meaningful interactions. We introduce the concept of co-existence for embodied artificial agents and argues that it is a prerequisite for meaningful, long-term interaction with humans. We take inspiration from biology and design theory to understand how human and non-human organisms foster entities that co-exist within their specific niches. Finally, we propose key research directions for the machine learning community to foster co-existing embodied agents, focusing on the principles, hardware and learning methods responsible for shaping them.

### Behavior-Regularized Diffusion Policy Optimization for Offline Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.04778)] [[cool](https://papers.cool/arxiv/2502.04778)] [[pdf](https://arxiv.org/pdf/2502.04778)]
> **Authors**: Chen-Xiao Gao,Chenyang Wu,Mingjun Cao,Chenjun Xiao,Yang Yu,Zongzhang Zhang
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: Under review
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: The primary focus of offline reinforcement learning (RL) is to manage the risk of hazardous exploitation of out-of-distribution actions. An effective approach to achieve this goal is through behavior regularization, which augments conventional RL objectives by incorporating constraints that enforce the policy to remain close to the behavior policy. Nevertheless, existing literature on behavior-regularized RL primarily focuses on explicit policy parameterizations, such as Gaussian policies. Consequently, it remains unclear how to extend this framework to more advanced policy parameterizations, such as diffusion models. In this paper, we introduce BDPO, a principled behavior-regularized RL framework tailored for diffusion-based policies, thereby combining the expressive power of diffusion policies and the robustness provided by regularization. The key ingredient of our method is to calculate the Kullback-Leibler (KL) regularization analytically as the accumulated discrepancies in reverse-time transition kernels along the diffusion trajectory. By integrating the regularization, we develop an efficient two-time-scale actor-critic RL algorithm that produces the optimal policy while respecting the behavior constraint. Comprehensive evaluations conducted on synthetic 2D tasks and continuous control tasks from the D4RL benchmark validate its effectiveness and superior performance.

### An Extended Benchmarking of Multi-Agent Reinforcement Learning Algorithms in Complex Fully Cooperative Tasks 
[[arxiv](https://arxiv.org/abs/2502.04773)] [[cool](https://papers.cool/arxiv/2502.04773)] [[pdf](https://arxiv.org/pdf/2502.04773)]
> **Authors**: George Papadopoulos,Andreas Kontogiannis,Foteini Papadopoulou,Chaido Poulianou,Ioannis Koumentis,George Vouros
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Multi-Agent Reinforcement Learning (MARL) has recently emerged as a significant area of research. However, MARL evaluation often lacks systematic diversity, hindering a comprehensive understanding of algorithms' capabilities. In particular, cooperative MARL algorithms are predominantly evaluated on benchmarks such as SMAC and GRF, which primarily feature team game scenarios without assessing adequately various aspects of agents' capabilities required in fully cooperative real-world tasks such as multi-robot cooperation and warehouse, resource management, search and rescue, and human-AI cooperation. Moreover, MARL algorithms are mainly evaluated on low dimensional state spaces, and thus their performance on high-dimensional (e.g., image) observations is not well-studied. To fill this gap, this paper highlights the crucial need for expanding systematic evaluation across a wider array of existing benchmarks. To this end, we conduct extensive evaluation and comparisons of well-known MARL algorithms on complex fully cooperative benchmarks, including tasks with images as agents' observations. Interestingly, our analysis shows that many algorithms, hailed as state-of-the-art on SMAC and GRF, may underperform standard MARL baselines on fully cooperative benchmarks. Finally, towards more systematic and better evaluation of cooperative MARL algorithms, we have open-sourced PyMARLzoo+, an extension of the widely used (E)PyMARL libraries, which addresses an open challenge from [TBG++21], facilitating seamless integration and support with all benchmarks of PettingZoo, as well as Overcooked, PressurePlate, Capture Target and Box Pushing.

### DMPA: Model Poisoning Attacks on Decentralized Federated Learning for Model Differences 
[[arxiv](https://arxiv.org/abs/2502.04771)] [[cool](https://papers.cool/arxiv/2502.04771)] [[pdf](https://arxiv.org/pdf/2502.04771)]
> **Authors**: Chao Feng,Yunlong Li,Yuanzhe Gao,Alberto Huertas Celdrán,Jan von der Assen,Gérôme Bovet,Burkhard Stiller
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 8 pages, 3 figures
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Federated learning (FL) has garnered significant attention as a prominent privacy-preserving Machine Learning (ML) paradigm. Decentralized FL (DFL) eschews traditional FL's centralized server architecture, enhancing the system's robustness and scalability. However, these advantages of DFL also create new vulnerabilities for malicious participants to execute adversarial attacks, especially model poisoning attacks. In model poisoning attacks, malicious participants aim to diminish the performance of benign models by creating and disseminating the compromised model. Existing research on model poisoning attacks has predominantly concentrated on undermining global models within the Centralized FL (CFL) paradigm, while there needs to be more research in DFL. To fill the research gap, this paper proposes an innovative model poisoning attack called DMPA. This attack calculates the differential characteristics of multiple malicious client models and obtains the most effective poisoning strategy, thereby orchestrating a collusive attack by multiple participants. The effectiveness of this attack is validated across multiple datasets, with results indicating that the DMPA approach consistently surpasses existing state-of-the-art FL model poisoning attack strategies.

### Graph Federated Learning Based Proactive Content Caching in Edge Computing 
[[arxiv](https://arxiv.org/abs/2502.04760)] [[cool](https://papers.cool/arxiv/2502.04760)] [[pdf](https://arxiv.org/pdf/2502.04760)]
> **Authors**: Rui Wang
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: With the rapid growth of mobile data traffic and the increasing prevalence of video streaming, proactive content caching in edge computing has become crucial for reducing latency and alleviating network congestion. However, traditional caching strategies such as FIFO, LRU, and LFU fail to effectively predict future content popularity, while existing proactive caching approaches often require users to upload data to a central server, raising concerns regarding privacy and scalability. To address these challenges, this paper proposes a Graph Federated Learning-based Proactive Content Caching (GFPCC) scheme that enhances caching efficiency while preserving user privacy. The proposed approach integrates federated learning and graph neural networks, enabling users to locally train Light Graph Convolutional Networks (LightGCN) to capture user-item relationships and predict content popularity. Instead of sharing raw data, only the trained model parameters are transmitted to the central server, where a federated averaging algorithm aggregates updates, refines the global model, and selects the most popular files for proactive caching. Experimental evaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC outperforms baseline caching algorithms by achieving higher cache efficiency through more accurate content popularity predictions. Moreover, the federated learning framework strengthens privacy protection while maintaining efficient model training; however, scalability remains a challenge in large-scale networks with dynamic user preferences.

### Learning Universal Multi-level Market Irrationality Factors to Improve Stock Return Forecasting 
[[arxiv](https://arxiv.org/abs/2502.04737)] [[cool](https://papers.cool/arxiv/2502.04737)] [[pdf](https://arxiv.org/pdf/2502.04737)]
> **Authors**: Chen Yang,Jingyuan Wang,Xiaohan Jiang,Junjie Wu
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: KDD2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Recent years have witnessed the perfect encounter of deep learning and quantitative trading has achieved great success in stock investment. Numerous deep learning-based models have been developed for forecasting stock returns, leveraging the powerful representation capabilities of neural networks to identify patterns and factors influencing stock prices. These models can effectively capture general patterns in the market, such as stock price trends, volume-price relationships, and time variations. However, the impact of special irrationality factors -- such as market sentiment, speculative behavior, market manipulation, and psychological biases -- have not been fully considered in existing deep stock forecasting models due to their relative abstraction as well as lack of explicit labels and data description. To fill this gap, we propose UMI, a Universal multi-level Market Irrationality factor model to enhance stock return forecasting. The UMI model learns factors that can reflect irrational behaviors in market from both individual stock and overall market levels. For the stock-level, UMI construct an estimated rational price for each stock, which is cointegrated with the stock's actual price. The discrepancy between the actual and the rational prices serves as a factor to indicate stock-level irrational events. Additionally, we define market-level irrational behaviors as anomalous synchronous fluctuations of stocks within a market. Using two self-supervised representation learning tasks, i.e., sub-market comparative learning and market synchronism prediction, the UMI model incorporates market-level irrationalities into a market representation vector, which is then used as the market-level irrationality factor.

### EigenLoRAx: Recycling Adapters to Find Principal Subspaces for Resource-Efficient Adaptation and Inference 
[[arxiv](https://arxiv.org/abs/2502.04700)] [[cool](https://papers.cool/arxiv/2502.04700)] [[pdf](https://arxiv.org/pdf/2502.04700)]
> **Authors**: Prakhar Kaushik,Ankit Vaidya,Shravan Chaudhari,Alan Yuille
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: The rapid growth of large models has raised concerns about their environmental impact and equity in accessibility due to significant computational costs. Low-Rank Adapters (LoRA) offer a lightweight solution for finetuning large models, resulting in an abundance of publicly available adapters tailored to diverse domains. We ask: Can these pretrained adapters be leveraged to further streamline adaptation to new tasks while addressing these challenges? We introduce EigenLoRAx, a parameter-efficient finetuning method that recycles existing adapters to create a principal subspace aligned with their shared domain knowledge which can be further augmented with orthogonal basis vectors in low-resource scenarios. This enables rapid adaptation to new tasks by learning only lightweight coefficients on the principal components of the subspace - eliminating the need to finetune entire adapters. EigenLoRAx requires significantly fewer parameters and memory, improving efficiency for both training and inference. Our method demonstrates strong performance across diverse domains and tasks, offering a scalable for edge-based applications, personalization, and equitable deployment of large models in resource-constrained environments.

### G2PDiffusion: Genotype-to-Phenotype Prediction with Diffusion Models 
[[arxiv](https://arxiv.org/abs/2502.04684)] [[cool](https://papers.cool/arxiv/2502.04684)] [[pdf](https://arxiv.org/pdf/2502.04684)]
> **Authors**: Mengdi Liu,Zhangyang Gao,Hong Chang,Stan Z. Li,Shiguang Shan,Xilin Chen
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Discovering the genotype-phenotype relationship is crucial for genetic engineering, which will facilitate advances in fields such as crop breeding, conservation biology, and personalized medicine. Current research usually focuses on single species and small datasets due to limitations in phenotypic data collection, especially for traits that require visual assessments or physical measurements. Deciphering complex and composite phenotypes, such as morphology, from genetic data at scale remains an open question. To break through traditional generic models that rely on simplified assumptions, this paper introduces G2PDiffusion, the first-of-its-kind diffusion model designed for genotype-to-phenotype generation across multiple species. Specifically, we use images to represent morphological phenotypes across species and redefine phenotype prediction as conditional image generation. To this end, this paper introduces an environment-enhanced DNA sequence conditioner and trains a stable diffusion model with a novel alignment method to improve genotype-to-phenotype consistency. Extensive experiments demonstrate that our approach enhances phenotype prediction accuracy across species, capturing subtle genetic variations that contribute to observable traits.

### Nearly Tight Bounds for Cross-Learning Contextual Bandits with Graphical Feedback 
[[arxiv](https://arxiv.org/abs/2502.04678)] [[cool](https://papers.cool/arxiv/2502.04678)] [[pdf](https://arxiv.org/pdf/2502.04678)]
> **Authors**: Ruiyuan Huang,Zengfeng Huang
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The cross-learning contextual bandit problem with graphical feedback has recently attracted significant attention. In this setting, there is a contextual bandit with a feedback graph over the arms, and pulling an arm reveals the loss for all neighboring arms in the feedback graph across all contexts. Initially proposed by Han et al. (2024), this problem has broad applications in areas such as bidding in first price auctions, and explores a novel frontier in the feedback structure of bandit problems. A key theoretical question is whether an algorithm with $\widetilde{O}(\sqrt{αT})$ regret exists, where $α$ represents the independence number of the feedback graph. This question is particularly interesting because it concerns whether an algorithm can achieve a regret bound entirely independent of the number of contexts and matching the minimax regret of vanilla graphical bandits. Previous work has demonstrated that such an algorithm is impossible for adversarial contexts, but the question remains open for stochastic contexts. In this work, we affirmatively answer this open question by presenting an algorithm that achieves the minimax $\widetilde{O}(\sqrt{αT})$ regret for cross-learning contextual bandits with graphical feedback and stochastic contexts. Notably, although that question is open even for stochastic bandits, we directly solve the strictly stronger adversarial bandit version of the problem.

### CCS: Controllable and Constrained Sampling with Diffusion Models via Initial Noise Perturbation 
[[arxiv](https://arxiv.org/abs/2502.04670)] [[cool](https://papers.cool/arxiv/2502.04670)] [[pdf](https://arxiv.org/pdf/2502.04670)]
> **Authors**: Bowen Song,Zecheng Zhang,Zhaoxu Luo,Jason Hu,Wei Yuan,Jing Jia,Zhengxu Tang,Guanyang Wang,Liyue Shen
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Diffusion models have emerged as powerful tools for generative tasks, producing high-quality outputs across diverse domains. However, how the generated data responds to the initial noise perturbation in diffusion models remains under-explored, which hinders understanding the controllability of the sampling process. In this work, we first observe an interesting phenomenon: the relationship between the change of generation outputs and the scale of initial noise perturbation is highly linear through the diffusion ODE sampling. Then we provide both theoretical and empirical study to justify this linearity property of this input-output (noise-generation data) relationship. Inspired by these new insights, we propose a novel Controllable and Constrained Sampling method (CCS) together with a new controller algorithm for diffusion models to sample with desired statistical properties while preserving good sample quality. We perform extensive experiments to compare our proposed sampling approach with other methods on both sampling controllability and sampled data quality. Results show that our CCS method achieves more precisely controlled sampling while maintaining superior sample quality and diversity.

### A Comprehensive Review on Noise Control of Diffusion Model 
[[arxiv](https://arxiv.org/abs/2502.04669)] [[cool](https://papers.cool/arxiv/2502.04669)] [[pdf](https://arxiv.org/pdf/2502.04669)]
> **Authors**: Zhehao Guo,Jiedong Lang,Shuyu Huang,Yunfei Gao,Xintong Ding
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Diffusion models have recently emerged as powerful generative frameworks for producing high-quality images. A pivotal component of these models is the noise schedule, which governs the rate of noise injection during the diffusion process. Since the noise schedule substantially influences sampling quality and training quality, understanding its design and implications is crucial. In this discussion, various noise schedules are examined, and their distinguishing features and performance characteristics are highlighted.

### Unveiling the Mechanisms of Explicit CoT Training: How Chain-of-Thought Enhances Reasoning Generalization 
[[arxiv](https://arxiv.org/abs/2502.04667)] [[cool](https://papers.cool/arxiv/2502.04667)] [[pdf](https://arxiv.org/pdf/2502.04667)]
> **Authors**: Xinhao Yao,Ruifeng Ren,Yun Liao,Yong Liu
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Training large language models (LLMs) with high-quality Chain-of-Thought (CoT) annotations has become a widely adopted strategy due to its significant enhancement of reasoning capabilities. To fully comprehend this approach, two questions naturally arise: (Q1) What advantages does training with CoT offer compared to training without CoT? (Q2) If there are advantages, what are the underlying mechanisms of explicit CoT training? Analyzing the advantages and mechanisms of CoT training is challenging due to the many factors involved. To address this, we conduct a detailed analysis using clear and controllable data distributions and, for the first time, reveal that CoT training offers the following advantages: (1) Training with CoT markedly improves reasoning generalization, extending it from in-distribution (ID) to both ID and out-of-distribution (OOD) scenarios, while also speeding up convergence; (2) Even when training with CoT includes a certain range of erroneous reasoning steps, it still enables the model to learn reasoning patterns, leading to systematic generalization. We further explore the underlying mechanisms from a circuit perspective: (1) The data distribution (e.g., ratio $λ$ and pattern) plays a crucial role in influencing the model's systematic generalization; (2) CoT training (with two-hop facts) internalizes reasoning into a two-stage generalizing circuit, where the number of stages corresponds to the explicit reasoning steps during training. Our findings elucidate the mechanisms underlying explicit CoT training and offer critical insights into tuning strategies for LLMs to achieve robust generalization.

### Implicit Bias of SignGD and Adam on Multiclass Separable Data 
[[arxiv](https://arxiv.org/abs/2502.04664)] [[cool](https://papers.cool/arxiv/2502.04664)] [[pdf](https://arxiv.org/pdf/2502.04664)]
> **Authors**: Chen Fan,Mark Schmidt,Christos Thrampoulidis
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,优化与控制
- **Abstract**: In the optimization of overparameterized models, different gradient-based methods can achieve zero training error yet converge to distinctly different solutions inducing different generalization properties. While a decade of research on implicit optimization bias has illuminated this phenomenon in various settings, even the foundational case of linear classification with separable data still has important open questions. We resolve a fundamental gap by characterizing the implicit bias of both Adam and Sign Gradient Descent in multi-class cross-entropy minimization: we prove that their iterates converge to solutions that maximize the margin with respect to the classifier matrix's max-norm and characterize the rate of convergence. We extend our results to general p-norm normalized steepest descent algorithms and to other multi-class losses.

### Adversarially-Robust TD Learning with Markovian Data: Finite-Time Rates and Fundamental Limits 
[[arxiv](https://arxiv.org/abs/2502.04662)] [[cool](https://papers.cool/arxiv/2502.04662)] [[pdf](https://arxiv.org/pdf/2502.04662)]
> **Authors**: Sreejeet Maity,Aritra Mitra
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: Accepted to AISTATS 2025
- **标题**: None
- **领域**: 机器学习,系统与控制,优化与控制
- **Abstract**: One of the most basic problems in reinforcement learning (RL) is policy evaluation: estimating the long-term return, i.e., value function, corresponding to a given fixed policy. The celebrated Temporal Difference (TD) learning algorithm addresses this problem, and recent work has investigated finite-time convergence guarantees for this algorithm and variants thereof. However, these guarantees hinge on the reward observations being always generated from a well-behaved (e.g., sub-Gaussian) true reward distribution. Motivated by harsh, real-world environments where such an idealistic assumption may no longer hold, we revisit the policy evaluation problem from the perspective of adversarial robustness. In particular, we consider a Huber-contaminated reward model where an adversary can arbitrarily corrupt each reward sample with a small probability $ε$. Under this observation model, we first show that the adversary can cause the vanilla TD algorithm to converge to any arbitrary value function. We then develop a novel algorithm called Robust-TD and prove that its finite-time guarantees match that of vanilla TD with linear function approximation up to a small $O(ε)$ term that captures the effect of corruption. We complement this result with a minimax lower bound, revealing that such an additive corruption-induced term is unavoidable. To our knowledge, these results are the first of their kind in the context of adversarial robustness of stochastic approximation schemes driven by Markov noise. The key new technical tool that enables our results is an analysis of the Median-of-Means estimator with corrupted, time-correlated data that might be of independent interest to the literature on robust statistics.

## 多代理系统(cs.MA:Multiagent Systems)

### Low-Rank Agent-Specific Adaptation (LoRASA) for Multi-Agent Policy Learning 
[[arxiv](https://arxiv.org/abs/2502.05573)] [[cool](https://papers.cool/arxiv/2502.05573)] [[pdf](https://arxiv.org/pdf/2502.05573)]
> **Authors**: Beining Zhang,Aditya Kapoor,Mingfei Sun
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: 31 pages, 20 figures, 13 tables
- **标题**: None
- **领域**: 多代理系统,人工智能,机器学习,机器人技术
- **Abstract**: Multi-agent reinforcement learning (MARL) often relies on \emph{parameter sharing (PS)} to scale efficiently. However, purely shared policies can stifle each agent's unique specialization, reducing overall performance in heterogeneous environments. We propose \textbf{Low-Rank Agent-Specific Adaptation (LoRASA)}, a novel approach that treats each agent's policy as a specialized ``task'' fine-tuned from a shared backbone. Drawing inspiration from parameter-efficient transfer methods, LoRASA appends small, low-rank adaptation matrices to each layer of the shared policy, naturally inducing \emph{parameter-space sparsity} that promotes both specialization and scalability. We evaluate LoRASA on challenging benchmarks including the StarCraft Multi-Agent Challenge (SMAC) and Multi-Agent MuJoCo (MAMuJoCo), implementing it atop widely used algorithms such as MAPPO and A2PO. Across diverse tasks, LoRASA matches or outperforms existing baselines \emph{while reducing memory and computational overhead}. Ablation studies on adapter rank, placement, and timing validate the method's flexibility and efficiency. Our results suggest LoRASA's potential to establish a new norm for MARL policy parameterization: combining a shared foundation for coordination with low-rank agent-specific refinements for individual specialization.

### Near-Optimal Online Learning for Multi-Agent Submodular Coordination: Tight Approximation and Communication Efficiency 
[[arxiv](https://arxiv.org/abs/2502.05028)] [[cool](https://papers.cool/arxiv/2502.05028)] [[pdf](https://arxiv.org/pdf/2502.05028)]
> **Authors**: Qixin Zhang,Zongqi Wan,Yu Yang,Li Shen,Dacheng Tao
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: Accepted to ICLR 2025
- **标题**: None
- **领域**: 多代理系统,机器学习,优化与控制
- **Abstract**: Coordinating multiple agents to collaboratively maximize submodular functions in unpredictable environments is a critical task with numerous applications in machine learning, robot planning and control. The existing approaches, such as the OSG algorithm, are often hindered by their poor approximation guarantees and the rigid requirement for a fully connected communication graph. To address these challenges, we firstly present a $\textbf{MA-OSMA}$ algorithm, which employs the multi-linear extension to transfer the discrete submodular maximization problem into a continuous optimization, thereby allowing us to reduce the strict dependence on a complete graph through consensus techniques. Moreover, $\textbf{MA-OSMA}$ leverages a novel surrogate gradient to avoid sub-optimal stationary points. To eliminate the computationally intensive projection operations in $\textbf{MA-OSMA}$, we also introduce a projection-free $\textbf{MA-OSEA}$ algorithm, which effectively utilizes the KL divergence by mixing a uniform distribution. Theoretically, we confirm that both algorithms achieve a regret bound of $\widetilde{O}(\sqrt{\frac{C_{T}T}{1-β}})$ against a $(\frac{1-e^{-c}}{c})$-approximation to the best comparator in hindsight, where $C_{T}$ is the deviation of maximizer sequence, $β$ is the spectral gap of the network and $c$ is the joint curvature of submodular objectives. This result significantly improves the $(\frac{1}{1+c})$-approximation provided by the state-of-the-art OSG algorithm. Finally, we demonstrate the effectiveness of our proposed algorithms through simulation-based multi-target tracking.

### : Temporal-Agent Reward Redistribution for Optimal Policy Preservation in Multi-Agent Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.04864)] [[cool](https://papers.cool/arxiv/2502.04864)] [[pdf](https://arxiv.org/pdf/2502.04864)]
> **Authors**: Aditya Kapoor,Kale-ab Tessera,Mayank Baranwal,Harshad Khadilkar,Stefano Albrecht,Mingfei Sun
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 23 pages, 5 figures, 4 tables
- **标题**: None
- **领域**: 多代理系统,人工智能,机器学习,机器人技术
- **Abstract**: In cooperative multi-agent reinforcement learning (MARL), learning effective policies is challenging when global rewards are sparse and delayed. This difficulty arises from the need to assign credit across both agents and time steps, a problem that existing methods often fail to address in episodic, long-horizon tasks. We propose Temporal-Agent Reward Redistribution $TAR^2$, a novel approach that decomposes sparse global rewards into agent-specific, time-step-specific components, thereby providing more frequent and accurate feedback for policy learning. Theoretically, we show that $TAR^2$ (i) aligns with potential-based reward shaping, preserving the same optimal policies as the original environment, and (ii) maintains policy gradient update directions identical to those under the original sparse reward, ensuring unbiased credit signals. Empirical results on two challenging benchmarks, SMACLite and Google Research Football, demonstrate that $TAR^2$ significantly stabilizes and accelerates convergence, outperforming strong baselines like AREL and STAS in both learning speed and final performance. These findings establish $TAR^2$ as a principled and practical solution for agent-temporal credit assignment in sparse-reward multi-agent systems.

## 多媒体(cs.MM:Multimedia)

### Semantic-Aware Adaptive Video Streaming Using Latent Diffusion Models for Wireless Networks 
[[arxiv](https://arxiv.org/abs/2502.05695)] [[cool](https://papers.cool/arxiv/2502.05695)] [[pdf](https://arxiv.org/pdf/2502.05695)]
> **Authors**: Zijiang Yan,Jianhua Pei,Hongda Wu,Hina Tabassum,Ping Wang
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: Submission for possible publication
- **标题**: None
- **领域**: 多媒体,人工智能,计算机视觉和模式识别,机器学习,图像和视频处理
- **Abstract**: This paper proposes a novel framework for real-time adaptive-bitrate video streaming by integrating latent diffusion models (LDMs) within the FFmpeg techniques. This solution addresses the challenges of high bandwidth usage, storage inefficiencies, and quality of experience (QoE) degradation associated with traditional constant bitrate streaming (CBS) and adaptive bitrate streaming (ABS). The proposed approach leverages LDMs to compress I-frames into a latent space, offering significant storage and semantic transmission savings without sacrificing high visual quality. While it keeps B-frames and P-frames as adjustment metadata to ensure efficient video reconstruction at the user side, the proposed framework is complemented with the most state-of-the-art denoising and video frame interpolation (VFI) techniques. These techniques mitigate semantic ambiguity and restore temporal coherence between frames, even in noisy wireless communication environments. Experimental results demonstrate the proposed method achieves high-quality video streaming with optimized bandwidth usage, outperforming state-of-the-art solutions in terms of QoE and resource efficiency. This work opens new possibilities for scalable real-time video streaming in 5G and future post-5G networks.

## 网络和互联网架构(cs.NI:Networking and Internet Architecture)

### GWRF: A Generalizable Wireless Radiance Field for Wireless Signal Propagation Modeling 
[[arxiv](https://arxiv.org/abs/2502.05708)] [[cool](https://papers.cool/arxiv/2502.05708)] [[pdf](https://arxiv.org/pdf/2502.05708)]
> **Authors**: Kang Yang,Yuning Chen,Wan Du
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 网络和互联网架构,机器学习
- **Abstract**: We present Generalizable Wireless Radiance Fields (GWRF), a framework for modeling wireless signal propagation at arbitrary 3D transmitter and receiver positions. Unlike previous methods that adapt vanilla Neural Radiance Fields (NeRF) from the optical to the wireless signal domain, requiring extensive per-scene training, GWRF generalizes effectively across scenes. First, a geometry-aware Transformer encoder-based wireless scene representation module incorporates information from geographically proximate transmitters to learn a generalizable wireless radiance field. Second, a neural-driven ray tracing algorithm operates on this field to automatically compute signal reception at the receiver. Experimental results demonstrate that GWRF outperforms existing methods on single scenes and achieves state-of-the-art performance on unseen scenes.

### Optimizing Wireless Resource Management and Synchronization in Digital Twin Networks 
[[arxiv](https://arxiv.org/abs/2502.05116)] [[cool](https://papers.cool/arxiv/2502.05116)] [[pdf](https://arxiv.org/pdf/2502.05116)]
> **Authors**: Hanzhi Yu,Yuchen Liu,Zhaohui Yang,Haijian Sun,Mingzhe Chen
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 12 pages, 6 figures
- **标题**: None
- **领域**: 网络和互联网架构,机器学习,系统与控制
- **Abstract**: In this paper, we investigate an accurate synchronization between a physical network and its digital network twin (DNT), which serves as a virtual representation of the physical network. The considered network includes a set of base stations (BSs) that must allocate its limited spectrum resources to serve a set of users while also transmitting its partially observed physical network information to a cloud server to generate the DNT. Since the DNT can predict the physical network status based on its historical status, the BSs may not need to send their physical network information at each time slot, allowing them to conserve spectrum resources to serve the users. However, if the DNT does not receive the physical network information of the BSs over a large time period, the DNT's accuracy in representing the physical network may degrade. To this end, each BS must decide when to send the physical network information to the cloud server to update the DNT, while also determining the spectrum resource allocation policy for both DNT synchronization and serving the users. We formulate this resource allocation task as an optimization problem, aiming to maximize the total data rate of all users while minimizing the asynchronization between the physical network and the DNT. To address this problem, we propose a method based on the GRUs and the value decomposition network (VDN). Simulation results show that our GRU and VDN based algorithm improves the weighted sum of data rates and the similarity between the status of the DNT and the physical network by up to 28.96%, compared to a baseline method combining GRU with the independent Q learning.

### Data-driven Modality Fusion: An AI-enabled Framework for Large-Scale Sensor Network Management 
[[arxiv](https://arxiv.org/abs/2502.04937)] [[cool](https://papers.cool/arxiv/2502.04937)] [[pdf](https://arxiv.org/pdf/2502.04937)]
> **Authors**: Hrishikesh Dutta,Roberto Minerva,Maira Alvi,Noel Crespi
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 网络和互联网架构,人工智能,机器学习
- **Abstract**: The development and operation of smart cities relyheavily on large-scale Internet-of-Things (IoT) networks and sensor infrastructures that continuously monitor various aspects of urban environments. These networks generate vast amounts of data, posing challenges related to bandwidth usage, energy consumption, and system scalability. This paper introduces a novel sensing paradigm called Data-driven Modality Fusion (DMF), designed to enhance the efficiency of smart city IoT network management. By leveraging correlations between timeseries data from different sensing modalities, the proposed DMF approach reduces the number of physical sensors required for monitoring, thereby minimizing energy expenditure, communication bandwidth, and overall deployment costs. The framework relocates computational complexity from the edge devices to the core, ensuring that resource-constrained IoT devices are not burdened with intensive processing tasks. DMF is validated using data from a real-world IoT deployment in Madrid, demonstrating the effectiveness of the proposed system in accurately estimating traffic, environmental, and pollution metrics from a reduced set of sensors. The proposed solution offers a scalable, efficient mechanism for managing urban IoT networks, while addressing issues of sensor failure and privacy concerns.

## 编程语言(cs.PL:Programming Languages)

### Oracular Programming: A Modular Foundation for Building LLM-Enabled Software 
[[arxiv](https://arxiv.org/abs/2502.05310)] [[cool](https://papers.cool/arxiv/2502.05310)] [[pdf](https://arxiv.org/pdf/2502.05310)]
> **Authors**: Jonathan Laurent,André Platzer
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 编程语言,人工智能
- **Abstract**: Large Language Models have proved surprisingly effective at solving a wide range of tasks from just a handful of examples. However, their lack of reliability and modularity limits their capacity to tackle large problems that require many steps of reasoning. In response, researchers have proposed advanced pipelines that leverage domain-specific knowledge to chain smaller prompts, provide intermediate feedback and improve performance through search. However, the current complexity of writing, tuning, maintaining and improving such pipelines has limited their sophistication. We propose oracular programming, a foundational paradigm for building LLM-enabled applications that lets domain experts express high-level problem-solving strategies as programs with unresolved choice points. These choice points are resolved at runtime by LLMs, which generalize from user-provided examples of correct and incorrect decisions. An oracular program is composed of three orthogonal components: a strategy that consists in a nondeterministic program with choice points that can be reified into a search tree, a policy that specifies how to navigate this tree with the help of LLM oracles, and a set of demonstrations that describe successful and unsuccessful search tree navigation scenarios across diverse problem instances. Each component is expressed in a dedicated programming language and can be independently improved or substituted. We address the key programming language design challenges of modularly composing oracular programs and enforcing consistency between their components as they evolve.

## 机器人技术(cs.RO:Robotics)

### Motion Control in Multi-Rotor Aerial Robots Using Deep Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.05996)] [[cool](https://papers.cool/arxiv/2502.05996)] [[pdf](https://arxiv.org/pdf/2502.05996)]
> **Authors**: Gaurav Shetty,Mahya Ramezani,Hamed Habibi,Holger Voos,Jose Luis Sanchez-Lopez
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,人工智能
- **Abstract**: This paper investigates the application of Deep Reinforcement (DRL) Learning to address motion control challenges in drones for additive manufacturing (AM). Drone-based additive manufacturing promises flexible and autonomous material deposition in large-scale or hazardous environments. However, achieving robust real-time control of a multi-rotor aerial robot under varying payloads and potential disturbances remains challenging. Traditional controllers like PID often require frequent parameter re-tuning, limiting their applicability in dynamic scenarios. We propose a DRL framework that learns adaptable control policies for multi-rotor drones performing waypoint navigation in AM tasks. We compare Deep Deterministic Policy Gradient (DDPG) and Twin Delayed Deep Deterministic Policy Gradient (TD3) within a curriculum learning scheme designed to handle increasing complexity. Our experiments show TD3 consistently balances training stability, accuracy, and success, particularly when mass variability is introduced. These findings provide a scalable path toward robust, autonomous drone control in additive manufacturing.

### DexVLA: Vision-Language Model with Plug-In Diffusion Expert for General Robot Control 
[[arxiv](https://arxiv.org/abs/2502.05855)] [[cool](https://papers.cool/arxiv/2502.05855)] [[pdf](https://arxiv.org/pdf/2502.05855)]
> **Authors**: Junjie Wen,Yichen Zhu,Jinming Li,Zhibin Tang,Chaomin Shen,Feifei Feng
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: The webpage is at https://dex-vla.github.io/
- **标题**: None
- **领域**: 机器人技术,计算机视觉和模式识别
- **Abstract**: Enabling robots to perform diverse tasks across varied environments is a central challenge in robot learning. While vision-language-action (VLA) models have shown promise for generalizable robot skills, realizing their full potential requires addressing limitations in action representation and efficient training. Current VLA models often focus on scaling the vision-language model (VLM) component, while the action space representation remains a critical bottleneck. This paper introduces DexVLA, a novel framework designed to enhance the efficiency and generalization capabilities of VLAs for complex, long-horizon tasks across diverse robot embodiments. DexVLA features a novel diffusion-based action expert, scaled to one billion parameters, designed for cross-embodiment learning. A novel embodiment curriculum learning strategy facilitates efficient training: (1) pre-training the diffusion expert that is separable from the VLA on cross-embodiment data, (2) aligning the VLA model to specific embodiments, and (3) post-training for rapid adaptation to new tasks. We conduct comprehensive experiments across multiple embodiments, including single-arm, bimanual, and dexterous hand, demonstrating DexVLA's adaptability to challenging tasks without task-specific adaptation, its ability to learn dexterous skills on novel embodiments with limited data, and its capacity to complete complex, long-horizon tasks using only direct language prompting, such as laundry folding. In all settings, our method demonstrates superior performance compared to state-of-the-art models like Octo, OpenVLA, and Diffusion Policy.

### Surprise Potential as a Measure of Interactivity in Driving Scenarios 
[[arxiv](https://arxiv.org/abs/2502.05677)] [[cool](https://papers.cool/arxiv/2502.05677)] [[pdf](https://arxiv.org/pdf/2502.05677)]
> **Authors**: Wenhao Ding,Sushant Veer,Karen Leung,Yulong Cao,Marco Pavone
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: 10 pages, 8 figures
- **标题**: None
- **领域**: 机器人技术,机器学习
- **Abstract**: Validating the safety and performance of an autonomous vehicle (AV) requires benchmarking on real-world driving logs. However, typical driving logs contain mostly uneventful scenarios with minimal interactions between road users. Identifying interactive scenarios in real-world driving logs enables the curation of datasets that amplify critical signals and provide a more accurate assessment of an AV's performance. In this paper, we present a novel metric that identifies interactive scenarios by measuring an AV's surprise potential on others. First, we identify three dimensions of the design space to describe a family of surprise potential measures. Second, we exhaustively evaluate and compare different instantiations of the surprise potential measure within this design space on the nuScenes dataset. To determine how well a surprise potential measure correctly identifies an interactive scenario, we use a reward model learned from human preferences to assess alignment with human intuition. Our proposed surprise potential, arising from this exhaustive comparative study, achieves a correlation of more than 0.82 with the human-aligned reward function, outperforming existing approaches. Lastly, we validate motion planners on curated interactive scenarios to demonstrate downstream applications.

### Towards Learning Scalable Agile Dynamic Motion Planning for Robosoccer Teams with Policy Optimization 
[[arxiv](https://arxiv.org/abs/2502.05526)] [[cool](https://papers.cool/arxiv/2502.05526)] [[pdf](https://arxiv.org/pdf/2502.05526)]
> **Authors**: Brandon Ho,Batuhan Altundas,Matthew Gombolay
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,人工智能,机器学习,多代理系统
- **Abstract**: In fast-paced, ever-changing environments, dynamic Motion Planning for Multi-Agent Systems in the presence of obstacles is a universal and unsolved problem. Be it from path planning around obstacles to the movement of robotic arms, or in planning navigation of robot teams in settings such as Robosoccer, dynamic motion planning is needed to avoid collisions while reaching the targeted destination when multiple agents occupy the same area. In continuous domains where the world changes quickly, existing classical Motion Planning algorithms such as RRT* and A* become computationally expensive to rerun at every time step. Many variations of classical and well-formulated non-learning path-planning methods have been proposed to solve this universal problem but fall short due to their limitations of speed, smoothness, optimally, etc. Deep Learning models overcome their challenges due to their ability to adapt to varying environments based on past experience. However, current learning motion planning models use discretized environments, do not account for heterogeneous agents or replanning, and build up to improve the classical motion planners' efficiency, leading to issues with scalability. To prevent collisions between heterogenous team members and collision to obstacles while trying to reach the target location, we present a learning-based dynamic navigation model and show our model working on a simple environment in the concept of a simple Robosoccer Game.

### Vision-Ultrasound Robotic System based on Deep Learning for Gas and Arc Hazard Detection in Manufacturing 
[[arxiv](https://arxiv.org/abs/2502.05500)] [[cool](https://papers.cool/arxiv/2502.05500)] [[pdf](https://arxiv.org/pdf/2502.05500)]
> **Authors**: Jin-Hee Lee,Dahyun Nam,Robin Inho Kee,YoungKey Kim,Seok-Jun Buu
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: Submitted to Engineering Applications of Artificial Intelligence
- **标题**: None
- **领域**: 机器人技术,人工智能
- **Abstract**: Gas leaks and arc discharges present significant risks in industrial environments, requiring robust detection systems to ensure safety and operational efficiency. Inspired by human protocols that combine visual identification with acoustic verification, this study proposes a deep learning-based robotic system for autonomously detecting and classifying gas leaks and arc discharges in manufacturing settings. The system is designed to execute all experimental tasks entirely onboard the robot. Utilizing a 112-channel acoustic camera operating at a 96 kHz sampling rate to capture ultrasonic frequencies, the system processes real-world datasets recorded in diverse industrial scenarios. These datasets include multiple gas leak configurations (e.g., pinhole, open end) and partial discharge types (Corona, Surface, Floating) under varying environmental noise conditions. Proposed system integrates visual detection and a beamforming-enhanced acoustic analysis pipeline. Signals are transformed using STFT and refined through Gamma Correction, enabling robust feature extraction. An Inception-inspired CNN further classifies hazards, achieving 99% gas leak detection accuracy. The system not only detects individual hazard sources but also enhances classification reliability by fusing multi-modal data from both vision and acoustic sensors. When tested in reverberation and noise-augmented environments, the system outperformed conventional models by up to 44%p, with experimental tasks meticulously designed to ensure fairness and reproducibility. Additionally, the system is optimized for real-time deployment, maintaining an inference time of 2.1 seconds on a mobile robotic platform. By emulating human-like inspection protocols and integrating vision with acoustic modalities, this study presents an effective solution for industrial automation, significantly improving safety and operational reliability.

### HAMSTER: Hierarchical Action Models For Open-World Robot Manipulation 
[[arxiv](https://arxiv.org/abs/2502.05485)] [[cool](https://papers.cool/arxiv/2502.05485)] [[pdf](https://arxiv.org/pdf/2502.05485)]
> **Authors**: Yi Li,Yuquan Deng,Jesse Zhang,Joel Jang,Marius Memmel,Raymond Yu,Caelan Reed Garrett,Fabio Ramos,Dieter Fox,Anqi Li,Abhishek Gupta,Ankit Goyal
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: to be published in ICLR 2025
- **标题**: None
- **领域**: 机器人技术,人工智能,计算机视觉和模式识别
- **Abstract**: Large foundation models have shown strong open-world generalization to complex problems in vision and language, but similar levels of generalization have yet to be achieved in robotics. One fundamental challenge is the lack of robotic data, which are typically obtained through expensive on-robot operation. A promising remedy is to leverage cheaper, off-domain data such as action-free videos, hand-drawn sketches or simulation data. In this work, we posit that hierarchical vision-language-action (VLA) models can be more effective in utilizing off-domain data than standard monolithic VLA models that directly finetune vision-language models (VLMs) to predict actions. In particular, we study a class of hierarchical VLA models, where the high-level VLM is finetuned to produce a coarse 2D path indicating the desired robot end-effector trajectory given an RGB image and a task description. The intermediate 2D path prediction is then served as guidance to the low-level, 3D-aware control policy capable of precise manipulation. Doing so alleviates the high-level VLM from fine-grained action prediction, while reducing the low-level policy's burden on complex task-level reasoning. We show that, with the hierarchical design, the high-level VLM can transfer across significant domain gaps between the off-domain finetuning data and real-robot testing scenarios, including differences on embodiments, dynamics, visual appearances and task semantics, etc. In the real-robot experiments, we observe an average of 20% improvement in success rate across seven different axes of generalization over OpenVLA, representing a 50% relative gain. Visual results are provided at: https://hamster-robot.github.io/

### Temporal Representation Alignment: Successor Features Enable Emergent Compositionality in Robot Instruction Following 
[[arxiv](https://arxiv.org/abs/2502.05454)] [[cool](https://papers.cool/arxiv/2502.05454)] [[pdf](https://arxiv.org/pdf/2502.05454)]
> **Authors**: Vivek Myers,Bill Chunyuan Zheng,Anca Dragan,Kuan Fang,Sergey Levine
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,机器学习
- **Abstract**: Effective task representations should facilitate compositionality, such that after learning a variety of basic tasks, an agent can perform compound tasks consisting of multiple steps simply by composing the representations of the constituent steps together. While this is conceptually simple and appealing, it is not clear how to automatically learn representations that enable this sort of compositionality. We show that learning to associate the representations of current and future states with a temporal alignment loss can improve compositional generalization, even in the absence of any explicit subtask planning or reinforcement learning. We evaluate our approach across diverse robotic manipulation tasks as well as in simulation, showing substantial improvements for tasks specified with either language or goal images.

### ConRFT: A Reinforced Fine-tuning Method for VLA Models via Consistency Policy 
[[arxiv](https://arxiv.org/abs/2502.05450)] [[cool](https://papers.cool/arxiv/2502.05450)] [[pdf](https://arxiv.org/pdf/2502.05450)]
> **Authors**: Yuhui Chen,Shuai Tian,Shugao Liu,Yingting Zhou,Haoran Li,Dongbin Zhao
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,人工智能
- **Abstract**: Vision-Language-Action (VLA) models have shown substantial potential in real-world robotic manipulation. However, fine-tuning these models through supervised learning struggles to achieve robust performance due to limited, inconsistent demonstrations, especially in contact-rich environments. In this paper, we propose a reinforced fine-tuning approach for VLA models, named ConRFT, which consists of offline and online fine-tuning with a unified consistency-based training objective, to address these challenges. In the offline stage, our method integrates behavior cloning and Q-learning to effectively extract policy from a small set of demonstrations and stabilize value estimating. In the online stage, the VLA model is further fine-tuned via consistency policy, with human interventions to ensure safe exploration and high sample efficiency. We evaluate our approach on eight diverse real-world manipulation tasks. It achieves an average success rate of 96.3% within 45-90 minutes of online fine-tuning, outperforming prior supervised methods with a 144% improvement in success rate and 1.9x shorter episode length. This work highlights the potential of integrating reinforcement learning to enhance the performance of VLA models for real-world robotic applications.

### Robotouille: An Asynchronous Planning Benchmark for LLM Agents 
[[arxiv](https://arxiv.org/abs/2502.05227)] [[cool](https://papers.cool/arxiv/2502.05227)] [[pdf](https://arxiv.org/pdf/2502.05227)]
> **Authors**: Gonzalo Gonzalez-Pumariega,Leong Su Yean,Neha Sunkara,Sanjiban Choudhury
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-10
> **comment**: 11 pages (not including references or appendix); 41 figures (7 main paper, 34 appendix); (v1) preprint
- **标题**: None
- **领域**: 机器人技术,人工智能,计算语言学
- **Abstract**: Effective asynchronous planning, or the ability to efficiently reason and plan over states and actions that must happen in parallel or sequentially, is essential for agents that must account for time delays, reason over diverse long-horizon tasks, and collaborate with other agents. While large language model (LLM) agents show promise in high-level task planning, current benchmarks focus primarily on short-horizon tasks and do not evaluate such asynchronous planning capabilities. We introduce Robotouille, a challenging benchmark environment designed to test LLM agents' ability to handle long-horizon asynchronous scenarios. Our synchronous and asynchronous datasets capture increasingly complex planning challenges that go beyond existing benchmarks, requiring agents to manage overlapping tasks and interruptions. Our results show that ReAct (gpt4-o) achieves 47% on synchronous tasks but only 11% on asynchronous tasks, highlighting significant room for improvement. We further analyze failure modes, demonstrating the need for LLM agents to better incorporate long-horizon feedback and self-audit their reasoning during task execution. Code is available at https://github.com/portal-cornell/robotouille.

### STRIDE: Automating Reward Design, Deep Reinforcement Learning Training and Feedback Optimization in Humanoid Robotics Locomotion 
[[arxiv](https://arxiv.org/abs/2502.04692)] [[cool](https://papers.cool/arxiv/2502.04692)] [[pdf](https://arxiv.org/pdf/2502.04692)]
> **Authors**: Zhenwei Wu,Jinxiong Lu,Yuxiao Chen,Yunxin Liu,Yueting Zhuang,Luhui Hu
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,机器学习
- **Abstract**: Humanoid robotics presents significant challenges in artificial intelligence, requiring precise coordination and control of high-degree-of-freedom systems. Designing effective reward functions for deep reinforcement learning (DRL) in this domain remains a critical bottleneck, demanding extensive manual effort, domain expertise, and iterative refinement. To overcome these challenges, we introduce STRIDE, a novel framework built on agentic engineering to automate reward design, DRL training, and feedback optimization for humanoid robot locomotion tasks. By combining the structured principles of agentic engineering with large language models (LLMs) for code-writing, zero-shot generation, and in-context optimization, STRIDE generates, evaluates, and iteratively refines reward functions without relying on task-specific prompts or templates. Across diverse environments featuring humanoid robot morphologies, STRIDE outperforms the state-of-the-art reward design framework EUREKA, achieving an average improvement of round 250% in efficiency and task performance. Using STRIDE-generated rewards, simulated humanoid robots achieve sprint-level locomotion across complex terrains, highlighting its ability to advance DRL workflows and humanoid robotics research.

## 声音(cs.SD:Sound)

### IndexTTS: An Industrial-Level Controllable and Efficient Zero-Shot Text-To-Speech System 
[[arxiv](https://arxiv.org/abs/2502.05512)] [[cool](https://papers.cool/arxiv/2502.05512)] [[pdf](https://arxiv.org/pdf/2502.05512)]
> **Authors**: Wei Deng,Siyi Zhou,Jingchen Shu,Jinchao Wang,Lu Wang
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 声音,人工智能,音频和语音处理
- **Abstract**: Recently, large language model (LLM) based text-to-speech (TTS) systems have gradually become the mainstream in the industry due to their high naturalness and powerful zero-shot voice cloning capabilities.Here, we introduce the IndexTTS system, which is mainly based on the XTTS and Tortoise model. We add some novel improvements. Specifically, in Chinese scenarios, we adopt a hybrid modeling method that combines characters and pinyin, making the pronunciations of polyphonic characters and long-tail characters controllable. We also performed a comparative analysis of the Vector Quantization (VQ) with Finite-Scalar Quantization (FSQ) for codebook utilization of acoustic speech tokens. To further enhance the effect and stability of voice cloning, we introduce a conformer-based speech conditional encoder and replace the speechcode decoder with BigVGAN2. Compared with XTTS, it has achieved significant improvements in naturalness, content consistency, and zero-shot voice cloning. As for the popular TTS systems in the open-source, such as Fish-Speech, CosyVoice2, FireRedTTS and F5-TTS, IndexTTS has a relatively simple training process, more controllable usage, and faster inference speed. Moreover, its performance surpasses that of these systems. Our demos are available at https://index-tts.github.io.

### Koel-TTS: Enhancing LLM based Speech Generation with Preference Alignment and Classifier Free Guidance 
[[arxiv](https://arxiv.org/abs/2502.05236)] [[cool](https://papers.cool/arxiv/2502.05236)] [[pdf](https://arxiv.org/pdf/2502.05236)]
> **Authors**: Shehzeen Hussain,Paarth Neekhara,Xuesong Yang,Edresson Casanova,Subhankar Ghosh,Mikyas T. Desta,Roy Fejgin,Rafael Valle,Jason Li
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 声音,人工智能,机器学习,音频和语音处理
- **Abstract**: While autoregressive speech token generation models produce speech with remarkable variety and naturalness, their inherent lack of controllability often results in issues such as hallucinations and undesired vocalizations that do not conform to conditioning inputs. We introduce Koel-TTS, a suite of enhanced encoder-decoder Transformer TTS models that address these challenges by incorporating preference alignment techniques guided by automatic speech recognition and speaker verification models. Additionally, we incorporate classifier-free guidance to further improve synthesis adherence to the transcript and reference speaker audio. Our experiments demonstrate that these optimizations significantly enhance target speaker similarity, intelligibility, and naturalness of synthesized speech. Notably, Koel-TTS directly maps text and context audio to acoustic tokens, and on the aforementioned metrics, outperforms state-of-the-art TTS models, despite being trained on a significantly smaller dataset. Audio samples and demos are available on our website.

### Aligner-Encoders: Self-Attention Transformers Can Be Self-Transducers 
[[arxiv](https://arxiv.org/abs/2502.05232)] [[cool](https://papers.cool/arxiv/2502.05232)] [[pdf](https://arxiv.org/pdf/2502.05232)]
> **Authors**: Adam Stooke,Rohit Prabhavalkar,Khe Chai Sim,Pedro Moreno Mengibar
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 声音,人工智能,机器学习,音频和语音处理
- **Abstract**: Modern systems for automatic speech recognition, including the RNN-Transducer and Attention-based Encoder-Decoder (AED), are designed so that the encoder is not required to alter the time-position of information from the audio sequence into the embedding; alignment to the final text output is processed during decoding. We discover that the transformer-based encoder adopted in recent years is actually capable of performing the alignment internally during the forward pass, prior to decoding. This new phenomenon enables a simpler and more efficient model, the "Aligner-Encoder". To train it, we discard the dynamic programming of RNN-T in favor of the frame-wise cross-entropy loss of AED, while the decoder employs the lighter text-only recurrence of RNN-T without learned cross-attention -- it simply scans embedding frames in order from the beginning, producing one token each until predicting the end-of-message. We conduct experiments demonstrating performance remarkably close to the state of the art, including a special inference configuration enabling long-form recognition. In a representative comparison, we measure the total inference time for our model to be 2x faster than RNN-T and 16x faster than AED. Lastly, we find that the audio-text alignment is clearly visible in the self-attention weights of a certain layer, which could be said to perform "self-transduction".

### Meta Audiobox Aesthetics: Unified Automatic Quality Assessment for Speech, Music, and Sound 
[[arxiv](https://arxiv.org/abs/2502.05139)] [[cool](https://papers.cool/arxiv/2502.05139)] [[pdf](https://arxiv.org/pdf/2502.05139)]
> **Authors**: Andros Tjandra,Yi-Chiao Wu,Baishan Guo,John Hoffman,Brian Ellis,Apoorv Vyas,Bowen Shi,Sanyuan Chen,Matt Le,Nick Zacharov,Carleigh Wood,Ann Lee,Wei-Ning Hsu
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: Repository: https://github.com/facebookresearch/audiobox-aesthetics Website: https://ai.meta.com/research/publications/meta-audiobox-aesthetics-unified-automatic-quality-assessment-for-speech-music-and-sound/
- **标题**: None
- **领域**: 声音,机器学习,音频和语音处理
- **Abstract**: The quantification of audio aesthetics remains a complex challenge in audio processing, primarily due to its subjective nature, which is influenced by human perception and cultural context. Traditional methods often depend on human listeners for evaluation, leading to inconsistencies and high resource demands. This paper addresses the growing need for automated systems capable of predicting audio aesthetics without human intervention. Such systems are crucial for applications like data filtering, pseudo-labeling large datasets, and evaluating generative audio models, especially as these models become more sophisticated. In this work, we introduce a novel approach to audio aesthetic evaluation by proposing new annotation guidelines that decompose human listening perspectives into four distinct axes. We develop and train no-reference, per-item prediction models that offer a more nuanced assessment of audio quality. Our models are evaluated against human mean opinion scores (MOS) and existing methods, demonstrating comparable or superior performance. This research not only advances the field of audio aesthetics but also provides open-source models and datasets to facilitate future work and benchmarking. We release our code and pre-trained model at: https://github.com/facebookresearch/audiobox-aesthetics

### Singing Voice Conversion with Accompaniment Using Self-Supervised Representation-Based Melody Features 
[[arxiv](https://arxiv.org/abs/2502.04722)] [[cool](https://papers.cool/arxiv/2502.04722)] [[pdf](https://arxiv.org/pdf/2502.04722)]
> **Authors**: Wei Chen,Binzhu Sha,Jing Yang,Zhuo Wang,Fan Fan,Zhiyong Wu
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: Accepted by ICASSP2025
- **标题**: None
- **领域**: 声音,机器学习,音频和语音处理
- **Abstract**: Melody preservation is crucial in singing voice conversion (SVC). However, in many scenarios, audio is often accompanied with background music (BGM), which can cause audio distortion and interfere with the extraction of melody and other key features, significantly degrading SVC performance. Previous methods have attempted to address this by using more robust neural network-based melody extractors, but their performance drops sharply in the presence of complex accompaniment. Other approaches involve performing source separation before conversion, but this often introduces noticeable artifacts, leading to a significant drop in conversion quality and increasing the user's operational costs. To address these issues, we introduce a novel SVC method that uses self-supervised representation-based melody features to improve melody modeling accuracy in the presence of BGM. In our experiments, we compare the effectiveness of different self-supervised learning (SSL) models for melody extraction and explore for the first time how SSL benefits the task of melody extraction. The experimental results demonstrate that our proposed SVC model significantly outperforms existing baseline methods in terms of melody accuracy and shows higher similarity and naturalness in both subjective and objective evaluations across noisy and clean audio environments.

## 软件工程(cs.SE:Software Engineering)

### CSR-Bench: Benchmarking LLM Agents in Deployment of Computer Science Research Repositories 
[[arxiv](https://arxiv.org/abs/2502.06111)] [[cool](https://papers.cool/arxiv/2502.06111)] [[pdf](https://arxiv.org/pdf/2502.06111)]
> **Authors**: Yijia Xiao,Runhui Wang,Luyang Kong,Davor Golac,Wei Wang
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,人工智能,机器学习
- **Abstract**: The increasing complexity of computer science research projects demands more effective tools for deploying code repositories. Large Language Models (LLMs), such as Anthropic Claude and Meta Llama, have demonstrated significant advancements across various fields of computer science research, including the automation of diverse software engineering tasks. To evaluate the effectiveness of LLMs in handling complex code development tasks of research projects, particularly for NLP/CV/AI/ML/DM topics, we introduce CSR-Bench, a benchmark for Computer Science Research projects. This benchmark assesses LLMs from various aspects including accuracy, efficiency, and deployment script quality, aiming to explore their potential in conducting computer science research autonomously. We also introduce a novel framework, CSR-Agents, that utilizes multiple LLM agents to automate the deployment of GitHub code repositories of computer science research projects. Specifically, by checking instructions from markdown files and interpreting repository structures, the model generates and iteratively improves bash commands that set up the experimental environments and deploy the code to conduct research tasks. Preliminary results from CSR-Bench indicate that LLM agents can significantly enhance the workflow of repository deployment, thereby boosting developer productivity and improving the management of developmental workflows.

### Benchmarking Prompt Engineering Techniques for Secure Code Generation with GPT Models 
[[arxiv](https://arxiv.org/abs/2502.06039)] [[cool](https://papers.cool/arxiv/2502.06039)] [[pdf](https://arxiv.org/pdf/2502.06039)]
> **Authors**: Marc Bruni,Fabio Gabrielli,Mohammad Ghafari,Martin Kropp
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: Accepted at the 2025 IEEE/ACM Second International Conference onAIFoundation Models and Software Engineering (Forge 2025). 10 pages, 7 figures, 5 tables
- **标题**: None
- **领域**: 软件工程,人工智能,密码学和安全
- **Abstract**: Prompt engineering reduces reasoning mistakes in Large Language Models (LLMs). However, its effectiveness in mitigating vulnerabilities in LLM-generated code remains underexplored. To address this gap, we implemented a benchmark to automatically assess the impact of various prompt engineering strategies on code security. Our benchmark leverages two peer-reviewed prompt datasets and employs static scanners to evaluate code security at scale. We tested multiple prompt engineering techniques on GPT-3.5-turbo, GPT-4o, and GPT-4o-mini. Our results show that for GPT-4o and GPT-4o-mini, a security-focused prompt prefix can reduce the occurrence of security vulnerabilities by up to 56%. Additionally, all tested models demonstrated the ability to detect and repair between 41.9% and 68.7% of vulnerabilities in previously generated code when using iterative prompting techniques. Finally, we introduce a "prompt agent" that demonstrates how the most effective techniques can be applied in real-world development workflows.

### Proving the Coding Interview: A Benchmark for Formally Verified Code Generation 
[[arxiv](https://arxiv.org/abs/2502.05714)] [[cool](https://papers.cool/arxiv/2502.05714)] [[pdf](https://arxiv.org/pdf/2502.05714)]
> **Authors**: Quinn Dougherty,Ronak Mehta
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: 8 pages, to appear at the 2025LLM4Code Workshop at ICSE 2025
- **标题**: None
- **领域**: 软件工程,人工智能,机器学习,计算机科学中的逻辑
- **Abstract**: We introduce the Formally Verified Automated Programming Progress Standards, or FVAPPS, a benchmark of 4715 samples for writing programs and proving their correctness, the largest formal verification benchmark, including 1083 curated and quality controlled samples. Previously, APPS provided a benchmark and dataset for programming puzzles to be completed in Python and checked against unit tests, of the kind seen in technical assessments in the software engineering industry. Building upon recent approaches for benchmarks in interactive theorem proving, we generalize the unit tests to Lean 4 theorems given without proof (i.e., using Lean's "sorry" keyword). On the 406 theorems of 100 randomly selected samples, Sonnet correctly proves 30% and Gemini correctly proves 18%. We challenge the machine learning and program synthesis communities to solve both each general purpose programming problem and its associated correctness specifications. The benchmark is available at https://huggingface.co/datasets/quinn-dougherty/fvapps.

### Otter: Generating Tests from Issues to Validate SWE Patches 
[[arxiv](https://arxiv.org/abs/2502.05368)] [[cool](https://papers.cool/arxiv/2502.05368)] [[pdf](https://arxiv.org/pdf/2502.05368)]
> **Authors**: Toufique Ahmed,Jatin Ganhotra,Rangeet Pan,Avraham Shinnar,Saurabh Sinha,Martin Hirzel
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,机器学习
- **Abstract**: While there has been plenty of work on generating tests from existing code, there has been limited work on generating tests from issues. A correct test must validate the code patch that resolves the issue. In this work, we focus on the scenario where the code patch does not exist yet. This approach supports two major use-cases. First, it supports TDD (test-driven development), the discipline of "test first, write code later" that has well-documented benefits for human software engineers. Second, it also validates SWE (software engineering) agents, which generate code patches for resolving issues. This paper introduces Otter, an LLM-based solution for generating tests from issues. Otter augments LLMs with rule-based analysis to check and repair their outputs, and introduces a novel self-reflective action planning stage. Experiments show Otter outperforming state-of-the-art systems for generating tests from issues, in addition to enhancing systems that generate patches from issues. We hope that Otter helps make developers more productive at resolving issues and leads to more robust, well-tested code.

### RAG-Verus: Repository-Level Program Verification with LLMs using Retrieval Augmented Generation 
[[arxiv](https://arxiv.org/abs/2502.05344)] [[cool](https://papers.cool/arxiv/2502.05344)] [[pdf](https://arxiv.org/pdf/2502.05344)]
> **Authors**: Sicheng Zhong,Jiading Zhu,Yifang Tian,Xujie Si
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: Scaling automated formal verification to real-world projects requires resolving cross-module dependencies and global contexts, which are challenges overlooked by existing function-centric methods. We introduce RagVerus, a framework that synergizes retrieval-augmented generation with context-aware prompting to automate proof synthesis for multi-module repositories, achieving a 27% relative improvement on our novel RepoVBench benchmark -- the first repository-level dataset for Verus with 383 proof completion tasks. RagVerus triples proof pass rates on existing benchmarks under constrained language model budgets, demonstrating a scalable and sample-efficient verification.

### Every Software as an Agent: Blueprint and Case Study 
[[arxiv](https://arxiv.org/abs/2502.04747)] [[cool](https://papers.cool/arxiv/2502.04747)] [[pdf](https://arxiv.org/pdf/2502.04747)]
> **Authors**: Mengwei Xu
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: The rise of (multimodal) large language models (LLMs) has shed light on software agent -- where software can understand and follow user instructions in natural language. However, existing approaches such as API-based and GUI-based agents are far from satisfactory at accuracy and efficiency aspects. Instead, we advocate to endow LLMs with access to the software internals (source code and runtime context) and the permission to dynamically inject generated code into software for execution. In such a whitebox setting, one may better leverage the software context and the coding ability of LLMs. We then present an overall design architecture and case studies on two popular web-based desktop applications. We also give in-depth discussion of the challenges and future directions. We deem that such a new paradigm has the potential to fundamentally overturn the existing software agent design, and finally creating a digital world in which software can comprehend, operate, collaborate, and even think to meet complex user needs.

## 音频和语音处理(eess.AS:Audio and Speech Processing)

### Unbiased Sliced Wasserstein Kernels for High-Quality Audio Captioning 
[[arxiv](https://arxiv.org/abs/2502.05435)] [[cool](https://papers.cool/arxiv/2502.05435)] [[pdf](https://arxiv.org/pdf/2502.05435)]
> **Authors**: Manh Luong,Khai Nguyen,Dinh Phung,Gholamreza Haffari,Lizhen Qu
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 17 pages, 9 tables, 2 figures
- **标题**: None
- **领域**: 音频和语音处理,人工智能,机器学习
- **Abstract**: Teacher-forcing training for audio captioning usually leads to exposure bias due to training and inference mismatch. Prior works propose the contrastive method to deal with caption degeneration. However, the contrastive method ignores the temporal information when measuring similarity across acoustic and linguistic modalities, leading to inferior performance. In this work, we develop the temporal-similarity score by introducing the unbiased sliced Wasserstein RBF (USW-RBF) kernel equipped with rotary positional embedding to account for temporal information across modalities. In contrast to the conventional sliced Wasserstein RBF kernel, we can form an unbiased estimation of USW-RBF kernel via Monte Carlo estimation. Therefore, it is well-suited to stochastic gradient optimization algorithms, and its approximation error decreases at a parametric rate of $\mathcal{O}(L^{-1/2})$ with $L$ Monte Carlo samples. Additionally, we introduce an audio captioning framework based on the unbiased sliced Wasserstein kernel, incorporating stochastic decoding methods to mitigate caption degeneration during the generation process. We conduct extensive quantitative and qualitative experiments on two datasets, AudioCaps and Clotho, to illustrate the capability of generating high-quality audio captions. Experimental results show that our framework is able to increase caption length, lexical diversity, and text-to-audio self-retrieval accuracy.

### Efficient Evaluation of Quantization-Effects in Neural Codecs 
[[arxiv](https://arxiv.org/abs/2502.04770)] [[cool](https://papers.cool/arxiv/2502.04770)] [[pdf](https://arxiv.org/pdf/2502.04770)]
> **Authors**: Wolfgang Mack,Ahmed Mustafa,Rafał Łaganowski,Samer Hijazy
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 音频和语音处理,机器学习
- **Abstract**: Neural codecs, comprising an encoder, quantizer, and decoder, enable signal transmission at exceptionally low bitrates. Training these systems requires techniques like the straight-through estimator, soft-to-hard annealing, or statistical quantizer emulation to allow a non-zero gradient across the quantizer. Evaluating the effect of quantization in neural codecs, like the influence of gradient passing techniques on the whole system, is often costly and time-consuming due to training demands and the lack of affordable and reliable metrics. This paper proposes an efficient evaluation framework for neural codecs using simulated data with a defined number of bits and low-complexity neural encoders/decoders to emulate the non-linear behavior in larger networks. Our system is highly efficient in terms of training time and computational and hardware requirements, allowing us to uncover distinct behaviors in neural codecs. We propose a modification to stabilize training with the straight-through estimator based on our findings. We validate our findings against an internal neural audio codec and against the state-of-the-art descript-audio-codec.

## 图像和视频处理(eess.IV:Image and Video Processing)

### Multi-modal Data Fusion and Deep Ensemble Learning for Accurate Crop Yield Prediction 
[[arxiv](https://arxiv.org/abs/2502.06062)] [[cool](https://papers.cool/arxiv/2502.06062)] [[pdf](https://arxiv.org/pdf/2502.06062)]
> **Authors**: Akshay Dagadu Yewle,Laman Mirzayeva,Oktay Karakuş
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: 28 pages, 7 figures and 5 tables
- **标题**: None
- **领域**: 图像和视频处理,人工智能
- **Abstract**: This study introduces RicEns-Net, a novel Deep Ensemble model designed to predict crop yields by integrating diverse data sources through multimodal data fusion techniques. The research focuses specifically on the use of synthetic aperture radar (SAR), optical remote sensing data from Sentinel 1, 2, and 3 satellites, and meteorological measurements such as surface temperature and rainfall. The initial field data for the study were acquired through Ernst & Young's (EY) Open Science Challenge 2023. The primary objective is to enhance the precision of crop yield prediction by developing a machine-learning framework capable of handling complex environmental data. A comprehensive data engineering process was employed to select the most informative features from over 100 potential predictors, reducing the set to 15 features from 5 distinct modalities. This step mitigates the ``curse of dimensionality" and enhances model performance. The RicEns-Net architecture combines multiple machine learning algorithms in a deep ensemble framework, integrating the strengths of each technique to improve predictive accuracy. Experimental results demonstrate that RicEns-Net achieves a mean absolute error (MAE) of 341 kg/Ha (roughly corresponds to 5-6\% of the lowest average yield in the region), significantly exceeding the performance of previous state-of-the-art models, including those developed during the EY challenge.

### A Generative Framework for Bidirectional Image-Report Understanding in Chest Radiography 
[[arxiv](https://arxiv.org/abs/2502.05926)] [[cool](https://papers.cool/arxiv/2502.05926)] [[pdf](https://arxiv.org/pdf/2502.05926)]
> **Authors**: Nicholas Evans,Stephen Baker,Miles Reed
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算语言学,计算机视觉和模式识别
- **Abstract**: The rapid advancements in large language models (LLMs) have unlocked their potential for multimodal tasks, where text and visual data are processed jointly. However, applying LLMs to medical imaging, particularly for chest X-rays (CXR), poses significant challenges due to the need for precise visual-textual alignment and the preservation of critical diagnostic details. In this paper, we propose Multi-Stage Adaptive Vision-Language Tuning (MAViLT), a novel framework designed to enhance multimodal reasoning and generation for CXR understanding. MAViLT incorporates a clinical gradient-weighted tokenization process and a hierarchical fine-tuning strategy, enabling it to generate accurate radiology reports, synthesize realistic CXRs from text, and answer vision-based clinical questions. We evaluate MAViLT on two benchmark datasets, MIMIC-CXR and Indiana University CXR, achieving state-of-the-art results across all tasks. Human evaluations further validate the clinical relevance and utility of MAViLT, making it a robust tool for real-world medical applications. This work demonstrates the feasibility of leveraging LLMs for multimodal medical imaging while addressing key challenges in vision-language integration.

### Inverse Problem Sampling in Latent Space Using Sequential Monte Carlo 
[[arxiv](https://arxiv.org/abs/2502.05908)] [[cool](https://papers.cool/arxiv/2502.05908)] [[pdf](https://arxiv.org/pdf/2502.05908)]
> **Authors**: Idan Achituve,Hai Victor Habi,Amir Rosenfeld,Arnon Netzer,Idit Diamant,Ethan Fetaya
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习
- **Abstract**: In image processing, solving inverse problems is the task of finding plausible reconstructions of an image that was corrupted by some (usually known) degradation model. Commonly, this process is done using a generative image model that can guide the reconstruction towards solutions that appear natural. The success of diffusion models over the last few years has made them a leading candidate for this task. However, the sequential nature of diffusion models makes this conditional sampling process challenging. Furthermore, since diffusion models are often defined in the latent space of an autoencoder, the encoder-decoder transformations introduce additional difficulties. Here, we suggest a novel sampling method based on sequential Monte Carlo (SMC) in the latent space of diffusion models. We use the forward process of the diffusion model to add additional auxiliary observations and then perform an SMC sampling as part of the backward process. Empirical evaluations on ImageNet and FFHQ show the benefits of our approach over competing methods on various inverse problem tasks.

### Image-Based Alzheimer's Disease Detection Using Pretrained Convolutional Neural Network Models 
[[arxiv](https://arxiv.org/abs/2502.05815)] [[cool](https://papers.cool/arxiv/2502.05815)] [[pdf](https://arxiv.org/pdf/2502.05815)]
> **Authors**: Nasser A Alsadhan
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: ef:Journal of Computer Science 19(7), 877-887 2023
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习
- **Abstract**: Alzheimer's disease is an untreatable, progressive brain disorder that slowly robs people of their memory, thinking abilities, and ultimately their capacity to complete even the most basic tasks. Among older adults, it is the most frequent cause of dementia. Although there is presently no treatment for Alzheimer's disease, scientific trials are ongoing to discover drugs to combat the condition. Treatments to slow the signs of dementia are also available. Many researchers throughout the world became interested in developing computer-aided diagnosis systems to aid in the early identification of this deadly disease and assure an accurate diagnosis. In particular, image based approaches have been coupled with machine learning techniques to address the challenges of Alzheimer's disease detection. This study proposes a computer aided diagnosis system to detect Alzheimer's disease from biomarkers captured using neuroimaging techniques. The proposed approach relies on deep learning techniques to extract the relevant visual features from the image collection to accurately predict the Alzheimer's class value. In the experiments, standard datasets and pre-trained deep learning models were investigated. Moreover, standard performance measures were used to assess the models' performances. The obtained results proved that VGG16-based models outperform the state of the art performance.

### 4D VQ-GAN: Synthesising Medical Scans at Any Time Point for Personalised Disease Progression Modelling of Idiopathic Pulmonary Fibrosis 
[[arxiv](https://arxiv.org/abs/2502.05713)] [[cool](https://papers.cool/arxiv/2502.05713)] [[pdf](https://arxiv.org/pdf/2502.05713)]
> **Authors**: An Zhao,Moucheng Xu,Ahmed H. Shahin,Wim Wuyts,Mark G. Jones,Joseph Jacob,Daniel C. Alexander
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: 4D image synthesis, VQ-GAN,neuralODEs, spatial temporal disease progression modelling, CT, IPF
- **标题**: None
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别,机器学习
- **Abstract**: Understanding the progression trajectories of diseases is crucial for early diagnosis and effective treatment planning. This is especially vital for life-threatening conditions such as Idiopathic Pulmonary Fibrosis (IPF), a chronic, progressive lung disease with a prognosis comparable to many cancers. Computed tomography (CT) imaging has been established as a reliable diagnostic tool for IPF. Accurately predicting future CT scans of early-stage IPF patients can aid in developing better treatment strategies, thereby improving survival outcomes. In this paper, we propose 4D Vector Quantised Generative Adversarial Networks (4D-VQ-GAN), a model capable of generating realistic CT volumes of IPF patients at any time point. The model is trained using a two-stage approach. In the first stage, a 3D-VQ-GAN is trained to reconstruct CT volumes. In the second stage, a Neural Ordinary Differential Equation (ODE) based temporal model is trained to capture the temporal dynamics of the quantised embeddings generated by the encoder in the first stage. We evaluate different configurations of our model for generating longitudinal CT scans and compare the results against ground truth data, both quantitatively and qualitatively. For validation, we conduct survival analysis using imaging biomarkers derived from generated CT scans and achieve a C-index comparable to that of biomarkers derived from the real CT scans. The survival analysis results demonstrate the potential clinical utility inherent to generated longitudinal CT scans, showing that they can reliably predict survival outcomes.

### A Novel Convolutional-Free Method for 3D Medical Imaging Segmentation 
[[arxiv](https://arxiv.org/abs/2502.05396)] [[cool](https://papers.cool/arxiv/2502.05396)] [[pdf](https://arxiv.org/pdf/2502.05396)]
> **Authors**: Canxuan Gang
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: technical report
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: Segmentation of 3D medical images is a critical task for accurate diagnosis and treatment planning. Convolutional neural networks (CNNs) have dominated the field, achieving significant success in 3D medical image segmentation. However, CNNs struggle with capturing long-range dependencies and global context, limiting their performance, particularly for fine and complex structures. Recent transformer-based models, such as TransUNet and nnFormer, have demonstrated promise in addressing these limitations, though they still rely on hybrid CNN-transformer architectures. This paper introduces a novel, fully convolutional-free model based on transformer architecture and self-attention mechanisms for 3D medical image segmentation. Our approach focuses on improving multi-semantic segmentation accuracy and addressing domain adaptation challenges between thick and thin slice CT images. We propose a joint loss function that facilitates effective segmentation of thin slices based on thick slice annotations, overcoming limitations in dataset availability. Furthermore, we present a benchmark dataset for multi-semantic segmentation on thin slices, addressing a gap in current medical imaging research. Our experiments demonstrate the superiority of the proposed model over traditional and hybrid architectures, offering new insights into the future of convolution-free medical image segmentation.

### Multi-Class Segmentation of Aortic Branches and Zones in Computed Tomography Angiography: The AortaSeg24 Challenge 
[[arxiv](https://arxiv.org/abs/2502.05330)] [[cool](https://papers.cool/arxiv/2502.05330)] [[pdf](https://arxiv.org/pdf/2502.05330)]
> **Authors**: Muhammad Imran,Jonathan R. Krebs,Vishal Balaji Sivaraman,Teng Zhang,Amarjeet Kumar,Walker R. Ueland,Michael J. Fassler,Jinlong Huang,Xiao Sun,Lisheng Wang,Pengcheng Shi,Maximilian Rokuss,Michael Baumgartner,Yannick Kirchhof,Klaus H. Maier-Hein,Fabian Isensee,Shuolin Liu,Bing Han,Bong Thanh Nguyen,Dong-jin Shin,Park Ji-Woo,Mathew Choi,Kwang-Hyun Uhm,Sung-Jea Ko,Chanwoong Lee, et al. (38 additional authors not shown)
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别,机器学习
- **Abstract**: Multi-class segmentation of the aorta in computed tomography angiography (CTA) scans is essential for diagnosing and planning complex endovascular treatments for patients with aortic dissections. However, existing methods reduce aortic segmentation to a binary problem, limiting their ability to measure diameters across different branches and zones. Furthermore, no open-source dataset is currently available to support the development of multi-class aortic segmentation methods. To address this gap, we organized the AortaSeg24 MICCAI Challenge, introducing the first dataset of 100 CTA volumes annotated for 23 clinically relevant aortic branches and zones. This dataset was designed to facilitate both model development and validation. The challenge attracted 121 teams worldwide, with participants leveraging state-of-the-art frameworks such as nnU-Net and exploring novel techniques, including cascaded models, data augmentation strategies, and custom loss functions. We evaluated the submitted algorithms using the Dice Similarity Coefficient (DSC) and Normalized Surface Distance (NSD), highlighting the approaches adopted by the top five performing teams. This paper presents the challenge design, dataset details, evaluation metrics, and an in-depth analysis of the top-performing algorithms. The annotated dataset, evaluation code, and implementations of the leading methods are publicly available to support further research. All resources can be accessed at https://aortaseg24.grand-challenge.org.

### CoRPA: Adversarial Image Generation for Chest X-rays Using Concept Vector Perturbations and Generative Models 
[[arxiv](https://arxiv.org/abs/2502.05214)] [[cool](https://papers.cool/arxiv/2502.05214)] [[pdf](https://arxiv.org/pdf/2502.05214)]
> **Authors**: Amy Rafferty,Rishi Ramaesh,Ajitha Rajan
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-10
> **comment**: 12 pages, 5 figures
- **标题**: None
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **Abstract**: Deep learning models for medical image classification tasks are becoming widely implemented in AI-assisted diagnostic tools, aiming to enhance diagnostic accuracy, reduce clinician workloads, and improve patient outcomes. However, their vulnerability to adversarial attacks poses significant risks to patient safety. Current attack methodologies use general techniques such as model querying or pixel value perturbations to generate adversarial examples designed to fool a model. These approaches may not adequately address the unique characteristics of clinical errors stemming from missed or incorrectly identified clinical features. We propose the Concept-based Report Perturbation Attack (CoRPA), a clinically-focused black-box adversarial attack framework tailored to the medical imaging domain. CoRPA leverages clinical concepts to generate adversarial radiological reports and images that closely mirror realistic clinical misdiagnosis scenarios. We demonstrate the utility of CoRPA using the MIMIC-CXR-JPG dataset of chest X-rays and radiological reports. Our evaluation reveals that deep learning models exhibiting strong resilience to conventional adversarial attacks are significantly less robust when subjected to CoRPA's clinically-focused perturbations. This underscores the importance of addressing domain-specific vulnerabilities in medical AI systems. By introducing a specialized adversarial attack framework, this study provides a foundation for developing robust, real-world-ready AI models in healthcare, ensuring their safe and reliable deployment in high-stakes clinical environments.

### CMamba: Learned Image Compression with State Space Models 
[[arxiv](https://arxiv.org/abs/2502.04988)] [[cool](https://papers.cool/arxiv/2502.04988)] [[pdf](https://arxiv.org/pdf/2502.04988)]
> **Authors**: Zhuojie Wu,Heming Du,Shuyun Wang,Ming Lu,Haiyang Sun,Yandong Guo,Xin Yu
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: Learned Image Compression (LIC) has explored various architectures, such as Convolutional Neural Networks (CNNs) and transformers, in modeling image content distributions in order to achieve compression effectiveness. However, achieving high rate-distortion performance while maintaining low computational complexity (\ie, parameters, FLOPs, and latency) remains challenging. In this paper, we propose a hybrid Convolution and State Space Models (SSMs) based image compression framework, termed \textit{CMamba}, to achieve superior rate-distortion performance with low computational complexity. Specifically, CMamba introduces two key components: a Content-Adaptive SSM (CA-SSM) module and a Context-Aware Entropy (CAE) module. First, we observed that SSMs excel in modeling overall content but tend to lose high-frequency details. In contrast, CNNs are proficient at capturing local details. Motivated by this, we propose the CA-SSM module that can dynamically fuse global content extracted by SSM blocks and local details captured by CNN blocks in both encoding and decoding stages. As a result, important image content is well preserved during compression. Second, our proposed CAE module is designed to reduce spatial and channel redundancies in latent representations after encoding. Specifically, our CAE leverages SSMs to parameterize the spatial content in latent representations. Benefiting from SSMs, CAE significantly improves spatial compression efficiency while reducing spatial content redundancies. Moreover, along the channel dimension, CAE reduces inter-channel redundancies of latent representations via an autoregressive manner, which can fully exploit prior knowledge from previous channels without sacrificing efficiency. Experimental results demonstrate that CMamba achieves superior rate-distortion performance.

### Wavelet-Assisted Multi-Frequency Attention Network for Pansharpening 
[[arxiv](https://arxiv.org/abs/2502.04903)] [[cool](https://papers.cool/arxiv/2502.04903)] [[pdf](https://arxiv.org/pdf/2502.04903)]
> **Authors**: Jie Huang,Rui Huang,Jinghao Xu,Siran Pen,Yule Duan,Liangjian Deng
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 12 pages, 13 figures
- **标题**: None
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **Abstract**: Pansharpening aims to combine a high-resolution panchromatic (PAN) image with a low-resolution multispectral (LRMS) image to produce a high-resolution multispectral (HRMS) image. Although pansharpening in the frequency domain offers clear advantages, most existing methods either continue to operate solely in the spatial domain or fail to fully exploit the benefits of the frequency domain. To address this issue, we innovatively propose Multi-Frequency Fusion Attention (MFFA), which leverages wavelet transforms to cleanly separate frequencies and enable lossless reconstruction across different frequency domains. Then, we generate Frequency-Query, Spatial-Key, and Fusion-Value based on the physical meanings represented by different features, which enables a more effective capture of specific information in the frequency domain. Additionally, we focus on the preservation of frequency features across different operations. On a broader level, our network employs a wavelet pyramid to progressively fuse information across multiple scales. Compared to previous frequency domain approaches, our network better prevents confusion and loss of different frequency features during the fusion process. Quantitative and qualitative experiments on multiple datasets demonstrate that our method outperforms existing approaches and shows significant generalization capabilities for real-world scenarios.

### ARTInp: CBCT-to-CT Image Inpainting and Image Translation in Radiotherapy 
[[arxiv](https://arxiv.org/abs/2502.04898)] [[cool](https://papers.cool/arxiv/2502.04898)] [[pdf](https://arxiv.org/pdf/2502.04898)]
> **Authors**: Ricardo Coimbra Brioso,Leonardo Crespi,Andrea Seghetto,Damiano Dei,Nicola Lambri,Pietro Mancosu,Marta Scorsetti,Daniele Loiacono
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **Abstract**: A key step in Adaptive Radiation Therapy (ART) workflows is the evaluation of the patient's anatomy at treatment time to ensure the accuracy of the delivery. To this end, Cone Beam Computerized Tomography (CBCT) is widely used being cost-effective and easy to integrate into the treatment process. Nonetheless, CBCT images have lower resolution and more artifacts than CT scans, making them less reliable for precise treatment validation. Moreover, in complex treatments such as Total Marrow and Lymph Node Irradiation (TMLI), where full-body visualization of the patient is critical for accurate dose delivery, the CBCT images are often discontinuous, leaving gaps that could contain relevant anatomical information. To address these limitations, we propose ARTInp (Adaptive Radiation Therapy Inpainting), a novel deep-learning framework combining image inpainting and CBCT-to-CT translation. ARTInp employs a dual-network approach: a completion network that fills anatomical gaps in CBCT volumes and a custom Generative Adversarial Network (GAN) to generate high-quality synthetic CT (sCT) images. We trained ARTInp on a dataset of paired CBCT and CT images from the SynthRad 2023 challenge, and the performance achieved on a test set of 18 patients demonstrates its potential for enhancing CBCT-based workflows in radiotherapy.

### MedMimic: Physician-Inspired Multimodal Fusion for Early Diagnosis of Fever of Unknown Origin 
[[arxiv](https://arxiv.org/abs/2502.04794)] [[cool](https://papers.cool/arxiv/2502.04794)] [[pdf](https://arxiv.org/pdf/2502.04794)]
> **Authors**: Minrui Chen,Yi Zhou,Huidong Jiang,Yuhan Zhu,Guanjie Zou,Minqi Chen,Rong Tian,Hiroto Saigo
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **Abstract**: Fever of unknown origin FUO remains a diagnostic challenge. MedMimic is introduced as a multimodal framework inspired by real-world diagnostic processes. It uses pretrained models such as DINOv2, Vision Transformer, and ResNet-18 to convert high-dimensional 18F-FDG PET/CT imaging into low-dimensional, semantically meaningful features. A learnable self-attention-based fusion network then integrates these imaging features with clinical data for classification. Using 416 FUO patient cases from Sichuan University West China Hospital from 2017 to 2023, the multimodal fusion classification network MFCN achieved macro-AUROC scores ranging from 0.8654 to 0.9291 across seven tasks, outperforming conventional machine learning and single-modality deep learning methods. Ablation studies and five-fold cross-validation further validated its effectiveness. By combining the strengths of pretrained large models and deep learning, MedMimic offers a promising solution for disease classification.

### Leveraging band diversity for feature selection in EO data 
[[arxiv](https://arxiv.org/abs/2502.04713)] [[cool](https://papers.cool/arxiv/2502.04713)] [[pdf](https://arxiv.org/pdf/2502.04713)]
> **Authors**: Sadia Hussain,Brejesh Lall
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: Hyperspectral imaging (HSI) is a powerful earth observation technology that captures and processes information across a wide spectrum of wavelengths. Hyperspectral imaging provides comprehensive and detailed spectral data that is invaluable for a wide range of reconstruction problems. However due to complexity in analysis it often becomes difficult to handle this data. To address the challenge of handling large number of bands in reconstructing high quality HSI, we propose to form groups of bands. In this position paper we propose a method of selecting diverse bands using determinantal point processes in correlated bands. To address the issue of overlapping bands that may arise from grouping, we use spectral angle mapper analysis. This analysis can be fed to any Machine learning model to enable detailed analysis and monitoring with high precision and accuracy.

## 信号处理(eess.SP:Signal Processing)

### Towards Smarter Sensing: 2D Clutter Mitigation in RL-Driven Cognitive MIMO Radar 
[[arxiv](https://arxiv.org/abs/2502.04967)] [[cool](https://papers.cool/arxiv/2502.04967)] [[pdf](https://arxiv.org/pdf/2502.04967)]
> **Authors**: Adam Umra,Aya Mostafa Ahmed,Aydin Sezgin
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 6 pages, 8 figures. Submitted to EuCNC 2025
- **标题**: None
- **领域**: 信号处理,机器学习
- **Abstract**: Motivated by the growing interest in integrated sensing and communication for 6th generation (6G) networks, this paper presents a cognitive Multiple-Input Multiple-Output (MIMO) radar system enhanced by reinforcement learning (RL) for robust multitarget detection in dynamic environments. The system employs a planar array configuration and adapts its transmitted waveforms and beamforming patterns to optimize detection performance in the presence of unknown two-dimensional (2D) disturbances. A robust Wald-type detector is integrated with a SARSA-based RL algorithm, enabling the radar to learn and adapt to complex clutter environments modeled by a 2D autoregressive process. Simulation results demonstrate significant improvements in detection probability compared to omnidirectional methods, particularly for low Signal-to-Noise Ratio (SNR) targets masked by clutter.

### Explainable and externally validated machine learning for neuropsychiatric diagnosis via electrocardiograms 
[[arxiv](https://arxiv.org/abs/2502.04918)] [[cool](https://papers.cool/arxiv/2502.04918)] [[pdf](https://arxiv.org/pdf/2502.04918)]
> **Authors**: Juan Miguel Lopez Alcaraz,Ebenezer Oloyede,David Taylor,Wilhelm Haverkamp,Nils Strodthoff
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 9 pages, 2 figures, source code under https://github.com/AI4HealthUOL/CardioDiag
- **标题**: None
- **领域**: 信号处理,机器学习
- **Abstract**: Electrocardiogram (ECG) analysis has emerged as a promising tool for identifying physiological changes associated with neuropsychiatric conditions. The relationship between cardiovascular health and neuropsychiatric disorders suggests that ECG abnormalities could serve as valuable biomarkers for more efficient detection, therapy monitoring, and risk stratification. However, the potential of the ECG to accurately distinguish neuropsychiatric conditions, particularly among diverse patient populations, remains underexplored. This study utilized ECG markers and basic demographic data to predict neuropsychiatric conditions using machine learning models, with targets defined through ICD-10 codes. Both internal and external validation were performed using the MIMIC-IV and ECG-View datasets respectively. Performance was assessed using AUROC scores. To enhance model interpretability, Shapley values were applied to provide insights into the contributions of individual ECG features to the predictions. Significant predictive performance was observed for conditions within the neurological and psychiatric groups. For the neurological group, Alzheimer's disease (G30) achieved an internal AUROC of 0.813 (0.812-0.814) and an external AUROC of 0.868 (0.867-0.868). In the psychiatric group, unspecified dementia (F03) showed an internal AUROC of 0.849 (0.848-0.849) and an external AUROC of 0.862 (0.861-0.863). Discriminative features align with known ECG markers but also provide hints on potentially new markers. ECG offers significant promise for diagnosing and monitoring neuropsychiatric conditions, with robust predictive performance across internal and external cohorts. Future work should focus on addressing potential confounders, such as therapy-related cardiotoxicity, and expanding the scope of ECG applications, including personalized care and early intervention strategies.

## 系统与控制(eess.SY:Systems and Control)

### Towards Autonomous Experimentation: Bayesian Optimization over Problem Formulation Space for Accelerated Alloy Development 
[[arxiv](https://arxiv.org/abs/2502.05735)] [[cool](https://papers.cool/arxiv/2502.05735)] [[pdf](https://arxiv.org/pdf/2502.05735)]
> **Authors**: Danial Khatamsaz,Joseph Wagner,Brent Vela,Raymundo Arroyave,Douglas L. Allaire
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 系统与控制,计算工程、金融和科学,机器学习,优化与控制,机器学习
- **Abstract**: Accelerated discovery in materials science demands autonomous systems capable of dynamically formulating and solving design problems. In this work, we introduce a novel framework that leverages Bayesian optimization over a problem formulation space to identify optimal design formulations in line with decision-maker preferences. By mapping various design scenarios to a multi attribute utility function, our approach enables the system to balance conflicting objectives such as ductility, yield strength, density, and solidification range without requiring an exact problem definition at the outset. We demonstrate the efficacy of our method through an in silico case study on a Mo-Nb-Ti-V-W alloy system targeted for gas turbine engine blade applications. The framework converges on a sweet spot that satisfies critical performance thresholds, illustrating that integrating problem formulation discovery into the autonomous design loop can significantly streamline the experimental process. Future work will incorporate human feedback to further enhance the adaptability of the system in real-world experimental settings.

## 高能物理-晶格(hep-lat:High Energy Physics - Lattice)

### Physics-Conditioned Diffusion Models for Lattice Gauge Theory 
[[arxiv](https://arxiv.org/abs/2502.05504)] [[cool](https://papers.cool/arxiv/2502.05504)] [[pdf](https://arxiv.org/pdf/2502.05504)]
> **Authors**: Qianteng Zhu,Gert Aarts,Wei Wang,Kai Zhou,Lingxiao Wang
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: 25 pages, 10 figures, comments are welcome! Codes are available at: https://github.com/zzzqt/DM4U1
- **标题**: None
- **领域**: 高能物理-晶格,机器学习
- **Abstract**: We develop diffusion models for simulating lattice gauge theories, where stochastic quantization is explicitly incorporated as a physical condition for sampling. We demonstrate the applicability of this novel sampler to U(1) gauge theory in two spacetime dimensions and find that a model trained at a small inverse coupling constant can be extrapolated to larger inverse coupling regions without encountering the topological freezing problem. Additionally, the trained model can be employed to sample configurations on different lattice sizes without requiring further training. The exactness of the generated samples is ensured by incorporating Metropolis-adjusted Langevin dynamics into the generation process. Furthermore, we demonstrate that this approach enables more efficient sampling of topological quantities compared to traditional algorithms such as Hybrid Monte Carlo and Langevin simulations.

## 高能物理 - 理论(hep-th:High Energy Physics - Theory)

### Refining Integration-by-Parts Reduction of Feynman Integrals with Machine Learning 
[[arxiv](https://arxiv.org/abs/2502.05121)] [[cool](https://papers.cool/arxiv/2502.05121)] [[pdf](https://arxiv.org/pdf/2502.05121)]
> **Authors**: Matt von Hippel,Matthias Wilhelm
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 28 pages, 9 figures
- **标题**: None
- **领域**: 高能物理 - 理论,机器学习,高能物理-现象学
- **Abstract**: Integration-by-parts reductions of Feynman integrals pose a frequent bottle-neck in state-of-the-art calculations in theoretical particle and gravitational-wave physics, and rely on heuristic approaches for selecting integration-by-parts identities, whose quality heavily influences the performance. In this paper, we investigate the use of machine-learning techniques to find improved heuristics. We use funsearch, a genetic programming variant based on code generation by a Large Language Model, in order to explore possible approaches, then use strongly typed genetic programming to zero in on useful solutions. Both approaches manage to re-discover the state-of-the-art heuristics recently incorporated into integration-by-parts solvers, and in one example find a small advance on this state of the art.

## 动力系统(math.DS:Dynamical Systems)

### Invariant Measures for Data-Driven Dynamical System Identification: Analysis and Application 
[[arxiv](https://arxiv.org/abs/2502.05204)] [[cool](https://papers.cool/arxiv/2502.05204)] [[pdf](https://arxiv.org/pdf/2502.05204)]
> **Authors**: Jonah Botvinick-Greenhouse
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-10
> **comment**: This article draws heavily from arXiv:2301.05193 and arXiv:2412.00589
- **标题**: None
- **领域**: 动力系统,机器学习,混沌动力学,数据分析、统计和概率
- **Abstract**: We propose a novel approach for performing dynamical system identification, based upon the comparison of simulated and observed physical invariant measures. While standard methods adopt a Lagrangian perspective by directly treating time-trajectories as inference data, we take on an Eulerian perspective and instead seek models fitting the observed global time-invariant statistics. With this change in perspective, we gain robustness against pervasive challenges in system identification including noise, chaos, and slow sampling. In the first half of this paper, we pose the system identification task as a partial differential equation (PDE) constrained optimization problem, in which synthetic stationary solutions of the Fokker-Planck equation, obtained as fixed points of a finite-volume discretization, are compared to physical invariant measures extracted from observed trajectory data. In the latter half of the paper, we improve upon this approach in two crucial directions. First, we develop a Galerkin-inspired modification to the finite-volume surrogate model, based on data-adaptive unstructured meshes and Monte-Carlo integration, enabling the approach to efficiently scale to high-dimensional problems. Second, we leverage Takens' seminal time-delay embedding theory to introduce a critical data-dependent coordinate transformation which can guarantee unique system identifiability from the invariant measure alone. This contribution resolves a major challenge of system identification through invariant measures, as systems exhibiting distinct transient behaviors may still share the same time-invariant statistics in their state-coordinates. Throughout, we present comprehensive numerical tests which highlight the effectiveness of our approach on a variety of challenging system identification tasks.

## 数值分析(math.NA:Numerical Analysis)

### Learning Memory and Material Dependent Constitutive Laws 
[[arxiv](https://arxiv.org/abs/2502.05463)] [[cool](https://papers.cool/arxiv/2502.05463)] [[pdf](https://arxiv.org/pdf/2502.05463)]
> **Authors**: Kaushik Bhattacharya,Lianghao Cao,George Stepaniants,Andrew Stuart,Margaret Trautner
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: 48 pages, 11 figures
- **标题**: None
- **领域**: 数值分析,机器学习
- **Abstract**: The theory of homogenization provides a systematic approach to the derivation of macroscale constitutive laws, obviating the need to repeatedly resolve complex microstructure. However, the unit cell problem that defines the constitutive model is typically not amenable to explicit evaluation. It is therefore of interest to learn constitutive models from data generated by the unit cell problem. Many viscoelastic and elastoviscoplastic materials are characterized by memory-dependent constitutive laws. In order to amortize the computational investment in finding such memory-dependent constitutive laws, it is desirable to learn their dependence on the material microstructure. While prior work has addressed learning memory dependence and material dependence separately, their joint learning has not been considered. This paper focuses on the joint learning problem and proposes a novel neural operator framework to address it. In order to provide firm foundations, the homogenization problem for linear Kelvin-Voigt viscoelastic materials is studied. The theoretical properties of the cell problem in this Kelvin-Voigt setting are used to motivate the proposed general neural operator framework; these theoretical properties are also used to prove a universal approximation theorem for the learned macroscale constitutive model. This formulation of learnable constitutive models is then deployed beyond the Kelvin-Voigt setting. Numerical experiments are presented showing that the resulting data-driven methodology accurately learns history- and microstructure-dependent linear viscoelastic and nonlinear elastoviscoplastic constitutive models, and numerical results also demonstrate that the resulting constitutive models can be deployed in macroscale simulation of material deformation.

### Symbolic Regression of Data-Driven Reduced Order Model Closures for Under-Resolved, Convection-Dominated Flows 
[[arxiv](https://arxiv.org/abs/2502.04703)] [[cool](https://papers.cool/arxiv/2502.04703)] [[pdf](https://arxiv.org/pdf/2502.04703)]
> **Authors**: Simone Manti,Ping-Hsuan Tsai,Alessandro Lucantonio,Traian Iliescu
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 数值分析,机器学习,流体动力学
- **Abstract**: Data-driven closures correct the standard reduced order models (ROMs) to increase their accuracy in under-resolved, convection-dominated flows. There are two types of data-driven ROM closures in current use: (i) structural, with simple ansatzes (e.g., linear or quadratic); and (ii) machine learning-based, with neural network ansatzes. We propose a novel symbolic regression (SR) data-driven ROM closure strategy, which combines the advantages of current approaches and eliminates their drawbacks. As a result, the new data-driven SR closures yield ROMs that are interpretable, parsimonious, accurate, generalizable, and robust. To compare the data-driven SR-ROM closures with the structural and machine learning-based ROM closures, we consider the data-driven variational multiscale ROM framework and two under-resolved, convection-dominated test problems: the flow past a cylinder and the lid-driven cavity flow at Reynolds numbers Re = 10000, 15000, and 20000. This numerical investigation shows that the new data-driven SR-ROM closures yield more accurate and robust ROMs than the structural and machine learning ROM closures.

## 优化与控制(math.OC:Optimization and Control)

### Contextual Scenario Generation for Two-Stage Stochastic Programming 
[[arxiv](https://arxiv.org/abs/2502.05349)] [[cool](https://papers.cool/arxiv/2502.05349)] [[pdf](https://arxiv.org/pdf/2502.05349)]
> **Authors**: David Islip,Roy H. Kwon,Sanghyeon Bae,Woo Chang Kim
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 47 pages, 10 figures
- **标题**: None
- **领域**: 优化与控制,机器学习
- **Abstract**: Two-stage stochastic programs (2SPs) are important tools for making decisions under uncertainty. Decision-makers use contextual information to generate a set of scenarios to represent the true conditional distribution. However, the number of scenarios required is a barrier to implementing 2SPs, motivating the problem of generating a small set of surrogate scenarios that yield high-quality decisions when they represent uncertainty. Current scenario generation approaches do not leverage contextual information or do not address computational concerns. In response, we propose contextual scenario generation (CSG) to learn a mapping between the context and a set of surrogate scenarios of user-specified size. First, we propose a distributional approach that learns the mapping by minimizing a distributional distance between the predicted surrogate scenarios and the true contextual distribution. Second, we propose a task-based approach that aims to produce surrogate scenarios that yield high-quality decisions. The task-based approach uses neural architectures to approximate the downstream objective and leverages the approximation to search for the mapping. The proposed approaches apply to various problem structures and loosely only require efficient solving of the associated subproblems and 2SPs defined on the reduced scenario sets. Numerical experiments demonstrating the effectiveness of the proposed methods are presented.

### Blackout DIFUSCO 
[[arxiv](https://arxiv.org/abs/2502.05221)] [[cool](https://papers.cool/arxiv/2502.05221)] [[pdf](https://arxiv.org/pdf/2502.05221)]
> **Authors**: Jun Pyo Seo
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-10
> **comment**: 12 pages
- **标题**: None
- **领域**: 优化与控制,人工智能
- **Abstract**: This study explores the integration of Blackout Diffusion into the DIFUSCO framework for combinatorial optimization, specifically targeting the Traveling Salesman Problem (TSP). Inspired by the success of discrete-time diffusion models (D3PM) in maintaining structural integrity, we extend the paradigm to a continuous-time framework, leveraging the unique properties of Blackout Diffusion. Continuous-time modeling introduces smoother transitions and refined control, hypothesizing enhanced solution quality over traditional discrete methods. We propose three key improvements to enhance the diffusion process. First, we transition from a discrete-time-based model to a continuous-time framework, providing a more refined and flexible formulation. Second, we refine the observation time scheduling to ensure a smooth and linear transformation throughout the diffusion process, allowing for a more natural progression of states. Finally, building upon the second improvement, we further enhance the reverse process by introducing finer time slices in regions that are particularly challenging for the model, thereby improving accuracy and stability in the reconstruction phase. Although the experimental results did not exceed the baseline performance, they demonstrate the effectiveness of these methods in balancing simplicity and complexity, offering new insights into diffusion-based combinatorial optimization. This work represents the first application of Blackout Diffusion to combinatorial optimization, providing a foundation for further advancements in this domain. * The code is available for review at https://github.com/Giventicket/BlackoutDIFUSCO.

### Coherent Local Explanations for Mathematical Optimization 
[[arxiv](https://arxiv.org/abs/2502.04840)] [[cool](https://papers.cool/arxiv/2502.04840)] [[pdf](https://arxiv.org/pdf/2502.04840)]
> **Authors**: Daan Otto,Jannis Kurtz,S. Ilker Birbil
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 优化与控制,机器学习
- **Abstract**: The surge of explainable artificial intelligence methods seeks to enhance transparency and explainability in machine learning models. At the same time, there is a growing demand for explaining decisions taken through complex algorithms used in mathematical optimization. However, current explanation methods do not take into account the structure of the underlying optimization problem, leading to unreliable outcomes. In response to this need, we introduce Coherent Local Explanations for Mathematical Optimization (CLEMO). CLEMO provides explanations for multiple components of optimization models, the objective value and decision variables, which are coherent with the underlying model structure. Our sampling-based procedure can provide explanations for the behavior of exact and heuristic solution algorithms. The effectiveness of CLEMO is illustrated by experiments for the shortest path problem, the knapsack problem, and the vehicle routing problem.

### A Regularized Newton Method for Nonconvex Optimization with Global and Local Complexity Guarantees 
[[arxiv](https://arxiv.org/abs/2502.04799)] [[cool](https://papers.cool/arxiv/2502.04799)] [[pdf](https://arxiv.org/pdf/2502.04799)]
> **Authors**: Yuhao Zhou,Jintao Xu,Chenglong Bao,Chao Ding,Jun Zhu
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 优化与控制,机器学习
- **Abstract**: We consider the problem of finding an $ε$-stationary point of a nonconvex function with a Lipschitz continuous Hessian and propose a quadratic regularized Newton method incorporating a new class of regularizers constructed from the current and previous gradients. The method leverages a recently developed linear conjugate gradient approach with a negative curvature monitor to solve the regularized Newton equation. Notably, our algorithm is adaptive, requiring no prior knowledge of the Lipschitz constant of the Hessian, and achieves a global complexity of $O(ε^{-\frac{3}{2}}) + \tilde O(1)$ in terms of the second-order oracle calls, and $\tilde O(ε^{-\frac{7}{4}})$ for Hessian-vector products, respectively. Moreover, when the iterates converge to a point where the Hessian is positive definite, the method exhibits quadratic local convergence. Preliminary numerical results illustrate the competitiveness of our algorithm.

## 可能性(math.PR:Probability)

### Noise Sensitivity of Hierarchical Functions and Deep Learning Lower Bounds in General Product Measures 
[[arxiv](https://arxiv.org/abs/2502.05073)] [[cool](https://papers.cool/arxiv/2502.05073)] [[pdf](https://arxiv.org/pdf/2502.05073)]
> **Authors**: Rupert Li,Elchanan Mossel
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 17 pages
- **标题**: None
- **领域**: 可能性,计算复杂度,机器学习,组合学
- **Abstract**: Recent works explore deep learning's success by examining functions or data with hierarchical structure. Complementarily, research on gradient descent performance for deep nets has shown that noise sensitivity of functions under independent and identically distributed (i.i.d.) Bernoulli inputs establishes learning complexity bounds. This paper aims to bridge these research streams by demonstrating that functions constructed through repeated composition of non-linear functions are noise sensitive under general product measures.

## 化学物理(physics.chem-ph:Chemical Physics)

### Machine-Learning Interatomic Potentials for Long-Range Systems 
[[arxiv](https://arxiv.org/abs/2502.04668)] [[cool](https://papers.cool/arxiv/2502.04668)] [[pdf](https://arxiv.org/pdf/2502.04668)]
> **Authors**: Yajie Ji,Jiuyang Liang,Zhenli Xu
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 7 pages, 4 figures
- **标题**: None
- **领域**: 化学物理,机器学习
- **Abstract**: Machine-learning interatomic potentials have emerged as a revolutionary class of force-field models in molecular simulations, delivering quantum-mechanical accuracy at a fraction of the computational cost and enabling the simulation of large-scale systems over extended timescales. However, they often focus on modeling local environments, neglecting crucial long-range interactions. We propose a Sum-of-Gaussians Neural Network (SOG-Net), a lightweight and versatile framework for integrating long-range interactions into machine learning force field. The SOG-Net employs a latent-variable learning network that seamlessly bridges short-range and long-range components, coupled with an efficient Fourier convolution layer that incorporates long-range effects. By learning sum-of-Gaussian multipliers across different convolution layers, the SOG-Net adaptively captures diverse long-range decay behaviors while maintaining close-to-linear computational complexity during training and simulation via non-uniform fast Fourier transforms. The method is demonstrated effective for a broad range of long-range systems.

## 地球物理学(physics.geo-ph:Geophysics)

### Inversion of Magnetic Data using Learned Dictionaries and Scale Space 
[[arxiv](https://arxiv.org/abs/2502.05451)] [[cool](https://papers.cool/arxiv/2502.05451)] [[pdf](https://arxiv.org/pdf/2502.05451)]
> **Authors**: Shadab Ahamed,Simon Ghyselincks,Pablo Chang Huang Arias,Julian Kloiber,Yasin Ranjbar,Jingrong Tang,Niloufar Zakariaei,Eldad Haber
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: 13 pages, 2 figures, 2 tables
- **标题**: None
- **领域**: 地球物理学,计算机视觉和模式识别,机器学习
- **Abstract**: Magnetic data inversion is an important tool in geophysics, used to infer subsurface magnetic susceptibility distributions from surface magnetic field measurements. This inverse problem is inherently ill-posed, characterized by non-unique solutions, depth ambiguity, and sensitivity to noise. Traditional inversion approaches rely on predefined regularization techniques to stabilize solutions, limiting their adaptability to complex or diverse geological scenarios. In this study, we propose an approach that integrates variable dictionary learning and scale-space methods to address these challenges. Our method employs learned dictionaries, allowing for adaptive representation of complex subsurface features that are difficult to capture with predefined bases. Additionally, we extend classical variational inversion by incorporating multi-scale representations through a scale-space framework, enabling the progressive introduction of structural detail while mitigating overfitting. We implement both fixed and dynamic dictionary learning techniques, with the latter introducing iteration-dependent dictionaries for enhanced flexibility. Using a synthetic dataset to simulate geological scenarios, we demonstrate significant improvements in reconstruction accuracy and robustness compared to conventional variational and dictionary-based methods. Our results highlight the potential of learned dictionaries, especially when coupled with scale-space dynamics, to improve model recovery and noise handling. These findings underscore the promise of our data-driven approach for advance magnetic data inversion and its applications in geophysical exploration, environmental assessment, and mineral prospecting. The code is publicly available at: https://github.com/ahxmeds/magnetic-inversion-dictionary.git.

### A finite element-based machine learning model for hydro-mechanical analysis of swelling behavior in clay-sulfate rocks 
[[arxiv](https://arxiv.org/abs/2502.05198)] [[cool](https://papers.cool/arxiv/2502.05198)] [[pdf](https://arxiv.org/pdf/2502.05198)]
> **Authors**: Reza Taherdangkoo,Mostafa Mollaali,Matthias Ehrhardt,Thomas Nagel,Lyesse Laloui,Alessio Ferrari,Christoph Butscher
> **First submission**: 2025-01-29
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 地球物理学,机器学习,计算物理
- **Abstract**: The hydro-mechanical behavior of clay-sulfate rocks, especially their swelling properties, poses significant challenges in geotechnical engineering. This study presents a hybrid constrained machine learning (ML) model developed using the categorical boosting algorithm (CatBoost) tuned with a Bayesian optimization algorithm to predict and analyze the swelling behavior of these complex geological materials. Initially, a coupled hydro-mechanical model based on the Richards' equation coupled to a deformation process with linear kinematics implemented within the finite element framework OpenGeoSys was used to simulate the observed ground heave in Staufen, Germany, caused by water inflow into the clay-sulfate bearing Triassic Grabfeld Formation. A systematic parametric analysis using Gaussian distributions of key parameters, including Young's modulus, Poisson's ratio, maximum swelling pressure, permeability, and air entry pressure, was performed to construct a synthetic database. The ML model takes time, spatial coordinates, and these parameter values as inputs, while water saturation, porosity, and vertical displacement are outputs. In addition, penalty terms were incorporated into the CatBoost objective function to enforce physically meaningful predictions. Results show that the hybrid approach effectively captures the nonlinear and dynamic interactions that govern hydro-mechanical processes. The study demonstrates the ability of the model to predict the swelling behavior of clay-sulfate rocks, providing a robust tool for risk assessment and management in affected regions. The results highlight the potential of ML-driven models to address complex geotechnical challenges.

### Physics-Trained Neural Network as Inverse Problem Solver for Potential Fields: An Example of Downward Continuation between Arbitrary Surfaces 
[[arxiv](https://arxiv.org/abs/2502.05190)] [[cool](https://papers.cool/arxiv/2502.05190)] [[pdf](https://arxiv.org/pdf/2502.05190)]
> **Authors**: Jing Sun,Lu Li,Liang Zhang
> **First submission**: 2025-01-26
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 地球物理学,机器学习
- **Abstract**: Downward continuation is a critical task in potential field processing, including gravity and magnetic fields, which aims to transfer data from one observation surface to another that is closer to the source of the field. Its effectiveness directly impacts the success of detecting and highlighting subsurface anomalous sources. We treat downward continuation as an inverse problem that relies on solving a forward problem defined by the formula for upward continuation, and we propose a new physics-trained deep neural network (DNN)-based solution for this task. We hard-code the upward continuation process into the DNN's learning framework, where the DNN itself learns to act as the inverse problem solver and can perform downward continuation without ever being shown any ground truth data. We test the proposed method on both synthetic magnetic data and real-world magnetic data from West Antarctica. The preliminary results demonstrate its effectiveness through comparison with selected benchmarks, opening future avenues for the combined use of DNNs and established geophysical theories to address broader potential field inverse problems, such as density and geometry modelling.

### Physics-Driven Self-Supervised Deep Learning for Free-Surface Multiple Elimination 
[[arxiv](https://arxiv.org/abs/2502.05189)] [[cool](https://papers.cool/arxiv/2502.05189)] [[pdf](https://arxiv.org/pdf/2502.05189)]
> **Authors**: Jing Sun,Tiexing Wang,Eric Verschuur,Ivan Vasconcelos
> **First submission**: 2025-01-26
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 地球物理学,机器学习
- **Abstract**: In recent years, deep learning (DL) has emerged as a promising alternative approach for various seismic processing tasks, including primary estimation (or multiple elimination), a crucial step for accurate subsurface imaging. In geophysics, DL methods are commonly based on supervised learning from large amounts of high-quality labelled data. Instead of relying on traditional supervised learning, in the context of free-surface multiple elimination, we propose a method in which the DL model learns to effectively parameterize the free-surface multiple-free wavefield from the full wavefield by incorporating the underlying physics into the loss computation. This, in turn, yields high-quality estimates without ever being shown any ground truth data. Currently, the network reparameterization is performed independently for each dataset. We demonstrate its effectiveness through tests on both synthetic and field data. We employ industry-standard Surface-Related Multiple Elimination (SRME) using, respectively, global least-squares adaptive subtraction and local least-squares adaptive subtraction as benchmarks. The comparison shows that the proposed method outperforms the benchmarks in estimation accuracy, achieving the most complete primary estimation and the least multiple energy leakage, but at the cost of a higher computational burden.

## 统计金融(q-fin.ST:Statistical Finance)

### FactorGCL: A Hypergraph-Based Factor Model with Temporal Residual Contrastive Learning for Stock Returns Prediction 
[[arxiv](https://arxiv.org/abs/2502.05218)] [[cool](https://papers.cool/arxiv/2502.05218)] [[pdf](https://arxiv.org/pdf/2502.05218)]
> **Authors**: Yitong Duan,Weiran Wang,Jian Li
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 统计金融,人工智能,机器学习
- **Abstract**: As a fundamental method in economics and finance, the factor model has been extensively utilized in quantitative investment. In recent years, there has been a paradigm shift from traditional linear models with expert-designed factors to more flexible nonlinear machine learning-based models with data-driven factors, aiming to enhance the effectiveness of these factor models. However, due to the low signal-to-noise ratio in market data, mining effective factors in data-driven models remains challenging. In this work, we propose a hypergraph-based factor model with temporal residual contrastive learning (FactorGCL) that employs a hypergraph structure to better capture high-order nonlinear relationships among stock returns and factors. To mine hidden factors that supplement human-designed prior factors for predicting stock returns, we design a cascading residual hypergraph architecture, in which the hidden factors are extracted from the residual information after removing the influence of prior factors. Additionally, we propose a temporal residual contrastive learning method to guide the extraction of effective and comprehensive hidden factors by contrasting stock-specific residual information over different time periods. Our extensive experiments on real stock market data demonstrate that FactorGCL not only outperforms existing state-of-the-art methods but also mines effective hidden factors for predicting stock returns.

### Regression and Forecasting of U.S. Stock Returns Based on LSTM 
[[arxiv](https://arxiv.org/abs/2502.05210)] [[cool](https://papers.cool/arxiv/2502.05210)] [[pdf](https://arxiv.org/pdf/2502.05210)]
> **Authors**: Shicheng Zhou,Zizhou Zhang,Rong Zhang,Yuchen Yin,Chia Hong Chang,Qinyan Shen
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-10
> **comment**: 5pages
- **标题**: None
- **领域**: 统计金融,机器学习
- **Abstract**: This paper analyses the investment returns of three stock sectors, Manuf, Hitec, and Other, in the U.S. stock market, based on the Fama-French three-factor model, the Carhart four-factor model, and the Fama-French five-factor model, in order to test the validity of the Fama-French three-factor model, the Carhart four-factor model, and the Fama-French five-factor model for the three sectors of the market. French five-factor model for the three sectors of the market. Also, the LSTM model is used to explore the additional factors affecting stock returns. The empirical results show that the Fama-French five-factor model has better validity for the three segments of the market under study, and the LSTM model has the ability to capture the factors affecting the returns of certain industries, and can better regress and predict the stock returns of the relevant industries. Keywords- Fama-French model; Carhart model; Factor model; LSTM model.

### Multimodal Stock Price Prediction 
[[arxiv](https://arxiv.org/abs/2502.05186)] [[cool](https://papers.cool/arxiv/2502.05186)] [[pdf](https://arxiv.org/pdf/2502.05186)]
> **Authors**: Furkan Karadaş,Bahaeddin Eravcı,Ahmet Murat Özbayoğlu
> **First submission**: 2025-01-23
> **First announcement**: 2025-02-10
> **comment**: 9 pages, 6 table
- **标题**: None
- **领域**: 统计金融,人工智能,机器学习
- **Abstract**: In an era where financial markets are heavily influenced by many static and dynamic factors, it has become increasingly critical to carefully integrate diverse data sources with machine learning for accurate stock price prediction. This paper explores a multimodal machine learning approach for stock price prediction by combining data from diverse sources, including traditional financial metrics, tweets, and news articles. We capture real-time market dynamics and investor mood through sentiment analysis on these textual data using both ChatGPT-4o and FinBERT models. We look at how these integrated data streams augment predictions made with a standard Long Short-Term Memory (LSTM model) to illustrate the extent of performance gains. Our study's results indicate that incorporating the mentioned data sources considerably increases the forecast effectiveness of the reference model by up to 5%. We also provide insights into the individual and combined predictive capacities of these modalities, highlighting the substantial impact of incorporating sentiment analysis from tweets and news articles. This research offers a systematic and effective framework for applying multimodal data analytics techniques in financial time series forecasting that provides a new view for investors to leverage data for decision-making.

## 量子物理学(quant-ph:Quantum Physics)

### Detection of Physiological Data Tampering Attacks with Quantum Machine Learning 
[[arxiv](https://arxiv.org/abs/2502.05966)] [[cool](https://papers.cool/arxiv/2502.05966)] [[pdf](https://arxiv.org/pdf/2502.05966)]
> **Authors**: Md. Saif Hassan Onim,Himanshu Thapliyal
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 量子物理学,机器学习
- **Abstract**: The widespread use of cloud-based medical devices and wearable sensors has made physiological data susceptible to tampering. These attacks can compromise the reliability of healthcare systems which can be critical and life-threatening. Detection of such data tampering is of immediate need. Machine learning has been used to detect anomalies in datasets but the performance of Quantum Machine Learning (QML) is still yet to be evaluated for physiological sensor data. Thus, our study compares the effectiveness of QML for detecting physiological data tampering, focusing on two types of white-box attacks: data poisoning and adversarial perturbation. The results show that QML models are better at identifying label-flipping attacks, achieving accuracy rates of 75%-95% depending on the data and attack severity. This superior performance is due to the ability of quantum algorithms to handle complex and high-dimensional data. However, both QML and classical models struggle to detect more sophisticated adversarial perturbation attacks, which subtly alter data without changing its statistical properties. Although QML performed poorly against this attack with around 45%-65% accuracy, it still outperformed classical algorithms in some cases.

### Quantum automated learning with provable and explainable trainability 
[[arxiv](https://arxiv.org/abs/2502.05264)] [[cool](https://papers.cool/arxiv/2502.05264)] [[pdf](https://arxiv.org/pdf/2502.05264)]
> **Authors**: Qi Ye,Shuangyue Geng,Zizhao Han,Weikang Li,L. -M. Duan,Dong-Ling Deng
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 21 pages, 7 figures
- **标题**: None
- **领域**: 量子物理学,人工智能,机器学习
- **Abstract**: Machine learning is widely believed to be one of the most promising practical applications of quantum computing. Existing quantum machine learning schemes typically employ a quantum-classical hybrid approach that relies crucially on gradients of model parameters. Such an approach lacks provable convergence to global minima and will become infeasible as quantum learning models scale up. Here, we introduce quantum automated learning, where no variational parameter is involved and the training process is converted to quantum state preparation. In particular, we encode training data into unitary operations and iteratively evolve a random initial state under these unitaries and their inverses, with a target-oriented perturbation towards higher prediction accuracy sandwiched in between. Under reasonable assumptions, we rigorously prove that the evolution converges exponentially to the desired state corresponding to the global minimum of the loss function. We show that such a training process can be understood from the perspective of preparing quantum states by imaginary time evolution, where the data-encoded unitaries together with target-oriented perturbations would train the quantum learning model in an automated fashion. We further prove that the quantum automated learning paradigm features good generalization ability with the generalization error upper bounded by the ratio between a logarithmic function of the Hilbert space dimension and the number of training samples. In addition, we carry out extensive numerical simulations on real-life images and quantum data to demonstrate the effectiveness of our approach and validate the assumptions. Our results establish an unconventional quantum learning strategy that is gradient-free with provable and explainable trainability, which would be crucial for large-scale practical applications of quantum computing in machine learning scenarios.

### Non-linear Quantum Monte Carlo 
[[arxiv](https://arxiv.org/abs/2502.05094)] [[cool](https://papers.cool/arxiv/2502.05094)] [[pdf](https://arxiv.org/pdf/2502.05094)]
> **Authors**: Jose Blanchet,Yassine Hamoudi,Mario Szegedy,Guanyang Wang
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 30 pages
- **标题**: None
- **领域**: 量子物理学,机器学习,数值分析,计算,机器学习
- **Abstract**: The mean of a random variable can be understood as a $\textit{linear}$ functional on the space of probability distributions. Quantum computing is known to provide a quadratic speedup over classical Monte Carlo methods for mean estimation. In this paper, we investigate whether a similar quadratic speedup is achievable for estimating $\textit{non-linear}$ functionals of probability distributions. We propose a quantum-inside-quantum Monte Carlo algorithm that achieves such a speedup for a broad class of non-linear estimation problems, including nested conditional expectations and stochastic optimization. Our algorithm improves upon the direct application of the quantum multilevel Monte Carlo algorithm introduced by An et al.. The existing lower bound indicates that our algorithm is optimal up polylogarithmic factors. A key innovation of our approach is a new sequence of multilevel Monte Carlo approximations specifically designed for quantum computing, which is central to the algorithm's improved performance.

### Differential Privacy of Quantum and Quantum-Inspired-Classical Recommendation Algorithms 
[[arxiv](https://arxiv.org/abs/2502.04758)] [[cool](https://papers.cool/arxiv/2502.04758)] [[pdf](https://arxiv.org/pdf/2502.04758)]
> **Authors**: Chenjian Li,Mingsheng Ying
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 17 pages, 6 figures in total(including appendix)
- **标题**: None
- **领域**: 量子物理学,密码学和安全,新兴技术,机器学习
- **Abstract**: We analyze the DP (differential privacy) properties of the quantum recommendation algorithm and the quantum-inspired-classical recommendation algorithm. We discover that the quantum recommendation algorithm is a privacy curating mechanism on its own, requiring no external noise, which is different from traditional differential privacy mechanisms. In our analysis, a novel perturbation method tailored for SVD (singular value decomposition) and low-rank matrix approximation problems is introduced. Using the perturbation method and random matrix theory, we are able to derive that both the quantum and quantum-inspired-classical algorithms are $\big(\tilde{\mathcal{O}}\big(\frac 1n\big),\,\, \tilde{\mathcal{O}}\big(\frac{1}{\min\{m,n\}}\big)\big)$-DP under some reasonable restrictions, where $m$ and $n$ are numbers of users and products in the input preference database respectively. Nevertheless, a comparison shows that the quantum algorithm has better privacy preserving potential than the classical one.

## 计算(stat.CO:Computation)

### Predictive Coresets 
[[arxiv](https://arxiv.org/abs/2502.05725)] [[cool](https://papers.cool/arxiv/2502.05725)] [[pdf](https://arxiv.org/pdf/2502.05725)]
> **Authors**: Bernardo Flores
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 计算,机器学习
- **Abstract**: Modern data analysis often involves massive datasets with hundreds of thousands of observations, making traditional inference algorithms computationally prohibitive. Coresets are selection methods designed to choose a smaller subset of observations while maintaining similar learning performance. Conventional coreset approaches determine these weights by minimizing the Kullback-Leibler (KL) divergence between the likelihood functions of the full and weighted datasets; as a result, this makes them ill-posed for nonparametric models, where the likelihood is often intractable. We propose an alternative variational method which employs randomized posteriors and finds weights to match the unknown posterior predictive distributions conditioned on the full and reduced datasets. Our approach provides a general algorithm based on predictive recursions suitable for nonparametric priors. We evaluate the performance of the proposed coreset construction on diverse problems, including random partitions and density estimation.

## 方法论(stat.ME:Methodology)

### $t$-Testing the Waters: Empirically Validating Assumptions for Reliable A/B-Testing 
[[arxiv](https://arxiv.org/abs/2502.04793)] [[cool](https://papers.cool/arxiv/2502.04793)] [[pdf](https://arxiv.org/pdf/2502.04793)]
> **Authors**: Olivier Jeunen
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 方法论,机器学习
- **Abstract**: A/B-tests are a cornerstone of experimental design on the web, with wide-ranging applications and use-cases. The statistical $t$-test comparing differences in means is the most commonly used method for assessing treatment effects, often justified through the Central Limit Theorem (CLT). The CLT ascertains that, as the sample size grows, the sampling distribution of the Average Treatment Effect converges to normality, making the $t$-test valid for sufficiently large sample sizes. When outcome measures are skewed or non-normal, quantifying what "sufficiently large" entails is not straightforward. To ensure that confidence intervals maintain proper coverage and that $p$-values accurately reflect the false positive rate, it is critical to validate this normality assumption. We propose a practical method to test this, by analysing repeatedly resampled A/A-tests. When the normality assumption holds, the resulting $p$-value distribution should be uniform, and this property can be tested using the Kolmogorov-Smirnov test. This provides an efficient and effective way to empirically assess whether the $t$-test's assumptions are met, and the A/B-test is valid. We demonstrate our methodology and highlight how it helps to identify scenarios prone to inflated Type-I errors. Our approach provides a practical framework to ensure and improve the reliability and robustness of A/B-testing practices.

## 机器学习(stat.ML:Machine Learning)

### Linear Bandits with Partially Observable Features 
[[arxiv](https://arxiv.org/abs/2502.06142)] [[cool](https://papers.cool/arxiv/2502.06142)] [[pdf](https://arxiv.org/pdf/2502.06142)]
> **Authors**: Wonyoung Kim,Sungwoo Park,Garud Iyengar,Assaf Zeevi,Min-hwan Oh
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: We introduce a novel linear bandit problem with partially observable features, resulting in partial reward information and spurious estimates. Without proper address for latent part, regret possibly grows linearly in decision horizon $T$, as their influence on rewards are unknown. To tackle this, we propose a novel analysis to handle the latent features and an algorithm that achieves sublinear regret. The core of our algorithm involves (i) augmenting basis vectors orthogonal to the observed feature space, and (ii) introducing an efficient doubly robust estimator. Our approach achieves a regret bound of $\tilde{O}(\sqrt{(d + d_h)T})$, where $d$ is the dimension of observed features, and $d_h$ is the unknown dimension of the subspace of the unobserved features. Notably, our algorithm requires no prior knowledge of the unobserved feature space, which may expand as more features become hidden. Numerical experiments confirm that our algorithm outperforms both non-contextual multi-armed bandits and linear bandit algorithms depending solely on observed features.

### Post-detection inference for sequential changepoint localization 
[[arxiv](https://arxiv.org/abs/2502.06096)] [[cool](https://papers.cool/arxiv/2502.06096)] [[pdf](https://arxiv.org/pdf/2502.06096)]
> **Authors**: Aytijhya Saha,Aaditya Ramdas
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习,方法论
- **Abstract**: This paper addresses a fundamental but largely unexplored challenge in sequential changepoint analysis: conducting inference following a detected change. We study the problem of localizing the changepoint using only the data observed up to a data-dependent stopping time at which a sequential detection algorithm $\mathcal A$ declares a change. We first construct confidence sets for the unknown changepoint when pre- and post-change distributions are assumed to be known. We then extend our framework to composite pre- and post-change scenarios. We impose no conditions on the observation space or on $\mathcal A$ -- we only need to be able to run $\mathcal A$ on simulated data sequences. In summary, this work offers both theoretically sound and practically effective tools for sequential changepoint localization.

### Lipschitz-Driven Inference: Bias-corrected Confidence Intervals for Spatial Linear Models 
[[arxiv](https://arxiv.org/abs/2502.06067)] [[cool](https://papers.cool/arxiv/2502.06067)] [[pdf](https://arxiv.org/pdf/2502.06067)]
> **Authors**: David R. Burt,Renato Berlinghieri,Stephen Bates,Tamara Broderick
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: 34 pages; 15 figures
- **标题**: None
- **领域**: 机器学习,机器学习,方法论
- **Abstract**: Linear models remain ubiquitous in modern spatial applications - including climate science, public health, and economics - due to their interpretability, speed, and reproducibility. While practitioners generally report a form of uncertainty, popular spatial uncertainty quantification methods do not jointly handle model misspecification and distribution shift - despite both being essentially always present in spatial problems. In the present paper, we show that existing methods for constructing confidence (or credible) intervals in spatial linear models fail to provide correct coverage due to unaccounted-for bias. In contrast to classical methods that rely on an i.i.d. assumption that is inappropriate in spatial problems, in the present work we instead make a spatial smoothness (Lipschitz) assumption. We are then able to propose a new confidence-interval construction that accounts for bias in the estimation procedure. We demonstrate that our new method achieves nominal coverage via both theory and experiments. Code to reproduce experiments is available at https://github.com/DavidRBurt/Lipschitz-Driven-Inference.

### Scalable Differentially Private Bayesian Optimization 
[[arxiv](https://arxiv.org/abs/2502.06044)] [[cool](https://papers.cool/arxiv/2502.06044)] [[pdf](https://arxiv.org/pdf/2502.06044)]
> **Authors**: Getoar Sopa,Juraj Marusic,Marco Avella-Medina,John P. Cunningham
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: 18 pages, 5 figures
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: In recent years, there has been much work on scaling Bayesian Optimization to high-dimensional problems, for example hyperparameter tuning in large neural network models. These scalable methods have been successful, finding high objective values much more quickly than traditional global Bayesian Optimization or random search-based methods. At the same time, these large neural network models often use sensitive data, but preservation of Differential Privacy has not scaled alongside these modern Bayesian Optimization procedures. Here we develop a method to privately estimate potentially high-dimensional parameter spaces using Gradient Informative Bayesian Optimization. Our theoretical results prove that under suitable conditions, our method converges exponentially fast to a ball around the optimal parameter configuration. Moreover, regardless of whether the assumptions are satisfied, we show that our algorithm maintains privacy and empirically demonstrates superior performance to existing methods in the high-dimensional hyperparameter setting.

### Nested subspace learning with flags 
[[arxiv](https://arxiv.org/abs/2502.06022)] [[cool](https://papers.cool/arxiv/2502.06022)] [[pdf](https://arxiv.org/pdf/2502.06022)]
> **Authors**: Tom Szwagier,Xavier Pennec
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Many machine learning methods look for low-dimensional representations of the data. The underlying subspace can be estimated by first choosing a dimension $q$ and then optimizing a certain objective function over the space of $q$-dimensional subspaces (the Grassmannian). Trying different $q$ yields in general non-nested subspaces, which raises an important issue of consistency between the data representations. In this paper, we propose a simple trick to enforce nestedness in subspace learning methods. It consists in lifting Grassmannian optimization problems to flag manifolds (the space of nested subspaces of increasing dimension) via nested projectors. We apply the flag trick to several classical machine learning methods and show that it successfully addresses the nestedness issue.

### Uncertainty Quantification and Causal Considerations for Off-Policy Decision Making 
[[arxiv](https://arxiv.org/abs/2502.06011)] [[cool](https://papers.cool/arxiv/2502.06011)] [[pdf](https://arxiv.org/pdf/2502.06011)]
> **Authors**: Muhammad Faaiz Taufiq
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: PhD thesis
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Off-policy evaluation (OPE) is a critical challenge in robust decision-making that seeks to assess the performance of a new policy using data collected under a different policy. However, the existing OPE methodologies suffer from several limitations arising from statistical uncertainty as well as causal considerations. In this thesis, we address these limitations by presenting three different works. Firstly, we consider the problem of high variance in the importance-sampling-based OPE estimators. We introduce the Marginal Ratio (MR) estimator, a novel OPE method that reduces variance by focusing on the marginal distribution of outcomes rather than direct policy shifts, improving robustness in contextual bandits. Next, we propose Conformal Off-Policy Prediction (COPP), a principled approach for uncertainty quantification in OPE that provides finite-sample predictive intervals, ensuring robust decision-making in risk-sensitive applications. Finally, we address causal unidentifiability in off-policy decision-making by developing novel bounds for sequential decision settings, which remain valid under arbitrary unmeasured confounding. We apply these bounds to assess the reliability of digital twin models, introducing a falsification framework to identify scenarios where model predictions diverge from real-world behaviour. Our contributions provide new insights into robust decision-making under uncertainty and establish principled methods for evaluating policies in both static and dynamic settings.

### Transformers versus the EM Algorithm in Multi-class Clustering 
[[arxiv](https://arxiv.org/abs/2502.06007)] [[cool](https://papers.cool/arxiv/2502.06007)] [[pdf](https://arxiv.org/pdf/2502.06007)]
> **Authors**: Yihan He,Hong-Yu Chen,Yuan Cao,Jianqing Fan,Han Liu
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: LLMs demonstrate significant inference capacities in complicated machine learning tasks, using the Transformer model as its backbone. Motivated by the limited understanding of such models on the unsupervised learning problems, we study the learning guarantees of Transformers in performing multi-class clustering of the Gaussian Mixture Models. We develop a theory drawing strong connections between the Softmax Attention layers and the workflow of the EM algorithm on clustering the mixture of Gaussians. Our theory provides approximation bounds for the Expectation and Maximization steps by proving the universal approximation abilities of multivariate mappings by Softmax functions. In addition to the approximation guarantees, we also show that with a sufficient number of pre-training samples and an initialization, Transformers can achieve the minimax optimal rate for the problem considered. Our extensive simulations empirically verified our theory by revealing the strong learning capacities of Transformers even beyond the assumptions in the theory, shedding light on the powerful inference capacities of LLMs.

### Diffusion Models for Inverse Problems in the Exponential Family 
[[arxiv](https://arxiv.org/abs/2502.05994)] [[cool](https://papers.cool/arxiv/2502.05994)] [[pdf](https://arxiv.org/pdf/2502.05994)]
> **Authors**: Alessandro Micheli,Mélodie Monod,Samir Bhatt
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Diffusion models have emerged as powerful tools for solving inverse problems, yet prior work has primarily focused on observations with Gaussian measurement noise, restricting their use in real-world scenarios. This limitation persists due to the intractability of the likelihood score, which until now has only been approximated in the simpler case of Gaussian likelihoods. In this work, we extend diffusion models to handle inverse problems where the observations follow a distribution from the exponential family, such as a Poisson or a Binomial distribution. By leveraging the conjugacy properties of exponential family distributions, we introduce the evidence trick, a method that provides a tractable approximation to the likelihood score. In our experiments, we demonstrate that our methodology effectively performs Bayesian inference on spatially inhomogeneous Poisson processes with intensities as intricate as ImageNet images. Furthermore, we demonstrate the real-world impact of our methodology by showing that it performs competitively with the current state-of-the-art in predicting malaria prevalence estimates in Sub-Saharan Africa.

### Asymptotic FDR Control with Model-X Knockoffs: Is Moments Matching Sufficient? 
[[arxiv](https://arxiv.org/abs/2502.05969)] [[cool](https://papers.cool/arxiv/2502.05969)] [[pdf](https://arxiv.org/pdf/2502.05969)]
> **Authors**: Yingying Fan,Lan Gao,Jinchi Lv,Xiaocong Xu
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: 90 pages
- **标题**: None
- **领域**: 机器学习,机器学习,统计理论
- **Abstract**: We propose a unified theoretical framework for studying the robustness of the model-X knockoffs framework by investigating the asymptotic false discovery rate (FDR) control of the practically implemented approximate knockoffs procedure. This procedure deviates from the model-X knockoffs framework by substituting the true covariate distribution with a user-specified distribution that can be learned using in-sample observations. By replacing the distributional exchangeability condition of the model-X knockoff variables with three conditions on the approximate knockoff statistics, we establish that the approximate knockoffs procedure achieves the asymptotic FDR control. Using our unified framework, we further prove that an arguably most popularly used knockoff variable generation method--the Gaussian knockoffs generator based on the first two moments matching--achieves the asymptotic FDR control when the two-moment-based knockoff statistics are employed in the knockoffs inference procedure. For the first time in the literature, our theoretical results justify formally the effectiveness and robustness of the Gaussian knockoffs generator. Simulation and real data examples are conducted to validate the theoretical findings.

### Propagation of Chaos for Mean-Field Langevin Dynamics and its Application to Model Ensemble 
[[arxiv](https://arxiv.org/abs/2502.05784)] [[cool](https://papers.cool/arxiv/2502.05784)] [[pdf](https://arxiv.org/pdf/2502.05784)]
> **Authors**: Atsushi Nitanda,Anzelle Lee,Damian Tan Xing Kai,Mizuki Sakaguchi,Taiji Suzuki
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-10
> **comment**: 23 pages,
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Mean-field Langevin dynamics (MFLD) is an optimization method derived by taking the mean-field limit of noisy gradient descent for two-layer neural networks in the mean-field regime. Recently, the propagation of chaos (PoC) for MFLD has gained attention as it provides a quantitative characterization of the optimization complexity in terms of the number of particles and iterations. A remarkable progress by Chen et al. (2022) showed that the approximation error due to finite particles remains uniform in time and diminishes as the number of particles increases. In this paper, by refining the defective log-Sobolev inequality -- a key result from that earlier work -- under the neural network training setting, we establish an improved PoC result for MFLD, which removes the exponential dependence on the regularization coefficient from the particle approximation term of the optimization complexity. As an application, we propose a PoC-based model ensemble strategy with theoretical guarantees.

### Dynamic Pricing in the Linear Valuation Model using Shape Constraints 
[[arxiv](https://arxiv.org/abs/2502.05776)] [[cool](https://papers.cool/arxiv/2502.05776)] [[pdf](https://arxiv.org/pdf/2502.05776)]
> **Authors**: Daniele Bracale,Moulinath Banerjee,Yuekai Sun,Kevin Stoll,Salam Turki
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: We propose a shape-constrained approach to dynamic pricing for censored data in the linear valuation model that eliminates the need for tuning parameters commonly required in existing methods. Previous works have addressed the challenge of unknown market noise distribution F using strategies ranging from kernel methods to reinforcement learning algorithms, such as bandit techniques and upper confidence bounds (UCB), under the Lipschitz (and stronger) assumption(s) on $F_0$. In contrast, our method relies on isotonic regression under the weaker assumption that $F_0$ is $α$-Holder continuous for some $α\in (0,1]$. We obtain an upper bound on the asymptotic expected regret that matches existing bounds in the literature for $α= 1$ (the Lipschitz case). Simulations and experiments with real-world data obtained by Welltower Inc (a major healthcare Real Estate Investment Trust) consistently demonstrate that our method attains better empirical regret in comparison to several existing methods in the literature while offering the advantage of being completely tuning-parameter free.

### TD(0) Learning converges for Polynomial mixing and non-linear functions 
[[arxiv](https://arxiv.org/abs/2502.05706)] [[cool](https://papers.cool/arxiv/2502.05706)] [[pdf](https://arxiv.org/pdf/2502.05706)]
> **Authors**: Anupama Sridhar,Alexander Johansen
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: 12 pages main text
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Theoretical work on Temporal Difference (TD) learning has provided finite-sample and high-probability guarantees for data generated from Markov chains. However, these bounds typically require linear function approximation, instance-dependent step sizes, algorithmic modifications, and restrictive mixing rates. We present theoretical findings for TD learning under more applicable assumptions, including instance-independent step sizes, full data utilization, and polynomial ergodicity, applicable to both linear and non-linear functions. \textbf{To our knowledge, this is the first proof of TD(0) convergence on Markov data under universal and instance-independent step sizes.} While each contribution is significant on its own, their combination allows these bounds to be effectively utilized in practical application settings. Our results include bounds for linear models and non-linear under generalized gradients and Hölder continuity.

### Generalized Venn and Venn-Abers Calibration with Applications in Conformal Prediction 
[[arxiv](https://arxiv.org/abs/2502.05676)] [[cool](https://papers.cool/arxiv/2502.05676)] [[pdf](https://arxiv.org/pdf/2502.05676)]
> **Authors**: Lars van der Laan,Ahmed Alaa
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习,方法论
- **Abstract**: Ensuring model calibration is critical for reliable predictions, yet popular distribution-free methods, such as histogram binning and isotonic regression, provide only asymptotic guarantees. We introduce a unified framework for Venn and Venn-Abers calibration, generalizing Vovk's binary classification approach to arbitrary prediction tasks and loss functions. Venn calibration leverages binning calibrators to construct prediction sets that contain at least one marginally perfectly calibrated point prediction in finite samples, capturing epistemic uncertainty in the calibration process. The width of these sets shrinks asymptotically to zero, converging to a conditionally calibrated point prediction. Furthermore, we propose Venn multicalibration, a novel methodology for finite-sample calibration across subpopulations. For quantile loss, group-conditional and multicalibrated conformal prediction arise as special cases of Venn multicalibration, and Venn calibration produces novel conformal prediction intervals that achieve quantile-conditional coverage. As a separate contribution, we extend distribution-free conditional calibration guarantees of histogram binning and isotonic calibration to general losses.

### On the Convergence and Stability of Upside-Down Reinforcement Learning, Goal-Conditioned Supervised Learning, and Online Decision Transformers 
[[arxiv](https://arxiv.org/abs/2502.05672)] [[cool](https://papers.cool/arxiv/2502.05672)] [[pdf](https://arxiv.org/pdf/2502.05672)]
> **Authors**: Miroslav Štrupl,Oleg Szehr,Francesco Faccio,Dylan R. Ashley,Rupesh Kumar Srivastava,Jürgen Schmidhuber
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: 85 pages in main text + 4 pages of references + 26 pages of appendices, 12 figures in main text + 2 figures in appendices; source code available at https://github.com/struplm/eUDRL-GCSL-ODT-Convergence-public
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习,神经和进化计算,系统与控制
- **Abstract**: This article provides a rigorous analysis of convergence and stability of Episodic Upside-Down Reinforcement Learning, Goal-Conditioned Supervised Learning and Online Decision Transformers. These algorithms performed competitively across various benchmarks, from games to robotic tasks, but their theoretical understanding is limited to specific environmental conditions. This work initiates a theoretical foundation for algorithms that build on the broad paradigm of approaching reinforcement learning through supervised learning or sequence modeling. At the core of this investigation lies the analysis of conditions on the underlying environment, under which the algorithms can identify optimal solutions. We also assess whether emerging solutions remain stable in situations where the environment is subject to tiny levels of noise. Specifically, we study the continuity and asymptotic convergence of command-conditioned policies, values and the goal-reaching objective depending on the transition kernel of the underlying Markov Decision Process. We demonstrate that near-optimal behavior is achieved if the transition kernel is located in a sufficiently small neighborhood of a deterministic kernel. The mentioned quantities are continuous (with respect to a specific topology) at deterministic kernels, both asymptotically and after a finite number of learning cycles. The developed methods allow us to present the first explicit estimates on the convergence and stability of policies and values in terms of the underlying transition kernels. On the theoretical side we introduce a number of new concepts to reinforcement learning, like working in segment spaces, studying continuity in quotient topologies and the application of the fixed-point theory of dynamical systems. The theoretical study is accompanied by a detailed investigation of example environments and numerical experiments.

### dynoGP: Deep Gaussian Processes for dynamic system identification 
[[arxiv](https://arxiv.org/abs/2502.05620)] [[cool](https://papers.cool/arxiv/2502.05620)] [[pdf](https://arxiv.org/pdf/2502.05620)]
> **Authors**: Alessio Benavoli,Dario Piga,Marco Forgione,Marco Zaffalon
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: In this work, we present a novel approach to system identification for dynamical systems, based on a specific class of Deep Gaussian Processes (Deep GPs). These models are constructed by interconnecting linear dynamic GPs (equivalent to stochastic linear time-invariant dynamical systems) and static GPs (to model static nonlinearities). Our approach combines the strengths of data-driven methods, such as those based on neural network architectures, with the ability to output a probability distribution. This offers a more comprehensive framework for system identification that includes uncertainty quantification. Using both simulated and real-world data, we demonstrate the effectiveness of the proposed approach.

### Online Covariance Estimation in Nonsmooth Stochastic Approximation 
[[arxiv](https://arxiv.org/abs/2502.05305)] [[cool](https://papers.cool/arxiv/2502.05305)] [[pdf](https://arxiv.org/pdf/2502.05305)]
> **Authors**: Liwei Jiang,Abhishek Roy,Krishna Balasubramanian,Damek Davis,Dmitriy Drusvyatskiy,Sen Na
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 46 pages, 1 figure
- **标题**: None
- **领域**: 机器学习,机器学习,优化与控制
- **Abstract**: We consider applying stochastic approximation (SA) methods to solve nonsmooth variational inclusion problems. Existing studies have shown that the averaged iterates of SA methods exhibit asymptotic normality, with an optimal limiting covariance matrix in the local minimax sense of Hájek and Le Cam. However, no methods have been proposed to estimate this covariance matrix in a nonsmooth and potentially non-monotone (nonconvex) setting. In this paper, we study an online batch-means covariance matrix estimator introduced in Zhu et al.(2023). The estimator groups the SA iterates appropriately and computes the sample covariance among batches as an estimate of the limiting covariance. Its construction does not require prior knowledge of the total sample size, and updates can be performed recursively as new data arrives. We establish that, as long as the batch size sequence is properly specified (depending on the stepsize sequence), the estimator achieves a convergence rate of order $O(\sqrt{d}n^{-1/8+\varepsilon})$ for any $\varepsilon>0$, where $d$ and $n$ denote the problem dimensionality and the number of iterations (or samples) used. Although the problem is nonsmooth and potentially non-monotone (nonconvex), our convergence rate matches the best-known rate for covariance estimation methods using only first-order information in smooth and strongly-convex settings. The consistency of this covariance estimator enables asymptotically valid statistical inference, including constructing confidence intervals and performing hypothesis testing.

### Distinguishing Cause from Effect with Causal Velocity Models 
[[arxiv](https://arxiv.org/abs/2502.05122)] [[cool](https://papers.cool/arxiv/2502.05122)] [[pdf](https://arxiv.org/pdf/2502.05122)]
> **Authors**: Johnny Xi,Hugh Dance,Peter Orbanz,Benjamin Bloem-Reddy
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习,方法论
- **Abstract**: Bivariate structural causal models (SCM) are often used to infer causal direction by examining their goodness-of-fit under restricted model classes. In this paper, we describe a parametrization of bivariate SCMs in terms of a causal velocity by viewing the cause variable as time in a dynamical system. The velocity implicitly defines counterfactual curves via the solution of initial value problems where the observation specifies the initial condition. Using tools from measure transport, we obtain a unique correspondence between SCMs and the score function of the generated distribution via its causal velocity. Based on this, we derive an objective function that directly regresses the velocity against the score function, the latter of which can be estimated non-parametrically from observational data. We use this to develop a method for bivariate causal discovery that extends beyond known model classes such as additive or location scale noise, and that requires no assumptions on the noise distributions. When the score is estimated well, the objective is also useful for detecting model non-identifiability and misspecification. We present positive results in simulation and benchmark experiments where many existing methods fail, and perform ablation studies to examine the method's sensitivity to accurate score estimation.

### Gradient-based Explanations for Deep Learning Survival Models 
[[arxiv](https://arxiv.org/abs/2502.04970)] [[cool](https://papers.cool/arxiv/2502.04970)] [[pdf](https://arxiv.org/pdf/2502.04970)]
> **Authors**: Sophie Hanna Langbein,Niklas Koenen,Marvin N. Wright
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Deep learning survival models often outperform classical methods in time-to-event predictions, particularly in personalized medicine, but their "black box" nature hinders broader adoption. We propose a framework for gradient-based explanation methods tailored to survival neural networks, extending their use beyond regression and classification. We analyze the implications of their theoretical assumptions for time-dependent explanations in the survival setting and propose effective visualizations incorporating the temporal dimension. Experiments on synthetic data show that gradient-based methods capture the magnitude and direction of local and global feature effects, including time dependencies. We introduce GradSHAP(t), a gradient-based counterpart to SurvSHAP(t), which outperforms SurvSHAP(t) and SurvLIME in a computational speed vs. accuracy trade-off. Finally, we apply these methods to medical data with multi-modal inputs, revealing relevant tabular features and visual patterns, as well as their temporal dynamics.

### Does Unsupervised Domain Adaptation Improve the Robustness of Amortized Bayesian Inference? A Systematic Evaluation 
[[arxiv](https://arxiv.org/abs/2502.04949)] [[cool](https://papers.cool/arxiv/2502.04949)] [[pdf](https://arxiv.org/pdf/2502.04949)]
> **Authors**: Lasse Elsemüller,Valentin Pratz,Mischa von Krause,Andreas Voss,Paul-Christian Bürkner,Stefan T. Radev
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习,方法论
- **Abstract**: Neural networks are fragile when confronted with data that significantly deviates from their training distribution. This is true in particular for simulation-based inference methods, such as neural amortized Bayesian inference (ABI), where models trained on simulated data are deployed on noisy real-world observations. Recent robust approaches employ unsupervised domain adaptation (UDA) to match the embedding spaces of simulated and observed data. However, the lack of comprehensive evaluations across different domain mismatches raises concerns about the reliability in high-stakes applications. We address this gap by systematically testing UDA approaches across a wide range of misspecification scenarios in both a controlled and a high-dimensional benchmark. We demonstrate that aligning summary spaces between domains effectively mitigates the impact of unmodeled phenomena or noise. However, the same alignment mechanism can lead to failures under prior misspecifications - a critical finding with practical consequences. Our results underscore the need for careful consideration of misspecification types when using UDA techniques to increase the robustness of ABI in practice.

### Scalable and consistent embedding of probability measures into Hilbert spaces via measure quantization 
[[arxiv](https://arxiv.org/abs/2502.04907)] [[cool](https://papers.cool/arxiv/2502.04907)] [[pdf](https://arxiv.org/pdf/2502.04907)]
> **Authors**: Erell Gachon,Elsa Cazelles,Jérémie Bigot
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: This paper is focused on statistical learning from data that come as probability measures. In this setting, popular approaches consist in embedding such data into a Hilbert space with either Linearized Optimal Transport or Kernel Mean Embedding. However, the cost of computing such embeddings prohibits their direct use in large-scale settings. We study two methods based on measure quantization for approximating input probability measures with discrete measures of small-support size. The first one is based on optimal quantization of each input measure, while the second one relies on mean-measure quantization. We study the consistency of such approximations, and its implication for scalable embeddings of probability measures into a Hilbert space at a low computational cost. We finally illustrate our findings with various numerical experiments.

### Any-stepsize Gradient Descent for Separable Data under Fenchel--Young Losses 
[[arxiv](https://arxiv.org/abs/2502.04889)] [[cool](https://papers.cool/arxiv/2502.04889)] [[pdf](https://arxiv.org/pdf/2502.04889)]
> **Authors**: Han Bao,Shinsaku Sakaue,Yuki Takezawa
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: The gradient descent (GD) has been one of the most common optimizer in machine learning. In particular, the loss landscape of a neural network is typically sharpened during the initial phase of training, making the training dynamics hover on the edge of stability. This is beyond our standard understanding of GD convergence in the stable regime where arbitrarily chosen stepsize is sufficiently smaller than the edge of stability. Recently, Wu et al. (COLT2024) have showed that GD converges with arbitrary stepsize under linearly separable logistic regression. Although their analysis hinges on the self-bounding property of the logistic loss, which seems to be a cornerstone to establish a modified descent lemma, our pilot study shows that other loss functions without the self-bounding property can make GD converge with arbitrary stepsize. To further understand what property of a loss function matters in GD, we aim to show arbitrary-stepsize GD convergence for a general loss function based on the framework of \emph{Fenchel--Young losses}. We essentially leverage the classical perceptron argument to derive the convergence rate for achieving $ε$-optimal loss, which is possible for a majority of Fenchel--Young losses. Among typical loss functions, the Tsallis entropy achieves the GD convergence rate $T=Ω(ε^{-1/2})$, and the R{é}nyi entropy achieves the far better rate $T=Ω(ε^{-1/3})$. We argue that these better rate is possible because of \emph{separation margin} of loss functions, instead of the self-bounding property.

### Statistical Collusion by Collectives on Learning Platforms 
[[arxiv](https://arxiv.org/abs/2502.04879)] [[cool](https://papers.cool/arxiv/2502.04879)] [[pdf](https://arxiv.org/pdf/2502.04879)]
> **Authors**: Etienne Gauthier,Francis Bach,Michael I. Jordan
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: Code available at: https://github.com/GauthierE/statistical-collusion
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: As platforms increasingly rely on learning algorithms, collectives may form and seek ways to influence these platforms to align with their own interests. This can be achieved by coordinated submission of altered data. To evaluate the potential impact of such behavior, it is essential to understand the computations that collectives must perform to impact platforms in this way. In particular, collectives need to make a priori assessments of the effect of the collective before taking action, as they may face potential risks when modifying their data. Moreover they need to develop implementable coordination algorithms based on quantities that can be inferred from observed data. We develop a framework that provides a theoretical and algorithmic treatment of these issues and present experimental results in a product evaluation domain.

### Advancing Wasserstein Convergence Analysis of Score-Based Models: Insights from Discretization and Second-Order Acceleration 
[[arxiv](https://arxiv.org/abs/2502.04849)] [[cool](https://papers.cool/arxiv/2502.04849)] [[pdf](https://arxiv.org/pdf/2502.04849)]
> **Authors**: Yifeng Yu,Lu Yu
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习,可能性
- **Abstract**: Score-based diffusion models have emerged as powerful tools in generative modeling, yet their theoretical foundations remain underexplored. In this work, we focus on the Wasserstein convergence analysis of score-based diffusion models. Specifically, we investigate the impact of various discretization schemes, including Euler discretization, exponential integrators, and midpoint randomization methods. Our analysis provides a quantitative comparison of these discrete approximations, emphasizing their influence on convergence behavior. Furthermore, we explore scenarios where Hessian information is available and propose an accelerated sampler based on the local linearization method. We demonstrate that this Hessian-based approach achieves faster convergence rates of order $\widetilde{\mathcal{O}}\left(\frac{1}{\varepsilon}\right)$ significantly improving upon the standard rate $\widetilde{\mathcal{O}}\left(\frac{1}{\varepsilon^2}\right)$ of vanilla diffusion models, where $\varepsilon$ denotes the target accuracy.

### Robust Conformal Outlier Detection under Contaminated Reference Data 
[[arxiv](https://arxiv.org/abs/2502.04807)] [[cool](https://papers.cool/arxiv/2502.04807)] [[pdf](https://arxiv.org/pdf/2502.04807)]
> **Authors**: Meshi Bashari,Matteo Sesia,Yaniv Romano
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习,方法论
- **Abstract**: Conformal prediction is a flexible framework for calibrating machine learning predictions, providing distribution-free statistical guarantees. In outlier detection, this calibration relies on a reference set of labeled inlier data to control the type-I error rate. However, obtaining a perfectly labeled inlier reference set is often unrealistic, and a more practical scenario involves access to a contaminated reference set containing a small fraction of outliers. This paper analyzes the impact of such contamination on the validity of conformal methods. We prove that under realistic, non-adversarial settings, calibration on contaminated data yields conservative type-I error control, shedding light on the inherent robustness of conformal methods. This conservativeness, however, typically results in a loss of power. To alleviate this limitation, we propose a novel, active data-cleaning framework that leverages a limited labeling budget and an outlier detection model to selectively annotate data points in the contaminated reference set that are suspected as outliers. By removing only the annotated outliers in this ``suspicious'' subset, we can effectively enhance power while mitigating the risk of inflating the type-I error rate, as supported by our theoretical analysis. Experiments on real datasets validate the conservative behavior of conformal methods under contamination and show that the proposed data-cleaning strategy improves power without sacrificing validity.

### Tighter sparse variational Gaussian processes 
[[arxiv](https://arxiv.org/abs/2502.04750)] [[cool](https://papers.cool/arxiv/2502.04750)] [[pdf](https://arxiv.org/pdf/2502.04750)]
> **Authors**: Thang D. Bui,Matthew Ashman,Richard E. Turner
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Sparse variational Gaussian process (GP) approximations based on inducing points have become the de facto standard for scaling GPs to large datasets, owing to their theoretical elegance, computational efficiency, and ease of implementation. This paper introduces a provably tighter variational approximation by relaxing the standard assumption that the conditional approximate posterior given the inducing points must match that in the prior. The key innovation is to modify the conditional posterior to have smaller variances than that of the prior at the training points. We derive the collapsed bound for the regression case, describe how to use the proposed approximation in large data settings, and discuss its application to handle orthogonally structured inducing points and GP latent variable models. Extensive experiments on regression benchmarks, classification, and latent variable models demonstrate that the proposed approximation consistently matches or outperforms standard sparse variational GPs while maintaining the same computational cost. An implementation will be made available in all popular GP packages.

### PhyloVAE: Unsupervised Learning of Phylogenetic Trees via Variational Autoencoders 
[[arxiv](https://arxiv.org/abs/2502.04730)] [[cool](https://papers.cool/arxiv/2502.04730)] [[pdf](https://arxiv.org/pdf/2502.04730)]
> **Authors**: Tianyu Xie,Harry Richman,Jiansi Gao,Frederick A. Matsen IV,Cheng Zhang
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: ICLR 2025. 22 pages, 14 figures
- **标题**: None
- **领域**: 机器学习,机器学习,种群与进化
- **Abstract**: Learning informative representations of phylogenetic tree structures is essential for analyzing evolutionary relationships. Classical distance-based methods have been widely used to project phylogenetic trees into Euclidean space, but they are often sensitive to the choice of distance metric and may lack sufficient resolution. In this paper, we introduce phylogenetic variational autoencoders (PhyloVAEs), an unsupervised learning framework designed for representation learning and generative modeling of tree topologies. Leveraging an efficient encoding mechanism inspired by autoregressive tree topology generation, we develop a deep latent-variable generative model that facilitates fast, parallelized topology generation. PhyloVAE combines this generative model with a collaborative inference model based on learnable topological features, allowing for high-resolution representations of phylogenetic tree samples. Extensive experiments demonstrate PhyloVAE's robust representation learning capabilities and fast generation of phylogenetic tree topologies.

### A Meta-learner for Heterogeneous Effects in Difference-in-Differences 
[[arxiv](https://arxiv.org/abs/2502.04699)] [[cool](https://papers.cool/arxiv/2502.04699)] [[pdf](https://arxiv.org/pdf/2502.04699)]
> **Authors**: Hui Lan,Haoge Chang,Eleanor Dillon,Vasilis Syrgkanis
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: We address the problem of estimating heterogeneous treatment effects in panel data, adopting the popular Difference-in-Differences (DiD) framework under the conditional parallel trends assumption. We propose a novel doubly robust meta-learner for the Conditional Average Treatment Effect on the Treated (CATT), reducing the estimation to a convex risk minimization problem involving a set of auxiliary models. Our framework allows for the flexible estimation of the CATT, when conditioning on any subset of variables of interest using generic machine learning. Leveraging Neyman orthogonality, our proposed approach is robust to estimation errors in the auxiliary models. As a generalization to our main result, we develop a meta-learning approach for the estimation of general conditional functionals under covariate shift. We also provide an extension to the instrumented DiD setting with non-compliance. Empirical results demonstrate the superiority of our approach over existing baselines.

### Optimistic Algorithms for Adaptive Estimation of the Average Treatment Effect 
[[arxiv](https://arxiv.org/abs/2502.04673)] [[cool](https://papers.cool/arxiv/2502.04673)] [[pdf](https://arxiv.org/pdf/2502.04673)]
> **Authors**: Ojash Neopane,Aaditya Ramdas,Aarti Singh
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-10
> **comment**: 15 pages, 2 Figures
- **标题**: None
- **领域**: 机器学习,机器学习,方法论
- **Abstract**: Estimation and inference for the Average Treatment Effect (ATE) is a cornerstone of causal inference and often serves as the foundation for developing procedures for more complicated settings. Although traditionally analyzed in a batch setting, recent advances in martingale theory have paved the way for adaptive methods that can enhance the power of downstream inference. Despite these advances, progress in understanding and developing adaptive algorithms remains in its early stages. Existing work either focus on asymptotic analyses that overlook exploration-exploitation tradeoffs relevant in finite-sample regimes or rely on simpler but suboptimal estimators. In this work, we address these limitations by studying adaptive sampling procedures that take advantage of the asymptotically optimal Augmented Inverse Probability Weighting (AIPW) estimator. Our analysis uncovers challenges obscured by asymptotic approaches and introduces a novel algorithmic design principle reminiscent of optimism in multiarmed bandits. This principled approach enables our algorithm to achieve significant theoretical and empirical gains compared to prior methods. Our findings mark a step forward in advancing adaptive causal inference methods in theory and practice.

## 其他论文

- [Real-Time LiDAR Point Cloud Compression and Transmission for Resource-constrained Robots](https://arxiv.org/abs/2502.06123)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [Token-Domain Multiple Access: Exploiting Semantic Orthogonality for Collision Mitigation](https://arxiv.org/abs/2502.06118)
  - **标题**: None
  - **Filtered Reason**: none of eess.SP,cs.IT in whitelist
- [Towards Bio-inspired Heuristically Accelerated Reinforcement Learning for Adaptive Underwater Multi-Agents Behaviour](https://arxiv.org/abs/2502.06113)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO,eess.SY in whitelist
- [An adaptive filter bank based neural network approach for time delay estimation and speech enhancement](https://arxiv.org/abs/2502.06098)
  - **标题**: None
  - **Filtered Reason**: none of cs.SD,eess.AS in whitelist
- [DeepMill: Neural Accessibility Learning for Subtractive Manufacturing](https://arxiv.org/abs/2502.06093)
  - **标题**: None
  - **Filtered Reason**: none of cs.GR in whitelist
- [Position: We Need An Adaptive Interpretation of Helpful, Honest, and Harmless Principles](https://arxiv.org/abs/2502.06059)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [Make the Fastest Faster: Importance Mask for Interactive Volume Visualization using Reconstruction Neural Networks](https://arxiv.org/abs/2502.06053)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC,cs.GR in whitelist
- [Hierarchical Polysemantic Feature Embedding for Autonomous Ransomware Detection](https://arxiv.org/abs/2502.06043)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [Media Bias Detector: Designing and Implementing a Tool for Real-Time Selection and Framing Bias Analysis in News Coverage](https://arxiv.org/abs/2502.06009)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC,cs.CY in whitelist
- [The AI Security Zugzwang](https://arxiv.org/abs/2502.06000)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [The Human Labour of Data Work: Capturing Cultural Diversity through World Wide Dishes](https://arxiv.org/abs/2502.05961)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [Characterization and Mitigation of ADC Noise by Reference Tuning in RRAM-Based Compute-In-Memory](https://arxiv.org/abs/2502.05948)
  - **标题**: None
  - **Filtered Reason**: none of cs.ET in whitelist
- [Energy-Efficient Autonomous Aerial Navigation with Dynamic Vision Sensors: A Physics-Guided Neuromorphic Approach](https://arxiv.org/abs/2502.05938)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [Can Generative Agent-Based Modeling Replicate the Friendship Paradox in Social Media Simulations?](https://arxiv.org/abs/2502.05919)
  - **标题**: None
  - **Filtered Reason**: none of cs.SI in whitelist
- [LpBound: Pessimistic Cardinality Estimation using $\ell_p$-Norms of Degree Sequences](https://arxiv.org/abs/2502.05912)
  - **标题**: None
  - **Filtered Reason**: none of cs.DB in whitelist
- [EvoAgent: Agent Autonomous Evolution with Continual World Model for Long-Horizon Tasks](https://arxiv.org/abs/2502.05907)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [Understanding Design Fixation in Generative AI](https://arxiv.org/abs/2502.05870)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Zak-Transform-Induced Optimal Sequences and Their Applications in OTFS](https://arxiv.org/abs/2502.05853)
  - **标题**: None
  - **Filtered Reason**: none of cs.IT in whitelist
- [On the use of Performer and Agent Attention for Spoken Language Identification](https://arxiv.org/abs/2502.05841)
  - **标题**: None
  - **Filtered Reason**: none of cs.SD,eess.AS in whitelist
- [Aerial Reliable Collaborative Communications for Terrestrial Mobile Users via Evolutionary Multi-Objective Deep Reinforcement Learning](https://arxiv.org/abs/2502.05824)
  - **标题**: None
  - **Filtered Reason**: none of cs.NE in whitelist
- [HCMRM: A High-Consistency Multimodal Relevance Model for Search Ads](https://arxiv.org/abs/2502.05822)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [First-Order Intuitionistic Linear Logic and Hypergraph Languages](https://arxiv.org/abs/2502.05816)
  - **标题**: None
  - **Filtered Reason**: none of math.LO,cs.FL in whitelist
- [Multi-Agent Reinforcement Learning in Wireless Distributed Networks for 6G](https://arxiv.org/abs/2502.05812)
  - **标题**: None
  - **Filtered Reason**: none of eess.SY,cs.IT in whitelist
- [StreamDCIM: A Tile-based Streaming Digital CIM Accelerator with Mixed-stationary Cross-forwarding Dataflow for Multimodal Transformer](https://arxiv.org/abs/2502.05798)
  - **标题**: None
  - **Filtered Reason**: none of cs.AR in whitelist
- [Seamless Integration: The Evolution, Design, and Future Impact of Wearable Technology](https://arxiv.org/abs/2502.05797)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Assessing confidence in frontier AI safety cases](https://arxiv.org/abs/2502.05791)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [Quality Assurance for LLM-RAG Systems: Empirical Insights from Tourism Application Testing](https://arxiv.org/abs/2502.05782)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Implicit Communication of Contextual Information in Human-Robot Collaboration](https://arxiv.org/abs/2502.05775)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC,cs.RO in whitelist
- [Understanding the Practices, Perceptions, and (Dis)Trust of Generative AI among Instructors: A Mixed-methods Study in the U.S. Higher Education](https://arxiv.org/abs/2502.05770)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC,cs.CY in whitelist
- [MADAR: Efficient Continual Learning for Malware Analysis with Diversity-Aware Replay](https://arxiv.org/abs/2502.05760)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [Large Language Model-based Nonnegative Matrix Factorization For Cardiorespiratory Sound Separation](https://arxiv.org/abs/2502.05757)
  - **标题**: None
  - **Filtered Reason**: none of cs.SD,eess.SP,eess.AS in whitelist
- [Visual Text Mining with Progressive Taxonomy Construction for Environmental Studies](https://arxiv.org/abs/2502.05731)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Attainability of Two-Point Testing Rates for Finite-Sample Location Estimation](https://arxiv.org/abs/2502.05730)
  - **标题**: None
  - **Filtered Reason**: none of stat.ML,math.ST,cs.DS in whitelist
- [Deep Reinforcement Learning for Backhaul Link Selection for Network Slices in IAB Networks](https://arxiv.org/abs/2502.05707)
  - **标题**: None
  - **Filtered Reason**: none of cs.NI in whitelist
- [A Conceptual Exploration of Generative AI-Induced Cognitive Dissonance and its Emergence in University-Level Academic Writing](https://arxiv.org/abs/2502.05698)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [Less is More for Synthetic Speech Detection in the Wild](https://arxiv.org/abs/2502.05674)
  - **标题**: None
  - **Filtered Reason**: none of cs.SD,eess.AS in whitelist
- [Online Controller Synthesis for Robot Collision Avoidance: A Case Study](https://arxiv.org/abs/2502.05667)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO,eess.SY in whitelist
- [Towards AI-driven Sign Language Generation with Non-manual Markers](https://arxiv.org/abs/2502.05661)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [FeatPCA: A feature subspace based principal component analysis technique for enhancing clustering of single-cell RNA-seq data](https://arxiv.org/abs/2502.05647)
  - **标题**: None
  - **Filtered Reason**: none of cs.CC in whitelist
- [ML DevOps Adoption in Practice: A Mixed-Method Study of Implementation Patterns and Organizational Benefits](https://arxiv.org/abs/2502.05634)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Rambler in the Wild: A Diary Study of LLM-Assisted Writing With Speech](https://arxiv.org/abs/2502.05612)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [AI-Driven Electronic Health Records System for Enhancing Patient Data Management and Diagnostic Support in Egypt](https://arxiv.org/abs/2502.05603)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [UbiMoE: A Ubiquitous Mixture-of-Experts Vision Transformer Accelerator With Hybrid Computation Pattern on FPGA](https://arxiv.org/abs/2502.05602)
  - **标题**: None
  - **Filtered Reason**: none of cs.AR in whitelist
- [Can Large Language Models Be Query Optimizer for Relational Databases?](https://arxiv.org/abs/2502.05562)
  - **标题**: None
  - **Filtered Reason**: none of cs.DB in whitelist
- [Coalition Formation for Heterogeneous Federated Learning Enabled Channel Estimation in RIS-assisted Cell-free MIMO](https://arxiv.org/abs/2502.05538)
  - **标题**: None
  - **Filtered Reason**: none of cs.IT in whitelist
- [User Identification Procedures with Human Mutations: Formal Analysis and Pilot Study (Extended Version)](https://arxiv.org/abs/2502.05530)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [Lie-algebra Adaptive Tracking Control for Rigid Body Dynamics](https://arxiv.org/abs/2502.05491)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO,eess.SY in whitelist
- [Robust Deep Signed Graph Clustering via Weak Balance Theory](https://arxiv.org/abs/2502.05472)
  - **标题**: None
  - **Filtered Reason**: none of cs.SI in whitelist
- [IllusionCAPTCHA: A CAPTCHA based on Visual Illusion](https://arxiv.org/abs/2502.05461)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [A Framework for On the Fly Input Refinement for Deep Learning Models](https://arxiv.org/abs/2502.05456)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [XPUTimer: Anomaly Diagnostics for Divergent LLM Training in GPU Clusters of Thousand-Plus Scale](https://arxiv.org/abs/2502.05413)
  - **标题**: None
  - **Filtered Reason**: none of cs.OS in whitelist
- [Demonstrating CavePI: Autonomous Exploration of Underwater Caves by Semantic Guidance](https://arxiv.org/abs/2502.05384)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [The Role of Human Creativity in the Presence of AI Creativity Tools at Work: A Case Study on AI-Driven Content Transformation in Journalism](https://arxiv.org/abs/2502.05347)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Atlas of AI Risks: Enhancing Public Understanding of AI Risks](https://arxiv.org/abs/2502.05324)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Benchmarking of Quantum and Classical Computing in Large-Scale Dynamic Portfolio Optimization Under Market Frictions](https://arxiv.org/abs/2502.05226)
  - **标题**: None
  - **Filtered Reason**: none of math.OC,cs.CC in whitelist
- [Advancing Geometry with AI: Multi-agent Generation of Polytopes](https://arxiv.org/abs/2502.05199)
  - **标题**: None
  - **Filtered Reason**: none of math.MG,math.CO,cs.CG in whitelist
- [Using Large Language Models for Solving Thermodynamic Problems](https://arxiv.org/abs/2502.05195)
  - **标题**: None
  - **Filtered Reason**: none of cs.CE in whitelist
- [pyMethods2Test: A Dataset of Python Tests Mapped to Focal Methods](https://arxiv.org/abs/2502.05143)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Information-Theoretic Guarantees for Recovering Low-Rank Tensors from Symmetric Rank-One Measurements](https://arxiv.org/abs/2502.05134)
  - **标题**: None
  - **Filtered Reason**: none of math.PR,stat.ML,math.ST,cs.IT in whitelist
- [Usability Issues With Mobile Applications: Insights From Practitioners and Future Research Directions](https://arxiv.org/abs/2502.05120)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Adoption of AI-Assisted E-Scooters: The Role of Perceived Trust, Safety, and Demographic Drivers](https://arxiv.org/abs/2502.05117)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Exploring the Generalizability of Geomagnetic Navigation: A Deep Reinforcement Learning approach with Policy Distillation](https://arxiv.org/abs/2502.05069)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [EcoServe: Designing Carbon-Aware AI Inference Systems](https://arxiv.org/abs/2502.05043)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC in whitelist
- [On the Possibility of Breaking Copyleft Licenses When Reusing Code Generated by ChatGPT](https://arxiv.org/abs/2502.05023)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [EnseSmells: Deep ensemble and programming language models for automated code smells detection](https://arxiv.org/abs/2502.05012)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [A Transformation-based Consistent Estimation Framework: Analysis, Design and Applications](https://arxiv.org/abs/2502.05008)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [MoGraphGPT: Creating Interactive Scenes Using Modular LLM and Graphical Control](https://arxiv.org/abs/2502.04983)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC,cs.GR in whitelist
- [Towards Multimodal Empathetic Response Generation: A Rich Text-Speech-Vision Avatar-based Benchmark](https://arxiv.org/abs/2502.04976)
  - **标题**: None
  - **Filtered Reason**: none of cs.MM in whitelist
- [Mobile Network-specialized Large Language Models for 6G: Architectures, Innovations, Challenges, and Future Trends](https://arxiv.org/abs/2502.04933)
  - **标题**: None
  - **Filtered Reason**: none of cs.NI in whitelist
- [Breaking the News: A LLM-based Game where Players Act as Influencer or Debunker for Raising Awareness About Misinformation](https://arxiv.org/abs/2502.04931)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Convergent NMPC-based Reinforcement Learning Using Deep Expected Sarsa and Nonlinear Temporal Difference Learning](https://arxiv.org/abs/2502.04925)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO,eess.SY in whitelist
- [Classification or Prompting: A Case Study on Legal Requirements Traceability](https://arxiv.org/abs/2502.04916)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Training-free Task-oriented Grasp Generation](https://arxiv.org/abs/2502.04873)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [Where does AI come from? A global case study across Europe, Africa, and Latin America](https://arxiv.org/abs/2502.04860)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [Reflecting on Design Paradigms of Animated Data Video Tools](https://arxiv.org/abs/2502.04801)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Comprehensive Formal Verification of Observational Correctness for the CHERIoT-Ibex Processor](https://arxiv.org/abs/2502.04738)
  - **标题**: None
  - **Filtered Reason**: none of cs.AR in whitelist
- [Dynamic Frequency-Adaptive Knowledge Distillation for Speech Enhancement](https://arxiv.org/abs/2502.04711)
  - **标题**: None
  - **Filtered Reason**: none of cs.SD,eess.AS in whitelist
- [Measuring SES-related traits relating to technology usage: Two validated surveys](https://arxiv.org/abs/2502.04710)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC,cs.SE in whitelist
- [Survey on Token-Based Distributed MutualExclusion Algorithms](https://arxiv.org/abs/2502.04708)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC in whitelist
- [LLM Query Scheduling with Prefix Reuse and Latency Constraints](https://arxiv.org/abs/2502.04677)
  - **标题**: None
  - **Filtered Reason**: none of cs.DS in whitelist
- [Enhancing Health Information Retrieval with RAG by Prioritizing Topical Relevance and Factual Accuracy](https://arxiv.org/abs/2502.04666)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
