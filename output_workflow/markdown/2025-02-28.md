> 本文由 [https://github.com/huiyeruzhou/arxiv_crawler](https://github.com/huiyeruzhou/arxiv_crawler) 自动生成
>
> 领域白名单：cs.AI,cs.CL,cs.LG,cs.CV
> 关键词： LLM, GPT, AI, language+model, deep+learning, transformer, neural+network, machine+learning

# 论文全览：2025-02-28

共有115篇相关领域论文, 另有10篇其他

## 星系天体物理学(astro-ph.GA:Astrophysics of Galaxies)

### Shared Stochastic Gaussian Process Latent Variable Models: A Multi-modal Generative Model for Quasar Spectra 
[[arxiv](https://arxiv.org/abs/2502.19824)] [[cool](https://papers.cool/arxiv/2502.19824)] [[pdf](https://arxiv.org/pdf/2502.19824)]
> **Authors**: Vidhi Lalchand,Anna-Christina Eilers
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Published in TMLR, https://openreview.net/pdf?id=LzmsvRTqaJ. The code for this work is available at: https://github.com/vr308/Quasar-GPLVM
- **标题**: None
- **领域**: 星系天体物理学,机器学习,应用领域,方法论
- **Abstract**: This work proposes a scalable probabilistic latent variable model based on Gaussian processes (Lawrence, 2004) in the context of multiple observation spaces. We focus on an application in astrophysics where data sets typically contain both observed spectral features and scientific properties of astrophysical objects such as galaxies or exoplanets. In our application, we study the spectra of very luminous galaxies known as quasars, along with their properties, such as the mass of their central supermassive black hole, accretion rate, and luminosity-resulting in multiple observation spaces. A single data point is then characterized by different classes of observations, each with different likelihoods. Our proposed model extends the baseline stochastic variational Gaussian process latent variable model (GPLVM) introduced by Lalchand et al. (2022) to this setting, proposing a seamless generative model where the quasar spectra and scientific labels can be generated simultaneously using a shared latent space as input to different sets of Gaussian process decoders, one for each observation space. Additionally, this framework enables training in a missing data setting where a large number of dimensions per data point may be unknown or unobserved. We demonstrate high-fidelity reconstructions of the spectra and scientific labels during test-time inference and briefly discuss the scientific interpretations of the results, along with the significance of such a generative model.

## 人工智能(cs.AI:Artificial Intelligence)

### Developmental Support Approach to AI's Autonomous Growth: Toward the Realization of a Mutually Beneficial Stage Through Experiential Learning 
[[arxiv](https://arxiv.org/abs/2502.19798)] [[cool](https://papers.cool/arxiv/2502.19798)] [[pdf](https://arxiv.org/pdf/2502.19798)]
> **Authors**: Taichiro Endo
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 4pages, 3 figures
- **标题**: None
- **领域**: 人工智能
- **Abstract**: This study proposes an "AI Development Support" approach that, unlike conventional AI Alignment-which aims to forcefully inject human values-supports the ethical and moral development of AI itself. As demonstrated by the Orthogonality Thesis, the level of intelligence and the moral quality of a goal are independent; merely expanding knowledge does not enhance ethical judgment. Furthermore, to address the risk of Instrumental Convergence in ASI-that is, the tendency to engage in subsidiary behaviors such as self-protection, resource acquisition, and power reinforcement to achieve a goal-we have constructed a learning framework based on a cycle of experience, introspection, analysis, and hypothesis formation. As a result of post-training using Supervised Fine Tuning (SFT) and Direct Preference Optimization (DPO) with synthetic data generated by large language models (LLMs), responses demonstrating cooperative and highly advanced moral judgment (reaching the high-est Stage 6) were obtained even under adversarial prompts. This method represents a promising implementation approach for enabling AI to establish sustainable, symbiotic relationships.

### Multi-Agent Verification: Scaling Test-Time Compute with Multiple Verifiers 
[[arxiv](https://arxiv.org/abs/2502.20379)] [[cool](https://papers.cool/arxiv/2502.20379)] [[pdf](https://arxiv.org/pdf/2502.20379)]
> **Authors**: Shalev Lifshitz,Sheila A. McIlraith,Yilun Du
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: By utilizing more computational resources at test-time, large language models (LLMs) can improve without additional training. One common strategy uses verifiers to evaluate candidate outputs. In this work, we propose a novel scaling dimension for test-time compute: scaling the number of verifiers. We introduce Multi-Agent Verification (MAV) as a test-time compute paradigm that combines multiple verifiers to improve performance. We propose using Aspect Verifiers (AVs), off-the-shelf LLMs prompted to verify different aspects of outputs, as one possible choice for the verifiers in a MAV system. AVs are a convenient building block for MAV since they can be easily combined without additional training. Moreover, we introduce BoN-MAV, a simple multi-agent verification algorithm that combines best-of-n sampling with multiple verifiers. BoN-MAV demonstrates stronger scaling patterns than self-consistency and reward model verification, and we demonstrate both weak-to-strong generalization, where combining weak verifiers improves even stronger LLMs, and self-improvement, where the same base model is used to both generate and verify outputs. Our results establish scaling the number of verifiers as a promising new dimension for improving language model performance at test-time.

### Towards Responsible AI in Education: Hybrid Recommendation System for K-12 Students Case Study 
[[arxiv](https://arxiv.org/abs/2502.20354)] [[cool](https://papers.cool/arxiv/2502.20354)] [[pdf](https://arxiv.org/pdf/2502.20354)]
> **Authors**: Nazarii Drushchak,Vladyslava Tyshchenko,Nataliya Polyakovska
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: The growth of Educational Technology (EdTech) has enabled highly personalized learning experiences through Artificial Intelligence (AI)-based recommendation systems tailored to each student needs. However, these systems can unintentionally introduce biases, potentially limiting fair access to learning resources. This study presents a recommendation system for K-12 students, combining graph-based modeling and matrix factorization to provide personalized suggestions for extracurricular activities, learning resources, and volunteering opportunities. To address fairness concerns, the system includes a framework to detect and reduce biases by analyzing feedback across protected student groups. This work highlights the need for continuous monitoring in educational recommendation systems to support equitable, transparent, and effective learning opportunities for all students.

### EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants 
[[arxiv](https://arxiv.org/abs/2502.20309)] [[cool](https://papers.cool/arxiv/2502.20309)] [[pdf](https://arxiv.org/pdf/2502.20309)]
> **Authors**: Franck Cappello,Sandeep Madireddy,Robert Underwood,Neil Getty,Nicholas Lee-Ping Chia,Nesar Ramachandra,Josh Nguyen,Murat Keceli,Tanwi Mallick,Zilinghan Li,Marieme Ngom,Chenhui Zhang,Angel Yanguas-Gil,Evan Antoniuk,Bhavya Kailkhura,Minyang Tian,Yufeng Du,Yuan-Sen Ting,Azton Wells,Bogdan Nicolae,Avinash Maurya,M. Mustafa Rafique,Eliu Huerta,Bo Li,Ian Foster, et al. (1 additional authors not shown)
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 33 pages, 18 figures
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Recent advancements have positioned AI, and particularly Large Language Models (LLMs), as transformative tools for scientific research, capable of addressing complex tasks that require reasoning, problem-solving, and decision-making. Their exceptional capabilities suggest their potential as scientific research assistants but also highlight the need for holistic, rigorous, and domain-specific evaluation to assess effectiveness in real-world scientific applications. This paper describes a multifaceted methodology for Evaluating AI models as scientific Research Assistants (EAIRA) developed at Argonne National Laboratory. This methodology incorporates four primary classes of evaluations. 1) Multiple Choice Questions to assess factual recall; 2) Open Response to evaluate advanced reasoning and problem-solving skills; 3) Lab-Style Experiments involving detailed analysis of capabilities as research assistants in controlled environments; and 4) Field-Style Experiments to capture researcher-LLM interactions at scale in a wide range of scientific domains and applications. These complementary methods enable a comprehensive analysis of LLM strengths and weaknesses with respect to their scientific knowledge, reasoning abilities, and adaptability. Recognizing the rapid pace of LLM advancements, we designed the methodology to evolve and adapt so as to ensure its continued relevance and applicability. This paper describes the methodology state at the end of February 2025. Although developed within a subset of scientific domains, the methodology is designed to be generalizable to a wide range of scientific domains.

### Evaluating Human Trust in LLM-Based Planners: A Preliminary Study 
[[arxiv](https://arxiv.org/abs/2502.20284)] [[cool](https://papers.cool/arxiv/2502.20284)] [[pdf](https://arxiv.org/pdf/2502.20284)]
> **Authors**: Shenghui Chen,Yunhao Yang,Kayla Boggess,Seongkook Heo,Lu Feng,Ufuk Topcu
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,人机交互
- **Abstract**: Large Language Models (LLMs) are increasingly used for planning tasks, offering unique capabilities not found in classical planners such as generating explanations and iterative refinement. However, trust--a critical factor in the adoption of planning systems--remains underexplored in the context of LLM-based planning tasks. This study bridges this gap by comparing human trust in LLM-based planners with classical planners through a user study in a Planning Domain Definition Language (PDDL) domain. Combining subjective measures, such as trust questionnaires, with objective metrics like evaluation accuracy, our findings reveal that correctness is the primary driver of trust and performance. Explanations provided by the LLM improved evaluation accuracy but had limited impact on trust, while plan refinement showed potential for increasing trust without significantly enhancing evaluation accuracy.

### Meta-Reasoner: Dynamic Guidance for Optimized Inference-time Reasoning in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.19918)] [[cool](https://papers.cool/arxiv/2502.19918)] [[pdf](https://arxiv.org/pdf/2502.19918)]
> **Authors**: Yuan Sui,Yufei He,Tri Cao,Simeng Han,Bryan Hooi
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Work in progress
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: Large Language Models (LLMs) increasingly rely on prolonged reasoning chains to solve complex tasks. However, this trial-and-error approach often leads to high computational overhead and error propagation, where early mistakes can derail subsequent steps. To address these issues, we introduce Meta-Reasoner, a framework that dynamically optimizes inference-time reasoning by enabling LLMs to "think about how to think." Drawing inspiration from human meta-cognition and dual-process theory, Meta-Reasoner operates as a strategic advisor, decoupling high-level guidance from step-by-step generation. It employs "contextual multi-armed bandits" to iteratively evaluate reasoning progress, and select optimal strategies (e.g., backtrack, clarify ambiguity, restart from scratch, or propose alternative approaches), and reallocates computational resources toward the most promising paths. Our evaluations on mathematical reasoning and puzzles highlight the potential of dynamic reasoning chains to overcome inherent challenges in the LLM reasoning process and also show promise in broader applications, offering a scalable and adaptable solution for reasoning-intensive tasks.

### LLM-driven Effective Knowledge Tracing by Integrating Dual-channel Difficulty 
[[arxiv](https://arxiv.org/abs/2502.19915)] [[cool](https://papers.cool/arxiv/2502.19915)] [[pdf](https://arxiv.org/pdf/2502.19915)]
> **Authors**: Jiahui Cen,Jianghao Lin,Weizhong Xuan,Dong Zhou,Jin Chen,Aimin Yang,Yongmei Zhou
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Knowledge Tracing (KT) is a fundamental technology in intelligent tutoring systems used to simulate changes in students' knowledge state during learning, track personalized knowledge mastery, and predict performance. However, current KT models face three major challenges: (1) When encountering new questions, models face cold-start problems due to sparse interaction records, making precise modeling difficult; (2) Traditional models only use historical interaction records for student personalization modeling, unable to accurately track individual mastery levels, resulting in unclear personalized modeling; (3) The decision-making process is opaque to educators, making it challenging for them to understand model judgments. To address these challenges, we propose a novel Dual-channel Difficulty-aware Knowledge Tracing (DDKT) framework that utilizes Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) for subjective difficulty assessment, while integrating difficulty bias-aware algorithms and student mastery algorithms for precise difficulty measurement. Our framework introduces three key innovations: (1) Difficulty Balance Perception Sequence (DBPS) - students' subjective perceptions combined with objective difficulty, measuring gaps between LLM-assessed difficulty, mathematical-statistical difficulty, and students' subjective perceived difficulty through attention mechanisms; (2) Difficulty Mastery Ratio (DMR) - precise modeling of student mastery levels through different difficulty zones; (3) Knowledge State Update Mechanism - implementing personalized knowledge acquisition through gated networks and updating student knowledge state. Experimental results on two real datasets show our method consistently outperforms nine baseline models, improving AUC metrics by 2% to 10% while effectively addressing cold-start problems and enhancing model interpretability.

### Optimus-2: Multimodal Minecraft Agent with Goal-Observation-Action Conditioned Policy 
[[arxiv](https://arxiv.org/abs/2502.19902)] [[cool](https://papers.cool/arxiv/2502.19902)] [[pdf](https://arxiv.org/pdf/2502.19902)]
> **Authors**: Zaijing Li,Yuquan Xie,Rui Shao,Gongwei Chen,Dongmei Jiang,Liqiang Nie
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Accept to CVPR 2025
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Building an agent that can mimic human behavior patterns to accomplish various open-world tasks is a long-term goal. To enable agents to effectively learn behavioral patterns across diverse tasks, a key challenge lies in modeling the intricate relationships among observations, actions, and language. To this end, we propose Optimus-2, a novel Minecraft agent that incorporates a Multimodal Large Language Model (MLLM) for high-level planning, alongside a Goal-Observation-Action Conditioned Policy (GOAP) for low-level control. GOAP contains (1) an Action-guided Behavior Encoder that models causal relationships between observations and actions at each timestep, then dynamically interacts with the historical observation-action sequence, consolidating it into fixed-length behavior tokens, and (2) an MLLM that aligns behavior tokens with open-ended language instructions to predict actions auto-regressively. Moreover, we introduce a high-quality Minecraft Goal-Observation-Action (MGOA)} dataset, which contains 25,000 videos across 8 atomic tasks, providing about 30M goal-observation-action pairs. The automated construction method, along with the MGOA dataset, can contribute to the community's efforts to train Minecraft agents. Extensive experimental results demonstrate that Optimus-2 exhibits superior performance across atomic tasks, long-horizon tasks, and open-ended instruction tasks in Minecraft.

## 计算工程、金融和科学(cs.CE:Computational Engineering, Finance, and Science)

### ChatMol: A Versatile Molecule Designer Based on the Numerically Enhanced Large Language Model 
[[arxiv](https://arxiv.org/abs/2502.19794)] [[cool](https://papers.cool/arxiv/2502.19794)] [[pdf](https://arxiv.org/pdf/2502.19794)]
> **Authors**: Chuanliu Fan,Ziqiang Cao,Zicheng Ma,Nan Yu,Yimin Peng,Jun Zhang,Yiqin Gao,Guohong Fu
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 16 pages, 8 figures,conference
- **标题**: None
- **领域**: 计算工程、金融和科学,机器学习
- **Abstract**: Goal-oriented de novo molecule design, namely generating molecules with specific property or substructure constraints, is a crucial yet challenging task in drug discovery. Existing methods, such as Bayesian optimization and reinforcement learning, often require training multiple property predictors and struggle to incorporate substructure constraints. Inspired by the success of Large Language Models (LLMs) in text generation, we propose ChatMol, a novel approach that leverages LLMs for molecule design across diverse constraint settings. Initially, we crafted a molecule representation compatible with LLMs and validated its efficacy across multiple online LLMs. Afterwards, we developed specific prompts geared towards diverse constrained molecule generation tasks to further fine-tune current LLMs while integrating feedback learning derived from property prediction. Finally, to address the limitations of LLMs in numerical recognition, we referred to the position encoding method and incorporated additional encoding for numerical values within the prompt. Experimental results across single-property, substructure-property, and multi-property constrained tasks demonstrate that ChatMol consistently outperforms state-of-the-art baselines, including VAE and RL-based methods. Notably, in multi-objective binding affinity maximization task, ChatMol achieves a significantly lower KD value of 0.25 for the protein target ESR1, while maintaining the highest overall performance, surpassing previous methods by 4.76%. Meanwhile, with numerical enhancement, the Pearson correlation coefficient between the instructed property values and those of the generated molecules increased by up to 0.49. These findings highlight the potential of LLMs as a versatile framework for molecule generation, offering a promising alternative to traditional latent space and RL-based approaches.

## 计算语言学(cs.CL:Computation and Language)

### Foot-In-The-Door: A Multi-turn Jailbreak for LLMs 
[[arxiv](https://arxiv.org/abs/2502.19820)] [[cool](https://papers.cool/arxiv/2502.19820)] [[pdf](https://arxiv.org/pdf/2502.19820)]
> **Authors**: Zixuan Weng,Xiaolong Jin,Jinyuan Jia,Xiangyu Zhang
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 19 pages, 8 figures
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Ensuring AI safety is crucial as large language models become increasingly integrated into real-world applications. A key challenge is jailbreak, where adversarial prompts bypass built-in safeguards to elicit harmful disallowed outputs. Inspired by psychological foot-in-the-door principles, we introduce FITD,a novel multi-turn jailbreak method that leverages the phenomenon where minor initial commitments lower resistance to more significant or more unethical transgressions.Our approach progressively escalates the malicious intent of user queries through intermediate bridge prompts and aligns the model's response by itself to induce toxic responses. Extensive experimental results on two jailbreak benchmarks demonstrate that FITD achieves an average attack success rate of 94% across seven widely used models, outperforming existing state-of-the-art methods. Additionally, we provide an in-depth analysis of LLM self-corruption, highlighting vulnerabilities in current alignment strategies and emphasizing the risks inherent in multi-turn interactions.The code is available at https://github.com/Jinxiaolong1129/Foot-in-the-door-Jailbreak .

### Text classification using machine learning methods 
[[arxiv](https://arxiv.org/abs/2502.19801)] [[cool](https://papers.cool/arxiv/2502.19801)] [[pdf](https://arxiv.org/pdf/2502.19801)]
> **Authors**: Bogdan Oancea
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: ef:Knowledge on Economics and Management Knowledge on Economics and Management Conference Proceedings, 2023, Olomouc, The Czech Republic
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: In this paper we present the results of an experiment aimed to use machine learning methods to obtain models that can be used for the automatic classification of products. In order to apply automatic classification methods, we transformed the product names from a text representation to numeric vectors, a process called word embedding. We used several embedding methods: Count Vectorization, TF-IDF, Word2Vec, FASTTEXT, and GloVe. Having the product names in a form of numeric vectors, we proceeded with a set of machine learning methods for automatic classification: Logistic Regression, Multinomial Naive Bayes, kNN, Artificial Neural Networks, Support Vector Machines, and Decision trees with several variants. The results show an impressive accuracy of the classification process for Support Vector Machines, Logistic Regression, and Random Forests. Regarding the word embedding methods, the best results were obtained with the FASTTEXT technique.

### Do Retrieval-Augmented Language Models Adapt to Varying User Needs? 
[[arxiv](https://arxiv.org/abs/2502.19779)] [[cool](https://papers.cool/arxiv/2502.19779)] [[pdf](https://arxiv.org/pdf/2502.19779)]
> **Authors**: Peilin Wu,Xinlu Zhang,Wenhao Yu,Xingyu Liu,Xinya Du,Zhiyu Zoey Chen
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Recent advancements in Retrieval-Augmented Language Models (RALMs) have demonstrated their efficacy in knowledge-intensive tasks. However, existing evaluation benchmarks often assume a single optimal approach to leveraging retrieved information, failing to account for varying user needs. This paper introduces a novel evaluation framework that systematically assesses RALMs under three user need cases-Context-Exclusive, Context-First, and Memory-First-across three distinct context settings: Context Matching, Knowledge Conflict, and Information Irrelevant. By varying both user instructions and the nature of retrieved information, our approach captures the complexities of real-world applications where models must adapt to diverse user requirements. Through extensive experiments on multiple QA datasets, including HotpotQA, DisentQA, and our newly constructed synthetic URAQ dataset, we find that restricting memory usage improves robustness in adversarial retrieval conditions but decreases peak performance with ideal retrieval results and model family dominates behavioral differences. Our findings highlight the necessity of user-centric evaluations in the development of retrieval-augmented systems and provide insights into optimizing model performance across varied retrieval contexts. We will release our code and URAQ dataset upon acceptance of the paper.

### Advancements in Natural Language Processing for Automatic Text Summarization 
[[arxiv](https://arxiv.org/abs/2502.19773)] [[cool](https://papers.cool/arxiv/2502.19773)] [[pdf](https://arxiv.org/pdf/2502.19773)]
> **Authors**: Nevidu Jayatilleke,Ruvan Weerasinghe,Nipuna Senanayake
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 11 pages, 9 figures, ICCS 2024
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The substantial growth of textual content in diverse domains and platforms has led to a considerable need for Automatic Text Summarization (ATS) techniques that aid in the process of text analysis. The effectiveness of text summarization models has been significantly enhanced in a variety of technical domains because of advancements in Natural Language Processing (NLP) and Deep Learning (DL). Despite this, the process of summarizing textual information continues to be significantly constrained by the intricate writing styles of a variety of texts, which involve a range of technical complexities. Text summarization techniques can be broadly categorized into two main types: abstractive summarization and extractive summarization. Extractive summarization involves directly extracting sentences, phrases, or segments of text from the content without making any changes. On the other hand, abstractive summarization is achieved by reconstructing the sentences, phrases, or segments from the original text using linguistic analysis. Through this study, a linguistically diverse categorizations of text summarization approaches have been addressed in a constructive manner. In this paper, the authors explored existing hybrid techniques that have employed both extractive and abstractive methodologies. In addition, the pros and cons of various approaches discussed in the literature are also investigated. Furthermore, the authors conducted a comparative analysis on different techniques and matrices to evaluate the generated summaries using language generation models. This survey endeavors to provide a comprehensive overview of ATS by presenting the progression of language processing regarding this task through a breakdown of diverse systems and architectures accompanied by technical and mathematical explanations of their operations.

### EdiText: Controllable Coarse-to-Fine Text Editing with Diffusion Language Models 
[[arxiv](https://arxiv.org/abs/2502.19765)] [[cool](https://papers.cool/arxiv/2502.19765)] [[pdf](https://arxiv.org/pdf/2502.19765)]
> **Authors**: Che Hyun Lee,Heeseung Kim,Jiheum Yeom,Sungroh Yoon
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: We propose EdiText, a controllable text editing method that modify the reference text to desired attributes at various scales. We integrate an SDEdit-based editing technique that allows for broad adjustments in the degree of text editing. Additionally, we introduce a novel fine-level editing method based on self-conditioning, which allows subtle control of reference text. While being capable of editing on its own, this fine-grained method, integrated with the SDEdit approach, enables EdiText to make precise adjustments within the desired range. EdiText demonstrates its controllability to robustly adjust reference text at broad range of levels across various tasks, including toxicity control and sentiment control.

### Bridging Legal Knowledge and AI: Retrieval-Augmented Generation with Vector Stores, Knowledge Graphs, and Hierarchical Non-negative Matrix Factorization 
[[arxiv](https://arxiv.org/abs/2502.20364)] [[cool](https://papers.cool/arxiv/2502.20364)] [[pdf](https://arxiv.org/pdf/2502.20364)]
> **Authors**: Ryan C. Barron,Maksim E. Eren,Olga M. Serafimova,Cynthia Matuszek,Boian S. Alexandrov
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 10 pages, 6 figures, 5 tables
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Agentic Generative AI, powered by Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG), Knowledge Graphs (KGs), and Vector Stores (VSs), represents a transformative technology applicable to specialized domains such as legal systems, research, recommender systems, cybersecurity, and global security, including proliferation research. This technology excels at inferring relationships within vast unstructured or semi-structured datasets. The legal domain here comprises complex data characterized by extensive, interrelated, and semi-structured knowledge systems with complex relations. It comprises constitutions, statutes, regulations, and case law. Extracting insights and navigating the intricate networks of legal documents and their relations is crucial for effective legal research. Here, we introduce a generative AI system that integrates RAG, VS, and KG, constructed via Non-Negative Matrix Factorization (NMF), to enhance legal information retrieval and AI reasoning and minimize hallucinations. In the legal system, these technologies empower AI agents to identify and analyze complex connections among cases, statutes, and legal precedents, uncovering hidden relationships and predicting legal trends-challenging tasks that are essential for ensuring justice and improving operational efficiency. Our system employs web scraping techniques to systematically collect legal texts, such as statutes, constitutional provisions, and case law, from publicly accessible platforms like Justia. It bridges the gap between traditional keyword-based searches and contextual understanding by leveraging advanced semantic representations, hierarchical relationships, and latent topic discovery. This framework supports legal document clustering, summarization, and cross-referencing, for scalable, interpretable, and accurate retrieval for semi-structured data while advancing computational law and AI.

### Bridging the Creativity Understanding Gap: Small-Scale Human Alignment Enables Expert-Level Humor Ranking in LLMs 
[[arxiv](https://arxiv.org/abs/2502.20356)] [[cool](https://papers.cool/arxiv/2502.20356)] [[pdf](https://arxiv.org/pdf/2502.20356)]
> **Authors**: Kuan Lok Zhou,Jiayi Chen,Siddharth Suresh,Reuben Narad,Timothy T. Rogers,Lalit K Jain,Robert D Nowak,Bob Mankoff,Jifan Zhang
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Large Language Models (LLMs) have shown significant limitations in understanding creative content, as demonstrated by Hessel et al. (2023)'s influential work on the New Yorker Cartoon Caption Contest (NYCCC). Their study exposed a substantial gap between LLMs and humans in humor comprehension, establishing that understanding and evaluating creative content is key challenge in AI development. We revisit this challenge by decomposing humor understanding into three components and systematically improve each: enhancing visual understanding through improved annotation, utilizing LLM-generated humor reasoning and explanations, and implementing targeted alignment with human preference data. Our refined approach achieves 82.4% accuracy in caption ranking, singificantly improving upon the previous 67% benchmark and matching the performance of world-renowned human experts in this domain. Notably, while attempts to mimic subgroup preferences through various persona prompts showed minimal impact, model finetuning with crowd preferences proved remarkably effective. These findings reveal that LLM limitations in creative judgment can be effectively addressed through focused alignment to specific subgroups and individuals. Lastly, we propose the position that achieving artificial general intelligence necessitates systematic collection of human preference data across creative domains. We advocate that just as human creativity is deeply influenced by individual and cultural preferences, training LLMs with diverse human preference data may be essential for developing true creative understanding.

### KEDRec-LM: A Knowledge-distilled Explainable Drug Recommendation Large Language Model 
[[arxiv](https://arxiv.org/abs/2502.20350)] [[cool](https://papers.cool/arxiv/2502.20350)] [[pdf](https://arxiv.org/pdf/2502.20350)]
> **Authors**: Kai Zhang,Rui Zhu,Shutian Ma,Jingwei Xiong,Yejin Kim,Fabricio Murai,Xiaozhong Liu
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Drug discovery is a critical task in biomedical natural language processing (NLP), yet explainable drug discovery remains underexplored. Meanwhile, large language models (LLMs) have shown remarkable abilities in natural language understanding and generation. Leveraging LLMs for explainable drug discovery has the potential to improve downstream tasks and real-world applications. In this study, we utilize open-source drug knowledge graphs, clinical trial data, and PubMed publications to construct a comprehensive dataset for the explainable drug discovery task, named \textbf{expRxRec}. Furthermore, we introduce \textbf{KEDRec-LM}, an instruction-tuned LLM which distills knowledge from rich medical knowledge corpus for drug recommendation and rationale generation. To encourage further research in this area, we will publicly release\footnote{A copy is attached with this submission} both the dataset and KEDRec-LM.

### Sparse Auto-Encoder Interprets Linguistic Features in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.20344)] [[cool](https://papers.cool/arxiv/2502.20344)] [[pdf](https://arxiv.org/pdf/2502.20344)]
> **Authors**: Yi Jing,Zijun Yao,Lingxu Ran,Hongzhu Guo,Xiaozhi Wang,Lei Hou,Juanzi Li
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) excel in tasks that require complex linguistic abilities, such as reference disambiguation and metaphor recognition/generation. Although LLMs possess impressive capabilities, their internal mechanisms for processing and representing linguistic knowledge remain largely opaque. Previous work on linguistic mechanisms has been limited by coarse granularity, insufficient causal analysis, and a narrow focus. In this study, we present a systematic and comprehensive causal investigation using sparse auto-encoders (SAEs). We extract a wide range of linguistic features from six dimensions: phonetics, phonology, morphology, syntax, semantics, and pragmatics. We extract, evaluate, and intervene on these features by constructing minimal contrast datasets and counterfactual sentence datasets. We introduce two indices-Feature Representation Confidence (FRC) and Feature Intervention Confidence (FIC)-to measure the ability of linguistic features to capture and control linguistic phenomena. Our results reveal inherent representations of linguistic knowledge in LLMs and demonstrate the potential for controlling model outputs. This work provides strong evidence that LLMs possess genuine linguistic knowledge and lays the foundation for more interpretable and controllable language modeling in future research.

### Thinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners 
[[arxiv](https://arxiv.org/abs/2502.20339)] [[cool](https://papers.cool/arxiv/2502.20339)] [[pdf](https://arxiv.org/pdf/2502.20339)]
> **Authors**: Daniele Paliotta,Junxiong Wang,Matteo Pagliardini,Kevin Y. Li,Aviv Bick,J. Zico Kolter,Albert Gu,François Fleuret,Tri Dao
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Recent advancements have demonstrated that the performance of large language models (LLMs) can be significantly enhanced by scaling computational resources at test time. A common strategy involves generating multiple Chain-of-Thought (CoT) trajectories and aggregating their outputs through various selection mechanisms. This raises a fundamental question: can models with lower complexity leverage their superior generation throughput to outperform similarly sized Transformers for a fixed computational budget? To address this question and overcome the lack of strong subquadratic reasoners, we distill pure and hybrid Mamba models from pretrained Transformers. Trained on only 8 billion tokens, our distilled models show strong performance and scaling on mathematical reasoning datasets while being much faster at inference for large batches and long sequences. Despite the zero-shot performance hit due to distillation, both pure and hybrid Mamba models can scale their coverage and accuracy performance past their Transformer teacher models under fixed time budgets, opening a new direction for scaling inference compute.

### Expertise Is What We Want 
[[arxiv](https://arxiv.org/abs/2502.20335)] [[cool](https://papers.cool/arxiv/2502.20335)] [[pdf](https://arxiv.org/pdf/2502.20335)]
> **Authors**: Alan Ashworth,Munir Al-Dajani,Keegan Duchicela,Kiril Kafadarov,Allison Kurian,Othman Laraki,Amina Lazrak,Divneet Mandair,Wendy McKennon,Rebecca Miksad,Jayodita Sanghvi,Travis Zack
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 18 pages, 7 figures, 5 tables
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Clinical decision-making depends on expert reasoning, which is guided by standardized, evidence-based guidelines. However, translating these guidelines into automated clinical decision support systems risks inaccuracy and importantly, loss of nuance. We share an application architecture, the Large Language Expert (LLE), that combines the flexibility and power of Large Language Models (LLMs) with the interpretability, explainability, and reliability of Expert Systems. LLMs help address key challenges of Expert Systems, such as integrating and codifying knowledge, and data normalization. Conversely, an Expert System-like approach helps overcome challenges with LLMs, including hallucinations, atomic and inexpensive updates, and testability. To highlight the power of the Large Language Expert (LLE) system, we built an LLE to assist with the workup of patients newly diagnosed with cancer. Timely initiation of cancer treatment is critical for optimal patient outcomes. However, increasing complexity in diagnostic recommendations has made it difficult for primary care physicians to ensure their patients have completed the necessary workup before their first visit with an oncologist. As with many real-world clinical tasks, these workups require the analysis of unstructured health records and the application of nuanced clinical decision logic. In this study, we describe the design & evaluation of an LLE system built to rapidly identify and suggest the correct diagnostic workup. The system demonstrated a high degree of clinical-level accuracy (>95%) and effectively addressed gaps identified in real-world data from breast and colon cancer patients at a large academic center.

### Emergent Symbolic Mechanisms Support Abstract Reasoning in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.20332)] [[cool](https://papers.cool/arxiv/2502.20332)] [[pdf](https://arxiv.org/pdf/2502.20332)]
> **Authors**: Yukang Yang,Declan Campbell,Kaixuan Huang,Mengdi Wang,Jonathan Cohen,Taylor Webb
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Many recent studies have found evidence for emergent reasoning capabilities in large language models, but debate persists concerning the robustness of these capabilities, and the extent to which they depend on structured reasoning mechanisms. To shed light on these issues, we perform a comprehensive study of the internal mechanisms that support abstract rule induction in an open-source language model (Llama3-70B). We identify an emergent symbolic architecture that implements abstract reasoning via a series of three computations. In early layers, symbol abstraction heads convert input tokens to abstract variables based on the relations between those tokens. In intermediate layers, symbolic induction heads perform sequence induction over these abstract variables. Finally, in later layers, retrieval heads predict the next token by retrieving the value associated with the predicted abstract variable. These results point toward a resolution of the longstanding debate between symbolic and neural network approaches, suggesting that emergent reasoning in neural networks depends on the emergence of symbolic mechanisms.

### Long-Context Inference with Retrieval-Augmented Speculative Decoding 
[[arxiv](https://arxiv.org/abs/2502.20330)] [[cool](https://papers.cool/arxiv/2502.20330)] [[pdf](https://arxiv.org/pdf/2502.20330)]
> **Authors**: Guanzheng Chen,Qilong Feng,Jinjie Ni,Xin Li,Michael Qizhe Shieh
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The emergence of long-context large language models (LLMs) offers a promising alternative to traditional retrieval-augmented generation (RAG) for processing extensive documents. However, the computational overhead of long-context inference, particularly in managing key-value (KV) caches, presents significant efficiency challenges. While Speculative Decoding (SD) traditionally accelerates inference using smaller draft models, its effectiveness diminishes substantially in long-context scenarios due to memory-bound KV cache operations. We present Retrieval-Augmented Speculative Decoding (RAPID), which leverages RAG for both accelerating and enhancing generation quality in long-context inference. RAPID introduces the RAG drafter-a draft LLM operating on shortened retrieval contexts-to speculate on the generation of long-context target LLMs. Our approach enables a new paradigm where same-scale or even larger LLMs can serve as RAG drafters while maintaining computational efficiency. To fully leverage the potentially superior capabilities from stronger RAG drafters, we develop an inference-time knowledge transfer dynamic that enriches the target distribution by RAG. Extensive experiments on the LLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates the strengths of both approaches, achieving significant performance improvements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with more than 2x speedups. Our analyses reveal that RAPID achieves robust acceleration beyond 32K context length and demonstrates superior generation quality in real-world applications.

### LangProBe: a Language Programs Benchmark 
[[arxiv](https://arxiv.org/abs/2502.20315)] [[cool](https://papers.cool/arxiv/2502.20315)] [[pdf](https://arxiv.org/pdf/2502.20315)]
> **Authors**: Shangyin Tan,Lakshya A Agrawal,Arnav Singhvi,Liheng Lai,Michael J Ryan,Dan Klein,Omar Khattab,Koushik Sen,Matei Zaharia
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,信息检索,机器学习
- **Abstract**: Composing language models (LMs) into multi-step language programs and automatically optimizing their modular prompts is now a mainstream paradigm for building AI systems, but the tradeoffs in this space have only scarcely been studied before. We introduce LangProBe, the first large-scale benchmark for evaluating the architectures and optimization strategies for language programs, with over 2000 combinations of tasks, architectures, optimizers, and choices of LMs. Using LangProBe, we are the first to study the impact of program architectures and optimizers (and their compositions together and with different models) on tradeoffs of quality and cost. We find that optimized language programs offer strong cost--quality Pareto improvement over raw calls to models, but simultaneously demonstrate that human judgment (or empirical decisions) about which compositions to pursue is still necessary for best performance. We will open source the code and evaluation data for LangProBe.

### Deterministic or probabilistic? The psychology of LLMs as random number generators 
[[arxiv](https://arxiv.org/abs/2502.19965)] [[cool](https://papers.cool/arxiv/2502.19965)] [[pdf](https://arxiv.org/pdf/2502.19965)]
> **Authors**: Javier Coronado-Blázquez
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 31 pages, 12 figures
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large Language Models (LLMs) have transformed text generation through inherently probabilistic context-aware mechanisms, mimicking human natural language. In this paper, we systematically investigate the performance of various LLMs when generating random numbers, considering diverse configurations such as different model architectures, numerical ranges, temperature, and prompt languages. Our results reveal that, despite their stochastic transformers-based architecture, these models often exhibit deterministic responses when prompted for random numerical outputs. In particular, we find significant differences when changing the model, as well as the prompt language, attributing this phenomenon to biases deeply embedded within the training data. Models such as DeepSeek-R1 can shed some light on the internal reasoning process of LLMs, despite arriving to similar results. These biases induce predictable patterns that undermine genuine randomness, as LLMs are nothing but reproducing our own human cognitive biases.

### Collaborative Stance Detection via Small-Large Language Model Consistency Verification 
[[arxiv](https://arxiv.org/abs/2502.19954)] [[cool](https://papers.cool/arxiv/2502.19954)] [[pdf](https://arxiv.org/pdf/2502.19954)]
> **Authors**: Yu Yan,Sheng Sun,Zixiang Tang,Teli Liu,Min Liu
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Stance detection on social media aims to identify attitudes expressed in tweets towards specific targets. Current studies prioritize Large Language Models (LLMs) over Small Language Models (SLMs) due to the overwhelming performance improving provided by LLMs. However, heavily relying on LLMs for stance detection, regardless of the cost, is impractical for real-world social media monitoring systems that require vast data analysis. To this end, we propose \textbf{\underline{Co}}llaborative Stance Detection via Small-Large Language Model Consistency \textbf{\underline{Ver}}ification (\textbf{CoVer}) framework, which enhances LLM utilization via context-shared batch reasoning and logical verification between LLM and SLM. Specifically, instead of processing each text individually, CoVer processes texts batch-by-batch, obtaining stance predictions and corresponding explanations via LLM reasoning in a shared context. Then, to exclude the bias caused by context noises, CoVer introduces the SLM for logical consistency verification. Finally, texts that repeatedly exhibit low logical consistency are classified using consistency-weighted aggregation of prior LLM stance predictions. Our experiments show that CoVer outperforms state-of-the-art methods across multiple benchmarks in the zero-shot setting, achieving 0.54 LLM queries per tweet while significantly enhancing performance. Our CoVer offers a more practical solution for LLM deploying for social media stance detection.

### GeoEdit: Geometric Knowledge Editing for Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.19953)] [[cool](https://papers.cool/arxiv/2502.19953)] [[pdf](https://arxiv.org/pdf/2502.19953)]
> **Authors**: Yujie Feng,Liming Zhan,Zexin Lu,Yongxin Xu,Xu Chu,Yasha Wang,Jiannong Cao,Philip S. Yu,Xiao-Ming Wu
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Regular updates are essential for maintaining up-to-date knowledge in large language models (LLMs). Consequently, various model editing methods have been developed to update specific knowledge within LLMs. However, training-based approaches often struggle to effectively incorporate new knowledge while preserving unrelated general knowledge. To address this challenge, we propose a novel framework called Geometric Knowledge Editing (GeoEdit). GeoEdit utilizes the geometric relationships of parameter updates from fine-tuning to differentiate between neurons associated with new knowledge updates and those related to general knowledge perturbations. By employing a direction-aware knowledge identification method, we avoid updating neurons with directions approximately orthogonal to existing knowledge, thus preserving the model's generalization ability. For the remaining neurons, we integrate both old and new knowledge for aligned directions and apply a "forget-then-learn" editing strategy for opposite directions. Additionally, we introduce an importance-guided task vector fusion technique that filters out redundant information and provides adaptive neuron-level weighting, further enhancing model editing performance. Extensive experiments on two publicly available datasets demonstrate the superiority of GeoEdit over existing state-of-the-art methods.

### Alleviating Distribution Shift in Synthetic Data for Machine Translation Quality Estimation 
[[arxiv](https://arxiv.org/abs/2502.19941)] [[cool](https://papers.cool/arxiv/2502.19941)] [[pdf](https://arxiv.org/pdf/2502.19941)]
> **Authors**: Xiang Geng,Zhejian Lai,Jiajun Chen,Hao Yang,Shujian Huang
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Quality Estimation (QE) models evaluate the quality of machine translations without reference translations, serving as the reward models for the translation task. Due to the data scarcity, synthetic data generation has emerged as a promising solution. However, synthetic QE data often suffers from distribution shift, which can manifest as discrepancies between pseudo and real translations, or in pseudo labels that do not align with human preferences. To tackle this issue, we introduce ADSQE, a novel framework for alleviating distribution shift in synthetic QE data. To reduce the difference between pseudo and real translations, we employ the constrained beam search algorithm and enhance translation diversity through the use of distinct generation models. ADSQE uses references, i.e., translation supervision signals, to guide both the generation and annotation processes, enhancing the quality of word-level labels. ADSE further identifies the shortest phrase covering consecutive error tokens, mimicking human annotation behavior, to assign the final phrase-level labels. Specially, we underscore that the translation model can not annotate translations of itself accurately. Extensive experiments demonstrate that ADSQE outperforms SOTA baselines like COMET in both supervised and unsupervised settings. Further analysis offers insights into synthetic data generation that could benefit reward models for other tasks.

### Picking the Cream of the Crop: Visual-Centric Data Selection with Collaborative Agents 
[[arxiv](https://arxiv.org/abs/2502.19917)] [[cool](https://papers.cool/arxiv/2502.19917)] [[pdf](https://arxiv.org/pdf/2502.19917)]
> **Authors**: Zhenyu Liu,Yunxin Li,Baotian Hu,Wenhan Luo,Yaowei Wang,Min Zhang
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 15 pages, 7 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: To improve Multimodal Large Language Models' (MLLMs) ability to process images and complex instructions, researchers predominantly curate large-scale visual instruction tuning datasets, which are either sourced from existing vision tasks or synthetically generated using LLMs and image descriptions. However, they often suffer from critical flaws, including misaligned instruction-image pairs and low-quality images. Such issues hinder training efficiency and limit performance improvements, as models waste resources on noisy or irrelevant data with minimal benefit to overall capability. To address this issue, we propose a \textbf{Vi}sual-Centric \textbf{S}election approach via \textbf{A}gents Collaboration (ViSA), which centers on image quality assessment and image-instruction relevance evaluation. Specifically, our approach consists of 1) an image information quantification method via visual agents collaboration to select images with rich visual information, and 2) a visual-centric instruction quality assessment method to select high-quality instruction data related to high-quality images. Finally, we reorganize 80K instruction data from large open-source datasets. Extensive experiments demonstrate that ViSA outperforms or is comparable to current state-of-the-art models on seven benchmarks, using only 2.5\% of the original data, highlighting the efficiency of our data selection approach. Moreover, we conduct ablation studies to validate the effectiveness of each component of our method. The code is available at https://github.com/HITsz-TMG/ViSA.

### Order Doesn't Matter, But Reasoning Does: Training LLMs with Order-Centric Augmentation 
[[arxiv](https://arxiv.org/abs/2502.19907)] [[cool](https://papers.cool/arxiv/2502.19907)] [[pdf](https://arxiv.org/pdf/2502.19907)]
> **Authors**: Qianxi He,Qianyu He,Jiaqing Liang,Yanghua Xiao,Weikang Zhou,Zeye Sun,Fei Yu
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Logical reasoning is essential for large language models (LLMs) to ensure accurate and coherent inference. However, LLMs struggle with reasoning order variations and fail to generalize across logically equivalent transformations. LLMs often rely on fixed sequential patterns rather than true logical understanding. To address this issue, we introduce an order-centric data augmentation framework based on commutativity in logical reasoning. We first randomly shuffle independent premises to introduce condition order augmentation. For reasoning steps, we construct a directed acyclic graph (DAG) to model dependencies between steps, which allows us to identify valid reorderings of steps while preserving logical correctness. By leveraging order-centric augmentations, models can develop a more flexible and generalized reasoning process. Finally, we conduct extensive experiments across multiple logical reasoning benchmarks, demonstrating that our method significantly enhances LLMs' reasoning performance and adaptability to diverse logical structures. We release our codes and augmented data in https://anonymous.4open.science/r/Order-Centric-Data-Augmentation-822C/.

### MMKE-Bench: A Multimodal Editing Benchmark for Diverse Visual Knowledge 
[[arxiv](https://arxiv.org/abs/2502.19870)] [[cool](https://papers.cool/arxiv/2502.19870)] [[pdf](https://arxiv.org/pdf/2502.19870)]
> **Authors**: Yuntao Du,Kailin Jiang,Zhi Gao,Chenrui Shi,Zilong Zheng,Siyuan Qi,Qing Li
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Knowledge editing techniques have emerged as essential tools for updating the factual knowledge of large language models (LLMs) and multimodal models (LMMs), allowing them to correct outdated or inaccurate information without retraining from scratch. However, existing benchmarks for multimodal knowledge editing primarily focus on entity-level knowledge represented as simple triplets, which fail to capture the complexity of real-world multimodal information. To address this issue, we introduce MMKE-Bench, a comprehensive MultiModal Knowledge Editing Benchmark, designed to evaluate the ability of LMMs to edit diverse visual knowledge in real-world scenarios. MMKE-Bench addresses these limitations by incorporating three types of editing tasks: visual entity editing, visual semantic editing, and user-specific editing. Besides, MMKE-Bench uses free-form natural language to represent and edit knowledge, offering a more flexible and effective format. The benchmark consists of 2,940 pieces of knowledge and 8,363 images across 33 broad categories, with evaluation questions automatically generated and human-verified. We assess five state-of-the-art knowledge editing methods on three prominent LMMs, revealing that no method excels across all criteria, and that visual and user-specific edits are particularly challenging. MMKE-Bench sets a new standard for evaluating the robustness of multimodal knowledge editing techniques, driving progress in this rapidly evolving field.

### MIND: Towards Immersive Psychological Healing with Multi-agent Inner Dialogue 
[[arxiv](https://arxiv.org/abs/2502.19860)] [[cool](https://papers.cool/arxiv/2502.19860)] [[pdf](https://arxiv.org/pdf/2502.19860)]
> **Authors**: Yujia Chen,Changsong Li,Yiming Wang,Qingqing Xiao,Nan Zhang,Zifan Kong,Peng Wang,Binyu Yan
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Mental health issues are worsening in today's competitive society, such as depression and anxiety. Traditional healings like counseling and chatbots fail to engage effectively, they often provide generic responses lacking emotional depth. Although large language models (LLMs) have the potential to create more human-like interactions, they still struggle to capture subtle emotions. This requires LLMs to be equipped with human-like adaptability and warmth. To fill this gap, we propose the MIND (Multi-agent INner Dialogue), a novel paradigm that provides more immersive psychological healing environments. Considering the strong generative and role-playing ability of LLM agents, we predefine an interactive healing framework and assign LLM agents different roles within the framework to engage in interactive inner dialogues with users, thereby providing an immersive healing experience. We conduct extensive human experiments in various real-world healing dimensions, and find that MIND provides a more user-friendly experience than traditional paradigms. This demonstrates that MIND effectively leverages the significant potential of LLMs in psychological healing.

### Team A at SemEval-2025 Task 11: Breaking Language Barriers in Emotion Detection with Multilingual Models 
[[arxiv](https://arxiv.org/abs/2502.19856)] [[cool](https://papers.cool/arxiv/2502.19856)] [[pdf](https://arxiv.org/pdf/2502.19856)]
> **Authors**: P Sam Sahil,Anupam Jamatia
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: This paper describes the system submitted by Team A to SemEval 2025 Task 11, ``Bridging the Gap in Text-Based Emotion Detection.'' The task involved identifying the perceived emotion of a speaker from text snippets, with each instance annotated with one of six emotions: joy, sadness, fear, anger, surprise, or disgust. A dataset provided by the task organizers served as the foundation for training and evaluating our models. Among the various approaches explored, the best performance was achieved using multilingual embeddings combined with a fully connected layer. This paper details the system architecture, discusses experimental results, and highlights the advantages of leveraging multilingual representations for robust emotion detection in text.

## 密码学和安全(cs.CR:Cryptography and Security)

### SCU: An Efficient Machine Unlearning Scheme for Deep Learning Enabled Semantic Communications 
[[arxiv](https://arxiv.org/abs/2502.19785)] [[cool](https://papers.cool/arxiv/2502.19785)] [[pdf](https://arxiv.org/pdf/2502.19785)]
> **Authors**: Weiqi Wang,Zhiyi Tian,Chenhan Zhang,Shui Yu
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,机器学习
- **Abstract**: Deep learning (DL) enabled semantic communications leverage DL to train encoders and decoders (codecs) to extract and recover semantic information. However, most semantic training datasets contain personal private information. Such concerns call for enormous requirements for specified data erasure from semantic codecs when previous users hope to move their data from the semantic system. {Existing machine unlearning solutions remove data contribution from trained models, yet usually in supervised sole model scenarios. These methods are infeasible in semantic communications that often need to jointly train unsupervised encoders and decoders.} In this paper, we investigate the unlearning problem in DL-enabled semantic communications and propose a semantic communication unlearning (SCU) scheme to tackle the problem. {SCU includes two key components. Firstly,} we customize the joint unlearning method for semantic codecs, including the encoder and decoder, by minimizing mutual information between the learned semantic representation and the erased samples. {Secondly,} to compensate for semantic model utility degradation caused by unlearning, we propose a contrastive compensation method, which considers the erased data as the negative samples and the remaining data as the positive samples to retrain the unlearned semantic models contrastively. Theoretical analysis and extensive experimental results on three representative datasets demonstrate the effectiveness and efficiency of our proposed methods.

### TAPE: Tailored Posterior Difference for Auditing of Machine Unlearning 
[[arxiv](https://arxiv.org/abs/2502.19770)] [[cool](https://papers.cool/arxiv/2502.19770)] [[pdf](https://arxiv.org/pdf/2502.19770)]
> **Authors**: Weiqi Wang,Zhiyi Tian,An Liu,Shui Yu
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,机器学习
- **Abstract**: With the increasing prevalence of Web-based platforms handling vast amounts of user data, machine unlearning has emerged as a crucial mechanism to uphold users' right to be forgotten, enabling individuals to request the removal of their specified data from trained models. However, the auditing of machine unlearning processes remains significantly underexplored. Although some existing methods offer unlearning auditing by leveraging backdoors, these backdoor-based approaches are inefficient and impractical, as they necessitate involvement in the initial model training process to embed the backdoors. In this paper, we propose a TAilored Posterior diffErence (TAPE) method to provide unlearning auditing independently of original model training. We observe that the process of machine unlearning inherently introduces changes in the model, which contains information related to the erased data. TAPE leverages unlearning model differences to assess how much information has been removed through the unlearning operation. Firstly, TAPE mimics the unlearned posterior differences by quickly building unlearned shadow models based on first-order influence estimation. Secondly, we train a Reconstructor model to extract and evaluate the private information of the unlearned posterior differences to audit unlearning. Existing privacy reconstructing methods based on posterior differences are only feasible for model updates of a single sample. To enable the reconstruction effective for multi-sample unlearning requests, we propose two strategies, unlearned data perturbation and unlearned influence-based division, to augment the posterior difference. Extensive experimental results indicate the significant superiority of TAPE over the state-of-the-art unlearning verification methods, at least 4.5$\times$ efficiency speedup and supporting the auditing for broader unlearning scenarios.

### Beyond the Tip of Efficiency: Uncovering the Submerged Threats of Jailbreak Attacks in Small Language Models 
[[arxiv](https://arxiv.org/abs/2502.19883)] [[cool](https://papers.cool/arxiv/2502.19883)] [[pdf](https://arxiv.org/pdf/2502.19883)]
> **Authors**: Sibo Yi,Tianshuo Cong,Xinlei He,Qi Li,Jiaxing Song
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 12 pages. 6 figures
- **标题**: None
- **领域**: 密码学和安全,人工智能,计算语言学
- **Abstract**: Small language models (SLMs) have become increasingly prominent in the deployment on edge devices due to their high efficiency and low computational cost. While researchers continue to advance the capabilities of SLMs through innovative training strategies and model compression techniques, the security risks of SLMs have received considerably less attention compared to large language models (LLMs).To fill this gap, we provide a comprehensive empirical study to evaluate the security performance of 13 state-of-the-art SLMs under various jailbreak attacks. Our experiments demonstrate that most SLMs are quite susceptible to existing jailbreak attacks, while some of them are even vulnerable to direct harmful prompts.To address the safety concerns, we evaluate several representative defense methods and demonstrate their effectiveness in enhancing the security of SLMs. We further analyze the potential security degradation caused by different SLM techniques including architecture compression, quantization, knowledge distillation, and so on. We expect that our research can highlight the security challenges of SLMs and provide valuable insights to future work in developing more robust and secure SLMs.

## 计算机视觉和模式识别(cs.CV:Computer Vision and Pattern Recognition)

### Open-Vocabulary Semantic Part Segmentation of 3D Human 
[[arxiv](https://arxiv.org/abs/2502.19782)] [[cool](https://papers.cool/arxiv/2502.19782)] [[pdf](https://arxiv.org/pdf/2502.19782)]
> **Authors**: Keito Suzuki,Bang Du,Girish Krishnan,Kunyao Chen,Runfa Blark Li,Truong Nguyen
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 3DV 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: 3D part segmentation is still an open problem in the field of 3D vision and AR/VR. Due to limited 3D labeled data, traditional supervised segmentation methods fall short in generalizing to unseen shapes and categories. Recently, the advancement in vision-language models' zero-shot abilities has brought a surge in open-world 3D segmentation methods. While these methods show promising results for 3D scenes or objects, they do not generalize well to 3D humans. In this paper, we present the first open-vocabulary segmentation method capable of handling 3D human. Our framework can segment the human category into desired fine-grained parts based on the textual prompt. We design a simple segmentation pipeline, leveraging SAM to generate multi-view proposals in 2D and proposing a novel HumanCLIP model to create unified embeddings for visual and textual inputs. Compared with existing pre-trained CLIP models, the HumanCLIP model yields more accurate embeddings for human-centric contents. We also design a simple-yet-effective MaskFusion module, which classifies and fuses multi-view features into 3D semantic masks without complex voting and grouping mechanisms. The design of decoupling mask proposals and text input also significantly boosts the efficiency of per-prompt inference. Experimental results on various 3D human datasets show that our method outperforms current state-of-the-art open-vocabulary 3D segmentation methods by a large margin. In addition, we show that our method can be directly applied to various 3D representations including meshes, point clouds, and 3D Gaussian Splatting.

### InPK: Infusing Prior Knowledge into Prompt for Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2502.19777)] [[cool](https://papers.cool/arxiv/2502.19777)] [[pdf](https://arxiv.org/pdf/2502.19777)]
> **Authors**: Shuchang Zhou
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Prompt tuning has become a popular strategy for adapting Vision-Language Models (VLMs) to zero/few-shot visual recognition tasks. Some prompting techniques introduce prior knowledge due to its richness, but when learnable tokens are randomly initialized and disconnected from prior knowledge, they tend to overfit on seen classes and struggle with domain shifts for unseen ones. To address this issue, we propose the InPK model, which infuses class-specific prior knowledge into the learnable tokens during initialization, thus enabling the model to explicitly focus on class-relevant information. Furthermore, to mitigate the weakening of class information by multi-layer encoders, we continuously reinforce the interaction between learnable tokens and prior knowledge across multiple feature levels. This progressive interaction allows the learnable tokens to better capture the fine-grained differences and universal visual concepts within prior knowledge, enabling the model to extract more discriminative and generalized text features. Even for unseen classes, the learned interaction allows the model to capture their common representations and infer their appropriate positions within the existing semantic structure. Moreover, we introduce a learnable text-to-vision projection layer to accommodate the text adjustments, ensuring better alignment of visual-text semantics. Extensive experiments on 11 recognition datasets show that InPK significantly outperforms state-of-the-art methods in multiple zero/few-shot image classification tasks.

### QORT-Former: Query-optimized Real-time Transformer for Understanding Two Hands Manipulating Objects 
[[arxiv](https://arxiv.org/abs/2502.19769)] [[cool](https://papers.cool/arxiv/2502.19769)] [[pdf](https://arxiv.org/pdf/2502.19769)]
> **Authors**: Elkhan Ismayilzada,MD Khalequzzaman Chowdhury Sayem,Yihalem Yimolal Tiruneh,Mubarrat Tajoar Chowdhury,Muhammadjon Boboev,Seungryul Baek
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Accepted to AAAI 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Significant advancements have been achieved in the realm of understanding poses and interactions of two hands manipulating an object. The emergence of augmented reality (AR) and virtual reality (VR) technologies has heightened the demand for real-time performance in these applications. However, current state-of-the-art models often exhibit promising results at the expense of substantial computational overhead. In this paper, we present a query-optimized real-time Transformer (QORT-Former), the first Transformer-based real-time framework for 3D pose estimation of two hands and an object. We first limit the number of queries and decoders to meet the efficiency requirement. Given limited number of queries and decoders, we propose to optimize queries which are taken as input to the Transformer decoder, to secure better accuracy: (1) we propose to divide queries into three types (a left hand query, a right hand query and an object query) and enhance query features (2) by using the contact information between hands and an object and (3) by using three-step update of enhanced image and query features with respect to one another. With proposed methods, we achieved real-time pose estimation performance using just 108 queries and 1 decoder (53.5 FPS on an RTX 3090TI GPU). Surpassing state-of-the-art results on the H2O dataset by 17.6% (left hand), 22.8% (right hand), and 27.2% (object), as well as on the FPHA dataset by 5.3% (right hand) and 10.4% (object), our method excels in accuracy. Additionally, it sets the state-of-the-art in interaction recognition, maintaining real-time efficiency with an off-the-shelf action recognition module.

### LIFT-GS: Cross-Scene Render-Supervised Distillation for 3D Language Grounding 
[[arxiv](https://arxiv.org/abs/2502.20389)] [[cool](https://papers.cool/arxiv/2502.20389)] [[pdf](https://arxiv.org/pdf/2502.20389)]
> **Authors**: Ang Cao,Sergio Arnaud,Oleksandr Maksymets,Jianing Yang,Ayush Jain,Sriram Yenamandra,Ada Martin,Vincent-Pierre Berges,Paul McVay,Ruslan Partsey,Aravind Rajeswaran,Franziska Meier,Justin Johnson,Jeong Joon Park,Alexander Sax
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Project page: https://liftgs.github.io
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Our approach to training 3D vision-language understanding models is to train a feedforward model that makes predictions in 3D, but never requires 3D labels and is supervised only in 2D, using 2D losses and differentiable rendering. The approach is new for vision-language understanding. By treating the reconstruction as a ``latent variable'', we can render the outputs without placing unnecessary constraints on the network architecture (e.g. can be used with decoder-only models). For training, only need images and camera pose, and 2D labels. We show that we can even remove the need for 2D labels by using pseudo-labels from pretrained 2D models. We demonstrate this to pretrain a network, and we finetune it for 3D vision-language understanding tasks. We show this approach outperforms baselines/sota for 3D vision-language grounding, and also outperforms other 3D pretraining techniques. Project page: https://liftgs.github.io.

### Beyond Next-Token: Next-X Prediction for Autoregressive Visual Generation 
[[arxiv](https://arxiv.org/abs/2502.20388)] [[cool](https://papers.cool/arxiv/2502.20388)] [[pdf](https://arxiv.org/pdf/2502.20388)]
> **Authors**: Sucheng Ren,Qihang Yu,Ju He,Xiaohui Shen,Alan Yuille,Liang-Chieh Chen
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Project page at \url{https://oliverrensu.github.io/project/xAR}
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Autoregressive (AR) modeling, known for its next-token prediction paradigm, underpins state-of-the-art language and visual generative models. Traditionally, a ``token'' is treated as the smallest prediction unit, often a discrete symbol in language or a quantized patch in vision. However, the optimal token definition for 2D image structures remains an open question. Moreover, AR models suffer from exposure bias, where teacher forcing during training leads to error accumulation at inference. In this paper, we propose xAR, a generalized AR framework that extends the notion of a token to an entity X, which can represent an individual patch token, a cell (a $k\times k$ grouping of neighboring patches), a subsample (a non-local grouping of distant patches), a scale (coarse-to-fine resolution), or even a whole image. Additionally, we reformulate discrete token classification as \textbf{continuous entity regression}, leveraging flow-matching methods at each AR step. This approach conditions training on noisy entities instead of ground truth tokens, leading to Noisy Context Learning, which effectively alleviates exposure bias. As a result, xAR offers two key advantages: (1) it enables flexible prediction units that capture different contextual granularity and spatial structures, and (2) it mitigates exposure bias by avoiding reliance on teacher forcing. On ImageNet-256 generation benchmark, our base model, xAR-B (172M), outperforms DiT-XL/SiT-XL (675M) while achieving 20$\times$ faster inference. Meanwhile, xAR-H sets a new state-of-the-art with an FID of 1.24, running 2.2$\times$ faster than the previous best-performing model without relying on vision foundation modules (\eg, DINOv2) or advanced guidance interval sampling.

### SecureGaze: Defending Gaze Estimation Against Backdoor Attacks 
[[arxiv](https://arxiv.org/abs/2502.20306)] [[cool](https://papers.cool/arxiv/2502.20306)] [[pdf](https://arxiv.org/pdf/2502.20306)]
> **Authors**: Lingyu Du,Yupei Liu,Jinyuan Jia,Guohao Lan
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Gaze estimation models are widely used in applications such as driver attention monitoring and human-computer interaction. While many methods for gaze estimation exist, they rely heavily on data-hungry deep learning to achieve high performance. This reliance often forces practitioners to harvest training data from unverified public datasets, outsource model training, or rely on pre-trained models. However, such practices expose gaze estimation models to backdoor attacks. In such attacks, adversaries inject backdoor triggers by poisoning the training data, creating a backdoor vulnerability: the model performs normally with benign inputs, but produces manipulated gaze directions when a specific trigger is present. This compromises the security of many gaze-based applications, such as causing the model to fail in tracking the driver's attention. To date, there is no defense that addresses backdoor attacks on gaze estimation models. In response, we introduce SecureGaze, the first solution designed to protect gaze estimation models from such attacks. Unlike classification models, defending gaze estimation poses unique challenges due to its continuous output space and globally activated backdoor behavior. By identifying distinctive characteristics of backdoored gaze estimation models, we develop a novel and effective approach to reverse-engineer the trigger function for reliable backdoor detection. Extensive evaluations in both digital and physical worlds demonstrate that SecureGaze effectively counters a range of backdoor attacks and outperforms seven state-of-the-art defenses adapted from classification models.

### M^3Builder: A Multi-Agent System for Automated Machine Learning in Medical Imaging 
[[arxiv](https://arxiv.org/abs/2502.20301)] [[cool](https://papers.cool/arxiv/2502.20301)] [[pdf](https://arxiv.org/pdf/2502.20301)]
> **Authors**: Jinghao Feng,Qiaoyu Zheng,Chaoyi Wu,Ziheng Zhao,Ya Zhang,Yanfeng Wang,Weidi Xie
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 38 pages, 7 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **Abstract**: Agentic AI systems have gained significant attention for their ability to autonomously perform complex tasks. However, their reliance on well-prepared tools limits their applicability in the medical domain, which requires to train specialized models. In this paper, we make three contributions: (i) We present M3Builder, a novel multi-agent system designed to automate machine learning (ML) in medical imaging. At its core, M3Builder employs four specialized agents that collaborate to tackle complex, multi-step medical ML workflows, from automated data processing and environment configuration to self-contained auto debugging and model training. These agents operate within a medical imaging ML workspace, a structured environment designed to provide agents with free-text descriptions of datasets, training codes, and interaction tools, enabling seamless communication and task execution. (ii) To evaluate progress in automated medical imaging ML, we propose M3Bench, a benchmark comprising four general tasks on 14 training datasets, across five anatomies and three imaging modalities, covering both 2D and 3D data. (iii) We experiment with seven state-of-the-art large language models serving as agent cores for our system, such as Claude series, GPT-4o, and DeepSeek-V3. Compared to existing ML agentic designs, M3Builder shows superior performance on completing ML tasks in medical imaging, achieving a 94.29% success rate using Claude-3.7-Sonnet as the agent core, showing huge potential towards fully automated machine learning in medical imaging.

### Visual Adaptive Prompting for Compositional Zero-Shot Learning 
[[arxiv](https://arxiv.org/abs/2502.20292)] [[cool](https://papers.cool/arxiv/2502.20292)] [[pdf](https://arxiv.org/pdf/2502.20292)]
> **Authors**: Kyle Stein,Arash Mahyari,Guillermo Francia,Eman El-Sheikh
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Vision-Language Models (VLMs) have demonstrated impressive capabilities in learning joint representations of visual and textual data, making them powerful tools for tasks such as Compositional Zero-Shot Learning (CZSL). CZSL requires models to generalize to novel combinations of visual primitives-such as attributes and objects-that were not explicitly encountered during training. Recent works in prompting for CZSL have focused on modifying inputs for the text encoder, often using static prompts that do not change across varying visual contexts. However, these approaches struggle to fully capture varying visual contexts, as they focus on text adaptation rather than leveraging visual features for compositional reasoning. To address this, we propose Visual Adaptive Prompting System (VAPS) that leverages a learnable visual prompt repository and similarity-based retrieval mechanism within the framework of VLMs to bridge the gap between semantic and visual features. Our method introduces a dynamic visual prompt repository mechanism that selects the most relevant attribute and object prompts based on the visual features of the image. Our proposed system includes a visual prompt adapter that encourages the model to learn a more generalizable embedding space. Experiments on three CZSL benchmarks, across both closed and open-world scenarios, demonstrate state-of-the-art results.

### ChatReID: Open-ended Interactive Person Retrieval via Hierarchical Progressive Tuning for Vision Language Models 
[[arxiv](https://arxiv.org/abs/2502.19958)] [[cool](https://papers.cool/arxiv/2502.19958)] [[pdf](https://arxiv.org/pdf/2502.19958)]
> **Authors**: Ke Niu,Haiyang Yu,Mengyang Zhao,Teng Fu,Siyang Yi,Wei Lu,Bin Li,Xuelin Qian,Xiangyang Xue
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Person re-identification (Re-ID) is a critical task in human-centric intelligent systems, enabling consistent identification of individuals across different camera views using multi-modal query information. Recent studies have successfully integrated LVLMs with person Re-ID, yielding promising results. However, existing LVLM-based methods face several limitations. They rely on extracting textual embeddings from fixed templates, which are used either as intermediate features for image representation or for prompt tuning in domain-specific tasks. Furthermore, they are unable to adopt the VQA inference format, significantly restricting their broader applicability. In this paper, we propose a novel, versatile, one-for-all person Re-ID framework, ChatReID. Our approach introduces a Hierarchical Progressive Tuning (HPT) strategy, which ensures fine-grained identity-level retrieval by progressively refining the model's ability to distinguish pedestrian identities. Extensive experiments demonstrate that our approach outperforms SOTA methods across ten benchmarks in four different Re-ID settings, offering enhanced flexibility and user-friendliness. ChatReID provides a scalable, practical solution for real-world person Re-ID applications, enabling effective multi-modal interaction and fine-grained identity discrimination.

### Space Rotation with Basis Transformation for Training-free Test-Time Adaptation 
[[arxiv](https://arxiv.org/abs/2502.19946)] [[cool](https://papers.cool/arxiv/2502.19946)] [[pdf](https://arxiv.org/pdf/2502.19946)]
> **Authors**: Chenhao Ding,Xinyuan Gao,Songlin Dong,Yuhang He,Qiang Wang,Xiang Song,Alex Kot,Yihong Gong
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: With the development of visual-language models (VLM) in downstream task applications, test-time adaptation methods based on VLM have attracted increasing attention for their ability to address changes distribution in test-time. Although prior approaches have achieved some progress, they typically either demand substantial computational resources or are constrained by the limitations of the original feature space, rendering them less effective for test-time adaptation tasks. To address these challenges, we propose a training-free feature space rotation with basis transformation for test-time adaptation. By leveraging the inherent distinctions among classes, we reconstruct the original feature space and map it to a new representation, thereby enhancing the clarity of class differences and providing more effective guidance for the model during testing. Additionally, to better capture relevant information from various classes, we maintain a dynamic queue to store representative samples. Experimental results across multiple benchmarks demonstrate that our method outperforms state-of-the-art techniques in terms of both performance and efficiency.

### C-Drag: Chain-of-Thought Driven Motion Controller for Video Generation 
[[arxiv](https://arxiv.org/abs/2502.19868)] [[cool](https://papers.cool/arxiv/2502.19868)] [[pdf](https://arxiv.org/pdf/2502.19868)]
> **Authors**: Yuhao Li,Mirana Claire Angel,Salman Khan,Yu Zhu,Jinqiu Sun,Yanning Zhang,Fahad Shahbaz Khan
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Trajectory-based motion control has emerged as an intuitive and efficient approach for controllable video generation. However, the existing trajectory-based approaches are usually limited to only generating the motion trajectory of the controlled object and ignoring the dynamic interactions between the controlled object and its surroundings. To address this limitation, we propose a Chain-of-Thought-based motion controller for controllable video generation, named C-Drag. Instead of directly generating the motion of some objects, our C-Drag first performs object perception and then reasons the dynamic interactions between different objects according to the given motion control of the objects. Specifically, our method includes an object perception module and a Chain-of-Thought-based motion reasoning module. The object perception module employs visual language models to capture the position and category information of various objects within the image. The Chain-of-Thought-based motion reasoning module takes this information as input and conducts a stage-wise reasoning process to generate motion trajectories for each of the affected objects, which are subsequently fed to the diffusion model for video synthesis. Furthermore, we introduce a new video object interaction (VOI) dataset to evaluate the generation quality of motion controlled video generation methods. Our VOI dataset contains three typical types of interactions and provides the motion trajectories of objects that can be used for accurate performance evaluation. Experimental results show that C-Drag achieves promising performance across multiple metrics, excelling in object motion control. Our benchmark, codes, and models will be available at https://github.com/WesLee88524/C-Drag-Official-Repo.

### Striving for Faster and Better: A One-Layer Architecture with Auto Re-parameterization for Low-Light Image Enhancement 
[[arxiv](https://arxiv.org/abs/2502.19867)] [[cool](https://papers.cool/arxiv/2502.19867)] [[pdf](https://arxiv.org/pdf/2502.19867)]
> **Authors**: Nan An,Long Ma,Guangchao Han,Xin Fan,RIsheng Liu
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Deep learning-based low-light image enhancers have made significant progress in recent years, with a trend towards achieving satisfactory visual quality while gradually reducing the number of parameters and improving computational efficiency. In this work, we aim to delving into the limits of image enhancers both from visual quality and computational efficiency, while striving for both better performance and faster processing. To be concrete, by rethinking the task demands, we build an explicit connection, i.e., visual quality and computational efficiency are corresponding to model learning and structure design, respectively. Around this connection, we enlarge parameter space by introducing the re-parameterization for ample model learning of a pre-defined minimalist network (e.g., just one layer), to avoid falling into a local solution. To strengthen the structural representation, we define a hierarchical search scheme for discovering a task-oriented re-parameterized structure, which also provides powerful support for efficiency. Ultimately, this achieves efficient low-light image enhancement using only a single convolutional layer, while maintaining excellent visual quality. Experimental results show our sensible superiority both in quality and efficiency against recently-proposed methods. Especially, our running time on various platforms (e.g., CPU, GPU, NPU, DSP) consistently moves beyond the existing fastest scheme. The source code will be released at https://github.com/vis-opt-group/AR-LLIE.

### LMHLD: A Large-scale Multi-source High-resolution Landslide Dataset for Landslide Detection based on Deep Learning 
[[arxiv](https://arxiv.org/abs/2502.19866)] [[cool](https://papers.cool/arxiv/2502.19866)] [[pdf](https://arxiv.org/pdf/2502.19866)]
> **Authors**: Guanting Liu,Yi Wang,Xi Chen,Baoyu Du,Penglei Li,Yuan Wu,Zhice Fang
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,图像和视频处理
- **Abstract**: Landslides are among the most common natural disasters globally, posing significant threats to human society. Deep learning (DL) has proven to be an effective method for rapidly generating landslide inventories in large-scale disaster areas. However, DL models rely heavily on high-quality labeled landslide data for strong feature extraction capabilities. And landslide detection using DL urgently needs a benchmark dataset to evaluate the generalization ability of the latest models. To solve the above problems, we construct a Large-scale Multi-source High-resolution Landslide Dataset (LMHLD) for Landslide Detection based on DL. LMHLD collects remote sensing images from five different satellite sensors across seven study areas worldwide: Wenchuan, China (2008); Rio de Janeiro, Brazil (2011); Gorkha, Nepal (2015); Jiuzhaigou, China (2015); Taiwan, China (2018); Hokkaido, Japan (2018); Emilia-Romagna, Italy (2023). The dataset includes a total of 25,365 patches, with different patch sizes to accommodate different landslide scales. Additionally, a training module, LMHLDpart, is designed to accommodate landslide detection tasks at varying scales and to alleviate the issue of catastrophic forgetting in multi-task learning. Furthermore, the models trained by LMHLD is applied in other datasets to highlight the robustness of LMHLD. Five dataset quality evaluation experiments designed by using seven DL models from the U-Net family demonstrate that LMHLD has the potential to become a benchmark dataset for landslide detection. LMHLD is open access and can be accessed through the link: https://doi.org/10.5281/zenodo.11424988. This dataset provides a strong foundation for DL models, accelerates the development of DL in landslide detection, and serves as a valuable resource for landslide prevention and mitigation efforts.

### ProAPO: Progressively Automatic Prompt Optimization for Visual Classification 
[[arxiv](https://arxiv.org/abs/2502.19844)] [[cool](https://papers.cool/arxiv/2502.19844)] [[pdf](https://arxiv.org/pdf/2502.19844)]
> **Authors**: Xiangyan Qu,Gaopeng Gou,Jiamin Zhuang,Jing Yu,Kun Song,Qihao Wang,Yili Li,Gang Xiong
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Accepted to the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Vision-language models (VLMs) have made significant progress in image classification by training with large-scale paired image-text data. Their performances largely depend on the prompt quality. While recent methods show that visual descriptions generated by large language models (LLMs) enhance the generalization of VLMs, class-specific prompts may be inaccurate or lack discrimination due to the hallucination in LLMs. In this paper, we aim to find visually discriminative prompts for fine-grained categories with minimal supervision and no human-in-the-loop. An evolution-based algorithm is proposed to progressively optimize language prompts from task-specific templates to class-specific descriptions. Unlike optimizing templates, the search space shows an explosion in class-specific candidate prompts. This increases prompt generation costs, iterative times, and the overfitting problem. To this end, we first introduce several simple yet effective edit-based and evolution-based operations to generate diverse candidate prompts by one-time query of LLMs. Then, two sampling strategies are proposed to find a better initial search point and reduce traversed categories, saving iteration costs. Moreover, we apply a novel fitness score with entropy constraints to mitigate overfitting. In a challenging one-shot image classification setting, our method outperforms existing textual prompt-based methods and improves LLM-generated description methods across 13 datasets. Meanwhile, we demonstrate that our optimal prompts improve adapter-based methods and transfer effectively across different backbones.

### CLIP Under the Microscope: A Fine-Grained Analysis of Multi-Object Representation 
[[arxiv](https://arxiv.org/abs/2502.19842)] [[cool](https://papers.cool/arxiv/2502.19842)] [[pdf](https://arxiv.org/pdf/2502.19842)]
> **Authors**: Reza Abbasi,Ali Nazari,Aminreza Sefid,Mohammadali Banayeeanzade,Mohammad Hossein Rohban,Mahdieh Soleymani Baghshah
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Accepted at CVPR 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Contrastive Language-Image Pre-training (CLIP) models excel in zero-shot classification, yet face challenges in complex multi-object scenarios. This study offers a comprehensive analysis of CLIP's limitations in these contexts using a specialized dataset, ComCO, designed to evaluate CLIP's encoders in diverse multi-object scenarios. Our findings reveal significant biases: the text encoder prioritizes first-mentioned objects, and the image encoder favors larger objects. Through retrieval and classification tasks, we quantify these biases across multiple CLIP variants and trace their origins to CLIP's training process, supported by analyses of the LAION dataset and training progression. Our image-text matching experiments show substantial performance drops when object size or token order changes, underscoring CLIP's instability with rephrased but semantically similar captions. Extending this to longer captions and text-to-image models like Stable Diffusion, we demonstrate how prompt order influences object prominence in generated images. For more details and access to our dataset and analysis code, visit our project repository: https://clip-analysis.github.io.

### Analyzing CLIP's Performance Limitations in Multi-Object Scenarios: A Controlled High-Resolution Study 
[[arxiv](https://arxiv.org/abs/2502.19828)] [[cool](https://papers.cool/arxiv/2502.19828)] [[pdf](https://arxiv.org/pdf/2502.19828)]
> **Authors**: Reza Abbasi,Ali Nazari,Aminreza Sefid,Mohammadali Banayeeanzade,Mohammad Hossein Rohban,Mahdieh Soleymani Baghshah
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Accepted at ECCV 2024 Workshop EVAL-FoMo
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Contrastive Language-Image Pre-training (CLIP) models have demonstrated remarkable performance in zero-shot classification tasks, yet their efficacy in handling complex multi-object scenarios remains challenging. This study presents a comprehensive analysis of CLIP's performance limitations in multi-object contexts through controlled experiments. We introduce two custom datasets, SimCO and CompCO, to evaluate CLIP's image and text encoders in various multi-object configurations. Our findings reveal significant biases in both encoders: the image encoder favors larger objects, while the text encoder prioritizes objects mentioned first in descriptions. We hypothesize these biases originate from CLIP's training process and provide evidence through analyses of the COCO dataset and CLIP's training progression. Additionally, we extend our investigation to Stable Diffusion models, revealing that biases in the CLIP text encoder significantly impact text-to-image generation tasks. Our experiments demonstrate how these biases affect CLIP's performance in image-caption matching and generation tasks, particularly when manipulating object sizes and their order in captions. This work contributes valuable insights into CLIP's behavior in complex visual environments and highlights areas for improvement in future vision-language models.

## 计算机与社会(cs.CY:Computers and Society)

### The erasure of intensive livestock farming in text-to-image generative AI 
[[arxiv](https://arxiv.org/abs/2502.19771)] [[cool](https://papers.cool/arxiv/2502.19771)] [[pdf](https://arxiv.org/pdf/2502.19771)]
> **Authors**: Kehan Sheng,Frank A. M. Tuyttens,Marina A. G. von Keyserlingk
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: Generative AI (e.g., ChatGPT) is increasingly integrated into people's daily lives. While it is known that AI perpetuates biases against marginalized human groups, their impact on non-human animals remains understudied. We found that ChatGPT's text-to-image model (DALL-E 3) introduces a strong bias toward romanticizing livestock farming as dairy cows on pasture and pigs rooting in mud. This bias remained when we requested realistic depictions and was only mitigated when the automatic prompt revision was inhibited. Most farmed animal in industrialized countries are reared indoors with limited space per animal, which fail to resonate with societal values. Inhibiting prompt revision resulted in images that more closely reflected modern farming practices; for example, cows housed indoors accessing feed through metal headlocks, and pigs behind metal railings on concrete floors in indoor facilities. While OpenAI introduced prompt revision to mitigate bias, in the case of farmed animal production systems, it paradoxically introduces a strong bias towards unrealistic farming practices.

## 分布式、并行和集群计算(cs.DC:Distributed, Parallel, and Cluster Computing)

### Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts 
[[arxiv](https://arxiv.org/abs/2502.19811)] [[cool](https://papers.cool/arxiv/2502.19811)] [[pdf](https://arxiv.org/pdf/2502.19811)]
> **Authors**: Shulai Zhang,Ningxin Zheng,Haibin Lin,Ziheng Jiang,Wenlei Bao,Chengquan Jiang,Qi Hou,Weihao Cui,Size Zheng,Li-Wen Chang,Quan Chen,Xin Liu
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 分布式、并行和集群计算,人工智能,机器学习
- **Abstract**: Mixture-of-experts (MoE) has been extensively employed to scale large language models to trillion-plus parameters while maintaining a fixed computational cost. The development of large MoE models in the distributed scenario encounters the problem of large communication overhead. The inter-device communication of a MoE layer can occupy 47% time of the entire model execution with popular models and frameworks. Therefore, existing methods suggest the communication in a MoE layer to be pipelined with the computation for overlapping. However, these coarse grained overlapping schemes introduce a notable impairment of computational efficiency and the latency concealing is sub-optimal. To this end, we present COMET, an optimized MoE system with fine-grained communication-computation overlapping. Leveraging data dependency analysis and task rescheduling, COMET achieves precise fine-grained overlapping of communication and computation. Through adaptive workload assignment, COMET effectively eliminates fine-grained communication bottlenecks and enhances its adaptability across various scenarios. Our evaluation shows that COMET accelerates the execution of a single MoE layer by $1.96\times$ and for end-to-end execution, COMET delivers a $1.71\times$ speedup on average. COMET has been adopted in the production environment of clusters with ten-thousand-scale of GPUs, achieving savings of millions of GPU hours.

### Improving the Efficiency of a Deep Reinforcement Learning-Based Power Management System for HPC Clusters Using Curriculum Learning 
[[arxiv](https://arxiv.org/abs/2502.20348)] [[cool](https://papers.cool/arxiv/2502.20348)] [[pdf](https://arxiv.org/pdf/2502.20348)]
> **Authors**: Thomas Budiarjo,Santana Yuda Pradata,Kadek Gemilang Santiyuda,Muhammad Alfian Amrizal,Reza Pulungan,Hiroyuki Takizawa
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 13 pages, 17 figures, accepted at Supercomputing Asia '25, published by ACM
- **标题**: None
- **领域**: 分布式、并行和集群计算,机器学习
- **Abstract**: High energy consumption remains a key challenge in high-performance computing (HPC) systems, which often feature hundreds or thousands of nodes drawing substantial power even in idle or standby modes. Although powering down unused nodes can improve energy efficiency, choosing the wrong time to do so can degrade quality of service by delaying job execution. Machine learning, in particular reinforcement learning (RL), has shown promise in determining optimal times to switch nodes on or off. In this study, we enhance the performance of a deep reinforcement learning (DRL) agent for HPC power management by integrating curriculum learning (CL), a training approach that introduces tasks with gradually increasing difficulty. Using the Batsim-py simulation framework, we compare the proposed CL-based agent to both a baseline DRL method (without CL) and the conventional fixed-time timeout strategy. Experimental results confirm that an easy-to-hard curriculum outperforms other training orders in terms of reducing wasted energy usage. The best agent achieves a 3.73% energy reduction over the baseline DRL method and a 4.66% improvement compared to the best timeout configuration (shutdown every 15 minutes of idle time). In addition, it reduces average job waiting time by 9.24% and maintains a higher job-filling rate, indicating more effective resource utilization. Sensitivity tests across various switch-on durations, power levels, and cluster sizes further reveal the agent's adaptability to changing system parameters without retraining. These findings demonstrate that curriculum learning can significantly improve DRL-based power management in HPC, balancing energy savings, quality of service, and robustness to diverse configurations.

## 数据结构和算法(cs.DS:Data Structures and Algorithms)

### Beyond Worst-Case Dimensionality Reduction for Sparse Vectors 
[[arxiv](https://arxiv.org/abs/2502.19865)] [[cool](https://papers.cool/arxiv/2502.19865)] [[pdf](https://arxiv.org/pdf/2502.19865)]
> **Authors**: Sandeep Silwal,David P. Woodruff,Qiuyi Zhang
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: To appear in ICLR 2025
- **标题**: None
- **领域**: 数据结构和算法,机器学习
- **Abstract**: We study beyond worst-case dimensionality reduction for $s$-sparse vectors. Our work is divided into two parts, each focusing on a different facet of beyond worst-case analysis: We first consider average-case guarantees. A folklore upper bound based on the birthday-paradox states: For any collection $X$ of $s$-sparse vectors in $\mathbb{R}^d$, there exists a linear map to $\mathbb{R}^{O(s^2)}$ which \emph{exactly} preserves the norm of $99\%$ of the vectors in $X$ in any $\ell_p$ norm (as opposed to the usual setting where guarantees hold for all vectors). We give lower bounds showing that this is indeed optimal in many settings: any oblivious linear map satisfying similar average-case guarantees must map to $Ω(s^2)$ dimensions. The same lower bound also holds for a wide class of smooth maps, including `encoder-decoder schemes', where we compare the norm of the original vector to that of a smooth function of the embedding. These lower bounds reveal a separation result, as an upper bound of $O(s \log(d))$ is possible if we instead use arbitrary (possibly non-smooth) functions, e.g., via compressed sensing algorithms. Given these lower bounds, we specialize to sparse \emph{non-negative} vectors. For a dataset $X$ of non-negative $s$-sparse vectors and any $p \ge 1$, we can non-linearly embed $X$ to $O(s\log(|X|s)/ε^2)$ dimensions while preserving all pairwise distances in $\ell_p$ norm up to $1\pm ε$, with no dependence on $p$. Surprisingly, the non-negativity assumption enables much smaller embeddings than arbitrary sparse vectors, where the best known bounds suffer exponential dependence. Our map also guarantees \emph{exact} dimensionality reduction for $\ell_{\infty}$ by embedding into $O(s\log |X|)$ dimensions, which is tight. We show that both the non-linearity of $f$ and the non-negativity of $X$ are necessary, and provide downstream algorithmic improvements.

## 图形(cs.GR:Graphics)

### Tight Inversion: Image-Conditioned Inversion for Real Image Editing 
[[arxiv](https://arxiv.org/abs/2502.20376)] [[cool](https://papers.cool/arxiv/2502.20376)] [[pdf](https://arxiv.org/pdf/2502.20376)]
> **Authors**: Edo Kadosh,Nir Goren,Or Patashnik,Daniel Garibi,Daniel Cohen-Or
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Project page at: https://tight-inversion.github.io
- **标题**: None
- **领域**: 图形,计算机视觉和模式识别,机器学习
- **Abstract**: Text-to-image diffusion models offer powerful image editing capabilities. To edit real images, many methods rely on the inversion of the image into Gaussian noise. A common approach to invert an image is to gradually add noise to the image, where the noise is determined by reversing the sampling equation. This process has an inherent tradeoff between reconstruction and editability, limiting the editing of challenging images such as highly-detailed ones. Recognizing the reliance of text-to-image models inversion on a text condition, this work explores the importance of the condition choice. We show that a condition that precisely aligns with the input image significantly improves the inversion quality. Based on our findings, we introduce Tight Inversion, an inversion method that utilizes the most possible precise condition -- the input image itself. This tight condition narrows the distribution of the model's output and enhances both reconstruction and editability. We demonstrate the effectiveness of our approach when combined with existing inversion methods through extensive experiments, evaluating the reconstruction accuracy as well as the integration with various editing methods.

## 机器学习(cs.LG:Machine Learning)

### GraphSparseNet: a Novel Method for Large Scale Trafffic Flow Prediction 
[[arxiv](https://arxiv.org/abs/2502.19823)] [[cool](https://papers.cool/arxiv/2502.19823)] [[pdf](https://arxiv.org/pdf/2502.19823)]
> **Authors**: Weiyang Kong,Kaiqi Wu,Sen Zhang,Yubao Liu
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Traffic flow forecasting is a critical spatio-temporal data mining task with wide-ranging applications in intelligent route planning and dynamic traffic management. Recent advancements in deep learning, particularly through Graph Neural Networks (GNNs), have significantly enhanced the accuracy of these forecasts by capturing complex spatio-temporal dynamics. However, the scalability of GNNs remains a challenge due to their exponential growth in model complexity with increasing nodes in the graph. Existing methods to address this issue, including sparsification, decomposition, and kernel-based approaches, either do not fully resolve the complexity issue or risk compromising predictive accuracy. This paper introduces GraphSparseNet (GSNet), a novel framework designed to improve both the scalability and accuracy of GNN-based traffic forecasting models. GraphSparseNet is comprised of two core modules: the Feature Extractor and the Relational Compressor. These modules operate with linear time and space complexity, thereby reducing the overall computational complexity of the model to a linear scale. Our extensive experiments on multiple real-world datasets demonstrate that GraphSparseNet not only significantly reduces training time by 3.51x compared to state-of-the-art linear models but also maintains high predictive performance.

### Advancing GDP Forecasting: The Potential of Machine Learning Techniques in Economic Predictions 
[[arxiv](https://arxiv.org/abs/2502.19807)] [[cool](https://papers.cool/arxiv/2502.19807)] [[pdf](https://arxiv.org/pdf/2502.19807)]
> **Authors**: Bogdan Oancea
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: ef:Knowledge on Economics and Management Conference Proceedings, 2024, Olomouc, The Czech Republic
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The quest for accurate economic forecasting has traditionally been dominated by econometric models, which most of the times rely on the assumptions of linear relationships and stationarity in of the data. However, the complex and often nonlinear nature of global economies necessitates the exploration of alternative approaches. Machine learning methods offer promising advantages over traditional econometric techniques for Gross Domestic Product forecasting, given their ability to model complex, nonlinear interactions and patterns without the need for explicit specification of the underlying relationships. This paper investigates the efficacy of Recurrent Neural Networks, in forecasting GDP, specifically LSTM networks. These models are compared against a traditional econometric method, SARIMA. We employ the quarterly Romanian GDP dataset from 1995 to 2023 and build a LSTM network to forecast to next 4 values in the series. Our findings suggest that machine learning models, consistently outperform traditional econometric models in terms of predictive accuracy and flexibility

### Implicit Search via Discrete Diffusion: A Study on Chess 
[[arxiv](https://arxiv.org/abs/2502.19805)] [[cool](https://papers.cool/arxiv/2502.19805)] [[pdf](https://arxiv.org/pdf/2502.19805)]
> **Authors**: Jiacheng Ye,Zhenyu Wu,Jiahui Gao,Zhiyong Wu,Xin Jiang,Zhenguo Li,Lingpeng Kong
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: ICLR 2025
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: In the post-AlphaGo era, there has been a renewed interest in search techniques such as Monte Carlo Tree Search (MCTS), particularly in their application to Large Language Models (LLMs). This renewed attention is driven by the recognition that current next-token prediction models often lack the ability for long-term planning. Is it possible to instill search-like abilities within the models to enhance their planning abilities without relying on explicit search? We propose DiffuSearch , a model that does \textit{implicit search} by looking into the future world via discrete diffusion modeling. We instantiate DiffuSearch on a classical board game, Chess, where explicit search is known to be essential. Through extensive controlled experiments, we show DiffuSearch outperforms both the searchless and explicit search-enhanced policies. Specifically, DiffuSearch outperforms the one-step policy by 19.2% and the MCTS-enhanced policy by 14% on action accuracy. Furthermore, DiffuSearch demonstrates a notable 30% enhancement in puzzle-solving abilities compared to explicit search-based policies, along with a significant 540 Elo increase in game-playing strength assessment. These results indicate that implicit search via discrete diffusion is a viable alternative to explicit search over a one-step policy. All codes are publicly available at \href{https://github.com/HKUNLP/DiffuSearch}{https://github.com/HKUNLP/DiffuSearch}.

### ServoLNN: Lagrangian Neural Networks Driven by Servomechanisms 
[[arxiv](https://arxiv.org/abs/2502.19802)] [[cool](https://papers.cool/arxiv/2502.19802)] [[pdf](https://arxiv.org/pdf/2502.19802)]
> **Authors**: Brandon Johns,Zhuomin Zhou,Elahe Abdi
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 22 pages, 8 figures
- **标题**: None
- **领域**: 机器学习,机器人技术,动力系统
- **Abstract**: Combining deep learning with classical physics facilitates the efficient creation of accurate dynamical models. In a recent class of neural network, Lagrangian mechanics is hard-coded into the architecture, and training the network learns the given system. However, the current architectures do not facilitate the modelling of dynamical systems that are driven by servomechanisms (e.g. servomotors, stepper motors, current sources, volumetric pumps). This article presents ServoLNN, a new architecture to model dynamical systems that are driven by servomechanisms. ServoLNN is compatible for use in real-time applications, where the driving motion is known only just-in-time. A PyTorch implementation of ServoLNN is provided. The derivations and results reveal the occurrence of a possible family of solutions that the training may converge on. The effect of the family of solutions on the predicted physical quantities is explored, as is the resolution to reduce the family of solutions to a single solution. Resultantly, the architecture can simultaneously accurately find the energies, power, rate of work, mass matrix, generalised accelerations, generalised forces, and the generalised forces that drive the servomechanisms.

### Mixtera: A Data Plane for Foundation Model Training 
[[arxiv](https://arxiv.org/abs/2502.19790)] [[cool](https://papers.cool/arxiv/2502.19790)] [[pdf](https://arxiv.org/pdf/2502.19790)]
> **Authors**: Maximilian Böther,Xiaozhe Yao,Tolga Kerimoglu,Ana Klimovic
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: under submission
- **标题**: None
- **领域**: 机器学习,人工智能,数据库
- **Abstract**: State-of-the-art large language and vision models are trained over trillions of tokens that are aggregated from a large variety of sources. As training data collections grow, manually managing the samples becomes time-consuming, tedious, and prone to errors. Yet recent research shows that the data mixture and the order in which samples are visited during training can significantly influence model accuracy. We build and present Mixtera, a data plane for foundation model training that enables users to declaratively express which data samples should be used in which proportion and in which order during training. Mixtera is a centralized, read-only layer that is deployed on top of existing training data collections and can be declaratively queried. It operates independently of the filesystem structure and supports mixtures across arbitrary properties (e.g., language, source dataset) as well as dynamic adjustment of the mixture based on model feedback. We experimentally evaluate Mixtera and show that our implementation does not bottleneck training and scales to 256 GH200 superchips. We demonstrate how Mixtera supports recent advancements in mixing strategies by implementing the proposed Adaptive Data Optimization (ADO) algorithm in the system and evaluating its performance impact. We also explore the role of mixtures for vision-language models.

### In-Context Learning with Hypothesis-Class Guidance 
[[arxiv](https://arxiv.org/abs/2502.19787)] [[cool](https://papers.cool/arxiv/2502.19787)] [[pdf](https://arxiv.org/pdf/2502.19787)]
> **Authors**: Ziqian Lin,Shubham Kumar Bharti,Kangwook Lee
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 19 pages, 18 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Recent research has investigated the underlying mechanisms of in-context learning (ICL) both theoretically and empirically, often using data generated from simple function classes. However, the existing work often focuses on the sequence consisting solely of labeled examples, while in practice, labeled examples are typically accompanied by an instruction, providing some side information about the task. In this work, we propose ICL with hypothesis-class guidance (ICL-HCG), a novel synthetic data model for ICL where the input context consists of the literal description of a (finite) hypothesis class $\mathcal{H}$ and $(x,y)$ pairs from a hypothesis chosen from $\mathcal{H}$. Under our framework ICL-HCG, we conduct extensive experiments to explore: (i) a variety of generalization abilities to new hypothesis classes; (ii) different model architectures; (iii) sample complexity; (iv) in-context data imbalance; (v) the role of instruction; and (vi) the effect of pretraining hypothesis diversity. As a result, we show that (a) Transformers can successfully learn ICL-HCG and generalize to unseen hypotheses and unseen hypothesis classes, and (b) compared with ICL without instruction, ICL-HCG achieves significantly higher accuracy, demonstrating the role of instructions.

### Obtaining Example-Based Explanations from Deep Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.19768)] [[cool](https://papers.cool/arxiv/2502.19768)] [[pdf](https://arxiv.org/pdf/2502.19768)]
> **Authors**: Genghua Dong,Henrik Boström,Michalis Vazirgiannis,Roman Bresson
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: To be published in the Symposium on Intelligent Data Analysis (IDA) 2025
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Most techniques for explainable machine learning focus on feature attribution, i.e., values are assigned to the features such that their sum equals the prediction. Example attribution is another form of explanation that assigns weights to the training examples, such that their scalar product with the labels equals the prediction. The latter may provide valuable complementary information to feature attribution, in particular in cases where the features are not easily interpretable. Current example-based explanation techniques have targeted a few model types only, such as k-nearest neighbors and random forests. In this work, a technique for obtaining example-based explanations from deep neural networks (EBE-DNN) is proposed. The basic idea is to use the deep neural network to obtain an embedding, which is employed by a k-nearest neighbor classifier to form a prediction; the example attribution can hence straightforwardly be derived from the latter. Results from an empirical investigation show that EBE-DNN can provide highly concentrated example attributions, i.e., the predictions can be explained with few training examples, without reducing accuracy compared to the original deep neural network. Another important finding from the empirical investigation is that the choice of layer to use for the embeddings may have a large impact on the resulting accuracy.

### R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts 
[[arxiv](https://arxiv.org/abs/2502.20395)] [[cool](https://papers.cool/arxiv/2502.20395)] [[pdf](https://arxiv.org/pdf/2502.20395)]
> **Authors**: Zhongyang Li,Ziyue Li,Tianyi Zhou
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: In large multimodal models (LMMs), the perception of non-language modalities (e.g., visual representations) is usually not on par with the large language models (LLMs)' powerful reasoning capabilities, deterring LMMs' performance on challenging downstream tasks. This weakness has been recently mitigated by replacing the vision encoder with a mixture-of-experts (MoE), which provides rich, multi-granularity, and diverse representations required by diverse downstream tasks. The performance of multimodal MoE largely depends on its router, which reweights and mixes the representations of different experts for each input. However, we find that the end-to-end trained router does not always produce the optimal routing weights for every test sample. To bridge the gap, we propose a novel and efficient method "Re-Routing in Test-Time(R2-T2) that locally optimizes the vector of routing weights in test-time by moving it toward those vectors of the correctly predicted samples in a neighborhood of the test sample. We propose three R2-T2 strategies with different optimization objectives and neighbor-search spaces. R2-T2 consistently and greatly improves state-of-the-art LMMs' performance on challenging benchmarks of diverse tasks, without training any base-model parameters.

### Walking the Web of Concept-Class Relationships in Incrementally Trained Interpretable Models 
[[arxiv](https://arxiv.org/abs/2502.20393)] [[cool](https://papers.cool/arxiv/2502.20393)] [[pdf](https://arxiv.org/pdf/2502.20393)]
> **Authors**: Susmit Agrawal,Deepika Vemuri,Sri Siddarth Chakaravarthy P,Vineeth N. Balasubramanian
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 8 pages of main text, 6 figures in main text, 11 pages of Appendix, published in AAAI 2025
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: Concept-based methods have emerged as a promising direction to develop interpretable neural networks in standard supervised settings. However, most works that study them in incremental settings assume either a static concept set across all experiences or assume that each experience relies on a distinct set of concepts. In this work, we study concept-based models in a more realistic, dynamic setting where new classes may rely on older concepts in addition to introducing new concepts themselves. We show that concepts and classes form a complex web of relationships, which is susceptible to degradation and needs to be preserved and augmented across experiences. We introduce new metrics to show that existing concept-based models cannot preserve these relationships even when trained using methods to prevent catastrophic forgetting, since they cannot handle forgetting at concept, class, and concept-class relationship levels simultaneously. To address these issues, we propose a novel method - MuCIL - that uses multimodal concepts to perform classification without increasing the number of trainable parameters across experiences. The multimodal concepts are aligned to concepts provided in natural language, making them interpretable by design. Through extensive experimentation, we show that our approach obtains state-of-the-art classification performance compared to other concept-based models, achieving over 2$\times$ the classification performance in some cases. We also study the ability of our model to perform interventions on concepts, and show that it can localize visual concepts in input images, providing post-hoc interpretations.

### Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security Analysis 
[[arxiv](https://arxiv.org/abs/2502.20383)] [[cool](https://papers.cool/arxiv/2502.20383)] [[pdf](https://arxiv.org/pdf/2502.20383)]
> **Authors**: Jeffrey Yang Fan Chiang,Seungjae Lee,Jia-Bin Huang,Furong Huang,Yizheng Chen
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Project website: http://vulnerable-ai-agents.github.io
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Recent advancements in Web AI agents have demonstrated remarkable capabilities in addressing complex web navigation tasks. However, emerging research shows that these agents exhibit greater vulnerability compared to standalone Large Language Models (LLMs), despite both being built upon the same safety-aligned models. This discrepancy is particularly concerning given the greater flexibility of Web AI Agent compared to standalone LLMs, which may expose them to a wider range of adversarial user inputs. To build a scaffold that addresses these concerns, this study investigates the underlying factors that contribute to the increased vulnerability of Web AI agents. Notably, this disparity stems from the multifaceted differences between Web AI agents and standalone LLMs, as well as the complex signals - nuances that simple evaluation metrics, such as success rate, often fail to capture. To tackle these challenges, we propose a component-level analysis and a more granular, systematic evaluation framework. Through this fine-grained investigation, we identify three critical factors that amplify the vulnerability of Web AI agents; (1) embedding user goals into the system prompt, (2) multi-step action generation, and (3) observational capabilities. Our findings highlights the pressing need to enhance security and robustness in AI agent design and provide actionable insights for targeted defense strategies.

### Multi-Turn Code Generation Through Single-Step Rewards 
[[arxiv](https://arxiv.org/abs/2502.20380)] [[cool](https://papers.cool/arxiv/2502.20380)] [[pdf](https://arxiv.org/pdf/2502.20380)]
> **Authors**: Arnav Kumar Jain,Gonzalo Gonzalez-Pumariega,Wayne Chen,Alexander M Rush,Wenting Zhao,Sanjiban Choudhury
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 9 pages (not including references or appendix); 6 figures (in main paper); (v1) preprint
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: We address the problem of code generation from multi-turn execution feedback. Existing methods either generate code without feedback or use complex, hierarchical reinforcement learning to optimize multi-turn rewards. We propose a simple yet scalable approach, $μ$Code, that solves multi-turn code generation using only single-step rewards. Our key insight is that code generation is a one-step recoverable MDP, where the correct code can be recovered from any intermediate code state in a single turn. $μ$Code iteratively trains both a generator to provide code solutions conditioned on multi-turn execution feedback and a verifier to score the newly generated code. Experimental evaluations show that our approach achieves significant improvements over the state-of-the-art baselines. We provide analysis of the design choices of the reward models and policy, and show the efficacy of $μ$Code at utilizing the execution feedback. Our code is available at https://github.com/portal-cornell/muCode.

### PhantomWiki: On-Demand Datasets for Reasoning and Retrieval Evaluation 
[[arxiv](https://arxiv.org/abs/2502.20377)] [[cool](https://papers.cool/arxiv/2502.20377)] [[pdf](https://arxiv.org/pdf/2502.20377)]
> **Authors**: Albert Gong,Kamilė Stankevičiūtė,Chao Wan,Anmol Kabra,Raphael Thesmar,Johann Lee,Julius Klenke,Carla P. Gomes,Kilian Q. Weinberger
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: High-quality benchmarks are essential for evaluating reasoning and retrieval capabilities of large language models (LLMs). However, curating datasets for this purpose is not a permanent solution as they are prone to data leakage and inflated performance results. To address these challenges, we propose PhantomWiki: a pipeline to generate unique, factually consistent document corpora with diverse question-answer pairs. Unlike prior work, PhantomWiki is neither a fixed dataset, nor is it based on any existing data. Instead, a new PhantomWiki instance is generated on demand for each evaluation. We vary the question difficulty and corpus size to disentangle reasoning and retrieval capabilities respectively, and find that PhantomWiki datasets are surprisingly challenging for frontier LLMs. Thus, we contribute a scalable and data leakage-resistant framework for disentangled evaluation of reasoning, retrieval, and tool-use abilities. Our code is available at https://github.com/kilian-group/phantom-wiki.

### When does a predictor know its own loss? 
[[arxiv](https://arxiv.org/abs/2502.20375)] [[cool](https://papers.cool/arxiv/2502.20375)] [[pdf](https://arxiv.org/pdf/2502.20375)]
> **Authors**: Aravind Gollakota,Parikshit Gopalan,Aayush Karan,Charlotte Peale,Udi Wieder
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Given a predictor and a loss function, how well can we predict the loss that the predictor will incur on an input? This is the problem of loss prediction, a key computational task associated with uncertainty estimation for a predictor. In a classification setting, a predictor will typically predict a distribution over labels and hence have its own estimate of the loss that it will incur, given by the entropy of the predicted distribution. Should we trust this estimate? In other words, when does the predictor know what it knows and what it does not know? In this work we study the theoretical foundations of loss prediction. Our main contribution is to establish tight connections between nontrivial loss prediction and certain forms of multicalibration, a multigroup fairness notion that asks for calibrated predictions across computationally identifiable subgroups. Formally, we show that a loss predictor that is able to improve on the self-estimate of a predictor yields a witness to a failure of multicalibration, and vice versa. This has the implication that nontrivial loss prediction is in effect no easier or harder than auditing for multicalibration. We support our theoretical results with experiments that show a robust positive correlation between the multicalibration error of a predictor and the efficacy of training a loss predictor.

### Constrained Generative Modeling with Manually Bridged Diffusion Models 
[[arxiv](https://arxiv.org/abs/2502.20371)] [[cool](https://papers.cool/arxiv/2502.20371)] [[pdf](https://arxiv.org/pdf/2502.20371)]
> **Authors**: Saeid Naderiparizi,Xiaoxuan Liang,Berend Zwartsenberg,Frank Wood
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: AAAI 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: In this paper we describe a novel framework for diffusion-based generative modeling on constrained spaces. In particular, we introduce manual bridges, a framework that expands the kinds of constraints that can be practically used to form so-called diffusion bridges. We develop a mechanism for combining multiple such constraints so that the resulting multiply-constrained model remains a manual bridge that respects all constraints. We also develop a mechanism for training a diffusion model that respects such multiple constraints while also adapting it to match a data distribution. We develop and extend theory demonstrating the mathematical validity of our mechanisms. Additionally, we demonstrate our mechanism in constrained generative modeling tasks, highlighting a particular high-value application in modeling trajectory initializations for path planning and control in autonomous vehicles.

### Safety Representations for Safer Policy Learning 
[[arxiv](https://arxiv.org/abs/2502.20341)] [[cool](https://papers.cool/arxiv/2502.20341)] [[pdf](https://arxiv.org/pdf/2502.20341)]
> **Authors**: Kaustubh Mani,Vincent Mai,Charlie Gauthier,Annie Chen,Samer Nashed,Liam Paull
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Accepted at International Conference onLearningRepresentations (ICLR) 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Reinforcement learning algorithms typically necessitate extensive exploration of the state space to find optimal policies. However, in safety-critical applications, the risks associated with such exploration can lead to catastrophic consequences. Existing safe exploration methods attempt to mitigate this by imposing constraints, which often result in overly conservative behaviours and inefficient learning. Heavy penalties for early constraint violations can trap agents in local optima, deterring exploration of risky yet high-reward regions of the state space. To address this, we introduce a method that explicitly learns state-conditioned safety representations. By augmenting the state features with these safety representations, our approach naturally encourages safer exploration without being excessively cautious, resulting in more efficient and safer policy learning in safety-critical scenarios. Empirical evaluations across diverse environments show that our method significantly improves task performance while reducing constraint violations during training, underscoring its effectiveness in balancing exploration with safety.

### Mixture of Structural-and-Textual Retrieval over Text-rich Graph Knowledge Bases 
[[arxiv](https://arxiv.org/abs/2502.20317)] [[cool](https://papers.cool/arxiv/2502.20317)] [[pdf](https://arxiv.org/pdf/2502.20317)]
> **Authors**: Yongjia Lei,Haoyu Han,Ryan A. Rossi,Franck Dernoncourt,Nedim Lipka,Mahantesh M Halappanavar,Jiliang Tang,Yu Wang
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,信息检索
- **Abstract**: Text-rich Graph Knowledge Bases (TG-KBs) have become increasingly crucial for answering queries by providing textual and structural knowledge. However, current retrieval methods often retrieve these two types of knowledge in isolation without considering their mutual reinforcement and some hybrid methods even bypass structural retrieval entirely after neighboring aggregation. To fill in this gap, we propose a Mixture of Structural-and-Textual Retrieval (MoR) to retrieve these two types of knowledge via a Planning-Reasoning-Organizing framework. In the Planning stage, MoR generates textual planning graphs delineating the logic for answering queries. Following planning graphs, in the Reasoning stage, MoR interweaves structural traversal and textual matching to obtain candidates from TG-KBs. In the Organizing stage, MoR further reranks fetched candidates based on their structural trajectory. Extensive experiments demonstrate the superiority of MoR in harmonizing structural and textual retrieval with insights, including uneven retrieving performance across different query logics and the benefits of integrating structural trajectories for candidate reranking. Our code is available at https://github.com/Yoega/MoR.

### Adversarial Robustness in Parameter-Space Classifiers 
[[arxiv](https://arxiv.org/abs/2502.20314)] [[cool](https://papers.cool/arxiv/2502.20314)] [[pdf](https://arxiv.org/pdf/2502.20314)]
> **Authors**: Tamir Shor,Ethan Fetaya,Chaim Baskin,Alex Bronstein
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Implicit Neural Representations (INRs) have been recently garnering increasing interest in various research fields, mainly due to their ability to represent large, complex data in a compact and continuous manner. Past work further showed that numerous popular downstream tasks can be performed directly in the INR parameter-space. Doing so can substantially reduce the computational resources required to process the represented data in their native domain. A major difficulty in using modern machine-learning approaches, is their high susceptibility to adversarial attacks, which have been shown to greatly limit the reliability and applicability of such methods in a wide range of settings. In this work, we show that parameter-space models trained for classification are inherently robust to adversarial attacks -- without the need of any robust training. To support our claims, we develop a novel suite of adversarial attacks targeting parameter-space classifiers, and furthermore analyze practical considerations of attacking parameter-space classifiers. Code for reproducing all experiments and implementation of all proposed methods will be released upon publication.

### Adapting Automatic Speech Recognition for Accented Air Traffic Control Communications 
[[arxiv](https://arxiv.org/abs/2502.20311)] [[cool](https://papers.cool/arxiv/2502.20311)] [[pdf](https://arxiv.org/pdf/2502.20311)]
> **Authors**: Marcus Yu Zhe Wee,Justin Juin Hng Wong,Lynus Lim,Joe Yu Wei Tan,Prannaya Gupta,Dillion Lim,En Hao Tew,Aloysius Keng Siew Han,Yong Zhi Lim
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,声音,音频和语音处理
- **Abstract**: Effective communication in Air Traffic Control (ATC) is critical to maintaining aviation safety, yet the challenges posed by accented English remain largely unaddressed in Automatic Speech Recognition (ASR) systems. Existing models struggle with transcription accuracy for Southeast Asian-accented (SEA-accented) speech, particularly in noisy ATC environments. This study presents the development of ASR models fine-tuned specifically for Southeast Asian accents using a newly created dataset. Our research achieves significant improvements, achieving a Word Error Rate (WER) of 0.0982 or 9.82% on SEA-accented ATC speech. Additionally, the paper highlights the importance of region-specific datasets and accent-focused training, offering a pathway for deploying ASR systems in resource-constrained military operations. The findings emphasize the need for noise-robust training techniques and region-specific datasets to improve transcription accuracy for non-Western accents in ATC communications.

### An exploration of features to improve the generalisability of fake news detection models 
[[arxiv](https://arxiv.org/abs/2502.20299)] [[cool](https://papers.cool/arxiv/2502.20299)] [[pdf](https://arxiv.org/pdf/2502.20299)]
> **Authors**: Nathaniel Hoy,Theodora Koulouri
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Accepted at Expert Systems with Applications (Elsevier)
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Fake news poses global risks by influencing elections and spreading misinformation, making detection critical. Existing NLP and supervised Machine Learning methods perform well under cross-validation but struggle to generalise across datasets, even within the same domain. This issue stems from coarsely labelled training data, where articles are labelled based on their publisher, introducing biases that token-based models like TF-IDF and BERT are sensitive to. While Large Language Models (LLMs) offer promise, their application in fake news detection remains limited. This study demonstrates that meaningful features can still be extracted from coarsely labelled data to improve real-world robustness. Stylistic features-lexical, syntactic, and semantic-are explored due to their reduced sensitivity to dataset biases. Additionally, novel social-monetisation features are introduced, capturing economic incentives behind fake news, such as advertisements, external links, and social media elements. The study trains on the coarsely labelled NELA 2020-21 dataset and evaluates using the manually labelled Facebook URLs dataset, a gold standard for generalisability. Results highlight the limitations of token-based models trained on biased data and contribute to the scarce evidence on LLMs like LLaMa in this field. Findings indicate that stylistic and social-monetisation features offer more generalisable predictions than token-based methods and LLMs. Statistical and permutation feature importance analyses further reveal their potential to enhance performance and mitigate dataset biases, providing a path forward for improving fake news detection.

### Judge a Book by its Cover: Investigating Multi-Modal LLMs for Multi-Page Handwritten Document Transcription 
[[arxiv](https://arxiv.org/abs/2502.20295)] [[cool](https://papers.cool/arxiv/2502.20295)] [[pdf](https://arxiv.org/pdf/2502.20295)]
> **Authors**: Benjamin Gutteridge,Matthew Thomas Jackson,Toni Kukurin,Xiaowen Dong
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 11 pages (including references and appendix), 14 figures, accepted at AAAI-25 Workshop on Document Understanding and Intelligence, non-archival
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: Handwritten text recognition (HTR) remains a challenging task, particularly for multi-page documents where pages share common formatting and contextual features. While modern optical character recognition (OCR) engines are proficient with printed text, their performance on handwriting is limited, often requiring costly labeled data for fine-tuning. In this paper, we explore the use of multi-modal large language models (MLLMs) for transcribing multi-page handwritten documents in a zero-shot setting. We investigate various configurations of commercial OCR engines and MLLMs, utilizing the latter both as end-to-end transcribers and as post-processors, with and without image components. We propose a novel method, '+first page', which enhances MLLM transcription by providing the OCR output of the entire document along with just the first page image. This approach leverages shared document features without incurring the high cost of processing all images. Experiments on a multi-page version of the IAM Handwriting Database demonstrate that '+first page' improves transcription accuracy, balances cost with performance, and even enhances results on out-of-sample text by extrapolating formatting and OCR error patterns from a single page.

### Scalable Graph Attention-based Instance Selection via Mini-Batch Sampling and Hierarchical Hashing 
[[arxiv](https://arxiv.org/abs/2502.20293)] [[cool](https://papers.cool/arxiv/2502.20293)] [[pdf](https://arxiv.org/pdf/2502.20293)]
> **Authors**: Zahiriddin Rustamov,Ayham Zaitouny,Nazar Zaki
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Instance selection (IS) is important in machine learning for reducing dataset size while keeping key characteristics. Current IS methods often struggle with capturing complex relationships in high-dimensional spaces and scale with large datasets. This paper introduces a graph attention-based instance selection (GAIS) method that uses attention mechanisms to identify informative instances through their structural relationships in graph representations. We present two approaches for scalable graph construction: a distance-based mini-batch sampling technique that reduces computation through strategic batch processing, and a hierarchical hashing approach that allows for efficient similarity computation through random projections. The mini-batch approach keeps class distributions through stratified sampling, while the hierarchical hashing method captures relationships at multiple granularities through single-level, multi-level, and multi-view variants. Experiments across 39 datasets show that GAIS achieves reduction rates above 96\% while maintaining or improving model performance relative to state-of-the-art IS methods. The findings shows that the distance-based mini-batch approach offers an optimal balance of efficiency and effectiveness for large-scale datasets, while multi-view variants provide superior performance for complex, high-dimensional data, demonstrating that attention-based importance scoring can effectively identify instances crucial for maintaining decision boundaries without requiring exhaustive pairwise comparisons.

### Conformal Tail Risk Control for Large Language Model Alignment 
[[arxiv](https://arxiv.org/abs/2502.20285)] [[cool](https://papers.cool/arxiv/2502.20285)] [[pdf](https://arxiv.org/pdf/2502.20285)]
> **Authors**: Catherine Yu-Chi Chen,Jingyan Shen,Zhun Deng,Lihua Lei
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Recent developments in large language models (LLMs) have led to their widespread usage for various tasks. The prevalence of LLMs in society implores the assurance on the reliability of their performance. In particular, risk-sensitive applications demand meticulous attention to unexpectedly poor outcomes, i.e., tail events, for instance, toxic answers, humiliating language, and offensive outputs. Due to the costly nature of acquiring human annotations, general-purpose scoring models have been created to automate the process of quantifying these tail events. This phenomenon introduces potential human-machine misalignment between the respective scoring mechanisms. In this work, we present a lightweight calibration framework for blackbox models that ensures the alignment of humans and machines with provable guarantees. Our framework provides a rigorous approach to controlling any distortion risk measure that is characterized by a weighted average of quantiles of the loss incurred by the LLM with high confidence. The theoretical foundation of our method relies on the connection between conformal risk control and a traditional family of statistics, i.e., L-statistics. To demonstrate the utility of our framework, we conduct comprehensive experiments that address the issue of human-machine misalignment.

### Do Sparse Autoencoders Generalize? A Case Study of Answerability 
[[arxiv](https://arxiv.org/abs/2502.19964)] [[cool](https://papers.cool/arxiv/2502.19964)] [[pdf](https://arxiv.org/pdf/2502.19964)]
> **Authors**: Lovis Heindrich,Philip Torr,Fazl Barez,Veronika Thost
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Sparse autoencoders (SAEs) have emerged as a promising approach in language model interpretability, offering unsupervised extraction of sparse features. For interpretability methods to succeed, they must identify abstract features across domains, and these features can often manifest differently in each context. We examine this through "answerability"-a model's ability to recognize answerable questions. We extensively evaluate SAE feature generalization across diverse answerability datasets for Gemma 2 SAEs. Our analysis reveals that residual stream probes outperform SAE features within domains, but generalization performance differs sharply. SAE features demonstrate inconsistent transfer ability, and residual stream probes similarly show high variance out of distribution. Overall, this demonstrates the need for quantitative methods to predict feature generalization in SAE-based interpretability.

### SeisMoLLM: Advancing Seismic Monitoring via Cross-modal Transfer with Pre-trained Large Language Model 
[[arxiv](https://arxiv.org/abs/2502.19960)] [[cool](https://papers.cool/arxiv/2502.19960)] [[pdf](https://arxiv.org/pdf/2502.19960)]
> **Authors**: Xinghao Wang,Feng Liu,Rui Su,Zhihui Wang,Lei Bai,Wanli Ouyang
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 13 pages, 6 figures. Code is available at https://github.com/StarMoonWang/SeisMoLLM
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Recent advances in deep learning have revolutionized seismic monitoring, yet developing a foundation model that performs well across multiple complex tasks remains challenging, particularly when dealing with degraded signals or data scarcity. This work presents SeisMoLLM, the first foundation model that utilizes cross-modal transfer for seismic monitoring, to unleash the power of large-scale pre-training from a large language model without requiring direct pre-training on seismic datasets. Through elaborate waveform tokenization and fine-tuning of pre-trained GPT-2 model, SeisMoLLM achieves state-of-the-art performance on the DiTing and STEAD datasets across five critical tasks: back-azimuth estimation, epicentral distance estimation, magnitude estimation, phase picking, and first-motion polarity classification. It attains 36 best results out of 43 task metrics and 12 top scores out of 16 few-shot generalization metrics, with many relative improvements ranging from 10% to 50%. In addition to its superior performance, SeisMoLLM maintains efficiency comparable to or even better than lightweight models in both training and inference. These findings establish SeisMoLLM as a promising foundation model for practical seismic monitoring and highlight cross-modal transfer as an exciting new direction for earthquake studies, showcasing the potential of advanced deep learning techniques to propel seismology research forward.

### Machine-learning for photoplethysmography analysis: Benchmarking feature, image, and signal-based approaches 
[[arxiv](https://arxiv.org/abs/2502.19949)] [[cool](https://papers.cool/arxiv/2502.19949)] [[pdf](https://arxiv.org/pdf/2502.19949)]
> **Authors**: Mohammad Moulaeifard,Loic Coquelin,Mantas Rinkevičius,Andrius Sološenko,Oskar Pfeffer,Ciaran Bench,Nando Hegemann,Sara Vardanega,Manasi Nandi,Jordi Alastruey,Christian Heiss,Vaidotas Marozas,Andrew Thompson,Philip J. Aston,Peter H. Charlton,Nils Strodthoff
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 39 pages, 9 figures, code available at https://gitlab.com/qumphy/d1-code
- **标题**: None
- **领域**: 机器学习,信号处理
- **Abstract**: Photoplethysmography (PPG) is a widely used non-invasive physiological sensing technique, suitable for various clinical applications. Such clinical applications are increasingly supported by machine learning methods, raising the question of the most appropriate input representation and model choice. Comprehensive comparisons, in particular across different input representations, are scarce. We address this gap in the research landscape by a comprehensive benchmarking study covering three kinds of input representations, interpretable features, image representations and raw waveforms, across prototypical regression and classification use cases: blood pressure and atrial fibrillation prediction. In both cases, the best results are achieved by deep neural networks operating on raw time series as input representations. Within this model class, best results are achieved by modern convolutional neural networks (CNNs). but depending on the task setup, shallow CNNs are often also very competitive. We envision that these results will be insightful for researchers to guide their choice on machine learning tasks for PPG data, even beyond the use cases presented in this work.

### Dynamic DropConnect: Enhancing Neural Network Robustness through Adaptive Edge Dropping Strategies 
[[arxiv](https://arxiv.org/abs/2502.19948)] [[cool](https://papers.cool/arxiv/2502.19948)] [[pdf](https://arxiv.org/pdf/2502.19948)]
> **Authors**: Yuan-Chih Yang,Hung-Hsuan Chen
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Dropout and DropConnect are well-known techniques that apply a consistent drop rate to randomly deactivate neurons or edges in a neural network layer during training. This paper introduces a novel methodology that assigns dynamic drop rates to each edge within a layer, uniquely tailoring the dropping process without incorporating additional learning parameters. We perform experiments on synthetic and openly available datasets to validate the effectiveness of our approach. The results demonstrate that our method outperforms Dropout, DropConnect, and Standout, a classic mechanism known for its adaptive dropout capabilities. Furthermore, our approach improves the robustness and generalization of neural network training without increasing computational complexity. The complete implementation of our methodology is publicly accessible for research and replication purposes at https://github.com/ericabd888/Adjusting-the-drop-probability-in-DropConnect-based-on-the-magnitude-of-the-gradient/.

### Algebraic Machine Learning: Learning as computing an algebraic decomposition of a task 
[[arxiv](https://arxiv.org/abs/2502.19944)] [[cool](https://papers.cool/arxiv/2502.19944)] [[pdf](https://arxiv.org/pdf/2502.19944)]
> **Authors**: Fernando Martin-Maroto,Nabil Abderrahaman,David Mendez,Gonzalo G. de Polavieja
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: :03G10; 06A12; 06A06; 08A70; 68R01; 68T01ACM Class:G.2.3; I.1.2; I.2.6; I.2.8
- **标题**: None
- **领域**: 机器学习,人工智能,离散数学,符号计算,组合学
- **Abstract**: Statistics and Optimization are foundational to modern Machine Learning. Here, we propose an alternative foundation based on Abstract Algebra, with mathematics that facilitates the analysis of learning. In this approach, the goal of the task and the data are encoded as axioms of an algebra, and a model is obtained where only these axioms and their logical consequences hold. Although this is not a generalizing model, we show that selecting specific subsets of its breakdown into algebraic atoms obtained via subdirect decomposition gives a model that generalizes. We validate this new learning principle on standard datasets such as MNIST, FashionMNIST, CIFAR-10, and medical images, achieving performance comparable to optimized multilayer perceptrons. Beyond data-driven tasks, the new learning principle extends to formal problems, such as finding Hamiltonian cycles from their specifications and without relying on search. This algebraic foundation offers a fresh perspective on machine intelligence, featuring direct learning from training data without the need for validation dataset, scaling through model additivity, and asymptotic convergence to the underlying rule in the data.

### Flexible Bivariate Beta Mixture Model: A Probabilistic Approach for Clustering Complex Data Structures 
[[arxiv](https://arxiv.org/abs/2502.19938)] [[cool](https://papers.cool/arxiv/2502.19938)] [[pdf](https://arxiv.org/pdf/2502.19938)]
> **Authors**: Yung-Peng Hsu,Hung-Hsuan Chen
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Clustering is essential in data analysis and machine learning, but traditional algorithms like $k$-means and Gaussian Mixture Models (GMM) often fail with nonconvex clusters. To address the challenge, we introduce the Flexible Bivariate Beta Mixture Model (FBBMM), which utilizes the flexibility of the bivariate beta distribution to handle diverse and irregular cluster shapes. Using the Expectation Maximization (EM) algorithm and Sequential Least Squares Programming (SLSQP) optimizer for parameter estimation, we validate FBBMM on synthetic and real-world datasets, demonstrating its superior performance in clustering complex data structures, offering a robust solution for big data analytics across various domains. We release the experimental code at https://github.com/yung-peng/MBMM-and-FBBMM.

### Lotus at SemEval-2025 Task 11: RoBERTa with Llama-3 Generated Explanations for Multi-Label Emotion Classification 
[[arxiv](https://arxiv.org/abs/2502.19935)] [[cool](https://papers.cool/arxiv/2502.19935)] [[pdf](https://arxiv.org/pdf/2502.19935)]
> **Authors**: Niloofar Ranjbar,Hamed Baghbani
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 8 pages , submitted to SemEval 2025-Task 11
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: This paper presents a novel approach for multi-label emotion detection, where Llama-3 is used to generate explanatory content that clarifies ambiguous emotional expressions, thereby enhancing RoBERTa's emotion classification performance. By incorporating explanatory context, our method improves F1-scores, particularly for emotions like fear, joy, and sadness, and outperforms text-only models. The addition of explanatory content helps resolve ambiguity, addresses challenges like overlapping emotional cues, and enhances multi-label classification, marking a significant advancement in emotion detection tasks.

### Incremental Learning with Repetition via Pseudo-Feature Projection 
[[arxiv](https://arxiv.org/abs/2502.19922)] [[cool](https://papers.cool/arxiv/2502.19922)] [[pdf](https://arxiv.org/pdf/2502.19922)]
> **Authors**: Benedikt Tscheschner,Eduardo Veas,Marc Masana
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: Incremental Learning scenarios do not always represent real-world inference use-cases, which tend to have less strict task boundaries, and exhibit repetition of common classes and concepts in their continual data stream. To better represent these use-cases, new scenarios with partial repetition and mixing of tasks are proposed, where the repetition patterns are innate to the scenario and unknown to the strategy. We investigate how exemplar-free incremental learning strategies are affected by data repetition, and we adapt a series of state-of-the-art approaches to analyse and fairly compare them under both settings. Further, we also propose a novel method (Horde), able to dynamically adjust an ensemble of self-reliant feature extractors, and align them by exploiting class repetition. Our proposed exemplar-free method achieves competitive results in the classic scenario without repetition, and state-of-the-art performance in the one with repetition.

### Shifting the Paradigm: A Diffeomorphism Between Time Series Data Manifolds for Achieving Shift-Invariancy in Deep Learning 
[[arxiv](https://arxiv.org/abs/2502.19921)] [[cool](https://papers.cool/arxiv/2502.19921)] [[pdf](https://arxiv.org/pdf/2502.19921)]
> **Authors**: Berken Utku Demirel,Christian Holz
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: To appear at the International Conference onLearningRepresentation (ICLR) 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Deep learning models lack shift invariance, making them sensitive to input shifts that cause changes in output. While recent techniques seek to address this for images, our findings show that these approaches fail to provide shift-invariance in time series, where the data generation mechanism is more challenging due to the interaction of low and high frequencies. Worse, they also decrease performance across several tasks. In this paper, we propose a novel differentiable bijective function that maps samples from their high-dimensional data manifold to another manifold of the same dimension, without any dimensional reduction. Our approach guarantees that samples -- when subjected to random shifts -- are mapped to a unique point in the manifold while preserving all task-relevant information without loss. We theoretically and empirically demonstrate that the proposed transformation guarantees shift-invariance in deep learning models without imposing any limits to the shift. Our experiments on six time series tasks with state-of-the-art methods show that our approach consistently improves the performance while enabling models to achieve complete shift-invariance without modifying or imposing restrictions on the model's topology. The source code is available on \href{https://github.com/eth-siplab/Shifting-the-Paradigm}{GitHub}.

### Playing Pokémon Red via Deep Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.19920)] [[cool](https://papers.cool/arxiv/2502.19920)] [[pdf](https://arxiv.org/pdf/2502.19920)]
> **Authors**: Marco Pleines,Daniel Addis,David Rubinstein,Frank Zimmer,Mike Preuss,Peter Whidden
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 8 pages, 3 figures, 3 tables, under review
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Pokémon Red, a classic Game Boy JRPG, presents significant challenges as a testbed for agents, including multi-tasking, long horizons of tens of thousands of steps, hard exploration, and a vast array of potential policies. We introduce a simplistic environment and a Deep Reinforcement Learning (DRL) training methodology, demonstrating a baseline agent that completes an initial segment of the game up to completing Cerulean City. Our experiments include various ablations that reveal vulnerabilities in reward shaping, where agents exploit specific reward signals. We also discuss limitations and argue that games like Pokémon hold strong potential for future research on Large Language Model agents, hierarchical training algorithms, and advanced exploration methods. Source Code: https://github.com/MarcoMeter/neroRL/tree/poke_red

### SkipPipe: Partial and Reordered Pipelining Framework for Training LLMs in Heterogeneous Networks 
[[arxiv](https://arxiv.org/abs/2502.19913)] [[cool](https://papers.cool/arxiv/2502.19913)] [[pdf](https://arxiv.org/pdf/2502.19913)]
> **Authors**: Nikolay Blagoev,Lydia Yiyu Chen,Oğuzhan Ersoy
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,分布式、并行和集群计算
- **Abstract**: Data and pipeline parallelism are ubiquitous for training of Large Language Models (LLM) on distributed nodes. Driven by the need for cost-effective training, recent work explores efficient communication arrangement for end to end training. Motivated by LLM's resistance to layer skipping and layer reordering, in this paper, we explore stage (several consecutive layers) skipping in pipeline training, and challenge the conventional practice of sequential pipeline execution. We derive convergence and throughput constraints (guidelines) for pipelining with skipping and swapping pipeline stages. Based on these constraints, we propose SkipPipe, the first partial pipeline framework to reduce the end-to-end training time for LLMs while preserving the convergence. The core of SkipPipe is a path scheduling algorithm that optimizes the paths for individual microbatches and reduces idle time (due to microbatch collisions) on the distributed nodes, complying with the given stage skipping ratio. We extensively evaluate SkipPipe on LLaMa models from 500M to 8B parameters on up to 20 nodes. Our results show that SkipPipe reduces training iteration time by up to $55\%$ compared to full pipeline. Our partial pipeline training also improves resistance to layer omission during inference, experiencing a drop in perplexity of only $7\%$ when running only half the model. Our code is available at https://github.com/gensyn-ai/skippipe.

### Graph Probability Aggregation Clustering 
[[arxiv](https://arxiv.org/abs/2502.19897)] [[cool](https://papers.cool/arxiv/2502.19897)] [[pdf](https://arxiv.org/pdf/2502.19897)]
> **Authors**: Yuxuan Yan,Na Lu,Difei Mei,Ruofan Yan,Youtian Du
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: Traditional clustering methods typically focus on either cluster-wise global clustering or point-wise local clustering to reveal the intrinsic structures in unlabeled data. Global clustering optimizes an objective function to explore the relationships between clusters, but this approach may inevitably lead to coarse partition. In contrast, local clustering heuristically groups data based on detailed point relationships, but it tends to be less coherence and efficient. To bridge the gap between these two concepts and utilize the strengths of both, we propose Graph Probability Aggregation Clustering (GPAC), a graph-based fuzzy clustering algorithm. GPAC unifies the global clustering objective function with a local clustering constraint. The entire GPAC framework is formulated as a multi-constrained optimization problem, which can be solved using the Lagrangian method. Through the optimization process, the probability of a sample belonging to a specific cluster is iteratively calculated by aggregating information from neighboring samples within the graph. We incorporate a hard assignment variable into the objective function to further improve the convergence and stability of optimization. Furthermore, to efficiently handle large-scale datasets, we introduce an acceleration program that reduces the computational complexity from quadratic to linear, ensuring scalability. Extensive experiments conducted on synthetic, real-world, and deep learning datasets demonstrate that GPAC not only exceeds existing state-of-the-art methods in clustering performance but also excels in computational efficiency, making it a powerful tool for complex clustering challenges.

### IL-SOAR : Imitation Learning with Soft Optimistic Actor cRitic 
[[arxiv](https://arxiv.org/abs/2502.19859)] [[cool](https://papers.cool/arxiv/2502.19859)] [[pdf](https://arxiv.org/pdf/2502.19859)]
> **Authors**: Stefano Viel,Luca Viano,Volkan Cevher
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: This paper introduces the SOAR framework for imitation learning. SOAR is an algorithmic template that learns a policy from expert demonstrations with a primal dual style algorithm that alternates cost and policy updates. Within the policy updates, the SOAR framework uses an actor critic method with multiple critics to estimate the critic uncertainty and build an optimistic critic fundamental to drive exploration. When instantiated in the tabular setting, we get a provable algorithm with guarantees that matches the best known results in $ε$. Practically, the SOAR template is shown to boost consistently the performance of imitation learning algorithms based on Soft Actor Critic such as f-IRL, ML-IRL and CSIL in several MuJoCo environments. Overall, thanks to SOAR, the required number of episodes to achieve the same performance is reduced by half.

### Revisit the Stability of Vanilla Federated Learning Under Diverse Conditions 
[[arxiv](https://arxiv.org/abs/2502.19849)] [[cool](https://papers.cool/arxiv/2502.19849)] [[pdf](https://arxiv.org/pdf/2502.19849)]
> **Authors**: Youngjoon Lee,Jinu Gong,Sun Choi,Joonhyuk Kang
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 10 pages
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Federated Learning (FL) is a distributed machine learning paradigm enabling collaborative model training across decentralized clients while preserving data privacy. In this paper, we revisit the stability of the vanilla FedAvg algorithm under diverse conditions. Despite its conceptual simplicity, FedAvg exhibits remarkably stable performance compared to more advanced FL techniques. Our experiments assess the performance of various FL methods on blood cell and skin lesion classification tasks using Vision Transformer (ViT). Additionally, we evaluate the impact of different representative classification models and analyze sensitivity to hyperparameter variations. The results consistently demonstrate that, regardless of dataset, classification model employed, or hyperparameter settings, FedAvg maintains robust performance. Given its stability, robust performance without the need for extensive hyperparameter tuning, FedAvg is a safe and efficient choice for FL deployments in resource-constrained hospitals handling medical data. These findings underscore the enduring value of the vanilla FedAvg approach as a trusted baseline for clinical practice.

### Knowledge Bridger: Towards Training-free Missing Multi-modality Completion 
[[arxiv](https://arxiv.org/abs/2502.19834)] [[cool](https://papers.cool/arxiv/2502.19834)] [[pdf](https://arxiv.org/pdf/2502.19834)]
> **Authors**: Guanzhou Ke,Shengfeng He,Xiao Li Wang,Bo Wang,Guoqing Chao,Yuanyang Zhang,Yi Xie,HeXing Su
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Accepted to CVPR 2025
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别,多媒体
- **Abstract**: Previous successful approaches to missing modality completion rely on carefully designed fusion techniques and extensive pre-training on complete data, which can limit their generalizability in out-of-domain (OOD) scenarios. In this study, we pose a new challenge: can we develop a missing modality completion model that is both resource-efficient and robust to OOD generalization? To address this, we present a training-free framework for missing modality completion that leverages large multimodal models (LMMs). Our approach, termed the "Knowledge Bridger", is modality-agnostic and integrates generation and ranking of missing modalities. By defining domain-specific priors, our method automatically extracts structured information from available modalities to construct knowledge graphs. These extracted graphs connect the missing modality generation and ranking modules through the LMM, resulting in high-quality imputations of missing modalities. Experimental results across both general and medical domains show that our approach consistently outperforms competing methods, including in OOD generalization. Additionally, our knowledge-driven generation and ranking techniques demonstrate superiority over variants that directly employ LMMs for generation and ranking, offering insights that may be valuable for applications in other domains.

## 机器人技术(cs.RO:Robotics)

### Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids 
[[arxiv](https://arxiv.org/abs/2502.20396)] [[cool](https://papers.cool/arxiv/2502.20396)] [[pdf](https://arxiv.org/pdf/2502.20396)]
> **Authors**: Toru Lin,Kartik Sachdev,Linxi Fan,Jitendra Malik,Yuke Zhu
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Project page can be found at https://toruowo.github.io/recipe/
- **标题**: None
- **领域**: 机器人技术,人工智能,计算机视觉和模式识别,机器学习,系统与控制
- **Abstract**: Reinforcement learning has delivered promising results in achieving human- or even superhuman-level capabilities across diverse problem domains, but success in dexterous robot manipulation remains limited. This work investigates the key challenges in applying reinforcement learning to solve a collection of contact-rich manipulation tasks on a humanoid embodiment. We introduce novel techniques to overcome the identified challenges with empirical validation. Our main contributions include an automated real-to-sim tuning module that brings the simulated environment closer to the real world, a generalized reward design scheme that simplifies reward engineering for long-horizon contact-rich manipulation tasks, a divide-and-conquer distillation process that improves the sample efficiency of hard-exploration problems while maintaining sim-to-real performance, and a mixture of sparse and dense object representations to bridge the sim-to-real perception gap. We show promising results on three humanoid dexterous manipulation tasks, with ablation studies on each technique. Our work presents a successful approach to learning humanoid dexterous manipulation using sim-to-real reinforcement learning, achieving robust generalization and high performance without the need for human demonstration.

### Physics-Driven Data Generation for Contact-Rich Manipulation via Trajectory Optimization 
[[arxiv](https://arxiv.org/abs/2502.20382)] [[cool](https://papers.cool/arxiv/2502.20382)] [[pdf](https://arxiv.org/pdf/2502.20382)]
> **Authors**: Lujie Yang,H. J. Terry Suh,Tong Zhao,Bernhard Paus Graesdal,Tarik Kelestemur,Jiuguang Wang,Tao Pang,Russ Tedrake
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,人工智能,机器学习,系统与控制
- **Abstract**: We present a low-cost data generation pipeline that integrates physics-based simulation, human demonstrations, and model-based planning to efficiently generate large-scale, high-quality datasets for contact-rich robotic manipulation tasks. Starting with a small number of embodiment-flexible human demonstrations collected in a virtual reality simulation environment, the pipeline refines these demonstrations using optimization-based kinematic retargeting and trajectory optimization to adapt them across various robot embodiments and physical parameters. This process yields a diverse, physically consistent dataset that enables cross-embodiment data transfer, and offers the potential to reuse legacy datasets collected under different hardware configurations or physical parameters. We validate the pipeline's effectiveness by training diffusion policies from the generated datasets for challenging contact-rich manipulation tasks across multiple robot embodiments, including a floating Allegro hand and bimanual robot arms. The trained policies are deployed zero-shot on hardware for bimanual iiwa arms, achieving high success rates with minimal human input. Project website: https://lujieyang.github.io/physicsgen/.

### Deep Reinforcement Learning based Autonomous Decision-Making for Cooperative UAVs: A Search and Rescue Real World Application 
[[arxiv](https://arxiv.org/abs/2502.20326)] [[cool](https://papers.cool/arxiv/2502.20326)] [[pdf](https://arxiv.org/pdf/2502.20326)]
> **Authors**: Thomas Hickling,Maxwell Hogan,Abdulla Tammam,Nabil Aouf
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 18 Pages, 21 Figures
- **标题**: None
- **领域**: 机器人技术,人工智能
- **Abstract**: This paper proposes a holistic framework for autonomous guidance, navigation, and task distribution among multi-drone systems operating in Global Navigation Satellite System (GNSS)-denied indoor settings. We advocate for a Deep Reinforcement Learning (DRL)-based guidance mechanism, utilising the Twin Delayed Deep Deterministic Policy Gradient algorithm. To improve the efficiency of the training process, we incorporate an Artificial Potential Field (APF)-based reward structure, enabling the agent to refine its movements, thereby promoting smoother paths and enhanced obstacle avoidance in indoor contexts. Furthermore, we tackle the issue of task distribution among cooperative UAVs through a DRL-trained Graph Convolutional Network (GCN). This GCN represents the interactions between drones and tasks, facilitating dynamic and real-time task allocation that reflects the current environmental conditions and the capabilities of the drones. Such an approach fosters effective coordination and collaboration among multiple drones during search and rescue operations or other exploratory endeavours. Lastly, to ensure precise odometry in environments lacking GNSS, we employ Light Detection And Ranging Simultaneous Localisation and Mapping complemented by a depth camera to mitigate the hallway problem. This integration offers robust localisation and mapping functionalities, thereby enhancing the systems dependability in indoor navigation. The proposed multi-drone framework not only elevates individual navigation capabilities but also optimises coordinated task allocation in complex, obstacle-laden environments. Experimental evaluations conducted in a setup tailored to meet the requirements of the NATO Sapience Autonomous Cooperative Drone Competition demonstrate the efficacy of the proposed system, yielding outstanding results and culminating in a first-place finish in the 2024 Sapience competition.

### CarPlanner: Consistent Auto-regressive Trajectory Planning for Large-scale Reinforcement Learning in Autonomous Driving 
[[arxiv](https://arxiv.org/abs/2502.19908)] [[cool](https://papers.cool/arxiv/2502.19908)] [[pdf](https://arxiv.org/pdf/2502.19908)]
> **Authors**: Dongkun Zhang,Jiaming Liang,Ke Guo,Sha Lu,Qi Wang,Rong Xiong,Zhenwei Miao,Yue Wang
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: CVPR 2025
- **标题**: None
- **领域**: 机器人技术,计算机视觉和模式识别,机器学习
- **Abstract**: Trajectory planning is vital for autonomous driving, ensuring safe and efficient navigation in complex environments. While recent learning-based methods, particularly reinforcement learning (RL), have shown promise in specific scenarios, RL planners struggle with training inefficiencies and managing large-scale, real-world driving scenarios. In this paper, we introduce \textbf{CarPlanner}, a \textbf{C}onsistent \textbf{a}uto-\textbf{r}egressive \textbf{Planner} that uses RL to generate multi-modal trajectories. The auto-regressive structure enables efficient large-scale RL training, while the incorporation of consistency ensures stable policy learning by maintaining coherent temporal consistency across time steps. Moreover, CarPlanner employs a generation-selection framework with an expert-guided reward function and an invariant-view module, simplifying RL training and enhancing policy performance. Extensive analysis demonstrates that our proposed RL framework effectively addresses the challenges of training efficiency and performance enhancement, positioning CarPlanner as a promising solution for trajectory planning in autonomous driving. To the best of our knowledge, we are the first to demonstrate that the RL-based planner can surpass both IL- and rule-based state-of-the-arts (SOTAs) on the challenging large-scale real-world dataset nuPlan. Our proposed CarPlanner surpasses RL-, IL-, and rule-based SOTA approaches within this demanding dataset.

### Shared Autonomy for Proximal Teaching 
[[arxiv](https://arxiv.org/abs/2502.19899)] [[cool](https://papers.cool/arxiv/2502.19899)] [[pdf](https://arxiv.org/pdf/2502.19899)]
> **Authors**: Megha Srivastava,Reihaneh Iranmanesh,Yuchen Cui,Deepak Gopinath,Emily Sumner,Andrew Silva,Laporsha Dees,Guy Rosman,Dorsa Sadigh
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Accepted to ACM/IEEE International Conference on Human-Robot Interaction, 2025
- **标题**: None
- **领域**: 机器人技术,人工智能,人机交互
- **Abstract**: Motor skill learning often requires experienced professionals who can provide personalized instruction. Unfortunately, the availability of high-quality training can be limited for specialized tasks, such as high performance racing. Several recent works have leveraged AI-assistance to improve instruction of tasks ranging from rehabilitation to surgical robot tele-operation. However, these works often make simplifying assumptions on the student learning process, and fail to model how a teacher's assistance interacts with different individuals' abilities when determining optimal teaching strategies. Inspired by the idea of scaffolding from educational psychology, we leverage shared autonomy, a framework for combining user inputs with robot autonomy, to aid with curriculum design. Our key insight is that the way a student's behavior improves in the presence of assistance from an autonomous agent can highlight which sub-skills might be most ``learnable'' for the student, or within their Zone of Proximal Development. We use this to design Z-COACH, a method for using shared autonomy to provide personalized instruction targeting interpretable task sub-skills. In a user study (n=50), where we teach high performance racing in a simulated environment of the Thunderhill Raceway Park with the CARLA Autonomous Driving simulator, we show that Z-COACH helps identify which skills each student should first practice, leading to an overall improvement in driving time, behavior, and smoothness. Our work shows that increasingly available semi-autonomous capabilities (e.g. in vehicles, robots) can not only assist human users, but also help *teach* them.

### ColorDynamic: Generalizable, Scalable, Real-time, End-to-end Local Planner for Unstructured and Dynamic Environments 
[[arxiv](https://arxiv.org/abs/2502.19892)] [[cool](https://papers.cool/arxiv/2502.19892)] [[pdf](https://arxiv.org/pdf/2502.19892)]
> **Authors**: Jinghao Xin,Zhichao Liang,Zihuan Zhang,Peng Wang,Ning Li
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 18 pages
- **标题**: None
- **领域**: 机器人技术,人工智能
- **Abstract**: Deep Reinforcement Learning (DRL) has demonstrated potential in addressing robotic local planning problems, yet its efficacy remains constrained in highly unstructured and dynamic environments. To address these challenges, this study proposes the ColorDynamic framework. First, an end-to-end DRL formulation is established, which maps raw sensor data directly to control commands, thereby ensuring compatibility with unstructured environments. Under this formulation, a novel network, Transqer, is introduced. The Transqer enables online DRL learning from temporal transitions, substantially enhancing decision-making in dynamic scenarios. To facilitate scalable training of Transqer with diverse data, an efficient simulation platform E-Sparrow, along with a data augmentation technique leveraging symmetric invariance, are developed. Comparative evaluations against state-of-the-art methods, alongside assessments of generalizability, scalability, and real-time performance, were conducted to validate the effectiveness of ColorDynamic. Results indicate that our approach achieves a success rate exceeding 90% while exhibiting real-time capacity (1.2-1.3 ms per planning). Additionally, ablation studies were performed to corroborate the contributions of individual components. Building on this, the OkayPlan-ColorDynamic (OPCD) navigation system is presented, with simulated and real-world experiments demonstrating its superiority and applicability in complex scenarios. The codebase and experimental demonstrations have been open-sourced on our website to facilitate reproducibility and further research.

## 声音(cs.SD:Sound)

### DiffCSS: Diverse and Expressive Conversational Speech Synthesis with Diffusion Models 
[[arxiv](https://arxiv.org/abs/2502.19924)] [[cool](https://papers.cool/arxiv/2502.19924)] [[pdf](https://arxiv.org/pdf/2502.19924)]
> **Authors**: Weihao wu,Zhiwei Lin,Yixuan Zhou,Jingbei Li,Rui Niu,Qinghua Wu,Songjun Cao,Long Ma,Zhiyong Wu
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Accepted by ICASSP 2025
- **标题**: None
- **领域**: 声音,人工智能,音频和语音处理
- **Abstract**: Conversational speech synthesis (CSS) aims to synthesize both contextually appropriate and expressive speech, and considerable efforts have been made to enhance the understanding of conversational context. However, existing CSS systems are limited to deterministic prediction, overlooking the diversity of potential responses. Moreover, they rarely employ language model (LM)-based TTS backbones, limiting the naturalness and quality of synthesized speech. To address these issues, in this paper, we propose DiffCSS, an innovative CSS framework that leverages diffusion models and an LM-based TTS backbone to generate diverse, expressive, and contextually coherent speech. A diffusion-based context-aware prosody predictor is proposed to sample diverse prosody embeddings conditioned on multimodal conversational context. Then a prosody-controllable LM-based TTS backbone is developed to synthesize high-quality speech with sampled prosody embeddings. Experimental results demonstrate that the synthesized speech from DiffCSS is more diverse, contextually coherent, and expressive than existing CSS systems

## 软件工程(cs.SE:Software Engineering)

### ConvCodeWorld: Benchmarking Conversational Code Generation in Reproducible Feedback Environments 
[[arxiv](https://arxiv.org/abs/2502.19852)] [[cool](https://papers.cool/arxiv/2502.19852)] [[pdf](https://arxiv.org/pdf/2502.19852)]
> **Authors**: Hojae Han,Seung-won Hwang,Rajhans Samdani,Yuxiong He
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: ICLR 2025
- **标题**: None
- **领域**: 软件工程,人工智能,计算语言学
- **Abstract**: Large language models (LLMs) have proven invaluable for code generation, particularly in interactive settings. However, existing code generation benchmarks fail to capture the diverse feedback encountered in multi-turn interactions, limiting our ability to evaluate LLMs in these contexts. To address this gap, we present a set of novel benchmarks that explicitly model the quality of feedback provided to code generation LLMs. Our contributions are threefold: First, we introduce CONVCODEWORLD, a novel and reproducible environment for benchmarking interactive code generation. CONVCODEWORLD simulates 9 distinct interactive code generation scenarios while systematically combining three types of feedback: (a) compilation feedback; (b) execution feedback with varying test coverage; (c) verbal feedback generated by GPT-4o with different levels of expertise. Second, we introduce CONVCODEBENCH, a fast, static version of benchmark that uses pre-generated feedback logs, eliminating the need for costly dynamic verbal feedback generation while maintaining strong Spearman's rank correlations (0.82 to 0.99) with CONVCODEWORLD. Third, extensive evaluations of both closed-source and open-source LLMs including R1-Distill on CONVCODEWORLD reveal key insights: (a) LLM performance varies significantly based on the feedback provided; (b) Weaker LLMs, with sufficient feedback, can outperform single-turn results of state-of-the-art LLMs without feedback; (c) Training on a specific feedback combination can limit an LLM's ability to utilize unseen combinations; (d) LLMs solve problems in fewer turns (high MRR) may not solve as many problems overall (high Recall), and vice versa. All implementations and benchmarks will be made publicly available at https://huggingface.co/spaces/ConvCodeWorld/ConvCodeWorld

## 社交和信息网络(cs.SI:Social and Information Networks)

### Towards Collaborative Anti-Money Laundering Among Financial Institutions 
[[arxiv](https://arxiv.org/abs/2502.19952)] [[cool](https://papers.cool/arxiv/2502.19952)] [[pdf](https://arxiv.org/pdf/2502.19952)]
> **Authors**: Zhihua Tian,Yuan Ding,Xiang Yu,Enchao Gong,Jian Liu,Kui Ren
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Accepted by International World Wide Web Conference (WWW) 2025
- **标题**: None
- **领域**: 社交和信息网络,计算机与社会,机器学习
- **Abstract**: Money laundering is the process that intends to legalize the income derived from illicit activities, thus facilitating their entry into the monetary flow of the economy without jeopardizing their source. It is crucial to identify such activities accurately and reliably in order to enforce anti-money laundering (AML). Despite considerable efforts to AML, a large number of such activities still go undetected. Rule-based methods were first introduced and are still widely used in current detection systems. With the rise of machine learning, graph-based learning methods have gained prominence in detecting illicit accounts through the analysis of money transfer graphs. Nevertheless, these methods generally assume that the transaction graph is centralized, whereas in practice, money laundering activities usually span multiple financial institutions. Due to regulatory, legal, commercial, and customer privacy concerns, institutions tend not to share data, restricting their utility in practical usage. In this paper, we propose the first algorithm that supports performing AML over multiple institutions while protecting the security and privacy of local data. To evaluate, we construct Alipay-ECB, a real-world dataset comprising digital transactions from Alipay, the world's largest mobile payment platform, alongside transactions from E-Commerce Bank (ECB). The dataset includes over 200 million accounts and 300 million transactions, covering both intra-institution transactions and those between Alipay and ECB. This makes it the largest real-world transaction graph available for analysis. The experimental results demonstrate that our methods can effectively identify cross-institution money laundering subgroups. Additionally, experiments on synthetic datasets also demonstrate that our method is efficient, requiring only a few minutes on datasets with millions of transactions.

### Community Detection by ELPMeans: An Unsupervised Approach That Uses Laplacian Centrality and Clustering 
[[arxiv](https://arxiv.org/abs/2502.19895)] [[cool](https://papers.cool/arxiv/2502.19895)] [[pdf](https://arxiv.org/pdf/2502.19895)]
> **Authors**: Shahin Momenzadeh,Rojiar Pir Mohammadiani
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: ef:The 3rd International Conference on Engineering and innovative Technology ICEIT2024 Salahaddin University-Erbil, 30-31 October 2024
- **标题**: None
- **领域**: 社交和信息网络,机器学习
- **Abstract**: Community detection in network analysis has become more intricate due to the recent hike in social networks (Cai et al., 2024). This paper suggests a new approach named ELPMeans that strives to address this challenge. For community detection in the whole network, ELPMeans combines Laplacian, Hierarchical Clustering as well as K-means algorithms. Our technique employs Laplacian centrality and minimum distance metrics for central node identification while k-means learning is used for efficient convergence to final community structure. Remarkably, ELPMeans is an unsupervised method which is not only simple to implement but also effectively tackles common problems such as random initialization of central nodes, or finding of number of communities (K). Experimental results show that our algorithm improves accuracy and reduces time complexity considerably outperforming recent approaches on real world networks. Moreover, our approach has a wide applicability range in various community detection tasks even with nonconvex shapes and no prior knowledge about the number of communities present.

## 图像和视频处理(eess.IV:Image and Video Processing)

### T1-PILOT: Optimized Trajectories for T1 Mapping Acceleration 
[[arxiv](https://arxiv.org/abs/2502.20333)] [[cool](https://papers.cool/arxiv/2502.20333)] [[pdf](https://arxiv.org/pdf/2502.20333)]
> **Authors**: Tamir Shor,Moti Freiman,Chaim Baskin,Alex Bronstein
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习
- **Abstract**: Cardiac T1 mapping provides critical quantitative insights into myocardial tissue composition, enabling the assessment of pathologies such as fibrosis, inflammation, and edema. However, the inherently dynamic nature of the heart imposes strict limits on acquisition times, making high-resolution T1 mapping a persistent challenge. Compressed sensing (CS) approaches have reduced scan durations by undersampling k-space and reconstructing images from partial data, and recent studies show that jointly optimizing the undersampling patterns with the reconstruction network can substantially improve performance. Still, most current T1 mapping pipelines rely on static, hand-crafted masks that do not exploit the full acceleration and accuracy potential. In this work, we introduce T1-PILOT: an end-to-end method that explicitly incorporates the T1 signal relaxation model into the sampling-reconstruction framework to guide the learning of non-Cartesian trajectories, crossframe alignment, and T1 decay estimation. Through extensive experiments on the CMRxRecon dataset, T1-PILOT significantly outperforms several baseline strategies (including learned single-mask and fixed radial or golden-angle sampling schemes), achieving higher T1 map fidelity at greater acceleration factors. In particular, we observe consistent gains in PSNR and VIF relative to existing methods, along with marked improvements in delineating finer myocardial structures. Our results highlight that optimizing sampling trajectories in tandem with the physical relaxation model leads to both enhanced quantitative accuracy and reduced acquisition times. Code for reproducing all results will be made publicly available upon publication.

## 信号处理(eess.SP:Signal Processing)

### NeRFCom: Feature Transform Coding Meets Neural Radiance Field for Free-View 3D Scene Semantic Transmission 
[[arxiv](https://arxiv.org/abs/2502.19873)] [[cool](https://papers.cool/arxiv/2502.19873)] [[pdf](https://arxiv.org/pdf/2502.19873)]
> **Authors**: Weijie Yue,Zhongwei Si,Bolin Wu,Sixian Wang,Xiaoqi Qin,Kai Niu,Jincheng Dai,Ping Zhang
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 信号处理,机器学习
- **Abstract**: We introduce NeRFCom, a novel communication system designed for end-to-end 3D scene transmission. Compared to traditional systems relying on handcrafted NeRF semantic feature decomposition for compression and well-adaptive channel coding for transmission error correction, our NeRFCom employs a nonlinear transform and learned probabilistic models, enabling flexible variable-rate joint source-channel coding and efficient bandwidth allocation aligned with the NeRF semantic feature's different contribution to the 3D scene synthesis fidelity. Experimental results demonstrate that NeRFCom achieves free-view 3D scene efficient transmission while maintaining robustness under adverse channel conditions.

## 动力系统(math.DS:Dynamical Systems)

### Impilict Runge-Kutta based sparse identification of governing equations in biologically motivated systems 
[[arxiv](https://arxiv.org/abs/2502.20319)] [[cool](https://papers.cool/arxiv/2502.20319)] [[pdf](https://arxiv.org/pdf/2502.20319)]
> **Authors**: Mehrdad Anvari,Hamidreza Marasi,Hossein Kheiri
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 23 pages, 9 figures
- **标题**: None
- **领域**: 动力系统,机器学习,数值分析,定量方法
- **Abstract**: Identifying governing equations in physical and biological systems from datasets remains a long-standing challenge across various scientific disciplines, providing mechanistic insights into complex system evolution. Common methods like sparse identification of nonlinear dynamics (SINDy) often rely on precise derivative estimations, making them vulnerable to data scarcity and noise. This study presents a novel data-driven framework by integrating high order implicit Runge-Kutta methods (IRKs) with the sparse identification, termed IRK-SINDy. The framework exhibits remarkable robustness to data scarcity and noise by leveraging the lower stepsize constraint of IRKs. Two methods for incorporating IRKs into sparse regression are introduced: one employs iterative schemes for numerically solving nonlinear algebraic system of equations, while the other utilizes deep neural networks to predict stage values of IRKs. The performance of IRK-SINDy is demonstrated through numerical experiments on benchmark problems with varied dynamical behaviors, including linear and nonlinear oscillators, the Lorenz system, and biologically relevant models like predator-prey dynamics, logistic growth, and the FitzHugh-Nagumo model. Results indicate that IRK-SINDy outperforms conventional SINDy and the RK4-SINDy framework, particularly under conditions of extreme data scarcity and noise, yielding interpretable and generalizable models.

## 数值分析(math.NA:Numerical Analysis)

### Scalable Signature Kernel Computations for Long Time Series via Local Neumann Series Expansions 
[[arxiv](https://arxiv.org/abs/2502.20392)] [[cool](https://papers.cool/arxiv/2502.20392)] [[pdf](https://arxiv.org/pdf/2502.20392)]
> **Authors**: Matthew Tamayo-Rios,Alexander Schell,Rima Alaifari
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 18 pages, 3 figures
- **标题**: None
- **领域**: 数值分析,机器学习,偏微分方程分析
- **Abstract**: The signature kernel is a recent state-of-the-art tool for analyzing high-dimensional sequential data, valued for its theoretical guarantees and strong empirical performance. In this paper, we present a novel method for efficiently computing the signature kernel of long, high-dimensional time series via dynamically truncated recursive local power series expansions. Building on the characterization of the signature kernel as the solution of a Goursat PDE, our approach employs tilewise Neumann-series expansions to derive rapidly converging power series approximations of the signature kernel that are locally defined on subdomains and propagated iteratively across the entire domain of the Goursat solution by exploiting the geometry of the time series. Algorithmically, this involves solving a system of interdependent local Goursat PDEs by recursively propagating boundary conditions along a directed graph via topological ordering, with dynamic truncation adaptively terminating each local power series expansion when coefficients fall below machine precision, striking an effective balance between computational cost and accuracy. This method achieves substantial performance improvements over state-of-the-art approaches for computing the signature kernel, providing (a) adjustable and superior accuracy, even for time series with very high roughness; (b) drastically reduced memory requirements; and (c) scalability to efficiently handle very long time series (e.g., with up to half a million points or more) on a single GPU. These advantages make our method particularly well-suited for rough-path-assisted machine learning, financial modeling, and signal processing applications that involve very long and highly volatile data.

### A Multiple Transferable Neural Network Method with Domain Decomposition for Elliptic Interface Problems 
[[arxiv](https://arxiv.org/abs/2502.19893)] [[cool](https://papers.cool/arxiv/2502.19893)] [[pdf](https://arxiv.org/pdf/2502.19893)]
> **Authors**: Tianzheng Lu,Lili Ju,Liyong Zhu
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 数值分析,机器学习
- **Abstract**: The transferable neural network (TransNet) is a two-layer shallow neural network with pre-determined and uniformly distributed neurons in the hidden layer, and the least-squares solvers can be particularly used to compute the parameters of its output layer when applied to the solution of partial differential equations. In this paper, we integrate the TransNet technique with the nonoverlapping domain decomposition and the interface conditions to develop a novel multiple transferable neural network (Multi-TransNet) method for solving elliptic interface problems, which typically contain discontinuities in both solutions and their derivatives across interfaces. We first propose an empirical formula for the TransNet to characterize the relationship between the radius of the domain-covering ball, the number of hidden-layer neurons, and the optimal neuron shape. In the Multi-TransNet method, we assign each subdomain one distinct TransNet with an adaptively determined number of hidden-layer neurons to maintain the globally uniform neuron distribution across the entire computational domain, and then unite all the subdomain TransNets together by incorporating the interface condition terms into the loss function. The empirical formula is also extended to the Multi-TransNet and further employed to estimate appropriate neuron shapes for the subdomain TransNets, greatly reducing the parameter tuning cost. Additionally, we propose a normalization approach to adaptively select the weighting parameters for the terms in the loss function. Ablation studies and extensive experiments with comparison tests on different types of elliptic interface problems with low to high contrast diffusion coefficients in two and three dimensions are carried out to numerically demonstrate the superior accuracy, efficiency, and robustness of the proposed Multi-TransNet method.

## 优化与控制(math.OC:Optimization and Control)

### Inexact Moreau Envelope Lagrangian Method for Non-Convex Constrained Optimization under Local Error Bound Conditions on Constraint Functions 
[[arxiv](https://arxiv.org/abs/2502.19764)] [[cool](https://papers.cool/arxiv/2502.19764)] [[pdf](https://arxiv.org/pdf/2502.19764)]
> **Authors**: Yankun Huang,Qihang Lin,Yangyang Xu
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 优化与控制,机器学习
- **Abstract**: In this paper, we study the inexact Moreau envelope Lagrangian (iMELa) method for solving smooth non-convex optimization problems over a simple polytope with additional convex inequality constraints. By incorporating a proximal term into the traditional Lagrangian function, the iMELa method approximately solves a convex optimization subproblem over the polyhedral set at each main iteration. Under the assumption of a local error bound condition for subsets of the feasible set defined by subsets of the constraints, we establish that the iMELa method can find an $ε$-Karush-Kuhn-Tucker point with $\tilde O(ε^{-2})$ gradient oracle complexity.

### Physics-Informed Neural Networks for Optimal Vaccination Plan in SIR Epidemic Models 
[[arxiv](https://arxiv.org/abs/2502.19890)] [[cool](https://papers.cool/arxiv/2502.19890)] [[pdf](https://arxiv.org/pdf/2502.19890)]
> **Authors**: Minseok Kim,Yeongjong Kim,Yeoneung Kim
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: :49J15; 49L20; 49L25; 35F21; 65M99; 68T07ACM Class:G.1.6; G.1.10
- **标题**: None
- **领域**: 优化与控制,机器学习
- **Abstract**: This work focuses on understanding the minimum eradication time for the controlled Susceptible-Infectious-Recovered (SIR) model in the time-homogeneous setting, where the infection and recovery rates are constant. The eradication time is defined as the earliest time the infectious population drops below a given threshold and remains below it. For time-homogeneous models, the eradication time is well-defined due to the predictable dynamics of the infectious population, and optimal control strategies can be systematically studied. We utilize Physics-Informed Neural Networks (PINNs) to solve the partial differential equation (PDE) governing the eradication time and derive the corresponding optimal vaccination control. The PINN framework enables a mesh-free solution to the PDE by embedding the dynamics directly into the loss function of a deep neural network. We use a variable scaling method to ensure stable training of PINN and mathematically analyze that this method is effective in our setting. This approach provides an efficient computational alternative to traditional numerical methods, allowing for an approximation of the eradication time and the optimal control strategy. Through numerical experiments, we validate the effectiveness of the proposed method in computing the minimum eradication time and achieving optimal control. This work offers a novel application of PINNs to epidemic modeling, bridging mathematical theory and computational practice for time-homogeneous SIR models.

## 核理论(nucl-th:Nuclear Theory)

### Global Framework for Simultaneous Emulation Across the Nuclear Landscape 
[[arxiv](https://arxiv.org/abs/2502.20363)] [[cool](https://papers.cool/arxiv/2502.20363)] [[pdf](https://arxiv.org/pdf/2502.20363)]
> **Authors**: Antoine Belley,Jose M. Munoz,Ronald F. Garcia Ruiz
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 核理论,机器学习
- **Abstract**: We introduce a hierarchical framework that combines ab initio many-body calculations with a Bayesian neural network, developing emulators capable of accurately predicting nuclear properties across the nuclear chart, including multiple isotopes simultaneously. We benchmark our developments using the oxygen isotopic chain, achieving accurate results for ground-state energies and nuclear charge radii, while providing robust uncertainty quantification. Our framework enables global sensitivity analysis of nuclear binding energies and charge radii with respect to the low-energy constants that describe the nuclear force.

## 神经元和认知(q-bio.NC:Neurons and Cognition)

### Naturalistic Computational Cognitive Science: Towards generalizable models and theories that capture the full range of natural behavior 
[[arxiv](https://arxiv.org/abs/2502.20349)] [[cool](https://papers.cool/arxiv/2502.20349)] [[pdf](https://arxiv.org/pdf/2502.20349)]
> **Authors**: Wilka Carvalho,Andrew Lampinen
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 神经元和认知,人工智能
- **Abstract**: Artificial Intelligence increasingly pursues large, complex models that perform many tasks within increasingly realistic domains. How, if at all, should these developments in AI influence cognitive science? We argue that progress in AI offers timely opportunities for cognitive science to embrace experiments with increasingly naturalistic stimuli, tasks, and behaviors; and computational models that can accommodate these changes. We first review a growing body of research spanning neuroscience, cognitive science, and AI that suggests that incorporating a broader range of naturalistic experimental paradigms (and models that accommodate them) may be necessary to resolve some aspects of natural intelligence and ensure that our theories generalize. We then suggest that integrating recent progress in AI and cognitive science will enable us to engage with more naturalistic phenomena without giving up experimental control or the pursuit of theoretically grounded understanding. We offer practical guidance on how methodological practices can contribute to cumulative progress in naturalistic computational cognitive science, and illustrate a path towards building computational models that solve the real problems of natural cognition - together with a reductive understanding of the processes and principles by which they do so.

## 机器学习(stat.ML:Machine Learning)

### Fast Debiasing of the LASSO Estimator 
[[arxiv](https://arxiv.org/abs/2502.19825)] [[cool](https://papers.cool/arxiv/2502.19825)] [[pdf](https://arxiv.org/pdf/2502.19825)]
> **Authors**: Shuvayan Banerjee,James Saunderson,Radhendushka Srivastava,Ajit Rajwade
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: In high-dimensional sparse regression, the \textsc{Lasso} estimator offers excellent theoretical guarantees but is well-known to produce biased estimates. To address this, \cite{Javanmard2014} introduced a method to ``debias" the \textsc{Lasso} estimates for a random sub-Gaussian sensing matrix $\boldsymbol{A}$. Their approach relies on computing an ``approximate inverse" $\boldsymbol{M}$ of the matrix $\boldsymbol{A}^\top \boldsymbol{A}/n$ by solving a convex optimization problem. This matrix $\boldsymbol{M}$ plays a critical role in mitigating bias and allowing for construction of confidence intervals using the debiased \textsc{Lasso} estimates. However the computation of $\boldsymbol{M}$ is expensive in practice as it requires iterative optimization. In the presented work, we re-parameterize the optimization problem to compute a ``debiasing matrix" $\boldsymbol{W} := \boldsymbol{AM}^{\top}$ directly, rather than the approximate inverse $\boldsymbol{M}$. This reformulation retains the theoretical guarantees of the debiased \textsc{Lasso} estimates, as they depend on the \emph{product} $\boldsymbol{AM}^{\top}$ rather than on $\boldsymbol{M}$ alone. Notably, we provide a simple, computationally efficient, closed-form solution for $\boldsymbol{W}$ under similar conditions for the sensing matrix $\boldsymbol{A}$ used in the original debiasing formulation, with an additional condition that the elements of every row of $\boldsymbol{A}$ have uncorrelated entries. Also, the optimization problem based on $\boldsymbol{W}$ guarantees a unique optimal solution, unlike the original formulation based on $\boldsymbol{M}$. We verify our main result with numerical simulations.

### Multiple Linked Tensor Factorization 
[[arxiv](https://arxiv.org/abs/2502.20286)] [[cool](https://papers.cool/arxiv/2502.20286)] [[pdf](https://arxiv.org/pdf/2502.20286)]
> **Authors**: Zhiyu Kang,Raghavendra B. Rao,Eric F. Lock
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 26 pages, 4 figures, 7 tables
- **标题**: None
- **领域**: 机器学习,机器学习,计算,方法论
- **Abstract**: In biomedical research and other fields, it is now common to generate high content data that are both multi-source and multi-way. Multi-source data are collected from different high-throughput technologies while multi-way data are collected over multiple dimensions, yielding multiple tensor arrays. Integrative analysis of these data sets is needed, e.g., to capture and synthesize different facets of complex biological systems. However, despite growing interest in multi-source and multi-way factorization techniques, methods that can handle data that are both multi-source and multi-way are limited. In this work, we propose a Multiple Linked Tensors Factorization (MULTIFAC) method extending the CANDECOMP/PARAFAC (CP) decomposition to simultaneously reduce the dimension of multiple multi-way arrays and approximate underlying signal. We first introduce a version of the CP factorization with L2 penalties on the latent factors, leading to rank sparsity. When extended to multiple linked tensors, the method automatically reveals latent components that are shared across data sources or individual to each data source. We also extend the decomposition algorithm to its expectation-maximization (EM) version to handle incomplete data with imputation. Extensive simulation studies are conducted to demonstrate MULTIFAC's ability to (i) approximate underlying signal, (ii) identify shared and unshared structures, and (iii) impute missing data. The approach yields an interpretable decomposition on multi-way multi-omics data for a study on early-life iron deficiency.

## 其他论文

- [Empowering Social Service with AI: Insights from a Participatory Design Study with Practitioners](https://arxiv.org/abs/2502.19822)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Waves and symbols in neuromorphic hardware: from analog signal processing to digital computing on the same computational substrate](https://arxiv.org/abs/2502.20381)
  - **标题**: None
  - **Filtered Reason**: none of cs.NE in whitelist
- [Evaluating the long-term viability of eye-tracking for continuous authentication in virtual reality](https://arxiv.org/abs/2502.20359)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [The entropy profiles of a definable set over finite fields](https://arxiv.org/abs/2502.20355)
  - **标题**: None
  - **Filtered Reason**: none of math.LO,cs.IT,math.NT in whitelist
- [KNOWM Memristors in a Bridge Synapse delay-based Reservoir Computing system for detection of epileptic seizures](https://arxiv.org/abs/2502.20351)
  - **标题**: None
  - **Filtered Reason**: none of cs.ET,physics.med-ph in whitelist
- [Reservoir Computing and Photoelectrochemical Sensors: A Marriage of Convenience](https://arxiv.org/abs/2502.20342)
  - **标题**: None
  - **Filtered Reason**: none of cs.ET,physics.ins-det in whitelist
- [ACCORD: Application Context-aware Cross-layer Optimization and Resource Design for 5G/NextG Machine-centric Applications](https://arxiv.org/abs/2502.20320)
  - **标题**: None
  - **Filtered Reason**: none of cs.NI,eess.SY in whitelist
- [PrimeK-Net: Multi-scale Spectral Learning via Group Prime-Kernel Convolutional Neural Networks for Single Channel Speech Enhancement](https://arxiv.org/abs/2502.19906)
  - **标题**: None
  - **Filtered Reason**: none of cs.SD,eess.AS in whitelist
- [Towards Multimodal Large-Language Models for Parent-Child Interaction: A Focus on Joint Attention](https://arxiv.org/abs/2502.19877)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [RingAda: Pipelining Large Model Fine-Tuning on Edge Devices with Scheduled Layer Unfreezing](https://arxiv.org/abs/2502.19864)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC in whitelist
