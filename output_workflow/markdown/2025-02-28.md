> 本文由 [https://github.com/huiyeruzhou/arxiv_crawler](https://github.com/huiyeruzhou/arxiv_crawler) 自动生成
>
> 领域白名单：cs.AI,cs.CL,cs.LG,cs.CV
> 关键词： LLM, GPT, AI, language+model, deep+learning, transformer, neural+network, machine+learning

# 论文全览：2025-02-28

共有206篇相关领域论文, 另有19篇其他

## 星系天体物理学(astro-ph.GA:Astrophysics of Galaxies)

### Shared Stochastic Gaussian Process Latent Variable Models: A Multi-modal Generative Model for Quasar Spectra 
[[arxiv](https://arxiv.org/abs/2502.19824)] [[cool](https://papers.cool/arxiv/2502.19824)] [[pdf](https://arxiv.org/pdf/2502.19824)]
> **Authors**: Vidhi Lalchand,Anna-Christina Eilers
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Published in TMLR, https://openreview.net/pdf?id=LzmsvRTqaJ. The code for this work is available at: https://github.com/vr308/Quasar-GPLVM
- **标题**: None
- **领域**: 星系天体物理学,机器学习,应用领域,方法论
- **Abstract**: This work proposes a scalable probabilistic latent variable model based on Gaussian processes (Lawrence, 2004) in the context of multiple observation spaces. We focus on an application in astrophysics where data sets typically contain both observed spectral features and scientific properties of astrophysical objects such as galaxies or exoplanets. In our application, we study the spectra of very luminous galaxies known as quasars, along with their properties, such as the mass of their central supermassive black hole, accretion rate, and luminosity-resulting in multiple observation spaces. A single data point is then characterized by different classes of observations, each with different likelihoods. Our proposed model extends the baseline stochastic variational Gaussian process latent variable model (GPLVM) introduced by Lalchand et al. (2022) to this setting, proposing a seamless generative model where the quasar spectra and scientific labels can be generated simultaneously using a shared latent space as input to different sets of Gaussian process decoders, one for each observation space. Additionally, this framework enables training in a missing data setting where a large number of dimensions per data point may be unknown or unobserved. We demonstrate high-fidelity reconstructions of the spectra and scientific labels during test-time inference and briefly discuss the scientific interpretations of the results, along with the significance of such a generative model.

## 人工智能(cs.AI:Artificial Intelligence)

### Multi-Agent Verification: Scaling Test-Time Compute with Multiple Verifiers 
[[arxiv](https://arxiv.org/abs/2502.20379)] [[cool](https://papers.cool/arxiv/2502.20379)] [[pdf](https://arxiv.org/pdf/2502.20379)]
> **Authors**: Shalev Lifshitz,Sheila A. McIlraith,Yilun Du
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: By utilizing more computational resources at test-time, large language models (LLMs) can improve without additional training. One common strategy uses verifiers to evaluate candidate outputs. In this work, we propose a novel scaling dimension for test-time compute: scaling the number of verifiers. We introduce Multi-Agent Verification (MAV) as a test-time compute paradigm that combines multiple verifiers to improve performance. We propose using Aspect Verifiers (AVs), off-the-shelf LLMs prompted to verify different aspects of outputs, as one possible choice for the verifiers in a MAV system. AVs are a convenient building block for MAV since they can be easily combined without additional training. Moreover, we introduce BoN-MAV, a simple multi-agent verification algorithm that combines best-of-n sampling with multiple verifiers. BoN-MAV demonstrates stronger scaling patterns than self-consistency and reward model verification, and we demonstrate both weak-to-strong generalization, where combining weak verifiers improves even stronger LLMs, and self-improvement, where the same base model is used to both generate and verify outputs. Our results establish scaling the number of verifiers as a promising new dimension for improving language model performance at test-time.

### Towards Responsible AI in Education: Hybrid Recommendation System for K-12 Students Case Study 
[[arxiv](https://arxiv.org/abs/2502.20354)] [[cool](https://papers.cool/arxiv/2502.20354)] [[pdf](https://arxiv.org/pdf/2502.20354)]
> **Authors**: Nazarii Drushchak,Vladyslava Tyshchenko,Nataliya Polyakovska
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: The growth of Educational Technology (EdTech) has enabled highly personalized learning experiences through Artificial Intelligence (AI)-based recommendation systems tailored to each student needs. However, these systems can unintentionally introduce biases, potentially limiting fair access to learning resources. This study presents a recommendation system for K-12 students, combining graph-based modeling and matrix factorization to provide personalized suggestions for extracurricular activities, learning resources, and volunteering opportunities. To address fairness concerns, the system includes a framework to detect and reduce biases by analyzing feedback across protected student groups. This work highlights the need for continuous monitoring in educational recommendation systems to support equitable, transparent, and effective learning opportunities for all students.

### EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants 
[[arxiv](https://arxiv.org/abs/2502.20309)] [[cool](https://papers.cool/arxiv/2502.20309)] [[pdf](https://arxiv.org/pdf/2502.20309)]
> **Authors**: Franck Cappello,Sandeep Madireddy,Robert Underwood,Neil Getty,Nicholas Lee-Ping Chia,Nesar Ramachandra,Josh Nguyen,Murat Keceli,Tanwi Mallick,Zilinghan Li,Marieme Ngom,Chenhui Zhang,Angel Yanguas-Gil,Evan Antoniuk,Bhavya Kailkhura,Minyang Tian,Yufeng Du,Yuan-Sen Ting,Azton Wells,Bogdan Nicolae,Avinash Maurya,M. Mustafa Rafique,Eliu Huerta,Bo Li,Ian Foster, et al. (1 additional authors not shown)
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 33 pages, 18 figures
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Recent advancements have positioned AI, and particularly Large Language Models (LLMs), as transformative tools for scientific research, capable of addressing complex tasks that require reasoning, problem-solving, and decision-making. Their exceptional capabilities suggest their potential as scientific research assistants but also highlight the need for holistic, rigorous, and domain-specific evaluation to assess effectiveness in real-world scientific applications. This paper describes a multifaceted methodology for Evaluating AI models as scientific Research Assistants (EAIRA) developed at Argonne National Laboratory. This methodology incorporates four primary classes of evaluations. 1) Multiple Choice Questions to assess factual recall; 2) Open Response to evaluate advanced reasoning and problem-solving skills; 3) Lab-Style Experiments involving detailed analysis of capabilities as research assistants in controlled environments; and 4) Field-Style Experiments to capture researcher-LLM interactions at scale in a wide range of scientific domains and applications. These complementary methods enable a comprehensive analysis of LLM strengths and weaknesses with respect to their scientific knowledge, reasoning abilities, and adaptability. Recognizing the rapid pace of LLM advancements, we designed the methodology to evolve and adapt so as to ensure its continued relevance and applicability. This paper describes the methodology state at the end of February 2025. Although developed within a subset of scientific domains, the methodology is designed to be generalizable to a wide range of scientific domains.

### Evaluating Human Trust in LLM-Based Planners: A Preliminary Study 
[[arxiv](https://arxiv.org/abs/2502.20284)] [[cool](https://papers.cool/arxiv/2502.20284)] [[pdf](https://arxiv.org/pdf/2502.20284)]
> **Authors**: Shenghui Chen,Yunhao Yang,Kayla Boggess,Seongkook Heo,Lu Feng,Ufuk Topcu
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,人机交互
- **Abstract**: Large Language Models (LLMs) are increasingly used for planning tasks, offering unique capabilities not found in classical planners such as generating explanations and iterative refinement. However, trust--a critical factor in the adoption of planning systems--remains underexplored in the context of LLM-based planning tasks. This study bridges this gap by comparing human trust in LLM-based planners with classical planners through a user study in a Planning Domain Definition Language (PDDL) domain. Combining subjective measures, such as trust questionnaires, with objective metrics like evaluation accuracy, our findings reveal that correctness is the primary driver of trust and performance. Explanations provided by the LLM improved evaluation accuracy but had limited impact on trust, while plan refinement showed potential for increasing trust without significantly enhancing evaluation accuracy.

### AI Will Always Love You: Studying Implicit Biases in Romantic AI Companions 
[[arxiv](https://arxiv.org/abs/2502.20231)] [[cool](https://papers.cool/arxiv/2502.20231)] [[pdf](https://arxiv.org/pdf/2502.20231)]
> **Authors**: Clare Grogan,Jackie Kay,María Pérez-Ortiz
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: While existing studies have recognised explicit biases in generative models, including occupational gender biases, the nuances of gender stereotypes and expectations of relationships between users and AI companions remain underexplored. In the meantime, AI companions have become increasingly popular as friends or gendered romantic partners to their users. This study bridges the gap by devising three experiments tailored for romantic, gender-assigned AI companions and their users, effectively evaluating implicit biases across various-sized LLMs. Each experiment looks at a different dimension: implicit associations, emotion responses, and sycophancy. This study aims to measure and compare biases manifested in different companion systems by quantitatively analysing persona-assigned model responses to a baseline through newly devised metrics. The results are noteworthy: they show that assigning gendered, relationship personas to Large Language Models significantly alters the responses of these models, and in certain situations in a biased, stereotypical way.

### An Extensive Evaluation of PDDL Capabilities in off-the-shelf LLMs 
[[arxiv](https://arxiv.org/abs/2502.20175)] [[cool](https://papers.cool/arxiv/2502.20175)] [[pdf](https://arxiv.org/pdf/2502.20175)]
> **Authors**: Kaustubh Vyas,Damien Graux,Sébastien Montella,Pavlos Vougiouklis,Ruofei Lai,Keshuang Li,Yang Ren,Jeff Z. Pan
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Under review
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: In recent advancements, large language models (LLMs) have exhibited proficiency in code generation and chain-of-thought reasoning, laying the groundwork for tackling automatic formal planning tasks. This study evaluates the potential of LLMs to understand and generate Planning Domain Definition Language (PDDL), an essential representation in artificial intelligence planning. We conduct an extensive analysis across 20 distinct models spanning 7 major LLM families, both commercial and open-source. Our comprehensive evaluation sheds light on the zero-shot LLM capabilities of parsing, generating, and reasoning with PDDL. Our findings indicate that while some models demonstrate notable effectiveness in handling PDDL, others pose limitations in more complex scenarios requiring nuanced planning knowledge. These results highlight the promise and current limitations of LLMs in formal planning tasks, offering insights into their application and guiding future efforts in AI-driven planning paradigms.

### Meta-Reasoner: Dynamic Guidance for Optimized Inference-time Reasoning in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.19918)] [[cool](https://papers.cool/arxiv/2502.19918)] [[pdf](https://arxiv.org/pdf/2502.19918)]
> **Authors**: Yuan Sui,Yufei He,Tri Cao,Simeng Han,Bryan Hooi
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Work in progress
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: Large Language Models (LLMs) increasingly rely on prolonged reasoning chains to solve complex tasks. However, this trial-and-error approach often leads to high computational overhead and error propagation, where early mistakes can derail subsequent steps. To address these issues, we introduce Meta-Reasoner, a framework that dynamically optimizes inference-time reasoning by enabling LLMs to "think about how to think." Drawing inspiration from human meta-cognition and dual-process theory, Meta-Reasoner operates as a strategic advisor, decoupling high-level guidance from step-by-step generation. It employs "contextual multi-armed bandits" to iteratively evaluate reasoning progress, and select optimal strategies (e.g., backtrack, clarify ambiguity, restart from scratch, or propose alternative approaches), and reallocates computational resources toward the most promising paths. Our evaluations on mathematical reasoning and puzzles highlight the potential of dynamic reasoning chains to overcome inherent challenges in the LLM reasoning process and also show promise in broader applications, offering a scalable and adaptable solution for reasoning-intensive tasks.

### LLM-driven Effective Knowledge Tracing by Integrating Dual-channel Difficulty 
[[arxiv](https://arxiv.org/abs/2502.19915)] [[cool](https://papers.cool/arxiv/2502.19915)] [[pdf](https://arxiv.org/pdf/2502.19915)]
> **Authors**: Jiahui Cen,Jianghao Lin,Weizhong Xuan,Dong Zhou,Jin Chen,Aimin Yang,Yongmei Zhou
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Knowledge Tracing (KT) is a fundamental technology in intelligent tutoring systems used to simulate changes in students' knowledge state during learning, track personalized knowledge mastery, and predict performance. However, current KT models face three major challenges: (1) When encountering new questions, models face cold-start problems due to sparse interaction records, making precise modeling difficult; (2) Traditional models only use historical interaction records for student personalization modeling, unable to accurately track individual mastery levels, resulting in unclear personalized modeling; (3) The decision-making process is opaque to educators, making it challenging for them to understand model judgments. To address these challenges, we propose a novel Dual-channel Difficulty-aware Knowledge Tracing (DDKT) framework that utilizes Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) for subjective difficulty assessment, while integrating difficulty bias-aware algorithms and student mastery algorithms for precise difficulty measurement. Our framework introduces three key innovations: (1) Difficulty Balance Perception Sequence (DBPS) - students' subjective perceptions combined with objective difficulty, measuring gaps between LLM-assessed difficulty, mathematical-statistical difficulty, and students' subjective perceived difficulty through attention mechanisms; (2) Difficulty Mastery Ratio (DMR) - precise modeling of student mastery levels through different difficulty zones; (3) Knowledge State Update Mechanism - implementing personalized knowledge acquisition through gated networks and updating student knowledge state. Experimental results on two real datasets show our method consistently outperforms nine baseline models, improving AUC metrics by 2% to 10% while effectively addressing cold-start problems and enhancing model interpretability.

### Optimus-2: Multimodal Minecraft Agent with Goal-Observation-Action Conditioned Policy 
[[arxiv](https://arxiv.org/abs/2502.19902)] [[cool](https://papers.cool/arxiv/2502.19902)] [[pdf](https://arxiv.org/pdf/2502.19902)]
> **Authors**: Zaijing Li,Yuquan Xie,Rui Shao,Gongwei Chen,Dongmei Jiang,Liqiang Nie
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Accept to CVPR 2025
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Building an agent that can mimic human behavior patterns to accomplish various open-world tasks is a long-term goal. To enable agents to effectively learn behavioral patterns across diverse tasks, a key challenge lies in modeling the intricate relationships among observations, actions, and language. To this end, we propose Optimus-2, a novel Minecraft agent that incorporates a Multimodal Large Language Model (MLLM) for high-level planning, alongside a Goal-Observation-Action Conditioned Policy (GOAP) for low-level control. GOAP contains (1) an Action-guided Behavior Encoder that models causal relationships between observations and actions at each timestep, then dynamically interacts with the historical observation-action sequence, consolidating it into fixed-length behavior tokens, and (2) an MLLM that aligns behavior tokens with open-ended language instructions to predict actions auto-regressively. Moreover, we introduce a high-quality Minecraft Goal-Observation-Action (MGOA)} dataset, which contains 25,000 videos across 8 atomic tasks, providing about 30M goal-observation-action pairs. The automated construction method, along with the MGOA dataset, can contribute to the community's efforts to train Minecraft agents. Extensive experimental results demonstrate that Optimus-2 exhibits superior performance across atomic tasks, long-horizon tasks, and open-ended instruction tasks in Minecraft.

### Developmental Support Approach to AI's Autonomous Growth: Toward the Realization of a Mutually Beneficial Stage Through Experiential Learning 
[[arxiv](https://arxiv.org/abs/2502.19798)] [[cool](https://papers.cool/arxiv/2502.19798)] [[pdf](https://arxiv.org/pdf/2502.19798)]
> **Authors**: Taichiro Endo
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 4pages, 3 figures
- **标题**: None
- **领域**: 人工智能
- **Abstract**: This study proposes an "AI Development Support" approach that, unlike conventional AI Alignment-which aims to forcefully inject human values-supports the ethical and moral development of AI itself. As demonstrated by the Orthogonality Thesis, the level of intelligence and the moral quality of a goal are independent; merely expanding knowledge does not enhance ethical judgment. Furthermore, to address the risk of Instrumental Convergence in ASI-that is, the tendency to engage in subsidiary behaviors such as self-protection, resource acquisition, and power reinforcement to achieve a goal-we have constructed a learning framework based on a cycle of experience, introspection, analysis, and hypothesis formation. As a result of post-training using Supervised Fine Tuning (SFT) and Direct Preference Optimization (DPO) with synthetic data generated by large language models (LLMs), responses demonstrating cooperative and highly advanced moral judgment (reaching the high-est Stage 6) were obtained even under adversarial prompts. This method represents a promising implementation approach for enabling AI to establish sustainable, symbiotic relationships.

## 计算工程、金融和科学(cs.CE:Computational Engineering, Finance, and Science)

### ChatMol: A Versatile Molecule Designer Based on the Numerically Enhanced Large Language Model 
[[arxiv](https://arxiv.org/abs/2502.19794)] [[cool](https://papers.cool/arxiv/2502.19794)] [[pdf](https://arxiv.org/pdf/2502.19794)]
> **Authors**: Chuanliu Fan,Ziqiang Cao,Zicheng Ma,Nan Yu,Yimin Peng,Jun Zhang,Yiqin Gao,Guohong Fu
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 16 pages, 8 figures,conference
- **标题**: None
- **领域**: 计算工程、金融和科学,机器学习
- **Abstract**: Goal-oriented de novo molecule design, namely generating molecules with specific property or substructure constraints, is a crucial yet challenging task in drug discovery. Existing methods, such as Bayesian optimization and reinforcement learning, often require training multiple property predictors and struggle to incorporate substructure constraints. Inspired by the success of Large Language Models (LLMs) in text generation, we propose ChatMol, a novel approach that leverages LLMs for molecule design across diverse constraint settings. Initially, we crafted a molecule representation compatible with LLMs and validated its efficacy across multiple online LLMs. Afterwards, we developed specific prompts geared towards diverse constrained molecule generation tasks to further fine-tune current LLMs while integrating feedback learning derived from property prediction. Finally, to address the limitations of LLMs in numerical recognition, we referred to the position encoding method and incorporated additional encoding for numerical values within the prompt. Experimental results across single-property, substructure-property, and multi-property constrained tasks demonstrate that ChatMol consistently outperforms state-of-the-art baselines, including VAE and RL-based methods. Notably, in multi-objective binding affinity maximization task, ChatMol achieves a significantly lower KD value of 0.25 for the protein target ESR1, while maintaining the highest overall performance, surpassing previous methods by 4.76%. Meanwhile, with numerical enhancement, the Pearson correlation coefficient between the instructed property values and those of the generated molecules increased by up to 0.49. These findings highlight the potential of LLMs as a versatile framework for molecule generation, offering a promising alternative to traditional latent space and RL-based approaches.

## 计算几何(cs.CG:Computational Geometry)

### Topological Autoencoders++: Fast and Accurate Cycle-Aware Dimensionality Reduction 
[[arxiv](https://arxiv.org/abs/2502.20215)] [[cool](https://papers.cool/arxiv/2502.20215)] [[pdf](https://arxiv.org/pdf/2502.20215)]
> **Authors**: Mattéo Clémot,Julie Digne,Julien Tierny
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算几何,图形,机器学习
- **Abstract**: This paper presents a novel topology-aware dimensionality reduction approach aiming at accurately visualizing the cyclic patterns present in high dimensional data. To that end, we build on the Topological Autoencoders (TopoAE) formulation. First, we provide a novel theoretical analysis of its associated loss and show that a zero loss indeed induces identical persistence pairs (in high and low dimensions) for the $0$-dimensional persistent homology (PH$^0$) of the Rips filtration. We also provide a counter example showing that this property no longer holds for a naive extension of TopoAE to PH$^d$ for $d\ge 1$. Based on this observation, we introduce a novel generalization of TopoAE to $1$-dimensional persistent homology (PH$^1$), called TopoAE++, for the accurate generation of cycle-aware planar embeddings, addressing the above failure case. This generalization is based on the notion of cascade distortion, a new penalty term favoring an isometric embedding of the $2$-chains filling persistent $1$-cycles, hence resulting in more faithful geometrical reconstructions of the $1$-cycles in the plane. We further introduce a novel, fast algorithm for the exact computation of PH for Rips filtrations in the plane, yielding improved runtimes over previously documented topology-aware methods. Our method also achieves a better balance between the topological accuracy, as measured by the Wasserstein distance, and the visual preservation of the cycles in low dimensions. Our C++ implementation is available at https://github.com/MClemot/TopologicalAutoencodersPlusPlus.

## 计算语言学(cs.CL:Computation and Language)

### Bridging Legal Knowledge and AI: Retrieval-Augmented Generation with Vector Stores, Knowledge Graphs, and Hierarchical Non-negative Matrix Factorization 
[[arxiv](https://arxiv.org/abs/2502.20364)] [[cool](https://papers.cool/arxiv/2502.20364)] [[pdf](https://arxiv.org/pdf/2502.20364)]
> **Authors**: Ryan C. Barron,Maksim E. Eren,Olga M. Serafimova,Cynthia Matuszek,Boian S. Alexandrov
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 10 pages, 6 figures, 5 tables
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Agentic Generative AI, powered by Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG), Knowledge Graphs (KGs), and Vector Stores (VSs), represents a transformative technology applicable to specialized domains such as legal systems, research, recommender systems, cybersecurity, and global security, including proliferation research. This technology excels at inferring relationships within vast unstructured or semi-structured datasets. The legal domain here comprises complex data characterized by extensive, interrelated, and semi-structured knowledge systems with complex relations. It comprises constitutions, statutes, regulations, and case law. Extracting insights and navigating the intricate networks of legal documents and their relations is crucial for effective legal research. Here, we introduce a generative AI system that integrates RAG, VS, and KG, constructed via Non-Negative Matrix Factorization (NMF), to enhance legal information retrieval and AI reasoning and minimize hallucinations. In the legal system, these technologies empower AI agents to identify and analyze complex connections among cases, statutes, and legal precedents, uncovering hidden relationships and predicting legal trends-challenging tasks that are essential for ensuring justice and improving operational efficiency. Our system employs web scraping techniques to systematically collect legal texts, such as statutes, constitutional provisions, and case law, from publicly accessible platforms like Justia. It bridges the gap between traditional keyword-based searches and contextual understanding by leveraging advanced semantic representations, hierarchical relationships, and latent topic discovery. This framework supports legal document clustering, summarization, and cross-referencing, for scalable, interpretable, and accurate retrieval for semi-structured data while advancing computational law and AI.

### Bridging the Creativity Understanding Gap: Small-Scale Human Alignment Enables Expert-Level Humor Ranking in LLMs 
[[arxiv](https://arxiv.org/abs/2502.20356)] [[cool](https://papers.cool/arxiv/2502.20356)] [[pdf](https://arxiv.org/pdf/2502.20356)]
> **Authors**: Kuan Lok Zhou,Jiayi Chen,Siddharth Suresh,Reuben Narad,Timothy T. Rogers,Lalit K Jain,Robert D Nowak,Bob Mankoff,Jifan Zhang
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Large Language Models (LLMs) have shown significant limitations in understanding creative content, as demonstrated by Hessel et al. (2023)'s influential work on the New Yorker Cartoon Caption Contest (NYCCC). Their study exposed a substantial gap between LLMs and humans in humor comprehension, establishing that understanding and evaluating creative content is key challenge in AI development. We revisit this challenge by decomposing humor understanding into three components and systematically improve each: enhancing visual understanding through improved annotation, utilizing LLM-generated humor reasoning and explanations, and implementing targeted alignment with human preference data. Our refined approach achieves 82.4% accuracy in caption ranking, singificantly improving upon the previous 67% benchmark and matching the performance of world-renowned human experts in this domain. Notably, while attempts to mimic subgroup preferences through various persona prompts showed minimal impact, model finetuning with crowd preferences proved remarkably effective. These findings reveal that LLM limitations in creative judgment can be effectively addressed through focused alignment to specific subgroups and individuals. Lastly, we propose the position that achieving artificial general intelligence necessitates systematic collection of human preference data across creative domains. We advocate that just as human creativity is deeply influenced by individual and cultural preferences, training LLMs with diverse human preference data may be essential for developing true creative understanding.

### KEDRec-LM: A Knowledge-distilled Explainable Drug Recommendation Large Language Model 
[[arxiv](https://arxiv.org/abs/2502.20350)] [[cool](https://papers.cool/arxiv/2502.20350)] [[pdf](https://arxiv.org/pdf/2502.20350)]
> **Authors**: Kai Zhang,Rui Zhu,Shutian Ma,Jingwei Xiong,Yejin Kim,Fabricio Murai,Xiaozhong Liu
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Drug discovery is a critical task in biomedical natural language processing (NLP), yet explainable drug discovery remains underexplored. Meanwhile, large language models (LLMs) have shown remarkable abilities in natural language understanding and generation. Leveraging LLMs for explainable drug discovery has the potential to improve downstream tasks and real-world applications. In this study, we utilize open-source drug knowledge graphs, clinical trial data, and PubMed publications to construct a comprehensive dataset for the explainable drug discovery task, named \textbf{expRxRec}. Furthermore, we introduce \textbf{KEDRec-LM}, an instruction-tuned LLM which distills knowledge from rich medical knowledge corpus for drug recommendation and rationale generation. To encourage further research in this area, we will publicly release\footnote{A copy is attached with this submission} both the dataset and KEDRec-LM.

### Sparse Auto-Encoder Interprets Linguistic Features in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.20344)] [[cool](https://papers.cool/arxiv/2502.20344)] [[pdf](https://arxiv.org/pdf/2502.20344)]
> **Authors**: Yi Jing,Zijun Yao,Lingxu Ran,Hongzhu Guo,Xiaozhi Wang,Lei Hou,Juanzi Li
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) excel in tasks that require complex linguistic abilities, such as reference disambiguation and metaphor recognition/generation. Although LLMs possess impressive capabilities, their internal mechanisms for processing and representing linguistic knowledge remain largely opaque. Previous work on linguistic mechanisms has been limited by coarse granularity, insufficient causal analysis, and a narrow focus. In this study, we present a systematic and comprehensive causal investigation using sparse auto-encoders (SAEs). We extract a wide range of linguistic features from six dimensions: phonetics, phonology, morphology, syntax, semantics, and pragmatics. We extract, evaluate, and intervene on these features by constructing minimal contrast datasets and counterfactual sentence datasets. We introduce two indices-Feature Representation Confidence (FRC) and Feature Intervention Confidence (FIC)-to measure the ability of linguistic features to capture and control linguistic phenomena. Our results reveal inherent representations of linguistic knowledge in LLMs and demonstrate the potential for controlling model outputs. This work provides strong evidence that LLMs possess genuine linguistic knowledge and lays the foundation for more interpretable and controllable language modeling in future research.

### Thinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners 
[[arxiv](https://arxiv.org/abs/2502.20339)] [[cool](https://papers.cool/arxiv/2502.20339)] [[pdf](https://arxiv.org/pdf/2502.20339)]
> **Authors**: Daniele Paliotta,Junxiong Wang,Matteo Pagliardini,Kevin Y. Li,Aviv Bick,J. Zico Kolter,Albert Gu,François Fleuret,Tri Dao
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Recent advancements have demonstrated that the performance of large language models (LLMs) can be significantly enhanced by scaling computational resources at test time. A common strategy involves generating multiple Chain-of-Thought (CoT) trajectories and aggregating their outputs through various selection mechanisms. This raises a fundamental question: can models with lower complexity leverage their superior generation throughput to outperform similarly sized Transformers for a fixed computational budget? To address this question and overcome the lack of strong subquadratic reasoners, we distill pure and hybrid Mamba models from pretrained Transformers. Trained on only 8 billion tokens, our distilled models show strong performance and scaling on mathematical reasoning datasets while being much faster at inference for large batches and long sequences. Despite the zero-shot performance hit due to distillation, both pure and hybrid Mamba models can scale their coverage and accuracy performance past their Transformer teacher models under fixed time budgets, opening a new direction for scaling inference compute.

### Expertise Is What We Want 
[[arxiv](https://arxiv.org/abs/2502.20335)] [[cool](https://papers.cool/arxiv/2502.20335)] [[pdf](https://arxiv.org/pdf/2502.20335)]
> **Authors**: Alan Ashworth,Munir Al-Dajani,Keegan Duchicela,Kiril Kafadarov,Allison Kurian,Othman Laraki,Amina Lazrak,Divneet Mandair,Wendy McKennon,Rebecca Miksad,Jayodita Sanghvi,Travis Zack
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 18 pages, 7 figures, 5 tables
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Clinical decision-making depends on expert reasoning, which is guided by standardized, evidence-based guidelines. However, translating these guidelines into automated clinical decision support systems risks inaccuracy and importantly, loss of nuance. We share an application architecture, the Large Language Expert (LLE), that combines the flexibility and power of Large Language Models (LLMs) with the interpretability, explainability, and reliability of Expert Systems. LLMs help address key challenges of Expert Systems, such as integrating and codifying knowledge, and data normalization. Conversely, an Expert System-like approach helps overcome challenges with LLMs, including hallucinations, atomic and inexpensive updates, and testability. To highlight the power of the Large Language Expert (LLE) system, we built an LLE to assist with the workup of patients newly diagnosed with cancer. Timely initiation of cancer treatment is critical for optimal patient outcomes. However, increasing complexity in diagnostic recommendations has made it difficult for primary care physicians to ensure their patients have completed the necessary workup before their first visit with an oncologist. As with many real-world clinical tasks, these workups require the analysis of unstructured health records and the application of nuanced clinical decision logic. In this study, we describe the design & evaluation of an LLE system built to rapidly identify and suggest the correct diagnostic workup. The system demonstrated a high degree of clinical-level accuracy (>95%) and effectively addressed gaps identified in real-world data from breast and colon cancer patients at a large academic center.

### Emergent Symbolic Mechanisms Support Abstract Reasoning in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.20332)] [[cool](https://papers.cool/arxiv/2502.20332)] [[pdf](https://arxiv.org/pdf/2502.20332)]
> **Authors**: Yukang Yang,Declan Campbell,Kaixuan Huang,Mengdi Wang,Jonathan Cohen,Taylor Webb
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Many recent studies have found evidence for emergent reasoning capabilities in large language models, but debate persists concerning the robustness of these capabilities, and the extent to which they depend on structured reasoning mechanisms. To shed light on these issues, we perform a comprehensive study of the internal mechanisms that support abstract rule induction in an open-source language model (Llama3-70B). We identify an emergent symbolic architecture that implements abstract reasoning via a series of three computations. In early layers, symbol abstraction heads convert input tokens to abstract variables based on the relations between those tokens. In intermediate layers, symbolic induction heads perform sequence induction over these abstract variables. Finally, in later layers, retrieval heads predict the next token by retrieving the value associated with the predicted abstract variable. These results point toward a resolution of the longstanding debate between symbolic and neural network approaches, suggesting that emergent reasoning in neural networks depends on the emergence of symbolic mechanisms.

### Long-Context Inference with Retrieval-Augmented Speculative Decoding 
[[arxiv](https://arxiv.org/abs/2502.20330)] [[cool](https://papers.cool/arxiv/2502.20330)] [[pdf](https://arxiv.org/pdf/2502.20330)]
> **Authors**: Guanzheng Chen,Qilong Feng,Jinjie Ni,Xin Li,Michael Qizhe Shieh
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The emergence of long-context large language models (LLMs) offers a promising alternative to traditional retrieval-augmented generation (RAG) for processing extensive documents. However, the computational overhead of long-context inference, particularly in managing key-value (KV) caches, presents significant efficiency challenges. While Speculative Decoding (SD) traditionally accelerates inference using smaller draft models, its effectiveness diminishes substantially in long-context scenarios due to memory-bound KV cache operations. We present Retrieval-Augmented Speculative Decoding (RAPID), which leverages RAG for both accelerating and enhancing generation quality in long-context inference. RAPID introduces the RAG drafter-a draft LLM operating on shortened retrieval contexts-to speculate on the generation of long-context target LLMs. Our approach enables a new paradigm where same-scale or even larger LLMs can serve as RAG drafters while maintaining computational efficiency. To fully leverage the potentially superior capabilities from stronger RAG drafters, we develop an inference-time knowledge transfer dynamic that enriches the target distribution by RAG. Extensive experiments on the LLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates the strengths of both approaches, achieving significant performance improvements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with more than 2x speedups. Our analyses reveal that RAPID achieves robust acceleration beyond 32K context length and demonstrates superior generation quality in real-world applications.

### LangProBe: a Language Programs Benchmark 
[[arxiv](https://arxiv.org/abs/2502.20315)] [[cool](https://papers.cool/arxiv/2502.20315)] [[pdf](https://arxiv.org/pdf/2502.20315)]
> **Authors**: Shangyin Tan,Lakshya A Agrawal,Arnav Singhvi,Liheng Lai,Michael J Ryan,Dan Klein,Omar Khattab,Koushik Sen,Matei Zaharia
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,信息检索,机器学习
- **Abstract**: Composing language models (LMs) into multi-step language programs and automatically optimizing their modular prompts is now a mainstream paradigm for building AI systems, but the tradeoffs in this space have only scarcely been studied before. We introduce LangProBe, the first large-scale benchmark for evaluating the architectures and optimization strategies for language programs, with over 2000 combinations of tasks, architectures, optimizers, and choices of LMs. Using LangProBe, we are the first to study the impact of program architectures and optimizers (and their compositions together and with different models) on tradeoffs of quality and cost. We find that optimized language programs offer strong cost--quality Pareto improvement over raw calls to models, but simultaneously demonstrate that human judgment (or empirical decisions) about which compositions to pursue is still necessary for best performance. We will open source the code and evaluation data for LangProBe.

### LLM as a Broken Telephone: Iterative Generation Distorts Information 
[[arxiv](https://arxiv.org/abs/2502.20258)] [[cool](https://papers.cool/arxiv/2502.20258)] [[pdf](https://arxiv.org/pdf/2502.20258)]
> **Authors**: Amr Mohamed,Mingmeng Geng,Michalis Vazirgiannis,Guokan Shang
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: As large language models are increasingly responsible for online content, concerns arise about the impact of repeatedly processing their own outputs. Inspired by the "broken telephone" effect in chained human communication, this study investigates whether LLMs similarly distort information through iterative generation. Through translation-based experiments, we find that distortion accumulates over time, influenced by language choice and chain complexity. While degradation is inevitable, it can be mitigated through strategic prompting techniques. These findings contribute to discussions on the long-term effects of AI-mediated information propagation, raising important questions about the reliability of LLM-generated content in iterative workflows.

### Beyond Natural Language Perplexity: Detecting Dead Code Poisoning in Code Generation Datasets 
[[arxiv](https://arxiv.org/abs/2502.20246)] [[cool](https://papers.cool/arxiv/2502.20246)] [[pdf](https://arxiv.org/pdf/2502.20246)]
> **Authors**: Chichien Tsai,Chiamu Yu,Yingdar Lin,Yusung Wu,Weibin Lee
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The increasing adoption of large language models (LLMs) for code-related tasks has raised concerns about the security of their training datasets. One critical threat is dead code poisoning, where syntactically valid but functionally redundant code is injected into training data to manipulate model behavior. Such attacks can degrade the performance of neural code search systems, leading to biased or insecure code suggestions. Existing detection methods, such as token-level perplexity analysis, fail to effectively identify dead code due to the structural and contextual characteristics of programming languages. In this paper, we propose DePA (Dead Code Perplexity Analysis), a novel line-level detection and cleansing method tailored to the structural properties of code. DePA computes line-level perplexity by leveraging the contextual relationships between code lines and identifies anomalous lines by comparing their perplexity to the overall distribution within the file. Our experiments on benchmark datasets demonstrate that DePA significantly outperforms existing methods, achieving 0.14-0.19 improvement in detection F1-score and a 44-65% increase in poisoned segment localization precision. Furthermore, DePA enhances detection speed by 0.62-23x, making it practical for large-scale dataset cleansing. Overall, by addressing the unique challenges of dead code poisoning, DePA provides a robust and efficient solution for safeguarding the integrity of code generation model training datasets.

### From Retrieval to Generation: Comparing Different Approaches 
[[arxiv](https://arxiv.org/abs/2502.20245)] [[cool](https://papers.cool/arxiv/2502.20245)] [[pdf](https://arxiv.org/pdf/2502.20245)]
> **Authors**: Abdelrahman Abdallah,Jamshid Mozafari,Bhawna Piryani,Mohammed Ali,Adam Jatowt
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: work on progress
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Knowledge-intensive tasks, particularly open-domain question answering (ODQA), document reranking, and retrieval-augmented language modeling, require a balance between retrieval accuracy and generative flexibility. Traditional retrieval models such as BM25 and Dense Passage Retrieval (DPR), efficiently retrieve from large corpora but often lack semantic depth. Generative models like GPT-4-o provide richer contextual understanding but face challenges in maintaining factual consistency. In this work, we conduct a systematic evaluation of retrieval-based, generation-based, and hybrid models, with a primary focus on their performance in ODQA and related retrieval-augmented tasks. Our results show that dense retrievers, particularly DPR, achieve strong performance in ODQA with a top-1 accuracy of 50.17\% on NQ, while hybrid models improve nDCG@10 scores on BEIR from 43.42 (BM25) to 52.59, demonstrating their strength in document reranking. Additionally, we analyze language modeling tasks using WikiText-103, showing that retrieval-based approaches like BM25 achieve lower perplexity compared to generative and hybrid methods, highlighting their utility in retrieval-augmented generation. By providing detailed comparisons and practical insights into the conditions where each approach excels, we aim to facilitate future optimizations in retrieval, reranking, and generative models for ODQA and related knowledge-intensive applications.

### FINEREASON: Evaluating and Improving LLMs' Deliberate Reasoning through Reflective Puzzle Solving 
[[arxiv](https://arxiv.org/abs/2502.20238)] [[cool](https://papers.cool/arxiv/2502.20238)] [[pdf](https://arxiv.org/pdf/2502.20238)]
> **Authors**: Guizhen Chen,Weiwen Xu,Hao Zhang,Hou Pong Chan,Chaoqun Liu,Lidong Bing,Deli Zhao,Anh Tuan Luu,Yu Rong
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Many challenging reasoning tasks require not just rapid, intuitive responses, but a more deliberate, multi-step approach. Recent progress in large language models (LLMs) highlights an important shift from the "System 1" way of quick reactions to the "System 2" style of reflection-and-correction problem solving. However, current benchmarks heavily rely on the final-answer accuracy, leaving much of a model's intermediate reasoning steps unexamined. This fails to assess the model's ability to reflect and rectify mistakes within the reasoning process. To bridge this gap, we introduce FINEREASON, a logic-puzzle benchmark for fine-grained evaluation of LLMs' reasoning capabilities. Each puzzle can be decomposed into atomic steps, making it ideal for rigorous validation of intermediate correctness. Building on this, we introduce two tasks: state checking, and state transition, for a comprehensive evaluation of how models assess the current situation and plan the next move. To support broader research, we also provide a puzzle training set aimed at enhancing performance on general mathematical tasks. We show that models trained on our state checking and transition data demonstrate gains in math reasoning by up to 5.1% on GSM8K.

### ChineseEcomQA: A Scalable E-commerce Concept Evaluation Benchmark for Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.20196)] [[cool](https://papers.cool/arxiv/2502.20196)] [[pdf](https://arxiv.org/pdf/2502.20196)]
> **Authors**: Haibin Chen,Kangtao Lv,Chengwei Hu,Yanshi Li,Yujin Yuan,Yancheng He,Xingyao Zhang,Langming Liu,Shilei Liu,Wenbo Su,Bo Zheng
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: With the increasing use of Large Language Models (LLMs) in fields such as e-commerce, domain-specific concept evaluation benchmarks are crucial for assessing their domain capabilities. Existing LLMs may generate factually incorrect information within the complex e-commerce applications. Therefore, it is necessary to build an e-commerce concept benchmark. Existing benchmarks encounter two primary challenges: (1) handle the heterogeneous and diverse nature of tasks, (2) distinguish between generality and specificity within the e-commerce field. To address these problems, we propose \textbf{ChineseEcomQA}, a scalable question-answering benchmark focused on fundamental e-commerce concepts. ChineseEcomQA is built on three core characteristics: \textbf{Focus on Fundamental Concept}, \textbf{E-commerce Generality} and \textbf{E-commerce Expertise}. Fundamental concepts are designed to be applicable across a diverse array of e-commerce tasks, thus addressing the challenge of heterogeneity and diversity. Additionally, by carefully balancing generality and specificity, ChineseEcomQA effectively differentiates between broad e-commerce concepts, allowing for precise validation of domain capabilities. We achieve this through a scalable benchmark construction process that combines LLM validation, Retrieval-Augmented Generation (RAG) validation, and rigorous manual annotation. Based on ChineseEcomQA, we conduct extensive evaluations on mainstream LLMs and provide some valuable insights. We hope that ChineseEcomQA could guide future domain-specific evaluations, and facilitate broader LLM adoption in e-commerce applications.

### Layer-Aware Task Arithmetic: Disentangling Task-Specific and Instruction-Following Knowledge 
[[arxiv](https://arxiv.org/abs/2502.20186)] [[cool](https://papers.cool/arxiv/2502.20186)] [[pdf](https://arxiv.org/pdf/2502.20186)]
> **Authors**: Yan-Lun Chen,Yi-Ru Wei,Chia-Yi Hsu,Chia-Mu Yu,Chun-Ying Huang,Ying-Dar Lin,Yu-Sung Wu,Wei-Bin Lee
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Large language models (LLMs) demonstrate strong task-specific capabilities through fine-tuning, but merging multiple fine-tuned models often leads to degraded performance due to overlapping instruction-following components. Task Arithmetic (TA), which combines task vectors derived from fine-tuning, enables multi-task learning and task forgetting but struggles to isolate task-specific knowledge from general instruction-following behavior. To address this, we propose Layer-Aware Task Arithmetic (LATA), a novel approach that assigns layer-specific weights to task vectors based on their alignment with instruction-following or task-specific components. By amplifying task-relevant layers and attenuating instruction-following layers, LATA improves task learning and forgetting performance while preserving overall model utility. Experiments on multiple benchmarks, including WikiText-2, GSM8K, and HumanEval, demonstrate that LATA outperforms existing methods in both multi-task learning and selective task forgetting, achieving higher task accuracy and alignment with minimal degradation in output quality. Our findings highlight the importance of layer-wise analysis in disentangling task-specific and general-purpose knowledge, offering a robust framework for efficient model merging and editing.

### Representing Signs as Signs: One-Shot ISLR to Facilitate Functional Sign Language Technologies 
[[arxiv](https://arxiv.org/abs/2502.20171)] [[cool](https://papers.cool/arxiv/2502.20171)] [[pdf](https://arxiv.org/pdf/2502.20171)]
> **Authors**: Toon Vandendriessche,Mathieu De Coster,Annelies Lejon,Joni Dambre
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,计算机视觉和模式识别
- **Abstract**: Isolated Sign Language Recognition (ISLR) is crucial for scalable sign language technology, yet language-specific approaches limit current models. To address this, we propose a one-shot learning approach that generalises across languages and evolving vocabularies. Our method involves pretraining a model to embed signs based on essential features and using a dense vector search for rapid, accurate recognition of unseen signs. We achieve state-of-the-art results, including 50.8% one-shot MRR on a large dictionary containing 10,235 unique signs from a different language than the training set. Our approach is robust across languages and support sets, offering a scalable, adaptable solution for ISLR. Co-created with the Deaf and Hard of Hearing (DHH) community, this method aligns with real-world needs, and advances scalable sign language recognition.

### Finite State Automata Inside Transformers with Chain-of-Thought: A Mechanistic Study on State Tracking 
[[arxiv](https://arxiv.org/abs/2502.20129)] [[cool](https://papers.cool/arxiv/2502.20129)] [[pdf](https://arxiv.org/pdf/2502.20129)]
> **Authors**: Yifan Zhang,Wenyu Du,Dongming Jin,Jie Fu,Zhi Jin
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Chain-of-Thought (CoT) significantly enhances the performance of large language models (LLMs) across a wide range of tasks, and prior research shows that CoT can theoretically increase expressiveness. However, there is limited mechanistic understanding of the algorithms that Transformer+CoT can learn. In this work, we (1) evaluate the state tracking capabilities of Transformer+CoT and its variants, confirming the effectiveness of CoT. (2) Next, we identify the circuit, a subset of model components, responsible for tracking the world state, finding that late-layer MLP neurons play a key role. We propose two metrics, compression and distinction, and show that the neuron sets for each state achieve nearly 100% accuracy, providing evidence of an implicit finite state automaton (FSA) embedded within the model. (3) Additionally, we explore three realistic settings: skipping intermediate steps, introducing data noise, and testing length generalization. Our results demonstrate that Transformer+CoT learns robust algorithms (FSA), highlighting its resilience in challenging scenarios.

### Self-Training Elicits Concise Reasoning in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.20122)] [[cool](https://papers.cool/arxiv/2502.20122)] [[pdf](https://arxiv.org/pdf/2502.20122)]
> **Authors**: Tergel Munkhbat,Namgyu Ho,Seohyun Kim,Yongjin Yang,Yujin Kim,Se-Young Yun
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 23 pages, 10 figures, 18 tables
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Chain-of-thought (CoT) reasoning has enabled large language models (LLMs) to utilize additional computation through intermediate tokens to solve complex tasks. However, we posit that typical reasoning traces contain many redundant tokens, incurring extraneous inference costs. Upon examination of the output distribution of current LLMs, we find evidence on their latent ability to reason more concisely, relative to their default behavior. To elicit this capability, we propose simple fine-tuning methods which leverage self-generated concise reasoning paths obtained by best-of-N sampling and few-shot conditioning, in task-specific settings. Our combined method achieves a 30% reduction in output tokens on average, across five model families on GSM8K and MATH, while maintaining average accuracy. By exploiting the fundamental stochasticity and in-context learning capabilities of LLMs, our self-training approach robustly elicits concise reasoning on a wide range of models, including those with extensive post-training. Code is available at https://github.com/TergelMunkhbat/concise-reasoning

### LongRoPE2: Near-Lossless LLM Context Window Scaling 
[[arxiv](https://arxiv.org/abs/2502.20082)] [[cool](https://papers.cool/arxiv/2502.20082)] [[pdf](https://arxiv.org/pdf/2502.20082)]
> **Authors**: Ning Shang,Li Lyna Zhang,Siyuan Wang,Gaokai Zhang,Gilsinia Lopez,Fan Yang,Weizhu Chen,Mao Yang
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: LongRoPE2 is a novel approach that extends the effective context window of pre-trained large language models (LLMs) to the target length, while preserving the performance on the original shorter context window. This is achieved by three contributions: (1) a hypothesis that insufficient training in higher RoPE dimensions contributes to the persistent out-of-distribution (OOD) issues observed in existing methods; (2) an effective RoPE rescaling algorithm that adopts evolutionary search guided by "needle-driven" perplexity to address the insufficient training problem; (3) a mixed context window training approach that fine-tunes model weights to adopt rescaled RoPE for long-context sequences while preserving the short-context performance with the original RoPE. Extensive experiments on LLaMA3-8B and Phi3-mini-3.8B across various benchmarks validate the hypothesis and demonstrate the effectiveness of LongRoPE2. Remarkably, LongRoPE2 extends LLaMA3-8B to achieve a 128K effective context length while retaining over 98.5% of short-context performance, using only 10B tokens -- 80x fewer than Meta's approach, which fails to reach the target effective context length. Code will be available at https://github.com/microsoft/LongRoPE.

### Collab-Overcooked: Benchmarking and Evaluating Large Language Models as Collaborative Agents 
[[arxiv](https://arxiv.org/abs/2502.20073)] [[cool](https://papers.cool/arxiv/2502.20073)] [[pdf](https://arxiv.org/pdf/2502.20073)]
> **Authors**: Haochen Sun,Shuwen Zhang,Lei Ren,Hao Xu,Hao Fu,Caixia Yuan,Xiaojie Wang
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 25 pages, 14 figures
- **标题**: None
- **领域**: 计算语言学,人工智能,多代理系统
- **Abstract**: Large language models (LLMs) based agent systems have made great strides in real-world applications beyond traditional NLP tasks. This paper proposes a new LLM-powered Multi-Agent System (LLM-MAS) benchmark, Collab-Overcooked, built on the popular Overcooked-AI game with more applicable and challenging tasks in interactive environments. Collab-Overcooked extends existing benchmarks from two novel perspectives. First, it provides a multi-agent framework supporting diverse tasks and objectives and encourages collaboration through natural language communication. Second, it introduces a spectrum of process-oriented evaluation metrics to assess the fine-grained collaboration capabilities of different LLM agents, a dimension often overlooked in prior work. We conduct extensive experiments over 10 popular LLMs and show that, while the LLMs present a strong ability in goal interpretation, there is a significant discrepancy in active collaboration and continuous adaption that are critical for efficiently fulfilling complicated tasks. Notably, we highlight the strengths and weaknesses in LLM-MAS and provide insights for improving and evaluating LLM-MAS on a unified and open-sourced benchmark. Environments, 30 open-ended tasks, and an integrated evaluation package are now publicly available at https://github.com/YusaeMeow/Collab-Overcooked.

### Connecting the Persian-speaking World through Transliteration 
[[arxiv](https://arxiv.org/abs/2502.20047)] [[cool](https://papers.cool/arxiv/2502.20047)] [[pdf](https://arxiv.org/pdf/2502.20047)]
> **Authors**: Rayyan Merchant,Akhilesh Kakolu Ramarao,Kevin Tang
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Despite speaking mutually intelligible varieties of the same language, speakers of Tajik Persian, written in a modified Cyrillic alphabet, cannot read Iranian and Afghan texts written in the Perso-Arabic script. As the vast majority of Persian text on the Internet is written in Perso-Arabic, monolingual Tajik speakers are unable to interface with the Internet in any meaningful way. Due to overwhelming similarity between the formal registers of these dialects and the scarcity of Tajik-Farsi parallel data, machine transliteration has been proposed as more a practical and appropriate solution than machine translation. This paper presents a transformer-based G2P approach to Tajik-Farsi transliteration, achieving chrF++ scores of 58.70 (Farsi to Tajik) and 74.20 (Tajik to Farsi) on novel digraphic datasets, setting a comparable baseline metric for future work. Our results also demonstrate the non-trivial difficulty of this task in both directions. We also provide an overview of the differences between the two scripts and the challenges they present, so as to aid future efforts in Tajik-Farsi transliteration.

### Polish-ASTE: Aspect-Sentiment Triplet Extraction Datasets for Polish 
[[arxiv](https://arxiv.org/abs/2502.20046)] [[cool](https://papers.cool/arxiv/2502.20046)] [[pdf](https://arxiv.org/pdf/2502.20046)]
> **Authors**: Marta Lango,Borys Naglik,Mateusz Lango,Iwo Naglik
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Aspect-Sentiment Triplet Extraction (ASTE) is one of the most challenging and complex tasks in sentiment analysis. It concerns the construction of triplets that contain an aspect, its associated sentiment polarity, and an opinion phrase that serves as a rationale for the assigned polarity. Despite the growing popularity of the task and the many machine learning methods being proposed to address it, the number of datasets for ASTE is very limited. In particular, no dataset is available for any of the Slavic languages. In this paper, we present two new datasets for ASTE containing customer opinions about hotels and purchased products expressed in Polish. We also perform experiments with two ASTE techniques combined with two large language models for Polish to investigate their performance and the difficulty of the assembled datasets. The new datasets are available under a permissive licence and have the same file format as the English datasets, facilitating their use in future research.

### Erasing Without Remembering: Safeguarding Knowledge Forgetting in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.19982)] [[cool](https://papers.cool/arxiv/2502.19982)] [[pdf](https://arxiv.org/pdf/2502.19982)]
> **Authors**: Huazheng Wang,Yongcheng Jing,Haifeng Sun,Yingjie Wang,Jingyu Wang,Jianxin Liao,Dacheng Tao
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: In this paper, we explore machine unlearning from a novel dimension, by studying how to safeguard model unlearning in large language models (LLMs). Our goal is to prevent unlearned models from recalling any related memory of the targeted knowledge.We begin by uncovering a surprisingly simple yet overlooked fact: existing methods typically erase only the exact expressions of the targeted knowledge, leaving paraphrased or related information intact. To rigorously measure such oversights, we introduce UGBench, the first benchmark tailored for evaluating the generalisation performance across 13 state-of-the-art methods.UGBench reveals that unlearned models can still recall paraphrased answers and retain target facts in intermediate layers. To address this, we propose PERMU, a perturbation-based method that significantly enhances the generalisation capabilities for safeguarding LLM unlearning.Experiments demonstrate that PERMU delivers up to a 50.13% improvement in unlearning while maintaining a 43.53% boost in robust generalisation. Our code can be found in https://github.com/MaybeLizzy/UGBench.

### The Lookahead Limitation: Why Multi-Operand Addition is Hard for LLMs 
[[arxiv](https://arxiv.org/abs/2502.19981)] [[cool](https://papers.cool/arxiv/2502.19981)] [[pdf](https://arxiv.org/pdf/2502.19981)]
> **Authors**: Tanja Baeumel,Josef van Genabith,Simon Ostermann
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Pre-print
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Autoregressive large language models (LLMs) exhibit impressive performance across various tasks but struggle with simple arithmetic, such as addition of two or more operands. We show that this struggle arises from LLMs' use of a simple one-digit lookahead heuristic, which works fairly well (but not perfect) for two-operand addition but fails in multi-operand cases, where the carry-over logic is more complex. Our probing experiments and digit-wise accuracy evaluation show that LLMs fail precisely where a one-digit lookahead is insufficient to account for cascading carries. We analyze the impact of tokenization strategies on arithmetic performance and show that all investigated models, regardless of tokenization, are inherently limited in the addition of multiple operands due to their reliance on a one-digit lookahead heuristic. Our findings reveal fundamental limitations that prevent LLMs from generalizing to more complex numerical reasoning.

### Deterministic or probabilistic? The psychology of LLMs as random number generators 
[[arxiv](https://arxiv.org/abs/2502.19965)] [[cool](https://papers.cool/arxiv/2502.19965)] [[pdf](https://arxiv.org/pdf/2502.19965)]
> **Authors**: Javier Coronado-Blázquez
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 31 pages, 12 figures
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large Language Models (LLMs) have transformed text generation through inherently probabilistic context-aware mechanisms, mimicking human natural language. In this paper, we systematically investigate the performance of various LLMs when generating random numbers, considering diverse configurations such as different model architectures, numerical ranges, temperature, and prompt languages. Our results reveal that, despite their stochastic transformers-based architecture, these models often exhibit deterministic responses when prompted for random numerical outputs. In particular, we find significant differences when changing the model, as well as the prompt language, attributing this phenomenon to biases deeply embedded within the training data. Models such as DeepSeek-R1 can shed some light on the internal reasoning process of LLMs, despite arriving to similar results. These biases induce predictable patterns that undermine genuine randomness, as LLMs are nothing but reproducing our own human cognitive biases.

### Collaborative Stance Detection via Small-Large Language Model Consistency Verification 
[[arxiv](https://arxiv.org/abs/2502.19954)] [[cool](https://papers.cool/arxiv/2502.19954)] [[pdf](https://arxiv.org/pdf/2502.19954)]
> **Authors**: Yu Yan,Sheng Sun,Zixiang Tang,Teli Liu,Min Liu
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Stance detection on social media aims to identify attitudes expressed in tweets towards specific targets. Current studies prioritize Large Language Models (LLMs) over Small Language Models (SLMs) due to the overwhelming performance improving provided by LLMs. However, heavily relying on LLMs for stance detection, regardless of the cost, is impractical for real-world social media monitoring systems that require vast data analysis. To this end, we propose \textbf{\underline{Co}}llaborative Stance Detection via Small-Large Language Model Consistency \textbf{\underline{Ver}}ification (\textbf{CoVer}) framework, which enhances LLM utilization via context-shared batch reasoning and logical verification between LLM and SLM. Specifically, instead of processing each text individually, CoVer processes texts batch-by-batch, obtaining stance predictions and corresponding explanations via LLM reasoning in a shared context. Then, to exclude the bias caused by context noises, CoVer introduces the SLM for logical consistency verification. Finally, texts that repeatedly exhibit low logical consistency are classified using consistency-weighted aggregation of prior LLM stance predictions. Our experiments show that CoVer outperforms state-of-the-art methods across multiple benchmarks in the zero-shot setting, achieving 0.54 LLM queries per tweet while significantly enhancing performance. Our CoVer offers a more practical solution for LLM deploying for social media stance detection.

### GeoEdit: Geometric Knowledge Editing for Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.19953)] [[cool](https://papers.cool/arxiv/2502.19953)] [[pdf](https://arxiv.org/pdf/2502.19953)]
> **Authors**: Yujie Feng,Liming Zhan,Zexin Lu,Yongxin Xu,Xu Chu,Yasha Wang,Jiannong Cao,Philip S. Yu,Xiao-Ming Wu
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Regular updates are essential for maintaining up-to-date knowledge in large language models (LLMs). Consequently, various model editing methods have been developed to update specific knowledge within LLMs. However, training-based approaches often struggle to effectively incorporate new knowledge while preserving unrelated general knowledge. To address this challenge, we propose a novel framework called Geometric Knowledge Editing (GeoEdit). GeoEdit utilizes the geometric relationships of parameter updates from fine-tuning to differentiate between neurons associated with new knowledge updates and those related to general knowledge perturbations. By employing a direction-aware knowledge identification method, we avoid updating neurons with directions approximately orthogonal to existing knowledge, thus preserving the model's generalization ability. For the remaining neurons, we integrate both old and new knowledge for aligned directions and apply a "forget-then-learn" editing strategy for opposite directions. Additionally, we introduce an importance-guided task vector fusion technique that filters out redundant information and provides adaptive neuron-level weighting, further enhancing model editing performance. Extensive experiments on two publicly available datasets demonstrate the superiority of GeoEdit over existing state-of-the-art methods.

### Alleviating Distribution Shift in Synthetic Data for Machine Translation Quality Estimation 
[[arxiv](https://arxiv.org/abs/2502.19941)] [[cool](https://papers.cool/arxiv/2502.19941)] [[pdf](https://arxiv.org/pdf/2502.19941)]
> **Authors**: Xiang Geng,Zhejian Lai,Jiajun Chen,Hao Yang,Shujian Huang
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Quality Estimation (QE) models evaluate the quality of machine translations without reference translations, serving as the reward models for the translation task. Due to the data scarcity, synthetic data generation has emerged as a promising solution. However, synthetic QE data often suffers from distribution shift, which can manifest as discrepancies between pseudo and real translations, or in pseudo labels that do not align with human preferences. To tackle this issue, we introduce ADSQE, a novel framework for alleviating distribution shift in synthetic QE data. To reduce the difference between pseudo and real translations, we employ the constrained beam search algorithm and enhance translation diversity through the use of distinct generation models. ADSQE uses references, i.e., translation supervision signals, to guide both the generation and annotation processes, enhancing the quality of word-level labels. ADSE further identifies the shortest phrase covering consecutive error tokens, mimicking human annotation behavior, to assign the final phrase-level labels. Specially, we underscore that the translation model can not annotate translations of itself accurately. Extensive experiments demonstrate that ADSQE outperforms SOTA baselines like COMET in both supervised and unsupervised settings. Further analysis offers insights into synthetic data generation that could benefit reward models for other tasks.

### Picking the Cream of the Crop: Visual-Centric Data Selection with Collaborative Agents 
[[arxiv](https://arxiv.org/abs/2502.19917)] [[cool](https://papers.cool/arxiv/2502.19917)] [[pdf](https://arxiv.org/pdf/2502.19917)]
> **Authors**: Zhenyu Liu,Yunxin Li,Baotian Hu,Wenhan Luo,Yaowei Wang,Min Zhang
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 15 pages, 7 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: To improve Multimodal Large Language Models' (MLLMs) ability to process images and complex instructions, researchers predominantly curate large-scale visual instruction tuning datasets, which are either sourced from existing vision tasks or synthetically generated using LLMs and image descriptions. However, they often suffer from critical flaws, including misaligned instruction-image pairs and low-quality images. Such issues hinder training efficiency and limit performance improvements, as models waste resources on noisy or irrelevant data with minimal benefit to overall capability. To address this issue, we propose a \textbf{Vi}sual-Centric \textbf{S}election approach via \textbf{A}gents Collaboration (ViSA), which centers on image quality assessment and image-instruction relevance evaluation. Specifically, our approach consists of 1) an image information quantification method via visual agents collaboration to select images with rich visual information, and 2) a visual-centric instruction quality assessment method to select high-quality instruction data related to high-quality images. Finally, we reorganize 80K instruction data from large open-source datasets. Extensive experiments demonstrate that ViSA outperforms or is comparable to current state-of-the-art models on seven benchmarks, using only 2.5\% of the original data, highlighting the efficiency of our data selection approach. Moreover, we conduct ablation studies to validate the effectiveness of each component of our method. The code is available at https://github.com/HITsz-TMG/ViSA.

### Order Doesn't Matter, But Reasoning Does: Training LLMs with Order-Centric Augmentation 
[[arxiv](https://arxiv.org/abs/2502.19907)] [[cool](https://papers.cool/arxiv/2502.19907)] [[pdf](https://arxiv.org/pdf/2502.19907)]
> **Authors**: Qianxi He,Qianyu He,Jiaqing Liang,Yanghua Xiao,Weikang Zhou,Zeye Sun,Fei Yu
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Logical reasoning is essential for large language models (LLMs) to ensure accurate and coherent inference. However, LLMs struggle with reasoning order variations and fail to generalize across logically equivalent transformations. LLMs often rely on fixed sequential patterns rather than true logical understanding. To address this issue, we introduce an order-centric data augmentation framework based on commutativity in logical reasoning. We first randomly shuffle independent premises to introduce condition order augmentation. For reasoning steps, we construct a directed acyclic graph (DAG) to model dependencies between steps, which allows us to identify valid reorderings of steps while preserving logical correctness. By leveraging order-centric augmentations, models can develop a more flexible and generalized reasoning process. Finally, we conduct extensive experiments across multiple logical reasoning benchmarks, demonstrating that our method significantly enhances LLMs' reasoning performance and adaptability to diverse logical structures. We release our codes and augmented data in https://anonymous.4open.science/r/Order-Centric-Data-Augmentation-822C/.

### MMKE-Bench: A Multimodal Editing Benchmark for Diverse Visual Knowledge 
[[arxiv](https://arxiv.org/abs/2502.19870)] [[cool](https://papers.cool/arxiv/2502.19870)] [[pdf](https://arxiv.org/pdf/2502.19870)]
> **Authors**: Yuntao Du,Kailin Jiang,Zhi Gao,Chenrui Shi,Zilong Zheng,Siyuan Qi,Qing Li
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Knowledge editing techniques have emerged as essential tools for updating the factual knowledge of large language models (LLMs) and multimodal models (LMMs), allowing them to correct outdated or inaccurate information without retraining from scratch. However, existing benchmarks for multimodal knowledge editing primarily focus on entity-level knowledge represented as simple triplets, which fail to capture the complexity of real-world multimodal information. To address this issue, we introduce MMKE-Bench, a comprehensive MultiModal Knowledge Editing Benchmark, designed to evaluate the ability of LMMs to edit diverse visual knowledge in real-world scenarios. MMKE-Bench addresses these limitations by incorporating three types of editing tasks: visual entity editing, visual semantic editing, and user-specific editing. Besides, MMKE-Bench uses free-form natural language to represent and edit knowledge, offering a more flexible and effective format. The benchmark consists of 2,940 pieces of knowledge and 8,363 images across 33 broad categories, with evaluation questions automatically generated and human-verified. We assess five state-of-the-art knowledge editing methods on three prominent LMMs, revealing that no method excels across all criteria, and that visual and user-specific edits are particularly challenging. MMKE-Bench sets a new standard for evaluating the robustness of multimodal knowledge editing techniques, driving progress in this rapidly evolving field.

### MIND: Towards Immersive Psychological Healing with Multi-agent Inner Dialogue 
[[arxiv](https://arxiv.org/abs/2502.19860)] [[cool](https://papers.cool/arxiv/2502.19860)] [[pdf](https://arxiv.org/pdf/2502.19860)]
> **Authors**: Yujia Chen,Changsong Li,Yiming Wang,Qingqing Xiao,Nan Zhang,Zifan Kong,Peng Wang,Binyu Yan
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Mental health issues are worsening in today's competitive society, such as depression and anxiety. Traditional healings like counseling and chatbots fail to engage effectively, they often provide generic responses lacking emotional depth. Although large language models (LLMs) have the potential to create more human-like interactions, they still struggle to capture subtle emotions. This requires LLMs to be equipped with human-like adaptability and warmth. To fill this gap, we propose the MIND (Multi-agent INner Dialogue), a novel paradigm that provides more immersive psychological healing environments. Considering the strong generative and role-playing ability of LLM agents, we predefine an interactive healing framework and assign LLM agents different roles within the framework to engage in interactive inner dialogues with users, thereby providing an immersive healing experience. We conduct extensive human experiments in various real-world healing dimensions, and find that MIND provides a more user-friendly experience than traditional paradigms. This demonstrates that MIND effectively leverages the significant potential of LLMs in psychological healing.

### Team A at SemEval-2025 Task 11: Breaking Language Barriers in Emotion Detection with Multilingual Models 
[[arxiv](https://arxiv.org/abs/2502.19856)] [[cool](https://papers.cool/arxiv/2502.19856)] [[pdf](https://arxiv.org/pdf/2502.19856)]
> **Authors**: P Sam Sahil,Anupam Jamatia
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: This paper describes the system submitted by Team A to SemEval 2025 Task 11, ``Bridging the Gap in Text-Based Emotion Detection.'' The task involved identifying the perceived emotion of a speaker from text snippets, with each instance annotated with one of six emotions: joy, sadness, fear, anger, surprise, or disgust. A dataset provided by the task organizers served as the foundation for training and evaluating our models. Among the various approaches explored, the best performance was achieved using multilingual embeddings combined with a fully connected layer. This paper details the system architecture, discusses experimental results, and highlights the advantages of leveraging multilingual representations for robust emotion detection in text.

### Foot-In-The-Door: A Multi-turn Jailbreak for LLMs 
[[arxiv](https://arxiv.org/abs/2502.19820)] [[cool](https://papers.cool/arxiv/2502.19820)] [[pdf](https://arxiv.org/pdf/2502.19820)]
> **Authors**: Zixuan Weng,Xiaolong Jin,Jinyuan Jia,Xiangyu Zhang
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 19 pages, 8 figures
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Ensuring AI safety is crucial as large language models become increasingly integrated into real-world applications. A key challenge is jailbreak, where adversarial prompts bypass built-in safeguards to elicit harmful disallowed outputs. Inspired by psychological foot-in-the-door principles, we introduce FITD,a novel multi-turn jailbreak method that leverages the phenomenon where minor initial commitments lower resistance to more significant or more unethical transgressions.Our approach progressively escalates the malicious intent of user queries through intermediate bridge prompts and aligns the model's response by itself to induce toxic responses. Extensive experimental results on two jailbreak benchmarks demonstrate that FITD achieves an average attack success rate of 94% across seven widely used models, outperforming existing state-of-the-art methods. Additionally, we provide an in-depth analysis of LLM self-corruption, highlighting vulnerabilities in current alignment strategies and emphasizing the risks inherent in multi-turn interactions.The code is available at https://github.com/Jinxiaolong1129/Foot-in-the-door-Jailbreak .

### Text classification using machine learning methods 
[[arxiv](https://arxiv.org/abs/2502.19801)] [[cool](https://papers.cool/arxiv/2502.19801)] [[pdf](https://arxiv.org/pdf/2502.19801)]
> **Authors**: Bogdan Oancea
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: ef:Knowledge on Economics and Management Knowledge on Economics and Management Conference Proceedings, 2023, Olomouc, The Czech Republic
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: In this paper we present the results of an experiment aimed to use machine learning methods to obtain models that can be used for the automatic classification of products. In order to apply automatic classification methods, we transformed the product names from a text representation to numeric vectors, a process called word embedding. We used several embedding methods: Count Vectorization, TF-IDF, Word2Vec, FASTTEXT, and GloVe. Having the product names in a form of numeric vectors, we proceeded with a set of machine learning methods for automatic classification: Logistic Regression, Multinomial Naive Bayes, kNN, Artificial Neural Networks, Support Vector Machines, and Decision trees with several variants. The results show an impressive accuracy of the classification process for Support Vector Machines, Logistic Regression, and Random Forests. Regarding the word embedding methods, the best results were obtained with the FASTTEXT technique.

### Do Retrieval-Augmented Language Models Adapt to Varying User Needs? 
[[arxiv](https://arxiv.org/abs/2502.19779)] [[cool](https://papers.cool/arxiv/2502.19779)] [[pdf](https://arxiv.org/pdf/2502.19779)]
> **Authors**: Peilin Wu,Xinlu Zhang,Wenhao Yu,Xingyu Liu,Xinya Du,Zhiyu Zoey Chen
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Recent advancements in Retrieval-Augmented Language Models (RALMs) have demonstrated their efficacy in knowledge-intensive tasks. However, existing evaluation benchmarks often assume a single optimal approach to leveraging retrieved information, failing to account for varying user needs. This paper introduces a novel evaluation framework that systematically assesses RALMs under three user need cases-Context-Exclusive, Context-First, and Memory-First-across three distinct context settings: Context Matching, Knowledge Conflict, and Information Irrelevant. By varying both user instructions and the nature of retrieved information, our approach captures the complexities of real-world applications where models must adapt to diverse user requirements. Through extensive experiments on multiple QA datasets, including HotpotQA, DisentQA, and our newly constructed synthetic URAQ dataset, we find that restricting memory usage improves robustness in adversarial retrieval conditions but decreases peak performance with ideal retrieval results and model family dominates behavioral differences. Our findings highlight the necessity of user-centric evaluations in the development of retrieval-augmented systems and provide insights into optimizing model performance across varied retrieval contexts. We will release our code and URAQ dataset upon acceptance of the paper.

### Advancements in Natural Language Processing for Automatic Text Summarization 
[[arxiv](https://arxiv.org/abs/2502.19773)] [[cool](https://papers.cool/arxiv/2502.19773)] [[pdf](https://arxiv.org/pdf/2502.19773)]
> **Authors**: Nevidu Jayatilleke,Ruvan Weerasinghe,Nipuna Senanayake
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 11 pages, 9 figures, ICCS 2024
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The substantial growth of textual content in diverse domains and platforms has led to a considerable need for Automatic Text Summarization (ATS) techniques that aid in the process of text analysis. The effectiveness of text summarization models has been significantly enhanced in a variety of technical domains because of advancements in Natural Language Processing (NLP) and Deep Learning (DL). Despite this, the process of summarizing textual information continues to be significantly constrained by the intricate writing styles of a variety of texts, which involve a range of technical complexities. Text summarization techniques can be broadly categorized into two main types: abstractive summarization and extractive summarization. Extractive summarization involves directly extracting sentences, phrases, or segments of text from the content without making any changes. On the other hand, abstractive summarization is achieved by reconstructing the sentences, phrases, or segments from the original text using linguistic analysis. Through this study, a linguistically diverse categorizations of text summarization approaches have been addressed in a constructive manner. In this paper, the authors explored existing hybrid techniques that have employed both extractive and abstractive methodologies. In addition, the pros and cons of various approaches discussed in the literature are also investigated. Furthermore, the authors conducted a comparative analysis on different techniques and matrices to evaluate the generated summaries using language generation models. This survey endeavors to provide a comprehensive overview of ATS by presenting the progression of language processing regarding this task through a breakdown of diverse systems and architectures accompanied by technical and mathematical explanations of their operations.

### EdiText: Controllable Coarse-to-Fine Text Editing with Diffusion Language Models 
[[arxiv](https://arxiv.org/abs/2502.19765)] [[cool](https://papers.cool/arxiv/2502.19765)] [[pdf](https://arxiv.org/pdf/2502.19765)]
> **Authors**: Che Hyun Lee,Heeseung Kim,Jiheum Yeom,Sungroh Yoon
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: We propose EdiText, a controllable text editing method that modify the reference text to desired attributes at various scales. We integrate an SDEdit-based editing technique that allows for broad adjustments in the degree of text editing. Additionally, we introduce a novel fine-level editing method based on self-conditioning, which allows subtle control of reference text. While being capable of editing on its own, this fine-grained method, integrated with the SDEdit approach, enables EdiText to make precise adjustments within the desired range. EdiText demonstrates its controllability to robustly adjust reference text at broad range of levels across various tasks, including toxicity control and sentiment control.

## 密码学和安全(cs.CR:Cryptography and Security)

### Beyond the Tip of Efficiency: Uncovering the Submerged Threats of Jailbreak Attacks in Small Language Models 
[[arxiv](https://arxiv.org/abs/2502.19883)] [[cool](https://papers.cool/arxiv/2502.19883)] [[pdf](https://arxiv.org/pdf/2502.19883)]
> **Authors**: Sibo Yi,Tianshuo Cong,Xinlei He,Qi Li,Jiaxing Song
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 12 pages. 6 figures
- **标题**: None
- **领域**: 密码学和安全,人工智能,计算语言学
- **Abstract**: Small language models (SLMs) have become increasingly prominent in the deployment on edge devices due to their high efficiency and low computational cost. While researchers continue to advance the capabilities of SLMs through innovative training strategies and model compression techniques, the security risks of SLMs have received considerably less attention compared to large language models (LLMs).To fill this gap, we provide a comprehensive empirical study to evaluate the security performance of 13 state-of-the-art SLMs under various jailbreak attacks. Our experiments demonstrate that most SLMs are quite susceptible to existing jailbreak attacks, while some of them are even vulnerable to direct harmful prompts.To address the safety concerns, we evaluate several representative defense methods and demonstrate their effectiveness in enhancing the security of SLMs. We further analyze the potential security degradation caused by different SLM techniques including architecture compression, quantization, knowledge distillation, and so on. We expect that our research can highlight the security challenges of SLMs and provide valuable insights to future work in developing more robust and secure SLMs.

### SCU: An Efficient Machine Unlearning Scheme for Deep Learning Enabled Semantic Communications 
[[arxiv](https://arxiv.org/abs/2502.19785)] [[cool](https://papers.cool/arxiv/2502.19785)] [[pdf](https://arxiv.org/pdf/2502.19785)]
> **Authors**: Weiqi Wang,Zhiyi Tian,Chenhan Zhang,Shui Yu
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,机器学习
- **Abstract**: Deep learning (DL) enabled semantic communications leverage DL to train encoders and decoders (codecs) to extract and recover semantic information. However, most semantic training datasets contain personal private information. Such concerns call for enormous requirements for specified data erasure from semantic codecs when previous users hope to move their data from the semantic system. {Existing machine unlearning solutions remove data contribution from trained models, yet usually in supervised sole model scenarios. These methods are infeasible in semantic communications that often need to jointly train unsupervised encoders and decoders.} In this paper, we investigate the unlearning problem in DL-enabled semantic communications and propose a semantic communication unlearning (SCU) scheme to tackle the problem. {SCU includes two key components. Firstly,} we customize the joint unlearning method for semantic codecs, including the encoder and decoder, by minimizing mutual information between the learned semantic representation and the erased samples. {Secondly,} to compensate for semantic model utility degradation caused by unlearning, we propose a contrastive compensation method, which considers the erased data as the negative samples and the remaining data as the positive samples to retrain the unlearned semantic models contrastively. Theoretical analysis and extensive experimental results on three representative datasets demonstrate the effectiveness and efficiency of our proposed methods.

### TAPE: Tailored Posterior Difference for Auditing of Machine Unlearning 
[[arxiv](https://arxiv.org/abs/2502.19770)] [[cool](https://papers.cool/arxiv/2502.19770)] [[pdf](https://arxiv.org/pdf/2502.19770)]
> **Authors**: Weiqi Wang,Zhiyi Tian,An Liu,Shui Yu
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,机器学习
- **Abstract**: With the increasing prevalence of Web-based platforms handling vast amounts of user data, machine unlearning has emerged as a crucial mechanism to uphold users' right to be forgotten, enabling individuals to request the removal of their specified data from trained models. However, the auditing of machine unlearning processes remains significantly underexplored. Although some existing methods offer unlearning auditing by leveraging backdoors, these backdoor-based approaches are inefficient and impractical, as they necessitate involvement in the initial model training process to embed the backdoors. In this paper, we propose a TAilored Posterior diffErence (TAPE) method to provide unlearning auditing independently of original model training. We observe that the process of machine unlearning inherently introduces changes in the model, which contains information related to the erased data. TAPE leverages unlearning model differences to assess how much information has been removed through the unlearning operation. Firstly, TAPE mimics the unlearned posterior differences by quickly building unlearned shadow models based on first-order influence estimation. Secondly, we train a Reconstructor model to extract and evaluate the private information of the unlearned posterior differences to audit unlearning. Existing privacy reconstructing methods based on posterior differences are only feasible for model updates of a single sample. To enable the reconstruction effective for multi-sample unlearning requests, we propose two strategies, unlearned data perturbation and unlearned influence-based division, to augment the posterior difference. Extensive experimental results indicate the significant superiority of TAPE over the state-of-the-art unlearning verification methods, at least 4.5$\times$ efficiency speedup and supporting the auditing for broader unlearning scenarios.

## 计算机视觉和模式识别(cs.CV:Computer Vision and Pattern Recognition)

### LIFT-GS: Cross-Scene Render-Supervised Distillation for 3D Language Grounding 
[[arxiv](https://arxiv.org/abs/2502.20389)] [[cool](https://papers.cool/arxiv/2502.20389)] [[pdf](https://arxiv.org/pdf/2502.20389)]
> **Authors**: Ang Cao,Sergio Arnaud,Oleksandr Maksymets,Jianing Yang,Ayush Jain,Sriram Yenamandra,Ada Martin,Vincent-Pierre Berges,Paul McVay,Ruslan Partsey,Aravind Rajeswaran,Franziska Meier,Justin Johnson,Jeong Joon Park,Alexander Sax
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Project page: https://liftgs.github.io
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Our approach to training 3D vision-language understanding models is to train a feedforward model that makes predictions in 3D, but never requires 3D labels and is supervised only in 2D, using 2D losses and differentiable rendering. The approach is new for vision-language understanding. By treating the reconstruction as a ``latent variable'', we can render the outputs without placing unnecessary constraints on the network architecture (e.g. can be used with decoder-only models). For training, only need images and camera pose, and 2D labels. We show that we can even remove the need for 2D labels by using pseudo-labels from pretrained 2D models. We demonstrate this to pretrain a network, and we finetune it for 3D vision-language understanding tasks. We show this approach outperforms baselines/sota for 3D vision-language grounding, and also outperforms other 3D pretraining techniques. Project page: https://liftgs.github.io.

### Beyond Next-Token: Next-X Prediction for Autoregressive Visual Generation 
[[arxiv](https://arxiv.org/abs/2502.20388)] [[cool](https://papers.cool/arxiv/2502.20388)] [[pdf](https://arxiv.org/pdf/2502.20388)]
> **Authors**: Sucheng Ren,Qihang Yu,Ju He,Xiaohui Shen,Alan Yuille,Liang-Chieh Chen
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Project page at \url{https://oliverrensu.github.io/project/xAR}
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Autoregressive (AR) modeling, known for its next-token prediction paradigm, underpins state-of-the-art language and visual generative models. Traditionally, a ``token'' is treated as the smallest prediction unit, often a discrete symbol in language or a quantized patch in vision. However, the optimal token definition for 2D image structures remains an open question. Moreover, AR models suffer from exposure bias, where teacher forcing during training leads to error accumulation at inference. In this paper, we propose xAR, a generalized AR framework that extends the notion of a token to an entity X, which can represent an individual patch token, a cell (a $k\times k$ grouping of neighboring patches), a subsample (a non-local grouping of distant patches), a scale (coarse-to-fine resolution), or even a whole image. Additionally, we reformulate discrete token classification as \textbf{continuous entity regression}, leveraging flow-matching methods at each AR step. This approach conditions training on noisy entities instead of ground truth tokens, leading to Noisy Context Learning, which effectively alleviates exposure bias. As a result, xAR offers two key advantages: (1) it enables flexible prediction units that capture different contextual granularity and spatial structures, and (2) it mitigates exposure bias by avoiding reliance on teacher forcing. On ImageNet-256 generation benchmark, our base model, xAR-B (172M), outperforms DiT-XL/SiT-XL (675M) while achieving 20$\times$ faster inference. Meanwhile, xAR-H sets a new state-of-the-art with an FID of 1.24, running 2.2$\times$ faster than the previous best-performing model without relying on vision foundation modules (\eg, DINOv2) or advanced guidance interval sampling.

### SecureGaze: Defending Gaze Estimation Against Backdoor Attacks 
[[arxiv](https://arxiv.org/abs/2502.20306)] [[cool](https://papers.cool/arxiv/2502.20306)] [[pdf](https://arxiv.org/pdf/2502.20306)]
> **Authors**: Lingyu Du,Yupei Liu,Jinyuan Jia,Guohao Lan
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Gaze estimation models are widely used in applications such as driver attention monitoring and human-computer interaction. While many methods for gaze estimation exist, they rely heavily on data-hungry deep learning to achieve high performance. This reliance often forces practitioners to harvest training data from unverified public datasets, outsource model training, or rely on pre-trained models. However, such practices expose gaze estimation models to backdoor attacks. In such attacks, adversaries inject backdoor triggers by poisoning the training data, creating a backdoor vulnerability: the model performs normally with benign inputs, but produces manipulated gaze directions when a specific trigger is present. This compromises the security of many gaze-based applications, such as causing the model to fail in tracking the driver's attention. To date, there is no defense that addresses backdoor attacks on gaze estimation models. In response, we introduce SecureGaze, the first solution designed to protect gaze estimation models from such attacks. Unlike classification models, defending gaze estimation poses unique challenges due to its continuous output space and globally activated backdoor behavior. By identifying distinctive characteristics of backdoored gaze estimation models, we develop a novel and effective approach to reverse-engineer the trigger function for reliable backdoor detection. Extensive evaluations in both digital and physical worlds demonstrate that SecureGaze effectively counters a range of backdoor attacks and outperforms seven state-of-the-art defenses adapted from classification models.

### M^3Builder: A Multi-Agent System for Automated Machine Learning in Medical Imaging 
[[arxiv](https://arxiv.org/abs/2502.20301)] [[cool](https://papers.cool/arxiv/2502.20301)] [[pdf](https://arxiv.org/pdf/2502.20301)]
> **Authors**: Jinghao Feng,Qiaoyu Zheng,Chaoyi Wu,Ziheng Zhao,Ya Zhang,Yanfeng Wang,Weidi Xie
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 38 pages, 7 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **Abstract**: Agentic AI systems have gained significant attention for their ability to autonomously perform complex tasks. However, their reliance on well-prepared tools limits their applicability in the medical domain, which requires to train specialized models. In this paper, we make three contributions: (i) We present M3Builder, a novel multi-agent system designed to automate machine learning (ML) in medical imaging. At its core, M3Builder employs four specialized agents that collaborate to tackle complex, multi-step medical ML workflows, from automated data processing and environment configuration to self-contained auto debugging and model training. These agents operate within a medical imaging ML workspace, a structured environment designed to provide agents with free-text descriptions of datasets, training codes, and interaction tools, enabling seamless communication and task execution. (ii) To evaluate progress in automated medical imaging ML, we propose M3Bench, a benchmark comprising four general tasks on 14 training datasets, across five anatomies and three imaging modalities, covering both 2D and 3D data. (iii) We experiment with seven state-of-the-art large language models serving as agent cores for our system, such as Claude series, GPT-4o, and DeepSeek-V3. Compared to existing ML agentic designs, M3Builder shows superior performance on completing ML tasks in medical imaging, achieving a 94.29% success rate using Claude-3.7-Sonnet as the agent core, showing huge potential towards fully automated machine learning in medical imaging.

### Visual Adaptive Prompting for Compositional Zero-Shot Learning 
[[arxiv](https://arxiv.org/abs/2502.20292)] [[cool](https://papers.cool/arxiv/2502.20292)] [[pdf](https://arxiv.org/pdf/2502.20292)]
> **Authors**: Kyle Stein,Arash Mahyari,Guillermo Francia,Eman El-Sheikh
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Vision-Language Models (VLMs) have demonstrated impressive capabilities in learning joint representations of visual and textual data, making them powerful tools for tasks such as Compositional Zero-Shot Learning (CZSL). CZSL requires models to generalize to novel combinations of visual primitives-such as attributes and objects-that were not explicitly encountered during training. Recent works in prompting for CZSL have focused on modifying inputs for the text encoder, often using static prompts that do not change across varying visual contexts. However, these approaches struggle to fully capture varying visual contexts, as they focus on text adaptation rather than leveraging visual features for compositional reasoning. To address this, we propose Visual Adaptive Prompting System (VAPS) that leverages a learnable visual prompt repository and similarity-based retrieval mechanism within the framework of VLMs to bridge the gap between semantic and visual features. Our method introduces a dynamic visual prompt repository mechanism that selects the most relevant attribute and object prompts based on the visual features of the image. Our proposed system includes a visual prompt adapter that encourages the model to learn a more generalizable embedding space. Experiments on three CZSL benchmarks, across both closed and open-world scenarios, demonstrate state-of-the-art results.

### Explainable, Multi-modal Wound Infection Classification from Images Augmented with Generated Captions 
[[arxiv](https://arxiv.org/abs/2502.20277)] [[cool](https://papers.cool/arxiv/2502.20277)] [[pdf](https://arxiv.org/pdf/2502.20277)]
> **Authors**: Palawat Busaranuvong,Emmanuel Agu,Reza Saadati Fard,Deepak Kumar,Shefalika Gautam,Bengisu Tulu,Diane Strong
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Infections in Diabetic Foot Ulcers (DFUs) can cause severe complications, including tissue death and limb amputation, highlighting the need for accurate, timely diagnosis. Previous machine learning methods have focused on identifying infections by analyzing wound images alone, without utilizing additional metadata such as medical notes. In this study, we aim to improve infection detection by introducing Synthetic Caption Augmented Retrieval for Wound Infection Detection (SCARWID), a novel deep learning framework that leverages synthetic textual descriptions to augment DFU images. SCARWID consists of two components: (1) Wound-BLIP, a Vision-Language Model (VLM) fine-tuned on GPT-4o-generated descriptions to synthesize consistent captions from images; and (2) an Image-Text Fusion module that uses cross-attention to extract cross-modal embeddings from an image and its corresponding Wound-BLIP caption. Infection status is determined by retrieving the top-k similar items from a labeled support set. To enhance the diversity of training data, we utilized a latent diffusion model to generate additional wound images. As a result, SCARWID outperformed state-of-the-art models, achieving average sensitivity, specificity, and accuracy of 0.85, 0.78, and 0.81, respectively, for wound infection classification. Displaying the generated captions alongside the wound images and infection detection results enhances interpretability and trust, enabling nurses to align SCARWID outputs with their medical knowledge. This is particularly valuable when wound notes are unavailable or when assisting novice nurses who may find it difficult to identify visual attributes of wound infection.

### HVI: A New color space for Low-light Image Enhancement 
[[arxiv](https://arxiv.org/abs/2502.20272)] [[cool](https://papers.cool/arxiv/2502.20272)] [[pdf](https://arxiv.org/pdf/2502.20272)]
> **Authors**: Qingsen Yan,Yixu Feng,Cheng Zhang,Guansong Pang,Kangbiao Shi,Peng Wu,Wei Dong,Jinqiu Sun,Yanning Zhang
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: *These authors contributed equally to this work
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: Low-Light Image Enhancement (LLIE) is a crucial computer vision task that aims to restore detailed visual information from corrupted low-light images. Many existing LLIE methods are based on standard RGB (sRGB) space, which often produce color bias and brightness artifacts due to inherent high color sensitivity in sRGB. While converting the images using Hue, Saturation and Value (HSV) color space helps resolve the brightness issue, it introduces significant red and black noise artifacts. To address this issue, we propose a new color space for LLIE, namely Horizontal/Vertical-Intensity (HVI), defined by polarized HS maps and learnable intensity. The former enforces small distances for red coordinates to remove the red artifacts, while the latter compresses the low-light regions to remove the black artifacts. To fully leverage the chromatic and intensity information, a novel Color and Intensity Decoupling Network (CIDNet) is further introduced to learn accurate photometric mapping function under different lighting conditions in the HVI space. Comprehensive results from benchmark and ablation experiments show that the proposed HVI color space with CIDNet outperforms the state-of-the-art methods on 10 datasets. The code is available at https://github.com/Fediory/HVI-CIDNet.

### Enhancing 3D Gaze Estimation in the Wild using Weak Supervision with Gaze Following Labels 
[[arxiv](https://arxiv.org/abs/2502.20249)] [[cool](https://papers.cool/arxiv/2502.20249)] [[pdf](https://arxiv.org/pdf/2502.20249)]
> **Authors**: Pierre Vuillecard,Jean-Marc Odobez
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Accurate 3D gaze estimation in unconstrained real-world environments remains a significant challenge due to variations in appearance, head pose, occlusion, and the limited availability of in-the-wild 3D gaze datasets. To address these challenges, we introduce a novel Self-Training Weakly-Supervised Gaze Estimation framework (ST-WSGE). This two-stage learning framework leverages diverse 2D gaze datasets, such as gaze-following data, which offer rich variations in appearances, natural scenes, and gaze distributions, and proposes an approach to generate 3D pseudo-labels and enhance model generalization. Furthermore, traditional modality-specific models, designed separately for images or videos, limit the effective use of available training data. To overcome this, we propose the Gaze Transformer (GaT), a modality-agnostic architecture capable of simultaneously learning static and dynamic gaze information from both image and video datasets. By combining 3D video datasets with 2D gaze target labels from gaze following tasks, our approach achieves the following key contributions: (i) Significant state-of-the-art improvements in within-domain and cross-domain generalization on unconstrained benchmarks like Gaze360 and GFIE, with notable cross-modal gains in video gaze estimation; (ii) Superior cross-domain performance on datasets such as MPIIFaceGaze and Gaze360 compared to frontal face methods. Code and pre-trained models will be released to the community.

### Deep Convolutional Neural Networks for Palm Fruit Maturity Classification 
[[arxiv](https://arxiv.org/abs/2502.20223)] [[cool](https://papers.cool/arxiv/2502.20223)] [[pdf](https://arxiv.org/pdf/2502.20223)]
> **Authors**: Mingqiang Han,Chunlin Yi
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: To maximize palm oil yield and quality, it is essential to harvest palm fruit at the optimal maturity stage. This project aims to develop an automated computer vision system capable of accurately classifying palm fruit images into five ripeness levels. We employ deep Convolutional Neural Networks (CNNs) to classify palm fruit images based on their maturity stage. A shallow CNN serves as the baseline model, while transfer learning and fine-tuning are applied to pre-trained ResNet50 and InceptionV3 architectures. The study utilizes a publicly available dataset of over 8,000 images with significant variations, which is split into 80\% for training and 20\% for testing. The proposed deep CNN models achieve test accuracies exceeding 85\% in classifying palm fruit maturity stages. This research highlights the potential of deep learning for automating palm fruit ripeness assessment, which can contribute to optimizing harvesting decisions and improving palm oil production efficiency.

### DIPSER: A Dataset for In-Person Student1 Engagement Recognition in the Wild 
[[arxiv](https://arxiv.org/abs/2502.20209)] [[cool](https://papers.cool/arxiv/2502.20209)] [[pdf](https://arxiv.org/pdf/2502.20209)]
> **Authors**: Luis Marquez-Carpintero,Sergio Suescun-Ferrandiz,Carolina Lorenzo Álvarez,Jorge Fernandez-Herrero,Diego Viejo,Rosabel Roig-Vila,Miguel Cazorla
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: In this paper, a novel dataset is introduced, designed to assess student attention within in-person classroom settings. This dataset encompasses RGB camera data, featuring multiple cameras per student to capture both posture and facial expressions, in addition to smartwatch sensor data for each individual. This dataset allows machine learning algorithms to be trained to predict attention and correlate it with emotion. A comprehensive suite of attention and emotion labels for each student is provided, generated through self-reporting as well as evaluations by four different experts. Our dataset uniquely combines facial and environmental camera data, smartwatch metrics, and includes underrepresented ethnicities in similar datasets, all within in-the-wild, in-person settings, making it the most comprehensive dataset of its kind currently available. The dataset presented offers an extensive and diverse collection of data pertaining to student interactions across different educational contexts, augmented with additional metadata from other tools. This initiative addresses existing deficiencies by offering a valuable resource for the analysis of student attention and emotion in face-to-face lessons.

### Multimodal Representation Alignment for Image Generation: Text-Image Interleaved Control Is Easier Than You Think 
[[arxiv](https://arxiv.org/abs/2502.20172)] [[cool](https://papers.cool/arxiv/2502.20172)] [[pdf](https://arxiv.org/pdf/2502.20172)]
> **Authors**: Liang Chen,Shuai Bai,Wenhao Chai,Weichu Xie,Haozhe Zhao,Leon Vinci,Junyang Lin,Baobao Chang
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 13 pages, 9 figures, codebase in https://github.com/chenllliang/DreamEngine
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算语言学
- **Abstract**: The field of advanced text-to-image generation is witnessing the emergence of unified frameworks that integrate powerful text encoders, such as CLIP and T5, with Diffusion Transformer backbones. Although there have been efforts to control output images with additional conditions, like canny and depth map, a comprehensive framework for arbitrary text-image interleaved control is still lacking. This gap is especially evident when attempting to merge concepts or visual elements from multiple images in the generation process. To mitigate the gap, we conducted preliminary experiments showing that large multimodal models (LMMs) offer an effective shared representation space, where image and text can be well-aligned to serve as a condition for external diffusion models. Based on this discovery, we propose Dream Engine, an efficient and unified framework designed for arbitrary text-image interleaved control in image generation models. Building on powerful text-to-image models like SD3.5, we replace the original text-only encoders by incorporating versatile multimodal information encoders such as QwenVL. Our approach utilizes a two-stage training paradigm, consisting of joint text-image alignment and multimodal interleaved instruction tuning. Our experiments demonstrate that this training method is effective, achieving a 0.69 overall score on the GenEval benchmark, and matching the performance of state-of-the-art text-to-image models like SD3.5 and FLUX.

### Adaptive H&E-IHC information fusion staining framework based on feature extra 
[[arxiv](https://arxiv.org/abs/2502.20156)] [[cool](https://papers.cool/arxiv/2502.20156)] [[pdf](https://arxiv.org/pdf/2502.20156)]
> **Authors**: Yifan Jia,Xingda Yu,Zhengyang Ji,Songning Lai,Yutao Yue
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Immunohistochemistry (IHC) staining plays a significant role in the evaluation of diseases such as breast cancer. The H&E-to-IHC transformation based on generative models provides a simple and cost-effective method for obtaining IHC images. Although previous models can perform digital coloring well, they still suffer from (i) coloring only through the pixel features that are not prominent in HE, which is easy to cause information loss in the coloring process; (ii) The lack of pixel-perfect H&E-IHC groundtruth pairs poses a challenge to the classical L1 loss.To address the above challenges, we propose an adaptive information enhanced coloring framework based on feature extractors. We first propose the VMFE module to effectively extract the color information features using multi-scale feature extraction and wavelet transform convolution, while combining the shared decoder for feature fusion. The high-performance dual feature extractor of H&E-IHC is trained by contrastive learning, which can effectively perform feature alignment of HE-IHC in high latitude space. At the same time, the trained feature encoder is used to enhance the features and adaptively adjust the loss in the HE section staining process to solve the problems related to unclear and asymmetric information. We have tested on different datasets and achieved excellent performance.Our code is available at https://github.com/babyinsunshine/CEFF

### Cutting-edge 3D reconstruction solutions for underwater coral reef images: A review and comparison 
[[arxiv](https://arxiv.org/abs/2502.20154)] [[cool](https://papers.cool/arxiv/2502.20154)] [[pdf](https://arxiv.org/pdf/2502.20154)]
> **Authors**: Jiageng Zhong,Ming Li,Armin Gruen,Konrad Schindler,Xuan Liao,Qinghua Guo
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Corals serve as the foundational habitat-building organisms within reef ecosystems, constructing extensive structures that extend over vast distances. However, their inherent fragility and vulnerability to various threats render them susceptible to significant damage and destruction. The application of advanced 3D reconstruction technologies for high-quality modeling is crucial for preserving them. These technologies help scientists to accurately document and monitor the state of coral reefs, including their structure, species distribution and changes over time. Photogrammetry-based approaches stand out among existing solutions, especially with recent advancements in underwater videography, photogrammetric computer vision, and machine learning. Despite continuous progress in image-based 3D reconstruction techniques, there remains a lack of systematic reviews and comprehensive evaluations of cutting-edge solutions specifically applied to underwater coral reef images. The emerging advanced methods may have difficulty coping with underwater imaging environments, complex coral structures, and computational resource constraints. They need to be reviewed and evaluated to bridge the gap between many cutting-edge technical studies and practical applications. This paper focuses on the two critical stages of these approaches: camera pose estimation and dense surface reconstruction. We systematically review and summarize classical and emerging methods, conducting comprehensive evaluations through real-world and simulated datasets. Based on our findings, we offer reference recommendations and discuss the development potential and challenges of existing approaches in depth. This work equips scientists and managers with a technical foundation and practical guidance for processing underwater coral reef images for 3D reconstruction....

### Robust sensitivity control in digital pathology via tile score distribution matching 
[[arxiv](https://arxiv.org/abs/2502.20144)] [[cool](https://papers.cool/arxiv/2502.20144)] [[pdf](https://arxiv.org/pdf/2502.20144)]
> **Authors**: Arthur Pignet,John Klein,Genevieve Robin,Antoine Olivier
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Preprint
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Deploying digital pathology models across medical centers is challenging due to distribution shifts. Recent advances in domain generalization improve model transferability in terms of aggregated performance measured by the Area Under Curve (AUC). However, clinical regulations often require to control the transferability of other metrics, such as prescribed sensitivity levels. We introduce a novel approach to control the sensitivity of whole slide image (WSI) classification models, based on optimal transport and Multiple Instance Learning (MIL). Validated across multiple cohorts and tasks, our method enables robust sensitivity control with only a handful of calibration samples, providing a practical solution for reliable deployment of computational pathology systems.

### Show and Tell: Visually Explainable Deep Neural Nets via Spatially-Aware Concept Bottleneck Models 
[[arxiv](https://arxiv.org/abs/2502.20134)] [[cool](https://papers.cool/arxiv/2502.20134)] [[pdf](https://arxiv.org/pdf/2502.20134)]
> **Authors**: Itay Benou,Tammy Riklin-Raviv
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Modern deep neural networks have now reached human-level performance across a variety of tasks. However, unlike humans they lack the ability to explain their decisions by showing where and telling what concepts guided them. In this work, we present a unified framework for transforming any vision neural network into a spatially and conceptually interpretable model. We introduce a spatially-aware concept bottleneck layer that projects "black-box" features of pre-trained backbone models into interpretable concept maps, without requiring human labels. By training a classification layer over this bottleneck, we obtain a self-explaining model that articulates which concepts most influenced its prediction, along with heatmaps that ground them in the input image. Accordingly, we name this method "Spatially-Aware and Label-Free Concept Bottleneck Model" (SALF-CBM). Our results show that the proposed SALF-CBM: (1) Outperforms non-spatial CBM methods, as well as the original backbone, on a variety of classification tasks; (2) Produces high-quality spatial explanations, outperforming widely used heatmap-based methods on a zero-shot segmentation task; (3) Facilitates model exploration and debugging, enabling users to query specific image regions and refine the model's decisions by locally editing its concept maps.

### QPM: Discrete Optimization for Globally Interpretable Image Classification 
[[arxiv](https://arxiv.org/abs/2502.20130)] [[cool](https://papers.cool/arxiv/2502.20130)] [[pdf](https://arxiv.org/pdf/2502.20130)]
> **Authors**: Thomas Norrenbrock,Timo Kaiser,Sovan Biswas,Ramesh Manuvinakurike,Bodo Rosenhahn
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: ef:The Thirteenth International Conference onLearningRepresentations, 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,人机交互,机器学习
- **Abstract**: Understanding the classifications of deep neural networks, e.g. used in safety-critical situations, is becoming increasingly important. While recent models can locally explain a single decision, to provide a faithful global explanation about an accurate model's general behavior is a more challenging open task. Towards that goal, we introduce the Quadratic Programming Enhanced Model (QPM), which learns globally interpretable class representations. QPM represents every class with a binary assignment of very few, typically 5, features, that are also assigned to other classes, ensuring easily comparable contrastive class representations. This compact binary assignment is found using discrete optimization based on predefined similarity measures and interpretability constraints. The resulting optimal assignment is used to fine-tune the diverse features, so that each of them becomes the shared general concept between the assigned classes. Extensive evaluations show that QPM delivers unprecedented global interpretability across small and large-scale datasets while setting the state of the art for the accuracy of interpretable models.

### Sketch & Paint: Stroke-by-Stroke Evolution of Visual Artworks 
[[arxiv](https://arxiv.org/abs/2502.20119)] [[cool](https://papers.cool/arxiv/2502.20119)] [[pdf](https://arxiv.org/pdf/2502.20119)]
> **Authors**: Jeripothula Prudviraj,Vikram Jamwal
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: ECCV 2024 Workshop:AIfor Visual Arts Workshop and Challenges (AI4VA)
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Understanding the stroke-based evolution of visual artworks is useful for advancing artwork learning, appreciation, and interactive display. While the stroke sequence of renowned artworks remains largely unknown, formulating this sequence for near-natural image drawing processes can significantly enhance our understanding of artistic techniques. This paper introduces a novel method for approximating artwork stroke evolution through a proximity-based clustering mechanism. We first convert pixel images into vector images via parametric curves and then explore the clustering approach to determine the sequence order of extracted strokes. Our proposed algorithm demonstrates the potential to infer stroke sequences in unknown artworks. We evaluate the performance of our method using WikiArt data and qualitatively demonstrate the plausible stroke sequences. Additionally, we demonstrate the robustness of our approach to handle a wide variety of input image types such as line art, face sketches, paintings, and photographic images. By exploring stroke extraction and sequence construction, we aim to improve our understanding of the intricacies of the art development techniques and the step-by-step reconstruction process behind visual artworks, thereby enriching our understanding of the creative journey from the initial sketch to the final artwork.

### MITracker: Multi-View Integration for Visual Object Tracking 
[[arxiv](https://arxiv.org/abs/2502.20111)] [[cool](https://papers.cool/arxiv/2502.20111)] [[pdf](https://arxiv.org/pdf/2502.20111)]
> **Authors**: Mengjie Xu,Yitao Zhu,Haotian Jiang,Jiaming Li,Zhenrong Shen,Sheng Wang,Haolin Huang,Xinyu Wang,Qing Yang,Han Zhang,Qian Wang
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Multi-view object tracking (MVOT) offers promising solutions to challenges such as occlusion and target loss, which are common in traditional single-view tracking. However, progress has been limited by the lack of comprehensive multi-view datasets and effective cross-view integration methods. To overcome these limitations, we compiled a Multi-View object Tracking (MVTrack) dataset of 234K high-quality annotated frames featuring 27 distinct objects across various scenes. In conjunction with this dataset, we introduce a novel MVOT method, Multi-View Integration Tracker (MITracker), to efficiently integrate multi-view object features and provide stable tracking outcomes. MITracker can track any object in video frames of arbitrary length from arbitrary viewpoints. The key advancements of our method over traditional single-view approaches come from two aspects: (1) MITracker transforms 2D image features into a 3D feature volume and compresses it into a bird's eye view (BEV) plane, facilitating inter-view information fusion; (2) we propose an attention mechanism that leverages geometric information from fused 3D feature volume to refine the tracking results at each view. MITracker outperforms existing methods on the MVTrack and GMTD datasets, achieving state-of-the-art performance. The code and the new dataset will be available at https://mii-laboratory.github.io/MITracker/.

### VDT-Auto: End-to-end Autonomous Driving with VLM-Guided Diffusion Transformers 
[[arxiv](https://arxiv.org/abs/2502.20108)] [[cool](https://papers.cool/arxiv/2502.20108)] [[pdf](https://arxiv.org/pdf/2502.20108)]
> **Authors**: Ziang Guo,Konstantin Gubernatorov,Selamawit Asfaw,Zakhar Yagudin,Dzmitry Tsetserukou
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Submitted paper
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器人技术
- **Abstract**: In autonomous driving, dynamic environment and corner cases pose significant challenges to the robustness of ego vehicle's decision-making. To address these challenges, commencing with the representation of state-action mapping in the end-to-end autonomous driving paradigm, we introduce a novel pipeline, VDT-Auto. Leveraging the advancement of the state understanding of Visual Language Model (VLM), incorporating with diffusion Transformer-based action generation, our VDT-Auto parses the environment geometrically and contextually for the conditioning of the diffusion process. Geometrically, we use a bird's-eye view (BEV) encoder to extract feature grids from the surrounding images. Contextually, the structured output of our fine-tuned VLM is processed into textual embeddings and noisy paths. During our diffusion process, the added noise for the forward process is sampled from the noisy path output of the fine-tuned VLM, while the extracted BEV feature grids and embedded texts condition the reverse process of our diffusion Transformers. Our VDT-Auto achieved 0.52m on average L2 errors and 21% on average collision rate in the nuScenes open-loop planning evaluation. Moreover, the real-world demonstration exhibited prominent generalizability of our VDT-Auto. The code and dataset will be released after acceptance.

### New Dataset and Methods for Fine-Grained Compositional Referring Expression Comprehension via Specialist-MLLM Collaboration 
[[arxiv](https://arxiv.org/abs/2502.20104)] [[cool](https://papers.cool/arxiv/2502.20104)] [[pdf](https://arxiv.org/pdf/2502.20104)]
> **Authors**: Xuzheng Yang,Junzhuo Liu,Peng Wang,Guoqing Wang,Yang Yang,Heng Tao Shen
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: TPAMI under review
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Referring Expression Comprehension (REC) is a foundational cross-modal task that evaluates the interplay of language understanding, image comprehension, and language-to-image grounding. To advance this field, we introduce a new REC dataset with two key features. First, it is designed with controllable difficulty levels, requiring fine-grained reasoning across object categories, attributes, and relationships. Second, it incorporates negative text and images generated through fine-grained editing, explicitly testing a model's ability to reject non-existent targets, an often-overlooked yet critical challenge in existing datasets. To address fine-grained compositional REC, we propose novel methods based on a Specialist-MLLM collaboration framework, leveraging the complementary strengths of them: Specialist Models handle simpler tasks efficiently, while MLLMs are better suited for complex reasoning. Based on this synergy, we introduce two collaborative strategies. The first, Slow-Fast Adaptation (SFA), employs a routing mechanism to adaptively delegate simple tasks to Specialist Models and complex tasks to MLLMs. Additionally, common error patterns in both models are mitigated through a target-refocus strategy. The second, Candidate Region Selection (CRS), generates multiple bounding box candidates based on Specialist Model and uses the advanced reasoning capabilities of MLLMs to identify the correct target. Extensive experiments on our dataset and other challenging compositional benchmarks validate the effectiveness of our approaches. The SFA strategy achieves a trade-off between localization accuracy and efficiency, and the CRS strategy greatly boosts the performance of both Specialist Models and MLLMs. We aim for this work to offer valuable insights into solving complex real-world tasks by strategically combining existing tools for maximum effectiveness, rather than reinventing them.

### 3D-AffordanceLLM: Harnessing Large Language Models for Open-Vocabulary Affordance Detection in 3D Worlds 
[[arxiv](https://arxiv.org/abs/2502.20041)] [[cool](https://papers.cool/arxiv/2502.20041)] [[pdf](https://arxiv.org/pdf/2502.20041)]
> **Authors**: Hengshuo Chu,Xiang Deng,Xiaoyang Chen,Yinchuan Li,Jianye Hao,Liqiang Nie
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: ICLR
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器人技术
- **Abstract**: 3D Affordance detection is a challenging problem with broad applications on various robotic tasks. Existing methods typically formulate the detection paradigm as a label-based semantic segmentation task. This paradigm relies on predefined labels and lacks the ability to comprehend complex natural language, resulting in limited generalization in open-world scene. To address these limitations, we reformulate the traditional affordance detection paradigm into \textit{Instruction Reasoning Affordance Segmentation} (IRAS) task. This task is designed to output a affordance mask region given a query reasoning text, which avoids fixed categories of input labels. We accordingly propose the \textit{3D-AffordanceLLM} (3D-ADLLM), a framework designed for reasoning affordance detection in 3D open-scene. Specifically, 3D-ADLLM introduces large language models (LLMs) to 3D affordance perception with a custom-designed decoder for generating affordance masks, thus achieving open-world reasoning affordance detection. In addition, given the scarcity of 3D affordance datasets for training large models, we seek to extract knowledge from general segmentation data and transfer it to affordance detection. Thus, we propose a multi-stage training strategy that begins with a novel pre-training task, i.e., \textit{Referring Object Part Segmentation}~(ROPS). This stage is designed to equip the model with general recognition and segmentation capabilities at the object-part level. Then followed by fine-tuning with the IRAS task, 3D-ADLLM obtains the reasoning ability for affordance detection. In summary, 3D-ADLLM leverages the rich world knowledge and human-object interaction reasoning ability of LLMs, achieving approximately an 8\% improvement in mIoU on open-vocabulary affordance detection tasks.

### A2-GNN: Angle-Annular GNN for Visual Descriptor-free Camera Relocalization 
[[arxiv](https://arxiv.org/abs/2502.20036)] [[cool](https://papers.cool/arxiv/2502.20036)] [[pdf](https://arxiv.org/pdf/2502.20036)]
> **Authors**: Yejun Zhang,Shuzhe Wang,Juho Kannala
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: To be published in 2025 International Conference on 3D Vision (3DV)
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Visual localization involves estimating the 6-degree-of-freedom (6-DoF) camera pose within a known scene. A critical step in this process is identifying pixel-to-point correspondences between 2D query images and 3D models. Most advanced approaches currently rely on extensive visual descriptors to establish these correspondences, facing challenges in storage, privacy issues and model maintenance. Direct 2D-3D keypoint matching without visual descriptors is becoming popular as it can overcome those challenges. However, existing descriptor-free methods suffer from low accuracy or heavy computation. Addressing this gap, this paper introduces the Angle-Annular Graph Neural Network (A2-GNN), a simple approach that efficiently learns robust geometric structural representations with annular feature extraction. Specifically, this approach clusters neighbors and embeds each group's distance information and angle as supplementary information to capture local structures. Evaluation on matching and visual localization datasets demonstrates that our approach achieves state-of-the-art accuracy with low computational overhead among visual description-free methods. Our code will be released on https://github.com/YejunZhang/a2-gnn.

### AsymLoRA: Harmonizing Data Conflicts and Commonalities in MLLMs 
[[arxiv](https://arxiv.org/abs/2502.20035)] [[cool](https://papers.cool/arxiv/2502.20035)] [[pdf](https://arxiv.org/pdf/2502.20035)]
> **Authors**: Xuyang Wei,Chunlin Tian,Li Li
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Effective instruction fine-tuning on diverse image-text datasets is crucial for developing a versatile Multimodal Large Language Model (MLLM), where dataset composition dictates the model's adaptability across multimodal tasks. However, complex datasets often contain inherent conflicts -- stemming from modality-specific optimization objectives -- and latent commonalities that enable cross-task transfer, which most existing approaches handle separately. To bridge this gap, we introduce AsymLoRA, a parameter-efficient tuning framework that unifies knowledge modularization and cross-modal coordination via asymmetric LoRA: task-specific low-rank projections (matrix B) that preserve distinct adaptation pathways for conflicting objectives, and a shared projection (matrix A) that consolidates cross-modal commonalities. Extensive evaluations demonstrate that AsymLoRA consistently surpasses both vanilla LoRA, which captures only commonalities, and LoRA-MoE, which focuses solely on conflicts, achieving superior model performance and system efficiency across diverse benchmarks.\href{Code}{https://github.com/Clin0212/HydraLoRA/blob/main/MLLM-HydraLoRA/README.md}.

### Vision-Encoders (Already) Know What They See: Mitigating Object Hallucination via Simple Fine-Grained CLIPScore 
[[arxiv](https://arxiv.org/abs/2502.20034)] [[cool](https://papers.cool/arxiv/2502.20034)] [[pdf](https://arxiv.org/pdf/2502.20034)]
> **Authors**: Hongseok Oh,Wonseok Hwang
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 4 pages
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算语言学
- **Abstract**: Recently, Large Vision-Language Models (LVLMs) show remarkable performance across various domains. However, these models suffer from object hallucination. This study revisits the previous claim that the primary cause of such hallucination lies in the limited representational capacity of the vision encoder. Our analysis reveals that the capacity of the vision encoder itself is already enough for detecting object hallucination. Based on this insight, we propose a Fine-grained CLIPScore (F-CLIPScore), a simple yet effective evaluation metric that enhances object-level granularity by incorporating text embeddings at the noun phrase level. Evaluations on the OHD-Caps benchmark show that F-CLIPScore significantly outperforms conventional CLIPScore in accuracy by a large margin of 39.6% without additional training. We further validate F-CLIPScore by showing that LVLM trained with the data filtered using F-CLIPScore exhibits reduced hallucination.

### Joint Fusion and Encoding: Advancing Multimodal Retrieval from the Ground Up 
[[arxiv](https://arxiv.org/abs/2502.20008)] [[cool](https://papers.cool/arxiv/2502.20008)] [[pdf](https://arxiv.org/pdf/2502.20008)]
> **Authors**: Lang Huang,Qiyu Wu,Zhongtao Miao,Toshihiko Yamasaki
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Information retrieval is indispensable for today's Internet applications, yet traditional semantic matching techniques often fall short in capturing the fine-grained cross-modal interactions required for complex queries. Although late-fusion two-tower architectures attempt to bridge this gap by independently encoding visual and textual data before merging them at a high level, they frequently overlook the subtle interplay essential for comprehensive understanding. In this work, we rigorously assess these limitations and introduce a unified retrieval framework that fuses visual and textual cues from the ground up, enabling early cross-modal interactions for enhancing context interpretation. Through a two-stage training process--comprising post-training adaptation followed by instruction tuning--we adapt MLLMs as retrievers using a simple one-tower architecture. Our approach outperforms conventional methods across diverse retrieval scenarios, particularly when processing complex multi-modal inputs. Notably, the joint fusion encoder yields greater improvements on tasks that require modality fusion compared to those that do not, underscoring the transformative potential of early integration strategies and pointing toward a promising direction for contextually aware and effective information retrieval.

### Can Large Language Models Unveil the Mysteries? An Exploration of Their Ability to Unlock Information in Complex Scenarios 
[[arxiv](https://arxiv.org/abs/2502.19973)] [[cool](https://papers.cool/arxiv/2502.19973)] [[pdf](https://arxiv.org/pdf/2502.19973)]
> **Authors**: Chao Wang,Luning Zhang,Zheng Wang,Yang Zhou
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 11pages
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Combining multiple perceptual inputs and performing combinatorial reasoning in complex scenarios is a sophisticated cognitive function in humans. With advancements in multi-modal large language models, recent benchmarks tend to evaluate visual understanding across multiple images. However, they often overlook the necessity of combinatorial reasoning across multiple perceptual information. To explore the ability of advanced models to integrate multiple perceptual inputs for combinatorial reasoning in complex scenarios, we introduce two benchmarks: Clue-Visual Question Answering (CVQA), with three task types to assess visual comprehension and synthesis, and Clue of Password-Visual Question Answering (CPVQA), with two task types focused on accurate interpretation and application of visual data. For our benchmarks, we present three plug-and-play approaches: utilizing model input for reasoning, enhancing reasoning through minimum margin decoding with randomness generation, and retrieving semantically relevant visual information for effective data integration. The combined results reveal current models' poor performance on combinatorial reasoning benchmarks, even the state-of-the-art (SOTA) closed-source model achieves only 33.04% accuracy on CVQA, and drops to 7.38% on CPVQA. Notably, our approach improves the performance of models on combinatorial reasoning, with a 22.17% boost on CVQA and 9.40% on CPVQA over the SOTA closed-source model, demonstrating its effectiveness in enhancing combinatorial reasoning with multiple perceptual inputs in complex scenarios. The code will be publicly available.

### ChatReID: Open-ended Interactive Person Retrieval via Hierarchical Progressive Tuning for Vision Language Models 
[[arxiv](https://arxiv.org/abs/2502.19958)] [[cool](https://papers.cool/arxiv/2502.19958)] [[pdf](https://arxiv.org/pdf/2502.19958)]
> **Authors**: Ke Niu,Haiyang Yu,Mengyang Zhao,Teng Fu,Siyang Yi,Wei Lu,Bin Li,Xuelin Qian,Xiangyang Xue
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Person re-identification (Re-ID) is a critical task in human-centric intelligent systems, enabling consistent identification of individuals across different camera views using multi-modal query information. Recent studies have successfully integrated LVLMs with person Re-ID, yielding promising results. However, existing LVLM-based methods face several limitations. They rely on extracting textual embeddings from fixed templates, which are used either as intermediate features for image representation or for prompt tuning in domain-specific tasks. Furthermore, they are unable to adopt the VQA inference format, significantly restricting their broader applicability. In this paper, we propose a novel, versatile, one-for-all person Re-ID framework, ChatReID. Our approach introduces a Hierarchical Progressive Tuning (HPT) strategy, which ensures fine-grained identity-level retrieval by progressively refining the model's ability to distinguish pedestrian identities. Extensive experiments demonstrate that our approach outperforms SOTA methods across ten benchmarks in four different Re-ID settings, offering enhanced flexibility and user-friendliness. ChatReID provides a scalable, practical solution for real-world person Re-ID applications, enabling effective multi-modal interaction and fine-grained identity discrimination.

### Space Rotation with Basis Transformation for Training-free Test-Time Adaptation 
[[arxiv](https://arxiv.org/abs/2502.19946)] [[cool](https://papers.cool/arxiv/2502.19946)] [[pdf](https://arxiv.org/pdf/2502.19946)]
> **Authors**: Chenhao Ding,Xinyuan Gao,Songlin Dong,Yuhang He,Qiang Wang,Xiang Song,Alex Kot,Yihong Gong
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: With the development of visual-language models (VLM) in downstream task applications, test-time adaptation methods based on VLM have attracted increasing attention for their ability to address changes distribution in test-time. Although prior approaches have achieved some progress, they typically either demand substantial computational resources or are constrained by the limitations of the original feature space, rendering them less effective for test-time adaptation tasks. To address these challenges, we propose a training-free feature space rotation with basis transformation for test-time adaptation. By leveraging the inherent distinctions among classes, we reconstruct the original feature space and map it to a new representation, thereby enhancing the clarity of class differences and providing more effective guidance for the model during testing. Additionally, to better capture relevant information from various classes, we maintain a dynamic queue to store representative samples. Experimental results across multiple benchmarks demonstrate that our method outperforms state-of-the-art techniques in terms of both performance and efficiency.

### C-Drag: Chain-of-Thought Driven Motion Controller for Video Generation 
[[arxiv](https://arxiv.org/abs/2502.19868)] [[cool](https://papers.cool/arxiv/2502.19868)] [[pdf](https://arxiv.org/pdf/2502.19868)]
> **Authors**: Yuhao Li,Mirana Claire Angel,Salman Khan,Yu Zhu,Jinqiu Sun,Yanning Zhang,Fahad Shahbaz Khan
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Trajectory-based motion control has emerged as an intuitive and efficient approach for controllable video generation. However, the existing trajectory-based approaches are usually limited to only generating the motion trajectory of the controlled object and ignoring the dynamic interactions between the controlled object and its surroundings. To address this limitation, we propose a Chain-of-Thought-based motion controller for controllable video generation, named C-Drag. Instead of directly generating the motion of some objects, our C-Drag first performs object perception and then reasons the dynamic interactions between different objects according to the given motion control of the objects. Specifically, our method includes an object perception module and a Chain-of-Thought-based motion reasoning module. The object perception module employs visual language models to capture the position and category information of various objects within the image. The Chain-of-Thought-based motion reasoning module takes this information as input and conducts a stage-wise reasoning process to generate motion trajectories for each of the affected objects, which are subsequently fed to the diffusion model for video synthesis. Furthermore, we introduce a new video object interaction (VOI) dataset to evaluate the generation quality of motion controlled video generation methods. Our VOI dataset contains three typical types of interactions and provides the motion trajectories of objects that can be used for accurate performance evaluation. Experimental results show that C-Drag achieves promising performance across multiple metrics, excelling in object motion control. Our benchmark, codes, and models will be available at https://github.com/WesLee88524/C-Drag-Official-Repo.

### Striving for Faster and Better: A One-Layer Architecture with Auto Re-parameterization for Low-Light Image Enhancement 
[[arxiv](https://arxiv.org/abs/2502.19867)] [[cool](https://papers.cool/arxiv/2502.19867)] [[pdf](https://arxiv.org/pdf/2502.19867)]
> **Authors**: Nan An,Long Ma,Guangchao Han,Xin Fan,RIsheng Liu
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Deep learning-based low-light image enhancers have made significant progress in recent years, with a trend towards achieving satisfactory visual quality while gradually reducing the number of parameters and improving computational efficiency. In this work, we aim to delving into the limits of image enhancers both from visual quality and computational efficiency, while striving for both better performance and faster processing. To be concrete, by rethinking the task demands, we build an explicit connection, i.e., visual quality and computational efficiency are corresponding to model learning and structure design, respectively. Around this connection, we enlarge parameter space by introducing the re-parameterization for ample model learning of a pre-defined minimalist network (e.g., just one layer), to avoid falling into a local solution. To strengthen the structural representation, we define a hierarchical search scheme for discovering a task-oriented re-parameterized structure, which also provides powerful support for efficiency. Ultimately, this achieves efficient low-light image enhancement using only a single convolutional layer, while maintaining excellent visual quality. Experimental results show our sensible superiority both in quality and efficiency against recently-proposed methods. Especially, our running time on various platforms (e.g., CPU, GPU, NPU, DSP) consistently moves beyond the existing fastest scheme. The source code will be released at https://github.com/vis-opt-group/AR-LLIE.

### LMHLD: A Large-scale Multi-source High-resolution Landslide Dataset for Landslide Detection based on Deep Learning 
[[arxiv](https://arxiv.org/abs/2502.19866)] [[cool](https://papers.cool/arxiv/2502.19866)] [[pdf](https://arxiv.org/pdf/2502.19866)]
> **Authors**: Guanting Liu,Yi Wang,Xi Chen,Baoyu Du,Penglei Li,Yuan Wu,Zhice Fang
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,图像和视频处理
- **Abstract**: Landslides are among the most common natural disasters globally, posing significant threats to human society. Deep learning (DL) has proven to be an effective method for rapidly generating landslide inventories in large-scale disaster areas. However, DL models rely heavily on high-quality labeled landslide data for strong feature extraction capabilities. And landslide detection using DL urgently needs a benchmark dataset to evaluate the generalization ability of the latest models. To solve the above problems, we construct a Large-scale Multi-source High-resolution Landslide Dataset (LMHLD) for Landslide Detection based on DL. LMHLD collects remote sensing images from five different satellite sensors across seven study areas worldwide: Wenchuan, China (2008); Rio de Janeiro, Brazil (2011); Gorkha, Nepal (2015); Jiuzhaigou, China (2015); Taiwan, China (2018); Hokkaido, Japan (2018); Emilia-Romagna, Italy (2023). The dataset includes a total of 25,365 patches, with different patch sizes to accommodate different landslide scales. Additionally, a training module, LMHLDpart, is designed to accommodate landslide detection tasks at varying scales and to alleviate the issue of catastrophic forgetting in multi-task learning. Furthermore, the models trained by LMHLD is applied in other datasets to highlight the robustness of LMHLD. Five dataset quality evaluation experiments designed by using seven DL models from the U-Net family demonstrate that LMHLD has the potential to become a benchmark dataset for landslide detection. LMHLD is open access and can be accessed through the link: https://doi.org/10.5281/zenodo.11424988. This dataset provides a strong foundation for DL models, accelerates the development of DL in landslide detection, and serves as a valuable resource for landslide prevention and mitigation efforts.

### ProAPO: Progressively Automatic Prompt Optimization for Visual Classification 
[[arxiv](https://arxiv.org/abs/2502.19844)] [[cool](https://papers.cool/arxiv/2502.19844)] [[pdf](https://arxiv.org/pdf/2502.19844)]
> **Authors**: Xiangyan Qu,Gaopeng Gou,Jiamin Zhuang,Jing Yu,Kun Song,Qihao Wang,Yili Li,Gang Xiong
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Accepted to the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Vision-language models (VLMs) have made significant progress in image classification by training with large-scale paired image-text data. Their performances largely depend on the prompt quality. While recent methods show that visual descriptions generated by large language models (LLMs) enhance the generalization of VLMs, class-specific prompts may be inaccurate or lack discrimination due to the hallucination in LLMs. In this paper, we aim to find visually discriminative prompts for fine-grained categories with minimal supervision and no human-in-the-loop. An evolution-based algorithm is proposed to progressively optimize language prompts from task-specific templates to class-specific descriptions. Unlike optimizing templates, the search space shows an explosion in class-specific candidate prompts. This increases prompt generation costs, iterative times, and the overfitting problem. To this end, we first introduce several simple yet effective edit-based and evolution-based operations to generate diverse candidate prompts by one-time query of LLMs. Then, two sampling strategies are proposed to find a better initial search point and reduce traversed categories, saving iteration costs. Moreover, we apply a novel fitness score with entropy constraints to mitigate overfitting. In a challenging one-shot image classification setting, our method outperforms existing textual prompt-based methods and improves LLM-generated description methods across 13 datasets. Meanwhile, we demonstrate that our optimal prompts improve adapter-based methods and transfer effectively across different backbones.

### CLIP Under the Microscope: A Fine-Grained Analysis of Multi-Object Representation 
[[arxiv](https://arxiv.org/abs/2502.19842)] [[cool](https://papers.cool/arxiv/2502.19842)] [[pdf](https://arxiv.org/pdf/2502.19842)]
> **Authors**: Reza Abbasi,Ali Nazari,Aminreza Sefid,Mohammadali Banayeeanzade,Mohammad Hossein Rohban,Mahdieh Soleymani Baghshah
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Accepted at CVPR 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Contrastive Language-Image Pre-training (CLIP) models excel in zero-shot classification, yet face challenges in complex multi-object scenarios. This study offers a comprehensive analysis of CLIP's limitations in these contexts using a specialized dataset, ComCO, designed to evaluate CLIP's encoders in diverse multi-object scenarios. Our findings reveal significant biases: the text encoder prioritizes first-mentioned objects, and the image encoder favors larger objects. Through retrieval and classification tasks, we quantify these biases across multiple CLIP variants and trace their origins to CLIP's training process, supported by analyses of the LAION dataset and training progression. Our image-text matching experiments show substantial performance drops when object size or token order changes, underscoring CLIP's instability with rephrased but semantically similar captions. Extending this to longer captions and text-to-image models like Stable Diffusion, we demonstrate how prompt order influences object prominence in generated images. For more details and access to our dataset and analysis code, visit our project repository: https://clip-analysis.github.io.

### Analyzing CLIP's Performance Limitations in Multi-Object Scenarios: A Controlled High-Resolution Study 
[[arxiv](https://arxiv.org/abs/2502.19828)] [[cool](https://papers.cool/arxiv/2502.19828)] [[pdf](https://arxiv.org/pdf/2502.19828)]
> **Authors**: Reza Abbasi,Ali Nazari,Aminreza Sefid,Mohammadali Banayeeanzade,Mohammad Hossein Rohban,Mahdieh Soleymani Baghshah
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Accepted at ECCV 2024 Workshop EVAL-FoMo
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Contrastive Language-Image Pre-training (CLIP) models have demonstrated remarkable performance in zero-shot classification tasks, yet their efficacy in handling complex multi-object scenarios remains challenging. This study presents a comprehensive analysis of CLIP's performance limitations in multi-object contexts through controlled experiments. We introduce two custom datasets, SimCO and CompCO, to evaluate CLIP's image and text encoders in various multi-object configurations. Our findings reveal significant biases in both encoders: the image encoder favors larger objects, while the text encoder prioritizes objects mentioned first in descriptions. We hypothesize these biases originate from CLIP's training process and provide evidence through analyses of the COCO dataset and CLIP's training progression. Additionally, we extend our investigation to Stable Diffusion models, revealing that biases in the CLIP text encoder significantly impact text-to-image generation tasks. Our experiments demonstrate how these biases affect CLIP's performance in image-caption matching and generation tasks, particularly when manipulating object sizes and their order in captions. This work contributes valuable insights into CLIP's behavior in complex visual environments and highlights areas for improvement in future vision-language models.

### Open-Vocabulary Semantic Part Segmentation of 3D Human 
[[arxiv](https://arxiv.org/abs/2502.19782)] [[cool](https://papers.cool/arxiv/2502.19782)] [[pdf](https://arxiv.org/pdf/2502.19782)]
> **Authors**: Keito Suzuki,Bang Du,Girish Krishnan,Kunyao Chen,Runfa Blark Li,Truong Nguyen
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 3DV 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: 3D part segmentation is still an open problem in the field of 3D vision and AR/VR. Due to limited 3D labeled data, traditional supervised segmentation methods fall short in generalizing to unseen shapes and categories. Recently, the advancement in vision-language models' zero-shot abilities has brought a surge in open-world 3D segmentation methods. While these methods show promising results for 3D scenes or objects, they do not generalize well to 3D humans. In this paper, we present the first open-vocabulary segmentation method capable of handling 3D human. Our framework can segment the human category into desired fine-grained parts based on the textual prompt. We design a simple segmentation pipeline, leveraging SAM to generate multi-view proposals in 2D and proposing a novel HumanCLIP model to create unified embeddings for visual and textual inputs. Compared with existing pre-trained CLIP models, the HumanCLIP model yields more accurate embeddings for human-centric contents. We also design a simple-yet-effective MaskFusion module, which classifies and fuses multi-view features into 3D semantic masks without complex voting and grouping mechanisms. The design of decoupling mask proposals and text input also significantly boosts the efficiency of per-prompt inference. Experimental results on various 3D human datasets show that our method outperforms current state-of-the-art open-vocabulary 3D segmentation methods by a large margin. In addition, we show that our method can be directly applied to various 3D representations including meshes, point clouds, and 3D Gaussian Splatting.

### InPK: Infusing Prior Knowledge into Prompt for Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2502.19777)] [[cool](https://papers.cool/arxiv/2502.19777)] [[pdf](https://arxiv.org/pdf/2502.19777)]
> **Authors**: Shuchang Zhou
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Prompt tuning has become a popular strategy for adapting Vision-Language Models (VLMs) to zero/few-shot visual recognition tasks. Some prompting techniques introduce prior knowledge due to its richness, but when learnable tokens are randomly initialized and disconnected from prior knowledge, they tend to overfit on seen classes and struggle with domain shifts for unseen ones. To address this issue, we propose the InPK model, which infuses class-specific prior knowledge into the learnable tokens during initialization, thus enabling the model to explicitly focus on class-relevant information. Furthermore, to mitigate the weakening of class information by multi-layer encoders, we continuously reinforce the interaction between learnable tokens and prior knowledge across multiple feature levels. This progressive interaction allows the learnable tokens to better capture the fine-grained differences and universal visual concepts within prior knowledge, enabling the model to extract more discriminative and generalized text features. Even for unseen classes, the learned interaction allows the model to capture their common representations and infer their appropriate positions within the existing semantic structure. Moreover, we introduce a learnable text-to-vision projection layer to accommodate the text adjustments, ensuring better alignment of visual-text semantics. Extensive experiments on 11 recognition datasets show that InPK significantly outperforms state-of-the-art methods in multiple zero/few-shot image classification tasks.

### QORT-Former: Query-optimized Real-time Transformer for Understanding Two Hands Manipulating Objects 
[[arxiv](https://arxiv.org/abs/2502.19769)] [[cool](https://papers.cool/arxiv/2502.19769)] [[pdf](https://arxiv.org/pdf/2502.19769)]
> **Authors**: Elkhan Ismayilzada,MD Khalequzzaman Chowdhury Sayem,Yihalem Yimolal Tiruneh,Mubarrat Tajoar Chowdhury,Muhammadjon Boboev,Seungryul Baek
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Accepted to AAAI 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Significant advancements have been achieved in the realm of understanding poses and interactions of two hands manipulating an object. The emergence of augmented reality (AR) and virtual reality (VR) technologies has heightened the demand for real-time performance in these applications. However, current state-of-the-art models often exhibit promising results at the expense of substantial computational overhead. In this paper, we present a query-optimized real-time Transformer (QORT-Former), the first Transformer-based real-time framework for 3D pose estimation of two hands and an object. We first limit the number of queries and decoders to meet the efficiency requirement. Given limited number of queries and decoders, we propose to optimize queries which are taken as input to the Transformer decoder, to secure better accuracy: (1) we propose to divide queries into three types (a left hand query, a right hand query and an object query) and enhance query features (2) by using the contact information between hands and an object and (3) by using three-step update of enhanced image and query features with respect to one another. With proposed methods, we achieved real-time pose estimation performance using just 108 queries and 1 decoder (53.5 FPS on an RTX 3090TI GPU). Surpassing state-of-the-art results on the H2O dataset by 17.6% (left hand), 22.8% (right hand), and 27.2% (object), as well as on the FPHA dataset by 5.3% (right hand) and 10.4% (object), our method excels in accuracy. Additionally, it sets the state-of-the-art in interaction recognition, maintaining real-time efficiency with an off-the-shelf action recognition module.

## 计算机与社会(cs.CY:Computers and Society)

### The erasure of intensive livestock farming in text-to-image generative AI 
[[arxiv](https://arxiv.org/abs/2502.19771)] [[cool](https://papers.cool/arxiv/2502.19771)] [[pdf](https://arxiv.org/pdf/2502.19771)]
> **Authors**: Kehan Sheng,Frank A. M. Tuyttens,Marina A. G. von Keyserlingk
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: Generative AI (e.g., ChatGPT) is increasingly integrated into people's daily lives. While it is known that AI perpetuates biases against marginalized human groups, their impact on non-human animals remains understudied. We found that ChatGPT's text-to-image model (DALL-E 3) introduces a strong bias toward romanticizing livestock farming as dairy cows on pasture and pigs rooting in mud. This bias remained when we requested realistic depictions and was only mitigated when the automatic prompt revision was inhibited. Most farmed animal in industrialized countries are reared indoors with limited space per animal, which fail to resonate with societal values. Inhibiting prompt revision resulted in images that more closely reflected modern farming practices; for example, cows housed indoors accessing feed through metal headlocks, and pigs behind metal railings on concrete floors in indoor facilities. While OpenAI introduced prompt revision to mitigate bias, in the case of farmed animal production systems, it paradoxically introduces a strong bias towards unrealistic farming practices.

## 数据库(cs.DB:Databases)

### Selective Use of Yannakakis' Algorithm to Improve Query Performance: Machine Learning to the Rescue 
[[arxiv](https://arxiv.org/abs/2502.20233)] [[cool](https://papers.cool/arxiv/2502.20233)] [[pdf](https://arxiv.org/pdf/2502.20233)]
> **Authors**: Daniela Böhm,Georg Gottlob,Matthias Lanzinger,Davide Longo,Cem Okulmus,Reinhard Pichler,Alexander Selzer
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 数据库,人工智能
- **Abstract**: Query optimization has played a central role in database research for decades. However, more often than not, the proposed optimization techniques lead to a performance improvement in some, but not in all, situations. Therefore, we urgently need a methodology for designing a decision procedure that decides for a given query whether the optimization technique should be applied or not. In this work, we propose such a methodology with a focus on Yannakakis-style query evaluation as our optimization technique of interest. More specifically, we formulate this decision problem as an algorithm selection problem and we present a Machine Learning based approach for its solution. Empirical results with several benchmarks on a variety of database systems show that our approach indeed leads to a statistically significant performance improvement.

## 分布式、并行和集群计算(cs.DC:Distributed, Parallel, and Cluster Computing)

### Improving the Efficiency of a Deep Reinforcement Learning-Based Power Management System for HPC Clusters Using Curriculum Learning 
[[arxiv](https://arxiv.org/abs/2502.20348)] [[cool](https://papers.cool/arxiv/2502.20348)] [[pdf](https://arxiv.org/pdf/2502.20348)]
> **Authors**: Thomas Budiarjo,Santana Yuda Pradata,Kadek Gemilang Santiyuda,Muhammad Alfian Amrizal,Reza Pulungan,Hiroyuki Takizawa
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 13 pages, 17 figures, accepted at Supercomputing Asia '25, published by ACM
- **标题**: None
- **领域**: 分布式、并行和集群计算,机器学习
- **Abstract**: High energy consumption remains a key challenge in high-performance computing (HPC) systems, which often feature hundreds or thousands of nodes drawing substantial power even in idle or standby modes. Although powering down unused nodes can improve energy efficiency, choosing the wrong time to do so can degrade quality of service by delaying job execution. Machine learning, in particular reinforcement learning (RL), has shown promise in determining optimal times to switch nodes on or off. In this study, we enhance the performance of a deep reinforcement learning (DRL) agent for HPC power management by integrating curriculum learning (CL), a training approach that introduces tasks with gradually increasing difficulty. Using the Batsim-py simulation framework, we compare the proposed CL-based agent to both a baseline DRL method (without CL) and the conventional fixed-time timeout strategy. Experimental results confirm that an easy-to-hard curriculum outperforms other training orders in terms of reducing wasted energy usage. The best agent achieves a 3.73% energy reduction over the baseline DRL method and a 4.66% improvement compared to the best timeout configuration (shutdown every 15 minutes of idle time). In addition, it reduces average job waiting time by 9.24% and maintains a higher job-filling rate, indicating more effective resource utilization. Sensitivity tests across various switch-on durations, power levels, and cluster sizes further reveal the agent's adaptability to changing system parameters without retraining. These findings demonstrate that curriculum learning can significantly improve DRL-based power management in HPC, balancing energy savings, quality of service, and robustness to diverse configurations.

### Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts 
[[arxiv](https://arxiv.org/abs/2502.19811)] [[cool](https://papers.cool/arxiv/2502.19811)] [[pdf](https://arxiv.org/pdf/2502.19811)]
> **Authors**: Shulai Zhang,Ningxin Zheng,Haibin Lin,Ziheng Jiang,Wenlei Bao,Chengquan Jiang,Qi Hou,Weihao Cui,Size Zheng,Li-Wen Chang,Quan Chen,Xin Liu
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 分布式、并行和集群计算,人工智能,机器学习
- **Abstract**: Mixture-of-experts (MoE) has been extensively employed to scale large language models to trillion-plus parameters while maintaining a fixed computational cost. The development of large MoE models in the distributed scenario encounters the problem of large communication overhead. The inter-device communication of a MoE layer can occupy 47% time of the entire model execution with popular models and frameworks. Therefore, existing methods suggest the communication in a MoE layer to be pipelined with the computation for overlapping. However, these coarse grained overlapping schemes introduce a notable impairment of computational efficiency and the latency concealing is sub-optimal. To this end, we present COMET, an optimized MoE system with fine-grained communication-computation overlapping. Leveraging data dependency analysis and task rescheduling, COMET achieves precise fine-grained overlapping of communication and computation. Through adaptive workload assignment, COMET effectively eliminates fine-grained communication bottlenecks and enhances its adaptability across various scenarios. Our evaluation shows that COMET accelerates the execution of a single MoE layer by $1.96\times$ and for end-to-end execution, COMET delivers a $1.71\times$ speedup on average. COMET has been adopted in the production environment of clusters with ten-thousand-scale of GPUs, achieving savings of millions of GPU hours.

## 数据结构和算法(cs.DS:Data Structures and Algorithms)

### Beyond Worst-Case Dimensionality Reduction for Sparse Vectors 
[[arxiv](https://arxiv.org/abs/2502.19865)] [[cool](https://papers.cool/arxiv/2502.19865)] [[pdf](https://arxiv.org/pdf/2502.19865)]
> **Authors**: Sandeep Silwal,David P. Woodruff,Qiuyi Zhang
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: To appear in ICLR 2025
- **标题**: None
- **领域**: 数据结构和算法,机器学习
- **Abstract**: We study beyond worst-case dimensionality reduction for $s$-sparse vectors. Our work is divided into two parts, each focusing on a different facet of beyond worst-case analysis: We first consider average-case guarantees. A folklore upper bound based on the birthday-paradox states: For any collection $X$ of $s$-sparse vectors in $\mathbb{R}^d$, there exists a linear map to $\mathbb{R}^{O(s^2)}$ which \emph{exactly} preserves the norm of $99\%$ of the vectors in $X$ in any $\ell_p$ norm (as opposed to the usual setting where guarantees hold for all vectors). We give lower bounds showing that this is indeed optimal in many settings: any oblivious linear map satisfying similar average-case guarantees must map to $Ω(s^2)$ dimensions. The same lower bound also holds for a wide class of smooth maps, including `encoder-decoder schemes', where we compare the norm of the original vector to that of a smooth function of the embedding. These lower bounds reveal a separation result, as an upper bound of $O(s \log(d))$ is possible if we instead use arbitrary (possibly non-smooth) functions, e.g., via compressed sensing algorithms. Given these lower bounds, we specialize to sparse \emph{non-negative} vectors. For a dataset $X$ of non-negative $s$-sparse vectors and any $p \ge 1$, we can non-linearly embed $X$ to $O(s\log(|X|s)/ε^2)$ dimensions while preserving all pairwise distances in $\ell_p$ norm up to $1\pm ε$, with no dependence on $p$. Surprisingly, the non-negativity assumption enables much smaller embeddings than arbitrary sparse vectors, where the best known bounds suffer exponential dependence. Our map also guarantees \emph{exact} dimensionality reduction for $\ell_{\infty}$ by embedding into $O(s\log |X|)$ dimensions, which is tight. We show that both the non-linearity of $f$ and the non-negativity of $X$ are necessary, and provide downstream algorithmic improvements.

## 图形(cs.GR:Graphics)

### Tight Inversion: Image-Conditioned Inversion for Real Image Editing 
[[arxiv](https://arxiv.org/abs/2502.20376)] [[cool](https://papers.cool/arxiv/2502.20376)] [[pdf](https://arxiv.org/pdf/2502.20376)]
> **Authors**: Edo Kadosh,Nir Goren,Or Patashnik,Daniel Garibi,Daniel Cohen-Or
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Project page at: https://tight-inversion.github.io
- **标题**: None
- **领域**: 图形,计算机视觉和模式识别,机器学习
- **Abstract**: Text-to-image diffusion models offer powerful image editing capabilities. To edit real images, many methods rely on the inversion of the image into Gaussian noise. A common approach to invert an image is to gradually add noise to the image, where the noise is determined by reversing the sampling equation. This process has an inherent tradeoff between reconstruction and editability, limiting the editing of challenging images such as highly-detailed ones. Recognizing the reliance of text-to-image models inversion on a text condition, this work explores the importance of the condition choice. We show that a condition that precisely aligns with the input image significantly improves the inversion quality. Based on our findings, we introduce Tight Inversion, an inversion method that utilizes the most possible precise condition -- the input image itself. This tight condition narrows the distribution of the model's output and enhances both reconstruction and editability. We demonstrate the effectiveness of our approach when combined with existing inversion methods through extensive experiments, evaluating the reconstruction accuracy as well as the integration with various editing methods.

## 计算机科学与博弈论(cs.GT:Computer Science and Game Theory)

### Swap Regret and Correlated Equilibria Beyond Normal-Form Games 
[[arxiv](https://arxiv.org/abs/2502.20229)] [[cool](https://papers.cool/arxiv/2502.20229)] [[pdf](https://arxiv.org/pdf/2502.20229)]
> **Authors**: Eshwar Ram Arunachaleswaran,Natalie Collina,Yishay Mansour,Mehryar Mohri,Jon Schneider,Balasubramanian Sivan
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算机科学与博弈论,机器学习
- **Abstract**: Swap regret is a notion that has proven itself to be central to the study of general-sum normal-form games, with swap-regret minimization leading to convergence to the set of correlated equilibria and guaranteeing non-manipulability against a self-interested opponent. However, the situation for more general classes of games -- such as Bayesian games and extensive-form games -- is less clear-cut, with multiple candidate definitions for swap-regret but no known efficiently minimizable variant of swap regret that implies analogous non-manipulability guarantees. In this paper, we present a new variant of swap regret for polytope games that we call ``profile swap regret'', with the property that obtaining sublinear profile swap regret is both necessary and sufficient for any learning algorithm to be non-manipulable by an opponent (resolving an open problem of Mansour et al., 2022). Although we show profile swap regret is NP-hard to compute given a transcript of play, we show it is nonetheless possible to design efficient learning algorithms that guarantee at most $O(\sqrt{T})$ profile swap regret. Finally, we explore the correlated equilibrium notion induced by low-profile-swap-regret play, and demonstrate a gap between the set of outcomes that can be implemented by this learning process and the set of outcomes that can be implemented by a third-party mediator (in contrast to the situation in normal-form games).

### Re-evaluating Open-ended Evaluation of Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.20170)] [[cool](https://papers.cool/arxiv/2502.20170)] [[pdf](https://arxiv.org/pdf/2502.20170)]
> **Authors**: Siqi Liu,Ian Gemp,Luke Marris,Georgios Piliouras,Nicolas Heess,Marc Lanctot
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Published at ICLR 2025
- **标题**: None
- **领域**: 计算机科学与博弈论,计算语言学,机器学习,机器学习
- **Abstract**: Evaluation has traditionally focused on ranking candidates for a specific skill. Modern generalist models, such as Large Language Models (LLMs), decidedly outpace this paradigm. Open-ended evaluation systems, where candidate models are compared on user-submitted prompts, have emerged as a popular solution. Despite their many advantages, we show that the current Elo-based rating systems can be susceptible to and even reinforce biases in data, intentional or accidental, due to their sensitivity to redundancies. To address this issue, we propose evaluation as a 3-player game, and introduce novel game-theoretic solution concepts to ensure robustness to redundancy. We show that our method leads to intuitive ratings and provide insights into the competitive landscape of LLM development.

### Hiring under Congestion and Algorithmic Monoculture: Value of Strategic Behavior 
[[arxiv](https://arxiv.org/abs/2502.20063)] [[cool](https://papers.cool/arxiv/2502.20063)] [[pdf](https://arxiv.org/pdf/2502.20063)]
> **Authors**: Jackie Baek,Hamsa Bastani,Shihan Chen
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 计算机科学与博弈论,计算机与社会,机器学习
- **Abstract**: We study the impact of strategic behavior in a setting where firms compete to hire from a shared pool of applicants, and firms use a common algorithm to evaluate them. Each applicant is associated with a scalar score that is observed by all firms, provided by the algorithm. Firms simultaneously make interview decisions, where the number of interviews is capacity-constrained. Job offers are given to those who pass the interview, and an applicant who receives multiple offers accepts one of them uniformly at random. We fully characterize the set of Nash equilibria under this model. Defining social welfare as the total number of applicants who find a job, we then compare the social welfare at a Nash equilibrium to a naive baseline where all firms interview applicants with the highest scores. We show that the Nash equilibrium greatly improves upon social welfare compared to the naive baseline, especially when the interview capacity is small and the number of firms is large. We also show that the price of anarchy is small, providing further appeal for the equilibrium solution. We then study how the firms may converge to a Nash equilibrium. We show that when firms make interview decisions sequentially and each firm takes the best response action assuming they are the last to act, this process converges to an equilibrium when interview capacities are small. However, we show that the task of computing the best response is difficult if firms have to use its own historical samples to estimate it, while this task becomes trivial if firms have information on the degree of competition for each applicant. Therefore, converging to an equilibrium can be greatly facilitated if firms have information on the level of competition for each applicant.

## 人机交互(cs.HC:Human-Computer Interaction)

### Telephone Surveys Meet Conversational AI: Evaluating a LLM-Based Telephone Survey System at Scale 
[[arxiv](https://arxiv.org/abs/2502.20140)] [[cool](https://papers.cool/arxiv/2502.20140)] [[pdf](https://arxiv.org/pdf/2502.20140)]
> **Authors**: Max M. Lang,Sol Eskenazi
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 人机交互,计算语言学
- **Abstract**: Telephone surveys remain a valuable tool for gathering insights but typically require substantial resources in training and coordinating human interviewers. This work presents an AI-driven telephone survey system integrating text-to-speech (TTS), a large language model (LLM), and speech-to-text (STT) that mimics the versatility of human-led interviews on scale. We tested the system across two populations, a pilot study in the United States (n = 75) and a large-scale deployment in Peru (n = 2,739), inviting participants via web-based links and contacting them via direct phone calls. The AI agent successfully administered open-ended and closed-ended questions, handled basic clarifications, and dynamically navigated branching logic, allowing fast large-scale survey deployment without interviewer recruitment or training. Our findings demonstrate that while the AI system's probing for qualitative depth was more limited than human interviewers, overall data quality approached human-led standards for structured items. This study represents one of the first successful large-scale deployments of an LLM-based telephone interviewer in a real-world survey context. The AI-powered telephone survey system has the potential for expanding scalable, consistent data collecting across market research, social science, and public opinion studies, thus improving operational efficiency while maintaining appropriate data quality for research.

## 信息检索(cs.IR:Information Retrieval)

### Granite Embedding Models 
[[arxiv](https://arxiv.org/abs/2502.20204)] [[cool](https://papers.cool/arxiv/2502.20204)] [[pdf](https://arxiv.org/pdf/2502.20204)]
> **Authors**: Parul Awasthy,Aashka Trivedi,Yulong Li,Mihaela Bornea,David Cox,Abraham Daniels,Martin Franz,Gabe Goodhart,Bhavani Iyer,Vishwajeet Kumar,Luis Lastras,Scott McCarley,Rudra Murthy,Vignesh P,Sara Rosenthal,Salim Roukos,Jaydeep Sen,Sukriti Sharma,Avirup Sil,Kate Soule,Arafat Sultan,Radu Florian
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,计算语言学
- **Abstract**: We introduce the Granite Embedding models, a family of encoder-based embedding models designed for retrieval tasks, spanning dense-retrieval and sparse retrieval architectures, with both English and Multilingual capabilities. This report provides the technical details of training these highly effective 12 layer embedding models, along with their efficient 6 layer distilled counterparts. Extensive evaluations show that the models, developed with techniques like retrieval oriented pretraining, contrastive finetuning, knowledge distillation, and model merging significantly outperform publicly available models of similar sizes on both internal IBM retrieval and search tasks, and have equivalent performance on widely used information retrieval benchmarks, while being trained on high-quality data suitable for enterprise use. We publicly release all our Granite Embedding models under the Apache 2.0 license, allowing both research and commercial use at https://huggingface.co/collections/ibm-granite.

## 机器学习(cs.LG:Machine Learning)

### R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts 
[[arxiv](https://arxiv.org/abs/2502.20395)] [[cool](https://papers.cool/arxiv/2502.20395)] [[pdf](https://arxiv.org/pdf/2502.20395)]
> **Authors**: Zhongyang Li,Ziyue Li,Tianyi Zhou
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: In large multimodal models (LMMs), the perception of non-language modalities (e.g., visual representations) is usually not on par with the large language models (LLMs)' powerful reasoning capabilities, deterring LMMs' performance on challenging downstream tasks. This weakness has been recently mitigated by replacing the vision encoder with a mixture-of-experts (MoE), which provides rich, multi-granularity, and diverse representations required by diverse downstream tasks. The performance of multimodal MoE largely depends on its router, which reweights and mixes the representations of different experts for each input. However, we find that the end-to-end trained router does not always produce the optimal routing weights for every test sample. To bridge the gap, we propose a novel and efficient method "Re-Routing in Test-Time(R2-T2) that locally optimizes the vector of routing weights in test-time by moving it toward those vectors of the correctly predicted samples in a neighborhood of the test sample. We propose three R2-T2 strategies with different optimization objectives and neighbor-search spaces. R2-T2 consistently and greatly improves state-of-the-art LMMs' performance on challenging benchmarks of diverse tasks, without training any base-model parameters.

### Walking the Web of Concept-Class Relationships in Incrementally Trained Interpretable Models 
[[arxiv](https://arxiv.org/abs/2502.20393)] [[cool](https://papers.cool/arxiv/2502.20393)] [[pdf](https://arxiv.org/pdf/2502.20393)]
> **Authors**: Susmit Agrawal,Deepika Vemuri,Sri Siddarth Chakaravarthy P,Vineeth N. Balasubramanian
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 8 pages of main text, 6 figures in main text, 11 pages of Appendix, published in AAAI 2025
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: Concept-based methods have emerged as a promising direction to develop interpretable neural networks in standard supervised settings. However, most works that study them in incremental settings assume either a static concept set across all experiences or assume that each experience relies on a distinct set of concepts. In this work, we study concept-based models in a more realistic, dynamic setting where new classes may rely on older concepts in addition to introducing new concepts themselves. We show that concepts and classes form a complex web of relationships, which is susceptible to degradation and needs to be preserved and augmented across experiences. We introduce new metrics to show that existing concept-based models cannot preserve these relationships even when trained using methods to prevent catastrophic forgetting, since they cannot handle forgetting at concept, class, and concept-class relationship levels simultaneously. To address these issues, we propose a novel method - MuCIL - that uses multimodal concepts to perform classification without increasing the number of trainable parameters across experiences. The multimodal concepts are aligned to concepts provided in natural language, making them interpretable by design. Through extensive experimentation, we show that our approach obtains state-of-the-art classification performance compared to other concept-based models, achieving over 2$\times$ the classification performance in some cases. We also study the ability of our model to perform interventions on concepts, and show that it can localize visual concepts in input images, providing post-hoc interpretations.

### Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security Analysis 
[[arxiv](https://arxiv.org/abs/2502.20383)] [[cool](https://papers.cool/arxiv/2502.20383)] [[pdf](https://arxiv.org/pdf/2502.20383)]
> **Authors**: Jeffrey Yang Fan Chiang,Seungjae Lee,Jia-Bin Huang,Furong Huang,Yizheng Chen
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Project website: http://vulnerable-ai-agents.github.io
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Recent advancements in Web AI agents have demonstrated remarkable capabilities in addressing complex web navigation tasks. However, emerging research shows that these agents exhibit greater vulnerability compared to standalone Large Language Models (LLMs), despite both being built upon the same safety-aligned models. This discrepancy is particularly concerning given the greater flexibility of Web AI Agent compared to standalone LLMs, which may expose them to a wider range of adversarial user inputs. To build a scaffold that addresses these concerns, this study investigates the underlying factors that contribute to the increased vulnerability of Web AI agents. Notably, this disparity stems from the multifaceted differences between Web AI agents and standalone LLMs, as well as the complex signals - nuances that simple evaluation metrics, such as success rate, often fail to capture. To tackle these challenges, we propose a component-level analysis and a more granular, systematic evaluation framework. Through this fine-grained investigation, we identify three critical factors that amplify the vulnerability of Web AI agents; (1) embedding user goals into the system prompt, (2) multi-step action generation, and (3) observational capabilities. Our findings highlights the pressing need to enhance security and robustness in AI agent design and provide actionable insights for targeted defense strategies.

### Multi-Turn Code Generation Through Single-Step Rewards 
[[arxiv](https://arxiv.org/abs/2502.20380)] [[cool](https://papers.cool/arxiv/2502.20380)] [[pdf](https://arxiv.org/pdf/2502.20380)]
> **Authors**: Arnav Kumar Jain,Gonzalo Gonzalez-Pumariega,Wayne Chen,Alexander M Rush,Wenting Zhao,Sanjiban Choudhury
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 9 pages (not including references or appendix); 6 figures (in main paper); (v1) preprint
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: We address the problem of code generation from multi-turn execution feedback. Existing methods either generate code without feedback or use complex, hierarchical reinforcement learning to optimize multi-turn rewards. We propose a simple yet scalable approach, $μ$Code, that solves multi-turn code generation using only single-step rewards. Our key insight is that code generation is a one-step recoverable MDP, where the correct code can be recovered from any intermediate code state in a single turn. $μ$Code iteratively trains both a generator to provide code solutions conditioned on multi-turn execution feedback and a verifier to score the newly generated code. Experimental evaluations show that our approach achieves significant improvements over the state-of-the-art baselines. We provide analysis of the design choices of the reward models and policy, and show the efficacy of $μ$Code at utilizing the execution feedback. Our code is available at https://github.com/portal-cornell/muCode.

### PhantomWiki: On-Demand Datasets for Reasoning and Retrieval Evaluation 
[[arxiv](https://arxiv.org/abs/2502.20377)] [[cool](https://papers.cool/arxiv/2502.20377)] [[pdf](https://arxiv.org/pdf/2502.20377)]
> **Authors**: Albert Gong,Kamilė Stankevičiūtė,Chao Wan,Anmol Kabra,Raphael Thesmar,Johann Lee,Julius Klenke,Carla P. Gomes,Kilian Q. Weinberger
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: High-quality benchmarks are essential for evaluating reasoning and retrieval capabilities of large language models (LLMs). However, curating datasets for this purpose is not a permanent solution as they are prone to data leakage and inflated performance results. To address these challenges, we propose PhantomWiki: a pipeline to generate unique, factually consistent document corpora with diverse question-answer pairs. Unlike prior work, PhantomWiki is neither a fixed dataset, nor is it based on any existing data. Instead, a new PhantomWiki instance is generated on demand for each evaluation. We vary the question difficulty and corpus size to disentangle reasoning and retrieval capabilities respectively, and find that PhantomWiki datasets are surprisingly challenging for frontier LLMs. Thus, we contribute a scalable and data leakage-resistant framework for disentangled evaluation of reasoning, retrieval, and tool-use abilities. Our code is available at https://github.com/kilian-group/phantom-wiki.

### When does a predictor know its own loss? 
[[arxiv](https://arxiv.org/abs/2502.20375)] [[cool](https://papers.cool/arxiv/2502.20375)] [[pdf](https://arxiv.org/pdf/2502.20375)]
> **Authors**: Aravind Gollakota,Parikshit Gopalan,Aayush Karan,Charlotte Peale,Udi Wieder
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Given a predictor and a loss function, how well can we predict the loss that the predictor will incur on an input? This is the problem of loss prediction, a key computational task associated with uncertainty estimation for a predictor. In a classification setting, a predictor will typically predict a distribution over labels and hence have its own estimate of the loss that it will incur, given by the entropy of the predicted distribution. Should we trust this estimate? In other words, when does the predictor know what it knows and what it does not know? In this work we study the theoretical foundations of loss prediction. Our main contribution is to establish tight connections between nontrivial loss prediction and certain forms of multicalibration, a multigroup fairness notion that asks for calibrated predictions across computationally identifiable subgroups. Formally, we show that a loss predictor that is able to improve on the self-estimate of a predictor yields a witness to a failure of multicalibration, and vice versa. This has the implication that nontrivial loss prediction is in effect no easier or harder than auditing for multicalibration. We support our theoretical results with experiments that show a robust positive correlation between the multicalibration error of a predictor and the efficacy of training a loss predictor.

### Constrained Generative Modeling with Manually Bridged Diffusion Models 
[[arxiv](https://arxiv.org/abs/2502.20371)] [[cool](https://papers.cool/arxiv/2502.20371)] [[pdf](https://arxiv.org/pdf/2502.20371)]
> **Authors**: Saeid Naderiparizi,Xiaoxuan Liang,Berend Zwartsenberg,Frank Wood
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: AAAI 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: In this paper we describe a novel framework for diffusion-based generative modeling on constrained spaces. In particular, we introduce manual bridges, a framework that expands the kinds of constraints that can be practically used to form so-called diffusion bridges. We develop a mechanism for combining multiple such constraints so that the resulting multiply-constrained model remains a manual bridge that respects all constraints. We also develop a mechanism for training a diffusion model that respects such multiple constraints while also adapting it to match a data distribution. We develop and extend theory demonstrating the mathematical validity of our mechanisms. Additionally, we demonstrate our mechanism in constrained generative modeling tasks, highlighting a particular high-value application in modeling trajectory initializations for path planning and control in autonomous vehicles.

### Safety Representations for Safer Policy Learning 
[[arxiv](https://arxiv.org/abs/2502.20341)] [[cool](https://papers.cool/arxiv/2502.20341)] [[pdf](https://arxiv.org/pdf/2502.20341)]
> **Authors**: Kaustubh Mani,Vincent Mai,Charlie Gauthier,Annie Chen,Samer Nashed,Liam Paull
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Accepted at International Conference onLearningRepresentations (ICLR) 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Reinforcement learning algorithms typically necessitate extensive exploration of the state space to find optimal policies. However, in safety-critical applications, the risks associated with such exploration can lead to catastrophic consequences. Existing safe exploration methods attempt to mitigate this by imposing constraints, which often result in overly conservative behaviours and inefficient learning. Heavy penalties for early constraint violations can trap agents in local optima, deterring exploration of risky yet high-reward regions of the state space. To address this, we introduce a method that explicitly learns state-conditioned safety representations. By augmenting the state features with these safety representations, our approach naturally encourages safer exploration without being excessively cautious, resulting in more efficient and safer policy learning in safety-critical scenarios. Empirical evaluations across diverse environments show that our method significantly improves task performance while reducing constraint violations during training, underscoring its effectiveness in balancing exploration with safety.

### Mixture of Structural-and-Textual Retrieval over Text-rich Graph Knowledge Bases 
[[arxiv](https://arxiv.org/abs/2502.20317)] [[cool](https://papers.cool/arxiv/2502.20317)] [[pdf](https://arxiv.org/pdf/2502.20317)]
> **Authors**: Yongjia Lei,Haoyu Han,Ryan A. Rossi,Franck Dernoncourt,Nedim Lipka,Mahantesh M Halappanavar,Jiliang Tang,Yu Wang
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,信息检索
- **Abstract**: Text-rich Graph Knowledge Bases (TG-KBs) have become increasingly crucial for answering queries by providing textual and structural knowledge. However, current retrieval methods often retrieve these two types of knowledge in isolation without considering their mutual reinforcement and some hybrid methods even bypass structural retrieval entirely after neighboring aggregation. To fill in this gap, we propose a Mixture of Structural-and-Textual Retrieval (MoR) to retrieve these two types of knowledge via a Planning-Reasoning-Organizing framework. In the Planning stage, MoR generates textual planning graphs delineating the logic for answering queries. Following planning graphs, in the Reasoning stage, MoR interweaves structural traversal and textual matching to obtain candidates from TG-KBs. In the Organizing stage, MoR further reranks fetched candidates based on their structural trajectory. Extensive experiments demonstrate the superiority of MoR in harmonizing structural and textual retrieval with insights, including uneven retrieving performance across different query logics and the benefits of integrating structural trajectories for candidate reranking. Our code is available at https://github.com/Yoega/MoR.

### Adversarial Robustness in Parameter-Space Classifiers 
[[arxiv](https://arxiv.org/abs/2502.20314)] [[cool](https://papers.cool/arxiv/2502.20314)] [[pdf](https://arxiv.org/pdf/2502.20314)]
> **Authors**: Tamir Shor,Ethan Fetaya,Chaim Baskin,Alex Bronstein
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Implicit Neural Representations (INRs) have been recently garnering increasing interest in various research fields, mainly due to their ability to represent large, complex data in a compact and continuous manner. Past work further showed that numerous popular downstream tasks can be performed directly in the INR parameter-space. Doing so can substantially reduce the computational resources required to process the represented data in their native domain. A major difficulty in using modern machine-learning approaches, is their high susceptibility to adversarial attacks, which have been shown to greatly limit the reliability and applicability of such methods in a wide range of settings. In this work, we show that parameter-space models trained for classification are inherently robust to adversarial attacks -- without the need of any robust training. To support our claims, we develop a novel suite of adversarial attacks targeting parameter-space classifiers, and furthermore analyze practical considerations of attacking parameter-space classifiers. Code for reproducing all experiments and implementation of all proposed methods will be released upon publication.

### Adapting Automatic Speech Recognition for Accented Air Traffic Control Communications 
[[arxiv](https://arxiv.org/abs/2502.20311)] [[cool](https://papers.cool/arxiv/2502.20311)] [[pdf](https://arxiv.org/pdf/2502.20311)]
> **Authors**: Marcus Yu Zhe Wee,Justin Juin Hng Wong,Lynus Lim,Joe Yu Wei Tan,Prannaya Gupta,Dillion Lim,En Hao Tew,Aloysius Keng Siew Han,Yong Zhi Lim
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,声音,音频和语音处理
- **Abstract**: Effective communication in Air Traffic Control (ATC) is critical to maintaining aviation safety, yet the challenges posed by accented English remain largely unaddressed in Automatic Speech Recognition (ASR) systems. Existing models struggle with transcription accuracy for Southeast Asian-accented (SEA-accented) speech, particularly in noisy ATC environments. This study presents the development of ASR models fine-tuned specifically for Southeast Asian accents using a newly created dataset. Our research achieves significant improvements, achieving a Word Error Rate (WER) of 0.0982 or 9.82% on SEA-accented ATC speech. Additionally, the paper highlights the importance of region-specific datasets and accent-focused training, offering a pathway for deploying ASR systems in resource-constrained military operations. The findings emphasize the need for noise-robust training techniques and region-specific datasets to improve transcription accuracy for non-Western accents in ATC communications.

### An exploration of features to improve the generalisability of fake news detection models 
[[arxiv](https://arxiv.org/abs/2502.20299)] [[cool](https://papers.cool/arxiv/2502.20299)] [[pdf](https://arxiv.org/pdf/2502.20299)]
> **Authors**: Nathaniel Hoy,Theodora Koulouri
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Accepted at Expert Systems with Applications (Elsevier)
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Fake news poses global risks by influencing elections and spreading misinformation, making detection critical. Existing NLP and supervised Machine Learning methods perform well under cross-validation but struggle to generalise across datasets, even within the same domain. This issue stems from coarsely labelled training data, where articles are labelled based on their publisher, introducing biases that token-based models like TF-IDF and BERT are sensitive to. While Large Language Models (LLMs) offer promise, their application in fake news detection remains limited. This study demonstrates that meaningful features can still be extracted from coarsely labelled data to improve real-world robustness. Stylistic features-lexical, syntactic, and semantic-are explored due to their reduced sensitivity to dataset biases. Additionally, novel social-monetisation features are introduced, capturing economic incentives behind fake news, such as advertisements, external links, and social media elements. The study trains on the coarsely labelled NELA 2020-21 dataset and evaluates using the manually labelled Facebook URLs dataset, a gold standard for generalisability. Results highlight the limitations of token-based models trained on biased data and contribute to the scarce evidence on LLMs like LLaMa in this field. Findings indicate that stylistic and social-monetisation features offer more generalisable predictions than token-based methods and LLMs. Statistical and permutation feature importance analyses further reveal their potential to enhance performance and mitigate dataset biases, providing a path forward for improving fake news detection.

### Judge a Book by its Cover: Investigating Multi-Modal LLMs for Multi-Page Handwritten Document Transcription 
[[arxiv](https://arxiv.org/abs/2502.20295)] [[cool](https://papers.cool/arxiv/2502.20295)] [[pdf](https://arxiv.org/pdf/2502.20295)]
> **Authors**: Benjamin Gutteridge,Matthew Thomas Jackson,Toni Kukurin,Xiaowen Dong
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 11 pages (including references and appendix), 14 figures, accepted at AAAI-25 Workshop on Document Understanding and Intelligence, non-archival
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: Handwritten text recognition (HTR) remains a challenging task, particularly for multi-page documents where pages share common formatting and contextual features. While modern optical character recognition (OCR) engines are proficient with printed text, their performance on handwriting is limited, often requiring costly labeled data for fine-tuning. In this paper, we explore the use of multi-modal large language models (MLLMs) for transcribing multi-page handwritten documents in a zero-shot setting. We investigate various configurations of commercial OCR engines and MLLMs, utilizing the latter both as end-to-end transcribers and as post-processors, with and without image components. We propose a novel method, '+first page', which enhances MLLM transcription by providing the OCR output of the entire document along with just the first page image. This approach leverages shared document features without incurring the high cost of processing all images. Experiments on a multi-page version of the IAM Handwriting Database demonstrate that '+first page' improves transcription accuracy, balances cost with performance, and even enhances results on out-of-sample text by extrapolating formatting and OCR error patterns from a single page.

### Scalable Graph Attention-based Instance Selection via Mini-Batch Sampling and Hierarchical Hashing 
[[arxiv](https://arxiv.org/abs/2502.20293)] [[cool](https://papers.cool/arxiv/2502.20293)] [[pdf](https://arxiv.org/pdf/2502.20293)]
> **Authors**: Zahiriddin Rustamov,Ayham Zaitouny,Nazar Zaki
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Instance selection (IS) is important in machine learning for reducing dataset size while keeping key characteristics. Current IS methods often struggle with capturing complex relationships in high-dimensional spaces and scale with large datasets. This paper introduces a graph attention-based instance selection (GAIS) method that uses attention mechanisms to identify informative instances through their structural relationships in graph representations. We present two approaches for scalable graph construction: a distance-based mini-batch sampling technique that reduces computation through strategic batch processing, and a hierarchical hashing approach that allows for efficient similarity computation through random projections. The mini-batch approach keeps class distributions through stratified sampling, while the hierarchical hashing method captures relationships at multiple granularities through single-level, multi-level, and multi-view variants. Experiments across 39 datasets show that GAIS achieves reduction rates above 96\% while maintaining or improving model performance relative to state-of-the-art IS methods. The findings shows that the distance-based mini-batch approach offers an optimal balance of efficiency and effectiveness for large-scale datasets, while multi-view variants provide superior performance for complex, high-dimensional data, demonstrating that attention-based importance scoring can effectively identify instances crucial for maintaining decision boundaries without requiring exhaustive pairwise comparisons.

### Conformal Tail Risk Control for Large Language Model Alignment 
[[arxiv](https://arxiv.org/abs/2502.20285)] [[cool](https://papers.cool/arxiv/2502.20285)] [[pdf](https://arxiv.org/pdf/2502.20285)]
> **Authors**: Catherine Yu-Chi Chen,Jingyan Shen,Zhun Deng,Lihua Lei
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Recent developments in large language models (LLMs) have led to their widespread usage for various tasks. The prevalence of LLMs in society implores the assurance on the reliability of their performance. In particular, risk-sensitive applications demand meticulous attention to unexpectedly poor outcomes, i.e., tail events, for instance, toxic answers, humiliating language, and offensive outputs. Due to the costly nature of acquiring human annotations, general-purpose scoring models have been created to automate the process of quantifying these tail events. This phenomenon introduces potential human-machine misalignment between the respective scoring mechanisms. In this work, we present a lightweight calibration framework for blackbox models that ensures the alignment of humans and machines with provable guarantees. Our framework provides a rigorous approach to controlling any distortion risk measure that is characterized by a weighted average of quantiles of the loss incurred by the LLM with high confidence. The theoretical foundation of our method relies on the connection between conformal risk control and a traditional family of statistics, i.e., L-statistics. To demonstrate the utility of our framework, we conduct comprehensive experiments that address the issue of human-machine misalignment.

### Online Meta-learning for AutoML in Real-time (OnMAR) 
[[arxiv](https://arxiv.org/abs/2502.20279)] [[cool](https://papers.cool/arxiv/2502.20279)] [[pdf](https://arxiv.org/pdf/2502.20279)]
> **Authors**: Mia Gerber,Anna Sergeevna Bosman,Johan Pieter de Villiers
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: First page is a graphical abstract, this is a journal article submission
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Automated machine learning (AutoML) is a research area focusing on using optimisation techniques to design machine learning (ML) algorithms, alleviating the need for a human to perform manual algorithm design. Real-time AutoML enables the design process to happen while the ML algorithm is being applied to a task. Real-time AutoML is an emerging research area, as such existing real-time AutoML techniques need improvement with respect to the quality of designs and time taken to create designs. To address these issues, this study proposes an Online Meta-learning for AutoML in Real-time (OnMAR) approach. Meta-learning gathers information about the optimisation process undertaken by the ML algorithm in the form of meta-features. Meta-features are used in conjunction with a meta-learner to optimise the optimisation process. The OnMAR approach uses a meta-learner to predict the accuracy of an ML design. If the accuracy predicted by the meta-learner is sufficient, the design is used, and if the predicted accuracy is low, an optimisation technique creates a new design. A genetic algorithm (GA) is the optimisation technique used as part of the OnMAR approach. Different meta-learners (k-nearest neighbours, random forest and XGBoost) are tested. The OnMAR approach is model-agnostic (i.e. not specific to a single real-time AutoML application) and therefore evaluated on three different real-time AutoML applications, namely: composing an image clustering algorithm, configuring the hyper-parameters of a convolutional neural network, and configuring a video classification pipeline. The OnMAR approach is effective, matching or outperforming existing real-time AutoML approaches, with the added benefit of a faster runtime.

### Large Language Models as Attribution Regularizers for Efficient Model Training 
[[arxiv](https://arxiv.org/abs/2502.20268)] [[cool](https://papers.cool/arxiv/2502.20268)] [[pdf](https://arxiv.org/pdf/2502.20268)]
> **Authors**: Davor Vukadin,Marin Šilić,Goran Delač
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: :I.2.6
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Large Language Models (LLMs) have demonstrated remarkable performance across diverse domains. However, effectively leveraging their vast knowledge for training smaller downstream models remains an open challenge, especially in domains like tabular data learning, where simpler models are often preferred due to interpretability and efficiency. In this paper, we introduce a novel yet straightforward method for incorporating LLM-generated global task feature attributions into the training process of smaller networks. Specifically, we propose an attribution-matching regularization term that aligns the training dynamics of the smaller model with the insights provided by the LLM. By doing so, our approach yields superior performance in few-shot learning scenarios. Notably, our method requires only black-box API access to the LLM, making it easy to integrate into existing training pipelines with minimal computational overhead. Furthermore, we demonstrate how this method can be used to address common issues in real-world datasets, such as skewness and bias. By integrating high-level knowledge from LLMs, our approach improves generalization, even when training data is limited or imbalanced. We validate its effectiveness through extensive experiments across multiple tasks, demonstrating improved learning efficiency and model robustness.

### On the Importance of Reward Design in Reinforcement Learning-based Dynamic Algorithm Configuration: A Case Study on OneMax with (1+($λ$,$λ$))-GA 
[[arxiv](https://arxiv.org/abs/2502.20265)] [[cool](https://papers.cool/arxiv/2502.20265)] [[pdf](https://arxiv.org/pdf/2502.20265)]
> **Authors**: Tai Nguyen,Phong Le,André Biendenkapp,Carola Doerr,Nguyen Dang
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,神经和进化计算
- **Abstract**: Dynamic Algorithm Configuration (DAC) has garnered significant attention in recent years, particularly in the prevalence of machine learning and deep learning algorithms. Numerous studies have leveraged the robustness of decision-making in Reinforcement Learning (RL) to address the optimization challenges associated with algorithm configuration. However, making an RL agent work properly is a non-trivial task, especially in reward design, which necessitates a substantial amount of handcrafted knowledge based on domain expertise. In this work, we study the importance of reward design in the context of DAC via a case study on controlling the population size of the $(1+(λ,λ))$-GA optimizing OneMax. We observed that a poorly designed reward can hinder the RL agent's ability to learn an optimal policy because of a lack of exploration, leading to both scalability and learning divergence issues. To address those challenges, we propose the application of a reward shaping mechanism to facilitate enhanced exploration of the environment by the RL agent. Our work not only demonstrates the ability of RL in dynamically configuring the $(1+(λ,λ))$-GA, but also confirms the advantages of reward shaping in the scalability of RL agents across various sizes of OneMax problems.

### Understanding the Limits of Deep Tabular Methods with Temporal Shift 
[[arxiv](https://arxiv.org/abs/2502.20260)] [[cool](https://papers.cool/arxiv/2502.20260)] [[pdf](https://arxiv.org/pdf/2502.20260)]
> **Authors**: Hao-Run Cai,Han-Jia Ye
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 17 pages, 9 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Deep tabular models have demonstrated remarkable success on i.i.d. data, excelling in a variety of structured data tasks. However, their performance often deteriorates under temporal distribution shifts, where trends and periodic patterns are present in the evolving data distribution over time. In this paper, we explore the underlying reasons for this failure in capturing temporal dependencies. We begin by investigating the training protocol, revealing a key issue in how model selection perform. While existing approaches use temporal ordering for splitting validation set, we show that even a random split can significantly improve model performance. By minimizing the time lag between training data and test time, while reducing the bias in validation, our proposed training protocol significantly improves generalization across various methods. Furthermore, we analyze how temporal data affects deep tabular representations, uncovering that these models often fail to capture crucial periodic and trend information. To address this gap, we introduce a plug-and-play temporal embedding method based on Fourier series expansion to learn and incorporate temporal patterns, offering an adaptive approach to handle temporal shifts. Our experiments demonstrate that this temporal embedding, combined with the improved training protocol, provides a more effective and robust framework for learning from temporal tabular data.

### The Impact of Transparency in AI Systems on Users' Data-Sharing Intentions: A Scenario-Based Experiment 
[[arxiv](https://arxiv.org/abs/2502.20243)] [[cool](https://papers.cool/arxiv/2502.20243)] [[pdf](https://arxiv.org/pdf/2502.20243)]
> **Authors**: Julian Rosenberger,Sophie Kuhlemann,Verena Tiefenbeck,Mathias Kraus,Patrick Zschech
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: This is the author's version of the paper presented at the 19th International Conference on Wirtschaftsinformatik (WI 2024). The official published version is available at https://aisel.aisnet.org/wi2024/38/
- **标题**: None
- **领域**: 机器学习,人机交互
- **Abstract**: Artificial Intelligence (AI) systems are frequently employed in online services to provide personalized experiences to users based on large collections of data. However, AI systems can be designed in different ways, with black-box AI systems appearing as complex data-processing engines and white-box AI systems appearing as fully transparent data-processors. As such, it is reasonable to assume that these different design choices also affect user perception and thus their willingness to share data. To this end, we conducted a pre-registered, scenario-based online experiment with 240 participants and investigated how transparent and non-transparent data-processing entities influenced data-sharing intentions. Surprisingly, our results revealed no significant difference in willingness to share data across entities, challenging the notion that transparency increases data-sharing willingness. Furthermore, we found that a general attitude of trust towards AI has a significant positive influence, especially in the transparent AI condition, whereas privacy concerns did not significantly affect data-sharing decisions.

### Teasing Apart Architecture and Initial Weights as Sources of Inductive Bias in Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.20237)] [[cool](https://papers.cool/arxiv/2502.20237)] [[pdf](https://arxiv.org/pdf/2502.20237)]
> **Authors**: Gianluca Bencomo,Max Gupta,Ioana Marinescu,R. Thomas McCoy,Thomas L. Griffiths
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 11 pages, 6 figures, 6 tables
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Artificial neural networks can acquire many aspects of human knowledge from data, making them promising as models of human learning. But what those networks can learn depends upon their inductive biases -- the factors other than the data that influence the solutions they discover -- and the inductive biases of neural networks remain poorly understood, limiting our ability to draw conclusions about human learning from the performance of these systems. Cognitive scientists and machine learning researchers often focus on the architecture of a neural network as a source of inductive bias. In this paper we explore the impact of another source of inductive bias -- the initial weights of the network -- using meta-learning as a tool for finding initial weights that are adapted for specific problems. We evaluate four widely-used architectures -- MLPs, CNNs, LSTMs, and Transformers -- by meta-training 430 different models across three tasks requiring different biases and forms of generalization. We find that meta-learning can substantially reduce or entirely eliminate performance differences across architectures and data representations, suggesting that these factors may be less important as sources of inductive bias than is typically assumed. When differences are present, architectures and data representations that perform well without meta-learning tend to meta-train more effectively. Moreover, all architectures generalize poorly on problems that are far from their meta-training experience, underscoring the need for stronger inductive biases for robust generalization.

### Mixture of Experts for Recognizing Depression from Interview and Reading Tasks 
[[arxiv](https://arxiv.org/abs/2502.20213)] [[cool](https://papers.cool/arxiv/2502.20213)] [[pdf](https://arxiv.org/pdf/2502.20213)]
> **Authors**: Loukas Ilias,Dimitris Askounis
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机与社会
- **Abstract**: Depression is a mental disorder and can cause a variety of symptoms, including psychological, physical, and social. Speech has been proved an objective marker for the early recognition of depression. For this reason, many studies have been developed aiming to recognize depression through speech. However, existing methods rely on the usage of only the spontaneous speech neglecting information obtained via read speech, use transcripts which are often difficult to obtain (manual) or come with high word-error rates (automatic), and do not focus on input-conditional computation methods. To resolve these limitations, this is the first study in depression recognition task obtaining representations of both spontaneous and read speech, utilizing multimodal fusion methods, and employing Mixture of Experts (MoE) models in a single deep neural network. Specifically, we use audio files corresponding to both interview and reading tasks and convert each audio file into log-Mel spectrogram, delta, and delta-delta. Next, the image representations of the two tasks pass through shared AlexNet models. The outputs of the AlexNet models are given as input to a multimodal fusion method. The resulting vector is passed through a MoE module. In this study, we employ three variants of MoE, namely sparsely-gated MoE and multilinear MoE based on factorization. Findings suggest that our proposed approach yields an Accuracy and F1-score of 87.00% and 86.66% respectively on the Androids corpus.

### Highly Parallelized Reinforcement Learning Training with Relaxed Assignment Dependencies 
[[arxiv](https://arxiv.org/abs/2502.20190)] [[cool](https://papers.cool/arxiv/2502.20190)] [[pdf](https://arxiv.org/pdf/2502.20190)]
> **Authors**: Zhouyu He,Peng Qiao,Rongchun Li,Yong Dou,Yusong Tan
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: As the demands for superior agents grow, the training complexity of Deep Reinforcement Learning (DRL) becomes higher. Thus, accelerating training of DRL has become a major research focus. Dividing the DRL training process into subtasks and using parallel computation can effectively reduce training costs. However, current DRL training systems lack sufficient parallelization due to data assignment between subtask components. This assignment issue has been ignored, but addressing it can further boost training efficiency. Therefore, we propose a high-throughput distributed RL training system called TianJi. It relaxes assignment dependencies between subtask components and enables event-driven asynchronous communication. Meanwhile, TianJi maintains clear boundaries between subtask components. To address convergence uncertainty from relaxed assignment dependencies, TianJi proposes a distributed strategy based on the balance of sample production and consumption. The strategy controls the staleness of samples to correct their quality, ensuring convergence. We conducted extensive experiments. TianJi achieves a convergence time acceleration ratio of up to 4.37 compared to related comparison systems. When scaled to eight computational nodes, TianJi shows a convergence time speedup of 1.6 and a throughput speedup of 7.13 relative to XingTian, demonstrating its capability to accelerate training and scalability. In data transmission efficiency experiments, TianJi significantly outperforms other systems, approaching hardware limits. TianJi also shows effectiveness in on-policy algorithms, achieving convergence time acceleration ratios of 4.36 and 2.95 compared to RLlib and XingTian. TianJi is accessible at https://github.com/HiPRL/TianJi.git.

### Mixture of Experts-augmented Deep Unfolding for Activity Detection in IRS-aided Systems 
[[arxiv](https://arxiv.org/abs/2502.20183)] [[cool](https://papers.cool/arxiv/2502.20183)] [[pdf](https://arxiv.org/pdf/2502.20183)]
> **Authors**: Zeyi Ren,Qingfeng Lin,Jingreng Lei,Yang Li,Yik-Chung Wu
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 5 pages, 4 figures, Submitted to IEEE Wireless Communications Letters
- **标题**: None
- **领域**: 机器学习
- **Abstract**: In the realm of activity detection for massive machine-type communications, intelligent reflecting surfaces (IRS) have shown significant potential in enhancing coverage for devices lacking direct connections to the base station (BS). However, traditional activity detection methods are typically designed for a single type of channel model, which does not reflect the complexities of real-world scenarios, particularly in systems incorporating IRS. To address this challenge, this paper introduces a novel approach that combines model-driven deep unfolding with a mixture of experts (MoE) framework. By automatically selecting one of three expert designs and applying it to the unfolded projected gradient method, our approach eliminates the need for prior knowledge of channel types between devices and the BS. Simulation results demonstrate that the proposed MoE-augmented deep unfolding method surpasses the traditional covariance-based method and black-box neural network design, delivering superior detection performance under mixed channel fading conditions.

### Similarity-Distance-Magnitude Universal Verification 
[[arxiv](https://arxiv.org/abs/2502.20167)] [[cool](https://papers.cool/arxiv/2502.20167)] [[pdf](https://arxiv.org/pdf/2502.20167)]
> **Authors**: Allen Schmaltz
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 35 pages (8 Tables, 4 Algorithms, 5 Listings)
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: We solve the neural network robustness problem by adding Similarity (i.e., correctly predicted depth-matches into training)-awareness and Distance-to-training-distribution-awareness to the existing output Magnitude (i.e., decision-boundary)-awareness of the softmax function. The resulting sdm activation function provides strong signals of the relative epistemic (reducible) predictive uncertainty. We use this novel behavior to further address the complementary HCI problem of mapping the output to human-interpretable summary statistics over relevant partitions of a held-out calibration set. Estimates of prediction-conditional uncertainty are obtained via a parsimonious learned transform over the class-conditional empirical CDFs of the output of a final-layer sdm activation function. For decision-making and as an intrinsic model check, estimates of class-conditional accuracy are obtained by further partitioning the high-probability regions of this calibrated output into class-conditional, region-specific CDFs. The uncertainty estimates from sdm calibration are remarkably robust to test-time distribution shifts and out-of-distribution inputs; incorporate awareness of the effective sample size; provide estimates of uncertainty from the learning and data splitting processes; and are well-suited for selective classification and conditional branching for additional test-time compute based on the predictive uncertainty, as for selective LLM generation, routing, and composition over multiple models and retrieval. Finally, we construct sdm networks, LLMs with uncertainty-aware verification and interpretability-by-exemplar as intrinsic properties. We provide open-source software implementing these results.

### Gradient-Guided Annealing for Domain Generalization 
[[arxiv](https://arxiv.org/abs/2502.20162)] [[cool](https://papers.cool/arxiv/2502.20162)] [[pdf](https://arxiv.org/pdf/2502.20162)]
> **Authors**: Aristotelis Ballas,Christos Diou
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Paper accepted in CVPR2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Domain Generalization (DG) research has gained considerable traction as of late, since the ability to generalize to unseen data distributions is a requirement that eludes even state-of-the-art training algorithms. In this paper we observe that the initial iterations of model training play a key role in domain generalization effectiveness, since the loss landscape may be significantly different across the training and test distributions, contrary to the case of i.i.d. data. Conflicts between gradients of the loss components of each domain lead the optimization procedure to undesirable local minima that do not capture the domain-invariant features of the target classes. We propose alleviating domain conflicts in model optimization, by iteratively annealing the parameters of a model in the early stages of training and searching for points where gradients align between domains. By discovering a set of parameter values where gradients are updated towards the same direction for each data distribution present in the training set, the proposed Gradient-Guided Annealing (GGA) algorithm encourages models to seek out minima that exhibit improved robustness against domain shifts. The efficacy of GGA is evaluated on five widely accepted and challenging image classification domain generalization benchmarks, where its use alone is able to establish highly competitive or even state-of-the-art performance. Moreover, when combined with previously proposed domain-generalization algorithms it is able to consistently improve their effectiveness by significant margins.

### Transfer Learning in Latent Contextual Bandits with Covariate Shift Through Causal Transportability 
[[arxiv](https://arxiv.org/abs/2502.20153)] [[cool](https://papers.cool/arxiv/2502.20153)] [[pdf](https://arxiv.org/pdf/2502.20153)]
> **Authors**: Mingwei Deng,Ville Kyrki,Dominik Baumann
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Accepted at the Conference of CausalLearningand Reasoning (CLeaR 2025), will be published in the Proceedings ofMachineLearningResearch
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Transferring knowledge from one environment to another is an essential ability of intelligent systems. Nevertheless, when two environments are different, naively transferring all knowledge may deteriorate the performance, a phenomenon known as negative transfer. In this paper, we address this issue within the framework of multi-armed bandits from the perspective of causal inference. Specifically, we consider transfer learning in latent contextual bandits, where the actual context is hidden, but a potentially high-dimensional proxy is observable. We further consider a covariate shift in the context across environments. We show that naively transferring all knowledge for classical bandit algorithms in this setting led to negative transfer. We then leverage transportability theory from causal inference to develop algorithms that explicitly transfer effective knowledge for estimating the causal effects of interest in the target environment. Besides, we utilize variational autoencoders to approximate causal effects under the presence of a high-dimensional proxy. We test our algorithms on synthetic and semi-synthetic datasets, empirically demonstrating consistently improved learning efficiency across different proxies compared to baseline algorithms, showing the effectiveness of our causal framework in transferring knowledge.

### Your contrastive learning problem is secretly a distribution alignment problem 
[[arxiv](https://arxiv.org/abs/2502.20141)] [[cool](https://papers.cool/arxiv/2502.20141)] [[pdf](https://arxiv.org/pdf/2502.20141)]
> **Authors**: Zihao Chen,Chi-Heng Lin,Ran Liu,Jingyun Xiao,Eva L Dyer
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 10 pages, 5 figures, NeurIPS 2024 submission, includes supplementary material
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Despite the success of contrastive learning (CL) in vision and language, its theoretical foundations and mechanisms for building representations remain poorly understood. In this work, we build connections between noise contrastive estimation losses widely used in CL and distribution alignment with entropic optimal transport (OT). This connection allows us to develop a family of different losses and multistep iterative variants for existing CL methods. Intuitively, by using more information from the distribution of latents, our approach allows a more distribution-aware manipulation of the relationships within augmented sample sets. We provide theoretical insights and experimental evidence demonstrating the benefits of our approach for {\em generalized contrastive alignment}. Through this framework, it is possible to leverage tools in OT to build unbalanced losses to handle noisy views and customize the representation space by changing the constraints on alignment. By reframing contrastive learning as an alignment problem and leveraging existing optimization tools for OT, our work provides new insights and connections between different self-supervised learning models in addition to new tools that can be more easily adapted to incorporate domain knowledge into learning.

### LimeSoDa: A Dataset Collection for Benchmarking of Machine Learning Regressors in Digital Soil Mapping 
[[arxiv](https://arxiv.org/abs/2502.20139)] [[cool](https://papers.cool/arxiv/2502.20139)] [[pdf](https://arxiv.org/pdf/2502.20139)]
> **Authors**: J. Schmidinger,S. Vogel,V. Barkov,A. -D. Pham,R. Gebbers,H. Tavakoli,J. Correa,T. R. Tavares,P. Filippi,E. J. Jones,V. Lukas,E. Boenecke,J. Ruehlmann,I. Schroeter,E. Kramer,S. Paetzold,M. Kodaira,A. M. J. -C. Wadoux,L. Bragazza,K. Metzger,J. Huang,D. S. M. Valente,J. L. Safanelli,E. L. Bottega,R. S. D. Dalmolin, et al. (11 additional authors not shown)
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Digital soil mapping (DSM) relies on a broad pool of statistical methods, yet determining the optimal method for a given context remains challenging and contentious. Benchmarking studies on multiple datasets are needed to reveal strengths and limitations of commonly used methods. Existing DSM studies usually rely on a single dataset with restricted access, leading to incomplete and potentially misleading conclusions. To address these issues, we introduce an open-access dataset collection called Precision Liming Soil Datasets (LimeSoDa). LimeSoDa consists of 31 field- and farm-scale datasets from various countries. Each dataset has three target soil properties: (1) soil organic matter or soil organic carbon, (2) clay content and (3) pH, alongside a set of features. Features are dataset-specific and were obtained by optical spectroscopy, proximal- and remote soil sensing. All datasets were aligned to a tabular format and are ready-to-use for modeling. We demonstrated the use of LimeSoDa for benchmarking by comparing the predictive performance of four learning algorithms across all datasets. This comparison included multiple linear regression (MLR), support vector regression (SVR), categorical boosting (CatBoost) and random forest (RF). The results showed that although no single algorithm was universally superior, certain algorithms performed better in specific contexts. MLR and SVR performed better on high-dimensional spectral datasets, likely due to better compatibility with principal components. In contrast, CatBoost and RF exhibited considerably better performances when applied to datasets with a moderate number (< 20) of features. These benchmarking results illustrate that the performance of a method is highly context-dependent. LimeSoDa therefore provides an important resource for improving the development and evaluation of statistical methods in DSM.

### Regional climate projections using a deep learning--based model-ranking and downscaling framework: Application to European climate zones 
[[arxiv](https://arxiv.org/abs/2502.20132)] [[cool](https://papers.cool/arxiv/2502.20132)] [[pdf](https://arxiv.org/pdf/2502.20132)]
> **Authors**: Parthiban Loganathan,Elias Zea,Ricardo Vinuesa,Evelyn Otero
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: This manuscript has been submitted to Environmental Science and Pollution Research (ESPR) for review
- **标题**: None
- **领域**: 机器学习,数值分析
- **Abstract**: Accurate regional climate forecast calls for high-resolution downscaling of Global Climate Models (GCMs). This work presents a deep-learning-based multi-model evaluation and downscaling framework ranking 32 Coupled Model Intercomparison Project Phase 6 (CMIP6) models using a Deep Learning-TOPSIS (DL-TOPSIS) mechanism and so refines outputs using advanced deep-learning models. Using nine performance criteria, five Köppen-Geiger climate zones -- Tropical, Arid, Temperate, Continental, and Polar -- are investigated over four seasons. While TaiESM1 and CMCC-CM2-SR5 show notable biases, ranking results show that NorESM2-LM, GISS-E2-1-G, and HadGEM3-GC31-LL outperform other models. Four models contribute to downscaling the top-ranked GCMs to 0.1$^{\circ}$ resolution: Vision Transformer (ViT), Geospatial Spatiotemporal Transformer with Attention and Imbalance-Aware Network (GeoSTANet), CNN-LSTM, and CNN-Long Short-Term Memory (ConvLSTM). Effectively capturing temperature extremes (TXx, TNn), GeoSTANet achieves the highest accuracy (Root Mean Square Error (RMSE) = 1.57$^{\circ}$C, Kling-Gupta Efficiency (KGE) = 0.89, Nash-Sutcliffe Efficiency (NSE) = 0.85, Correlation ($r$) = 0.92), so reducing RMSE by 20\% over ConvLSTM. CNN-LSTM and ConvLSTM do well in Continental and Temperate zones; ViT finds fine-scale temperature fluctuations difficult. These results confirm that multi-criteria ranking improves GCM selection for regional climate studies and transformer-based downscaling exceeds conventional deep-learning methods. This framework offers a scalable method to enhance high-resolution climate projections, benefiting impact assessments and adaptation plans.

### FlexiDiT: Your Diffusion Transformer Can Easily Generate High-Quality Samples with Less Compute 
[[arxiv](https://arxiv.org/abs/2502.20126)] [[cool](https://papers.cool/arxiv/2502.20126)] [[pdf](https://arxiv.org/pdf/2502.20126)]
> **Authors**: Sotiris Anagnostidis,Gregor Bachmann,Yeongmin Kim,Jonas Kohler,Markos Georgopoulos,Artsiom Sanakoyeu,Yuming Du,Albert Pumarola,Ali Thabet,Edgar Schönfeld
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: Despite their remarkable performance, modern Diffusion Transformers are hindered by substantial resource requirements during inference, stemming from the fixed and large amount of compute needed for each denoising step. In this work, we revisit the conventional static paradigm that allocates a fixed compute budget per denoising iteration and propose a dynamic strategy instead. Our simple and sample-efficient framework enables pre-trained DiT models to be converted into \emph{flexible} ones -- dubbed FlexiDiT -- allowing them to process inputs at varying compute budgets. We demonstrate how a single \emph{flexible} model can generate images without any drop in quality, while reducing the required FLOPs by more than $40$\% compared to their static counterparts, for both class-conditioned and text-conditioned image generation. Our method is general and agnostic to input and conditioning modalities. We show how our approach can be readily extended for video generation, where FlexiDiT models generate samples with up to $75$\% less compute without compromising performance.

### Exploring Open-world Continual Learning with Knowns-Unknowns Knowledge Transfer 
[[arxiv](https://arxiv.org/abs/2502.20124)] [[cool](https://papers.cool/arxiv/2502.20124)] [[pdf](https://arxiv.org/pdf/2502.20124)]
> **Authors**: Yujie Li,Guannan Lai,Xin Yang,Yonghao Li,Marcello Bonsangue,Tianrui Li
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Open-World Continual Learning (OWCL) is a challenging paradigm where models must incrementally learn new knowledge without forgetting while operating under an open-world assumption. This requires handling incomplete training data and recognizing unknown samples during inference. However, existing OWCL methods often treat open detection and continual learning as separate tasks, limiting their ability to integrate open-set detection and incremental classification in OWCL. Moreover, current approaches primarily focus on transferring knowledge from known samples, neglecting the insights derived from unknown/open samples. To address these limitations, we formalize four distinct OWCL scenarios and conduct comprehensive empirical experiments to explore potential challenges in OWCL. Our findings reveal a significant interplay between the open detection of unknowns and incremental classification of knowns, challenging a widely held assumption that unknown detection and known classification are orthogonal processes. Building on our insights, we propose \textbf{HoliTrans} (Holistic Knowns-Unknowns Knowledge Transfer), a novel OWCL framework that integrates nonlinear random projection (NRP) to create a more linearly separable embedding space and distribution-aware prototypes (DAPs) to construct an adaptive knowledge space. Particularly, our HoliTrans effectively supports knowledge transfer for both known and unknown samples while dynamically updating representations of open samples during OWCL. Extensive experiments across various OWCL scenarios demonstrate that HoliTrans outperforms 22 competitive baselines, bridging the gap between OWCL theory and practice and providing a robust, scalable framework for advancing open-world learning paradigms.

### Identifiable Multi-View Causal Discovery Without Non-Gaussianity 
[[arxiv](https://arxiv.org/abs/2502.20115)] [[cool](https://papers.cool/arxiv/2502.20115)] [[pdf](https://arxiv.org/pdf/2502.20115)]
> **Authors**: Ambroise Heurtebise,Omar Chehab,Pierre Ablin,Alexandre Gramfort,Aapo Hyvärinen
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: :62R07 (Primary) 68T05; 05C82 (Secondary)ACM Class:I.2.6; I.5.1
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: We propose a novel approach to linear causal discovery in the framework of multi-view Structural Equation Models (SEM). Our proposed model relaxes the well-known assumption of non-Gaussian disturbances by alternatively assuming diversity of variances over views, making it more broadly applicable. We prove the identifiability of all the parameters of the model without any further assumptions on the structure of the SEM other than it being acyclic. We further propose an estimation algorithm based on recent advances in multi-view Independent Component Analysis (ICA). The proposed methodology is validated through simulations and application on real neuroimaging data, where it enables the estimation of causal graphs between brain regions.

### Forward-Cooperation-Backward (FCB) learning in a Multi-Encoding Uni-Decoding neural network architecture 
[[arxiv](https://arxiv.org/abs/2502.20113)] [[cool](https://papers.cool/arxiv/2502.20113)] [[pdf](https://arxiv.org/pdf/2502.20113)]
> **Authors**: Prasun Dutta,Koustab Ghosh,Rajat K. De
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别,神经和进化计算
- **Abstract**: The most popular technique to train a neural network is backpropagation. Recently, the Forward-Forward technique has also been introduced for certain learning tasks. However, in real life, human learning does not follow any of these techniques exclusively. The way a human learns is basically a combination of forward learning, backward propagation and cooperation. Humans start learning a new concept by themselves and try to refine their understanding hierarchically during which they might come across several doubts. The most common approach to doubt solving is a discussion with peers, which can be called cooperation. Cooperation/discussion/knowledge sharing among peers is one of the most important steps of learning that humans follow. However, there might still be a few doubts even after the discussion. Then the difference between the understanding of the concept and the original literature is identified and minimized over several revisions. Inspired by this, the paper introduces Forward-Cooperation-Backward (FCB) learning in a deep neural network framework mimicking the human nature of learning a new concept. A novel deep neural network architecture, called Multi Encoding Uni Decoding neural network model, has been designed which learns using the notion of FCB. A special lateral synaptic connection has also been introduced to realize cooperation. The models have been justified in terms of their performance in dimension reduction on four popular datasets. The ability to preserve the granular properties of data in low-rank embedding has been tested to justify the quality of dimension reduction. For downstream analyses, classification has also been performed. An experimental study on convergence analysis has been performed to establish the efficacy of the FCB learning strategy.

### Sanity Checking Causal Representation Learning on a Simple Real-World System 
[[arxiv](https://arxiv.org/abs/2502.20099)] [[cool](https://papers.cool/arxiv/2502.20099)] [[pdf](https://arxiv.org/pdf/2502.20099)]
> **Authors**: Juan L. Gamella,Simon Bing,Jakob Runge
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 24 pages, 12 figures
- **标题**: None
- **领域**: 机器学习,人工智能,方法论
- **Abstract**: We evaluate methods for causal representation learning (CRL) on a simple, real-world system where these methods are expected to work. The system consists of a controlled optical experiment specifically built for this purpose, which satisfies the core assumptions of CRL and where the underlying causal factors (the inputs to the experiment) are known, providing a ground truth. We select methods representative of different approaches to CRL and find that they all fail to recover the underlying causal factors. To understand the failure modes of the evaluated algorithms, we perform an ablation on the data by substituting the real data-generating process with a simpler synthetic equivalent. The results reveal a reproducibility problem, as most methods already fail on this synthetic ablation despite its simple data-generating process. Additionally, we observe that common assumptions on the mixing function are crucial for the performance of some of the methods but do not hold in the real data. Our efforts highlight the contrast between the theoretical promise of the state of the art and the challenges in its application. We hope the benchmark serves as a simple, real-world sanity check to further develop and validate methodology, bridging the gap towards CRL methods that work in practice. We make all code and datasets publicly available at github.com/simonbing/CRLSanityCheck

### RIZE: Regularized Imitation Learning via Distributional Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.20089)] [[cool](https://papers.cool/arxiv/2502.20089)] [[pdf](https://arxiv.org/pdf/2502.20089)]
> **Authors**: Adib Karimi,Mohammad Mehdi Ebadzadeh
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器人技术
- **Abstract**: We introduce a novel Inverse Reinforcement Learning (IRL) approach that overcomes limitations of fixed reward assignments and constrained flexibility in implicit reward regularization. By extending the Maximum Entropy IRL framework with a squared temporal-difference (TD) regularizer and adaptive targets, dynamically adjusted during training, our method indirectly optimizes a reward function while incorporating reinforcement learning principles. Furthermore, we integrate distributional RL to capture richer return information. Our approach achieves state-of-the-art performance on challenging MuJoCo tasks, demonstrating expert-level results on the Humanoid task with only 3 demonstrations. Extensive experiments and ablation studies validate the effectiveness of our method, providing insights into adaptive targets and reward dynamics in imitation learning.

### A Generative Model Enhanced Multi-Agent Reinforcement Learning Method for Electric Vehicle Charging Navigation 
[[arxiv](https://arxiv.org/abs/2502.20068)] [[cool](https://papers.cool/arxiv/2502.20068)] [[pdf](https://arxiv.org/pdf/2502.20068)]
> **Authors**: Tianyang Qi,Shibo Chen,Jun Zhang
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: With the widespread adoption of electric vehicles (EVs), navigating for EV drivers to select a cost-effective charging station has become an important yet challenging issue due to dynamic traffic conditions, fluctuating electricity prices, and potential competition from other EVs. The state-of-the-art deep reinforcement learning (DRL) algorithms for solving this task still require global information about all EVs at the execution stage, which not only increases communication costs but also raises privacy issues among EV drivers. To overcome these drawbacks, we introduce a novel generative model-enhanced multi-agent DRL algorithm that utilizes only the EV's local information while achieving performance comparable to these state-of-the-art algorithms. Specifically, the policy network is implemented on the EV side, and a Conditional Variational Autoencoder-Long Short Term Memory (CVAE-LSTM)-based recommendation model is developed to provide recommendation information. Furthermore, a novel future charging competition encoder is designed to effectively compress global information, enhancing training performance. The multi-gradient descent algorithm (MGDA) is also utilized to adaptively balance the weight between the two parts of the training objective, resulting in a more stable training process. Simulations are conducted based on a practical area in Xián, China. Experimental results show that our proposed algorithm, which relies on local information, outperforms existing local information-based methods and achieves less than 8\% performance loss compared to global information-based methods.

### Recommendations from Sparse Comparison Data: Provably Fast Convergence for Nonconvex Matrix Factorization 
[[arxiv](https://arxiv.org/abs/2502.20033)] [[cool](https://papers.cool/arxiv/2502.20033)] [[pdf](https://arxiv.org/pdf/2502.20033)]
> **Authors**: Suryanarayana Sankagiri,Jalal Etesami,Matthias Grossglauser
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 42 pages, 1 figure
- **标题**: None
- **领域**: 机器学习
- **Abstract**: This paper provides a theoretical analysis of a new learning problem for recommender systems where users provide feedback by comparing pairs of items instead of rating them individually. We assume that comparisons stem from latent user and item features, which reduces the task of predicting preferences to learning these features from comparison data. Similar to the classical matrix factorization problem, the main challenge in this learning task is that the resulting loss function is nonconvex. Our analysis shows that the loss function exhibits (restricted) strong convexity near the true solution, which ensures gradient-based methods converge exponentially, given an appropriate warm start. Importantly, this result holds in a sparse data regime, where each user compares only a few pairs of items. Our main technical contribution is to extend certain concentration inequalities commonly used in matrix completion to our model. Our work demonstrates that learning personalized recommendations from comparison data is computationally and statistically efficient.

### Order-Robust Class Incremental Learning: Graph-Driven Dynamic Similarity Grouping 
[[arxiv](https://arxiv.org/abs/2502.20032)] [[cool](https://papers.cool/arxiv/2502.20032)] [[pdf](https://arxiv.org/pdf/2502.20032)]
> **Authors**: Guannan Lai,Yujie Li,Xiangkun Wang,Junbo Zhang,Tianrui Li,Xin Yang
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Accepted by the proceeding of CVPR2025
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Class Incremental Learning (CIL) requires a model to continuously learn new classes without forgetting previously learned ones. While recent studies have significantly alleviated the problem of catastrophic forgetting (CF), more and more research reveals that the order in which classes appear have significant influences on CIL models. Specifically, prioritizing the learning of classes with lower similarity will enhance the model's generalization performance and its ability to mitigate forgetting. Hence, it is imperative to develop an order-robust class incremental learning model that maintains stable performance even when faced with varying levels of class similarity in different orders. In response, we first provide additional theoretical analysis, which reveals that when the similarity among a group of classes is lower, the model demonstrates increased robustness to the class order. Then, we introduce a novel \textbf{G}raph-\textbf{D}riven \textbf{D}ynamic \textbf{S}imilarity \textbf{G}rouping (\textbf{GDDSG}) method, which leverages a graph coloring algorithm for class-based similarity grouping. The proposed approach trains independent CIL models for each group of classes, ultimately combining these models to facilitate joint prediction. Experimental results demonstrate that our method effectively addresses the issue of class order sensitivity while achieving optimal performance in both model accuracy and anti-forgetting capability. Our code is available at https://github.com/AIGNLAI/GDDSG.

### Offline Reinforcement Learning via Inverse Optimization 
[[arxiv](https://arxiv.org/abs/2502.20030)] [[cool](https://papers.cool/arxiv/2502.20030)] [[pdf](https://arxiv.org/pdf/2502.20030)]
> **Authors**: Ioannis Dimanidis,Tolga Ok,Peyman Mohajerin Esfahani
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: preprint
- **标题**: None
- **领域**: 机器学习,系统与控制,优化与控制
- **Abstract**: Inspired by the recent successes of Inverse Optimization (IO) across various application domains, we propose a novel offline Reinforcement Learning (ORL) algorithm for continuous state and action spaces, leveraging the convex loss function called ``sub-optimality loss" from the IO literature. To mitigate the distribution shift commonly observed in ORL problems, we further employ a robust and non-causal Model Predictive Control (MPC) expert steering a nominal model of the dynamics using in-hindsight information stemming from the model mismatch. Unlike the existing literature, our robust MPC expert enjoys an exact and tractable convex reformulation. In the second part of this study, we show that the IO hypothesis class, trained by the proposed convex loss function, enjoys ample expressiveness and achieves competitive performance comparing with the state-of-the-art (SOTA) methods in the low-data regime of the MuJoCo benchmark while utilizing three orders of magnitude fewer parameters, thereby requiring significantly fewer computational resources. To facilitate the reproducibility of our results, we provide an open-source package implementing the proposed algorithms and the experiments.

### Climate And Resource Awareness is Imperative to Achieving Sustainable AI (and Preventing a Global AI Arms Race) 
[[arxiv](https://arxiv.org/abs/2502.20016)] [[cool](https://papers.cool/arxiv/2502.20016)] [[pdf](https://arxiv.org/pdf/2502.20016)]
> **Authors**: Pedram Bakhtiarifard,Pınar Tözün,Christian Igel,Raghavendra Selvan
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 19 pages, 6 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Sustainability encompasses three key facets: economic, environmental, and social. However, the nascent discourse that is emerging on sustainable artificial intelligence (AI) has predominantly focused on the environmental sustainability of AI, often neglecting the economic and social aspects. Achieving truly sustainable AI necessitates addressing the tension between its climate awareness and its social sustainability, which hinges on equitable access to AI development resources. The concept of resource awareness advocates for broader access to the infrastructure required to develop AI, fostering equity in AI innovation. Yet, this push for improving accessibility often overlooks the environmental costs of expanding such resource usage. In this position paper, we argue that reconciling climate and resource awareness is essential to realizing the full potential of sustainable AI. We use the framework of base-superstructure to analyze how the material conditions are influencing the current AI discourse. We also introduce the Climate and Resource Aware Machine Learning (CARAML) framework to address this conflict and propose actionable recommendations spanning individual, community, industry, government, and global levels to achieve sustainable AI.

### Learning Classifiers That Induce Markets 
[[arxiv](https://arxiv.org/abs/2502.20012)] [[cool](https://papers.cool/arxiv/2502.20012)] [[pdf](https://arxiv.org/pdf/2502.20012)]
> **Authors**: Yonatan Sommer,Ivri Hikri,Lotan Amit,Nir Rosenfeld
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: When learning is used to inform decisions about humans, such as for loans, hiring, or admissions, this can incentivize users to strategically modify their features to obtain positive predictions. A key assumption is that modifications are costly, and are governed by a cost function that is exogenous and predetermined. We challenge this assumption, and assert that the deployment of a classifier is what creates costs. Our idea is simple: when users seek positive predictions, this creates demand for important features; and if features are available for purchase, then a market will form, and competition will give rise to prices. We extend the strategic classification framework to support this notion, and study learning in a setting where a classifier can induce a market for features. We present an analysis of the learning task, devise an algorithm for computing market prices, propose a differentiable learning framework, and conduct experiments to explore our novel setting and approach.

### Learning Hamiltonian Density Using DeepONet 
[[arxiv](https://arxiv.org/abs/2502.19994)] [[cool](https://papers.cool/arxiv/2502.19994)] [[pdf](https://arxiv.org/pdf/2502.19994)]
> **Authors**: Baige Xu,Yusuke Tanaka,Takashi Matsubara,Takaharu Yaguchi
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: In recent years, deep learning for modeling physical phenomena which can be described by partial differential equations (PDEs) have received significant attention. For example, for learning Hamiltonian mechanics, methods based on deep neural networks such as Hamiltonian Neural Networks (HNNs) and their variants have achieved progress. However, existing methods typically depend on the discretization of data, and the determination of required differential operators is often necessary. Instead, in this work, we propose an operator learning approach for modeling wave equations. In particular, we present a method to compute the variational derivatives that are needed to formulate the equations using the automatic differentiation algorithm. The experiments demonstrated that the proposed method is able to learn the operator that defines the Hamiltonian density of waves from data with unspecific discretization without determination of the differential operators.

### Dam Volume Prediction Model Development Using ML Algorithms 
[[arxiv](https://arxiv.org/abs/2502.19989)] [[cool](https://papers.cool/arxiv/2502.19989)] [[pdf](https://arxiv.org/pdf/2502.19989)]
> **Authors**: Hugo Retief,Mariangel Garcia Andarcia,Chris Dickens,Surajit Ghosh
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 22 pages, 18 Figures and 4 Tables
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Reliable reservoir volume estimates are crucial for water resource management, especially in arid and semi-arid regions. The present study investigates applying three machine learning regression techniques - Gradient Boosting, Random Forest, and ElasticNet to predict key dam performance characteristics of the Loskop Dam in South Africa. The models were trained and validated on a dataset comprising geospatial elevation measurements paired with corresponding reservoir supply capacity values. The best-performing approach was a threshold-based blended model that combined random forest for higher volumes with Ridge regression for lower volumes. This model achieved an RMSE of 4.88 MCM and an R2 of 0.99. These findings highlight the ability of ensemble learning techniques to capture complex relationships in dam datasets and underscore their practical utility for reliable dam performance modelling in real-world water resource management scenarios.

### WaveGAS: Waveform Relaxation for Scaling Graph Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.19986)] [[cool](https://papers.cool/arxiv/2502.19986)] [[pdf](https://arxiv.org/pdf/2502.19986)]
> **Authors**: Jana Vatter,Mykhaylo Zayats,Marcos Martínez Galindo,Vanessa López,Ruben Mayer,Hans-Arno Jacobsen,Hoang Thanh Lam
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: With the ever-growing size of real-world graphs, numerous techniques to overcome resource limitations when training Graph Neural Networks (GNNs) have been developed. One such approach, GNNAutoScale (GAS), uses graph partitioning to enable training under constrained GPU memory. GAS also stores historical embedding vectors, which are retrieved from one-hop neighbors in other partitions, ensuring critical information is captured across partition boundaries. The historical embeddings which come from the previous training iteration are stale compared to the GAS estimated embeddings, resulting in approximation errors of the training algorithm. Furthermore, these errors accumulate over multiple layers, leading to suboptimal node embeddings. To address this shortcoming, we propose two enhancements: first, WaveGAS, inspired by waveform relaxation, performs multiple forward passes within GAS before the backward pass, refining the approximation of historical embeddings and gradients to improve accuracy; second, a gradient-tracking method that stores and utilizes more accurate historical gradients during training. Empirical results show that WaveGAS enhances GAS and achieves better accuracy, even outperforming methods that train on full graphs, thanks to its robust estimation of node embeddings.

### Efficient Time Series Forecasting via Hyper-Complex Models and Frequency Aggregation 
[[arxiv](https://arxiv.org/abs/2502.19983)] [[cool](https://papers.cool/arxiv/2502.19983)] [[pdf](https://arxiv.org/pdf/2502.19983)]
> **Authors**: Eyal Yakir,Dor Tsur,Haim Permuter
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 12 pages, 5 figures. Still awaiting conference submission approval
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Time series forecasting is a long-standing problem in statistics and machine learning. One of the key challenges is processing sequences with long-range dependencies. To that end, a recent line of work applied the short-time Fourier transform (STFT), which partitions the sequence into multiple subsequences and applies a Fourier transform to each separately. We propose the Frequency Information Aggregation (FIA)-Net, which is based on a novel complex-valued MLP architecture that aggregates adjacent window information in the frequency domain. To further increase the receptive field of the FIA-Net, we treat the set of windows as hyper-complex (HC) valued vectors and employ HC algebra to efficiently combine information from all STFT windows altogether. Using the HC-MLP backbone allows for improved handling of sequences with long-term dependence. Furthermore, due to the nature of HC operations, the HC-MLP uses up to three times fewer parameters than the equivalent standard window aggregation method. We evaluate the FIA-Net on various time-series benchmarks and show that the proposed methodologies outperform existing state of the art methods in terms of both accuracy and efficiency. Our code is publicly available on https://anonymous.4open.science/r/research-1803/.

### Can Textual Gradient Work in Federated Learning? 
[[arxiv](https://arxiv.org/abs/2502.19980)] [[cool](https://papers.cool/arxiv/2502.19980)] [[pdf](https://arxiv.org/pdf/2502.19980)]
> **Authors**: Minghui Chen,Ruinan Jin,Wenlong Deng,Yuanyuan Chen,Zhi Huang,Han Yu,Xiaoxiao Li
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Accepted at ICLR 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Recent studies highlight the promise of LLM-based prompt optimization, especially with TextGrad, which automates differentiation'' via texts and backpropagates textual feedback. This approach facilitates training in various real-world applications that do not support numerical gradient propagation or loss calculation. In this paper, we systematically explore the potential and challenges of incorporating textual gradient into Federated Learning (FL). Our contributions are fourfold. Firstly, we introduce a novel FL paradigm, Federated Textual Gradient (FedTextGrad), that allows clients to upload locally optimized prompts derived from textual gradients, while the server aggregates the received prompts. Unlike traditional FL frameworks, which are designed for numerical aggregation, FedTextGrad is specifically tailored for handling textual data, expanding the applicability of FL to a broader range of problems that lack well-defined numerical loss functions. Secondly, building on this design, we conduct extensive experiments to explore the feasibility of FedTextGrad. Our findings highlight the importance of properly tuning key factors (e.g., local steps) in FL training. Thirdly, we highlight a major challenge in FedTextGrad aggregation: retaining essential information from distributed prompt updates. Last but not least, in response to this issue, we improve the vanilla variant of FedTextGrad by providing actionable guidance to the LLM when summarizing client prompts by leveraging the Uniform Information Density principle. Through this principled study, we enable the adoption of textual gradients in FL for optimizing LLMs, identify important issues, and pinpoint future directions, thereby opening up a new research area that warrants further investigation.

### Do Sparse Autoencoders Generalize? A Case Study of Answerability 
[[arxiv](https://arxiv.org/abs/2502.19964)] [[cool](https://papers.cool/arxiv/2502.19964)] [[pdf](https://arxiv.org/pdf/2502.19964)]
> **Authors**: Lovis Heindrich,Philip Torr,Fazl Barez,Veronika Thost
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Sparse autoencoders (SAEs) have emerged as a promising approach in language model interpretability, offering unsupervised extraction of sparse features. For interpretability methods to succeed, they must identify abstract features across domains, and these features can often manifest differently in each context. We examine this through "answerability"-a model's ability to recognize answerable questions. We extensively evaluate SAE feature generalization across diverse answerability datasets for Gemma 2 SAEs. Our analysis reveals that residual stream probes outperform SAE features within domains, but generalization performance differs sharply. SAE features demonstrate inconsistent transfer ability, and residual stream probes similarly show high variance out of distribution. Overall, this demonstrates the need for quantitative methods to predict feature generalization in SAE-based interpretability.

### SeisMoLLM: Advancing Seismic Monitoring via Cross-modal Transfer with Pre-trained Large Language Model 
[[arxiv](https://arxiv.org/abs/2502.19960)] [[cool](https://papers.cool/arxiv/2502.19960)] [[pdf](https://arxiv.org/pdf/2502.19960)]
> **Authors**: Xinghao Wang,Feng Liu,Rui Su,Zhihui Wang,Lei Bai,Wanli Ouyang
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 13 pages, 6 figures. Code is available at https://github.com/StarMoonWang/SeisMoLLM
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Recent advances in deep learning have revolutionized seismic monitoring, yet developing a foundation model that performs well across multiple complex tasks remains challenging, particularly when dealing with degraded signals or data scarcity. This work presents SeisMoLLM, the first foundation model that utilizes cross-modal transfer for seismic monitoring, to unleash the power of large-scale pre-training from a large language model without requiring direct pre-training on seismic datasets. Through elaborate waveform tokenization and fine-tuning of pre-trained GPT-2 model, SeisMoLLM achieves state-of-the-art performance on the DiTing and STEAD datasets across five critical tasks: back-azimuth estimation, epicentral distance estimation, magnitude estimation, phase picking, and first-motion polarity classification. It attains 36 best results out of 43 task metrics and 12 top scores out of 16 few-shot generalization metrics, with many relative improvements ranging from 10% to 50%. In addition to its superior performance, SeisMoLLM maintains efficiency comparable to or even better than lightweight models in both training and inference. These findings establish SeisMoLLM as a promising foundation model for practical seismic monitoring and highlight cross-modal transfer as an exciting new direction for earthquake studies, showcasing the potential of advanced deep learning techniques to propel seismology research forward.

### Machine-learning for photoplethysmography analysis: Benchmarking feature, image, and signal-based approaches 
[[arxiv](https://arxiv.org/abs/2502.19949)] [[cool](https://papers.cool/arxiv/2502.19949)] [[pdf](https://arxiv.org/pdf/2502.19949)]
> **Authors**: Mohammad Moulaeifard,Loic Coquelin,Mantas Rinkevičius,Andrius Sološenko,Oskar Pfeffer,Ciaran Bench,Nando Hegemann,Sara Vardanega,Manasi Nandi,Jordi Alastruey,Christian Heiss,Vaidotas Marozas,Andrew Thompson,Philip J. Aston,Peter H. Charlton,Nils Strodthoff
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 39 pages, 9 figures, code available at https://gitlab.com/qumphy/d1-code
- **标题**: None
- **领域**: 机器学习,信号处理
- **Abstract**: Photoplethysmography (PPG) is a widely used non-invasive physiological sensing technique, suitable for various clinical applications. Such clinical applications are increasingly supported by machine learning methods, raising the question of the most appropriate input representation and model choice. Comprehensive comparisons, in particular across different input representations, are scarce. We address this gap in the research landscape by a comprehensive benchmarking study covering three kinds of input representations, interpretable features, image representations and raw waveforms, across prototypical regression and classification use cases: blood pressure and atrial fibrillation prediction. In both cases, the best results are achieved by deep neural networks operating on raw time series as input representations. Within this model class, best results are achieved by modern convolutional neural networks (CNNs). but depending on the task setup, shallow CNNs are often also very competitive. We envision that these results will be insightful for researchers to guide their choice on machine learning tasks for PPG data, even beyond the use cases presented in this work.

### Dynamic DropConnect: Enhancing Neural Network Robustness through Adaptive Edge Dropping Strategies 
[[arxiv](https://arxiv.org/abs/2502.19948)] [[cool](https://papers.cool/arxiv/2502.19948)] [[pdf](https://arxiv.org/pdf/2502.19948)]
> **Authors**: Yuan-Chih Yang,Hung-Hsuan Chen
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Dropout and DropConnect are well-known techniques that apply a consistent drop rate to randomly deactivate neurons or edges in a neural network layer during training. This paper introduces a novel methodology that assigns dynamic drop rates to each edge within a layer, uniquely tailoring the dropping process without incorporating additional learning parameters. We perform experiments on synthetic and openly available datasets to validate the effectiveness of our approach. The results demonstrate that our method outperforms Dropout, DropConnect, and Standout, a classic mechanism known for its adaptive dropout capabilities. Furthermore, our approach improves the robustness and generalization of neural network training without increasing computational complexity. The complete implementation of our methodology is publicly accessible for research and replication purposes at https://github.com/ericabd888/Adjusting-the-drop-probability-in-DropConnect-based-on-the-magnitude-of-the-gradient/.

### Algebraic Machine Learning: Learning as computing an algebraic decomposition of a task 
[[arxiv](https://arxiv.org/abs/2502.19944)] [[cool](https://papers.cool/arxiv/2502.19944)] [[pdf](https://arxiv.org/pdf/2502.19944)]
> **Authors**: Fernando Martin-Maroto,Nabil Abderrahaman,David Mendez,Gonzalo G. de Polavieja
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: :03G10; 06A12; 06A06; 08A70; 68R01; 68T01ACM Class:G.2.3; I.1.2; I.2.6; I.2.8
- **标题**: None
- **领域**: 机器学习,人工智能,离散数学,符号计算,组合学
- **Abstract**: Statistics and Optimization are foundational to modern Machine Learning. Here, we propose an alternative foundation based on Abstract Algebra, with mathematics that facilitates the analysis of learning. In this approach, the goal of the task and the data are encoded as axioms of an algebra, and a model is obtained where only these axioms and their logical consequences hold. Although this is not a generalizing model, we show that selecting specific subsets of its breakdown into algebraic atoms obtained via subdirect decomposition gives a model that generalizes. We validate this new learning principle on standard datasets such as MNIST, FashionMNIST, CIFAR-10, and medical images, achieving performance comparable to optimized multilayer perceptrons. Beyond data-driven tasks, the new learning principle extends to formal problems, such as finding Hamiltonian cycles from their specifications and without relying on search. This algebraic foundation offers a fresh perspective on machine intelligence, featuring direct learning from training data without the need for validation dataset, scaling through model additivity, and asymptotic convergence to the underlying rule in the data.

### Flexible Bivariate Beta Mixture Model: A Probabilistic Approach for Clustering Complex Data Structures 
[[arxiv](https://arxiv.org/abs/2502.19938)] [[cool](https://papers.cool/arxiv/2502.19938)] [[pdf](https://arxiv.org/pdf/2502.19938)]
> **Authors**: Yung-Peng Hsu,Hung-Hsuan Chen
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Clustering is essential in data analysis and machine learning, but traditional algorithms like $k$-means and Gaussian Mixture Models (GMM) often fail with nonconvex clusters. To address the challenge, we introduce the Flexible Bivariate Beta Mixture Model (FBBMM), which utilizes the flexibility of the bivariate beta distribution to handle diverse and irregular cluster shapes. Using the Expectation Maximization (EM) algorithm and Sequential Least Squares Programming (SLSQP) optimizer for parameter estimation, we validate FBBMM on synthetic and real-world datasets, demonstrating its superior performance in clustering complex data structures, offering a robust solution for big data analytics across various domains. We release the experimental code at https://github.com/yung-peng/MBMM-and-FBBMM.

### Lotus at SemEval-2025 Task 11: RoBERTa with Llama-3 Generated Explanations for Multi-Label Emotion Classification 
[[arxiv](https://arxiv.org/abs/2502.19935)] [[cool](https://papers.cool/arxiv/2502.19935)] [[pdf](https://arxiv.org/pdf/2502.19935)]
> **Authors**: Niloofar Ranjbar,Hamed Baghbani
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 8 pages , submitted to SemEval 2025-Task 11
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: This paper presents a novel approach for multi-label emotion detection, where Llama-3 is used to generate explanatory content that clarifies ambiguous emotional expressions, thereby enhancing RoBERTa's emotion classification performance. By incorporating explanatory context, our method improves F1-scores, particularly for emotions like fear, joy, and sadness, and outperforms text-only models. The addition of explanatory content helps resolve ambiguity, addresses challenges like overlapping emotional cues, and enhances multi-label classification, marking a significant advancement in emotion detection tasks.

### Incremental Learning with Repetition via Pseudo-Feature Projection 
[[arxiv](https://arxiv.org/abs/2502.19922)] [[cool](https://papers.cool/arxiv/2502.19922)] [[pdf](https://arxiv.org/pdf/2502.19922)]
> **Authors**: Benedikt Tscheschner,Eduardo Veas,Marc Masana
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: Incremental Learning scenarios do not always represent real-world inference use-cases, which tend to have less strict task boundaries, and exhibit repetition of common classes and concepts in their continual data stream. To better represent these use-cases, new scenarios with partial repetition and mixing of tasks are proposed, where the repetition patterns are innate to the scenario and unknown to the strategy. We investigate how exemplar-free incremental learning strategies are affected by data repetition, and we adapt a series of state-of-the-art approaches to analyse and fairly compare them under both settings. Further, we also propose a novel method (Horde), able to dynamically adjust an ensemble of self-reliant feature extractors, and align them by exploiting class repetition. Our proposed exemplar-free method achieves competitive results in the classic scenario without repetition, and state-of-the-art performance in the one with repetition.

### Shifting the Paradigm: A Diffeomorphism Between Time Series Data Manifolds for Achieving Shift-Invariancy in Deep Learning 
[[arxiv](https://arxiv.org/abs/2502.19921)] [[cool](https://papers.cool/arxiv/2502.19921)] [[pdf](https://arxiv.org/pdf/2502.19921)]
> **Authors**: Berken Utku Demirel,Christian Holz
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: To appear at the International Conference onLearningRepresentation (ICLR) 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Deep learning models lack shift invariance, making them sensitive to input shifts that cause changes in output. While recent techniques seek to address this for images, our findings show that these approaches fail to provide shift-invariance in time series, where the data generation mechanism is more challenging due to the interaction of low and high frequencies. Worse, they also decrease performance across several tasks. In this paper, we propose a novel differentiable bijective function that maps samples from their high-dimensional data manifold to another manifold of the same dimension, without any dimensional reduction. Our approach guarantees that samples -- when subjected to random shifts -- are mapped to a unique point in the manifold while preserving all task-relevant information without loss. We theoretically and empirically demonstrate that the proposed transformation guarantees shift-invariance in deep learning models without imposing any limits to the shift. Our experiments on six time series tasks with state-of-the-art methods show that our approach consistently improves the performance while enabling models to achieve complete shift-invariance without modifying or imposing restrictions on the model's topology. The source code is available on \href{https://github.com/eth-siplab/Shifting-the-Paradigm}{GitHub}.

### Playing Pokémon Red via Deep Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.19920)] [[cool](https://papers.cool/arxiv/2502.19920)] [[pdf](https://arxiv.org/pdf/2502.19920)]
> **Authors**: Marco Pleines,Daniel Addis,David Rubinstein,Frank Zimmer,Mike Preuss,Peter Whidden
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 8 pages, 3 figures, 3 tables, under review
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Pokémon Red, a classic Game Boy JRPG, presents significant challenges as a testbed for agents, including multi-tasking, long horizons of tens of thousands of steps, hard exploration, and a vast array of potential policies. We introduce a simplistic environment and a Deep Reinforcement Learning (DRL) training methodology, demonstrating a baseline agent that completes an initial segment of the game up to completing Cerulean City. Our experiments include various ablations that reveal vulnerabilities in reward shaping, where agents exploit specific reward signals. We also discuss limitations and argue that games like Pokémon hold strong potential for future research on Large Language Model agents, hierarchical training algorithms, and advanced exploration methods. Source Code: https://github.com/MarcoMeter/neroRL/tree/poke_red

### SkipPipe: Partial and Reordered Pipelining Framework for Training LLMs in Heterogeneous Networks 
[[arxiv](https://arxiv.org/abs/2502.19913)] [[cool](https://papers.cool/arxiv/2502.19913)] [[pdf](https://arxiv.org/pdf/2502.19913)]
> **Authors**: Nikolay Blagoev,Lydia Yiyu Chen,Oğuzhan Ersoy
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,分布式、并行和集群计算
- **Abstract**: Data and pipeline parallelism are ubiquitous for training of Large Language Models (LLM) on distributed nodes. Driven by the need for cost-effective training, recent work explores efficient communication arrangement for end to end training. Motivated by LLM's resistance to layer skipping and layer reordering, in this paper, we explore stage (several consecutive layers) skipping in pipeline training, and challenge the conventional practice of sequential pipeline execution. We derive convergence and throughput constraints (guidelines) for pipelining with skipping and swapping pipeline stages. Based on these constraints, we propose SkipPipe, the first partial pipeline framework to reduce the end-to-end training time for LLMs while preserving the convergence. The core of SkipPipe is a path scheduling algorithm that optimizes the paths for individual microbatches and reduces idle time (due to microbatch collisions) on the distributed nodes, complying with the given stage skipping ratio. We extensively evaluate SkipPipe on LLaMa models from 500M to 8B parameters on up to 20 nodes. Our results show that SkipPipe reduces training iteration time by up to $55\%$ compared to full pipeline. Our partial pipeline training also improves resistance to layer omission during inference, experiencing a drop in perplexity of only $7\%$ when running only half the model. Our code is available at https://github.com/gensyn-ai/skippipe.

### Graph Probability Aggregation Clustering 
[[arxiv](https://arxiv.org/abs/2502.19897)] [[cool](https://papers.cool/arxiv/2502.19897)] [[pdf](https://arxiv.org/pdf/2502.19897)]
> **Authors**: Yuxuan Yan,Na Lu,Difei Mei,Ruofan Yan,Youtian Du
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: Traditional clustering methods typically focus on either cluster-wise global clustering or point-wise local clustering to reveal the intrinsic structures in unlabeled data. Global clustering optimizes an objective function to explore the relationships between clusters, but this approach may inevitably lead to coarse partition. In contrast, local clustering heuristically groups data based on detailed point relationships, but it tends to be less coherence and efficient. To bridge the gap between these two concepts and utilize the strengths of both, we propose Graph Probability Aggregation Clustering (GPAC), a graph-based fuzzy clustering algorithm. GPAC unifies the global clustering objective function with a local clustering constraint. The entire GPAC framework is formulated as a multi-constrained optimization problem, which can be solved using the Lagrangian method. Through the optimization process, the probability of a sample belonging to a specific cluster is iteratively calculated by aggregating information from neighboring samples within the graph. We incorporate a hard assignment variable into the objective function to further improve the convergence and stability of optimization. Furthermore, to efficiently handle large-scale datasets, we introduce an acceleration program that reduces the computational complexity from quadratic to linear, ensuring scalability. Extensive experiments conducted on synthetic, real-world, and deep learning datasets demonstrate that GPAC not only exceeds existing state-of-the-art methods in clustering performance but also excels in computational efficiency, making it a powerful tool for complex clustering challenges.

### IL-SOAR : Imitation Learning with Soft Optimistic Actor cRitic 
[[arxiv](https://arxiv.org/abs/2502.19859)] [[cool](https://papers.cool/arxiv/2502.19859)] [[pdf](https://arxiv.org/pdf/2502.19859)]
> **Authors**: Stefano Viel,Luca Viano,Volkan Cevher
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: This paper introduces the SOAR framework for imitation learning. SOAR is an algorithmic template that learns a policy from expert demonstrations with a primal dual style algorithm that alternates cost and policy updates. Within the policy updates, the SOAR framework uses an actor critic method with multiple critics to estimate the critic uncertainty and build an optimistic critic fundamental to drive exploration. When instantiated in the tabular setting, we get a provable algorithm with guarantees that matches the best known results in $ε$. Practically, the SOAR template is shown to boost consistently the performance of imitation learning algorithms based on Soft Actor Critic such as f-IRL, ML-IRL and CSIL in several MuJoCo environments. Overall, thanks to SOAR, the required number of episodes to achieve the same performance is reduced by half.

### Revisit the Stability of Vanilla Federated Learning Under Diverse Conditions 
[[arxiv](https://arxiv.org/abs/2502.19849)] [[cool](https://papers.cool/arxiv/2502.19849)] [[pdf](https://arxiv.org/pdf/2502.19849)]
> **Authors**: Youngjoon Lee,Jinu Gong,Sun Choi,Joonhyuk Kang
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 10 pages
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Federated Learning (FL) is a distributed machine learning paradigm enabling collaborative model training across decentralized clients while preserving data privacy. In this paper, we revisit the stability of the vanilla FedAvg algorithm under diverse conditions. Despite its conceptual simplicity, FedAvg exhibits remarkably stable performance compared to more advanced FL techniques. Our experiments assess the performance of various FL methods on blood cell and skin lesion classification tasks using Vision Transformer (ViT). Additionally, we evaluate the impact of different representative classification models and analyze sensitivity to hyperparameter variations. The results consistently demonstrate that, regardless of dataset, classification model employed, or hyperparameter settings, FedAvg maintains robust performance. Given its stability, robust performance without the need for extensive hyperparameter tuning, FedAvg is a safe and efficient choice for FL deployments in resource-constrained hospitals handling medical data. These findings underscore the enduring value of the vanilla FedAvg approach as a trusted baseline for clinical practice.

### Knowledge Bridger: Towards Training-free Missing Multi-modality Completion 
[[arxiv](https://arxiv.org/abs/2502.19834)] [[cool](https://papers.cool/arxiv/2502.19834)] [[pdf](https://arxiv.org/pdf/2502.19834)]
> **Authors**: Guanzhou Ke,Shengfeng He,Xiao Li Wang,Bo Wang,Guoqing Chao,Yuanyang Zhang,Yi Xie,HeXing Su
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Accepted to CVPR 2025
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别,多媒体
- **Abstract**: Previous successful approaches to missing modality completion rely on carefully designed fusion techniques and extensive pre-training on complete data, which can limit their generalizability in out-of-domain (OOD) scenarios. In this study, we pose a new challenge: can we develop a missing modality completion model that is both resource-efficient and robust to OOD generalization? To address this, we present a training-free framework for missing modality completion that leverages large multimodal models (LMMs). Our approach, termed the "Knowledge Bridger", is modality-agnostic and integrates generation and ranking of missing modalities. By defining domain-specific priors, our method automatically extracts structured information from available modalities to construct knowledge graphs. These extracted graphs connect the missing modality generation and ranking modules through the LMM, resulting in high-quality imputations of missing modalities. Experimental results across both general and medical domains show that our approach consistently outperforms competing methods, including in OOD generalization. Additionally, our knowledge-driven generation and ranking techniques demonstrate superiority over variants that directly employ LMMs for generation and ranking, offering insights that may be valuable for applications in other domains.

### GraphSparseNet: a Novel Method for Large Scale Trafffic Flow Prediction 
[[arxiv](https://arxiv.org/abs/2502.19823)] [[cool](https://papers.cool/arxiv/2502.19823)] [[pdf](https://arxiv.org/pdf/2502.19823)]
> **Authors**: Weiyang Kong,Kaiqi Wu,Sen Zhang,Yubao Liu
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Traffic flow forecasting is a critical spatio-temporal data mining task with wide-ranging applications in intelligent route planning and dynamic traffic management. Recent advancements in deep learning, particularly through Graph Neural Networks (GNNs), have significantly enhanced the accuracy of these forecasts by capturing complex spatio-temporal dynamics. However, the scalability of GNNs remains a challenge due to their exponential growth in model complexity with increasing nodes in the graph. Existing methods to address this issue, including sparsification, decomposition, and kernel-based approaches, either do not fully resolve the complexity issue or risk compromising predictive accuracy. This paper introduces GraphSparseNet (GSNet), a novel framework designed to improve both the scalability and accuracy of GNN-based traffic forecasting models. GraphSparseNet is comprised of two core modules: the Feature Extractor and the Relational Compressor. These modules operate with linear time and space complexity, thereby reducing the overall computational complexity of the model to a linear scale. Our extensive experiments on multiple real-world datasets demonstrate that GraphSparseNet not only significantly reduces training time by 3.51x compared to state-of-the-art linear models but also maintains high predictive performance.

### Advancing GDP Forecasting: The Potential of Machine Learning Techniques in Economic Predictions 
[[arxiv](https://arxiv.org/abs/2502.19807)] [[cool](https://papers.cool/arxiv/2502.19807)] [[pdf](https://arxiv.org/pdf/2502.19807)]
> **Authors**: Bogdan Oancea
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: ef:Knowledge on Economics and Management Conference Proceedings, 2024, Olomouc, The Czech Republic
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The quest for accurate economic forecasting has traditionally been dominated by econometric models, which most of the times rely on the assumptions of linear relationships and stationarity in of the data. However, the complex and often nonlinear nature of global economies necessitates the exploration of alternative approaches. Machine learning methods offer promising advantages over traditional econometric techniques for Gross Domestic Product forecasting, given their ability to model complex, nonlinear interactions and patterns without the need for explicit specification of the underlying relationships. This paper investigates the efficacy of Recurrent Neural Networks, in forecasting GDP, specifically LSTM networks. These models are compared against a traditional econometric method, SARIMA. We employ the quarterly Romanian GDP dataset from 1995 to 2023 and build a LSTM network to forecast to next 4 values in the series. Our findings suggest that machine learning models, consistently outperform traditional econometric models in terms of predictive accuracy and flexibility

### Implicit Search via Discrete Diffusion: A Study on Chess 
[[arxiv](https://arxiv.org/abs/2502.19805)] [[cool](https://papers.cool/arxiv/2502.19805)] [[pdf](https://arxiv.org/pdf/2502.19805)]
> **Authors**: Jiacheng Ye,Zhenyu Wu,Jiahui Gao,Zhiyong Wu,Xin Jiang,Zhenguo Li,Lingpeng Kong
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: ICLR 2025
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: In the post-AlphaGo era, there has been a renewed interest in search techniques such as Monte Carlo Tree Search (MCTS), particularly in their application to Large Language Models (LLMs). This renewed attention is driven by the recognition that current next-token prediction models often lack the ability for long-term planning. Is it possible to instill search-like abilities within the models to enhance their planning abilities without relying on explicit search? We propose DiffuSearch , a model that does \textit{implicit search} by looking into the future world via discrete diffusion modeling. We instantiate DiffuSearch on a classical board game, Chess, where explicit search is known to be essential. Through extensive controlled experiments, we show DiffuSearch outperforms both the searchless and explicit search-enhanced policies. Specifically, DiffuSearch outperforms the one-step policy by 19.2% and the MCTS-enhanced policy by 14% on action accuracy. Furthermore, DiffuSearch demonstrates a notable 30% enhancement in puzzle-solving abilities compared to explicit search-based policies, along with a significant 540 Elo increase in game-playing strength assessment. These results indicate that implicit search via discrete diffusion is a viable alternative to explicit search over a one-step policy. All codes are publicly available at \href{https://github.com/HKUNLP/DiffuSearch}{https://github.com/HKUNLP/DiffuSearch}.

### ServoLNN: Lagrangian Neural Networks Driven by Servomechanisms 
[[arxiv](https://arxiv.org/abs/2502.19802)] [[cool](https://papers.cool/arxiv/2502.19802)] [[pdf](https://arxiv.org/pdf/2502.19802)]
> **Authors**: Brandon Johns,Zhuomin Zhou,Elahe Abdi
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 22 pages, 8 figures
- **标题**: None
- **领域**: 机器学习,机器人技术,动力系统
- **Abstract**: Combining deep learning with classical physics facilitates the efficient creation of accurate dynamical models. In a recent class of neural network, Lagrangian mechanics is hard-coded into the architecture, and training the network learns the given system. However, the current architectures do not facilitate the modelling of dynamical systems that are driven by servomechanisms (e.g. servomotors, stepper motors, current sources, volumetric pumps). This article presents ServoLNN, a new architecture to model dynamical systems that are driven by servomechanisms. ServoLNN is compatible for use in real-time applications, where the driving motion is known only just-in-time. A PyTorch implementation of ServoLNN is provided. The derivations and results reveal the occurrence of a possible family of solutions that the training may converge on. The effect of the family of solutions on the predicted physical quantities is explored, as is the resolution to reduce the family of solutions to a single solution. Resultantly, the architecture can simultaneously accurately find the energies, power, rate of work, mass matrix, generalised accelerations, generalised forces, and the generalised forces that drive the servomechanisms.

### Mixtera: A Data Plane for Foundation Model Training 
[[arxiv](https://arxiv.org/abs/2502.19790)] [[cool](https://papers.cool/arxiv/2502.19790)] [[pdf](https://arxiv.org/pdf/2502.19790)]
> **Authors**: Maximilian Böther,Xiaozhe Yao,Tolga Kerimoglu,Ana Klimovic
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: under submission
- **标题**: None
- **领域**: 机器学习,人工智能,数据库
- **Abstract**: State-of-the-art large language and vision models are trained over trillions of tokens that are aggregated from a large variety of sources. As training data collections grow, manually managing the samples becomes time-consuming, tedious, and prone to errors. Yet recent research shows that the data mixture and the order in which samples are visited during training can significantly influence model accuracy. We build and present Mixtera, a data plane for foundation model training that enables users to declaratively express which data samples should be used in which proportion and in which order during training. Mixtera is a centralized, read-only layer that is deployed on top of existing training data collections and can be declaratively queried. It operates independently of the filesystem structure and supports mixtures across arbitrary properties (e.g., language, source dataset) as well as dynamic adjustment of the mixture based on model feedback. We experimentally evaluate Mixtera and show that our implementation does not bottleneck training and scales to 256 GH200 superchips. We demonstrate how Mixtera supports recent advancements in mixing strategies by implementing the proposed Adaptive Data Optimization (ADO) algorithm in the system and evaluating its performance impact. We also explore the role of mixtures for vision-language models.

### In-Context Learning with Hypothesis-Class Guidance 
[[arxiv](https://arxiv.org/abs/2502.19787)] [[cool](https://papers.cool/arxiv/2502.19787)] [[pdf](https://arxiv.org/pdf/2502.19787)]
> **Authors**: Ziqian Lin,Shubham Kumar Bharti,Kangwook Lee
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 19 pages, 18 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Recent research has investigated the underlying mechanisms of in-context learning (ICL) both theoretically and empirically, often using data generated from simple function classes. However, the existing work often focuses on the sequence consisting solely of labeled examples, while in practice, labeled examples are typically accompanied by an instruction, providing some side information about the task. In this work, we propose ICL with hypothesis-class guidance (ICL-HCG), a novel synthetic data model for ICL where the input context consists of the literal description of a (finite) hypothesis class $\mathcal{H}$ and $(x,y)$ pairs from a hypothesis chosen from $\mathcal{H}$. Under our framework ICL-HCG, we conduct extensive experiments to explore: (i) a variety of generalization abilities to new hypothesis classes; (ii) different model architectures; (iii) sample complexity; (iv) in-context data imbalance; (v) the role of instruction; and (vi) the effect of pretraining hypothesis diversity. As a result, we show that (a) Transformers can successfully learn ICL-HCG and generalize to unseen hypotheses and unseen hypothesis classes, and (b) compared with ICL without instruction, ICL-HCG achieves significantly higher accuracy, demonstrating the role of instructions.

### Obtaining Example-Based Explanations from Deep Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.19768)] [[cool](https://papers.cool/arxiv/2502.19768)] [[pdf](https://arxiv.org/pdf/2502.19768)]
> **Authors**: Genghua Dong,Henrik Boström,Michalis Vazirgiannis,Roman Bresson
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: To be published in the Symposium on Intelligent Data Analysis (IDA) 2025
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Most techniques for explainable machine learning focus on feature attribution, i.e., values are assigned to the features such that their sum equals the prediction. Example attribution is another form of explanation that assigns weights to the training examples, such that their scalar product with the labels equals the prediction. The latter may provide valuable complementary information to feature attribution, in particular in cases where the features are not easily interpretable. Current example-based explanation techniques have targeted a few model types only, such as k-nearest neighbors and random forests. In this work, a technique for obtaining example-based explanations from deep neural networks (EBE-DNN) is proposed. The basic idea is to use the deep neural network to obtain an embedding, which is employed by a k-nearest neighbor classifier to form a prediction; the example attribution can hence straightforwardly be derived from the latter. Results from an empirical investigation show that EBE-DNN can provide highly concentrated example attributions, i.e., the predictions can be explained with few training examples, without reducing accuracy compared to the original deep neural network. Another important finding from the empirical investigation is that the choice of layer to use for the embeddings may have a large impact on the resulting accuracy.

## 多代理系统(cs.MA:Multiagent Systems)

### RouteRL: Multi-agent reinforcement learning framework for urban route choice with autonomous vehicles 
[[arxiv](https://arxiv.org/abs/2502.20065)] [[cool](https://papers.cool/arxiv/2502.20065)] [[pdf](https://arxiv.org/pdf/2502.20065)]
> **Authors**: Ahmet Onur Akman,Anastasia Psarou,Łukasz Gorczyca,Zoltán György Varga,Grzegorz Jamróz,Rafał Kucharski
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 多代理系统,机器学习
- **Abstract**: RouteRL is a novel framework that integrates multi-agent reinforcement learning (MARL) with a microscopic traffic simulation, facilitating the testing and development of efficient route choice strategies for autonomous vehicles (AVs). The proposed framework simulates the daily route choices of driver agents in a city, including two types: human drivers, emulated using behavioral route choice models, and AVs, modeled as MARL agents optimizing their policies for a predefined objective. RouteRL aims to advance research in MARL, transport modeling, and human-AI interaction for transportation applications. This study presents a technical report on RouteRL, outlines its potential research contributions, and showcases its impact via illustrative examples.

## 机器人技术(cs.RO:Robotics)

### Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids 
[[arxiv](https://arxiv.org/abs/2502.20396)] [[cool](https://papers.cool/arxiv/2502.20396)] [[pdf](https://arxiv.org/pdf/2502.20396)]
> **Authors**: Toru Lin,Kartik Sachdev,Linxi Fan,Jitendra Malik,Yuke Zhu
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Project page can be found at https://toruowo.github.io/recipe/
- **标题**: None
- **领域**: 机器人技术,人工智能,计算机视觉和模式识别,机器学习,系统与控制
- **Abstract**: Reinforcement learning has delivered promising results in achieving human- or even superhuman-level capabilities across diverse problem domains, but success in dexterous robot manipulation remains limited. This work investigates the key challenges in applying reinforcement learning to solve a collection of contact-rich manipulation tasks on a humanoid embodiment. We introduce novel techniques to overcome the identified challenges with empirical validation. Our main contributions include an automated real-to-sim tuning module that brings the simulated environment closer to the real world, a generalized reward design scheme that simplifies reward engineering for long-horizon contact-rich manipulation tasks, a divide-and-conquer distillation process that improves the sample efficiency of hard-exploration problems while maintaining sim-to-real performance, and a mixture of sparse and dense object representations to bridge the sim-to-real perception gap. We show promising results on three humanoid dexterous manipulation tasks, with ablation studies on each technique. Our work presents a successful approach to learning humanoid dexterous manipulation using sim-to-real reinforcement learning, achieving robust generalization and high performance without the need for human demonstration.

### Physics-Driven Data Generation for Contact-Rich Manipulation via Trajectory Optimization 
[[arxiv](https://arxiv.org/abs/2502.20382)] [[cool](https://papers.cool/arxiv/2502.20382)] [[pdf](https://arxiv.org/pdf/2502.20382)]
> **Authors**: Lujie Yang,H. J. Terry Suh,Tong Zhao,Bernhard Paus Graesdal,Tarik Kelestemur,Jiuguang Wang,Tao Pang,Russ Tedrake
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,人工智能,机器学习,系统与控制
- **Abstract**: We present a low-cost data generation pipeline that integrates physics-based simulation, human demonstrations, and model-based planning to efficiently generate large-scale, high-quality datasets for contact-rich robotic manipulation tasks. Starting with a small number of embodiment-flexible human demonstrations collected in a virtual reality simulation environment, the pipeline refines these demonstrations using optimization-based kinematic retargeting and trajectory optimization to adapt them across various robot embodiments and physical parameters. This process yields a diverse, physically consistent dataset that enables cross-embodiment data transfer, and offers the potential to reuse legacy datasets collected under different hardware configurations or physical parameters. We validate the pipeline's effectiveness by training diffusion policies from the generated datasets for challenging contact-rich manipulation tasks across multiple robot embodiments, including a floating Allegro hand and bimanual robot arms. The trained policies are deployed zero-shot on hardware for bimanual iiwa arms, achieving high success rates with minimal human input. Project website: https://lujieyang.github.io/physicsgen/.

### Deep Reinforcement Learning based Autonomous Decision-Making for Cooperative UAVs: A Search and Rescue Real World Application 
[[arxiv](https://arxiv.org/abs/2502.20326)] [[cool](https://papers.cool/arxiv/2502.20326)] [[pdf](https://arxiv.org/pdf/2502.20326)]
> **Authors**: Thomas Hickling,Maxwell Hogan,Abdulla Tammam,Nabil Aouf
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 18 Pages, 21 Figures
- **标题**: None
- **领域**: 机器人技术,人工智能
- **Abstract**: This paper proposes a holistic framework for autonomous guidance, navigation, and task distribution among multi-drone systems operating in Global Navigation Satellite System (GNSS)-denied indoor settings. We advocate for a Deep Reinforcement Learning (DRL)-based guidance mechanism, utilising the Twin Delayed Deep Deterministic Policy Gradient algorithm. To improve the efficiency of the training process, we incorporate an Artificial Potential Field (APF)-based reward structure, enabling the agent to refine its movements, thereby promoting smoother paths and enhanced obstacle avoidance in indoor contexts. Furthermore, we tackle the issue of task distribution among cooperative UAVs through a DRL-trained Graph Convolutional Network (GCN). This GCN represents the interactions between drones and tasks, facilitating dynamic and real-time task allocation that reflects the current environmental conditions and the capabilities of the drones. Such an approach fosters effective coordination and collaboration among multiple drones during search and rescue operations or other exploratory endeavours. Lastly, to ensure precise odometry in environments lacking GNSS, we employ Light Detection And Ranging Simultaneous Localisation and Mapping complemented by a depth camera to mitigate the hallway problem. This integration offers robust localisation and mapping functionalities, thereby enhancing the systems dependability in indoor navigation. The proposed multi-drone framework not only elevates individual navigation capabilities but also optimises coordinated task allocation in complex, obstacle-laden environments. Experimental evaluations conducted in a setup tailored to meet the requirements of the NATO Sapience Autonomous Cooperative Drone Competition demonstrate the efficacy of the proposed system, yielding outstanding results and culminating in a first-place finish in the 2024 Sapience competition.

### Accelerating Model-Based Reinforcement Learning with State-Space World Models 
[[arxiv](https://arxiv.org/abs/2502.20168)] [[cool](https://papers.cool/arxiv/2502.20168)] [[pdf](https://arxiv.org/pdf/2502.20168)]
> **Authors**: Maria Krinner,Elie Aljalbout,Angel Romero,Davide Scaramuzza
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: :I.2.9; I.2.10; I.2.6
- **标题**: None
- **领域**: 机器人技术,人工智能,机器学习,神经和进化计算,机器学习
- **Abstract**: Reinforcement learning (RL) is a powerful approach for robot learning. However, model-free RL (MFRL) requires a large number of environment interactions to learn successful control policies. This is due to the noisy RL training updates and the complexity of robotic systems, which typically involve highly non-linear dynamics and noisy sensor signals. In contrast, model-based RL (MBRL) not only trains a policy but simultaneously learns a world model that captures the environment's dynamics and rewards. The world model can either be used for planning, for data collection, or to provide first-order policy gradients for training. Leveraging a world model significantly improves sample efficiency compared to model-free RL. However, training a world model alongside the policy increases the computational complexity, leading to longer training times that are often intractable for complex real-world scenarios. In this work, we propose a new method for accelerating model-based RL using state-space world models. Our approach leverages state-space models (SSMs) to parallelize the training of the dynamics model, which is typically the main computational bottleneck. Additionally, we propose an architecture that provides privileged information to the world model during training, which is particularly relevant for partially observable environments. We evaluate our method in several real-world agile quadrotor flight tasks, involving complex dynamics, for both fully and partially observable environments. We demonstrate a significant speedup, reducing the world model training time by up to 10 times, and the overall MBRL training time by up to 4 times. This benefit comes without compromising performance, as our method achieves similar sample efficiency and task rewards to state-of-the-art MBRL methods.

### Discovering Antagonists in Networks of Systems: Robot Deployment 
[[arxiv](https://arxiv.org/abs/2502.20125)] [[cool](https://papers.cool/arxiv/2502.20125)] [[pdf](https://arxiv.org/pdf/2502.20125)]
> **Authors**: Ingeborg Wenger,Peter Eberhard,Henrik Ebel
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: :I.2.6; I.2.9; I.2.11; G.3
- **标题**: None
- **领域**: 机器人技术,机器学习,多代理系统
- **Abstract**: A contextual anomaly detection method is proposed and applied to the physical motions of a robot swarm executing a coverage task. Using simulations of a swarm's normal behavior, a normalizing flow is trained to predict the likelihood of a robot motion within the current context of its environment. During application, the predicted likelihood of the observed motions is used by a detection criterion that categorizes a robot agent as normal or antagonistic. The proposed method is evaluated on five different strategies of antagonistic behavior. Importantly, only readily available simulated data of normal robot behavior is used for training such that the nature of the anomalies need not be known beforehand. The best detection criterion correctly categorizes at least 80% of each antagonistic type while maintaining a false positive rate of less than 5% for normal robot agents. Additionally, the method is validated in hardware experiments, yielding results similar to the simulated scenarios. Compared to the state-of-the-art approach, both the predictive performance of the normalizing flow and the robustness of the detection criterion are increased.

### Minds on the Move: Decoding Trajectory Prediction in Autonomous Driving with Cognitive Insights 
[[arxiv](https://arxiv.org/abs/2502.20084)] [[cool](https://papers.cool/arxiv/2502.20084)] [[pdf](https://arxiv.org/pdf/2502.20084)]
> **Authors**: Haicheng Liao,Chengyue Wang,Kaiqun Zhu,Yilong Ren,Bolin Gao,Shengbo Eben Li,Chengzhong Xu,Zhenning Li
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,人工智能
- **Abstract**: In mixed autonomous driving environments, accurately predicting the future trajectories of surrounding vehicles is crucial for the safe operation of autonomous vehicles (AVs). In driving scenarios, a vehicle's trajectory is determined by the decision-making process of human drivers. However, existing models primarily focus on the inherent statistical patterns in the data, often neglecting the critical aspect of understanding the decision-making processes of human drivers. This oversight results in models that fail to capture the true intentions of human drivers, leading to suboptimal performance in long-term trajectory prediction. To address this limitation, we introduce a Cognitive-Informed Transformer (CITF) that incorporates a cognitive concept, Perceived Safety, to interpret drivers' decision-making mechanisms. Perceived Safety encapsulates the varying risk tolerances across drivers with different driving behaviors. Specifically, we develop a Perceived Safety-aware Module that includes a Quantitative Safety Assessment for measuring the subject risk levels within scenarios, and Driver Behavior Profiling for characterizing driver behaviors. Furthermore, we present a novel module, Leanformer, designed to capture social interactions among vehicles. CITF demonstrates significant performance improvements on three well-established datasets. In terms of long-term prediction, it surpasses existing benchmarks by 12.0% on the NGSIM, 28.2% on the HighD, and 20.8% on the MoCAD dataset. Additionally, its robustness in scenarios with limited or missing data is evident, surpassing most state-of-the-art (SOTA) baselines, and paving the way for real-world applications.

### Multi-Keypoint Affordance Representation for Functional Dexterous Grasping 
[[arxiv](https://arxiv.org/abs/2502.20018)] [[cool](https://papers.cool/arxiv/2502.20018)] [[pdf](https://arxiv.org/pdf/2502.20018)]
> **Authors**: Fan Yang,Dongsheng Luo,Wenrui Chen,Jiacheng Lin,Junjie Cai,Kailun Yang,Zhiyong Li,Yaonan Wang
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: The source code and demo videos will be publicly available at https://github.com/PopeyePxx/MKA
- **标题**: None
- **领域**: 机器人技术,计算机视觉和模式识别,图像和视频处理
- **Abstract**: Functional dexterous grasping requires precise hand-object interaction, going beyond simple gripping. Existing affordance-based methods primarily predict coarse interaction regions and cannot directly constrain the grasping posture, leading to a disconnection between visual perception and manipulation. To address this issue, we propose a multi-keypoint affordance representation for functional dexterous grasping, which directly encodes task-driven grasp configurations by localizing functional contact points. Our method introduces Contact-guided Multi-Keypoint Affordance (CMKA), leveraging human grasping experience images for weak supervision combined with Large Vision Models for fine affordance feature extraction, achieving generalization while avoiding manual keypoint annotations. Additionally, we present a Keypoint-based Grasp matrix Transformation (KGT) method, ensuring spatial consistency between hand keypoints and object contact points, thus providing a direct link between visual perception and dexterous grasping actions. Experiments on public real-world FAH datasets, IsaacGym simulation, and challenging robotic tasks demonstrate that our method significantly improves affordance localization accuracy, grasp consistency, and generalization to unseen tools and tasks, bridging the gap between visual affordance learning and dexterous robotic manipulation. The source code and demo videos will be publicly available at https://github.com/PopeyePxx/MKA.

### CarPlanner: Consistent Auto-regressive Trajectory Planning for Large-scale Reinforcement Learning in Autonomous Driving 
[[arxiv](https://arxiv.org/abs/2502.19908)] [[cool](https://papers.cool/arxiv/2502.19908)] [[pdf](https://arxiv.org/pdf/2502.19908)]
> **Authors**: Dongkun Zhang,Jiaming Liang,Ke Guo,Sha Lu,Qi Wang,Rong Xiong,Zhenwei Miao,Yue Wang
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: CVPR 2025
- **标题**: None
- **领域**: 机器人技术,计算机视觉和模式识别,机器学习
- **Abstract**: Trajectory planning is vital for autonomous driving, ensuring safe and efficient navigation in complex environments. While recent learning-based methods, particularly reinforcement learning (RL), have shown promise in specific scenarios, RL planners struggle with training inefficiencies and managing large-scale, real-world driving scenarios. In this paper, we introduce \textbf{CarPlanner}, a \textbf{C}onsistent \textbf{a}uto-\textbf{r}egressive \textbf{Planner} that uses RL to generate multi-modal trajectories. The auto-regressive structure enables efficient large-scale RL training, while the incorporation of consistency ensures stable policy learning by maintaining coherent temporal consistency across time steps. Moreover, CarPlanner employs a generation-selection framework with an expert-guided reward function and an invariant-view module, simplifying RL training and enhancing policy performance. Extensive analysis demonstrates that our proposed RL framework effectively addresses the challenges of training efficiency and performance enhancement, positioning CarPlanner as a promising solution for trajectory planning in autonomous driving. To the best of our knowledge, we are the first to demonstrate that the RL-based planner can surpass both IL- and rule-based state-of-the-arts (SOTAs) on the challenging large-scale real-world dataset nuPlan. Our proposed CarPlanner surpasses RL-, IL-, and rule-based SOTA approaches within this demanding dataset.

### Shared Autonomy for Proximal Teaching 
[[arxiv](https://arxiv.org/abs/2502.19899)] [[cool](https://papers.cool/arxiv/2502.19899)] [[pdf](https://arxiv.org/pdf/2502.19899)]
> **Authors**: Megha Srivastava,Reihaneh Iranmanesh,Yuchen Cui,Deepak Gopinath,Emily Sumner,Andrew Silva,Laporsha Dees,Guy Rosman,Dorsa Sadigh
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Accepted to ACM/IEEE International Conference on Human-Robot Interaction, 2025
- **标题**: None
- **领域**: 机器人技术,人工智能,人机交互
- **Abstract**: Motor skill learning often requires experienced professionals who can provide personalized instruction. Unfortunately, the availability of high-quality training can be limited for specialized tasks, such as high performance racing. Several recent works have leveraged AI-assistance to improve instruction of tasks ranging from rehabilitation to surgical robot tele-operation. However, these works often make simplifying assumptions on the student learning process, and fail to model how a teacher's assistance interacts with different individuals' abilities when determining optimal teaching strategies. Inspired by the idea of scaffolding from educational psychology, we leverage shared autonomy, a framework for combining user inputs with robot autonomy, to aid with curriculum design. Our key insight is that the way a student's behavior improves in the presence of assistance from an autonomous agent can highlight which sub-skills might be most ``learnable'' for the student, or within their Zone of Proximal Development. We use this to design Z-COACH, a method for using shared autonomy to provide personalized instruction targeting interpretable task sub-skills. In a user study (n=50), where we teach high performance racing in a simulated environment of the Thunderhill Raceway Park with the CARLA Autonomous Driving simulator, we show that Z-COACH helps identify which skills each student should first practice, leading to an overall improvement in driving time, behavior, and smoothness. Our work shows that increasingly available semi-autonomous capabilities (e.g. in vehicles, robots) can not only assist human users, but also help *teach* them.

### ColorDynamic: Generalizable, Scalable, Real-time, End-to-end Local Planner for Unstructured and Dynamic Environments 
[[arxiv](https://arxiv.org/abs/2502.19892)] [[cool](https://papers.cool/arxiv/2502.19892)] [[pdf](https://arxiv.org/pdf/2502.19892)]
> **Authors**: Jinghao Xin,Zhichao Liang,Zihuan Zhang,Peng Wang,Ning Li
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 18 pages
- **标题**: None
- **领域**: 机器人技术,人工智能
- **Abstract**: Deep Reinforcement Learning (DRL) has demonstrated potential in addressing robotic local planning problems, yet its efficacy remains constrained in highly unstructured and dynamic environments. To address these challenges, this study proposes the ColorDynamic framework. First, an end-to-end DRL formulation is established, which maps raw sensor data directly to control commands, thereby ensuring compatibility with unstructured environments. Under this formulation, a novel network, Transqer, is introduced. The Transqer enables online DRL learning from temporal transitions, substantially enhancing decision-making in dynamic scenarios. To facilitate scalable training of Transqer with diverse data, an efficient simulation platform E-Sparrow, along with a data augmentation technique leveraging symmetric invariance, are developed. Comparative evaluations against state-of-the-art methods, alongside assessments of generalizability, scalability, and real-time performance, were conducted to validate the effectiveness of ColorDynamic. Results indicate that our approach achieves a success rate exceeding 90% while exhibiting real-time capacity (1.2-1.3 ms per planning). Additionally, ablation studies were performed to corroborate the contributions of individual components. Building on this, the OkayPlan-ColorDynamic (OPCD) navigation system is presented, with simulated and real-world experiments demonstrating its superiority and applicability in complex scenarios. The codebase and experimental demonstrations have been open-sourced on our website to facilitate reproducibility and further research.

## 声音(cs.SD:Sound)

### DiffCSS: Diverse and Expressive Conversational Speech Synthesis with Diffusion Models 
[[arxiv](https://arxiv.org/abs/2502.19924)] [[cool](https://papers.cool/arxiv/2502.19924)] [[pdf](https://arxiv.org/pdf/2502.19924)]
> **Authors**: Weihao wu,Zhiwei Lin,Yixuan Zhou,Jingbei Li,Rui Niu,Qinghua Wu,Songjun Cao,Long Ma,Zhiyong Wu
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Accepted by ICASSP 2025
- **标题**: None
- **领域**: 声音,人工智能,音频和语音处理
- **Abstract**: Conversational speech synthesis (CSS) aims to synthesize both contextually appropriate and expressive speech, and considerable efforts have been made to enhance the understanding of conversational context. However, existing CSS systems are limited to deterministic prediction, overlooking the diversity of potential responses. Moreover, they rarely employ language model (LM)-based TTS backbones, limiting the naturalness and quality of synthesized speech. To address these issues, in this paper, we propose DiffCSS, an innovative CSS framework that leverages diffusion models and an LM-based TTS backbone to generate diverse, expressive, and contextually coherent speech. A diffusion-based context-aware prosody predictor is proposed to sample diverse prosody embeddings conditioned on multimodal conversational context. Then a prosody-controllable LM-based TTS backbone is developed to synthesize high-quality speech with sampled prosody embeddings. Experimental results demonstrate that the synthesized speech from DiffCSS is more diverse, contextually coherent, and expressive than existing CSS systems

## 软件工程(cs.SE:Software Engineering)

### SoRFT: Issue Resolving with Subtask-oriented Reinforced Fine-Tuning 
[[arxiv](https://arxiv.org/abs/2502.20127)] [[cool](https://papers.cool/arxiv/2502.20127)] [[pdf](https://arxiv.org/pdf/2502.20127)]
> **Authors**: Zexiong Ma,Chao Peng,Pengfei Gao,Xiangxin Meng,Yanzhen Zou,Bing Xie
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,人工智能,计算语言学
- **Abstract**: Mainstream issue-resolving frameworks predominantly rely on commercial models, leading to high costs and privacy concerns. Existing training approaches for issue resolving struggle with poor generalization and fail to fully leverage open-source development resources. We propose Subtask-oriented Reinforced Fine-Tuning (SoRFT), a novel training approach to enhance the issue resolving capability of LLMs. We decomposes issue resolving into structured subtasks: file localization, function localization, line localization, and code edit generation. SoRFT consists of two training stages: (1) rejection-sampled supervised fine-tuning, Chain of Thought (CoT) data is filtered using ground-truth before fine-tuning the LLM, and (2) rule-based reinforcement learning, which leverages PPO with ground-truth based rewards. We evaluate the SoRFT-trained model on SWE-Bench Verified and SWE-Bench Lite, achieving state-of-the-art (SOTA) performance among open-source models (e.g., resolve 21.4% issues on SWE-Bench Verified with SoRFT-Qwen-7B). The experimental results demonstrate that SoRFT significantly enhances issue-resolving performance, improves model generalization, and provides a cost-efficient alternative to commercial models.

### ConvCodeWorld: Benchmarking Conversational Code Generation in Reproducible Feedback Environments 
[[arxiv](https://arxiv.org/abs/2502.19852)] [[cool](https://papers.cool/arxiv/2502.19852)] [[pdf](https://arxiv.org/pdf/2502.19852)]
> **Authors**: Hojae Han,Seung-won Hwang,Rajhans Samdani,Yuxiong He
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: ICLR 2025
- **标题**: None
- **领域**: 软件工程,人工智能,计算语言学
- **Abstract**: Large language models (LLMs) have proven invaluable for code generation, particularly in interactive settings. However, existing code generation benchmarks fail to capture the diverse feedback encountered in multi-turn interactions, limiting our ability to evaluate LLMs in these contexts. To address this gap, we present a set of novel benchmarks that explicitly model the quality of feedback provided to code generation LLMs. Our contributions are threefold: First, we introduce CONVCODEWORLD, a novel and reproducible environment for benchmarking interactive code generation. CONVCODEWORLD simulates 9 distinct interactive code generation scenarios while systematically combining three types of feedback: (a) compilation feedback; (b) execution feedback with varying test coverage; (c) verbal feedback generated by GPT-4o with different levels of expertise. Second, we introduce CONVCODEBENCH, a fast, static version of benchmark that uses pre-generated feedback logs, eliminating the need for costly dynamic verbal feedback generation while maintaining strong Spearman's rank correlations (0.82 to 0.99) with CONVCODEWORLD. Third, extensive evaluations of both closed-source and open-source LLMs including R1-Distill on CONVCODEWORLD reveal key insights: (a) LLM performance varies significantly based on the feedback provided; (b) Weaker LLMs, with sufficient feedback, can outperform single-turn results of state-of-the-art LLMs without feedback; (c) Training on a specific feedback combination can limit an LLM's ability to utilize unseen combinations; (d) LLMs solve problems in fewer turns (high MRR) may not solve as many problems overall (high Recall), and vice versa. All implementations and benchmarks will be made publicly available at https://huggingface.co/spaces/ConvCodeWorld/ConvCodeWorld

## 社交和信息网络(cs.SI:Social and Information Networks)

### Towards Collaborative Anti-Money Laundering Among Financial Institutions 
[[arxiv](https://arxiv.org/abs/2502.19952)] [[cool](https://papers.cool/arxiv/2502.19952)] [[pdf](https://arxiv.org/pdf/2502.19952)]
> **Authors**: Zhihua Tian,Yuan Ding,Xiang Yu,Enchao Gong,Jian Liu,Kui Ren
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Accepted by International World Wide Web Conference (WWW) 2025
- **标题**: None
- **领域**: 社交和信息网络,计算机与社会,机器学习
- **Abstract**: Money laundering is the process that intends to legalize the income derived from illicit activities, thus facilitating their entry into the monetary flow of the economy without jeopardizing their source. It is crucial to identify such activities accurately and reliably in order to enforce anti-money laundering (AML). Despite considerable efforts to AML, a large number of such activities still go undetected. Rule-based methods were first introduced and are still widely used in current detection systems. With the rise of machine learning, graph-based learning methods have gained prominence in detecting illicit accounts through the analysis of money transfer graphs. Nevertheless, these methods generally assume that the transaction graph is centralized, whereas in practice, money laundering activities usually span multiple financial institutions. Due to regulatory, legal, commercial, and customer privacy concerns, institutions tend not to share data, restricting their utility in practical usage. In this paper, we propose the first algorithm that supports performing AML over multiple institutions while protecting the security and privacy of local data. To evaluate, we construct Alipay-ECB, a real-world dataset comprising digital transactions from Alipay, the world's largest mobile payment platform, alongside transactions from E-Commerce Bank (ECB). The dataset includes over 200 million accounts and 300 million transactions, covering both intra-institution transactions and those between Alipay and ECB. This makes it the largest real-world transaction graph available for analysis. The experimental results demonstrate that our methods can effectively identify cross-institution money laundering subgroups. Additionally, experiments on synthetic datasets also demonstrate that our method is efficient, requiring only a few minutes on datasets with millions of transactions.

### Community Detection by ELPMeans: An Unsupervised Approach That Uses Laplacian Centrality and Clustering 
[[arxiv](https://arxiv.org/abs/2502.19895)] [[cool](https://papers.cool/arxiv/2502.19895)] [[pdf](https://arxiv.org/pdf/2502.19895)]
> **Authors**: Shahin Momenzadeh,Rojiar Pir Mohammadiani
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: ef:The 3rd International Conference on Engineering and innovative Technology ICEIT2024 Salahaddin University-Erbil, 30-31 October 2024
- **标题**: None
- **领域**: 社交和信息网络,机器学习
- **Abstract**: Community detection in network analysis has become more intricate due to the recent hike in social networks (Cai et al., 2024). This paper suggests a new approach named ELPMeans that strives to address this challenge. For community detection in the whole network, ELPMeans combines Laplacian, Hierarchical Clustering as well as K-means algorithms. Our technique employs Laplacian centrality and minimum distance metrics for central node identification while k-means learning is used for efficient convergence to final community structure. Remarkably, ELPMeans is an unsupervised method which is not only simple to implement but also effectively tackles common problems such as random initialization of central nodes, or finding of number of communities (K). Experimental results show that our algorithm improves accuracy and reduces time complexity considerably outperforming recent approaches on real world networks. Moreover, our approach has a wide applicability range in various community detection tasks even with nonconvex shapes and no prior knowledge about the number of communities present.

## 音频和语音处理(eess.AS:Audio and Speech Processing)

### CleanMel: Mel-Spectrogram Enhancement for Improving Both Speech Quality and ASR 
[[arxiv](https://arxiv.org/abs/2502.20040)] [[cool](https://papers.cool/arxiv/2502.20040)] [[pdf](https://arxiv.org/pdf/2502.20040)]
> **Authors**: Nian Shao,Rui Zhou,Pengyu Wang,Xian Li,Ying Fang,Yujie Yang,Xiaofei Li
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Submission to IEEE/ACM Trans. on TASLP
- **标题**: None
- **领域**: 音频和语音处理,人工智能,声音
- **Abstract**: In this work, we propose CleanMel, a single-channel Mel-spectrogram denoising and dereverberation network for improving both speech quality and automatic speech recognition (ASR) performance. The proposed network takes as input the noisy and reverberant microphone recording and predicts the corresponding clean Mel-spectrogram. The enhanced Mel-spectrogram can be either transformed to speech waveform with a neural vocoder or directly used for ASR. The proposed network is composed of interleaved cross-band and narrow-band processing in the Mel-frequency domain, for learning the full-band spectral pattern and the narrow-band properties of signals, respectively. Compared to linear-frequency domain or time-domain speech enhancement, the key advantage of Mel-spectrogram enhancement is that Mel-frequency presents speech in a more compact way and thus is easier to learn, which will benefit both speech quality and ASR. Experimental results on four English and one Chinese datasets demonstrate a significant improvement in both speech quality and ASR performance achieved by the proposed model. Code and audio examples of our model are available online in https://audio.westlake.edu.cn/Research/CleanMel.html.

## 图像和视频处理(eess.IV:Image and Video Processing)

### T1-PILOT: Optimized Trajectories for T1 Mapping Acceleration 
[[arxiv](https://arxiv.org/abs/2502.20333)] [[cool](https://papers.cool/arxiv/2502.20333)] [[pdf](https://arxiv.org/pdf/2502.20333)]
> **Authors**: Tamir Shor,Moti Freiman,Chaim Baskin,Alex Bronstein
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习
- **Abstract**: Cardiac T1 mapping provides critical quantitative insights into myocardial tissue composition, enabling the assessment of pathologies such as fibrosis, inflammation, and edema. However, the inherently dynamic nature of the heart imposes strict limits on acquisition times, making high-resolution T1 mapping a persistent challenge. Compressed sensing (CS) approaches have reduced scan durations by undersampling k-space and reconstructing images from partial data, and recent studies show that jointly optimizing the undersampling patterns with the reconstruction network can substantially improve performance. Still, most current T1 mapping pipelines rely on static, hand-crafted masks that do not exploit the full acceleration and accuracy potential. In this work, we introduce T1-PILOT: an end-to-end method that explicitly incorporates the T1 signal relaxation model into the sampling-reconstruction framework to guide the learning of non-Cartesian trajectories, crossframe alignment, and T1 decay estimation. Through extensive experiments on the CMRxRecon dataset, T1-PILOT significantly outperforms several baseline strategies (including learned single-mask and fixed radial or golden-angle sampling schemes), achieving higher T1 map fidelity at greater acceleration factors. In particular, we observe consistent gains in PSNR and VIF relative to existing methods, along with marked improvements in delineating finer myocardial structures. Our results highlight that optimizing sampling trajectories in tandem with the physical relaxation model leads to both enhanced quantitative accuracy and reduced acquisition times. Code for reproducing all results will be made publicly available upon publication.

### RURANET++: An Unsupervised Learning Method for Diabetic Macular Edema Based on SCSE Attention Mechanisms and Dynamic Multi-Projection Head Clustering 
[[arxiv](https://arxiv.org/abs/2502.20224)] [[cool](https://papers.cool/arxiv/2502.20224)] [[pdf](https://arxiv.org/pdf/2502.20224)]
> **Authors**: Wei Yang,Yiran Zhu,Jiayu Shen,Yuhan Tang,Chengchang Pan,Hui He,Yan Su,Honggang Qi
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 10 pages, 2 figures, 5 tables, submitted to The 28th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2025)
- **标题**: None
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **Abstract**: Diabetic Macular Edema (DME), a prevalent complication among diabetic patients, constitutes a major cause of visual impairment and blindness. Although deep learning has achieved remarkable progress in medical image analysis, traditional DME diagnosis still relies on extensive annotated data and subjective ophthalmologist assessments, limiting practical applications. To address this, we present RURANET++, an unsupervised learning-based automated DME diagnostic system. This framework incorporates an optimized U-Net architecture with embedded Spatial and Channel Squeeze & Excitation (SCSE) attention mechanisms to enhance lesion feature extraction. During feature processing, a pre-trained GoogLeNet model extracts deep features from retinal images, followed by PCA-based dimensionality reduction to 50 dimensions for computational efficiency. Notably, we introduce a novel clustering algorithm employing multi-projection heads to explicitly control cluster diversity while dynamically adjusting similarity thresholds, thereby optimizing intra-class consistency and inter-class discrimination. Experimental results demonstrate superior performance across multiple metrics, achieving maximum accuracy (0.8411), precision (0.8593), recall (0.8411), and F1-score (0.8390), with exceptional clustering quality. This work provides an efficient unsupervised solution for DME diagnosis with significant clinical implications.

### Balanced Rate-Distortion Optimization in Learned Image Compression 
[[arxiv](https://arxiv.org/abs/2502.20161)] [[cool](https://papers.cool/arxiv/2502.20161)] [[pdf](https://arxiv.org/pdf/2502.20161)]
> **Authors**: Yichi Zhang,Zhihao Duan,Yuning Huang,Fengqing Zhu
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: Preliminary version. Camera ready version and source code will be uploaded later. Accepted to CVPR 2025
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: Learned image compression (LIC) using deep learning architectures has seen significant advancements, yet standard rate-distortion (R-D) optimization often encounters imbalanced updates due to diverse gradients of the rate and distortion objectives. This imbalance can lead to suboptimal optimization, where one objective dominates, thereby reducing overall compression efficiency. To address this challenge, we reformulate R-D optimization as a multi-objective optimization (MOO) problem and introduce two balanced R-D optimization strategies that adaptively adjust gradient updates to achieve more equitable improvements in both rate and distortion. The first proposed strategy utilizes a coarse-to-fine gradient descent approach along standard R-D optimization trajectories, making it particularly suitable for training LIC models from scratch. The second proposed strategy analytically addresses the reformulated optimization as a quadratic programming problem with an equality constraint, which is ideal for fine-tuning existing models. Experimental results demonstrate that both proposed methods enhance the R-D performance of LIC models, achieving around a 2\% BD-Rate reduction with acceptable additional training cost, leading to a more balanced and efficient optimization process. The code will be made publicly available.

### Generative augmentations for improved cardiac ultrasound segmentation using diffusion models 
[[arxiv](https://arxiv.org/abs/2502.20100)] [[cool](https://papers.cool/arxiv/2502.20100)] [[pdf](https://arxiv.org/pdf/2502.20100)]
> **Authors**: Gilles Van De Vyver,Aksel Try Lenz,Erik Smistad,Sindre Hellum Olaisen,Bjørnar Grenne,Espen Holte,Håavard Dalen,Lasse Løvstakken
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: One of the main challenges in current research on segmentation in cardiac ultrasound is the lack of large and varied labeled datasets and the differences in annotation conventions between datasets. This makes it difficult to design robust segmentation models that generalize well to external datasets. This work utilizes diffusion models to create generative augmentations that can significantly improve diversity of the dataset and thus the generalisability of segmentation models without the need for more annotated data. The augmentations are applied in addition to regular augmentations. A visual test survey showed that experts cannot clearly distinguish between real and fully generated images. Using the proposed generative augmentations, segmentation robustness was increased when training on an internal dataset and testing on an external dataset with an improvement of over 20 millimeters in Hausdorff distance. Additionally, the limits of agreement for automatic ejection fraction estimation improved by up to 20% of absolute ejection fraction value on out of distribution cases. These improvements come exclusively from the increased variation of the training data using the generative augmentations, without modifying the underlying machine learning model. The augmentation tool is available as an open source Python library at https://github.com/GillesVanDeVyver/EchoGAINS.

## 信号处理(eess.SP:Signal Processing)

### NeRFCom: Feature Transform Coding Meets Neural Radiance Field for Free-View 3D Scene Semantic Transmission 
[[arxiv](https://arxiv.org/abs/2502.19873)] [[cool](https://papers.cool/arxiv/2502.19873)] [[pdf](https://arxiv.org/pdf/2502.19873)]
> **Authors**: Weijie Yue,Zhongwei Si,Bolin Wu,Sixian Wang,Xiaoqi Qin,Kai Niu,Jincheng Dai,Ping Zhang
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 信号处理,机器学习
- **Abstract**: We introduce NeRFCom, a novel communication system designed for end-to-end 3D scene transmission. Compared to traditional systems relying on handcrafted NeRF semantic feature decomposition for compression and well-adaptive channel coding for transmission error correction, our NeRFCom employs a nonlinear transform and learned probabilistic models, enabling flexible variable-rate joint source-channel coding and efficient bandwidth allocation aligned with the NeRF semantic feature's different contribution to the 3D scene synthesis fidelity. Experimental results demonstrate that NeRFCom achieves free-view 3D scene efficient transmission while maintaining robustness under adverse channel conditions.

## 高能物理-现象学(hep-ph:High Energy Physics - Phenomenology)

### Generative adversarial neural networks for simulating neutrino interactions 
[[arxiv](https://arxiv.org/abs/2502.20244)] [[cool](https://papers.cool/arxiv/2502.20244)] [[pdf](https://arxiv.org/pdf/2502.20244)]
> **Authors**: Jose L. Bonilla,Krzysztof M. Graczyk,Artur M. Ankowski,Rwik Dharmapal Banerjee,Beata E. Kowal,Hemant Prasad,Jan T. Sobczyk
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 14 pages, 14 figures
- **标题**: None
- **领域**: 高能物理-现象学,机器学习,高能物理-实验,核实验,核理论
- **Abstract**: We propose a new approach to simulate neutrino scattering events as an alternative to the standard Monte Carlo generator approach. Generative adversarial neural network (GAN) models are developed to simulate neutrino-carbon collisions in the few-GeV energy range. The models produce scattering events for a given neutrino energy. GAN models are trained on simulation data from NuWro Monte Carlo event generator. Two GAN models have been obtained: one simulating only quasielastic neutrino-nucleus scatterings and another simulating all interactions at given neutrino energy. The performance of both models has been assessed using two statistical metrics. It is shown that both GAN models successfully reproduce the event distributions.

## 动力系统(math.DS:Dynamical Systems)

### Impilict Runge-Kutta based sparse identification of governing equations in biologically motivated systems 
[[arxiv](https://arxiv.org/abs/2502.20319)] [[cool](https://papers.cool/arxiv/2502.20319)] [[pdf](https://arxiv.org/pdf/2502.20319)]
> **Authors**: Mehrdad Anvari,Hamidreza Marasi,Hossein Kheiri
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 23 pages, 9 figures
- **标题**: None
- **领域**: 动力系统,机器学习,数值分析,定量方法
- **Abstract**: Identifying governing equations in physical and biological systems from datasets remains a long-standing challenge across various scientific disciplines, providing mechanistic insights into complex system evolution. Common methods like sparse identification of nonlinear dynamics (SINDy) often rely on precise derivative estimations, making them vulnerable to data scarcity and noise. This study presents a novel data-driven framework by integrating high order implicit Runge-Kutta methods (IRKs) with the sparse identification, termed IRK-SINDy. The framework exhibits remarkable robustness to data scarcity and noise by leveraging the lower stepsize constraint of IRKs. Two methods for incorporating IRKs into sparse regression are introduced: one employs iterative schemes for numerically solving nonlinear algebraic system of equations, while the other utilizes deep neural networks to predict stage values of IRKs. The performance of IRK-SINDy is demonstrated through numerical experiments on benchmark problems with varied dynamical behaviors, including linear and nonlinear oscillators, the Lorenz system, and biologically relevant models like predator-prey dynamics, logistic growth, and the FitzHugh-Nagumo model. Results indicate that IRK-SINDy outperforms conventional SINDy and the RK4-SINDy framework, particularly under conditions of extreme data scarcity and noise, yielding interpretable and generalizable models.

## 数值分析(math.NA:Numerical Analysis)

### Scalable Signature Kernel Computations for Long Time Series via Local Neumann Series Expansions 
[[arxiv](https://arxiv.org/abs/2502.20392)] [[cool](https://papers.cool/arxiv/2502.20392)] [[pdf](https://arxiv.org/pdf/2502.20392)]
> **Authors**: Matthew Tamayo-Rios,Alexander Schell,Rima Alaifari
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 18 pages, 3 figures
- **标题**: None
- **领域**: 数值分析,机器学习,偏微分方程分析
- **Abstract**: The signature kernel is a recent state-of-the-art tool for analyzing high-dimensional sequential data, valued for its theoretical guarantees and strong empirical performance. In this paper, we present a novel method for efficiently computing the signature kernel of long, high-dimensional time series via dynamically truncated recursive local power series expansions. Building on the characterization of the signature kernel as the solution of a Goursat PDE, our approach employs tilewise Neumann-series expansions to derive rapidly converging power series approximations of the signature kernel that are locally defined on subdomains and propagated iteratively across the entire domain of the Goursat solution by exploiting the geometry of the time series. Algorithmically, this involves solving a system of interdependent local Goursat PDEs by recursively propagating boundary conditions along a directed graph via topological ordering, with dynamic truncation adaptively terminating each local power series expansion when coefficients fall below machine precision, striking an effective balance between computational cost and accuracy. This method achieves substantial performance improvements over state-of-the-art approaches for computing the signature kernel, providing (a) adjustable and superior accuracy, even for time series with very high roughness; (b) drastically reduced memory requirements; and (c) scalability to efficiently handle very long time series (e.g., with up to half a million points or more) on a single GPU. These advantages make our method particularly well-suited for rough-path-assisted machine learning, financial modeling, and signal processing applications that involve very long and highly volatile data.

### A Multiple Transferable Neural Network Method with Domain Decomposition for Elliptic Interface Problems 
[[arxiv](https://arxiv.org/abs/2502.19893)] [[cool](https://papers.cool/arxiv/2502.19893)] [[pdf](https://arxiv.org/pdf/2502.19893)]
> **Authors**: Tianzheng Lu,Lili Ju,Liyong Zhu
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 数值分析,机器学习
- **Abstract**: The transferable neural network (TransNet) is a two-layer shallow neural network with pre-determined and uniformly distributed neurons in the hidden layer, and the least-squares solvers can be particularly used to compute the parameters of its output layer when applied to the solution of partial differential equations. In this paper, we integrate the TransNet technique with the nonoverlapping domain decomposition and the interface conditions to develop a novel multiple transferable neural network (Multi-TransNet) method for solving elliptic interface problems, which typically contain discontinuities in both solutions and their derivatives across interfaces. We first propose an empirical formula for the TransNet to characterize the relationship between the radius of the domain-covering ball, the number of hidden-layer neurons, and the optimal neuron shape. In the Multi-TransNet method, we assign each subdomain one distinct TransNet with an adaptively determined number of hidden-layer neurons to maintain the globally uniform neuron distribution across the entire computational domain, and then unite all the subdomain TransNets together by incorporating the interface condition terms into the loss function. The empirical formula is also extended to the Multi-TransNet and further employed to estimate appropriate neuron shapes for the subdomain TransNets, greatly reducing the parameter tuning cost. Additionally, we propose a normalization approach to adaptively select the weighting parameters for the terms in the loss function. Ablation studies and extensive experiments with comparison tests on different types of elliptic interface problems with low to high contrast diffusion coefficients in two and three dimensions are carried out to numerically demonstrate the superior accuracy, efficiency, and robustness of the proposed Multi-TransNet method.

## 优化与控制(math.OC:Optimization and Control)

### Physics-Informed Neural Networks for Optimal Vaccination Plan in SIR Epidemic Models 
[[arxiv](https://arxiv.org/abs/2502.19890)] [[cool](https://papers.cool/arxiv/2502.19890)] [[pdf](https://arxiv.org/pdf/2502.19890)]
> **Authors**: Minseok Kim,Yeongjong Kim,Yeoneung Kim
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: :49J15; 49L20; 49L25; 35F21; 65M99; 68T07ACM Class:G.1.6; G.1.10
- **标题**: None
- **领域**: 优化与控制,机器学习
- **Abstract**: This work focuses on understanding the minimum eradication time for the controlled Susceptible-Infectious-Recovered (SIR) model in the time-homogeneous setting, where the infection and recovery rates are constant. The eradication time is defined as the earliest time the infectious population drops below a given threshold and remains below it. For time-homogeneous models, the eradication time is well-defined due to the predictable dynamics of the infectious population, and optimal control strategies can be systematically studied. We utilize Physics-Informed Neural Networks (PINNs) to solve the partial differential equation (PDE) governing the eradication time and derive the corresponding optimal vaccination control. The PINN framework enables a mesh-free solution to the PDE by embedding the dynamics directly into the loss function of a deep neural network. We use a variable scaling method to ensure stable training of PINN and mathematically analyze that this method is effective in our setting. This approach provides an efficient computational alternative to traditional numerical methods, allowing for an approximation of the eradication time and the optimal control strategy. Through numerical experiments, we validate the effectiveness of the proposed method in computing the minimum eradication time and achieving optimal control. This work offers a novel application of PINNs to epidemic modeling, bridging mathematical theory and computational practice for time-homogeneous SIR models.

### Inexact Moreau Envelope Lagrangian Method for Non-Convex Constrained Optimization under Local Error Bound Conditions on Constraint Functions 
[[arxiv](https://arxiv.org/abs/2502.19764)] [[cool](https://papers.cool/arxiv/2502.19764)] [[pdf](https://arxiv.org/pdf/2502.19764)]
> **Authors**: Yankun Huang,Qihang Lin,Yangyang Xu
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 优化与控制,机器学习
- **Abstract**: In this paper, we study the inexact Moreau envelope Lagrangian (iMELa) method for solving smooth non-convex optimization problems over a simple polytope with additional convex inequality constraints. By incorporating a proximal term into the traditional Lagrangian function, the iMELa method approximately solves a convex optimization subproblem over the polyhedral set at each main iteration. Under the assumption of a local error bound condition for subsets of the feasible set defined by subsets of the constraints, we establish that the iMELa method can find an $ε$-Karush-Kuhn-Tucker point with $\tilde O(ε^{-2})$ gradient oracle complexity.

## 核理论(nucl-th:Nuclear Theory)

### Global Framework for Simultaneous Emulation Across the Nuclear Landscape 
[[arxiv](https://arxiv.org/abs/2502.20363)] [[cool](https://papers.cool/arxiv/2502.20363)] [[pdf](https://arxiv.org/pdf/2502.20363)]
> **Authors**: Antoine Belley,Jose M. Munoz,Ronald F. Garcia Ruiz
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 核理论,机器学习
- **Abstract**: We introduce a hierarchical framework that combines ab initio many-body calculations with a Bayesian neural network, developing emulators capable of accurately predicting nuclear properties across the nuclear chart, including multiple isotopes simultaneously. We benchmark our developments using the oxygen isotopic chain, achieving accurate results for ground-state energies and nuclear charge radii, while providing robust uncertainty quantification. Our framework enables global sensitivity analysis of nuclear binding energies and charge radii with respect to the low-energy constants that describe the nuclear force.

## 神经元和认知(q-bio.NC:Neurons and Cognition)

### Naturalistic Computational Cognitive Science: Towards generalizable models and theories that capture the full range of natural behavior 
[[arxiv](https://arxiv.org/abs/2502.20349)] [[cool](https://papers.cool/arxiv/2502.20349)] [[pdf](https://arxiv.org/pdf/2502.20349)]
> **Authors**: Wilka Carvalho,Andrew Lampinen
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 神经元和认知,人工智能
- **Abstract**: Artificial Intelligence increasingly pursues large, complex models that perform many tasks within increasingly realistic domains. How, if at all, should these developments in AI influence cognitive science? We argue that progress in AI offers timely opportunities for cognitive science to embrace experiments with increasingly naturalistic stimuli, tasks, and behaviors; and computational models that can accommodate these changes. We first review a growing body of research spanning neuroscience, cognitive science, and AI that suggests that incorporating a broader range of naturalistic experimental paradigms (and models that accommodate them) may be necessary to resolve some aspects of natural intelligence and ensure that our theories generalize. We then suggest that integrating recent progress in AI and cognitive science will enable us to engage with more naturalistic phenomena without giving up experimental control or the pursuit of theoretically grounded understanding. We offer practical guidance on how methodological practices can contribute to cumulative progress in naturalistic computational cognitive science, and illustrate a path towards building computational models that solve the real problems of natural cognition - together with a reductive understanding of the processes and principles by which they do so.

## 量子物理学(quant-ph:Quantum Physics)

### Efficient and Universal Neural-Network Decoder for Stabilizer-Based Quantum Error Correction 
[[arxiv](https://arxiv.org/abs/2502.19971)] [[cool](https://papers.cool/arxiv/2502.19971)] [[pdf](https://arxiv.org/pdf/2502.19971)]
> **Authors**: Gengyuan Hu,Wanli Ouyang,Chao-Yang Lu,Chen Lin,Han-Sen Zhong
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 量子物理学,人工智能
- **Abstract**: Quantum error correction is crucial for large-scale quantum computing, but the absence of efficient decoders for new codes like quantum low-density parity-check (QLDPC) codes has hindered progress. Here we introduce a universal decoder based on linear attention sequence modeling and graph neural network that operates directly on any stabilizer code's graph structure. Our numerical experiments demonstrate that this decoder outperforms specialized algorithms in both accuracy and speed across diverse stabilizer codes, including surface codes, color codes, and QLDPC codes. The decoder maintains linear time scaling with syndrome measurements and requires no structural modifications between different codes. For the Bivariate Bicycle code with distance 12, our approach achieves a 39.4% lower logical error rate than previous best decoders while requiring only ~1% of the decoding time. These results provide a practical, universal solution for quantum error correction, eliminating the need for code-specific decoders.

## 方法论(stat.ME:Methodology)

### Qini curve estimation under clustered network interference 
[[arxiv](https://arxiv.org/abs/2502.20097)] [[cool](https://papers.cool/arxiv/2502.20097)] [[pdf](https://arxiv.org/pdf/2502.20097)]
> **Authors**: Rickard K. A. Karlsson,Bram van den Akker,Felipe Moraes,Hugo M. Proença,Jesse H. Krijthe
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 方法论,机器学习,机器学习
- **Abstract**: Qini curves are a widely used tool for assessing treatment policies under allocation constraints as they visualize the incremental gain of a new treatment policy versus the cost of its implementation. Standard Qini curve estimation assumes no interference between units: that is, that treating one unit does not influence the outcome of any other unit. In many real-life applications such as public policy or marketing, however, the presence of interference is common. Ignoring interference in these scenarios can lead to systematically biased Qini curves that over- or under-estimate a treatment policy's cost-effectiveness. In this paper, we address the problem of Qini curve estimation under clustered network interference, where interfering units form independent clusters. We propose a formal description of the problem setting with an experimental study design under which we can account for clustered network interference. Within this framework, we introduce three different estimation strategies suited for different conditions. Moreover, we introduce a marketplace simulator that emulates clustered network interference in a typical e-commerce setting. From both theoretical and empirical insights, we provide recommendations in choosing the best estimation strategy by identifying an inherent bias-variance trade-off among the estimation strategies.

## 机器学习(stat.ML:Machine Learning)

### Multiple Linked Tensor Factorization 
[[arxiv](https://arxiv.org/abs/2502.20286)] [[cool](https://papers.cool/arxiv/2502.20286)] [[pdf](https://arxiv.org/pdf/2502.20286)]
> **Authors**: Zhiyu Kang,Raghavendra B. Rao,Eric F. Lock
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: 26 pages, 4 figures, 7 tables
- **标题**: None
- **领域**: 机器学习,机器学习,计算,方法论
- **Abstract**: In biomedical research and other fields, it is now common to generate high content data that are both multi-source and multi-way. Multi-source data are collected from different high-throughput technologies while multi-way data are collected over multiple dimensions, yielding multiple tensor arrays. Integrative analysis of these data sets is needed, e.g., to capture and synthesize different facets of complex biological systems. However, despite growing interest in multi-source and multi-way factorization techniques, methods that can handle data that are both multi-source and multi-way are limited. In this work, we propose a Multiple Linked Tensors Factorization (MULTIFAC) method extending the CANDECOMP/PARAFAC (CP) decomposition to simultaneously reduce the dimension of multiple multi-way arrays and approximate underlying signal. We first introduce a version of the CP factorization with L2 penalties on the latent factors, leading to rank sparsity. When extended to multiple linked tensors, the method automatically reveals latent components that are shared across data sources or individual to each data source. We also extend the decomposition algorithm to its expectation-maximization (EM) version to handle incomplete data with imputation. Extensive simulation studies are conducted to demonstrate MULTIFAC's ability to (i) approximate underlying signal, (ii) identify shared and unshared structures, and (iii) impute missing data. The approach yields an interpretable decomposition on multi-way multi-omics data for a study on early-life iron deficiency.

### Asymptotics of Non-Convex Generalized Linear Models in High-Dimensions: A proof of the replica formula 
[[arxiv](https://arxiv.org/abs/2502.20003)] [[cool](https://papers.cool/arxiv/2502.20003)] [[pdf](https://arxiv.org/pdf/2502.20003)]
> **Authors**: Matteo Vilucchio,Yatin Dandi,Cedric Gerbelot,Florent Krzakala
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: The analytic characterization of the high-dimensional behavior of optimization for Generalized Linear Models (GLMs) with Gaussian data has been a central focus in statistics and probability in recent years. While convex cases, such as the LASSO, ridge regression, and logistic regression, have been extensively studied using a variety of techniques, the non-convex case remains far less understood despite its significance. A non-rigorous statistical physics framework has provided remarkable predictions for the behavior of high-dimensional optimization problems, but rigorously establishing their validity for non-convex problems has remained a fundamental challenge. In this work, we address this challenge by developing a systematic framework that rigorously proves replica-symmetric formulas for non-convex GLMs and precisely determines the conditions under which these formulas are valid. Remarkably, the rigorous replica-symmetric predictions align exactly with the conjectures made by physicists, and the so-called replicon condition. The originality of our approach lies in connecting two powerful theoretical tools: the Gaussian Min-Max Theorem, which we use to provide precise lower bounds, and Approximate Message Passing (AMP), which is shown to achieve these bounds algorithmically. We demonstrate the utility of this framework through significant applications: (i) by proving the optimality of the Tukey loss over the more commonly used Huber loss under a $\varepsilon$ contaminated data model, (ii) establishing the optimality of negative regularization in high-dimensional non-convex regression and (iii) characterizing the performance limits of linearized AMP algorithms. By rigorously validating statistical physics predictions in non-convex settings, we aim to open new pathways for analyzing increasingly complex optimization landscapes beyond the convex regime.

### Fast Debiasing of the LASSO Estimator 
[[arxiv](https://arxiv.org/abs/2502.19825)] [[cool](https://papers.cool/arxiv/2502.19825)] [[pdf](https://arxiv.org/pdf/2502.19825)]
> **Authors**: Shuvayan Banerjee,James Saunderson,Radhendushka Srivastava,Ajit Rajwade
> **First submission**: 2025-02-27
> **First announcement**: 2025-02-28
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: In high-dimensional sparse regression, the \textsc{Lasso} estimator offers excellent theoretical guarantees but is well-known to produce biased estimates. To address this, \cite{Javanmard2014} introduced a method to ``debias" the \textsc{Lasso} estimates for a random sub-Gaussian sensing matrix $\boldsymbol{A}$. Their approach relies on computing an ``approximate inverse" $\boldsymbol{M}$ of the matrix $\boldsymbol{A}^\top \boldsymbol{A}/n$ by solving a convex optimization problem. This matrix $\boldsymbol{M}$ plays a critical role in mitigating bias and allowing for construction of confidence intervals using the debiased \textsc{Lasso} estimates. However the computation of $\boldsymbol{M}$ is expensive in practice as it requires iterative optimization. In the presented work, we re-parameterize the optimization problem to compute a ``debiasing matrix" $\boldsymbol{W} := \boldsymbol{AM}^{\top}$ directly, rather than the approximate inverse $\boldsymbol{M}$. This reformulation retains the theoretical guarantees of the debiased \textsc{Lasso} estimates, as they depend on the \emph{product} $\boldsymbol{AM}^{\top}$ rather than on $\boldsymbol{M}$ alone. Notably, we provide a simple, computationally efficient, closed-form solution for $\boldsymbol{W}$ under similar conditions for the sensing matrix $\boldsymbol{A}$ used in the original debiasing formulation, with an additional condition that the elements of every row of $\boldsymbol{A}$ have uncorrelated entries. Also, the optimization problem based on $\boldsymbol{W}$ guarantees a unique optimal solution, unlike the original formulation based on $\boldsymbol{M}$. We verify our main result with numerical simulations.

## 其他论文

- [Waves and symbols in neuromorphic hardware: from analog signal processing to digital computing on the same computational substrate](https://arxiv.org/abs/2502.20381)
  - **标题**: None
  - **Filtered Reason**: none of cs.NE in whitelist
- [Evaluating the long-term viability of eye-tracking for continuous authentication in virtual reality](https://arxiv.org/abs/2502.20359)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [The entropy profiles of a definable set over finite fields](https://arxiv.org/abs/2502.20355)
  - **标题**: None
  - **Filtered Reason**: none of math.LO,math.NT,cs.IT in whitelist
- [KNOWM Memristors in a Bridge Synapse delay-based Reservoir Computing system for detection of epileptic seizures](https://arxiv.org/abs/2502.20351)
  - **标题**: None
  - **Filtered Reason**: none of cs.ET,physics.med-ph in whitelist
- [Reservoir Computing and Photoelectrochemical Sensors: A Marriage of Convenience](https://arxiv.org/abs/2502.20342)
  - **标题**: None
  - **Filtered Reason**: none of cs.ET,physics.ins-det in whitelist
- [ACCORD: Application Context-aware Cross-layer Optimization and Resource Design for 5G/NextG Machine-centric Applications](https://arxiv.org/abs/2502.20320)
  - **标题**: None
  - **Filtered Reason**: none of eess.SY,cs.NI in whitelist
- [Solving Maker-Breaker Games on 5-uniform hypergraphs is PSPACE-complete](https://arxiv.org/abs/2502.20271)
  - **标题**: None
  - **Filtered Reason**: none of cs.DM,math.CO in whitelist
- [DIN-CTS: Low-Complexity Depthwise-Inception Neural Network with Contrastive Training Strategy for Deepfake Speech Detection](https://arxiv.org/abs/2502.20225)
  - **标题**: None
  - **Filtered Reason**: none of cs.SD,cs.CR,eess.AS in whitelist
- [MARVEL: Multi-Agent Reinforcement Learning for constrained field-of-View multi-robot Exploration in Large-scale environments](https://arxiv.org/abs/2502.20217)
  - **标题**: None
  - **Filtered Reason**: none of cs.MA,cs.RO in whitelist
- [Bisecting K-Means in RAG for Enhancing Question-Answering Tasks Performance in Telecommunications](https://arxiv.org/abs/2502.20188)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [Methodology for GPU Frequency Switching Latency Measurement](https://arxiv.org/abs/2502.20075)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC in whitelist
- [A high-performance and portable implementation of the SISSO method for CPUs and GPUs](https://arxiv.org/abs/2502.20072)
  - **标题**: None
  - **Filtered Reason**: none of cs.PF in whitelist
- [UniCodec: Unified Audio Codec with Single Domain-Adaptive Codebook](https://arxiv.org/abs/2502.20067)
  - **标题**: None
  - **Filtered Reason**: none of cs.SD,eess.AS in whitelist
- [FuseGrasp: Radar-Camera Fusion for Robotic Grasping of Transparent Objects](https://arxiv.org/abs/2502.20037)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [InCoRe -- An Interactive Co-Regulation Model: Training Teacher Communication Skills in Demanding Classroom Situations](https://arxiv.org/abs/2502.20025)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [PrimeK-Net: Multi-scale Spectral Learning via Group Prime-Kernel Convolutional Neural Networks for Single Channel Speech Enhancement](https://arxiv.org/abs/2502.19906)
  - **标题**: None
  - **Filtered Reason**: none of cs.SD,eess.AS in whitelist
- [Towards Multimodal Large-Language Models for Parent-Child Interaction: A Focus on Joint Attention](https://arxiv.org/abs/2502.19877)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [RingAda: Pipelining Large Model Fine-Tuning on Edge Devices with Scheduled Layer Unfreezing](https://arxiv.org/abs/2502.19864)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC in whitelist
- [Empowering Social Service with AI: Insights from a Participatory Design Study with Practitioners](https://arxiv.org/abs/2502.19822)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
