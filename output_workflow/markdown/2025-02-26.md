> 本文由 [https://github.com/huiyeruzhou/arxiv_crawler](https://github.com/huiyeruzhou/arxiv_crawler) 自动生成
>
> 领域白名单：cs.AI,cs.CL,cs.LG,cs.CV
> 关键词： LLM, GPT, AI, language+model, deep+learning, transformer, neural+network, machine+learning

# 论文全览：2025-02-26

共有137篇相关领域论文, 另有6篇其他

## 天体物理学仪器和方法(astro-ph.IM:Instrumentation and Methods for Astrophysics)

### Transfer Learning for Transient Classification: From Simulations to Real Data and ZTF to LSST 
[[arxiv](https://arxiv.org/abs/2502.18558)] [[cool](https://papers.cool/arxiv/2502.18558)] [[pdf](https://arxiv.org/pdf/2502.18558)]
> **Authors**: Rithwik Gupta,Daniel Muthukrishna
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 6 pages, 3 figures, 1 table
- **标题**: None
- **领域**: 天体物理学仪器和方法,高能天体物理现象,机器学习
- **Abstract**: Machine learning has become essential for automated classification of astronomical transients, but current approaches face significant limitations: classifiers trained on simulations struggle with real data, models developed for one survey cannot be easily applied to another, and new surveys require prohibitively large amounts of labelled training data. These challenges are particularly pressing as we approach the era of the Vera Rubin Observatory's Legacy Survey of Space and Time (LSST), where existing classification models will need to be retrained using LSST observations. We demonstrate that transfer learning can overcome these challenges by repurposing existing models trained on either simulations or data from other surveys. Starting with a model trained on simulated Zwicky Transient Facility (ZTF) light curves, we show that transfer learning reduces the amount of labelled real ZTF transients needed by 75\% while maintaining equivalent performance to models trained from scratch. Similarly, when adapting ZTF models for LSST simulations, transfer learning achieves 95\% of the baseline performance while requiring only 30\% of the training data. These findings have significant implications for the early operations of LSST, suggesting that reliable automated classification will be possible soon after the survey begins, rather than waiting months or years to accumulate sufficient training data.

## 材料科学(cond-mat.mtrl-sci:Materials Science)

### Mind the Gap: Bridging the Divide Between AI Aspirations and the Reality of Autonomous Characterization 
[[arxiv](https://arxiv.org/abs/2502.18604)] [[cool](https://papers.cool/arxiv/2502.18604)] [[pdf](https://arxiv.org/pdf/2502.18604)]
> **Authors**: Grace Guinan,Addison Salvador,Michelle A. Smeaton,Andrew Glaws,Hilary Egan,Brian C. Wyatt,Babak Anasori,Kevin R. Fiedler,Matthew J. Olszta,Steven R. Spurgeon
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 33 pages, 6 figures
- **标题**: None
- **领域**: 材料科学,人工智能
- **Abstract**: What does materials science look like in the "Age of Artificial Intelligence?" Each materials domain-synthesis, characterization, and modeling-has a different answer to this question, motivated by unique challenges and constraints. This work focuses on the tremendous potential of autonomous characterization within electron microscopy. We present our recent advancements in developing domain-aware, multimodal models for microscopy analysis capable of describing complex atomic systems. We then address the critical gap between the theoretical promise of autonomous microscopy and its current practical limitations, showcasing recent successes while highlighting the necessary developments to achieve robust, real-world autonomy.

## 人工智能(cs.AI:Artificial Intelligence)

### CuDIP: Enhancing Theorem Proving in LLMs via Curriculum Learning-based Direct Preference Optimization 
[[arxiv](https://arxiv.org/abs/2502.18532)] [[cool](https://papers.cool/arxiv/2502.18532)] [[pdf](https://arxiv.org/pdf/2502.18532)]
> **Authors**: Shuming Shi,Ruobing Zuo,Gaolei He,Jianlin Wang,Chenyang Xu,Zhengfeng Yang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: Automated theorem proving (ATP) is one of the most challenging mathematical reasoning tasks for Large Language Models (LLMs). Most existing LLM-based ATP methods rely on supervised fine-tuning, which results in a limited alignment between the theorem proving process and human preferences. Direct Preference Optimization (DPO), which aligns LLMs with human preferences, has shown positive effects for certain tasks. However, the lack of high-quality preference data for theorem proving presents a significant challenge. In this paper, we innovatively apply DPO to formal automated theorem proving and introduces a Curriculum Learning-based DPO Iterative Theorem Proving (CuDIP) method. Specifically, we propose a method for constructing preference data which utilizes LLMs and existing theorem proving data to enhance the diversity of the preference data while reducing the reliance on human preference annotations. We then integrate this preference data construction method with curriculum learning to iteratively fine-tune the theorem proving model through DPO. Experimental results on the MiniF2F and ProofNet datasets demonstrate the effectiveness of the proposed method.

### Enhancing Hepatopathy Clinical Trial Efficiency: A Secure, Large Language Model-Powered Pre-Screening Pipeline 
[[arxiv](https://arxiv.org/abs/2502.18531)] [[cool](https://papers.cool/arxiv/2502.18531)] [[pdf](https://arxiv.org/pdf/2502.18531)]
> **Authors**: Xiongbin Gui,Hanlin Lv,Xiao Wang,Longting Lv,Yi Xiao,Lei Wang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-26
> **comment**: 30 pages, 5 figures
- **标题**: None
- **领域**: 人工智能,计算语言学,机器学习
- **Abstract**: Background: Recruitment for cohorts involving complex liver diseases, such as hepatocellular carcinoma and liver cirrhosis, often requires interpreting semantically complex criteria. Traditional manual screening methods are time-consuming and prone to errors. While AI-powered pre-screening offers potential solutions, challenges remain regarding accuracy, efficiency, and data privacy. Methods: We developed a novel patient pre-screening pipeline that leverages clinical expertise to guide the precise, safe, and efficient application of large language models. The pipeline breaks down complex criteria into a series of composite questions and then employs two strategies to perform semantic question-answering through electronic health records - (1) Pathway A, Anthropomorphized Experts' Chain of Thought strategy, and (2) Pathway B, Preset Stances within an Agent Collaboration strategy, particularly in managing complex clinical reasoning scenarios. The pipeline is evaluated on three key metrics-precision, time consumption, and counterfactual inference - at both the question and criterion levels. Results: Our pipeline achieved high precision (0.921, in criteria level) and efficiency (0.44s per task). Pathway B excelled in complex reasoning, while Pathway A was effective in precise data extraction with faster processing times. Both pathways achieved comparable precision. The pipeline showed promising results in hepatocellular carcinoma (0.878) and cirrhosis trials (0.843). Conclusions: This data-secure and time-efficient pipeline shows high precision in hepatopathy trials, providing promising solutions for streamlining clinical trial workflows. Its efficiency and adaptability make it suitable for improving patient recruitment. And its capability to function in resource-constrained environments further enhances its utility in clinical settings.

### The Gradient of Algebraic Model Counting 
[[arxiv](https://arxiv.org/abs/2502.18406)] [[cool](https://papers.cool/arxiv/2502.18406)] [[pdf](https://arxiv.org/pdf/2502.18406)]
> **Authors**: Jaron Maene,Luc De Raedt
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Published at AAAI 2025
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: Algebraic model counting unifies many inference tasks on logic formulas by exploiting semirings. Rather than focusing on inference, we consider learning, especially in statistical-relational and neurosymbolic AI, which combine logical, probabilistic and neural representations. Concretely, we show that the very same semiring perspective of algebraic model counting also applies to learning. This allows us to unify various learning algorithms by generalizing gradients and backpropagation to different semirings. Furthermore, we show how cancellation and ordering properties of a semiring can be exploited for more memory-efficient backpropagation. This allows us to obtain some interesting variations of state-of-the-art gradient-based optimisation methods for probabilistic logical models. We also discuss why algebraic model counting on tractable circuits does not lead to more efficient second-order optimization. Empirically, our algebraic backpropagation exhibits considerable speed-ups as compared to existing approaches.

### How Far are LLMs from Real Search? A Comprehensive Study on Efficiency, Completeness, and Inherent Capabilities 
[[arxiv](https://arxiv.org/abs/2502.18387)] [[cool](https://papers.cool/arxiv/2502.18387)] [[pdf](https://arxiv.org/pdf/2502.18387)]
> **Authors**: Minhua Lin,Hui Liu,Xianfeng Tang,Jingying Zeng,Zhenwei Dai,Chen Luo,Zheng Li,Xiang Zhang,Qi He,Suhang Wang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 31 pages, 9 figures, 18 tables
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Search plays a fundamental role in problem-solving across various domains, with most real-world decision-making problems being solvable through systematic search. Drawing inspiration from recent discussions on search and learning, we systematically explore the complementary relationship between search and Large Language Models (LLMs) from three perspectives. First, we analyze how learning can enhance search efficiency and propose Search via Learning (SeaL), a framework that leverages LLMs for effective and efficient search. Second, we further extend SeaL to SeaL-C to ensure rigorous completeness during search. Our evaluation across three real-world planning tasks demonstrates that SeaL achieves near-perfect accuracy while reducing search spaces by up to 99.1% compared to traditional approaches. Finally, we explore how far LLMs are from real search by investigating whether they can develop search capabilities independently. Our analysis reveals that while current LLMs struggle with efficient search in complex problems, incorporating systematic search strategies significantly enhances their problem-solving capabilities. These findings not only validate the effectiveness of our approach but also highlight the need for improving LLMs' search abilities for real-world applications.

### MindMem: Multimodal for Predicting Advertisement Memorability Using LLMs and Deep Learning 
[[arxiv](https://arxiv.org/abs/2502.18371)] [[cool](https://papers.cool/arxiv/2502.18371)] [[pdf](https://arxiv.org/pdf/2502.18371)]
> **Authors**: Sepehr Asgarian,Qayam Jetha,Jouhyun Jeon
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 7 pages, 5 figures, 4 Tables, AAAI 2025 Economics of Modern ML: Markets, Incentives, and GenerativeAIWorkshop
- **标题**: None
- **领域**: 人工智能
- **Abstract**: In the competitive landscape of advertising, success hinges on effectively navigating and leveraging complex interactions among consumers, advertisers, and advertisement platforms. These multifaceted interactions compel advertisers to optimize strategies for modeling consumer behavior, enhancing brand recall, and tailoring advertisement content. To address these challenges, we present MindMem, a multimodal predictive model for advertisement memorability. By integrating textual, visual, and auditory data, MindMem achieves state-of-the-art performance, with a Spearman's correlation coefficient of 0.631 on the LAMBDA and 0.731 on the Memento10K dataset, consistently surpassing existing methods. Furthermore, our analysis identified key factors influencing advertisement memorability, such as video pacing, scene complexity, and emotional resonance. Expanding on this, we introduced MindMem-ReAd (MindMem-Driven Re-generated Advertisement), which employs Large Language Model-based simulations to optimize advertisement content and placement, resulting in up to a 74.12% improvement in advertisement memorability. Our results highlight the transformative potential of Artificial Intelligence in advertising, offering advertisers a robust tool to drive engagement, enhance competitiveness, and maximize impact in a rapidly evolving market.

### GraphRank Pro+: Advancing Talent Analytics Through Knowledge Graphs and Sentiment-Enhanced Skill Profiling 
[[arxiv](https://arxiv.org/abs/2502.18315)] [[cool](https://papers.cool/arxiv/2502.18315)] [[pdf](https://arxiv.org/pdf/2502.18315)]
> **Authors**: Sirisha Velampalli,Chandrashekar Muniyappa
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: :05C81ACM Class:I.2.7
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: The extraction of information from semi-structured text, such as resumes, has long been a challenge due to the diverse formatting styles and subjective content organization. Conventional solutions rely on specialized logic tailored for specific use cases. However, we propose a revolutionary approach leveraging structured Graphs, Natural Language Processing (NLP), and Deep Learning. By abstracting intricate logic into Graph structures, we transform raw data into a comprehensive Knowledge Graph. This innovative framework enables precise information extraction and sophisticated querying. We systematically construct dictionaries assigning skill weights, paving the way for nuanced talent analysis. Our system not only benefits job recruiters and curriculum designers but also empowers job seekers with targeted query-based filtering and ranking capabilities.

### LeanProgress: Guiding Search for Neural Theorem Proving via Proof Progress Prediction 
[[arxiv](https://arxiv.org/abs/2502.17925)] [[cool](https://papers.cool/arxiv/2502.17925)] [[pdf](https://arxiv.org/pdf/2502.17925)]
> **Authors**: Suozhi Huang,Peiyang Song,Robert Joseph George,Anima Anandkumar
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Mathematical reasoning remains a significant challenge for Large Language Models (LLMs) due to hallucinations. When combined with formal proof assistants like Lean, these hallucinations can be eliminated through rigorous verification, making theorem proving reliable. However, even with formal verification, LLMs still struggle with long proofs and complex mathematical formalizations. While Lean with LLMs offers valuable assistance with retrieving lemmas, generating tactics, or even complete proofs, it lacks a crucial capability: providing a sense of proof progress. This limitation particularly impacts the overall development efficiency in large formalization projects. We introduce LeanProgress, a method that predicts the progress in the proof. Training and evaluating our models made on a large corpus of Lean proofs from Lean Workbook Plus and Mathlib4 and how many steps remain to complete it, we employ data preprocessing and balancing techniques to handle the skewed distribution of proof lengths. Our experiments show that LeanProgress achieves an overall prediction accuracy of 75.1\% in predicting the amount of progress and, hence, the remaining number of steps. When integrated into a best-first search framework using Reprover, our method shows a 3.8\% improvement on Mathlib4 compared to baseline performances of 41.2\%, particularly for longer proofs. These results demonstrate how proof progress prediction can enhance both automated and interactive theorem proving, enabling users to make more informed decisions about proof strategies.

### Towards Sustainable Web Agents: A Plea for Transparency and Dedicated Metrics for Energy Consumption 
[[arxiv](https://arxiv.org/abs/2502.17903)] [[cool](https://papers.cool/arxiv/2502.17903)] [[pdf](https://arxiv.org/pdf/2502.17903)]
> **Authors**: Lars Krupp,Daniel Geißler,Paul Lukowicz,Jakob Karolus
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,人机交互
- **Abstract**: Improvements in the area of large language models have shifted towards the construction of models capable of using external tools and interpreting their outputs. These so-called web agents have the ability to interact autonomously with the internet. This allows them to become powerful daily assistants handling time-consuming, repetitive tasks while supporting users in their daily activities. While web agent research is thriving, the sustainability aspect of this research direction remains largely unexplored. We provide an initial exploration of the energy and CO2 cost associated with web agents. Our results show how different philosophies in web agent creation can severely impact the associated expended energy. We highlight lacking transparency regarding the disclosure of model parameters and processes used for some web agents as a limiting factor when estimating energy consumption. As such, our work advocates a change in thinking when evaluating web agents, warranting dedicated metrics for energy consumption and sustainability.

### Science Across Languages: Assessing LLM Multilingual Translation of Scientific Papers 
[[arxiv](https://arxiv.org/abs/2502.17882)] [[cool](https://papers.cool/arxiv/2502.17882)] [[pdf](https://arxiv.org/pdf/2502.17882)]
> **Authors**: Hannah Calzi Kleidermacher,James Zou
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: Scientific research is inherently global. However, the vast majority of academic journals are published exclusively in English, creating barriers for non-native-English-speaking researchers. In this study, we leverage large language models (LLMs) to translate published scientific articles while preserving their native JATS XML formatting, thereby developing a practical, automated approach for implementation by academic journals. Using our approach, we translate articles across multiple scientific disciplines into 28 languages. To evaluate translation accuracy, we introduce a novel question-and-answer (QA) benchmarking method, in which an LLM generates comprehension-based questions from the original text and then answers them based on the translated text. Our benchmark results show an average performance of 95.9%, showing that the key scientific details are accurately conveyed. In a user study, we translate the scientific papers of 15 researchers into their native languages, finding that the authors consistently found the translations to accurately capture the original information in their articles. Interestingly, a third of the authors found many technical terms "overtranslated," expressing a preference to keep terminology more familiar in English untranslated. Finally, we demonstrate how in-context learning techniques can be used to align translations with domain-specific preferences such as mitigating overtranslation, highlighting the adaptability and utility of LLM-driven scientific translation. The code and translated articles are available at https://hankleid.github.io/ProjectMundo.

## 计算语言学(cs.CL:Computation and Language)

### Chain of Draft: Thinking Faster by Writing Less 
[[arxiv](https://arxiv.org/abs/2502.18600)] [[cool](https://papers.cool/arxiv/2502.18600)] [[pdf](https://arxiv.org/pdf/2502.18600)]
> **Authors**: Silei Xu,Wenhao Xie,Lingxiao Zhao,Pengcheng He
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: :I.2.7
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models (LLMs) have demonstrated remarkable performance in solving complex reasoning tasks through mechanisms like Chain-of-Thought (CoT) prompting, which emphasizes verbose, step-by-step reasoning. However, humans typically employ a more efficient strategy: drafting concise intermediate thoughts that capture only essential information. In this work, we propose Chain of Draft (CoD), a novel paradigm inspired by human cognitive processes, where LLMs generate minimalistic yet informative intermediate reasoning outputs while solving tasks. By reducing verbosity and focusing on critical insights, CoD matches or surpasses CoT in accuracy while using as little as only 7.6% of the tokens, significantly reducing cost and latency across various reasoning tasks.

### Neurobiber: Fast and Interpretable Stylistic Feature Extraction 
[[arxiv](https://arxiv.org/abs/2502.18590)] [[cool](https://papers.cool/arxiv/2502.18590)] [[pdf](https://arxiv.org/pdf/2502.18590)]
> **Authors**: Kenan Alkiek,Anna Wegmann,Jian Zhu,David Jurgens
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Linguistic style is pivotal for understanding how texts convey meaning and fulfill communicative purposes, yet extracting detailed stylistic features at scale remains challenging. We present Neurobiber, a transformer-based system for fast, interpretable style profiling built on Biber's Multidimensional Analysis (MDA). Neurobiber predicts 96 Biber-style features from our open-source BiberPlus library (a Python toolkit that computes stylistic features and provides integrated analytics, e.g., PCA and factor analysis). Despite being up to 56 times faster than existing open source systems, Neurobiber replicates classic MDA insights on the CORE corpus and achieves competitive performance on the PAN 2020 authorship verification task without extensive retraining. Its efficient and interpretable representations readily integrate into downstream NLP pipelines, facilitating large-scale stylometric research, forensic analysis, and real-time text monitoring. All components are made publicly available.

### What are Foundation Models Cooking in the Post-Soviet World? 
[[arxiv](https://arxiv.org/abs/2502.18583)] [[cool](https://papers.cool/arxiv/2502.18583)] [[pdf](https://arxiv.org/pdf/2502.18583)]
> **Authors**: Anton Lavrouk,Tarek Naous,Alan Ritter,Wei Xu
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The culture of the Post-Soviet states is complex, shaped by a turbulent history that continues to influence current events. In this study, we investigate the Post-Soviet cultural food knowledge of foundation models by constructing BORSch, a multimodal dataset encompassing 1147 and 823 dishes in the Russian and Ukrainian languages, centered around the Post-Soviet region. We demonstrate that leading models struggle to correctly identify the origins of dishes from Post-Soviet nations in both text-only and multimodal Question Answering (QA), instead over-predicting countries linked to the language the question is asked in. Through analysis of pretraining data, we show that these results can be explained by misleading dish-origin co-occurrences, along with linguistic phenomena such as Russian-Ukrainian code mixing. Finally, to move beyond QA-based assessments, we test models' abilities to produce accurate visual descriptions of dishes. The weak correlation between this task and QA suggests that QA alone may be insufficient as an evaluation of cultural understanding. To foster further research, we will make BORSch publicly available at https://github.com/alavrouk/BORSch.

### Scalable Best-of-N Selection for Large Language Models via Self-Certainty 
[[arxiv](https://arxiv.org/abs/2502.18581)] [[cool](https://papers.cool/arxiv/2502.18581)] [[pdf](https://arxiv.org/pdf/2502.18581)]
> **Authors**: Zhewei Kang,Xuandong Zhao,Dawn Song
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Best-of-N selection is a key technique for improving the reasoning performance of Large Language Models (LLMs) through increased test-time computation. Current state-of-the-art methods often employ computationally intensive reward models for response evaluation and selection. Reward-free alternatives, like self-consistency and universal self-consistency, are limited in their ability to handle open-ended generation tasks or scale effectively. To address these limitations, we propose self-certainty, a novel and efficient metric that leverages the inherent probability distribution of LLM outputs to estimate response quality without requiring external reward models. We hypothesize that higher distributional self-certainty, aggregated across multiple samples, correlates with improved response accuracy, as it reflects greater confidence in the generated output. Through extensive experiments on various reasoning tasks, we demonstrate that self-certainty (1) scales effectively with increasing sample size $N$, akin to reward models but without the computational overhead; (2) complements chain-of-thought, improving reasoning performance beyond greedy decoding; and (3) generalizes to open-ended tasks where traditional self-consistency methods fall short. Our findings establish self-certainty as a practical and efficient way for improving LLM reasoning capabilities. The code is available at https://github.com/backprop07/Self-Certainty

### FactReasoner: A Probabilistic Approach to Long-Form Factuality Assessment for Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.18573)] [[cool](https://papers.cool/arxiv/2502.18573)] [[pdf](https://arxiv.org/pdf/2502.18573)]
> **Authors**: Radu Marinescu,Debarun Bhattacharjya,Junkyu Lee,Tigran Tchrakian,Javier Carnerero Cano,Yufang Hou,Elizabeth Daly,Alessandra Pascale
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large language models (LLMs) have demonstrated vast capabilities on generative tasks in recent years, yet they struggle with guaranteeing the factual correctness of the generated content. This makes these models unreliable in realistic situations where factually accurate responses are expected. In this paper, we propose FactReasoner, a new factuality assessor that relies on probabilistic reasoning to assess the factuality of a long-form generated response. Specifically, FactReasoner decomposes the response into atomic units, retrieves relevant contexts for them from an external knowledge source, and constructs a joint probability distribution over the atoms and contexts using probabilistic encodings of the logical relationships (entailment, contradiction) between the textual utterances corresponding to the atoms and contexts. FactReasoner then computes the posterior probability of whether atomic units in the response are supported by the retrieved contexts. Our experiments on labeled and unlabeled benchmark datasets demonstrate clearly that FactReasoner improves considerably over state-of-the-art prompt-based approaches in terms of both factual precision and recall.

### GLEAN: Generalized Category Discovery with Diverse and Quality-Enhanced LLM Feedback 
[[arxiv](https://arxiv.org/abs/2502.18414)] [[cool](https://papers.cool/arxiv/2502.18414)] [[pdf](https://arxiv.org/pdf/2502.18414)]
> **Authors**: Henry Peng Zou,Siffi Singh,Yi Nian,Jianfeng He,Jason Cai,Saab Mansour,Hang Su
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Generalized Category Discovery (GCD) is a practical and challenging open-world task that aims to recognize both known and novel categories in unlabeled data using limited labeled data from known categories. Due to the lack of supervision, previous GCD methods face significant challenges, such as difficulty in rectifying errors for confusing instances, and inability to effectively uncover and leverage the semantic meanings of discovered clusters. Therefore, additional annotations are usually required for real-world applicability. However, human annotation is extremely costly and inefficient. To address these issues, we propose GLEAN, a unified framework for generalized category discovery that actively learns from diverse and quality-enhanced LLM feedback. Our approach leverages three different types of LLM feedback to: (1) improve instance-level contrastive features, (2) generate category descriptions, and (3) align uncertain instances with LLM-selected category descriptions. Extensive experiments demonstrate the superior performance of \MethodName over state-of-the-art models across diverse datasets, metrics, and supervision settings. Our code is available at https://github.com/amazon-science/Glean.

### AgentRM: Enhancing Agent Generalization with Reward Modeling 
[[arxiv](https://arxiv.org/abs/2502.18407)] [[cool](https://papers.cool/arxiv/2502.18407)] [[pdf](https://arxiv.org/pdf/2502.18407)]
> **Authors**: Yu Xia,Jingru Fan,Weize Chen,Siyu Yan,Xin Cong,Zhong Zhang,Yaxi Lu,Yankai Lin,Zhiyuan Liu,Maosong Sun
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Existing LLM-based agents have achieved strong performance on held-in tasks, but their generalizability to unseen tasks remains poor. Hence, some recent work focus on fine-tuning the policy model with more diverse tasks to improve the generalizability. In this work, we find that finetuning a reward model to guide the policy model is more robust than directly finetuning the policy model. Based on this finding, we propose AgentRM, a generalizable reward model, to guide the policy model for effective test-time search. We comprehensively investigate three approaches to construct the reward model, including explicit reward modeling, implicit reward modeling and LLM-as-a-judge. We then use AgentRM to guide the answer generation with Best-of-N sampling and step-level beam search. On four types of nine agent tasks, AgentRM enhances the base policy model by $8.8$ points on average, surpassing the top general agent by $4.0$. Moreover, it demonstrates weak-to-strong generalization, yielding greater improvement of $12.6$ on LLaMA-3-70B policy model. As for the specializability, AgentRM can also boost a finetuned policy model and outperform the top specialized agent by $11.4$ on three held-in tasks. Further analysis verifies its effectiveness in test-time scaling. Codes will be released to facilitate the research in this area.

### KiRAG: Knowledge-Driven Iterative Retriever for Enhancing Retrieval-Augmented Generation 
[[arxiv](https://arxiv.org/abs/2502.18397)] [[cool](https://papers.cool/arxiv/2502.18397)] [[pdf](https://arxiv.org/pdf/2502.18397)]
> **Authors**: Jinyuan Fang,Zaiqiao Meng,Craig Macdonald
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Iterative retrieval-augmented generation (iRAG) models offer an effective approach for multi-hop question answering (QA). However, their retrieval process faces two key challenges: (1) it can be disrupted by irrelevant documents or factually inaccurate chain-of-thoughts; (2) their retrievers are not designed to dynamically adapt to the evolving information needs in multi-step reasoning, making it difficult to identify and retrieve the missing information required at each iterative step. Therefore, we propose KiRAG, which uses a knowledge-driven iterative retriever model to enhance the retrieval process of iRAG. Specifically, KiRAG decomposes documents into knowledge triples and performs iterative retrieval with these triples to enable a factually reliable retrieval process. Moreover, KiRAG integrates reasoning into the retrieval process to dynamically identify and retrieve knowledge that bridges information gaps, effectively adapting to the evolving information needs. Empirical results show that KiRAG significantly outperforms existing iRAG models, with an average improvement of 9.40% in R@3 and 5.14% in F1 on multi-hop QA.

### Monte Carlo Temperature: a robust sampling strategy for LLM's uncertainty quantification methods 
[[arxiv](https://arxiv.org/abs/2502.18389)] [[cool](https://papers.cool/arxiv/2502.18389)] [[pdf](https://arxiv.org/pdf/2502.18389)]
> **Authors**: Nicola Cecere,Andrea Bacciu,Ignacio Fernández Tobías,Amin Mantrach
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Uncertainty quantification (UQ) in Large Language Models (LLMs) is essential for their safe and reliable deployment, particularly in critical applications where incorrect outputs can have serious consequences. Current UQ methods typically rely on querying the model multiple times using non-zero temperature sampling to generate diverse outputs for uncertainty estimation. However, the impact of selecting a given temperature parameter is understudied, and our analysis reveals that temperature plays a fundamental role in the quality of uncertainty estimates. The conventional approach of identifying optimal temperature values requires expensive hyperparameter optimization (HPO) that must be repeated for each new model-dataset combination. We propose Monte Carlo Temperature (MCT), a robust sampling strategy that eliminates the need for temperature calibration. Our analysis reveals that: 1) MCT provides more robust uncertainty estimates across a wide range of temperatures, 2) MCT improves the performance of UQ methods by replacing fixed-temperature strategies that do not rely on HPO, and 3) MCT achieves statistical parity with oracle temperatures, which represent the ideal outcome of a well-tuned but computationally expensive HPO process. These findings demonstrate that effective UQ can be achieved without the computational burden of temperature parameter calibration.

### DBR: Divergence-Based Regularization for Debiasing Natural Language Understanding Models 
[[arxiv](https://arxiv.org/abs/2502.18353)] [[cool](https://papers.cool/arxiv/2502.18353)] [[pdf](https://arxiv.org/pdf/2502.18353)]
> **Authors**: Zihao Li,Ruixiang Tang,Lu Cheng,Shuaiqiang Wang,Dawei Yin,Mengnan Du
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Accepted by SIGKDD Explorations
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Pre-trained language models (PLMs) have achieved impressive results on various natural language processing tasks. However, recent research has revealed that these models often rely on superficial features and shortcuts instead of developing a genuine understanding of language, especially for natural language understanding (NLU) tasks. Consequently, the models struggle to generalize to out-of-domain data. In this work, we propose Divergence Based Regularization (DBR) to mitigate this shortcut learning behavior. Our method measures the divergence between the output distributions for original examples and examples where shortcut tokens have been masked. This process prevents the model's predictions from being overly influenced by shortcut features or biases. We evaluate our model on three NLU tasks and find that it improves out-of-domain performance with little loss of in-domain accuracy. Our results demonstrate that reducing the reliance on shortcuts and superficial features can enhance the generalization ability of large pre-trained language models.

### BRIDO: Bringing Democratic Order to Abstractive Summarization 
[[arxiv](https://arxiv.org/abs/2502.18342)] [[cool](https://papers.cool/arxiv/2502.18342)] [[pdf](https://arxiv.org/pdf/2502.18342)]
> **Authors**: Junhyun Lee,Harshith Goka,Hyeonmok Ko
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 13 pages, 1 figure; AAAI-25 Workshop on PDLM camera ready
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Hallucination refers to the inaccurate, irrelevant, and inconsistent text generated from large language models (LLMs). While the LLMs have shown great promise in a variety of tasks, the issue of hallucination still remains a major challenge for many practical uses. In this paper, we tackle the issue of hallucination in abstract text summarization by mitigating exposure bias. Existing models targeted for exposure bias mitigation, namely BRIO, aim for better summarization quality in the ROUGE score. We propose a model that uses a similar exposure bias mitigation strategy but with a goal that is aligned with less hallucination. We conjecture that among a group of candidate outputs, ones with hallucinations will comprise the minority of the whole group. That is, candidates with less similarity with others will have a higher chance of containing hallucinated content. Our method uses this aspect and utilizes contrastive learning, incentivizing candidates with high inter-candidate ROUGE scores. We performed experiments on the XSum and CNN/DM summarization datasets, and our method showed 6.25% and 3.82% improvement, respectively, on the consistency G-Eval score over BRIO.

### Correlating and Predicting Human Evaluations of Language Models from Natural Language Processing Benchmarks 
[[arxiv](https://arxiv.org/abs/2502.18339)] [[cool](https://papers.cool/arxiv/2502.18339)] [[pdf](https://arxiv.org/pdf/2502.18339)]
> **Authors**: Rylan Schaeffer,Punit Singh Koura,Binh Tang,Ranjan Subramanian,Aaditya K Singh,Todor Mihaylov,Prajjwal Bhargava,Lovish Madaan,Niladri S. Chatterji,Vedanuj Goswami,Sergey Edunov,Dieuwke Hupkes,Sanmi Koyejo,Sharan Narang
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: The explosion of high-performing conversational language models (LMs) has spurred a shift from classic natural language processing (NLP) benchmarks to expensive, time-consuming and noisy human evaluations - yet the relationship between these two evaluation strategies remains hazy. In this paper, we conduct a large-scale study of four Chat Llama 2 models, comparing their performance on 160 standard NLP benchmarks (e.g., MMLU, ARC, BIG-Bench Hard) against extensive human preferences on more than 11k single-turn and 2k multi-turn dialogues from over 2k human annotators. Our findings are striking: most NLP benchmarks strongly correlate with human evaluations, suggesting that cheaper, automated metrics can serve as surprisingly reliable predictors of human preferences. Three human evaluations, such as adversarial dishonesty and safety, are anticorrelated with NLP benchmarks, while two are uncorrelated. Moreover, through overparameterized linear regressions, we show that NLP scores can accurately predict human evaluations across different model scales, offering a path to reduce costly human annotation without sacrificing rigor. Overall, our results affirm the continued value of classic benchmarks and illuminate how to harness them to anticipate real-world user satisfaction - pointing to how NLP benchmarks can be leveraged to meet evaluation needs of our new era of conversational AI.

### BottleHumor: Self-Informed Humor Explanation using the Information Bottleneck Principle 
[[arxiv](https://arxiv.org/abs/2502.18331)] [[cool](https://papers.cool/arxiv/2502.18331)] [[pdf](https://arxiv.org/pdf/2502.18331)]
> **Authors**: EunJeong Hwang,Peter West,Vered Shwartz
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Humor is prevalent in online communications and it often relies on more than one modality (e.g., cartoons and memes). Interpreting humor in multimodal settings requires drawing on diverse types of knowledge, including metaphorical, sociocultural, and commonsense knowledge. However, identifying the most useful knowledge remains an open question. We introduce \method{}, a method inspired by the information bottleneck principle that elicits relevant world knowledge from vision and language models which is iteratively refined for generating an explanation of the humor in an unsupervised manner. Our experiments on three datasets confirm the advantage of our method over a range of baselines. Our method can further be adapted in the future for additional tasks that can benefit from eliciting and conditioning on relevant world knowledge and open new research avenues in this direction.

### Mapping of Subjective Accounts into Interpreted Clusters (MOSAIC): Topic Modelling and LLM applied to Stroboscopic Phenomenology 
[[arxiv](https://arxiv.org/abs/2502.18318)] [[cool](https://papers.cool/arxiv/2502.18318)] [[pdf](https://arxiv.org/pdf/2502.18318)]
> **Authors**: Romy Beauté,David J. Schwartzman,Guillaume Dumas,Jennifer Crook,Fiona Macpherson,Adam B. Barrett,Anil K. Seth
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,神经元和认知
- **Abstract**: Stroboscopic light stimulation (SLS) on closed eyes typically induces simple visual hallucinations (VHs), characterised by vivid, geometric and colourful patterns. A dataset of 862 sentences, extracted from 422 open subjective reports, was recently compiled as part of the Dreamachine programme (Collective Act, 2022), an immersive multisensory experience that combines SLS and spatial sound in a collective setting. Although open reports extend the range of reportable phenomenology, their analysis presents significant challenges, particularly in systematically identifying patterns. To address this challenge, we implemented a data-driven approach leveraging Large Language Models and Topic Modelling to uncover and interpret latent experiential topics directly from the Dreamachine's text-based reports. Our analysis confirmed the presence of simple VHs typically documented in scientific studies of SLS, while also revealing experiences of altered states of consciousness and complex hallucinations. Building on these findings, our computational approach expands the systematic study of subjective experience by enabling data-driven analyses of open-ended phenomenological reports, capturing experiences not readily identified through standard questionnaires. By revealing rich and multifaceted aspects of experiences, our study broadens our understanding of stroboscopically-induced phenomena while highlighting the potential of Natural Language Processing and Large Language Models in the emerging field of computational (neuro)phenomenology. More generally, this approach provides a practically applicable methodology for uncovering subtle hidden patterns of subjective experience across diverse research domains.

### WiCkeD: A Simple Method to Make Multiple Choice Benchmarks More Challenging 
[[arxiv](https://arxiv.org/abs/2502.18316)] [[cool](https://papers.cool/arxiv/2502.18316)] [[pdf](https://arxiv.org/pdf/2502.18316)]
> **Authors**: Ahmed Elhady,Eneko Agirre,Mikel Artetxe
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: We introduce WiCkeD, a simple method to increase the complexity of existing multiple-choice benchmarks by randomly replacing a choice with "None of the above", a method often used in educational tests. We show that WiCkeD can be automatically applied to any existing benchmark, making it more challenging. We apply WiCkeD to 6 popular benchmarks and use it to evaluate 18 open-weight LLMs. The performance of the models drops 12.1 points on average with respect to the original versions of the datasets. When using chain-of-thought on 3 MMLU datasets, the performance drop for the WiCkeD variant is similar to the one observed when using the LLMs directly, showing that WiCkeD is also challenging for models with enhanced reasoning abilities. WiCkeD also uncovers that some models are more sensitive to the extra reasoning required, providing additional information with respect to the original benchmarks. We relase our code and data at https://github.com/ahmedselhady/wicked-benchmarks.

### Looking forward: Linguistic theory and methods 
[[arxiv](https://arxiv.org/abs/2502.18313)] [[cool](https://papers.cool/arxiv/2502.18313)] [[pdf](https://arxiv.org/pdf/2502.18313)]
> **Authors**: John Mansfield,Ethan Gotlieb Wilcox
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: This chapter examines current developments in linguistic theory and methods, focusing on the increasing integration of computational, cognitive, and evolutionary perspectives. We highlight four major themes shaping contemporary linguistics: (1) the explicit testing of hypotheses about symbolic representation, such as efficiency, locality, and conceptual semantic grounding; (2) the impact of artificial neural networks on theoretical debates and linguistic analysis; (3) the importance of intersubjectivity in linguistic theory; and (4) the growth of evolutionary linguistics. By connecting linguistics with computer science, psychology, neuroscience, and biology, we provide a forward-looking perspective on the changing landscape of linguistic research.

### RefuteBench 2.0 -- Agentic Benchmark for Dynamic Evaluation of LLM Responses to Refutation Instruction 
[[arxiv](https://arxiv.org/abs/2502.18308)] [[cool](https://papers.cool/arxiv/2502.18308)] [[pdf](https://arxiv.org/pdf/2502.18308)]
> **Authors**: Jianhao Yan,Yun Luo,Yue Zhang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Work on progess
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: In the multi-turn interaction schema, large language models (LLMs) can leverage user feedback to enhance the quality and relevance of their responses. However, evaluating an LLM's ability to incorporate user refutation feedback is crucial yet challenging. In this study, we introduce RefuteBench 2.0, which significantly extends the original RefuteBench by incorporating LLM agents as refuters and evaluators, which allows for flexible and comprehensive assessment. We design both transient and persistent refutation instructions with different validity periods. Meta-evaluation shows that the LLM-based refuter could generate more human-like refutations and the evaluators could assign scores with high correlation with humans. Experimental results of various LLMs show that current models could effectively satisfy the refutation but fail to memorize the refutation information. Interestingly, we also observe that the performance of the initial task decreases as the refutations increase. Analysis of the attention scores further shows a potential weakness of current LLMs: they struggle to retain and correctly use previous information during long context dialogues. https://github.com/ElliottYan/RefuteBench-2.0

### How Vital is the Jurisprudential Relevance: Law Article Intervened Legal Case Retrieval and Matching 
[[arxiv](https://arxiv.org/abs/2502.18292)] [[cool](https://papers.cool/arxiv/2502.18292)] [[pdf](https://arxiv.org/pdf/2502.18292)]
> **Authors**: Nuo Xu,Pinghui Wang,Zi Liang,Junzhou Zhao,Xiaohong Guan
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,信息检索
- **Abstract**: Legal case retrieval (LCR) aims to automatically scour for comparable legal cases based on a given query, which is crucial for offering relevant precedents to support the judgment in intelligent legal systems. Due to similar goals, it is often associated with a similar case matching (LCM) task. To address them, a daunting challenge is assessing the uniquely defined legal-rational similarity within the judicial domain, which distinctly deviates from the semantic similarities in general text retrieval. Past works either tagged domain-specific factors or incorporated reference laws to capture legal-rational information. However, their heavy reliance on expert or unrealistic assumptions restricts their practical applicability in real-world scenarios. In this paper, we propose an end-to-end model named LCM-LAI to solve the above challenges. Through meticulous theoretical analysis, LCM-LAI employs a dependent multi-task learning framework to capture legal-rational information within legal cases by a law article prediction (LAP) sub-task, without any additional assumptions in inference. Besides, LCM-LAI proposes an article-aware attention mechanism to evaluate the legal-rational similarity between across-case sentences based on law distribution, which is more effective than conventional semantic similarity. Weperform a series of exhaustive experiments including two different tasks involving four real-world datasets. Results demonstrate that LCM-LAI achieves state-of-the-art performance.

### DeepSeek-R1 Outperforms Gemini 2.0 Pro, OpenAI o1, and o3-mini in Bilingual Complex Ophthalmology Reasoning 
[[arxiv](https://arxiv.org/abs/2502.17947)] [[cool](https://papers.cool/arxiv/2502.17947)] [[pdf](https://arxiv.org/pdf/2502.17947)]
> **Authors**: Pusheng Xu,Yue Wu,Kai Jin,Xiaolan Chen,Mingguang He,Danli Shi
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 29 pages, 4 figures, 1 table
- **标题**: None
- **领域**: 计算语言学,人工智能,表现
- **Abstract**: Purpose: To evaluate the accuracy and reasoning ability of DeepSeek-R1 and three other recently released large language models (LLMs) in bilingual complex ophthalmology cases. Methods: A total of 130 multiple-choice questions (MCQs) related to diagnosis (n = 39) and management (n = 91) were collected from the Chinese ophthalmology senior professional title examination and categorized into six topics. These MCQs were translated into English using DeepSeek-R1. The responses of DeepSeek-R1, Gemini 2.0 Pro, OpenAI o1 and o3-mini were generated under default configurations between February 15 and February 20, 2025. Accuracy was calculated as the proportion of correctly answered questions, with omissions and extra answers considered incorrect. Reasoning ability was evaluated through analyzing reasoning logic and the causes of reasoning error. Results: DeepSeek-R1 demonstrated the highest overall accuracy, achieving 0.862 in Chinese MCQs and 0.808 in English MCQs. Gemini 2.0 Pro, OpenAI o1, and OpenAI o3-mini attained accuracies of 0.715, 0.685, and 0.692 in Chinese MCQs (all P<0.001 compared with DeepSeek-R1), and 0.746 (P=0.115), 0.723 (P=0.027), and 0.577 (P<0.001) in English MCQs, respectively. DeepSeek-R1 achieved the highest accuracy across five topics in both Chinese and English MCQs. It also excelled in management questions conducted in Chinese (all P<0.05). Reasoning ability analysis showed that the four LLMs shared similar reasoning logic. Ignoring key positive history, ignoring key positive signs, misinterpretation medical data, and too aggressive were the most common causes of reasoning errors. Conclusion: DeepSeek-R1 demonstrated superior performance in bilingual complex ophthalmology reasoning tasks than three other state-of-the-art LLMs. While its clinical applicability remains challenging, it shows promise for supporting diagnosis and clinical decision-making.

### Assessing Large Language Models in Agentic Multilingual National Bias 
[[arxiv](https://arxiv.org/abs/2502.17945)] [[cool](https://papers.cool/arxiv/2502.17945)] [[pdf](https://arxiv.org/pdf/2502.17945)]
> **Authors**: Qianying Liu,Katrina Qiyao Wang,Fei Cheng,Sadao Kurohashi
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 13 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models have garnered significant attention for their capabilities in multilingual natural language processing, while studies on risks associated with cross biases are limited to immediate context preferences. Cross-language disparities in reasoning-based recommendations remain largely unexplored, with a lack of even descriptive analysis. This study is the first to address this gap. We test LLM's applicability and capability in providing personalized advice across three key scenarios: university applications, travel, and relocation. We investigate multilingual bias in state-of-the-art LLMs by analyzing their responses to decision-making tasks across multiple languages. We quantify bias in model-generated scores and assess the impact of demographic factors and reasoning strategies (e.g., Chain-of-Thought prompting) on bias patterns. Our findings reveal that local language bias is prevalent across different tasks, with GPT-4 and Sonnet reducing bias for English-speaking countries compared to GPT-3.5 but failing to achieve robust multilingual alignment, highlighting broader implications for multilingual AI agents and applications such as education.

### CaseGen: A Benchmark for Multi-Stage Legal Case Documents Generation 
[[arxiv](https://arxiv.org/abs/2502.17943)] [[cool](https://papers.cool/arxiv/2502.17943)] [[pdf](https://arxiv.org/pdf/2502.17943)]
> **Authors**: Haitao Li,Jiaying Ye,Yiran Hu,Jia Chen,Qingyao Ai,Yueyue Wu,Junjie Chen,Yifan Chen,Cheng Luo,Quan Zhou,Yiqun Liu
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 18 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Legal case documents play a critical role in judicial proceedings. As the number of cases continues to rise, the reliance on manual drafting of legal case documents is facing increasing pressure and challenges. The development of large language models (LLMs) offers a promising solution for automating document generation. However, existing benchmarks fail to fully capture the complexities involved in drafting legal case documents in real-world scenarios. To address this gap, we introduce CaseGen, the benchmark for multi-stage legal case documents generation in the Chinese legal domain. CaseGen is based on 500 real case samples annotated by legal experts and covers seven essential case sections. It supports four key tasks: drafting defense statements, writing trial facts, composing legal reasoning, and generating judgment results. To the best of our knowledge, CaseGen is the first benchmark designed to evaluate LLMs in the context of legal case document generation. To ensure an accurate and comprehensive evaluation, we design the LLM-as-a-judge evaluation framework and validate its effectiveness through human annotations. We evaluate several widely used general-domain LLMs and legal-specific LLMs, highlighting their limitations in case document generation and pinpointing areas for potential improvement. This work marks a step toward a more effective framework for automating legal case documents drafting, paving the way for the reliable application of AI in the legal field. The dataset and code are publicly available at https://github.com/CSHaitao/CaseGen.

### Advantage-Guided Distillation for Preference Alignment in Small Language Models 
[[arxiv](https://arxiv.org/abs/2502.17927)] [[cool](https://papers.cool/arxiv/2502.17927)] [[pdf](https://arxiv.org/pdf/2502.17927)]
> **Authors**: Shiping Gao,Fanqi Wan,Jiajian Guo,Xiaojun Quan,Qifan Wang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Accepted by ICLR 2025(spotlight)
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Alignment techniques enable Large Language Models (LLMs) to generate outputs that align with human preferences and play a crucial role in their effectiveness. However, their impact often diminishes when applied to Small Language Models (SLMs), likely due to the limited capacity of these models. Instead of directly applying existing alignment techniques to SLMs, we propose to utilize a well-aligned teacher LLM to guide the alignment process for these models, thereby facilitating the transfer of the teacher's knowledge of human preferences to the student model. To achieve this, we first explore a straightforward approach, Dual-Constrained Knowledge Distillation (DCKD), that employs knowledge distillation with two KL-divergence constraints from the aligned teacher to the unaligned student. To further enhance the student's ability to distinguish between preferred and dispreferred responses, we then propose Advantage-Guided Distillation for Preference Alignment (ADPA), which leverages an advantage function from the aligned teacher to deliver more nuanced, distribution-level reward signals for the student's alignment. Our experimental results show that these two approaches appreciably improve the alignment of SLMs and narrow the performance gap with larger counterparts. Among them, ADPA demonstrates superior performance and achieves even greater effectiveness when integrated with DCKD. Our code is available at https://github.com/SLIT-AI/ADPA.

### FACT-AUDIT: An Adaptive Multi-Agent Framework for Dynamic Fact-Checking Evaluation of Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.17924)] [[cool](https://papers.cool/arxiv/2502.17924)] [[pdf](https://arxiv.org/pdf/2502.17924)]
> **Authors**: Hongzhan Lin,Yang Deng,Yuxuan Gu,Wenxuan Zhang,Jing Ma,See-Kiong Ng,Tat-Seng Chua
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models (LLMs) have significantly advanced the fact-checking studies. However, existing automated fact-checking evaluation methods rely on static datasets and classification metrics, which fail to automatically evaluate the justification production and uncover the nuanced limitations of LLMs in fact-checking. In this work, we introduce FACT-AUDIT, an agent-driven framework that adaptively and dynamically assesses LLMs' fact-checking capabilities. Leveraging importance sampling principles and multi-agent collaboration, FACT-AUDIT generates adaptive and scalable datasets, performs iterative model-centric evaluations, and updates assessments based on model-specific responses. By incorporating justification production alongside verdict prediction, this framework provides a comprehensive and evolving audit of LLMs' factual reasoning capabilities, to investigate their trustworthiness. Extensive experiments demonstrate that FACT-AUDIT effectively differentiates among state-of-the-art LLMs, providing valuable insights into model strengths and limitations in model-centric fact-checking analysis.

### Scaling LLM Pre-training with Vocabulary Curriculum 
[[arxiv](https://arxiv.org/abs/2502.17910)] [[cool](https://papers.cool/arxiv/2502.17910)] [[pdf](https://arxiv.org/pdf/2502.17910)]
> **Authors**: Fangyuan Yu
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: under review
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Modern language models rely on static vocabularies, fixed before pretraining, in contrast to the adaptive vocabulary acquisition observed in human language learning. To bridge this gap, we introduce vocabulary curriculum learning, an approach that improves pretraining efficiency with log-linear scaling gains relative to vocabulary size. Our method alternates between entropy-guided vocabulary expansion and model optimization, enabling models to learn transferable representations across diverse tokenization granularities. This approach naturally gives rise to an optimal computation allocation pattern: longer tokens capture predictable content, while shorter tokens focus on more complex, harder-to-predict contexts. Experiments on small-scale GPT models demonstrate improved scaling efficiency, reinforcing the effectiveness of dynamic tokenization. We release our code to support further research and plan to extend our experiments to larger models and diverse domains.

### Can Large Language Models Identify Implicit Suicidal Ideation? An Empirical Evaluation 
[[arxiv](https://arxiv.org/abs/2502.17899)] [[cool](https://papers.cool/arxiv/2502.17899)] [[pdf](https://arxiv.org/pdf/2502.17899)]
> **Authors**: Tong Li,Shu Yang,Junchao Wu,Jiyao Wei,Lijie Hu,Mengdi Li,Derek F. Wong,Joshua R. Oltmanns,Di Wang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: We present a comprehensive evaluation framework for assessing Large Language Models' (LLMs) capabilities in suicide prevention, focusing on two critical aspects: the Identification of Implicit Suicidal ideation (IIS) and the Provision of Appropriate Supportive responses (PAS). We introduce \ourdata, a novel dataset of 1,308 test cases built upon psychological frameworks including D/S-IAT and Negative Automatic Thinking, alongside real-world scenarios. Through extensive experiments with 8 widely used LLMs under different contextual settings, we find that current models struggle significantly with detecting implicit suicidal ideation and providing appropriate support, highlighting crucial limitations in applying LLMs to mental health contexts. Our findings underscore the need for more sophisticated approaches in developing and evaluating LLMs for sensitive psychological applications.

### RankCoT: Refining Knowledge for Retrieval-Augmented Generation through Ranking Chain-of-Thoughts 
[[arxiv](https://arxiv.org/abs/2502.17888)] [[cool](https://papers.cool/arxiv/2502.17888)] [[pdf](https://arxiv.org/pdf/2502.17888)]
> **Authors**: Mingyan Wu,Zhenghao Liu,Yukun Yan,Xinze Li,Shi Yu,Zheni Zeng,Yu Gu,Ge Yu
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Retrieval-Augmented Generation (RAG) enhances the performance of Large Language Models (LLMs) by incorporating external knowledge. However, LLMs still encounter challenges in effectively utilizing the knowledge from retrieved documents, often being misled by irrelevant or noisy information. To address this issue, we introduce RankCoT, a knowledge refinement method that incorporates reranking signals in generating CoT-based summarization for knowledge refinement based on given query and all retrieval documents. During training, RankCoT prompts the LLM to generate Chain-of-Thought (CoT) candidates based on the query and individual documents. It then fine-tunes the LLM to directly reproduce the best CoT from these candidate outputs based on all retrieved documents, which requires LLM to filter out irrelevant documents during generating CoT-style summarization. Additionally, RankCoT incorporates a self-reflection mechanism that further refines the CoT outputs, resulting in higher-quality training data. Our experiments demonstrate the effectiveness of RankCoT, showing its superior performance over other knowledge refinement models. Further analysis reveals that RankCoT can provide shorter but effective refinement results, enabling the generator to produce more accurate answers. All code and data are available at https://github.com/NEUIR/RankCoT.

### Towards Enhanced Immersion and Agency for LLM-based Interactive Drama 
[[arxiv](https://arxiv.org/abs/2502.17878)] [[cool](https://papers.cool/arxiv/2502.17878)] [[pdf](https://arxiv.org/pdf/2502.17878)]
> **Authors**: Hongqiu Wu,Weiqi Wu,Tianyang Xu,Jiameng Zhang,Hai Zhao
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: LLM-based Interactive Drama is a novel AI-based dialogue scenario, where the user (i.e. the player) plays the role of a character in the story, has conversations with characters played by LLM agents, and experiences an unfolding story. This paper begins with understanding interactive drama from two aspects: Immersion, the player's feeling of being present in the story, and Agency, the player's ability to influence the story world. Both are crucial to creating an enjoyable interactive experience, while they have been underexplored in previous work. To enhance these two aspects, we first propose Playwriting-guided Generation, a novel method that helps LLMs craft dramatic stories with substantially improved structures and narrative quality. Additionally, we introduce Plot-based Reflection for LLM agents to refine their reactions to align with the player's intentions. Our evaluation relies on human judgment to assess the gains of our methods in terms of immersion and agency.

### SYNTHEMPATHY: A Scalable Empathy Corpus Generated Using LLMs Without Any Crowdsourcing 
[[arxiv](https://arxiv.org/abs/2502.17857)] [[cool](https://papers.cool/arxiv/2502.17857)] [[pdf](https://arxiv.org/pdf/2502.17857)]
> **Authors**: Run Chen,Jun Shin,Julia Hirschberg
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 10 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Previous research has shown that humans are more receptive towards language models that that exhibit empathetic behavior. While empathy is essential for developing helpful dialogue agents, very few large corpora containing empathetic dialogues are available for fine-tune LLMs. The few existing corpora have largely relied on crowdsourcing to simulate empathetic conversations, a process that is expensive, time-consuming, and not scalable to larger datasets. We propose a data generation framework for developing SYNTHEMPATHY, a large corpus containing 105k empathetic responses to real-life situations compiled through LLM generation. A base Mistral 7B model fine-tuned on our SYNTHEMPATHY corpus exhibits an increase in the average empathy score.

## 密码学和安全(cs.CR:Cryptography and Security)

### Toward Breaking Watermarks in Distortion-free Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.18608)] [[cool](https://papers.cool/arxiv/2502.18608)] [[pdf](https://arxiv.org/pdf/2502.18608)]
> **Authors**: Shayleen Reynolds,Saheed Obitayo,Niccolò Dalmasso,Dung Daniel T. Ngo,Vamsi K. Potluru,Manuela Veloso
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 5 pages, AAAI'25 Workshop on Preventing and DetectingLLMGenerated Misinformation
- **标题**: None
- **领域**: 密码学和安全,机器学习
- **Abstract**: In recent years, LLM watermarking has emerged as an attractive safeguard against AI-generated content, with promising applications in many real-world domains. However, there are growing concerns that the current LLM watermarking schemes are vulnerable to expert adversaries wishing to reverse-engineer the watermarking mechanisms. Prior work in "breaking" or "stealing" LLM watermarks mainly focuses on the distribution-modifying algorithm of Kirchenbauer et al. (2023), which perturbs the logit vector before sampling. In this work, we focus on reverse-engineering the other prominent LLM watermarking scheme, distortion-free watermarking (Kuditipudi et al. 2024), which preserves the underlying token distribution by using a hidden watermarking key sequence. We demonstrate that, even under a more sophisticated watermarking scheme, it is possible to "compromise" the LLM and carry out a "spoofing" attack. Specifically, we propose a mixed integer linear programming framework that accurately estimates the secret key used for watermarking using only a few samples of the watermarked dataset. Our initial findings challenge the current theoretical claims on the robustness and usability of existing LLM watermarking techniques.

### Steganography Beyond Space-Time With Chain of Multimodal AI Agents 
[[arxiv](https://arxiv.org/abs/2502.18547)] [[cool](https://papers.cool/arxiv/2502.18547)] [[pdf](https://arxiv.org/pdf/2502.18547)]
> **Authors**: Ching-Chun Chang,Isao Echizen
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能,多代理系统,多媒体
- **Abstract**: Steganography is the art and science of covert writing, with a broad range of applications interwoven within the realm of cybersecurity. As artificial intelligence continues to evolve, its ability to synthesise realistic content emerges as a threat in the hands of cybercriminals who seek to manipulate and misrepresent the truth. Such synthetic content introduces a non-trivial risk of overwriting the subtle changes made for the purpose of steganography. When the signals in both the spatial and temporal domains are vulnerable to unforeseen overwriting, it calls for reflection on what can remain invariant after all. This study proposes a paradigm in steganography for audiovisual media, where messages are concealed beyond both spatial and temporal domains. A chain of multimodal agents is developed to deconstruct audiovisual content into a cover text, embed a message within the linguistic domain, and then reconstruct the audiovisual content through synchronising both aural and visual modalities with the resultant stego text. The message is encoded by biasing the word sampling process of a language generation model and decoded by analysing the probability distribution of word choices. The accuracy of message transmission is evaluated under both zero-bit and multi-bit capacity settings. Fidelity is assessed through both biometric and semantic similarities, capturing the identities of the recorded face and voice, as well as the core ideas conveyed through the media. Secrecy is examined through statistical comparisons between cover and stego texts. Robustness is tested across various scenarios, including audiovisual compression, face-swapping, voice-cloning and their combinations.

### PII-Bench: Evaluating Query-Aware Privacy Protection Systems 
[[arxiv](https://arxiv.org/abs/2502.18545)] [[cool](https://papers.cool/arxiv/2502.18545)] [[pdf](https://arxiv.org/pdf/2502.18545)]
> **Authors**: Hao Shen,Zhouhong Gu,Haokai Hong,Weili Han
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能,计算语言学,信息检索
- **Abstract**: The widespread adoption of Large Language Models (LLMs) has raised significant privacy concerns regarding the exposure of personally identifiable information (PII) in user prompts. To address this challenge, we propose a query-unrelated PII masking strategy and introduce PII-Bench, the first comprehensive evaluation framework for assessing privacy protection systems. PII-Bench comprises 2,842 test samples across 55 fine-grained PII categories, featuring diverse scenarios from single-subject descriptions to complex multi-party interactions. Each sample is carefully crafted with a user query, context description, and standard answer indicating query-relevant PII. Our empirical evaluation reveals that while current models perform adequately in basic PII detection, they show significant limitations in determining PII query relevance. Even state-of-the-art LLMs struggle with this task, particularly in handling complex multi-subject scenarios, indicating substantial room for improvement in achieving intelligent PII masking.

### A Survey of Zero-Knowledge Proof Based Verifiable Machine Learning 
[[arxiv](https://arxiv.org/abs/2502.18535)] [[cool](https://papers.cool/arxiv/2502.18535)] [[pdf](https://arxiv.org/pdf/2502.18535)]
> **Authors**: Zhizhi Peng,Taotao Wang,Chonghe Zhao,Guofu Liao,Zibin Lin,Yifeng Liu,Bin Cao,Long Shi,Qing Yang,Shengli Zhang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 24 pages, 5 figures, 3 tables
- **标题**: None
- **领域**: 密码学和安全,人工智能,机器学习
- **Abstract**: As machine learning technologies advance rapidly across various domains, concerns over data privacy and model security have grown significantly. These challenges are particularly pronounced when models are trained and deployed on cloud platforms or third-party servers due to the computational resource limitations of users' end devices. In response, zero-knowledge proof (ZKP) technology has emerged as a promising solution, enabling effective validation of model performance and authenticity in both training and inference processes without disclosing sensitive data. Thus, ZKP ensures the verifiability and security of machine learning models, making it a valuable tool for privacy-preserving AI. Although some research has explored the verifiable machine learning solutions that exploit ZKP, a comprehensive survey and summary of these efforts remain absent. This survey paper aims to bridge this gap by reviewing and analyzing all the existing Zero-Knowledge Machine Learning (ZKML) research from June 2017 to December 2024. We begin by introducing the concept of ZKML and outlining its ZKP algorithmic setups under three key categories: verifiable training, verifiable inference, and verifiable testing. Next, we provide a comprehensive categorization of existing ZKML research within these categories and analyze the works in detail. Furthermore, we explore the implementation challenges faced in this field and discuss the improvement works to address these obstacles. Additionally, we highlight several commercial applications of ZKML technology. Finally, we propose promising directions for future advancements in this domain.

### ARACNE: An LLM-Based Autonomous Shell Pentesting Agent 
[[arxiv](https://arxiv.org/abs/2502.18528)] [[cool](https://papers.cool/arxiv/2502.18528)] [[pdf](https://arxiv.org/pdf/2502.18528)]
> **Authors**: Tomas Nieponice,Veronica Valeros,Sebastian Garcia
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-26
> **comment**: 7 pages, 2 figures, 3 tables
- **标题**: None
- **领域**: 密码学和安全,人工智能,机器人技术
- **Abstract**: We introduce ARACNE, a fully autonomous LLM-based pentesting agent tailored for SSH services that can execute commands on real Linux shell systems. Introduces a new agent architecture with multi-LLM model support. Experiments show that ARACNE can reach a 60\% success rate against the autonomous defender ShelLM and a 57.58\% success rate against the Over The Wire Bandit CTF challenges, improving over the state-of-the-art. When winning, the average number of actions taken by the agent to accomplish the goals was less than 5. The results show that the use of multi-LLM is a promising approach to increase accuracy in the actions.

### GOD model: Privacy Preserved AI School for Personal Assistant 
[[arxiv](https://arxiv.org/abs/2502.18527)] [[cool](https://papers.cool/arxiv/2502.18527)] [[pdf](https://arxiv.org/pdf/2502.18527)]
> **Authors**: PIN AI Team,Bill Qingyun Sun,Laura Florescu,Boliang Zhang,Regan Peng,Smile Hu,Shouqiao Wang,Ben Wu,Xi Wang,Davide Crapis,Gavin Zhen Guo
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能,机器学习
- **Abstract**: Personal AI assistants (e.g., Apple Intelligence, Meta AI) offer proactive recommendations that simplify everyday tasks, but their reliance on sensitive user data raises concerns about privacy and trust. To address these challenges, we introduce the Guardian of Data (GOD), a secure, privacy-preserving framework for training and evaluating AI assistants directly on-device. Unlike traditional benchmarks, the GOD model measures how well assistants can anticipate user needs-such as suggesting gifts-while protecting user data and autonomy. Functioning like an AI school, it addresses the cold start problem by simulating user queries and employing a curriculum-based approach to refine the performance of each assistant. Running within a Trusted Execution Environment (TEE), it safeguards user data while applying reinforcement and imitation learning to refine AI recommendations. A token-based incentive system encourages users to share data securely, creating a data flywheel that drives continuous improvement. By integrating privacy, personalization, and trust, the GOD model provides a scalable, responsible path for advancing personal AI assistants. For community collaboration, part of the framework is open-sourced at https://github.com/PIN-AI/God-Model.

### Class-Conditional Neural Polarizer: A Lightweight and Effective Backdoor Defense by Purifying Poisoned Features 
[[arxiv](https://arxiv.org/abs/2502.18520)] [[cool](https://papers.cool/arxiv/2502.18520)] [[pdf](https://arxiv.org/pdf/2502.18520)]
> **Authors**: Mingli Zhu,Shaokui Wei,Hongyuan Zha,Baoyuan Wu
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Recent studies have highlighted the vulnerability of deep neural networks to backdoor attacks, where models are manipulated to rely on embedded triggers within poisoned samples, despite the presence of both benign and trigger information. While several defense methods have been proposed, they often struggle to balance backdoor mitigation with maintaining benign performance.In this work, inspired by the concept of optical polarizer-which allows light waves of specific polarizations to pass while filtering others-we propose a lightweight backdoor defense approach, NPD. This method integrates a neural polarizer (NP) as an intermediate layer within the compromised model, implemented as a lightweight linear transformation optimized via bi-level optimization. The learnable NP filters trigger information from poisoned samples while preserving benign content. Despite its effectiveness, we identify through empirical studies that NPD's performance degrades when the target labels (required for purification) are inaccurately estimated. To address this limitation while harnessing the potential of targeted adversarial mitigation, we propose class-conditional neural polarizer-based defense (CNPD). The key innovation is a fusion module that integrates the backdoored model's predicted label with the features to be purified. This architecture inherently mimics targeted adversarial defense mechanisms without requiring label estimation used in NPD. We propose three implementations of CNPD: the first is r-CNPD, which trains a replicated NP layer for each class and, during inference, selects the appropriate NP layer for defense based on the predicted class from the backdoored model. To efficiently handle a large number of classes, two variants are designed: e-CNPD, which embeds class information as additional features, and a-CNPD, which directs network attention using class information.

### Swallowing the Poison Pills: Insights from Vulnerability Disparity Among LLMs 
[[arxiv](https://arxiv.org/abs/2502.18518)] [[cool](https://papers.cool/arxiv/2502.18518)] [[pdf](https://arxiv.org/pdf/2502.18518)]
> **Authors**: Peng Yifeng,Wu Zhizheng,Chen Chen
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Modern large language models (LLMs) exhibit critical vulnerabilities to poison pill attacks: localized data poisoning that alters specific factual knowledge while preserving overall model utility. We systematically demonstrate these attacks exploit inherent architectural properties of LLMs, achieving 54.6% increased retrieval inaccuracy on long-tail knowledge versus dominant topics and up to 25.5% increase retrieval inaccuracy on compressed models versus original architectures. Through controlled mutations (e.g., temporal/spatial/entity alterations) and, our method induces localized memorization deterioration with negligible impact on models' performance on regular standard benchmarks (e.g., <2% performance drop on MMLU/GPQA), leading to potential detection evasion. Our findings suggest: (1) Disproportionate vulnerability in long-tail knowledge may result from reduced parameter redundancy; (2) Model compression may increase attack surfaces, with pruned/distilled models requiring 30% fewer poison samples for equivalent damage; (3) Associative memory enables both spread of collateral damage to related concepts and amplification of damage from simultaneous attack, particularly for dominant topics. These findings raise concerns over current scaling paradigms since attack costs are lowering while defense complexity is rising. Our work establishes poison pills as both a security threat and diagnostic tool, revealing critical security-efficiency trade-offs in language model compression that challenges prevailing safety assumptions.

### RewardDS: Privacy-Preserving Fine-Tuning for Large Language Models via Reward Driven Data Synthesis 
[[arxiv](https://arxiv.org/abs/2502.18517)] [[cool](https://papers.cool/arxiv/2502.18517)] [[pdf](https://arxiv.org/pdf/2502.18517)]
> **Authors**: Jianwei Wang,Junyao Yang,Haoran Li,Huiping Zhuang,Cen Chen,Ziqian Zeng
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: The success of large language models (LLMs) has attracted many individuals to fine-tune them for domain-specific tasks by uploading their data. However, in sensitive areas like healthcare and finance, privacy concerns often arise. One promising solution is to sample synthetic data with Differential Privacy (DP) guarantees to replace private data. However, these synthetic data contain significant flawed data, which are considered as noise. Existing solutions typically rely on naive filtering by comparing ROUGE-L scores or embedding similarities, which are ineffective in addressing the noise. To address this issue, we propose RewardDS, a novel privacy-preserving framework that fine-tunes a reward proxy model and uses reward signals to guide the synthetic data generation. Our RewardDS introduces two key modules, Reward Guided Filtering and Self-Optimizing Refinement, to both filter and refine the synthetic data, effectively mitigating the noise. Extensive experiments across medical, financial, and code generation domains demonstrate the effectiveness of our method.

### A Multi-Agent Framework for Automated Vulnerability Detection and Repair in Solidity and Move Smart Contracts 
[[arxiv](https://arxiv.org/abs/2502.18515)] [[cool](https://papers.cool/arxiv/2502.18515)] [[pdf](https://arxiv.org/pdf/2502.18515)]
> **Authors**: Rabimba Karanjai,Sam Blackshear,Lei Xu,Weidong Shi
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能,多代理系统,软件工程
- **Abstract**: The rapid growth of the blockchain ecosystem and the increasing value locked in smart contracts necessitate robust security measures. While languages like Solidity and Move aim to improve smart contract security, vulnerabilities persist. This paper presents Smartify, a novel multi-agent framework leveraging Large Language Models (LLMs) to automatically detect and repair vulnerabilities in Solidity and Move smart contracts. Unlike traditional methods that rely solely on vast pre-training datasets, Smartify employs a team of specialized agents working on different specially fine-tuned LLMs to analyze code based on underlying programming concepts and language-specific security principles. We evaluated Smartify on a dataset for Solidity and a curated dataset for Move, demonstrating its effectiveness in fixing a wide range of vulnerabilities. Our results show that Smartify (Gemma2+codegemma) achieves state-of-the-art performance, surpassing existing LLMs and enhancing general-purpose models' capabilities, such as Llama 3.1. Notably, Smartify can incorporate language-specific knowledge, such as the nuances of Move, without requiring massive language-specific pre-training datasets. This work offers a detailed analysis of various LLMs' performance on smart contract repair, highlighting the strengths of our multi-agent approach and providing a blueprint for developing more secure and reliable decentralized applications in the growing blockchain landscape. We also provide a detailed recipe for extending this to other similar use cases.

### CipherFace: A Fully Homomorphic Encryption-Driven Framework for Secure Cloud-Based Facial Recognition 
[[arxiv](https://arxiv.org/abs/2502.18514)] [[cool](https://papers.cool/arxiv/2502.18514)] [[pdf](https://arxiv.org/pdf/2502.18514)]
> **Authors**: Sefik Serengil,Alper Ozpinar
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,计算机视觉和模式识别,机器学习
- **Abstract**: Facial recognition systems rely on embeddings to represent facial images and determine identity by verifying if the distance between embeddings is below a pre-tuned threshold. While embeddings are not reversible to original images, they still contain sensitive information, making their security critical. Traditional encryption methods like AES are limited in securely utilizing cloud computational power for distance calculations. Homomorphic Encryption, allowing calculations on encrypted data, offers a robust alternative. This paper introduces CipherFace, a homomorphic encryption-driven framework for secure cloud-based facial recognition, which we have open-sourced at http://github.com/serengil/cipherface. By leveraging FHE, CipherFace ensures the privacy of embeddings while utilizing the cloud for efficient distance computation. Furthermore, we propose a novel encrypted distance computation method for both Euclidean and Cosine distances, addressing key challenges in performing secure similarity calculations on encrypted data. We also conducted experiments with different facial recognition models, various embedding sizes, and cryptosystem configurations, demonstrating the scalability and effectiveness of CipherFace in real-world applications.

### VVRec: Reconstruction Attacks on DL-based Volumetric Video Upstreaming via Latent Diffusion Model with Gamma Distribution 
[[arxiv](https://arxiv.org/abs/2502.17880)] [[cool](https://papers.cool/arxiv/2502.17880)] [[pdf](https://arxiv.org/pdf/2502.17880)]
> **Authors**: Rui Lu,Bihai Zhang,Dan Wang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,计算机视觉和模式识别,多媒体
- **Abstract**: With the popularity of 3D volumetric video applications, such as Autonomous Driving, Virtual Reality, and Mixed Reality, current developers have turned to deep learning for compressing volumetric video frames, i.e., point clouds for video upstreaming. The latest deep learning-based solutions offer higher efficiency, lower distortion, and better hardware support compared to traditional ones like MPEG and JPEG. However, privacy threats arise, especially reconstruction attacks targeting to recover the original input point cloud from the intermediate results. In this paper, we design VVRec, to the best of our knowledge, which is the first targeting DL-based Volumetric Video Reconstruction attack scheme. VVRec demonstrates the ability to reconstruct high-quality point clouds from intercepted transmission intermediate results using four well-trained neural network modules we design. Leveraging the latest latent diffusion models with Gamma distribution and a refinement algorithm, VVRec excels in reconstruction quality, color recovery, and surpasses existing defenses. We evaluate VVRec using three volumetric video datasets. The results demonstrate that VVRec achieves 64.70dB reconstruction accuracy, with an impressive 46.39% reduction of distortion over baselines.

## 计算机视觉和模式识别(cs.CV:Computer Vision and Pattern Recognition)

### DeBUGCN -- Detecting Backdoors in CNNs Using Graph Convolutional Networks 
[[arxiv](https://arxiv.org/abs/2502.18592)] [[cool](https://papers.cool/arxiv/2502.18592)] [[pdf](https://arxiv.org/pdf/2502.18592)]
> **Authors**: Akash Vartak,Khondoker Murad Hossain,Tim Oates
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 18 pages, 11 tables, 8 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Deep neural networks (DNNs) are becoming commonplace in critical applications, making their susceptibility to backdoor (trojan) attacks a significant problem. In this paper, we introduce a novel backdoor attack detection pipeline, detecting attacked models using graph convolution networks (DeBUGCN). To the best of our knowledge, ours is the first use of GCNs for trojan detection. We use the static weights of a DNN to create a graph structure of its layers. A GCN is then used as a binary classifier on these graphs, yielding a trojan or clean determination for the DNN. To demonstrate the efficacy of our pipeline, we train hundreds of clean and trojaned CNN models on the MNIST handwritten digits and CIFAR-10 image datasets, and show the DNN classification results using DeBUGCN. For a true In-the-Wild use case, our pipeline is evaluated on the TrojAI dataset which consists of various CNN architectures, thus showing the robustness and model-agnostic behaviour of DeBUGCN. Furthermore, on comparing our results on several datasets with state-of-the-art trojan detection algorithms, DeBUGCN is faster and more accurate.

### Application of Attention Mechanism with Bidirectional Long Short-Term Memory (BiLSTM) and CNN for Human Conflict Detection using Computer Vision 
[[arxiv](https://arxiv.org/abs/2502.18555)] [[cool](https://papers.cool/arxiv/2502.18555)] [[pdf](https://arxiv.org/pdf/2502.18555)]
> **Authors**: Erick da Silva Farias,Eduardo Palhares Junior
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: The automatic detection of human conflicts through videos is a crucial area in computer vision, with significant applications in monitoring and public safety policies. However, the scarcity of public datasets and the complexity of human interactions make this task challenging. This study investigates the integration of advanced deep learning techniques, including Attention Mechanism, Convolutional Neural Networks (CNNs), and Bidirectional Long ShortTerm Memory (BiLSTM), to improve the detection of violent behaviors in videos. The research explores how the use of the attention mechanism can help focus on the most relevant parts of the video, enhancing the accuracy and robustness of the model. The experiments indicate that the combination of CNNs with BiLSTM and the attention mechanism provides a promising solution for conflict monitoring, offering insights into the effectiveness of different strategies. This work opens new possibilities for the development of automated surveillance systems that can operate more efficiently in real-time detection of violent events.

### Multi-class Seismic Building Damage Assessment from InSAR Imagery using Quadratic Variational Causal Bayesian Inference 
[[arxiv](https://arxiv.org/abs/2502.18546)] [[cool](https://papers.cool/arxiv/2502.18546)] [[pdf](https://arxiv.org/pdf/2502.18546)]
> **Authors**: Xuechun Li,Susu Xu
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Submitted to Remote Sensing and Environment
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Interferometric Synthetic Aperture Radar (InSAR) technology uses satellite radar to detect surface deformation patterns and monitor earthquake impacts on buildings. While vital for emergency response planning, extracting multi-class building damage classifications from InSAR data faces challenges: overlapping damage signatures with environmental noise, computational complexity in multi-class scenarios, and the need for rapid regional-scale processing. Our novel multi-class variational causal Bayesian inference framework with quadratic variational bounds provides rigorous approximations while ensuring efficiency. By integrating InSAR observations with USGS ground failure models and building fragility functions, our approach separates building damage signals while maintaining computational efficiency through strategic pruning. Evaluation across five major earthquakes (Haiti 2021, Puerto Rico 2020, Zagreb 2020, Italy 2016, Ridgecrest 2019) shows improved damage classification accuracy (AUC: 0.94-0.96), achieving up to 35.7% improvement over existing methods. Our approach maintains high accuracy (AUC > 0.93) across all damage categories while reducing computational overhead by over 40% without requiring extensive ground truth data.

### FilterRAG: Zero-Shot Informed Retrieval-Augmented Generation to Mitigate Hallucinations in VQA 
[[arxiv](https://arxiv.org/abs/2502.18536)] [[cool](https://papers.cool/arxiv/2502.18536)] [[pdf](https://arxiv.org/pdf/2502.18536)]
> **Authors**: S M Sarwar
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 12 pages, 6 figures and 2 tables
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算语言学,信息检索,机器学习
- **Abstract**: Visual Question Answering requires models to generate accurate answers by integrating visual and textual understanding. However, VQA models still struggle with hallucinations, producing convincing but incorrect answers, particularly in knowledge-driven and Out-of-Distribution scenarios. We introduce FilterRAG, a retrieval-augmented framework that combines BLIP-VQA with Retrieval-Augmented Generation to ground answers in external knowledge sources like Wikipedia and DBpedia. FilterRAG achieves 36.5% accuracy on the OK-VQA dataset, demonstrating its effectiveness in reducing hallucinations and improving robustness in both in-domain and Out-of-Distribution settings. These findings highlight the potential of FilterRAG to improve Visual Question Answering systems for real-world deployment.

### Convolutional neural networks for mineral prospecting through alteration mapping with remote sensing data 
[[arxiv](https://arxiv.org/abs/2502.18533)] [[cool](https://papers.cool/arxiv/2502.18533)] [[pdf](https://arxiv.org/pdf/2502.18533)]
> **Authors**: Ehsan Farahbakhsh,Dakshi Goel,Dhiraj Pimparkar,R. Dietmar Muller,Rohitash Chandra
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习,图像和视频处理
- **Abstract**: Traditional geological mapping, based on field observations and rock sample analysis, is inefficient for continuous spatial mapping of features like alteration zones. Deep learning models, such as convolutional neural networks (CNNs), have revolutionised remote sensing data analysis by automatically extracting features for classification and regression tasks. CNNs can detect specific mineralogical changes linked to mineralisation by identifying subtle features in remote sensing data. This study uses CNNs with Landsat 8, Landsat 9, and ASTER data to map alteration zones north of Broken Hill, New South Wales, Australia. The model is trained using ground truth data and an automated approach with selective principal component analysis (PCA). We compare CNNs with traditional machine learning models, including k-nearest neighbours, support vector machines, and multilayer perceptron. Results show that ground truth-based training yields more reliable maps, with CNNs slightly outperforming conventional models in capturing spatial patterns. Landsat 9 outperforms Landsat 8 in mapping iron oxide areas using ground truth-trained CNNs, while ASTER data provides the most accurate argillic and propylitic alteration maps. This highlights CNNs' effectiveness in improving geological mapping precision, especially for identifying subtle mineralisation-related alterations.

### IMPROVE: Iterative Model Pipeline Refinement and Optimization Leveraging LLM Agents 
[[arxiv](https://arxiv.org/abs/2502.18530)] [[cool](https://papers.cool/arxiv/2502.18530)] [[pdf](https://arxiv.org/pdf/2502.18530)]
> **Authors**: Eric Xue,Zeyi Huang,Yuyang Ji,Haohan Wang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Computer vision is a critical component in a wide range of real-world applications, including plant monitoring in agriculture and handwriting classification in digital systems. However, developing high-performance computer vision models traditionally demands both machine learning (ML) expertise and domain-specific knowledge, making the process costly, labor-intensive, and inaccessible to many. Large language model (LLM) agents have emerged as a promising solution to automate this workflow, but most existing methods share a common limitation: they attempt to optimize entire pipelines in a single step before evaluation, making it difficult to attribute improvements to specific changes. This lack of granularity leads to unstable optimization and slower convergence, limiting their effectiveness. To address this, we introduce Iterative Refinement, a novel strategy for LLM-driven ML pipeline design inspired by how human ML experts iteratively refine models, focusing on one component at a time rather than making sweeping changes all at once. By systematically updating individual components based on real training feedback, Iterative Refinement improves stability, interpretability, and overall model performance. We implement this strategy in IMPROVE, an end-to-end LLM agent framework for automating and optimizing object classification pipelines. Through extensive evaluations across datasets of varying sizes and domains, including standard benchmarks and Kaggle competition datasets, we demonstrate that Iterative Refinement enables IMPROVE to consistently achieve better performance over existing zero-shot LLM-based approaches. These findings establish Iterative Refinement as an effective new strategy for LLM-driven ML automation and position IMPROVE as an accessible solution for building high-quality computer vision models without requiring ML expertise.

### Optimized Custom CNN for Real-Time Tomato Leaf Disease Detection 
[[arxiv](https://arxiv.org/abs/2502.18521)] [[cool](https://papers.cool/arxiv/2502.18521)] [[pdf](https://arxiv.org/pdf/2502.18521)]
> **Authors**: Mangsura Kabir Oni,Tabia Tanzin Prama
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: In Bangladesh, tomatoes are a staple vegetable, prized for their versatility in various culinary applications. However, the cultivation of tomatoes is often hindered by a range of diseases that can significantly reduce crop yields and quality. Early detection of these diseases is crucial for implementing timely interventions and ensuring the sustainability of tomato production. Traditional manual inspection methods, while effective, are labor-intensive and prone to human error. To address these challenges, this research paper sought to develop an automated disease detection system using Convolutional Neural Networks (CNNs). A comprehensive dataset of tomato leaves was collected from the Brahmanbaria district, preprocessed to enhance image quality, and then applied to various deep learning models. Comparative performance analysis was conducted between YOLOv5, MobileNetV2, ResNet18, and our custom CNN model. In our study, the Custom CNN model achieved an impressive accuracy of 95.2%, significantly outperforming the other models, which achieved an accuracy of 77%, 89.38% and 71.88% respectively. While other models showed solid performance, our Custom CNN demonstrated superior results specifically tailored for the task of tomato leaf disease detection. These findings highlight the strong potential of deep learning techniques for improving early disease detection in tomato crops. By leveraging these advanced technologies, farmers can gain valuable insights to detect diseases at an early stage, allowing for more effective management practices. This approach not only promises to boost tomato yields but also contributes to the sustainability and resilience of the agricultural sector, helping to mitigate the impact of plant diseases on crop production.

### GHOST 2.0: generative high-fidelity one shot transfer of heads 
[[arxiv](https://arxiv.org/abs/2502.18417)] [[cool](https://papers.cool/arxiv/2502.18417)] [[pdf](https://arxiv.org/pdf/2502.18417)]
> **Authors**: Alexander Groshev,Anastasiia Iashchenko,Pavel Paramonov,Denis Dimitrov,Andrey Kuznetsov
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: While the task of face swapping has recently gained attention in the research community, a related problem of head swapping remains largely unexplored. In addition to skin color transfer, head swap poses extra challenges, such as the need to preserve structural information of the whole head during synthesis and inpaint gaps between swapped head and background. In this paper, we address these concerns with GHOST 2.0, which consists of two problem-specific modules. First, we introduce enhanced Aligner model for head reenactment, which preserves identity information at multiple scales and is robust to extreme pose variations. Secondly, we use a Blender module that seamlessly integrates the reenacted head into the target background by transferring skin color and inpainting mismatched regions. Both modules outperform the baselines on the corresponding tasks, allowing to achieve state of the art results in head swapping. We also tackle complex cases, such as large difference in hair styles of source and target. Code is available at https://github.com/ai-forever/ghost-2.0

### MedKAN: An Advanced Kolmogorov-Arnold Network for Medical Image Classification 
[[arxiv](https://arxiv.org/abs/2502.18416)] [[cool](https://papers.cool/arxiv/2502.18416)] [[pdf](https://arxiv.org/pdf/2502.18416)]
> **Authors**: Zhuoqin Yang,Jiansong Zhang,Xiaoling Luo,Zheng Lu,Linlin Shen
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Recent advancements in deep learning for image classification predominantly rely on convolutional neural networks (CNNs) or Transformer-based architectures. However, these models face notable challenges in medical imaging, particularly in capturing intricate texture details and contextual features. Kolmogorov-Arnold Networks (KANs) represent a novel class of architectures that enhance nonlinear transformation modeling, offering improved representation of complex features. In this work, we present MedKAN, a medical image classification framework built upon KAN and its convolutional extensions. MedKAN features two core modules: the Local Information KAN (LIK) module for fine-grained feature extraction and the Global Information KAN (GIK) module for global context integration. By combining these modules, MedKAN achieves robust feature modeling and fusion. To address diverse computational needs, we introduce three scalable variants--MedKAN-S, MedKAN-B, and MedKAN-L. Experimental results on nine public medical imaging datasets demonstrate that MedKAN achieves superior performance compared to CNN- and Transformer-based models, highlighting its effectiveness and generalizability in medical image analysis.

### OmniAlign-V: Towards Enhanced Alignment of MLLMs with Human Preference 
[[arxiv](https://arxiv.org/abs/2502.18411)] [[cool](https://papers.cool/arxiv/2502.18411)] [[pdf](https://arxiv.org/pdf/2502.18411)]
> **Authors**: Xiangyu Zhao,Shengyuan Ding,Zicheng Zhang,Haian Huang,Maosong Cao,Weiyun Wang,Jiaqi Wang,Xinyu Fang,Wenhai Wang,Guangtao Zhai,Haodong Duan,Hua Yang,Kai Chen
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Recent advancements in open-source multi-modal large language models (MLLMs) have primarily focused on enhancing foundational capabilities, leaving a significant gap in human preference alignment. This paper introduces OmniAlign-V, a comprehensive dataset of 200K high-quality training samples featuring diverse images, complex questions, and varied response formats to improve MLLMs' alignment with human preferences. We also present MM-AlignBench, a human-annotated benchmark specifically designed to evaluate MLLMs' alignment with human values. Experimental results show that finetuning MLLMs with OmniAlign-V, using Supervised Fine-Tuning (SFT) or Direct Preference Optimization (DPO), significantly enhances human preference alignment while maintaining or enhancing performance on standard VQA benchmarks, preserving their fundamental capabilities. Our datasets, benchmark, code and checkpoints have been released at https://github.com/PhoenixZ810/OmniAlign-V.

### EgoSim: An Egocentric Multi-view Simulator and Real Dataset for Body-worn Cameras during Motion and Activity 
[[arxiv](https://arxiv.org/abs/2502.18373)] [[cool](https://papers.cool/arxiv/2502.18373)] [[pdf](https://arxiv.org/pdf/2502.18373)]
> **Authors**: Dominik Hollidt,Paul Streli,Jiaxi Jiang,Yasaman Haghighi,Changlin Qian,Xintong Liu,Christian Holz
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: Research on egocentric tasks in computer vision has mostly focused on head-mounted cameras, such as fisheye cameras or embedded cameras inside immersive headsets. We argue that the increasing miniaturization of optical sensors will lead to the prolific integration of cameras into many more body-worn devices at various locations. This will bring fresh perspectives to established tasks in computer vision and benefit key areas such as human motion tracking, body pose estimation, or action recognition -- particularly for the lower body, which is typically occluded. In this paper, we introduce EgoSim, a novel simulator of body-worn cameras that generates realistic egocentric renderings from multiple perspectives across a wearer's body. A key feature of EgoSim is its use of real motion capture data to render motion artifacts, which are especially noticeable with arm- or leg-worn cameras. In addition, we introduce MultiEgoView, a dataset of egocentric footage from six body-worn cameras and ground-truth full-body 3D poses during several activities: 119 hours of data are derived from AMASS motion sequences in four high-fidelity virtual environments, which we augment with 5 hours of real-world motion data from 13 participants using six GoPro cameras and 3D body pose references from an Xsens motion capture suit. We demonstrate EgoSim's effectiveness by training an end-to-end video-only 3D pose estimation network. Analyzing its domain gap, we show that our dataset and simulator substantially aid training for inference on real-world data. EgoSim code & MultiEgoView dataset: https://siplab.org/projects/EgoSim

### Near-Shore Mapping for Detection and Tracking of Vessels 
[[arxiv](https://arxiv.org/abs/2502.18368)] [[cool](https://papers.cool/arxiv/2502.18368)] [[pdf](https://arxiv.org/pdf/2502.18368)]
> **Authors**: Nicholas Dalhaug,Annette Stahl,Rudolf Mester,Edmund Førland Brekke
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Submitted to FUSION 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: For an autonomous surface vessel (ASV) to dock, it must track other vessels close to the docking area. Kayaks present a particular challenge due to their proximity to the dock and relatively small size. Maritime target tracking has typically employed land masking to filter out land and the dock. However, imprecise land masking makes it difficult to track close-to-dock objects. Our approach uses Light Detection And Ranging (LiDAR) data and maps the docking area offline. The precise 3D measurements allow for precise map creation. However, the mapping could result in static, yet potentially moving, objects being mapped. We detect and filter out potentially moving objects from the LiDAR data by utilizing image data. The visual vessel detection and segmentation method is a neural network that is trained on our labeled data. Close-to-shore tracking improves with an accurate map and is demonstrated on a recently gathered real-world dataset. The dataset contains multiple sequences of a kayak and a day cruiser moving close to the dock, in a collision path with an autonomous ferry prototype.

### ART: Anonymous Region Transformer for Variable Multi-Layer Transparent Image Generation 
[[arxiv](https://arxiv.org/abs/2502.18364)] [[cool](https://papers.cool/arxiv/2502.18364)] [[pdf](https://arxiv.org/pdf/2502.18364)]
> **Authors**: Yifan Pu,Yiming Zhao,Zhicong Tang,Ruihong Yin,Haoxing Ye,Yuhui Yuan,Dong Chen,Jianmin Bao,Sirui Zhang,Yanbin Wang,Lin Liang,Lijuan Wang,Ji Li,Xiu Li,Zhouhui Lian,Gao Huang,Baining Guo
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Project page: https://art-msra.github.io/
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Multi-layer image generation is a fundamental task that enables users to isolate, select, and edit specific image layers, thereby revolutionizing interactions with generative models. In this paper, we introduce the Anonymous Region Transformer (ART), which facilitates the direct generation of variable multi-layer transparent images based on a global text prompt and an anonymous region layout. Inspired by Schema theory suggests that knowledge is organized in frameworks (schemas) that enable people to interpret and learn from new information by linking it to prior knowledge.}, this anonymous region layout allows the generative model to autonomously determine which set of visual tokens should align with which text tokens, which is in contrast to the previously dominant semantic layout for the image generation task. In addition, the layer-wise region crop mechanism, which only selects the visual tokens belonging to each anonymous region, significantly reduces attention computation costs and enables the efficient generation of images with numerous distinct layers (e.g., 50+). When compared to the full attention approach, our method is over 12 times faster and exhibits fewer layer conflicts. Furthermore, we propose a high-quality multi-layer transparent image autoencoder that supports the direct encoding and decoding of the transparency of variable multi-layer images in a joint manner. By enabling precise control and scalable layer generation, ART establishes a new paradigm for interactive content creation.

### Self-Supervised Data Generation for Precision Agriculture: Blending Simulated Environments with Real Imagery 
[[arxiv](https://arxiv.org/abs/2502.18320)] [[cool](https://papers.cool/arxiv/2502.18320)] [[pdf](https://arxiv.org/pdf/2502.18320)]
> **Authors**: Leonardo Saraceni,Ionut Marian Motoi,Daniele Nardi,Thomas Alessandro Ciarfuglia
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Presented at 2024 IEEE 20th International Conference on Automation Science and Engineering (CASE)
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习,机器人技术
- **Abstract**: In precision agriculture, the scarcity of labeled data and significant covariate shifts pose unique challenges for training machine learning models. This scarcity is particularly problematic due to the dynamic nature of the environment and the evolving appearance of agricultural subjects as living things. We propose a novel system for generating realistic synthetic data to address these challenges. Utilizing a vineyard simulator based on the Unity engine, our system employs a cut-and-paste technique with geometrical consistency considerations to produce accurate photo-realistic images and labels from synthetic environments to train detection algorithms. This approach generates diverse data samples across various viewpoints and lighting conditions. We demonstrate considerable performance improvements in training a state-of-the-art detector by applying our method to table grapes cultivation. The combination of techniques can be easily automated, an increasingly important consideration for adoption in agricultural practice.

### LDGen: Enhancing Text-to-Image Synthesis via Large Language Model-Driven Language Representation 
[[arxiv](https://arxiv.org/abs/2502.18302)] [[cool](https://papers.cool/arxiv/2502.18302)] [[pdf](https://arxiv.org/pdf/2502.18302)]
> **Authors**: Pengzhi Li,Pengfei Yu,Zide Liu,Wei He,Xuhao Pan,Xudong Rao,Tao Wei,Wei Chen
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: In this paper, we introduce LDGen, a novel method for integrating large language models (LLMs) into existing text-to-image diffusion models while minimizing computational demands. Traditional text encoders, such as CLIP and T5, exhibit limitations in multilingual processing, hindering image generation across diverse languages. We address these challenges by leveraging the advanced capabilities of LLMs. Our approach employs a language representation strategy that applies hierarchical caption optimization and human instruction techniques to derive precise semantic information,. Subsequently, we incorporate a lightweight adapter and a cross-modal refiner to facilitate efficient feature alignment and interaction between LLMs and image features. LDGen reduces training time and enables zero-shot multilingual image generation. Experimental results indicate that our method surpasses baseline models in both prompt adherence and image aesthetic quality, while seamlessly supporting multiple languages. Project page: https://zrealli.github.io/LDGen.

### Stealthy Backdoor Attack in Self-Supervised Learning Vision Encoders for Large Vision Language Models 
[[arxiv](https://arxiv.org/abs/2502.18290)] [[cool](https://papers.cool/arxiv/2502.18290)] [[pdf](https://arxiv.org/pdf/2502.18290)]
> **Authors**: Zhaoyi Liu,Huan Zhang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Self-supervised learning (SSL) vision encoders learn high-quality image representations and thus have become a vital part of developing vision modality of large vision language models (LVLMs). Due to the high cost of training such encoders, pre-trained encoders are widely shared and deployed into many LVLMs, which are security-critical or bear societal significance. Under this practical scenario, we reveal a new backdoor threat that significant visual hallucinations can be induced into these LVLMs by merely compromising vision encoders. Because of the sharing and reuse of these encoders, many downstream LVLMs may inherit backdoor behaviors from encoders, leading to widespread backdoors. In this work, we propose BadVision, the first method to exploit this vulnerability in SSL vision encoders for LVLMs with novel trigger optimization and backdoor learning techniques. We evaluate BadVision on two types of SSL encoders and LVLMs across eight benchmarks. We show that BadVision effectively drives the LVLMs to attacker-chosen hallucination with over 99% attack success rate, causing a 77.6% relative visual understanding error while maintaining the stealthiness. SoTA backdoor detection methods cannot detect our attack effectively.

### Robust Polyp Detection and Diagnosis through Compositional Prompt-Guided Diffusion Models 
[[arxiv](https://arxiv.org/abs/2502.17951)] [[cool](https://papers.cool/arxiv/2502.17951)] [[pdf](https://arxiv.org/pdf/2502.17951)]
> **Authors**: Jia Yu,Yan Zhu,Peiyao Fu,Tianyi Chen,Junbo Huang,Quanlin Li,Pinghong Zhou,Zhihua Wang,Fei Wu,Shuo Wang,Xian Yang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Colorectal cancer (CRC) is a significant global health concern, and early detection through screening plays a critical role in reducing mortality. While deep learning models have shown promise in improving polyp detection, classification, and segmentation, their generalization across diverse clinical environments, particularly with out-of-distribution (OOD) data, remains a challenge. Multi-center datasets like PolypGen have been developed to address these issues, but their collection is costly and time-consuming. Traditional data augmentation techniques provide limited variability, failing to capture the complexity of medical images. Diffusion models have emerged as a promising solution for generating synthetic polyp images, but the image generation process in current models mainly relies on segmentation masks as the condition, limiting their ability to capture the full clinical context. To overcome these limitations, we propose a Progressive Spectrum Diffusion Model (PSDM) that integrates diverse clinical annotations-such as segmentation masks, bounding boxes, and colonoscopy reports-by transforming them into compositional prompts. These prompts are organized into coarse and fine components, allowing the model to capture both broad spatial structures and fine details, generating clinically accurate synthetic images. By augmenting training data with PSDM-generated samples, our model significantly improves polyp detection, classification, and segmentation. For instance, on the PolypGen dataset, PSDM increases the F1 score by 2.12% and the mean average precision by 3.09%, demonstrating superior performance in OOD scenarios and enhanced generalization.

### Optimal Brain Apoptosis 
[[arxiv](https://arxiv.org/abs/2502.17941)] [[cool](https://papers.cool/arxiv/2502.17941)] [[pdf](https://arxiv.org/pdf/2502.17941)]
> **Authors**: Mingyuan Sun,Zheng Fang,Jiaxu Wang,Junjie Jiang,Delei Kong,Chenming Hu,Yuetong Fang,Renjing Xu
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Accepted to ICLR 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: The increasing complexity and parameter count of Convolutional Neural Networks (CNNs) and Transformers pose challenges in terms of computational efficiency and resource demands. Pruning has been identified as an effective strategy to address these challenges by removing redundant elements such as neurons, channels, or connections, thereby enhancing computational efficiency without heavily compromising performance. This paper builds on the foundational work of Optimal Brain Damage (OBD) by advancing the methodology of parameter importance estimation using the Hessian matrix. Unlike previous approaches that rely on approximations, we introduce Optimal Brain Apoptosis (OBA), a novel pruning method that calculates the Hessian-vector product value directly for each parameter. By decomposing the Hessian matrix across network layers and identifying conditions under which inter-layer Hessian submatrices are non-zero, we propose a highly efficient technique for computing the second-order Taylor expansion of parameters. This approach allows for a more precise pruning process, particularly in the context of CNNs and Transformers, as validated in our experiments including VGG19, ResNet32, ResNet50, and ViT-B/16 on CIFAR10, CIFAR100 and Imagenet datasets. Our code is available at https://github.com/NEU-REAL/OBA.

### BD Currency Detection: A CNN Based Approach with Mobile App Integration 
[[arxiv](https://arxiv.org/abs/2502.17907)] [[cool](https://papers.cool/arxiv/2502.17907)] [[pdf](https://arxiv.org/pdf/2502.17907)]
> **Authors**: Syed Jubayer Jaman,Md. Zahurul Haque,Md Robiul Islam,Usama Abdun Noor
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习,网络和互联网架构
- **Abstract**: Currency recognition plays a vital role in banking, commerce, and assistive technology for visually impaired individuals. Traditional methods, such as manual verification and optical scanning, often suffer from limitations in accuracy and efficiency. This study introduces an advanced currency recognition system utilizing Convolutional Neural Networks (CNNs) to accurately classify Bangladeshi banknotes. A dataset comprising 50,334 images was collected, preprocessed, and used to train a CNN model optimized for high performance classification. The trained model achieved an accuracy of 98.5%, surpassing conventional image based currency recognition approaches. To enable real time and offline functionality, the model was converted into TensorFlow Lite format and integrated into an Android mobile application. The results highlight the effectiveness of deep learning in currency recognition, providing a fast, secure, and accessible solution that enhances financial transactions and assistive technologies.

### From underwater to aerial: a novel multi-scale knowledge distillation approach for coral reef monitoring 
[[arxiv](https://arxiv.org/abs/2502.17883)] [[cool](https://papers.cool/arxiv/2502.17883)] [[pdf](https://arxiv.org/pdf/2502.17883)]
> **Authors**: Matteo Contini,Victor Illien,Julien Barde,Sylvain Poulain,Serge Bernard,Alexis Joly,Sylvain Bonhommeau
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Drone-based remote sensing combined with AI-driven methodologies has shown great potential for accurate mapping and monitoring of coral reef ecosystems. This study presents a novel multi-scale approach to coral reef monitoring, integrating fine-scale underwater imagery with medium-scale aerial imagery. Underwater images are captured using an Autonomous Surface Vehicle (ASV), while aerial images are acquired with an aerial drone. A transformer-based deep-learning model is trained on underwater images to detect the presence of 31 classes covering various coral morphotypes, associated fauna, and habitats. These predictions serve as annotations for training a second model applied to aerial images. The transfer of information across scales is achieved through a weighted footprint method that accounts for partial overlaps between underwater image footprints and aerial image tiles. The results show that the multi-scale methodology successfully extends fine-scale classification to larger reef areas, achieving a high degree of accuracy in predicting coral morphotypes and associated habitats. The method showed a strong alignment between underwater-derived annotations and ground truth data, reflected by an AUC (Area Under the Curve) score of 0.9251. This shows that the integration of underwater and aerial imagery, supported by deep-learning models, can facilitate scalable and accurate reef assessments. This study demonstrates the potential of combining multi-scale imaging and AI to facilitate the monitoring and conservation of coral reefs. Our approach leverages the strengths of underwater and aerial imagery, ensuring the precision of fine-scale analysis while extending it to cover a broader reef area.

### HRR: Hierarchical Retrospection Refinement for Generated Image Detection 
[[arxiv](https://arxiv.org/abs/2502.17862)] [[cool](https://papers.cool/arxiv/2502.17862)] [[pdf](https://arxiv.org/pdf/2502.17862)]
> **Authors**: Peipei Yuan,Zijing Xie,Shuo Ye,Hong Chen,Yulong Wang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Generative artificial intelligence holds significant potential for abuse, and generative image detection has become a key focus of research. However, existing methods primarily focused on detecting a specific generative model and emphasizing the localization of synthetic regions, while neglecting the interference caused by image size and style on model learning. Our goal is to reach a fundamental conclusion: Is the image real or generated? To this end, we propose a diffusion model-based generative image detection framework termed Hierarchical Retrospection Refinement~(HRR). It designs a multi-scale style retrospection module that encourages the model to generate detailed and realistic multi-scale representations, while alleviating the learning biases introduced by dataset styles and generative models. Additionally, based on the principle of correntropy sparse additive machine, a feature refinement module is designed to reduce the impact of redundant features on learning and capture the intrinsic structure and patterns of the data, thereby improving the model's generalization ability. Extensive experiments demonstrate the HRR framework consistently delivers significant performance improvements, outperforming state-of-the-art methods in generated image detection task.

### UniGS: Unified Language-Image-3D Pretraining with Gaussian Splatting 
[[arxiv](https://arxiv.org/abs/2502.17860)] [[cool](https://papers.cool/arxiv/2502.17860)] [[pdf](https://arxiv.org/pdf/2502.17860)]
> **Authors**: Haoyuan Li,Yanpeng Zhou,Tao Tang,Jifei Song,Yihan Zeng,Michael Kampffmeyer,Hang Xu,Xiaodan Liang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: ICLR 2025; Corrected citation of Uni3D;
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Recent advancements in multi-modal 3D pre-training methods have shown promising efficacy in learning joint representations of text, images, and point clouds. However, adopting point clouds as 3D representation fails to fully capture the intricacies of the 3D world and exhibits a noticeable gap between the discrete points and the dense 2D pixels of images. To tackle this issue, we propose UniGS, integrating 3D Gaussian Splatting (3DGS) into multi-modal pre-training to enhance the 3D representation. We first rely on the 3DGS representation to model the 3D world as a collection of 3D Gaussians with color and opacity, incorporating all the information of the 3D scene while establishing a strong connection with 2D images. Then, to achieve Language-Image-3D pertaining, UniGS starts with a pre-trained vision-language model to establish a shared visual and textual space through extensive real-world image-text pairs. Subsequently, UniGS employs a 3D encoder to align the optimized 3DGS with the Language-Image representations to learn unified multi-modal representations. To facilitate the extraction of global explicit 3D features by the 3D encoder and achieve better cross-modal alignment, we additionally introduce a novel Gaussian-Aware Guidance module that guides the learning of fine-grained representations of the 3D domain. Through extensive experiments across the Objaverse, ABO, MVImgNet and SUN RGBD datasets with zero-shot classification, text-driven retrieval and open-world understanding tasks, we demonstrate the effectiveness of UniGS in learning a more general and stronger aligned multi-modal representation. Specifically, UniGS achieves leading results across different 3D tasks with remarkable improvements over previous SOTA, Uni3D, including on zero-shot classification (+9.36%), text-driven retrieval (+4.3%) and open-world understanding (+7.92%).

## 数据结构和算法(cs.DS:Data Structures and Algorithms)

### Graph Inference with Effective Resistance Queries 
[[arxiv](https://arxiv.org/abs/2502.18350)] [[cool](https://papers.cool/arxiv/2502.18350)] [[pdf](https://arxiv.org/pdf/2502.18350)]
> **Authors**: Huck Bennett,Mitchell Black,Amir Nayyeri,Evelyn Warton
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 数据结构和算法,离散数学,机器学习
- **Abstract**: The goal of graph inference is to design algorithms for learning properties of a hidden graph using queries to an oracle that returns information about the graph. Graph reconstruction, verification, and property testing are all types of graph inference. In this work, we study graph inference using an oracle that returns the effective resistance (ER) between a pair of vertices. Effective resistance is a distance originating from the study of electrical circuits with many applications. However, ER has received little attention from a graph inference perspective. Indeed, although it is known that an $n$-vertex graph can be uniquely reconstructed from all $\binom{n}{2}$ possible ER queries, little else is known. We address this gap with several new results, including: 1. $O(n)$-query algorithms for testing whether a graph is a tree; deciding whether two graphs are equal assuming one is a subgraph of the other; and testing whether a given vertex (or edge) is a cut vertex (or cut edge). 2. Property testing algorithms, including for testing whether a graph is vertex- or edge-biconnected. We also give a reduction to adapt property testing results from the bounded-degree model to our ER query model. This yields ER-query-based algorithms for testing $k$-connectivity, bipartiteness, planarity, and containment of a fixed subgraph. 3. Graph reconstruction algorithms, including an algorithm for reconstructing a graph from a low-width tree decomposition; a $Θ(k^2)$-query, polynomial-time algorithm for recovering the adjacency matrix $A$ of a hidden graph, given $A$ with $k$ of its entries deleted; and a $k$-query, exponential-time algorithm for the same task. We also compare the power of ER queries and shortest path queries, which are closely related but better studied. Interestingly, we show that the two query models are incomparable in power.

## 计算机科学与博弈论(cs.GT:Computer Science and Game Theory)

### Expected Variational Inequalities 
[[arxiv](https://arxiv.org/abs/2502.18605)] [[cool](https://papers.cool/arxiv/2502.18605)] [[pdf](https://arxiv.org/pdf/2502.18605)]
> **Authors**: Brian Hu Zhang,Ioannis Anagnostides,Emanuel Tewolde,Ratip Emin Berker,Gabriele Farina,Vincent Conitzer,Tuomas Sandholm
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机科学与博弈论,机器学习,优化与控制
- **Abstract**: Variational inequalities (VIs) encompass many fundamental problems in diverse areas ranging from engineering to economics and machine learning. However, their considerable expressivity comes at the cost of computational intractability. In this paper, we introduce and analyze a natural relaxation -- which we refer to as expected variational inequalities (EVIs) -- where the goal is to find a distribution that satisfies the VI constraint in expectation. By adapting recent techniques from game theory, we show that, unlike VIs, EVIs can be solved in polynomial time under general (nonmonotone) operators. EVIs capture the seminal notion of correlated equilibria, but enjoy a greater reach beyond games. We also employ our framework to capture and generalize several existing disparate results, including from settings such as smooth games, and games with coupled constraints or nonconcave utilities.

## 人机交互(cs.HC:Human-Computer Interaction)

### Which Contributions Deserve Credit? Perceptions of Attribution in Human-AI Co-Creation 
[[arxiv](https://arxiv.org/abs/2502.18357)] [[cool](https://papers.cool/arxiv/2502.18357)] [[pdf](https://arxiv.org/pdf/2502.18357)]
> **Authors**: Jessica He,Stephanie Houde,Justin D. Weisz
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 30 pages, 5 figures. In CHI Conference on Human Factors in Computing Systems (CHI '25), April 26-May 1, 2025, Yokohama, Japan
- **标题**: None
- **领域**: 人机交互,人工智能,计算机与社会
- **Abstract**: AI systems powered by large language models can act as capable assistants for writing and editing. In these tasks, the AI system acts as a co-creative partner, making novel contributions to an artifact-under-creation alongside its human partner(s). One question that arises in these scenarios is the extent to which AI should be credited for its contributions. We examined knowledge workers' views of attribution through a survey study (N=155) and found that they assigned different levels of credit across different contribution types, amounts, and initiative. Compared to a human partner, we observed a consistent pattern in which AI was assigned less credit for equivalent contributions. Participants felt that disclosing AI involvement was important and used a variety of criteria to make attribution judgments, including the quality of contributions, personal values, and technology considerations. Our results motivate and inform new approaches for crediting AI contributions to co-created work.

### FactFlow: Automatic Fact Sheet Generation and Customization from Tabular Dataset via AI Chain Design & Implementation 
[[arxiv](https://arxiv.org/abs/2502.17909)] [[cool](https://papers.cool/arxiv/2502.17909)] [[pdf](https://arxiv.org/pdf/2502.17909)]
> **Authors**: Minh Duc Vu,Jieshan Chen,Zhenchang Xing,Qinghua Lu,Xiwei Xu,Qian Fu
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 11 pages, 6 figures
- **标题**: None
- **领域**: 人机交互,人工智能
- **Abstract**: With the proliferation of data across various domains, there is a critical demand for tools that enable non-experts to derive meaningful insights without deep data analysis skills. To address this need, existing automatic fact sheet generation tools offer heuristic-based solutions to extract facts and generate stories. However, they inadequately grasp the semantics of data and struggle to generate narratives that fully capture the semantics of the dataset or align the fact sheet with specific user needs. Addressing these shortcomings, this paper introduces \tool, a novel tool designed for the automatic generation and customisation of fact sheets. \tool applies the concept of collaborative AI workers to transform raw tabular dataset into comprehensive, visually compelling fact sheets. We define effective taxonomy to profile AI worker for specialised tasks. Furthermore, \tool empowers users to refine these fact sheets through intuitive natural language commands, ensuring the final outputs align closely with individual preferences and requirements. Our user evaluation with 18 participants confirms that \tool not only surpasses state-of-the-art baselines in automated fact sheet production but also provides a positive user experience during customization tasks.

### VeriPlan: Integrating Formal Verification and LLMs into End-User Planning 
[[arxiv](https://arxiv.org/abs/2502.17898)] [[cool](https://papers.cool/arxiv/2502.17898)] [[pdf](https://arxiv.org/pdf/2502.17898)]
> **Authors**: Christine Lee,David Porfirio,Xinyu Jessica Wang,Kevin Zhao,Bilge Mutlu
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: In CHI Conference on Human Factors in Computing Systems (CHI '25), April 26-May 1, 2025, Yokohama, Japan. ACM, New York, NY, USA, 19 pages
- **标题**: None
- **领域**: 人机交互,人工智能
- **Abstract**: Automated planning is traditionally the domain of experts, utilized in fields like manufacturing and healthcare with the aid of expert planning tools. Recent advancements in LLMs have made planning more accessible to everyday users due to their potential to assist users with complex planning tasks. However, LLMs face several application challenges within end-user planning, including consistency, accuracy, and user trust issues. This paper introduces VeriPlan, a system that applies formal verification techniques, specifically model checking, to enhance the reliability and flexibility of LLMs for end-user planning. In addition to the LLM planner, VeriPlan includes three additional core features -- a rule translator, flexibility sliders, and a model checker -- that engage users in the verification process. Through a user study (n=12), we evaluate VeriPlan, demonstrating improvements in the perceived quality, usability, and user satisfaction of LLMs. Our work shows the effective integration of formal verification and user-control features with LLMs for end-user planning tasks.

## 信息检索(cs.IR:Information Retrieval)

### Rank1: Test-Time Compute for Reranking in Information Retrieval 
[[arxiv](https://arxiv.org/abs/2502.18418)] [[cool](https://papers.cool/arxiv/2502.18418)] [[pdf](https://arxiv.org/pdf/2502.18418)]
> **Authors**: Orion Weller,Kathryn Ricci,Eugene Yang,Andrew Yates,Dawn Lawrie,Benjamin Van Durme
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,计算语言学,机器学习
- **Abstract**: We introduce Rank1, the first reranking model trained to take advantage of test-time compute. Rank1 demonstrates the applicability within retrieval of using a reasoning language model (i.e. OpenAI's o1, Deepseek's R1, etc.) for distillation in order to rapidly improve the performance of a smaller model. We gather and open-source a dataset of more than 600,000 examples of R1 reasoning traces from queries and passages in MS MARCO. Models trained on this dataset show: (1) state-of-the-art performance on advanced reasoning and instruction following datasets; (2) work remarkably well out of distribution due to the ability to respond to user-input prompts; and (3) have explainable reasoning chains that can be given to users or RAG-based systems. Further, we demonstrate that quantized versions of these models retain strong performance while using less compute/memory. Overall, Rank1 shows that test-time compute allows for a fundamentally new type of explainable and performant reranker model for search.

### Neural Network Graph Similarity Computation Based on Graph Fusion 
[[arxiv](https://arxiv.org/abs/2502.18291)] [[cool](https://papers.cool/arxiv/2502.18291)] [[pdf](https://arxiv.org/pdf/2502.18291)]
> **Authors**: Zenghui Chang,Yiqiao Zhang,Hong Cai Chen
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 9 pages, 4 figures, 4 tables
- **标题**: None
- **领域**: 信息检索,机器学习
- **Abstract**: Graph similarity learning, crucial for tasks such as graph classification and similarity search, focuses on measuring the similarity between two graph-structured entities. The core challenge in this field is effectively managing the interactions between graphs. Traditional methods often entail separate, redundant computations for each graph pair, leading to unnecessary complexity. This paper revolutionizes the approach by introducing a parallel graph interaction method called graph fusion. By merging the node sequences of graph pairs into a single large graph, our method leverages a global attention mechanism to facilitate interaction computations and to harvest cross-graph insights. We further assess the similarity between graph pairs at two distinct levels-graph-level and node-level-introducing two innovative, yet straightforward, similarity computation algorithms. Extensive testing across five public datasets shows that our model not only outperforms leading baseline models in graph-to-graph classification and regression tasks but also sets a new benchmark for performance and efficiency. The code for this paper is open-source and available at https://github.com/LLiRarry/GFM-code.git

## 机器学习(cs.LG:Machine Learning)

### Tighten The Lasso: A Convex Hull Volume-based Anomaly Detection Method 
[[arxiv](https://arxiv.org/abs/2502.18601)] [[cool](https://papers.cool/arxiv/2502.18601)] [[pdf](https://arxiv.org/pdf/2502.18601)]
> **Authors**: Uri Itai,Asael Bar Ilan,Teddy Lazebnik
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The rapid advancements in data-driven methodologies have underscored the critical importance of ensuring data quality. Consequently, detecting out-of-distribution (OOD) data has emerged as an essential task to maintain the reliability and robustness of data-driven models, in general, and machine and deep learning models, in particular. In this study, we leveraged the convex hull property of a dataset and the fact that anomalies highly contribute to the increase of the CH's volume to propose a novel anomaly detection algorithm. Our algorithm computes the CH's volume as an increasing number of data points are removed from the dataset to define a decision line between OOD and in-distribution data points. We compared the proposed algorithm to seven widely used anomaly detection algorithms over ten datasets, showing comparable results for state-of-the-art (SOTA) algorithms. Moreover, we show that with a computationally cheap and simple check, one can detect datasets that are well-suited for the proposed algorithm which outperforms the SOTA anomaly detection algorithms.

### Error-related Potential driven Reinforcement Learning for adaptive Brain-Computer Interfaces 
[[arxiv](https://arxiv.org/abs/2502.18594)] [[cool](https://papers.cool/arxiv/2502.18594)] [[pdf](https://arxiv.org/pdf/2502.18594)]
> **Authors**: Aline Xavier Fidêncio,Felix Grün,Christian Klaes,Ioannis Iossifidis
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: :F.2.0; I.2.6; I.6.6; I.2.m
- **标题**: None
- **领域**: 机器学习,人机交互,神经元和认知
- **Abstract**: Brain-computer interfaces (BCIs) provide alternative communication methods for individuals with motor disabilities by allowing control and interaction with external devices. Non-invasive BCIs, especially those using electroencephalography (EEG), are practical and safe for various applications. However, their performance is often hindered by EEG non-stationarities, caused by changing mental states or device characteristics like electrode impedance. This challenge has spurred research into adaptive BCIs that can handle such variations. In recent years, interest has grown in using error-related potentials (ErrPs) to enhance BCI performance. ErrPs, neural responses to errors, can be detected non-invasively and have been integrated into different BCI paradigms to improve performance through error correction or adaptation. This research introduces a novel adaptive ErrP-based BCI approach using reinforcement learning (RL). We demonstrate the feasibility of an RL-driven adaptive framework incorporating ErrPs and motor imagery. Utilizing two RL agents, the framework adapts dynamically to EEG non-stationarities. Validation was conducted using a publicly available motor imagery dataset and a fast-paced game designed to boost user engagement. Results show the framework's promise, with RL agents learning control policies from user interactions and achieving robust performance across datasets. However, a critical insight from the game-based protocol revealed that motor imagery in a high-speed interaction paradigm was largely ineffective for participants, highlighting task design limitations in real-time BCI applications. These findings underscore the potential of RL for adaptive BCIs while pointing out practical constraints related to task complexity and user responsiveness.

### Transported Memory Networks accelerating Computational Fluid Dynamics 
[[arxiv](https://arxiv.org/abs/2502.18591)] [[cool](https://papers.cool/arxiv/2502.18591)] [[pdf](https://arxiv.org/pdf/2502.18591)]
> **Authors**: Matthias Schulz,Gwendal Jouan,Daniel Berger,Stefan Gavranovic,Dirk Hartmann
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,流体动力学
- **Abstract**: In recent years, augmentation of differentiable PDE solvers with neural networks has shown promising results, particularly in fluid simulations. However, most approaches rely on convolutional neural networks and custom solvers operating on Cartesian grids with efficient access to cell data. This particular choice poses challenges for industrial-grade solvers that operate on unstructured meshes, where access is restricted to neighboring cells only. In this work, we address this limitation using a novel architecture, named Transported Memory Networks. The architecture draws inspiration from both traditional turbulence models and recurrent neural networks, and it is fully compatible with generic discretizations. Our results show that it is point-wise and statistically comparable to, or improves upon, previous methods in terms of both accuracy and computational efficiency.

### Differentially Private Iterative Screening Rules for Linear Regression 
[[arxiv](https://arxiv.org/abs/2502.18578)] [[cool](https://papers.cool/arxiv/2502.18578)] [[pdf](https://arxiv.org/pdf/2502.18578)]
> **Authors**: Amol Khanna,Fred Lu,Edward Raff
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Proceedings of the 15th ACM Conference on Data and Application Security and Privacy
- **标题**: None
- **领域**: 机器学习,人工智能,密码学和安全
- **Abstract**: Linear $L_1$-regularized models have remained one of the simplest and most effective tools in data science. Over the past decade, screening rules have risen in popularity as a way to eliminate features when producing the sparse regression weights of $L_1$ models. However, despite the increasing need of privacy-preserving models for data analysis, to the best of our knowledge, no differentially private screening rule exists. In this paper, we develop the first private screening rule for linear regression. We initially find that this screening rule is too strong: it screens too many coefficients as a result of the private screening step. However, a weakened implementation of private screening reduces overscreening and improves performance.

### Target Defense with Multiple Defenders and an Agile Attacker via Residual Policy Learning 
[[arxiv](https://arxiv.org/abs/2502.18549)] [[cool](https://papers.cool/arxiv/2502.18549)] [[pdf](https://arxiv.org/pdf/2502.18549)]
> **Authors**: Jiyue Tao,Tongsheng Shen,Dexin Zhao,Feitian Zhang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,密码学和安全
- **Abstract**: The target defense problem involves intercepting an attacker before it reaches a designated target region using one or more defenders. This letter focuses on a particularly challenging scenario in which the attacker is more agile than the defenders, significantly increasing the difficulty of effective interception. To address this challenge, we propose a novel residual policy framework that integrates deep reinforcement learning (DRL) with the force-based Boids model. In this framework, the Boids model serves as a baseline policy, while DRL learns a residual policy to refine and optimize the defenders' actions. Simulation experiments demonstrate that the proposed method consistently outperforms traditional interception policies, whether learned via vanilla DRL or fine-tuned from force-based methods. Moreover, the learned policy exhibits strong scalability and adaptability, effectively handling scenarios with varying numbers of defenders and attackers with different agility levels.

### What is the Alignment Objective of GRPO? 
[[arxiv](https://arxiv.org/abs/2502.18548)] [[cool](https://papers.cool/arxiv/2502.18548)] [[pdf](https://arxiv.org/pdf/2502.18548)]
> **Authors**: Milan Vojnovic,Se-Young Yun
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: In this note, we examine the aggregation of preferences achieved by the Group Policy Optimisation (GRPO) algorithm, a reinforcement learning method used to train advanced artificial intelligence models such as DeepSeek-R1-Zero and DeepSeekMath. The GRPO algorithm trains a policy using a reward preference model, which is computed by sampling a set of outputs for a given context, observing the corresponding rewards, and applying shift-and-scale normalisation to these reward values. Additionally, it incorporates a penalty function to discourage deviations from a reference policy. We present a framework that enables us to characterise the stationary policies of the GRPO algorithm. This analysis reveals that the aggregation of preferences differs fundamentally from standard logarithmic pooling, which is implemented by other approaches such as RLHF. The precise form of preference aggregation arises from the way the reward preference model is defined and from the penalty function, which we show to essentially correspond to the reverse Kullback-Leibler (KL) divergence between the aggregation policy and the reference policy. Interestingly, we demonstrate that for groups of size two, the reward preference model corresponds to pairwise comparison preferences, similar to those in other alignment methods based on pairwise comparison feedback. We provide explicit characterisations of the aggregate preference for binary questions, for groups of size two, and in the limit of large group size. This provides insights into the dependence of the aggregate preference on parameters such as the regularisation constant and the confidence margin of question answers. Finally, we discuss the aggregation of preferences obtained by modifying the GRPO algorithm to use direct KL divergence as the penalty or to use rewards without scale normalisation.

### Revisiting Convolution Architecture in the Realm of DNA Foundation Models 
[[arxiv](https://arxiv.org/abs/2502.18538)] [[cool](https://papers.cool/arxiv/2502.18538)] [[pdf](https://arxiv.org/pdf/2502.18538)]
> **Authors**: Yu Bo,Weian Mao,Yanjun Shao,Weiqiang Bai,Peng Ye,Xinzhu Ma,Junbo Zhao,Hao Chen,Chunhua Shen
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: In recent years, a variety of methods based on Transformer and state space model (SSM) architectures have been proposed, advancing foundational DNA language models. However, there is a lack of comparison between these recent approaches and the classical architecture convolutional networks (CNNs) on foundation model benchmarks. This raises the question: are CNNs truly being surpassed by these recent approaches based on transformer and SSM architectures? In this paper, we develop a simple but well-designed CNN-based method termed ConvNova. ConvNova identifies and proposes three effective designs: 1) dilated convolutions, 2) gated convolutions, and 3) a dual-branch framework for gating mechanisms. Through extensive empirical experiments, we demonstrate that ConvNova significantly outperforms recent methods on more than half of the tasks across several foundation model benchmarks. For example, in histone-related tasks, ConvNova exceeds the second-best method by an average of 5.8%, while generally utilizing fewer parameters and enabling faster computation. In addition, the experiments observed findings that may be related to biological characteristics. This indicates that CNNs are still a strong competitor compared to Transformers and SSMs. We anticipate that this work will spark renewed interest in CNN-based methods for DNA foundation models.

### Reinforcement Learning-based Approach for Vehicle-to-Building Charging with Heterogeneous Agents and Long Term Rewards 
[[arxiv](https://arxiv.org/abs/2502.18526)] [[cool](https://papers.cool/arxiv/2502.18526)] [[pdf](https://arxiv.org/pdf/2502.18526)]
> **Authors**: Fangqi Liu,Rishav Sen,Jose Paolo Talusan,Ava Pettet,Aaron Kandel,Yoshinori Suzue,Ayan Mukhopadhyay,Abhishek Dubey
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Strategic aggregation of electric vehicle batteries as energy reservoirs can optimize power grid demand, benefiting smart and connected communities, especially large office buildings that offer workplace charging. This involves optimizing charging and discharging to reduce peak energy costs and net peak demand, monitored over extended periods (e.g., a month), which involves making sequential decisions under uncertainty and delayed and sparse rewards, a continuous action space, and the complexity of ensuring generalization across diverse conditions. Existing algorithmic approaches, e.g., heuristic-based strategies, fall short in addressing real-time decision-making under dynamic conditions, and traditional reinforcement learning (RL) models struggle with large state-action spaces, multi-agent settings, and the need for long-term reward optimization. To address these challenges, we introduce a novel RL framework that combines the Deep Deterministic Policy Gradient approach (DDPG) with action masking and efficient MILP-driven policy guidance. Our approach balances the exploration of continuous action spaces to meet user charging demands. Using real-world data from a major electric vehicle manufacturer, we show that our approach comprehensively outperforms many well-established baselines and several scalable heuristic approaches, achieving significant cost savings while meeting all charging requirements. Our results show that the proposed approach is one of the first scalable and general approaches to solving the V2B energy management challenge.

### Comparative Analysis of MDL-VAE vs. Standard VAE on 202 Years of Gynecological Data 
[[arxiv](https://arxiv.org/abs/2502.18412)] [[cool](https://papers.cool/arxiv/2502.18412)] [[pdf](https://arxiv.org/pdf/2502.18412)]
> **Authors**: Paula Santos
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 12 pagas, 5 figures, 9th International Conference on Signal, Image Processing (SIPO 2025), Vancouver CA
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: This study presents a comparative evaluation of a Variational Autoencoder (VAE) enhanced with Minimum Description Length (MDL) regularization against a Standard Autoencoder for reconstructing high-dimensional gynecological data. The MDL-VAE exhibits significantly lower reconstruction errors (MSE, MAE, RMSE) and more structured latent representations, driven by effective KL divergence regularization. Statistical analyses confirm these performance improvements are significant. Furthermore, the MDL-VAE shows consistent training and validation losses and achieves efficient inference times, underscoring its robustness and practical viability. Our findings suggest that incorporating MDL principles into VAE architectures can substantially improve data reconstruction and generalization, making it a promising approach for advanced applications in healthcare data modeling and analysis.

### TSKANMixer: Kolmogorov-Arnold Networks with MLP-Mixer Model for Time Series Forecasting 
[[arxiv](https://arxiv.org/abs/2502.18410)] [[cool](https://papers.cool/arxiv/2502.18410)] [[pdf](https://arxiv.org/pdf/2502.18410)]
> **Authors**: Young-Chae Hong,Bei Xiao,Yangho Chen
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 8 pages, 4 figures, 7 tables and accepted at the AI4TS:AIfor Time Series Analysis workshop, AAAI 2025
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Time series forecasting has long been a focus of research across diverse fields, including economics, energy, healthcare, and traffic management. Recent works have introduced innovative architectures for time series models, such as the Time-Series Mixer (TSMixer), which leverages multi-layer perceptrons (MLPs) to enhance prediction accuracy by effectively capturing both spatial and temporal dependencies within the data. In this paper, we investigate the capabilities of the Kolmogorov-Arnold Networks (KANs) for time-series forecasting by modifying TSMixer with a KAN layer (TSKANMixer). Experimental results demonstrate that TSKANMixer tends to improve prediction accuracy over the original TSMixer across multiple datasets, ranking among the top-performing models compared to other time series approaches. Our results show that the KANs are promising alternatives to improve the performance of time series forecasting by replacing or extending traditional MLPs.

### Enhancing DNA Foundation Models to Address Masking Inefficiencies 
[[arxiv](https://arxiv.org/abs/2502.18405)] [[cool](https://papers.cool/arxiv/2502.18405)] [[pdf](https://arxiv.org/pdf/2502.18405)]
> **Authors**: Monireh Safari,Pablo Millan Arias,Scott C. Lowe,Lila Kari,Angel X. Chang,Graham W. Taylor
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 10 pages, 5 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Masked language modelling (MLM) as a pretraining objective has been widely adopted in genomic sequence modelling. While pretrained models can successfully serve as encoders for various downstream tasks, the distribution shift between pretraining and inference detrimentally impacts performance, as the pretraining task is to map [MASK] tokens to predictions, yet the [MASK] is absent during downstream applications. This means the encoder does not prioritize its encodings of non-[MASK] tokens, and expends parameters and compute on work only relevant to the MLM task, despite this being irrelevant at deployment time. In this work, we propose a modified encoder-decoder architecture based on the masked autoencoder framework, designed to address this inefficiency within a BERT-based transformer. We empirically show that the resulting mismatch is particularly detrimental in genomic pipelines where models are often used for feature extraction without fine-tuning. We evaluate our approach on the BIOSCAN-5M dataset, comprising over 2 million unique DNA barcodes. We achieve substantial performance gains in both closed-world and open-world classification tasks when compared against causal models and bidirectional architectures pretrained with MLM tasks.

### The FFT Strikes Back: An Efficient Alternative to Self-Attention 
[[arxiv](https://arxiv.org/abs/2502.18394)] [[cool](https://papers.cool/arxiv/2502.18394)] [[pdf](https://arxiv.org/pdf/2502.18394)]
> **Authors**: Jacob Fein-Ashley
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Conventional self-attention mechanisms incur quadratic complexity, limiting their scalability on long sequences. We introduce FFTNet, an adaptive spectral filtering framework that leverages the Fast Fourier Transform (FFT) to achieve global token mixing in $\mathcal{O}(n\log n)$ time. By transforming inputs into the frequency domain, FFTNet exploits the orthogonality and energy preservation guaranteed by Parseval's theorem to capture long-range dependencies efficiently. A learnable spectral filter and modReLU activation dynamically emphasize salient frequency components, providing a rigorous and adaptive alternative to traditional self-attention. Experiments on the Long Range Arena and ImageNet benchmarks validate our theoretical insights and demonstrate superior performance over fixed Fourier and standard attention models.

### Mechanistic PDE Networks for Discovery of Governing Equations 
[[arxiv](https://arxiv.org/abs/2502.18377)] [[cool](https://papers.cool/arxiv/2502.18377)] [[pdf](https://arxiv.org/pdf/2502.18377)]
> **Authors**: Adeel Pervez,Efstratios Gavves,Francesco Locatello
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We present Mechanistic PDE Networks -- a model for discovery of governing partial differential equations from data. Mechanistic PDE Networks represent spatiotemporal data as space-time dependent linear partial differential equations in neural network hidden representations. The represented PDEs are then solved and decoded for specific tasks. The learned PDE representations naturally express the spatiotemporal dynamics in data in neural network hidden space, enabling increased power for dynamical modeling. Solving the PDE representations in a compute and memory-efficient way, however, is a significant challenge. We develop a native, GPU-capable, parallel, sparse, and differentiable multigrid solver specialized for linear partial differential equations that acts as a module in Mechanistic PDE Networks. Leveraging the PDE solver, we propose a discovery architecture that can discover nonlinear PDEs in complex settings while also being robust to noise. We validate PDE discovery on a number of PDEs, including reaction-diffusion and Navier-Stokes equations.

### WebGames: Challenging General-Purpose Web-Browsing AI Agents 
[[arxiv](https://arxiv.org/abs/2502.18356)] [[cool](https://papers.cool/arxiv/2502.18356)] [[pdf](https://arxiv.org/pdf/2502.18356)]
> **Authors**: George Thomas,Alex J. Chan,Jikun Kang,Wenqi Wu,Filippos Christianos,Fraser Greenlee,Andy Toulis,Marvin Purtorab
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We introduce WebGames, a comprehensive benchmark suite designed to evaluate general-purpose web-browsing AI agents through a collection of 50+ interactive challenges. These challenges are specifically crafted to be straightforward for humans while systematically testing the limitations of current AI systems across fundamental browser interactions, advanced input processing, cognitive tasks, workflow automation, and interactive entertainment. Our framework eliminates external dependencies through a hermetic testing environment, ensuring reproducible evaluation with verifiable ground-truth solutions. We evaluate leading vision-language models including GPT-4o, Claude Computer-Use, Gemini-1.5-Pro, and Qwen2-VL against human performance. Results reveal a substantial capability gap, with the best AI system achieving only 43.1% success rate compared to human performance of 95.7%, highlighting fundamental limitations in current AI systems' ability to handle common web interaction patterns that humans find intuitive. The benchmark is publicly available at webgames.convergence.ai, offering a lightweight, client-side implementation that facilitates rapid evaluation cycles. Through its modular architecture and standardized challenge specifications, WebGames provides a robust foundation for measuring progress in development of more capable web-browsing agents.

### Structural Alignment Improves Graph Test-Time Adaptation 
[[arxiv](https://arxiv.org/abs/2502.18334)] [[cool](https://papers.cool/arxiv/2502.18334)] [[pdf](https://arxiv.org/pdf/2502.18334)]
> **Authors**: Hans Hao-Hsun Hsu,Shikun Liu,Han Zhao,Pan Li
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Graph-based learning has achieved remarkable success in domains ranging from recommendation to fraud detection and particle physics by effectively capturing underlying interaction patterns. However, it often struggles to generalize when distribution shifts occur, particularly those involving changes in network connectivity or interaction patterns. Existing approaches designed to mitigate such shifts typically require retraining with full access to source data, rendering them infeasible under strict computational or privacy constraints. To address this limitation, we propose a test-time structural alignment (TSA) algorithm for Graph Test-Time Adaptation (GTTA), a novel method that aligns graph structures during inference without revisiting the source domain. Built upon a theoretically grounded treatment of graph data distribution shifts, TSA integrates three key strategies: an uncertainty-aware neighborhood weighting that accommodates structure shifts, an adaptive balancing of self-node and neighborhood-aggregated representations driven by node representations' signal-to-noise ratio, and a decision boundary refinement that corrects remaining label and feature shifts. Extensive experiments on synthetic and real-world datasets demonstrate that TSA can consistently outperform both non-graph TTA methods and state-of-the-art GTTA baselines.

### Pretraining Frequency Predicts Compositional Generalization of CLIP on Real-World Tasks 
[[arxiv](https://arxiv.org/abs/2502.18326)] [[cool](https://papers.cool/arxiv/2502.18326)] [[pdf](https://arxiv.org/pdf/2502.18326)]
> **Authors**: Thaddäus Wiedemer,Yash Sharma,Ameya Prabhu,Matthias Bethge,Wieland Brendel
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-26
> **comment**: NeurIPS 2024 Workshop on CompositionalLearning: Perspectives, Methods, and Paths Forward
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We investigate the success conditions for compositional generalization of CLIP models on real-world data through performance prediction. Prior work shows that CLIP requires exponentially more pretraining data for linear performance gains on individual concepts. This sample-inefficient scaling could be mitigated if CLIP systematically understood new inputs as compositions of learned components, allowing rare observation to be mapped to common concepts. To explore CLIP's compositional generalization ability, we filter retrieval corpora for samples with object combinations not present in the pretraining corpus. We show that CLIP's performance on these samples can be accurately predicted from the pretraining frequencies of individual objects. Our findings demonstrate that CLIP learns to disentangle objects observed in its pretraining data and can recompose them straightforwardly. Additionally, we are the first to show how this ability scales with pretraining data. For data curation in practice, our results suggest that balancing object occurrences improves generalization, which should benefit CLIP's efficiency and accuracy without scaling data volume.

### Accelerated Training on Low-Power Edge Devices 
[[arxiv](https://arxiv.org/abs/2502.18323)] [[cool](https://papers.cool/arxiv/2502.18323)] [[pdf](https://arxiv.org/pdf/2502.18323)]
> **Authors**: Mohamed Aboelenien Ahmed,Kilian Pfeiffer,Heba Khdr,Osama Abboud,Ramin Khalili,Jörg Henkel
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,操作系统
- **Abstract**: Training on edge devices poses several challenges as these devices are generally resource-constrained, especially in terms of power. State-of-the-art techniques at the device level reduce the GPU frequency to enforce power constraints, leading to a significant increase in training time. To accelerate training, we propose to jointly adjust the system and application parameters (in our case, the GPU frequency and the batch size of the training task) while adhering to the power constraints on devices. We introduce a novel cross-layer methodology that combines predictions of batch size efficiency and device profiling to achieve the desired optimization. Our evaluation on real hardware shows that our method outperforms the current baselines that depend on state of the art techniques, reducing the training time by $2.4\times$ with results very close to optimal. Our measurements also indicate a substantial reduction in the overall energy used for the training process. These gains are achieved without reduction in the performance of the trained model.

### Global-Decision-Focused Neural ODEs for Proactive Grid Resilience Management 
[[arxiv](https://arxiv.org/abs/2502.18321)] [[cool](https://papers.cool/arxiv/2502.18321)] [[pdf](https://arxiv.org/pdf/2502.18321)]
> **Authors**: Shuyi Chen,Ferdinando Fioretto,Feng Qiu,Shixiang Zhu
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Extreme hazard events such as wildfires and hurricanes increasingly threaten power systems, causing widespread outages and disrupting critical services. Recently, predict-then-optimize approaches have gained traction in grid operations, where system functionality forecasts are first generated and then used as inputs for downstream decision-making. However, this two-stage method often results in a misalignment between prediction and optimization objectives, leading to suboptimal resource allocation. To address this, we propose predict-all-then-optimize-globally (PATOG), a framework that integrates outage prediction with globally optimized interventions. At its core, our global-decision-focused (GDF) neural ODE model captures outage dynamics while optimizing resilience strategies in a decision-aware manner. Unlike conventional methods, our approach ensures spatially and temporally coherent decision-making, improving both predictive accuracy and operational efficiency. Experiments on synthetic and real-world datasets demonstrate significant improvements in outage prediction consistency and grid resilience.

### Bayesian Computation in Deep Learning 
[[arxiv](https://arxiv.org/abs/2502.18300)] [[cool](https://papers.cool/arxiv/2502.18300)] [[pdf](https://arxiv.org/pdf/2502.18300)]
> **Authors**: Wenlong Chen,Bolian Li,Ruqi Zhang,Yingzhen Li
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 43 pages, 7 figures
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: This review paper is intended for the 2nd edition of the Handbook of Markov chain Monte Carlo. We provide an introduction to approximate inference techniques as Bayesian computation methods applied to deep learning models. We organize the chapter by presenting popular computational methods for Bayesian neural networks and deep generative models, explaining their unique challenges in posterior inference as well as the solutions.

### DeepCircuitX: A Comprehensive Repository-Level Dataset for RTL Code Understanding, Generation, and PPA Analysis 
[[arxiv](https://arxiv.org/abs/2502.18297)] [[cool](https://papers.cool/arxiv/2502.18297)] [[pdf](https://arxiv.org/pdf/2502.18297)]
> **Authors**: Zeju Li,Changran Xu,Zhengyuan Shi,Zedong Peng,Yi Liu,Yunhao Zhou,Lingfeng Zhou,Chengyu Ma,Jianyuan Zhong,Xi Wang,Jieru Zhao,Zhufei Chu,Xiaoyan Yang,Qiang Xu
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 8 pages, 3 figures
- **标题**: None
- **领域**: 机器学习,编程语言
- **Abstract**: This paper introduces DeepCircuitX, a comprehensive repository-level dataset designed to advance RTL (Register Transfer Level) code understanding, generation, and power-performance-area (PPA) analysis. Unlike existing datasets that are limited to either file-level RTL code or physical layout data, DeepCircuitX provides a holistic, multilevel resource that spans repository, file, module, and block-level RTL code. This structure enables more nuanced training and evaluation of large language models (LLMs) for RTL-specific tasks. DeepCircuitX is enriched with Chain of Thought (CoT) annotations, offering detailed descriptions of functionality and structure at multiple levels. These annotations enhance its utility for a wide range of tasks, including RTL code understanding, generation, and completion. Additionally, the dataset includes synthesized netlists and PPA metrics, facilitating early-stage design exploration and enabling accurate PPA prediction directly from RTL code. We demonstrate the dataset's effectiveness on various LLMs finetuned with our dataset and confirm the quality with human evaluations. Our results highlight DeepCircuitX as a critical resource for advancing RTL-focused machine learning applications in hardware design automation.Our data is available at https://zeju.gitbook.io/lcm-team.

### AMPO: Active Multi-Preference Optimization 
[[arxiv](https://arxiv.org/abs/2502.18293)] [[cool](https://papers.cool/arxiv/2502.18293)] [[pdf](https://arxiv.org/pdf/2502.18293)]
> **Authors**: Taneesh Gupta,Rahul Madhavan,Xuchao Zhang,Chetan Bansal,Saravan Rajmohan
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Multi-preference optimization enriches language-model alignment beyond pairwise preferences by contrasting entire sets of helpful and undesired responses, thereby enabling richer training signals for large language models. During self-play alignment, these models often produce numerous candidate answers per query, rendering it computationally infeasible to include all responses in the training objective. In this work, we propose $\textit{Active Multi-Preference Optimization}$ (AMPO), a novel approach that combines on-policy generation, a multi-preference group-contrastive loss, and active subset selection. Specifically, we score and embed large candidate pools of responses and then select a small, yet informative, subset that covers reward extremes and distinct semantic clusters for preference optimization. Our contrastive training scheme is capable of identifying not only the best and worst answers but also subtle, underexplored modes that are crucial for robust alignment. Theoretically, we provide guarantees for expected reward maximization using our active selection method, and empirically, AMPO achieves state-of-the-art results on $\textit{AlpacaEval}$ using Llama 8B.

### Late Breaking Results: The Art of Beating the Odds with Predictor-Guided Random Design Space Exploration 
[[arxiv](https://arxiv.org/abs/2502.17936)] [[cool](https://papers.cool/arxiv/2502.17936)] [[pdf](https://arxiv.org/pdf/2502.17936)]
> **Authors**: Felix Arnold,Maxence Bouvier,Ryan Amaudruz,Renzo Andri,Lukas Cavigelli
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 2 pages, 3 figures, conference, this research manuscript is currently under review for publication in an IEEE conference
- **标题**: None
- **领域**: 机器学习,硬件架构
- **Abstract**: This work introduces an innovative method for improving combinational digital circuits through random exploration in MIG-based synthesis. High-quality circuits are crucial for performance, power, and cost, making this a critical area of active research. Our approach incorporates next-state prediction and iterative selection, significantly accelerating the synthesis process. This novel method achieves up to 14x synthesis speedup and up to 20.94% better MIG minimization on the EPFL Combinational Benchmark Suite compared to state-of-the-art techniques. We further explore various predictor models and show that increased prediction accuracy does not guarantee an equivalent increase in synthesis quality of results or speedup, observing that randomness remains a desirable factor.

### Integrating Boosted learning with Differential Evolution (DE) Optimizer: A Prediction of Groundwater Quality Risk Assessment in Odisha 
[[arxiv](https://arxiv.org/abs/2502.17929)] [[cool](https://papers.cool/arxiv/2502.17929)] [[pdf](https://arxiv.org/pdf/2502.17929)]
> **Authors**: Sonalika Subudhi,Alok Kumar Pati,Sephali Bose,Subhasmita Sahoo,Avipsa Pattanaik,Biswa Mohan Acharya
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 9 Figures (8 figs in paper and one additional graphical abstract), 9 Tables
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Groundwater is eventually undermined by human exercises, such as fast industrialization, urbanization, over-extraction, and contamination from agrarian and urban sources. From among the different contaminants, the presence of heavy metals like cadmium (Cd), chromium (Cr), arsenic (As), and lead (Pb) proves to have serious dangers when present in huge concentrations in groundwater. Long-term usage of these poisonous components may lead to neurological disorders, kidney failure and different sorts of cancer. To address these issues, this study developed a machine learning-based predictive model to evaluate the Groundwater Quality Index (GWQI) and identify the main contaminants which are affecting the water quality. It has been achieved with the help of a hybrid machine learning model i.e. LCBoost Fusion . The model has undergone several processes like data preprocessing, hyperparameter tuning using Differential Evolution (DE) optimization, and evaluation through cross-validation. The LCBoost Fusion model outperforms individual models (CatBoost and LightGBM), by achieving low RMSE (0.6829), MSE (0.5102), MAE (0.3147) and a high R$^2$ score of 0.9809. Feature importance analysis highlights Potassium (K), Fluoride (F) and Total Hardness (TH) as the most influential indicators of groundwater contamination. This research successfully demonstrates the application of machine learning in assessing groundwater quality risks in Odisha. The proposed LCBoost Fusion model offers a reliable and efficient approach for real-time groundwater monitoring and risk mitigation. These findings will help the environmental organizations and the policy makers to map out targeted places for sustainable groundwater management. Future work will focus on using remote sensing data and developing an interactive decision-making system for groundwater quality assessment.

### Techniques for Enhancing Memory Capacity of Reservoir Computing 
[[arxiv](https://arxiv.org/abs/2502.17923)] [[cool](https://papers.cool/arxiv/2502.17923)] [[pdf](https://arxiv.org/pdf/2502.17923)]
> **Authors**: Atsuki Yokota,Ichiro Kawashima,Yohei Saito,Hakaru Tamukoh,Osamu Nomura,Takashi Morie
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Reservoir Computing (RC) is a bio-inspired machine learning framework, and various models have been proposed. RC is a well-suited model for time series data processing, but there is a trade-off between memory capacity and nonlinearity. In this study, we propose methods to improve the memory capacity of reservoir models by modifying their network configuration except for the inside of reservoirs. The Delay method retains past inputs by adding delay node chains to the input layer with the specified number of delay steps. To suppress the effect of input value increase due to the Delay method, we divide the input weights by the number of added delay steps. The Pass through method feeds input values directly to the output layer. The Clustering method divides the input and reservoir nodes into multiple parts and integrates them at the output layer. We applied these methods to an echo state network (ESN), a typical RC model, and the chaotic Boltzmann machine (CBM)-RC, which can be efficiently implemented in integrated circuits. We evaluated their performance on the NARMA task, and measured information processing capacity (IPC) to evaluate the trade-off between memory capacity and nonlinearity.

### C-LoRA: Continual Low-Rank Adaptation for Pre-trained Models 
[[arxiv](https://arxiv.org/abs/2502.17920)] [[cool](https://papers.cool/arxiv/2502.17920)] [[pdf](https://arxiv.org/pdf/2502.17920)]
> **Authors**: Xin Zhang,Liang Bai,Xian Yang,Jiye Liang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Low-Rank Adaptation (LoRA) is an efficient fine-tuning method that has been extensively applied in areas such as natural language processing and computer vision. Existing LoRA fine-tuning approaches excel in static environments but struggle in dynamic learning due to reliance on multiple adapter modules, increasing overhead and complicating inference. We propose Continual Low-Rank Adaptation (C-LoRA), a novel extension of LoRA for continual learning. C-LoRA uses a learnable routing matrix to dynamically manage parameter updates across tasks, ensuring efficient reuse of learned subspaces while enforcing orthogonality to minimize interference and forgetting. Unlike existing approaches that require separate adapters for each task, C-LoRA enables a integrated approach for task adaptation, achieving both scalability and parameter efficiency in sequential learning scenarios. C-LoRA achieves state-of-the-art accuracy and parameter efficiency on benchmarks while providing theoretical insights into its routing matrix's role in retaining and transferring knowledge, establishing a scalable framework for continual learning.

### AirCast: Improving Air Pollution Forecasting Through Multi-Variable Data Alignment 
[[arxiv](https://arxiv.org/abs/2502.17919)] [[cool](https://papers.cool/arxiv/2502.17919)] [[pdf](https://arxiv.org/pdf/2502.17919)]
> **Authors**: Vishal Nedungadi,Muhammad Akhtar Munir,Marc Rußwurm,Ron Sarafian,Ioannis N. Athanasiadis,Yinon Rudich,Fahad Shahbaz Khan,Salman Khan
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: Air pollution remains a leading global health risk, exacerbated by rapid industrialization and urbanization, contributing significantly to morbidity and mortality rates. In this paper, we introduce AirCast, a novel multi-variable air pollution forecasting model, by combining weather and air quality variables. AirCast employs a multi-task head architecture that simultaneously forecasts atmospheric conditions and pollutant concentrations, improving its understanding of how weather patterns affect air quality. Predicting extreme pollution events is challenging due to their rare occurrence in historic data, resulting in a heavy-tailed distribution of pollution levels. To address this, we propose a novel Frequency-weighted Mean Absolute Error (fMAE) loss, adapted from the class-balanced loss for regression tasks. Informed from domain knowledge, we investigate the selection of key variables known to influence pollution levels. Additionally, we align existing weather and chemical datasets across spatial and temporal dimensions. AirCast's integrated approach, combining multi-task learning, frequency weighted loss and domain informed variable selection, enables more accurate pollution forecasts. Our source code and models are made public here (https://github.com/vishalned/AirCast.git)

### Batch normalization does not improve initialization 
[[arxiv](https://arxiv.org/abs/2502.17913)] [[cool](https://papers.cool/arxiv/2502.17913)] [[pdf](https://arxiv.org/pdf/2502.17913)]
> **Authors**: Joris Dannemann,Gero Junike
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,可能性
- **Abstract**: Batch normalization is one of the most important regularization techniques for neural networks, significantly improving training by centering the layers of the neural network. There have been several attempts to provide a theoretical justification for batch ormalization. Santurkar and Tsipras (2018) [How does batch normalization help optimization? Advances in neural information rocessing systems, 31] claim that batch normalization improves initialization. We provide a counterexample showing that this claim s not true, i.e., batch normalization does not improve initialization.

### Decoupled Graph Energy-based Model for Node Out-of-Distribution Detection on Heterophilic Graphs 
[[arxiv](https://arxiv.org/abs/2502.17912)] [[cool](https://papers.cool/arxiv/2502.17912)] [[pdf](https://arxiv.org/pdf/2502.17912)]
> **Authors**: Yuhan Chen,Yihong Luo,Yifan Song,Pengwen Dai,Jing Tang,Xiaochun Cao
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: The first two authors contributed equally to this work; ICLR 2025
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Despite extensive research efforts focused on OOD detection on images, OOD detection on nodes in graph learning remains underexplored. The dependence among graph nodes hinders the trivial adaptation of existing approaches on images that assume inputs to be i.i.d. sampled, since many unique features and challenges specific to graphs are not considered, such as the heterophily issue. Recently, GNNSafe, which considers node dependence, adapted energy-based detection to the graph domain with state-of-the-art performance, however, it has two serious issues: 1) it derives node energy from classification logits without specifically tailored training for modeling data distribution, making it less effective at recognizing OOD data; 2) it highly relies on energy propagation, which is based on homophily assumption and will cause significant performance degradation on heterophilic graphs, where the node tends to have dissimilar distribution with its neighbors. To address the above issues, we suggest training EBMs by MLE to enhance data distribution modeling and remove energy propagation to overcome the heterophily issues. However, training EBMs via MLE requires performing MCMC sampling on both node feature and node neighbors, which is challenging due to the node interdependence and discrete graph topology. To tackle the sampling challenge, we introduce DeGEM, which decomposes the learning process into two parts: a graph encoder that leverages topology information for node representations and an energy head that operates in latent space. Extensive experiments validate that DeGEM, without OOD exposure during training, surpasses previous state-of-the-art methods, achieving an average AUROC improvement of 6.71% on homophilic graphs and 20.29% on heterophilic graphs, and even outperform methods trained with OOD exposure. Our code is available at: https://github.com/draym28/DeGEM.

### Knowledge-enhanced Multimodal ECG Representation Learning with Arbitrary-Lead Inputs 
[[arxiv](https://arxiv.org/abs/2502.17900)] [[cool](https://papers.cool/arxiv/2502.17900)] [[pdf](https://arxiv.org/pdf/2502.17900)]
> **Authors**: Che Liu,Cheng Ouyang,Zhongwei Wan,Haozhe Wang,Wenjia Bai,Rossella Arcucci
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Recent advances in multimodal ECG representation learning center on aligning ECG signals with paired free-text reports. However, suboptimal alignment persists due to the complexity of medical language and the reliance on a full 12-lead setup, which is often unavailable in under-resourced settings. To tackle these issues, we propose **K-MERL**, a knowledge-enhanced multimodal ECG representation learning framework. **K-MERL** leverages large language models to extract structured knowledge from free-text reports and employs a lead-aware ECG encoder with dynamic lead masking to accommodate arbitrary lead inputs. Evaluations on six external ECG datasets show that **K-MERL** achieves state-of-the-art performance in zero-shot classification and linear probing tasks, while delivering an average **16%** AUC improvement over existing methods in partial-lead zero-shot classification.

### Arrhythmia Classification from 12-Lead ECG Signals Using Convolutional and Transformer-Based Deep Learning Models 
[[arxiv](https://arxiv.org/abs/2502.17887)] [[cool](https://papers.cool/arxiv/2502.17887)] [[pdf](https://arxiv.org/pdf/2502.17887)]
> **Authors**: Andrei Apostol,Maria Nutu
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 34 pages, 17 figures
- **标题**: None
- **领域**: 机器学习,人工智能,信号处理
- **Abstract**: In Romania, cardiovascular problems are the leading cause of death, accounting for nearly one-third of annual fatalities. The severity of this situation calls for innovative diagnosis method for cardiovascular diseases. This article aims to explore efficient, light-weight and rapid methods for arrhythmia diagnosis, in resource-constrained healthcare settings. Due to the lack of Romanian public medical data, we trained our systems using international public datasets, having in mind that the ECG signals are the same regardless the patients' nationality. Within this purpose, we combined multiple datasets, usually used in the field of arrhythmias classification: PTB-XL electrocardiography dataset , PTB Diagnostic ECG Database, China 12-Lead ECG Challenge Database, Georgia 12-Lead ECG Challenge Database, and St. Petersburg INCART 12-lead Arrhythmia Database. For the input data, we employed ECG signal processing methods, specifically a variant of the Pan-Tompkins algorithm, useful in arrhythmia classification because it provides a robust and efficient method for detecting QRS complexes in ECG signals. Additionally, we used machine learning techniques, widely used for the task of classification, including convolutional neural networks (1D CNNs, 2D CNNs, ResNet) and Vision Transformers (ViTs). The systems were evaluated in terms of accuracy and F1 score. We annalysed our dataset from two perspectives. First, we fed the systems with the ECG signals and the GRU-based 1D CNN model achieved the highest accuracy of 93.4% among all the tested architectures. Secondly, we transformed ECG signals into images and the CNN2D model achieved an accuracy of 92.16%.

### Neural Graph Matching Improves Retrieval Augmented Generation in Molecular Machine Learning 
[[arxiv](https://arxiv.org/abs/2502.17874)] [[cool](https://papers.cool/arxiv/2502.17874)] [[pdf](https://arxiv.org/pdf/2502.17874)]
> **Authors**: Runzhong Wang,Rui-Xi Wang,Mrunali Manjrekar,Connor W. Coley
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,定量方法
- **Abstract**: Molecular machine learning has gained popularity with the advancements of geometric deep learning. In parallel, retrieval-augmented generation has become a principled approach commonly used with language models. However, the optimal integration of retrieval augmentation into molecular machine learning remains unclear. Graph neural networks stand to benefit from clever matching to understand the structural alignment of retrieved molecules to a query molecule. Neural graph matching offers a compelling solution by explicitly modeling node and edge affinities between two structural graphs while employing a noise-robust, end-to-end neural network to learn affinity metrics. We apply this approach to mass spectrum simulation and introduce MARASON, a novel model that incorporates neural graph matching to enhance a fragmentation-based neural network. Experimental results highlight the effectiveness of our design, with MARASON achieving 28% top-1 accuracy, a substantial improvement over the non-retrieval state-of-the-art accuracy of 19%. Moreover, MARASON outperforms both naive retrieval-augmented generation methods and traditional graph matching approaches.

### EEGM2: An Efficient Mamba-2-Based Self-Supervised Framework for Long-Sequence EEG Modeling 
[[arxiv](https://arxiv.org/abs/2502.17873)] [[cool](https://papers.cool/arxiv/2502.17873)] [[pdf](https://arxiv.org/pdf/2502.17873)]
> **Authors**: Jiazhen Hong,Geoffrey Mackellar,Soheila Ghane
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 10 pages, 7 figures
- **标题**: None
- **领域**: 机器学习,信号处理
- **Abstract**: Deep learning has achieved significant progress in the development of electroencephalogram (EEG) foundation models, with Transformer-based architectures excelling at capturing long-range dependencies. However, their quadratic computational complexity presents challenges in memory efficiency, training, and inference speed, limiting their scalability and generalizability as a foundation model. In this paper, we propose EEGM2, a self-supervised framework based on structured state space duality (SSD) that overcomes these limitations. EEGM2 introduces three key innovations: (1) a reconstruction-based framework that captures both local and global EEG features through Mamba-2 structured state space models, (2) a spatiotemporal-aware loss function that enhances robustness to noise and preserves spectral information, and (3) a multi-branch receptive field input embedding strategy that improves cross-subject generalization and stability for EEG sequences of varying lengths. In comparison to traditional pretraining methods, on raw EEG or latent representation spaces, EEGM2 shows superior performance on long-sequence tasks, where conventional models struggle. Our experimental results on six EEG datasets validate that EEGM2 not only achieves state-of-the-art cross-domain accuracy but also reduces computational overhead, making it a more efficient solution for deployment on resource-constrained BCI devices.

### Contrastive Learning with Nasty Noise 
[[arxiv](https://arxiv.org/abs/2502.17872)] [[cool](https://papers.cool/arxiv/2502.17872)] [[pdf](https://arxiv.org/pdf/2502.17872)]
> **Authors**: Ziruo Zhao
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Contrastive learning has emerged as a powerful paradigm for self-supervised representation learning. This work analyzes the theoretical limits of contrastive learning under nasty noise, where an adversary modifies or replaces training samples. Using PAC learning and VC-dimension analysis, lower and upper bounds on sample complexity in adversarial settings are established. Additionally, data-dependent sample complexity bounds based on the l2-distance function are derived.

### Mitigating Attrition: Data-Driven Approach Using Machine Learning and Data Engineering 
[[arxiv](https://arxiv.org/abs/2502.17865)] [[cool](https://papers.cool/arxiv/2502.17865)] [[pdf](https://arxiv.org/pdf/2502.17865)]
> **Authors**: Naveen Edapurath Vijayan
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 7 pages
- **标题**: None
- **领域**: 机器学习,计算机与社会,软件工程
- **Abstract**: This paper presents a novel data-driven approach to mitigating employee attrition using machine learning and data engineering techniques. The proposed framework integrates data from various human resources systems and leverages advanced feature engineering to capture a comprehensive set of factors influencing attrition. The study outlines a robust modeling approach that addresses challenges such as imbalanced datasets, categorical data handling, and model interpretation. The methodology includes careful consideration of training and testing strategies, baseline model establishment, and the development of calibrated predictive models. The research emphasizes the importance of model interpretation using techniques like SHAP values to provide actionable insights for organizations. Key design choices in algorithm selection, hyperparameter tuning, and probability calibration are discussed. This approach enables organizations to proactively identify attrition risks and develop targeted retention strategies, ultimately redu

## 多代理系统(cs.MA:Multiagent Systems)

### MA-GTS: A Multi-Agent Framework for Solving Complex Graph Problems in Real-World Applications 
[[arxiv](https://arxiv.org/abs/2502.18540)] [[cool](https://papers.cool/arxiv/2502.18540)] [[pdf](https://arxiv.org/pdf/2502.18540)]
> **Authors**: Zike Yuan,Ming Liu,Hui Wang,Bing Qin
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 多代理系统,人工智能
- **Abstract**: Graph-theoretic problems arise in real-world applications like logistics, communication networks, and traffic optimization. These problems are often complex, noisy, and irregular, posing challenges for traditional algorithms. Large language models (LLMs) offer potential solutions but face challenges, including limited accuracy and input length constraints. To address these challenges, we propose MA-GTS (Multi-Agent Graph Theory Solver), a multi-agent framework that decomposes these complex problems through agent collaboration. MA-GTS maps the implicitly expressed text-based graph data into clear, structured graph representations and dynamically selects the most suitable algorithm based on problem constraints and graph structure scale. This approach ensures that the solution process remains efficient and the resulting reasoning path is interpretable. We validate MA-GTS using the G-REAL dataset, a real-world-inspired graph theory dataset we created. Experimental results show that MA-GTS outperforms state-of-the-art approaches in terms of efficiency, accuracy, and scalability, with strong results across multiple benchmarks (G-REAL 94.2%, GraCoRe 96.9%, NLGraph 98.4%).MA-GTS is open-sourced at https://github.com/ZIKEYUAN/MA-GTS.git.

### MAFE: Multi-Agent Fair Environments for Decision-Making Systems 
[[arxiv](https://arxiv.org/abs/2502.18534)] [[cool](https://papers.cool/arxiv/2502.18534)] [[pdf](https://arxiv.org/pdf/2502.18534)]
> **Authors**: Zachary McBride Lazri,Anirudh Nakra,Ivan Brugere,Danial Dervovic,Antigoni Polychroniadou,Furong Huang,Dana Dachman-Soled,Min Wu
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 多代理系统,人工智能
- **Abstract**: Fairness constraints applied to machine learning (ML) models in static contexts have been shown to potentially produce adverse outcomes among demographic groups over time. To address this issue, emerging research focuses on creating fair solutions that persist over time. While many approaches treat this as a single-agent decision-making problem, real-world systems often consist of multiple interacting entities that influence outcomes. Explicitly modeling these entities as agents enables more flexible analysis of their interventions and the effects they have on a system's underlying dynamics. A significant challenge in conducting research on multi-agent systems is the lack of realistic environments that leverage the limited real-world data available for analysis. To address this gap, we introduce the concept of a Multi-Agent Fair Environment (MAFE) and present and analyze three MAFEs that model distinct social systems. Experimental results demonstrate the utility of our MAFEs as testbeds for developing multi-agent fair algorithms.

### Heterogeneous Decision Making in Mixed Traffic: Uncertainty-aware Planning and Bounded Rationality 
[[arxiv](https://arxiv.org/abs/2502.18529)] [[cool](https://papers.cool/arxiv/2502.18529)] [[pdf](https://arxiv.org/pdf/2502.18529)]
> **Authors**: Hang Wang,Qiaoyi Fang,Junshan Zhang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-26
> **comment**: CPAL 2025
- **标题**: None
- **领域**: 多代理系统,人工智能,人机交互
- **Abstract**: The past few years have witnessed a rapid growth of the deployment of automated vehicles (AVs). Clearly, AVs and human-driven vehicles (HVs) will co-exist for many years, and AVs will have to operate around HVs, pedestrians, cyclists, and more, calling for fundamental breakthroughs in AI designed for mixed traffic to achieve mixed autonomy. Thus motivated, we study heterogeneous decision making by AVs and HVs in a mixed traffic environment, aiming to capture the interactions between human and machine decision-making and develop an AI foundation that enables vehicles to operate safely and efficiently. There are a number of challenges to achieve mixed autonomy, including 1) humans drivers make driving decisions with bounded rationality, and it remains open to develop accurate models for HVs' decision making; and 2) uncertainty-aware planning plays a critical role for AVs to take safety maneuvers in response to the human behavior. In this paper, we introduce a formulation of AV-HV interaction, where the HV makes decisions with bounded rationality and the AV employs uncertainty-aware planning based on the prediction on HV's future actions. We conduct a comprehensive analysis on AV and HV's learning regret to answer the questions: 1) {How does the learning performance depend on HV's bounded rationality and AV's planning}; 2) {How do different decision making strategies impact the overall learning performance}? Our findings reveal some intriguing phenomena, such as Goodhart's Law in AV's learning performance and compounding effects in HV's decision making process. By examining the dynamics of the regrets, we gain insights into the interplay between human and machine decision making.

## 多媒体(cs.MM:Multimedia)

### Deep-JGAC: End-to-End Deep Joint Geometry and Attribute Compression for Dense Colored Point Clouds 
[[arxiv](https://arxiv.org/abs/2502.17939)] [[cool](https://papers.cool/arxiv/2502.17939)] [[pdf](https://arxiv.org/pdf/2502.17939)]
> **Authors**: Yun Zhang,Zixi Guo,Linwei Zhu,C. -C. Jay Kuo
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 多媒体,计算机视觉和模式识别
- **Abstract**: Colored point cloud becomes a fundamental representation in the realm of 3D vision. Effective Point Cloud Compression (PCC) is urgently needed due to huge amount of data. In this paper, we propose an end-to-end Deep Joint Geometry and Attribute point cloud Compression (Deep-JGAC) framework for dense colored point clouds, which exploits the correlation between the geometry and attribute for high compression efficiency. Firstly, we propose a flexible Deep-JGAC framework, where the geometry and attribute sub-encoders are compatible to either learning or non-learning based geometry and attribute encoders. Secondly, we propose an attribute-assisted deep geometry encoder that enhances the geometry latent representation with the help of attribute, where the geometry decoding remains unchanged. Moreover, Attribute Information Fusion Module (AIFM) is proposed to fuse attribute information in geometry coding. Thirdly, to solve the mismatch between the point cloud geometry and attribute caused by the geometry compression distortion, we present an optimized re-colorization module to attach the attribute to the geometrically distorted point cloud for attribute coding. It enhances the colorization and lowers the computational complexity. Extensive experimental results demonstrate that in terms of the geometry quality metric D1-PSNR, the proposed Deep-JGAC achieves an average of 82.96%, 36.46%, 41.72%, and 31.16% bit-rate reductions as compared to the state-of-the-art G-PCC, V-PCC, GRASP, and PCGCv2, respectively. In terms of perceptual joint quality metric MS-GraphSIM, the proposed Deep-JGAC achieves an average of 48.72%, 14.67%, and 57.14% bit-rate reductions compared to the G-PCC, V-PCC, and IT-DL-PCC, respectively. The encoding/decoding time costs are also reduced by 94.29%/24.70%, and 96.75%/91.02% on average as compared with the V-PCC and IT-DL-PCC.

## 机器人技术(cs.RO:Robotics)

### InVDriver: Intra-Instance Aware Vectorized Query-Based Autonomous Driving Transformer 
[[arxiv](https://arxiv.org/abs/2502.17949)] [[cool](https://papers.cool/arxiv/2502.17949)] [[pdf](https://arxiv.org/pdf/2502.17949)]
> **Authors**: Bo Zhang,Heye Huang,Chunyang Liu,Yaqin Zhang,Zhenhua Xu
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Submitted to JICV (Journal of Intelligent and Connected Vehicles)
- **标题**: None
- **领域**: 机器人技术,计算机视觉和模式识别
- **Abstract**: End-to-end autonomous driving with its holistic optimization capabilities, has gained increasing traction in academia and industry. Vectorized representations, which preserve instance-level topological information while reducing computational overhead, have emerged as a promising paradigm. While existing vectorized query-based frameworks often overlook the inherent spatial correlations among intra-instance points, resulting in geometrically inconsistent outputs (e.g., fragmented HD map elements or oscillatory trajectories). To address these limitations, we propose InVDriver, a novel vectorized query-based system that systematically models intra-instance spatial dependencies through masked self-attention layers, thereby enhancing planning accuracy and trajectory smoothness. Across all core modules, i.e., perception, prediction, and planning, InVDriver incorporates masked self-attention mechanisms that restrict attention to intra-instance point interactions, enabling coordinated refinement of structural elements while suppressing irrelevant inter-instance noise. Experimental results on the nuScenes benchmark demonstrate that InVDriver achieves state-of-the-art performance, surpassing prior methods in both accuracy and safety, while maintaining high computational efficiency. Our work validates that explicit modeling of intra-instance geometric coherence is critical for advancing vectorized autonomous driving systems, bridging the gap between theoretical advantages of end-to-end frameworks and practical deployment requirements.

## 声音(cs.SD:Sound)

### Enhancing Speech Quality through the Integration of BGRU and Transformer Architectures 
[[arxiv](https://arxiv.org/abs/2502.17911)] [[cool](https://papers.cool/arxiv/2502.17911)] [[pdf](https://arxiv.org/pdf/2502.17911)]
> **Authors**: Souliman Alghnam,Mohammad Alhussien,Khaled Shaheen
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 声音,人工智能,音频和语音处理
- **Abstract**: Speech enhancement plays an essential role in improving the quality of speech signals in noisy environments. This paper investigates the efficacy of integrating Bidirectional Gated Recurrent Units (BGRU) and Transformer models for speech enhancement tasks. Through a comprehensive experimental evaluation, our study demonstrates the superiority of this hybrid architecture over traditional methods and standalone models. The combined BGRU-Transformer framework excels in capturing temporal dependencies and learning complex signal patterns, leading to enhanced noise reduction and improved speech quality. Results show significant performance gains compared to existing approaches, highlighting the potential of this integrated model in real-world applications. The seamless integration of BGRU and Transformer architectures not only enhances system robustness but also opens the road for advanced speech processing techniques. This research contributes to the ongoing efforts in speech enhancement technology and sets a solid foundation for future investigations into optimizing model architectures, exploring many application scenarios, and advancing the field of speech processing in noisy environments.

## 软件工程(cs.SE:Software Engineering)

### Programming with Pixels: Computer-Use Meets Software Engineering 
[[arxiv](https://arxiv.org/abs/2502.18525)] [[cool](https://papers.cool/arxiv/2502.18525)] [[pdf](https://arxiv.org/pdf/2502.18525)]
> **Authors**: Pranjal Aggarwal,Sean Welleck
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,机器学习
- **Abstract**: Recent advancements in software engineering (SWE) agents have largely followed a $\textit{tool-based paradigm}$, where agents interact with hand-engineered tool APIs to perform specific tasks. While effective for specialized tasks, these methods fundamentally lack generalization, as they require predefined tools for each task and do not scale across programming languages and domains. We introduce $\texttt{Programming with Pixels}$ (PwP), an agent environment that unifies software development tasks by enabling $\textit{computer-use agents}$-agents that operate directly within an IDE through visual perception, typing, and clicking, rather than relying on predefined tool APIs. To systematically evaluate these agents, we propose $\texttt{PwP-Bench}$, a benchmark that unifies existing SWE benchmarks spanning tasks across multiple programming languages, modalities, and domains under a task-agnostic state and action space. Our experiments demonstrate that general-purpose computer-use agents can approach or even surpass specialized tool-based agents on a variety of SWE tasks without the need for hand-engineered tools. However, our analysis shows that current models suffer from limited visual grounding and fail to exploit many IDE tools that could simplify their tasks. When agents can directly access IDE tools, without visual interaction, they show significant performance improvements, highlighting the untapped potential of leveraging built-in IDE capabilities. Our results establish PwP as a scalable testbed for building and evaluating the next wave of software engineering agents. We release code and data at https://programmingwithpixels.com

## 社交和信息网络(cs.SI:Social and Information Networks)

### Analyzing User Perceptions of Large Language Models (LLMs) on Reddit: Sentiment and Topic Modeling of ChatGPT and DeepSeek Discussions 
[[arxiv](https://arxiv.org/abs/2502.18513)] [[cool](https://papers.cool/arxiv/2502.18513)] [[pdf](https://arxiv.org/pdf/2502.18513)]
> **Authors**: Krishnaveni Katta
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-26
> **comment**: 13 pages, 8 figures
- **标题**: None
- **领域**: 社交和信息网络,计算语言学,人机交互
- **Abstract**: While there is an increased discourse on large language models (LLMs) like ChatGPT and DeepSeek, there is no comprehensive understanding of how users of online platforms, like Reddit, perceive these models. This is an important omission because public opinion can influence AI development, trust, and future policy. This study aims at analyzing Reddit discussions about ChatGPT and DeepSeek using sentiment and topic modeling to advance the understanding of user attitudes. Some of the significant topics such as trust in AI, user expectations, potential uses of the tools, reservations about AI biases, and ethical implications of their use are explored in this study. By examining these concerns, the study provides a sense of how public sentiment might shape the direction of AI development going forward. The report also mentions whether users have faith in the technology and what they see as its future. A word frequency approach is used to identify broad topics and sentiment trends. Also, topic modeling through the Latent Dirichlet Allocation (LDA) method identifies top topics in users' language, for example, potential benefits of LLMs, their technological applications, and their overall social ramifications. The study aims to inform developers and policymakers by making it easier to see how users comprehend and experience these game-changing technologies.

### Structure-prior Informed Diffusion Model for Graph Source Localization with Limited Data 
[[arxiv](https://arxiv.org/abs/2502.17928)] [[cool](https://papers.cool/arxiv/2502.17928)] [[pdf](https://arxiv.org/pdf/2502.17928)]
> **Authors**: Hongyi Chen,Jingtao Ding,Xiaojun Liang,Yong Li,Xiao-Ping Zhang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 社交和信息网络,人工智能,机器学习
- **Abstract**: The source localization problem in graph information propagation is crucial for managing various network disruptions, from misinformation spread to infrastructure failures. While recent deep generative approaches have shown promise in this domain, their effectiveness is limited by the scarcity of real-world propagation data. This paper introduces SIDSL (\textbf{S}tructure-prior \textbf{I}nformed \textbf{D}iffusion model for \textbf{S}ource \textbf{L}ocalization), a novel framework that addresses three key challenges in limited-data scenarios: unknown propagation patterns, complex topology-propagation relationships, and class imbalance between source and non-source nodes. SIDSL incorporates topology-aware priors through graph label propagation and employs a propagation-enhanced conditional denoiser with a GNN-parameterized label propagation module (GNN-LP). Additionally, we propose a structure-prior biased denoising scheme that initializes from structure-based source estimations rather than random noise, effectively countering class imbalance issues. Experimental results across four real-world datasets demonstrate SIDSL's superior performance, achieving 7.5-13.3% improvements in F1 scores compared to state-of-the-art methods. Notably, when pretrained with simulation data of synthetic patterns, SIDSL maintains robust performance with only 10% of training data, surpassing baselines by more than 18.8%. These results highlight SIDSL's effectiveness in real-world applications where labeled data is scarce.

## 图像和视频处理(eess.IV:Image and Video Processing)

### A Comparative Review of the Histogram-based Image Segmentation Methods 
[[arxiv](https://arxiv.org/abs/2502.18550)] [[cool](https://papers.cool/arxiv/2502.18550)] [[pdf](https://arxiv.org/pdf/2502.18550)]
> **Authors**: ZhenZhou Wang
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: The histogram of an image is the accurate graphical representation of the numerical grayscale distribution and it is also an estimate of the probability distribution of image pixels. Therefore, histogram has been widely adopted to calculate the clustering means and partitioning thresholds for image segmentation. There have been many classical histogram-based image segmentation methods proposed and played important roles in both academics and industry. In this article, the histories and recent advances of the histogram-based image segmentation techniques are first reviewed and then they are divided into four categories: (1), the means-based method; (2), the Gaussian-mixture-model-based method; (3), the entropy-based method and (4) the feature-points-based method. The principles of the classical histogram-based image segmentation methods are described at first and then their performances are compared objectively. In addition, the histogram-based image segmentation methods are compared with the general-purpose deep learning methods in segmenting objects with uniform or simple backgrounds. The histogram-based image segmentation methods are more accurate than the universal deep-learning methods without special training in segmenting many types of images.

### End-to-End Deep Learning for Structural Brain Imaging: A Unified Framework 
[[arxiv](https://arxiv.org/abs/2502.18523)] [[cool](https://papers.cool/arxiv/2502.18523)] [[pdf](https://arxiv.org/pdf/2502.18523)]
> **Authors**: Yao Su,Keqi Han,Mingjie Zeng,Lichao Sun,Liang Zhan,Carl Yang,Lifang He,Xiangnan Kong
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **Abstract**: Brain imaging analysis is fundamental in neuroscience, providing valuable insights into brain structure and function. Traditional workflows follow a sequential pipeline-brain extraction, registration, segmentation, parcellation, network generation, and classification-treating each step as an independent task. These methods rely heavily on task-specific training data and expert intervention to correct intermediate errors, making them particularly burdensome for high-dimensional neuroimaging data, where annotations and quality control are costly and time-consuming. We introduce UniBrain, a unified end-to-end framework that integrates all processing steps into a single optimization process, allowing tasks to interact and refine each other. Unlike traditional approaches that require extensive task-specific annotations, UniBrain operates with minimal supervision, leveraging only low-cost labels (i.e., classification and extraction) and a single labeled atlas. By jointly optimizing extraction, registration, segmentation, parcellation, network generation, and classification, UniBrain enhances both accuracy and computational efficiency while significantly reducing annotation effort. Experimental results demonstrate its superiority over existing methods across multiple tasks, offering a more scalable and reliable solution for neuroimaging analysis. Our code and data can be found at https://github.com/Anonymous7852/UniBrain

### Rewards-based image analysis in microscopy 
[[arxiv](https://arxiv.org/abs/2502.18522)] [[cool](https://papers.cool/arxiv/2502.18522)] [[pdf](https://arxiv.org/pdf/2502.18522)]
> **Authors**: Kamyar Barakati,Yu Liu,Utkarsh Pratiush,Boris N. Slautin,Sergei V. Kalinin
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-26
> **comment**: 38 pages, 11 figures
- **标题**: None
- **领域**: 图像和视频处理,材料科学,计算机视觉和模式识别,机器学习,应用物理
- **Abstract**: Analyzing imaging and hyperspectral data is crucial across scientific fields, including biology, medicine, chemistry, and physics. The primary goal is to transform high-resolution or high-dimensional data into an interpretable format to generate actionable insights, aiding decision-making and advancing knowledge. Currently, this task relies on complex, human-designed workflows comprising iterative steps such as denoising, spatial sampling, keypoint detection, feature generation, clustering, dimensionality reduction, and physics-based deconvolutions. The introduction of machine learning over the past decade has accelerated tasks like image segmentation and object detection via supervised learning, and dimensionality reduction via unsupervised methods. However, both classical and NN-based approaches still require human input, whether for hyperparameter tuning, data labeling, or both. The growing use of automated imaging tools, from atomically resolved imaging to biological applications, demands unsupervised methods that optimize data representation for human decision-making or autonomous experimentation. Here, we discuss advances in reward-based workflows, which adopt expert decision-making principles and demonstrate strong transfer learning across diverse tasks. We represent image analysis as a decision-making process over possible operations and identify desiderata and their mappings to classical decision-making frameworks. Reward-driven workflows enable a shift from supervised, black-box models sensitive to distribution shifts to explainable, unsupervised, and robust optimization in image analysis. They can function as wrappers over classical and DCNN-based methods, making them applicable to both unsupervised and supervised workflows (e.g., classification, regression for structure-property mapping) across imaging and hyperspectral data.

### FreeTumor: Large-Scale Generative Tumor Synthesis in Computed Tomography Images for Improving Tumor Recognition 
[[arxiv](https://arxiv.org/abs/2502.18519)] [[cool](https://papers.cool/arxiv/2502.18519)] [[pdf](https://arxiv.org/pdf/2502.18519)]
> **Authors**: Linshan Wu,Jiaxin Zhuang,Yanning Zhou,Sunan He,Jiabo Ma,Luyang Luo,Xi Wang,Xuefeng Ni,Xiaoling Zhong,Mingxiang Wu,Yinghua Zhao,Xiaohui Duan,Varut Vardhanabhuti,Pranav Rajpurkar,Hao Chen
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **Abstract**: Tumor is a leading cause of death worldwide, with an estimated 10 million deaths attributed to tumor-related diseases every year. AI-driven tumor recognition unlocks new possibilities for more precise and intelligent tumor screening and diagnosis. However, the progress is heavily hampered by the scarcity of annotated datasets, which demands extensive annotation efforts by radiologists. To tackle this challenge, we introduce FreeTumor, an innovative Generative AI (GAI) framework to enable large-scale tumor synthesis for mitigating data scarcity. Specifically, FreeTumor effectively leverages a combination of limited labeled data and large-scale unlabeled data for tumor synthesis training. Unleashing the power of large-scale data, FreeTumor is capable of synthesizing a large number of realistic tumors on images for augmenting training datasets. To this end, we create the largest training dataset for tumor synthesis and recognition by curating 161,310 publicly available Computed Tomography (CT) volumes from 33 sources, with only 2.3% containing annotated tumors. To validate the fidelity of synthetic tumors, we engaged 13 board-certified radiologists in a Visual Turing Test to discern between synthetic and real tumors. Rigorous clinician evaluation validates the high quality of our synthetic tumors, as they achieved only 51.1% sensitivity and 60.8% accuracy in distinguishing our synthetic tumors from real ones. Through high-quality tumor synthesis, FreeTumor scales up the recognition training datasets by over 40 times, showcasing a notable superiority over state-of-the-art AI methods including various synthesis methods and foundation models. These findings indicate promising prospects of FreeTumor in clinical applications, potentially advancing tumor treatments and improving the survival rates of patients.

### 3D Anatomical Structure-guided Deep Learning for Accurate Diffusion Microstructure Imaging 
[[arxiv](https://arxiv.org/abs/2502.17933)] [[cool](https://papers.cool/arxiv/2502.17933)] [[pdf](https://arxiv.org/pdf/2502.17933)]
> **Authors**: Xinrui Ma,Jian Cheng,Wenxin Fan,Ruoyou Wu,Yongquan Ye,Shanshan Wang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: Diffusion magnetic resonance imaging (dMRI) is a crucial non-invasive technique for exploring the microstructure of the living human brain. Traditional hand-crafted and model-based tissue microstructure reconstruction methods often require extensive diffusion gradient sampling, which can be time-consuming and limits the clinical applicability of tissue microstructure information. Recent advances in deep learning have shown promise in microstructure estimation; however, accurately estimating tissue microstructure from clinically feasible dMRI scans remains challenging without appropriate constraints. This paper introduces a novel framework that achieves high-fidelity and rapid diffusion microstructure imaging by simultaneously leveraging anatomical information from macro-level priors and mutual information across parameters. This approach enhances time efficiency while maintaining accuracy in microstructure estimation. Experimental results demonstrate that our method outperforms four state-of-the-art techniques, achieving a peak signal-to-noise ratio (PSNR) of 30.51$\pm$0.58 and a structural similarity index measure (SSIM) of 0.97$\pm$0.004 in estimating parametric maps of multiple diffusion models. Notably, our method achieves a 15$\times$ acceleration compared to the dense sampling approach, which typically utilizes 270 diffusion gradients.

### A graph neural network-based multispectral-view learning model for diabetic macular ischemia detection from color fundus photographs 
[[arxiv](https://arxiv.org/abs/2502.17886)] [[cool](https://papers.cool/arxiv/2502.17886)] [[pdf](https://arxiv.org/pdf/2502.17886)]
> **Authors**: Qinghua He,Hongyang Jiang,Danqi Fang,Dawei Yang,Truong X. Nguyen,Anran Ran,Clement C. Tham,Simon K. H. Szeto,Sobha Sivaprasad,Carol Y. Cheung
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **Abstract**: Diabetic macular ischemia (DMI), marked by the loss of retinal capillaries in the macular area, contributes to vision impairment in patients with diabetes. Although color fundus photographs (CFPs), combined with artificial intelligence (AI), have been extensively applied in detecting various eye diseases, including diabetic retinopathy (DR), their applications in detecting DMI remain unexplored, partly due to skepticism among ophthalmologists regarding its feasibility. In this study, we propose a graph neural network-based multispectral view learning (GNN-MSVL) model designed to detect DMI from CFPs. The model leverages higher spectral resolution to capture subtle changes in fundus reflectance caused by ischemic tissue, enhancing sensitivity to DMI-related features. The proposed approach begins with computational multispectral imaging (CMI) to reconstruct 24-wavelength multispectral fundus images from CFPs. ResNeXt101 is employed as the backbone for multi-view learning to extract features from the reconstructed images. Additionally, a GNN with a customized jumper connection strategy is designed to enhance cross-spectral relationships, facilitating comprehensive and efficient multispectral view learning. The study included a total of 1,078 macula-centered CFPs from 1,078 eyes of 592 patients with diabetes, of which 530 CFPs from 530 eyes of 300 patients were diagnosed with DMI. The model achieved an accuracy of 84.7 percent and an area under the receiver operating characteristic curve (AUROC) of 0.900 (95 percent CI: 0.852-0.937) on eye-level, outperforming both the baseline model trained from CFPs and human experts (p-values less than 0.01). These findings suggest that AI-based CFP analysis holds promise for detecting DMI, contributing to its early and low-cost screening.

## 系统与控制(eess.SY:Systems and Control)

### Sample-efficient diffusion-based control of complex nonlinear systems 
[[arxiv](https://arxiv.org/abs/2502.17893)] [[cool](https://papers.cool/arxiv/2502.17893)] [[pdf](https://arxiv.org/pdf/2502.17893)]
> **Authors**: Hongyi Chen,Jingtao Ding,Jianhai Shu,Xinchun Yu,Xiaojun Liang,Yong Li,Xiao-Ping Zhang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 系统与控制,人工智能,机器学习
- **Abstract**: Complex nonlinear system control faces challenges in achieving sample-efficient, reliable performance. While diffusion-based methods have demonstrated advantages over classical and reinforcement learning approaches in long-term control performance, they are limited by sample efficiency. This paper presents SEDC (Sample-Efficient Diffusion-based Control), a novel diffusion-based control framework addressing three core challenges: high-dimensional state-action spaces, nonlinear system dynamics, and the gap between non-optimal training data and near-optimal control solutions. Through three innovations - Decoupled State Diffusion, Dual-Mode Decomposition, and Guided Self-finetuning - SEDC achieves 39.5\%-49.4\% better control accuracy than baselines while using only 10\% of the training samples, as validated across three complex nonlinear dynamic systems. Our approach represents a significant advancement in sample-efficient control of complex nonlinear systems. The implementation of the code can be found at https://anonymous.4open.science/r/DIFOCON-C019.

## 几何拓扑(math.GT:Geometric Topology)

### Colored Jones Polynomials and the Volume Conjecture 
[[arxiv](https://arxiv.org/abs/2502.18575)] [[cool](https://papers.cool/arxiv/2502.18575)] [[pdf](https://arxiv.org/pdf/2502.18575)]
> **Authors**: Mark Hughes,Vishnu Jejjala,P. Ramadevi,Pratik Roy,Vivek Kumar Singh
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 27 pages, 16 figures
- **标题**: None
- **领域**: 几何拓扑,机器学习,高能物理 - 理论
- **Abstract**: Using the vertex model approach for braid representations, we compute polynomials for spin-1 placed on hyperbolic knots up to 15 crossings. These polynomials are referred to as 3-colored Jones polynomials or adjoint Jones polynomials. Training a subset of the data using a fully connected feedforward neural network, we predict the volume of the knot complement of hyperbolic knots from the adjoint Jones polynomial or its evaluations with 99.34% accuracy. A function of the adjoint Jones polynomial evaluated at the phase $q=e^{ 8 πi / 15 }$ predicts the volume with nearly the same accuracy as the neural network. From an analysis of 2-colored and 3-colored Jones polynomials, we conjecture the best phase for $n$-colored Jones polynomials, and use this hypothesis to motivate an improved statement of the volume conjecture. This is tested for knots for which closed form expressions for the $n$-colored Jones polynomial are known, and we show improved convergence to the volume.

## 可能性(math.PR:Probability)

### Tight Bounds on the Binomial CDF, and the Minimum of i.i.d Binomials, in terms of KL-Divergence 
[[arxiv](https://arxiv.org/abs/2502.18611)] [[cool](https://papers.cool/arxiv/2502.18611)] [[pdf](https://arxiv.org/pdf/2502.18611)]
> **Authors**: Xiaohan Zhu,Mesrob I. Ohannessian,Nathan Srebro
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 可能性,机器学习,机器学习
- **Abstract**: We provide finite sample upper and lower bounds on the Binomial tail probability which are a direct application of Sanov's theorem. We then use these to obtain high probability upper and lower bounds on the minimum of i.i.d. Binomial random variables. Both bounds are finite sample, asymptotically tight, and expressed in terms of the KL-divergence.

## 统计理论(math.ST:Statistics Theory)

### Learning sparse generalized linear models with binary outcomes via iterative hard thresholding 
[[arxiv](https://arxiv.org/abs/2502.18393)] [[cool](https://papers.cool/arxiv/2502.18393)] [[pdf](https://arxiv.org/pdf/2502.18393)]
> **Authors**: Namiko Matsumoto,Arya Mazumdar
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 统计理论,数据结构和算法,信息论,机器学习,机器学习
- **Abstract**: In statistics, generalized linear models (GLMs) are widely used for modeling data and can expressively capture potential nonlinear dependence of the model's outcomes on its covariates. Within the broad family of GLMs, those with binary outcomes, which include logistic and probit regressions, are motivated by common tasks such as binary classification with (possibly) non-separable data. In addition, in modern machine learning and statistics, data is often high-dimensional yet has a low intrinsic dimension, making sparsity constraints in models another reasonable consideration. In this work, we propose to use and analyze an iterative hard thresholding (projected gradient descent on the ReLU loss) algorithm, called binary iterative hard thresholding (BIHT), for parameter estimation in sparse GLMs with binary outcomes. We establish that BIHT is statistically efficient and converges to the correct solution for parameter estimation in a general class of sparse binary GLMs. Unlike many other methods for learning GLMs, including maximum likelihood estimation, generalized approximate message passing, and GLM-tron (Kakade et al. 2011; Bahmani et al. 2016), BIHT does not require knowledge of the GLM's link function, offering flexibility and generality in allowing the algorithm to learn arbitrary binary GLMs. As two applications, logistic and probit regression are additionally studied. In this regard, it is shown that in logistic regression, the algorithm is in fact statistically optimal in the sense that the order-wise sample complexity matches (up to logarithmic factors) the lower bound obtained previously. To the best of our knowledge, this is the first work achieving statistical optimality for logistic regression in all noise regimes with a computationally efficient algorithm. Moreover, for probit regression, our sample complexity is on the same order as that obtained for logistic regression.

## 计算物理(physics.comp-ph:Computational Physics)

### Learning atomic forces from uncertainty-calibrated adversarial attacks 
[[arxiv](https://arxiv.org/abs/2502.18314)] [[cool](https://papers.cool/arxiv/2502.18314)] [[pdf](https://arxiv.org/pdf/2502.18314)]
> **Authors**: Henrique Musseli Cezar,Tilmann Bodenstein,Henrik Andersen Sveinsson,Morten Ledum,Simen Reine,Sigbjørn Løland Bore
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算物理,机器学习
- **Abstract**: Adversarial approaches, which intentionally challenge machine learning models by generating difficult examples, are increasingly being adopted to improve machine learning interatomic potentials (MLIPs). While already providing great practical value, little is known about the actual prediction errors of MLIPs on adversarial structures and whether these errors can be controlled. We propose the Calibrated Adversarial Geometry Optimization (CAGO) algorithm to discover adversarial structures with user-assigned errors. Through uncertainty calibration, the estimated uncertainty of MLIPs is unified with real errors. By performing geometry optimization for calibrated uncertainty, we reach adversarial structures with the user-assigned target MLIP prediction error. Integrating with active learning pipelines, we benchmark CAGO, demonstrating stable MLIPs that systematically converge structural, dynamical, and thermodynamical properties for liquid water and water adsorption in a metal-organic framework within only hundreds of training structures, where previously many thousands were typically required.

## 定量方法(q-bio.QM:Quantitative Methods)

### Exploring proteomic signatures in sepsis and non-infectious systemic inflammatory response syndrome 
[[arxiv](https://arxiv.org/abs/2502.18305)] [[cool](https://papers.cool/arxiv/2502.18305)] [[pdf](https://arxiv.org/pdf/2502.18305)]
> **Authors**: Adolfo Ruiz-Sanmartín,Vicent Ribas,David Suñol,Luis Chiscano-Camón,Laura Martín,Iván Bajaña,Juliana Bastida,Nieves Larrosa,Juan José González,M Dolores Carrasco,Núria Canela,Ricard Ferrer,Juan Carlos Ruiz-Rodrígue
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 定量方法,机器学习
- **Abstract**: Background: The search for new biomarkers that allow an early diagnosis in sepsis has become a necessity in medicine. The objective of this study is to identify potential protein biomarkers of differential expression between sepsis and non-infectious systemic inflammatory response syndrome (NISIRS). Methods: Prospective observational study of a cohort of septic patients activated by the Sepsis Code and patients admitted with NISIRS, during the period 2016-2017. A mass spectrometry-based approach was used to analyze the plasma proteins in the enrolled subjects. Subsequently, using recursive feature elimination (RFE) classification and cross-validation with a vector classifier, an association of these proteins in patients with sepsis compared to patients with NISIRS. The protein-protein interaction network was analyzed with String software. Results: A total of 277 patients (141 with sepsis and 136 with NISIRS) were included. After performing RFE, 25 proteins in the study patient cohort showed statistical significance, with an accuracy of 0.960, specificity of 0.920, sensitivity of 0.973, and an AUC of 0.985. Of these, 14 proteins (vWF, PPBP, C5, C1RL, FCN3, SAA2, ORM1, ITIH3, GSN, C1QA, CA1, CFB, C3, LBP) have a greater relationship with sepsis while 11 proteins (FN1, IGFALS, SERPINA4, APOE, APOH, C6, SERPINA3, AHSG, LUM, ITIH2, SAA1) are more expressed in NISIRS.

## 机器学习(stat.ML:Machine Learning)

### Learning and Computation of $Φ$-Equilibria at the Frontier of Tractability 
[[arxiv](https://arxiv.org/abs/2502.18582)] [[cool](https://papers.cool/arxiv/2502.18582)] [[pdf](https://arxiv.org/pdf/2502.18582)]
> **Authors**: Brian Hu Zhang,Ioannis Anagnostides,Emanuel Tewolde,Ratip Emin Berker,Gabriele Farina,Vincent Conitzer,Tuomas Sandholm
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机科学与博弈论,机器学习
- **Abstract**: $Φ$-equilibria -- and the associated notion of $Φ$-regret -- are a powerful and flexible framework at the heart of online learning and game theory, whereby enriching the set of deviations $Φ$ begets stronger notions of rationality. Recently, Daskalakis, Farina, Fishelson, Pipis, and Schneider (STOC '24) -- abbreviated as DFFPS -- settled the existence of efficient algorithms when $Φ$ contains only linear maps under a general, $d$-dimensional convex constraint set $\mathcal{X}$. In this paper, we significantly extend their work by resolving the case where $Φ$ is $k$-dimensional; degree-$\ell$ polynomials constitute a canonical such example with $k = d^{O(\ell)}$. In particular, positing only oracle access to $\mathcal{X}$, we obtain two main positive results: i) a $\text{poly}(n, d, k, \text{log}(1/ε))$-time algorithm for computing $ε$-approximate $Φ$-equilibria in $n$-player multilinear games, and ii) an efficient online algorithm that incurs average $Φ$-regret at most $ε$ using $\text{poly}(d, k)/ε^2$ rounds. We also show nearly matching lower bounds in the online learning setting, thereby obtaining for the first time a family of deviations that captures the learnability of $Φ$-regret. From a technical standpoint, we extend the framework of DFFPS from linear maps to the more challenging case of maps with polynomial dimension. At the heart of our approach is a polynomial-time algorithm for computing an expected fixed point of any $φ: \mathcal{X} \to \mathcal{X}$ based on the ellipsoid against hope (EAH) algorithm of Papadimitriou and Roughgarden (JACM '08). In particular, our algorithm for computing $Φ$-equilibria is based on executing EAH in a nested fashion -- each step of EAH itself being implemented by invoking a separate call to EAH.

### Applications of Statistical Field Theory in Deep Learning 
[[arxiv](https://arxiv.org/abs/2502.18553)] [[cool](https://papers.cool/arxiv/2502.18553)] [[pdf](https://arxiv.org/pdf/2502.18553)]
> **Authors**: Zohar Ringel,Noa Rubin,Edo Mor,Moritz Helias,Inbar Seroussi
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,无序系统和神经网络,人工智能,机器学习
- **Abstract**: Deep learning algorithms have made incredible strides in the past decade yet due to the complexity of these algorithms, the science of deep learning remains in its early stages. Being an experimentally driven field, it is natural to seek a theory of deep learning within the physics paradigm. As deep learning is largely about learning functions and distributions over functions, statistical field theory, a rich and versatile toolbox for tackling complex distributions over functions (fields) is an obvious choice of formalism. Research efforts carried out in the past few years have demonstrated the ability of field theory to provide useful insights on generalization, implicit bias, and feature learning effects. Here we provide a pedagogical review of this emerging line of research.

## 其他论文

- [Investigating Youth AI Auditing](https://arxiv.org/abs/2502.18576)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC,cs.CY in whitelist
- [When Benchmarks Talk: Re-Evaluating Code LLMs with Interactive Feedback](https://arxiv.org/abs/2502.18413)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- ["Why do we do this?": Moral Stress and the Affective Experience of Ethics in Practice](https://arxiv.org/abs/2502.18395)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Semantic and Goal-oriented Wireless Network Coverage: The Area of Effectiveness](https://arxiv.org/abs/2502.18381)
  - **标题**: None
  - **Filtered Reason**: none of cs.NI,eess.SP in whitelist
- [Responsible AI Agents](https://arxiv.org/abs/2502.18359)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [Exploring K-12 Physical Education Teachers' Perspectives on Opportunities and Challenges of AI Integration Through Ideation Workshops](https://arxiv.org/abs/2502.17855)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
