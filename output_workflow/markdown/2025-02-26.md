> 本文由 [https://github.com/huiyeruzhou/arxiv_crawler](https://github.com/huiyeruzhou/arxiv_crawler) 自动生成
>
> 领域白名单：cs.AI,cs.CL,cs.LG,cs.CV
> 关键词： LLM, GPT, AI, language+model, deep+learning, transformer, neural+network, machine+learning

# 论文全览：2025-02-26

共有340篇相关领域论文, 另有37篇其他

## 天体物理学仪器和方法(astro-ph.IM:Instrumentation and Methods for Astrophysics)

### Transfer Learning for Transient Classification: From Simulations to Real Data and ZTF to LSST 
[[arxiv](https://arxiv.org/abs/2502.18558)] [[cool](https://papers.cool/arxiv/2502.18558)] [[pdf](https://arxiv.org/pdf/2502.18558)]
> **Authors**: Rithwik Gupta,Daniel Muthukrishna
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 6 pages, 3 figures, 1 table
- **标题**: None
- **领域**: 天体物理学仪器和方法,高能天体物理现象,机器学习
- **Abstract**: Machine learning has become essential for automated classification of astronomical transients, but current approaches face significant limitations: classifiers trained on simulations struggle with real data, models developed for one survey cannot be easily applied to another, and new surveys require prohibitively large amounts of labelled training data. These challenges are particularly pressing as we approach the era of the Vera Rubin Observatory's Legacy Survey of Space and Time (LSST), where existing classification models will need to be retrained using LSST observations. We demonstrate that transfer learning can overcome these challenges by repurposing existing models trained on either simulations or data from other surveys. Starting with a model trained on simulated Zwicky Transient Facility (ZTF) light curves, we show that transfer learning reduces the amount of labelled real ZTF transients needed by 75\% while maintaining equivalent performance to models trained from scratch. Similarly, when adapting ZTF models for LSST simulations, transfer learning achieves 95\% of the baseline performance while requiring only 30\% of the training data. These findings have significant implications for the early operations of LSST, suggesting that reliable automated classification will be possible soon after the survey begins, rather than waiting months or years to accumulate sufficient training data.

## 材料科学(cond-mat.mtrl-sci:Materials Science)

### Mind the Gap: Bridging the Divide Between AI Aspirations and the Reality of Autonomous Characterization 
[[arxiv](https://arxiv.org/abs/2502.18604)] [[cool](https://papers.cool/arxiv/2502.18604)] [[pdf](https://arxiv.org/pdf/2502.18604)]
> **Authors**: Grace Guinan,Addison Salvador,Michelle A. Smeaton,Andrew Glaws,Hilary Egan,Brian C. Wyatt,Babak Anasori,Kevin R. Fiedler,Matthew J. Olszta,Steven R. Spurgeon
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 33 pages, 6 figures
- **标题**: None
- **领域**: 材料科学,人工智能
- **Abstract**: What does materials science look like in the "Age of Artificial Intelligence?" Each materials domain-synthesis, characterization, and modeling-has a different answer to this question, motivated by unique challenges and constraints. This work focuses on the tremendous potential of autonomous characterization within electron microscopy. We present our recent advancements in developing domain-aware, multimodal models for microscopy analysis capable of describing complex atomic systems. We then address the critical gap between the theoretical promise of autonomous microscopy and its current practical limitations, showcasing recent successes while highlighting the necessary developments to achieve robust, real-world autonomy.

### Inverse Materials Design by Large Language Model-Assisted Generative Framework 
[[arxiv](https://arxiv.org/abs/2502.18127)] [[cool](https://papers.cool/arxiv/2502.18127)] [[pdf](https://arxiv.org/pdf/2502.18127)]
> **Authors**: Yun Hao,Che Fan,Beilin Ye,Wenhao Lu,Zhen Lu,Peilin Zhao,Zhifeng Gao,Qingyao Wu,Yanhui Liu,Tongqi Wen
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 材料科学,机器学习
- **Abstract**: Deep generative models hold great promise for inverse materials design, yet their efficiency and accuracy remain constrained by data scarcity and model architecture. Here, we introduce AlloyGAN, a closed-loop framework that integrates Large Language Model (LLM)-assisted text mining with Conditional Generative Adversarial Networks (CGANs) to enhance data diversity and improve inverse design. Taking alloy discovery as a case study, AlloyGAN systematically refines material candidates through iterative screening and experimental validation. For metallic glasses, the framework predicts thermodynamic properties with discrepancies of less than 8% from experiments, demonstrating its robustness. By bridging generative AI with domain knowledge and validation workflows, AlloyGAN offers a scalable approach to accelerate the discovery of materials with tailored properties, paving the way for broader applications in materials science.

## 统计力学(cond-mat.stat-mech:Statistical Mechanics)

### Controlling dynamics of stochastic systems with deep reinforcement learning 
[[arxiv](https://arxiv.org/abs/2502.18111)] [[cool](https://papers.cool/arxiv/2502.18111)] [[pdf](https://arxiv.org/pdf/2502.18111)]
> **Authors**: Ruslan Mukhamadiarov
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 8 pages, 3 figures
- **标题**: None
- **领域**: 统计力学,机器学习,计算物理
- **Abstract**: A properly designed controller can help improve the quality of experimental measurements or force a dynamical system to follow a completely new time-evolution path. Recent developments in deep reinforcement learning have made steep advances toward designing effective control schemes for fairly complex systems. However, a general simulation scheme that employs deep reinforcement learning for exerting control in stochastic systems is yet to be established. In this paper, we attempt to further bridge a gap between control theory and deep reinforcement learning by proposing a simulation algorithm that allows achieving control of the dynamics of stochastic systems through the use of trained artificial neural networks. Specifically, we use agent-based simulations where the neural network plays the role of the controller that drives local state-to-state transitions. We demonstrate the workflow and the effectiveness of the proposed control methods by considering the following two stochastic processes: particle coalescence on a lattice and a totally asymmetric exclusion process.

## 人工智能(cs.AI:Artificial Intelligence)

### Data-Efficient Multi-Agent Spatial Planning with LLMs 
[[arxiv](https://arxiv.org/abs/2502.18822)] [[cool](https://papers.cool/arxiv/2502.18822)] [[pdf](https://arxiv.org/pdf/2502.18822)]
> **Authors**: Huangyuan Su,Aaron Walsman,Daniel Garces,Sham Kakade,Stephanie Gil
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,多代理系统
- **Abstract**: In this project, our goal is to determine how to leverage the world-knowledge of pretrained large language models for efficient and robust learning in multiagent decision making. We examine this in a taxi routing and assignment problem where agents must decide how to best pick up passengers in order to minimize overall waiting time. While this problem is situated on a graphical road network, we show that with the proper prompting zero-shot performance is quite strong on this task. Furthermore, with limited fine-tuning along with the one-at-a-time rollout algorithm for look ahead, LLMs can out-compete existing approaches with 50 times fewer environmental interactions. We also explore the benefits of various linguistic prompting approaches and show that including certain easy-to-compute information in the prompt significantly improves performance. Finally, we highlight the LLM's built-in semantic understanding, showing its ability to adapt to environmental factors through simple prompts.

### Holistic Audit Dataset Generation for LLM Unlearning via Knowledge Graph Traversal and Redundancy Removal 
[[arxiv](https://arxiv.org/abs/2502.18810)] [[cool](https://papers.cool/arxiv/2502.18810)] [[pdf](https://arxiv.org/pdf/2502.18810)]
> **Authors**: Weipeng Jiang,Juan Zhai,Shiqing Ma,Ziyan Lei,Xiaofei Xie,Yige Wang,Chao Shen
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 11 pages, 4 figures
- **标题**: None
- **领域**: 人工智能,计算语言学,机器学习
- **Abstract**: In recent years, Large Language Models (LLMs) have faced increasing demands to selectively remove sensitive information, protect privacy, and comply with copyright regulations through unlearning, by Machine Unlearning. While evaluating unlearning effectiveness is crucial, existing benchmarks are limited in scale and comprehensiveness, typically containing only a few hundred test cases. We identify two critical challenges in generating holistic audit datasets: ensuring audit adequacy and handling knowledge redundancy between forget and retain dataset. To address these challenges, we propose HANKER, an automated framework for holistic audit dataset generation leveraging knowledge graphs to achieve fine-grained coverage and eliminate redundant knowledge. Applying HANKER to the popular MUSE benchmark, we successfully generated over 69,000 and 111,000 audit cases for the News and Books datasets respectively, identifying thousands of knowledge memorization instances that the previous benchmark failed to detect. Our empirical analysis uncovers how knowledge redundancy significantly skews unlearning effectiveness metrics, with redundant instances artificially inflating the observed memorization measurements ROUGE from 19.7% to 26.1% and Entailment Scores from 32.4% to 35.2%, highlighting the necessity of systematic deduplication for accurate assessment.

### Like Father, Like Son: Kinship-Aware Preference Mapping (KARMA) for Automatic Alignment in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.18744)] [[cool](https://papers.cool/arxiv/2502.18744)] [[pdf](https://arxiv.org/pdf/2502.18744)]
> **Authors**: Jeesu Jung,Chanjun Park,Sangkeun Jung
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 14 pages,5 figures,3 tables,4 graphs
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: Recent advancements in Large Language Model (LLM) alignment have sought to mitigate the cost of human annotations by leveraging pretrained models to generate preference data. However, existing methods often compare responses from models with substantially different capabilities, yielding superficial distinctions that fail to provide meaningful guidance on what constitutes a superior response. To address this limitation, we propose Kinship-Aware pReference MApping (KARMA), a novel framework that systematically pairs responses from models with comparable competencies. By constraining preference comparisons to outputs of similar complexity and quality, KARMA enhances the informativeness of preference data and improves the granularity of alignment signals. Empirical evaluations demonstrate that our kinship-aware approach leads to more consistent and interpretable alignment outcomes, ultimately facilitating a more principled and reliable pathway for aligning LLM behavior with human preferences.

### Talking to the brain: Using Large Language Models as Proxies to Model Brain Semantic Representation 
[[arxiv](https://arxiv.org/abs/2502.18725)] [[cool](https://papers.cool/arxiv/2502.18725)] [[pdf](https://arxiv.org/pdf/2502.18725)]
> **Authors**: Xin Liu,Ziyue Zhang,Jingxin Nie
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 20 pages, 6 figures
- **标题**: None
- **领域**: 人工智能,计算语言学,神经元和认知
- **Abstract**: Traditional psychological experiments utilizing naturalistic stimuli face challenges in manual annotation and ecological validity. To address this, we introduce a novel paradigm leveraging multimodal large language models (LLMs) as proxies to extract rich semantic information from naturalistic images through a Visual Question Answering (VQA) strategy for analyzing human visual semantic representation. LLM-derived representations successfully predict established neural activity patterns measured by fMRI (e.g., faces, buildings), validating its feasibility and revealing hierarchical semantic organization across cortical regions. A brain semantic network constructed from LLM-derived representations identifies meaningful clusters reflecting functional and contextual associations. This innovative methodology offers a powerful solution for investigating brain semantic organization with naturalistic stimuli, overcoming limitations of traditional annotation methods and paving the way for more ecologically valid explorations of human cognition.

### TrajLLM: A Modular LLM-Enhanced Agent-Based Framework for Realistic Human Trajectory Simulation 
[[arxiv](https://arxiv.org/abs/2502.18712)] [[cool](https://papers.cool/arxiv/2502.18712)] [[pdf](https://arxiv.org/pdf/2502.18712)]
> **Authors**: Chenlu Ju,Jiaxin Liu,Shobhit Sinha,Hao Xue,Flora Salim
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Accepted WWW2025 Demo Paper
- **标题**: None
- **领域**: 人工智能,社交和信息网络
- **Abstract**: This work leverages Large Language Models (LLMs) to simulate human mobility, addressing challenges like high costs and privacy concerns in traditional models. Our hierarchical framework integrates persona generation, activity selection, and destination prediction, using real-world demographic and psychological data to create realistic movement patterns. Both physical models and language models are employed to explore and demonstrate different methodologies for human mobility simulation. By structuring data with summarization and weighted density metrics, the system ensures scalable memory management while retaining actionable insights. Preliminary results indicate that LLM-driven simulations align with observed real-world patterns, offering scalable, interpretable insights for social problems such as urban planning, traffic management, and public health. The framework's ability to dynamically generate personas and activities enables it to provide adaptable and realistic daily routines. This study demonstrates the transformative potential of LLMs in advancing mobility modeling for societal and urban applications. The source code and interactive demo for our framework are available at https://github.com/cju0/TrajLLM.

### Hybrid Voting-Based Task Assignment in Role-Playing Games 
[[arxiv](https://arxiv.org/abs/2502.18690)] [[cool](https://papers.cool/arxiv/2502.18690)] [[pdf](https://arxiv.org/pdf/2502.18690)]
> **Authors**: Daniel Weiner,Raj Korpan
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Accepted for presentation at Dungeons, Neurons, and Dialogues: Social Interaction Dynamics in Contextual Games Workshop at 20th Annual ACM/IEEE International Conference on Human-Robot Interaction (HRI 2025)
- **标题**: None
- **领域**: 人工智能,机器人技术
- **Abstract**: In role-playing games (RPGs), the level of immersion is critical-especially when an in-game agent conveys tasks, hints, or ideas to the player. For an agent to accurately interpret the player's emotional state and contextual nuances, a foundational level of understanding is required, which can be achieved using a Large Language Model (LLM). Maintaining the LLM's focus across multiple context changes, however, necessitates a more robust approach, such as integrating the LLM with a dedicated task allocation model to guide its performance throughout gameplay. In response to this need, we introduce Voting-Based Task Assignment (VBTA), a framework inspired by human reasoning in task allocation and completion. VBTA assigns capability profiles to agents and task descriptions to tasks, then generates a suitability matrix that quantifies the alignment between an agent's abilities and a task's requirements. Leveraging six distinct voting methods, a pre-trained LLM, and integrating conflict-based search (CBS) for path planning, VBTA efficiently identifies and assigns the most suitable agent to each task. While existing approaches focus on generating individual aspects of gameplay, such as single quests, or combat encounters, our method shows promise when generating both unique combat encounters and narratives because of its generalizable nature.

### Speaking the Right Language: The Impact of Expertise Alignment in User-AI Interactions 
[[arxiv](https://arxiv.org/abs/2502.18685)] [[cool](https://papers.cool/arxiv/2502.18685)] [[pdf](https://arxiv.org/pdf/2502.18685)]
> **Authors**: Shramay Palta,Nirupama Chandrasekaran,Rachel Rudinger,Scott Counts
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: arXiv Version
- **标题**: None
- **领域**: 人工智能,计算语言学,人机交互
- **Abstract**: Using a sample of 25,000 Bing Copilot conversations, we study how the agent responds to users of varying levels of domain expertise and the resulting impact on user experience along multiple dimensions. Our findings show that across a variety of topical domains, the agent largely responds at proficient or expert levels of expertise (77% of conversations) which correlates with positive user experience regardless of the user's level of expertise. Misalignment, such that the agent responds at a level of expertise below that of the user, has a negative impact on overall user experience, with the impact more profound for more complex tasks. We also show that users engage more, as measured by the number of words in the conversation, when the agent responds at a level of expertise commensurate with that of the user. Our findings underscore the importance of alignment between user and AI when designing human-centered AI systems, to ensure satisfactory and productive interactions.

### Independent Mobility GPT (IDM-GPT): A Self-Supervised Multi-Agent Large Language Model Framework for Customized Traffic Mobility Analysis Using Machine Learning Models 
[[arxiv](https://arxiv.org/abs/2502.18652)] [[cool](https://papers.cool/arxiv/2502.18652)] [[pdf](https://arxiv.org/pdf/2502.18652)]
> **Authors**: Fengze Yang,Xiaoyue Cathy Liu,Lingjiu Lu,Bingzhang Wang,Chenxi,Liu
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 24 pages, 4 figures, TRR accepted
- **标题**: None
- **领域**: 人工智能,计算机与社会
- **Abstract**: With the urbanization process, an increasing number of sensors are being deployed in transportation systems, leading to an explosion of big data. To harness the power of this vast transportation data, various machine learning (ML) and artificial intelligence (AI) methods have been introduced to address numerous transportation challenges. However, these methods often require significant investment in data collection, processing, storage, and the employment of professionals with expertise in transportation and ML. Additionally, privacy issues are a major concern when processing data for real-world traffic control and management. To address these challenges, the research team proposes an innovative Multi-agent framework named Independent Mobility GPT (IDM-GPT) based on large language models (LLMs) for customized traffic analysis, management suggestions, and privacy preservation. IDM-GPT efficiently connects users, transportation databases, and ML models economically. IDM-GPT trains, customizes, and applies various LLM-based AI agents for multiple functions, including user query comprehension, prompts optimization, data analysis, model selection, and performance evaluation and enhancement. With IDM-GPT, users without any background in transportation or ML can efficiently and intuitively obtain data analysis and customized suggestions in near real-time based on their questions. Experimental results demonstrate that IDM-GPT delivers satisfactory performance across multiple traffic-related tasks, providing comprehensive and actionable insights that support effective traffic management and urban mobility improvement.

### Automated Knowledge Component Generation and Knowledge Tracing for Coding Problems 
[[arxiv](https://arxiv.org/abs/2502.18632)] [[cool](https://papers.cool/arxiv/2502.18632)] [[pdf](https://arxiv.org/pdf/2502.18632)]
> **Authors**: Zhangqi Duan,Nigel Fernandez,Sri Kanakadandi,Bita Akram,Andrew Lan
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学,计算机与社会,机器学习,软件工程
- **Abstract**: Knowledge components (KCs) mapped to problems help model student learning, tracking their mastery levels on fine-grained skills thereby facilitating personalized learning and feedback in online learning platforms. However, crafting and tagging KCs to problems, traditionally performed by human domain experts, is highly labor-intensive. We present a fully automated, LLM-based pipeline for KC generation and tagging for open-ended programming problems. We also develop an LLM-based knowledge tracing (KT) framework to leverage these LLM-generated KCs, which we refer to as KCGen-KT. We conduct extensive quantitative and qualitative evaluations validating the effectiveness of KCGen-KT. On a real-world dataset of student code submissions to open-ended programming problems, KCGen-KT outperforms existing KT methods. We investigate the learning curves of generated KCs and show that LLM-generated KCs have a comparable level-of-fit to human-written KCs under the performance factor analysis (PFA) model. We also conduct a human evaluation to show that the KC tagging accuracy of our pipeline is reasonably accurate when compared to that by human domain experts.

### CuDIP: Enhancing Theorem Proving in LLMs via Curriculum Learning-based Direct Preference Optimization 
[[arxiv](https://arxiv.org/abs/2502.18532)] [[cool](https://papers.cool/arxiv/2502.18532)] [[pdf](https://arxiv.org/pdf/2502.18532)]
> **Authors**: Shuming Shi,Ruobing Zuo,Gaolei He,Jianlin Wang,Chenyang Xu,Zhengfeng Yang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: Automated theorem proving (ATP) is one of the most challenging mathematical reasoning tasks for Large Language Models (LLMs). Most existing LLM-based ATP methods rely on supervised fine-tuning, which results in a limited alignment between the theorem proving process and human preferences. Direct Preference Optimization (DPO), which aligns LLMs with human preferences, has shown positive effects for certain tasks. However, the lack of high-quality preference data for theorem proving presents a significant challenge. In this paper, we innovatively apply DPO to formal automated theorem proving and introduces a Curriculum Learning-based DPO Iterative Theorem Proving (CuDIP) method. Specifically, we propose a method for constructing preference data which utilizes LLMs and existing theorem proving data to enhance the diversity of the preference data while reducing the reliance on human preference annotations. We then integrate this preference data construction method with curriculum learning to iteratively fine-tune the theorem proving model through DPO. Experimental results on the MiniF2F and ProofNet datasets demonstrate the effectiveness of the proposed method.

### Enhancing Hepatopathy Clinical Trial Efficiency: A Secure, Large Language Model-Powered Pre-Screening Pipeline 
[[arxiv](https://arxiv.org/abs/2502.18531)] [[cool](https://papers.cool/arxiv/2502.18531)] [[pdf](https://arxiv.org/pdf/2502.18531)]
> **Authors**: Xiongbin Gui,Hanlin Lv,Xiao Wang,Longting Lv,Yi Xiao,Lei Wang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-26
> **comment**: 30 pages, 5 figures
- **标题**: None
- **领域**: 人工智能,计算语言学,机器学习
- **Abstract**: Background: Recruitment for cohorts involving complex liver diseases, such as hepatocellular carcinoma and liver cirrhosis, often requires interpreting semantically complex criteria. Traditional manual screening methods are time-consuming and prone to errors. While AI-powered pre-screening offers potential solutions, challenges remain regarding accuracy, efficiency, and data privacy. Methods: We developed a novel patient pre-screening pipeline that leverages clinical expertise to guide the precise, safe, and efficient application of large language models. The pipeline breaks down complex criteria into a series of composite questions and then employs two strategies to perform semantic question-answering through electronic health records - (1) Pathway A, Anthropomorphized Experts' Chain of Thought strategy, and (2) Pathway B, Preset Stances within an Agent Collaboration strategy, particularly in managing complex clinical reasoning scenarios. The pipeline is evaluated on three key metrics-precision, time consumption, and counterfactual inference - at both the question and criterion levels. Results: Our pipeline achieved high precision (0.921, in criteria level) and efficiency (0.44s per task). Pathway B excelled in complex reasoning, while Pathway A was effective in precise data extraction with faster processing times. Both pathways achieved comparable precision. The pipeline showed promising results in hepatocellular carcinoma (0.878) and cirrhosis trials (0.843). Conclusions: This data-secure and time-efficient pipeline shows high precision in hepatopathy trials, providing promising solutions for streamlining clinical trial workflows. Its efficiency and adaptability make it suitable for improving patient recruitment. And its capability to function in resource-constrained environments further enhances its utility in clinical settings.

### MAPoRL: Multi-Agent Post-Co-Training for Collaborative Large Language Models with Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.18439)] [[cool](https://papers.cool/arxiv/2502.18439)] [[pdf](https://arxiv.org/pdf/2502.18439)]
> **Authors**: Chanwoo Park,Seungju Han,Xingzhi Guo,Asuman Ozdaglar,Kaiqing Zhang,Joo-Kyung Kim
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Leveraging multiple large language models (LLMs) to build collaborative multi-agentic workflows has demonstrated significant potential. However, most previous studies focus on prompting the out-of-the-box LLMs, relying on their innate capability for collaboration, which may not improve LLMs' performance as shown recently. In this paper, we introduce a new post-training paradigm MAPoRL (Multi-Agent Post-co-training for collaborative LLMs with Reinforcement Learning), to explicitly elicit the collaborative behaviors and further unleash the power of multi-agentic LLM frameworks. In MAPoRL, multiple LLMs first generate their own responses independently and engage in a multi-turn discussion to collaboratively improve the final answer. In the end, a MAPoRL verifier evaluates both the answer and the discussion, by assigning a score that verifies the correctness of the answer, while adding incentives to encourage corrective and persuasive discussions. The score serves as the co-training reward, and is then maximized through multi-agent RL. Unlike existing LLM post-training paradigms, MAPoRL advocates the co-training of multiple LLMs together using RL for better generalization. Accompanied by analytical insights, our experiments demonstrate that training individual LLMs alone is insufficient to induce effective collaboration. In contrast, multi-agent co-training can boost the collaboration performance across benchmarks, with generalization to unseen domains.

### PyEvalAI: AI-assisted evaluation of Jupyter Notebooks for immediate personalized feedback 
[[arxiv](https://arxiv.org/abs/2502.18425)] [[cool](https://papers.cool/arxiv/2502.18425)] [[pdf](https://arxiv.org/pdf/2502.18425)]
> **Authors**: Nils Wandel,David Stotko,Alexander Schier,Reinhard Klein
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Grading student assignments in STEM courses is a laborious and repetitive task for tutors, often requiring a week to assess an entire class. For students, this delay of feedback prevents iterating on incorrect solutions, hampers learning, and increases stress when exercise scores determine admission to the final exam. Recent advances in AI-assisted education, such as automated grading and tutoring systems, aim to address these challenges by providing immediate feedback and reducing grading workload. However, existing solutions often fall short due to privacy concerns, reliance on proprietary closed-source models, lack of support for combining Markdown, LaTeX and Python code, or excluding course tutors from the grading process. To overcome these limitations, we introduce PyEvalAI, an AI-assisted evaluation system, which automatically scores Jupyter notebooks using a combination of unit tests and a locally hosted language model to preserve privacy. Our approach is free, open-source, and ensures tutors maintain full control over the grading process. A case study demonstrates its effectiveness in improving feedback speed and grading efficiency for exercises in a university-level course on numerics.

### The Gradient of Algebraic Model Counting 
[[arxiv](https://arxiv.org/abs/2502.18406)] [[cool](https://papers.cool/arxiv/2502.18406)] [[pdf](https://arxiv.org/pdf/2502.18406)]
> **Authors**: Jaron Maene,Luc De Raedt
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Published at AAAI 2025
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: Algebraic model counting unifies many inference tasks on logic formulas by exploiting semirings. Rather than focusing on inference, we consider learning, especially in statistical-relational and neurosymbolic AI, which combine logical, probabilistic and neural representations. Concretely, we show that the very same semiring perspective of algebraic model counting also applies to learning. This allows us to unify various learning algorithms by generalizing gradients and backpropagation to different semirings. Furthermore, we show how cancellation and ordering properties of a semiring can be exploited for more memory-efficient backpropagation. This allows us to obtain some interesting variations of state-of-the-art gradient-based optimisation methods for probabilistic logical models. We also discuss why algebraic model counting on tractable circuits does not lead to more efficient second-order optimization. Empirically, our algebraic backpropagation exhibits considerable speed-ups as compared to existing approaches.

### How Far are LLMs from Real Search? A Comprehensive Study on Efficiency, Completeness, and Inherent Capabilities 
[[arxiv](https://arxiv.org/abs/2502.18387)] [[cool](https://papers.cool/arxiv/2502.18387)] [[pdf](https://arxiv.org/pdf/2502.18387)]
> **Authors**: Minhua Lin,Hui Liu,Xianfeng Tang,Jingying Zeng,Zhenwei Dai,Chen Luo,Zheng Li,Xiang Zhang,Qi He,Suhang Wang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 31 pages, 9 figures, 18 tables
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Search plays a fundamental role in problem-solving across various domains, with most real-world decision-making problems being solvable through systematic search. Drawing inspiration from recent discussions on search and learning, we systematically explore the complementary relationship between search and Large Language Models (LLMs) from three perspectives. First, we analyze how learning can enhance search efficiency and propose Search via Learning (SeaL), a framework that leverages LLMs for effective and efficient search. Second, we further extend SeaL to SeaL-C to ensure rigorous completeness during search. Our evaluation across three real-world planning tasks demonstrates that SeaL achieves near-perfect accuracy while reducing search spaces by up to 99.1% compared to traditional approaches. Finally, we explore how far LLMs are from real search by investigating whether they can develop search capabilities independently. Our analysis reveals that while current LLMs struggle with efficient search in complex problems, incorporating systematic search strategies significantly enhances their problem-solving capabilities. These findings not only validate the effectiveness of our approach but also highlight the need for improving LLMs' search abilities for real-world applications.

### MindMem: Multimodal for Predicting Advertisement Memorability Using LLMs and Deep Learning 
[[arxiv](https://arxiv.org/abs/2502.18371)] [[cool](https://papers.cool/arxiv/2502.18371)] [[pdf](https://arxiv.org/pdf/2502.18371)]
> **Authors**: Sepehr Asgarian,Qayam Jetha,Jouhyun Jeon
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 7 pages, 5 figures, 4 Tables, AAAI 2025 Economics of Modern ML: Markets, Incentives, and GenerativeAIWorkshop
- **标题**: None
- **领域**: 人工智能
- **Abstract**: In the competitive landscape of advertising, success hinges on effectively navigating and leveraging complex interactions among consumers, advertisers, and advertisement platforms. These multifaceted interactions compel advertisers to optimize strategies for modeling consumer behavior, enhancing brand recall, and tailoring advertisement content. To address these challenges, we present MindMem, a multimodal predictive model for advertisement memorability. By integrating textual, visual, and auditory data, MindMem achieves state-of-the-art performance, with a Spearman's correlation coefficient of 0.631 on the LAMBDA and 0.731 on the Memento10K dataset, consistently surpassing existing methods. Furthermore, our analysis identified key factors influencing advertisement memorability, such as video pacing, scene complexity, and emotional resonance. Expanding on this, we introduced MindMem-ReAd (MindMem-Driven Re-generated Advertisement), which employs Large Language Model-based simulations to optimize advertisement content and placement, resulting in up to a 74.12% improvement in advertisement memorability. Our results highlight the transformative potential of Artificial Intelligence in advertising, offering advertisers a robust tool to drive engagement, enhance competitiveness, and maximize impact in a rapidly evolving market.

### GraphRank Pro+: Advancing Talent Analytics Through Knowledge Graphs and Sentiment-Enhanced Skill Profiling 
[[arxiv](https://arxiv.org/abs/2502.18315)] [[cool](https://papers.cool/arxiv/2502.18315)] [[pdf](https://arxiv.org/pdf/2502.18315)]
> **Authors**: Sirisha Velampalli,Chandrashekar Muniyappa
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: :05C81ACM Class:I.2.7
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: The extraction of information from semi-structured text, such as resumes, has long been a challenge due to the diverse formatting styles and subjective content organization. Conventional solutions rely on specialized logic tailored for specific use cases. However, we propose a revolutionary approach leveraging structured Graphs, Natural Language Processing (NLP), and Deep Learning. By abstracting intricate logic into Graph structures, we transform raw data into a comprehensive Knowledge Graph. This innovative framework enables precise information extraction and sophisticated querying. We systematically construct dictionaries assigning skill weights, paving the way for nuanced talent analysis. Our system not only benefits job recruiters and curriculum designers but also empowers job seekers with targeted query-based filtering and ranking capabilities.

### Citrus: Leveraging Expert Cognitive Pathways in a Medical Language Model for Advanced Medical Decision Support 
[[arxiv](https://arxiv.org/abs/2502.18274)] [[cool](https://papers.cool/arxiv/2502.18274)] [[pdf](https://arxiv.org/pdf/2502.18274)]
> **Authors**: Guoxin Wang,Minyu Gao,Shuai Yang,Ya Zhang,Lizhi He,Liang Huang,Hanlin Xiao,Yexuan Zhang,Wanyue Li,Lu Chen,Jintao Fei,Xin Li
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: Large language models (LLMs), particularly those with reasoning capabilities, have rapidly advanced in recent years, demonstrating significant potential across a wide range of applications. However, their deployment in healthcare, especially in disease reasoning tasks, is hindered by the challenge of acquiring expert-level cognitive data. In this paper, we introduce Citrus, a medical language model that bridges the gap between clinical expertise and AI reasoning by emulating the cognitive processes of medical experts. The model is trained on a large corpus of simulated expert disease reasoning data, synthesized using a novel approach that accurately captures the decision-making pathways of clinicians. This approach enables Citrus to better simulate the complex reasoning processes involved in diagnosing and treating medical conditions. To further address the lack of publicly available datasets for medical reasoning tasks, we release the last-stage training data, including a custom-built medical diagnostic dialogue dataset. This open-source contribution aims to support further research and development in the field. Evaluations using authoritative benchmarks such as MedQA, covering tasks in medical reasoning and language understanding, show that Citrus achieves superior performance compared to other models of similar size. These results highlight Citrus potential to significantly enhance medical decision support systems, providing a more accurate and efficient tool for clinical decision-making.

### ChatMotion: A Multimodal Multi-Agent for Human Motion Analysis 
[[arxiv](https://arxiv.org/abs/2502.18180)] [[cool](https://papers.cool/arxiv/2502.18180)] [[pdf](https://arxiv.org/pdf/2502.18180)]
> **Authors**: Lei Li,Sen Jia,Jianhao Wang,Zhaochong An,Jiaang Li,Jenq-Neng Hwang,Serge Belongie
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,多代理系统
- **Abstract**: Advancements in Multimodal Large Language Models (MLLMs) have improved human motion understanding. However, these models remain constrained by their "instruct-only" nature, lacking interactivity and adaptability for diverse analytical perspectives. To address these challenges, we introduce ChatMotion, a multimodal multi-agent framework for human motion analysis. ChatMotion dynamically interprets user intent, decomposes complex tasks into meta-tasks, and activates specialized function modules for motion comprehension. It integrates multiple specialized modules, such as the MotionCore, to analyze human motion from various perspectives. Extensive experiments demonstrate ChatMotion's precision, adaptability, and user engagement for human motion understanding.

### Defining bias in AI-systems: Biased models are fair models 
[[arxiv](https://arxiv.org/abs/2502.18060)] [[cool](https://papers.cool/arxiv/2502.18060)] [[pdf](https://arxiv.org/pdf/2502.18060)]
> **Authors**: Chiara Lindloff,Ingo Siegert
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: ISCA/ITG Workshop on Diversity in Large Speech andLanguageModels
- **标题**: None
- **领域**: 人工智能,计算语言学,计算机与社会
- **Abstract**: The debate around bias in AI systems is central to discussions on algorithmic fairness. However, the term bias often lacks a clear definition, despite frequently being contrasted with fairness, implying that an unbiased model is inherently fair. In this paper, we challenge this assumption and argue that a precise conceptualization of bias is necessary to effectively address fairness concerns. Rather than viewing bias as inherently negative or unfair, we highlight the importance of distinguishing between bias and discrimination. We further explore how this shift in focus can foster a more constructive discourse within academic debates on fairness in AI systems.

### GNN-XAR: A Graph Neural Network for Explainable Activity Recognition in Smart Homes 
[[arxiv](https://arxiv.org/abs/2502.17999)] [[cool](https://papers.cool/arxiv/2502.17999)] [[pdf](https://arxiv.org/pdf/2502.17999)]
> **Authors**: Michele Fiori,Davide Mor,Gabriele Civitarese,Claudio Bettini
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: This is a preprint. Paper accepted for publication at the 21st EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services (Mobiquitous)
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: Sensor-based Human Activity Recognition (HAR) in smart home environments is crucial for several applications, especially in the healthcare domain. The majority of the existing approaches leverage deep learning models. While these approaches are effective, the rationale behind their outputs is opaque. Recently, eXplainable Artificial Intelligence (XAI) approaches emerged to provide intuitive explanations to the output of HAR models. To the best of our knowledge, these approaches leverage classic deep models like CNNs or RNNs. Recently, Graph Neural Networks (GNNs) proved to be effective for sensor-based HAR. However, existing approaches are not designed with explainability in mind. In this work, we propose the first explainable Graph Neural Network explicitly designed for smart home HAR. Our results on two public datasets show that this approach provides better explanations than state-of-the-art methods while also slightly improving the recognition rate.

### LeanProgress: Guiding Search for Neural Theorem Proving via Proof Progress Prediction 
[[arxiv](https://arxiv.org/abs/2502.17925)] [[cool](https://papers.cool/arxiv/2502.17925)] [[pdf](https://arxiv.org/pdf/2502.17925)]
> **Authors**: Suozhi Huang,Peiyang Song,Robert Joseph George,Anima Anandkumar
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Mathematical reasoning remains a significant challenge for Large Language Models (LLMs) due to hallucinations. When combined with formal proof assistants like Lean, these hallucinations can be eliminated through rigorous verification, making theorem proving reliable. However, even with formal verification, LLMs still struggle with long proofs and complex mathematical formalizations. While Lean with LLMs offers valuable assistance with retrieving lemmas, generating tactics, or even complete proofs, it lacks a crucial capability: providing a sense of proof progress. This limitation particularly impacts the overall development efficiency in large formalization projects. We introduce LeanProgress, a method that predicts the progress in the proof. Training and evaluating our models made on a large corpus of Lean proofs from Lean Workbook Plus and Mathlib4 and how many steps remain to complete it, we employ data preprocessing and balancing techniques to handle the skewed distribution of proof lengths. Our experiments show that LeanProgress achieves an overall prediction accuracy of 75.1\% in predicting the amount of progress and, hence, the remaining number of steps. When integrated into a best-first search framework using Reprover, our method shows a 3.8\% improvement on Mathlib4 compared to baseline performances of 41.2\%, particularly for longer proofs. These results demonstrate how proof progress prediction can enhance both automated and interactive theorem proving, enabling users to make more informed decisions about proof strategies.

### Towards Sustainable Web Agents: A Plea for Transparency and Dedicated Metrics for Energy Consumption 
[[arxiv](https://arxiv.org/abs/2502.17903)] [[cool](https://papers.cool/arxiv/2502.17903)] [[pdf](https://arxiv.org/pdf/2502.17903)]
> **Authors**: Lars Krupp,Daniel Geißler,Paul Lukowicz,Jakob Karolus
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,人机交互
- **Abstract**: Improvements in the area of large language models have shifted towards the construction of models capable of using external tools and interpreting their outputs. These so-called web agents have the ability to interact autonomously with the internet. This allows them to become powerful daily assistants handling time-consuming, repetitive tasks while supporting users in their daily activities. While web agent research is thriving, the sustainability aspect of this research direction remains largely unexplored. We provide an initial exploration of the energy and CO2 cost associated with web agents. Our results show how different philosophies in web agent creation can severely impact the associated expended energy. We highlight lacking transparency regarding the disclosure of model parameters and processes used for some web agents as a limiting factor when estimating energy consumption. As such, our work advocates a change in thinking when evaluating web agents, warranting dedicated metrics for energy consumption and sustainability.

### Science Across Languages: Assessing LLM Multilingual Translation of Scientific Papers 
[[arxiv](https://arxiv.org/abs/2502.17882)] [[cool](https://papers.cool/arxiv/2502.17882)] [[pdf](https://arxiv.org/pdf/2502.17882)]
> **Authors**: Hannah Calzi Kleidermacher,James Zou
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: Scientific research is inherently global. However, the vast majority of academic journals are published exclusively in English, creating barriers for non-native-English-speaking researchers. In this study, we leverage large language models (LLMs) to translate published scientific articles while preserving their native JATS XML formatting, thereby developing a practical, automated approach for implementation by academic journals. Using our approach, we translate articles across multiple scientific disciplines into 28 languages. To evaluate translation accuracy, we introduce a novel question-and-answer (QA) benchmarking method, in which an LLM generates comprehension-based questions from the original text and then answers them based on the translated text. Our benchmark results show an average performance of 95.9%, showing that the key scientific details are accurately conveyed. In a user study, we translate the scientific papers of 15 researchers into their native languages, finding that the authors consistently found the translations to accurately capture the original information in their articles. Interestingly, a third of the authors found many technical terms "overtranslated," expressing a preference to keep terminology more familiar in English untranslated. Finally, we demonstrate how in-context learning techniques can be used to align translations with domain-specific preferences such as mitigating overtranslation, highlighting the adaptability and utility of LLM-driven scientific translation. The code and translated articles are available at https://hankleid.github.io/ProjectMundo.

## 计算语言学(cs.CL:Computation and Language)

### Evidence-Driven Marker Extraction for Social Media Suicide Risk Detection 
[[arxiv](https://arxiv.org/abs/2502.18823)] [[cool](https://papers.cool/arxiv/2502.18823)] [[pdf](https://arxiv.org/pdf/2502.18823)]
> **Authors**: Carter Adams,Caleb Carter,Jackson Simmons
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Early detection of suicide risk from social media text is crucial for timely intervention. While Large Language Models (LLMs) offer promising capabilities in this domain, challenges remain in terms of interpretability and computational efficiency. This paper introduces Evidence-Driven LLM (ED-LLM), a novel approach for clinical marker extraction and suicide risk classification. ED-LLM employs a multi-task learning framework, jointly training a Mistral-7B based model to identify clinical marker spans and classify suicide risk levels. This evidence-driven strategy enhances interpretability by explicitly highlighting textual evidence supporting risk assessments. Evaluated on the CLPsych datasets, ED-LLM demonstrates competitive performance in risk classification and superior capability in clinical marker span identification compared to baselines including fine-tuned LLMs, traditional machine learning, and prompt-based methods. The results highlight the effectiveness of multi-task learning for interpretable and efficient LLM-based suicide risk assessment, paving the way for clinically relevant applications.

### Judge as A Judge: Improving the Evaluation of Retrieval-Augmented Generation through the Judge-Consistency of Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.18817)] [[cool](https://papers.cool/arxiv/2502.18817)] [[pdf](https://arxiv.org/pdf/2502.18817)]
> **Authors**: Shuliang Liu,Xinze Li,Zhenghao Liu,Yukun Yan,Cheng Yang,Zheni Zeng,Zhiyuan Liu,Maosong Sun,Ge Yu
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Retrieval-Augmented Generation (RAG) has proven its effectiveness in alleviating hallucinations for Large Language Models (LLMs). However, existing automated evaluation metrics cannot fairly evaluate the outputs generated by RAG models during training and evaluation. LLM-based judgment models provide the potential to produce high-quality judgments, but they are highly sensitive to evaluation prompts, leading to inconsistencies when judging the output of RAG models. This paper introduces the Judge-Consistency (ConsJudge) method, which aims to enhance LLMs to generate more accurate evaluations for RAG models. Specifically, ConsJudge prompts LLMs to generate different judgments based on various combinations of judgment dimensions, utilize the judge-consistency to evaluate these judgments and select the accepted and rejected judgments for DPO training. Our experiments show that ConsJudge can effectively provide more accurate judgments for optimizing RAG models across various RAG models and datasets. Further analysis reveals that judgments generated by ConsJudge have a high agreement with the superior LLM. All codes are available at https://github.com/OpenBMB/ConsJudge.

### Language Models Grow Less Humanlike beyond Phase Transition 
[[arxiv](https://arxiv.org/abs/2502.18802)] [[cool](https://papers.cool/arxiv/2502.18802)] [[pdf](https://arxiv.org/pdf/2502.18802)]
> **Authors**: Tatsuya Aoyama,Ethan Wilcox
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: LMs' alignment with human reading behavior (i.e. psychometric predictive power; PPP) is known to improve during pretraining up to a tipping point, beyond which it either plateaus or degrades. Various factors, such as word frequency, recency bias in attention, and context size, have been theorized to affect PPP, yet there is no current account that explains why such a tipping point exists, and how it interacts with LMs' pretraining dynamics more generally. We hypothesize that the underlying factor is a pretraining phase transition, characterized by the rapid emergence of specialized attention heads. We conduct a series of correlational and causal experiments to show that such a phase transition is responsible for the tipping point in PPP. We then show that, rather than producing attention patterns that contribute to the degradation in PPP, phase transitions alter the subsequent learning dynamics of the model, such that further training keeps damaging PPP.

### ANPMI: Assessing the True Comprehension Capabilities of LLMs for Multiple Choice Questions 
[[arxiv](https://arxiv.org/abs/2502.18798)] [[cool](https://papers.cool/arxiv/2502.18798)] [[pdf](https://arxiv.org/pdf/2502.18798)]
> **Authors**: Gyeongje Cho,Yeonkyoung So,Jaejin Lee
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Multiple-choice benchmarks, consisting of various prompts and choices, are among the most widely used methods to assess a language model's natural language understanding capability. Given a specific prompt, we typically compute $P(Choice|Prompt)$ to evaluate how likely a language model is to generate the correct choice compared to incorrect ones. However, we observe that performance measured using this approach reflects not only the model's comprehension of the prompt but also its inherent biases for certain choices regardless of the prompt. This issue makes it challenging to accurately measure a model's natural language understanding, as models may select the answer without fully understanding the prompt. To address this limitation, we propose a novel metric called ANPMI, which normalizes Pointwise Mutual Information (PMI) by $-\log P(Choice)$. ANPMI provides a more accurate assessment of the model's natural language understanding by ensuring that it is challenging to answer a question without properly understanding the prompt.

### Anything Goes? A Crosslinguistic Study of (Im)possible Language Learning in LMs 
[[arxiv](https://arxiv.org/abs/2502.18795)] [[cool](https://papers.cool/arxiv/2502.18795)] [[pdf](https://arxiv.org/pdf/2502.18795)]
> **Authors**: Xiulin Yang,Tatsuya Aoyama,Yuekun Yao,Ethan Wilcox
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Do LLMs offer insights into human language learning? A common argument against this idea is that because their architecture and training paradigm are so vastly different from humans, LLMs can learn arbitrary inputs as easily as natural languages. In this paper, we test this claim by training LMs to model impossible and typologically unattested languages. Unlike previous work, which has focused exclusively on English, we conduct experiments on 12 natural languages from 4 language families. Our results show that while GPT-2 small can primarily distinguish attested languages from their impossible counterparts, it does not achieve perfect separation between all the attested languages and all the impossible ones. We further test whether GPT-2 small distinguishes typologically attested from unattested languages with different NP orders by manipulating word order based on Greenberg's Universal 20. We find that the model's perplexity scores do not distinguish attested vs. unattested word orders, as long as the unattested variants maintain constituency structure. These findings suggest that language models exhibit some human-like inductive biases, though these biases are weaker than those found in human learners.

### Seeing the Forest for the Trees: A Large Scale, Continuously Updating Meta-Analysis of Frontier LLMs 
[[arxiv](https://arxiv.org/abs/2502.18791)] [[cool](https://papers.cool/arxiv/2502.18791)] [[pdf](https://arxiv.org/pdf/2502.18791)]
> **Authors**: Jungsoo Park,Junmo Kang,Gabriel Stanovsky,Alan Ritter
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 21 pages, 9 figures
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: The surge of LLM studies makes synthesizing their findings challenging. Meta-analysis can uncover important trends across studies, but its use is limited by the time-consuming nature of manual data extraction. Our study presents a semi-automated approach for meta-analysis that accelerates data extraction using LLMs. It automatically identifies relevant arXiv papers, extracts experimental results and related attributes, and organizes them into a structured dataset. We conduct a comprehensive meta-analysis of frontier LLMs using an automatically extracted dataset, reducing the effort of paper surveying and data extraction by more than 93\% compared to manual approaches. We validate our dataset by showing that it reproduces key findings from a recent manual meta-analysis about Chain-of-Thought (CoT), and also uncovers new insights that go beyond it, showing for example that in-context examples benefit multimodal tasks but offer limited gains in mathematical tasks compared to CoT. Our automatically updatable dataset enables continuous tracking of target models by extracting evaluation studies as new data becomes available. Through our scientific artifacts and empirical analysis, we provide novel insights into LLMs while facilitating ongoing meta-analyses of their behavior.

### Active Few-Shot Learning for Text Classification 
[[arxiv](https://arxiv.org/abs/2502.18782)] [[cool](https://papers.cool/arxiv/2502.18782)] [[pdf](https://arxiv.org/pdf/2502.18782)]
> **Authors**: Saeed Ahmadnia,Arash Yousefi Jordehi,Mahsa Hosseini Khasheh Heyran,Seyed Abolghasem Mirroshandel,Owen Rambow,Cornelia Caragea
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Accepted to NAACL 2025 Main Conference; 18 pages, 8 figures, 13 tables including Appendix
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The rise of Large Language Models (LLMs) has boosted the use of Few-Shot Learning (FSL) methods in natural language processing, achieving acceptable performance even when working with limited training data. The goal of FSL is to effectively utilize a small number of annotated samples in the learning process. However, the performance of FSL suffers when unsuitable support samples are chosen. This problem arises due to the heavy reliance on a limited number of support samples, which hampers consistent performance improvement even when more support samples are added. To address this challenge, we propose an active learning-based instance selection mechanism that identifies effective support instances from the unlabeled pool and can work with different LLMs. Our experiments on five tasks show that our method frequently improves the performance of FSL. We make our implementation available on GitHub.

### Towards Optimal Multi-draft Speculative Decoding 
[[arxiv](https://arxiv.org/abs/2502.18779)] [[cool](https://papers.cool/arxiv/2502.18779)] [[pdf](https://arxiv.org/pdf/2502.18779)]
> **Authors**: Zhengmian Hu,Tong Zheng,Vignesh Viswanathan,Ziyi Chen,Ryan A. Rossi,Yihan Wu,Dinesh Manocha,Heng Huang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,数据结构和算法
- **Abstract**: Large Language Models (LLMs) have become an indispensable part of natural language processing tasks. However, autoregressive sampling has become an efficiency bottleneck. Multi-Draft Speculative Decoding (MDSD) is a recent approach where, when generating each token, a small draft model generates multiple drafts, and the target LLM verifies them in parallel, ensuring that the final output conforms to the target model distribution. The two main design choices in MDSD are the draft sampling method and the verification algorithm. For a fixed draft sampling method, the optimal acceptance rate is a solution to an optimal transport problem, but the complexity of this problem makes it difficult to solve for the optimal acceptance rate and measure the gap between existing verification algorithms and the theoretical upper bound. This paper discusses the dual of the optimal transport problem, providing a way to efficiently compute the optimal acceptance rate. For the first time, we measure the theoretical upper bound of MDSD efficiency for vocabulary sizes in the thousands and quantify the gap between existing verification algorithms and this bound. We also compare different draft sampling methods based on their optimal acceptance rates. Our results show that the draft sampling method strongly influences the optimal acceptance rate, with sampling without replacement outperforming sampling with replacement. Additionally, existing verification algorithms do not reach the theoretical upper bound for both without replacement and with replacement sampling. Our findings suggest that carefully designed draft sampling methods can potentially improve the optimal acceptance rate and enable the development of verification algorithms that closely match the theoretical upper bound.

### Plutus: Benchmarking Large Language Models in Low-Resource Greek Finance 
[[arxiv](https://arxiv.org/abs/2502.18772)] [[cool](https://papers.cool/arxiv/2502.18772)] [[pdf](https://arxiv.org/pdf/2502.18772)]
> **Authors**: Xueqing Peng,Triantafillos Papadopoulos,Efstathia Soufleri,Polydoros Giannouris,Ruoyu Xiang,Yan Wang,Lingfei Qian,Jimin Huang,Qianqian Xie,Sophia Ananiadou
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 18 pages, 6 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Despite Greece's pivotal role in the global economy, large language models (LLMs) remain underexplored for Greek financial context due to the linguistic complexity of Greek and the scarcity of domain-specific datasets. Previous efforts in multilingual financial natural language processing (NLP) have exposed considerable performance disparities, yet no dedicated Greek financial benchmarks or Greek-specific financial LLMs have been developed until now. To bridge this gap, we introduce Plutus-ben, the first Greek Financial Evaluation Benchmark, and Plutus-8B, the pioneering Greek Financial LLM, fine-tuned with Greek domain-specific data. Plutus-ben addresses five core financial NLP tasks in Greek: numeric and textual named entity recognition, question answering, abstractive summarization, and topic classification, thereby facilitating systematic and reproducible LLM assessments. To underpin these tasks, we present three novel, high-quality Greek financial datasets, thoroughly annotated by expert native Greek speakers, augmented by two existing resources. Our comprehensive evaluation of 22 LLMs on Plutus-ben reveals that Greek financial NLP remains challenging due to linguistic complexity, domain-specific terminology, and financial reasoning gaps. These findings underscore the limitations of cross-lingual transfer, the necessity for financial expertise in Greek-trained models, and the challenges of adapting financial LLMs to Greek text. We release Plutus-ben, Plutus-8B, and all associated datasets publicly to promote reproducible research and advance Greek financial NLP, fostering broader multilingual inclusivity in finance.

### Automatic Prompt Optimization via Heuristic Search: A Survey 
[[arxiv](https://arxiv.org/abs/2502.18746)] [[cool](https://papers.cool/arxiv/2502.18746)] [[pdf](https://arxiv.org/pdf/2502.18746)]
> **Authors**: Wendi Cui,Jiaxin Zhang,Zhuohang Li,Hao Sun,Damien Lopez,Kamalika Das,Bradley A. Malin,Sricharan Kumar
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Recent advances in Large Language Models have led to remarkable achievements across a variety of Natural Language Processing tasks, making prompt engineering increasingly central to guiding model outputs. While manual methods can be effective, they typically rely on intuition and do not automatically refine prompts over time. In contrast, automatic prompt optimization employing heuristic-based search algorithms can systematically explore and improve prompts with minimal human oversight. This survey proposes a comprehensive taxonomy of these methods, categorizing them by where optimization occurs, what is optimized, what criteria drive the optimization, which operators generate new prompts, and which iterative search algorithms are applied. We further highlight specialized datasets and tools that support and accelerate automated prompt refinement. We conclude by discussing key open challenges pointing toward future opportunities for more robust and versatile LLM applications.

### Random Forest-of-Thoughts: Uncertainty-aware Reasoning for Computational Social Science 
[[arxiv](https://arxiv.org/abs/2502.18729)] [[cool](https://papers.cool/arxiv/2502.18729)] [[pdf](https://arxiv.org/pdf/2502.18729)]
> **Authors**: Xiaohua Wu,Xiaohui Tao,Wenjie Wu,Yuefeng Li,Lin Li
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 11 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Social surveys in computational social science are well-designed by elaborate domain theories that can effectively reflect the interviewee's deep thoughts without concealing their true feelings. The candidate questionnaire options highly depend on the interviewee's previous answer, which results in the complexity of social survey analysis, the time, and the expertise required. The ability of large language models (LLMs) to perform complex reasoning is well-enhanced by prompting learning such as Chain-of-thought (CoT) but still confined to left-to-right decision-making processes or limited paths during inference. This means they can fall short in problems that require exploration and uncertainty searching. In response, a novel large language model prompting method, called Random Forest of Thoughts (RFoT), is proposed for generating uncertainty reasoning to fit the area of computational social science. The RFoT allows LLMs to perform deliberate decision-making by generating diverse thought space and randomly selecting the sub-thoughts to build the forest of thoughts. It can extend the exploration and prediction of overall performance, benefiting from the extensive research space of response. The method is applied to optimize computational social science analysis on two datasets covering a spectrum of social survey analysis problems. Our experiments show that RFoT significantly enhances language models' abilities on two novel social survey analysis problems requiring non-trivial reasoning.

### MPO: An Efficient Post-Processing Framework for Mixing Diverse Preference Alignment 
[[arxiv](https://arxiv.org/abs/2502.18699)] [[cool](https://papers.cool/arxiv/2502.18699)] [[pdf](https://arxiv.org/pdf/2502.18699)]
> **Authors**: Tianze Wang,Dongnan Gui,Yifan Hu,Shuhang Lin,Linjun Zhang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习,方法论
- **Abstract**: Reinforcement Learning from Human Feedback (RLHF) has shown promise in aligning large language models (LLMs). Yet its reliance on a singular reward model often overlooks the diversity of human preferences. Recent approaches address this limitation by leveraging multi-dimensional feedback to fine-tune corresponding reward models and train LLMs using reinforcement learning. However, the process is costly and unstable, especially given the competing and heterogeneous nature of human preferences. In this paper, we propose Mixing Preference Optimization (MPO), a post-processing framework for aggregating single-objective policies as an alternative to both multi-objective RLHF (MORLHF) and MaxMin-RLHF. MPO avoids alignment from scratch. Instead, it log-linearly combines existing policies into a unified one with the weight of each policy computed via a batch stochastic mirror descent. Empirical results demonstrate that MPO achieves balanced performance across diverse preferences, outperforming or matching existing models with significantly reduced computational costs.

### Discriminative Finetuning of Generative Large Language Models without Reward Models and Preference Data 
[[arxiv](https://arxiv.org/abs/2502.18679)] [[cool](https://papers.cool/arxiv/2502.18679)] [[pdf](https://arxiv.org/pdf/2502.18679)]
> **Authors**: Siqi Guo,Ilgee Hong,Vicente Balmaseda,Tuo Zhao,Tianbao Yang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 15 pages, 6 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Supervised fine-tuning (SFT) followed by preference optimization (PO) denoted by SFT$\rightarrow$PO has become the standard for improving pretrained large language models (LLMs), with PO demonstrating significant performance gains. However, PO methods rely on either human-labeled preference data or a strong reward model to generate preference data. Can we fine-tune LLMs without preference data or reward models while achieving competitive performance to SFT$\rightarrow$PO? We address this question by introducing Discriminative Fine-Tuning (DFT), a novel approach that eliminates the need for preference data. Unlike SFT, which employs a generative approach and overlooks negative data, DFT adopts a discriminative paradigm that that increases the probability of positive answers while suppressing potentially negative ones, shifting from token prediction to data prediction. Our contributions include: (i) a discriminative probabilistic framework for fine-tuning LLMs by explicitly modeling the discriminative likelihood of an answer among all possible outputs given an input; (ii) efficient algorithms to optimize this discriminative likelihood; and (iii) extensive experiments demonstrating DFT's effectiveness, achieving performance better than SFT and comparable to if not better than SFT$\rightarrow$PO. The code can be found at https://github.com/PenGuln/DFT.

### Enhancing Text Classification with a Novel Multi-Agent Collaboration Framework Leveraging BERT 
[[arxiv](https://arxiv.org/abs/2502.18653)] [[cool](https://papers.cool/arxiv/2502.18653)] [[pdf](https://arxiv.org/pdf/2502.18653)]
> **Authors**: Hediyeh Baban,Sai A Pidapar,Aashutosh Nema,Sichen Lu
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: We introduce a novel multi-agent collaboration framework designed to enhance the accuracy and robustness of text classification models. Leveraging BERT as the primary classifier, our framework dynamically escalates low-confidence predictions to a specialized multi-agent system comprising Lexical, Contextual, Logic, Consensus, and Explainability agents. This collaborative approach allows for comprehensive analysis and consensus-driven decision-making, significantly improving classification performance across diverse text classification tasks. Empirical evaluations on benchmark datasets demonstrate that our framework achieves a 5.5% increase in accuracy compared to standard BERT-based classifiers, underscoring its effectiveness and academic novelty in advancing multi-agent systems within natural language processing.

### Single- vs. Dual-Prompt Dialogue Generation with LLMs for Job Interviews in Human Resources 
[[arxiv](https://arxiv.org/abs/2502.18650)] [[cool](https://papers.cool/arxiv/2502.18650)] [[pdf](https://arxiv.org/pdf/2502.18650)]
> **Authors**: Joachim De Baer,A. Seza Doğruöz,Thomas Demeester,Chris Develder
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 11 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Optimizing language models for use in conversational agents requires large quantities of example dialogues. Increasingly, these dialogues are synthetically generated by using powerful large language models (LLMs), especially in domains with challenges to obtain authentic human data. One such domain is human resources (HR). In this context, we compare two LLM-based dialogue generation methods for the use case of generating HR job interviews, and assess whether one method generates higher-quality dialogues that are more challenging to distinguish from genuine human discourse. The first method uses a single prompt to generate the complete interview dialog. The second method uses two agents that converse with each other. To evaluate dialogue quality under each method, we ask a judge LLM to determine whether AI was used for interview generation, using pairwise interview comparisons. We demonstrate that despite a sixfold increase in token cost, interviews generated with the dual-prompt method achieve a win rate up to ten times higher than those generated with the single-prompt method. This difference remains consistent regardless of whether GPT-4o or Llama 3.3 70B is used for either interview generation or judging quality.

### Steered Generation via Gradient Descent on Sparse Features 
[[arxiv](https://arxiv.org/abs/2502.18644)] [[cool](https://papers.cool/arxiv/2502.18644)] [[pdf](https://arxiv.org/pdf/2502.18644)]
> **Authors**: Sumanta Bhattacharyya,Pedram Rooshenas
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) encode a diverse range of linguistic features within their latent representations, which can be harnessed to steer their output toward specific target characteristics. In this paper, we modify the internal structure of LLMs by training sparse autoencoders to learn a sparse representation of the query embedding, allowing precise control over the model's attention distribution. We demonstrate that manipulating this sparse representation effectively transforms the output toward different stylistic and cognitive targets. Specifically, in an educational setting, we show that the cognitive complexity of LLM-generated feedback can be systematically adjusted by modifying the encoded query representation at a specific layer. To achieve this, we guide the learned sparse embedding toward the representation of samples from the desired cognitive complexity level, using gradient-based optimization in the latent space.

### Chain of Draft: Thinking Faster by Writing Less 
[[arxiv](https://arxiv.org/abs/2502.18600)] [[cool](https://papers.cool/arxiv/2502.18600)] [[pdf](https://arxiv.org/pdf/2502.18600)]
> **Authors**: Silei Xu,Wenhao Xie,Lingxiao Zhao,Pengcheng He
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: :I.2.7
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models (LLMs) have demonstrated remarkable performance in solving complex reasoning tasks through mechanisms like Chain-of-Thought (CoT) prompting, which emphasizes verbose, step-by-step reasoning. However, humans typically employ a more efficient strategy: drafting concise intermediate thoughts that capture only essential information. In this work, we propose Chain of Draft (CoD), a novel paradigm inspired by human cognitive processes, where LLMs generate minimalistic yet informative intermediate reasoning outputs while solving tasks. By reducing verbosity and focusing on critical insights, CoD matches or surpasses CoT in accuracy while using as little as only 7.6% of the tokens, significantly reducing cost and latency across various reasoning tasks.

### Neurobiber: Fast and Interpretable Stylistic Feature Extraction 
[[arxiv](https://arxiv.org/abs/2502.18590)] [[cool](https://papers.cool/arxiv/2502.18590)] [[pdf](https://arxiv.org/pdf/2502.18590)]
> **Authors**: Kenan Alkiek,Anna Wegmann,Jian Zhu,David Jurgens
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Linguistic style is pivotal for understanding how texts convey meaning and fulfill communicative purposes, yet extracting detailed stylistic features at scale remains challenging. We present Neurobiber, a transformer-based system for fast, interpretable style profiling built on Biber's Multidimensional Analysis (MDA). Neurobiber predicts 96 Biber-style features from our open-source BiberPlus library (a Python toolkit that computes stylistic features and provides integrated analytics, e.g., PCA and factor analysis). Despite being up to 56 times faster than existing open source systems, Neurobiber replicates classic MDA insights on the CORE corpus and achieves competitive performance on the PAN 2020 authorship verification task without extensive retraining. Its efficient and interpretable representations readily integrate into downstream NLP pipelines, facilitating large-scale stylometric research, forensic analysis, and real-time text monitoring. All components are made publicly available.

### What are Foundation Models Cooking in the Post-Soviet World? 
[[arxiv](https://arxiv.org/abs/2502.18583)] [[cool](https://papers.cool/arxiv/2502.18583)] [[pdf](https://arxiv.org/pdf/2502.18583)]
> **Authors**: Anton Lavrouk,Tarek Naous,Alan Ritter,Wei Xu
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The culture of the Post-Soviet states is complex, shaped by a turbulent history that continues to influence current events. In this study, we investigate the Post-Soviet cultural food knowledge of foundation models by constructing BORSch, a multimodal dataset encompassing 1147 and 823 dishes in the Russian and Ukrainian languages, centered around the Post-Soviet region. We demonstrate that leading models struggle to correctly identify the origins of dishes from Post-Soviet nations in both text-only and multimodal Question Answering (QA), instead over-predicting countries linked to the language the question is asked in. Through analysis of pretraining data, we show that these results can be explained by misleading dish-origin co-occurrences, along with linguistic phenomena such as Russian-Ukrainian code mixing. Finally, to move beyond QA-based assessments, we test models' abilities to produce accurate visual descriptions of dishes. The weak correlation between this task and QA suggests that QA alone may be insufficient as an evaluation of cultural understanding. To foster further research, we will make BORSch publicly available at https://github.com/alavrouk/BORSch.

### Scalable Best-of-N Selection for Large Language Models via Self-Certainty 
[[arxiv](https://arxiv.org/abs/2502.18581)] [[cool](https://papers.cool/arxiv/2502.18581)] [[pdf](https://arxiv.org/pdf/2502.18581)]
> **Authors**: Zhewei Kang,Xuandong Zhao,Dawn Song
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Best-of-N selection is a key technique for improving the reasoning performance of Large Language Models (LLMs) through increased test-time computation. Current state-of-the-art methods often employ computationally intensive reward models for response evaluation and selection. Reward-free alternatives, like self-consistency and universal self-consistency, are limited in their ability to handle open-ended generation tasks or scale effectively. To address these limitations, we propose self-certainty, a novel and efficient metric that leverages the inherent probability distribution of LLM outputs to estimate response quality without requiring external reward models. We hypothesize that higher distributional self-certainty, aggregated across multiple samples, correlates with improved response accuracy, as it reflects greater confidence in the generated output. Through extensive experiments on various reasoning tasks, we demonstrate that self-certainty (1) scales effectively with increasing sample size $N$, akin to reward models but without the computational overhead; (2) complements chain-of-thought, improving reasoning performance beyond greedy decoding; and (3) generalizes to open-ended tasks where traditional self-consistency methods fall short. Our findings establish self-certainty as a practical and efficient way for improving LLM reasoning capabilities. The code is available at https://github.com/backprop07/Self-Certainty

### FactReasoner: A Probabilistic Approach to Long-Form Factuality Assessment for Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.18573)] [[cool](https://papers.cool/arxiv/2502.18573)] [[pdf](https://arxiv.org/pdf/2502.18573)]
> **Authors**: Radu Marinescu,Debarun Bhattacharjya,Junkyu Lee,Tigran Tchrakian,Javier Carnerero Cano,Yufang Hou,Elizabeth Daly,Alessandra Pascale
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large language models (LLMs) have demonstrated vast capabilities on generative tasks in recent years, yet they struggle with guaranteeing the factual correctness of the generated content. This makes these models unreliable in realistic situations where factually accurate responses are expected. In this paper, we propose FactReasoner, a new factuality assessor that relies on probabilistic reasoning to assess the factuality of a long-form generated response. Specifically, FactReasoner decomposes the response into atomic units, retrieves relevant contexts for them from an external knowledge source, and constructs a joint probability distribution over the atoms and contexts using probabilistic encodings of the logical relationships (entailment, contradiction) between the textual utterances corresponding to the atoms and contexts. FactReasoner then computes the posterior probability of whether atomic units in the response are supported by the retrieved contexts. Our experiments on labeled and unlabeled benchmark datasets demonstrate clearly that FactReasoner improves considerably over state-of-the-art prompt-based approaches in terms of both factual precision and recall.

### MixLLM: Dynamic Routing in Mixed Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.18482)] [[cool](https://papers.cool/arxiv/2502.18482)] [[pdf](https://arxiv.org/pdf/2502.18482)]
> **Authors**: Xinyuan Wang,Yanchi Liu,Wei Cheng,Xujiang Zhao,Zhengzhang Chen,Wenchao Yu,Yanjie Fu,Haifeng Chen
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-26
> **comment**: 11 pages, 7 figures, accepted by NAACL 2025 main conference
- **标题**: None
- **领域**: 计算语言学,人工智能,数据库,信息检索
- **Abstract**: Large Language Models (LLMs) exhibit potential artificial generic intelligence recently, however, their usage is costly with high response latency. Given mixed LLMs with their own strengths and weaknesses, LLM routing aims to identify the most suitable model for each query in the stream to maximize response quality and minimize cost and latency. However, the challenges involve: (1) dynamic trade-offs among quality, cost, and latency; (2) enabling continual learning in deployed systems; and (3) navigating a varying (e.g., new LLM addition or old LLM removal) set of LLM candidates over time. To bridge these gaps, we develop MixLLM, a dynamic contextual-bandit-based routing system for query-LLM assignment. Specifically, we first leverage query tags to enhance query embeddings for the routing task. Next, we design lightweight prediction models to estimate the response qualities and costs of queries over LLMs. We then devise a meta-decision maker to choose the query-LLM assignments to best tradeoff response quality, cost, and latency. Finally, the system benefits from continual training, allowing it to adapt to evolving queries and user feedback over time. Our extensive experiments show that MixLLM achieves the best trade-offs in response quality, cost, and latency (97.25% of GPT-4's quality at 24.18% of the cost under the time constraint).

### DRAMA: Diverse Augmentation from Large Language Models to Smaller Dense Retrievers 
[[arxiv](https://arxiv.org/abs/2502.18460)] [[cool](https://papers.cool/arxiv/2502.18460)] [[pdf](https://arxiv.org/pdf/2502.18460)]
> **Authors**: Xueguang Ma,Xi Victoria Lin,Barlas Oguz,Jimmy Lin,Wen-tau Yih,Xilun Chen
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,信息检索
- **Abstract**: Large language models (LLMs) have demonstrated strong effectiveness and robustness while fine-tuned as dense retrievers. However, their large parameter size brings significant inference time computational challenges, including high encoding costs for large-scale corpora and increased query latency, limiting their practical deployment. While smaller retrievers offer better efficiency, they often fail to generalize effectively with limited supervised fine-tuning data. In this work, we introduce DRAMA, a training framework that leverages LLMs to train smaller generalizable dense retrievers. In particular, we adopt pruned LLMs as the backbone and train on diverse LLM-augmented data in a single-stage contrastive learning setup. Experiments show that DRAMA offers better multilingual and long-context capabilities than traditional encoder-based retrievers, and achieves strong performance across multiple tasks and languages. These highlight the potential of connecting the training of smaller retrievers with the growing advancements in LLMs, bridging the gap between efficiency and generalization.

### FRIDA to the Rescue! Analyzing Synthetic Data Effectiveness in Object-Based Common Sense Reasoning for Disaster Response 
[[arxiv](https://arxiv.org/abs/2502.18452)] [[cool](https://papers.cool/arxiv/2502.18452)] [[pdf](https://arxiv.org/pdf/2502.18452)]
> **Authors**: Mollie Shichman,Claire Bonial,Austin Blodgett,Taylor Hudson,Francis Ferraro,Rachel Rudinger
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 8 pages, 3 figures, 5 tables
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large Language Models (LLMs) have the potential for substantial common sense reasoning. However, these capabilities are often emergent in larger models. This means smaller models that can be run locally are less helpful and capable with respect to certain reasoning tasks. To meet our problem space requirements, we fine-tune smaller LLMs to disaster domains, as these domains involve complex and low-frequency physical common sense knowledge. We introduce a pipeline to create Field Ready Instruction Decoding Agent (FRIDA) models, where domain experts and linguists combine their knowledge to make high-quality seed data that is used to generate synthetic data for fine-tuning. We create a set of 130 seed instructions for synthetic generation, a synthetic dataset of 25000 instructions, and 119 evaluation instructions relating to both general and earthquake-specific object affordances. We fine-tune several LLaMa and Mistral instruction-tuned models and find that FRIDA models outperform their base models at a variety of sizes. We then run an ablation study to understand which kinds of synthetic data most affect performance and find that training physical state and object function common sense knowledge alone improves over FRIDA models trained on all data. We conclude that the FRIDA pipeline is capable of instilling general common sense, but needs to be augmented with information retrieval for specific domain knowledge.

### Disambiguate First Parse Later: Generating Interpretations for Ambiguity Resolution in Semantic Parsing 
[[arxiv](https://arxiv.org/abs/2502.18448)] [[cool](https://papers.cool/arxiv/2502.18448)] [[pdf](https://arxiv.org/pdf/2502.18448)]
> **Authors**: Irina Saparina,Mirella Lapata
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Handling ambiguity and underspecification is an important challenge in natural language interfaces, particularly for tasks like text-to-SQL semantic parsing. We propose a modular approach that resolves ambiguity using natural language interpretations before mapping these to logical forms (e.g., SQL queries). Although LLMs excel at parsing unambiguous utterances, they show strong biases for ambiguous ones, typically predicting only preferred interpretations. We constructively exploit this bias to generate an initial set of preferred disambiguations and then apply a specialized infilling model to identify and generate missing interpretations. To train the infilling model, we introduce an annotation method that uses SQL execution to validate different meanings. Our approach improves interpretation coverage and generalizes across datasets with different annotation styles, database structures, and ambiguity types.

### olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language Models 
[[arxiv](https://arxiv.org/abs/2502.18443)] [[cool](https://papers.cool/arxiv/2502.18443)] [[pdf](https://arxiv.org/pdf/2502.18443)]
> **Authors**: Jake Poznanski,Jon Borchardt,Jason Dunkelberger,Regan Huff,Daniel Lin,Aman Rangapur,Christopher Wilhelm,Kyle Lo,Luca Soldaini
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: PDF documents have the potential to provide trillions of novel, high-quality tokens for training language models. However, these documents come in a diversity of types with differing formats and visual layouts that pose a challenge when attempting to extract and faithfully represent the underlying content for language model use. We present olmOCR, an open-source Python toolkit for processing PDFs into clean, linearized plain text in natural reading order while preserving structured content like sections, tables, lists, equations, and more. Our toolkit runs a fine-tuned 7B vision language model (VLM) trained on a sample of 260,000 pages from over 100,000 crawled PDFs with diverse properties, including graphics, handwritten text and poor quality scans. olmOCR is optimized for large-scale batch processing, able to scale flexibly to different hardware setups and convert a million PDF pages for only $190 USD. We release all components of olmOCR including VLM weights, data and training code, as well as inference code built on serving frameworks including vLLM and SGLang.

### Reversal Blessing: Thinking Backward May Outpace Thinking Forward in Multi-choice Questions 
[[arxiv](https://arxiv.org/abs/2502.18435)] [[cool](https://papers.cool/arxiv/2502.18435)] [[pdf](https://arxiv.org/pdf/2502.18435)]
> **Authors**: Yizhe Zhang,Richard Bai,Zijin Gu,Ruixiang Zhang,Jiatao Gu,Emmanuel Abbe,Samy Bengio,Navdeep Jaitly
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,信息论,机器学习
- **Abstract**: Language models usually use left-to-right (L2R) autoregressive factorization. However, L2R factorization may not always be the best inductive bias. Therefore, we investigate whether alternative factorizations of the text distribution could be beneficial in some tasks. We investigate right-to-left (R2L) training as a compelling alternative, focusing on multiple-choice questions (MCQs) as a test bed for knowledge extraction and reasoning. Through extensive experiments across various model sizes (2B-8B parameters) and training datasets, we find that R2L models can significantly outperform L2R models on several MCQ benchmarks, including logical reasoning, commonsense understanding, and truthfulness assessment tasks. Our analysis reveals that this performance difference may be fundamentally linked to multiple factors including calibration, computability and directional conditional entropy. We ablate the impact of these factors through controlled simulation studies using arithmetic tasks, where the impacting factors can be better disentangled. Our work demonstrates that exploring alternative factorizations of the text distribution can lead to improvements in LLM capabilities and provides theoretical insights into optimal factorization towards approximating human language distribution, and when each reasoning order might be more advantageous.

### Exploring Gender Disparities in Automatic Speech Recognition Technology 
[[arxiv](https://arxiv.org/abs/2502.18434)] [[cool](https://papers.cool/arxiv/2502.18434)] [[pdf](https://arxiv.org/pdf/2502.18434)]
> **Authors**: Hend ElGhazaly,Bahman Mirheidari,Nafise Sadat Moosavi,Heidi Christensen
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: ISCA/ITG Workshop on Diversity in Large Speech andLanguageModels
- **标题**: None
- **领域**: 计算语言学,声音,音频和语音处理
- **Abstract**: This study investigates factors influencing Automatic Speech Recognition (ASR) systems' fairness and performance across genders, beyond the conventional examination of demographics. Using the LibriSpeech dataset and the Whisper small model, we analyze how performance varies across different gender representations in training data. Our findings suggest a complex interplay between the gender ratio in training data and ASR performance. Optimal fairness occurs at specific gender distributions rather than a simple 50-50 split. Furthermore, our findings suggest that factors like pitch variability can significantly affect ASR accuracy. This research contributes to a deeper understanding of biases in ASR systems, highlighting the importance of carefully curated training data in mitigating gender bias.

### TextGames: Learning to Self-Play Text-Based Puzzle Games via Language Model Reasoning 
[[arxiv](https://arxiv.org/abs/2502.18431)] [[cool](https://papers.cool/arxiv/2502.18431)] [[pdf](https://arxiv.org/pdf/2502.18431)]
> **Authors**: Frederikus Hudi,Genta Indra Winata,Ruochen Zhang,Alham Fikri Aji
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Reasoning is a fundamental capability of large language models (LLMs), enabling them to comprehend, analyze, and solve complex problems. In this paper, we introduce TextGames, an innovative benchmark specifically crafted to assess LLMs through demanding text-based games that require advanced skills in pattern recognition, spatial awareness, arithmetic, and logical reasoning. Our analysis probes LLMs' performance in both single-turn and multi-turn reasoning, and their abilities in leveraging feedback to correct subsequent answers through self-reflection. Our findings reveal that, although LLMs exhibit proficiency in addressing most easy and medium-level problems, they face significant challenges with more difficult tasks. In contrast, humans are capable of solving all tasks when given sufficient time. Moreover, we observe that LLMs show improved performance in multi-turn predictions through self-reflection, yet they still struggle with sequencing, counting, and following complex rules consistently. Additionally, models optimized for reasoning outperform pre-trained LLMs that prioritize instruction following, highlighting the crucial role of reasoning skills in addressing highly complex problems.

### Compressing Language Models for Specialized Domains 
[[arxiv](https://arxiv.org/abs/2502.18424)] [[cool](https://papers.cool/arxiv/2502.18424)] [[pdf](https://arxiv.org/pdf/2502.18424)]
> **Authors**: Miles Williams,George Chrysostomou,Vitor Jeronymo,Nikolaos Aletras
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Work in progress
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Compression techniques such as pruning and quantization offer a solution for more efficient deployment of language models (LMs), albeit with small performance drops in benchmark performance. However, general-purpose LM compression methods can negatively affect performance in specialized domains (e.g. biomedical or legal). Recent work has sought to address this, yet requires computationally expensive full-parameter fine-tuning. To this end, we propose cross-calibration, a novel training-free approach for improving the domain performance of compressed LMs. Our approach effectively leverages Hessian-based sensitivity to identify weights that are influential for both in-domain and general performance. Through extensive experimentation, we demonstrate that cross-calibration substantially outperforms existing approaches on domain-specific tasks, without compromising general performance. Notably, these gains come without additional computational overhead, displaying remarkable potential towards extracting domain-specialized compressed models from general-purpose LMs.

### GLEAN: Generalized Category Discovery with Diverse and Quality-Enhanced LLM Feedback 
[[arxiv](https://arxiv.org/abs/2502.18414)] [[cool](https://papers.cool/arxiv/2502.18414)] [[pdf](https://arxiv.org/pdf/2502.18414)]
> **Authors**: Henry Peng Zou,Siffi Singh,Yi Nian,Jianfeng He,Jason Cai,Saab Mansour,Hang Su
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Generalized Category Discovery (GCD) is a practical and challenging open-world task that aims to recognize both known and novel categories in unlabeled data using limited labeled data from known categories. Due to the lack of supervision, previous GCD methods face significant challenges, such as difficulty in rectifying errors for confusing instances, and inability to effectively uncover and leverage the semantic meanings of discovered clusters. Therefore, additional annotations are usually required for real-world applicability. However, human annotation is extremely costly and inefficient. To address these issues, we propose GLEAN, a unified framework for generalized category discovery that actively learns from diverse and quality-enhanced LLM feedback. Our approach leverages three different types of LLM feedback to: (1) improve instance-level contrastive features, (2) generate category descriptions, and (3) align uncertain instances with LLM-selected category descriptions. Extensive experiments demonstrate the superior performance of \MethodName over state-of-the-art models across diverse datasets, metrics, and supervision settings. Our code is available at https://github.com/amazon-science/Glean.

### AgentRM: Enhancing Agent Generalization with Reward Modeling 
[[arxiv](https://arxiv.org/abs/2502.18407)] [[cool](https://papers.cool/arxiv/2502.18407)] [[pdf](https://arxiv.org/pdf/2502.18407)]
> **Authors**: Yu Xia,Jingru Fan,Weize Chen,Siyu Yan,Xin Cong,Zhong Zhang,Yaxi Lu,Yankai Lin,Zhiyuan Liu,Maosong Sun
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Existing LLM-based agents have achieved strong performance on held-in tasks, but their generalizability to unseen tasks remains poor. Hence, some recent work focus on fine-tuning the policy model with more diverse tasks to improve the generalizability. In this work, we find that finetuning a reward model to guide the policy model is more robust than directly finetuning the policy model. Based on this finding, we propose AgentRM, a generalizable reward model, to guide the policy model for effective test-time search. We comprehensively investigate three approaches to construct the reward model, including explicit reward modeling, implicit reward modeling and LLM-as-a-judge. We then use AgentRM to guide the answer generation with Best-of-N sampling and step-level beam search. On four types of nine agent tasks, AgentRM enhances the base policy model by $8.8$ points on average, surpassing the top general agent by $4.0$. Moreover, it demonstrates weak-to-strong generalization, yielding greater improvement of $12.6$ on LLaMA-3-70B policy model. As for the specializability, AgentRM can also boost a finetuned policy model and outperform the top specialized agent by $11.4$ on three held-in tasks. Further analysis verifies its effectiveness in test-time scaling. Codes will be released to facilitate the research in this area.

### KiRAG: Knowledge-Driven Iterative Retriever for Enhancing Retrieval-Augmented Generation 
[[arxiv](https://arxiv.org/abs/2502.18397)] [[cool](https://papers.cool/arxiv/2502.18397)] [[pdf](https://arxiv.org/pdf/2502.18397)]
> **Authors**: Jinyuan Fang,Zaiqiao Meng,Craig Macdonald
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Iterative retrieval-augmented generation (iRAG) models offer an effective approach for multi-hop question answering (QA). However, their retrieval process faces two key challenges: (1) it can be disrupted by irrelevant documents or factually inaccurate chain-of-thoughts; (2) their retrievers are not designed to dynamically adapt to the evolving information needs in multi-step reasoning, making it difficult to identify and retrieve the missing information required at each iterative step. Therefore, we propose KiRAG, which uses a knowledge-driven iterative retriever model to enhance the retrieval process of iRAG. Specifically, KiRAG decomposes documents into knowledge triples and performs iterative retrieval with these triples to enable a factually reliable retrieval process. Moreover, KiRAG integrates reasoning into the retrieval process to dynamically identify and retrieve knowledge that bridges information gaps, effectively adapting to the evolving information needs. Empirical results show that KiRAG significantly outperforms existing iRAG models, with an average improvement of 9.40% in R@3 and 5.14% in F1 on multi-hop QA.

### Monte Carlo Temperature: a robust sampling strategy for LLM's uncertainty quantification methods 
[[arxiv](https://arxiv.org/abs/2502.18389)] [[cool](https://papers.cool/arxiv/2502.18389)] [[pdf](https://arxiv.org/pdf/2502.18389)]
> **Authors**: Nicola Cecere,Andrea Bacciu,Ignacio Fernández Tobías,Amin Mantrach
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Uncertainty quantification (UQ) in Large Language Models (LLMs) is essential for their safe and reliable deployment, particularly in critical applications where incorrect outputs can have serious consequences. Current UQ methods typically rely on querying the model multiple times using non-zero temperature sampling to generate diverse outputs for uncertainty estimation. However, the impact of selecting a given temperature parameter is understudied, and our analysis reveals that temperature plays a fundamental role in the quality of uncertainty estimates. The conventional approach of identifying optimal temperature values requires expensive hyperparameter optimization (HPO) that must be repeated for each new model-dataset combination. We propose Monte Carlo Temperature (MCT), a robust sampling strategy that eliminates the need for temperature calibration. Our analysis reveals that: 1) MCT provides more robust uncertainty estimates across a wide range of temperatures, 2) MCT improves the performance of UQ methods by replacing fixed-temperature strategies that do not rely on HPO, and 3) MCT achieves statistical parity with oracle temperatures, which represent the ideal outcome of a well-tuned but computationally expensive HPO process. These findings demonstrate that effective UQ can be achieved without the computational burden of temperature parameter calibration.

### DBR: Divergence-Based Regularization for Debiasing Natural Language Understanding Models 
[[arxiv](https://arxiv.org/abs/2502.18353)] [[cool](https://papers.cool/arxiv/2502.18353)] [[pdf](https://arxiv.org/pdf/2502.18353)]
> **Authors**: Zihao Li,Ruixiang Tang,Lu Cheng,Shuaiqiang Wang,Dawei Yin,Mengnan Du
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Accepted by SIGKDD Explorations
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Pre-trained language models (PLMs) have achieved impressive results on various natural language processing tasks. However, recent research has revealed that these models often rely on superficial features and shortcuts instead of developing a genuine understanding of language, especially for natural language understanding (NLU) tasks. Consequently, the models struggle to generalize to out-of-domain data. In this work, we propose Divergence Based Regularization (DBR) to mitigate this shortcut learning behavior. Our method measures the divergence between the output distributions for original examples and examples where shortcut tokens have been masked. This process prevents the model's predictions from being overly influenced by shortcut features or biases. We evaluate our model on three NLU tasks and find that it improves out-of-domain performance with little loss of in-domain accuracy. Our results demonstrate that reducing the reliance on shortcuts and superficial features can enhance the generalization ability of large pre-trained language models.

### BRIDO: Bringing Democratic Order to Abstractive Summarization 
[[arxiv](https://arxiv.org/abs/2502.18342)] [[cool](https://papers.cool/arxiv/2502.18342)] [[pdf](https://arxiv.org/pdf/2502.18342)]
> **Authors**: Junhyun Lee,Harshith Goka,Hyeonmok Ko
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 13 pages, 1 figure; AAAI-25 Workshop on PDLM camera ready
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Hallucination refers to the inaccurate, irrelevant, and inconsistent text generated from large language models (LLMs). While the LLMs have shown great promise in a variety of tasks, the issue of hallucination still remains a major challenge for many practical uses. In this paper, we tackle the issue of hallucination in abstract text summarization by mitigating exposure bias. Existing models targeted for exposure bias mitigation, namely BRIO, aim for better summarization quality in the ROUGE score. We propose a model that uses a similar exposure bias mitigation strategy but with a goal that is aligned with less hallucination. We conjecture that among a group of candidate outputs, ones with hallucinations will comprise the minority of the whole group. That is, candidates with less similarity with others will have a higher chance of containing hallucinated content. Our method uses this aspect and utilizes contrastive learning, incentivizing candidates with high inter-candidate ROUGE scores. We performed experiments on the XSum and CNN/DM summarization datasets, and our method showed 6.25% and 3.82% improvement, respectively, on the consistency G-Eval score over BRIO.

### Correlating and Predicting Human Evaluations of Language Models from Natural Language Processing Benchmarks 
[[arxiv](https://arxiv.org/abs/2502.18339)] [[cool](https://papers.cool/arxiv/2502.18339)] [[pdf](https://arxiv.org/pdf/2502.18339)]
> **Authors**: Rylan Schaeffer,Punit Singh Koura,Binh Tang,Ranjan Subramanian,Aaditya K Singh,Todor Mihaylov,Prajjwal Bhargava,Lovish Madaan,Niladri S. Chatterji,Vedanuj Goswami,Sergey Edunov,Dieuwke Hupkes,Sanmi Koyejo,Sharan Narang
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: The explosion of high-performing conversational language models (LMs) has spurred a shift from classic natural language processing (NLP) benchmarks to expensive, time-consuming and noisy human evaluations - yet the relationship between these two evaluation strategies remains hazy. In this paper, we conduct a large-scale study of four Chat Llama 2 models, comparing their performance on 160 standard NLP benchmarks (e.g., MMLU, ARC, BIG-Bench Hard) against extensive human preferences on more than 11k single-turn and 2k multi-turn dialogues from over 2k human annotators. Our findings are striking: most NLP benchmarks strongly correlate with human evaluations, suggesting that cheaper, automated metrics can serve as surprisingly reliable predictors of human preferences. Three human evaluations, such as adversarial dishonesty and safety, are anticorrelated with NLP benchmarks, while two are uncorrelated. Moreover, through overparameterized linear regressions, we show that NLP scores can accurately predict human evaluations across different model scales, offering a path to reduce costly human annotation without sacrificing rigor. Overall, our results affirm the continued value of classic benchmarks and illuminate how to harness them to anticipate real-world user satisfaction - pointing to how NLP benchmarks can be leveraged to meet evaluation needs of our new era of conversational AI.

### BottleHumor: Self-Informed Humor Explanation using the Information Bottleneck Principle 
[[arxiv](https://arxiv.org/abs/2502.18331)] [[cool](https://papers.cool/arxiv/2502.18331)] [[pdf](https://arxiv.org/pdf/2502.18331)]
> **Authors**: EunJeong Hwang,Peter West,Vered Shwartz
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Humor is prevalent in online communications and it often relies on more than one modality (e.g., cartoons and memes). Interpreting humor in multimodal settings requires drawing on diverse types of knowledge, including metaphorical, sociocultural, and commonsense knowledge. However, identifying the most useful knowledge remains an open question. We introduce \method{}, a method inspired by the information bottleneck principle that elicits relevant world knowledge from vision and language models which is iteratively refined for generating an explanation of the humor in an unsupervised manner. Our experiments on three datasets confirm the advantage of our method over a range of baselines. Our method can further be adapted in the future for additional tasks that can benefit from eliciting and conditioning on relevant world knowledge and open new research avenues in this direction.

### Mapping of Subjective Accounts into Interpreted Clusters (MOSAIC): Topic Modelling and LLM applied to Stroboscopic Phenomenology 
[[arxiv](https://arxiv.org/abs/2502.18318)] [[cool](https://papers.cool/arxiv/2502.18318)] [[pdf](https://arxiv.org/pdf/2502.18318)]
> **Authors**: Romy Beauté,David J. Schwartzman,Guillaume Dumas,Jennifer Crook,Fiona Macpherson,Adam B. Barrett,Anil K. Seth
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,神经元和认知
- **Abstract**: Stroboscopic light stimulation (SLS) on closed eyes typically induces simple visual hallucinations (VHs), characterised by vivid, geometric and colourful patterns. A dataset of 862 sentences, extracted from 422 open subjective reports, was recently compiled as part of the Dreamachine programme (Collective Act, 2022), an immersive multisensory experience that combines SLS and spatial sound in a collective setting. Although open reports extend the range of reportable phenomenology, their analysis presents significant challenges, particularly in systematically identifying patterns. To address this challenge, we implemented a data-driven approach leveraging Large Language Models and Topic Modelling to uncover and interpret latent experiential topics directly from the Dreamachine's text-based reports. Our analysis confirmed the presence of simple VHs typically documented in scientific studies of SLS, while also revealing experiences of altered states of consciousness and complex hallucinations. Building on these findings, our computational approach expands the systematic study of subjective experience by enabling data-driven analyses of open-ended phenomenological reports, capturing experiences not readily identified through standard questionnaires. By revealing rich and multifaceted aspects of experiences, our study broadens our understanding of stroboscopically-induced phenomena while highlighting the potential of Natural Language Processing and Large Language Models in the emerging field of computational (neuro)phenomenology. More generally, this approach provides a practically applicable methodology for uncovering subtle hidden patterns of subjective experience across diverse research domains.

### WiCkeD: A Simple Method to Make Multiple Choice Benchmarks More Challenging 
[[arxiv](https://arxiv.org/abs/2502.18316)] [[cool](https://papers.cool/arxiv/2502.18316)] [[pdf](https://arxiv.org/pdf/2502.18316)]
> **Authors**: Ahmed Elhady,Eneko Agirre,Mikel Artetxe
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: We introduce WiCkeD, a simple method to increase the complexity of existing multiple-choice benchmarks by randomly replacing a choice with "None of the above", a method often used in educational tests. We show that WiCkeD can be automatically applied to any existing benchmark, making it more challenging. We apply WiCkeD to 6 popular benchmarks and use it to evaluate 18 open-weight LLMs. The performance of the models drops 12.1 points on average with respect to the original versions of the datasets. When using chain-of-thought on 3 MMLU datasets, the performance drop for the WiCkeD variant is similar to the one observed when using the LLMs directly, showing that WiCkeD is also challenging for models with enhanced reasoning abilities. WiCkeD also uncovers that some models are more sensitive to the extra reasoning required, providing additional information with respect to the original benchmarks. We relase our code and data at https://github.com/ahmedselhady/wicked-benchmarks.

### Looking forward: Linguistic theory and methods 
[[arxiv](https://arxiv.org/abs/2502.18313)] [[cool](https://papers.cool/arxiv/2502.18313)] [[pdf](https://arxiv.org/pdf/2502.18313)]
> **Authors**: John Mansfield,Ethan Gotlieb Wilcox
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: This chapter examines current developments in linguistic theory and methods, focusing on the increasing integration of computational, cognitive, and evolutionary perspectives. We highlight four major themes shaping contemporary linguistics: (1) the explicit testing of hypotheses about symbolic representation, such as efficiency, locality, and conceptual semantic grounding; (2) the impact of artificial neural networks on theoretical debates and linguistic analysis; (3) the importance of intersubjectivity in linguistic theory; and (4) the growth of evolutionary linguistics. By connecting linguistics with computer science, psychology, neuroscience, and biology, we provide a forward-looking perspective on the changing landscape of linguistic research.

### RefuteBench 2.0 -- Agentic Benchmark for Dynamic Evaluation of LLM Responses to Refutation Instruction 
[[arxiv](https://arxiv.org/abs/2502.18308)] [[cool](https://papers.cool/arxiv/2502.18308)] [[pdf](https://arxiv.org/pdf/2502.18308)]
> **Authors**: Jianhao Yan,Yun Luo,Yue Zhang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Work on progess
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: In the multi-turn interaction schema, large language models (LLMs) can leverage user feedback to enhance the quality and relevance of their responses. However, evaluating an LLM's ability to incorporate user refutation feedback is crucial yet challenging. In this study, we introduce RefuteBench 2.0, which significantly extends the original RefuteBench by incorporating LLM agents as refuters and evaluators, which allows for flexible and comprehensive assessment. We design both transient and persistent refutation instructions with different validity periods. Meta-evaluation shows that the LLM-based refuter could generate more human-like refutations and the evaluators could assign scores with high correlation with humans. Experimental results of various LLMs show that current models could effectively satisfy the refutation but fail to memorize the refutation information. Interestingly, we also observe that the performance of the initial task decreases as the refutations increase. Analysis of the attention scores further shows a potential weakness of current LLMs: they struggle to retain and correctly use previous information during long context dialogues. https://github.com/ElliottYan/RefuteBench-2.0

### How Vital is the Jurisprudential Relevance: Law Article Intervened Legal Case Retrieval and Matching 
[[arxiv](https://arxiv.org/abs/2502.18292)] [[cool](https://papers.cool/arxiv/2502.18292)] [[pdf](https://arxiv.org/pdf/2502.18292)]
> **Authors**: Nuo Xu,Pinghui Wang,Zi Liang,Junzhou Zhao,Xiaohong Guan
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,信息检索
- **Abstract**: Legal case retrieval (LCR) aims to automatically scour for comparable legal cases based on a given query, which is crucial for offering relevant precedents to support the judgment in intelligent legal systems. Due to similar goals, it is often associated with a similar case matching (LCM) task. To address them, a daunting challenge is assessing the uniquely defined legal-rational similarity within the judicial domain, which distinctly deviates from the semantic similarities in general text retrieval. Past works either tagged domain-specific factors or incorporated reference laws to capture legal-rational information. However, their heavy reliance on expert or unrealistic assumptions restricts their practical applicability in real-world scenarios. In this paper, we propose an end-to-end model named LCM-LAI to solve the above challenges. Through meticulous theoretical analysis, LCM-LAI employs a dependent multi-task learning framework to capture legal-rational information within legal cases by a law article prediction (LAP) sub-task, without any additional assumptions in inference. Besides, LCM-LAI proposes an article-aware attention mechanism to evaluate the legal-rational similarity between across-case sentences based on law distribution, which is more effective than conventional semantic similarity. Weperform a series of exhaustive experiments including two different tasks involving four real-world datasets. Results demonstrate that LCM-LAI achieves state-of-the-art performance.

### Uncertainty Modeling in Multimodal Speech Analysis Across the Psychosis Spectrum 
[[arxiv](https://arxiv.org/abs/2502.18285)] [[cool](https://papers.cool/arxiv/2502.18285)] [[pdf](https://arxiv.org/pdf/2502.18285)]
> **Authors**: Morteza Rohanian,Roya M. Hüppi,Farhad Nooralahzadeh,Noemi Dannecker,Yves Pauli,Werner Surbeck,Iris Sommer,Wolfram Hinzen,Nicolas Langer,Michael Krauthammer,Philipp Homan
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Capturing subtle speech disruptions across the psychosis spectrum is challenging because of the inherent variability in speech patterns. This variability reflects individual differences and the fluctuating nature of symptoms in both clinical and non-clinical populations. Accounting for uncertainty in speech data is essential for predicting symptom severity and improving diagnostic precision. Speech disruptions characteristic of psychosis appear across the spectrum, including in non-clinical individuals. We develop an uncertainty-aware model integrating acoustic and linguistic features to predict symptom severity and psychosis-related traits. Quantifying uncertainty in specific modalities allows the model to address speech variability, improving prediction accuracy. We analyzed speech data from 114 participants, including 32 individuals with early psychosis and 82 with low or high schizotypy, collected through structured interviews, semi-structured autobiographical tasks, and narrative-driven interactions in German. The model improved prediction accuracy, reducing RMSE and achieving an F1-score of 83% with ECE = 4.5e-2, showing robust performance across different interaction contexts. Uncertainty estimation improved model interpretability by identifying reliability differences in speech markers such as pitch variability, fluency disruptions, and spectral instability. The model dynamically adjusted to task structures, weighting acoustic features more in structured settings and linguistic features in unstructured contexts. This approach strengthens early detection, personalized assessment, and clinical decision-making in psychosis-spectrum research.

### Better Aligned with Survey Respondents or Training Data? Unveiling Political Leanings of LLMs on U.S. Supreme Court Cases 
[[arxiv](https://arxiv.org/abs/2502.18282)] [[cool](https://papers.cool/arxiv/2502.18282)] [[pdf](https://arxiv.org/pdf/2502.18282)]
> **Authors**: Shanshan Xu,T. Y. S. S Santosh,Yanai Elazar,Quirin Vogel,Barbara Plank,Matthias Grabmair
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: under review
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The increased adoption of Large Language Models (LLMs) and their potential to shape public opinion have sparked interest in assessing these models' political leanings. Building on previous research that compared LLMs and human opinions and observed political bias in system responses, we take a step further to investigate the underlying causes of such biases by empirically examining how the values and biases embedded in training corpora shape model outputs. Specifically, we propose a method to quantitatively evaluate political leanings embedded in the large pretraining corpora. Subsequently we investigate to whom are the LLMs' political leanings more aligned with, their pretrainig corpora or the surveyed human opinions. As a case study, we focus on probing the political leanings of LLMs in 32 U.S. Supreme Court cases, addressing contentious topics such as abortion and voting rights. Our findings reveal that LLMs strongly reflect the political leanings in their training data, and no strong correlation is observed with their alignment to human opinions as expressed in surveys. These results underscore the importance of responsible curation of training data and the need for robust evaluation metrics to ensure LLMs' alignment with human-centered values.

### Self-Adjust Softmax 
[[arxiv](https://arxiv.org/abs/2502.18277)] [[cool](https://papers.cool/arxiv/2502.18277)] [[pdf](https://arxiv.org/pdf/2502.18277)]
> **Authors**: Chuanyang Zheng,Yihang Gao,Guoxuan Chen,Han Shi,Jing Xiong,Xiaozhe Ren,Chao Huang,Xin Jiang,Zhenguo Li,Yu Li
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Tech Report
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The softmax function is crucial in Transformer attention, which normalizes each row of the attention scores with summation to one, achieving superior performances over other alternative functions. However, the softmax function can face a gradient vanishing issue when some elements of the attention scores approach extreme values, such as probabilities close to one or zero. In this paper, we propose Self-Adjust Softmax (SA-Softmax) to address this issue by modifying $softmax(x)$ to $x \cdot softmax(x)$ and its normalized variant $\frac{(x - min(x_{\min},0))}{max(0,x_{max})-min(x_{min},0)} \cdot softmax(x)$. We theoretically show that SA-Softmax provides enhanced gradient properties compared to the vanilla softmax function. Moreover, SA-Softmax Attention can be seamlessly integrated into existing Transformer models to their attention mechanisms with minor adjustments. We conducted experiments to evaluate the empirical performance of Transformer models using SA-Softmax compared to the vanilla softmax function. These experiments, involving models with up to 2.7 billion parameters, are conducted across diverse datasets, language tasks, and positional encoding methods.

### Beyond In-Distribution Success: Scaling Curves of CoT Granularity for Language Model Generalization 
[[arxiv](https://arxiv.org/abs/2502.18273)] [[cool](https://papers.cool/arxiv/2502.18273)] [[pdf](https://arxiv.org/pdf/2502.18273)]
> **Authors**: Ru Wang,Wei Huang,Selena Song,Haoyu Zhang,Yusuke Iwasawa,Yutaka Matsuo,Jiaxian Guo
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Generalization to novel compound tasks under distribution shift is important for deploying transformer-based language models (LMs). This work investigates Chain-of-Thought (CoT) reasoning as a means to enhance OOD generalization. Through controlled experiments across several compound tasks, we reveal three key insights: (1) While QA-trained models achieve near-perfect in-distribution accuracy, their OOD performance degrades catastrophically, even with 10000k+ training examples; (2) the granularity of CoT data strongly correlates with generalization performance; finer-grained CoT data leads to better generalization; (3) CoT exhibits remarkable sample efficiency, matching QA performance with much less (even 80%) data. Theoretically, we demonstrate that compound tasks inherently permit shortcuts in Q-A data that misalign with true reasoning principles, while CoT forces internalization of valid dependency structures, and thus can achieve better generalization. Further, we show that transformer positional embeddings can amplify generalization by emphasizing subtask condition recurrence in long CoT sequences. Our combined theoretical and empirical analysis provides compelling evidence for CoT reasoning as a crucial training paradigm for enabling LM generalization under real-world distributional shifts for compound tasks.

### Debt Collection Negotiations with Large Language Models: An Evaluation System and Optimizing Decision Making with Multi-Agent 
[[arxiv](https://arxiv.org/abs/2502.18228)] [[cool](https://papers.cool/arxiv/2502.18228)] [[pdf](https://arxiv.org/pdf/2502.18228)]
> **Authors**: Xiaofeng Wang,Zhixin Zhang,Jinguang Zheng,Yiming Ai,Rui Wang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 21 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Debt collection negotiations (DCN) are vital for managing non-performing loans (NPLs) and reducing creditor losses. Traditional methods are labor-intensive, while large language models (LLMs) offer promising automation potential. However, prior systems lacked dynamic negotiation and real-time decision-making capabilities. This paper explores LLMs in automating DCN and proposes a novel evaluation framework with 13 metrics across 4 aspects. Our experiments reveal that LLMs tend to over-concede compared to human negotiators. To address this, we propose the Multi-Agent Debt Negotiation (MADeN) framework, incorporating planning and judging modules to improve decision rationality. We also apply post-training techniques, including DPO with rejection sampling, to optimize performance. Our studies provide valuable insights for practitioners and researchers seeking to enhance efficiency and outcomes in this domain.

### Connecting Voices: LoReSpeech as a Low-Resource Speech Parallel Corpus 
[[arxiv](https://arxiv.org/abs/2502.18215)] [[cool](https://papers.cool/arxiv/2502.18215)] [[pdf](https://arxiv.org/pdf/2502.18215)]
> **Authors**: Samy Ouzerrout
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: ISCA/ITG Workshop on Diversity in Large Speech andLanguageModels
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Aligned audio corpora are fundamental to NLP technologies such as ASR and speech translation, yet they remain scarce for underrepresented languages, hindering their technological integration. This paper introduces a methodology for constructing LoReSpeech, a low-resource speech-to-speech translation corpus. Our approach begins with LoReASR, a sub-corpus of short audios aligned with their transcriptions, created through a collaborative platform. Building on LoReASR, long-form audio recordings, such as biblical texts, are aligned using tools like the MFA. LoReSpeech delivers both intra- and inter-language alignments, enabling advancements in multilingual ASR systems, direct speech-to-speech translation models, and linguistic preservation efforts, while fostering digital inclusivity. This work is conducted within Tutlayt AI project (https://tutlayt.fr).

### LAG: LLM agents for Leaderboard Auto Generation on Demanding 
[[arxiv](https://arxiv.org/abs/2502.18209)] [[cool](https://papers.cool/arxiv/2502.18209)] [[pdf](https://arxiv.org/pdf/2502.18209)]
> **Authors**: Jian Wu,Jiayu Zhang,Dongyuan Li,Linyi Yang,Aoxiao Zhong,Renhe Jiang,Qingsong Wen,Yue Zhang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: This paper introduces Leaderboard Auto Generation (LAG), a novel and well-organized framework for automatic generation of leaderboards on a given research topic in rapidly evolving fields like Artificial Intelligence (AI). Faced with a large number of AI papers updated daily, it becomes difficult for researchers to track every paper's proposed methods, experimental results, and settings, prompting the need for efficient automatic leaderboard construction. While large language models (LLMs) offer promise in automating this process, challenges such as multi-document summarization, leaderboard generation, and experiment fair comparison still remain under exploration. LAG solves these challenges through a systematic approach that involves the paper collection, experiment results extraction and integration, leaderboard generation, and quality evaluation. Our contributions include a comprehensive solution to the leaderboard construction problem, a reliable evaluation method, and experimental results showing the high quality of leaderboards.

### Grandes modelos de lenguaje: de la predicción de palabras a la comprensión? 
[[arxiv](https://arxiv.org/abs/2502.18205)] [[cool](https://papers.cool/arxiv/2502.18205)] [[pdf](https://arxiv.org/pdf/2502.18205)]
> **Authors**: Carlos Gómez-Rodríguez
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 26 pages, in Spanish. Chapter from book "La Inteligencia Artificial hoy y sus aplicaciones con Big Data", (Amparo Alonso Betanzos, Daniel Peña y Pilar Poncela, eds.). Publisher: Funcas. ISBN 978-84-17609-94-8
- **标题**: None
- **领域**: 计算语言学,计算机与社会
- **Abstract**: Large language models, such as the well-known ChatGPT, have brought about an unexpected revolution in the field of artificial intelligence. On the one hand, they have numerous practical applications and enormous potential still to be explored. On the other hand, they are also the subject of debate from scientific, philosophical, and social perspectives: there are doubts about the exact mechanisms of their functioning and their actual capacity for language comprehension, and their applications raise ethical dilemmas. In this chapter, we describe how this technology has been developed and the fundamentals of its operation, allowing us to better understand its capabilities and limitations and to introduce some of the main debates surrounding its development and use. -- Los grandes modelos de lenguaje, como el conocido ChatGPT, han supuesto una inesperada revolución en el ámbito de la inteligencia artificial. Por un lado, cuentan con multitud de aplicaciones prácticas y un enorme potencial todavía por explorar. Por otro lado, son también objeto de debate, tanto desde el punto de vista científico y filosófico como social: hay dudas sobre los mecanismos exactos de su funcionamiento y su capacidad real de comprensión del lenguaje, y sus aplicaciones plantean dilemas éticos. En este capítulo describimos cómo se ha llegado a esta tecnología y los fundamentos de su funcionamiento, permitiéndonos así comprender mejor sus capacidades y limitaciones e introducir algunos de los principales debates que rodean su desarrollo y uso.

### Problem Solved? Information Extraction Design Space for Layout-Rich Documents using LLMs 
[[arxiv](https://arxiv.org/abs/2502.18179)] [[cool](https://papers.cool/arxiv/2502.18179)] [[pdf](https://arxiv.org/pdf/2502.18179)]
> **Authors**: Gaye Colakoglu,Gürkan Solmaz,Jonathan Fürst
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: This paper defines and explores the design space for information extraction (IE) from layout-rich documents using large language models (LLMs). The three core challenges of layout-aware IE with LLMs are 1) data structuring, 2) model engagement, and 3) output refinement. Our study delves into the sub-problems within these core challenges, such as input representation, chunking, prompting, and selection of LLMs and multimodal models. It examines the outcomes of different design choices through a new layout-aware IE test suite, benchmarking against the state-of-art (SoA) model LayoutLMv3. The results show that the configuration from one-factor-at-a-time (OFAT) trial achieves near-optimal results with 14.1 points F1-score gain from the baseline model, while full factorial exploration yields only a slightly higher 15.1 points gain at around 36x greater token usage. We demonstrate that well-configured general-purpose LLMs can match the performance of specialized models, providing a cost-effective alternative. Our test-suite is freely available at https://github.com/gayecolakoglu/LayIE-LLM.

### SECURA: Sigmoid-Enhanced CUR Decomposition with Uninterrupted Retention and Low-Rank Adaptation in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.18168)] [[cool](https://papers.cool/arxiv/2502.18168)] [[pdf](https://arxiv.org/pdf/2502.18168)]
> **Authors**: Yuxuan Zhang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: New work on Parameter-Efficient Fine-Tuning (PEFT) for largelanguagemodels. Includes new techniques SigNorm and CABR-LoRA for optimizing fine-tune performance and Knowledge retention
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: With the rapid development of large language models (LLMs), fully fine-tuning (FT) these models has become increasingly impractical due to the high computational demands. Additionally, FT can lead to catastrophic forgetting. As an alternative, Low-Rank Adaptation (LoRA) has been proposed, which fine-tunes only a small subset of parameters, achieving similar performance to FT while significantly reducing resource requirements. However, since LoRA inherits FT's design, the issue of catastrophic forgetting remains. To address these challenges, we propose SECURA: Sigmoid-Enhanced CUR Decomposition LoRA, a novel parameter-efficient fine-tuning (PEFT) variant that mitigates catastrophic forgetting while improving fine-tuning performance. Our method introduces a new normalization technique, SigNorm, to enhance parameter retention and overall performance. SECURA has been evaluated on a variety of tasks, including mathematical problem-solving (GSM8K), challenging question-answering (CNNDM), translation (NewsDE), and complex multiple-choice reasoning (LogiQA). Experimental results show that SECURA achieves an average fine-tuning improvement of 3.59% across four multiple-choice question (MCQ) tasks and a 2.51% improvement across five question-answering (QA) tasks on models such as Gemma2 2b, Qwen2 1.5b, Qwen 2 7b, Llama3 8b, and Llama3.1 8b, compared to DoRA. Moreover, SECURA demonstrates superior knowledge retention capabilities, maintaining more than 70% accuracy on basic LLM knowledge across 16 continual learning tests, outperforming Experience Replay (ER), Sequential Learning (SEQ), EWC, I-LoRA, and CUR-LoRA.

### Can LLMs Explain Themselves Counterfactually? 
[[arxiv](https://arxiv.org/abs/2502.18156)] [[cool](https://papers.cool/arxiv/2502.18156)] [[pdf](https://arxiv.org/pdf/2502.18156)]
> **Authors**: Zahra Dehghanighobadi,Asja Fischer,Muhammad Bilal Zafar
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Explanations are an important tool for gaining insights into the behavior of ML models, calibrating user trust and ensuring regulatory compliance. Past few years have seen a flurry of post-hoc methods for generating model explanations, many of which involve computing model gradients or solving specially designed optimization problems. However, owing to the remarkable reasoning abilities of Large Language Model (LLMs), self-explanation, that is, prompting the model to explain its outputs has recently emerged as a new paradigm. In this work, we study a specific type of self-explanations, self-generated counterfactual explanations (SCEs). We design tests for measuring the efficacy of LLMs in generating SCEs. Analysis over various LLM families, model sizes, temperature settings, and datasets reveals that LLMs sometimes struggle to generate SCEs. Even when they do, their prediction often does not agree with their own counterfactual reasoning.

### NusaAksara: A Multimodal and Multilingual Benchmark for Preserving Indonesian Indigenous Scripts 
[[arxiv](https://arxiv.org/abs/2502.18148)] [[cool](https://papers.cool/arxiv/2502.18148)] [[pdf](https://arxiv.org/pdf/2502.18148)]
> **Authors**: Muhammad Farid Adilazuarda,Musa Izzanardi Wijanarko,Lucky Susanto,Khumaisa Nur'aini,Derry Wijaya,Alham Fikri Aji
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Indonesia is rich in languages and scripts. However, most NLP progress has been made using romanized text. In this paper, we present NusaAksara, a novel public benchmark for Indonesian languages that includes their original scripts. Our benchmark covers both text and image modalities and encompasses diverse tasks such as image segmentation, OCR, transliteration, translation, and language identification. Our data is constructed by human experts through rigorous steps. NusaAksara covers 8 scripts across 7 languages, including low-resource languages not commonly seen in NLP benchmarks. Although unsupported by Unicode, the Lampung script is included in this dataset. We benchmark our data across several models, from LLMs and VLMs such as GPT-4o, Llama 3.2, and Aya 23 to task-specific systems such as PP-OCR and LangID, and show that most NLP technologies cannot handle Indonesia's local scripts, with many achieving near-zero performance.

### LevelRAG: Enhancing Retrieval-Augmented Generation with Multi-hop Logic Planning over Rewriting Augmented Searchers 
[[arxiv](https://arxiv.org/abs/2502.18139)] [[cool](https://papers.cool/arxiv/2502.18139)] [[pdf](https://arxiv.org/pdf/2502.18139)]
> **Authors**: Zhuocheng Zhang,Yang Feng,Min Zhang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: First submit
- **标题**: None
- **领域**: 计算语言学,信息检索
- **Abstract**: Retrieval-Augmented Generation (RAG) is a crucial method for mitigating hallucinations in Large Language Models (LLMs) and integrating external knowledge into their responses. Existing RAG methods typically employ query rewriting to clarify the user intent and manage multi-hop logic, while using hybrid retrieval to expand search scope. However, the tight coupling of query rewriting to the dense retriever limits its compatibility with hybrid retrieval, impeding further RAG performance improvements. To address this challenge, we introduce a high-level searcher that decomposes complex queries into atomic queries, independent of any retriever-specific optimizations. Additionally, to harness the strengths of sparse retrievers for precise keyword retrieval, we have developed a new sparse searcher that employs Lucene syntax to enhance retrieval accuracy.Alongside web and dense searchers, these components seamlessly collaborate within our proposed method, \textbf{LevelRAG}. In LevelRAG, the high-level searcher orchestrates the retrieval logic, while the low-level searchers (sparse, web, and dense) refine the queries for optimal retrieval. This approach enhances both the completeness and accuracy of the retrieval process, overcoming challenges associated with current query rewriting techniques in hybrid retrieval scenarios. Empirical experiments conducted on five datasets, encompassing both single-hop and multi-hop question answering tasks, demonstrate the superior performance of LevelRAG compared to existing RAG methods. Notably, LevelRAG outperforms the state-of-the-art proprietary model, GPT4o, underscoring its effectiveness and potential impact on the RAG field.

### Uncertainty Quantification in Retrieval Augmented Question Answering 
[[arxiv](https://arxiv.org/abs/2502.18108)] [[cool](https://papers.cool/arxiv/2502.18108)] [[pdf](https://arxiv.org/pdf/2502.18108)]
> **Authors**: Laura Perez-Beltrachini,Mirella Lapata
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Retrieval augmented Question Answering (QA) helps QA models overcome knowledge gaps by incorporating retrieved evidence, typically a set of passages, alongside the question at test time. Previous studies show that this approach improves QA performance and reduces hallucinations, without, however, assessing whether the retrieved passages are indeed useful at answering correctly. In this work, we propose to quantify the uncertainty of a QA model via estimating the utility of the passages it is provided with. We train a lightweight neural model to predict passage utility for a target QA model and show that while simple information theoretic metrics can predict answer correctness up to a certain extent, our approach efficiently approximates or outperforms more expensive sampling-based methods. Code and data are available at https://github.com/lauhaide/ragu.

### Towards Thinking-Optimal Scaling of Test-Time Compute for LLM Reasoning 
[[arxiv](https://arxiv.org/abs/2502.18080)] [[cool](https://papers.cool/arxiv/2502.18080)] [[pdf](https://arxiv.org/pdf/2502.18080)]
> **Authors**: Wenkai Yang,Shuming Ma,Yankai Lin,Furu Wei
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Recent studies have shown that making a model spend more time thinking through longer Chain of Thoughts (CoTs) enables it to gain significant improvements in complex reasoning tasks. While current researches continue to explore the benefits of increasing test-time compute by extending the CoT lengths of Large Language Models (LLMs), we are concerned about a potential issue hidden behind the current pursuit of test-time scaling: Would excessively scaling the CoT length actually bring adverse effects to a model's reasoning performance? Our explorations on mathematical reasoning tasks reveal an unexpected finding that scaling with longer CoTs can indeed impair the reasoning performance of LLMs in certain domains. Moreover, we discover that there exists an optimal scaled length distribution that differs across different domains. Based on these insights, we propose a Thinking-Optimal Scaling strategy. Our method first uses a small set of seed data with varying response length distributions to teach the model to adopt different reasoning efforts for deep thinking. Then, the model selects its shortest correct response under different reasoning efforts on additional problems for self-improvement. Our self-improved models built upon Qwen2.5-32B-Instruct outperform other distillation-based 32B o1-like models across various math benchmarks, and achieve performance on par with QwQ-32B-Preview.

### Uncertainty-aware abstention in medical diagnosis based on medical texts 
[[arxiv](https://arxiv.org/abs/2502.18050)] [[cool](https://papers.cool/arxiv/2502.18050)] [[pdf](https://arxiv.org/pdf/2502.18050)]
> **Authors**: Artem Vazhentsev,Ivan Sviridov,Alvard Barseghyan,Gleb Kuzmin,Alexander Panchenko,Aleksandr Nesterov,Artem Shelmanov,Maxim Panov
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: This study addresses the critical issue of reliability for AI-assisted medical diagnosis. We focus on the selection prediction approach that allows the diagnosis system to abstain from providing the decision if it is not confident in the diagnosis. Such selective prediction (or abstention) approaches are usually based on the modeling predictive uncertainty of machine learning models involved. This study explores uncertainty quantification in machine learning models for medical text analysis, addressing diverse tasks across multiple datasets. We focus on binary mortality prediction from textual data in MIMIC-III, multi-label medical code prediction using ICD-10 codes from MIMIC-IV, and multi-class classification with a private outpatient visits dataset. Additionally, we analyze mental health datasets targeting depression and anxiety detection, utilizing various text-based sources, such as essays, social media posts, and clinical descriptions. In addition to comparing uncertainty methods, we introduce HUQ-2, a new state-of-the-art method for enhancing reliability in selective prediction tasks. Our results provide a detailed comparison of uncertainty quantification methods. They demonstrate the effectiveness of HUQ-2 in capturing and evaluating uncertainty, paving the way for more reliable and interpretable applications in medical text analysis.

### Harnessing Multiple Large Language Models: A Survey on LLM Ensemble 
[[arxiv](https://arxiv.org/abs/2502.18036)] [[cool](https://papers.cool/arxiv/2502.18036)] [[pdf](https://arxiv.org/pdf/2502.18036)]
> **Authors**: Zhijun Chen,Jingzheng Li,Pengpeng Chen,Zhuoran Li,Kai Sun,Yuankai Luo,Qianren Mao,Dingqi Yang,Hailong Sun,Philip S. Yu
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 9 pages, 2 figures, codebase: https://github.com/junchenzhi/Awesome-LLM-Ensemble
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: LLM Ensemble -- which involves the comprehensive use of multiple large language models (LLMs), each aimed at handling user queries during downstream inference, to benefit from their individual strengths -- has gained substantial attention recently. The widespread availability of LLMs, coupled with their varying strengths and out-of-the-box usability, has profoundly advanced the field of LLM Ensemble. This paper presents the first systematic review of recent developments in LLM Ensemble. First, we introduce our taxonomy of LLM Ensemble and discuss several related research problems. Then, we provide a more in-depth classification of the methods under the broad categories of "ensemble-before-inference, ensemble-during-inference, ensemble-after-inference", and review all relevant methods. Finally, we introduce related benchmarks and applications, summarize existing studies, and suggest several future research directions. A curated list of papers on LLM Ensemble is available at https://github.com/junchenzhi/Awesome-LLM-Ensemble.

### Detecting Knowledge Boundary of Vision Large Language Models by Sampling-Based Inference 
[[arxiv](https://arxiv.org/abs/2502.18023)] [[cool](https://papers.cool/arxiv/2502.18023)] [[pdf](https://arxiv.org/pdf/2502.18023)]
> **Authors**: Zhuo Chen,Xinyu Wang,Yong Jiang,Zhen Zhang,Xinyu Geng,Pengjun Xie,Fei Huang,Kewei Tu
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Under review
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Despite the advancements made in Visual Large Language Models (VLLMs), like text Large Language Models (LLMs), they have limitations in addressing questions that require real-time information or are knowledge-intensive. Indiscriminately adopting Retrieval Augmented Generation (RAG) techniques is an effective yet expensive way to enable models to answer queries beyond their knowledge scopes. To mitigate the dependence on retrieval and simultaneously maintain, or even improve, the performance benefits provided by retrieval, we propose a method to detect the knowledge boundary of VLLMs, allowing for more efficient use of techniques like RAG. Specifically, we propose a method with two variants that fine-tunes a VLLM on an automatically constructed dataset for boundary identification. Experimental results on various types of Visual Question Answering datasets show that our method successfully depicts a VLLM's knowledge boundary based on which we are able to reduce indiscriminate retrieval while maintaining or improving the performance. In addition, we show that the knowledge boundary identified by our method for one VLLM can be used as a surrogate boundary for other VLLMs. Code will be released at https://github.com/Chord-Chen-30/VLLM-KnowledgeBoundary

### AfroXLMR-Comet: Multilingual Knowledge Distillation with Attention Matching for Low-Resource languages 
[[arxiv](https://arxiv.org/abs/2502.18020)] [[cool](https://papers.cool/arxiv/2502.18020)] [[pdf](https://arxiv.org/pdf/2502.18020)]
> **Authors**: Joshua Sakthivel Raju,Sanjay S,Jaskaran Singh Walia,Srinivas Raghav,Vukosi Marivate
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,信息检索,机器学习
- **Abstract**: Language model compression through knowledge distillation has emerged as a promising approach for deploying large language models in resource-constrained environments. However, existing methods often struggle to maintain performance when distilling multilingual models, especially for low-resource languages. In this paper, we present a novel hybrid distillation approach that combines traditional knowledge distillation with a simplified attention matching mechanism, specifically designed for multilingual contexts. Our method introduces an extremely compact student model architecture, significantly smaller than conventional multilingual models. We evaluate our approach on five African languages: Kinyarwanda, Swahili, Hausa, Igbo, and Yoruba. The distilled student model; AfroXLMR-Comet successfully captures both the output distribution and internal attention patterns of a larger teacher model (AfroXLMR-Large) while reducing the model size by over 85%. Experimental results demonstrate that our hybrid approach achieves competitive performance compared to the teacher model, maintaining an accuracy within 85% of the original model's performance while requiring substantially fewer computational resources. Our work provides a practical framework for deploying efficient multilingual models in resource-constrained environments, particularly benefiting applications involving African languages.

### Verdict: A Library for Scaling Judge-Time Compute 
[[arxiv](https://arxiv.org/abs/2502.18018)] [[cool](https://papers.cool/arxiv/2502.18018)] [[pdf](https://arxiv.org/pdf/2502.18018)]
> **Authors**: Nimit Kalra,Leonard Tang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The use of LLMs as automated judges ("LLM-as-a-judge") is now widespread, yet standard judges suffer from a multitude of reliability issues. To address these challenges, we introduce Verdict, an open-source library for scaling judge-time compute to enhance the accuracy, reliability, and interpretability of automated evaluators. Verdict leverages the composition of modular reasoning units -- such as verification, debate, and aggregation -- and increased inference-time compute to improve LLM judge quality. Across a variety of challenging tasks such as content moderation, fact-checking, and hallucination detection, Verdict judges achieve state-of-the-art (SOTA) or near-SOTA performance, surpassing orders-of-magnitude larger fine-tuned judges, prompted judges, and reasoning models. Ultimately, we hope Verdict serves as a useful framework for researchers and practitioners building scalable, interpretable, and reliable LLM-based evaluators.

### Unveiling the Key Factors for Distilling Chain-of-Thought Reasoning 
[[arxiv](https://arxiv.org/abs/2502.18001)] [[cool](https://papers.cool/arxiv/2502.18001)] [[pdf](https://arxiv.org/pdf/2502.18001)]
> **Authors**: Xinghao Chen,Zhijing Sun,Wenjin Guo,Miaoran Zhang,Yanjun Chen,Yirong Sun,Hui Su,Yijie Pan,Dietrich Klakow,Wenjie Li,Xiaoyu Shen
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models (LLMs) excel in reasoning tasks through Chain-of-Thought (CoT) prompting. However, CoT prompting greatly increases computational demands, which has prompted growing interest in distilling CoT capabilities into Small Language Models (SLMs). This study systematically examines the factors influencing CoT distillation, including the choice of granularity, format and teacher model. Through experiments involving four teacher models and seven student models across seven mathematical and commonsense reasoning datasets, we uncover three key findings: (1) Unlike LLMs, SLMs exhibit a non-monotonic relationship with granularity, with stronger models benefiting from finer-grained reasoning and weaker models performing better with simpler CoT supervision; (2) CoT format significantly impacts LLMs but has minimal effect on SLMs, likely due to their reliance on supervised fine-tuning rather than pretraining preferences; (3) Stronger teacher models do NOT always produce better student models, as diversity and complexity in CoT supervision can outweigh accuracy alone. These findings emphasize the need to tailor CoT strategies to specific student model, offering actionable insights for optimizing CoT distillation in SLMs. The code and datasets are available at https://github.com/EIT-NLP/Distilling-CoT-Reasoning.

### MAGE: Multi-Head Attention Guided Embeddings for Low Resource Sentiment Classification 
[[arxiv](https://arxiv.org/abs/2502.17987)] [[cool](https://papers.cool/arxiv/2502.17987)] [[pdf](https://arxiv.org/pdf/2502.17987)]
> **Authors**: Varun Vashisht,Samar Singh,Mihir Konduskar,Jaskaran Singh Walia,Vukosi Marivate
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,信息检索,机器学习
- **Abstract**: Due to the lack of quality data for low-resource Bantu languages, significant challenges are presented in text classification and other practical implementations. In this paper, we introduce an advanced model combining Language-Independent Data Augmentation (LiDA) with Multi-Head Attention based weighted embeddings to selectively enhance critical data points and improve text classification performance. This integration allows us to create robust data augmentation strategies that are effective across various linguistic contexts, ensuring that our model can handle the unique syntactic and semantic features of Bantu languages. This approach not only addresses the data scarcity issue but also sets a foundation for future research in low-resource language processing and classification tasks.

### On Synthetic Data Strategies for Domain-Specific Generative Retrieval 
[[arxiv](https://arxiv.org/abs/2502.17957)] [[cool](https://papers.cool/arxiv/2502.17957)] [[pdf](https://arxiv.org/pdf/2502.17957)]
> **Authors**: Haoyang Wen,Jiang Guo,Yi Zhang,Jiarong Jiang,Zhiguo Wang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,信息检索
- **Abstract**: This paper investigates synthetic data generation strategies in developing generative retrieval models for domain-specific corpora, thereby addressing the scalability challenges inherent in manually annotating in-domain queries. We study the data strategies for a two-stage training framework: in the first stage, which focuses on learning to decode document identifiers from queries, we investigate LLM-generated queries across multiple granularity (e.g. chunks, sentences) and domain-relevant search constraints that can better capture nuanced relevancy signals. In the second stage, which aims to refine document ranking through preference learning, we explore the strategies for mining hard negatives based on the initial model's predictions. Experiments on public datasets over diverse domains demonstrate the effectiveness of our synthetic data generation and hard negative sampling approach.

### Towards Better Understanding of Program-of-Thought Reasoning in Cross-Lingual and Multilingual Environments 
[[arxiv](https://arxiv.org/abs/2502.17956)] [[cool](https://papers.cool/arxiv/2502.17956)] [[pdf](https://arxiv.org/pdf/2502.17956)]
> **Authors**: Patomporn Payoungkhamdee,Pume Tuchinda,Jinheon Baek,Samuel Cahyawijaya,Can Udomcharoenchaikit,Potsawee Manakul,Peerat Limkonchotiwat,Ekapol Chuangsuwanich,Sarana Nutanong
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Multi-step reasoning is essential for large language models (LLMs), yet multilingual performance remains challenging. While Chain-of-Thought (CoT) prompting improves reasoning, it struggles with non-English languages due to the entanglement of reasoning and execution. Program-of-Thought (PoT) prompting separates reasoning from execution, offering a promising alternative but shifting the challenge to generating programs from non-English questions. We propose a framework to evaluate PoT by separating multilingual reasoning from code execution to examine (i) the impact of fine-tuning on question-reasoning alignment and (ii) how reasoning quality affects answer correctness. Our findings demonstrate that PoT fine-tuning substantially enhances multilingual reasoning, outperforming CoT fine-tuned models. We further demonstrate a strong correlation between reasoning quality (measured through code quality) and answer accuracy, highlighting its potential as a test-time performance improvement heuristic.

### Language Models' Factuality Depends on the Language of Inquiry 
[[arxiv](https://arxiv.org/abs/2502.17955)] [[cool](https://papers.cool/arxiv/2502.17955)] [[pdf](https://arxiv.org/pdf/2502.17955)]
> **Authors**: Tushar Aggarwal,Kumar Tanmay,Ayush Agrawal,Kumar Ayush,Hamid Palangi,Paul Pu Liang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Multilingual language models (LMs) are expected to recall factual knowledge consistently across languages, yet they often fail to transfer knowledge between languages even when they possess the correct information in one of the languages. For example, we find that an LM may correctly identify Rashed Al Shashai as being from Saudi Arabia when asked in Arabic, but consistently fails to do so when asked in English or Swahili. To systematically investigate this limitation, we introduce a benchmark of 10,000 country-related facts across 13 languages and propose three novel metrics: Factual Recall Score, Knowledge Transferability Score, and Cross-Lingual Factual Knowledge Transferability Score-to quantify factual recall and knowledge transferability in LMs across different languages. Our results reveal fundamental weaknesses in today's state-of-the-art LMs, particularly in cross-lingual generalization where models fail to transfer knowledge effectively across different languages, leading to inconsistent performance sensitive to the language used. Our findings emphasize the need for LMs to recognize language-specific factual reliability and leverage the most trustworthy information across languages. We release our benchmark and evaluation framework to drive future research in multilingual knowledge transfer.

### DeepSeek-R1 Outperforms Gemini 2.0 Pro, OpenAI o1, and o3-mini in Bilingual Complex Ophthalmology Reasoning 
[[arxiv](https://arxiv.org/abs/2502.17947)] [[cool](https://papers.cool/arxiv/2502.17947)] [[pdf](https://arxiv.org/pdf/2502.17947)]
> **Authors**: Pusheng Xu,Yue Wu,Kai Jin,Xiaolan Chen,Mingguang He,Danli Shi
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 29 pages, 4 figures, 1 table
- **标题**: None
- **领域**: 计算语言学,人工智能,表现
- **Abstract**: Purpose: To evaluate the accuracy and reasoning ability of DeepSeek-R1 and three other recently released large language models (LLMs) in bilingual complex ophthalmology cases. Methods: A total of 130 multiple-choice questions (MCQs) related to diagnosis (n = 39) and management (n = 91) were collected from the Chinese ophthalmology senior professional title examination and categorized into six topics. These MCQs were translated into English using DeepSeek-R1. The responses of DeepSeek-R1, Gemini 2.0 Pro, OpenAI o1 and o3-mini were generated under default configurations between February 15 and February 20, 2025. Accuracy was calculated as the proportion of correctly answered questions, with omissions and extra answers considered incorrect. Reasoning ability was evaluated through analyzing reasoning logic and the causes of reasoning error. Results: DeepSeek-R1 demonstrated the highest overall accuracy, achieving 0.862 in Chinese MCQs and 0.808 in English MCQs. Gemini 2.0 Pro, OpenAI o1, and OpenAI o3-mini attained accuracies of 0.715, 0.685, and 0.692 in Chinese MCQs (all P<0.001 compared with DeepSeek-R1), and 0.746 (P=0.115), 0.723 (P=0.027), and 0.577 (P<0.001) in English MCQs, respectively. DeepSeek-R1 achieved the highest accuracy across five topics in both Chinese and English MCQs. It also excelled in management questions conducted in Chinese (all P<0.05). Reasoning ability analysis showed that the four LLMs shared similar reasoning logic. Ignoring key positive history, ignoring key positive signs, misinterpretation medical data, and too aggressive were the most common causes of reasoning errors. Conclusion: DeepSeek-R1 demonstrated superior performance in bilingual complex ophthalmology reasoning tasks than three other state-of-the-art LLMs. While its clinical applicability remains challenging, it shows promise for supporting diagnosis and clinical decision-making.

### Assessing Large Language Models in Agentic Multilingual National Bias 
[[arxiv](https://arxiv.org/abs/2502.17945)] [[cool](https://papers.cool/arxiv/2502.17945)] [[pdf](https://arxiv.org/pdf/2502.17945)]
> **Authors**: Qianying Liu,Katrina Qiyao Wang,Fei Cheng,Sadao Kurohashi
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 13 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models have garnered significant attention for their capabilities in multilingual natural language processing, while studies on risks associated with cross biases are limited to immediate context preferences. Cross-language disparities in reasoning-based recommendations remain largely unexplored, with a lack of even descriptive analysis. This study is the first to address this gap. We test LLM's applicability and capability in providing personalized advice across three key scenarios: university applications, travel, and relocation. We investigate multilingual bias in state-of-the-art LLMs by analyzing their responses to decision-making tasks across multiple languages. We quantify bias in model-generated scores and assess the impact of demographic factors and reasoning strategies (e.g., Chain-of-Thought prompting) on bias patterns. Our findings reveal that local language bias is prevalent across different tasks, with GPT-4 and Sonnet reducing bias for English-speaking countries compared to GPT-3.5 but failing to achieve robust multilingual alignment, highlighting broader implications for multilingual AI agents and applications such as education.

### CaseGen: A Benchmark for Multi-Stage Legal Case Documents Generation 
[[arxiv](https://arxiv.org/abs/2502.17943)] [[cool](https://papers.cool/arxiv/2502.17943)] [[pdf](https://arxiv.org/pdf/2502.17943)]
> **Authors**: Haitao Li,Jiaying Ye,Yiran Hu,Jia Chen,Qingyao Ai,Yueyue Wu,Junjie Chen,Yifan Chen,Cheng Luo,Quan Zhou,Yiqun Liu
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 18 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Legal case documents play a critical role in judicial proceedings. As the number of cases continues to rise, the reliance on manual drafting of legal case documents is facing increasing pressure and challenges. The development of large language models (LLMs) offers a promising solution for automating document generation. However, existing benchmarks fail to fully capture the complexities involved in drafting legal case documents in real-world scenarios. To address this gap, we introduce CaseGen, the benchmark for multi-stage legal case documents generation in the Chinese legal domain. CaseGen is based on 500 real case samples annotated by legal experts and covers seven essential case sections. It supports four key tasks: drafting defense statements, writing trial facts, composing legal reasoning, and generating judgment results. To the best of our knowledge, CaseGen is the first benchmark designed to evaluate LLMs in the context of legal case document generation. To ensure an accurate and comprehensive evaluation, we design the LLM-as-a-judge evaluation framework and validate its effectiveness through human annotations. We evaluate several widely used general-domain LLMs and legal-specific LLMs, highlighting their limitations in case document generation and pinpointing areas for potential improvement. This work marks a step toward a more effective framework for automating legal case documents drafting, paving the way for the reliable application of AI in the legal field. The dataset and code are publicly available at https://github.com/CSHaitao/CaseGen.

### Advantage-Guided Distillation for Preference Alignment in Small Language Models 
[[arxiv](https://arxiv.org/abs/2502.17927)] [[cool](https://papers.cool/arxiv/2502.17927)] [[pdf](https://arxiv.org/pdf/2502.17927)]
> **Authors**: Shiping Gao,Fanqi Wan,Jiajian Guo,Xiaojun Quan,Qifan Wang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Accepted by ICLR 2025(spotlight)
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Alignment techniques enable Large Language Models (LLMs) to generate outputs that align with human preferences and play a crucial role in their effectiveness. However, their impact often diminishes when applied to Small Language Models (SLMs), likely due to the limited capacity of these models. Instead of directly applying existing alignment techniques to SLMs, we propose to utilize a well-aligned teacher LLM to guide the alignment process for these models, thereby facilitating the transfer of the teacher's knowledge of human preferences to the student model. To achieve this, we first explore a straightforward approach, Dual-Constrained Knowledge Distillation (DCKD), that employs knowledge distillation with two KL-divergence constraints from the aligned teacher to the unaligned student. To further enhance the student's ability to distinguish between preferred and dispreferred responses, we then propose Advantage-Guided Distillation for Preference Alignment (ADPA), which leverages an advantage function from the aligned teacher to deliver more nuanced, distribution-level reward signals for the student's alignment. Our experimental results show that these two approaches appreciably improve the alignment of SLMs and narrow the performance gap with larger counterparts. Among them, ADPA demonstrates superior performance and achieves even greater effectiveness when integrated with DCKD. Our code is available at https://github.com/SLIT-AI/ADPA.

### FACT-AUDIT: An Adaptive Multi-Agent Framework for Dynamic Fact-Checking Evaluation of Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.17924)] [[cool](https://papers.cool/arxiv/2502.17924)] [[pdf](https://arxiv.org/pdf/2502.17924)]
> **Authors**: Hongzhan Lin,Yang Deng,Yuxuan Gu,Wenxuan Zhang,Jing Ma,See-Kiong Ng,Tat-Seng Chua
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models (LLMs) have significantly advanced the fact-checking studies. However, existing automated fact-checking evaluation methods rely on static datasets and classification metrics, which fail to automatically evaluate the justification production and uncover the nuanced limitations of LLMs in fact-checking. In this work, we introduce FACT-AUDIT, an agent-driven framework that adaptively and dynamically assesses LLMs' fact-checking capabilities. Leveraging importance sampling principles and multi-agent collaboration, FACT-AUDIT generates adaptive and scalable datasets, performs iterative model-centric evaluations, and updates assessments based on model-specific responses. By incorporating justification production alongside verdict prediction, this framework provides a comprehensive and evolving audit of LLMs' factual reasoning capabilities, to investigate their trustworthiness. Extensive experiments demonstrate that FACT-AUDIT effectively differentiates among state-of-the-art LLMs, providing valuable insights into model strengths and limitations in model-centric fact-checking analysis.

### Scaling LLM Pre-training with Vocabulary Curriculum 
[[arxiv](https://arxiv.org/abs/2502.17910)] [[cool](https://papers.cool/arxiv/2502.17910)] [[pdf](https://arxiv.org/pdf/2502.17910)]
> **Authors**: Fangyuan Yu
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: under review
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Modern language models rely on static vocabularies, fixed before pretraining, in contrast to the adaptive vocabulary acquisition observed in human language learning. To bridge this gap, we introduce vocabulary curriculum learning, an approach that improves pretraining efficiency with log-linear scaling gains relative to vocabulary size. Our method alternates between entropy-guided vocabulary expansion and model optimization, enabling models to learn transferable representations across diverse tokenization granularities. This approach naturally gives rise to an optimal computation allocation pattern: longer tokens capture predictable content, while shorter tokens focus on more complex, harder-to-predict contexts. Experiments on small-scale GPT models demonstrate improved scaling efficiency, reinforcing the effectiveness of dynamic tokenization. We release our code to support further research and plan to extend our experiments to larger models and diverse domains.

### Can Large Language Models Identify Implicit Suicidal Ideation? An Empirical Evaluation 
[[arxiv](https://arxiv.org/abs/2502.17899)] [[cool](https://papers.cool/arxiv/2502.17899)] [[pdf](https://arxiv.org/pdf/2502.17899)]
> **Authors**: Tong Li,Shu Yang,Junchao Wu,Jiyao Wei,Lijie Hu,Mengdi Li,Derek F. Wong,Joshua R. Oltmanns,Di Wang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: We present a comprehensive evaluation framework for assessing Large Language Models' (LLMs) capabilities in suicide prevention, focusing on two critical aspects: the Identification of Implicit Suicidal ideation (IIS) and the Provision of Appropriate Supportive responses (PAS). We introduce \ourdata, a novel dataset of 1,308 test cases built upon psychological frameworks including D/S-IAT and Negative Automatic Thinking, alongside real-world scenarios. Through extensive experiments with 8 widely used LLMs under different contextual settings, we find that current models struggle significantly with detecting implicit suicidal ideation and providing appropriate support, highlighting crucial limitations in applying LLMs to mental health contexts. Our findings underscore the need for more sophisticated approaches in developing and evaluating LLMs for sensitive psychological applications.

### RankCoT: Refining Knowledge for Retrieval-Augmented Generation through Ranking Chain-of-Thoughts 
[[arxiv](https://arxiv.org/abs/2502.17888)] [[cool](https://papers.cool/arxiv/2502.17888)] [[pdf](https://arxiv.org/pdf/2502.17888)]
> **Authors**: Mingyan Wu,Zhenghao Liu,Yukun Yan,Xinze Li,Shi Yu,Zheni Zeng,Yu Gu,Ge Yu
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Retrieval-Augmented Generation (RAG) enhances the performance of Large Language Models (LLMs) by incorporating external knowledge. However, LLMs still encounter challenges in effectively utilizing the knowledge from retrieved documents, often being misled by irrelevant or noisy information. To address this issue, we introduce RankCoT, a knowledge refinement method that incorporates reranking signals in generating CoT-based summarization for knowledge refinement based on given query and all retrieval documents. During training, RankCoT prompts the LLM to generate Chain-of-Thought (CoT) candidates based on the query and individual documents. It then fine-tunes the LLM to directly reproduce the best CoT from these candidate outputs based on all retrieved documents, which requires LLM to filter out irrelevant documents during generating CoT-style summarization. Additionally, RankCoT incorporates a self-reflection mechanism that further refines the CoT outputs, resulting in higher-quality training data. Our experiments demonstrate the effectiveness of RankCoT, showing its superior performance over other knowledge refinement models. Further analysis reveals that RankCoT can provide shorter but effective refinement results, enabling the generator to produce more accurate answers. All code and data are available at https://github.com/NEUIR/RankCoT.

### Towards Enhanced Immersion and Agency for LLM-based Interactive Drama 
[[arxiv](https://arxiv.org/abs/2502.17878)] [[cool](https://papers.cool/arxiv/2502.17878)] [[pdf](https://arxiv.org/pdf/2502.17878)]
> **Authors**: Hongqiu Wu,Weiqi Wu,Tianyang Xu,Jiameng Zhang,Hai Zhao
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: LLM-based Interactive Drama is a novel AI-based dialogue scenario, where the user (i.e. the player) plays the role of a character in the story, has conversations with characters played by LLM agents, and experiences an unfolding story. This paper begins with understanding interactive drama from two aspects: Immersion, the player's feeling of being present in the story, and Agency, the player's ability to influence the story world. Both are crucial to creating an enjoyable interactive experience, while they have been underexplored in previous work. To enhance these two aspects, we first propose Playwriting-guided Generation, a novel method that helps LLMs craft dramatic stories with substantially improved structures and narrative quality. Additionally, we introduce Plot-based Reflection for LLM agents to refine their reactions to align with the player's intentions. Our evaluation relies on human judgment to assess the gains of our methods in terms of immersion and agency.

### SYNTHEMPATHY: A Scalable Empathy Corpus Generated Using LLMs Without Any Crowdsourcing 
[[arxiv](https://arxiv.org/abs/2502.17857)] [[cool](https://papers.cool/arxiv/2502.17857)] [[pdf](https://arxiv.org/pdf/2502.17857)]
> **Authors**: Run Chen,Jun Shin,Julia Hirschberg
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 10 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Previous research has shown that humans are more receptive towards language models that that exhibit empathetic behavior. While empathy is essential for developing helpful dialogue agents, very few large corpora containing empathetic dialogues are available for fine-tune LLMs. The few existing corpora have largely relied on crowdsourcing to simulate empathetic conversations, a process that is expensive, time-consuming, and not scalable to larger datasets. We propose a data generation framework for developing SYNTHEMPATHY, a large corpus containing 105k empathetic responses to real-life situations compiled through LLM generation. A base Mistral 7B model fine-tuned on our SYNTHEMPATHY corpus exhibits an increase in the average empathy score.

## 密码学和安全(cs.CR:Cryptography and Security)

### Toward Breaking Watermarks in Distortion-free Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.18608)] [[cool](https://papers.cool/arxiv/2502.18608)] [[pdf](https://arxiv.org/pdf/2502.18608)]
> **Authors**: Shayleen Reynolds,Saheed Obitayo,Niccolò Dalmasso,Dung Daniel T. Ngo,Vamsi K. Potluru,Manuela Veloso
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 5 pages, AAAI'25 Workshop on Preventing and DetectingLLMGenerated Misinformation
- **标题**: None
- **领域**: 密码学和安全,机器学习
- **Abstract**: In recent years, LLM watermarking has emerged as an attractive safeguard against AI-generated content, with promising applications in many real-world domains. However, there are growing concerns that the current LLM watermarking schemes are vulnerable to expert adversaries wishing to reverse-engineer the watermarking mechanisms. Prior work in "breaking" or "stealing" LLM watermarks mainly focuses on the distribution-modifying algorithm of Kirchenbauer et al. (2023), which perturbs the logit vector before sampling. In this work, we focus on reverse-engineering the other prominent LLM watermarking scheme, distortion-free watermarking (Kuditipudi et al. 2024), which preserves the underlying token distribution by using a hidden watermarking key sequence. We demonstrate that, even under a more sophisticated watermarking scheme, it is possible to "compromise" the LLM and carry out a "spoofing" attack. Specifically, we propose a mixed integer linear programming framework that accurately estimates the secret key used for watermarking using only a few samples of the watermarked dataset. Our initial findings challenge the current theoretical claims on the robustness and usability of existing LLM watermarking techniques.

### Steganography Beyond Space-Time With Chain of Multimodal AI Agents 
[[arxiv](https://arxiv.org/abs/2502.18547)] [[cool](https://papers.cool/arxiv/2502.18547)] [[pdf](https://arxiv.org/pdf/2502.18547)]
> **Authors**: Ching-Chun Chang,Isao Echizen
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能,多代理系统,多媒体
- **Abstract**: Steganography is the art and science of covert writing, with a broad range of applications interwoven within the realm of cybersecurity. As artificial intelligence continues to evolve, its ability to synthesise realistic content emerges as a threat in the hands of cybercriminals who seek to manipulate and misrepresent the truth. Such synthetic content introduces a non-trivial risk of overwriting the subtle changes made for the purpose of steganography. When the signals in both the spatial and temporal domains are vulnerable to unforeseen overwriting, it calls for reflection on what can remain invariant after all. This study proposes a paradigm in steganography for audiovisual media, where messages are concealed beyond both spatial and temporal domains. A chain of multimodal agents is developed to deconstruct audiovisual content into a cover text, embed a message within the linguistic domain, and then reconstruct the audiovisual content through synchronising both aural and visual modalities with the resultant stego text. The message is encoded by biasing the word sampling process of a language generation model and decoded by analysing the probability distribution of word choices. The accuracy of message transmission is evaluated under both zero-bit and multi-bit capacity settings. Fidelity is assessed through both biometric and semantic similarities, capturing the identities of the recorded face and voice, as well as the core ideas conveyed through the media. Secrecy is examined through statistical comparisons between cover and stego texts. Robustness is tested across various scenarios, including audiovisual compression, face-swapping, voice-cloning and their combinations.

### PII-Bench: Evaluating Query-Aware Privacy Protection Systems 
[[arxiv](https://arxiv.org/abs/2502.18545)] [[cool](https://papers.cool/arxiv/2502.18545)] [[pdf](https://arxiv.org/pdf/2502.18545)]
> **Authors**: Hao Shen,Zhouhong Gu,Haokai Hong,Weili Han
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能,计算语言学,信息检索
- **Abstract**: The widespread adoption of Large Language Models (LLMs) has raised significant privacy concerns regarding the exposure of personally identifiable information (PII) in user prompts. To address this challenge, we propose a query-unrelated PII masking strategy and introduce PII-Bench, the first comprehensive evaluation framework for assessing privacy protection systems. PII-Bench comprises 2,842 test samples across 55 fine-grained PII categories, featuring diverse scenarios from single-subject descriptions to complex multi-party interactions. Each sample is carefully crafted with a user query, context description, and standard answer indicating query-relevant PII. Our empirical evaluation reveals that while current models perform adequately in basic PII detection, they show significant limitations in determining PII query relevance. Even state-of-the-art LLMs struggle with this task, particularly in handling complex multi-subject scenarios, indicating substantial room for improvement in achieving intelligent PII masking.

### A Survey of Zero-Knowledge Proof Based Verifiable Machine Learning 
[[arxiv](https://arxiv.org/abs/2502.18535)] [[cool](https://papers.cool/arxiv/2502.18535)] [[pdf](https://arxiv.org/pdf/2502.18535)]
> **Authors**: Zhizhi Peng,Taotao Wang,Chonghe Zhao,Guofu Liao,Zibin Lin,Yifeng Liu,Bin Cao,Long Shi,Qing Yang,Shengli Zhang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 24 pages, 5 figures, 3 tables
- **标题**: None
- **领域**: 密码学和安全,人工智能,机器学习
- **Abstract**: As machine learning technologies advance rapidly across various domains, concerns over data privacy and model security have grown significantly. These challenges are particularly pronounced when models are trained and deployed on cloud platforms or third-party servers due to the computational resource limitations of users' end devices. In response, zero-knowledge proof (ZKP) technology has emerged as a promising solution, enabling effective validation of model performance and authenticity in both training and inference processes without disclosing sensitive data. Thus, ZKP ensures the verifiability and security of machine learning models, making it a valuable tool for privacy-preserving AI. Although some research has explored the verifiable machine learning solutions that exploit ZKP, a comprehensive survey and summary of these efforts remain absent. This survey paper aims to bridge this gap by reviewing and analyzing all the existing Zero-Knowledge Machine Learning (ZKML) research from June 2017 to December 2024. We begin by introducing the concept of ZKML and outlining its ZKP algorithmic setups under three key categories: verifiable training, verifiable inference, and verifiable testing. Next, we provide a comprehensive categorization of existing ZKML research within these categories and analyze the works in detail. Furthermore, we explore the implementation challenges faced in this field and discuss the improvement works to address these obstacles. Additionally, we highlight several commercial applications of ZKML technology. Finally, we propose promising directions for future advancements in this domain.

### ARACNE: An LLM-Based Autonomous Shell Pentesting Agent 
[[arxiv](https://arxiv.org/abs/2502.18528)] [[cool](https://papers.cool/arxiv/2502.18528)] [[pdf](https://arxiv.org/pdf/2502.18528)]
> **Authors**: Tomas Nieponice,Veronica Valeros,Sebastian Garcia
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-26
> **comment**: 7 pages, 2 figures, 3 tables
- **标题**: None
- **领域**: 密码学和安全,人工智能,机器人技术
- **Abstract**: We introduce ARACNE, a fully autonomous LLM-based pentesting agent tailored for SSH services that can execute commands on real Linux shell systems. Introduces a new agent architecture with multi-LLM model support. Experiments show that ARACNE can reach a 60\% success rate against the autonomous defender ShelLM and a 57.58\% success rate against the Over The Wire Bandit CTF challenges, improving over the state-of-the-art. When winning, the average number of actions taken by the agent to accomplish the goals was less than 5. The results show that the use of multi-LLM is a promising approach to increase accuracy in the actions.

### GOD model: Privacy Preserved AI School for Personal Assistant 
[[arxiv](https://arxiv.org/abs/2502.18527)] [[cool](https://papers.cool/arxiv/2502.18527)] [[pdf](https://arxiv.org/pdf/2502.18527)]
> **Authors**: PIN AI Team,Bill Qingyun Sun,Laura Florescu,Boliang Zhang,Regan Peng,Smile Hu,Shouqiao Wang,Ben Wu,Xi Wang,Davide Crapis,Gavin Zhen Guo
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能,机器学习
- **Abstract**: Personal AI assistants (e.g., Apple Intelligence, Meta AI) offer proactive recommendations that simplify everyday tasks, but their reliance on sensitive user data raises concerns about privacy and trust. To address these challenges, we introduce the Guardian of Data (GOD), a secure, privacy-preserving framework for training and evaluating AI assistants directly on-device. Unlike traditional benchmarks, the GOD model measures how well assistants can anticipate user needs-such as suggesting gifts-while protecting user data and autonomy. Functioning like an AI school, it addresses the cold start problem by simulating user queries and employing a curriculum-based approach to refine the performance of each assistant. Running within a Trusted Execution Environment (TEE), it safeguards user data while applying reinforcement and imitation learning to refine AI recommendations. A token-based incentive system encourages users to share data securely, creating a data flywheel that drives continuous improvement. By integrating privacy, personalization, and trust, the GOD model provides a scalable, responsible path for advancing personal AI assistants. For community collaboration, part of the framework is open-sourced at https://github.com/PIN-AI/God-Model.

### Class-Conditional Neural Polarizer: A Lightweight and Effective Backdoor Defense by Purifying Poisoned Features 
[[arxiv](https://arxiv.org/abs/2502.18520)] [[cool](https://papers.cool/arxiv/2502.18520)] [[pdf](https://arxiv.org/pdf/2502.18520)]
> **Authors**: Mingli Zhu,Shaokui Wei,Hongyuan Zha,Baoyuan Wu
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Recent studies have highlighted the vulnerability of deep neural networks to backdoor attacks, where models are manipulated to rely on embedded triggers within poisoned samples, despite the presence of both benign and trigger information. While several defense methods have been proposed, they often struggle to balance backdoor mitigation with maintaining benign performance.In this work, inspired by the concept of optical polarizer-which allows light waves of specific polarizations to pass while filtering others-we propose a lightweight backdoor defense approach, NPD. This method integrates a neural polarizer (NP) as an intermediate layer within the compromised model, implemented as a lightweight linear transformation optimized via bi-level optimization. The learnable NP filters trigger information from poisoned samples while preserving benign content. Despite its effectiveness, we identify through empirical studies that NPD's performance degrades when the target labels (required for purification) are inaccurately estimated. To address this limitation while harnessing the potential of targeted adversarial mitigation, we propose class-conditional neural polarizer-based defense (CNPD). The key innovation is a fusion module that integrates the backdoored model's predicted label with the features to be purified. This architecture inherently mimics targeted adversarial defense mechanisms without requiring label estimation used in NPD. We propose three implementations of CNPD: the first is r-CNPD, which trains a replicated NP layer for each class and, during inference, selects the appropriate NP layer for defense based on the predicted class from the backdoored model. To efficiently handle a large number of classes, two variants are designed: e-CNPD, which embeds class information as additional features, and a-CNPD, which directs network attention using class information.

### Swallowing the Poison Pills: Insights from Vulnerability Disparity Among LLMs 
[[arxiv](https://arxiv.org/abs/2502.18518)] [[cool](https://papers.cool/arxiv/2502.18518)] [[pdf](https://arxiv.org/pdf/2502.18518)]
> **Authors**: Peng Yifeng,Wu Zhizheng,Chen Chen
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Modern large language models (LLMs) exhibit critical vulnerabilities to poison pill attacks: localized data poisoning that alters specific factual knowledge while preserving overall model utility. We systematically demonstrate these attacks exploit inherent architectural properties of LLMs, achieving 54.6% increased retrieval inaccuracy on long-tail knowledge versus dominant topics and up to 25.5% increase retrieval inaccuracy on compressed models versus original architectures. Through controlled mutations (e.g., temporal/spatial/entity alterations) and, our method induces localized memorization deterioration with negligible impact on models' performance on regular standard benchmarks (e.g., <2% performance drop on MMLU/GPQA), leading to potential detection evasion. Our findings suggest: (1) Disproportionate vulnerability in long-tail knowledge may result from reduced parameter redundancy; (2) Model compression may increase attack surfaces, with pruned/distilled models requiring 30% fewer poison samples for equivalent damage; (3) Associative memory enables both spread of collateral damage to related concepts and amplification of damage from simultaneous attack, particularly for dominant topics. These findings raise concerns over current scaling paradigms since attack costs are lowering while defense complexity is rising. Our work establishes poison pills as both a security threat and diagnostic tool, revealing critical security-efficiency trade-offs in language model compression that challenges prevailing safety assumptions.

### RewardDS: Privacy-Preserving Fine-Tuning for Large Language Models via Reward Driven Data Synthesis 
[[arxiv](https://arxiv.org/abs/2502.18517)] [[cool](https://papers.cool/arxiv/2502.18517)] [[pdf](https://arxiv.org/pdf/2502.18517)]
> **Authors**: Jianwei Wang,Junyao Yang,Haoran Li,Huiping Zhuang,Cen Chen,Ziqian Zeng
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: The success of large language models (LLMs) has attracted many individuals to fine-tune them for domain-specific tasks by uploading their data. However, in sensitive areas like healthcare and finance, privacy concerns often arise. One promising solution is to sample synthetic data with Differential Privacy (DP) guarantees to replace private data. However, these synthetic data contain significant flawed data, which are considered as noise. Existing solutions typically rely on naive filtering by comparing ROUGE-L scores or embedding similarities, which are ineffective in addressing the noise. To address this issue, we propose RewardDS, a novel privacy-preserving framework that fine-tunes a reward proxy model and uses reward signals to guide the synthetic data generation. Our RewardDS introduces two key modules, Reward Guided Filtering and Self-Optimizing Refinement, to both filter and refine the synthetic data, effectively mitigating the noise. Extensive experiments across medical, financial, and code generation domains demonstrate the effectiveness of our method.

### A Multi-Agent Framework for Automated Vulnerability Detection and Repair in Solidity and Move Smart Contracts 
[[arxiv](https://arxiv.org/abs/2502.18515)] [[cool](https://papers.cool/arxiv/2502.18515)] [[pdf](https://arxiv.org/pdf/2502.18515)]
> **Authors**: Rabimba Karanjai,Sam Blackshear,Lei Xu,Weidong Shi
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能,多代理系统,软件工程
- **Abstract**: The rapid growth of the blockchain ecosystem and the increasing value locked in smart contracts necessitate robust security measures. While languages like Solidity and Move aim to improve smart contract security, vulnerabilities persist. This paper presents Smartify, a novel multi-agent framework leveraging Large Language Models (LLMs) to automatically detect and repair vulnerabilities in Solidity and Move smart contracts. Unlike traditional methods that rely solely on vast pre-training datasets, Smartify employs a team of specialized agents working on different specially fine-tuned LLMs to analyze code based on underlying programming concepts and language-specific security principles. We evaluated Smartify on a dataset for Solidity and a curated dataset for Move, demonstrating its effectiveness in fixing a wide range of vulnerabilities. Our results show that Smartify (Gemma2+codegemma) achieves state-of-the-art performance, surpassing existing LLMs and enhancing general-purpose models' capabilities, such as Llama 3.1. Notably, Smartify can incorporate language-specific knowledge, such as the nuances of Move, without requiring massive language-specific pre-training datasets. This work offers a detailed analysis of various LLMs' performance on smart contract repair, highlighting the strengths of our multi-agent approach and providing a blueprint for developing more secure and reliable decentralized applications in the growing blockchain landscape. We also provide a detailed recipe for extending this to other similar use cases.

### CipherFace: A Fully Homomorphic Encryption-Driven Framework for Secure Cloud-Based Facial Recognition 
[[arxiv](https://arxiv.org/abs/2502.18514)] [[cool](https://papers.cool/arxiv/2502.18514)] [[pdf](https://arxiv.org/pdf/2502.18514)]
> **Authors**: Sefik Serengil,Alper Ozpinar
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,计算机视觉和模式识别,机器学习
- **Abstract**: Facial recognition systems rely on embeddings to represent facial images and determine identity by verifying if the distance between embeddings is below a pre-tuned threshold. While embeddings are not reversible to original images, they still contain sensitive information, making their security critical. Traditional encryption methods like AES are limited in securely utilizing cloud computational power for distance calculations. Homomorphic Encryption, allowing calculations on encrypted data, offers a robust alternative. This paper introduces CipherFace, a homomorphic encryption-driven framework for secure cloud-based facial recognition, which we have open-sourced at http://github.com/serengil/cipherface. By leveraging FHE, CipherFace ensures the privacy of embeddings while utilizing the cloud for efficient distance computation. Furthermore, we propose a novel encrypted distance computation method for both Euclidean and Cosine distances, addressing key challenges in performing secure similarity calculations on encrypted data. We also conducted experiments with different facial recognition models, various embedding sizes, and cryptosystem configurations, demonstrating the scalability and effectiveness of CipherFace in real-world applications.

### ELBA-Bench: An Efficient Learning Backdoor Attacks Benchmark for Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.18511)] [[cool](https://papers.cool/arxiv/2502.18511)] [[pdf](https://arxiv.org/pdf/2502.18511)]
> **Authors**: Xuxu Liu,Siyuan Liang,Mengya Han,Yong Luo,Aishan Liu,Xiantao Cai,Zheng He,Dacheng Tao
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Generative large language models are crucial in natural language processing, but they are vulnerable to backdoor attacks, where subtle triggers compromise their behavior. Although backdoor attacks against LLMs are constantly emerging, existing benchmarks remain limited in terms of sufficient coverage of attack, metric system integrity, backdoor attack alignment. And existing pre-trained backdoor attacks are idealized in practice due to resource access constraints. Therefore we establish $\textit{ELBA-Bench}$, a comprehensive and unified framework that allows attackers to inject backdoor through parameter efficient fine-tuning ($\textit{e.g.,}$ LoRA) or without fine-tuning techniques ($\textit{e.g.,}$ In-context-learning). $\textit{ELBA-Bench}$ provides over 1300 experiments encompassing the implementations of 12 attack methods, 18 datasets, and 12 LLMs. Extensive experiments provide new invaluable findings into the strengths and limitations of various attack strategies. For instance, PEFT attack consistently outperform without fine-tuning approaches in classification tasks while showing strong cross-dataset generalization with optimized triggers boosting robustness; Task-relevant backdoor optimization techniques or attack prompts along with clean and adversarial demonstrations can enhance backdoor attack success while preserving model performance on clean samples. Additionally, we introduce a universal toolbox designed for standardized backdoor attack research, with the goal of propelling further progress in this vital area.

### Protecting Users From Themselves: Safeguarding Contextual Privacy in Interactions with Conversational Agents 
[[arxiv](https://arxiv.org/abs/2502.18509)] [[cool](https://papers.cool/arxiv/2502.18509)] [[pdf](https://arxiv.org/pdf/2502.18509)]
> **Authors**: Ivoline Ngong,Swanand Kadhe,Hao Wang,Keerthiram Murugesan,Justin D. Weisz,Amit Dhurandhar,Karthikeyan Natesan Ramamurthy
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-26
> **comment**: 22 pages, 2 figures
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Conversational agents are increasingly woven into individuals' personal lives, yet users often underestimate the privacy risks involved. The moment users share information with these agents (e.g., LLMs), their private information becomes vulnerable to exposure. In this paper, we characterize the notion of contextual privacy for user interactions with LLMs. It aims to minimize privacy risks by ensuring that users (sender) disclose only information that is both relevant and necessary for achieving their intended goals when interacting with LLMs (untrusted receivers). Through a formative design user study, we observe how even "privacy-conscious" users inadvertently reveal sensitive information through indirect disclosures. Based on insights from this study, we propose a locally-deployable framework that operates between users and LLMs, and identifies and reformulates out-of-context information in user prompts. Our evaluation using examples from ShareGPT shows that lightweight models can effectively implement this framework, achieving strong gains in contextual privacy while preserving the user's intended interaction goals through different approaches to classify information relevant to the intended goals.

### REFINE: Inversion-Free Backdoor Defense via Model Reprogramming 
[[arxiv](https://arxiv.org/abs/2502.18508)] [[cool](https://papers.cool/arxiv/2502.18508)] [[pdf](https://arxiv.org/pdf/2502.18508)]
> **Authors**: Yukun Chen,Shuo Shao,Enhao Huang,Yiming Li,Pin-Yu Chen,Zhan Qin,Kui Ren
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-26
> **comment**: This paper is accept by ICLR 2025. The first two authors contributed equally to this work. Our code is available at BackdoorBox (https://github.com/THUYimingLi/BackdoorBox) and Github repository (https://github.com/WhitolfChen/REFINE). 28 pages
- **标题**: None
- **领域**: 密码学和安全,人工智能,计算机视觉和模式识别,机器学习
- **Abstract**: Backdoor attacks on deep neural networks (DNNs) have emerged as a significant security threat, allowing adversaries to implant hidden malicious behaviors during the model training phase. Pre-processing-based defense, which is one of the most important defense paradigms, typically focuses on input transformations or backdoor trigger inversion (BTI) to deactivate or eliminate embedded backdoor triggers during the inference process. However, these methods suffer from inherent limitations: transformation-based defenses often fail to balance model utility and defense performance, while BTI-based defenses struggle to accurately reconstruct trigger patterns without prior knowledge. In this paper, we propose REFINE, an inversion-free backdoor defense method based on model reprogramming. REFINE consists of two key components: \textbf{(1)} an input transformation module that disrupts both benign and backdoor patterns, generating new benign features; and \textbf{(2)} an output remapping module that redefines the model's output domain to guide the input transformations effectively. By further integrating supervised contrastive loss, REFINE enhances the defense capabilities while maintaining model utility. Extensive experiments on various benchmark datasets demonstrate the effectiveness of our REFINE and its resistance to potential adaptive attacks.

### TurboFuzzLLM: Turbocharging Mutation-based Fuzzing for Effectively Jailbreaking Large Language Models in Practice 
[[arxiv](https://arxiv.org/abs/2502.18504)] [[cool](https://papers.cool/arxiv/2502.18504)] [[pdf](https://arxiv.org/pdf/2502.18504)]
> **Authors**: Aman Goel,Xian Carrie Wu,Zhe Wang,Dmitriy Bespalov,Yanjun Qi
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-26
> **comment**: Accepted at NAACL 2025 industry track, 12 pages, 5 figures
- **标题**: None
- **领域**: 密码学和安全,人工智能,计算语言学,机器学习
- **Abstract**: Jailbreaking large-language models (LLMs) involves testing their robustness against adversarial prompts and evaluating their ability to withstand prompt attacks that could elicit unauthorized or malicious responses. In this paper, we present TurboFuzzLLM, a mutation-based fuzzing technique for efficiently finding a collection of effective jailbreaking templates that, when combined with harmful questions, can lead a target LLM to produce harmful responses through black-box access via user prompts. We describe the limitations of directly applying existing template-based attacking techniques in practice, and present functional and efficiency-focused upgrades we added to mutation-based fuzzing to generate effective jailbreaking templates automatically. TurboFuzzLLM achieves $\geq$ 95\% attack success rates (ASR) on public datasets for leading LLMs (including GPT-4o \& GPT-4 Turbo), shows impressive generalizability to unseen harmful questions, and helps in improving model defenses to prompt attacks.

### Deep Learning-based Dual Watermarking for Image Copyright Protection and Authentication 
[[arxiv](https://arxiv.org/abs/2502.18501)] [[cool](https://papers.cool/arxiv/2502.18501)] [[pdf](https://arxiv.org/pdf/2502.18501)]
> **Authors**: Sudev Kumar Padhi,Archana Tiwari,Sk. Subidh Ali
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-26
> **comment**: IEEE Transactions on Artificial Intelligence. 2024 Oct 24
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Advancements in digital technologies make it easy to modify the content of digital images. Hence, ensuring digital images integrity and authenticity is necessary to protect them against various attacks that manipulate them. We present a Deep Learning (DL) based dual invisible watermarking technique for performing source authentication, content authentication, and protecting digital content copyright of images sent over the internet. Beyond securing images, the proposed technique demonstrates robustness to content-preserving image manipulations. It is also impossible to imitate or overwrite watermarks because the cryptographic hash of the image and the dominant features of the image in the form of perceptual hash are used as watermarks. We highlighted the need for source authentication to safeguard image integrity and authenticity, along with identifying similar content for copyright protection. After exhaustive testing, we obtained a high peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM), which implies there is a minute change in the original image after embedding our watermarks. Our trained model achieves high watermark extraction accuracy and to the best of our knowledge, this is the first deep learning-based dual watermarking technique proposed in the literature.

### VVRec: Reconstruction Attacks on DL-based Volumetric Video Upstreaming via Latent Diffusion Model with Gamma Distribution 
[[arxiv](https://arxiv.org/abs/2502.17880)] [[cool](https://papers.cool/arxiv/2502.17880)] [[pdf](https://arxiv.org/pdf/2502.17880)]
> **Authors**: Rui Lu,Bihai Zhang,Dan Wang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,计算机视觉和模式识别,多媒体
- **Abstract**: With the popularity of 3D volumetric video applications, such as Autonomous Driving, Virtual Reality, and Mixed Reality, current developers have turned to deep learning for compressing volumetric video frames, i.e., point clouds for video upstreaming. The latest deep learning-based solutions offer higher efficiency, lower distortion, and better hardware support compared to traditional ones like MPEG and JPEG. However, privacy threats arise, especially reconstruction attacks targeting to recover the original input point cloud from the intermediate results. In this paper, we design VVRec, to the best of our knowledge, which is the first targeting DL-based Volumetric Video Reconstruction attack scheme. VVRec demonstrates the ability to reconstruct high-quality point clouds from intercepted transmission intermediate results using four well-trained neural network modules we design. Leveraging the latest latent diffusion models with Gamma distribution and a refinement algorithm, VVRec excels in reconstruction quality, color recovery, and surpasses existing defenses. We evaluate VVRec using three volumetric video datasets. The results demonstrate that VVRec achieves 64.70dB reconstruction accuracy, with an impressive 46.39% reduction of distortion over baselines.

## 计算机视觉和模式识别(cs.CV:Computer Vision and Pattern Recognition)

### Grad-ECLIP: Gradient-based Visual and Textual Explanations for CLIP 
[[arxiv](https://arxiv.org/abs/2502.18816)] [[cool](https://papers.cool/arxiv/2502.18816)] [[pdf](https://arxiv.org/pdf/2502.18816)]
> **Authors**: Chenyang Zhao,Kun Wang,Janet H. Hsiao,Antoni B. Chan
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Significant progress has been achieved on the improvement and downstream usages of the Contrastive Language-Image Pre-training (CLIP) vision-language model, while less attention is paid to the interpretation of CLIP. We propose a Gradient-based visual and textual Explanation method for CLIP (Grad-ECLIP), which interprets the matching result of CLIP for specific input image-text pair. By decomposing the architecture of the encoder and discovering the relationship between the matching similarity and intermediate spatial features, Grad-ECLIP produces effective heat maps that show the influence of image regions or words on the CLIP results. Different from the previous Transformer interpretation methods that focus on the utilization of self-attention maps, which are typically extremely sparse in CLIP, we produce high-quality visual explanations by applying channel and spatial weights on token features. Qualitative and quantitative evaluations verify the effectiveness and superiority of Grad-ECLIP compared with the state-of-the-art methods. Furthermore, a series of analysis are conducted based on our visual and textual explanation results, from which we explore the working mechanism of image-text matching, the strengths and limitations in attribution identification of CLIP, and the relationship between the concreteness/abstractness of a word and its usage in CLIP. Finally, based on the ability of explanation map that indicates text-specific saliency region of input image, we also propose an application with Grad-ECLIP, which is adopted to boost the fine-grained alignment in the CLIP fine-tuning. The code of Grad-ECLIP is available here: https://github.com/Cyang-Zhao/Grad-Eclip.

### Spectral-Enhanced Transformers: Leveraging Large-Scale Pretrained Models for Hyperspectral Object Tracking 
[[arxiv](https://arxiv.org/abs/2502.18748)] [[cool](https://papers.cool/arxiv/2502.18748)] [[pdf](https://arxiv.org/pdf/2502.18748)]
> **Authors**: Shaheer Mohamed,Tharindu Fernando,Sridha Sridharan,Peyman Moghadam,Clinton Fookes
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Accepted to 14th Workshop on Hyperspectral Imaging and Signal Processing: Evolution in Remote Sensing (WHISPERS)
- **标题**: None
- **领域**: 计算机视觉和模式识别,图像和视频处理
- **Abstract**: Hyperspectral object tracking using snapshot mosaic cameras is emerging as it provides enhanced spectral information alongside spatial data, contributing to a more comprehensive understanding of material properties. Using transformers, which have consistently outperformed convolutional neural networks (CNNs) in learning better feature representations, would be expected to be effective for Hyperspectral object tracking. However, training large transformers necessitates extensive datasets and prolonged training periods. This is particularly critical for complex tasks like object tracking, and the scarcity of large datasets in the hyperspectral domain acts as a bottleneck in achieving the full potential of powerful transformer models. This paper proposes an effective methodology that adapts large pretrained transformer-based foundation models for hyperspectral object tracking. We propose an adaptive, learnable spatial-spectral token fusion module that can be extended to any transformer-based backbone for learning inherent spatial-spectral features in hyperspectral data. Furthermore, our model incorporates a cross-modality training pipeline that facilitates effective learning across hyperspectral datasets collected with different sensor modalities. This enables the extraction of complementary knowledge from additional modalities, whether or not they are present during testing. Our proposed model also achieves superior performance with minimal training iterations.

### Beyond RNNs: Benchmarking Attention-Based Image Captioning Models 
[[arxiv](https://arxiv.org/abs/2502.18734)] [[cool](https://papers.cool/arxiv/2502.18734)] [[pdf](https://arxiv.org/pdf/2502.18734)]
> **Authors**: Hemanth Teja Yanambakkam,Rahul Chinthala
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 10 pages, 6 figures. Code and additional results are available on GitHub under the handle HemanthTejaY
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算语言学
- **Abstract**: Image captioning is a challenging task at the intersection of computer vision and natural language processing, requiring models to generate meaningful textual descriptions of images. Traditional approaches rely on recurrent neural networks (RNNs), but recent advancements in attention mechanisms have demonstrated significant improvements. This study benchmarks the performance of attention-based image captioning models against RNN-based approaches using the MS-COCO dataset. We evaluate the effectiveness of Bahdanau attention in enhancing the alignment between image features and generated captions. The models are assessed using natural language processing metrics such as BLEU, METEOR, GLEU, and WER. Our results show that attention-based models outperform RNNs in generating more accurate and semantically rich captions, with better alignment to human evaluation. This work provides insights into the impact of attention mechanisms in image captioning and highlights areas for future improvements.

### Adversarial Universal Stickers: Universal Perturbation Attacks on Traffic Sign using Stickers 
[[arxiv](https://arxiv.org/abs/2502.18724)] [[cool](https://papers.cool/arxiv/2502.18724)] [[pdf](https://arxiv.org/pdf/2502.18724)]
> **Authors**: Anthony Etim,Jakub Szefer
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,密码学和安全
- **Abstract**: Adversarial attacks on deep learning models have proliferated in recent years. In many cases, a different adversarial perturbation is required to be added to each image to cause the deep learning model to misclassify it. This is ineffective as each image has to be modified in a different way. Meanwhile, research on universal perturbations focuses on designing a single perturbation that can be applied to all images in a data set, and cause a deep learning model to misclassify the images. This work advances the field of universal perturbations by exploring universal perturbations in the context of traffic signs and autonomous vehicle systems. This work introduces a novel method for generating universal perturbations that visually look like simple black and white stickers, and using them to cause incorrect street sign predictions. Unlike traditional adversarial perturbations, the adversarial universal stickers are designed to be applicable to any street sign: same sticker, or stickers, can be applied in same location to any street sign and cause it to be misclassified. Further, to enable safe experimentation with adversarial images and street signs, this work presents a virtual setting that leverages Street View images of street signs, rather than the need to physically modify street signs, to test the attacks. The experiments in the virtual setting demonstrate that these stickers can consistently mislead deep learning models used commonly in street sign recognition, and achieve high attack success rates on dataset of US traffic signs. The findings highlight the practical security risks posed by simple stickers applied to traffic signs, and the ease with which adversaries can generate adversarial universal stickers that can be applied to many street signs.

### Enhancing Image Classification with Augmentation: Data Augmentation Techniques for Improved Image Classification 
[[arxiv](https://arxiv.org/abs/2502.18691)] [[cool](https://papers.cool/arxiv/2502.18691)] [[pdf](https://arxiv.org/pdf/2502.18691)]
> **Authors**: Saorj Kumar,Prince Asiamah,Oluwatoyin Jolaoso,Ugochukwu Esiowu
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Convolutional Neural Networks (CNNs) serve as the workhorse of deep learning, finding applications in various fields that rely on images. Given sufficient data, they exhibit the capacity to learn a wide range of concepts across diverse settings. However, a notable limitation of CNNs is their susceptibility to overfitting when trained on small datasets. The augmentation of such datasets can significantly enhance CNN performance by introducing additional data points for learning. In this study, we explore the effectiveness of 11 different sets of data augmentation techniques, which include three novel sets proposed in this work. The first set of data augmentation employs pairwise channel transfer, transferring Red, Green, Blue, Hue, and Saturation values from randomly selected images in the database to all images in the dataset. The second set introduces a novel occlusion approach, where objects in the images are occluded by randomly selected objects from the dataset. The third set involves a novel masking approach, using vertical, horizontal, circular, and checkered masks to occlude portions of the images. In addition to these novel techniques, we investigate other existing augmentation methods, including rotation, horizontal and vertical flips, resizing, translation, blur, color jitter, and random erasing, and their effects on accuracy and overfitting. We fine-tune a base EfficientNet-B0 model for each augmentation method and conduct a comparative analysis to showcase their efficacy. For the evaluation and comparison of these augmentation techniques, we utilize the Caltech-101 dataset. The ensemble of image augmentation techniques proposed emerges as the most effective on the Caltech-101 dataset. The results demonstrate that diverse data augmentation techniques present a viable means of enhancing datasets for improved image classification.

### Diffusion Models for conditional MRI generation 
[[arxiv](https://arxiv.org/abs/2502.18620)] [[cool](https://papers.cool/arxiv/2502.18620)] [[pdf](https://arxiv.org/pdf/2502.18620)]
> **Authors**: Miguel Herencia García del Castillo,Ricardo Moya Garcia,Manuel Jesús Cerezo Mazón,Ekaitz Arriola Garcia,Pablo Menéndez Fernández-Miranda
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: In this article, we present a Latent Diffusion Model (LDM) for the generation of brain Magnetic Resonance Imaging (MRI), conditioning its generation based on pathology (Healthy, Glioblastoma, Sclerosis, Dementia) and acquisition modality (T1w, T1ce, T2w, Flair, PD). To evaluate the quality of the generated images, the Fréchet Inception Distance (FID) and Multi-Scale Structural Similarity Index (MS-SSIM) metrics were employed. The results indicate that the model generates images with a distribution similar to real ones, maintaining a balance between visual fidelity and diversity. Additionally, the model demonstrates extrapolation capability, enabling the generation of configurations that were not present in the training data. The results validate the potential of the model to increase in the number of samples in clinical datasets, balancing underrepresented classes, and evaluating AI models in medicine, contributing to the development of diagnostic tools in radiology without compromising patient privacy.

### DeBUGCN -- Detecting Backdoors in CNNs Using Graph Convolutional Networks 
[[arxiv](https://arxiv.org/abs/2502.18592)] [[cool](https://papers.cool/arxiv/2502.18592)] [[pdf](https://arxiv.org/pdf/2502.18592)]
> **Authors**: Akash Vartak,Khondoker Murad Hossain,Tim Oates
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 18 pages, 11 tables, 8 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Deep neural networks (DNNs) are becoming commonplace in critical applications, making their susceptibility to backdoor (trojan) attacks a significant problem. In this paper, we introduce a novel backdoor attack detection pipeline, detecting attacked models using graph convolution networks (DeBUGCN). To the best of our knowledge, ours is the first use of GCNs for trojan detection. We use the static weights of a DNN to create a graph structure of its layers. A GCN is then used as a binary classifier on these graphs, yielding a trojan or clean determination for the DNN. To demonstrate the efficacy of our pipeline, we train hundreds of clean and trojaned CNN models on the MNIST handwritten digits and CIFAR-10 image datasets, and show the DNN classification results using DeBUGCN. For a true In-the-Wild use case, our pipeline is evaluated on the TrojAI dataset which consists of various CNN architectures, thus showing the robustness and model-agnostic behaviour of DeBUGCN. Furthermore, on comparing our results on several datasets with state-of-the-art trojan detection algorithms, DeBUGCN is faster and more accurate.

### Application of Attention Mechanism with Bidirectional Long Short-Term Memory (BiLSTM) and CNN for Human Conflict Detection using Computer Vision 
[[arxiv](https://arxiv.org/abs/2502.18555)] [[cool](https://papers.cool/arxiv/2502.18555)] [[pdf](https://arxiv.org/pdf/2502.18555)]
> **Authors**: Erick da Silva Farias,Eduardo Palhares Junior
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: The automatic detection of human conflicts through videos is a crucial area in computer vision, with significant applications in monitoring and public safety policies. However, the scarcity of public datasets and the complexity of human interactions make this task challenging. This study investigates the integration of advanced deep learning techniques, including Attention Mechanism, Convolutional Neural Networks (CNNs), and Bidirectional Long ShortTerm Memory (BiLSTM), to improve the detection of violent behaviors in videos. The research explores how the use of the attention mechanism can help focus on the most relevant parts of the video, enhancing the accuracy and robustness of the model. The experiments indicate that the combination of CNNs with BiLSTM and the attention mechanism provides a promising solution for conflict monitoring, offering insights into the effectiveness of different strategies. This work opens new possibilities for the development of automated surveillance systems that can operate more efficiently in real-time detection of violent events.

### Multi-class Seismic Building Damage Assessment from InSAR Imagery using Quadratic Variational Causal Bayesian Inference 
[[arxiv](https://arxiv.org/abs/2502.18546)] [[cool](https://papers.cool/arxiv/2502.18546)] [[pdf](https://arxiv.org/pdf/2502.18546)]
> **Authors**: Xuechun Li,Susu Xu
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Submitted to Remote Sensing and Environment
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Interferometric Synthetic Aperture Radar (InSAR) technology uses satellite radar to detect surface deformation patterns and monitor earthquake impacts on buildings. While vital for emergency response planning, extracting multi-class building damage classifications from InSAR data faces challenges: overlapping damage signatures with environmental noise, computational complexity in multi-class scenarios, and the need for rapid regional-scale processing. Our novel multi-class variational causal Bayesian inference framework with quadratic variational bounds provides rigorous approximations while ensuring efficiency. By integrating InSAR observations with USGS ground failure models and building fragility functions, our approach separates building damage signals while maintaining computational efficiency through strategic pruning. Evaluation across five major earthquakes (Haiti 2021, Puerto Rico 2020, Zagreb 2020, Italy 2016, Ridgecrest 2019) shows improved damage classification accuracy (AUC: 0.94-0.96), achieving up to 35.7% improvement over existing methods. Our approach maintains high accuracy (AUC > 0.93) across all damage categories while reducing computational overhead by over 40% without requiring extensive ground truth data.

### FilterRAG: Zero-Shot Informed Retrieval-Augmented Generation to Mitigate Hallucinations in VQA 
[[arxiv](https://arxiv.org/abs/2502.18536)] [[cool](https://papers.cool/arxiv/2502.18536)] [[pdf](https://arxiv.org/pdf/2502.18536)]
> **Authors**: S M Sarwar
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 12 pages, 6 figures and 2 tables
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算语言学,信息检索,机器学习
- **Abstract**: Visual Question Answering requires models to generate accurate answers by integrating visual and textual understanding. However, VQA models still struggle with hallucinations, producing convincing but incorrect answers, particularly in knowledge-driven and Out-of-Distribution scenarios. We introduce FilterRAG, a retrieval-augmented framework that combines BLIP-VQA with Retrieval-Augmented Generation to ground answers in external knowledge sources like Wikipedia and DBpedia. FilterRAG achieves 36.5% accuracy on the OK-VQA dataset, demonstrating its effectiveness in reducing hallucinations and improving robustness in both in-domain and Out-of-Distribution settings. These findings highlight the potential of FilterRAG to improve Visual Question Answering systems for real-world deployment.

### Convolutional neural networks for mineral prospecting through alteration mapping with remote sensing data 
[[arxiv](https://arxiv.org/abs/2502.18533)] [[cool](https://papers.cool/arxiv/2502.18533)] [[pdf](https://arxiv.org/pdf/2502.18533)]
> **Authors**: Ehsan Farahbakhsh,Dakshi Goel,Dhiraj Pimparkar,R. Dietmar Muller,Rohitash Chandra
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习,图像和视频处理
- **Abstract**: Traditional geological mapping, based on field observations and rock sample analysis, is inefficient for continuous spatial mapping of features like alteration zones. Deep learning models, such as convolutional neural networks (CNNs), have revolutionised remote sensing data analysis by automatically extracting features for classification and regression tasks. CNNs can detect specific mineralogical changes linked to mineralisation by identifying subtle features in remote sensing data. This study uses CNNs with Landsat 8, Landsat 9, and ASTER data to map alteration zones north of Broken Hill, New South Wales, Australia. The model is trained using ground truth data and an automated approach with selective principal component analysis (PCA). We compare CNNs with traditional machine learning models, including k-nearest neighbours, support vector machines, and multilayer perceptron. Results show that ground truth-based training yields more reliable maps, with CNNs slightly outperforming conventional models in capturing spatial patterns. Landsat 9 outperforms Landsat 8 in mapping iron oxide areas using ground truth-trained CNNs, while ASTER data provides the most accurate argillic and propylitic alteration maps. This highlights CNNs' effectiveness in improving geological mapping precision, especially for identifying subtle mineralisation-related alterations.

### IMPROVE: Iterative Model Pipeline Refinement and Optimization Leveraging LLM Agents 
[[arxiv](https://arxiv.org/abs/2502.18530)] [[cool](https://papers.cool/arxiv/2502.18530)] [[pdf](https://arxiv.org/pdf/2502.18530)]
> **Authors**: Eric Xue,Zeyi Huang,Yuyang Ji,Haohan Wang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Computer vision is a critical component in a wide range of real-world applications, including plant monitoring in agriculture and handwriting classification in digital systems. However, developing high-performance computer vision models traditionally demands both machine learning (ML) expertise and domain-specific knowledge, making the process costly, labor-intensive, and inaccessible to many. Large language model (LLM) agents have emerged as a promising solution to automate this workflow, but most existing methods share a common limitation: they attempt to optimize entire pipelines in a single step before evaluation, making it difficult to attribute improvements to specific changes. This lack of granularity leads to unstable optimization and slower convergence, limiting their effectiveness. To address this, we introduce Iterative Refinement, a novel strategy for LLM-driven ML pipeline design inspired by how human ML experts iteratively refine models, focusing on one component at a time rather than making sweeping changes all at once. By systematically updating individual components based on real training feedback, Iterative Refinement improves stability, interpretability, and overall model performance. We implement this strategy in IMPROVE, an end-to-end LLM agent framework for automating and optimizing object classification pipelines. Through extensive evaluations across datasets of varying sizes and domains, including standard benchmarks and Kaggle competition datasets, we demonstrate that Iterative Refinement enables IMPROVE to consistently achieve better performance over existing zero-shot LLM-based approaches. These findings establish Iterative Refinement as an effective new strategy for LLM-driven ML automation and position IMPROVE as an accessible solution for building high-quality computer vision models without requiring ML expertise.

### Optimized Custom CNN for Real-Time Tomato Leaf Disease Detection 
[[arxiv](https://arxiv.org/abs/2502.18521)] [[cool](https://papers.cool/arxiv/2502.18521)] [[pdf](https://arxiv.org/pdf/2502.18521)]
> **Authors**: Mangsura Kabir Oni,Tabia Tanzin Prama
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: In Bangladesh, tomatoes are a staple vegetable, prized for their versatility in various culinary applications. However, the cultivation of tomatoes is often hindered by a range of diseases that can significantly reduce crop yields and quality. Early detection of these diseases is crucial for implementing timely interventions and ensuring the sustainability of tomato production. Traditional manual inspection methods, while effective, are labor-intensive and prone to human error. To address these challenges, this research paper sought to develop an automated disease detection system using Convolutional Neural Networks (CNNs). A comprehensive dataset of tomato leaves was collected from the Brahmanbaria district, preprocessed to enhance image quality, and then applied to various deep learning models. Comparative performance analysis was conducted between YOLOv5, MobileNetV2, ResNet18, and our custom CNN model. In our study, the Custom CNN model achieved an impressive accuracy of 95.2%, significantly outperforming the other models, which achieved an accuracy of 77%, 89.38% and 71.88% respectively. While other models showed solid performance, our Custom CNN demonstrated superior results specifically tailored for the task of tomato leaf disease detection. These findings highlight the strong potential of deep learning techniques for improving early disease detection in tomato crops. By leveraging these advanced technologies, farmers can gain valuable insights to detect diseases at an early stage, allowing for more effective management practices. This approach not only promises to boost tomato yields but also contributes to the sustainability and resilience of the agricultural sector, helping to mitigate the impact of plant diseases on crop production.

### FCoT-VL:Advancing Text-oriented Large Vision-Language Models with Efficient Visual Token Compression 
[[arxiv](https://arxiv.org/abs/2502.18512)] [[cool](https://papers.cool/arxiv/2502.18512)] [[pdf](https://arxiv.org/pdf/2502.18512)]
> **Authors**: Jianjian Li,Junquan Fan,Feng Tang,Gang Huang,Shitao Zhu,Songlin Liu,Nian Xie,Wulong Liu,Yong Liao
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-26
> **comment**: 20 pages, 18 figures, 6 tables
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: The rapid success of Vision Large Language Models (VLLMs) often depends on the high-resolution images with abundant visual tokens, which hinders training and deployment efficiency. Current training-free visual token compression methods exhibit serious performance degradation in tasks involving high-resolution, text-oriented image understanding and reasoning. In this paper, we propose an efficient visual token compression framework for text-oriented VLLMs in high-resolution scenarios. In particular, we employ a light-weight self-distillation pre-training stage to compress the visual tokens, requiring a limited numbers of image-text pairs and minimal learnable parameters. Afterwards, to mitigate potential performance degradation of token-compressed models, we construct a high-quality post-train stage. To validate the effectiveness of our method, we apply it to an advanced VLLMs, InternVL2. Experimental results show that our approach significantly reduces computational overhead while outperforming the baselines across a range of text-oriented benchmarks. We will release the models and code soon.

### GHOST 2.0: generative high-fidelity one shot transfer of heads 
[[arxiv](https://arxiv.org/abs/2502.18417)] [[cool](https://papers.cool/arxiv/2502.18417)] [[pdf](https://arxiv.org/pdf/2502.18417)]
> **Authors**: Alexander Groshev,Anastasiia Iashchenko,Pavel Paramonov,Denis Dimitrov,Andrey Kuznetsov
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: While the task of face swapping has recently gained attention in the research community, a related problem of head swapping remains largely unexplored. In addition to skin color transfer, head swap poses extra challenges, such as the need to preserve structural information of the whole head during synthesis and inpaint gaps between swapped head and background. In this paper, we address these concerns with GHOST 2.0, which consists of two problem-specific modules. First, we introduce enhanced Aligner model for head reenactment, which preserves identity information at multiple scales and is robust to extreme pose variations. Secondly, we use a Blender module that seamlessly integrates the reenacted head into the target background by transferring skin color and inpainting mismatched regions. Both modules outperform the baselines on the corresponding tasks, allowing to achieve state of the art results in head swapping. We also tackle complex cases, such as large difference in hair styles of source and target. Code is available at https://github.com/ai-forever/ghost-2.0

### MedKAN: An Advanced Kolmogorov-Arnold Network for Medical Image Classification 
[[arxiv](https://arxiv.org/abs/2502.18416)] [[cool](https://papers.cool/arxiv/2502.18416)] [[pdf](https://arxiv.org/pdf/2502.18416)]
> **Authors**: Zhuoqin Yang,Jiansong Zhang,Xiaoling Luo,Zheng Lu,Linlin Shen
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Recent advancements in deep learning for image classification predominantly rely on convolutional neural networks (CNNs) or Transformer-based architectures. However, these models face notable challenges in medical imaging, particularly in capturing intricate texture details and contextual features. Kolmogorov-Arnold Networks (KANs) represent a novel class of architectures that enhance nonlinear transformation modeling, offering improved representation of complex features. In this work, we present MedKAN, a medical image classification framework built upon KAN and its convolutional extensions. MedKAN features two core modules: the Local Information KAN (LIK) module for fine-grained feature extraction and the Global Information KAN (GIK) module for global context integration. By combining these modules, MedKAN achieves robust feature modeling and fusion. To address diverse computational needs, we introduce three scalable variants--MedKAN-S, MedKAN-B, and MedKAN-L. Experimental results on nine public medical imaging datasets demonstrate that MedKAN achieves superior performance compared to CNN- and Transformer-based models, highlighting its effectiveness and generalizability in medical image analysis.

### OmniAlign-V: Towards Enhanced Alignment of MLLMs with Human Preference 
[[arxiv](https://arxiv.org/abs/2502.18411)] [[cool](https://papers.cool/arxiv/2502.18411)] [[pdf](https://arxiv.org/pdf/2502.18411)]
> **Authors**: Xiangyu Zhao,Shengyuan Ding,Zicheng Zhang,Haian Huang,Maosong Cao,Weiyun Wang,Jiaqi Wang,Xinyu Fang,Wenhai Wang,Guangtao Zhai,Haodong Duan,Hua Yang,Kai Chen
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Recent advancements in open-source multi-modal large language models (MLLMs) have primarily focused on enhancing foundational capabilities, leaving a significant gap in human preference alignment. This paper introduces OmniAlign-V, a comprehensive dataset of 200K high-quality training samples featuring diverse images, complex questions, and varied response formats to improve MLLMs' alignment with human preferences. We also present MM-AlignBench, a human-annotated benchmark specifically designed to evaluate MLLMs' alignment with human values. Experimental results show that finetuning MLLMs with OmniAlign-V, using Supervised Fine-Tuning (SFT) or Direct Preference Optimization (DPO), significantly enhances human preference alignment while maintaining or enhancing performance on standard VQA benchmarks, preserving their fundamental capabilities. Our datasets, benchmark, code and checkpoints have been released at https://github.com/PhoenixZ810/OmniAlign-V.

### EgoSim: An Egocentric Multi-view Simulator and Real Dataset for Body-worn Cameras during Motion and Activity 
[[arxiv](https://arxiv.org/abs/2502.18373)] [[cool](https://papers.cool/arxiv/2502.18373)] [[pdf](https://arxiv.org/pdf/2502.18373)]
> **Authors**: Dominik Hollidt,Paul Streli,Jiaxi Jiang,Yasaman Haghighi,Changlin Qian,Xintong Liu,Christian Holz
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: Research on egocentric tasks in computer vision has mostly focused on head-mounted cameras, such as fisheye cameras or embedded cameras inside immersive headsets. We argue that the increasing miniaturization of optical sensors will lead to the prolific integration of cameras into many more body-worn devices at various locations. This will bring fresh perspectives to established tasks in computer vision and benefit key areas such as human motion tracking, body pose estimation, or action recognition -- particularly for the lower body, which is typically occluded. In this paper, we introduce EgoSim, a novel simulator of body-worn cameras that generates realistic egocentric renderings from multiple perspectives across a wearer's body. A key feature of EgoSim is its use of real motion capture data to render motion artifacts, which are especially noticeable with arm- or leg-worn cameras. In addition, we introduce MultiEgoView, a dataset of egocentric footage from six body-worn cameras and ground-truth full-body 3D poses during several activities: 119 hours of data are derived from AMASS motion sequences in four high-fidelity virtual environments, which we augment with 5 hours of real-world motion data from 13 participants using six GoPro cameras and 3D body pose references from an Xsens motion capture suit. We demonstrate EgoSim's effectiveness by training an end-to-end video-only 3D pose estimation network. Analyzing its domain gap, we show that our dataset and simulator substantially aid training for inference on real-world data. EgoSim code & MultiEgoView dataset: https://siplab.org/projects/EgoSim

### Near-Shore Mapping for Detection and Tracking of Vessels 
[[arxiv](https://arxiv.org/abs/2502.18368)] [[cool](https://papers.cool/arxiv/2502.18368)] [[pdf](https://arxiv.org/pdf/2502.18368)]
> **Authors**: Nicholas Dalhaug,Annette Stahl,Rudolf Mester,Edmund Førland Brekke
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Submitted to FUSION 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: For an autonomous surface vessel (ASV) to dock, it must track other vessels close to the docking area. Kayaks present a particular challenge due to their proximity to the dock and relatively small size. Maritime target tracking has typically employed land masking to filter out land and the dock. However, imprecise land masking makes it difficult to track close-to-dock objects. Our approach uses Light Detection And Ranging (LiDAR) data and maps the docking area offline. The precise 3D measurements allow for precise map creation. However, the mapping could result in static, yet potentially moving, objects being mapped. We detect and filter out potentially moving objects from the LiDAR data by utilizing image data. The visual vessel detection and segmentation method is a neural network that is trained on our labeled data. Close-to-shore tracking improves with an accurate map and is demonstrated on a recently gathered real-world dataset. The dataset contains multiple sequences of a kayak and a day cruiser moving close to the dock, in a collision path with an autonomous ferry prototype.

### ART: Anonymous Region Transformer for Variable Multi-Layer Transparent Image Generation 
[[arxiv](https://arxiv.org/abs/2502.18364)] [[cool](https://papers.cool/arxiv/2502.18364)] [[pdf](https://arxiv.org/pdf/2502.18364)]
> **Authors**: Yifan Pu,Yiming Zhao,Zhicong Tang,Ruihong Yin,Haoxing Ye,Yuhui Yuan,Dong Chen,Jianmin Bao,Sirui Zhang,Yanbin Wang,Lin Liang,Lijuan Wang,Ji Li,Xiu Li,Zhouhui Lian,Gao Huang,Baining Guo
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Project page: https://art-msra.github.io/
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Multi-layer image generation is a fundamental task that enables users to isolate, select, and edit specific image layers, thereby revolutionizing interactions with generative models. In this paper, we introduce the Anonymous Region Transformer (ART), which facilitates the direct generation of variable multi-layer transparent images based on a global text prompt and an anonymous region layout. Inspired by Schema theory suggests that knowledge is organized in frameworks (schemas) that enable people to interpret and learn from new information by linking it to prior knowledge.}, this anonymous region layout allows the generative model to autonomously determine which set of visual tokens should align with which text tokens, which is in contrast to the previously dominant semantic layout for the image generation task. In addition, the layer-wise region crop mechanism, which only selects the visual tokens belonging to each anonymous region, significantly reduces attention computation costs and enables the efficient generation of images with numerous distinct layers (e.g., 50+). When compared to the full attention approach, our method is over 12 times faster and exhibits fewer layer conflicts. Furthermore, we propose a high-quality multi-layer transparent image autoencoder that supports the direct encoding and decoding of the transparency of variable multi-layer images in a joint manner. By enabling precise control and scalable layer generation, ART establishes a new paradigm for interactive content creation.

### Self-Supervised Data Generation for Precision Agriculture: Blending Simulated Environments with Real Imagery 
[[arxiv](https://arxiv.org/abs/2502.18320)] [[cool](https://papers.cool/arxiv/2502.18320)] [[pdf](https://arxiv.org/pdf/2502.18320)]
> **Authors**: Leonardo Saraceni,Ionut Marian Motoi,Daniele Nardi,Thomas Alessandro Ciarfuglia
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Presented at 2024 IEEE 20th International Conference on Automation Science and Engineering (CASE)
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习,机器人技术
- **Abstract**: In precision agriculture, the scarcity of labeled data and significant covariate shifts pose unique challenges for training machine learning models. This scarcity is particularly problematic due to the dynamic nature of the environment and the evolving appearance of agricultural subjects as living things. We propose a novel system for generating realistic synthetic data to address these challenges. Utilizing a vineyard simulator based on the Unity engine, our system employs a cut-and-paste technique with geometrical consistency considerations to produce accurate photo-realistic images and labels from synthetic environments to train detection algorithms. This approach generates diverse data samples across various viewpoints and lighting conditions. We demonstrate considerable performance improvements in training a state-of-the-art detector by applying our method to table grapes cultivation. The combination of techniques can be easily automated, an increasingly important consideration for adoption in agricultural practice.

### LDGen: Enhancing Text-to-Image Synthesis via Large Language Model-Driven Language Representation 
[[arxiv](https://arxiv.org/abs/2502.18302)] [[cool](https://papers.cool/arxiv/2502.18302)] [[pdf](https://arxiv.org/pdf/2502.18302)]
> **Authors**: Pengzhi Li,Pengfei Yu,Zide Liu,Wei He,Xuhao Pan,Xudong Rao,Tao Wei,Wei Chen
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: In this paper, we introduce LDGen, a novel method for integrating large language models (LLMs) into existing text-to-image diffusion models while minimizing computational demands. Traditional text encoders, such as CLIP and T5, exhibit limitations in multilingual processing, hindering image generation across diverse languages. We address these challenges by leveraging the advanced capabilities of LLMs. Our approach employs a language representation strategy that applies hierarchical caption optimization and human instruction techniques to derive precise semantic information,. Subsequently, we incorporate a lightweight adapter and a cross-modal refiner to facilitate efficient feature alignment and interaction between LLMs and image features. LDGen reduces training time and enables zero-shot multilingual image generation. Experimental results indicate that our method surpasses baseline models in both prompt adherence and image aesthetic quality, while seamlessly supporting multiple languages. Project page: https://zrealli.github.io/LDGen.

### Stealthy Backdoor Attack in Self-Supervised Learning Vision Encoders for Large Vision Language Models 
[[arxiv](https://arxiv.org/abs/2502.18290)] [[cool](https://papers.cool/arxiv/2502.18290)] [[pdf](https://arxiv.org/pdf/2502.18290)]
> **Authors**: Zhaoyi Liu,Huan Zhang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Self-supervised learning (SSL) vision encoders learn high-quality image representations and thus have become a vital part of developing vision modality of large vision language models (LVLMs). Due to the high cost of training such encoders, pre-trained encoders are widely shared and deployed into many LVLMs, which are security-critical or bear societal significance. Under this practical scenario, we reveal a new backdoor threat that significant visual hallucinations can be induced into these LVLMs by merely compromising vision encoders. Because of the sharing and reuse of these encoders, many downstream LVLMs may inherit backdoor behaviors from encoders, leading to widespread backdoors. In this work, we propose BadVision, the first method to exploit this vulnerability in SSL vision encoders for LVLMs with novel trigger optimization and backdoor learning techniques. We evaluate BadVision on two types of SSL encoders and LVLMs across eight benchmarks. We show that BadVision effectively drives the LVLMs to attacker-chosen hallucination with over 99% attack success rate, causing a 77.6% relative visual understanding error while maintaining the stealthiness. SoTA backdoor detection methods cannot detect our attack effectively.

### Multi-label out-of-distribution detection via evidential learning 
[[arxiv](https://arxiv.org/abs/2502.18224)] [[cool](https://papers.cool/arxiv/2502.18224)] [[pdf](https://arxiv.org/pdf/2502.18224)]
> **Authors**: Eduardo Aguilar,Bogdan Raducanu,Petia Radeva
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Accepted at Uncertainty Quantification for Computer Vision workshop (ECCVW 2024)
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: A crucial requirement for machine learning algorithms is not only to perform well, but also to show robustness and adaptability when encountering novel scenarios. One way to achieve these characteristics is to endow the deep learning models with the ability to detect out-of-distribution (OOD) data, i.e. data that belong to distributions different from the one used during their training. It is even a more complicated situation, when these data usually are multi-label. In this paper, we propose an approach based on evidential deep learning in order to meet these challenges applied to visual recognition problems. More concretely, we designed a CNN architecture that uses a Beta Evidential Neural Network to compute both the likelihood and the predictive uncertainty of the samples. Based on these results, we propose afterwards two new uncertainty-based scores for OOD data detection: (i) OOD - score Max, based on the maximum evidence; and (ii) OOD score - Sum, which considers the evidence from all outputs. Extensive experiments have been carried out to validate the proposed approach using three widely-used datasets: PASCAL-VOC, MS-COCO and NUS-WIDE, demonstrating its outperformance over several State-of-the-Art methods.

### Learning Structure-Supporting Dependencies via Keypoint Interactive Transformer for General Mammal Pose Estimation 
[[arxiv](https://arxiv.org/abs/2502.18214)] [[cool](https://papers.cool/arxiv/2502.18214)] [[pdf](https://arxiv.org/pdf/2502.18214)]
> **Authors**: Tianyang Xu,Jiyong Rao,Xiaoning Song,Zhenhua Feng,Xiao-Jun Wu
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: accepted by IJCV 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: General mammal pose estimation is an important and challenging task in computer vision, which is essential for understanding mammal behaviour in real-world applications. However, existing studies are at their preliminary research stage, which focus on addressing the problem for only a few specific mammal species. In principle, from specific to general mammal pose estimation, the biggest issue is how to address the huge appearance and pose variances for different species. We argue that given appearance context, instance-level prior and the structural relation among keypoints can serve as complementary evidence. To this end, we propose a Keypoint Interactive Transformer (KIT) to learn instance-level structure-supporting dependencies for general mammal pose estimation. Specifically, our KITPose consists of two coupled components. The first component is to extract keypoint features and generate body part prompts. The features are supervised by a dedicated generalised heatmap regression loss (GHRL). Instead of introducing external visual/text prompts, we devise keypoints clustering to generate body part biases, aligning them with image context to generate corresponding instance-level prompts. Second, we propose a novel interactive transformer that takes feature slices as input tokens without performing spatial splitting. In addition, to enhance the capability of the KIT model, we design an adaptive weight strategy to address the imbalance issue among different keypoints.

### CLIPure: Purification in Latent Space via CLIP for Adversarially Robust Zero-Shot Classification 
[[arxiv](https://arxiv.org/abs/2502.18176)] [[cool](https://papers.cool/arxiv/2502.18176)] [[pdf](https://arxiv.org/pdf/2502.18176)]
> **Authors**: Mingkun Zhang,Keping Bi,Wei Chen,Jiafeng Guo,Xueqi Cheng
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: accepted by ICLR 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: In this paper, we aim to build an adversarially robust zero-shot image classifier. We ground our work on CLIP, a vision-language pre-trained encoder model that can perform zero-shot classification by matching an image with text prompts ``a photo of a <class-name>.''. Purification is the path we choose since it does not require adversarial training on specific attack types and thus can cope with any foreseen attacks. We then formulate purification risk as the KL divergence between the joint distributions of the purification process of denoising the adversarial samples and the attack process of adding perturbations to benign samples, through bidirectional Stochastic Differential Equations (SDEs). The final derived results inspire us to explore purification in the multi-modal latent space of CLIP. We propose two variants for our CLIPure approach: CLIPure-Diff which models the likelihood of images' latent vectors with the DiffusionPrior module in DaLLE-2 (modeling the generation process of CLIP's latent vectors), and CLIPure-Cos which models the likelihood with the cosine similarity between the embeddings of an image and ``a photo of a.''. As far as we know, CLIPure is the first purification method in multi-modal latent space and CLIPure-Cos is the first purification method that is not based on generative models, which substantially improves defense efficiency. We conducted extensive experiments on CIFAR-10, ImageNet, and 13 datasets that previous CLIP-based defense methods used for evaluating zero-shot classification robustness. Results show that CLIPure boosts the SOTA robustness by a large margin, e.g., from 71.7% to 91.1% on CIFAR10, from 59.6% to 72.6% on ImageNet, and 108% relative improvements of average robustness on the 13 datasets over previous SOTA. The code is available at https://github.com/TMLResearchGroup-CAS/CLIPure.

### Monitoring snow avalanches from SAR data with deep learning 
[[arxiv](https://arxiv.org/abs/2502.18157)] [[cool](https://papers.cool/arxiv/2502.18157)] [[pdf](https://arxiv.org/pdf/2502.18157)]
> **Authors**: Filippo Maria Bianchi,Jakob Grahn
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,图像和视频处理
- **Abstract**: Snow avalanches present significant risks to human life and infrastructure, particularly in mountainous regions, making effective monitoring crucial. Traditional monitoring methods, such as field observations, are limited by accessibility, weather conditions, and cost. Satellite-borne Synthetic Aperture Radar (SAR) data has become an important tool for large-scale avalanche detection, as it can capture data in all weather conditions and across remote areas. However, traditional processing methods struggle with the complexity and variability of avalanches. This chapter reviews the application of deep learning for detecting and segmenting snow avalanches from SAR data. Early efforts focused on the binary classification of SAR images, while recent advances have enabled pixel-level segmentation, providing greater accuracy and spatial resolution. A case study using Sentinel-1 SAR data demonstrates the effectiveness of deep learning models for avalanche segmentation, achieving superior results over traditional methods. We also present an extension of this work, testing recent state-of-the-art segmentation architectures on an expanded dataset of over 4,500 annotated SAR images. The best-performing model among those tested was applied for large-scale avalanche detection across the whole of Norway, revealing important spatial and temporal patterns over several winter seasons.

### Personalized Federated Learning for Egocentric Video Gaze Estimation with Comprehensive Parameter Frezzing 
[[arxiv](https://arxiv.org/abs/2502.18123)] [[cool](https://papers.cool/arxiv/2502.18123)] [[pdf](https://arxiv.org/pdf/2502.18123)]
> **Authors**: Yuhu Feng,Keisuke Maeda,Takahiro Ogawa,Miki Haseyama
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Egocentric video gaze estimation requires models to capture individual gaze patterns while adapting to diverse user data. Our approach leverages a transformer-based architecture, integrating it into a PFL framework where only the most significant parameters, those exhibiting the highest rate of change during training, are selected and frozen for personalization in client models. Through extensive experimentation on the EGTEA Gaze+ and Ego4D datasets, we demonstrate that FedCPF significantly outperforms previously reported federated learning methods, achieving superior recall, precision, and F1-score. These results confirm the effectiveness of our comprehensive parameters freezing strategy in enhancing model personalization, making FedCPF a promising approach for tasks requiring both adaptability and accuracy in federated learning settings.

### Bayesian Optimization for Controlled Image Editing via LLMs 
[[arxiv](https://arxiv.org/abs/2502.18116)] [[cool](https://papers.cool/arxiv/2502.18116)] [[pdf](https://arxiv.org/pdf/2502.18116)]
> **Authors**: Chengkun Cai,Haoliang Liu,Xu Zhao,Zhongyu Jiang,Tianfang Zhang,Zongkai Wu,Jenq-Neng Hwang,Serge Belongie,Lei Li
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 8 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **Abstract**: In the rapidly evolving field of image generation, achieving precise control over generated content and maintaining semantic consistency remain significant limitations, particularly concerning grounding techniques and the necessity for model fine-tuning. To address these challenges, we propose BayesGenie, an off-the-shelf approach that integrates Large Language Models (LLMs) with Bayesian Optimization to facilitate precise and user-friendly image editing. Our method enables users to modify images through natural language descriptions without manual area marking, while preserving the original image's semantic integrity. Unlike existing techniques that require extensive pre-training or fine-tuning, our approach demonstrates remarkable adaptability across various LLMs through its model-agnostic design. BayesGenie employs an adapted Bayesian optimization strategy to automatically refine the inference process parameters, achieving high-precision image editing with minimal user intervention. Through extensive experiments across diverse scenarios, we demonstrate that our framework significantly outperforms existing methods in both editing accuracy and semantic preservation, as validated using different LLMs including Claude3 and GPT-4.

### Detecting Offensive Memes with Social Biases in Singapore Context Using Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.18101)] [[cool](https://papers.cool/arxiv/2502.18101)] [[pdf](https://arxiv.org/pdf/2502.18101)]
> **Authors**: Cao Yuxuan,Wu Jiayang,Alistair Cheong Liang Chuen,Bryan Shan Guanrong,Theodore Lee Chong Jen,Sherman Chann Zhi Shen
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算语言学
- **Abstract**: Traditional online content moderation systems struggle to classify modern multimodal means of communication, such as memes, a highly nuanced and information-dense medium. This task is especially hard in a culturally diverse society like Singapore, where low-resource languages are used and extensive knowledge on local context is needed to interpret online content. We curate a large collection of 112K memes labeled by GPT-4V for fine-tuning a VLM to classify offensive memes in Singapore context. We show the effectiveness of fine-tuned VLMs on our dataset, and propose a pipeline containing OCR, translation and a 7-billion parameter-class VLM. Our solutions reach 80.62% accuracy and 0.8192 AUROC on a held-out test set, and can greatly aid human in moderating online contents. The dataset, code, and model weights will be open-sourced at https://github.com/aliencaocao/vlm-for-memes-aisg.

### FwNet-ECA: Facilitating Window Attention with Global Receptive Fields through Fourier Filtering Operations 
[[arxiv](https://arxiv.org/abs/2502.18094)] [[cool](https://papers.cool/arxiv/2502.18094)] [[pdf](https://arxiv.org/pdf/2502.18094)]
> **Authors**: Shengtian Mian,Ya Wang,Nannan Gu,Yuping Wang,Xiaoqing Li
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Windowed attention mechanisms were introduced to mitigate the issue of excessive computation inherent in global attention mechanisms. However, In this paper, we present FwNet-ECA, a novel method that utilizes Fourier transforms paired with learnable weight matrices to enhance the spectral features of images. This strategy facilitates inter-window connectivity, thereby maximizing the receptive field. Additionally, we incorporate the Efficient Channel Attention (ECA) module to improve communication between different channels. Instead of relying on physically shifted windows, our approach leverages frequency domain enhancement to implicitly bridge information across spatial regions. We validate our model on the iCartoonFace dataset and conduct downstream tasks on ImageNet, demonstrating that our model achieves lower parameter counts and computational overheads compared to shifted window approaches, while maintaining competitive accuracy. This work offers a more efficient and effective alternative for leveraging attention mechanisms in visual processing tasks, alleviating the challenges associated with windowed attention models. Code is available at https://github.com/qingxiaoli/FwNet-ECA.

### A Fusion Model for Artwork Identification Based on Convolutional Neural Networks and Transformers 
[[arxiv](https://arxiv.org/abs/2502.18083)] [[cool](https://papers.cool/arxiv/2502.18083)] [[pdf](https://arxiv.org/pdf/2502.18083)]
> **Authors**: Zhenyu Wang,Heng Song
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: The identification of artwork is crucial in areas like cultural heritage protection, art market analysis, and historical research. With the advancement of deep learning, Convolutional Neural Networks (CNNs) and Transformer models have become key tools for image classification. While CNNs excel in local feature extraction, they struggle with global context, and Transformers are strong in capturing global dependencies but weak in fine-grained local details. To address these challenges, this paper proposes a fusion model combining CNNs and Transformers for artwork identification. The model first extracts local features using CNNs, then captures global context with a Transformer, followed by a feature fusion mechanism to enhance classification accuracy. Experiments on Chinese and oil painting datasets show the fusion model outperforms individual CNN and Transformer models, improving classification accuracy by 9.7% and 7.1%, respectively, and increasing F1 scores by 0.06 and 0.05. The results demonstrate the model's effectiveness and potential for future improvements, such as multimodal integration and architecture optimization.

### Examining the Threat Landscape: Foundation Models and Model Stealing 
[[arxiv](https://arxiv.org/abs/2502.18077)] [[cool](https://papers.cool/arxiv/2502.18077)] [[pdf](https://arxiv.org/pdf/2502.18077)]
> **Authors**: Ankita Raj,Deepankar Varma,Chetan Arora
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Accepted to BMVC 2024
- **标题**: None
- **领域**: 计算机视觉和模式识别,密码学和安全,机器学习
- **Abstract**: Foundation models (FMs) for computer vision learn rich and robust representations, enabling their adaptation to task/domain-specific deployments with little to no fine-tuning. However, we posit that the very same strength can make applications based on FMs vulnerable to model stealing attacks. Through empirical analysis, we reveal that models fine-tuned from FMs harbor heightened susceptibility to model stealing, compared to conventional vision architectures like ResNets. We hypothesize that this behavior is due to the comprehensive encoding of visual patterns and features learned by FMs during pre-training, which are accessible to both the attacker and the victim. We report that an attacker is able to obtain 94.28% agreement (matched predictions with victim) for a Vision Transformer based victim model (ViT-L/16) trained on CIFAR-10 dataset, compared to only 73.20% agreement for a ResNet-18 victim, when using ViT-L/16 as the thief model. We arguably show, for the first time, that utilizing FMs for downstream tasks may not be the best choice for deployment in commercial APIs due to their susceptibility to model theft. We thereby alert model owners towards the associated security risks, and highlight the need for robust security measures to safeguard such models against theft. Code is available at https://github.com/rajankita/foundation_model_stealing.

### Escaping The Big Data Paradigm in Self-Supervised Representation Learning 
[[arxiv](https://arxiv.org/abs/2502.18056)] [[cool](https://papers.cool/arxiv/2502.18056)] [[pdf](https://arxiv.org/pdf/2502.18056)]
> **Authors**: Carlos Vélez García,Miguel Cazorla,Jorge Pomares
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Code and implementation available at: https://github.com/inescopresearch/scott
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: The reliance on large-scale datasets and extensive computational resources has become a major barrier to advancing representation learning in vision, especially in data-scarce domains. In this paper, we address the critical question: Can we escape the big data paradigm in self-supervised representation learning from images? We introduce SCOTT (Sparse Convolutional Tokenizer for Transformers), a shallow tokenization architecture that is compatible with Masked Image Modeling (MIM) tasks. SCOTT injects convolutional inductive biases into Vision Transformers (ViTs), enhancing their efficacy in small-scale data regimes. Alongside, we propose to train on a Joint-Embedding Predictive Architecture within a MIM framework (MIM-JEPA), operating in latent representation space to capture more semantic features. Our approach enables ViTs to be trained from scratch on datasets orders of magnitude smaller than traditionally required --without relying on massive external datasets for pretraining. We validate our method on three small-size, standard-resoultion, fine-grained datasets: Oxford Flowers-102, Oxford IIIT Pets-37, and ImageNet-100. Despite the challenges of limited data and high intra-class similarity, frozen SCOTT models pretrained with MIM-JEPA significantly outperform fully supervised methods and achieve competitive results with SOTA approaches that rely on large-scale pretraining, complex image augmentations and bigger model sizes. By demonstrating that robust off-the-shelf representations can be learned with limited data, compute, and model sizes, our work paves the way for computer applications in resource constrained environments such as medical imaging or robotics. Our findings challenge the prevailing notion that vast amounts of data are indispensable for effective representation learning in vision, offering a new pathway toward more accessible and inclusive advancements in the field.

### Progressive Local Alignment for Medical Multimodal Pre-training 
[[arxiv](https://arxiv.org/abs/2502.18047)] [[cool](https://papers.cool/arxiv/2502.18047)] [[pdf](https://arxiv.org/pdf/2502.18047)]
> **Authors**: Huimin Yan,Xian Yang,Liang Bai,Jiye Liang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Local alignment between medical images and text is essential for accurate diagnosis, though it remains challenging due to the absence of natural local pairings and the limitations of rigid region recognition methods. Traditional approaches rely on hard boundaries, which introduce uncertainty, whereas medical imaging demands flexible soft region recognition to handle irregular structures. To overcome these challenges, we propose the Progressive Local Alignment Network (PLAN), which designs a novel contrastive learning-based approach for local alignment to establish meaningful word-pixel relationships and introduces a progressive learning strategy to iteratively refine these relationships, enhancing alignment precision and robustness. By combining these techniques, PLAN effectively improves soft region recognition while suppressing noise interference. Extensive experiments on multiple medical datasets demonstrate that PLAN surpasses state-of-the-art methods in phrase grounding, image-text retrieval, object detection, and zero-shot classification, setting a new benchmark for medical image-text alignment.

### VLM-E2E: Enhancing End-to-End Autonomous Driving with Multimodal Driver Attention Fusion 
[[arxiv](https://arxiv.org/abs/2502.18042)] [[cool](https://papers.cool/arxiv/2502.18042)] [[pdf](https://arxiv.org/pdf/2502.18042)]
> **Authors**: Pei Liu,Haipeng Liu,Haichao Liu,Xin Liu,Jinxin Ni,Jun Ma
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Human drivers adeptly navigate complex scenarios by utilizing rich attentional semantics, but the current autonomous systems struggle to replicate this ability, as they often lose critical semantic information when converting 2D observations into 3D space. In this sense, it hinders their effective deployment in dynamic and complex environments. Leveraging the superior scene understanding and reasoning abilities of Vision-Language Models (VLMs), we propose VLM-E2E, a novel framework that uses the VLMs to enhance training by providing attentional cues. Our method integrates textual representations into Bird's-Eye-View (BEV) features for semantic supervision, which enables the model to learn richer feature representations that explicitly capture the driver's attentional semantics. By focusing on attentional semantics, VLM-E2E better aligns with human-like driving behavior, which is critical for navigating dynamic and complex environments. Furthermore, we introduce a BEV-Text learnable weighted fusion strategy to address the issue of modality importance imbalance in fusing multimodal information. This approach dynamically balances the contributions of BEV and text features, ensuring that the complementary information from visual and textual modality is effectively utilized. By explicitly addressing the imbalance in multimodal fusion, our method facilitates a more holistic and robust representation of driving environments. We evaluate VLM-E2E on the nuScenes dataset and demonstrate its superiority over state-of-the-art approaches, showcasing significant improvements in performance.

### OpenFly: A Versatile Toolchain and Large-scale Benchmark for Aerial Vision-Language Navigation 
[[arxiv](https://arxiv.org/abs/2502.18041)] [[cool](https://papers.cool/arxiv/2502.18041)] [[pdf](https://arxiv.org/pdf/2502.18041)]
> **Authors**: Yunpeng Gao,Chenhui Li,Zhongrui You,Junli Liu,Zhen Li,Pengan Chen,Qizhi Chen,Zhonghan Tang,Liansheng Wang,Penghui Yang,Yiwen Tang,Yuhang Tang,Shuai Liang,Songyi Zhu,Ziqin Xiong,Yifei Su,Xinyi Ye,Jianan Li,Yan Ding,Dong Wang,Zhigang Wang,Bin Zhao,Xuelong Li
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器人技术
- **Abstract**: Vision-Language Navigation (VLN) aims to guide agents through an environment by leveraging both language instructions and visual cues, playing a pivotal role in embodied AI. Indoor VLN has been extensively studied, whereas outdoor aerial VLN remains underexplored. The potential reason is that outdoor aerial view encompasses vast areas, making data collection more challenging, which results in a lack of benchmarks. To address this problem, we propose OpenFly, a platform comprising a versatile toolchain and large-scale benchmark for aerial VLN. Firstly, we develop a highly automated toolchain for data collection, enabling automatic point cloud acquisition, scene semantic segmentation, flight trajectory creation, and instruction generation. Secondly, based on the toolchain, we construct a large-scale aerial VLN dataset with 100k trajectories, covering diverse heights and lengths across 18 scenes. The corresponding visual data are generated using various rendering engines and advanced techniques, including Unreal Engine, GTA V, Google Earth, and 3D Gaussian Splatting (3D GS). All data exhibit high visual quality. Particularly, 3D GS supports real-to-sim rendering, further enhancing the realism of the dataset. Thirdly, we propose OpenFly-Agent, a keyframe-aware VLN model, which takes language instructions, current observations, and historical keyframes as input, and outputs flight actions directly. Extensive analyses and experiments are conducted, showcasing the superiority of our OpenFly platform and OpenFly-Agent. The toolchain, dataset, and codes will be open-sourced.

### ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic Iterative Reasoning Agents 
[[arxiv](https://arxiv.org/abs/2502.18017)] [[cool](https://papers.cool/arxiv/2502.18017)] [[pdf](https://arxiv.org/pdf/2502.18017)]
> **Authors**: Qiuchen Wang,Ruixue Ding,Zehui Chen,Weiqi Wu,Shihang Wang,Pengjun Xie,Feng Zhao
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,信息检索
- **Abstract**: Understanding information from visually rich documents remains a significant challenge for traditional Retrieval-Augmented Generation (RAG) methods. Existing benchmarks predominantly focus on image-based question answering (QA), overlooking the fundamental challenges of efficient retrieval, comprehension, and reasoning within dense visual documents. To bridge this gap, we introduce ViDoSeek, a novel dataset designed to evaluate RAG performance on visually rich documents requiring complex reasoning. Based on it, we identify key limitations in current RAG approaches: (i) purely visual retrieval methods struggle to effectively integrate both textual and visual features, and (ii) previous approaches often allocate insufficient reasoning tokens, limiting their effectiveness. To address these challenges, we propose ViDoRAG, a novel multi-agent RAG framework tailored for complex reasoning across visual documents. ViDoRAG employs a Gaussian Mixture Model (GMM)-based hybrid strategy to effectively handle multi-modal retrieval. To further elicit the model's reasoning capabilities, we introduce an iterative agent workflow incorporating exploration, summarization, and reflection, providing a framework for investigating test-time scaling in RAG domains. Extensive experiments on ViDoSeek validate the effectiveness and generalization of our approach. Notably, ViDoRAG outperforms existing methods by over 10% on the competitive ViDoSeek benchmark.

### Shedding Light on the Polymer's Identity: Microplastic Detection and Identification Through Nile Red Staining and Multispectral Imaging (FIMAP) 
[[arxiv](https://arxiv.org/abs/2502.17997)] [[cool](https://papers.cool/arxiv/2502.17997)] [[pdf](https://arxiv.org/pdf/2502.17997)]
> **Authors**: Derek Ho,Haotian Feng
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 20 pages (with additional supplementary material), 5 Figures, 3 Tables
- **标题**: None
- **领域**: 计算机视觉和模式识别,新兴技术,机器学习
- **Abstract**: The widespread distribution of microplastics (MPs) in the environment presents significant challenges for their detection and identification. Fluorescence imaging has emerged as a promising technique for enhancing plastic particle detectability and enabling accurate classification based on fluorescence behavior. However, conventional segmentation techniques face limitations, including poor signal-to-noise ratio, inconsistent illumination, thresholding difficulties, and false positives from natural organic matter (NOM). To address these challenges, this study introduces the Fluorescence Imaging Microplastic Analysis Platform (FIMAP), a retrofitted multispectral camera with four optical filters and five excitation wavelengths. FIMAP enables comprehensive characterization of the fluorescence behavior of ten Nile Red-stained MPs: HDPE, LDPE, PP, PS, EPS, ABS, PVC, PC, PET, and PA, while effectively excluding NOM. Using K-means clustering for robust segmentation (Intersection over Union = 0.877) and a 20-dimensional color coordinate multivariate nearest neighbor approach for MP classification (>3.14 mm), FIMAP achieves 90% precision, 90% accuracy, 100% recall, and an F1 score of 94.7%. Only PS was occasionally misclassified as EPS. For smaller MPs (35-104 microns), classification accuracy declined, likely due to reduced stain sorption, fewer detectable pixels, and camera instability. Integrating FIMAP with higher-magnification instruments, such as a microscope, may enhance MP identification. This study presents FIMAP as an automated, high-throughput framework for detecting and classifying MPs across large environmental sample volumes.

### Robust Polyp Detection and Diagnosis through Compositional Prompt-Guided Diffusion Models 
[[arxiv](https://arxiv.org/abs/2502.17951)] [[cool](https://papers.cool/arxiv/2502.17951)] [[pdf](https://arxiv.org/pdf/2502.17951)]
> **Authors**: Jia Yu,Yan Zhu,Peiyao Fu,Tianyi Chen,Junbo Huang,Quanlin Li,Pinghong Zhou,Zhihua Wang,Fei Wu,Shuo Wang,Xian Yang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Colorectal cancer (CRC) is a significant global health concern, and early detection through screening plays a critical role in reducing mortality. While deep learning models have shown promise in improving polyp detection, classification, and segmentation, their generalization across diverse clinical environments, particularly with out-of-distribution (OOD) data, remains a challenge. Multi-center datasets like PolypGen have been developed to address these issues, but their collection is costly and time-consuming. Traditional data augmentation techniques provide limited variability, failing to capture the complexity of medical images. Diffusion models have emerged as a promising solution for generating synthetic polyp images, but the image generation process in current models mainly relies on segmentation masks as the condition, limiting their ability to capture the full clinical context. To overcome these limitations, we propose a Progressive Spectrum Diffusion Model (PSDM) that integrates diverse clinical annotations-such as segmentation masks, bounding boxes, and colonoscopy reports-by transforming them into compositional prompts. These prompts are organized into coarse and fine components, allowing the model to capture both broad spatial structures and fine details, generating clinically accurate synthetic images. By augmenting training data with PSDM-generated samples, our model significantly improves polyp detection, classification, and segmentation. For instance, on the PolypGen dataset, PSDM increases the F1 score by 2.12% and the mean average precision by 3.09%, demonstrating superior performance in OOD scenarios and enhanced generalization.

### Optimal Brain Apoptosis 
[[arxiv](https://arxiv.org/abs/2502.17941)] [[cool](https://papers.cool/arxiv/2502.17941)] [[pdf](https://arxiv.org/pdf/2502.17941)]
> **Authors**: Mingyuan Sun,Zheng Fang,Jiaxu Wang,Junjie Jiang,Delei Kong,Chenming Hu,Yuetong Fang,Renjing Xu
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Accepted to ICLR 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: The increasing complexity and parameter count of Convolutional Neural Networks (CNNs) and Transformers pose challenges in terms of computational efficiency and resource demands. Pruning has been identified as an effective strategy to address these challenges by removing redundant elements such as neurons, channels, or connections, thereby enhancing computational efficiency without heavily compromising performance. This paper builds on the foundational work of Optimal Brain Damage (OBD) by advancing the methodology of parameter importance estimation using the Hessian matrix. Unlike previous approaches that rely on approximations, we introduce Optimal Brain Apoptosis (OBA), a novel pruning method that calculates the Hessian-vector product value directly for each parameter. By decomposing the Hessian matrix across network layers and identifying conditions under which inter-layer Hessian submatrices are non-zero, we propose a highly efficient technique for computing the second-order Taylor expansion of parameters. This approach allows for a more precise pruning process, particularly in the context of CNNs and Transformers, as validated in our experiments including VGG19, ResNet32, ResNet50, and ViT-B/16 on CIFAR10, CIFAR100 and Imagenet datasets. Our code is available at https://github.com/NEU-REAL/OBA.

### BD Currency Detection: A CNN Based Approach with Mobile App Integration 
[[arxiv](https://arxiv.org/abs/2502.17907)] [[cool](https://papers.cool/arxiv/2502.17907)] [[pdf](https://arxiv.org/pdf/2502.17907)]
> **Authors**: Syed Jubayer Jaman,Md. Zahurul Haque,Md Robiul Islam,Usama Abdun Noor
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习,网络和互联网架构
- **Abstract**: Currency recognition plays a vital role in banking, commerce, and assistive technology for visually impaired individuals. Traditional methods, such as manual verification and optical scanning, often suffer from limitations in accuracy and efficiency. This study introduces an advanced currency recognition system utilizing Convolutional Neural Networks (CNNs) to accurately classify Bangladeshi banknotes. A dataset comprising 50,334 images was collected, preprocessed, and used to train a CNN model optimized for high performance classification. The trained model achieved an accuracy of 98.5%, surpassing conventional image based currency recognition approaches. To enable real time and offline functionality, the model was converted into TensorFlow Lite format and integrated into an Android mobile application. The results highlight the effectiveness of deep learning in currency recognition, providing a fast, secure, and accessible solution that enhances financial transactions and assistive technologies.

### From underwater to aerial: a novel multi-scale knowledge distillation approach for coral reef monitoring 
[[arxiv](https://arxiv.org/abs/2502.17883)] [[cool](https://papers.cool/arxiv/2502.17883)] [[pdf](https://arxiv.org/pdf/2502.17883)]
> **Authors**: Matteo Contini,Victor Illien,Julien Barde,Sylvain Poulain,Serge Bernard,Alexis Joly,Sylvain Bonhommeau
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Drone-based remote sensing combined with AI-driven methodologies has shown great potential for accurate mapping and monitoring of coral reef ecosystems. This study presents a novel multi-scale approach to coral reef monitoring, integrating fine-scale underwater imagery with medium-scale aerial imagery. Underwater images are captured using an Autonomous Surface Vehicle (ASV), while aerial images are acquired with an aerial drone. A transformer-based deep-learning model is trained on underwater images to detect the presence of 31 classes covering various coral morphotypes, associated fauna, and habitats. These predictions serve as annotations for training a second model applied to aerial images. The transfer of information across scales is achieved through a weighted footprint method that accounts for partial overlaps between underwater image footprints and aerial image tiles. The results show that the multi-scale methodology successfully extends fine-scale classification to larger reef areas, achieving a high degree of accuracy in predicting coral morphotypes and associated habitats. The method showed a strong alignment between underwater-derived annotations and ground truth data, reflected by an AUC (Area Under the Curve) score of 0.9251. This shows that the integration of underwater and aerial imagery, supported by deep-learning models, can facilitate scalable and accurate reef assessments. This study demonstrates the potential of combining multi-scale imaging and AI to facilitate the monitoring and conservation of coral reefs. Our approach leverages the strengths of underwater and aerial imagery, ensuring the precision of fine-scale analysis while extending it to cover a broader reef area.

### HRR: Hierarchical Retrospection Refinement for Generated Image Detection 
[[arxiv](https://arxiv.org/abs/2502.17862)] [[cool](https://papers.cool/arxiv/2502.17862)] [[pdf](https://arxiv.org/pdf/2502.17862)]
> **Authors**: Peipei Yuan,Zijing Xie,Shuo Ye,Hong Chen,Yulong Wang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Generative artificial intelligence holds significant potential for abuse, and generative image detection has become a key focus of research. However, existing methods primarily focused on detecting a specific generative model and emphasizing the localization of synthetic regions, while neglecting the interference caused by image size and style on model learning. Our goal is to reach a fundamental conclusion: Is the image real or generated? To this end, we propose a diffusion model-based generative image detection framework termed Hierarchical Retrospection Refinement~(HRR). It designs a multi-scale style retrospection module that encourages the model to generate detailed and realistic multi-scale representations, while alleviating the learning biases introduced by dataset styles and generative models. Additionally, based on the principle of correntropy sparse additive machine, a feature refinement module is designed to reduce the impact of redundant features on learning and capture the intrinsic structure and patterns of the data, thereby improving the model's generalization ability. Extensive experiments demonstrate the HRR framework consistently delivers significant performance improvements, outperforming state-of-the-art methods in generated image detection task.

### UniGS: Unified Language-Image-3D Pretraining with Gaussian Splatting 
[[arxiv](https://arxiv.org/abs/2502.17860)] [[cool](https://papers.cool/arxiv/2502.17860)] [[pdf](https://arxiv.org/pdf/2502.17860)]
> **Authors**: Haoyuan Li,Yanpeng Zhou,Tao Tang,Jifei Song,Yihan Zeng,Michael Kampffmeyer,Hang Xu,Xiaodan Liang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: ICLR 2025; Corrected citation of Uni3D;
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Recent advancements in multi-modal 3D pre-training methods have shown promising efficacy in learning joint representations of text, images, and point clouds. However, adopting point clouds as 3D representation fails to fully capture the intricacies of the 3D world and exhibits a noticeable gap between the discrete points and the dense 2D pixels of images. To tackle this issue, we propose UniGS, integrating 3D Gaussian Splatting (3DGS) into multi-modal pre-training to enhance the 3D representation. We first rely on the 3DGS representation to model the 3D world as a collection of 3D Gaussians with color and opacity, incorporating all the information of the 3D scene while establishing a strong connection with 2D images. Then, to achieve Language-Image-3D pertaining, UniGS starts with a pre-trained vision-language model to establish a shared visual and textual space through extensive real-world image-text pairs. Subsequently, UniGS employs a 3D encoder to align the optimized 3DGS with the Language-Image representations to learn unified multi-modal representations. To facilitate the extraction of global explicit 3D features by the 3D encoder and achieve better cross-modal alignment, we additionally introduce a novel Gaussian-Aware Guidance module that guides the learning of fine-grained representations of the 3D domain. Through extensive experiments across the Objaverse, ABO, MVImgNet and SUN RGBD datasets with zero-shot classification, text-driven retrieval and open-world understanding tasks, we demonstrate the effectiveness of UniGS in learning a more general and stronger aligned multi-modal representation. Specifically, UniGS achieves leading results across different 3D tasks with remarkable improvements over previous SOTA, Uni3D, including on zero-shot classification (+9.36%), text-driven retrieval (+4.3%) and open-world understanding (+7.92%).

## 计算机与社会(cs.CY:Computers and Society)

### Policy-as-Prompt: Rethinking Content Moderation in the Age of Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.18695)] [[cool](https://papers.cool/arxiv/2502.18695)] [[pdf](https://arxiv.org/pdf/2502.18695)]
> **Authors**: Konstantina Palla,José Luis Redondo García,Claudia Hauff,Francesco Fabbri,Henrik Lindström,Daniel R. Taber,Andreas Damianou,Mounia Lalmas
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 14 pages, 5 figures
- **标题**: None
- **领域**: 计算机与社会,人工智能,社交和信息网络
- **Abstract**: Content moderation plays a critical role in shaping safe and inclusive online environments, balancing platform standards, user expectations, and regulatory frameworks. Traditionally, this process involves operationalising policies into guidelines, which are then used by downstream human moderators for enforcement, or to further annotate datasets for training machine learning moderation models. However, recent advancements in large language models (LLMs) are transforming this landscape. These models can now interpret policies directly as textual inputs, eliminating the need for extensive data curation. This approach offers unprecedented flexibility, as moderation can be dynamically adjusted through natural language interactions. This paradigm shift raises important questions about how policies are operationalised and the implications for content moderation practices. In this paper, we formalise the emerging policy-as-prompt framework and identify five key challenges across four domains: Technical Implementation (1. translating policy to prompts, 2. sensitivity to prompt structure and formatting), Sociotechnical (3. the risk of technological determinism in policy formation), Organisational (4. evolving roles between policy and machine learning teams), and Governance (5. model governance and accountability). Through analysing these challenges across technical, sociotechnical, organisational, and governance dimensions, we discuss potential mitigation approaches. This research provides actionable insights for practitioners and lays the groundwork for future exploration of scalable and adaptive content moderation systems in digital ecosystems.

## 数据结构和算法(cs.DS:Data Structures and Algorithms)

### Graph Inference with Effective Resistance Queries 
[[arxiv](https://arxiv.org/abs/2502.18350)] [[cool](https://papers.cool/arxiv/2502.18350)] [[pdf](https://arxiv.org/pdf/2502.18350)]
> **Authors**: Huck Bennett,Mitchell Black,Amir Nayyeri,Evelyn Warton
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 数据结构和算法,离散数学,机器学习
- **Abstract**: The goal of graph inference is to design algorithms for learning properties of a hidden graph using queries to an oracle that returns information about the graph. Graph reconstruction, verification, and property testing are all types of graph inference. In this work, we study graph inference using an oracle that returns the effective resistance (ER) between a pair of vertices. Effective resistance is a distance originating from the study of electrical circuits with many applications. However, ER has received little attention from a graph inference perspective. Indeed, although it is known that an $n$-vertex graph can be uniquely reconstructed from all $\binom{n}{2}$ possible ER queries, little else is known. We address this gap with several new results, including: 1. $O(n)$-query algorithms for testing whether a graph is a tree; deciding whether two graphs are equal assuming one is a subgraph of the other; and testing whether a given vertex (or edge) is a cut vertex (or cut edge). 2. Property testing algorithms, including for testing whether a graph is vertex- or edge-biconnected. We also give a reduction to adapt property testing results from the bounded-degree model to our ER query model. This yields ER-query-based algorithms for testing $k$-connectivity, bipartiteness, planarity, and containment of a fixed subgraph. 3. Graph reconstruction algorithms, including an algorithm for reconstructing a graph from a low-width tree decomposition; a $Θ(k^2)$-query, polynomial-time algorithm for recovering the adjacency matrix $A$ of a hidden graph, given $A$ with $k$ of its entries deleted; and a $k$-query, exponential-time algorithm for the same task. We also compare the power of ER queries and shortest path queries, which are closely related but better studied. Interestingly, we show that the two query models are incomparable in power.

### Near-optimal Active Regression of Single-Index Models 
[[arxiv](https://arxiv.org/abs/2502.18213)] [[cool](https://papers.cool/arxiv/2502.18213)] [[pdf](https://arxiv.org/pdf/2502.18213)]
> **Authors**: Yi Li,Wai Ming Tai
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 数据结构和算法,机器学习
- **Abstract**: The active regression problem of the single-index model is to solve $\min_x \lVert f(Ax)-b\rVert_p$, where $A$ is fully accessible and $b$ can only be accessed via entry queries, with the goal of minimizing the number of queries to the entries of $b$. When $f$ is Lipschitz, previous results only obtain constant-factor approximations. This work presents the first algorithm that provides a $(1+\varepsilon)$-approximation solution by querying $\tilde{O}(d^{\frac{p}{2}\vee 1}/\varepsilon^{p\vee 2})$ entries of $b$. This query complexity is also shown to be optimal up to logarithmic factors for $p\in [1,2]$ and the $\varepsilon$-dependence of $1/\varepsilon^p$ is shown to be optimal for $p>2$.

## 新兴技术(cs.ET:Emerging Technologies)

### Quantum Machine Learning in Precision Medicine and Drug Discovery -- A Game Changer for Tailored Treatments? 
[[arxiv](https://arxiv.org/abs/2502.18639)] [[cool](https://papers.cool/arxiv/2502.18639)] [[pdf](https://arxiv.org/pdf/2502.18639)]
> **Authors**: Markus Bertl,Alan Mott,Salvatore Sinno,Bhavika Bhalgamiya
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: presented at AISoLA 2024
- **标题**: None
- **领域**: 新兴技术,人工智能,量子物理学
- **Abstract**: The digitization of healthcare presents numerous challenges, including the complexity of biological systems, vast data generation, and the need for personalized treatment plans. Traditional computational methods often fall short, leading to delayed and sometimes ineffective diagnoses and treatments. Quantum Computing (QC) and Quantum Machine Learning (QML) offer transformative advancements with the potential to revolutionize medicine. This paper summarizes areas where QC promises unprecedented computational power, enabling faster, more accurate diagnostics, personalized treatments, and enhanced drug discovery processes. However, integrating quantum technologies into precision medicine also presents challenges, including errors in algorithms and high costs. We show that mathematically-based techniques for specifying, developing, and verifying software (formal methods) can enhance the reliability and correctness of QC. By providing a rigorous mathematical framework, formal methods help to specify, develop, and verify systems with high precision. In genomic data analysis, formal specification languages can precisely (1) define the behavior and properties of quantum algorithms designed to identify genetic markers associated with diseases. Model checking tools can systematically explore all possible states of the algorithm to (2) ensure it behaves correctly under all conditions, while theorem proving techniques provide mathematical (3) proof that the algorithm meets its specified properties, ensuring accuracy and reliability. Additionally, formal optimization techniques can (4) enhance the efficiency and performance of quantum algorithms by reducing resource usage, such as the number of qubits and gate operations. Therefore, we posit that formal methods can significantly contribute to enabling QC to realize its full potential as a game changer in precision medicine.

## 计算机科学与博弈论(cs.GT:Computer Science and Game Theory)

### Expected Variational Inequalities 
[[arxiv](https://arxiv.org/abs/2502.18605)] [[cool](https://papers.cool/arxiv/2502.18605)] [[pdf](https://arxiv.org/pdf/2502.18605)]
> **Authors**: Brian Hu Zhang,Ioannis Anagnostides,Emanuel Tewolde,Ratip Emin Berker,Gabriele Farina,Vincent Conitzer,Tuomas Sandholm
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算机科学与博弈论,机器学习,优化与控制
- **Abstract**: Variational inequalities (VIs) encompass many fundamental problems in diverse areas ranging from engineering to economics and machine learning. However, their considerable expressivity comes at the cost of computational intractability. In this paper, we introduce and analyze a natural relaxation -- which we refer to as expected variational inequalities (EVIs) -- where the goal is to find a distribution that satisfies the VI constraint in expectation. By adapting recent techniques from game theory, we show that, unlike VIs, EVIs can be solved in polynomial time under general (nonmonotone) operators. EVIs capture the seminal notion of correlated equilibria, but enjoy a greater reach beyond games. We also employ our framework to capture and generalize several existing disparate results, including from settings such as smooth games, and games with coupled constraints or nonconcave utilities.

## 人机交互(cs.HC:Human-Computer Interaction)

### Intent Tagging: Exploring Micro-Prompting Interactions for Supporting Granular Human-GenAI Co-Creation Workflows 
[[arxiv](https://arxiv.org/abs/2502.18737)] [[cool](https://papers.cool/arxiv/2502.18737)] [[pdf](https://arxiv.org/pdf/2502.18737)]
> **Authors**: Frederic Gmeiner,Nicolai Marquardt,Michael Bentley,Hugo Romat,Michel Pahud,David Brown,Asta Roseway,Nikolas Martelaro,Kenneth Holstein,Ken Hinckley,Nathalie Riche
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 31 pages, 30 figures, 3 tables. To appear in the Proceedings of the 2025 ACM CHI Conference on Human Factors in Computing Systems, Yokohama, Japan
- **标题**: None
- **领域**: 人机交互,人工智能
- **Abstract**: Despite Generative AI (GenAI) systems' potential for enhancing content creation, users often struggle to effectively integrate GenAI into their creative workflows. Core challenges include misalignment of AI-generated content with user intentions (intent elicitation and alignment), user uncertainty around how to best communicate their intents to the AI system (prompt formulation), and insufficient flexibility of AI systems to support diverse creative workflows (workflow flexibility). Motivated by these challenges, we created IntentTagger: a system for slide creation based on the notion of Intent Tags - small, atomic conceptual units that encapsulate user intent - for exploring granular and non-linear micro-prompting interactions for Human-GenAI co-creation workflows. Our user study with 12 participants provides insights into the value of flexibly expressing intent across varying levels of ambiguity, meta-intent elicitation, and the benefits and challenges of intent tag-driven workflows. We conclude by discussing the broader implications of our findings and design considerations for GenAI-supported content creation workflows.

### AI-Instruments: Embodying Prompts as Instruments to Abstract & Reflect Graphical Interface Commands as General-Purpose Tools 
[[arxiv](https://arxiv.org/abs/2502.18736)] [[cool](https://papers.cool/arxiv/2502.18736)] [[pdf](https://arxiv.org/pdf/2502.18736)]
> **Authors**: Nathalie Riche,Anna Offenwanger,Frederic Gmeiner,David Brown,Hugo Romat,Michel Pahud,Nicolai Marquardt,Kori Inkpen,Ken Hinckley
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 18 pages, 10 figures. To appear in the Proceedings of the 2025 ACM CHI Conference on Human Factors in Computing Systems, Yokohama, Japan. https://hugoromat.github.io/ai_instruments/
- **标题**: None
- **领域**: 人机交互,人工智能
- **Abstract**: Chat-based prompts respond with verbose linear-sequential texts, making it difficult to explore and refine ambiguous intents, back up and reinterpret, or shift directions in creative AI-assisted design work. AI-Instruments instead embody "prompts" as interface objects via three key principles: (1) Reification of user-intent as reusable direct-manipulation instruments; (2) Reflection of multiple interpretations of ambiguous user-intents (Reflection-in-intent) as well as the range of AI-model responses (Reflection-in-response) to inform design "moves" towards a desired result; and (3) Grounding to instantiate an instrument from an example, result, or extrapolation directly from another instrument. Further, AI-Instruments leverage LLM's to suggest, vary, and refine new instruments, enabling a system that goes beyond hard-coded functionality by generating its own instrumental controls from content. We demonstrate four technology probes, applied to image generation, and qualitative insights from twelve participants, showing how AI-Instruments address challenges of intent formulation, steering via direct manipulation, and non-linear iterative workflows to reflect and resolve ambiguous intents.

### AI Mismatches: Identifying Potential Algorithmic Harms Before AI Development 
[[arxiv](https://arxiv.org/abs/2502.18682)] [[cool](https://papers.cool/arxiv/2502.18682)] [[pdf](https://arxiv.org/pdf/2502.18682)]
> **Authors**: Devansh Saxena,Ji-Youn Jung,Jodi Forlizzi,Kenneth Holstein,John Zimmerman
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: CHI Conference on Human Factors in Computing Systems (CHI '25), April 26-May 1, 2025, Yokohama, Japan
- **标题**: None
- **领域**: 人机交互,人工智能
- **Abstract**: AI systems are often introduced with high expectations, yet many fail to deliver, resulting in unintended harm and missed opportunities for benefit. We frequently observe significant "AI Mismatches", where the system's actual performance falls short of what is needed to ensure safety and co-create value. These mismatches are particularly difficult to address once development is underway, highlighting the need for early-stage intervention. Navigating complex, multi-dimensional risk factors that contribute to AI Mismatches is a persistent challenge. To address it, we propose an AI Mismatch approach to anticipate and mitigate risks early on, focusing on the gap between realistic model performance and required task performance. Through an analysis of 774 AI cases, we extracted a set of critical factors, which informed the development of seven matrices that map the relationships between these factors and highlight high-risk areas. Through case studies, we demonstrate how our approach can help reduce risks in AI development.

### Comparing Native and Non-native English Speakers' Behaviors in Collaborative Writing through Visual Analytics 
[[arxiv](https://arxiv.org/abs/2502.18681)] [[cool](https://papers.cool/arxiv/2502.18681)] [[pdf](https://arxiv.org/pdf/2502.18681)]
> **Authors**: Yuexi Chen,Yimin Xiao,Kazi Tasnim Zinat,Naomi Yamashita,Ge Gao,Zhicheng Liu
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: accepted by CHI 2025
- **标题**: None
- **领域**: 人机交互,人工智能,计算机与社会
- **Abstract**: Understanding collaborative writing dynamics between native speakers (NS) and non-native speakers (NNS) is critical for enhancing collaboration quality and team inclusivity. In this paper, we partnered with communication researchers to develop visual analytics solutions for comparing NS and NNS behaviors in 162 writing sessions across 27 teams. The primary challenges in analyzing writing behaviors are data complexity and the uncertainties introduced by automated methods. In response, we present \textsc{COALA}, a novel visual analytics tool that improves model interpretability by displaying uncertainties in author clusters, generating behavior summaries using large language models, and visualizing writing-related actions at multiple granularities. We validated the effectiveness of \textsc{COALA} through user studies with domain experts (N=2+2) and researchers with relevant experience (N=8). We present the insights discovered by participants using \textsc{COALA}, suggest features for future AI-assisted collaborative writing tools, and discuss the broader implications for analyzing collaborative processes beyond writing.

### Scaffolding Empathy: Training Counselors with Simulated Patients and Utterance-level Performance Visualizations 
[[arxiv](https://arxiv.org/abs/2502.18673)] [[cool](https://papers.cool/arxiv/2502.18673)] [[pdf](https://arxiv.org/pdf/2502.18673)]
> **Authors**: Ian Steenstra,Farnaz Nouraei,Timothy W. Bickmore
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: This is a preprint version of the paper conditionally accepted to CHI'25
- **标题**: None
- **领域**: 人机交互,计算语言学,多代理系统
- **Abstract**: Learning therapeutic counseling involves significant role-play experience with mock patients, with current manual training methods providing only intermittent granular feedback. We seek to accelerate and optimize counselor training by providing frequent, detailed feedback to trainees as they interact with a simulated patient. Our first application domain involves training motivational interviewing skills for counselors. Motivational interviewing is a collaborative counseling style in which patients are guided to talk about changing their behavior, with empathetic counseling an essential ingredient. We developed and evaluated an LLM-powered training system that features a simulated patient and visualizations of turn-by-turn performance feedback tailored to the needs of counselors learning motivational interviewing. We conducted an evaluation study with professional and student counselors, demonstrating high usability and satisfaction with the system. We present design implications for the development of automated systems that train users in counseling skills and their generalizability to other types of social skills training.

### Assistance or Disruption? Exploring and Evaluating the Design and Trade-offs of Proactive AI Programming Support 
[[arxiv](https://arxiv.org/abs/2502.18658)] [[cool](https://papers.cool/arxiv/2502.18658)] [[pdf](https://arxiv.org/pdf/2502.18658)]
> **Authors**: Kevin Pu,Daniel Lazaro,Ian Arawjo,Haijun Xia,Ziang Xiao,Tovi Grossman,Yan Chen
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 人机交互,人工智能,软件工程
- **Abstract**: AI programming tools enable powerful code generation, and recent prototypes attempt to reduce user effort with proactive AI agents, but their impact on programming workflows remains unexplored. We introduce and evaluate Codellaborator, a design probe LLM agent that initiates programming assistance based on editor activities and task context. We explored three interface variants to assess trade-offs between increasingly salient AI support: prompt-only, proactive agent, and proactive agent with presence and context (Codellaborator). In a within-subject study (N=18), we find that proactive agents increase efficiency compared to prompt-only paradigm, but also incur workflow disruptions. However, presence indicators and \revise{interaction context support} alleviated disruptions and improved users' awareness of AI processes. We underscore trade-offs of Codellaborator on user control, ownership, and code understanding, emphasizing the need to adapt proactivity to programming processes. Our research contributes to the design exploration and evaluation of proactive AI systems, presenting design implications on AI-integrated programming workflow.

### WhatELSE: Shaping Narrative Spaces at Configurable Level of Abstraction for AI-bridged Interactive Storytelling 
[[arxiv](https://arxiv.org/abs/2502.18641)] [[cool](https://papers.cool/arxiv/2502.18641)] [[pdf](https://arxiv.org/pdf/2502.18641)]
> **Authors**: Zhuoran Lu,Qian Zhou,Yi Wang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: In Proceedings of CHI 2025
- **标题**: None
- **领域**: 人机交互,人工智能
- **Abstract**: Generative AI significantly enhances player agency in interactive narratives (IN) by enabling just-in-time content generation that adapts to player actions. While delegating generation to AI makes IN more interactive, it becomes challenging for authors to control the space of possible narratives - within which the final story experienced by the player emerges from their interaction with AI. In this paper, we present WhatELSE, an AI-bridged IN authoring system that creates narrative possibility spaces from example stories. WhatELSE provides three views (narrative pivot, outline, and variants) to help authors understand the narrative space and corresponding tools leveraging linguistic abstraction to control the boundaries of the narrative space. Taking innovative LLM-based narrative planning approaches, WhatELSE further unfolds the narrative space into executable game events. Through a user study (N=12) and technical evaluations, we found that WhatELSE enables authors to perceive and edit the narrative space and generates engaging interactive narratives at play-time.

### Which Contributions Deserve Credit? Perceptions of Attribution in Human-AI Co-Creation 
[[arxiv](https://arxiv.org/abs/2502.18357)] [[cool](https://papers.cool/arxiv/2502.18357)] [[pdf](https://arxiv.org/pdf/2502.18357)]
> **Authors**: Jessica He,Stephanie Houde,Justin D. Weisz
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 30 pages, 5 figures. In CHI Conference on Human Factors in Computing Systems (CHI '25), April 26-May 1, 2025, Yokohama, Japan
- **标题**: None
- **领域**: 人机交互,人工智能,计算机与社会
- **Abstract**: AI systems powered by large language models can act as capable assistants for writing and editing. In these tasks, the AI system acts as a co-creative partner, making novel contributions to an artifact-under-creation alongside its human partner(s). One question that arises in these scenarios is the extent to which AI should be credited for its contributions. We examined knowledge workers' views of attribution through a survey study (N=155) and found that they assigned different levels of credit across different contribution types, amounts, and initiative. Compared to a human partner, we observed a consistent pattern in which AI was assigned less credit for equivalent contributions. Participants felt that disclosing AI involvement was important and used a variety of criteria to make attribution judgments, including the quality of contributions, personal values, and technology considerations. Our results motivate and inform new approaches for crediting AI contributions to co-created work.

### FactFlow: Automatic Fact Sheet Generation and Customization from Tabular Dataset via AI Chain Design & Implementation 
[[arxiv](https://arxiv.org/abs/2502.17909)] [[cool](https://papers.cool/arxiv/2502.17909)] [[pdf](https://arxiv.org/pdf/2502.17909)]
> **Authors**: Minh Duc Vu,Jieshan Chen,Zhenchang Xing,Qinghua Lu,Xiwei Xu,Qian Fu
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 11 pages, 6 figures
- **标题**: None
- **领域**: 人机交互,人工智能
- **Abstract**: With the proliferation of data across various domains, there is a critical demand for tools that enable non-experts to derive meaningful insights without deep data analysis skills. To address this need, existing automatic fact sheet generation tools offer heuristic-based solutions to extract facts and generate stories. However, they inadequately grasp the semantics of data and struggle to generate narratives that fully capture the semantics of the dataset or align the fact sheet with specific user needs. Addressing these shortcomings, this paper introduces \tool, a novel tool designed for the automatic generation and customisation of fact sheets. \tool applies the concept of collaborative AI workers to transform raw tabular dataset into comprehensive, visually compelling fact sheets. We define effective taxonomy to profile AI worker for specialised tasks. Furthermore, \tool empowers users to refine these fact sheets through intuitive natural language commands, ensuring the final outputs align closely with individual preferences and requirements. Our user evaluation with 18 participants confirms that \tool not only surpasses state-of-the-art baselines in automated fact sheet production but also provides a positive user experience during customization tasks.

### VeriPlan: Integrating Formal Verification and LLMs into End-User Planning 
[[arxiv](https://arxiv.org/abs/2502.17898)] [[cool](https://papers.cool/arxiv/2502.17898)] [[pdf](https://arxiv.org/pdf/2502.17898)]
> **Authors**: Christine Lee,David Porfirio,Xinyu Jessica Wang,Kevin Zhao,Bilge Mutlu
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: In CHI Conference on Human Factors in Computing Systems (CHI '25), April 26-May 1, 2025, Yokohama, Japan. ACM, New York, NY, USA, 19 pages
- **标题**: None
- **领域**: 人机交互,人工智能
- **Abstract**: Automated planning is traditionally the domain of experts, utilized in fields like manufacturing and healthcare with the aid of expert planning tools. Recent advancements in LLMs have made planning more accessible to everyday users due to their potential to assist users with complex planning tasks. However, LLMs face several application challenges within end-user planning, including consistency, accuracy, and user trust issues. This paper introduces VeriPlan, a system that applies formal verification techniques, specifically model checking, to enhance the reliability and flexibility of LLMs for end-user planning. In addition to the LLM planner, VeriPlan includes three additional core features -- a rule translator, flexibility sliders, and a model checker -- that engage users in the verification process. Through a user study (n=12), we evaluate VeriPlan, demonstrating improvements in the perceived quality, usability, and user satisfaction of LLMs. Our work shows the effective integration of formal verification and user-control features with LLMs for end-user planning tasks.

## 信息检索(cs.IR:Information Retrieval)

### AgentSociety Challenge: Designing LLM Agents for User Modeling and Recommendation on Web Platforms 
[[arxiv](https://arxiv.org/abs/2502.18754)] [[cool](https://papers.cool/arxiv/2502.18754)] [[pdf](https://arxiv.org/pdf/2502.18754)]
> **Authors**: Yuwei Yan,Yu Shang,Qingbin Zeng,Yu Li,Keyu Zhao,Zhiheng Zheng,Xuefei Ning,Tianji Wu,Shengen Yan,Yu Wang,Fengli Xu,Yong Li
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 8 pages, 10 figures, in Proceedings of the ACM Web Conference 2025 (WWW '25)
- **标题**: None
- **领域**: 信息检索,人工智能
- **Abstract**: The AgentSociety Challenge is the first competition in the Web Conference that aims to explore the potential of Large Language Model (LLM) agents in modeling user behavior and enhancing recommender systems on web platforms. The Challenge consists of two tracks: the User Modeling Track and the Recommendation Track. Participants are tasked to utilize a combined dataset from Yelp, Amazon, and Goodreads, along with an interactive environment simulator, to develop innovative LLM agents. The Challenge has attracted 295 teams across the globe and received over 1,400 submissions in total over the course of 37 official competition days. The participants have achieved 21.9% and 20.3% performance improvement for Track 1 and Track 2 in the Development Phase, and 9.1% and 15.9% in the Final Phase, representing a significant accomplishment. This paper discusses the detailed designs of the Challenge, analyzes the outcomes, and highlights the most successful LLM agent designs. To support further research and development, we have open-sourced the benchmark environment at https://tsinghua-fib-lab.github.io/AgentSocietyChallenge.

### A Cooperative Multi-Agent Framework for Zero-Shot Named Entity Recognition 
[[arxiv](https://arxiv.org/abs/2502.18702)] [[cool](https://papers.cool/arxiv/2502.18702)] [[pdf](https://arxiv.org/pdf/2502.18702)]
> **Authors**: Zihan Wang,Ziqi Zhao,Yougang Lyu,Zhumin Chen,Maarten de Rijke,Zhaochun Ren
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Accepted at WWW 2025
- **标题**: None
- **领域**: 信息检索,计算语言学
- **Abstract**: Zero-shot named entity recognition (NER) aims to develop entity recognition systems from unannotated text corpora. This task presents substantial challenges due to minimal human intervention. Recent work has adapted large language models (LLMs) for zero-shot NER by crafting specialized prompt templates. It advances model self-learning abilities by incorporating self-annotated demonstrations. However, two important challenges persist: (i) Correlations between contexts surrounding entities are overlooked, leading to wrong type predictions or entity omissions. (ii) The indiscriminate use of task demonstrations, retrieved through shallow similarity-based strategies, severely misleads LLMs during inference. In this paper, we introduce the cooperative multi-agent system (CMAS), a novel framework for zero-shot NER that uses the collective intelligence of multiple agents to address the challenges outlined above. CMAS has four main agents: (i) a self-annotator, (ii) a type-related feature (TRF) extractor, (iii) a demonstration discriminator, and (iv) an overall predictor. To explicitly capture correlations between contexts surrounding entities, CMAS reformulates NER into two subtasks: recognizing named entities and identifying entity type-related features within the target sentence. To enable controllable utilization of demonstrations, a demonstration discriminator is established to incorporate the self-reflection mechanism, automatically evaluating helpfulness scores for the target sentence. Experimental results show that CMAS significantly improves zero-shot NER performance across six benchmarks, including both domain-specific and general-domain scenarios. Furthermore, CMAS demonstrates its effectiveness in few-shot settings and with various LLM backbones.

### AI Enhanced Ontology Driven NLP for Intelligent Cloud Resource Query Processing Using Knowledge Graphs 
[[arxiv](https://arxiv.org/abs/2502.18484)] [[cool](https://papers.cool/arxiv/2502.18484)] [[pdf](https://arxiv.org/pdf/2502.18484)]
> **Authors**: Krishna Chaitanya Sunkara,Krishnaiah Narukulla
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-26
> **comment**: 8 pages, 5 figures, 4 tables. This paper not published at else where yet. The experimental setup has a potential to be revised using real time resources. Authors: Krishna Chaitanya Sunkara (IEEE Senior Member, Raleigh, NC, USA, Independent Researcher), Krishnaiah Narukulla (IEEE Senior Member, San Jose, CA, USA, Independent Researcher)
- **标题**: None
- **领域**: 信息检索,人工智能
- **Abstract**: The conventional resource search in cloud infrastructure relies on keyword-based searches or GUIDs, which demand exact matches and significant user effort to locate resources. These conventional search approaches often fail to interpret the intent behind natural language queries, making resource discovery inefficient and inaccessible to users. Though there exists some form of NLP based search engines, they are limited and focused more on analyzing the NLP query itself and extracting identifiers to find the resources. But they fail to search resources based on their behavior or operations or their capabilities or relationships or features or business relevance or the dynamic changing state or the knowledge these resources have. The search criteria has been changing with the inundation of AI based services which involved discovering not just the requested resources and identifiers but seeking insights. The real intent of a search has never been to just to list the resources but with some actual context such as to understand causes of some behavior in the system, compliance checks, capacity estimations, network constraints, or troubleshooting or business insights. This paper proposes an advanced Natural Language Processing (NLP) enhanced by ontology-based semantics to enable intuitive, human-readable queries which allows users to actually discover the intent-of-search itself. By constructing an ontology of cloud resources, their interactions, and behaviors, the proposed framework enables dynamic intent extraction and relevance ranking using Latent Semantic Indexing (LSI) and AI models. It introduces an automated pipeline which integrates ontology extraction by AI powered data crawlers, building a semantic knowledge base for context aware resource discovery.

### Modeling Churn in Recommender Systems with Aggregated Preferences 
[[arxiv](https://arxiv.org/abs/2502.18483)] [[cool](https://papers.cool/arxiv/2502.18483)] [[pdf](https://arxiv.org/pdf/2502.18483)]
> **Authors**: Gur Keinan,Omer Ben-Porat
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,人工智能,机器学习
- **Abstract**: While recommender systems (RSs) traditionally rely on extensive individual user data, regulatory and technological shifts necessitate reliance on aggregated user information. This shift significantly impacts the recommendation process, requiring RSs to engage in intensive exploration to identify user preferences. However, this approach risks user churn due to potentially unsatisfactory recommendations. In this paper, we propose a model that addresses the dual challenges of leveraging aggregated user information and mitigating churn risk. Our model assumes that the RS operates with a probabilistic prior over user types and aggregated satisfaction levels for various content types. We demonstrate that optimal policies naturally transition from exploration to exploitation in finite time, develop a branch-and-bound algorithm for computing these policies, and empirically validate its effectiveness.

### QExplorer: Large Language Model Based Query Extraction for Toxic Content Exploration 
[[arxiv](https://arxiv.org/abs/2502.18480)] [[cool](https://papers.cool/arxiv/2502.18480)] [[pdf](https://arxiv.org/pdf/2502.18480)]
> **Authors**: Shaola Ren,Li Ke,Longtao Huang,Dehong Gao,Hui Xue
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,人工智能,计算语言学
- **Abstract**: Automatically extracting effective queries is challenging in information retrieval, especially in toxic content exploration, as such content is likely to be disguised. With the recent achievements in generative Large Language Model (LLM), we are able to leverage the capabilities of LLMs to extract effective queries for similar content exploration directly. This study proposes QExplorer, an approach of large language model based Query Extraction for toxic content Exploration. The QExplorer approach involves a 2-stage training process: instruction Supervised FineTuning (SFT) and preference alignment using Direct Preference Optimization (DPO), as well as the datasets construction with feedback of search system. To verify the effectiveness of QExplorer, a series of offline and online experiments are conducted on our real-world system. The offline empirical results demonstrate that the performance of our automatic query extraction outperforms that of several LLMs and humans. The online deployment shows a significant increase in the detection of toxic items.

### Beyond Self-Consistency: Loss-Balanced Perturbation-Based Regularization Improves Industrial-Scale Ads Ranking 
[[arxiv](https://arxiv.org/abs/2502.18478)] [[cool](https://papers.cool/arxiv/2502.18478)] [[pdf](https://arxiv.org/pdf/2502.18478)]
> **Authors**: Ilqar Ramazanli,Hamid Eghbalzadeh,Xiaoyi Liu,Yang Wang,Jiaxiang Fu,Kaushik Rangadurai,Sem Park,Bo Long,Xue Feng
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,人工智能,机器学习
- **Abstract**: Perturbation-based regularization techniques address many challenges in industrial-scale large models, particularly with sparse labels, and emphasize consistency and invariance for perturbation in model predictions. One of the popular regularization techniques has been various forms of self-consistency, which involve making small modifications to input data while preserving contextual information and enforcing similar predictions through auxiliary loss functions. In this work, we explore the first successful application of perturbation-based regularization algorithms in large-scale ads ranking models, and further propose a novel regularization algorithm, namely, Loss-Balanced Small Perturbation Regularization (LSPR) that can be used in potentially any deep learning model. We have successfully demonstrate that both Self-Consistency Regularization approaches (SCR) and LSPR are scalable and can improve ads delivery systems. By conducting industrial-scale experiments, and numerical analysis, we additionally show that our proposed LSPR, performs consistently better compared to SCR, across various groups and signal availability setups. Finally, we report a successful application of the proposed LSPR in a billion-scale industrial ranking system, which to the best of our knowledge, is the first of its kind, and it is specially designed to address the various scalability challenges (e.g, various surfaces, geological locations, clients and so on) as we will mention in this paper.

### Recommendations Beyond Catalogs: Diffusion Models for Personalized Generation 
[[arxiv](https://arxiv.org/abs/2502.18477)] [[cool](https://papers.cool/arxiv/2502.18477)] [[pdf](https://arxiv.org/pdf/2502.18477)]
> **Authors**: Gabriel Patron,Zhiwei Xu,Ishan Kapnadak,Felipe Maia Polo
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,人工智能,机器学习
- **Abstract**: Modern recommender systems follow the guiding principle of serving the right user, the right item at the right time. One of their main limitations is that they are typically limited to items already in the catalog. We propose REcommendations BEyond CAtalogs, REBECA, a new class of probabilistic diffusion-based recommender systems that synthesize new items tailored to individual tastes rather than retrieve items from the catalog. REBECA combines efficient training in embedding space with a novel diffusion prior that only requires users' past ratings of items. We evaluate REBECA on real-world data and propose novel personalization metrics for generative recommender systems. Extensive experiments demonstrate that REBECA produces high-quality, personalized recommendations, generating images that align with users' unique preferences.

### FinBloom: Knowledge Grounding Large Language Model with Real-time Financial Data 
[[arxiv](https://arxiv.org/abs/2502.18471)] [[cool](https://papers.cool/arxiv/2502.18471)] [[pdf](https://arxiv.org/pdf/2502.18471)]
> **Authors**: Ankur Sinha,Chaitanya Agarwal,Pekka Malo
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-26
> **comment**: 27 pages, 9 tables
- **标题**: None
- **领域**: 信息检索,人工智能,计算语言学,机器学习,统计金融
- **Abstract**: Large language models (LLMs) excel at generating human-like responses but often struggle with interactive tasks that require access to real-time information. This limitation poses challenges in finance, where models must access up-to-date information, such as recent news or price movements, to support decision-making. To address this, we introduce Financial Agent, a knowledge-grounding approach for LLMs to handle financial queries using real-time text and tabular data. Our contributions are threefold: First, we develop a Financial Context Dataset of over 50,000 financial queries paired with the required context. Second, we train FinBloom 7B, a custom 7 billion parameter LLM, on 14 million financial news articles from Reuters and Deutsche Presse-Agentur, alongside 12 million Securities and Exchange Commission (SEC) filings. Third, we fine-tune FinBloom 7B using the Financial Context Dataset to serve as a Financial Agent. This agent generates relevant financial context, enabling efficient real-time data retrieval to answer user queries. By reducing latency and eliminating the need for users to manually provide accurate data, our approach significantly enhances the capability of LLMs to handle dynamic financial tasks. Our proposed approach makes real-time financial decisions, algorithmic trading and other related tasks streamlined, and is valuable in contexts with high-velocity data flows.

### Spatial-RAG: Spatial Retrieval Augmented Generation for Real-World Spatial Reasoning Questions 
[[arxiv](https://arxiv.org/abs/2502.18470)] [[cool](https://papers.cool/arxiv/2502.18470)] [[pdf](https://arxiv.org/pdf/2502.18470)]
> **Authors**: Dazhou Yu,Riyang Bao,Gengchen Mai,Liang Zhao
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,新兴技术,机器学习
- **Abstract**: Spatial reasoning remains a challenge for Large Language Models (LLMs), which struggle with spatial data retrieval and reasoning. We propose Spatial Retrieval-Augmented Generation (Spatial-RAG), a framework that extends RAG to spatial tasks by integrating sparse spatial retrieval (spatial databases) and dense semantic retrieval (LLM-based similarity). A multi-objective ranking strategy balances spatial constraints and semantic relevance, while an LLM-guided generator ensures coherent responses. Experiments on a real-world tourism dataset show that Spatial-RAG significantly improves spatial question answering, bridging the gap between LLMs and spatial intelligence.

### Rank1: Test-Time Compute for Reranking in Information Retrieval 
[[arxiv](https://arxiv.org/abs/2502.18418)] [[cool](https://papers.cool/arxiv/2502.18418)] [[pdf](https://arxiv.org/pdf/2502.18418)]
> **Authors**: Orion Weller,Kathryn Ricci,Eugene Yang,Andrew Yates,Dawn Lawrie,Benjamin Van Durme
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,计算语言学,机器学习
- **Abstract**: We introduce Rank1, the first reranking model trained to take advantage of test-time compute. Rank1 demonstrates the applicability within retrieval of using a reasoning language model (i.e. OpenAI's o1, Deepseek's R1, etc.) for distillation in order to rapidly improve the performance of a smaller model. We gather and open-source a dataset of more than 600,000 examples of R1 reasoning traces from queries and passages in MS MARCO. Models trained on this dataset show: (1) state-of-the-art performance on advanced reasoning and instruction following datasets; (2) work remarkably well out of distribution due to the ability to respond to user-input prompts; and (3) have explainable reasoning chains that can be given to users or RAG-based systems. Further, we demonstrate that quantized versions of these models retain strong performance while using less compute/memory. Overall, Rank1 shows that test-time compute allows for a fundamentally new type of explainable and performant reranker model for search.

### Neural Network Graph Similarity Computation Based on Graph Fusion 
[[arxiv](https://arxiv.org/abs/2502.18291)] [[cool](https://papers.cool/arxiv/2502.18291)] [[pdf](https://arxiv.org/pdf/2502.18291)]
> **Authors**: Zenghui Chang,Yiqiao Zhang,Hong Cai Chen
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 9 pages, 4 figures, 4 tables
- **标题**: None
- **领域**: 信息检索,机器学习
- **Abstract**: Graph similarity learning, crucial for tasks such as graph classification and similarity search, focuses on measuring the similarity between two graph-structured entities. The core challenge in this field is effectively managing the interactions between graphs. Traditional methods often entail separate, redundant computations for each graph pair, leading to unnecessary complexity. This paper revolutionizes the approach by introducing a parallel graph interaction method called graph fusion. By merging the node sequences of graph pairs into a single large graph, our method leverages a global attention mechanism to facilitate interaction computations and to harvest cross-graph insights. We further assess the similarity between graph pairs at two distinct levels-graph-level and node-level-introducing two innovative, yet straightforward, similarity computation algorithms. Extensive testing across five public datasets shows that our model not only outperforms leading baseline models in graph-to-graph classification and regression tasks but also sets a new benchmark for performance and efficiency. The code for this paper is open-source and available at https://github.com/LLiRarry/GFM-code.git

### HyperG: Hypergraph-Enhanced LLMs for Structured Knowledge 
[[arxiv](https://arxiv.org/abs/2502.18125)] [[cool](https://papers.cool/arxiv/2502.18125)] [[pdf](https://arxiv.org/pdf/2502.18125)]
> **Authors**: Sirui Huang,Hanqian Li,Yanggan Gu,Xuming Hu,Qing Li,Guandong Xu
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,计算语言学
- **Abstract**: Given that substantial amounts of domain-specific knowledge are stored in structured formats, such as web data organized through HTML, Large Language Models (LLMs) are expected to fully comprehend this structured information to broaden their applications in various real-world downstream tasks. Current approaches for applying LLMs to structured data fall into two main categories: serialization-based and operation-based methods. Both approaches, whether relying on serialization or using SQL-like operations as an intermediary, encounter difficulties in fully capturing structural relationships and effectively handling sparse data. To address these unique characteristics of structured data, we propose HyperG, a hypergraph-based generation framework aimed at enhancing LLMs' ability to process structured knowledge. Specifically, HyperG first augment sparse data with contextual information, leveraging the generative power of LLMs, and incorporate a prompt-attentive hypergraph learning (PHL) network to encode both the augmented information and the intricate structural relationships within the data. To validate the effectiveness and generalization of HyperG, we conduct extensive experiments across two different downstream tasks requiring structured knowledge.

## 机器学习(cs.LG:Machine Learning)

### CAMEx: Curvature-aware Merging of Experts 
[[arxiv](https://arxiv.org/abs/2502.18821)] [[cool](https://papers.cool/arxiv/2502.18821)] [[pdf](https://arxiv.org/pdf/2502.18821)]
> **Authors**: Dung V. Nguyen,Minh H. Nguyen,Luc Q. Nguyen,Rachel S. Y. Teo,Tan M. Nguyen,Linh Duy Tran
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: ICLR 2025 Poster
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Existing methods for merging experts during model training and fine-tuning predominantly rely on Euclidean geometry, which assumes a flat parameter space. This assumption can limit the model's generalization ability, especially during the pre-training phase, where the parameter manifold might exhibit more complex curvature. Curvature-aware merging methods typically require additional information and computational resources to approximate the Fisher Information Matrix, adding memory overhead. In this paper, we introduce CAMEx (\textbf{C}urvature-\textbf{A}ware \textbf{M}erging of \textbf{Ex}perts), a novel expert merging protocol that incorporates natural gradients to account for the non-Euclidean curvature of the parameter manifold. By leveraging natural gradients, CAMEx adapts more effectively to the structure of the parameter space, improving alignment between model updates and the manifold's geometry. This approach enhances both pre-training and fine-tuning, resulting in better optimization trajectories and improved generalization without the substantial memory overhead typically associated with curvature-aware methods. Our contributions are threefold: (1) CAMEx significantly outperforms traditional Euclidean-based expert merging techniques across various natural language processing tasks, leading to enhanced performance during pre-training and fine-tuning; (2) we introduce a dynamic merging architecture that optimizes resource utilization, achieving high performance while reducing computational costs, facilitating efficient scaling of large language models; and (3) we provide both theoretical and empirical evidence to demonstrate the efficiency of our proposed method.

### Optimal Stochastic Trace Estimation in Generative Modeling 
[[arxiv](https://arxiv.org/abs/2502.18808)] [[cool](https://papers.cool/arxiv/2502.18808)] [[pdf](https://arxiv.org/pdf/2502.18808)]
> **Authors**: Xinyang Liu,Hengrong Du,Wei Deng,Ruqi Zhang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Accepted by AISTATS 2025
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Hutchinson estimators are widely employed in training divergence-based likelihoods for diffusion models to ensure optimal transport (OT) properties. However, this estimator often suffers from high variance and scalability concerns. To address these challenges, we investigate Hutch++, an optimal stochastic trace estimator for generative models, designed to minimize training variance while maintaining transport optimality. Hutch++ is particularly effective for handling ill-conditioned matrices with large condition numbers, which commonly arise when high-dimensional data exhibits a low-dimensional structure. To mitigate the need for frequent and costly QR decompositions, we propose practical schemes that balance frequency and accuracy, backed by theoretical guarantees. Our analysis demonstrates that Hutch++ leads to generations of higher quality. Furthermore, this method exhibits effective variance reduction in various applications, including simulations, conditional time series forecasts, and image generation.

### BatteryLife: A Comprehensive Dataset and Benchmark for Battery Life Prediction 
[[arxiv](https://arxiv.org/abs/2502.18807)] [[cool](https://papers.cool/arxiv/2502.18807)] [[pdf](https://arxiv.org/pdf/2502.18807)]
> **Authors**: Ruifeng Tan,Weixiang Hong,Jiayue Tang,Xibin Lu,Ruijun Ma,Xiang Zheng,Jia Li,Jiaqiang Huang,Tong-Yi Zhang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Under review
- **标题**: None
- **领域**: 机器学习,人工智能,数字图书馆
- **Abstract**: Battery Life Prediction (BLP), which relies on time series data produced by battery degradation tests, is crucial for battery utilization, optimization, and production. Despite impressive advancements, this research area faces three key challenges. Firstly, the limited size of existing datasets impedes insights into modern battery life data. Secondly, most datasets are restricted to small-capacity lithium-ion batteries tested under a narrow range of diversity in labs, raising concerns about the generalizability of findings. Thirdly, inconsistent and limited benchmarks across studies obscure the effectiveness of baselines and leave it unclear if models popular in other time series fields are effective for BLP. To address these challenges, we propose BatteryLife, a comprehensive dataset and benchmark for BLP. BatteryLife integrates 16 datasets, offering a 2.4 times sample size compared to the previous largest dataset, and provides the most diverse battery life resource with batteries from 8 formats, 80 chemical systems, 12 operating temperatures, and 646 charge/discharge protocols, including both laboratory and industrial tests. Notably, BatteryLife is the first to release battery life datasets of zinc-ion batteries, sodium-ion batteries, and industry-tested large-capacity lithium-ion batteries. With the comprehensive dataset, we revisit the effectiveness of baselines popular in this and other time series fields. Furthermore, we propose CyclePatch, a plug-in technique that can be employed in a series of neural networks. Extensive benchmarking of 18 methods reveals that models popular in other time series fields can be unsuitable for BLP, and CyclePatch consistently improves model performance establishing state-of-the-art benchmarks. Moreover, BatteryLife evaluates model performance across aging conditions and domains. BatteryLife is available at https://github.com/Ruifeng-Tan/BatteryLife.

### M2-omni: Advancing Omni-MLLM for Comprehensive Modality Support with Competitive Performance 
[[arxiv](https://arxiv.org/abs/2502.18778)] [[cool](https://papers.cool/arxiv/2502.18778)] [[pdf](https://arxiv.org/pdf/2502.18778)]
> **Authors**: Qingpei Guo,Kaiyou Song,Zipeng Feng,Ziping Ma,Qinglong Zhang,Sirui Gao,Xuzheng Yu,Yunxiao Sun,Tai-WeiChang,Jingdong Chen,Ming Yang,Jun Zhou
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: We present M2-omni, a cutting-edge, open-source omni-MLLM that achieves competitive performance to GPT-4o. M2-omni employs a unified multimodal sequence modeling framework, which empowers Large Language Models(LLMs) to acquire comprehensive cross-modal understanding and generation capabilities. Specifically, M2-omni can process arbitrary combinations of audio, video, image, and text modalities as input, generating multimodal sequences interleaving with audio, image, or text outputs, thereby enabling an advanced and interactive real-time experience. The training of such an omni-MLLM is challenged by significant disparities in data quantity and convergence rates across modalities. To address these challenges, we propose a step balance strategy during pre-training to handle the quantity disparities in modality-specific data. Additionally, a dynamically adaptive balance strategy is introduced during the instruction tuning stage to synchronize the modality-wise training progress, ensuring optimal convergence. Notably, we prioritize preserving strong performance on pure text tasks to maintain the robustness of M2-omni's language understanding capability throughout the training process. To our best knowledge, M2-omni is currently a very competitive open-source model to GPT-4o, characterized by its comprehensive modality and task support, as well as its exceptional performance. We expect M2-omni will advance the development of omni-MLLMs, thus facilitating future research in this domain.

### Research on Edge Computing and Cloud Collaborative Resource Scheduling Optimization Based on Deep Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.18773)] [[cool](https://papers.cool/arxiv/2502.18773)] [[pdf](https://arxiv.org/pdf/2502.18773)]
> **Authors**: Yuqing Wang,Xiao Yang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,分布式、并行和集群计算
- **Abstract**: This study addresses the challenge of resource scheduling optimization in edge-cloud collaborative computing using deep reinforcement learning (DRL). The proposed DRL-based approach improves task processing efficiency, reduces overall processing time, enhances resource utilization, and effectively controls task migrations. Experimental results demonstrate the superiority of DRL over traditional scheduling algorithms, particularly in managing complex task allocation, dynamic workloads, and multiple resource constraints. Despite its advantages, further improvements are needed to enhance learning efficiency, reduce training time, and address convergence issues. Future research should focus on increasing the algorithm's fault tolerance to handle more complex and uncertain scheduling scenarios, thereby advancing the intelligence and efficiency of edge-cloud computing systems.

### Exploring Graph Tasks with Pure LLMs: A Comprehensive Benchmark and Investigation 
[[arxiv](https://arxiv.org/abs/2502.18771)] [[cool](https://papers.cool/arxiv/2502.18771)] [[pdf](https://arxiv.org/pdf/2502.18771)]
> **Authors**: Yuxiang Wang,Xinnan Dai,Wenqi Fan,Yao Ma
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,社交和信息网络
- **Abstract**: Graph-structured data has become increasingly prevalent across various domains, raising the demand for effective models to handle graph tasks like node classification and link prediction. Traditional graph learning models like Graph Neural Networks (GNNs) have made significant strides, but their capabilities in handling graph data remain limited in certain contexts. In recent years, large language models (LLMs) have emerged as promising candidates for graph tasks, yet most studies focus primarily on performance benchmarks and fail to address their broader potential, including their ability to handle limited data, their transferability across tasks, and their robustness. In this work, we provide a comprehensive exploration of LLMs applied to graph tasks. We evaluate the performance of pure LLMs, including those without parameter optimization and those fine-tuned with instructions, across various scenarios. Our analysis goes beyond accuracy, assessing LLM ability to perform in few-shot/zero-shot settings, transfer across domains, understand graph structures, and demonstrate robustness in challenging scenarios. We conduct extensive experiments with 16 graph learning models alongside 6 LLMs (e.g., Llama3B, GPT-4o, Qwen-plus), comparing their performance on datasets like Cora, PubMed, ArXiv, and Products. Our findings show that LLMs, particularly those with instruction tuning, outperform traditional models in few-shot settings, exhibit strong domain transferability, and demonstrate excellent generalization and robustness. This work offers valuable insights into the capabilities of LLMs for graph learning, highlighting their advantages and potential for real-world applications, and paving the way for future research in this area. Codes and datasets are released in https://github.com/myflashbarry/LLM-benchmarking.

### Reward Shaping to Mitigate Reward Hacking in RLHF 
[[arxiv](https://arxiv.org/abs/2502.18770)] [[cool](https://papers.cool/arxiv/2502.18770)] [[pdf](https://arxiv.org/pdf/2502.18770)]
> **Authors**: Jiayi Fu,Xuandong Zhao,Chengyuan Yao,Heng Wang,Qi Han,Yanghua Xiao
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 19 pages
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Reinforcement Learning from Human Feedback (RLHF) is essential for aligning large language models (LLMs) with human values. However, RLHF is susceptible to reward hacking, where the agent exploits flaws in the reward function rather than learning the intended behavior, thus degrading alignment. While reward shaping helps stabilize RLHF and partially mitigate reward hacking, a systematic investigation into shaping techniques and their underlying principles remains lacking. To bridge this gap, we present a comprehensive study of the prevalent reward shaping methods. Our analysis suggests three key design principles: (1) RL reward is ideally bounded, (2) RL benefits from rapid initial growth followed by gradual convergence, and (3) RL reward is best formulated as a function of centered reward. Guided by these insights, we propose Preference As Reward (PAR), a novel approach that leverages the latent preferences embedded within the reward model itself as the signal for reinforcement learning. We evaluated PAR on two base models, Gemma2-2B and Llama3-8B, using two datasets, Ultrafeedback-Binarized and HH-RLHF. Experimental results demonstrate PAR's superior performance over other reward shaping methods. On the AlpacaEval 2.0 benchmark, PAR achieves a win rate at least 5 percentage points higher than competing approaches. Furthermore, PAR exhibits remarkable data efficiency, requiring only a single reference reward for optimal performance, and maintains robustness against reward hacking even after two full epochs of training. Code is available at https://github.com/PorUna-byte/PAR.

### Online Prototypes and Class-Wise Hypergradients for Online Continual Learning with Pre-Trained Models 
[[arxiv](https://arxiv.org/abs/2502.18762)] [[cool](https://papers.cool/arxiv/2502.18762)] [[pdf](https://arxiv.org/pdf/2502.18762)]
> **Authors**: Nicolas Michel,Maorong Wang,Jiangpeng He,Toshihiko Yamasaki
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Under review
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Continual Learning (CL) addresses the problem of learning from a data sequence where the distribution changes over time. Recently, efficient solutions leveraging Pre-Trained Models (PTM) have been widely explored in the offline CL (offCL) scenario, where the data corresponding to each incremental task is known beforehand and can be seen multiple times. However, such solutions often rely on 1) prior knowledge regarding task changes and 2) hyper-parameter search, particularly regarding the learning rate. Both assumptions remain unavailable in online CL (onCL) scenarios, where incoming data distribution is unknown and the model can observe each datum only once. Therefore, existing offCL strategies fall largely behind performance-wise in onCL, with some proving difficult or impossible to adapt to the online scenario. In this paper, we tackle both problems by leveraging Online Prototypes (OP) and Class-Wise Hypergradients (CWH). OP leverages stable output representations of PTM by updating its value on the fly to act as replay samples without requiring task boundaries or storing past data. CWH learns class-dependent gradient coefficients during training to improve over sub-optimal learning rates. We show through experiments that both introduced strategies allow for a consistent gain in accuracy when integrated with existing approaches. We will make the code fully available upon acceptance.

### Cross-Modality Investigation on WESAD Stress Classification 
[[arxiv](https://arxiv.org/abs/2502.18733)] [[cool](https://papers.cool/arxiv/2502.18733)] [[pdf](https://arxiv.org/pdf/2502.18733)]
> **Authors**: Eric Oliver,Sagnik Dakshit
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Deep learning's growing prevalence has driven its widespread use in healthcare, where AI and sensor advancements enhance diagnosis, treatment, and monitoring. In mobile health, AI-powered tools enable early diagnosis and continuous monitoring of conditions like stress. Wearable technologies and multimodal physiological data have made stress detection increasingly viable, but model efficacy depends on data quality, quantity, and modality. This study develops transformer models for stress detection using the WESAD dataset, training on electrocardiograms (ECG), electrodermal activity (EDA), electromyography (EMG), respiration rate (RESP), temperature (TEMP), and 3-axis accelerometer (ACC) signals. The results demonstrate the effectiveness of single-modality transformers in analyzing physiological signals, achieving state-of-the-art performance with accuracy, precision and recall values in the range of $99.73\%$ to $99.95\%$ for stress detection. Furthermore, this study explores cross-modal performance and also explains the same using 2D visualization of the learned embedding space and quantitative analysis based on data variance. Despite the large body of work on stress detection and monitoring, the robustness and generalization of these models across different modalities has not been explored. This research represents one of the initial efforts to interpret embedding spaces for stress detection, providing valuable information on cross-modal performance.

### Bandit and Delayed Feedback in Online Structured Prediction 
[[arxiv](https://arxiv.org/abs/2502.18709)] [[cool](https://papers.cool/arxiv/2502.18709)] [[pdf](https://arxiv.org/pdf/2502.18709)]
> **Authors**: Yuki Shibukawa,Taira Tsuchiya,Shinsaku Sakaue,Kenji Yamanishi
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 33 pages
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Online structured prediction is a task of sequentially predicting outputs with complex structures based on inputs and past observations, encompassing online classification. Recent studies showed that in the full information setup, we can achieve finite bounds on the surrogate regret, i.e., the extra target loss relative to the best possible surrogate loss. In practice, however, full information feedback is often unrealistic as it requires immediate access to the whole structure of complex outputs. Motivated by this, we propose algorithms that work with less demanding feedback, bandit and delayed feedback. For the bandit setting, using a standard inverse-weighted gradient estimator, we achieve a surrogate regret bound of $O(\sqrt{KT})$ for the time horizon $T$ and the size of the output set $K$. However, $K$ can be extremely large when outputs are highly complex, making this result less desirable. To address this, we propose an algorithm that achieves a surrogate regret bound of $O(T^{2/3})$, which is independent of $K$. This is enabled with a carefully designed pseudo-inverse matrix estimator. Furthermore, for the delayed full information feedback setup, we obtain a surrogate regret bound of $O(D^{2/3} T^{1/3})$ for the delay time $D$. We also provide algorithms for the delayed bandit feedback setup. Finally, we numerically evaluate the performance of the proposed algorithms in online classification with bandit feedback.

### Differentially Private Federated Learning With Time-Adaptive Privacy Spending 
[[arxiv](https://arxiv.org/abs/2502.18706)] [[cool](https://papers.cool/arxiv/2502.18706)] [[pdf](https://arxiv.org/pdf/2502.18706)]
> **Authors**: Shahrzad Kiani,Nupur Kulkarni,Adam Dziedzic,Stark Draper,Franziska Boenisch
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: International Conference onLearningRepresentations (ICLR), April 2025, Singapore
- **标题**: None
- **领域**: 机器学习,密码学和安全,分布式、并行和集群计算
- **Abstract**: Federated learning (FL) with differential privacy (DP) provides a framework for collaborative machine learning, enabling clients to train a shared model while adhering to strict privacy constraints. The framework allows each client to have an individual privacy guarantee, e.g., by adding different amounts of noise to each client's model updates. One underlying assumption is that all clients spend their privacy budgets uniformly over time (learning rounds). However, it has been shown in the literature that learning in early rounds typically focuses on more coarse-grained features that can be learned at lower signal-to-noise ratios while later rounds learn fine-grained features that benefit from higher signal-to-noise ratios. Building on this intuition, we propose a time-adaptive DP-FL framework that expends the privacy budget non-uniformly across both time and clients. Our framework enables each client to save privacy budget in early rounds so as to be able to spend more in later rounds when additional accuracy is beneficial in learning more fine-grained features. We theoretically prove utility improvements in the case that clients with stricter privacy budgets spend budgets unevenly across rounds, compared to clients with more relaxed budgets, who have sufficient budgets to distribute their spend more evenly. Our practical experiments on standard benchmark datasets support our theoretical results and show that, in practice, our algorithms improve the privacy-utility trade-offs compared to baseline schemes.

### Tukey Depth Mechanisms for Practical Private Mean Estimation 
[[arxiv](https://arxiv.org/abs/2502.18698)] [[cool](https://papers.cool/arxiv/2502.18698)] [[pdf](https://arxiv.org/pdf/2502.18698)]
> **Authors**: Gavin Brown,Lydia Zakynthinou
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 17 pages, 10 figures
- **标题**: None
- **领域**: 机器学习,方法论
- **Abstract**: Mean estimation is a fundamental task in statistics and a focus within differentially private statistical estimation. While univariate methods based on the Gaussian mechanism are widely used in practice, more advanced techniques such as the exponential mechanism over quantiles offer robustness and improved performance, especially for small sample sizes. Tukey depth mechanisms carry these advantages to multivariate data, providing similar strong theoretical guarantees. However, practical implementations fall behind these theoretical developments. In this work, we take the first step to bridge this gap by implementing the (Restricted) Tukey Depth Mechanism, a theoretically optimal mean estimator for multivariate Gaussian distributions, yielding improved practical methods for private mean estimation. Our implementations enable the use of these mechanisms for small sample sizes or low-dimensional data. Additionally, we implement variants of these mechanisms that use approximate versions of Tukey depth, trading off accuracy for faster computation. We demonstrate their efficiency in practice, showing that they are viable options for modest dimensions. Given their strong accuracy and robustness guarantees, we contend that they are competitive approaches for mean estimation in this regime. We explore future directions for improving the computational efficiency of these algorithms by leveraging fast polytope volume approximation techniques, paving the way for more accurate private mean estimation in higher dimensions.

### H-FLTN: A Privacy-Preserving Hierarchical Framework for Electric Vehicle Spatio-Temporal Charge Prediction 
[[arxiv](https://arxiv.org/abs/2502.18697)] [[cool](https://papers.cool/arxiv/2502.18697)] [[pdf](https://arxiv.org/pdf/2502.18697)]
> **Authors**: Robert Marlin,Raja Jurdak,Alsharif Abuadbba
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 14 pages, 7 tables, 2 figures, Journal Paper
- **标题**: None
- **领域**: 机器学习,人工智能,密码学和安全
- **Abstract**: The widespread adoption of Electric Vehicles (EVs) poses critical challenges for energy providers, particularly in predicting charging time (temporal prediction), ensuring user privacy, and managing resources efficiently in mobility-driven networks. This paper introduces the Hierarchical Federated Learning Transformer Network (H-FLTN) framework to address these challenges. H-FLTN employs a three-tier hierarchical architecture comprising EVs, community Distributed Energy Resource Management Systems (DERMS), and the Energy Provider Data Centre (EPDC) to enable accurate spatio-temporal predictions of EV charging needs while preserving privacy. Temporal prediction is enhanced using Transformer-based learning, capturing complex dependencies in charging behavior. Privacy is ensured through Secure Aggregation, Additive Secret Sharing, and Peer-to-Peer (P2P) Sharing with Augmentation, which allow only secret shares of model weights to be exchanged while securing all transmissions. To improve training efficiency and resource management, H-FLTN integrates Dynamic Client Capping Mechanism (DCCM) and Client Rotation Management (CRM), ensuring that training remains both computationally and temporally efficient as the number of participating EVs increases. DCCM optimises client participation by limiting excessive computational loads, while CRM balances training contributions across epochs, preventing imbalanced participation. Our simulation results based on large-scale empirical vehicle mobility data reveal that DCCM and CRM reduce the training time complexity with increasing EVs from linear to constant. Its integration into real-world smart city infrastructure enhances energy demand forecasting, resource allocation, and grid stability, ensuring reliability and sustainability in future mobility ecosystems.

### CayleyPy RL: Pathfinding and Reinforcement Learning on Cayley Graphs 
[[arxiv](https://arxiv.org/abs/2502.18663)] [[cool](https://papers.cool/arxiv/2502.18663)] [[pdf](https://arxiv.org/pdf/2502.18663)]
> **Authors**: A. Chervov,A. Soibelman,S. Lytkin,I. Kiselev,S. Fironov,A. Lukyanenko,A. Dolgorukova,A. Ogurtsov,F. Petrov,S. Krymskii,M. Evseev,L. Grunvald,D. Gorodkov,G. Antiufeev,G. Verbii,V. Zamkovoy,L. Cheldieva,I. Koltsov,A. Sychev,M. Obozov,A. Eliseev,S. Nikolenko,N. Narynbaev,R. Turtayev,N. Rokotyan, et al. (9 additional authors not shown)
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 28 pages
- **标题**: None
- **领域**: 机器学习,离散数学,社交和信息网络,组合学,群论
- **Abstract**: This paper is the second in a series of studies on developing efficient artificial intelligence-based approaches to pathfinding on extremely large graphs (e.g. $10^{70}$ nodes) with a focus on Cayley graphs and mathematical applications. The open-source CayleyPy project is a central component of our research. The present paper proposes a novel combination of a reinforcement learning approach with a more direct diffusion distance approach from the first paper. Our analysis includes benchmarking various choices for the key building blocks of the approach: architectures of the neural network, generators for the random walks and beam search pathfinding. We compared these methods against the classical computer algebra system GAP, demonstrating that they "overcome the GAP" for the considered examples. As a particular mathematical application we examine the Cayley graph of the symmetric group with cyclic shift and transposition generators. We provide strong support for the OEIS-A186783 conjecture that the diameter is equal to n(n-1)/2 by machine learning and mathematical methods. We identify the conjectured longest element and generate its decomposition of the desired length. We prove a diameter lower bound of n(n-1)/2-n/2 and an upper bound of n(n-1)/2+ 3n by presenting the algorithm with given complexity. We also present several conjectures motivated by numerical experiments, including observations on the central limit phenomenon (with growth approximated by a Gumbel distribution), the uniform distribution for the spectrum of the graph, and a numerical study of sorting networks. To stimulate crowdsourcing activity, we create challenges on the Kaggle platform and invite contributions to improve and benchmark approaches on Cayley graph pathfinding and other tasks.

### Provably Efficient RL for Linear MDPs under Instantaneous Safety Constraints in Non-Convex Feature Spaces 
[[arxiv](https://arxiv.org/abs/2502.18655)] [[cool](https://papers.cool/arxiv/2502.18655)] [[pdf](https://arxiv.org/pdf/2502.18655)]
> **Authors**: Amirhossein Roknilamouki,Arnob Ghosh,Ming Shi,Fatemeh Nourzad,Eylem Ekici,Ness B. Shroff
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: In Reinforcement Learning (RL), tasks with instantaneous hard constraints present significant challenges, particularly when the decision space is non-convex or non-star-convex. This issue is especially relevant in domains like autonomous vehicles and robotics, where constraints such as collision avoidance often take a non-convex form. In this paper, we establish a regret bound of $\tilde{\mathcal{O}}\bigl(\bigl(1 + \tfrac{1}τ\bigr) \sqrt{\log(\tfrac{1}τ) d^3 H^4 K} \bigr)$, applicable to both star-convex and non-star-convex cases, where $d$ is the feature dimension, $H$ the episode length, $K$ the number of episodes, and $τ$ the safety threshold. Moreover, the violation of safety constraints is zero with high probability throughout the learning process. A key technical challenge in these settings is bounding the covering number of the value-function class, which is essential for achieving value-aware uniform concentration in model-free function approximation. For the star-convex setting, we develop a novel technique called Objective Constraint-Decomposition (OCD) to properly bound the covering number. This result also resolves an error in a previous work on constrained RL. In non-star-convex scenarios, where the covering number can become infinitely large, we propose a two-phase algorithm, Non-Convex Safe Least Squares Value Iteration (NCS-LSVI), which first reduces uncertainty about the safe set by playing a known safe policy. After that, it carefully balances exploration and exploitation to achieve the regret bound. Finally, numerical simulations on an autonomous driving scenario demonstrate the effectiveness of NCS-LSVI.

### Faster, Cheaper, Better: Multi-Objective Hyperparameter Optimization for LLM and RAG Systems 
[[arxiv](https://arxiv.org/abs/2502.18635)] [[cool](https://papers.cool/arxiv/2502.18635)] [[pdf](https://arxiv.org/pdf/2502.18635)]
> **Authors**: Matthew Barker,Andrew Bell,Evan Thomas,James Carr,Thomas Andrews,Umang Bhatt
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: :68T20; 68Q32; 90C29; 62P30ACM Class:I.2.6; I.2.7; G.1.6; G.3
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: While Retrieval Augmented Generation (RAG) has emerged as a popular technique for improving Large Language Model (LLM) systems, it introduces a large number of choices, parameters and hyperparameters that must be made or tuned. This includes the LLM, embedding, and ranker models themselves, as well as hyperparameters governing individual RAG components. Yet, collectively optimizing the entire configuration in a RAG or LLM system remains under-explored - especially in multi-objective settings - due to intractably large solution spaces, noisy objective evaluations, and the high cost of evaluations. In this work, we introduce the first approach for multi-objective parameter optimization of cost, latency, safety and alignment over entire LLM and RAG systems. We find that Bayesian optimization methods significantly outperform baseline approaches, obtaining a superior Pareto front on two new RAG benchmark tasks. We conclude our work with important considerations for practitioners who are designing multi-objective RAG systems, highlighting nuances such as how optimal configurations may not generalize across tasks and objectives.

### On the Privacy-Preserving Properties of Spiking Neural Networks with Unique Surrogate Gradients and Quantization Levels 
[[arxiv](https://arxiv.org/abs/2502.18623)] [[cool](https://papers.cool/arxiv/2502.18623)] [[pdf](https://arxiv.org/pdf/2502.18623)]
> **Authors**: Ayana Moshruba,Shay Snyder,Hamed Poursiami,Maryam Parsa
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,神经和进化计算
- **Abstract**: As machine learning models increasingly process sensitive data, understanding their vulnerability to privacy attacks is vital. Membership inference attacks (MIAs) exploit model responses to infer whether specific data points were used during training, posing a significant privacy risk. Prior research suggests that spiking neural networks (SNNs), which rely on event-driven computation and discrete spike-based encoding, exhibit greater resilience to MIAs than artificial neural networks (ANNs). This resilience stems from their non-differentiable activations and inherent stochasticity, which obscure the correlation between model responses and individual training samples. To enhance privacy in SNNs, we explore two techniques: quantization and surrogate gradients. Quantization, which reduces precision to limit information leakage, has improved privacy in ANNs. Given SNNs' sparse and irregular activations, quantization may further disrupt the activation patterns exploited by MIAs. We assess the vulnerability of SNNs and ANNs under weight and activation quantization across multiple datasets, using the attack model's receiver operating characteristic (ROC) curve area under the curve (AUC) metric, where lower values indicate stronger privacy, and evaluate the privacy-accuracy trade-off. Our findings show that quantization enhances privacy in both architectures with minimal performance loss, though full-precision SNNs remain more resilient than quantized ANNs. Additionally, we examine the impact of surrogate gradients on privacy in SNNs. Among five evaluated gradients, spike rate escape provides the best privacy-accuracy trade-off, while arctangent increases vulnerability to MIAs. These results reinforce SNNs' inherent privacy advantages and demonstrate that quantization and surrogate gradient selection significantly influence privacy-accuracy trade-offs in SNNs.

### Tighten The Lasso: A Convex Hull Volume-based Anomaly Detection Method 
[[arxiv](https://arxiv.org/abs/2502.18601)] [[cool](https://papers.cool/arxiv/2502.18601)] [[pdf](https://arxiv.org/pdf/2502.18601)]
> **Authors**: Uri Itai,Asael Bar Ilan,Teddy Lazebnik
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The rapid advancements in data-driven methodologies have underscored the critical importance of ensuring data quality. Consequently, detecting out-of-distribution (OOD) data has emerged as an essential task to maintain the reliability and robustness of data-driven models, in general, and machine and deep learning models, in particular. In this study, we leveraged the convex hull property of a dataset and the fact that anomalies highly contribute to the increase of the CH's volume to propose a novel anomaly detection algorithm. Our algorithm computes the CH's volume as an increasing number of data points are removed from the dataset to define a decision line between OOD and in-distribution data points. We compared the proposed algorithm to seven widely used anomaly detection algorithms over ten datasets, showing comparable results for state-of-the-art (SOTA) algorithms. Moreover, we show that with a computationally cheap and simple check, one can detect datasets that are well-suited for the proposed algorithm which outperforms the SOTA anomaly detection algorithms.

### Error-related Potential driven Reinforcement Learning for adaptive Brain-Computer Interfaces 
[[arxiv](https://arxiv.org/abs/2502.18594)] [[cool](https://papers.cool/arxiv/2502.18594)] [[pdf](https://arxiv.org/pdf/2502.18594)]
> **Authors**: Aline Xavier Fidêncio,Felix Grün,Christian Klaes,Ioannis Iossifidis
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: :F.2.0; I.2.6; I.6.6; I.2.m
- **标题**: None
- **领域**: 机器学习,人机交互,神经元和认知
- **Abstract**: Brain-computer interfaces (BCIs) provide alternative communication methods for individuals with motor disabilities by allowing control and interaction with external devices. Non-invasive BCIs, especially those using electroencephalography (EEG), are practical and safe for various applications. However, their performance is often hindered by EEG non-stationarities, caused by changing mental states or device characteristics like electrode impedance. This challenge has spurred research into adaptive BCIs that can handle such variations. In recent years, interest has grown in using error-related potentials (ErrPs) to enhance BCI performance. ErrPs, neural responses to errors, can be detected non-invasively and have been integrated into different BCI paradigms to improve performance through error correction or adaptation. This research introduces a novel adaptive ErrP-based BCI approach using reinforcement learning (RL). We demonstrate the feasibility of an RL-driven adaptive framework incorporating ErrPs and motor imagery. Utilizing two RL agents, the framework adapts dynamically to EEG non-stationarities. Validation was conducted using a publicly available motor imagery dataset and a fast-paced game designed to boost user engagement. Results show the framework's promise, with RL agents learning control policies from user interactions and achieving robust performance across datasets. However, a critical insight from the game-based protocol revealed that motor imagery in a high-speed interaction paradigm was largely ineffective for participants, highlighting task design limitations in real-time BCI applications. These findings underscore the potential of RL for adaptive BCIs while pointing out practical constraints related to task complexity and user responsiveness.

### Transported Memory Networks accelerating Computational Fluid Dynamics 
[[arxiv](https://arxiv.org/abs/2502.18591)] [[cool](https://papers.cool/arxiv/2502.18591)] [[pdf](https://arxiv.org/pdf/2502.18591)]
> **Authors**: Matthias Schulz,Gwendal Jouan,Daniel Berger,Stefan Gavranovic,Dirk Hartmann
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,流体动力学
- **Abstract**: In recent years, augmentation of differentiable PDE solvers with neural networks has shown promising results, particularly in fluid simulations. However, most approaches rely on convolutional neural networks and custom solvers operating on Cartesian grids with efficient access to cell data. This particular choice poses challenges for industrial-grade solvers that operate on unstructured meshes, where access is restricted to neighboring cells only. In this work, we address this limitation using a novel architecture, named Transported Memory Networks. The architecture draws inspiration from both traditional turbulence models and recurrent neural networks, and it is fully compatible with generic discretizations. Our results show that it is point-wise and statistically comparable to, or improves upon, previous methods in terms of both accuracy and computational efficiency.

### Differentially Private Iterative Screening Rules for Linear Regression 
[[arxiv](https://arxiv.org/abs/2502.18578)] [[cool](https://papers.cool/arxiv/2502.18578)] [[pdf](https://arxiv.org/pdf/2502.18578)]
> **Authors**: Amol Khanna,Fred Lu,Edward Raff
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Proceedings of the 15th ACM Conference on Data and Application Security and Privacy
- **标题**: None
- **领域**: 机器学习,人工智能,密码学和安全
- **Abstract**: Linear $L_1$-regularized models have remained one of the simplest and most effective tools in data science. Over the past decade, screening rules have risen in popularity as a way to eliminate features when producing the sparse regression weights of $L_1$ models. However, despite the increasing need of privacy-preserving models for data analysis, to the best of our knowledge, no differentially private screening rule exists. In this paper, we develop the first private screening rule for linear regression. We initially find that this screening rule is too strong: it screens too many coefficients as a result of the private screening step. However, a weakened implementation of private screening reduces overscreening and improves performance.

### Target Defense with Multiple Defenders and an Agile Attacker via Residual Policy Learning 
[[arxiv](https://arxiv.org/abs/2502.18549)] [[cool](https://papers.cool/arxiv/2502.18549)] [[pdf](https://arxiv.org/pdf/2502.18549)]
> **Authors**: Jiyue Tao,Tongsheng Shen,Dexin Zhao,Feitian Zhang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,密码学和安全
- **Abstract**: The target defense problem involves intercepting an attacker before it reaches a designated target region using one or more defenders. This letter focuses on a particularly challenging scenario in which the attacker is more agile than the defenders, significantly increasing the difficulty of effective interception. To address this challenge, we propose a novel residual policy framework that integrates deep reinforcement learning (DRL) with the force-based Boids model. In this framework, the Boids model serves as a baseline policy, while DRL learns a residual policy to refine and optimize the defenders' actions. Simulation experiments demonstrate that the proposed method consistently outperforms traditional interception policies, whether learned via vanilla DRL or fine-tuned from force-based methods. Moreover, the learned policy exhibits strong scalability and adaptability, effectively handling scenarios with varying numbers of defenders and attackers with different agility levels.

### What is the Alignment Objective of GRPO? 
[[arxiv](https://arxiv.org/abs/2502.18548)] [[cool](https://papers.cool/arxiv/2502.18548)] [[pdf](https://arxiv.org/pdf/2502.18548)]
> **Authors**: Milan Vojnovic,Se-Young Yun
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: In this note, we examine the aggregation of preferences achieved by the Group Policy Optimisation (GRPO) algorithm, a reinforcement learning method used to train advanced artificial intelligence models such as DeepSeek-R1-Zero and DeepSeekMath. The GRPO algorithm trains a policy using a reward preference model, which is computed by sampling a set of outputs for a given context, observing the corresponding rewards, and applying shift-and-scale normalisation to these reward values. Additionally, it incorporates a penalty function to discourage deviations from a reference policy. We present a framework that enables us to characterise the stationary policies of the GRPO algorithm. This analysis reveals that the aggregation of preferences differs fundamentally from standard logarithmic pooling, which is implemented by other approaches such as RLHF. The precise form of preference aggregation arises from the way the reward preference model is defined and from the penalty function, which we show to essentially correspond to the reverse Kullback-Leibler (KL) divergence between the aggregation policy and the reference policy. Interestingly, we demonstrate that for groups of size two, the reward preference model corresponds to pairwise comparison preferences, similar to those in other alignment methods based on pairwise comparison feedback. We provide explicit characterisations of the aggregate preference for binary questions, for groups of size two, and in the limit of large group size. This provides insights into the dependence of the aggregate preference on parameters such as the regularisation constant and the confidence margin of question answers. Finally, we discuss the aggregation of preferences obtained by modifying the GRPO algorithm to use direct KL divergence as the penalty or to use rewards without scale normalisation.

### Revisiting Convolution Architecture in the Realm of DNA Foundation Models 
[[arxiv](https://arxiv.org/abs/2502.18538)] [[cool](https://papers.cool/arxiv/2502.18538)] [[pdf](https://arxiv.org/pdf/2502.18538)]
> **Authors**: Yu Bo,Weian Mao,Yanjun Shao,Weiqiang Bai,Peng Ye,Xinzhu Ma,Junbo Zhao,Hao Chen,Chunhua Shen
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: In recent years, a variety of methods based on Transformer and state space model (SSM) architectures have been proposed, advancing foundational DNA language models. However, there is a lack of comparison between these recent approaches and the classical architecture convolutional networks (CNNs) on foundation model benchmarks. This raises the question: are CNNs truly being surpassed by these recent approaches based on transformer and SSM architectures? In this paper, we develop a simple but well-designed CNN-based method termed ConvNova. ConvNova identifies and proposes three effective designs: 1) dilated convolutions, 2) gated convolutions, and 3) a dual-branch framework for gating mechanisms. Through extensive empirical experiments, we demonstrate that ConvNova significantly outperforms recent methods on more than half of the tasks across several foundation model benchmarks. For example, in histone-related tasks, ConvNova exceeds the second-best method by an average of 5.8%, while generally utilizing fewer parameters and enabling faster computation. In addition, the experiments observed findings that may be related to biological characteristics. This indicates that CNNs are still a strong competitor compared to Transformers and SSMs. We anticipate that this work will spark renewed interest in CNN-based methods for DNA foundation models.

### Reinforcement Learning-based Approach for Vehicle-to-Building Charging with Heterogeneous Agents and Long Term Rewards 
[[arxiv](https://arxiv.org/abs/2502.18526)] [[cool](https://papers.cool/arxiv/2502.18526)] [[pdf](https://arxiv.org/pdf/2502.18526)]
> **Authors**: Fangqi Liu,Rishav Sen,Jose Paolo Talusan,Ava Pettet,Aaron Kandel,Yoshinori Suzue,Ayan Mukhopadhyay,Abhishek Dubey
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Strategic aggregation of electric vehicle batteries as energy reservoirs can optimize power grid demand, benefiting smart and connected communities, especially large office buildings that offer workplace charging. This involves optimizing charging and discharging to reduce peak energy costs and net peak demand, monitored over extended periods (e.g., a month), which involves making sequential decisions under uncertainty and delayed and sparse rewards, a continuous action space, and the complexity of ensuring generalization across diverse conditions. Existing algorithmic approaches, e.g., heuristic-based strategies, fall short in addressing real-time decision-making under dynamic conditions, and traditional reinforcement learning (RL) models struggle with large state-action spaces, multi-agent settings, and the need for long-term reward optimization. To address these challenges, we introduce a novel RL framework that combines the Deep Deterministic Policy Gradient approach (DDPG) with action masking and efficient MILP-driven policy guidance. Our approach balances the exploration of continuous action spaces to meet user charging demands. Using real-world data from a major electric vehicle manufacturer, we show that our approach comprehensively outperforms many well-established baselines and several scalable heuristic approaches, achieving significant cost savings while meeting all charging requirements. Our results show that the proposed approach is one of the first scalable and general approaches to solving the V2B energy management challenge.

### Allocating Variance to Maximize Expectation 
[[arxiv](https://arxiv.org/abs/2502.18463)] [[cool](https://papers.cool/arxiv/2502.18463)] [[pdf](https://arxiv.org/pdf/2502.18463)]
> **Authors**: Renato Purita Paes Leme,Cliff Stein,Yifeng Teng,Pratik Worah
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: We design efficient approximation algorithms for maximizing the expectation of the supremum of families of Gaussian random variables. In particular, let $\mathrm{OPT}:=\max_{σ_1,\cdots,σ_n}\mathbb{E}\left[\sum_{j=1}^{m}\max_{i\in S_j} X_i\right]$, where $X_i$ are Gaussian, $S_j\subset[n]$ and $\sum_iσ_i^2=1$, then our theoretical results include: - We characterize the optimal variance allocation -- it concentrates on a small subset of variables as $|S_j|$ increases, - A polynomial time approximation scheme (PTAS) for computing $\mathrm{OPT}$ when $m=1$, and - An $O(\log n)$ approximation algorithm for computing $\mathrm{OPT}$ for general $m>1$. Such expectation maximization problems occur in diverse applications, ranging from utility maximization in auctions markets to learning mixture models in quantitative genetics.

### Scalable Equilibrium Sampling with Sequential Boltzmann Generators 
[[arxiv](https://arxiv.org/abs/2502.18462)] [[cool](https://papers.cool/arxiv/2502.18462)] [[pdf](https://arxiv.org/pdf/2502.18462)]
> **Authors**: Charlie B. Tan,Avishek Joey Bose,Chen Lin,Leon Klein,Michael M. Bronstein,Alexander Tong
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Preprint
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Scalable sampling of molecular states in thermodynamic equilibrium is a long-standing challenge in statistical physics. Boltzmann generators tackle this problem by pairing powerful normalizing flows with importance sampling to obtain statistically independent samples under the target distribution. In this paper, we extend the Boltzmann generator framework and introduce Sequential Boltzmann generators (SBG) with two key improvements. The first is a highly efficient non-equivariant Transformer-based normalizing flow operating directly on all-atom Cartesian coordinates. In contrast to equivariant continuous flows of prior methods, we leverage exactly invertible non-equivariant architectures which are highly efficient both during sample generation and likelihood computation. As a result, this unlocks more sophisticated inference strategies beyond standard importance sampling. More precisely, as a second key improvement we perform inference-time scaling of flow samples using annealed Langevin dynamics which transports samples toward the target distribution leading to lower variance (annealed) importance weights which enable higher fidelity resampling with sequential Monte Carlo. SBG achieves state-of-the-art performance w.r.t. all metrics on molecular systems, demonstrating the first equilibrium sampling in Cartesian coordinates of tri, tetra, and hexapeptides that were so far intractable for prior Boltzmann generators.

### Supervised Reward Inference 
[[arxiv](https://arxiv.org/abs/2502.18447)] [[cool](https://papers.cool/arxiv/2502.18447)] [[pdf](https://arxiv.org/pdf/2502.18447)]
> **Authors**: Will Schwarzer,Jordan Schneider,Philip S. Thomas,Scott Niekum
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 16 pages, 4 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Existing approaches to reward inference from behavior typically assume that humans provide demonstrations according to specific models of behavior. However, humans often indicate their goals through a wide range of behaviors, from actions that are suboptimal due to poor planning or execution to behaviors which are intended to communicate goals rather than achieve them. We propose that supervised learning offers a unified framework to infer reward functions from any class of behavior, and show that such an approach is asymptotically Bayes-optimal under mild assumptions. Experiments on simulated robotic manipulation tasks show that our method can efficiently infer rewards from a wide variety of arbitrarily suboptimal demonstrations.

### Comparative Analysis of MDL-VAE vs. Standard VAE on 202 Years of Gynecological Data 
[[arxiv](https://arxiv.org/abs/2502.18412)] [[cool](https://papers.cool/arxiv/2502.18412)] [[pdf](https://arxiv.org/pdf/2502.18412)]
> **Authors**: Paula Santos
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 12 pagas, 5 figures, 9th International Conference on Signal, Image Processing (SIPO 2025), Vancouver CA
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: This study presents a comparative evaluation of a Variational Autoencoder (VAE) enhanced with Minimum Description Length (MDL) regularization against a Standard Autoencoder for reconstructing high-dimensional gynecological data. The MDL-VAE exhibits significantly lower reconstruction errors (MSE, MAE, RMSE) and more structured latent representations, driven by effective KL divergence regularization. Statistical analyses confirm these performance improvements are significant. Furthermore, the MDL-VAE shows consistent training and validation losses and achieves efficient inference times, underscoring its robustness and practical viability. Our findings suggest that incorporating MDL principles into VAE architectures can substantially improve data reconstruction and generalization, making it a promising approach for advanced applications in healthcare data modeling and analysis.

### TSKANMixer: Kolmogorov-Arnold Networks with MLP-Mixer Model for Time Series Forecasting 
[[arxiv](https://arxiv.org/abs/2502.18410)] [[cool](https://papers.cool/arxiv/2502.18410)] [[pdf](https://arxiv.org/pdf/2502.18410)]
> **Authors**: Young-Chae Hong,Bei Xiao,Yangho Chen
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 8 pages, 4 figures, 7 tables and accepted at the AI4TS:AIfor Time Series Analysis workshop, AAAI 2025
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Time series forecasting has long been a focus of research across diverse fields, including economics, energy, healthcare, and traffic management. Recent works have introduced innovative architectures for time series models, such as the Time-Series Mixer (TSMixer), which leverages multi-layer perceptrons (MLPs) to enhance prediction accuracy by effectively capturing both spatial and temporal dependencies within the data. In this paper, we investigate the capabilities of the Kolmogorov-Arnold Networks (KANs) for time-series forecasting by modifying TSMixer with a KAN layer (TSKANMixer). Experimental results demonstrate that TSKANMixer tends to improve prediction accuracy over the original TSMixer across multiple datasets, ranking among the top-performing models compared to other time series approaches. Our results show that the KANs are promising alternatives to improve the performance of time series forecasting by replacing or extending traditional MLPs.

### Enhancing DNA Foundation Models to Address Masking Inefficiencies 
[[arxiv](https://arxiv.org/abs/2502.18405)] [[cool](https://papers.cool/arxiv/2502.18405)] [[pdf](https://arxiv.org/pdf/2502.18405)]
> **Authors**: Monireh Safari,Pablo Millan Arias,Scott C. Lowe,Lila Kari,Angel X. Chang,Graham W. Taylor
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 10 pages, 5 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Masked language modelling (MLM) as a pretraining objective has been widely adopted in genomic sequence modelling. While pretrained models can successfully serve as encoders for various downstream tasks, the distribution shift between pretraining and inference detrimentally impacts performance, as the pretraining task is to map [MASK] tokens to predictions, yet the [MASK] is absent during downstream applications. This means the encoder does not prioritize its encodings of non-[MASK] tokens, and expends parameters and compute on work only relevant to the MLM task, despite this being irrelevant at deployment time. In this work, we propose a modified encoder-decoder architecture based on the masked autoencoder framework, designed to address this inefficiency within a BERT-based transformer. We empirically show that the resulting mismatch is particularly detrimental in genomic pipelines where models are often used for feature extraction without fine-tuning. We evaluate our approach on the BIOSCAN-5M dataset, comprising over 2 million unique DNA barcodes. We achieve substantial performance gains in both closed-world and open-world classification tasks when compared against causal models and bidirectional architectures pretrained with MLM tasks.

### The FFT Strikes Back: An Efficient Alternative to Self-Attention 
[[arxiv](https://arxiv.org/abs/2502.18394)] [[cool](https://papers.cool/arxiv/2502.18394)] [[pdf](https://arxiv.org/pdf/2502.18394)]
> **Authors**: Jacob Fein-Ashley
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Conventional self-attention mechanisms incur quadratic complexity, limiting their scalability on long sequences. We introduce FFTNet, an adaptive spectral filtering framework that leverages the Fast Fourier Transform (FFT) to achieve global token mixing in $\mathcal{O}(n\log n)$ time. By transforming inputs into the frequency domain, FFTNet exploits the orthogonality and energy preservation guaranteed by Parseval's theorem to capture long-range dependencies efficiently. A learnable spectral filter and modReLU activation dynamically emphasize salient frequency components, providing a rigorous and adaptive alternative to traditional self-attention. Experiments on the Long Range Arena and ImageNet benchmarks validate our theoretical insights and demonstrate superior performance over fixed Fourier and standard attention models.

### Mechanistic PDE Networks for Discovery of Governing Equations 
[[arxiv](https://arxiv.org/abs/2502.18377)] [[cool](https://papers.cool/arxiv/2502.18377)] [[pdf](https://arxiv.org/pdf/2502.18377)]
> **Authors**: Adeel Pervez,Efstratios Gavves,Francesco Locatello
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We present Mechanistic PDE Networks -- a model for discovery of governing partial differential equations from data. Mechanistic PDE Networks represent spatiotemporal data as space-time dependent linear partial differential equations in neural network hidden representations. The represented PDEs are then solved and decoded for specific tasks. The learned PDE representations naturally express the spatiotemporal dynamics in data in neural network hidden space, enabling increased power for dynamical modeling. Solving the PDE representations in a compute and memory-efficient way, however, is a significant challenge. We develop a native, GPU-capable, parallel, sparse, and differentiable multigrid solver specialized for linear partial differential equations that acts as a module in Mechanistic PDE Networks. Leveraging the PDE solver, we propose a discovery architecture that can discover nonlinear PDEs in complex settings while also being robust to noise. We validate PDE discovery on a number of PDEs, including reaction-diffusion and Navier-Stokes equations.

### WebGames: Challenging General-Purpose Web-Browsing AI Agents 
[[arxiv](https://arxiv.org/abs/2502.18356)] [[cool](https://papers.cool/arxiv/2502.18356)] [[pdf](https://arxiv.org/pdf/2502.18356)]
> **Authors**: George Thomas,Alex J. Chan,Jikun Kang,Wenqi Wu,Filippos Christianos,Fraser Greenlee,Andy Toulis,Marvin Purtorab
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We introduce WebGames, a comprehensive benchmark suite designed to evaluate general-purpose web-browsing AI agents through a collection of 50+ interactive challenges. These challenges are specifically crafted to be straightforward for humans while systematically testing the limitations of current AI systems across fundamental browser interactions, advanced input processing, cognitive tasks, workflow automation, and interactive entertainment. Our framework eliminates external dependencies through a hermetic testing environment, ensuring reproducible evaluation with verifiable ground-truth solutions. We evaluate leading vision-language models including GPT-4o, Claude Computer-Use, Gemini-1.5-Pro, and Qwen2-VL against human performance. Results reveal a substantial capability gap, with the best AI system achieving only 43.1% success rate compared to human performance of 95.7%, highlighting fundamental limitations in current AI systems' ability to handle common web interaction patterns that humans find intuitive. The benchmark is publicly available at webgames.convergence.ai, offering a lightweight, client-side implementation that facilitates rapid evaluation cycles. Through its modular architecture and standardized challenge specifications, WebGames provides a robust foundation for measuring progress in development of more capable web-browsing agents.

### Structural Alignment Improves Graph Test-Time Adaptation 
[[arxiv](https://arxiv.org/abs/2502.18334)] [[cool](https://papers.cool/arxiv/2502.18334)] [[pdf](https://arxiv.org/pdf/2502.18334)]
> **Authors**: Hans Hao-Hsun Hsu,Shikun Liu,Han Zhao,Pan Li
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Graph-based learning has achieved remarkable success in domains ranging from recommendation to fraud detection and particle physics by effectively capturing underlying interaction patterns. However, it often struggles to generalize when distribution shifts occur, particularly those involving changes in network connectivity or interaction patterns. Existing approaches designed to mitigate such shifts typically require retraining with full access to source data, rendering them infeasible under strict computational or privacy constraints. To address this limitation, we propose a test-time structural alignment (TSA) algorithm for Graph Test-Time Adaptation (GTTA), a novel method that aligns graph structures during inference without revisiting the source domain. Built upon a theoretically grounded treatment of graph data distribution shifts, TSA integrates three key strategies: an uncertainty-aware neighborhood weighting that accommodates structure shifts, an adaptive balancing of self-node and neighborhood-aggregated representations driven by node representations' signal-to-noise ratio, and a decision boundary refinement that corrects remaining label and feature shifts. Extensive experiments on synthetic and real-world datasets demonstrate that TSA can consistently outperform both non-graph TTA methods and state-of-the-art GTTA baselines.

### Pretraining Frequency Predicts Compositional Generalization of CLIP on Real-World Tasks 
[[arxiv](https://arxiv.org/abs/2502.18326)] [[cool](https://papers.cool/arxiv/2502.18326)] [[pdf](https://arxiv.org/pdf/2502.18326)]
> **Authors**: Thaddäus Wiedemer,Yash Sharma,Ameya Prabhu,Matthias Bethge,Wieland Brendel
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-26
> **comment**: NeurIPS 2024 Workshop on CompositionalLearning: Perspectives, Methods, and Paths Forward
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We investigate the success conditions for compositional generalization of CLIP models on real-world data through performance prediction. Prior work shows that CLIP requires exponentially more pretraining data for linear performance gains on individual concepts. This sample-inefficient scaling could be mitigated if CLIP systematically understood new inputs as compositions of learned components, allowing rare observation to be mapped to common concepts. To explore CLIP's compositional generalization ability, we filter retrieval corpora for samples with object combinations not present in the pretraining corpus. We show that CLIP's performance on these samples can be accurately predicted from the pretraining frequencies of individual objects. Our findings demonstrate that CLIP learns to disentangle objects observed in its pretraining data and can recompose them straightforwardly. Additionally, we are the first to show how this ability scales with pretraining data. For data curation in practice, our results suggest that balancing object occurrences improves generalization, which should benefit CLIP's efficiency and accuracy without scaling data volume.

### Accelerated Training on Low-Power Edge Devices 
[[arxiv](https://arxiv.org/abs/2502.18323)] [[cool](https://papers.cool/arxiv/2502.18323)] [[pdf](https://arxiv.org/pdf/2502.18323)]
> **Authors**: Mohamed Aboelenien Ahmed,Kilian Pfeiffer,Heba Khdr,Osama Abboud,Ramin Khalili,Jörg Henkel
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,操作系统
- **Abstract**: Training on edge devices poses several challenges as these devices are generally resource-constrained, especially in terms of power. State-of-the-art techniques at the device level reduce the GPU frequency to enforce power constraints, leading to a significant increase in training time. To accelerate training, we propose to jointly adjust the system and application parameters (in our case, the GPU frequency and the batch size of the training task) while adhering to the power constraints on devices. We introduce a novel cross-layer methodology that combines predictions of batch size efficiency and device profiling to achieve the desired optimization. Our evaluation on real hardware shows that our method outperforms the current baselines that depend on state of the art techniques, reducing the training time by $2.4\times$ with results very close to optimal. Our measurements also indicate a substantial reduction in the overall energy used for the training process. These gains are achieved without reduction in the performance of the trained model.

### Global-Decision-Focused Neural ODEs for Proactive Grid Resilience Management 
[[arxiv](https://arxiv.org/abs/2502.18321)] [[cool](https://papers.cool/arxiv/2502.18321)] [[pdf](https://arxiv.org/pdf/2502.18321)]
> **Authors**: Shuyi Chen,Ferdinando Fioretto,Feng Qiu,Shixiang Zhu
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Extreme hazard events such as wildfires and hurricanes increasingly threaten power systems, causing widespread outages and disrupting critical services. Recently, predict-then-optimize approaches have gained traction in grid operations, where system functionality forecasts are first generated and then used as inputs for downstream decision-making. However, this two-stage method often results in a misalignment between prediction and optimization objectives, leading to suboptimal resource allocation. To address this, we propose predict-all-then-optimize-globally (PATOG), a framework that integrates outage prediction with globally optimized interventions. At its core, our global-decision-focused (GDF) neural ODE model captures outage dynamics while optimizing resilience strategies in a decision-aware manner. Unlike conventional methods, our approach ensures spatially and temporally coherent decision-making, improving both predictive accuracy and operational efficiency. Experiments on synthetic and real-world datasets demonstrate significant improvements in outage prediction consistency and grid resilience.

### Bayesian Computation in Deep Learning 
[[arxiv](https://arxiv.org/abs/2502.18300)] [[cool](https://papers.cool/arxiv/2502.18300)] [[pdf](https://arxiv.org/pdf/2502.18300)]
> **Authors**: Wenlong Chen,Bolian Li,Ruqi Zhang,Yingzhen Li
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 43 pages, 7 figures
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: This review paper is intended for the 2nd edition of the Handbook of Markov chain Monte Carlo. We provide an introduction to approximate inference techniques as Bayesian computation methods applied to deep learning models. We organize the chapter by presenting popular computational methods for Bayesian neural networks and deep generative models, explaining their unique challenges in posterior inference as well as the solutions.

### DeepCircuitX: A Comprehensive Repository-Level Dataset for RTL Code Understanding, Generation, and PPA Analysis 
[[arxiv](https://arxiv.org/abs/2502.18297)] [[cool](https://papers.cool/arxiv/2502.18297)] [[pdf](https://arxiv.org/pdf/2502.18297)]
> **Authors**: Zeju Li,Changran Xu,Zhengyuan Shi,Zedong Peng,Yi Liu,Yunhao Zhou,Lingfeng Zhou,Chengyu Ma,Jianyuan Zhong,Xi Wang,Jieru Zhao,Zhufei Chu,Xiaoyan Yang,Qiang Xu
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 8 pages, 3 figures
- **标题**: None
- **领域**: 机器学习,编程语言
- **Abstract**: This paper introduces DeepCircuitX, a comprehensive repository-level dataset designed to advance RTL (Register Transfer Level) code understanding, generation, and power-performance-area (PPA) analysis. Unlike existing datasets that are limited to either file-level RTL code or physical layout data, DeepCircuitX provides a holistic, multilevel resource that spans repository, file, module, and block-level RTL code. This structure enables more nuanced training and evaluation of large language models (LLMs) for RTL-specific tasks. DeepCircuitX is enriched with Chain of Thought (CoT) annotations, offering detailed descriptions of functionality and structure at multiple levels. These annotations enhance its utility for a wide range of tasks, including RTL code understanding, generation, and completion. Additionally, the dataset includes synthesized netlists and PPA metrics, facilitating early-stage design exploration and enabling accurate PPA prediction directly from RTL code. We demonstrate the dataset's effectiveness on various LLMs finetuned with our dataset and confirm the quality with human evaluations. Our results highlight DeepCircuitX as a critical resource for advancing RTL-focused machine learning applications in hardware design automation.Our data is available at https://zeju.gitbook.io/lcm-team.

### AMPO: Active Multi-Preference Optimization 
[[arxiv](https://arxiv.org/abs/2502.18293)] [[cool](https://papers.cool/arxiv/2502.18293)] [[pdf](https://arxiv.org/pdf/2502.18293)]
> **Authors**: Taneesh Gupta,Rahul Madhavan,Xuchao Zhang,Chetan Bansal,Saravan Rajmohan
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Multi-preference optimization enriches language-model alignment beyond pairwise preferences by contrasting entire sets of helpful and undesired responses, thereby enabling richer training signals for large language models. During self-play alignment, these models often produce numerous candidate answers per query, rendering it computationally infeasible to include all responses in the training objective. In this work, we propose $\textit{Active Multi-Preference Optimization}$ (AMPO), a novel approach that combines on-policy generation, a multi-preference group-contrastive loss, and active subset selection. Specifically, we score and embed large candidate pools of responses and then select a small, yet informative, subset that covers reward extremes and distinct semantic clusters for preference optimization. Our contrastive training scheme is capable of identifying not only the best and worst answers but also subtle, underexplored modes that are crucial for robust alignment. Theoretically, we provide guarantees for expected reward maximization using our active selection method, and empirically, AMPO achieves state-of-the-art results on $\textit{AlpacaEval}$ using Llama 8B.

### Iterative Counterfactual Data Augmentation 
[[arxiv](https://arxiv.org/abs/2502.18249)] [[cool](https://papers.cool/arxiv/2502.18249)] [[pdf](https://arxiv.org/pdf/2502.18249)]
> **Authors**: Mitchell Plyler,Min Chi
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: AAAI 2025
- **标题**: None
- **领域**: 机器学习,计算语言学,信息论
- **Abstract**: Counterfactual data augmentation (CDA) is a method for controlling information or biases in training datasets by generating a complementary dataset with typically opposing biases. Prior work often either relies on hand-crafted rules or algorithmic CDA methods which can leave unwanted information in the augmented dataset. In this work, we show iterative CDA (ICDA) with initial, high-noise interventions can converge to a state with significantly lower noise. Our ICDA procedure produces a dataset where one target signal in the training dataset maintains high mutual information with a corresponding label and the information of spurious signals are reduced. We show training on the augmented datasets produces rationales on documents that better align with human annotation. Our experiments include six human produced datasets and two large-language model generated datasets.

### Causal AI-based Root Cause Identification: Research to Practice at Scale 
[[arxiv](https://arxiv.org/abs/2502.18240)] [[cool](https://papers.cool/arxiv/2502.18240)] [[pdf](https://arxiv.org/pdf/2502.18240)]
> **Authors**: Saurabh Jha,Ameet Rahane,Laura Shwartz,Marc Palaci-Olgun,Frank Bagehorn,Jesus Rios,Dan Stingaciu,Ragu Kattinakere,Debasish Banerjee
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,分布式、并行和集群计算,软件工程
- **Abstract**: Modern applications are built as large, distributed systems spanning numerous modules, teams, and data centers. Despite robust engineering and recovery strategies, failures and performance issues remain inevitable, risking significant disruptions and affecting end users. Rapid and accurate root cause identification is therefore vital to ensure system reliability and maintain key service metrics. We have developed a novel causality-based Root Cause Identification (RCI) algorithm that emphasizes causation over correlation. This algorithm has been integrated into IBM Instana-bridging research to practice at scale-and is now in production use by enterprise customers. By leveraging "causal AI," Instana stands apart from typical Application Performance Management (APM) tools, pinpointing issues in near real-time. This paper highlights Instana's advanced failure diagnosis capabilities, discussing both the theoretical underpinnings and practical implementations of the RCI algorithm. Real-world examples illustrate how our causality-based approach enhances reliability and performance in today's complex system landscapes.

### Unveiling and Causalizing CoT: A Causal Pespective 
[[arxiv](https://arxiv.org/abs/2502.18239)] [[cool](https://papers.cool/arxiv/2502.18239)] [[pdf](https://arxiv.org/pdf/2502.18239)]
> **Authors**: Jiarun Fu,Lizhong Ding,Hao Li,Pengqi Li,Qiuning Wei,Xu Chen
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Although Chain-of-Thought (CoT) has achieved remarkable success in enhancing the reasoning ability of large language models (LLMs), the mechanism of CoT remains a ``black box''. Even if the correct answers can frequently be obtained, existing CoTs struggle to make the reasoning understandable to human. In this paper, we unveil and causalize CoT from a causal perspective to ensure both correctness and understandability of all reasoning steps (to the best of our knowledge, the first such). We model causality of CoT via structural causal models (SCM) to unveil the reasoning mechanism of CoT. To measure the causality of CoT, we define the CoT Average Causal Effect (CACE) to test the causal relations between steps. For those steps without causality (wrong or unintelligible steps), we design a role-playing causal query algorithm to causalize these steps, resulting a causalized CoT with all steps correct and understandable. Experimental results on both open-source and closed-source LLMs demonstrate that the causal errors commonly in steps are effectively corrected and the reasoning ability of LLMs is significantly improved.

### Beyond the convexity assumption: Realistic tabular data generation under quantifier-free real linear constraints 
[[arxiv](https://arxiv.org/abs/2502.18237)] [[cool](https://papers.cool/arxiv/2502.18237)] [[pdf](https://arxiv.org/pdf/2502.18237)]
> **Authors**: Mihaela Cătălina Stoian,Eleonora Giunchiglia
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Accepted at ICLR 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Synthetic tabular data generation has traditionally been a challenging problem due to the high complexity of the underlying distributions that characterise this type of data. Despite recent advances in deep generative models (DGMs), existing methods often fail to produce realistic datapoints that are well-aligned with available background knowledge. In this paper, we address this limitation by introducing Disjunctive Refinement Layer (DRL), a novel layer designed to enforce the alignment of generated data with the background knowledge specified in user-defined constraints. DRL is the first method able to automatically make deep learning models inherently compliant with constraints as expressive as quantifier-free linear formulas, which can define non-convex and even disconnected spaces. Our experimental analysis shows that DRL not only guarantees constraint satisfaction but also improves efficacy in downstream tasks. Notably, when applied to DGMs that frequently violate constraints, DRL eliminates violations entirely. Further, it improves performance metrics by up to 21.4% in F1-score and 20.9% in Area Under the ROC Curve, thus demonstrating its practical impact on data generation.

### Software implemented fault diagnosis of natural gas pumping unit based on feedforward neural network 
[[arxiv](https://arxiv.org/abs/2502.18233)] [[cool](https://papers.cool/arxiv/2502.18233)] [[pdf](https://arxiv.org/pdf/2502.18233)]
> **Authors**: Mykola Kozlenko,Olena Zamikhovska,Leonid Zamikhovskyi
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 11 pages, 9 figures
- **标题**: None
- **领域**: 机器学习,信号处理
- **Abstract**: In recent years, more and more attention has been paid to the use of artificial neural networks (ANN) for diagnostics of gas pumping units (GPU). Usually, ANN training is carried out on models of GPU workflows, and generated sets of diagnostic data are used to simulate defect conditions. At the same time, the results obtained do not allow assessing the real state of the GPU. It is proposed to use the values of the characteristics of the acoustic and vibration processes of the GPU as the input data of the ANN. A descriptive statistical analysis of real vibration and acoustic processes generated by the operation of the GPU type GTK-25-i (Nuovo Pignone, Italy) has been carried out. The formation of packets of diagnostic signs arriving at the input of the ANN has been carried out. The diagnostic features are the five maximum amplitude components of the acoustic and vibration signals, as well as the value of the standard deviation for each sample. Diagnostic signs are calculated directly in the input pipeline of ANN data in real time for three technical states of the GPU. Using the frameworks TensorFlow, Keras, NumPy, pandas, in the Python 3 programming language, an architecture was developed for a deep fully connected feedforward ANN, training on the error backpropagation algorithm. The results of training and testing of the developed ANN are presented. During testing, it was found that the signal classification precision for the "nominal" state of all 1475 signal samples is 1.0000, for the "current" state, precision equils 0.9853, and for the "defective" state, precision is 0.9091. The use of the developed ANN makes it possible to classify the technical states of the GPU with an accuracy sufficient for practical use, which will prevent the occurrence of GPU failures. ANN can be used to diagnose GPU of any type and power.

### DenoMAE2.0: Improving Denoising Masked Autoencoders by Classifying Local Patches 
[[arxiv](https://arxiv.org/abs/2502.18202)] [[cool](https://papers.cool/arxiv/2502.18202)] [[pdf](https://arxiv.org/pdf/2502.18202)]
> **Authors**: Atik Faysal,Mohammad Rostami,Taha Boushine,Reihaneh Gh. Roshan,Huaxia Wang,Nikhil Muralidhar
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: We introduce DenoMAE2.0, an enhanced denoising masked autoencoder that integrates a local patch classification objective alongside traditional reconstruction loss to improve representation learning and robustness. Unlike conventional Masked Autoencoders (MAE), which focus solely on reconstructing missing inputs, DenoMAE2.0 introduces position-aware classification of unmasked patches, enabling the model to capture fine-grained local features while maintaining global coherence. This dual-objective approach is particularly beneficial in semi-supervised learning for wireless communication, where high noise levels and data scarcity pose significant challenges. We conduct extensive experiments on modulation signal classification across a wide range of signal-to-noise ratios (SNRs), from extremely low to moderately high conditions and in a low data regime. Our results demonstrate that DenoMAE2.0 surpasses its predecessor, Deno-MAE, and other baselines in both denoising quality and downstream classification accuracy. DenoMAE2.0 achieves a 1.1% improvement over DenoMAE on our dataset and 11.83%, 16.55% significant improved accuracy gains on the RadioML benchmark, over DenoMAE, for constellation diagram classification of modulation signals.

### Training Consistency Models with Variational Noise Coupling 
[[arxiv](https://arxiv.org/abs/2502.18197)] [[cool](https://papers.cool/arxiv/2502.18197)] [[pdf](https://arxiv.org/pdf/2502.18197)]
> **Authors**: Gianluigi Silvestri,Luca Ambrogioni,Chieh-Hsin Lai,Yuhta Takida,Yuki Mitsufuji
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 23 pages, 11 figures
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: Consistency Training (CT) has recently emerged as a promising alternative to diffusion models, achieving competitive performance in image generation tasks. However, non-distillation consistency training often suffers from high variance and instability, and analyzing and improving its training dynamics is an active area of research. In this work, we propose a novel CT training approach based on the Flow Matching framework. Our main contribution is a trained noise-coupling scheme inspired by the architecture of Variational Autoencoders (VAE). By training a data-dependent noise emission model implemented as an encoder architecture, our method can indirectly learn the geometry of the noise-to-data mapping, which is instead fixed by the choice of the forward process in classical CT. Empirical results across diverse image datasets show significant generative improvements, with our model outperforming baselines and achieving the state-of-the-art (SoTA) non-distillation CT FID on CIFAR-10, and attaining FID on par with SoTA on ImageNet at $64 \times 64$ resolution in 2-step generation. Our code is available at https://github.com/sony/vct .

### Graph Augmentation for Cross Graph Domain Generalization 
[[arxiv](https://arxiv.org/abs/2502.18188)] [[cool](https://papers.cool/arxiv/2502.18188)] [[pdf](https://arxiv.org/pdf/2502.18188)]
> **Authors**: Guanzi Chen,Jiying Zhang,Yang Li
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Cross-graph node classification, utilizing the abundant labeled nodes from one graph to help classify unlabeled nodes in another graph, can be viewed as a domain generalization problem of graph neural networks (GNNs) due to the structure shift commonly appearing among various graphs. Nevertheless, current endeavors for cross-graph node classification mainly focus on model training. Data augmentation approaches, a simple and easy-to-implement domain generalization technique, remain under-explored. In this paper, we develop a new graph structure augmentation for the crossgraph domain generalization problem. Specifically, low-weight edgedropping is applied to remove potential noise edges that may hinder the generalization ability of GNNs, stimulating the GNNs to capture the essential invariant information underlying different structures. Meanwhile, clustering-based edge-adding is proposed to generate invariant structures based on the node features from the same distribution. Consequently, with these augmentation techniques, the GNNs can maintain the domain invariant structure information that can improve the generalization ability. The experiments on out-ofdistribution citation network datasets verify our method achieves state-of-the-art performance among conventional augmentations.

### Sharper Concentration Inequalities for Multi-Graph Dependent Variables 
[[arxiv](https://arxiv.org/abs/2502.18167)] [[cool](https://papers.cool/arxiv/2502.18167)] [[pdf](https://arxiv.org/pdf/2502.18167)]
> **Authors**: Xiao Shao,Guoqiang Wu
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 34 pages
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: In multi-task learning (MTL) with each task involving graph-dependent data, generalization results of existing theoretical analyses yield a sub-optimal risk bound of $O(\frac{1}{\sqrt{n}})$, where $n$ is the number of training samples.This is attributed to the lack of a foundational sharper concentration inequality for multi-graph dependent random variables. To fill this gap, this paper proposes a new corresponding Bennett inequality, enabling the derivation of a sharper risk bound of $O(\frac{\log n}{n})$. Specifically, building on the proposed Bennett inequality, we propose a new corresponding Talagrand inequality for the empirical process and further develop an analytical framework of the local Rademacher complexity to enhance theoretical generalization analyses in MTL with multi-graph dependent data. Finally, we apply the theoretical advancements to applications such as Macro-AUC Optimization, demonstrating the superiority of our theoretical results over previous work, which is also corroborated by experimental results.

### SASSHA: Sharpness-aware Adaptive Second-order Optimization with Stable Hessian Approximation 
[[arxiv](https://arxiv.org/abs/2502.18153)] [[cool](https://papers.cool/arxiv/2502.18153)] [[pdf](https://arxiv.org/pdf/2502.18153)]
> **Authors**: Dahun Shin,Dongyeop Lee,Jinseok Chung,Namhoon Lee
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Approximate second-order optimization methods often exhibit poorer generalization compared to first-order approaches. In this work, we look into this issue through the lens of the loss landscape and find that existing second-order methods tend to converge to sharper minima compared to SGD. In response, we propose Sassha, a novel second-order method designed to enhance generalization by explicitly reducing sharpness of the solution, while stabilizing the computation of approximate Hessians along the optimization trajectory. In fact, this sharpness minimization scheme is crafted also to accommodate lazy Hessian updates, so as to secure efficiency besides flatness. To validate its effectiveness, we conduct a wide range of standard deep learning experiments where Sassha demonstrates its outstanding generalization performance that is comparable to, and mostly better than, other methods. We provide a comprehensive set of analyses including convergence, robustness, stability, efficiency, and cost.

### Jacobian Sparse Autoencoders: Sparsify Computations, Not Just Activations 
[[arxiv](https://arxiv.org/abs/2502.18147)] [[cool](https://papers.cool/arxiv/2502.18147)] [[pdf](https://arxiv.org/pdf/2502.18147)]
> **Authors**: Lucy Farnik,Tim Lawson,Conor Houghton,Laurence Aitchison
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Sparse autoencoders (SAEs) have been successfully used to discover sparse and human-interpretable representations of the latent activations of LLMs. However, we would ultimately like to understand the computations performed by LLMs and not just their representations. The extent to which SAEs can help us understand computations is unclear because they are not designed to "sparsify" computations in any sense, only latent activations. To solve this, we propose Jacobian SAEs (JSAEs), which yield not only sparsity in the input and output activations of a given model component but also sparsity in the computation (formally, the Jacobian) connecting them. With a naïve implementation, the Jacobians in LLMs would be computationally intractable due to their size. One key technical contribution is thus finding an efficient way of computing Jacobians in this setup. We find that JSAEs extract a relatively large degree of computational sparsity while preserving downstream LLM performance approximately as well as traditional SAEs. We also show that Jacobians are a reasonable proxy for computational sparsity because MLPs are approximately linear when rewritten in the JSAE basis. Lastly, we show that JSAEs achieve a greater degree of computational sparsity on pre-trained LLMs than on the equivalent randomized LLM. This shows that the sparsity of the computational graph appears to be a property that LLMs learn through training, and suggests that JSAEs might be more suitable for understanding learned transformer computations than standard SAEs.

### Actively Inferring Optimal Measurement Sequences 
[[arxiv](https://arxiv.org/abs/2502.18142)] [[cool](https://papers.cool/arxiv/2502.18142)] [[pdf](https://arxiv.org/pdf/2502.18142)]
> **Authors**: Catherine F. Higham,Paul Henderson,Roderick Murray-Smith
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Measurement of a physical quantity such as light intensity is an integral part of many reconstruction and decision scenarios but can be costly in terms of acquisition time, invasion of or damage to the environment and storage. Data minimisation and compliance with data protection laws is also an important consideration. Where there are a range of measurements that can be made, some may be more informative and compliant with the overall measurement objective than others. We develop an active sequential inference algorithm that uses the low dimensional representational latent space from a variational autoencoder (VAE) to choose which measurement to make next. Our aim is to recover high dimensional data by making as few measurements as possible. We adapt the VAE encoder to map partial data measurements on to the latent space of the complete data. The algorithm draws samples from this latent space and uses the VAE decoder to generate data conditional on the partial measurements. Estimated measurements are made on the generated data and fed back through the partial VAE encoder to the latent space where they can be evaluated prior to making a measurement. Starting from no measurements and a normal prior on the latent space, we consider alternative strategies for choosing the next measurement and updating the predictive posterior prior for the next step. The algorithm is illustrated using the Fashion MNIST dataset and a novel convolutional Hadamard pattern measurement basis. We see that useful patterns are chosen within 10 steps, leading to the convergence of the guiding generative images. Compared with using stochastic variational inference to infer the parameters of the posterior distribution for each generated data point individually, the partial VAE framework can efficiently process batches of generated data and obtains superior results with minimal measurements.

### SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference 
[[arxiv](https://arxiv.org/abs/2502.18137)] [[cool](https://papers.cool/arxiv/2502.18137)] [[pdf](https://arxiv.org/pdf/2502.18137)]
> **Authors**: Jintao Zhang,Chendong Xiang,Haofeng Huang,Jia Wei,Haocheng Xi,Jun Zhu,Jianfei Chen
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别,表现
- **Abstract**: An efficient attention implementation is essential for large models due to its quadratic time complexity. Fortunately, attention commonly exhibits sparsity, i.e., many values in the attention map are near zero, allowing for the omission of corresponding computations. Many studies have utilized the sparse pattern to accelerate attention. However, most existing works focus on optimizing attention within specific models by exploiting certain sparse patterns of the attention map. A universal sparse attention that guarantees both the speedup and end-to-end performance of diverse models remains elusive. In this paper, we propose SpargeAttn, a universal sparse and quantized attention for any model. Our method uses a two-stage online filter: in the first stage, we rapidly and accurately predict the attention map, enabling the skip of some matrix multiplications in attention. In the second stage, we design an online softmax-aware filter that incurs no extra overhead and further skips some matrix multiplications. Experiments show that our method significantly accelerates diverse models, including language, image, and video generation, without sacrificing end-to-end metrics. The codes are available at https://github.com/thu-ml/SpargeAttn.

### EU-Nets: Enhanced, Explainable and Parsimonious U-Nets 
[[arxiv](https://arxiv.org/abs/2502.18122)] [[cool](https://papers.cool/arxiv/2502.18122)] [[pdf](https://arxiv.org/pdf/2502.18122)]
> **Authors**: B. Sun,P. Liò
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: In this study, we propose MHEX+, a framework adaptable to any U-Net architecture. Built upon MHEX+, we introduce novel U-Net variants, EU-Nets, which enhance explainability and uncertainty estimation, addressing the limitations of traditional U-Net models while improving performance and stability. A key innovation is the Equivalent Convolutional Kernel, which unifies consecutive convolutional layers, boosting interpretability. For uncertainty estimation, we propose the collaboration gradient approach, measuring gradient consistency across decoder layers. Notably, EU-Nets achieve an average accuracy improvement of 1.389\% and a variance reduction of 0.83\% across all networks and datasets in our experiments, requiring fewer than 0.1M parameters.

### Stackelberg Game Preference Optimization for Data-Efficient Alignment of Language Models 
[[arxiv](https://arxiv.org/abs/2502.18099)] [[cool](https://papers.cool/arxiv/2502.18099)] [[pdf](https://arxiv.org/pdf/2502.18099)]
> **Authors**: Xu Chu,Zhixin Zhang,Tianyu Jia,Yujie Jin
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Aligning language models with human preferences is critical for real-world deployment, but existing methods often require large amounts of high-quality human annotations. Aiming at a data-efficient alignment method, we propose Stackelberg Game Preference Optimization (SGPO), a framework that models alignment as a two-player Stackelberg game, where a policy (leader) optimizes against a worst-case preference distribution (follower) within an $ε$-Wasserstein ball, ensuring robustness to (self-)annotation noise and distribution shifts. SGPO guarantees $O(ε)$-bounded regret, unlike Direct Preference Optimization (DPO), which suffers from linear regret growth in the distribution mismatch. We instantiate SGPO with the Stackelberg Self-Annotated Preference Optimization (SSAPO) algorithm, which iteratively self-annotates preferences and adversarially reweights synthetic annotated preferences. Using only 2K seed preferences, from the UltraFeedback dataset, i.e., 1/30 of human labels in the dataset, our method achieves 35.82% GPT-4 win-rate with Mistral-7B and 40.12% with Llama3-8B-Instruct within three rounds of SSAPO.

### The Built-In Robustness of Decentralized Federated Averaging to Bad Data 
[[arxiv](https://arxiv.org/abs/2502.18097)] [[cool](https://papers.cool/arxiv/2502.18097)] [[pdf](https://arxiv.org/pdf/2502.18097)]
> **Authors**: Samuele Sabella,Chiara Boldrini,Lorenzo Valerio,Andrea Passarella,Marco Conti
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Funding: SoBigData PPP (101079043), SoBigData.it (PNRR IR0000013), FAIR (PNRR PE00000013), RESTART (PNRR PE00000001)
- **标题**: None
- **领域**: 机器学习,人工智能,分布式、并行和集群计算
- **Abstract**: Decentralized federated learning (DFL) enables devices to collaboratively train models over complex network topologies without relying on a central controller. In this setting, local data remains private, but its quality and quantity can vary significantly across nodes. The extent to which a fully decentralized system is vulnerable to poor-quality or corrupted data remains unclear, but several factors could contribute to potential risks. Without a central authority, there can be no unified mechanism to detect or correct errors, and each node operates with a localized view of the data distribution, making it difficult for the node to assess whether its perspective aligns with the true distribution. Moreover, models trained on low-quality data can propagate through the network, amplifying errors. To explore the impact of low-quality data on DFL, we simulate two scenarios with degraded data quality -- one where the corrupted data is evenly distributed in a subset of nodes and one where it is concentrated on a single node -- using a decentralized implementation of FedAvg. Our results reveal that averaging-based decentralized learning is remarkably robust to localized bad data, even when the corrupted data resides in the most influential nodes of the network. Counterintuitively, this robustness is further enhanced when the corrupted data is concentrated on a single node, regardless of its centrality in the communication network topology. This phenomenon is explained by the averaging process, which ensures that no single node -- however central -- can disproportionately influence the overall learning process.

### HEROS-GAN: Honed-Energy Regularized and Optimal Supervised GAN for Enhancing Accuracy and Range of Low-Cost Accelerometers 
[[arxiv](https://arxiv.org/abs/2502.18064)] [[cool](https://papers.cool/arxiv/2502.18064)] [[pdf](https://arxiv.org/pdf/2502.18064)]
> **Authors**: Yifeng Wang,Yi Zhao
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: AAAI Oral;AIfor Sensors; GenerativeDeepLearning
- **标题**: None
- **领域**: 机器学习,人工智能,机器人技术,信号处理,可能性
- **Abstract**: Low-cost accelerometers play a crucial role in modern society due to their advantages of small size, ease of integration, wearability, and mass production, making them widely applicable in automotive systems, aerospace, and wearable technology. However, this widely used sensor suffers from severe accuracy and range limitations. To this end, we propose a honed-energy regularized and optimal supervised GAN (HEROS-GAN), which transforms low-cost sensor signals into high-cost equivalents, thereby overcoming the precision and range limitations of low-cost accelerometers. Due to the lack of frame-level paired low-cost and high-cost signals for training, we propose an Optimal Transport Supervision (OTS), which leverages optimal transport theory to explore potential consistency between unpaired data, thereby maximizing supervisory information. Moreover, we propose a Modulated Laplace Energy (MLE), which injects appropriate energy into the generator to encourage it to break range limitations, enhance local changes, and enrich signal details. Given the absence of a dedicated dataset, we specifically establish a Low-cost Accelerometer Signal Enhancement Dataset (LASED) containing tens of thousands of samples, which is the first dataset serving to improve the accuracy and range of accelerometers and is released in Github. Experimental results demonstrate that a GAN combined with either OTS or MLE alone can surpass the previous signal enhancement SOTA methods by an order of magnitude. Integrating both OTS and MLE, the HEROS-GAN achieves remarkable results, which doubles the accelerometer range while reducing signal noise by two orders of magnitude, establishing a benchmark in the accelerometer signal processing.

### A Market for Accuracy: Classification under Competition 
[[arxiv](https://arxiv.org/abs/2502.18052)] [[cool](https://papers.cool/arxiv/2502.18052)] [[pdf](https://arxiv.org/pdf/2502.18052)]
> **Authors**: Ohad Einav,Nir Rosenfeld
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 26 pages
- **标题**: None
- **领域**: 机器学习,计算机科学与博弈论
- **Abstract**: Machine learning models play a key role for service providers looking to gain market share in consumer markets. However, traditional learning approaches do not take into account the existence of additional providers, who compete with each other for consumers. Our work aims to study learning in this market setting, as it affects providers, consumers, and the market itself. We begin by analyzing such markets through the lens of the learning objective, and show that accuracy cannot be the only consideration. We then propose a method for classification under competition, so that a learner can maximize market share in the presence of competitors. We show that our approach benefits the providers as well as the consumers, and find that the timing of market entry and model updates can be crucial. We display the effectiveness of our approach across a range of domains, from simple distributions to noisy datasets, and show that the market as a whole remains stable by converging quickly to an equilibrium.

### Enhancing 5G O-RAN Communication Efficiency Through AI-Based Latency Forecasting 
[[arxiv](https://arxiv.org/abs/2502.18046)] [[cool](https://papers.cool/arxiv/2502.18046)] [[pdf](https://arxiv.org/pdf/2502.18046)]
> **Authors**: Raúl Parada,Ebrahim Abu-Helalah,Jordi Serra,Anton Aguilar,Paolo Dini
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The increasing complexity and dynamic nature of 5G open radio access networks (O-RAN) pose significant challenges to maintaining low latency, high throughput, and resource efficiency. While existing methods leverage machine learning for latency prediction and resource management, they often lack real-world scalability and hardware validation. This paper addresses these limitations by presenting an artificial intelligence-driven latency forecasting system integrated into a functional O-RAN prototype. The system uses a bidirectional long short-term memory model to predict latency in real time within a scalable, open-source framework built with FlexRIC. Experimental results demonstrate the model's efficacy, achieving a loss metric below 0.04, thus validating its applicability in dynamic 5G environments.

### ExPath: Towards Explaining Targeted Pathways for Biological Knowledge Bases 
[[arxiv](https://arxiv.org/abs/2502.18026)] [[cool](https://papers.cool/arxiv/2502.18026)] [[pdf](https://arxiv.org/pdf/2502.18026)]
> **Authors**: Rikuto Kotoge,Ziwei Yang,Zheng Chen,Yushun Dong,Yasuko Matsubara,Jimeng Sun,Yasushi Sakurai
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Under review
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Biological knowledge bases provide systemically functional pathways of cells or organisms in terms of molecular interaction. However, recognizing more targeted pathways, particularly when incorporating wet-lab experimental data, remains challenging and typically requires downstream biological analyses and expertise. In this paper, we frame this challenge as a solvable graph learning and explaining task and propose a novel pathway inference framework, ExPath, that explicitly integrates experimental data, specifically amino acid sequences (AA-seqs), to classify various graphs (bio-networks) in biological databases. The links (representing pathways) that contribute more to classification can be considered as targeted pathways. Technically, ExPath comprises three components: (1) a large protein language model (pLM) that encodes and embeds AA-seqs into graph, overcoming traditional obstacles in processing AA-seq data, such as BLAST; (2) PathMamba, a hybrid architecture combining graph neural networks (GNNs) with state-space sequence modeling (Mamba) to capture both local interactions and global pathway-level dependencies; and (3) PathExplainer, a subgraph learning module that identifies functionally critical nodes and edges through trainable pathway masks. We also propose ML-oriented biological evaluations and a new metric. The experiments involving 301 bio-networks evaluations demonstrate that pathways inferred by ExPath maintain biological meaningfulness. We will publicly release curated 301 bio-network data soon.

### Patient Trajectory Prediction: Integrating Clinical Notes with Transformers 
[[arxiv](https://arxiv.org/abs/2502.18009)] [[cool](https://papers.cool/arxiv/2502.18009)] [[pdf](https://arxiv.org/pdf/2502.18009)]
> **Authors**: Sifal Klioui,Sana Sellami,Youssef Trardi
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Predicting disease trajectories from electronic health records (EHRs) is a complex task due to major challenges such as data non-stationarity, high granularity of medical codes, and integration of multimodal data. EHRs contain both structured data, such as diagnostic codes, and unstructured data, such as clinical notes, which hold essential information often overlooked. Current models, primarily based on structured data, struggle to capture the complete medical context of patients, resulting in a loss of valuable information. To address this issue, we propose an approach that integrates unstructured clinical notes into transformer-based deep learning models for sequential disease prediction. This integration enriches the representation of patients' medical histories, thereby improving the accuracy of diagnosis predictions. Experiments on MIMIC-IV datasets demonstrate that the proposed approach outperforms traditional models relying solely on structured data.

### Radon-Nikodým Derivative: Re-imagining Anomaly Detection from a Measure Theoretic Perspective 
[[arxiv](https://arxiv.org/abs/2502.18002)] [[cool](https://papers.cool/arxiv/2502.18002)] [[pdf](https://arxiv.org/pdf/2502.18002)]
> **Authors**: Shlok Mehendale,Aditya Challa,Rahul Yedida,Sravan Danda,Santonu Sarkar,Snehanshu Saha
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Which principle underpins the design of an effective anomaly detection loss function? The answer lies in the concept of \rnthm{} theorem, a fundamental concept in measure theory. The key insight is -- Multiplying the vanilla loss function with the \rnthm{} derivative improves the performance across the board. We refer to this as RN-Loss. This is established using PAC learnability of anomaly detection. We further show that the \rnthm{} derivative offers important insights into unsupervised clustering based anomaly detections as well. We evaluate our algorithm on 96 datasets, including univariate and multivariate data from diverse domains, including healthcare, cybersecurity, and finance. We show that RN-Derivative algorithms outperform state-of-the-art methods on 68\% of Multivariate datasets (based on F-1 scores) and also achieves peak F1-scores on 72\% of time series (Univariate) datasets.

### A Perspective on Symbolic Machine Learning in Physical Sciences 
[[arxiv](https://arxiv.org/abs/2502.17993)] [[cool](https://papers.cool/arxiv/2502.17993)] [[pdf](https://arxiv.org/pdf/2502.17993)]
> **Authors**: Nour Makke,Sanjay Chawla
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: MachineLearningand the Physical Sciences Workshop at NeurIPS 2024
- **标题**: None
- **领域**: 机器学习,高能物理-现象学,高能物理 - 理论
- **Abstract**: Machine learning is rapidly making its pathway across all of the natural sciences, including physical sciences. The rate at which ML is impacting non-scientific disciplines is incomparable to that in the physical sciences. This is partly due to the uninterpretable nature of deep neural networks. Symbolic machine learning stands as an equal and complementary partner to numerical machine learning in speeding up scientific discovery in physics. This perspective discusses the main differences between the ML and scientific approaches. It stresses the need to develop and apply symbolic machine learning to physics problems equally, in parallel to numerical machine learning, because of the dual nature of physics research.

### Broadening Discovery through Structural Models: Multimodal Combination of Local and Structural Properties for Predicting Chemical Features 
[[arxiv](https://arxiv.org/abs/2502.17986)] [[cool](https://papers.cool/arxiv/2502.17986)] [[pdf](https://arxiv.org/pdf/2502.17986)]
> **Authors**: Nikolai Rekut,Alexey Orlov,Klea Ziu,Elizaveta Starykh,Martin Takac,Aleksandr Beznosikov
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: In recent years, machine learning has profoundly reshaped the field of chemistry, facilitating significant advancements across various applications, including the prediction of molecular properties and the generation of molecular structures. Language models and graph-based models are extensively utilized within this domain, consistently achieving state-of-the-art results across an array of tasks. However, the prevailing practice of representing chemical compounds in the SMILES format -- used by most datasets and many language models -- presents notable limitations as a training data format. In contrast, chemical fingerprints offer a more physically informed representation of compounds, thereby enhancing their suitability for model training. This study aims to develop a language model that is specifically trained on fingerprints. Furthermore, we introduce a bimodal architecture that integrates this language model with a graph model. Our proposed methodology synthesizes these approaches, utilizing RoBERTa as the language model and employing Graph Isomorphism Networks (GIN), Graph Convolutional Networks (GCN) and Graphormer as graph models. This integration results in a significant improvement in predictive performance compared to conventional strategies for tasks such as Quantitative Structure-Activity Relationship (QSAR) and the prediction of nuclear magnetic resonance (NMR) spectra, among others.

### Generalized Decision Focused Learning under Imprecise Uncertainty--Theoretical Study 
[[arxiv](https://arxiv.org/abs/2502.17984)] [[cool](https://papers.cool/arxiv/2502.17984)] [[pdf](https://arxiv.org/pdf/2502.17984)]
> **Authors**: Keivan Shariatmadar,Neil Yorke-Smith,Ahmad Osman,Fabio Cuzzolin,Hans Hallez,David Moens
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 13 pages
- **标题**: None
- **领域**: 机器学习,优化与控制,可能性
- **Abstract**: Decision Focused Learning has emerged as a critical paradigm for integrating machine learning with downstream optimisation. Despite its promise, existing methodologies predominantly rely on probabilistic models and focus narrowly on task objectives, overlooking the nuanced challenges posed by epistemic uncertainty, non-probabilistic modelling approaches, and the integration of uncertainty into optimisation constraints. This paper bridges these gaps by introducing innovative frameworks: (i) a non-probabilistic lens for epistemic uncertainty representation, leveraging intervals (the least informative uncertainty model), Contamination (hybrid model), and probability boxes (the most informative uncertainty model); (ii) methodologies to incorporate uncertainty into constraints, expanding Decision-Focused Learning's utility in constrained environments; (iii) the adoption of Imprecise Decision Theory for ambiguity-rich decision-making contexts; and (iv) strategies for addressing sparse data challenges. Empirical evaluations on benchmark optimisation problems demonstrate the efficacy of these approaches in improving decision quality and robustness and dealing with said gaps.

### Provable Performance Bounds for Digital Twin-driven Deep Reinforcement Learning in Wireless Networks: A Novel Digital-Twin Bisimulation Metric 
[[arxiv](https://arxiv.org/abs/2502.17983)] [[cool](https://papers.cool/arxiv/2502.17983)] [[pdf](https://arxiv.org/pdf/2502.17983)]
> **Authors**: Zhenyu Tao,Wei Xu,Xiaohu You
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Digital twin (DT)-driven deep reinforcement learning (DRL) has emerged as a promising paradigm for wireless network optimization, offering safe and efficient training environment for policy exploration. However, in theory existing methods cannot always guarantee real-world performance of DT-trained policies before actual deployment, due to the absence of a universal metric for assessing DT's ability to support reliable DRL training transferrable to physical networks. In this paper, we propose the DT bisimulation metric (DT-BSM), a novel metric based on the Wasserstein distance, to quantify the discrepancy between Markov decision processes (MDPs) in both the DT and the corresponding real-world wireless network environment. We prove that for any DT-trained policy, the sub-optimality of its performance (regret) in the real-world deployment is bounded by a weighted sum of the DT-BSM and its sub-optimality within the MDP in the DT. Then, a modified DT-BSM based on the total variation distance is also introduced to avoid the prohibitive calculation complexity of Wasserstein distance for large-scale wireless network scenarios. Further, to tackle the challenge of obtaining accurate transition probabilities of the MDP in real world for the DT-BSM calculation, we propose an empirical DT-BSM method based on statistical sampling. We prove that the empirical DT-BSM always converges to the desired theoretical one, and quantitatively establish the relationship between the required sample size and the target level of approximation accuracy. Numerical experiments validate this first theoretical finding on the provable and calculable performance bounds for DT-driven DRL.

### XGBoost-Based Prediction of ICU Mortality in Sepsis-Associated Acute Kidney Injury Patients Using MIMIC-IV Database with Validation from eICU Database 
[[arxiv](https://arxiv.org/abs/2502.17978)] [[cool](https://papers.cool/arxiv/2502.17978)] [[pdf](https://arxiv.org/pdf/2502.17978)]
> **Authors**: Shuheng Chen,Junyi Fan,Elham Pishgar,Kamiar Alaei,Greg Placencia,Maryam Pishgar
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Background: Sepsis-Associated Acute Kidney Injury (SA-AKI) leads to high mortality in intensive care. This study develops machine learning models using the Medical Information Mart for Intensive Care IV (MIMIC-IV) database to predict Intensive Care Unit (ICU) mortality in SA-AKI patients. External validation is conducted using the eICU Collaborative Research Database. Methods: For 9,474 identified SA-AKI patients in MIMIC-IV, key features like lab results, vital signs, and comorbidities were selected using Variance Inflation Factor (VIF), Recursive Feature Elimination (RFE), and expert input, narrowing to 24 predictive variables. An Extreme Gradient Boosting (XGBoost) model was built for in-hospital mortality prediction, with hyperparameters optimized using GridSearch. Model interpretability was enhanced with SHapley Additive exPlanations (SHAP) and Local Interpretable Model-agnostic Explanations (LIME). External validation was conducted using the eICU database. Results: The proposed XGBoost model achieved an internal Area Under the Receiver Operating Characteristic curve (AUROC) of 0.878 (95% Confidence Interval: 0.859-0.897). SHAP identified Sequential Organ Failure Assessment (SOFA), serum lactate, and respiratory rate as key mortality predictors. LIME highlighted serum lactate, Acute Physiology and Chronic Health Evaluation II (APACHE II) score, total urine output, and serum calcium as critical features. Conclusions: The integration of advanced techniques with the XGBoost algorithm yielded a highly accurate and interpretable model for predicting SA-AKI mortality across diverse populations. It supports early identification of high-risk patients, enhancing clinical decision-making in intensive care. Future work needs to focus on enhancing adaptability, versatility, and real-world applications.

### Model-Free Adversarial Purification via Coarse-To-Fine Tensor Network Representation 
[[arxiv](https://arxiv.org/abs/2502.17972)] [[cool](https://papers.cool/arxiv/2502.17972)] [[pdf](https://arxiv.org/pdf/2502.17972)]
> **Authors**: Guang Lin,Duc Thien Nguyen,Zerui Tao,Konstantinos Slavakis,Toshihisa Tanaka,Qibin Zhao
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Deep neural networks are known to be vulnerable to well-designed adversarial attacks. Although numerous defense strategies have been proposed, many are tailored to the specific attacks or tasks and often fail to generalize across diverse scenarios. In this paper, we propose Tensor Network Purification (TNP), a novel model-free adversarial purification method by a specially designed tensor network decomposition algorithm. TNP depends neither on the pre-trained generative model nor the specific dataset, resulting in strong robustness across diverse adversarial scenarios. To this end, the key challenge lies in relaxing Gaussian-noise assumptions of classical decompositions and accommodating the unknown distribution of adversarial perturbations. Unlike the low-rank representation of classical decompositions, TNP aims to reconstruct the unobserved clean examples from an adversarial example. Specifically, TNP leverages progressive downsampling and introduces a novel adversarial optimization objective to address the challenge of minimizing reconstruction error but without inadvertently restoring adversarial perturbations. Extensive experiments conducted on CIFAR-10, CIFAR-100, and ImageNet demonstrate that our method generalizes effectively across various norm threats, attack types, and tasks, providing a versatile and promising adversarial purification technique.

### LLM Knows Geometry Better than Algebra: Numerical Understanding of LLM-Based Agents in A Trading Arena 
[[arxiv](https://arxiv.org/abs/2502.17967)] [[cool](https://papers.cool/arxiv/2502.17967)] [[pdf](https://arxiv.org/pdf/2502.17967)]
> **Authors**: Tianmi Ma,Jiawei Du,Wenxin Huang,Wenjie Wang,Liang Xie,Xian Zhong,Joey Tianyi Zhou
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,多代理系统,统计金融
- **Abstract**: Recent advancements in large language models (LLMs) have significantly improved performance in natural language processing tasks. However, their ability to generalize to dynamic, unseen tasks, particularly in numerical reasoning, remains a challenge. Existing benchmarks mainly evaluate LLMs on problems with predefined optimal solutions, which may not align with real-world scenarios where clear answers are absent. To bridge this gap, we design the Agent Trading Arena, a virtual numerical game simulating complex economic systems through zero-sum games, where agents invest in stock portfolios. Our experiments reveal that LLMs, including GPT-4o, struggle with algebraic reasoning when dealing with plain-text stock data, often focusing on local details rather than global trends. In contrast, LLMs perform significantly better with geometric reasoning when presented with visual data, such as scatter plots or K-line charts, suggesting that visual representations enhance numerical reasoning. This capability is further improved by incorporating the reflection module, which aids in the analysis and interpretation of complex data. We validate our findings on NASDAQ Stock dataset, where LLMs demonstrate stronger reasoning with visual data compared to text. Our code and data are publicly available at https://github.com/wekjsdvnm/Agent-Trading-Arena.git.

### Late Breaking Results: The Art of Beating the Odds with Predictor-Guided Random Design Space Exploration 
[[arxiv](https://arxiv.org/abs/2502.17936)] [[cool](https://papers.cool/arxiv/2502.17936)] [[pdf](https://arxiv.org/pdf/2502.17936)]
> **Authors**: Felix Arnold,Maxence Bouvier,Ryan Amaudruz,Renzo Andri,Lukas Cavigelli
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 2 pages, 3 figures, conference, this research manuscript is currently under review for publication in an IEEE conference
- **标题**: None
- **领域**: 机器学习,硬件架构
- **Abstract**: This work introduces an innovative method for improving combinational digital circuits through random exploration in MIG-based synthesis. High-quality circuits are crucial for performance, power, and cost, making this a critical area of active research. Our approach incorporates next-state prediction and iterative selection, significantly accelerating the synthesis process. This novel method achieves up to 14x synthesis speedup and up to 20.94% better MIG minimization on the EPFL Combinational Benchmark Suite compared to state-of-the-art techniques. We further explore various predictor models and show that increased prediction accuracy does not guarantee an equivalent increase in synthesis quality of results or speedup, observing that randomness remains a desirable factor.

### Integrating Boosted learning with Differential Evolution (DE) Optimizer: A Prediction of Groundwater Quality Risk Assessment in Odisha 
[[arxiv](https://arxiv.org/abs/2502.17929)] [[cool](https://papers.cool/arxiv/2502.17929)] [[pdf](https://arxiv.org/pdf/2502.17929)]
> **Authors**: Sonalika Subudhi,Alok Kumar Pati,Sephali Bose,Subhasmita Sahoo,Avipsa Pattanaik,Biswa Mohan Acharya
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 9 Figures (8 figs in paper and one additional graphical abstract), 9 Tables
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Groundwater is eventually undermined by human exercises, such as fast industrialization, urbanization, over-extraction, and contamination from agrarian and urban sources. From among the different contaminants, the presence of heavy metals like cadmium (Cd), chromium (Cr), arsenic (As), and lead (Pb) proves to have serious dangers when present in huge concentrations in groundwater. Long-term usage of these poisonous components may lead to neurological disorders, kidney failure and different sorts of cancer. To address these issues, this study developed a machine learning-based predictive model to evaluate the Groundwater Quality Index (GWQI) and identify the main contaminants which are affecting the water quality. It has been achieved with the help of a hybrid machine learning model i.e. LCBoost Fusion . The model has undergone several processes like data preprocessing, hyperparameter tuning using Differential Evolution (DE) optimization, and evaluation through cross-validation. The LCBoost Fusion model outperforms individual models (CatBoost and LightGBM), by achieving low RMSE (0.6829), MSE (0.5102), MAE (0.3147) and a high R$^2$ score of 0.9809. Feature importance analysis highlights Potassium (K), Fluoride (F) and Total Hardness (TH) as the most influential indicators of groundwater contamination. This research successfully demonstrates the application of machine learning in assessing groundwater quality risks in Odisha. The proposed LCBoost Fusion model offers a reliable and efficient approach for real-time groundwater monitoring and risk mitigation. These findings will help the environmental organizations and the policy makers to map out targeted places for sustainable groundwater management. Future work will focus on using remote sensing data and developing an interactive decision-making system for groundwater quality assessment.

### Techniques for Enhancing Memory Capacity of Reservoir Computing 
[[arxiv](https://arxiv.org/abs/2502.17923)] [[cool](https://papers.cool/arxiv/2502.17923)] [[pdf](https://arxiv.org/pdf/2502.17923)]
> **Authors**: Atsuki Yokota,Ichiro Kawashima,Yohei Saito,Hakaru Tamukoh,Osamu Nomura,Takashi Morie
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Reservoir Computing (RC) is a bio-inspired machine learning framework, and various models have been proposed. RC is a well-suited model for time series data processing, but there is a trade-off between memory capacity and nonlinearity. In this study, we propose methods to improve the memory capacity of reservoir models by modifying their network configuration except for the inside of reservoirs. The Delay method retains past inputs by adding delay node chains to the input layer with the specified number of delay steps. To suppress the effect of input value increase due to the Delay method, we divide the input weights by the number of added delay steps. The Pass through method feeds input values directly to the output layer. The Clustering method divides the input and reservoir nodes into multiple parts and integrates them at the output layer. We applied these methods to an echo state network (ESN), a typical RC model, and the chaotic Boltzmann machine (CBM)-RC, which can be efficiently implemented in integrated circuits. We evaluated their performance on the NARMA task, and measured information processing capacity (IPC) to evaluate the trade-off between memory capacity and nonlinearity.

### C-LoRA: Continual Low-Rank Adaptation for Pre-trained Models 
[[arxiv](https://arxiv.org/abs/2502.17920)] [[cool](https://papers.cool/arxiv/2502.17920)] [[pdf](https://arxiv.org/pdf/2502.17920)]
> **Authors**: Xin Zhang,Liang Bai,Xian Yang,Jiye Liang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Low-Rank Adaptation (LoRA) is an efficient fine-tuning method that has been extensively applied in areas such as natural language processing and computer vision. Existing LoRA fine-tuning approaches excel in static environments but struggle in dynamic learning due to reliance on multiple adapter modules, increasing overhead and complicating inference. We propose Continual Low-Rank Adaptation (C-LoRA), a novel extension of LoRA for continual learning. C-LoRA uses a learnable routing matrix to dynamically manage parameter updates across tasks, ensuring efficient reuse of learned subspaces while enforcing orthogonality to minimize interference and forgetting. Unlike existing approaches that require separate adapters for each task, C-LoRA enables a integrated approach for task adaptation, achieving both scalability and parameter efficiency in sequential learning scenarios. C-LoRA achieves state-of-the-art accuracy and parameter efficiency on benchmarks while providing theoretical insights into its routing matrix's role in retaining and transferring knowledge, establishing a scalable framework for continual learning.

### AirCast: Improving Air Pollution Forecasting Through Multi-Variable Data Alignment 
[[arxiv](https://arxiv.org/abs/2502.17919)] [[cool](https://papers.cool/arxiv/2502.17919)] [[pdf](https://arxiv.org/pdf/2502.17919)]
> **Authors**: Vishal Nedungadi,Muhammad Akhtar Munir,Marc Rußwurm,Ron Sarafian,Ioannis N. Athanasiadis,Yinon Rudich,Fahad Shahbaz Khan,Salman Khan
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: Air pollution remains a leading global health risk, exacerbated by rapid industrialization and urbanization, contributing significantly to morbidity and mortality rates. In this paper, we introduce AirCast, a novel multi-variable air pollution forecasting model, by combining weather and air quality variables. AirCast employs a multi-task head architecture that simultaneously forecasts atmospheric conditions and pollutant concentrations, improving its understanding of how weather patterns affect air quality. Predicting extreme pollution events is challenging due to their rare occurrence in historic data, resulting in a heavy-tailed distribution of pollution levels. To address this, we propose a novel Frequency-weighted Mean Absolute Error (fMAE) loss, adapted from the class-balanced loss for regression tasks. Informed from domain knowledge, we investigate the selection of key variables known to influence pollution levels. Additionally, we align existing weather and chemical datasets across spatial and temporal dimensions. AirCast's integrated approach, combining multi-task learning, frequency weighted loss and domain informed variable selection, enables more accurate pollution forecasts. Our source code and models are made public here (https://github.com/vishalned/AirCast.git)

### Batch normalization does not improve initialization 
[[arxiv](https://arxiv.org/abs/2502.17913)] [[cool](https://papers.cool/arxiv/2502.17913)] [[pdf](https://arxiv.org/pdf/2502.17913)]
> **Authors**: Joris Dannemann,Gero Junike
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,可能性
- **Abstract**: Batch normalization is one of the most important regularization techniques for neural networks, significantly improving training by centering the layers of the neural network. There have been several attempts to provide a theoretical justification for batch ormalization. Santurkar and Tsipras (2018) [How does batch normalization help optimization? Advances in neural information rocessing systems, 31] claim that batch normalization improves initialization. We provide a counterexample showing that this claim s not true, i.e., batch normalization does not improve initialization.

### Decoupled Graph Energy-based Model for Node Out-of-Distribution Detection on Heterophilic Graphs 
[[arxiv](https://arxiv.org/abs/2502.17912)] [[cool](https://papers.cool/arxiv/2502.17912)] [[pdf](https://arxiv.org/pdf/2502.17912)]
> **Authors**: Yuhan Chen,Yihong Luo,Yifan Song,Pengwen Dai,Jing Tang,Xiaochun Cao
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: The first two authors contributed equally to this work; ICLR 2025
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Despite extensive research efforts focused on OOD detection on images, OOD detection on nodes in graph learning remains underexplored. The dependence among graph nodes hinders the trivial adaptation of existing approaches on images that assume inputs to be i.i.d. sampled, since many unique features and challenges specific to graphs are not considered, such as the heterophily issue. Recently, GNNSafe, which considers node dependence, adapted energy-based detection to the graph domain with state-of-the-art performance, however, it has two serious issues: 1) it derives node energy from classification logits without specifically tailored training for modeling data distribution, making it less effective at recognizing OOD data; 2) it highly relies on energy propagation, which is based on homophily assumption and will cause significant performance degradation on heterophilic graphs, where the node tends to have dissimilar distribution with its neighbors. To address the above issues, we suggest training EBMs by MLE to enhance data distribution modeling and remove energy propagation to overcome the heterophily issues. However, training EBMs via MLE requires performing MCMC sampling on both node feature and node neighbors, which is challenging due to the node interdependence and discrete graph topology. To tackle the sampling challenge, we introduce DeGEM, which decomposes the learning process into two parts: a graph encoder that leverages topology information for node representations and an energy head that operates in latent space. Extensive experiments validate that DeGEM, without OOD exposure during training, surpasses previous state-of-the-art methods, achieving an average AUROC improvement of 6.71% on homophilic graphs and 20.29% on heterophilic graphs, and even outperform methods trained with OOD exposure. Our code is available at: https://github.com/draym28/DeGEM.

### Knowledge-enhanced Multimodal ECG Representation Learning with Arbitrary-Lead Inputs 
[[arxiv](https://arxiv.org/abs/2502.17900)] [[cool](https://papers.cool/arxiv/2502.17900)] [[pdf](https://arxiv.org/pdf/2502.17900)]
> **Authors**: Che Liu,Cheng Ouyang,Zhongwei Wan,Haozhe Wang,Wenjia Bai,Rossella Arcucci
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Recent advances in multimodal ECG representation learning center on aligning ECG signals with paired free-text reports. However, suboptimal alignment persists due to the complexity of medical language and the reliance on a full 12-lead setup, which is often unavailable in under-resourced settings. To tackle these issues, we propose **K-MERL**, a knowledge-enhanced multimodal ECG representation learning framework. **K-MERL** leverages large language models to extract structured knowledge from free-text reports and employs a lead-aware ECG encoder with dynamic lead masking to accommodate arbitrary lead inputs. Evaluations on six external ECG datasets show that **K-MERL** achieves state-of-the-art performance in zero-shot classification and linear probing tasks, while delivering an average **16%** AUC improvement over existing methods in partial-lead zero-shot classification.

### Arrhythmia Classification from 12-Lead ECG Signals Using Convolutional and Transformer-Based Deep Learning Models 
[[arxiv](https://arxiv.org/abs/2502.17887)] [[cool](https://papers.cool/arxiv/2502.17887)] [[pdf](https://arxiv.org/pdf/2502.17887)]
> **Authors**: Andrei Apostol,Maria Nutu
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 34 pages, 17 figures
- **标题**: None
- **领域**: 机器学习,人工智能,信号处理
- **Abstract**: In Romania, cardiovascular problems are the leading cause of death, accounting for nearly one-third of annual fatalities. The severity of this situation calls for innovative diagnosis method for cardiovascular diseases. This article aims to explore efficient, light-weight and rapid methods for arrhythmia diagnosis, in resource-constrained healthcare settings. Due to the lack of Romanian public medical data, we trained our systems using international public datasets, having in mind that the ECG signals are the same regardless the patients' nationality. Within this purpose, we combined multiple datasets, usually used in the field of arrhythmias classification: PTB-XL electrocardiography dataset , PTB Diagnostic ECG Database, China 12-Lead ECG Challenge Database, Georgia 12-Lead ECG Challenge Database, and St. Petersburg INCART 12-lead Arrhythmia Database. For the input data, we employed ECG signal processing methods, specifically a variant of the Pan-Tompkins algorithm, useful in arrhythmia classification because it provides a robust and efficient method for detecting QRS complexes in ECG signals. Additionally, we used machine learning techniques, widely used for the task of classification, including convolutional neural networks (1D CNNs, 2D CNNs, ResNet) and Vision Transformers (ViTs). The systems were evaluated in terms of accuracy and F1 score. We annalysed our dataset from two perspectives. First, we fed the systems with the ECG signals and the GRU-based 1D CNN model achieved the highest accuracy of 93.4% among all the tested architectures. Secondly, we transformed ECG signals into images and the CNN2D model achieved an accuracy of 92.16%.

### Neural Graph Matching Improves Retrieval Augmented Generation in Molecular Machine Learning 
[[arxiv](https://arxiv.org/abs/2502.17874)] [[cool](https://papers.cool/arxiv/2502.17874)] [[pdf](https://arxiv.org/pdf/2502.17874)]
> **Authors**: Runzhong Wang,Rui-Xi Wang,Mrunali Manjrekar,Connor W. Coley
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,定量方法
- **Abstract**: Molecular machine learning has gained popularity with the advancements of geometric deep learning. In parallel, retrieval-augmented generation has become a principled approach commonly used with language models. However, the optimal integration of retrieval augmentation into molecular machine learning remains unclear. Graph neural networks stand to benefit from clever matching to understand the structural alignment of retrieved molecules to a query molecule. Neural graph matching offers a compelling solution by explicitly modeling node and edge affinities between two structural graphs while employing a noise-robust, end-to-end neural network to learn affinity metrics. We apply this approach to mass spectrum simulation and introduce MARASON, a novel model that incorporates neural graph matching to enhance a fragmentation-based neural network. Experimental results highlight the effectiveness of our design, with MARASON achieving 28% top-1 accuracy, a substantial improvement over the non-retrieval state-of-the-art accuracy of 19%. Moreover, MARASON outperforms both naive retrieval-augmented generation methods and traditional graph matching approaches.

### EEGM2: An Efficient Mamba-2-Based Self-Supervised Framework for Long-Sequence EEG Modeling 
[[arxiv](https://arxiv.org/abs/2502.17873)] [[cool](https://papers.cool/arxiv/2502.17873)] [[pdf](https://arxiv.org/pdf/2502.17873)]
> **Authors**: Jiazhen Hong,Geoffrey Mackellar,Soheila Ghane
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 10 pages, 7 figures
- **标题**: None
- **领域**: 机器学习,信号处理
- **Abstract**: Deep learning has achieved significant progress in the development of electroencephalogram (EEG) foundation models, with Transformer-based architectures excelling at capturing long-range dependencies. However, their quadratic computational complexity presents challenges in memory efficiency, training, and inference speed, limiting their scalability and generalizability as a foundation model. In this paper, we propose EEGM2, a self-supervised framework based on structured state space duality (SSD) that overcomes these limitations. EEGM2 introduces three key innovations: (1) a reconstruction-based framework that captures both local and global EEG features through Mamba-2 structured state space models, (2) a spatiotemporal-aware loss function that enhances robustness to noise and preserves spectral information, and (3) a multi-branch receptive field input embedding strategy that improves cross-subject generalization and stability for EEG sequences of varying lengths. In comparison to traditional pretraining methods, on raw EEG or latent representation spaces, EEGM2 shows superior performance on long-sequence tasks, where conventional models struggle. Our experimental results on six EEG datasets validate that EEGM2 not only achieves state-of-the-art cross-domain accuracy but also reduces computational overhead, making it a more efficient solution for deployment on resource-constrained BCI devices.

### Contrastive Learning with Nasty Noise 
[[arxiv](https://arxiv.org/abs/2502.17872)] [[cool](https://papers.cool/arxiv/2502.17872)] [[pdf](https://arxiv.org/pdf/2502.17872)]
> **Authors**: Ziruo Zhao
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Contrastive learning has emerged as a powerful paradigm for self-supervised representation learning. This work analyzes the theoretical limits of contrastive learning under nasty noise, where an adversary modifies or replaces training samples. Using PAC learning and VC-dimension analysis, lower and upper bounds on sample complexity in adversarial settings are established. Additionally, data-dependent sample complexity bounds based on the l2-distance function are derived.

### Mitigating Attrition: Data-Driven Approach Using Machine Learning and Data Engineering 
[[arxiv](https://arxiv.org/abs/2502.17865)] [[cool](https://papers.cool/arxiv/2502.17865)] [[pdf](https://arxiv.org/pdf/2502.17865)]
> **Authors**: Naveen Edapurath Vijayan
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 7 pages
- **标题**: None
- **领域**: 机器学习,计算机与社会,软件工程
- **Abstract**: This paper presents a novel data-driven approach to mitigating employee attrition using machine learning and data engineering techniques. The proposed framework integrates data from various human resources systems and leverages advanced feature engineering to capture a comprehensive set of factors influencing attrition. The study outlines a robust modeling approach that addresses challenges such as imbalanced datasets, categorical data handling, and model interpretation. The methodology includes careful consideration of training and testing strategies, baseline model establishment, and the development of calibrated predictive models. The research emphasizes the importance of model interpretation using techniques like SHAP values to provide actionable insights for organizations. Key design choices in algorithm selection, hyperparameter tuning, and probability calibration are discussed. This approach enables organizations to proactively identify attrition risks and develop targeted retention strategies, ultimately redu

## 多代理系统(cs.MA:Multiagent Systems)

### MA-GTS: A Multi-Agent Framework for Solving Complex Graph Problems in Real-World Applications 
[[arxiv](https://arxiv.org/abs/2502.18540)] [[cool](https://papers.cool/arxiv/2502.18540)] [[pdf](https://arxiv.org/pdf/2502.18540)]
> **Authors**: Zike Yuan,Ming Liu,Hui Wang,Bing Qin
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 多代理系统,人工智能
- **Abstract**: Graph-theoretic problems arise in real-world applications like logistics, communication networks, and traffic optimization. These problems are often complex, noisy, and irregular, posing challenges for traditional algorithms. Large language models (LLMs) offer potential solutions but face challenges, including limited accuracy and input length constraints. To address these challenges, we propose MA-GTS (Multi-Agent Graph Theory Solver), a multi-agent framework that decomposes these complex problems through agent collaboration. MA-GTS maps the implicitly expressed text-based graph data into clear, structured graph representations and dynamically selects the most suitable algorithm based on problem constraints and graph structure scale. This approach ensures that the solution process remains efficient and the resulting reasoning path is interpretable. We validate MA-GTS using the G-REAL dataset, a real-world-inspired graph theory dataset we created. Experimental results show that MA-GTS outperforms state-of-the-art approaches in terms of efficiency, accuracy, and scalability, with strong results across multiple benchmarks (G-REAL 94.2%, GraCoRe 96.9%, NLGraph 98.4%).MA-GTS is open-sourced at https://github.com/ZIKEYUAN/MA-GTS.git.

### MAFE: Multi-Agent Fair Environments for Decision-Making Systems 
[[arxiv](https://arxiv.org/abs/2502.18534)] [[cool](https://papers.cool/arxiv/2502.18534)] [[pdf](https://arxiv.org/pdf/2502.18534)]
> **Authors**: Zachary McBride Lazri,Anirudh Nakra,Ivan Brugere,Danial Dervovic,Antigoni Polychroniadou,Furong Huang,Dana Dachman-Soled,Min Wu
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 多代理系统,人工智能
- **Abstract**: Fairness constraints applied to machine learning (ML) models in static contexts have been shown to potentially produce adverse outcomes among demographic groups over time. To address this issue, emerging research focuses on creating fair solutions that persist over time. While many approaches treat this as a single-agent decision-making problem, real-world systems often consist of multiple interacting entities that influence outcomes. Explicitly modeling these entities as agents enables more flexible analysis of their interventions and the effects they have on a system's underlying dynamics. A significant challenge in conducting research on multi-agent systems is the lack of realistic environments that leverage the limited real-world data available for analysis. To address this gap, we introduce the concept of a Multi-Agent Fair Environment (MAFE) and present and analyze three MAFEs that model distinct social systems. Experimental results demonstrate the utility of our MAFEs as testbeds for developing multi-agent fair algorithms.

### Heterogeneous Decision Making in Mixed Traffic: Uncertainty-aware Planning and Bounded Rationality 
[[arxiv](https://arxiv.org/abs/2502.18529)] [[cool](https://papers.cool/arxiv/2502.18529)] [[pdf](https://arxiv.org/pdf/2502.18529)]
> **Authors**: Hang Wang,Qiaoyi Fang,Junshan Zhang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-26
> **comment**: CPAL 2025
- **标题**: None
- **领域**: 多代理系统,人工智能,人机交互
- **Abstract**: The past few years have witnessed a rapid growth of the deployment of automated vehicles (AVs). Clearly, AVs and human-driven vehicles (HVs) will co-exist for many years, and AVs will have to operate around HVs, pedestrians, cyclists, and more, calling for fundamental breakthroughs in AI designed for mixed traffic to achieve mixed autonomy. Thus motivated, we study heterogeneous decision making by AVs and HVs in a mixed traffic environment, aiming to capture the interactions between human and machine decision-making and develop an AI foundation that enables vehicles to operate safely and efficiently. There are a number of challenges to achieve mixed autonomy, including 1) humans drivers make driving decisions with bounded rationality, and it remains open to develop accurate models for HVs' decision making; and 2) uncertainty-aware planning plays a critical role for AVs to take safety maneuvers in response to the human behavior. In this paper, we introduce a formulation of AV-HV interaction, where the HV makes decisions with bounded rationality and the AV employs uncertainty-aware planning based on the prediction on HV's future actions. We conduct a comprehensive analysis on AV and HV's learning regret to answer the questions: 1) {How does the learning performance depend on HV's bounded rationality and AV's planning}; 2) {How do different decision making strategies impact the overall learning performance}? Our findings reveal some intriguing phenomena, such as Goodhart's Law in AV's learning performance and compounding effects in HV's decision making process. By examining the dynamics of the regrets, we gain insights into the interplay between human and machine decision making.

### ToMCAT: Theory-of-Mind for Cooperative Agents in Teams via Multiagent Diffusion Policies 
[[arxiv](https://arxiv.org/abs/2502.18438)] [[cool](https://papers.cool/arxiv/2502.18438)] [[pdf](https://arxiv.org/pdf/2502.18438)]
> **Authors**: Pedro Sequeira,Vidyasagar Sadhu,Melinda Gervasio
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 多代理系统,人工智能,机器学习
- **Abstract**: In this paper we present ToMCAT (Theory-of-Mind for Cooperative Agents in Teams), a new framework for generating ToM-conditioned trajectories. It combines a meta-learning mechanism, that performs ToM reasoning over teammates' underlying goals and future behavior, with a multiagent denoising-diffusion model, that generates plans for an agent and its teammates conditioned on both the agent's goals and its teammates' characteristics, as computed via ToM. We implemented an online planning system that dynamically samples new trajectories (replans) from the diffusion model whenever it detects a divergence between a previously generated plan and the current state of the world. We conducted several experiments using ToMCAT in a simulated cooking domain. Our results highlight the importance of the dynamic replanning mechanism in reducing the usage of resources without sacrificing team performance. We also show that recent observations about the world and teammates' behavior collected by an agent over the course of an episode combined with ToM inferences are crucial to generate team-aware plans for dynamic adaptation to teammates, especially when no prior information is provided about them.

## 多媒体(cs.MM:Multimedia)

### A Comprehensive Survey on Composed Image Retrieval 
[[arxiv](https://arxiv.org/abs/2502.18495)] [[cool](https://papers.cool/arxiv/2502.18495)] [[pdf](https://arxiv.org/pdf/2502.18495)]
> **Authors**: Xuemeng Song,Haoqiang Lin,Haokun Wen,Bohan Hou,Mingzhu Xu,Liqiang Nie
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 多媒体,人工智能,计算机视觉和模式识别,信息检索
- **Abstract**: Composed Image Retrieval (CIR) is an emerging yet challenging task that allows users to search for target images using a multimodal query, comprising a reference image and a modification text specifying the user's desired changes to the reference image. Given its significant academic and practical value, CIR has become a rapidly growing area of interest in the computer vision and machine learning communities, particularly with the advances in deep learning. To the best of our knowledge, there is currently no comprehensive review of CIR to provide a timely overview of this field. Therefore, we synthesize insights from over 120 publications in top conferences and journals, including ACM TOIS, SIGIR, and CVPR In particular, we systematically categorize existing supervised CIR and zero-shot CIR models using a fine-grained taxonomy. For a comprehensive review, we also briefly discuss approaches for tasks closely related to CIR, such as attribute-based CIR and dialog-based CIR. Additionally, we summarize benchmark datasets for evaluation and analyze existing supervised and zero-shot CIR methods by comparing experimental results across multiple datasets. Furthermore, we present promising future directions in this field, offering practical insights for researchers interested in further exploration.

### Deep-JGAC: End-to-End Deep Joint Geometry and Attribute Compression for Dense Colored Point Clouds 
[[arxiv](https://arxiv.org/abs/2502.17939)] [[cool](https://papers.cool/arxiv/2502.17939)] [[pdf](https://arxiv.org/pdf/2502.17939)]
> **Authors**: Yun Zhang,Zixi Guo,Linwei Zhu,C. -C. Jay Kuo
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 多媒体,计算机视觉和模式识别
- **Abstract**: Colored point cloud becomes a fundamental representation in the realm of 3D vision. Effective Point Cloud Compression (PCC) is urgently needed due to huge amount of data. In this paper, we propose an end-to-end Deep Joint Geometry and Attribute point cloud Compression (Deep-JGAC) framework for dense colored point clouds, which exploits the correlation between the geometry and attribute for high compression efficiency. Firstly, we propose a flexible Deep-JGAC framework, where the geometry and attribute sub-encoders are compatible to either learning or non-learning based geometry and attribute encoders. Secondly, we propose an attribute-assisted deep geometry encoder that enhances the geometry latent representation with the help of attribute, where the geometry decoding remains unchanged. Moreover, Attribute Information Fusion Module (AIFM) is proposed to fuse attribute information in geometry coding. Thirdly, to solve the mismatch between the point cloud geometry and attribute caused by the geometry compression distortion, we present an optimized re-colorization module to attach the attribute to the geometrically distorted point cloud for attribute coding. It enhances the colorization and lowers the computational complexity. Extensive experimental results demonstrate that in terms of the geometry quality metric D1-PSNR, the proposed Deep-JGAC achieves an average of 82.96%, 36.46%, 41.72%, and 31.16% bit-rate reductions as compared to the state-of-the-art G-PCC, V-PCC, GRASP, and PCGCv2, respectively. In terms of perceptual joint quality metric MS-GraphSIM, the proposed Deep-JGAC achieves an average of 48.72%, 14.67%, and 57.14% bit-rate reductions compared to the G-PCC, V-PCC, and IT-DL-PCC, respectively. The encoding/decoding time costs are also reduced by 94.29%/24.70%, and 96.75%/91.02% on average as compared with the V-PCC and IT-DL-PCC.

## 神经和进化计算(cs.NE:Neural and Evolutionary Computing)

### NeuroTree: Hierarchical Functional Brain Pathway Decoding for Mental Health Disorders 
[[arxiv](https://arxiv.org/abs/2502.18786)] [[cool](https://papers.cool/arxiv/2502.18786)] [[pdf](https://arxiv.org/pdf/2502.18786)]
> **Authors**: Jun-En Ding,Dongsheng Luo,Anna Zilverstand,Feng Liu
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 神经和进化计算,人工智能,神经元和认知
- **Abstract**: Analyzing functional brain networks using functional magnetic resonance imaging (fMRI) is crucial for understanding psychiatric disorders and addictive behaviors. While existing fMRI-based graph convolutional networks (GCNs) show considerable promise for feature extraction, they often fall short in characterizing complex relationships between brain regions and demographic factors and accounting for interpretable variables linked to psychiatric conditions. We propose NeuroTree to overcome these limitations, integrating a k-hop AGE-GCN with neural ordinary differential equations (ODEs). This framework leverages an attention mechanism to optimize functional connectivity (FC), thereby enhancing dynamic FC feature learning for brain disease classification. Furthermore, NeuroTree effectively decodes fMRI network features into tree structures, which improves the capture of high-order brain regional pathway features and enables the identification of hierarchical neural behavioral patterns essential for understanding disease-related brain subnetworks. Our empirical evaluations demonstrate that NeuroTree achieves state-of-the-art performance across two distinct mental disorder datasets and provides valuable insights into age-related deterioration patterns. These findings underscore the model's efficacy in predicting psychiatric disorders and elucidating their underlying neural mechanisms.

## 机器人技术(cs.RO:Robotics)

### Learning Autonomy: Off-Road Navigation Enhanced by Human Input 
[[arxiv](https://arxiv.org/abs/2502.18760)] [[cool](https://papers.cool/arxiv/2502.18760)] [[pdf](https://arxiv.org/pdf/2502.18760)]
> **Authors**: Akhil Nagariya,Dimitar Filev,Srikanth Saripalli,Gaurav Pandey
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,人工智能,机器学习
- **Abstract**: In the area of autonomous driving, navigating off-road terrains presents a unique set of challenges, from unpredictable surfaces like grass and dirt to unexpected obstacles such as bushes and puddles. In this work, we present a novel learning-based local planner that addresses these challenges by directly capturing human driving nuances from real-world demonstrations using only a monocular camera. The key features of our planner are its ability to navigate in challenging off-road environments with various terrain types and its fast learning capabilities. By utilizing minimal human demonstration data (5-10 mins), it quickly learns to navigate in a wide array of off-road conditions. The local planner significantly reduces the real world data required to learn human driving preferences. This allows the planner to apply learned behaviors to real-world scenarios without the need for manual fine-tuning, demonstrating quick adjustment and adaptability in off-road autonomous driving technology.

### MaskPlanner: Learning-Based Object-Centric Motion Generation from 3D Point Clouds 
[[arxiv](https://arxiv.org/abs/2502.18745)] [[cool](https://papers.cool/arxiv/2502.18745)] [[pdf](https://arxiv.org/pdf/2502.18745)]
> **Authors**: Gabriele Tiboni,Raffaello Camoriano,Tatiana Tommasi
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Project website at https://gabrieletiboni.github.io/MaskPlanner/
- **标题**: None
- **领域**: 机器人技术,计算机视觉和模式识别
- **Abstract**: Object-Centric Motion Generation (OCMG) plays a key role in a variety of industrial applications$\unicode{x2014}$such as robotic spray painting and welding$\unicode{x2014}$requiring efficient, scalable, and generalizable algorithms to plan multiple long-horizon trajectories over free-form 3D objects. However, existing solutions rely on specialized heuristics, expensive optimization routines, or restrictive geometry assumptions that limit their adaptability to real-world scenarios. In this work, we introduce a novel, fully data-driven framework that tackles OCMG directly from 3D point clouds, learning to generalize expert path patterns across free-form surfaces. We propose MaskPlanner, a deep learning method that predicts local path segments for a given object while simultaneously inferring "path masks" to group these segments into distinct paths. This design induces the network to capture both local geometric patterns and global task requirements in a single forward pass. Extensive experimentation on a realistic robotic spray painting scenario shows that our approach attains near-complete coverage (above 99%) for unseen objects, while it remains task-agnostic and does not explicitly optimize for paint deposition. Moreover, our real-world validation on a 6-DoF specialized painting robot demonstrates that the generated trajectories are directly executable and yield expert-level painting quality. Our findings crucially highlight the potential of the proposed learning method for OCMG to reduce engineering overhead and seamlessly adapt to several industrial use cases.

### QueryAdapter: Rapid Adaptation of Vision-Language Models in Response to Natural Language Queries 
[[arxiv](https://arxiv.org/abs/2502.18735)] [[cool](https://papers.cool/arxiv/2502.18735)] [[pdf](https://arxiv.org/pdf/2502.18735)]
> **Authors**: Nicolas Harvey Chapman,Feras Dayoub,Will Browne,Christopher Lehnert
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,计算机视觉和模式识别
- **Abstract**: A domain shift exists between the large-scale, internet data used to train a Vision-Language Model (VLM) and the raw image streams collected by a robot. Existing adaptation strategies require the definition of a closed-set of classes, which is impractical for a robot that must respond to diverse natural language queries. In response, we present QueryAdapter; a novel framework for rapidly adapting a pre-trained VLM in response to a natural language query. QueryAdapter leverages unlabelled data collected during previous deployments to align VLM features with semantic classes related to the query. By optimising learnable prompt tokens and actively selecting objects for training, an adapted model can be produced in a matter of minutes. We also explore how objects unrelated to the query should be dealt with when using real-world data for adaptation. In turn, we propose the use of object captions as negative class labels, helping to produce better calibrated confidence scores during adaptation. Extensive experiments on ScanNet++ demonstrate that QueryAdapter significantly enhances object retrieval performance compared to state-of-the-art unsupervised VLM adapters and 3D scene graph methods. Furthermore, the approach exhibits robust generalization to abstract affordance queries and other datasets, such as Ego4D.

### A Distributional Treatment of Real2Sim2Real for Vision-Driven Deformable Linear Object Manipulation 
[[arxiv](https://arxiv.org/abs/2502.18615)] [[cool](https://papers.cool/arxiv/2502.18615)] [[pdf](https://arxiv.org/pdf/2502.18615)]
> **Authors**: Georgios Kamaras,Subramanian Ramamoorthy
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,机器学习
- **Abstract**: We present an integrated (or end-to-end) framework for the Real2Sim2Real problem of manipulating deformable linear objects (DLOs) based on visual perception. Working with a parameterised set of DLOs, we use likelihood-free inference (LFI) to compute the posterior distributions for the physical parameters using which we can approximately simulate the behaviour of each specific DLO. We use these posteriors for domain randomisation while training, in simulation, object-specific visuomotor policies for a visuomotor DLO reaching task, using model-free reinforcement learning. We demonstrate the utility of this approach by deploying sim-trained DLO manipulation policies in the real world in a zero-shot manner, i.e. without any further fine-tuning. In this context, we evaluate the capacity of a prominent LFI method to perform fine classification over the parametric set of DLOs, using only visual and proprioceptive data obtained in a dynamic manipulation trajectory. We then study the implications of the resulting domain distributions in sim-based policy learning and real-world performance.

### Enhancing Reusability of Learned Skills for Robot Manipulation via Gaze and Bottleneck 
[[arxiv](https://arxiv.org/abs/2502.18121)] [[cool](https://papers.cool/arxiv/2502.18121)] [[pdf](https://arxiv.org/pdf/2502.18121)]
> **Authors**: Ryo Takizawa,Izumi Karino,Koki Nakagawa,Yoshiyuki Ohmura,Yasuo Kuniyoshi
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,计算机视觉和模式识别
- **Abstract**: Autonomous agents capable of diverse object manipulations should be able to acquire a wide range of manipulation skills with high reusability. Although advances in deep learning have made it increasingly feasible to replicate the dexterity of human teleoperation in robots, generalizing these acquired skills to previously unseen scenarios remains a significant challenge. In this study, we propose a novel algorithm, Gaze-based Bottleneck-aware Robot Manipulation (GazeBot), which enables high reusability of the learned motions even when the object positions and end-effector poses differ from those in the provided demonstrations. By leveraging gaze information and motion bottlenecks, both crucial features for object manipulation, GazeBot achieves high generalization performance compared with state-of-the-art imitation learning methods, without sacrificing its dexterity and reactivity. Furthermore, the training process of GazeBot is entirely data-driven once a demonstration dataset with gaze data is provided. Videos and code are available at https://crumbyrobotics.github.io/gazebot.

### MRBTP: Efficient Multi-Robot Behavior Tree Planning and Collaboration 
[[arxiv](https://arxiv.org/abs/2502.18072)] [[cool](https://papers.cool/arxiv/2502.18072)] [[pdf](https://arxiv.org/pdf/2502.18072)]
> **Authors**: Yishuai Cai,Xinglin Chen,Zhongxuan Cai,Yunxin Mao,Minglong Li,Wenjing Yang,Ji Wang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,人工智能,多代理系统
- **Abstract**: Multi-robot task planning and collaboration are critical challenges in robotics. While Behavior Trees (BTs) have been established as a popular control architecture and are plannable for a single robot, the development of effective multi-robot BT planning algorithms remains challenging due to the complexity of coordinating diverse action spaces. We propose the Multi-Robot Behavior Tree Planning (MRBTP) algorithm, with theoretical guarantees of both soundness and completeness. MRBTP features cross-tree expansion to coordinate heterogeneous actions across different BTs to achieve the team's goal. For homogeneous actions, we retain backup structures among BTs to ensure robustness and prevent redundant execution through intention sharing. While MRBTP is capable of generating BTs for both homogeneous and heterogeneous robot teams, its efficiency can be further improved. We then propose an optional plugin for MRBTP when Large Language Models (LLMs) are available to reason goal-related actions for each robot. These relevant actions can be pre-planned to form long-horizon subtrees, significantly enhancing the planning speed and collaboration efficiency of MRBTP. We evaluate our algorithm in warehouse management and everyday service scenarios. Results demonstrate MRBTP's robustness and execution efficiency under varying settings, as well as the ability of the pre-trained LLM to generate effective task-specific subtrees for MRBTP.

### InVDriver: Intra-Instance Aware Vectorized Query-Based Autonomous Driving Transformer 
[[arxiv](https://arxiv.org/abs/2502.17949)] [[cool](https://papers.cool/arxiv/2502.17949)] [[pdf](https://arxiv.org/pdf/2502.17949)]
> **Authors**: Bo Zhang,Heye Huang,Chunyang Liu,Yaqin Zhang,Zhenhua Xu
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Submitted to JICV (Journal of Intelligent and Connected Vehicles)
- **标题**: None
- **领域**: 机器人技术,计算机视觉和模式识别
- **Abstract**: End-to-end autonomous driving with its holistic optimization capabilities, has gained increasing traction in academia and industry. Vectorized representations, which preserve instance-level topological information while reducing computational overhead, have emerged as a promising paradigm. While existing vectorized query-based frameworks often overlook the inherent spatial correlations among intra-instance points, resulting in geometrically inconsistent outputs (e.g., fragmented HD map elements or oscillatory trajectories). To address these limitations, we propose InVDriver, a novel vectorized query-based system that systematically models intra-instance spatial dependencies through masked self-attention layers, thereby enhancing planning accuracy and trajectory smoothness. Across all core modules, i.e., perception, prediction, and planning, InVDriver incorporates masked self-attention mechanisms that restrict attention to intra-instance point interactions, enabling coordinated refinement of structural elements while suppressing irrelevant inter-instance noise. Experimental results on the nuScenes benchmark demonstrate that InVDriver achieves state-of-the-art performance, surpassing prior methods in both accuracy and safety, while maintaining high computational efficiency. Our work validates that explicit modeling of intra-instance geometric coherence is critical for advancing vectorized autonomous driving systems, bridging the gap between theoretical advantages of end-to-end frameworks and practical deployment requirements.

## 声音(cs.SD:Sound)

### Steering Language Model to Stable Speech Emotion Recognition via Contextual Perception and Chain of Thought 
[[arxiv](https://arxiv.org/abs/2502.18186)] [[cool](https://papers.cool/arxiv/2502.18186)] [[pdf](https://arxiv.org/pdf/2502.18186)]
> **Authors**: Zhixian Zhao,Xinfa Zhu,Xinsheng Wang,Shuiyuan Wang,Xuelong Geng,Wenjie Tian,Lei Xie
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 声音,计算语言学,音频和语音处理
- **Abstract**: Large-scale audio language models (ALMs), such as Qwen2-Audio, are capable of comprehending diverse audio signal, performing audio analysis and generating textual responses. However, in speech emotion recognition (SER), ALMs often suffer from hallucinations, resulting in misclassifications or irrelevant outputs. To address these challenges, we propose C$^2$SER, a novel ALM designed to enhance the stability and accuracy of SER through Contextual perception and Chain of Thought (CoT). C$^2$SER integrates the Whisper encoder for semantic perception and Emotion2Vec-S for acoustic perception, where Emotion2Vec-S extends Emotion2Vec with semi-supervised learning to enhance emotional discrimination. Additionally, C$^2$SER employs a CoT approach, processing SER in a step-by-step manner while leveraging speech content and speaking styles to improve recognition. To further enhance stability, C$^2$SER introduces self-distillation from explicit CoT to implicit CoT, mitigating error accumulation and boosting recognition accuracy. Extensive experiments show that C$^2$SER outperforms existing popular ALMs, such as Qwen2-Audio and SECap, delivering more stable and precise emotion recognition. We release the training code, checkpoints, and test sets to facilitate further research.

### NotaGen: Advancing Musicality in Symbolic Music Generation with Large Language Model Training Paradigms 
[[arxiv](https://arxiv.org/abs/2502.18008)] [[cool](https://papers.cool/arxiv/2502.18008)] [[pdf](https://arxiv.org/pdf/2502.18008)]
> **Authors**: Yashan Wang,Shangda Wu,Jianhuai Hu,Xingjian Du,Yueqi Peng,Yongxin Huang,Shuai Fan,Xiaobing Li,Feng Yu,Maosong Sun
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 声音,人工智能,音频和语音处理
- **Abstract**: We introduce NotaGen, a symbolic music generation model aiming to explore the potential of producing high-quality classical sheet music. Inspired by the success of Large Language Models (LLMs), NotaGen adopts pre-training, fine-tuning, and reinforcement learning paradigms (henceforth referred to as the LLM training paradigms). It is pre-trained on 1.6M pieces of music, and then fine-tuned on approximately 9K high-quality classical compositions conditioned on "period-composer-instrumentation" prompts. For reinforcement learning, we propose the CLaMP-DPO method, which further enhances generation quality and controllability without requiring human annotations or predefined rewards. Our experiments demonstrate the efficacy of CLaMP-DPO in symbolic music generation models with different architectures and encoding schemes. Furthermore, subjective A/B tests show that NotaGen outperforms baseline models against human compositions, greatly advancing musical aesthetics in symbolic music generation.

### Enhancing Speech Quality through the Integration of BGRU and Transformer Architectures 
[[arxiv](https://arxiv.org/abs/2502.17911)] [[cool](https://papers.cool/arxiv/2502.17911)] [[pdf](https://arxiv.org/pdf/2502.17911)]
> **Authors**: Souliman Alghnam,Mohammad Alhussien,Khaled Shaheen
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 声音,人工智能,音频和语音处理
- **Abstract**: Speech enhancement plays an essential role in improving the quality of speech signals in noisy environments. This paper investigates the efficacy of integrating Bidirectional Gated Recurrent Units (BGRU) and Transformer models for speech enhancement tasks. Through a comprehensive experimental evaluation, our study demonstrates the superiority of this hybrid architecture over traditional methods and standalone models. The combined BGRU-Transformer framework excels in capturing temporal dependencies and learning complex signal patterns, leading to enhanced noise reduction and improved speech quality. Results show significant performance gains compared to existing approaches, highlighting the potential of this integrated model in real-world applications. The seamless integration of BGRU and Transformer architectures not only enhances system robustness but also opens the road for advanced speech processing techniques. This research contributes to the ongoing efforts in speech enhancement technology and sets a solid foundation for future investigations into optimizing model architectures, exploring many application scenarios, and advancing the field of speech processing in noisy environments.

## 软件工程(cs.SE:Software Engineering)

### Deep-Bench: Deep Learning Benchmark Dataset for Code Generation 
[[arxiv](https://arxiv.org/abs/2502.18726)] [[cool](https://papers.cool/arxiv/2502.18726)] [[pdf](https://arxiv.org/pdf/2502.18726)]
> **Authors**: Alireza Daghighfarsoodeh,Chung-Yu Wang,Hamed Taherkhani,Melika Sepidband,Mohammad Abdollahi,Hadi Hemmati,Hung Viet Pham
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,人工智能,机器学习
- **Abstract**: Deep learning (DL) has revolutionized areas such as computer vision, natural language processing, and more. However, developing DL systems is challenging due to the complexity of DL workflows. Large Language Models (LLMs), such as GPT, Claude, Llama, Mistral, etc., have emerged as promising tools to assist in DL code generation, offering potential solutions to these challenges. Despite this, existing benchmarks such as DS-1000 are limited, as they primarily focus on small DL code snippets related to pre/post-processing tasks and lack a comprehensive coverage of the full DL pipeline, including different DL phases and input data types. To address this, we introduce DeepBench, a novel benchmark dataset designed for function-level DL code generation. DeepBench categorizes DL problems based on three key aspects: phases such as pre-processing, model construction, and training; tasks, including classification, regression, and recommendation; and input data types such as tabular, image, and text. GPT-4o -- the state-of-the-art LLM -- achieved 31% accuracy on DeepBench, significantly lower than its 60% on DS-1000. We observed similar difficulty for other LLMs (e.g., 28% vs. 54% for Claude, 21% vs. 41% for LLaMA, and 15% vs. 20% for Mistral). This result underscores DeepBench's greater complexity. We also construct a taxonomy of issues and bugs found in LLM-generated DL code, which highlights the distinct challenges that LLMs face when generating DL code compared to general code. Furthermore, our analysis also reveals substantial performance variations across categories, with differences of up to 7% among phases and 37% among tasks. These disparities suggest that DeepBench offers valuable insights into the LLMs' performance and areas for potential improvement in the DL domain.

### Programming with Pixels: Computer-Use Meets Software Engineering 
[[arxiv](https://arxiv.org/abs/2502.18525)] [[cool](https://papers.cool/arxiv/2502.18525)] [[pdf](https://arxiv.org/pdf/2502.18525)]
> **Authors**: Pranjal Aggarwal,Sean Welleck
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,机器学习
- **Abstract**: Recent advancements in software engineering (SWE) agents have largely followed a $\textit{tool-based paradigm}$, where agents interact with hand-engineered tool APIs to perform specific tasks. While effective for specialized tasks, these methods fundamentally lack generalization, as they require predefined tools for each task and do not scale across programming languages and domains. We introduce $\texttt{Programming with Pixels}$ (PwP), an agent environment that unifies software development tasks by enabling $\textit{computer-use agents}$-agents that operate directly within an IDE through visual perception, typing, and clicking, rather than relying on predefined tool APIs. To systematically evaluate these agents, we propose $\texttt{PwP-Bench}$, a benchmark that unifies existing SWE benchmarks spanning tasks across multiple programming languages, modalities, and domains under a task-agnostic state and action space. Our experiments demonstrate that general-purpose computer-use agents can approach or even surpass specialized tool-based agents on a variety of SWE tasks without the need for hand-engineered tools. However, our analysis shows that current models suffer from limited visual grounding and fail to exploit many IDE tools that could simplify their tasks. When agents can directly access IDE tools, without visual interaction, they show significant performance improvements, highlighting the untapped potential of leveraging built-in IDE capabilities. Our results establish PwP as a scalable testbed for building and evaluating the next wave of software engineering agents. We release code and data at https://programmingwithpixels.com

### Comprehensive Analysis of Transparency and Accessibility of ChatGPT, DeepSeek, And other SoTA Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.18505)] [[cool](https://papers.cool/arxiv/2502.18505)] [[pdf](https://arxiv.org/pdf/2502.18505)]
> **Authors**: Ranjan Sapkota,Shaina Raza,Manoj Karkee
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,人工智能,计算语言学
- **Abstract**: Despite increasing discussions on open-source Artificial Intelligence (AI), existing research lacks a discussion on the transparency and accessibility of state-of-the-art (SoTA) Large Language Models (LLMs). The Open Source Initiative (OSI) has recently released its first formal definition of open-source software. This definition, when combined with standard dictionary definitions and the sparse published literature, provide an initial framework to support broader accessibility to AI models such as LLMs, but more work is essential to capture the unique dynamics of openness in AI. In addition, concerns about open-washing, where models claim openness but lack full transparency, has been raised, which limits the reproducibility, bias mitigation, and domain adaptation of these models. In this context, our study critically analyzes SoTA LLMs from the last five years, including ChatGPT, DeepSeek, LLaMA, and others, to assess their adherence to transparency standards and the implications of partial openness. Specifically, we examine transparency and accessibility from two perspectives: open-source vs. open-weight models. Our findings reveal that while some models are labeled as open-source, this does not necessarily mean they are fully open-sourced. Even in the best cases, open-source models often do not report model training data, and code as well as key metrics, such as weight accessibility, and carbon emissions. To the best of our knowledge, this is the first study that systematically examines the transparency and accessibility of over 100 different SoTA LLMs through the dual lens of open-source and open-weight models. The findings open avenues for further research and call for responsible and sustainable AI practices to ensure greater transparency, accountability, and ethical deployment of these models.(DeepSeek transparency, ChatGPT accessibility, open source, DeepSeek open source)

### Mechanistic Understanding of Language Models in Syntactic Code Completion 
[[arxiv](https://arxiv.org/abs/2502.18499)] [[cool](https://papers.cool/arxiv/2502.18499)] [[pdf](https://arxiv.org/pdf/2502.18499)]
> **Authors**: Samuel Miller,Daking Rai,Ziyu Yao
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-26
> **comment**: 10 pages, 4 figures, accepted to the AAAI 2025 Workshop on Towards Knowledgeable Foundation Models
- **标题**: None
- **领域**: 软件工程,人工智能,计算语言学
- **Abstract**: Recently, language models (LMs) have shown impressive proficiency in code generation tasks, especially when fine-tuned on code-specific datasets, commonly known as Code LMs. However, our understanding of the internal decision-making processes of Code LMs, such as how they use their (syntactic or semantic) knowledge, remains limited, which could lead to unintended harm as they are increasingly used in real life. This motivates us to conduct one of the first Mechanistic Interpretability works to understand how Code LMs perform a syntactic completion task, specifically the closing parenthesis task, on the CodeLlama-7b model (Roziere et al. 2023). Our findings reveal that the model requires middle-later layers until it can confidently predict the correct label for the closing parenthesis task. Additionally, we identify that while both multi-head attention (MHA) and feed-forward (FF) sub-layers play essential roles, MHA is particularly crucial. Furthermore, we also discover attention heads that keep track of the number of already closed parentheses precisely but may or may not promote a correct number of closing parentheses that are still missing, leading to a positive or negative impact on the model's performance.

### LLM4EFFI: Leveraging Large Language Models to Enhance Code Efficiency and Correctness 
[[arxiv](https://arxiv.org/abs/2502.18489)] [[cool](https://papers.cool/arxiv/2502.18489)] [[pdf](https://arxiv.org/pdf/2502.18489)]
> **Authors**: Tong Ye,Weigang Huang,Xuhong Zhang,Tengfei Ma,Peiyu Liu,Jianwei Yin,Wenhai Wang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: Large Language Models (LLMs), particularly Code LLMs, have demonstrated impressive performance in code generation. Current research primarily focuses on the correctness of generated code, while efficiency remains less explored. Recent works have focused on modifying the initial version of the code to improve its efficiency. However, such refinements are limited by the algorithmic design and overall logic of the initial code, resulting in only incremental improvements. In contrast, when human developers write high-quality code, they typically begin by designing several potential solutions at the logical level, evaluating various algorithms and their complexities, and then proceeding to implement and optimize the solution. In this study, we introduce \tool: \uline{L}arge \uline{L}anguage \uline{M}odel for Code \uline{Effi}ciency, a novel framework that enables LLMs to generate code that balances both efficiency and correctness. Specifically, \tool divides the efficiency optimization process into two domains: algorithmic exploration in the logic domain and implementation optimization in the code domain. The correctness of the code is then guaranteed through a synthetic test case refinement process. This approach, which prioritizes efficiency before ensuring correctness, offers a new paradigm for efficient code generation. Experiments demonstrate that \tool consistently improves both efficiency and correctness, achieving new state-of-the-art performance in code efficiency benchmarks across various LLM backbones.

### AuPair: Golden Example Pairs for Code Repair 
[[arxiv](https://arxiv.org/abs/2502.18487)] [[cool](https://papers.cool/arxiv/2502.18487)] [[pdf](https://arxiv.org/pdf/2502.18487)]
> **Authors**: Aditi Mavalankar,Hassan Mansoor,Zita Marinho,Masha Samsikova,Tom Schaul
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,人工智能,计算语言学,机器学习
- **Abstract**: Scaling up inference-time compute has proven to be a valuable strategy in improving the performance of Large Language Models (LLMs) without fine-tuning. An important task that can benefit from additional inference-time compute is self-repair; given an initial flawed response, or guess, the LLM corrects its own mistake and produces an improved response, or fix. We leverage the in-context learning ability of LLMs to perform self-repair in the coding domain. The key contribution of our paper is an approach that synthesises and selects an ordered set of golden example pairs, or AuPairs, of these initial guesses and subsequent fixes for the corresponding problems. Each such AuPair is provided as a single in-context example at inference time to generate a repaired solution. For an inference-time compute budget of $N$ LLM calls per problem, $N$ AuPairs are used to generate $N$ repaired solutions, out of which the highest-scoring solution is selected as the final answer. The underlying intuition is that if the LLM is given a different example of fixing an incorrect guess each time, it can subsequently generate a diverse set of repaired solutions. Our algorithm selects these AuPairs in a manner that maximises complementarity and usefulness. We demonstrate the results of our algorithm on 5 LLMs across 7 competitive programming datasets for the code repair task. Our algorithm yields a significant boost in performance compared to best-of-$N$ and self-repair, and also exhibits strong generalisation across datasets and models. Moreover, our approach shows significantly stronger scaling with inference-time compute budget compared to baselines.

### A Contemporary Survey of Large Language Model Assisted Program Analysis 
[[arxiv](https://arxiv.org/abs/2502.18474)] [[cool](https://papers.cool/arxiv/2502.18474)] [[pdf](https://arxiv.org/pdf/2502.18474)]
> **Authors**: Jiayimei Wang,Tao Ni,Wei-Bin Lee,Qingchuan Zhao
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: The increasing complexity of software systems has driven significant advancements in program analysis, as traditional methods unable to meet the demands of modern software development. To address these limitations, deep learning techniques, particularly Large Language Models (LLMs), have gained attention due to their context-aware capabilities in code comprehension. Recognizing the potential of LLMs, researchers have extensively explored their application in program analysis since their introduction. Despite existing surveys on LLM applications in cybersecurity, comprehensive reviews specifically addressing their role in program analysis remain scarce. In this survey, we systematically review the application of LLMs in program analysis, categorizing the existing work into static analysis, dynamic analysis, and hybrid approaches. Moreover, by examining and synthesizing recent studies, we identify future directions and challenges in the field. This survey aims to demonstrate the potential of LLMs in advancing program analysis practices and offer actionable insights for security researchers seeking to enhance detection frameworks or develop domain-specific models.

### Disproving Program Equivalence with LLMs 
[[arxiv](https://arxiv.org/abs/2502.18473)] [[cool](https://papers.cool/arxiv/2502.18473)] [[pdf](https://arxiv.org/pdf/2502.18473)]
> **Authors**: Miltiadis Allamanis,Pengcheng Yin
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,机器学习
- **Abstract**: To evaluate large language models (LLMs) for code, research has used manually created unit test-based benchmarks. However, these tests are often inadequate, missing corner cases and other implementation-specific oddities. This work introduces ProbeGen, a whitebox method that takes two or more executable pieces of code and searches for counterexamples to their equivalence. Comparing code semantics requires a deep understanding of code. We demonstrate that LLMs with execution feedback perform well at this task. In a common code synthesis benchmark, ProbeGen disproves 18% of samples considered equivalent to the ground truth by the benchmark-provided unit tests. Additionally, using ProbeGen, we can semantically cluster LLM samples for semantic self-consistency, improving pass@1 by 10% by unifying syntactically distinct but semantically similar samples.

### SOK: Exploring Hallucinations and Security Risks in AI-Assisted Software Development with Insights for LLM Deployment 
[[arxiv](https://arxiv.org/abs/2502.18468)] [[cool](https://papers.cool/arxiv/2502.18468)] [[pdf](https://arxiv.org/pdf/2502.18468)]
> **Authors**: Ariful Haque,Sunzida Siddique,Md. Mahfuzur Rahman,Ahmed Rafi Hasan,Laxmi Rani Das,Marufa Kamal,Tasnim Masura,Kishor Datta Gupta
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,人工智能,密码学和安全
- **Abstract**: The integration of Large Language Models (LLMs) such as GitHub Copilot, ChatGPT, Cursor AI, and Codeium AI into software development has revolutionized the coding landscape, offering significant productivity gains, automation, and enhanced debugging capabilities. These tools have proven invaluable for generating code snippets, refactoring existing code, and providing real-time support to developers. However, their widespread adoption also presents notable challenges, particularly in terms of security vulnerabilities, code quality, and ethical concerns. This paper provides a comprehensive analysis of the benefits and risks associated with AI-powered coding tools, drawing on user feedback, security analyses, and practical use cases. We explore the potential for these tools to replicate insecure coding practices, introduce biases, and generate incorrect or non-sensical code (hallucinations). In addition, we discuss the risks of data leaks, intellectual property violations and the need for robust security measures to mitigate these threats. By comparing the features and performance of these tools, we aim to guide developers in making informed decisions about their use, ensuring that the benefits of AI-assisted coding are maximized while minimizing associated risks.

### ChatGPT vs. DeepSeek: A Comparative Study on AI-Based Code Generation 
[[arxiv](https://arxiv.org/abs/2502.18467)] [[cool](https://papers.cool/arxiv/2502.18467)] [[pdf](https://arxiv.org/pdf/2502.18467)]
> **Authors**: Md Motaleb Hossen Manik
> **First submission**: 2025-01-30
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: Background: AI-powered code generation, fueled by Large Language Models (LLMs), is revolutionizing software development. Models like OpenAI's Codex and GPT-4, alongside DeepSeek, leverage vast code and natural language datasets. However, ensuring code quality, correctness, and managing complex tasks remains challenging, necessitating thorough evaluation. Methodology: This research compares ChatGPT (version o1) and DeepSeek (version R1) for Python code generation using online judge coding challenges. It evaluates correctness (online judge verdicts, up to three attempts), code quality (Pylint/Flake8), and efficiency (execution time/memory usage). Results: DeepSeek demonstrated higher correctness, particularly on algorithmic tasks, often achieving 'Accepted' on the first attempt. ChatGPT sometimes requires multiple attempts or failures. ChatGPT encountered fewer issues, used comparable or slightly less memory, consumed less execution times and wrote fewer lines of code. Conclusion: DeepSeek exhibited superior correctness in Python code generation, often requiring fewer attempts, suggesting an advantage in algorithmic problem-solving. Both models showed almost similar efficiency in execution time and memory use. Finally, this research provides insights for developers choosing AI coding assistants and informs future AI-driven software development research.

### MLScent A tool for Anti-pattern detection in ML projects 
[[arxiv](https://arxiv.org/abs/2502.18466)] [[cool](https://papers.cool/arxiv/2502.18466)] [[pdf](https://arxiv.org/pdf/2502.18466)]
> **Authors**: Karthik Shivashankar,Antonio Martini
> **First submission**: 2025-01-30
> **First announcement**: 2025-02-26
> **comment**: 4th International Conference onAIEngineering Software Engineering forAI, CAIN 2025
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: Machine learning (ML) codebases face unprecedented challenges in maintaining code quality and sustainability as their complexity grows exponentially. While traditional code smell detection tools exist, they fail to address ML-specific issues that can significantly impact model performance, reproducibility, and maintainability. This paper introduces MLScent, a novel static analysis tool that leverages sophisticated Abstract Syntax Tree (AST) analysis to detect anti-patterns and code smells specific to ML projects. MLScent implements 76 distinct detectors across major ML frameworks including TensorFlow (13 detectors), PyTorch (12 detectors), Scikit-learn (9 detectors), and Hugging Face (10 detectors), along with data science libraries like Pandas and NumPy (8 detectors each). The tool's architecture also integrates general ML smell detection (16 detectors), and specialized analysis for data preprocessing and model training workflows. Our evaluation demonstrates MLScent's effectiveness through both quantitative classification metrics and qualitative assessment via user studies feedback with ML practitioners. Results show high accuracy in identifying framework-specific anti-patterns, data handling issues, and general ML code smells across real-world projects.

### LLM-Based Design Pattern Detection 
[[arxiv](https://arxiv.org/abs/2502.18458)] [[cool](https://papers.cool/arxiv/2502.18458)] [[pdf](https://arxiv.org/pdf/2502.18458)]
> **Authors**: Christian Schindler,Andreas Rausch
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Submitted Version, that was accepted at PATTERNS 2025
- **标题**: None
- **领域**: 软件工程,机器学习
- **Abstract**: Detecting design pattern instances in unfamiliar codebases remains a challenging yet essential task for improving software quality and maintainability. Traditional static analysis tools often struggle with the complexity, variability, and lack of explicit annotations that characterize real-world pattern implementations. In this paper, we present a novel approach leveraging Large Language Models to automatically identify design pattern instances across diverse codebases. Our method focuses on recognizing the roles classes play within the pattern instances. By providing clearer insights into software structure and intent, this research aims to support developers, improve comprehension, and streamline tasks such as refactoring, maintenance, and adherence to best practices.

### SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open Software Evolution 
[[arxiv](https://arxiv.org/abs/2502.18449)] [[cool](https://papers.cool/arxiv/2502.18449)] [[pdf](https://arxiv.org/pdf/2502.18449)]
> **Authors**: Yuxiang Wei,Olivier Duchenne,Jade Copet,Quentin Carbonneaux,Lingming Zhang,Daniel Fried,Gabriel Synnaeve,Rishabh Singh,Sida I. Wang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,人工智能,计算语言学
- **Abstract**: The recent DeepSeek-R1 release has demonstrated the immense potential of reinforcement learning (RL) in enhancing the general reasoning capabilities of large language models (LLMs). While DeepSeek-R1 and other follow-up work primarily focus on applying RL to competitive coding and math problems, this paper introduces SWE-RL, the first approach to scale RL-based LLM reasoning for real-world software engineering. Leveraging a lightweight rule-based reward (e.g., the similarity score between ground-truth and LLM-generated solutions), SWE-RL enables LLMs to autonomously recover a developer's reasoning processes and solutions by learning from extensive open-source software evolution data -- the record of a software's entire lifecycle, including its code snapshots, code changes, and events such as issues and pull requests. Trained on top of Llama 3, our resulting reasoning model, Llama3-SWE-RL-70B, achieves a 41.0% solve rate on SWE-bench Verified -- a human-verified collection of real-world GitHub issues. To our knowledge, this is the best performance reported for medium-sized (<100B) LLMs to date, even comparable to leading proprietary LLMs like GPT-4o. Surprisingly, despite performing RL solely on software evolution data, Llama3-SWE-RL has even emerged with generalized reasoning skills. For example, it shows improved results on five out-of-domain tasks, namely, function coding, library use, code reasoning, mathematics, and general language understanding, whereas a supervised-finetuning baseline even leads to performance degradation on average. Overall, SWE-RL opens up a new direction to improve the reasoning capabilities of LLMs through reinforcement learning on massive software engineering data.

## 社交和信息网络(cs.SI:Social and Information Networks)

### Analyzing User Perceptions of Large Language Models (LLMs) on Reddit: Sentiment and Topic Modeling of ChatGPT and DeepSeek Discussions 
[[arxiv](https://arxiv.org/abs/2502.18513)] [[cool](https://papers.cool/arxiv/2502.18513)] [[pdf](https://arxiv.org/pdf/2502.18513)]
> **Authors**: Krishnaveni Katta
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-26
> **comment**: 13 pages, 8 figures
- **标题**: None
- **领域**: 社交和信息网络,计算语言学,人机交互
- **Abstract**: While there is an increased discourse on large language models (LLMs) like ChatGPT and DeepSeek, there is no comprehensive understanding of how users of online platforms, like Reddit, perceive these models. This is an important omission because public opinion can influence AI development, trust, and future policy. This study aims at analyzing Reddit discussions about ChatGPT and DeepSeek using sentiment and topic modeling to advance the understanding of user attitudes. Some of the significant topics such as trust in AI, user expectations, potential uses of the tools, reservations about AI biases, and ethical implications of their use are explored in this study. By examining these concerns, the study provides a sense of how public sentiment might shape the direction of AI development going forward. The report also mentions whether users have faith in the technology and what they see as its future. A word frequency approach is used to identify broad topics and sentiment trends. Also, topic modeling through the Latent Dirichlet Allocation (LDA) method identifies top topics in users' language, for example, potential benefits of LLMs, their technological applications, and their overall social ramifications. The study aims to inform developers and policymakers by making it easier to see how users comprehend and experience these game-changing technologies.

### Large Language Model Driven Agents for Simulating Echo Chamber Formation 
[[arxiv](https://arxiv.org/abs/2502.18138)] [[cool](https://papers.cool/arxiv/2502.18138)] [[pdf](https://arxiv.org/pdf/2502.18138)]
> **Authors**: Chenhao Gu,Ling Luo,Zainab Razia Zaidi,Shanika Karunasekera
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 社交和信息网络,人工智能
- **Abstract**: The rise of echo chambers on social media platforms has heightened concerns about polarization and the reinforcement of existing beliefs. Traditional approaches for simulating echo chamber formation have often relied on predefined rules and numerical simulations, which, while insightful, may lack the nuance needed to capture complex, real-world interactions. In this paper, we present a novel framework that leverages large language models (LLMs) as generative agents to simulate echo chamber dynamics within social networks. The novelty of our approach is that it incorporates both opinion updates and network rewiring behaviors driven by LLMs, allowing for a context-aware and semantically rich simulation of social interactions. Additionally, we utilize real-world Twitter (now X) data to benchmark the LLM-based simulation against actual social media behaviors, providing insights into the accuracy and realism of the generated opinion trends. Our results demonstrate the efficacy of LLMs in modeling echo chamber formation, capturing both structural and semantic dimensions of opinion clustering. %This work contributes to a deeper understanding of social influence dynamics and offers a new tool for studying polarization in online communities.

### AutoCas: Autoregressive Cascade Predictor in Social Networks via Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.18040)] [[cool](https://papers.cool/arxiv/2502.18040)] [[pdf](https://arxiv.org/pdf/2502.18040)]
> **Authors**: Yuhao Zheng,Chenghua Gong,Rui Sun,Juyuan Zhang,Liming Pan,Linyuan Lv
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 12 pages
- **标题**: None
- **领域**: 社交和信息网络,人工智能
- **Abstract**: Popularity prediction in information cascades plays a crucial role in social computing, with broad applications in viral marketing, misinformation control, and content recommendation. However, information propagation mechanisms, user behavior, and temporal activity patterns exhibit significant diversity, necessitating a foundational model capable of adapting to such variations. At the same time, the amount of available cascade data remains relatively limited compared to the vast datasets used for training large language models (LLMs). Recent studies have demonstrated the feasibility of leveraging LLMs for time-series prediction by exploiting commonalities across different time-series domains. Building on this insight, we introduce the Autoregressive Information Cascade Predictor (AutoCas), an LLM-enhanced model designed specifically for cascade popularity prediction. Unlike natural language sequences, cascade data is characterized by complex local topologies, diffusion contexts, and evolving dynamics, requiring specialized adaptations for effective LLM integration. To address these challenges, we first tokenize cascade data to align it with sequence modeling principles. Next, we reformulate cascade diffusion as an autoregressive modeling task to fully harness the architectural strengths of LLMs. Beyond conventional approaches, we further introduce prompt learning to enhance the synergy between LLMs and cascade prediction. Extensive experiments demonstrate that AutoCas significantly outperforms baseline models in cascade popularity prediction while exhibiting scaling behavior inherited from LLMs. Code is available at this repository: https://anonymous.4open.science/r/AutoCas-85C6

### Structure-prior Informed Diffusion Model for Graph Source Localization with Limited Data 
[[arxiv](https://arxiv.org/abs/2502.17928)] [[cool](https://papers.cool/arxiv/2502.17928)] [[pdf](https://arxiv.org/pdf/2502.17928)]
> **Authors**: Hongyi Chen,Jingtao Ding,Xiaojun Liang,Yong Li,Xiao-Ping Zhang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 社交和信息网络,人工智能,机器学习
- **Abstract**: The source localization problem in graph information propagation is crucial for managing various network disruptions, from misinformation spread to infrastructure failures. While recent deep generative approaches have shown promise in this domain, their effectiveness is limited by the scarcity of real-world propagation data. This paper introduces SIDSL (\textbf{S}tructure-prior \textbf{I}nformed \textbf{D}iffusion model for \textbf{S}ource \textbf{L}ocalization), a novel framework that addresses three key challenges in limited-data scenarios: unknown propagation patterns, complex topology-propagation relationships, and class imbalance between source and non-source nodes. SIDSL incorporates topology-aware priors through graph label propagation and employs a propagation-enhanced conditional denoiser with a GNN-parameterized label propagation module (GNN-LP). Additionally, we propose a structure-prior biased denoising scheme that initializes from structure-based source estimations rather than random noise, effectively countering class imbalance issues. Experimental results across four real-world datasets demonstrate SIDSL's superior performance, achieving 7.5-13.3% improvements in F1 scores compared to state-of-the-art methods. Notably, when pretrained with simulation data of synthetic patterns, SIDSL maintains robust performance with only 10% of training data, surpassing baselines by more than 18.8%. These results highlight SIDSL's effectiveness in real-world applications where labeled data is scarce.

## 图像和视频处理(eess.IV:Image and Video Processing)

### Subclass Classification of Gliomas Using MRI Fusion Technique 
[[arxiv](https://arxiv.org/abs/2502.18775)] [[cool](https://papers.cool/arxiv/2502.18775)] [[pdf](https://arxiv.org/pdf/2502.18775)]
> **Authors**: Kiranmayee Janardhan,Christy Bobby Thomas
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 15 pages, 7 figures, 1 algorithm, 4 tables, journal paper
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习
- **Abstract**: Glioma, the prevalent primary brain tumor, exhibits diverse aggressiveness levels and prognoses. Precise classification of glioma is paramount for treatment planning and predicting prognosis. This study aims to develop an algorithm to fuse the MRI images from T1, T2, T1ce, and fluid-attenuated inversion recovery (FLAIR) sequences to enhance the efficacy of glioma subclass classification as no tumor, necrotic core, peritumoral edema, and enhancing tumor. The MRI images from BraTS datasets were used in this work. The images were pre-processed using max-min normalization to ensure consistency in pixel intensity values across different images. The segmentation of the necrotic core, peritumoral edema, and enhancing tumor was performed on 2D and 3D images separately using UNET architecture. Further, the segmented regions from multimodal MRI images were fused using the weighted averaging technique. Integrating 2D and 3D segmented outputs enhances classification accuracy by capturing detailed features like tumor shape, boundaries, and intensity distribution in slices, while also providing a comprehensive view of spatial extent, shape, texture, and localization within the brain volume. The fused images were used as input to the pre-trained ResNet50 model for glioma subclass classification. The network is trained on 80% and validated on 20% of the data. The proposed method achieved a classification of accuracy of 99.25%, precision of 99.30%, recall of 99.10, F1 score of 99.19%, Intersection Over Union of 84.49%, and specificity of 99.76, which showed a significantly higher performance than existing techniques. These findings emphasize the significance of glioma segmentation and classification in aiding accurate diagnosis.

### TerraTrace: Temporal Signature Land Use Mapping System 
[[arxiv](https://arxiv.org/abs/2502.18704)] [[cool](https://papers.cool/arxiv/2502.18704)] [[pdf](https://arxiv.org/pdf/2502.18704)]
> **Authors**: Angela Busheska,Vikram Iyer,Bruno Silva,Peder Olsen,Ranveer Chandra,Vaishnavi Ranganathan
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: Understanding land use over time is critical to tracking events related to climate change, like deforestation. However, satellite-based remote sensing tools which are used for monitoring struggle to differentiate vegetation types in farms and orchards from forests. We observe that metrics such as the Normalized Difference Vegetation Index (NDVI), based on plant photosynthesis, have unique temporal signatures that reflect agricultural practices and seasonal cycles. We analyze yearly NDVI changes on 20 farms for 10 unique crops. Initial results show that NDVI curves are coherent with agricultural practices, are unique to each crop, consistent globally, and can differentiate farms from forests. We develop a novel longitudinal NDVI dataset for the state of California from 2020-2023 with 500~m resolution and over 70 million points. We use this to develop the TerraTrace platform, an end-to-end analytic tool that classifies land use using NDVI signatures and allows users to query the system through an LLM chatbot and graphical interface.

### A Comparative Review of the Histogram-based Image Segmentation Methods 
[[arxiv](https://arxiv.org/abs/2502.18550)] [[cool](https://papers.cool/arxiv/2502.18550)] [[pdf](https://arxiv.org/pdf/2502.18550)]
> **Authors**: ZhenZhou Wang
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: The histogram of an image is the accurate graphical representation of the numerical grayscale distribution and it is also an estimate of the probability distribution of image pixels. Therefore, histogram has been widely adopted to calculate the clustering means and partitioning thresholds for image segmentation. There have been many classical histogram-based image segmentation methods proposed and played important roles in both academics and industry. In this article, the histories and recent advances of the histogram-based image segmentation techniques are first reviewed and then they are divided into four categories: (1), the means-based method; (2), the Gaussian-mixture-model-based method; (3), the entropy-based method and (4) the feature-points-based method. The principles of the classical histogram-based image segmentation methods are described at first and then their performances are compared objectively. In addition, the histogram-based image segmentation methods are compared with the general-purpose deep learning methods in segmenting objects with uniform or simple backgrounds. The histogram-based image segmentation methods are more accurate than the universal deep-learning methods without special training in segmenting many types of images.

### End-to-End Deep Learning for Structural Brain Imaging: A Unified Framework 
[[arxiv](https://arxiv.org/abs/2502.18523)] [[cool](https://papers.cool/arxiv/2502.18523)] [[pdf](https://arxiv.org/pdf/2502.18523)]
> **Authors**: Yao Su,Keqi Han,Mingjie Zeng,Lichao Sun,Liang Zhan,Carl Yang,Lifang He,Xiangnan Kong
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **Abstract**: Brain imaging analysis is fundamental in neuroscience, providing valuable insights into brain structure and function. Traditional workflows follow a sequential pipeline-brain extraction, registration, segmentation, parcellation, network generation, and classification-treating each step as an independent task. These methods rely heavily on task-specific training data and expert intervention to correct intermediate errors, making them particularly burdensome for high-dimensional neuroimaging data, where annotations and quality control are costly and time-consuming. We introduce UniBrain, a unified end-to-end framework that integrates all processing steps into a single optimization process, allowing tasks to interact and refine each other. Unlike traditional approaches that require extensive task-specific annotations, UniBrain operates with minimal supervision, leveraging only low-cost labels (i.e., classification and extraction) and a single labeled atlas. By jointly optimizing extraction, registration, segmentation, parcellation, network generation, and classification, UniBrain enhances both accuracy and computational efficiency while significantly reducing annotation effort. Experimental results demonstrate its superiority over existing methods across multiple tasks, offering a more scalable and reliable solution for neuroimaging analysis. Our code and data can be found at https://github.com/Anonymous7852/UniBrain

### Rewards-based image analysis in microscopy 
[[arxiv](https://arxiv.org/abs/2502.18522)] [[cool](https://papers.cool/arxiv/2502.18522)] [[pdf](https://arxiv.org/pdf/2502.18522)]
> **Authors**: Kamyar Barakati,Yu Liu,Utkarsh Pratiush,Boris N. Slautin,Sergei V. Kalinin
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-26
> **comment**: 38 pages, 11 figures
- **标题**: None
- **领域**: 图像和视频处理,材料科学,计算机视觉和模式识别,机器学习,应用物理
- **Abstract**: Analyzing imaging and hyperspectral data is crucial across scientific fields, including biology, medicine, chemistry, and physics. The primary goal is to transform high-resolution or high-dimensional data into an interpretable format to generate actionable insights, aiding decision-making and advancing knowledge. Currently, this task relies on complex, human-designed workflows comprising iterative steps such as denoising, spatial sampling, keypoint detection, feature generation, clustering, dimensionality reduction, and physics-based deconvolutions. The introduction of machine learning over the past decade has accelerated tasks like image segmentation and object detection via supervised learning, and dimensionality reduction via unsupervised methods. However, both classical and NN-based approaches still require human input, whether for hyperparameter tuning, data labeling, or both. The growing use of automated imaging tools, from atomically resolved imaging to biological applications, demands unsupervised methods that optimize data representation for human decision-making or autonomous experimentation. Here, we discuss advances in reward-based workflows, which adopt expert decision-making principles and demonstrate strong transfer learning across diverse tasks. We represent image analysis as a decision-making process over possible operations and identify desiderata and their mappings to classical decision-making frameworks. Reward-driven workflows enable a shift from supervised, black-box models sensitive to distribution shifts to explainable, unsupervised, and robust optimization in image analysis. They can function as wrappers over classical and DCNN-based methods, making them applicable to both unsupervised and supervised workflows (e.g., classification, regression for structure-property mapping) across imaging and hyperspectral data.

### FreeTumor: Large-Scale Generative Tumor Synthesis in Computed Tomography Images for Improving Tumor Recognition 
[[arxiv](https://arxiv.org/abs/2502.18519)] [[cool](https://papers.cool/arxiv/2502.18519)] [[pdf](https://arxiv.org/pdf/2502.18519)]
> **Authors**: Linshan Wu,Jiaxin Zhuang,Yanning Zhou,Sunan He,Jiabo Ma,Luyang Luo,Xi Wang,Xuefeng Ni,Xiaoling Zhong,Mingxiang Wu,Yinghua Zhao,Xiaohui Duan,Varut Vardhanabhuti,Pranav Rajpurkar,Hao Chen
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **Abstract**: Tumor is a leading cause of death worldwide, with an estimated 10 million deaths attributed to tumor-related diseases every year. AI-driven tumor recognition unlocks new possibilities for more precise and intelligent tumor screening and diagnosis. However, the progress is heavily hampered by the scarcity of annotated datasets, which demands extensive annotation efforts by radiologists. To tackle this challenge, we introduce FreeTumor, an innovative Generative AI (GAI) framework to enable large-scale tumor synthesis for mitigating data scarcity. Specifically, FreeTumor effectively leverages a combination of limited labeled data and large-scale unlabeled data for tumor synthesis training. Unleashing the power of large-scale data, FreeTumor is capable of synthesizing a large number of realistic tumors on images for augmenting training datasets. To this end, we create the largest training dataset for tumor synthesis and recognition by curating 161,310 publicly available Computed Tomography (CT) volumes from 33 sources, with only 2.3% containing annotated tumors. To validate the fidelity of synthetic tumors, we engaged 13 board-certified radiologists in a Visual Turing Test to discern between synthetic and real tumors. Rigorous clinician evaluation validates the high quality of our synthetic tumors, as they achieved only 51.1% sensitivity and 60.8% accuracy in distinguishing our synthetic tumors from real ones. Through high-quality tumor synthesis, FreeTumor scales up the recognition training datasets by over 40 times, showcasing a notable superiority over state-of-the-art AI methods including various synthesis methods and foundation models. These findings indicate promising prospects of FreeTumor in clinical applications, potentially advancing tumor treatments and improving the survival rates of patients.

### Exploring Patient Data Requirements in Training Effective AI Models for MRI-based Breast Cancer Classification 
[[arxiv](https://arxiv.org/abs/2502.18506)] [[cool](https://papers.cool/arxiv/2502.18506)] [[pdf](https://arxiv.org/pdf/2502.18506)]
> **Authors**: Solha Kang,Wesley De Neve,Francois Rameau,Utku Ozbulak
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-26
> **comment**: Accepted for publication in MICCAI 2024DeepBreast Workshop onAIand Imaging for Diagnostic and Treatment Challenges in Breast Care
- **标题**: None
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **Abstract**: The past decade has witnessed a substantial increase in the number of startups and companies offering AI-based solutions for clinical decision support in medical institutions. However, the critical nature of medical decision-making raises several concerns about relying on external software. Key issues include potential variations in image modalities and the medical devices used to obtain these images, potential legal issues, and adversarial attacks. Fortunately, the open-source nature of machine learning research has made foundation models publicly available and straightforward to use for medical applications. This accessibility allows medical institutions to train their own AI-based models, thereby mitigating the aforementioned concerns. Given this context, an important question arises: how much data do medical institutions need to train effective AI models? In this study, we explore this question in relation to breast cancer detection, a particularly contested area due to the prevalence of this disease, which affects approximately 1 in every 8 women. Through large-scale experiments on various patient sizes in the training set, we show that medical institutions do not need a decade's worth of MRI images to train an AI model that performs competitively with the state-of-the-art, provided the model leverages foundation models. Furthermore, we observe that for patient counts greater than 50, the number of patients in the training set has a negligible impact on the performance of models and that simple ensembles further improve the results without additional complexity.

### Liver Cirrhosis Stage Estimation from MRI with Deep Learning 
[[arxiv](https://arxiv.org/abs/2502.18225)] [[cool](https://papers.cool/arxiv/2502.18225)] [[pdf](https://arxiv.org/pdf/2502.18225)]
> **Authors**: Jun Zeng,Debesh Jha,Ertugrul Aktas,Elif Keles,Alpay Medetalibeyoglu,Matthew Antalek,Amir A. Borhani,Daniela P. Ladner,Gorkem Durak,Ulas Bagci
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-26
> **comment**: 8 pages, 1 figure
- **标题**: None
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **Abstract**: We present an end-to-end deep learning framework for automated liver cirrhosis stage estimation from multi-sequence MRI. Cirrhosis is the severe scarring (fibrosis) of the liver and a common endpoint of various chronic liver diseases. Early diagnosis is vital to prevent complications such as decompensation and cancer, which significantly decreases life expectancy. However, diagnosing cirrhosis in its early stages is challenging, and patients often present with life-threatening complications. Our approach integrates multi-scale feature learning with sequence-specific attention mechanisms to capture subtle tissue variations across cirrhosis progression stages. Using CirrMRI600+, a large-scale publicly available dataset of 628 high-resolution MRI scans from 339 patients, we demonstrate state-of-the-art performance in three-stage cirrhosis classification. Our best model achieves 72.8% accuracy on T1W and 63.8% on T2W sequences, significantly outperforming traditional radiomics-based approaches. Through extensive ablation studies, we show that our architecture effectively learns stage-specific imaging biomarkers. We establish new benchmarks for automated cirrhosis staging and provide insights for developing clinically applicable deep learning systems. The source code will be available at https://github.com/JunZengz/CirrhosisStage.

### VesselSAM: Leveraging SAM for Aortic Vessel Segmentation with LoRA and Atrous Attention 
[[arxiv](https://arxiv.org/abs/2502.18185)] [[cool](https://papers.cool/arxiv/2502.18185)] [[pdf](https://arxiv.org/pdf/2502.18185)]
> **Authors**: Adnan Iltaf,Rayan Merghani Ahmed,Bin Li,Shoujun Zhou
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: Submitted to IEEE JBHI
- **标题**: None
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **Abstract**: Medical image segmentation is crucial for clinical diagnosis and treatment planning, particularly for complex anatomical structures like vessels. In this work, we propose VesselSAM, a modified version of the Segmentation Anything Model (SAM), specifically designed for aortic vessel segmentation. VesselSAM incorporates AtrousLoRA, a novel module that combines Atrous Attention with Low-Rank Adaptation (LoRA), to improve segmentation performance. Atrous Attention enables the model to capture multi-scale contextual information, preserving both fine local details and broader global context. At the same time, LoRA facilitates efficient fine-tuning of the frozen SAM image encoder, reducing the number of trainable parameters and ensuring computational efficiency. We evaluate VesselSAM on two challenging datasets: the Aortic Vessel Tree (AVT) dataset and the Type-B Aortic Dissection (TBAD) dataset. VesselSAM achieves state-of-the-art performance with DSC scores of 93.50\%, 93.25\%, 93.02\%, and 93.26\% across multiple medical centers. Our results demonstrate that VesselSAM delivers high segmentation accuracy while significantly reducing computational overhead compared to existing large-scale models. This development paves the way for enhanced AI-based aortic vessel segmentation in clinical environments. The code and models will be released at https://github.com/Adnan-CAS/AtrousLora.

### 3D Anatomical Structure-guided Deep Learning for Accurate Diffusion Microstructure Imaging 
[[arxiv](https://arxiv.org/abs/2502.17933)] [[cool](https://papers.cool/arxiv/2502.17933)] [[pdf](https://arxiv.org/pdf/2502.17933)]
> **Authors**: Xinrui Ma,Jian Cheng,Wenxin Fan,Ruoyou Wu,Yongquan Ye,Shanshan Wang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: Diffusion magnetic resonance imaging (dMRI) is a crucial non-invasive technique for exploring the microstructure of the living human brain. Traditional hand-crafted and model-based tissue microstructure reconstruction methods often require extensive diffusion gradient sampling, which can be time-consuming and limits the clinical applicability of tissue microstructure information. Recent advances in deep learning have shown promise in microstructure estimation; however, accurately estimating tissue microstructure from clinically feasible dMRI scans remains challenging without appropriate constraints. This paper introduces a novel framework that achieves high-fidelity and rapid diffusion microstructure imaging by simultaneously leveraging anatomical information from macro-level priors and mutual information across parameters. This approach enhances time efficiency while maintaining accuracy in microstructure estimation. Experimental results demonstrate that our method outperforms four state-of-the-art techniques, achieving a peak signal-to-noise ratio (PSNR) of 30.51$\pm$0.58 and a structural similarity index measure (SSIM) of 0.97$\pm$0.004 in estimating parametric maps of multiple diffusion models. Notably, our method achieves a 15$\times$ acceleration compared to the dense sampling approach, which typically utilizes 270 diffusion gradients.

### A graph neural network-based multispectral-view learning model for diabetic macular ischemia detection from color fundus photographs 
[[arxiv](https://arxiv.org/abs/2502.17886)] [[cool](https://papers.cool/arxiv/2502.17886)] [[pdf](https://arxiv.org/pdf/2502.17886)]
> **Authors**: Qinghua He,Hongyang Jiang,Danqi Fang,Dawei Yang,Truong X. Nguyen,Anran Ran,Clement C. Tham,Simon K. H. Szeto,Sobha Sivaprasad,Carol Y. Cheung
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **Abstract**: Diabetic macular ischemia (DMI), marked by the loss of retinal capillaries in the macular area, contributes to vision impairment in patients with diabetes. Although color fundus photographs (CFPs), combined with artificial intelligence (AI), have been extensively applied in detecting various eye diseases, including diabetic retinopathy (DR), their applications in detecting DMI remain unexplored, partly due to skepticism among ophthalmologists regarding its feasibility. In this study, we propose a graph neural network-based multispectral view learning (GNN-MSVL) model designed to detect DMI from CFPs. The model leverages higher spectral resolution to capture subtle changes in fundus reflectance caused by ischemic tissue, enhancing sensitivity to DMI-related features. The proposed approach begins with computational multispectral imaging (CMI) to reconstruct 24-wavelength multispectral fundus images from CFPs. ResNeXt101 is employed as the backbone for multi-view learning to extract features from the reconstructed images. Additionally, a GNN with a customized jumper connection strategy is designed to enhance cross-spectral relationships, facilitating comprehensive and efficient multispectral view learning. The study included a total of 1,078 macula-centered CFPs from 1,078 eyes of 592 patients with diabetes, of which 530 CFPs from 530 eyes of 300 patients were diagnosed with DMI. The model achieved an accuracy of 84.7 percent and an area under the receiver operating characteristic curve (AUROC) of 0.900 (95 percent CI: 0.852-0.937) on eye-level, outperforming both the baseline model trained from CFPs and human experts (p-values less than 0.01). These findings suggest that AI-based CFP analysis holds promise for detecting DMI, contributing to its early and low-cost screening.

## 系统与控制(eess.SY:Systems and Control)

### Sample-efficient diffusion-based control of complex nonlinear systems 
[[arxiv](https://arxiv.org/abs/2502.17893)] [[cool](https://papers.cool/arxiv/2502.17893)] [[pdf](https://arxiv.org/pdf/2502.17893)]
> **Authors**: Hongyi Chen,Jingtao Ding,Jianhai Shu,Xinchun Yu,Xiaojun Liang,Yong Li,Xiao-Ping Zhang
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 系统与控制,人工智能,机器学习
- **Abstract**: Complex nonlinear system control faces challenges in achieving sample-efficient, reliable performance. While diffusion-based methods have demonstrated advantages over classical and reinforcement learning approaches in long-term control performance, they are limited by sample efficiency. This paper presents SEDC (Sample-Efficient Diffusion-based Control), a novel diffusion-based control framework addressing three core challenges: high-dimensional state-action spaces, nonlinear system dynamics, and the gap between non-optimal training data and near-optimal control solutions. Through three innovations - Decoupled State Diffusion, Dual-Mode Decomposition, and Guided Self-finetuning - SEDC achieves 39.5\%-49.4\% better control accuracy than baselines while using only 10\% of the training samples, as validated across three complex nonlinear dynamic systems. Our approach represents a significant advancement in sample-efficient control of complex nonlinear systems. The implementation of the code can be found at https://anonymous.4open.science/r/DIFOCON-C019.

## 几何拓扑(math.GT:Geometric Topology)

### Colored Jones Polynomials and the Volume Conjecture 
[[arxiv](https://arxiv.org/abs/2502.18575)] [[cool](https://papers.cool/arxiv/2502.18575)] [[pdf](https://arxiv.org/pdf/2502.18575)]
> **Authors**: Mark Hughes,Vishnu Jejjala,P. Ramadevi,Pratik Roy,Vivek Kumar Singh
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 27 pages, 16 figures
- **标题**: None
- **领域**: 几何拓扑,机器学习,高能物理 - 理论
- **Abstract**: Using the vertex model approach for braid representations, we compute polynomials for spin-1 placed on hyperbolic knots up to 15 crossings. These polynomials are referred to as 3-colored Jones polynomials or adjoint Jones polynomials. Training a subset of the data using a fully connected feedforward neural network, we predict the volume of the knot complement of hyperbolic knots from the adjoint Jones polynomial or its evaluations with 99.34% accuracy. A function of the adjoint Jones polynomial evaluated at the phase $q=e^{ 8 πi / 15 }$ predicts the volume with nearly the same accuracy as the neural network. From an analysis of 2-colored and 3-colored Jones polynomials, we conjecture the best phase for $n$-colored Jones polynomials, and use this hypothesis to motivate an improved statement of the volume conjecture. This is tested for knots for which closed form expressions for the $n$-colored Jones polynomial are known, and we show improved convergence to the volume.

## 可能性(math.PR:Probability)

### Tight Bounds on the Binomial CDF, and the Minimum of i.i.d Binomials, in terms of KL-Divergence 
[[arxiv](https://arxiv.org/abs/2502.18611)] [[cool](https://papers.cool/arxiv/2502.18611)] [[pdf](https://arxiv.org/pdf/2502.18611)]
> **Authors**: Xiaohan Zhu,Mesrob I. Ohannessian,Nathan Srebro
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 可能性,机器学习,机器学习
- **Abstract**: We provide finite sample upper and lower bounds on the Binomial tail probability which are a direct application of Sanov's theorem. We then use these to obtain high probability upper and lower bounds on the minimum of i.i.d. Binomial random variables. Both bounds are finite sample, asymptotically tight, and expressed in terms of the KL-divergence.

### Global law of conjugate kernel random matrices with heavy-tailed weights 
[[arxiv](https://arxiv.org/abs/2502.18428)] [[cool](https://papers.cool/arxiv/2502.18428)] [[pdf](https://arxiv.org/pdf/2502.18428)]
> **Authors**: Alice Guionnet,Vanessa Piccolo
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 45 pages, 1 figure
- **标题**: None
- **领域**: 可能性,机器学习,机器学习
- **Abstract**: We study the asymptotic spectral behavior of the conjugate kernel random matrix $YY^\top$, where $Y= f(WX)$ arises from a two-layer neural network model. We consider the setting where $W$ and $X$ are both random rectangular matrices with i.i.d. entries, where the entries of $W$ follow a heavy-tailed distribution, while those of $X$ have light tails. Our assumptions on $W$ include a broad class of heavy-tailed distributions, such as symmetric $α$-stable laws with $α\in (0,2)$ and sparse matrices with $\mathcal{O}(1)$ nonzero entries per row. The activation function $f$, applied entrywise, is nonlinear, smooth, and odd. By computing the eigenvalue distribution of $YY^\top$ through its moments, we show that heavy-tailed weights induce strong correlations between the entries of $Y$, leading to richer and fundamentally different spectral behavior compared to models with light-tailed weights.

## 统计理论(math.ST:Statistics Theory)

### Learning sparse generalized linear models with binary outcomes via iterative hard thresholding 
[[arxiv](https://arxiv.org/abs/2502.18393)] [[cool](https://papers.cool/arxiv/2502.18393)] [[pdf](https://arxiv.org/pdf/2502.18393)]
> **Authors**: Namiko Matsumoto,Arya Mazumdar
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 统计理论,数据结构和算法,信息论,机器学习,机器学习
- **Abstract**: In statistics, generalized linear models (GLMs) are widely used for modeling data and can expressively capture potential nonlinear dependence of the model's outcomes on its covariates. Within the broad family of GLMs, those with binary outcomes, which include logistic and probit regressions, are motivated by common tasks such as binary classification with (possibly) non-separable data. In addition, in modern machine learning and statistics, data is often high-dimensional yet has a low intrinsic dimension, making sparsity constraints in models another reasonable consideration. In this work, we propose to use and analyze an iterative hard thresholding (projected gradient descent on the ReLU loss) algorithm, called binary iterative hard thresholding (BIHT), for parameter estimation in sparse GLMs with binary outcomes. We establish that BIHT is statistically efficient and converges to the correct solution for parameter estimation in a general class of sparse binary GLMs. Unlike many other methods for learning GLMs, including maximum likelihood estimation, generalized approximate message passing, and GLM-tron (Kakade et al. 2011; Bahmani et al. 2016), BIHT does not require knowledge of the GLM's link function, offering flexibility and generality in allowing the algorithm to learn arbitrary binary GLMs. As two applications, logistic and probit regression are additionally studied. In this regard, it is shown that in logistic regression, the algorithm is in fact statistically optimal in the sense that the order-wise sample complexity matches (up to logarithmic factors) the lower bound obtained previously. To the best of our knowledge, this is the first work achieving statistical optimality for logistic regression in all noise regimes with a computationally efficient algorithm. Moreover, for probit regression, our sample complexity is on the same order as that obtained for logistic regression.

## 加速器物理(physics.acc-ph:Accelerator Physics)

### Adaptive conditional latent diffusion maps beam loss to 2D phase space projections 
[[arxiv](https://arxiv.org/abs/2502.18684)] [[cool](https://papers.cool/arxiv/2502.18684)] [[pdf](https://arxiv.org/pdf/2502.18684)]
> **Authors**: Alexander Scheinker,Alan Williams
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 加速器物理,机器学习
- **Abstract**: Beam loss (BLM) and beam current monitors (BCM) are ubiquitous at particle accelerator around the world. These simple devices provide non-invasive high level beam measurements, but give no insight into the detailed 6D (x,y,z,px,py,pz) beam phase space distributions or dynamics. We show that generative conditional latent diffusion models can learn intricate patterns to map waveforms of tens of BLMs or BCMs along an accelerator to detailed 2D projections of a charged particle beam's 6D phase space density. This transformational method can be used at any particle accelerator to transform simple non-invasive devices into detailed beam phase space diagnostics. We demonstrate this concept via multi-particle simulations of the high intensity beam in the kilometer-long LANSCE linear proton accelerator.

## 计算物理(physics.comp-ph:Computational Physics)

### Learning atomic forces from uncertainty-calibrated adversarial attacks 
[[arxiv](https://arxiv.org/abs/2502.18314)] [[cool](https://papers.cool/arxiv/2502.18314)] [[pdf](https://arxiv.org/pdf/2502.18314)]
> **Authors**: Henrique Musseli Cezar,Tilmann Bodenstein,Henrik Andersen Sveinsson,Morten Ledum,Simen Reine,Sigbjørn Løland Bore
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 计算物理,机器学习
- **Abstract**: Adversarial approaches, which intentionally challenge machine learning models by generating difficult examples, are increasingly being adopted to improve machine learning interatomic potentials (MLIPs). While already providing great practical value, little is known about the actual prediction errors of MLIPs on adversarial structures and whether these errors can be controlled. We propose the Calibrated Adversarial Geometry Optimization (CAGO) algorithm to discover adversarial structures with user-assigned errors. Through uncertainty calibration, the estimated uncertainty of MLIPs is unified with real errors. By performing geometry optimization for calibrated uncertainty, we reach adversarial structures with the user-assigned target MLIP prediction error. Integrating with active learning pipelines, we benchmark CAGO, demonstrating stable MLIPs that systematically converge structural, dynamical, and thermodynamical properties for liquid water and water adsorption in a metal-organic framework within only hundreds of training structures, where previously many thousands were typically required.

## 神经元和认知(q-bio.NC:Neurons and Cognition)

### Bridging Critical Gaps in Convergent Learning: How Representational Alignment Evolves Across Layers, Training, and Distribution Shifts 
[[arxiv](https://arxiv.org/abs/2502.18710)] [[cool](https://papers.cool/arxiv/2502.18710)] [[pdf](https://arxiv.org/pdf/2502.18710)]
> **Authors**: Chaitanya Kapoor,Sudhanshu Srivastava,Meenakshi Khosla
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 神经元和认知,人工智能
- **Abstract**: Understanding convergent learning -- the extent to which artificial and biological neural networks develop similar representations -- is crucial for neuroscience and AI, as it reveals shared learning principles and guides brain-like model design. While several studies have noted convergence in early and late layers of vision networks, key gaps remain. First, much existing work relies on a limited set of metrics, overlooking transformation invariances required for proper alignment. We compare three metrics that ignore specific irrelevant transformations: linear regression (ignoring affine transformations), Procrustes (ignoring rotations and reflections), and permutation/soft-matching (ignoring unit order). Notably, orthogonal transformations align representations nearly as effectively as more flexible linear ones, and although permutation scores are lower, they significantly exceed chance, indicating a robust representational basis. A second critical gap lies in understanding when alignment emerges during training. Contrary to expectations that convergence builds gradually with task-specific learning, our findings reveal that nearly all convergence occurs within the first epoch -- long before networks achieve optimal performance. This suggests that shared input statistics, architectural biases, or early training dynamics drive convergence rather than the final task solution. Finally, prior studies have not systematically examined how changes in input statistics affect alignment. Our work shows that out-of-distribution (OOD) inputs consistently amplify differences in later layers, while early layers remain aligned for both in-distribution and OOD inputs, suggesting that this alignment is driven by generalizable features stable across distribution shifts. These findings fill critical gaps in our understanding of representational convergence, with implications for neuroscience and AI.

### Deciphering Functions of Neurons in Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2502.18485)] [[cool](https://papers.cool/arxiv/2502.18485)] [[pdf](https://arxiv.org/pdf/2502.18485)]
> **Authors**: Jiaqi Xu,Cuiling Lan,Xuejin Chen,Yan Lu
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-26
> **comment**: 22 pages, 23 figures
- **标题**: None
- **领域**: 神经元和认知,计算机视觉和模式识别
- **Abstract**: The burgeoning growth of open-sourced vision-language models (VLMs) has catalyzed a plethora of applications across diverse domains. Ensuring the transparency and interpretability of these models is critical for fostering trustworthy and responsible AI systems. In this study, our objective is to delve into the internals of VLMs to interpret the functions of individual neurons. We observe the activations of neurons with respects to the input visual tokens and text tokens, and reveal some interesting findings. Particularly, we found that there are neurons responsible for only visual or text information, or both, respectively, which we refer to them as visual neurons, text neurons, and multi-modal neurons, respectively. We build a framework that automates the explanation of neurons with the assistant of GPT-4o. Meanwhile, for visual neurons, we propose an activation simulator to assess the reliability of the explanations for visual neurons. System statistical analyses on top of one representative VLM of LLaVA, uncover the behaviors/characteristics of different categories of neurons.

## 定量方法(q-bio.QM:Quantitative Methods)

### Exploring proteomic signatures in sepsis and non-infectious systemic inflammatory response syndrome 
[[arxiv](https://arxiv.org/abs/2502.18305)] [[cool](https://papers.cool/arxiv/2502.18305)] [[pdf](https://arxiv.org/pdf/2502.18305)]
> **Authors**: Adolfo Ruiz-Sanmartín,Vicent Ribas,David Suñol,Luis Chiscano-Camón,Laura Martín,Iván Bajaña,Juliana Bastida,Nieves Larrosa,Juan José González,M Dolores Carrasco,Núria Canela,Ricard Ferrer,Juan Carlos Ruiz-Rodrígue
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 定量方法,机器学习
- **Abstract**: Background: The search for new biomarkers that allow an early diagnosis in sepsis has become a necessity in medicine. The objective of this study is to identify potential protein biomarkers of differential expression between sepsis and non-infectious systemic inflammatory response syndrome (NISIRS). Methods: Prospective observational study of a cohort of septic patients activated by the Sepsis Code and patients admitted with NISIRS, during the period 2016-2017. A mass spectrometry-based approach was used to analyze the plasma proteins in the enrolled subjects. Subsequently, using recursive feature elimination (RFE) classification and cross-validation with a vector classifier, an association of these proteins in patients with sepsis compared to patients with NISIRS. The protein-protein interaction network was analyzed with String software. Results: A total of 277 patients (141 with sepsis and 136 with NISIRS) were included. After performing RFE, 25 proteins in the study patient cohort showed statistical significance, with an accuracy of 0.960, specificity of 0.920, sensitivity of 0.973, and an AUC of 0.985. Of these, 14 proteins (vWF, PPBP, C5, C1RL, FCN3, SAA2, ORM1, ITIH3, GSN, C1QA, CA1, CFB, C3, LBP) have a greater relationship with sepsis while 11 proteins (FN1, IGFALS, SERPINA4, APOE, APOH, C6, SERPINA3, AHSG, LUM, ITIH2, SAA1) are more expressed in NISIRS.

## 统计金融(q-fin.ST:Statistical Finance)

### Recurrent Neural Networks for Dynamic VWAP Execution: Adaptive Trading Strategies with Temporal Kolmogorov-Arnold Networks 
[[arxiv](https://arxiv.org/abs/2502.18177)] [[cool](https://papers.cool/arxiv/2502.18177)] [[pdf](https://arxiv.org/pdf/2502.18177)]
> **Authors**: Remi Genet
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 统计金融,机器学习
- **Abstract**: The execution of Volume Weighted Average Price (VWAP) orders remains a critical challenge in modern financial markets, particularly as trading volumes and market complexity continue to increase. In my previous work arXiv:2502.13722, I introduced a novel deep learning approach that demonstrated significant improvements over traditional VWAP execution methods by directly optimizing the execution problem rather than relying on volume curve predictions. However, that model was static because it employed the fully linear approach described in arXiv:2410.21448, which is not designed for dynamic adjustment. This paper extends that foundation by developing a dynamic neural VWAP framework that adapts to evolving market conditions in real time. We introduce two key innovations: first, the integration of recurrent neural networks to capture complex temporal dependencies in market dynamics, and second, a sophisticated dynamic adjustment mechanism that continuously optimizes execution decisions based on market feedback. The empirical analysis, conducted across five major cryptocurrency markets, demonstrates that this dynamic approach achieves substantial improvements over both traditional methods and our previous static implementation, with execution performance gains of 10 to 15% in liquid markets and consistent outperformance across varying conditions. These results suggest that adaptive neural architectures can effectively address the challenges of modern VWAP execution while maintaining computational efficiency suitable for practical deployment.

## 机器学习(stat.ML:Machine Learning)

### Nonlinear Sparse Generalized Canonical Correlation Analysis for Multi-view High-dimensional Data 
[[arxiv](https://arxiv.org/abs/2502.18756)] [[cool](https://papers.cool/arxiv/2502.18756)] [[pdf](https://arxiv.org/pdf/2502.18756)]
> **Authors**: Rong Wu,Ziqi Chen,Gen Li,Hai Shu
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Motivation: Biomedical studies increasingly produce multi-view high-dimensional datasets (e.g., multi-omics) that demand integrative analysis. Existing canonical correlation analysis (CCA) and generalized CCA methods address at most two of the following three key aspects simultaneously: (i) nonlinear dependence, (ii) sparsity for variable selection, and (iii) generalization to more than two data views. There is a pressing need for CCA methods that integrate all three aspects to effectively analyze multi-view high-dimensional data. Results: We propose three nonlinear, sparse, generalized CCA methods, HSIC-SGCCA, SA-KGCCA, and TS-KGCCA, for variable selection in multi-view high-dimensional data. These methods extend existing SCCA-HSIC, SA-KCCA, and TS-KCCA from two-view to multi-view settings. While SA-KGCCA and TS-KGCCA yield multi-convex optimization problems solved via block coordinate descent, HSIC-SGCCA introduces a necessary unit-variance constraint previously ignored in SCCA-HSIC, resulting in a nonconvex, non-multiconvex problem. We efficiently address this challenge by integrating the block prox-linear method with the linearized alternating direction method of multipliers. Simulations and TCGA-BRCA data analysis demonstrate that HSIC-SGCCA outperforms competing methods in multi-view variable selection.

### Learning and Computation of $Φ$-Equilibria at the Frontier of Tractability 
[[arxiv](https://arxiv.org/abs/2502.18582)] [[cool](https://papers.cool/arxiv/2502.18582)] [[pdf](https://arxiv.org/pdf/2502.18582)]
> **Authors**: Brian Hu Zhang,Ioannis Anagnostides,Emanuel Tewolde,Ratip Emin Berker,Gabriele Farina,Vincent Conitzer,Tuomas Sandholm
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机科学与博弈论,机器学习
- **Abstract**: $Φ$-equilibria -- and the associated notion of $Φ$-regret -- are a powerful and flexible framework at the heart of online learning and game theory, whereby enriching the set of deviations $Φ$ begets stronger notions of rationality. Recently, Daskalakis, Farina, Fishelson, Pipis, and Schneider (STOC '24) -- abbreviated as DFFPS -- settled the existence of efficient algorithms when $Φ$ contains only linear maps under a general, $d$-dimensional convex constraint set $\mathcal{X}$. In this paper, we significantly extend their work by resolving the case where $Φ$ is $k$-dimensional; degree-$\ell$ polynomials constitute a canonical such example with $k = d^{O(\ell)}$. In particular, positing only oracle access to $\mathcal{X}$, we obtain two main positive results: i) a $\text{poly}(n, d, k, \text{log}(1/ε))$-time algorithm for computing $ε$-approximate $Φ$-equilibria in $n$-player multilinear games, and ii) an efficient online algorithm that incurs average $Φ$-regret at most $ε$ using $\text{poly}(d, k)/ε^2$ rounds. We also show nearly matching lower bounds in the online learning setting, thereby obtaining for the first time a family of deviations that captures the learnability of $Φ$-regret. From a technical standpoint, we extend the framework of DFFPS from linear maps to the more challenging case of maps with polynomial dimension. At the heart of our approach is a polynomial-time algorithm for computing an expected fixed point of any $φ: \mathcal{X} \to \mathcal{X}$ based on the ellipsoid against hope (EAH) algorithm of Papadimitriou and Roughgarden (JACM '08). In particular, our algorithm for computing $Φ$-equilibria is based on executing EAH in a nested fashion -- each step of EAH itself being implemented by invoking a separate call to EAH.

### Applications of Statistical Field Theory in Deep Learning 
[[arxiv](https://arxiv.org/abs/2502.18553)] [[cool](https://papers.cool/arxiv/2502.18553)] [[pdf](https://arxiv.org/pdf/2502.18553)]
> **Authors**: Zohar Ringel,Noa Rubin,Edo Mor,Moritz Helias,Inbar Seroussi
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,无序系统和神经网络,人工智能,机器学习
- **Abstract**: Deep learning algorithms have made incredible strides in the past decade yet due to the complexity of these algorithms, the science of deep learning remains in its early stages. Being an experimentally driven field, it is natural to seek a theory of deep learning within the physics paradigm. As deep learning is largely about learning functions and distributions over functions, statistical field theory, a rich and versatile toolbox for tackling complex distributions over functions (fields) is an obvious choice of formalism. Research efforts carried out in the past few years have demonstrated the ability of field theory to provide useful insights on generalization, implicit bias, and feature learning effects. Here we provide a pedagogical review of this emerging line of research.

### Nested Expectations with Kernel Quadrature 
[[arxiv](https://arxiv.org/abs/2502.18284)] [[cool](https://papers.cool/arxiv/2502.18284)] [[pdf](https://arxiv.org/pdf/2502.18284)]
> **Authors**: Zonghao Chen,Masha Naslidnyk,François-Xavier Briol
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: This paper considers the challenging computational task of estimating nested expectations. Existing algorithms, such as nested Monte Carlo or multilevel Monte Carlo, are known to be consistent but require a large number of samples at both inner and outer levels to converge. Instead, we propose a novel estimator consisting of nested kernel quadrature estimators and we prove that it has a faster convergence rate than all baseline methods when the integrands have sufficient smoothness. We then demonstrate empirically that our proposed method does indeed require fewer samples to estimate nested expectations on real-world applications including Bayesian optimisation, option pricing, and health economics.

### Near-Optimal Approximations for Bayesian Inference in Function Space 
[[arxiv](https://arxiv.org/abs/2502.18279)] [[cool](https://papers.cool/arxiv/2502.18279)] [[pdf](https://arxiv.org/pdf/2502.18279)]
> **Authors**: Veit Wild,James Wu,Dino Sejdinovic,Jeremias Knoblauch
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: 59 pages (26 pages main paper + 33 pages appendices); 6 figures
- **标题**: None
- **领域**: 机器学习,机器学习,方法论
- **Abstract**: We propose a scalable inference algorithm for Bayes posteriors defined on a reproducing kernel Hilbert space (RKHS). Given a likelihood function and a Gaussian random element representing the prior, the corresponding Bayes posterior measure $Π_{\text{B}}$ can be obtained as the stationary distribution of an RKHS-valued Langevin diffusion. We approximate the infinite-dimensional Langevin diffusion via a projection onto the first $M$ components of the Kosambi-Karhunen-Loève expansion. Exploiting the thus obtained approximate posterior for these $M$ components, we perform inference for $Π_{\text{B}}$ by relying on the law of total probability and a sufficiency assumption. The resulting method scales as $O(M^3+JM^2)$, where $J$ is the number of samples produced from the posterior measure $Π_{\text{B}}$. Interestingly, the algorithm recovers the posterior arising from the sparse variational Gaussian process (SVGP) (see Titsias, 2009) as a special case, owed to the fact that the sufficiency assumption underlies both methods. However, whereas the SVGP is parametrically constrained to be a Gaussian process, our method is based on a non-parametric variational family $\mathcal{P}(\mathbb{R}^M)$ consisting of all probability measures on $\mathbb{R}^M$. As a result, our method is provably close to the optimal $M$-dimensional variational approximation of the Bayes posterior $Π_{\text{B}}$ in $\mathcal{P}(\mathbb{R}^M)$ for convex and Lipschitz continuous negative log likelihoods, and coincides with SVGP for the special case of a Gaussian error likelihood.

### Golden Ratio Mixing of Real and Synthetic Data for Stabilizing Generative Model Training 
[[arxiv](https://arxiv.org/abs/2502.18049)] [[cool](https://papers.cool/arxiv/2502.18049)] [[pdf](https://arxiv.org/pdf/2502.18049)]
> **Authors**: Hengzhi He,Shirong Xu,Guang Cheng
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-26
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Recent studies identified an intriguing phenomenon in recursive generative model training known as model collapse, where models trained on data generated by previous models exhibit severe performance degradation. Addressing this issue and developing more effective training strategies have become central challenges in generative model research. In this paper, we investigate this phenomenon theoretically within a novel framework, where generative models are iteratively trained on a combination of newly collected real data and synthetic data from the previous training step. To develop an optimal training strategy for integrating real and synthetic data, we evaluate the performance of a weighted training scheme in various scenarios, including Gaussian distribution estimation and linear regression. We theoretically characterize the impact of the mixing proportion and weighting scheme of synthetic data on the final model's performance. Our key finding is that, across different settings, the optimal weighting scheme under different proportions of synthetic data asymptotically follows a unified expression, revealing a fundamental trade-off between leveraging synthetic data and generative model performance. Notably, in some cases, the optimal weight assigned to real data corresponds precisely to the reciprocal of the golden ratio. Finally, we validate our theoretical results on extensive simulated datasets and a real tabular dataset.

## 其他论文

- [On Aggregation Queries over Predicted Nearest Neighbors](https://arxiv.org/abs/2502.18803)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR,cs.DB,cs.DS in whitelist
- [SolEval: Benchmarking Large Language Models for Repository-level Solidity Code Generation](https://arxiv.org/abs/2502.18793)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [CommGPT: A Graph and Retrieval-Augmented Multimodal Communication Foundation Model](https://arxiv.org/abs/2502.18763)
  - **标题**: None
  - **Filtered Reason**: none of cs.IT in whitelist
- [Training Large Recommendation Models via Graph-Language Token Alignment](https://arxiv.org/abs/2502.18757)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [M-ANT: Efficient Low-bit Group Quantization for LLMs via Mathematically Adaptive Numerical Type](https://arxiv.org/abs/2502.18755)
  - **标题**: None
  - **Filtered Reason**: none of cs.AR in whitelist
- [O-RIS-ing: Evaluating RIS-Assisted NextG Open RAN](https://arxiv.org/abs/2502.18753)
  - **标题**: None
  - **Filtered Reason**: none of eess.SP,cs.NI in whitelist
- [From Cluttered to Clear: Improving the Web Accessibility Design for Screen Reader Users in E-commerce With Generative AI](https://arxiv.org/abs/2502.18701)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Requirements-Driven Automated Software Testing: A Systematic Review](https://arxiv.org/abs/2502.18694)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Emerging Practices in Participatory AI Design in Public Sector Innovation](https://arxiv.org/abs/2502.18689)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Rapidly Built Medical Crash Cart! Lessons Learned and Impacts on High-Stakes Team Collaboration in the Emergency Room](https://arxiv.org/abs/2502.18688)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC,cs.RO in whitelist
- [Robots, Chatbots, Self-Driving Cars: Perceptions of Mind and Morality Across Artificial Intelligences](https://arxiv.org/abs/2502.18683)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Longitudinal Analysis of GPU Workloads on Perlmutter](https://arxiv.org/abs/2502.18680)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC in whitelist
- [Interacting with Thoughtful AI](https://arxiv.org/abs/2502.18676)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [PacQ: A SIMT Microarchitecture for Efficient Dataflow in Hyper-asymmetric GEMMs](https://arxiv.org/abs/2502.18627)
  - **标题**: None
  - **Filtered Reason**: none of cs.AR in whitelist
- [Investigating Youth AI Auditing](https://arxiv.org/abs/2502.18576)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC,cs.CY in whitelist
- [Disrupt Your Research Using Generative AI Powered ScienceSage](https://arxiv.org/abs/2502.18479)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [AI's Impact on Traditional Software Development](https://arxiv.org/abs/2502.18476)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Using LLM-Based Approaches to Enhance and Automate Topic Labeling](https://arxiv.org/abs/2502.18469)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [Empirical Research on Utilizing LLM-based Agents for Automated Bug Fixing via LangGraph](https://arxiv.org/abs/2502.18465)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Evaluating the Effectiveness of Small Language Models in Detecting Refactoring Bugs](https://arxiv.org/abs/2502.18454)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [CRESSim-MPM: A Material Point Method Library for Surgical Soft Body Simulation with Cutting and Suturing](https://arxiv.org/abs/2502.18437)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [Is OpenAlex Suitable for Research Quality Evaluation and Which Citation Indicator is Best?](https://arxiv.org/abs/2502.18427)
  - **标题**: None
  - **Filtered Reason**: none of cs.DL in whitelist
- [When Benchmarks Talk: Re-Evaluating Code LLMs with Interactive Feedback](https://arxiv.org/abs/2502.18413)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- ["Why do we do this?": Moral Stress and the Affective Experience of Ethics in Practice](https://arxiv.org/abs/2502.18395)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Semantic and Goal-oriented Wireless Network Coverage: The Area of Effectiveness](https://arxiv.org/abs/2502.18381)
  - **标题**: None
  - **Filtered Reason**: none of eess.SP,cs.NI in whitelist
- [Responsible AI Agents](https://arxiv.org/abs/2502.18359)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [Imperfect Knowledge Management (IKM) in GEFRED (GENeralized model for Fuzzy RElational Databases)](https://arxiv.org/abs/2502.18255)
  - **标题**: None
  - **Filtered Reason**: none of cs.DB in whitelist
- [From ChatGPT to DeepSeek: Can LLMs Simulate Humanity?](https://arxiv.org/abs/2502.18210)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [Intersubjective Model of AI-mediated Communication: Augmenting Human-Human Text Chat through LLM-based Adaptive Agent Pair](https://arxiv.org/abs/2502.18201)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Machine Learning for Future Wireless Communications: Channel Prediction Perspectives](https://arxiv.org/abs/2502.18196)
  - **标题**: None
  - **Filtered Reason**: none of eess.SP,cs.IT in whitelist
- [Carbon and Silicon, Coexist or Compete? A Survey on Human-AI Interactions in Agent-based Modeling and Simulation](https://arxiv.org/abs/2502.18145)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [To Deepfake or Not to Deepfake: Higher Education Stakeholders' Perceptions and Intentions towards Synthetic Media](https://arxiv.org/abs/2502.18066)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC,cs.CY in whitelist
- [Multimodal Interaction and Intention Communication for Industrial Robots](https://arxiv.org/abs/2502.17971)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC,cs.RO in whitelist
- [Quadrotor Neural Dead Reckoning in Periodic Trajectories](https://arxiv.org/abs/2502.17964)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO,eess.SP in whitelist
- [The Dynamics of Collective Creativity in Human-AI Social Networks](https://arxiv.org/abs/2502.17962)
  - **标题**: None
  - **Filtered Reason**: none of cs.SI in whitelist
- [Advising Agent for Supporting Human-Multi-Drone Team Collaboration](https://arxiv.org/abs/2502.17960)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Exploring K-12 Physical Education Teachers' Perspectives on Opportunities and Challenges of AI Integration Through Ideation Workshops](https://arxiv.org/abs/2502.17855)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
