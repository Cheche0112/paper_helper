> 本文由 [https://github.com/huiyeruzhou/arxiv_crawler](https://github.com/huiyeruzhou/arxiv_crawler) 自动生成
>
> 领域白名单：cs.AI,cs.CL,cs.LG,cs.CV
> 关键词： LLM, GPT, AI, language+model, deep+learning, transformer, neural+network, machine+learning

# 论文全览：2025-02-14

共有300篇相关领域论文, 另有51篇其他

## 宇宙学和非银河系天体物理学(astro-ph.CO:Cosmology and Nongalactic Astrophysics)

### $Λ$CDM and early dark energy in latent space: a data-driven parametrization of the CMB temperature power spectrum 
[[arxiv](https://arxiv.org/abs/2502.09810)] [[cool](https://papers.cool/arxiv/2502.09810)] [[pdf](https://arxiv.org/pdf/2502.09810)]
> **Authors**: Davide Piras,Laura Herold,Luisa Lucie-Smith,Eiichiro Komatsu
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 17 pages, 12 figures, comments welcome
- **标题**: None
- **领域**: 宇宙学和非银河系天体物理学,天体物理学仪器和方法,机器学习
- **Abstract**: Finding the best parametrization for cosmological models in the absence of first-principle theories is an open question. We propose a data-driven parametrization of cosmological models given by the disentangled 'latent' representation of a variational autoencoder (VAE) trained to compress cosmic microwave background (CMB) temperature power spectra. We consider a broad range of $Λ$CDM and beyond-$Λ$CDM cosmologies with an additional early dark energy (EDE) component. We show that these spectra can be compressed into 5 ($Λ$CDM) or 8 (EDE) independent latent parameters, as expected when using temperature power spectra alone, and which reconstruct spectra at an accuracy well within the Planck errors. These latent parameters have a physical interpretation in terms of well-known features of the CMB temperature spectrum: these include the position, height and even-odd modulation of the acoustic peaks, as well as the gravitational lensing effect. The VAE also discovers one latent parameter which entirely isolates the EDE effects from those related to $Λ$CDM parameters, thus revealing a previously unknown degree of freedom in the CMB temperature power spectrum. We further showcase how to place constraints on the latent parameters using Planck data as typically done for cosmological parameters, obtaining latent values consistent with previous $Λ$CDM and EDE cosmological constraints. Our work demonstrates the potential of a data-driven reformulation of current beyond-$Λ$CDM phenomenological models into the independent degrees of freedom to which the data observables are sensitive.

## 地球和行星天体物理学(astro-ph.EP:Earth and Planetary Astrophysics)

### ExoMiner++ on TESS with Transfer Learning from Kepler: Transit Classification and Vetting Catalog for 2-min Data 
[[arxiv](https://arxiv.org/abs/2502.09790)] [[cool](https://papers.cool/arxiv/2502.09790)] [[pdf](https://arxiv.org/pdf/2502.09790)]
> **Authors**: Hamed Valizadegan,Miguel J. S. Martinho,Jon M. Jenkins,Joseph D. Twicken,Douglas A. Caldwell,Patrick Maynard,Hongbo Wei,William Zhong,Charles Yates,Sam Donald,Karen A. Collins,David Latham,Khalid Barkaoui,Perry Berlind,Michael L. Calkins,Kylee Carden,Nikita Chazov,Gilbert A. Esquerdo,Tristan Guillot,Vadim Krushinsky,Grzegorz Nowak,Benjamin V. Rackham,Amaury Triaud,Richard P. Schwarz,Denise Stephens, et al. (4 additional authors not shown)
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 地球和行星天体物理学,天体物理学仪器和方法,机器学习
- **Abstract**: We present ExoMiner++, an enhanced deep learning model that builds on the success of ExoMiner to improve transit signal classification in 2-minute TESS data. ExoMiner++ incorporates additional diagnostic inputs, including periodogram, flux trend, difference image, unfolded flux, and spacecraft attitude control data, all of which are crucial for effectively distinguishing transit signals from more challenging sources of false positives. To further enhance performance, we leverage transfer learning from high-quality labeled data from the Kepler space telescope, mitigating the impact of TESS's noisier and more ambiguous labels. ExoMiner++ achieves high accuracy across various classification and ranking metrics, significantly narrowing the search space for follow-up investigations to confirm new planets. To serve the exoplanet community, we introduce new TESS catalogs containing ExoMiner++ classifications and confidence scores for each transit signal. Among the 147,568 unlabeled TCEs, ExoMiner++ identifies 7,330 as planet candidates, with the remainder classified as false positives. These 7,330 planet candidates correspond to 1,868 existing TESS Objects of Interest (TOIs), 69 Community TESS Objects of Interest (CTOIs), and 50 newly introduced CTOIs. 1,797 out of the 2,506 TOIs previously labeled as planet candidates in ExoFOP are classified as planet candidates by ExoMiner++. This reduction in plausible candidates combined with the excellent ranking quality of ExoMiner++ allows the follow-up efforts to be focused on the most likely candidates, increasing the overall planet yield.

## 材料科学(cond-mat.mtrl-sci:Materials Science)

### Atom identification in bilayer moire materials with Gomb-Net 
[[arxiv](https://arxiv.org/abs/2502.09791)] [[cool](https://papers.cool/arxiv/2502.09791)] [[pdf](https://arxiv.org/pdf/2502.09791)]
> **Authors**: Austin C. Houston,Sumner B. Harris,Hao Wang,Yu-Chuan Lin,David B. Geohegan,Kai Xiao,Gerd Duscher
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 材料科学,计算机视觉和模式识别
- **Abstract**: Moire patterns in van der Waals bilayer materials complicate the analysis of atomic-resolution images, hindering the atomic-scale insight typically attainable with scanning transmission electron microscopy. Here, we report a method to detect the positions and identities of atoms in each of the individual layers that compose twisted bilayer heterostructures. We developed a deep learning model, Gomb-Net, which identifies the coordinates and atomic species in each layer, effectively deconvoluting the moire pattern. This enables layer-specific mapping of quantities like strain and dopant distributions, unlike other commonly used segmentation models which struggle with moire-induced complexity. Using this approach, we explored the Se atom substitutional site distribution in a twisted fractional Janus WS2-WS2(1-x)Se2x heterostructure and found that layer-specific implantation sites are unaffected by the moire pattern's local energetic or electronic modulation. This advancement enables atom identification within material regimes where it was not possible before, opening new insights into previously inaccessible material physics.

### DiffRenderGAN: Addressing Training Data Scarcity in Deep Segmentation Networks for Quantitative Nanomaterial Analysis through Differentiable Rendering and Generative Modelling 
[[arxiv](https://arxiv.org/abs/2502.09477)] [[cool](https://papers.cool/arxiv/2502.09477)] [[pdf](https://arxiv.org/pdf/2502.09477)]
> **Authors**: Dennis Possart,Leonid Mill,Florian Vollnhals,Tor Hildebrand,Peter Suter,Mathis Hoffmann,Jonas Utz,Daniel Augsburger,Mareike Thies,Mingxuan Wu,Fabian Wagner,George Sarau,Silke Christiansen,Katharina Breininger
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 材料科学,计算机视觉和模式识别,机器学习
- **Abstract**: Nanomaterials exhibit distinctive properties governed by parameters such as size, shape, and surface characteristics, which critically influence their applications and interactions across technological, biological, and environmental contexts. Accurate quantification and understanding of these materials are essential for advancing research and innovation. In this regard, deep learning segmentation networks have emerged as powerful tools that enable automated insights and replace subjective methods with precise quantitative analysis. However, their efficacy depends on representative annotated datasets, which are challenging to obtain due to the costly imaging of nanoparticles and the labor-intensive nature of manual annotations. To overcome these limitations, we introduce DiffRenderGAN, a novel generative model designed to produce annotated synthetic data. By integrating a differentiable renderer into a Generative Adversarial Network (GAN) framework, DiffRenderGAN optimizes textural rendering parameters to generate realistic, annotated nanoparticle images from non-annotated real microscopy images. This approach reduces the need for manual intervention and enhances segmentation performance compared to existing synthetic data methods by generating diverse and realistic data. Tested on multiple ion and electron microscopy cases, including titanium dioxide (TiO$_2$), silicon dioxide (SiO$_2$)), and silver nanowires (AgNW), DiffRenderGAN bridges the gap between synthetic and real data, advancing the quantification and understanding of complex nanomaterial systems.

### Transformer-Enhanced Variational Autoencoder for Crystal Structure Prediction 
[[arxiv](https://arxiv.org/abs/2502.09423)] [[cool](https://papers.cool/arxiv/2502.09423)] [[pdf](https://arxiv.org/pdf/2502.09423)]
> **Authors**: Ziyi Chen,Yang Yuan,Siming Zheng,Jialong Guo,Sihan Liang,Yangang Wang,Zongguo Wang
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 材料科学,人工智能
- **Abstract**: Crystal structure forms the foundation for understanding the physical and chemical properties of materials. Generative models have emerged as a new paradigm in crystal structure prediction(CSP), however, accurately capturing key characteristics of crystal structures, such as periodicity and symmetry, remains a significant challenge. In this paper, we propose a Transformer-Enhanced Variational Autoencoder for Crystal Structure Prediction (TransVAE-CSP), who learns the characteristic distribution space of stable materials, enabling both the reconstruction and generation of crystal structures. TransVAE-CSP integrates adaptive distance expansion with irreducible representation to effectively capture the periodicity and symmetry of crystal structures, and the encoder is a transformer network based on an equivariant dot product attention mechanism. Experimental results on the carbon_24, perov_5, and mp_20 datasets demonstrate that TransVAE-CSP outperforms existing methods in structure reconstruction and generation tasks under various modeling metrics, offering a powerful tool for crystal structure design and optimization.

## 人工智能(cs.AI:Artificial Intelligence)

### AutoS$^2$earch: Unlocking the Reasoning Potential of Large Models for Web-based Source Search 
[[arxiv](https://arxiv.org/abs/2502.09913)] [[cool](https://papers.cool/arxiv/2502.09913)] [[pdf](https://arxiv.org/pdf/2502.09913)]
> **Authors**: Zhengqiu Zhu,Yatai Ji,Jiaheng Huang,Yong Zhao,Sihang Qiu,Rusheng Ju
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,人机交互
- **Abstract**: Web-based management systems have been widely used in risk control and industrial safety. However, effectively integrating source search capabilities into these systems, to enable decision-makers to locate and address the hazard (e.g., gas leak detection) remains a challenge. While prior efforts have explored using web crowdsourcing and AI algorithms for source search decision support, these approaches suffer from overheads in recruiting human participants and slow response times in time-sensitive situations. To address this, we introduce AutoS$^2$earch, a novel framework leveraging large models for zero-shot source search in web applications. AutoS$^2$earch operates on a simplified visual environment projected through a web-based display, utilizing a chain-of-thought prompt designed to emulate human reasoning. The multi-modal large language model (MLLMs) dynamically converts visual observations into language descriptions, enabling the LLM to perform linguistic reasoning on four directional choices. Extensive experiments demonstrate that AutoS$^2$earch achieves performance nearly equivalent to human-AI collaborative source search while eliminating dependency on crowdsourced labor. Our work offers valuable insights in using web engineering to design such autonomous systems in other industrial applications.

### The Ann Arbor Architecture for Agent-Oriented Programming 
[[arxiv](https://arxiv.org/abs/2502.09903)] [[cool](https://papers.cool/arxiv/2502.09903)] [[pdf](https://arxiv.org/pdf/2502.09903)]
> **Authors**: Wei Dong
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,人机交互,软件工程
- **Abstract**: In this paper, we reexamine prompt engineering for large language models through the lens of automata theory. We argue that language models function as automata and, like all automata, should be programmed in the languages they accept, a unified collection of all natural and formal languages. Therefore, traditional software engineering practices--conditioned on the clear separation of programming languages and natural languages--must be rethought. We introduce the Ann Arbor Architecture, a conceptual framework for agent-oriented programming of language models, as a higher-level abstraction over raw token generation, and provide a new perspective on in-context learning. Based on this framework, we present the design of our agent platform Postline, and report on our initial experiments in agent training.

### Artificial Intelligence in Spectroscopy: Advancing Chemistry from Prediction to Generation and Beyond 
[[arxiv](https://arxiv.org/abs/2502.09897)] [[cool](https://papers.cool/arxiv/2502.09897)] [[pdf](https://arxiv.org/pdf/2502.09897)]
> **Authors**: Kehan Guo,Yili Shen,Gisela Abigail Gonzalez-Montiel,Yue Huang,Yujun Zhou,Mihir Surve,Zhichun Guo,Prayel Das,Nitesh V Chawla,Olaf Wiest,Xiangliang Zhang
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: The rapid advent of machine learning (ML) and artificial intelligence (AI) has catalyzed major transformations in chemistry, yet the application of these methods to spectroscopic and spectrometric data, referred to as Spectroscopy Machine Learning (SpectraML), remains relatively underexplored. Modern spectroscopic techniques (MS, NMR, IR, Raman, UV-Vis) generate an ever-growing volume of high-dimensional data, creating a pressing need for automated and intelligent analysis beyond traditional expert-based workflows. In this survey, we provide a unified review of SpectraML, systematically examining state-of-the-art approaches for both forward tasks (molecule-to-spectrum prediction) and inverse tasks (spectrum-to-molecule inference). We trace the historical evolution of ML in spectroscopy, from early pattern recognition to the latest foundation models capable of advanced reasoning, and offer a taxonomy of representative neural architectures, including graph-based and transformer-based methods. Addressing key challenges such as data quality, multimodal integration, and computational scalability, we highlight emerging directions such as synthetic data generation, large-scale pretraining, and few- or zero-shot learning. To foster reproducible research, we also release an open-source repository containing recent papers and their corresponding curated datasets (https://github.com/MINE-Lab-ND/SpectrumML_Survey_Papers). Our survey serves as a roadmap for researchers, guiding progress at the intersection of spectroscopy and AI.

### A Scoresheet for Explainable AI 
[[arxiv](https://arxiv.org/abs/2502.09861)] [[cool](https://papers.cool/arxiv/2502.09861)] [[pdf](https://arxiv.org/pdf/2502.09861)]
> **Authors**: Michael Winikoff,John Thangarajah,Sebastian Rodriguez
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: To appear at AAMAS 2025 - arXiv version also includes appendices
- **标题**: None
- **领域**: 人工智能,多代理系统,软件工程
- **Abstract**: Explainability is important for the transparency of autonomous and intelligent systems and for helping to support the development of appropriate levels of trust. There has been considerable work on developing approaches for explaining systems and there are standards that specify requirements for transparency. However, there is a gap: the standards are too high-level and do not adequately specify requirements for explainability. This paper develops a scoresheet that can be used to specify explainability requirements or to assess the explainability aspects provided for particular applications. The scoresheet is developed by considering the requirements of a range of stakeholders and is applicable to Multiagent Systems as well as other AI technologies. We also provide guidance for how to use the scoresheet and illustrate its generality and usefulness by applying it to a range of applications.

### MuDoC: An Interactive Multimodal Document-grounded Conversational AI System 
[[arxiv](https://arxiv.org/abs/2502.09843)] [[cool](https://papers.cool/arxiv/2502.09843)] [[pdf](https://arxiv.org/pdf/2502.09843)]
> **Authors**: Karan Taneja,Ashok K. Goel
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 5 pages, 3 figures, AAAI-MAKE 2025
- **标题**: None
- **领域**: 人工智能,人机交互,多媒体
- **Abstract**: Multimodal AI is an important step towards building effective tools to leverage multiple modalities in human-AI communication. Building a multimodal document-grounded AI system to interact with long documents remains a challenge. Our work aims to fill the research gap of directly leveraging grounded visuals from documents alongside textual content in documents for response generation. We present an interactive conversational AI agent 'MuDoC' based on GPT-4o to generate document-grounded responses with interleaved text and figures. MuDoC's intelligent textbook interface promotes trustworthiness and enables verification of system responses by allowing instant navigation to source text and figures in the documents. We also discuss qualitative observations based on MuDoC responses highlighting its strengths and limitations.

### Imit Diff: Semantics Guided Diffusion Transformer with Dual Resolution Fusion for Imitation Learning 
[[arxiv](https://arxiv.org/abs/2502.09649)] [[cool](https://papers.cool/arxiv/2502.09649)] [[pdf](https://arxiv.org/pdf/2502.09649)]
> **Authors**: Yuhang Dong,Haizhou Ge,Yupei Zeng,Jiangning Zhang,Beiwen Tian,Guanzhong Tian,Hongrui Zhu,Yufei Jia,Ruixiang Wang,Ran Yi,Guyue Zhou,Longhua Ma
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算机视觉和模式识别,机器学习,机器人技术
- **Abstract**: Visuomotor imitation learning enables embodied agents to effectively acquire manipulation skills from video demonstrations and robot proprioception. However, as scene complexity and visual distractions increase, existing methods that perform well in simple scenes tend to degrade in performance. To address this challenge, we introduce Imit Diff, a semanstic guided diffusion transformer with dual resolution fusion for imitation learning. Our approach leverages prior knowledge from vision language foundation models to translate high-level semantic instruction into pixel-level visual localization. This information is explicitly integrated into a multi-scale visual enhancement framework, constructed with a dual resolution encoder. Additionally, we introduce an implementation of Consistency Policy within the diffusion transformer architecture to improve both real-time performance and motion smoothness in embodied agent control.We evaluate Imit Diff on several challenging real-world tasks. Due to its task-oriented visual localization and fine-grained scene perception, it significantly outperforms state-of-the-art methods, especially in complex scenes with visual distractions, including zero-shot experiments focused on visual distraction and category generalization. The code will be made publicly available.

### Efficient and Trustworthy Block Propagation for Blockchain-enabled Mobile Embodied AI Networks: A Graph Resfusion Approach 
[[arxiv](https://arxiv.org/abs/2502.09624)] [[cool](https://papers.cool/arxiv/2502.09624)] [[pdf](https://arxiv.org/pdf/2502.09624)]
> **Authors**: Jiawen Kang,Jiana Liao,Runquan Gao,Jinbo Wen,Huawei Huang,Maomao Zhang,Changyan Yi,Tao Zhang,Dusit Niyato,Zibin Zheng
> **First submission**: 2025-01-26
> **First announcement**: 2025-02-14
> **comment**: 15 pages, 11 figures
- **标题**: None
- **领域**: 人工智能,密码学和安全
- **Abstract**: By synergistically integrating mobile networks and embodied artificial intelligence (AI), Mobile Embodied AI Networks (MEANETs) represent an advanced paradigm that facilitates autonomous, context-aware, and interactive behaviors within dynamic environments. Nevertheless, the rapid development of MEANETs is accompanied by challenges in trustworthiness and operational efficiency. Fortunately, blockchain technology, with its decentralized and immutable characteristics, offers promising solutions for MEANETs. However, existing block propagation mechanisms suffer from challenges such as low propagation efficiency and weak security for block propagation, which results in delayed transmission of vehicular messages or vulnerability to malicious tampering, potentially causing severe traffic accidents in blockchain-enabled MEANETs. Moreover, current block propagation strategies cannot effectively adapt to real-time changes of dynamic topology in MEANETs. Therefore, in this paper, we propose a graph Resfusion model-based trustworthy block propagation optimization framework for consortium blockchain-enabled MEANETs. Specifically, we propose an innovative trust calculation mechanism based on the trust cloud model, which comprehensively accounts for randomness and fuzziness in the miner trust evaluation. Furthermore, by leveraging the strengths of graph neural networks and diffusion models, we develop a graph Resfusion model to effectively and adaptively generate the optimal block propagation trajectory. Simulation results demonstrate that the proposed model outperforms other routing mechanisms in terms of block propagation efficiency and trustworthiness. Additionally, the results highlight its strong adaptability to dynamic environments, making it particularly suitable for rapidly changing MEANETs.

### CoT-Valve: Length-Compressible Chain-of-Thought Tuning 
[[arxiv](https://arxiv.org/abs/2502.09601)] [[cool](https://papers.cool/arxiv/2502.09601)] [[pdf](https://arxiv.org/pdf/2502.09601)]
> **Authors**: Xinyin Ma,Guangnian Wan,Runpeng Yu,Gongfan Fang,Xinchao Wang
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: Work in progress. Code will be released at https://github.com/horseee/CoT-Valve
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: Chain-of-Thought significantly enhances a model's reasoning capability, but it also comes with a considerable increase in inference costs due to long chains. With the observation that the reasoning path can be easily compressed under easy tasks but struggle on hard tasks, we explore the feasibility of elastically controlling the length of reasoning paths with only one model, thereby reducing the inference overhead of reasoning models dynamically based on task difficulty. We introduce a new tuning and inference strategy named CoT-Valve, designed to allow models to generate reasoning chains of varying lengths. To achieve this, we propose to identify a direction in the parameter space that, when manipulated, can effectively control the length of generated CoT. Moreover, we show that this property is valuable for compressing the reasoning chain. We construct datasets with chains from long to short for the same questions and explore two enhanced strategies for CoT-Valve: (1) a precise length-compressible CoT tuning method, and (2) a progressive chain length compression approach. Our experiments show that CoT-Valve successfully enables controllability and compressibility of the chain and shows better performance than the prompt-based control. We applied this method to QwQ-32B-Preview, reducing reasoning chains on GSM8K from 741 to 225 tokens with a minor performance drop (95.07% to 94.92%) and on AIME from 6827 to 4629 tokens, with only one additional incorrect answer.

### KIMAs: A Configurable Knowledge Integrated Multi-Agent System 
[[arxiv](https://arxiv.org/abs/2502.09596)] [[cool](https://papers.cool/arxiv/2502.09596)] [[pdf](https://arxiv.org/pdf/2502.09596)]
> **Authors**: Zitao Li,Fei Wei,Yuexiang Xie,Dawei Gao,Weirui Kuang,Zhijian Ma,Bingchen Qian,Yaliang Li,Bolin Ding
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,多代理系统
- **Abstract**: Knowledge-intensive conversations supported by large language models (LLMs) have become one of the most popular and helpful applications that can assist people in different aspects. Many current knowledge-intensive applications are centered on retrieval-augmented generation (RAG) techniques. While many open-source RAG frameworks facilitate the development of RAG-based applications, they often fall short in handling practical scenarios complicated by heterogeneous data in topics and formats, conversational context management, and the requirement of low-latency response times. This technical report presents a configurable knowledge integrated multi-agent system, KIMAs, to address these challenges. KIMAs features a flexible and configurable system for integrating diverse knowledge sources with 1) context management and query rewrite mechanisms to improve retrieval accuracy and multi-turn conversational coherency, 2) efficient knowledge routing and retrieval, 3) simple but effective filter and reference generation mechanisms, and 4) optimized parallelizable multi-agent pipeline execution. Our work provides a scalable framework for advancing the deployment of LLMs in real-world settings. To show how KIMAs can help developers build knowledge-intensive applications with different scales and emphases, we demonstrate how we configure the system to three applications already running in practice with reliable performance.

### MDCrow: Automating Molecular Dynamics Workflows with Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.09565)] [[cool](https://papers.cool/arxiv/2502.09565)] [[pdf](https://arxiv.org/pdf/2502.09565)]
> **Authors**: Quintina Campbell,Sam Cox,Jorge Medina,Brittany Watterson,Andrew D. White
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,化学物理
- **Abstract**: Molecular dynamics (MD) simulations are essential for understanding biomolecular systems but remain challenging to automate. Recent advances in large language models (LLM) have demonstrated success in automating complex scientific tasks using LLM-based agents. In this paper, we introduce MDCrow, an agentic LLM assistant capable of automating MD workflows. MDCrow uses chain-of-thought over 40 expert-designed tools for handling and processing files, setting up simulations, analyzing the simulation outputs, and retrieving relevant information from literature and databases. We assess MDCrow's performance across 25 tasks of varying required subtasks and difficulty, and we evaluate the agent's robustness to both difficulty and prompt style. \texttt{gpt-4o} is able to complete complex tasks with low variance, followed closely by \texttt{llama3-405b}, a compelling open-source model. While prompt style does not influence the best models' performance, it has significant effects on smaller models.

### EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents 
[[arxiv](https://arxiv.org/abs/2502.09560)] [[cool](https://papers.cool/arxiv/2502.09560)] [[pdf](https://arxiv.org/pdf/2502.09560)]
> **Authors**: Rui Yang,Hanyang Chen,Junyu Zhang,Mark Zhao,Cheng Qian,Kangrui Wang,Qineng Wang,Teja Venkat Koripella,Marziyeh Movahedi,Manling Li,Heng Ji,Huan Zhang,Tong Zhang
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 52 pages
- **标题**: None
- **领域**: 人工智能,计算语言学,计算机视觉和模式识别
- **Abstract**: Leveraging Multi-modal Large Language Models (MLLMs) to create embodied agents offers a promising avenue for tackling real-world tasks. While language-centric embodied agents have garnered substantial attention, MLLM-based embodied agents remain underexplored due to the lack of comprehensive evaluation frameworks. To bridge this gap, we introduce EmbodiedBench, an extensive benchmark designed to evaluate vision-driven embodied agents. EmbodiedBench features: (1) a diverse set of 1,128 testing tasks across four environments, ranging from high-level semantic tasks (e.g., household) to low-level tasks involving atomic actions (e.g., navigation and manipulation); and (2) six meticulously curated subsets evaluating essential agent capabilities like commonsense reasoning, complex instruction understanding, spatial awareness, visual perception, and long-term planning. Through extensive experiments, we evaluated 19 leading proprietary and open-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excel at high-level tasks but struggle with low-level manipulation, with the best model, GPT-4o, scoring only 28.9% on average. EmbodiedBench provides a multifaceted standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance MLLM-based embodied agents. Our code is available at https://embodiedbench.github.io.

### Dual Formulation for Non-Rectangular Lp Robust Markov Decision Processes 
[[arxiv](https://arxiv.org/abs/2502.09432)] [[cool](https://papers.cool/arxiv/2502.09432)] [[pdf](https://arxiv.org/pdf/2502.09432)]
> **Authors**: Navdeep Kumar,Adarsh Gupta,Maxence Mohamed Elfatihi,Giorgia Ramponi,Kfir Yehuda Levy,Shie Mannor
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: We study robust Markov decision processes (RMDPs) with non-rectangular uncertainty sets, which capture interdependencies across states unlike traditional rectangular models. While non-rectangular robust policy evaluation is generally NP-hard, even in approximation, we identify a powerful class of $L_p$-bounded uncertainty sets that avoid these complexity barriers due to their structural simplicity. We further show that this class can be decomposed into infinitely many \texttt{sa}-rectangular $L_p$-bounded sets and leverage its structural properties to derive a novel dual formulation for $L_p$ RMDPs. This formulation provides key insights into the adversary's strategy and enables the development of the first robust policy evaluation algorithms for non-rectangular RMDPs. Empirical results demonstrate that our approach significantly outperforms brute-force methods, establishing a promising foundation for future investigation into non-rectangular robust MDPs.

### A Deep Inverse-Mapping Model for a Flapping Robotic Wing 
[[arxiv](https://arxiv.org/abs/2502.09378)] [[cool](https://papers.cool/arxiv/2502.09378)] [[pdf](https://arxiv.org/pdf/2502.09378)]
> **Authors**: Hadar Sharvit,Raz Karl,Tsevi Beatus
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: Accepted to ICLR 2025. 10 Pages 5 figures + 2 figures in appendix
- **标题**: None
- **领域**: 人工智能,机器人技术
- **Abstract**: In systems control, the dynamics of a system are governed by modulating its inputs to achieve a desired outcome. For example, to control the thrust of a quad-copter propeller the controller modulates its rotation rate, relying on a straightforward mapping between the input rotation rate and the resulting thrust. This mapping can be inverted to determine the rotation rate needed to generate a desired thrust. However, in complex systems, such as flapping-wing robots where intricate fluid motions are involved, mapping inputs (wing kinematics) to outcomes (aerodynamic forces) is nontrivial and inverting this mapping for real-time control is computationally impractical. Here, we report a machine-learning solution for the inverse mapping of a flapping-wing system based on data from an experimental system we have developed. Our model learns the input wing motion required to generate a desired aerodynamic force outcome. We used a sequence-to-sequence model tailored for time-series data and augmented it with a novel adaptive-spectrum layer that implements representation learning in the frequency domain. To train our model, we developed a flapping wing system that simultaneously measures the wing's aerodynamic force and its 3D motion using high-speed cameras. We demonstrate the performance of our system on an additional open-source dataset of a flapping wing in a different flow regime. Results show superior performance compared with more complex state-of-the-art transformer-based models, with 11% improvement on the test datasets median loss. Moreover, our model shows superior inference time, making it practical for onboard robotic control. Our open-source data and framework may improve modeling and real-time control of systems governed by complex dynamics, from biomimetic robots to biomedical devices.

### Indeterminacy in Affective Computing: Considering Meaning and Context in Data Collection Practices 
[[arxiv](https://arxiv.org/abs/2502.09294)] [[cool](https://papers.cool/arxiv/2502.09294)] [[pdf](https://arxiv.org/pdf/2502.09294)]
> **Authors**: Bernd Dudzik,Tiffany Matej Hrkalovic,Chenxu Hao,Chirag Raman,Masha Tsfasman
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: Accepted at: 12th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Automatic Affect Prediction (AAP) uses computational analysis of input data such as text, speech, images, and physiological signals to predict various affective phenomena (e.g., emotions or moods). These models are typically constructed using supervised machine-learning algorithms, which rely heavily on labeled training datasets. In this position paper, we posit that all AAP training data are derived from human Affective Interpretation Processes, resulting in a form of Affective Meaning. Research on human affect indicates a form of complexity that is fundamental to such meaning: it can possess what we refer to here broadly as Qualities of Indeterminacy (QIs) - encompassing Subjectivity (meaning depends on who is interpreting), Uncertainty (lack of confidence regarding meanings' correctness), Ambiguity (meaning contains mutually exclusive concepts) and Vagueness (meaning is situated at different levels in a nested hierarchy). Failing to appropriately consider QIs leads to results incapable of meaningful and reliable predictions. Based on this premise, we argue that a crucial step in adequately addressing indeterminacy in AAP is the development of data collection practices for modeling corpora that involve the systematic consideration of 1) a relevant set of QIs and 2) context for the associated interpretation processes. To this end, we are 1) outlining a conceptual model of AIPs and the QIs associated with the meaning these produce and a conceptual structure of relevant context, supporting understanding of its role. Finally, we use our framework for 2) discussing examples of context-sensitivity-related challenges for addressing QIs in data collection setups. We believe our efforts can stimulate a structured discussion of both the role of aspects of indeterminacy and context in research on AAP, informing the development of better practices for data collection and analysis.

### From large language models to multimodal AI: A scoping review on the potential of generative AI in medicine 
[[arxiv](https://arxiv.org/abs/2502.09242)] [[cool](https://papers.cool/arxiv/2502.09242)] [[pdf](https://arxiv.org/pdf/2502.09242)]
> **Authors**: Lukas Buess,Matthias Keicher,Nassir Navab,Andreas Maier,Soroosh Tayebi Arasteh
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Generative artificial intelligence (AI) models, such as diffusion models and OpenAI's ChatGPT, are transforming medicine by enhancing diagnostic accuracy and automating clinical workflows. The field has advanced rapidly, evolving from text-only large language models for tasks such as clinical documentation and decision support to multimodal AI systems capable of integrating diverse data modalities, including imaging, text, and structured data, within a single model. The diverse landscape of these technologies, along with rising interest, highlights the need for a comprehensive review of their applications and potential. This scoping review explores the evolution of multimodal AI, highlighting its methods, applications, datasets, and evaluation in clinical settings. Adhering to PRISMA-ScR guidelines, we systematically queried PubMed, IEEE Xplore, and Web of Science, prioritizing recent studies published up to the end of 2024. After rigorous screening, 144 papers were included, revealing key trends and challenges in this dynamic field. Our findings underscore a shift from unimodal to multimodal approaches, driving innovations in diagnostic support, medical report generation, drug discovery, and conversational AI. However, critical challenges remain, including the integration of heterogeneous data types, improving model interpretability, addressing ethical concerns, and validating AI systems in real-world clinical settings. This review summarizes the current state of the art, identifies critical gaps, and provides insights to guide the development of scalable, trustworthy, and clinically impactful multimodal AI solutions in healthcare.

### Commonsense Reasoning-Aided Autonomous Vehicle Systems 
[[arxiv](https://arxiv.org/abs/2502.09233)] [[cool](https://papers.cool/arxiv/2502.09233)] [[pdf](https://arxiv.org/pdf/2502.09233)]
> **Authors**: Keegan Kimbrell
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: In Proceedings ICLP 2024, arXiv:2502.08453
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Autonomous Vehicle (AV) systems have been developed with a strong reliance on machine learning techniques. While machine learning approaches, such as deep learning, are extremely effective at tasks that involve observation and classification, they struggle when it comes to performing higher level reasoning about situations on the road. This research involves incorporating commonsense reasoning models that use image data to improve AV systems. This will allow AV systems to perform more accurate reasoning while also making them more adjustable, explainable, and ethical. This paper will discuss the findings so far and motivate its direction going forward.

### Generating Causally Compliant Counterfactual Explanations using ASP 
[[arxiv](https://arxiv.org/abs/2502.09226)] [[cool](https://papers.cool/arxiv/2502.09226)] [[pdf](https://arxiv.org/pdf/2502.09226)]
> **Authors**: Sopam Dasgupta
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: In Proceedings ICLP 2024, arXiv:2502.08453
- **标题**: None
- **领域**: 人工智能
- **Abstract**: This research is focused on generating achievable counterfactual explanations. Given a negative outcome computed by a machine learning model or a decision system, the novel CoGS approach generates (i) a counterfactual solution that represents a positive outcome and (ii) a path that will take us from the negative outcome to the positive one, where each node in the path represents a change in an attribute (feature) value. CoGS computes paths that respect the causal constraints among features. Thus, the counterfactuals computed by CoGS are realistic. CoGS utilizes rule-based machine learning algorithms to model causal dependencies between features. The paper discusses the current status of the research and the preliminary results obtained.

### Pearce's Characterisation in an Epistemic Domain 
[[arxiv](https://arxiv.org/abs/2502.09221)] [[cool](https://papers.cool/arxiv/2502.09221)] [[pdf](https://arxiv.org/pdf/2502.09221)]
> **Authors**: Ezgi Iraz Su
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: In Proceedings ICLP 2024, arXiv:2502.08453
- **标题**: None
- **领域**: 人工智能,计算机科学中的逻辑,编程语言
- **Abstract**: Answer-set programming (ASP) is a successful problem-solving approach in logic-based AI. In ASP, problems are represented as declarative logic programs, and solutions are identified through their answer sets. Equilibrium logic (EL) is a general-purpose nonmonotonic reasoning formalism, based on a monotonic logic called here-and-there logic. EL was basically proposed by Pearce as a foundational framework of ASP. Epistemic specifications (ES) are extensions of ASP-programs with subjective literals. These new modal constructs in the ASP-language make it possible to check whether a regular literal of ASP is true in every (or some) answer-set of a program. ES-programs are interpreted by world-views, which are essentially collections of answer-sets. (Reflexive) autoepistemic logic is a nonmonotonic formalism, modeling self-belief (knowledge) of ideally rational agents. A relatively new semantics for ES is based on a combination of EL and (reflexive) autoepistemic logic. In this paper, we first propose an overarching framework in the epistemic ASP domain. We then establish a correspondence between existing (reflexive) (auto)epistemic equilibrium logics and our easily-adaptable comprehensive framework, building on Pearce's characterisation of answer-sets as equilibrium models. We achieve this by extending Ferraris' work on answer sets for propositional theories to the epistemic case and reveal the relationship between some ES-semantic proposals.

### Mind the Gaps: Logical English, Prolog, and Multi-agent Systems for Autonomous Vehicles 
[[arxiv](https://arxiv.org/abs/2502.09216)] [[cool](https://papers.cool/arxiv/2502.09216)] [[pdf](https://arxiv.org/pdf/2502.09216)]
> **Authors**: Galileo Sartor,Adam Wyner,Giuseppe Contissa
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: In Proceedings ICLP 2024, arXiv:2502.08453
- **标题**: None
- **领域**: 人工智能,计算语言学,计算机科学中的逻辑,多代理系统
- **Abstract**: In this paper, we present a modular system for representing and reasoning with legal aspects of traffic rules for autonomous vehicles. We focus on a subset of the United Kingdom's Highway Code (HC) related to junctions. As human drivers and automated vehicles (AVs) will interact on the roads, especially in urban environments, we claim that an accessible, unitary, high-level computational model should exist and be applicable to both users. Autonomous vehicles introduce a shift in liability that should not bring disadvantages or increased burden on human drivers. We develop a system "in silico" of the model. The proposed system is built of three main components: a natural language interface, using Logical English, which encodes the rules; an internal representation of the rules in Prolog; and an multi-agent-based simulation environment, built in NetLogo. The three components interact: Logical English is translated into and out of Prolog (along with some support code); Prolog and NetLogo interface via predicates. Such a modular approach enables the different components to carry different "burdens" in the overall system; it also allows swapping of modules. Given NetLogo, we can visualize the effect of the modeled rules as well as validate the system with a simple dynamic running scenario. Designated agents monitor the behaviour of the vehicles for compliance and record potential violations where they occur. The information on potential violations is then utilized by Validators, to determine whether the violation is punishable, differentiating between exceptions and cases.

### LP-LM: No Hallucinations in Question Answering with Logic Programming 
[[arxiv](https://arxiv.org/abs/2502.09212)] [[cool](https://papers.cool/arxiv/2502.09212)] [[pdf](https://arxiv.org/pdf/2502.09212)]
> **Authors**: Katherine Wu,Yanhong A. Liu
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: In Proceedings ICLP 2024, arXiv:2502.08453
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: Large language models (LLMs) are able to generate human-like responses to user queries. However, LLMs exhibit inherent limitations, especially because they hallucinate. This paper introduces LP-LM, a system that grounds answers to questions in known facts contained in a knowledge base (KB), facilitated through semantic parsing in Prolog, and always produces answers that are reliable. LP-LM generates a most probable constituency parse tree along with a corresponding Prolog term for an input question via Prolog definite clause grammar (DCG) parsing. The term is then executed against a KB of natural language sentences also represented as Prolog terms for question answering. By leveraging DCG and tabling, LP-LM runs in linear time in the size of input sentences for sufficiently many grammar rules. Performing experiments comparing LP-LM with current well-known LLMs in accuracy, we show that LLMs hallucinate on even simple questions, unlike LP-LM.

### Visual Graph Question Answering with ASP and LLMs for Language Parsing 
[[arxiv](https://arxiv.org/abs/2502.09211)] [[cool](https://papers.cool/arxiv/2502.09211)] [[pdf](https://arxiv.org/pdf/2502.09211)]
> **Authors**: Jakob Johannes Bauer,Thomas Eiter,Nelson Higuera Ruiz,Johannes Oetsch
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: In Proceedings ICLP 2024, arXiv:2502.08453. This work was partially funded from the Bosch Center forAI
- **标题**: None
- **领域**: 人工智能,计算机视觉和模式识别,计算机科学中的逻辑
- **Abstract**: Visual Question Answering (VQA) is a challenging problem that requires to process multimodal input. Answer-Set Programming (ASP) has shown great potential in this regard to add interpretability and explainability to modular VQA architectures. In this work, we address the problem of how to integrate ASP with modules for vision and natural language processing to solve a new and demanding VQA variant that is concerned with images of graphs (not graphs in symbolic form). Images containing graph-based structures are an ubiquitous and popular form of visualisation. Here, we deal with the particular problem of graphs inspired by transit networks, and we introduce a novel dataset that amends an existing one by adding images of graphs that resemble metro lines. Our modular neuro-symbolic approach combines optical graph recognition for graph parsing, a pretrained optical character recognition neural network for parsing labels, Large Language Models (LLMs) for language processing, and ASP for reasoning. This method serves as a first baseline and achieves an overall average accuracy of 73% on the dataset. Our evaluation provides further evidence of the potential of modular neuro-symbolic systems, in particular with pretrained models that do not involve any further training and logic programming for reasoning, to solve complex VQA tasks.

### On LLM-generated Logic Programs and their Inference Execution Methods 
[[arxiv](https://arxiv.org/abs/2502.09209)] [[cool](https://papers.cool/arxiv/2502.09209)] [[pdf](https://arxiv.org/pdf/2502.09209)]
> **Authors**: Paul Tarau
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: In Proceedings ICLP 2024, arXiv:2502.08453
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Large Language Models (LLMs) trained on petabytes of data are highly compressed repositories of a significant proportion of the knowledge accumulated and distilled so far. In this paper we study techniques to elicit this knowledge in the form of several classes of logic programs, including propositional Horn clauses, Dual Horn clauses, relational triplets and Definite Clause Grammars. Exposing this knowledge as logic programs enables sound reasoning methods that can verify alignment of LLM outputs to their intended uses and extend their inference capabilities. We study new execution methods for the generated programs, including soft-unification of abducible facts against LLM-generated content stored in a vector database as well as GPU-based acceleration of minimal model computation that supports inference with large LLM-generated programs.

### Counterfactual Explanations as Plans 
[[arxiv](https://arxiv.org/abs/2502.09205)] [[cool](https://papers.cool/arxiv/2502.09205)] [[pdf](https://arxiv.org/pdf/2502.09205)]
> **Authors**: Vaishak Belle
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: In Proceedings ICLP 2024, arXiv:2502.08453
- **标题**: None
- **领域**: 人工智能,计算机科学中的逻辑
- **Abstract**: There has been considerable recent interest in explainability in AI, especially with black-box machine learning models. As correctly observed by the planning community, when the application at hand is not a single-shot decision or prediction, but a sequence of actions that depend on observations, a richer notion of explanations are desirable. In this paper, we look to provide a formal account of ``counterfactual explanations," based in terms of action sequences. We then show that this naturally leads to an account of model reconciliation, which might take the form of the user correcting the agent's model, or suggesting actions to the agent's plan. For this, we will need to articulate what is true versus what is known, and we appeal to a modal fragment of the situation calculus to formalise these intuitions. We consider various settings: the agent knowing partial truths, weakened truths and having false beliefs, and show that our definitions easily generalize to these different settings.

### Logical Lease Litigation: Prolog and LLMs for Rental Law Compliance in New York 
[[arxiv](https://arxiv.org/abs/2502.09204)] [[cool](https://papers.cool/arxiv/2502.09204)] [[pdf](https://arxiv.org/pdf/2502.09204)]
> **Authors**: Sanskar Sehgal,Yanhong A. Liu
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: In Proceedings ICLP 2024, arXiv:2502.08453
- **标题**: None
- **领域**: 人工智能,计算机科学中的逻辑
- **Abstract**: Legal cases require careful logical reasoning following the laws, whereas interactions with non-technical users must be in natural language. As an application combining logical reasoning using Prolog and natural language processing using large language models (LLMs), this paper presents a novel approach and system, LogicLease, to automate the analysis of landlord-tenant legal cases in the state of New York. LogicLease determines compliance with relevant legal requirements by analyzing case descriptions and citing all relevant laws. It leverages LLMs for information extraction and Prolog for legal reasoning. By separating information extraction from legal reasoning, LogicLease achieves greater transparency and control over the legal logic applied to each case. We evaluate the accuracy, efficiency, and robustness of LogicLease through a series of tests, achieving 100% accuracy and an average processing time of 2.57 seconds. LogicLease presents advantages over state-of-the-art LLM-based legal analysis systems by providing clear, step-by-step reasoning, citing specific laws, and distinguishing itself by its ability to avoid hallucinations -- a common issue in LLMs.

### Logical Reasoning in Large Language Models: A Survey 
[[arxiv](https://arxiv.org/abs/2502.09100)] [[cool](https://papers.cool/arxiv/2502.09100)] [[pdf](https://arxiv.org/pdf/2502.09100)]
> **Authors**: Hanmeng Liu,Zhizhang Fu,Mengru Ding,Ruoxi Ning,Chaoli Zhang,Xiaozhang Liu,Yue Zhang
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: With the emergence of advanced reasoning models like OpenAI o3 and DeepSeek-R1, large language models (LLMs) have demonstrated remarkable reasoning capabilities. However, their ability to perform rigorous logical reasoning remains an open question. This survey synthesizes recent advancements in logical reasoning within LLMs, a critical area of AI research. It outlines the scope of logical reasoning in LLMs, its theoretical foundations, and the benchmarks used to evaluate reasoning proficiency. We analyze existing capabilities across different reasoning paradigms - deductive, inductive, abductive, and analogical - and assess strategies to enhance reasoning performance, including data-centric tuning, reinforcement learning, decoding strategies, and neuro-symbolic approaches. The review concludes with future directions, emphasizing the need for further exploration to strengthen logical reasoning in AI systems.

### Cost-Saving LLM Cascades with Early Abstention 
[[arxiv](https://arxiv.org/abs/2502.09054)] [[cool](https://papers.cool/arxiv/2502.09054)] [[pdf](https://arxiv.org/pdf/2502.09054)]
> **Authors**: Michael J. Zellinger,Rex Liu,Matt Thomson
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 6 pages, 1 figure
- **标题**: None
- **领域**: 人工智能
- **Abstract**: LLM cascades are based on the idea that processing all queries with the largest and most expensive LLMs is inefficient. Instead, cascades deploy small LLMs to answer the majority of queries, limiting the use of large and expensive LLMs to only the most difficult queries. This approach can significantly reduce costs without impacting performance. However, risk-sensitive domains such as finance or medicine place an additional premium on avoiding model errors. Recognizing that even the most expensive models may make mistakes, applications in these domains benefit from allowing LLM systems to completely abstain from answering a query when the chance of making a mistake is significant. However, giving a cascade the ability to abstain poses an immediate design question for LLM cascades: should abstention only be allowed at the final model or also at earlier models? Since the error patterns of small and large models are correlated, the latter strategy may further reduce inference costs by letting inexpensive models anticipate abstention decisions by expensive models, thereby obviating the need to run the expensive models. We investigate the benefits of "early abstention" in LLM cascades and find that it reduces the overall test loss by 2.2% on average across six benchmarks (GSM8K, MedMCQA, MMLU, TriviaQA, TruthfulQA, and XSum). These gains result from a more effective use of abstention, which trades a 4.1% average increase in the overall abstention rate for a 13.0% reduction in cost and a 5.0% reduction in error rate. Our findings demonstrate that it is possible to leverage correlations between the error patterns of different language models to drive performance improvements for LLM systems with abstention.

### Game Theory Meets Large Language Models: A Systematic Survey 
[[arxiv](https://arxiv.org/abs/2502.09053)] [[cool](https://papers.cool/arxiv/2502.09053)] [[pdf](https://arxiv.org/pdf/2502.09053)]
> **Authors**: Haoran Sun,Yusen Wu,Yukun Cheng,Xu Chu
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 10 pages
- **标题**: None
- **领域**: 人工智能,计算机科学与博弈论,机器学习
- **Abstract**: Game theory establishes a fundamental framework for analyzing strategic interactions among rational decision-makers. The rapid advancement of large language models (LLMs) has sparked extensive research exploring the intersection of these two fields. Specifically, game-theoretic methods are being applied to evaluate and enhance LLM capabilities, while LLMs themselves are reshaping classic game models. This paper presents a comprehensive survey of the intersection of these fields, exploring a bidirectional relationship from three perspectives: (1) Establishing standardized game-based benchmarks for evaluating LLM behavior; (2) Leveraging game-theoretic methods to improve LLM performance through algorithmic innovations; (3) Characterizing the societal impacts of LLMs through game modeling. Among these three aspects, we also highlight how the equilibrium analysis for traditional game models is impacted by LLMs' advanced language understanding, which in turn extends the study of game theory. Finally, we identify key challenges and future research directions, assessing their feasibility based on the current state of the field. By bridging theoretical rigor with emerging AI capabilities, this survey aims to foster interdisciplinary collaboration and drive progress in this evolving research area.

### AoI-Sensitive Data Forwarding with Distributed Beamforming in UAV-Assisted IoT 
[[arxiv](https://arxiv.org/abs/2502.09038)] [[cool](https://papers.cool/arxiv/2502.09038)] [[pdf](https://arxiv.org/pdf/2502.09038)]
> **Authors**: Zifan Lang,Guixia Liu,Geng Sun,Jiahui Li,Zemin Sun,Jiacheng Wang,Victor C. M. Leung
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 6 pages, 4 figures, ICC2025
- **标题**: None
- **领域**: 人工智能
- **Abstract**: This paper proposes a UAV-assisted forwarding system based on distributed beamforming to enhance age of information (AoI) in Internet of Things (IoT). Specifically, UAVs collect and relay data between sensor nodes (SNs) and the remote base station (BS). However, flight delays increase the AoI and degrade the network performance. To mitigate this, we adopt distributed beamforming to extend the communication range, reduce the flight frequency and ensure the continuous data relay and efficient energy utilization. Then, we formulate an optimization problem to minimize AoI and UAV energy consumption, by jointly optimizing the UAV trajectories and communication schedules. The problem is non-convex and with high dynamic, and thus we propose a deep reinforcement learning (DRL)-based algorithm to solve the problem, thereby enhancing the stability and accelerate convergence speed. Simulation results show that the proposed algorithm effectively addresses the problem and outperforms other benchmark algorithms.

### Mechanistic Unveiling of Transformer Circuits: Self-Influence as a Key to Model Reasoning 
[[arxiv](https://arxiv.org/abs/2502.09022)] [[cool](https://papers.cool/arxiv/2502.09022)] [[pdf](https://arxiv.org/pdf/2502.09022)]
> **Authors**: Lin Zhang,Lijie Hu,Di Wang
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: Accepted by NAACL2025
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Transformer-based language models have achieved significant success; however, their internal mechanisms remain largely opaque due to the complexity of non-linear interactions and high-dimensional operations. While previous studies have demonstrated that these models implicitly embed reasoning trees, humans typically employ various distinct logical reasoning mechanisms to complete the same task. It is still unclear which multi-step reasoning mechanisms are used by language models to solve such tasks. In this paper, we aim to address this question by investigating the mechanistic interpretability of language models, particularly in the context of multi-step reasoning tasks. Specifically, we employ circuit analysis and self-influence functions to evaluate the changing importance of each token throughout the reasoning process, allowing us to map the reasoning paths adopted by the model. We apply this methodology to the GPT-2 model on a prediction task (IOI) and demonstrate that the underlying circuits reveal a human-interpretable reasoning process used by the model.

## 计算语言学(cs.CL:Computation and Language)

### Efficient Multitask Learning in Small Language Models Through Upside-Down Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.09854)] [[cool](https://papers.cool/arxiv/2502.09854)] [[pdf](https://arxiv.org/pdf/2502.09854)]
> **Authors**: Yu-Chen Lin,Sanat Sharma,Hari Manikandan,Jayant Kumar,Tracy Holloway King,Jing Zheng
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: In this work, we demonstrate that small language models (SLMs), specifically a 100M parameter GPT-2 model, can achieve competitive performance in multitask prompt generation tasks while requiring only a fraction of the computational resources needed by large language models (LLMs). Through a novel combination of upside-down reinforcement learning and synthetic data distillation from a powerful LLM, Llama-3, we train an SLM that achieves relevance scores within 5% of state-of-the-art models, including Llama-3, Qwen2, and Mistral, despite being up to 80 times smaller, making it highly suitable for resource-constrained and real-time applications. This study highlights the potential of SLMs as efficient multitask learners in multimodal settings, providing a promising alternative to LLMs for scalable, low-latency deployments.

### Statistical Coherence Alignment for Large Language Model Representation Learning Through Tensor Field Convergence 
[[arxiv](https://arxiv.org/abs/2502.09815)] [[cool](https://papers.cool/arxiv/2502.09815)] [[pdf](https://arxiv.org/pdf/2502.09815)]
> **Authors**: Jonathan Gale,Godfrey Aldington,Harriet Thistlewood,Thomas Tattershall,Basil Wentworth,Vincent Enoasmo
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Representation learning plays a central role in structuring internal embeddings to capture the statistical properties of language, influencing the coherence and contextual consistency of generated text. Statistical Coherence Alignment is introduced as a method to enforce structured token representations through tensor field convergence, guiding embeddings to reflect statistical dependencies inherent in linguistic data. A mathematical framework is established to quantify coherence alignment, integrating a loss function that optimizes representational consistency across training iterations. Empirical evaluations demonstrate that applying coherence constraints improves perplexity, enhances classification accuracy, and refines rare word embeddings, contributing to a more stable representation space. Comparative analyses with baseline models reveal that the proposed method fosters a more interpretable internal structure, ensuring that embeddings retain contextual dependencies while mitigating representation collapse. The impact on coherence score distributions suggests that the alignment mechanism strengthens semantic integrity across diverse linguistic constructs, leading to a more balanced organization of learned embeddings. Computational assessments indicate that while the method introduces additional memory and training costs, the structured optimization process justifies the trade-offs in applications requiring heightened contextual fidelity. Experimental results validate the effectiveness of coherence alignment in optimizing token representations, providing insights into how statistical dependencies can be leveraged to improve language model training.

### INJONGO: A Multicultural Intent Detection and Slot-filling Dataset for 16 African Languages 
[[arxiv](https://arxiv.org/abs/2502.09814)] [[cool](https://papers.cool/arxiv/2502.09814)] [[pdf](https://arxiv.org/pdf/2502.09814)]
> **Authors**: Hao Yu,Jesujoba O. Alabi,Andiswa Bukula,Jian Yun Zhuang,En-Shiun Annie Lee,Tadesse Kebede Guge,Israel Abebe Azime,Happy Buzaaba,Blessing Kudzaishe Sibanda,Godson K. Kalipe,Jonathan Mukiibi,Salomon Kabongo Kabenamualu,Mmasibidi Setaka,Lolwethu Ndolela,Nkiruka Odu,Rooweither Mabuya,Shamsuddeen Hassan Muhammad,Salomey Osei,Sokhar Samb,Juliet W. Murage,Dietrich Klakow,David Ifeoluwa Adelani
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Slot-filling and intent detection are well-established tasks in Conversational AI. However, current large-scale benchmarks for these tasks often exclude evaluations of low-resource languages and rely on translations from English benchmarks, thereby predominantly reflecting Western-centric concepts. In this paper, we introduce Injongo -- a multicultural, open-source benchmark dataset for 16 African languages with utterances generated by native speakers across diverse domains, including banking, travel, home, and dining. Through extensive experiments, we benchmark the fine-tuning multilingual transformer models and the prompting large language models (LLMs), and show the advantage of leveraging African-cultural utterances over Western-centric utterances for improving cross-lingual transfer from the English language. Experimental results reveal that current LLMs struggle with the slot-filling task, with GPT-4o achieving an average performance of 26 F1-score. In contrast, intent detection performance is notably better, with an average accuracy of 70.6%, though it still falls behind the fine-tuning baselines. Compared to the English language, GPT-4o and fine-tuning baselines perform similarly on intent detection, achieving an accuracy of approximately 81%. Our findings suggest that the performance of LLMs is still behind for many low-resource African languages, and more work is needed to further improve their downstream performance.

### Prompt and circumstance: A word-by-word LLM prompting approach to interlinear glossing for low-resource languages 
[[arxiv](https://arxiv.org/abs/2502.09778)] [[cool](https://papers.cool/arxiv/2502.09778)] [[pdf](https://arxiv.org/pdf/2502.09778)]
> **Authors**: Micha Elsner,David Liu
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Partly automated creation of interlinear glossed text (IGT) has the potential to assist in linguistic documentation. We argue that LLMs can make this process more accessible to linguists because of their capacity to follow natural-language instructions. We investigate the effectiveness of a retrieval-based LLM prompting approach to glossing, applied to the seven languages from the SIGMORPHON 2023 shared task. Our system beats the BERT-based shared task baseline for every language in the morpheme-level score category, and we show that a simple 3-best oracle has higher word-level scores than the challenge winner (a tuned sequence model) in five languages. In a case study on Tsez, we ask the LLM to automatically create and follow linguistic instructions, reducing errors on a confusing grammatical feature. Our results thus demonstrate the potential contributions which LLMs can make in interactive systems for glossing, both in making suggestions to human annotators and following directions.

### The Widespread Adoption of Large Language Model-Assisted Writing Across Society 
[[arxiv](https://arxiv.org/abs/2502.09747)] [[cool](https://papers.cool/arxiv/2502.09747)] [[pdf](https://arxiv.org/pdf/2502.09747)]
> **Authors**: Weixin Liang,Yaohui Zhang,Mihai Codreanu,Jiayu Wang,Hancheng Cao,James Zou
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The recent advances in large language models (LLMs) attracted significant public and policymaker interest in its adoption patterns. In this paper, we systematically analyze LLM-assisted writing across four domains-consumer complaints, corporate communications, job postings, and international organization press releases-from January 2022 to September 2024. Our dataset includes 687,241 consumer complaints, 537,413 corporate press releases, 304.3 million job postings, and 15,919 United Nations (UN) press releases. Using a robust population-level statistical framework, we find that LLM usage surged following the release of ChatGPT in November 2022. By late 2024, roughly 18% of financial consumer complaint text appears to be LLM-assisted, with adoption patterns spread broadly across regions and slightly higher in urban areas. For corporate press releases, up to 24% of the text is attributable to LLMs. In job postings, LLM-assisted writing accounts for just below 10% in small firms, and is even more common among younger firms. UN press releases also reflect this trend, with nearly 14% of content being generated or modified by LLMs. Although adoption climbed rapidly post-ChatGPT, growth appears to have stabilized by 2024, reflecting either saturation in LLM adoption or increasing subtlety of more advanced models. Our study shows the emergence of a new reality in which firms, consumers and even international organizations substantially rely on generative AI for communications.

### FoNE: Precise Single-Token Number Embeddings via Fourier Features 
[[arxiv](https://arxiv.org/abs/2502.09741)] [[cool](https://papers.cool/arxiv/2502.09741)] [[pdf](https://arxiv.org/pdf/2502.09741)]
> **Authors**: Tianyi Zhou,Deqing Fu,Mahdi Soltanolkotabi,Robin Jia,Vatsal Sharan
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Large Language Models (LLMs) typically represent numbers using multiple tokens, which requires the model to aggregate these tokens to interpret numerical values. This fragmentation makes both training and inference less efficient and adversely affects the model's performance on number-related tasks. Inspired by the observation that pre-trained LLMs internally learn Fourier-like features for number tokens, we propose Fourier Number Embedding (FoNE), a novel method that directly maps numbers into the embedding space with their Fourier features. FoNE encodes each number as a single token with only two embedding dimensions per digit, effectively capturing numerical values without fragmentation. This compact representation accelerates both training and inference. Compared to traditional subword and digit-wise embeddings, FoNE not only reduces computational overhead but also achieves higher accuracy across various numerical tasks including addition, subtraction and multiplication. On 6-digit decimal addition, FoNE requires 64$\times$ less data to achieve 99% accuracy than subword and digit-wise embeddings while using 3$\times$ and 6$\times$ fewer tokens per number, respectively. Furthermore, FoNE is the only method that yields 100% accuracy on over 100,000 test examples for addition, subtraction, and multiplication. The codes and visualization are available at https://fouriernumber.github.io/.

### Trust at Your Own Peril: A Mixed Methods Exploration of the Ability of Large Language Models to Generate Expert-Like Systems Engineering Artifacts and a Characterization of Failure Modes 
[[arxiv](https://arxiv.org/abs/2502.09690)] [[cool](https://papers.cool/arxiv/2502.09690)] [[pdf](https://arxiv.org/pdf/2502.09690)]
> **Authors**: Taylan G. Topcu,Mohammed Husain,Max Ofsa,Paul Wach
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 41 pages, 10 figures
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Multi-purpose Large Language Models (LLMs), a subset of generative Artificial Intelligence (AI), have recently made significant progress. While expectations for LLMs to assist systems engineering (SE) tasks are paramount; the interdisciplinary and complex nature of systems, along with the need to synthesize deep-domain knowledge and operational context, raise questions regarding the efficacy of LLMs to generate SE artifacts, particularly given that they are trained using data that is broadly available on the internet. To that end, we present results from an empirical exploration, where a human expert-generated SE artifact was taken as a benchmark, parsed, and fed into various LLMs through prompt engineering to generate segments of typical SE artifacts. This procedure was applied without any fine-tuning or calibration to document baseline LLM performance. We then adopted a two-fold mixed-methods approach to compare AI generated artifacts against the benchmark. First, we quantitatively compare the artifacts using natural language processing algorithms and find that when prompted carefully, the state-of-the-art algorithms cannot differentiate AI-generated artifacts from the human-expert benchmark. Second, we conduct a qualitative deep dive to investigate how they differ in terms of quality. We document that while the two-material appear very similar, AI generated artifacts exhibit serious failure modes that could be difficult to detect. We characterize these as: premature requirements definition, unsubstantiated numerical estimates, and propensity to overspecify. We contend that this study tells a cautionary tale about why the SE community must be more cautious adopting AI suggested feedback, at least when generated by multi-purpose LLMs.

### Large Language Models and Provenance Metadata for Determining the Relevance of Images and Videos in News Stories 
[[arxiv](https://arxiv.org/abs/2502.09689)] [[cool](https://papers.cool/arxiv/2502.09689)] [[pdf](https://arxiv.org/pdf/2502.09689)]
> **Authors**: Tomas Peterka,Matyas Bohacek
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,计算机视觉和模式识别,计算机与社会
- **Abstract**: The most effective misinformation campaigns are multimodal, often combining text with images and videos taken out of context -- or fabricating them entirely -- to support a given narrative. Contemporary methods for detecting misinformation, whether in deepfakes or text articles, often miss the interplay between multiple modalities. Built around a large language model, the system proposed in this paper addresses these challenges. It analyzes both the article's text and the provenance metadata of included images and videos to determine whether they are relevant. We open-source the system prototype and interactive web interface.

### Mind What You Ask For: Emotional and Rational Faces of Persuasion by Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.09687)] [[cool](https://papers.cool/arxiv/2502.09687)] [[pdf](https://arxiv.org/pdf/2502.09687)]
> **Authors**: Wiktoria Mieleszczenko-Kowszewicz,Beata Bajcar,Jolanta Babiak,Berenika Dyczek,Jakub Świstak,Przemysław Biecek
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,人机交互
- **Abstract**: Be careful what you ask for, you just might get it. This saying fits with the way large language models (LLMs) are trained, which, instead of being rewarded for correctness, are increasingly rewarded for pleasing the recipient. So, they are increasingly effective at persuading us that their answers are valuable. But what tricks do they use in this persuasion? In this study, we examine what are the psycholinguistic features of the responses used by twelve different language models. By grouping response content according to rational or emotional prompts and exploring social influence principles employed by LLMs, we ask whether and how we can mitigate the risks of LLM-driven mass misinformation. We position this study within the broader discourse on human-centred AI, emphasizing the need for interdisciplinary approaches to mitigate cognitive and societal risks posed by persuasive AI responses.

### Multi-level Conflict-Aware Network for Multi-modal Sentiment Analysis 
[[arxiv](https://arxiv.org/abs/2502.09675)] [[cool](https://papers.cool/arxiv/2502.09675)] [[pdf](https://arxiv.org/pdf/2502.09675)]
> **Authors**: Yubo Gao,Haotian Wu,Lei Zhang
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 5 pages, 1 figure
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Multimodal Sentiment Analysis (MSA) aims to recognize human emotions by exploiting textual, acoustic, and visual modalities, and thus how to make full use of the interactions between different modalities is a central challenge of MSA. Interaction contains alignment and conflict aspects. Current works mainly emphasize alignment and the inherent differences between unimodal modalities, neglecting the fact that there are also potential conflicts between bimodal combinations. Additionally, multi-task learning-based conflict modeling methods often rely on the unstable generated labels. To address these challenges, we propose a novel multi-level conflict-aware network (MCAN) for multimodal sentiment analysis, which progressively segregates alignment and conflict constituents from unimodal and bimodal representations, and further exploits the conflict constituents with the conflict modeling branch. In the conflict modeling branch, we conduct discrepancy constraints at both the representation and predicted output levels, avoiding dependence on the generated labels. Experimental results on the CMU-MOSI and CMU-MOSEI datasets demonstrate the effectiveness of the proposed MCAN.

### The Hidden Dimensions of LLM Alignment: A Multi-Dimensional Safety Analysis 
[[arxiv](https://arxiv.org/abs/2502.09674)] [[cool](https://papers.cool/arxiv/2502.09674)] [[pdf](https://arxiv.org/pdf/2502.09674)]
> **Authors**: Wenbo Pan,Zhichao Liu,Qiguang Chen,Xiangyang Zhou,Haining Yu,Xiaohua Jia
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: Code and artifacts: https://github.com/BMPixel/safety-residual-space
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large Language Models' safety-aligned behaviors, such as refusing harmful queries, can be represented by linear directions in activation space. Previous research modeled safety behavior with a single direction, limiting mechanistic understanding to an isolated safety feature. In this work, we discover that safety-aligned behavior is jointly controlled by multi-dimensional directions. Namely, we study the vector space of representation shifts during safety fine-tuning on Llama 3 8B for refusing jailbreaks. By studying orthogonal directions in the space, we first find that a dominant direction governs the model's refusal behavior, while multiple smaller directions represent distinct and interpretable features like hypothetical narrative and role-playing. We then measure how different directions promote or suppress the dominant direction, showing the important role of secondary directions in shaping the model's refusal representation. Finally, we demonstrate that removing certain trigger tokens in harmful queries can mitigate these directions to bypass the learned safety capability, providing new insights on understanding safety alignment vulnerability from a multi-dimensional perspective. Code and artifacts are available at https://github.com/BMPixel/safety-residual-space.

### Are Smarter LLMs Safer? Exploring Safety-Reasoning Trade-offs in Prompting and Fine-Tuning 
[[arxiv](https://arxiv.org/abs/2502.09673)] [[cool](https://papers.cool/arxiv/2502.09673)] [[pdf](https://arxiv.org/pdf/2502.09673)]
> **Authors**: Ang Li,Yichuan Mo,Mingjie Li,Yifei Wang,Yisen Wang
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large Language Models (LLMs) have demonstrated remarkable success across various NLP benchmarks. However, excelling in complex tasks that require nuanced reasoning and precise decision-making demands more than raw language proficiency--LLMs must reason, i.e., think logically, draw from past experiences, and synthesize information to reach conclusions and take action. To enhance reasoning abilities, approaches such as prompting and fine-tuning have been widely explored. While these methods have led to clear improvements in reasoning, their impact on LLM safety remains less understood. In this work, we investigate the interplay between reasoning and safety in LLMs. We highlight the latent safety risks that arise as reasoning capabilities improve, shedding light on previously overlooked vulnerabilities. At the same time, we explore how reasoning itself can be leveraged to enhance safety, uncovering potential mitigation strategies. By examining both the risks and opportunities in reasoning-driven LLM safety, our study provides valuable insights for developing models that are not only more capable but also more trustworthy in real-world deployments.

### The Science of Evaluating Foundation Models 
[[arxiv](https://arxiv.org/abs/2502.09670)] [[cool](https://papers.cool/arxiv/2502.09670)] [[pdf](https://arxiv.org/pdf/2502.09670)]
> **Authors**: Jiayi Yuan,Jiamu Zhang,Andrew Wen,Xia Hu
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: The emergent phenomena of large foundation models have revolutionized natural language processing. However, evaluating these models presents significant challenges due to their size, capabilities, and deployment across diverse applications. Existing literature often focuses on individual aspects, such as benchmark performance or specific tasks, but fails to provide a cohesive process that integrates the nuances of diverse use cases with broader ethical and operational considerations. This work focuses on three key aspects: (1) Formalizing the Evaluation Process by providing a structured framework tailored to specific use-case contexts, (2) Offering Actionable Tools and Frameworks such as checklists and templates to ensure thorough, reproducible, and practical evaluations, and (3) Surveying Recent Work with a targeted review of advancements in LLM evaluation, emphasizing real-world applications.

### k-LLMmeans: Summaries as Centroids for Interpretable and Scalable LLM-Based Text Clustering 
[[arxiv](https://arxiv.org/abs/2502.09667)] [[cool](https://papers.cool/arxiv/2502.09667)] [[pdf](https://arxiv.org/pdf/2502.09667)]
> **Authors**: Jairo Diaz-Rodriguez
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习,机器学习
- **Abstract**: We introduce k-LLMmeans, a novel modification of the k-means clustering algorithm that utilizes LLMs to generate textual summaries as cluster centroids, thereby capturing contextual and semantic nuances often lost when relying on purely numerical means of document embeddings. This modification preserves the properties of k-means while offering greater interpretability: the cluster centroid is represented by an LLM-generated summary, whose embedding guides cluster assignments. We also propose a mini-batch variant, enabling efficient online clustering for streaming text data and providing real-time interpretability of evolving cluster centroids. Through extensive simulations, we show that our methods outperform vanilla k-means on multiple metrics while incurring only modest LLM usage that does not scale with dataset size. Finally, We present a case study showcasing the interpretability of evolving cluster centroids in sequential text streams. As part of our evaluation, we compile a new dataset from StackExchange, offering a benchmark for text-stream clustering.

### Cancer Vaccine Adjuvant Name Recognition from Biomedical Literature using Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.09659)] [[cool](https://papers.cool/arxiv/2502.09659)] [[pdf](https://arxiv.org/pdf/2502.09659)]
> **Authors**: Hasin Rehana,Jie Zheng,Leo Yeh,Benu Bansal,Nur Bengisu Çam,Christianah Jemiyo,Brett McGregor,Arzucan Özgür,Yongqun He,Junguk Hur
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-14
> **comment**: 10 pages, 6 figures, 4 tables
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机与社会
- **Abstract**: Motivation: An adjuvant is a chemical incorporated into vaccines that enhances their efficacy by improving the immune response. Identifying adjuvant names from cancer vaccine studies is essential for furthering research and enhancing immunotherapies. However, the manual curation from the constantly expanding biomedical literature poses significant challenges. This study explores the automated recognition of vaccine adjuvant names using Large Language Models (LLMs), specifically Generative Pretrained Transformers (GPT) and Large Language Model Meta AI (Llama). Methods: We utilized two datasets: 97 clinical trial records from AdjuvareDB and 290 abstracts annotated with the Vaccine Adjuvant Compendium (VAC). GPT-4o and Llama 3.2 were employed in zero-shot and few-shot learning paradigms with up to four examples per prompt. Prompts explicitly targeted adjuvant names, testing the impact of contextual information such as substances or interventions. Outputs underwent automated and manual validation for accuracy and consistency. Results: GPT-4o attained 100% Precision across all situations while exhibiting notable improve in Recall and F1-scores, particularly with incorporating interventions. On the VAC dataset, GPT-4o achieved a maximum F1-score of 77.32% with interventions, surpassing Llama-3.2-3B by approximately 2%. On the AdjuvareDB dataset, GPT-4o reached an F1-score of 81.67% for three-shot prompting with interventions, surpassing Llama-3.2-3 B's maximum F1-score of 65.62%. Conclusion: Our findings demonstrate that LLMs excel at identifying adjuvant names, including rare variations of naming representation. This study emphasizes the capability of LLMs to enhance cancer vaccine development by efficiently extracting insights. Future work aims to broaden the framework to encompass various biomedical literature and enhance model generalizability across various vaccines and adjuvants.

### Neuro-Conceptual Artificial Intelligence: Integrating OPM with Deep Learning to Enhance Question Answering Quality 
[[arxiv](https://arxiv.org/abs/2502.09658)] [[cool](https://papers.cool/arxiv/2502.09658)] [[pdf](https://arxiv.org/pdf/2502.09658)]
> **Authors**: Xin Kang,Veronika Shteingardt,Yuhan Wang,Dov Dori
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-14
> **comment**: 15 pages, 3 figures,
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Knowledge representation and reasoning are critical challenges in Artificial Intelligence (AI), particularly in integrating neural and symbolic approaches to achieve explainable and transparent AI systems. Traditional knowledge representation methods often fall short of capturing complex processes and state changes. We introduce Neuro-Conceptual Artificial Intelligence (NCAI), a specialization of the neuro-symbolic AI approach that integrates conceptual modeling using Object-Process Methodology (OPM) ISO 19450:2024 with deep learning to enhance question-answering (QA) quality. By converting natural language text into OPM models using in-context learning, NCAI leverages the expressive power of OPM to represent complex OPM elements-processes, objects, and states-beyond what traditional triplet-based knowledge graphs can easily capture. This rich structured knowledge representation improves reasoning transparency and answer accuracy in an OPM-QA system. We further propose transparency evaluation metrics to quantitatively measure how faithfully the predicted reasoning aligns with OPM-based conceptual logic. Our experiments demonstrate that NCAI outperforms traditional methods, highlighting its potential for advancing neuro-symbolic AI by providing rich knowledge representations, measurable transparency, and improved reasoning.

### AI-VERDE: A Gateway for Egalitarian Access to Large Language Model-Based Resources For Educational Institutions 
[[arxiv](https://arxiv.org/abs/2502.09651)] [[cool](https://papers.cool/arxiv/2502.09651)] [[pdf](https://arxiv.org/pdf/2502.09651)]
> **Authors**: Paul Mithun,Enrique Noriega-Atala,Nirav Merchant,Edwin Skidmore
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-14
> **comment**: 7 Pages, includes appendix. Submitted to NAACL System demonstrations track 2025
- **标题**: None
- **领域**: 计算语言学,计算机与社会
- **Abstract**: We present AI-VERDE, a unified LLM-as-a-platform service designed to facilitate seamless integration of commercial, cloud-hosted, and on-premise open LLMs in academic settings. AI-VERDE streamlines access management for instructional and research groups by providing features such as robust access control, privacy-preserving mechanisms, native Retrieval-Augmented Generation (RAG) support, budget management for third-party LLM services, and both a conversational web interface and API access. In a pilot deployment at a large public university, AI-VERDE demonstrated significant engagement across diverse educational and research groups, enabling activities that would typically require substantial budgets for commercial LLM services with limited user and team management capabilities. To the best of our knowledge, AI-Verde is the first platform to address both academic and research needs for LLMs within an higher education institutional framework.

### Principled Data Selection for Alignment: The Hidden Risks of Difficult Examples 
[[arxiv](https://arxiv.org/abs/2502.09650)] [[cool](https://papers.cool/arxiv/2502.09650)] [[pdf](https://arxiv.org/pdf/2502.09650)]
> **Authors**: Chengqian Gao,Haonan Li,Liu Liu,Zeke Xie,Peilin Zhao,Zhiqiang Xu
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: The alignment of large language models (LLMs) often assumes that using more clean data yields better outcomes, overlooking the match between model capacity and example difficulty. Challenging this, we propose a new principle: Preference data vary in difficulty, and overly difficult examples hinder alignment, by exceeding the model's capacity. Through systematic experimentation, we validate this principle with three key findings: (1) preference examples vary in difficulty, as evidenced by consistent learning orders across alignment runs; (2) overly difficult examples significantly degrade performance across four LLMs and two datasets; and (3) the capacity of a model dictates its threshold for handling difficult examples, underscoring a critical relationship between data selection and model capacity. Building on this principle, we introduce Selective DPO, which filters out overly difficult examples. This simple adjustment improves alignment performance by 9-16% in win rates on the AlpacaEval 2 benchmark compared to the DPO baseline, suppressing a series of DPO variants with different algorithmic adjustments. Together, these results illuminate the importance of aligning data difficulty with model capacity, offering a transformative perspective for improving alignment strategies in LLMs. Code is available at https://github.com/glorgao/SelectiveDPO.

### Unveiling Simplicities of Attention: Adaptive Long-Context Head Identification 
[[arxiv](https://arxiv.org/abs/2502.09647)] [[cool](https://papers.cool/arxiv/2502.09647)] [[pdf](https://arxiv.org/pdf/2502.09647)]
> **Authors**: Konstantin Donhauser,Charles Arnal,Mohammad Pezeshki,Vivien Cabannes,David Lopez-Paz,Kartik Ahuja
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: The ability to process long contexts is crucial for many natural language processing tasks, yet it remains a significant challenge. While substantial progress has been made in enhancing the efficiency of attention mechanisms, there is still a gap in understanding how attention heads function in long-context settings. In this paper, we observe that while certain heads consistently attend to local information only, others swing between attending to local and long-context information depending on the query. This raises the question: can we identify which heads require long-context information to predict the next token accurately? We demonstrate that it's possible to predict which heads are crucial for long-context processing using only local keys. The core idea here is to exploit a simple model for the long-context scores via second moment approximations. These findings unveil simple properties of attention in the context of long sequences, and open the door to potentially significant gains in efficiency.

### From No to Know: Taxonomy, Challenges, and Opportunities for Negation Understanding in Multimodal Foundation Models 
[[arxiv](https://arxiv.org/abs/2502.09645)] [[cool](https://papers.cool/arxiv/2502.09645)] [[pdf](https://arxiv.org/pdf/2502.09645)]
> **Authors**: Mayank Vatsa,Aparna Bharati,Surbhi Mittal,Richa Singh
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Negation, a linguistic construct conveying absence, denial, or contradiction, poses significant challenges for multilingual multimodal foundation models. These models excel in tasks like machine translation, text-guided generation, image captioning, audio interactions, and video processing but often struggle to accurately interpret negation across diverse languages and cultural contexts. In this perspective paper, we propose a comprehensive taxonomy of negation constructs, illustrating how structural, semantic, and cultural factors influence multimodal foundation models. We present open research questions and highlight key challenges, emphasizing the importance of addressing these issues to achieve robust negation handling. Finally, we advocate for specialized benchmarks, language-specific tokenization, fine-grained attention mechanisms, and advanced multimodal architectures. These strategies can foster more adaptable and semantically precise multimodal foundation models, better equipped to navigate and accurately interpret the complexities of negation in multilingual, multimodal environments.

### Krutrim LLM: Multilingual Foundational Model for over a Billion People 
[[arxiv](https://arxiv.org/abs/2502.09642)] [[cool](https://papers.cool/arxiv/2502.09642)] [[pdf](https://arxiv.org/pdf/2502.09642)]
> **Authors**: Aditya Kallappa,Palash Kamble,Abhinav Ravi,Akshat Patidar,Vinayak Dhruv,Deepak Kumar,Raghav Awasthi,Arveti Manjunath,Himanshu Gupta,Shubham Agarwal,Kumar Ashish,Gautam Bhargava,Chandra Khatri
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: India is a diverse society with unique challenges in developing AI systems, including linguistic diversity, oral traditions, data accessibility, and scalability. Existing foundation models are primarily trained on English, limiting their effectiveness for India's population. Indic languages comprise only 1 percent of Common Crawl corpora despite India representing 18 percent of the global population, leading to linguistic biases. Thousands of regional languages, dialects, and code mixing create additional representation challenges due to sparse training data. We introduce Krutrim LLM, a 2 trillion token multilingual model designed for India's linguistic landscape. It incorporates the largest known Indic dataset, mitigating data scarcity and ensuring balanced performance across dialects. Krutrim outperforms or matches state-of-the-art models on Indic benchmarks while maintaining competitive English performance. Despite being significantly smaller in training flops, Krutrim LLM matches or exceeds models like LLAMA-2 on 10 out of 16 tasks, with an average score of 0.57 versus 0.55. This evidences Krutrim's flexible multilingual fluency across diverse linguistic contexts. Krutrim is integrated with real-time search to improve factual accuracy in conversational AI applications. This enhances accessibility for over 1 billion users worldwide. Through intentional design choices addressing data imbalances, Krutrim LLM signifies meaningful progress in building ethical, globally representative AI models.

### Online Social Support Detection in Spanish Social Media Texts 
[[arxiv](https://arxiv.org/abs/2502.09640)] [[cool](https://papers.cool/arxiv/2502.09640)] [[pdf](https://arxiv.org/pdf/2502.09640)]
> **Authors**: Moein Shahiki Tash,Luis Ramos,Zahra Ahani,Raul Monroy,Olga kolesnikova,Hiram Calvo,Grigori Sidorov
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: The advent of social media has transformed communication, enabling individuals to share their experiences, seek support, and participate in diverse discussions. While extensive research has focused on identifying harmful content like hate speech, the recognition and promotion of positive and supportive interactions remain largely unexplored. This study proposes an innovative approach to detecting online social support in Spanish-language social media texts. We introduce the first annotated dataset specifically created for this task, comprising 3,189 YouTube comments classified as supportive or non-supportive. To address data imbalance, we employed GPT-4o to generate paraphrased comments and create a balanced dataset. We then evaluated social support classification using traditional machine learning models, deep learning architectures, and transformer-based models, including GPT-4o, but only on the unbalanced dataset. Subsequently, we utilized a transformer model to compare the performance between the balanced and unbalanced datasets. Our findings indicate that the balanced dataset yielded improved results for Task 2 (Individual and Group) and Task 3 (Nation, Other, LGBTQ, Black Community, Women, Religion), whereas GPT-4o performed best for Task 1 (Social Support and Non-Support). This study highlights the significance of fostering a supportive online environment and lays the groundwork for future research in automated social support detection.

### Jailbreaking to Jailbreak 
[[arxiv](https://arxiv.org/abs/2502.09638)] [[cool](https://papers.cool/arxiv/2502.09638)] [[pdf](https://arxiv.org/pdf/2502.09638)]
> **Authors**: Jeremy Kritz,Vaughn Robinson,Robert Vacareanu,Bijan Varjavand,Michael Choi,Bobby Gogov,Scale Red Team,Summer Yue,Willow E. Primack,Zifan Wang
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Refusal training on Large Language Models (LLMs) prevents harmful outputs, yet this defense remains vulnerable to both automated and human-crafted jailbreaks. We present a novel LLM-as-red-teamer approach in which a human jailbreaks a refusal-trained LLM to make it willing to jailbreak itself or other LLMs. We refer to the jailbroken LLMs as $J_2$ attackers, which can systematically evaluate target models using various red teaming strategies and improve its performance via in-context learning from the previous failures. Our experiments demonstrate that Sonnet 3.5 and Gemini 1.5 pro outperform other LLMs as $J_2$, achieving 93.0% and 91.0% attack success rates (ASRs) respectively against GPT-4o (and similar results across other capable LLMs) on Harmbench. Our work not only introduces a scalable approach to strategic red teaming, drawing inspiration from human red teamers, but also highlights jailbreaking-to-jailbreak as an overlooked failure mode of the safeguard. Specifically, an LLM can bypass its own safeguards by employing a jailbroken version of itself that is willing to assist in further jailbreaking. To prevent any direct misuse with $J_2$, while advancing research in AI safety, we publicly share our methodology while keeping specific prompting details private.

### Reading between the Lines: Can LLMs Identify Cross-Cultural Communication Gaps? 
[[arxiv](https://arxiv.org/abs/2502.09636)] [[cool](https://papers.cool/arxiv/2502.09636)] [[pdf](https://arxiv.org/pdf/2502.09636)]
> **Authors**: Sougata Saha,Saurabh Kumar Pandey,Harshit Gupta,Monojit Choudhury
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: In a rapidly globalizing and digital world, content such as book and product reviews created by people from diverse cultures are read and consumed by others from different corners of the world. In this paper, we investigate the extent and patterns of gaps in understandability of book reviews due to the presence of culturally-specific items and elements that might be alien to users from another culture. Our user-study on 57 book reviews from Goodreads reveal that 83\% of the reviews had at least one culture-specific difficult-to-understand element. We also evaluate the efficacy of GPT-4o in identifying such items, given the cultural background of the reader; the results are mixed, implying a significant scope for improvement. Our datasets are available here: https://github.com/sougata-ub/reading_between_lines

### CORRECT: Context- and Reference-Augmented Reasoning and Prompting for Fact-Checking 
[[arxiv](https://arxiv.org/abs/2502.09635)] [[cool](https://papers.cool/arxiv/2502.09635)] [[pdf](https://arxiv.org/pdf/2502.09635)]
> **Authors**: Delvin Ce Zhang,Dongwon Lee
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-14
> **comment**: Accepted to NAACL-25
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Fact-checking the truthfulness of claims usually requires reasoning over multiple evidence sentences. Oftentimes, evidence sentences may not be always self-contained, and may require additional contexts and references from elsewhere to understand coreferential expressions, acronyms, and the scope of a reported finding. For example, evidence sentences from an academic paper may need contextual sentences in the paper and descriptions in its cited papers to determine the scope of a research discovery. However, most fact-checking models mainly focus on the reasoning within evidence sentences, and ignore the auxiliary contexts and references. To address this problem, we propose a novel method, Context- and Reference-augmented Reasoning and Prompting. For evidence reasoning, we construct a three-layer evidence graph with evidence, context, and reference layers. We design intra- and cross-layer reasoning to integrate three graph layers into a unified evidence embedding. For verdict prediction, we design evidence-conditioned prompt encoder, which produces unique prompt embeddings for each claim. These evidence-conditioned prompt embeddings and claims are unified for fact-checking. Experiments verify the strength of our model.

### Human-LLM Coevolution: Evidence from Academic Writing 
[[arxiv](https://arxiv.org/abs/2502.09606)] [[cool](https://papers.cool/arxiv/2502.09606)] [[pdf](https://arxiv.org/pdf/2502.09606)]
> **Authors**: Mingmeng Geng,Roberto Trotta
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机与社会,数字图书馆,机器学习
- **Abstract**: With a statistical analysis of arXiv paper abstracts, we report a marked drop in the frequency of several words previously identified as overused by ChatGPT, such as "delve", starting soon after they were pointed out in early 2024. The frequency of certain other words favored by ChatGPT, such as "significant", has instead kept increasing. These phenomena suggest that some authors of academic papers have adapted their use of large language models (LLMs), for example, by selecting outputs or applying modifications to the LLM-generated content. Such coevolution and cooperation of humans and LLMs thus introduce additional challenges to the detection of machine-generated text in real-world scenarios. Estimating the impact of LLMs on academic writing by examining word frequency remains feasible, and more attention should be paid to words that were already frequently employed, including those that have decreased in frequency due to LLMs' disfavor.

### SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.09604)] [[cool](https://papers.cool/arxiv/2502.09604)] [[pdf](https://arxiv.org/pdf/2502.09604)]
> **Authors**: Yung-Sung Chuang,Benjamin Cohen-Wang,Shannon Zejiang Shen,Zhaofeng Wu,Hu Xu,Xi Victoria Lin,James Glass,Shang-Wen Li,Wen-tau Yih
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: Implementation available at https://github.com/voidism/SelfCite
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: We introduce SelfCite, a novel self-supervised approach that aligns LLMs to generate high-quality, fine-grained, sentence-level citations for the statements in their generated responses. Instead of only relying on costly and labor-intensive annotations, SelfCite leverages a reward signal provided by the LLM itself through context ablation: If a citation is necessary, removing the cited text from the context should prevent the same response; if sufficient, retaining the cited text alone should preserve the same response. This reward can guide the inference-time best-of-N sampling strategy to improve citation quality significantly, as well as be used in preference optimization to directly fine-tune the models for generating better citations. The effectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3 points on the LongBench-Cite benchmark across five long-form question answering tasks.

### Logical forms complement probability in understanding language model (and human) performance 
[[arxiv](https://arxiv.org/abs/2502.09589)] [[cool](https://papers.cool/arxiv/2502.09589)] [[pdf](https://arxiv.org/pdf/2502.09589)]
> **Authors**: Yixuan Wang,Freda Shi
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: Preprint
- **标题**: None
- **领域**: 计算语言学,计算机科学中的逻辑
- **Abstract**: With the increasing interest in using large language models (LLMs) for planning in natural language, understanding their behaviors becomes an important research question. This work conducts a systematic investigation of LLMs' ability to perform logical reasoning in natural language. We introduce a controlled dataset of hypothetical and disjunctive syllogisms in propositional and modal logic and use it as the testbed for understanding LLM performance. Our results lead to novel insights in predicting LLM behaviors: in addition to the probability of input (Gonen et al., 2023; McCoy et al., 2024), logical forms should be considered as important factors. In addition, we show similarities and discrepancies between the logical reasoning performances of humans and LLMs by collecting and comparing behavioral data from both.

### MorphNLI: A Stepwise Approach to Natural Language Inference Using Text Morphing 
[[arxiv](https://arxiv.org/abs/2502.09567)] [[cool](https://papers.cool/arxiv/2502.09567)] [[pdf](https://arxiv.org/pdf/2502.09567)]
> **Authors**: Vlad Andrei Negru,Robert Vacareanu,Camelia Lemnaru,Mihai Surdeanu,Rodica Potolea
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 16 pages, 11 figures, 8 tables. Accepted for NAACL 2025 Findings
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: We introduce MorphNLI, a modular step-by-step approach to natural language inference (NLI). When classifying the premise-hypothesis pairs into {entailment, contradiction, neutral}, we use a language model to generate the necessary edits to incrementally transform (i.e., morph) the premise into the hypothesis. Then, using an off-the-shelf NLI model we track how the entailment progresses with these atomic changes, aggregating these intermediate labels into a final output. We demonstrate the advantages of our proposed method particularly in realistic cross-domain settings, where our method always outperforms strong baselines with improvements up to 12.6% (relative). Further, our proposed approach is explainable as the atomic edits can be used to understand the overall NLI label.

### Zero-shot generation of synthetic neurosurgical data with large language models 
[[arxiv](https://arxiv.org/abs/2502.09566)] [[cool](https://papers.cool/arxiv/2502.09566)] [[pdf](https://arxiv.org/pdf/2502.09566)]
> **Authors**: Austin A. Barr,Eddie Guo,Emre Sezgin
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 13 pages, 4 figures, 4 tables (updated version, fixed typos and formatting)
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Clinical data is fundamental to advance neurosurgical research, but access is often constrained by data availability, small sample sizes, privacy regulations, and resource-intensive preprocessing and de-identification procedures. Synthetic data offers a potential solution to challenges associated with accessing and using real-world data (RWD). This study aims to evaluate the capability of zero-shot generation of synthetic neurosurgical data with a large language model (LLM), GPT-4o, by benchmarking with the conditional tabular generative adversarial network (CTGAN). Synthetic datasets were compared to real-world neurosurgical data to assess fidelity (means, proportions, distributions, and bivariate correlations), utility (ML classifier performance on RWD), and privacy (duplication of records from RWD). The GPT-4o-generated datasets matched or exceeded CTGAN performance, despite no fine-tuning or access to RWD for pre-training. Datasets demonstrated high univariate and bivariate fidelity to RWD without directly exposing any real patient records, even at amplified sample size. Training an ML classifier on GPT-4o-generated data and testing on RWD for a binary prediction task showed an F1 score (0.706) with comparable performance to training on the CTGAN data (0.705) for predicting postoperative functional status deterioration. GPT-4o demonstrated a promising ability to generate high-fidelity synthetic neurosurgical data. These findings also indicate that data synthesized with GPT-4o can effectively augment clinical data with small sample sizes, and train ML models for prediction of neurosurgical outcomes. Further investigation is necessary to improve the preservation of distributional characteristics and boost classifier performance.

### Mind the Gap! Choice Independence in Using Multilingual LLMs for Persuasive Co-Writing Tasks in Different Languages 
[[arxiv](https://arxiv.org/abs/2502.09532)] [[cool](https://papers.cool/arxiv/2502.09532)] [[pdf](https://arxiv.org/pdf/2502.09532)]
> **Authors**: Shreyan Biswas,Alexander Erlei,Ujwal Gadiraju
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,人机交互
- **Abstract**: Recent advances in generative AI have precipitated a proliferation of novel writing assistants. These systems typically rely on multilingual large language models (LLMs), providing globalized workers the ability to revise or create diverse forms of content in different languages. However, there is substantial evidence indicating that the performance of multilingual LLMs varies between languages. Users who employ writing assistance for multiple languages are therefore susceptible to disparate output quality. Importantly, recent research has shown that people tend to generalize algorithmic errors across independent tasks, violating the behavioral axiom of choice independence. In this paper, we analyze whether user utilization of novel writing assistants in a charity advertisement writing task is affected by the AI's performance in a second language. Furthermore, we quantify the extent to which these patterns translate into the persuasiveness of generated charity advertisements, as well as the role of peoples' beliefs about LLM utilization in their donation choices. Our results provide evidence that writers who engage with an LLM-based writing assistant violate choice independence, as prior exposure to a Spanish LLM reduces subsequent utilization of an English LLM. While these patterns do not affect the aggregate persuasiveness of the generated advertisements, people's beliefs about the source of an advertisement (human versus AI) do. In particular, Spanish-speaking female participants who believed that they read an AI-generated advertisement strongly adjusted their donation behavior downwards. Furthermore, people are generally not able to adequately differentiate between human-generated and LLM-generated ads. Our work has important implications for the design, development, integration, and adoption of multilingual LLMs as assistive agents -- particularly in writing tasks.

### Improve LLM-based Automatic Essay Scoring with Linguistic Features 
[[arxiv](https://arxiv.org/abs/2502.09497)] [[cool](https://papers.cool/arxiv/2502.09497)] [[pdf](https://arxiv.org/pdf/2502.09497)]
> **Authors**: Zhaoyi Joey Hou,Alejandro Ciuba,Xiang Lorraine Li
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: To be published in the workshop Innovation and Responsibility inAI-Supported Education (iRaise) at the 2025 Conference on Artificial Intelligence (AAAI)
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Automatic Essay Scoring (AES) assigns scores to student essays, reducing the grading workload for instructors. Developing a scoring system capable of handling essays across diverse prompts is challenging due to the flexibility and diverse nature of the writing task. Existing methods typically fall into two categories: supervised feature-based approaches and large language model (LLM)-based methods. Supervised feature-based approaches often achieve higher performance but require resource-intensive training. In contrast, LLM-based methods are computationally efficient during inference but tend to suffer from lower performance. This paper combines these approaches by incorporating linguistic features into LLM-based scoring. Experimental results show that this hybrid method outperforms baseline models for both in-domain and out-of-domain writing prompts.

### Objective quantification of mood states using large language models 
[[arxiv](https://arxiv.org/abs/2502.09487)] [[cool](https://papers.cool/arxiv/2502.09487)] [[pdf](https://arxiv.org/pdf/2502.09487)]
> **Authors**: Jakub Onysk,Quentin Huys
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: main text - 9 pages, 5 figures;
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Emotional states influence human behaviour and cognition, leading to diverse thought trajectories. Similarly, Large Language Models (LLMs) showcase an excellent level of response consistency across wide-ranging contexts (prompts). We leverage these parallels to establish a framework for quantifying mental states. Our approach utilises self-report questionnaires that reliably assess these states due to their inherent sensitivity to patterns of co-occurring responses. Specifically, we recruited a large sample of participants (N=422) to investigate how well an LLM (Mistral-7B-OpenOrca) quantifies a heterogenous set of depressive mood states measured with participants' open-ended responses to a depression questionnaire. We show LLM responses to held-out multiple-choice questions, given participants' open-ended answers, correlate strongly (r: 0.52-0.84) with true questionnaire scores, demonstrating LLM's generalisation from mood representations. We explore a link between these representations and factor analysis. Using ridge regression, we find depression-related subspaces within LLM hidden states. We show these subspaces to be predictive of participants' "Depression" and "Somatic & Emotional Distress" factor scores, as well as suicidality severity. Overall, LLMs can provide quantitative measures of mental states. The reliability of these hinges upon how informative the questions we ask participants are. Used correctly, this approach could supplement mental state assessment in a variety of settings.

### The Multilingual Mind : A Survey of Multilingual Reasoning in Language Models 
[[arxiv](https://arxiv.org/abs/2502.09457)] [[cool](https://papers.cool/arxiv/2502.09457)] [[pdf](https://arxiv.org/pdf/2502.09457)]
> **Authors**: Akash Ghosh,Debayan Datta,Sriparna Saha,Chirag Agarwal
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: While reasoning and multilingual capabilities in Language Models (LMs) have achieved remarkable progress in recent years, their integration into a unified paradigm, multilingual reasoning, is at a nascent stage. Multilingual reasoning requires language models to handle logical reasoning across languages while addressing misalignment, biases, and challenges in low-resource settings. This survey provides the first in-depth review of multilingual reasoning in LMs. In this survey, we provide a systematic overview of existing methods that leverage LMs for multilingual reasoning, specifically outlining the challenges, motivations, and foundational aspects of applying language models to reason across diverse languages. We provide an overview of the standard data resources used for training multilingual reasoning in LMs and the evaluation benchmarks employed to assess their multilingual capabilities. Next, we analyze various state-of-the-art methods and their performance on these benchmarks. Finally, we explore future research opportunities to improve multilingual reasoning in LMs, focusing on enhancing their ability to handle diverse languages and complex reasoning tasks.

### On multi-token prediction for efficient LLM inference 
[[arxiv](https://arxiv.org/abs/2502.09419)] [[cool](https://papers.cool/arxiv/2502.09419)] [[pdf](https://arxiv.org/pdf/2502.09419)]
> **Authors**: Somesh Mehra,Javier Alonso Garcia,Lukas Mauch
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: We systematically investigate multi-token prediction (MTP) capabilities within LLMs pre-trained for next-token prediction (NTP). We first show that such models inherently possess MTP capabilities via numerical marginalization over intermediate token probabilities, though performance is data-dependent and improves with model scale. Furthermore, we explore the challenges of integrating MTP heads into frozen LLMs and find that their hidden layers are strongly specialized for NTP, making adaptation non-trivial. Finally, we show that while joint training of MTP heads with the backbone improves performance, it cannot fully overcome this barrier, prompting further research in this direction. Our findings provide a deeper understanding of MTP applied to pretrained LLMs, informing strategies for accelerating inference through parallel token prediction.

### Rethinking Evaluation Metrics for Grammatical Error Correction: Why Use a Different Evaluation Process than Human? 
[[arxiv](https://arxiv.org/abs/2502.09416)] [[cool](https://papers.cool/arxiv/2502.09416)] [[pdf](https://arxiv.org/pdf/2502.09416)]
> **Authors**: Takumi Goto,Yusuke Sakai,Taro Watanabe
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 4 pages, 2 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: One of the goals of automatic evaluation metrics in grammatical error correction (GEC) is to rank GEC systems such that it matches human preferences. However, current automatic evaluations are based on procedures that diverge from human evaluation. Specifically, human evaluation derives rankings by aggregating sentence-level relative evaluation results, e.g., pairwise comparisons, using a rating algorithm, whereas automatic evaluation averages sentence-level absolute scores to obtain corpus-level scores, which are then sorted to determine rankings. In this study, we propose an aggregation method for existing automatic evaluation metrics which aligns with human evaluation methods to bridge this gap. We conducted experiments using various metrics, including edit-based metrics, $n$-gram based metrics, and sentence-level metrics, and show that resolving the gap improves results for the most of metrics on the SEEDA benchmark. We also found that even BERT-based metrics sometimes outperform the metrics of GPT-4. We publish our unified implementation of the metrics and meta-evaluations.

### SQuARE: Sequential Question Answering Reasoning Engine for Enhanced Chain-of-Thought in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.09390)] [[cool](https://papers.cool/arxiv/2502.09390)] [[pdf](https://arxiv.org/pdf/2502.09390)]
> **Authors**: Daniel Fleischer,Moshe Berchansky,Gad Markovits,Moshe Wasserblat
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 14 pages
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: In the rapidly evolving field of Natural Language Processing, Large Language Models (LLMs) are tasked with increasingly complex reasoning challenges. Traditional methods like chain-of-thought prompting have shown promise but often fall short in fully leveraging a model's reasoning capabilities. This paper introduces SQuARE (Sequential Question Answering Reasoning Engine), a novel prompting technique designed to improve reasoning through a self-interrogation paradigm. Building upon CoT frameworks, SQuARE prompts models to generate and resolve multiple auxiliary questions before tackling the main query, promoting a more thorough exploration of various aspects of a topic. Our expansive evaluations, conducted with Llama 3 and GPT-4o models across multiple question-answering datasets, demonstrate that SQuARE significantly surpasses traditional CoT prompts and existing rephrase-and-respond methods. By systematically decomposing queries, SQuARE advances LLM capabilities in reasoning tasks. The code is publicly available at https://github.com/IntelLabs/RAG-FiT/tree/square.

### Truth Knows No Language: Evaluating Truthfulness Beyond English 
[[arxiv](https://arxiv.org/abs/2502.09387)] [[cool](https://papers.cool/arxiv/2502.09387)] [[pdf](https://arxiv.org/pdf/2502.09387)]
> **Authors**: Blanca Calvo Figueras,Eneko Sagarzazu,Julen Etxaniz,Jeremy Barnes,Pablo Gamallo,Iria De Dios Flores,Rodrigo Agerri
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 14 pages, 6 figures, 8 tables
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机与社会
- **Abstract**: We introduce a professionally translated extension of the TruthfulQA benchmark designed to evaluate truthfulness in Basque, Catalan, Galician, and Spanish. Truthfulness evaluations of large language models (LLMs) have primarily been conducted in English. However, the ability of LLMs to maintain truthfulness across languages remains under-explored. Our study evaluates 12 state-of-the-art open LLMs, comparing base and instruction-tuned models using human evaluation, multiple-choice metrics, and LLM-as-a-Judge scoring. Our findings reveal that, while LLMs perform best in English and worst in Basque (the lowest-resourced language), overall truthfulness discrepancies across languages are smaller than anticipated. Furthermore, we show that LLM-as-a-Judge correlates more closely with human judgments than multiple-choice metrics, and that informativeness plays a critical role in truthfulness assessment. Our results also indicate that machine translation provides a viable approach for extending truthfulness benchmarks to additional languages, offering a scalable alternative to professional translation. Finally, we observe that universal knowledge questions are better handled across languages than context- and time-dependent ones, highlighting the need for truthfulness evaluations that account for cultural and temporal variability. Dataset and code are publicly available under open licenses.

### Beyond English: The Impact of Prompt Translation Strategies across Languages and Tasks in Multilingual LLMs 
[[arxiv](https://arxiv.org/abs/2502.09331)] [[cool](https://papers.cool/arxiv/2502.09331)] [[pdf](https://arxiv.org/pdf/2502.09331)]
> **Authors**: Itai Mondshine,Tzuf Paz-Argaman,Reut Tsarfaty
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: Accepted for NAACL findings 2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Despite advances in the multilingual capabilities of Large Language Models (LLMs) across diverse tasks, English remains the dominant language for LLM research and development. So, when working with a different language, this has led to the widespread practice of pre-translation, i.e., translating the task prompt into English before inference. Selective pre-translation, a more surgical approach, focuses on translating specific prompt components. However, its current use is sporagic and lacks a systematic research foundation. Consequently, the optimal pre-translation strategy for various multilingual settings and tasks remains unclear. In this work, we aim to uncover the optimal setup for pre-translation by systematically assessing its use. Specifically, we view the prompt as a modular entity, composed of four functional parts: instruction, context, examples, and output, either of which could be translated or not. We evaluate pre-translation strategies across 35 languages covering both low and high-resource languages, on various tasks including Question Answering (QA), Natural Language Inference (NLI), Named Entity Recognition (NER), and Abstractive Summarization. Our experiments show the impact of factors as similarity to English, translation quality and the size of pre-trained data, on the model performance with pre-translation. We suggest practical guidelines for choosing optimal strategies in various multilingual settings.

### A Judge-free LLM Open-ended Generation Benchmark Based on the Distributional Hypothesis 
[[arxiv](https://arxiv.org/abs/2502.09316)] [[cool](https://papers.cool/arxiv/2502.09316)] [[pdf](https://arxiv.org/pdf/2502.09316)]
> **Authors**: Kentaro Imajo,Masanori Hirano,Shuji Suzuki,Hiroaki Mikami
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 13 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Evaluating the open-ended text generation of large language models (LLMs) is challenging because of the lack of a clear ground truth and the high cost of human or LLM-based assessments. We propose a novel benchmark that evaluates LLMs using n-gram statistics and rules, without relying on human judgement or LLM-as-a-judge approaches. Using 50 question and reference answer sets, we introduce three new metrics based on n-grams and rules: Fluency, Truthfulness, and Helpfulness. Our benchmark strongly correlates with GPT-4o-based evaluations while requiring significantly fewer computational resources, demonstrating its effectiveness as a scalable alternative for assessing LLMs' open-ended generation capabilities.

### When the LM misunderstood the human chuckled: Analyzing garden path effects in humans and language models 
[[arxiv](https://arxiv.org/abs/2502.09307)] [[cool](https://papers.cool/arxiv/2502.09307)] [[pdf](https://arxiv.org/pdf/2502.09307)]
> **Authors**: Samuel Joseph Amouyal,Aya Meltzer-Asscher,Jonathan Berant
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Modern Large Language Models (LLMs) have shown human-like abilities in many language tasks, sparking interest in comparing LLMs' and humans' language processing. In this paper, we conduct a detailed comparison of the two on a sentence comprehension task using garden-path constructions, which are notoriously challenging for humans. Based on psycholinguistic research, we formulate hypotheses on why garden-path sentences are hard, and test these hypotheses on human participants and a large suite of LLMs using comprehension questions. Our findings reveal that both LLMs and humans struggle with specific syntactic complexities, with some models showing high correlation with human comprehension. To complement our findings, we test LLM comprehension of garden-path constructions with paraphrasing and text-to-image generation tasks, and find that the results mirror the sentence comprehension question results, further validating our findings on LLM understanding of these constructions.

### SparQLe: Speech Queries to Text Translation Through LLMs 
[[arxiv](https://arxiv.org/abs/2502.09284)] [[cool](https://papers.cool/arxiv/2502.09284)] [[pdf](https://arxiv.org/pdf/2502.09284)]
> **Authors**: Amirbek Djanibekov,Hanan Aldarmaki
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: With the growing influence of Large Language Models (LLMs), there is increasing interest in integrating speech representations with them to enable more seamless multi-modal processing and speech understanding. This study introduces a novel approach that leverages self-supervised speech representations in combination with instruction-tuned LLMs for speech-to-text translation. The proposed approach leverages a modality adapter to align extracted speech features with instruction-tuned LLMs using English-language data. Our experiments demonstrate that this method effectively preserves the semantic content of the input speech and serves as an effective bridge between self-supervised speech models and instruction-tuned LLMs, offering a promising solution for various speech understanding applications.

### The Joint Entity-Relation Extraction Model Based on Span and Interactive Fusion Representation for Chinese Medical Texts with Complex Semantics 
[[arxiv](https://arxiv.org/abs/2502.09247)] [[cool](https://papers.cool/arxiv/2502.09247)] [[pdf](https://arxiv.org/pdf/2502.09247)]
> **Authors**: Danni Feng,Runzhi Li,Jing Wang,Siyu Yan,Lihong Ma,Yunli Xing
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Joint entity-relation extraction is a critical task in transforming unstructured or semi-structured text into triplets, facilitating the construction of large-scale knowledge graphs, and supporting various downstream applications. Despite its importance, research on Chinese text, particularly with complex semantics in specialized domains like medicine, remains limited. To address this gap, we introduce the CH-DDI, a Chinese drug-drug interactions dataset designed to capture the intricacies of medical text. Leveraging the strengths of attention mechanisms in capturing long-range dependencies, we propose the SEA module, which enhances the extraction of complex contextual semantic information, thereby improving entity recognition and relation extraction. Additionally, to address the inefficiencies of existing methods in facilitating information exchange between entity recognition and relation extraction, we present an interactive fusion representation module. This module employs Cross Attention for bidirectional information exchange between the tasks and further refines feature extraction through BiLSTM. Experimental results on both our CH-DDI dataset and public CoNLL04 dataset demonstrate that our model exhibits strong generalization capabilities. On the CH-DDI dataset, our model achieves an F1-score of 96.73% for entity recognition and 78.43% for relation extraction. On the CoNLL04 dataset, it attains an entity recognition precision of 89.54% and a relation extraction accuracy of 71.64%.

### Thinking beyond the anthropomorphic paradigm benefits LLM research 
[[arxiv](https://arxiv.org/abs/2502.09192)] [[cool](https://papers.cool/arxiv/2502.09192)] [[pdf](https://arxiv.org/pdf/2502.09192)]
> **Authors**: Lujain Ibrahim,Myra Cheng
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Anthropomorphism, or the attribution of human traits to technology, is an automatic and unconscious response that occurs even in those with advanced technical expertise. In this position paper, we analyze hundreds of thousands of computer science research articles from the past decade and present empirical evidence of the prevalence and growth of anthropomorphic terminology in research on large language models (LLMs). This terminology reflects deeper anthropomorphic conceptualizations which shape how we think about and conduct LLM research. We argue these conceptualizations may be limiting, and that challenging them opens up new pathways for understanding and improving LLMs beyond human analogies. To illustrate this, we identify and analyze five core anthropomorphic assumptions shaping prominent methodologies across the LLM development lifecycle, from the assumption that models must use natural language for reasoning tasks to the assumption that model capabilities should be evaluated through human-centric benchmarks. For each assumption, we demonstrate how non-anthropomorphic alternatives can open new directions for research and development.

### Matina: A Large-Scale 73B Token Persian Text Corpus 
[[arxiv](https://arxiv.org/abs/2502.09188)] [[cool](https://papers.cool/arxiv/2502.09188)] [[pdf](https://arxiv.org/pdf/2502.09188)]
> **Authors**: Sara Bourbour Hosseinbeigi,Fatemeh Taherinezhad,Heshaam Faili,Hamed Baghbani,Fatemeh Nadi,Mostafa Amiri
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Text corpora are essential for training models used in tasks like summarization, translation, and large language models (LLMs). While various efforts have been made to collect monolingual and multilingual datasets in many languages, Persian has often been underrepresented due to limited resources for data collection and preprocessing. Existing Persian datasets are typically small and lack content diversity, consisting mainly of weblogs and news articles. This shortage of high-quality, varied data has slowed the development of NLP models and open-source LLMs for Persian. Since model performance depends heavily on the quality of training data, we address this gap by introducing the Matina corpus, a new Persian dataset of 72.9B tokens, carefully preprocessed and deduplicated to ensure high data quality. We further assess its effectiveness by training and evaluating transformer-based models on key NLP tasks. Both the dataset and preprocessing codes are publicly available, enabling researchers to build on and improve this resource for future Persian NLP advancements.

### RefineCoder: Iterative Improving of Large Language Models via Adaptive Critique Refinement for Code Generation 
[[arxiv](https://arxiv.org/abs/2502.09183)] [[cool](https://papers.cool/arxiv/2502.09183)] [[pdf](https://arxiv.org/pdf/2502.09183)]
> **Authors**: Changzhi Zhou,Xinyu Zhang,Dandan Song,Xiancai Chen,Wanli Gu,Huipeng Ma,Yuhang Tian,Mengdi Zhang,Linmei Hu
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: work in process
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Code generation has attracted increasing attention with the rise of Large Language Models (LLMs). Many studies have developed powerful code LLMs by synthesizing code-related instruction data and applying supervised fine-tuning. However, these methods are limited by teacher model distillation and ignore the potential of iterative refinement by self-generated code. In this paper, we propose Adaptive Critique Refinement (ACR), which enables the model to refine itself by self-generated code and external critique, rather than directly imitating the code responses of the teacher model. Concretely, ACR includes a composite scoring system with LLM-as-a-Judge to evaluate the quality of code responses and a selective critique strategy with LLM-as-a-Critic to critique self-generated low-quality code responses. We develop the RefineCoder series by iteratively applying ACR, achieving continuous performance improvement on multiple code generation benchmarks. Compared to the baselines of the same size, our proposed RefineCoder series can achieve comparable or even superior performance using less data.

### Musical Heritage Historical Entity Linking 
[[arxiv](https://arxiv.org/abs/2502.09168)] [[cool](https://papers.cool/arxiv/2502.09168)] [[pdf](https://arxiv.org/pdf/2502.09168)]
> **Authors**: Arianna Graciotti,Nicolas Lazzari,Valentina Presutti,Rocco Tripodi
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: To appear in Artificial Intelligence Review Journal
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Linking named entities occurring in text to their corresponding entity in a Knowledge Base (KB) is challenging, especially when dealing with historical texts. In this work, we introduce Musical Heritage named Entities Recognition, Classification and Linking (MHERCL), a novel benchmark consisting of manually annotated sentences extrapolated from historical periodicals of the music domain. MHERCL contains named entities under-represented or absent in the most famous KBs. We experiment with several State-of-the-Art models on the Entity Linking (EL) task and show that MHERCL is a challenging dataset for all of them. We propose a novel unsupervised EL model and a method to extend supervised entity linkers by using Knowledge Graphs (KGs) to tackle the main difficulties posed by historical documents. Our experiments reveal that relying on unsupervised techniques and improving models with logical constraints based on KGs and heuristics to predict NIL entities (entities not represented in the KB of reference) results in better EL performance on historical documents.

### Improving TCM Question Answering through Tree-Organized Self-Reflective Retrieval with LLMs 
[[arxiv](https://arxiv.org/abs/2502.09156)] [[cool](https://papers.cool/arxiv/2502.09156)] [[pdf](https://arxiv.org/pdf/2502.09156)]
> **Authors**: Chang Liu,Ying Chang,Jianmin Li,Yiqian Qu,Yu Li,Lingyong Cao,Shuyuan Lin
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Objectives: Large language models (LLMs) can harness medical knowledge for intelligent question answering (Q&A), promising support for auxiliary diagnosis and medical talent cultivation. However, there is a deficiency of highly efficient retrieval-augmented generation (RAG) frameworks within the domain of Traditional Chinese Medicine (TCM). Our purpose is to observe the effect of the Tree-Organized Self-Reflective Retrieval (TOSRR) framework on LLMs in TCM Q&A tasks. Materials and Methods: We introduce the novel approach of knowledge organization, constructing a tree structure knowledge base with hierarchy. At inference time, our self-reflection framework retrieves from this knowledge base, integrating information across chapters. Questions from the TCM Medical Licensing Examination (MLE) and the college Classics Course Exam (CCE) were randomly selected as benchmark datasets. Results: By coupling with GPT-4, the framework can improve the best performance on the TCM MLE benchmark by 19.85% in absolute accuracy, and improve recall accuracy from 27% to 38% on CCE datasets. In manual evaluation, the framework improves a total of 18.52 points across dimensions of safety, consistency, explainability, compliance, and coherence. Conclusion: The TOSRR framework can effectively improve LLM's capability in Q&A tasks of TCM.

### A Novel Dialect-Aware Framework for the Classification of Arabic Dialects and Emotions 
[[arxiv](https://arxiv.org/abs/2502.09128)] [[cool](https://papers.cool/arxiv/2502.09128)] [[pdf](https://arxiv.org/pdf/2502.09128)]
> **Authors**: Nasser A Alsadhan
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: ef:Journal of Computer Science, 21(1) 2025, 88-95
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Arabic is one of the oldest languages still in use today. As a result, several Arabic-speaking regions have developed dialects that are unique to them. Dialect and emotion recognition have various uses in Arabic text analysis, such as determining an online customer's origin based on their comments. Furthermore, intelligent chatbots that are aware of a user's emotions can respond appropriately to the user. Current research in emotion detection in the Arabic language lacks awareness of how emotions are exhibited in different dialects, which motivates the work found in this study. This research addresses the problems of dialect and emotion classification in Arabic. Specifically, this is achieved by building a novel framework that can identify and predict Arabic dialects and emotions from a given text. The framework consists of three modules: A text-preprocessing module, a classification module, and a clustering module with the novel capability of building new dialect-aware emotion lexicons. The proposed framework generated a new emotional lexicon for different dialects. It achieved an accuracy of 88.9% in classifying Arabic dialects, which outperforms the state-of-the-art results by 6.45 percentage points. Furthermore, the framework achieved 89.1-79% accuracy in detecting emotions in the Egyptian and Gulf dialects, respectively.

### The influence of visual and linguistic cues on ignorance inference in Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2502.09120)] [[cool](https://papers.cool/arxiv/2502.09120)] [[pdf](https://arxiv.org/pdf/2502.09120)]
> **Authors**: Ye-eun Cho,Yunho Maeng
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 13 pages, 3 figures, 3 tables
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: This study explored how Vision-Language Models (VLMs) process ignorance implicatures with visual and linguistic cues. Particularly, we focused on the effects of contexts (precise and approximate contexts) and modifier types (bare numerals, superlative, and comparative modifiers), which were considered pragmatic and semantic factors respectively. Methodologically, we conducted a truth-value judgment task in visually grounded settings using GPT-4o and Gemini 1.5 Pro. The results indicate that while both models exhibited sensitivity to linguistic cues (modifier), they failed to process ignorance implicatures with visual cues (context) as humans do. Specifically, the influence of context was weaker and inconsistent across models, indicating challenges in pragmatic reasoning for VLMs. On the other hand, superlative modifiers were more strongly associated with ignorance implicatures as compared to comparative modifiers, supporting the semantic view. These findings highlight the need for further advancements in VLMs to process language-vision information in a context-dependent way to achieve human-like pragmatic inference.

### A Hybrid Transformer Model for Fake News Detection: Leveraging Bayesian Optimization and Bidirectional Recurrent Unit 
[[arxiv](https://arxiv.org/abs/2502.09097)] [[cool](https://papers.cool/arxiv/2502.09097)] [[pdf](https://arxiv.org/pdf/2502.09097)]
> **Authors**: Tianyi Huang,Zeqiu Xu,Peiyang Yu,Jingyuan Yi,Xiaochuan Xu
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 6 pages, 7 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: In this paper, we propose an optimized Transformer model that integrates Bayesian algorithms with a Bidirectional Gated Recurrent Unit (BiGRU), and apply it to fake news classification for the first time. First, we employ the TF-IDF method to extract features from news texts and transform them into numeric representations to facilitate subsequent machine learning tasks. Two sets of experiments are then conducted for fake news detection and classification: one using a Transformer model optimized only with BiGRU, and the other incorporating Bayesian algorithms into the BiGRU-based Transformer. Experimental results show that the BiGRU-optimized Transformer achieves 100% accuracy on the training set and 99.67% on the test set, while the addition of the Bayesian algorithm maintains 100% accuracy on the training set and slightly improves test-set accuracy to 99.73%. This indicates that the Bayesian algorithm boosts model accuracy by 0.06%, further enhancing the detection capability for fake news. Moreover, the proposed algorithm converges rapidly at around the 10th training epoch with accuracy nearing 100%, demonstrating both its effectiveness and its fast classification ability. Overall, the optimized Transformer model, enhanced by the Bayesian algorithm and BiGRU, exhibits excellent continuous learning and detection performance, offering a robust technical means to combat the spread of fake news in the current era of information overload.

### A Hybrid Model for Few-Shot Text Classification Using Transfer and Meta-Learning 
[[arxiv](https://arxiv.org/abs/2502.09086)] [[cool](https://papers.cool/arxiv/2502.09086)] [[pdf](https://arxiv.org/pdf/2502.09086)]
> **Authors**: Jia Gao,Shuangquan Lyu,Guiran Liu,Binrong Zhu,Hongye Zheng,Xiaoxuan Liao
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: With the continuous development of natural language processing (NLP) technology, text classification tasks have been widely used in multiple application fields. However, obtaining labeled data is often expensive and difficult, especially in few-shot learning scenarios. To solve this problem, this paper proposes a few-shot text classification model based on transfer learning and meta-learning. The model uses the knowledge of the pre-trained model for transfer and optimizes the model's rapid adaptability in few-sample tasks through a meta-learning mechanism. Through a series of comparative experiments and ablation experiments, we verified the effectiveness of the proposed method. The experimental results show that under the conditions of few samples and medium samples, the model based on transfer learning and meta-learning significantly outperforms traditional machine learning and deep learning methods. In addition, ablation experiments further analyzed the contribution of each component to the model performance and confirmed the key role of transfer learning and meta-learning in improving model accuracy. Finally, this paper discusses future research directions and looks forward to the potential of this method in practical applications.

### CoSER: Coordinating LLM-Based Persona Simulation of Established Roles 
[[arxiv](https://arxiv.org/abs/2502.09082)] [[cool](https://papers.cool/arxiv/2502.09082)] [[pdf](https://arxiv.org/pdf/2502.09082)]
> **Authors**: Xintao Wang,Heng Wang,Yifei Zhang,Xinfeng Yuan,Rui Xu,Jen-tse Huang,Siyu Yuan,Haoran Guo,Jiangjie Chen,Wei Wang,Yanghua Xiao,Shuchang Zhou
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Role-playing language agents (RPLAs) have emerged as promising applications of large language models (LLMs). However, simulating established characters presents a challenging task for RPLAs, due to the lack of authentic character datasets and nuanced evaluation methods using such data. In this paper, we present CoSER, a collection of a high-quality dataset, open models, and an evaluation protocol towards effective RPLAs of established characters. The CoSER dataset covers 17,966 characters from 771 renowned books. It provides authentic dialogues with real-world intricacies, as well as diverse data types such as conversation setups, character experiences and internal thoughts. Drawing from acting methodology, we introduce given-circumstance acting for training and evaluating role-playing LLMs, where LLMs sequentially portray multiple characters in book scenes. Using our dataset, we develop CoSER 8B and CoSER 70B, i.e., advanced open role-playing LLMs built on LLaMA-3.1 models. Extensive experiments demonstrate the value of the CoSER dataset for RPLA training, evaluation and retrieval. Moreover, CoSER 70B exhibits state-of-the-art performance surpassing or matching GPT-4o on our evaluation and three existing benchmarks, i.e., achieving 75.80% and 93.47% accuracy on the InCharacter and LifeChoice benchmarks respectively.

### Enhancing RAG with Active Learning on Conversation Records: Reject Incapables and Answer Capables 
[[arxiv](https://arxiv.org/abs/2502.09073)] [[cool](https://papers.cool/arxiv/2502.09073)] [[pdf](https://arxiv.org/pdf/2502.09073)]
> **Authors**: Xuzhao Geng,Haozhao Wang,Jun Wang,Wei Liu,Ruixuan Li
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Retrieval-augmented generation (RAG) is a key technique for leveraging external knowledge and reducing hallucinations in large language models (LLMs). However, RAG still struggles to fully prevent hallucinated responses. To address this, it is essential to identify samples prone to hallucination or guide LLMs toward correct responses, which experts then annotate to develop high-quality datasets for refining LLMs. However, the growing scarcity of such datasets makes their creation challenging. This paper proposes using the vast amount of conversations from widespread LLM usage to build these datasets, training LLMs to avoid hallucination-prone questions while accurately responding to manageable ones. Given the impracticality of expert-annotating all conversation records, the paper introduces AL4RAG, which uses active learning to select the most suitable conversation samples for annotation, optimizing performance within an annotation budget. Additionally, recognizing that traditional active learning methods are not fully compatible with RAG due to unsuitable distance metrics, we develop a novel sample distance measurement for RAG active learning. Extensive experiments show that our method consistently outperforms baselines across multiple metrics.

### Adapting Language-Specific LLMs to a Reasoning Model in One Day via Model Merging -- An Open Recipe 
[[arxiv](https://arxiv.org/abs/2502.09056)] [[cool](https://papers.cool/arxiv/2502.09056)] [[pdf](https://arxiv.org/pdf/2502.09056)]
> **Authors**: Kunat Pipatanakul,Pittawat Taveekitworachai,Potsawee Manakul,Kasima Tharnpipitchai
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 9 pages
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: This paper investigates data selection and model merging methodologies aimed at incorporating advanced reasoning capabilities such as those of DeepSeek R1 into language-specific large language models (LLMs), with a particular focus on the Thai LLM. Our goal is to enhance the reasoning capabilities of language-specific LLMs while maintaining their target language abilities. DeepSeek R1 excels in reasoning but primarily benefits high-resource languages such as English and Chinese. However, low-resource languages remain underserved due to the dominance of English-centric training data and model optimizations, which limit performance in these languages. This limitation results in unreliable code-switching and diminished effectiveness on tasks in low-resource languages. Meanwhile, local and regional LLM initiatives have attempted to bridge this gap by developing language-specific LLMs that focus on improving local linguistic fidelity. We demonstrate that, with only publicly available datasets and a computational budget of $120, it is possible to enhance the reasoning capabilities of language-specific LLMs to match the level of DeepSeek R1, without compromising their performance on target language tasks.

### Typhoon T1: An Open Thai Reasoning Model 
[[arxiv](https://arxiv.org/abs/2502.09042)] [[cool](https://papers.cool/arxiv/2502.09042)] [[pdf](https://arxiv.org/pdf/2502.09042)]
> **Authors**: Pittawat Taveekitworachai,Potsawee Manakul,Kasima Tharnpipitchai,Kunat Pipatanakul
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 25 pages, 6 figures
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: This paper introduces Typhoon T1, an open effort to develop an open Thai reasoning model. A reasoning model is a relatively new type of generative model built on top of large language models (LLMs). A reasoning model generates a long chain of thought before arriving at a final answer, an approach found to improve performance on complex tasks. However, details on developing such a model are limited, especially for reasoning models that can generate traces in a low-resource language. Typhoon T1 presents an open effort that dives into the details of developing a reasoning model in a more cost-effective way by leveraging supervised fine-tuning using open datasets, instead of reinforcement learning. This paper shares the details about synthetic data generation and training, as well as our dataset and model weights. Additionally, we provide insights gained from developing a reasoning model that generalizes across domains and is capable of generating reasoning traces in a low-resource language, using Thai as an example. We hope this open effort provides a foundation for further research in this field.

### Diversity Enhances an LLM's Performance in RAG and Long-context Task 
[[arxiv](https://arxiv.org/abs/2502.09017)] [[cool](https://papers.cool/arxiv/2502.09017)] [[pdf](https://arxiv.org/pdf/2502.09017)]
> **Authors**: Zhchao Wang,Bin Bi,Yanqi Luo,Sitaram Asur,Claire Na Cheng
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: The rapid advancements in large language models (LLMs) have highlighted the challenge of context window limitations, primarily due to the quadratic time complexity of the self-attention mechanism (\(O(N^2)\), where \(N\) denotes the context window length). This constraint impacts tasks such as retrieval-augmented generation (RAG) in question answering (Q\&A) and long context summarization. A common approach involves selecting content with the highest similarity to the query; however, this often leads to redundancy and the exclusion of diverse yet relevant information. Building on principles from Maximal Marginal Relevance (MMR) and Farthest Point Sampling (FPS), we integrate diversity into the content selection process. Our findings reveal that incorporating diversity substantially increases the recall of selecting relevant sentences or chunks before LLM-based Q\&A and summarization. These results highlight the importance of maintaining diversity in future LLM applications to further improve summarization and Q\&A outcomes.

### Hope vs. Hate: Understanding User Interactions with LGBTQ+ News Content in Mainstream US News Media through the Lens of Hope Speech 
[[arxiv](https://arxiv.org/abs/2502.09004)] [[cool](https://papers.cool/arxiv/2502.09004)] [[pdf](https://arxiv.org/pdf/2502.09004)]
> **Authors**: Jonathan Pofcher,Christopher M. Homan,Randall Sell,Ashiqur R. KhudaBukhsh
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,计算机与社会,机器学习
- **Abstract**: This paper makes three contributions. First, via a substantial corpus of 1,419,047 comments posted on 3,161 YouTube news videos of major US cable news outlets, we analyze how users engage with LGBTQ+ news content. Our analyses focus both on positive and negative content. In particular, we construct a fine-grained hope speech classifier that detects positive (hope speech), negative, neutral, and irrelevant content. Second, in consultation with a public health expert specializing on LGBTQ+ health, we conduct an annotation study with a balanced and diverse political representation and release a dataset of 3,750 instances with fine-grained labels and detailed annotator demographic information. Finally, beyond providing a vital resource for the LGBTQ+ community, our annotation study and subsequent in-the-wild assessments reveal (1) strong association between rater political beliefs and how they rate content relevant to a marginalized community; (2) models trained on individual political beliefs exhibit considerable in-the-wild disagreement; and (3) zero-shot large language models (LLMs) align more with liberal raters.

### Tuning-Free Personalized Alignment via Trial-Error-Explain In-Context Learning 
[[arxiv](https://arxiv.org/abs/2502.08972)] [[cool](https://papers.cool/arxiv/2502.08972)] [[pdf](https://arxiv.org/pdf/2502.08972)]
> **Authors**: Hyundong Cho,Karishma Sharma,Nicolaas Jedema,Leonardo F. R. Ribeiro,Alessandro Moschitti,Ravi Krishnan,Jonathan May
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: NAACL 2025 Findings
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Language models are aligned to the collective voice of many, resulting in generic outputs that do not align with specific users' styles. In this work, we present Trial-Error-Explain In-Context Learning (TICL), a tuning-free method that personalizes language models for text generation tasks with fewer than 10 examples per user. TICL iteratively expands an in-context learning prompt via a trial-error-explain process, adding model-generated negative samples and explanations that provide fine-grained guidance towards a specific user's style. TICL achieves favorable win rates on pairwise comparisons with LLM-as-a-judge up to 91.5% against the previous state-of-the-art and outperforms competitive tuning-free baselines for personalized alignment tasks of writing emails, essays and news articles. Both lexical and qualitative analyses show that the negative samples and explanations enable language models to learn stylistic context more effectively and overcome the bias towards structural and formal phrases observed in their zero-shot outputs. By front-loading inference compute to create a user-specific in-context learning prompt that does not require extra generation steps at test time, TICL presents a novel yet simple approach for personalized alignment.

## 密码学和安全(cs.CR:Cryptography and Security)

### AgentGuard: Repurposing Agentic Orchestrator for Safety Evaluation of Tool Orchestration 
[[arxiv](https://arxiv.org/abs/2502.09809)] [[cool](https://papers.cool/arxiv/2502.09809)] [[pdf](https://arxiv.org/pdf/2502.09809)]
> **Authors**: Jizhou Chen,Samuel Lee Cong
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: Project report of AgentGuard inLLMAgent MOOC Hackathon hosted by UC Berkeley in 2024
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: The integration of tool use into large language models (LLMs) enables agentic systems with real-world impact. In the meantime, unlike standalone LLMs, compromised agents can execute malicious workflows with more consequential impact, signified by their tool-use capability. We propose AgentGuard, a framework to autonomously discover and validate unsafe tool-use workflows, followed by generating safety constraints to confine the behaviors of agents, achieving the baseline of safety guarantee at deployment. AgentGuard leverages the LLM orchestrator's innate capabilities - knowledge of tool functionalities, scalable and realistic workflow generation, and tool execution privileges - to act as its own safety evaluator. The framework operates through four phases: identifying unsafe workflows, validating them in real-world execution, generating safety constraints, and validating constraint efficacy. The output, an evaluation report with unsafe workflows, test cases, and validated constraints, enables multiple security applications. We empirically demonstrate AgentGuard's feasibility with experiments. With this exploratory work, we hope to inspire the establishment of standardized testing and hardening procedures for LLM agents to enhance their trustworthiness in real-world applications.

### Enhancing Jailbreak Attacks via Compliance-Refusal-Based Initialization 
[[arxiv](https://arxiv.org/abs/2502.09755)] [[cool](https://papers.cool/arxiv/2502.09755)] [[pdf](https://arxiv.org/pdf/2502.09755)]
> **Authors**: Amit Levi,Rom Himelstein,Yaniv Nemcovsky,Avi Mendelson,Chaim Baskin
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,机器学习
- **Abstract**: Jailbreak attacks aim to exploit large language models (LLMs) and pose a significant threat to their proper conduct; they seek to bypass models' safeguards and often provoke transgressive behaviors. However, existing automatic jailbreak attacks require extensive computational resources and are prone to converge on suboptimal solutions. In this work, we propose \textbf{C}ompliance \textbf{R}efusal \textbf{I}nitialization (CRI), a novel, attack-agnostic framework that efficiently initializes the optimization in the proximity of the compliance subspace of harmful prompts. By narrowing the initial gap to the adversarial objective, CRI substantially improves adversarial success rates (ASR) and drastically reduces computational overhead -- often requiring just a single optimization step. We evaluate CRI on the widely-used AdvBench dataset over the standard jailbreak attacks of GCG and AutoDAN. Results show that CRI boosts ASR and decreases the median steps to success by up to \textbf{\(\times 60\)}. The project page, along with the reference implementation, is publicly available at \texttt{https://amit1221levi.github.io/CRI-Jailbreak-Init-LLMs-evaluation/}.

### Making Them a Malicious Database: Exploiting Query Code to Jailbreak Aligned Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.09723)] [[cool](https://papers.cool/arxiv/2502.09723)] [[pdf](https://arxiv.org/pdf/2502.09723)]
> **Authors**: Qingsong Zou,Jingyu Xiao,Qing Li,Zhi Yan,Yuhang Wang,Li Xu,Wenxuan Wang,Kuofeng Gao,Ruoyu Li,Yong Jiang
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 15 pages, 11 figures
- **标题**: None
- **领域**: 密码学和安全,人工智能,计算语言学
- **Abstract**: Recent advances in large language models (LLMs) have demonstrated remarkable potential in the field of natural language processing. Unfortunately, LLMs face significant security and ethical risks. Although techniques such as safety alignment are developed for defense, prior researches reveal the possibility of bypassing such defenses through well-designed jailbreak attacks. In this paper, we propose QueryAttack, a novel framework to examine the generalizability of safety alignment. By treating LLMs as knowledge databases, we translate malicious queries in natural language into structured non-natural query language to bypass the safety alignment mechanisms of LLMs. We conduct extensive experiments on mainstream LLMs, and the results show that QueryAttack not only can achieve high attack success rates (ASRs), but also can jailbreak various defense methods. Furthermore, we tailor a defense method against QueryAttack, which can reduce ASR by up to 64% on GPT-4-1106. Our code is available at https://github.com/horizonsinzqs/QueryAttack.

### SyntheticPop: Attacking Speaker Verification Systems With Synthetic VoicePops 
[[arxiv](https://arxiv.org/abs/2502.09553)] [[cool](https://papers.cool/arxiv/2502.09553)] [[pdf](https://arxiv.org/pdf/2502.09553)]
> **Authors**: Eshaq Jamdar,Amith Kamath Belman
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,机器学习
- **Abstract**: Voice Authentication (VA), also known as Automatic Speaker Verification (ASV), is a widely adopted authentication method, particularly in automated systems like banking services, where it serves as a secondary layer of user authentication. Despite its popularity, VA systems are vulnerable to various attacks, including replay, impersonation, and the emerging threat of deepfake audio that mimics the voice of legitimate users. To mitigate these risks, several defense mechanisms have been proposed. One such solution, Voice Pops, aims to distinguish an individual's unique phoneme pronunciations during the enrollment process. While promising, the effectiveness of VA+VoicePop against a broader range of attacks, particularly logical or adversarial attacks, remains insufficiently explored. We propose a novel attack method, which we refer to as SyntheticPop, designed to target the phoneme recognition capabilities of the VA+VoicePop system. The SyntheticPop attack involves embedding synthetic "pop" noises into spoofed audio samples, significantly degrading the model's performance. We achieve an attack success rate of over 95% while poisoning 20% of the training dataset. Our experiments demonstrate that VA+VoicePop achieves 69% accuracy under normal conditions, 37% accuracy when subjected to a baseline label flipping attack, and just 14% accuracy under our proposed SyntheticPop attack, emphasizing the effectiveness of our method.

### PenTest++: Elevating Ethical Hacking with AI and Automation 
[[arxiv](https://arxiv.org/abs/2502.09484)] [[cool](https://papers.cool/arxiv/2502.09484)] [[pdf](https://arxiv.org/pdf/2502.09484)]
> **Authors**: Haitham S. Al-Sinani,Chris J. Mitchell
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 27 pages, 6 figures
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Traditional ethical hacking relies on skilled professionals and time-intensive command management, which limits its scalability and efficiency. To address these challenges, we introduce PenTest++, an AI-augmented system that integrates automation with generative AI (GenAI) to optimise ethical hacking workflows. Developed in a controlled virtual environment, PenTest++ streamlines critical penetration testing tasks, including reconnaissance, scanning, enumeration, exploitation, and documentation, while maintaining a modular and adaptable design. The system balances automation with human oversight, ensuring informed decision-making at key stages, and offers significant benefits such as enhanced efficiency, scalability, and adaptability. However, it also raises ethical considerations, including privacy concerns and the risks of AI-generated inaccuracies (hallucinations). This research underscores the potential of AI-driven systems like PenTest++ to complement human expertise in cybersecurity by automating routine tasks, enabling professionals to focus on strategic decision-making. By incorporating robust ethical safeguards and promoting ongoing refinement, PenTest++ demonstrates how AI can be responsibly harnessed to address operational and ethical challenges in the evolving cybersecurity landscape.

### FLAME: Flexible LLM-Assisted Moderation Engine 
[[arxiv](https://arxiv.org/abs/2502.09175)] [[cool](https://papers.cool/arxiv/2502.09175)] [[pdf](https://arxiv.org/pdf/2502.09175)]
> **Authors**: Ivan Bakulin,Ilia Kopanichuk,Iaroslav Bespalov,Nikita Radchenko,Vladimir Shaposhnikov,Dmitry Dylov,Ivan Oseledets
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能,计算语言学
- **Abstract**: The rapid advancement of Large Language Models (LLMs) has introduced significant challenges in moderating user-model interactions. While LLMs demonstrate remarkable capabilities, they remain vulnerable to adversarial attacks, particularly ``jailbreaking'' techniques that bypass content safety measures. Current content moderation systems, which primarily rely on input prompt filtering, have proven insufficient, with techniques like Best-of-N (BoN) jailbreaking achieving success rates of 80% or more against popular LLMs. In this paper, we introduce Flexible LLM-Assisted Moderation Engine (FLAME): a new approach that shifts the focus from input filtering to output moderation. Unlike traditional circuit-breaking methods that analyze user queries, FLAME evaluates model responses, offering several key advantages: (1) computational efficiency in both training and inference, (2) enhanced resistance to BoN jailbreaking attacks, and (3) flexibility in defining and updating safety criteria through customizable topic filtering. Our experiments demonstrate that FLAME significantly outperforms current moderation systems. For example, FLAME reduces attack success rate in GPT-4o-mini and DeepSeek-v3 by a factor of ~9, while maintaining low computational overhead. We provide comprehensive evaluation on various LLMs and analyze the engine's efficiency against the state-of-the-art jailbreaking. This work contributes to the development of more robust and adaptable content moderation systems for LLMs.

### Application of Tabular Transformer Architectures for Operating System Fingerprinting 
[[arxiv](https://arxiv.org/abs/2502.09084)] [[cool](https://papers.cool/arxiv/2502.09084)] [[pdf](https://arxiv.org/pdf/2502.09084)]
> **Authors**: Rubén Pérez-Jove,Cristian R. Munteanu,Alejandro Pazos,Jose Vázquez-Naya
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: Submitted as a preprint (not peer reviewed). 22 pages, 9 figures. Code and datasets available at: https://github.com/rubenpjove/tabularT-OS-fingerprinting
- **标题**: None
- **领域**: 密码学和安全,机器学习,网络和互联网架构
- **Abstract**: Operating System (OS) fingerprinting is essential for network management and cybersecurity, enabling accurate device identification based on network traffic analysis. Traditional rule-based tools such as Nmap and p0f face challenges in dynamic environments due to frequent OS updates and obfuscation techniques. While Machine Learning (ML) approaches have been explored, Deep Learning (DL) models, particularly Transformer architectures, remain unexploited in this domain. This study investigates the application of Tabular Transformer architectures-specifically TabTransformer and FT-Transformer-for OS fingerprinting, leveraging structured network data from three publicly available datasets. Our experiments demonstrate that FT-Transformer generally outperforms traditional ML models, previous approaches and TabTransformer across multiple classification levels (OS family, major, and minor versions). The results establish a strong foundation for DL-based OS fingerprinting, improving accuracy and adaptability in complex network environments. Furthermore, we ensure the reproducibility of our research by providing an open-source implementation.

### RLSA-PFL: Robust Lightweight Secure Aggregation with Model Inconsistency Detection in Privacy-Preserving Federated Learning 
[[arxiv](https://arxiv.org/abs/2502.08989)] [[cool](https://papers.cool/arxiv/2502.08989)] [[pdf](https://arxiv.org/pdf/2502.08989)]
> **Authors**: Nazatul H. Sultan,Yan Bo,Yansong Gao,Seyit Camtepe,Arash Mahboubi,Hang Thanh Bui,Aufeef Chauhan,Hamed Aboutorab,Michael Bewong,Praveen Gauravaram,Rafiqul Islam,Sharif Abuadbba
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 16 pages, 10 Figures
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Federated Learning (FL) allows users to collaboratively train a global machine learning model by sharing local model only, without exposing their private data to a central server. This distributed learning is particularly appealing in scenarios where data privacy is crucial, and it has garnered substantial attention from both industry and academia. However, studies have revealed privacy vulnerabilities in FL, where adversaries can potentially infer sensitive information from the shared model parameters. In this paper, we present an efficient masking-based secure aggregation scheme utilizing lightweight cryptographic primitives to mitigate privacy risks. Our scheme offers several advantages over existing methods. First, it requires only a single setup phase for the entire FL training session, significantly reducing communication overhead. Second, it minimizes user-side overhead by eliminating the need for user-to-user interactions, utilizing an intermediate server layer and a lightweight key negotiation method. Third, the scheme is highly resilient to user dropouts, and the users can join at any FL round. Fourth, it can detect and defend against malicious server activities, including recently discovered model inconsistency attacks. Finally, our scheme ensures security in both semi-honest and malicious settings. We provide security analysis to formally prove the robustness of our approach. Furthermore, we implemented an end-to-end prototype of our scheme. We conducted comprehensive experiments and comparisons, which show that it outperforms existing solutions in terms of communication and computation overhead, functionality, and security.

### RTBAS: Defending LLM Agents Against Prompt Injection and Privacy Leakage 
[[arxiv](https://arxiv.org/abs/2502.08966)] [[cool](https://papers.cool/arxiv/2502.08966)] [[pdf](https://arxiv.org/pdf/2502.08966)]
> **Authors**: Peter Yong Zhong,Siyuan Chen,Ruiqi Wang,McKenna McCall,Ben L. Titzer,Heather Miller,Phillip B. Gibbons
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Tool-Based Agent Systems (TBAS) allow Language Models (LMs) to use external tools for tasks beyond their standalone capabilities, such as searching websites, booking flights, or making financial transactions. However, these tools greatly increase the risks of prompt injection attacks, where malicious content hijacks the LM agent to leak confidential data or trigger harmful actions. Existing defenses (OpenAI GPTs) require user confirmation before every tool call, placing onerous burdens on users. We introduce Robust TBAS (RTBAS), which automatically detects and executes tool calls that preserve integrity and confidentiality, requiring user confirmation only when these safeguards cannot be ensured. RTBAS adapts Information Flow Control to the unique challenges presented by TBAS. We present two novel dependency screeners, using LM-as-a-judge and attention-based saliency, to overcome these challenges. Experimental results on the AgentDojo Prompt Injection benchmark show RTBAS prevents all targeted attacks with only a 2% loss of task utility when under attack, and further tests confirm its ability to obtain near-oracle performance on detecting both subtle and direct privacy leaks.

## 计算机视觉和模式识别(cs.CV:Computer Vision and Pattern Recognition)

### Insect-Foundation: A Foundation Model and Large Multimodal Dataset for Vision-Language Insect Understanding 
[[arxiv](https://arxiv.org/abs/2502.09906)] [[cool](https://papers.cool/arxiv/2502.09906)] [[pdf](https://arxiv.org/pdf/2502.09906)]
> **Authors**: Thanh-Dat Truong,Hoang-Quan Nguyen,Xuan-Bac Nguyen,Ashley Dowling,Xin Li,Khoa Luu
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Multimodal conversational generative AI has shown impressive capabilities in various vision and language understanding through learning massive text-image data. However, current conversational models still lack knowledge about visual insects since they are often trained on the general knowledge of vision-language data. Meanwhile, understanding insects is a fundamental problem in precision agriculture, helping to promote sustainable development in agriculture. Therefore, this paper proposes a novel multimodal conversational model, Insect-LLaVA, to promote visual understanding in insect-domain knowledge. In particular, we first introduce a new large-scale Multimodal Insect Dataset with Visual Insect Instruction Data that enables the capability of learning the multimodal foundation models. Our proposed dataset enables conversational models to comprehend the visual and semantic features of the insects. Second, we propose a new Insect-LLaVA model, a new general Large Language and Vision Assistant in Visual Insect Understanding. Then, to enhance the capability of learning insect features, we develop an Insect Foundation Model by introducing a new micro-feature self-supervised learning with a Patch-wise Relevant Attention mechanism to capture the subtle differences among insect images. We also present Description Consistency loss to improve micro-feature learning via text descriptions. The experimental results evaluated on our new Visual Insect Question Answering benchmarks illustrate the effective performance of our proposed approach in visual insect understanding and achieve State-of-the-Art performance on standard benchmarks of insect-related tasks.

### Learning to Calibrate for Reliable Visual Fire Detection 
[[arxiv](https://arxiv.org/abs/2502.09872)] [[cool](https://papers.cool/arxiv/2502.09872)] [[pdf](https://arxiv.org/pdf/2502.09872)]
> **Authors**: Ziqi Zhang,Xiuzhuang Zhou,Xiangyang Gong
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Fire is characterized by its sudden onset and destructive power, making early fire detection crucial for ensuring human safety and protecting property. With the advancement of deep learning, the application of computer vision in fire detection has significantly improved. However, deep learning models often exhibit a tendency toward overconfidence, and most existing works focus primarily on enhancing classification performance, with limited attention given to uncertainty modeling. To address this issue, we propose transforming the Expected Calibration Error (ECE), a metric for measuring uncertainty, into a differentiable ECE loss function. This loss is then combined with the cross-entropy loss to guide the training process of multi-class fire detection models. Additionally, to achieve a good balance between classification accuracy and reliable decision, we introduce a curriculum learning-based approach that dynamically adjusts the weight of the ECE loss during training. Extensive experiments are conducted on two widely used multi-class fire detection datasets, DFAN and EdgeFireSmoke, validating the effectiveness of our uncertainty modeling method.

### HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation 
[[arxiv](https://arxiv.org/abs/2502.09838)] [[cool](https://papers.cool/arxiv/2502.09838)] [[pdf](https://arxiv.org/pdf/2502.09838)]
> **Authors**: Tianwei Lin,Wenqiao Zhang,Sijing Li,Yuqian Yuan,Binhe Yu,Haoyuan Li,Wanggui He,Hao Jiang,Mengze Li,Xiaohui Song,Siliang Tang,Jun Xiao,Hui Lin,Yueting Zhuang,Beng Chin Ooi
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: Comments: added project page
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: We present HealthGPT, a powerful Medical Large Vision-Language Model (Med-LVLM) that integrates medical visual comprehension and generation capabilities within a unified autoregressive paradigm. Our bootstrapping philosophy is to progressively adapt heterogeneous comprehension and generation knowledge to pre-trained large language models (LLMs). This is achieved through a novel heterogeneous low-rank adaptation (H-LoRA) technique, which is complemented by a tailored hierarchical visual perception approach and a three-stage learning strategy. To effectively learn the HealthGPT, we devise a comprehensive medical domain-specific comprehension and generation dataset called VL-Health. Experimental results demonstrate exceptional performance and scalability of HealthGPT in medical visual unified tasks. Our project can be accessed at https://github.com/DCDmllm/HealthGPT.

### A Solver-Aided Hierarchical Language for LLM-Driven CAD Design 
[[arxiv](https://arxiv.org/abs/2502.09819)] [[cool](https://papers.cool/arxiv/2502.09819)] [[pdf](https://arxiv.org/pdf/2502.09819)]
> **Authors**: Benjamin T. Jones,Felix Hähnlein,Zihan Zhang,Maaz Ahmad,Vladimir Kim,Adriana Schulz
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,图形,机器学习,编程语言
- **Abstract**: Large language models (LLMs) have been enormously successful in solving a wide variety of structured and unstructured generative tasks, but they struggle to generate procedural geometry in Computer Aided Design (CAD). These difficulties arise from an inability to do spatial reasoning and the necessity to guide a model through complex, long range planning to generate complex geometry. We enable generative CAD Design with LLMs through the introduction of a solver-aided, hierarchical domain specific language (DSL) called AIDL, which offloads the spatial reasoning requirements to a geometric constraint solver. Additionally, we show that in the few-shot regime, AIDL outperforms even a language with in-training data (OpenSCAD), both in terms of generating visual results closer to the prompt and creating objects that are easier to post-process and reason about.

### On the robustness of multimodal language model towards distractions 
[[arxiv](https://arxiv.org/abs/2502.09818)] [[cool](https://papers.cool/arxiv/2502.09818)] [[pdf](https://arxiv.org/pdf/2502.09818)]
> **Authors**: Ming Liu,Hao Chen,Jindong Wang,Wensheng Zhang
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Although vision-language models (VLMs) have achieved significant success in various applications such as visual question answering, their resilience to prompt variations remains an under-explored area. Understanding how distractions affect VLMs is crucial for improving their real-world applicability, as inputs could have noisy and irrelevant information in many practical scenarios. This paper aims to assess the robustness of VLMs against both visual and textual distractions in the context of science question answering. Built on the ScienceQA dataset, we developed a new benchmark that introduces distractions in both the visual and textual contexts to evaluate the reasoning capacity of VLMs amid these distractions. Our findings reveal that most-of-the-art VLMs, including GPT-4, are vulnerable to various types of distractions, experiencing noticeable degradation in reasoning capabilities when confronted with distractions. Notably, models such as InternVL2 demonstrate a higher degree of robustness to these distractions. We also found that models exhibit greater sensitivity to textual distractions than visual ones. Additionally, we explored various mitigation strategies, such as prompt engineering, to counteract the impact of distractions. While these strategies improved solution accuracy, our analysis shows that there remain significant opportunities for improvement.

### Face Deepfakes -- A Comprehensive Review 
[[arxiv](https://arxiv.org/abs/2502.09812)] [[cool](https://papers.cool/arxiv/2502.09812)] [[pdf](https://arxiv.org/pdf/2502.09812)]
> **Authors**: Tharindu Fernando,Darshana Priyasad,Sridha Sridharan,Arun Ross,Clinton Fookes
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: In recent years, remarkable advancements in deep-fake generation technology have led to unprecedented leaps in its realism and capabilities. Despite these advances, we observe a notable lack of structured and deep analysis deepfake technology. The principal aim of this survey is to contribute a thorough theoretical analysis of state-of-the-art face deepfake generation and detection methods. Furthermore, we provide a coherent and systematic evaluation of the implications of deepfakes on face biometric recognition approaches. In addition, we outline key applications of face deepfake technology, elucidating both positive and negative applications of the technology, provide a detailed discussion regarding the gaps in existing research, and propose key research directions for further investigation.

### Vision-based Geo-Localization of Future Mars Rotorcraft in Challenging Illumination Conditions 
[[arxiv](https://arxiv.org/abs/2502.09795)] [[cool](https://papers.cool/arxiv/2502.09795)] [[pdf](https://arxiv.org/pdf/2502.09795)]
> **Authors**: Dario Pisanti,Robert Hewitt,Roland Brockers,Georgios Georgakis
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器人技术
- **Abstract**: Planetary exploration using aerial assets has the potential for unprecedented scientific discoveries on Mars. While NASA's Mars helicopter Ingenuity proved flight in Martian atmosphere is possible, future Mars rotocrafts will require advanced navigation capabilities for long-range flights. One such critical capability is Map-based Localization (MbL) which registers an onboard image to a reference map during flight in order to mitigate cumulative drift from visual odometry. However, significant illumination differences between rotocraft observations and a reference map prove challenging for traditional MbL systems, restricting the operational window of the vehicle. In this work, we investigate a new MbL system and propose Geo-LoFTR, a geometry-aided deep learning model for image registration that is more robust under large illumination differences than prior models. The system is supported by a custom simulation framework that uses real orbital maps to produce large amounts of realistic images of the Martian terrain. Comprehensive evaluations show that our proposed system outperforms prior MbL efforts in terms of localization accuracy under significant lighting and scale variations. Furthermore, we demonstrate the validity of our approach across a simulated Martian day.

### A CNN Approach to Automated Detection and Classification of Brain Tumors 
[[arxiv](https://arxiv.org/abs/2502.09731)] [[cool](https://papers.cool/arxiv/2502.09731)] [[pdf](https://arxiv.org/pdf/2502.09731)]
> **Authors**: Md. Zahid Hasan,Abdullah Tamim,D. M. Asadujjaman,Md. Mahfujur Rahman,Md. Abu Ahnaf Mollick,Nosin Anjum Dristi,Abdullah-Al-Noman
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: :68T07ACM Class:J.3
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Brain tumors require an assessment to ensure timely diagnosis and effective patient treatment. Morphological factors such as size, location, texture, and variable appearance complicate tumor inspection. Medical imaging presents challenges, including noise and incomplete images. This research article presents a methodology for processing Magnetic Resonance Imaging (MRI) data, encompassing techniques for image classification and denoising. The effective use of MRI images allows medical professionals to detect brain disorders, including tumors. This research aims to categorize healthy brain tissue and brain tumors by analyzing the provided MRI data. Unlike alternative methods like Computed Tomography (CT), MRI technology offers a more detailed representation of internal anatomical components, making it a suitable option for studying data related to brain tumors. The MRI picture is first subjected to a denoising technique utilizing an Anisotropic diffusion filter. The dataset utilized for the models creation is a publicly accessible and validated Brain Tumour Classification (MRI) database, comprising 3,264 brain MRI scans. SMOTE was employed for data augmentation and dataset balancing. Convolutional Neural Networks(CNN) such as ResNet152V2, VGG, ViT, and EfficientNet were employed for the classification procedure. EfficientNet attained an accuracy of 98%, the highest recorded.

### Towards Virtual Clinical Trials of Radiology AI with Conditional Generative Modeling 
[[arxiv](https://arxiv.org/abs/2502.09688)] [[cool](https://papers.cool/arxiv/2502.09688)] [[pdf](https://arxiv.org/pdf/2502.09688)]
> **Authors**: Benjamin D. Killeen,Bohua Wan,Aditya V. Kulkarni,Nathan Drenkow,Michael Oberst,Paul H. Yi,Mathias Unberath
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 35 pages
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: Artificial intelligence (AI) is poised to transform healthcare by enabling personalized and efficient care through data-driven insights. Although radiology is at the forefront of AI adoption, in practice, the potential of AI models is often overshadowed by severe failures to generalize: AI models can have performance degradation of up to 20% when transitioning from controlled test environments to clinical use by radiologists. This mismatch raises concerns that radiologists will be misled by incorrect AI predictions in practice and/or grow to distrust AI, rendering these promising technologies practically ineffectual. Exhaustive clinical trials of AI models on abundant and diverse data is thus critical to anticipate AI model degradation when encountering varied data samples. Achieving these goals, however, is challenging due to the high costs of collecting diverse data samples and corresponding annotations. To overcome these limitations, we introduce a novel conditional generative AI model designed for virtual clinical trials (VCTs) of radiology AI, capable of realistically synthesizing full-body CT images of patients with specified attributes. By learning the joint distribution of images and anatomical structures, our model enables precise replication of real-world patient populations with unprecedented detail at this scale. We demonstrate meaningful evaluation of radiology AI models through VCTs powered by our synthetic CT study populations, revealing model degradation and facilitating algorithmic auditing for bias-inducing data attributes. Our generative AI approach to VCTs is a promising avenue towards a scalable solution to assess model robustness, mitigate biases, and safeguard patient care by enabling simpler testing and evaluation of AI models in any desired range of diverse patient populations.

### Object-Centric Latent Action Learning 
[[arxiv](https://arxiv.org/abs/2502.09680)] [[cool](https://papers.cool/arxiv/2502.09680)] [[pdf](https://arxiv.org/pdf/2502.09680)]
> **Authors**: Albina Klepach,Alexander Nikulin,Ilya Zisman,Denis Tarasov,Alexander Derevyagin,Andrei Polubarov,Nikita Lyubaykin,Vladislav Kurenkov
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: Preprint. In review
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Leveraging vast amounts of internet video data for Embodied AI is currently bottle-necked by the lack of action annotations and the presence of action-correlated distractors. We propose a novel object-centric latent action learning approach, based on VideoSaur and LAPO, that employs self-supervised decomposition of scenes into object representations and annotates video data with proxy-action labels. This method effectively disentangles causal agent-object interactions from irrelevant background noise and reduces the performance degradation of latent action learning approaches caused by distractors. Our preliminary experiments with the Distracting Control Suite show that latent action pretraining based on object decompositions improve the quality of inferred latent actions by x2.7 and efficiency of downstream fine-tuning with a small set of labeled actions, increasing return by x2.6 on average.

### Meta-INR: Efficient Encoding of Volumetric Data via Meta-Learning Implicit Neural Representation 
[[arxiv](https://arxiv.org/abs/2502.09669)] [[cool](https://papers.cool/arxiv/2502.09669)] [[pdf](https://arxiv.org/pdf/2502.09669)]
> **Authors**: Maizhe Yang,Kaiyuan Tang,Chaoli Wang
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-14
> **comment**: Accepted by PVIS Short Paper Track
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,图形
- **Abstract**: Implicit neural representation (INR) has emerged as a promising solution for encoding volumetric data, offering continuous representations and seamless compatibility with the volume rendering pipeline. However, optimizing an INR network from randomly initialized parameters for each new volume is computationally inefficient, especially for large-scale time-varying or ensemble volumetric datasets where volumes share similar structural patterns but require independent training. To close this gap, we propose Meta-INR, a pretraining strategy adapted from meta-learning algorithms to learn initial INR parameters from partial observation of a volumetric dataset. Compared to training an INR from scratch, the learned initial parameters provide a strong prior that enhances INR generalizability, allowing significantly faster convergence with just a few gradient updates when adapting to a new volume and better interpretability when analyzing the parameters of the adapted INRs. We demonstrate that Meta-INR can effectively extract high-quality generalizable features that help encode unseen similar volume data across diverse datasets. Furthermore, we highlight its utility in tasks such as simulation parameter analysis and representative timestep selection. The code is available at https://github.com/spacefarers/MetaINR.

### Image Super-Resolution with Guarantees via Conformal Generative Models 
[[arxiv](https://arxiv.org/abs/2502.09664)] [[cool](https://papers.cool/arxiv/2502.09664)] [[pdf](https://arxiv.org/pdf/2502.09664)]
> **Authors**: Eduardo Adame,Daniel Csillag,Guilherme Tegoni Goedert
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-14
> **comment**: 11 pages, 7 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习,机器学习
- **Abstract**: The increasing use of generative ML foundation models for image super-resolution calls for robust and interpretable uncertainty quantification methods. We address this need by presenting a novel approach based on conformal prediction techniques to create a "confidence mask" capable of reliably and intuitively communicating where the generated image can be trusted. Our method is adaptable to any black-box generative model, including those locked behind an opaque API, requires only easily attainable data for calibration, and is highly customizable via the choice of a local image similarity metric. We prove strong theoretical guarantees for our method that span fidelity error control (according to our local image similarity metric), reconstruction quality, and robustness in the face of data leakage. Finally, we empirically evaluate these results and establish our method's solid performance.

### DiffEx: Explaining a Classifier with Diffusion Models to Identify Microscopic Cellular Variations 
[[arxiv](https://arxiv.org/abs/2502.09663)] [[cool](https://papers.cool/arxiv/2502.09663)] [[pdf](https://arxiv.org/pdf/2502.09663)]
> **Authors**: Anis Bourou,Saranga Kingkor Mahanta,Thomas Boyer,Valérie Mezger,Auguste Genovesio
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习,细胞行为
- **Abstract**: In recent years, deep learning models have been extensively applied to biological data across various modalities. Discriminative deep learning models have excelled at classifying images into categories (e.g., healthy versus diseased, treated versus untreated). However, these models are often perceived as black boxes due to their complexity and lack of interpretability, limiting their application in real-world biological contexts. In biological research, explainability is essential: understanding classifier decisions and identifying subtle differences between conditions are critical for elucidating the effects of treatments, disease progression, and biological processes. To address this challenge, we propose DiffEx, a method for generating visually interpretable attributes to explain classifiers and identify microscopic cellular variations between different conditions. We demonstrate the effectiveness of DiffEx in explaining classifiers trained on natural and biological images. Furthermore, we use DiffEx to uncover phenotypic differences within microscopy datasets. By offering insights into cellular variations through classifier explanations, DiffEx has the potential to advance the understanding of diseases and aid drug discovery by identifying novel biomarkers.

### Integrating Spatiotemporal Vision Transformer into Digital Twins for High-Resolution Heat Stress Forecasting in Campus Environments 
[[arxiv](https://arxiv.org/abs/2502.09657)] [[cool](https://papers.cool/arxiv/2502.09657)] [[pdf](https://arxiv.org/pdf/2502.09657)]
> **Authors**: Wenjing Gong,Xinyue Ye,Keshu Wu,Suphanut Jamonnak,Wenyu Zhang,Yifan Yang,Xiao Huang
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Extreme heat events exacerbated by climate change pose significant challenges to urban resilience and planning. This study introduces a climate-responsive digital twin framework integrating the Spatiotemporal Vision Transformer (ST-ViT) model to enhance heat stress forecasting and decision-making. Using a Texas campus as a testbed, we synthesized high-resolution physical model simulations with spatial and meteorological data to develop fine-scale human thermal predictions. The ST-ViT-powered digital twin enables efficient, data-driven insights for planners, policymakers, and campus stakeholders, supporting targeted heat mitigation strategies and advancing climate-adaptive urban design.

### Bidirectional Diffusion Bridge Models 
[[arxiv](https://arxiv.org/abs/2502.09655)] [[cool](https://papers.cool/arxiv/2502.09655)] [[pdf](https://arxiv.org/pdf/2502.09655)]
> **Authors**: Duc Kieu,Kien Do,Toan Nguyen,Dang Nguyen,Thin Nguyen
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-14
> **comment**: Source code: https://github.com/kvmduc/BDBM
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Diffusion bridges have shown potential in paired image-to-image (I2I) translation tasks. However, existing methods are limited by their unidirectional nature, requiring separate models for forward and reverse translations. This not only doubles the computational cost but also restricts their practicality. In this work, we introduce the Bidirectional Diffusion Bridge Model (BDBM), a scalable approach that facilitates bidirectional translation between two coupled distributions using a single network. BDBM leverages the Chapman-Kolmogorov Equation for bridges, enabling it to model data distribution shifts across timesteps in both forward and backward directions by exploiting the interchangeability of the initial and target timesteps within this framework. Notably, when the marginal distribution given endpoints is Gaussian, BDBM's transition kernels in both directions possess analytical forms, allowing for efficient learning with a single network. We demonstrate the connection between BDBM and existing bridge methods, such as Doob's h-transform and variational approaches, and highlight its advantages. Extensive experiments on high-resolution I2I translation tasks demonstrate that BDBM not only enables bidirectional translation with minimal additional cost but also outperforms state-of-the-art bridge models. Our source code is available at [https://github.com/kvmduc/BDBM||https://github.com/kvmduc/BDBM].

### GraphCompNet: A Position-Aware Model for Predicting and Compensating Shape Deviations in 3D Printing 
[[arxiv](https://arxiv.org/abs/2502.09652)] [[cool](https://papers.cool/arxiv/2502.09652)] [[pdf](https://arxiv.org/pdf/2502.09652)]
> **Authors**: Lei,Chen,Juheon Lee,Juan Carlos Catana,Tsegai Yhdego,Nathan Moroney,Mohammad Amin Nabian,Hui Wang,Jun Zeng
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-14
> **comment**: Errors in the Paper: significant mathematical errors that were not noticed before submission, withdraw the paper for corrections
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: This paper introduces a data-driven algorithm for modeling and compensating shape deviations in additive manufacturing (AM), addressing challenges in geometric accuracy and batch production. While traditional methods, such as analytical models and metrology, laid the groundwork for geometric precision, they are often impractical for large-scale production. Recent advancements in machine learning (ML) have improved compensation precision, but issues remain in generalizing across complex geometries and adapting to position-dependent variations. We present a novel approach for powder bed fusion (PBF) processes, using GraphCompNet, which is a computational framework combining graph-based neural networks with a generative adversarial network (GAN)-inspired training process. By leveraging point cloud data and dynamic graph convolutional neural networks (DGCNNs), GraphCompNet models complex shapes and incorporates position-specific thermal and mechanical factors. A two-stage adversarial training procedure iteratively refines compensated designs via a compensator-predictor architecture, offering real-time feedback and optimization. Experimental validation across diverse shapes and positions shows the framework significantly improves compensation accuracy (35 to 65 percent) across the entire print space, adapting to position-dependent variations. This work advances the development of Digital Twin technology for AM, enabling scalable, real-time monitoring and compensation, and addressing critical gaps in AM process control. The proposed method supports high-precision, automated industrial-scale design and manufacturing systems.

### Embed Any NeRF: Graph Meta-Networks for Neural Tasks on Arbitrary NeRF Architectures 
[[arxiv](https://arxiv.org/abs/2502.09623)] [[cool](https://papers.cool/arxiv/2502.09623)] [[pdf](https://arxiv.org/pdf/2502.09623)]
> **Authors**: Francesco Ballerini,Pierluigi Zama Ramirez,Samuele Salti,Luigi Di Stefano
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: Under review
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Neural Radiance Fields (NeRFs) have emerged as a groundbreaking paradigm for representing 3D objects and scenes by encoding shape and appearance information into the weights of a neural network. Recent works have shown how such weights can be used as input to frameworks processing them to solve deep learning tasks. Yet, these frameworks can only process NeRFs with a specific, predefined architecture. In this paper, we present the first framework that can ingest NeRFs with multiple architectures and perform inference on architectures unseen at training time. We achieve this goal by training a Graph Meta-Network in a representation learning framework. Moreover, we show how a contrastive objective is conducive to obtaining an architecture-agnostic latent space. In experiments on both MLP-based and tri-planar NeRFs, our approach demonstrates robust performance in classification and retrieval tasks that either matches or exceeds that of existing frameworks constrained to single architectures, thus providing the first architecture-agnostic method to perform tasks on NeRFs by processing their weights.

### MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency 
[[arxiv](https://arxiv.org/abs/2502.09621)] [[cool](https://papers.cool/arxiv/2502.09621)] [[pdf](https://arxiv.org/pdf/2502.09621)]
> **Authors**: Dongzhi Jiang,Renrui Zhang,Ziyu Guo,Yanwei Li,Yu Qi,Xinyan Chen,Liuhui Wang,Jianhan Jin,Claire Guo,Shen Yan,Bo Zhang,Chaoyou Fu,Peng Gao,Hongsheng Li
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: Project Page: https://mmecot.github.io/
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **Abstract**: Answering questions with Chain-of-Thought (CoT) has significantly enhanced the reasoning capabilities of Large Language Models (LLMs), yet its impact on Large Multimodal Models (LMMs) still lacks a systematic assessment and in-depth investigation. In this paper, we introduce MME-CoT, a specialized benchmark evaluating the CoT reasoning performance of LMMs, spanning six domains: math, science, OCR, logic, space-time, and general scenes. As the first comprehensive study in this area, we propose a thorough evaluation suite incorporating three novel metrics that assess the reasoning quality, robustness, and efficiency at a fine-grained level. Leveraging curated high-quality data and a unique evaluation strategy, we conduct an in-depth analysis of state-of-the-art LMMs, uncovering several key insights: 1) Models with reflection mechanism demonstrate a superior CoT quality, with Kimi k1.5 outperforming GPT-4o and demonstrating the highest quality results; 2) CoT prompting often degrades LMM performance on perception-heavy tasks, suggesting a potentially harmful overthinking behavior; and 3) Although the CoT quality is high, LMMs with reflection exhibit significant inefficiency in both normal response and self-correction phases. We hope MME-CoT serves as a foundation for advancing multimodal reasoning in LMMs. Project Page: https://mmecot.github.io/

### Exploring the Potential of Encoder-free Architectures in 3D LMMs 
[[arxiv](https://arxiv.org/abs/2502.09620)] [[cool](https://papers.cool/arxiv/2502.09620)] [[pdf](https://arxiv.org/pdf/2502.09620)]
> **Authors**: Yiwen Tang,Zoey Guo,Zhuhao Wang,Ray Zhang,Qizhi Chen,Junli Liu,Delin Qu,Zhigang Wang,Dong Wang,Xuelong Li,Bin Zhao
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: The code is released at https://github.com/Ivan-Tang-3D/ENEL
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **Abstract**: Encoder-free architectures have been preliminarily explored in the 2D visual domain, yet it remains an open question whether they can be effectively applied to 3D understanding scenarios. In this paper, we present the first comprehensive investigation into the potential of encoder-free architectures to overcome the challenges of encoder-based 3D Large Multimodal Models (LMMs). These challenges include the failure to adapt to varying point cloud resolutions and the point features from the encoder not meeting the semantic needs of Large Language Models (LLMs). We identify key aspects for 3D LMMs to remove the encoder and enable the LLM to assume the role of the 3D encoder: 1) We propose the LLM-embedded Semantic Encoding strategy in the pre-training stage, exploring the effects of various point cloud self-supervised losses. And we present the Hybrid Semantic Loss to extract high-level semantics. 2) We introduce the Hierarchical Geometry Aggregation strategy in the instruction tuning stage. This incorporates inductive bias into the LLM early layers to focus on the local details of the point clouds. To the end, we present the first Encoder-free 3D LMM, ENEL. Our 7B model rivals the current state-of-the-art model, ShapeLLM-13B, achieving 55.0%, 50.92%, and 42.7% on the classification, captioning, and VQA tasks, respectively. Our results demonstrate that the encoder-free architecture is highly promising for replacing encoder-based architectures in the field of 3D understanding. The code is released at https://github.com/Ivan-Tang-3D/ENEL

### RigAnything: Template-Free Autoregressive Rigging for Diverse 3D Assets 
[[arxiv](https://arxiv.org/abs/2502.09615)] [[cool](https://papers.cool/arxiv/2502.09615)] [[pdf](https://arxiv.org/pdf/2502.09615)]
> **Authors**: Isabella Liu,Zhan Xu,Wang Yifan,Hao Tan,Zexiang Xu,Xiaolong Wang,Hao Su,Zifan Shi
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: Project page: https://www.liuisabella.com/RigAnything
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: We present RigAnything, a novel autoregressive transformer-based model, which makes 3D assets rig-ready by probabilistically generating joints, skeleton topologies, and assigning skinning weights in a template-free manner. Unlike most existing auto-rigging methods, which rely on predefined skeleton template and are limited to specific categories like humanoid, RigAnything approaches the rigging problem in an autoregressive manner, iteratively predicting the next joint based on the global input shape and the previous prediction. While autoregressive models are typically used to generate sequential data, RigAnything extends their application to effectively learn and represent skeletons, which are inherently tree structures. To achieve this, we organize the joints in a breadth-first search (BFS) order, enabling the skeleton to be defined as a sequence of 3D locations and the parent index. Furthermore, our model improves the accuracy of position prediction by leveraging diffusion modeling, ensuring precise and consistent placement of joints within the hierarchy. This formulation allows the autoregressive model to efficiently capture both spatial and hierarchical relationships within the skeleton. Trained end-to-end on both RigNet and Objaverse datasets, RigAnything demonstrates state-of-the-art performance across diverse object types, including humanoids, quadrupeds, marine creatures, insects, and many more, surpassing prior methods in quality, robustness, generalizability, and efficiency. Please check our website for more details: https://www.liuisabella.com/RigAnything.

### GAIA: A Global, Multi-modal, Multi-scale Vision-Language Dataset for Remote Sensing Image Analysis 
[[arxiv](https://arxiv.org/abs/2502.09598)] [[cool](https://papers.cool/arxiv/2502.09598)] [[pdf](https://arxiv.org/pdf/2502.09598)]
> **Authors**: Angelos Zavras,Dimitrios Michail,Xiao Xiang Zhu,Begüm Demir,Ioannis Papoutsis
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 22 pages, 13 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: The continuous operation of Earth-orbiting satellites generates vast and ever-growing archives of Remote Sensing (RS) images. Natural language presents an intuitive interface for accessing, querying, and interpreting the data from such archives. However, existing Vision-Language Models (VLMs) are predominantly trained on web-scraped, noisy image-text data, exhibiting limited exposure to the specialized domain of RS. This deficiency results in poor performance on RS-specific tasks, as commonly used datasets often lack detailed, scientifically accurate textual descriptions and instead emphasize solely on attributes like date and location. To bridge this critical gap, we introduce GAIA, a novel dataset designed for multi-scale, multi-sensor, and multi-modal RS image analysis. GAIA comprises of 205,150 meticulously curated RS image-text pairs, representing a diverse range of RS modalities associated to different spatial resolutions. Unlike existing vision-language datasets in RS, GAIA specifically focuses on capturing a diverse range of RS applications, providing unique information about environmental changes, natural disasters, and various other dynamic phenomena. The dataset provides a spatially and temporally balanced distribution, spanning across the globe, covering the last 25 years with a balanced temporal distribution of observations. GAIA's construction involved a two-stage process: (1) targeted web-scraping of images and accompanying text from reputable RS-related sources, and (2) generation of five high-quality, scientifically grounded synthetic captions for each image using carefully crafted prompts that leverage the advanced vision-language capabilities of GPT-4o. Our extensive experiments, including fine-tuning of CLIP and BLIP2 models, demonstrate that GAIA significantly improves performance on RS image classification, cross-modal retrieval and image captioning tasks.

### Optimizing GPT for Video Understanding: Zero-Shot Performance and Prompt Engineering 
[[arxiv](https://arxiv.org/abs/2502.09573)] [[cool](https://papers.cool/arxiv/2502.09573)] [[pdf](https://arxiv.org/pdf/2502.09573)]
> **Authors**: Mark Beliaev,Victor Yang,Madhura Raju,Jiachen Sun,Xinghai Hu
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算语言学,机器学习
- **Abstract**: In this study, we tackle industry challenges in video content classification by exploring and optimizing GPT-based models for zero-shot classification across seven critical categories of video quality. We contribute a novel approach to improving GPT's performance through prompt optimization and policy refinement, demonstrating that simplifying complex policies significantly reduces false negatives. Additionally, we introduce a new decomposition-aggregation-based prompt engineering technique, which outperforms traditional single-prompt methods. These experiments, conducted on real industry problems, show that thoughtful prompt design can substantially enhance GPT's performance without additional finetuning, offering an effective and scalable solution for improving video classification systems across various domains in industry.

### Long-Term TalkingFace Generation via Motion-Prior Conditional Diffusion Model 
[[arxiv](https://arxiv.org/abs/2502.09533)] [[cool](https://papers.cool/arxiv/2502.09533)] [[pdf](https://arxiv.org/pdf/2502.09533)]
> **Authors**: Fei Shen,Cong Wang,Junyao Gao,Qin Guo,Jisheng Dang,Jinhui Tang,Tat-Seng Chua
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Recent advances in conditional diffusion models have shown promise for generating realistic TalkingFace videos, yet challenges persist in achieving consistent head movement, synchronized facial expressions, and accurate lip synchronization over extended generations. To address these, we introduce the \textbf{M}otion-priors \textbf{C}onditional \textbf{D}iffusion \textbf{M}odel (\textbf{MCDM}), which utilizes both archived and current clip motion priors to enhance motion prediction and ensure temporal consistency. The model consists of three key elements: (1) an archived-clip motion-prior that incorporates historical frames and a reference frame to preserve identity and context; (2) a present-clip motion-prior diffusion model that captures multimodal causality for accurate predictions of head movements, lip sync, and expressions; and (3) a memory-efficient temporal attention mechanism that mitigates error accumulation by dynamically storing and updating motion features. We also release the \textbf{TalkingFace-Wild} dataset, a multilingual collection of over 200 hours of footage across 10 languages. Experimental results demonstrate the effectiveness of MCDM in maintaining identity and motion continuity for long-term TalkingFace generation. Code, models, and datasets will be publicly available.

### SteROI-D: System Design and Mapping for Stereo Depth Inference on Regions of Interest 
[[arxiv](https://arxiv.org/abs/2502.09528)] [[cool](https://papers.cool/arxiv/2502.09528)] [[pdf](https://arxiv.org/pdf/2502.09528)]
> **Authors**: Jack Erhardt,Ziang Li,Reid Pinkham,Andrew Berkovich,Zhengya Zhang
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: Accepted as a full paper by the 2025 EDGEAIFOUNDATION Austin
- **标题**: None
- **领域**: 计算机视觉和模式识别,硬件架构
- **Abstract**: Machine learning algorithms have enabled high quality stereo depth estimation to run on Augmented and Virtual Reality (AR/VR) devices. However, high energy consumption across the full image processing stack prevents stereo depth algorithms from running effectively on battery-limited devices. This paper introduces SteROI-D, a full stereo depth system paired with a mapping methodology. SteROI-D exploits Region-of-Interest (ROI) and temporal sparsity at the system level to save energy. SteROI-D's flexible and heterogeneous compute fabric supports diverse ROIs. Importantly, we introduce a systematic mapping methodology to effectively handle dynamic ROIs, thereby maximizing energy savings. Using these techniques, our 28nm prototype SteROI-D design achieves up to 4.35x reduction in total system energy compared to a baseline ASIC.

### Pixel-Level Reasoning Segmentation via Multi-turn Conversations 
[[arxiv](https://arxiv.org/abs/2502.09447)] [[cool](https://papers.cool/arxiv/2502.09447)] [[pdf](https://arxiv.org/pdf/2502.09447)]
> **Authors**: Dexian Cai,Xiaocui Yang,Yongkang Liu,Daling Wang,Shi Feng,Yifei Zhang,Soujanya Poria
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算语言学
- **Abstract**: Existing visual perception systems focus on region-level segmentation in single-turn dialogues, relying on complex and explicit query instructions. Such systems cannot reason at the pixel level and comprehend dynamic user intent that changes over interaction. Our work tackles this issue by introducing a novel task, Pixel-level Reasoning Segmentation (Pixel-level RS) based on multi-turn conversations, tracking evolving user intent via multi-turn interactions for fine-grained segmentation. To establish a benchmark for this novel task, we build a Pixel-level ReasonIng Segmentation Dataset Based on Multi-Turn Conversations (PRIST), comprising 24k utterances from 8.3k multi-turn conversational scenarios with segmentation targets. Building on PRIST, we further propose MIRAS, a Multi-turn Interactive ReAsoning Segmentation framework, integrates pixel-level segmentation with robust multi-turn conversation understanding, generating pixel-grounded explanations aligned with user intent. The PRIST dataset and MIRSA framework fill the gap in pixel-level reasoning segmentation. Experimental results on the PRIST dataset demonstrate that our method outperforms current segmentation-specific baselines in terms of segmentation and LLM-based reasoning metrics. The code and data are available at: https://github.com/ccccai239/PixelRIST.

### A 3D Facial Reconstruction Evaluation Methodology: Comparing Smartphone Scans with Deep Learning Based Methods Using Geometry and Morphometry Criteria 
[[arxiv](https://arxiv.org/abs/2502.09425)] [[cool](https://papers.cool/arxiv/2502.09425)] [[pdf](https://arxiv.org/pdf/2502.09425)]
> **Authors**: Álvaro Heredia-Lidón,Alejandro Moñux-Bernal,Alejandro González,Luis M. Echeverry-Quiceno,Max Rubert,Aroa Casado,María Esther Esteban,Mireia Andreu-Montoriol,Susanna Gallardo,Cristina Ruffo,Neus Martínez-Abadías,Xavier Sevillano
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Three-dimensional (3D) facial shape analysis has gained interest due to its potential clinical applications. However, the high cost of advanced 3D facial acquisition systems limits their widespread use, driving the development of low-cost acquisition and reconstruction methods. This study introduces a novel evaluation methodology that goes beyond traditional geometry-based benchmarks by integrating morphometric shape analysis techniques, providing a statistical framework for assessing facial morphology preservation. As a case study, we compare smartphone-based 3D scans with state-of-the-art deep learning reconstruction methods from 2D images, using high-end stereophotogrammetry models as ground truth. This methodology enables a quantitative assessment of global and local shape differences, offering a biologically meaningful validation approach for low-cost 3D facial acquisition and reconstruction techniques.

### Galileo: Learning Global and Local Features in Pretrained Remote Sensing Models 
[[arxiv](https://arxiv.org/abs/2502.09356)] [[cool](https://papers.cool/arxiv/2502.09356)] [[pdf](https://arxiv.org/pdf/2502.09356)]
> **Authors**: Gabriel Tseng,Anthony Fuller,Marlena Reil,Henry Herzog,Patrick Beukema,Favyen Bastani,James R. Green,Evan Shelhamer,Hannah Kerner,David Rolnick
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: From crop mapping to flood detection, machine learning in remote sensing has a wide range of societally beneficial applications. The commonalities between remote sensing data in these applications present an opportunity for pretrained machine learning models tailored to remote sensing to reduce the labeled data and effort required to solve individual tasks. However, such models must be: (i) flexible enough to ingest input data of varying sensor modalities and shapes (i.e., of varying spatial and temporal dimensions), and (ii) able to model Earth surface phenomena of varying scales and types. To solve this gap, we present Galileo, a family of pretrained remote sensing models designed to flexibly process multimodal remote sensing data. We also introduce a novel and highly effective self-supervised learning approach to learn both large- and small-scale features, a challenge not addressed by previous models. Our Galileo models obtain state-of-the-art results across diverse remote sensing tasks.

### A Benchmark for Crime Surveillance Video Analysis with Large Models 
[[arxiv](https://arxiv.org/abs/2502.09325)] [[cool](https://papers.cool/arxiv/2502.09325)] [[pdf](https://arxiv.org/pdf/2502.09325)]
> **Authors**: Haoran Chen,Dong Yi,Moyan Cao,Chensen Huang,Guibo Zhu,Jinqiao Wang
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Anomaly analysis in surveillance videos is a crucial topic in computer vision. In recent years, multimodal large language models (MLLMs) have outperformed task-specific models in various domains. Although MLLMs are particularly versatile, their abilities to understand anomalous concepts and details are insufficiently studied because of the outdated benchmarks of this field not providing MLLM-style QAs and efficient algorithms to assess the model's open-ended text responses. To fill this gap, we propose a benchmark for crime surveillance video analysis with large models denoted as UCVL, including 1,829 videos and reorganized annotations from the UCF-Crime and UCF-Crime Annotation datasets. We design six types of questions and generate diverse QA pairs. Then we develop detailed instructions and use OpenAI's GPT-4o for accurate assessment. We benchmark eight prevailing MLLMs ranging from 0.5B to 40B parameters, and the results demonstrate the reliability of this bench. Moreover, we finetune LLaVA-OneVision on UCVL's training set. The improvement validates our data's high quality for video anomaly analysis.

### A Physics-Informed Deep Learning Model for MRI Brain Motion Correction 
[[arxiv](https://arxiv.org/abs/2502.09296)] [[cool](https://papers.cool/arxiv/2502.09296)] [[pdf](https://arxiv.org/pdf/2502.09296)]
> **Authors**: Mojtaba Safari,Shansong Wang,Zach Eidex,Richard Qiu,Chih-Wei Chang,David S. Yu,Xiaofeng Yang
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,医学物理
- **Abstract**: Background: MRI is crucial for brain imaging but is highly susceptible to motion artifacts due to long acquisition times. This study introduces PI-MoCoNet, a physics-informed motion correction network that integrates spatial and k-space information to remove motion artifacts without explicit motion parameter estimation, enhancing image fidelity and diagnostic reliability. Materials and Methods: PI-MoCoNet consists of a motion detection network (U-net with spatial averaging) to identify corrupted k-space lines and a motion correction network (U-net with Swin Transformer blocks) to reconstruct motion-free images. The correction is guided by three loss functions: reconstruction (L1), perceptual (LPIPS), and data consistency (Ldc). Motion artifacts were simulated via rigid phase encoding perturbations and evaluated on IXI and MR-ART datasets against Pix2Pix, CycleGAN, and U-net using PSNR, SSIM, and NMSE. Results: PI-MoCoNet significantly improved image quality. On IXI, for minor artifacts, PSNR increased from 34.15 dB to 45.95 dB, SSIM from 0.87 to 1.00, and NMSE reduced from 0.55% to 0.04%. For moderate artifacts, PSNR improved from 30.23 dB to 42.16 dB, SSIM from 0.80 to 0.99, and NMSE from 1.32% to 0.09%. For heavy artifacts, PSNR rose from 27.99 dB to 36.01 dB, SSIM from 0.75 to 0.97, and NMSE decreased from 2.21% to 0.36%. On MR-ART, PI-MoCoNet achieved PSNR gains of ~10 dB and SSIM improvements of up to 0.20, with NMSE reductions of ~6%. Ablation studies confirmed the importance of data consistency and perceptual losses, yielding a 1 dB PSNR gain and 0.17% NMSE reduction. Conclusions: PI-MoCoNet effectively mitigates motion artifacts in brain MRI, outperforming existing methods. Its ability to integrate spatial and k-space information makes it a promising tool for clinical use in motion-prone settings. Code: https://github.com/mosaf/PI-MoCoNet.git.

### EmoAssist: Emotional Assistant for Visual Impairment Community 
[[arxiv](https://arxiv.org/abs/2502.09285)] [[cool](https://papers.cool/arxiv/2502.09285)] [[pdf](https://arxiv.org/pdf/2502.09285)]
> **Authors**: Xingyu Qi,He Li,Linjie Li,Zhenyu Wu
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算机与社会
- **Abstract**: The rapid advancement of large multi-modality models (LMMs) has significantly propelled the integration of artificial intelligence into practical applications. Visual Question Answering (VQA) systems, which can process multi-modal data including vision, text, and audio, hold great potential for assisting the Visual Impairment (VI) community in navigating complex and dynamic real-world environments. However, existing VI assistive LMMs overlook the emotional needs of VI individuals, and current benchmarks lack emotional evaluation of these LMMs. To address these gaps, this paper introduces the EmoAssist Benchmark, a comprehensive benchmark designed to evaluate the assistive performance of LMMs for the VI community. To the best of our knowledge, this is the first benchmark that incorporates emotional intelligence as a key consideration. Furthermore, we propose the EmoAssist Model, an Emotion-Assistive LMM specifically designed for the VI community. The EmoAssist Model utilizes Direct Preference Optimization (DPO) to align outputs with human emotional preferences. Experiment results demonstrate that the EmoAssist Model significantly enhances the recognition of implicit emotions and intentions of VI users, delivers empathetic responses, and provides actionable guidance. Specifically, it shows respective improvements of 147.8% and 89.7% in the Empathy and Suggestion metrics on the EmoAssist Benchmark, compared to the pre-tuning LMM, and even outperforms state-of-the-art LLMs such as GPT-4o.

### FE-LWS: Refined Image-Text Representations via Decoder Stacking and Fused Encodings for Remote Sensing Image Captioning 
[[arxiv](https://arxiv.org/abs/2502.09282)] [[cool](https://papers.cool/arxiv/2502.09282)] [[pdf](https://arxiv.org/pdf/2502.09282)]
> **Authors**: Swadhin Das,Raksha Sharma
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人机交互,机器学习
- **Abstract**: Remote sensing image captioning aims to generate descriptive text from remote sensing images, typically employing an encoder-decoder framework. In this setup, a convolutional neural network (CNN) extracts feature representations from the input image, which then guide the decoder in a sequence-to-sequence caption generation process. Although much research has focused on refining the decoder, the quality of image representations from the encoder remains crucial for accurate captioning. This paper introduces a novel approach that integrates features from two distinct CNN based encoders, capturing complementary information to enhance caption generation. Additionally, we propose a weighted averaging technique to combine the outputs of all GRUs in the stacked decoder. Furthermore, a comparison-based beam search strategy is incorporated to refine caption selection. The results demonstrate that our fusion-based approach, along with the enhanced stacked decoder, significantly outperforms both the transformer-based state-of-the-art model and other LSTM-based baselines.

### ConsistentDreamer: View-Consistent Meshes Through Balanced Multi-View Gaussian Optimization 
[[arxiv](https://arxiv.org/abs/2502.09278)] [[cool](https://papers.cool/arxiv/2502.09278)] [[pdf](https://arxiv.org/pdf/2502.09278)]
> **Authors**: Onat Şahin,Mohammad Altillawi,George Eskandar,Carlos Carbone,Ziyuan Liu
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: Manuscript accepted by Pattern Recognition Letters. Project Page: https://onatsahin.github.io/ConsistentDreamer/
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Recent advances in diffusion models have significantly improved 3D generation, enabling the use of assets generated from an image for embodied AI simulations. However, the one-to-many nature of the image-to-3D problem limits their use due to inconsistent content and quality across views. Previous models optimize a 3D model by sampling views from a view-conditioned diffusion prior, but diffusion models cannot guarantee view consistency. Instead, we present ConsistentDreamer, where we first generate a set of fixed multi-view prior images and sample random views between them with another diffusion model through a score distillation sampling (SDS) loss. Thereby, we limit the discrepancies between the views guided by the SDS loss and ensure a consistent rough shape. In each iteration, we also use our generated multi-view prior images for fine-detail reconstruction. To balance between the rough shape and the fine-detail optimizations, we introduce dynamic task-dependent weights based on homoscedastic uncertainty, updated automatically in each iteration. Additionally, we employ opacity, depth distortion, and normal alignment losses to refine the surface for mesh extraction. Our method ensures better view consistency and visual quality compared to the state-of-the-art.

### E-MD3C: Taming Masked Diffusion Transformers for Efficient Zero-Shot Object Customization 
[[arxiv](https://arxiv.org/abs/2502.09164)] [[cool](https://papers.cool/arxiv/2502.09164)] [[pdf](https://arxiv.org/pdf/2502.09164)]
> **Authors**: Trung X. Pham,Zhang Kang,Ji Woo Hong,Xuran Zheng,Chang D. Yoo
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 16 pages, 14 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: We propose E-MD3C ($\underline{E}$fficient $\underline{M}$asked $\underline{D}$iffusion Transformer with Disentangled $\underline{C}$onditions and $\underline{C}$ompact $\underline{C}$ollector), a highly efficient framework for zero-shot object image customization. Unlike prior works reliant on resource-intensive Unet architectures, our approach employs lightweight masked diffusion transformers operating on latent patches, offering significantly improved computational efficiency. The framework integrates three core components: (1) an efficient masked diffusion transformer for processing autoencoder latents, (2) a disentangled condition design that ensures compactness while preserving background alignment and fine details, and (3) a learnable Conditions Collector that consolidates multiple inputs into a compact representation for efficient denoising and learning. E-MD3C outperforms the existing approach on the VITON-HD dataset across metrics such as PSNR, FID, SSIM, and LPIPS, demonstrating clear advantages in parameters, memory efficiency, and inference speed. With only $\frac{1}{4}$ of the parameters, our Transformer-based 468M model delivers $2.5\times$ faster inference and uses $\frac{2}{3}$ of the GPU memory compared to an 1720M Unet-based latent diffusion model.

### Feature-based Graph Attention Networks Improve Online Continual Learning 
[[arxiv](https://arxiv.org/abs/2502.09143)] [[cool](https://papers.cool/arxiv/2502.09143)] [[pdf](https://arxiv.org/pdf/2502.09143)]
> **Authors**: Adjovi Sim,Zhengkui Wang,Aik Beng Ng,Shalini De Mello,Simon See,Wonmin Byeon
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 16 pages
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Online continual learning for image classification is crucial for models to adapt to new data while retaining knowledge of previously learned tasks. This capability is essential to address real-world challenges involving dynamic environments and evolving data distributions. Traditional approaches predominantly employ Convolutional Neural Networks, which are limited to processing images as grids and primarily capture local patterns rather than relational information. Although the emergence of transformer architectures has improved the ability to capture relationships, these models often require significantly larger resources. In this paper, we present a novel online continual learning framework based on Graph Attention Networks (GATs), which effectively capture contextual relationships and dynamically update the task-specific representation via learned attention weights. Our approach utilizes a pre-trained feature extractor to convert images into graphs using hierarchical feature maps, representing information at varying levels of granularity. These graphs are then processed by a GAT and incorporate an enhanced global pooling strategy to improve classification performance for continual learning. In addition, we propose the rehearsal memory duplication technique that improves the representation of the previous tasks while maintaining the memory budget. Comprehensive evaluations on benchmark datasets, including SVHN, CIFAR10, CIFAR100, and MiniImageNet, demonstrate the superiority of our method compared to the state-of-the-art methods.

### Automatic Pruning via Structured Lasso with Class-wise Information 
[[arxiv](https://arxiv.org/abs/2502.09125)] [[cool](https://papers.cool/arxiv/2502.09125)] [[pdf](https://arxiv.org/pdf/2502.09125)]
> **Authors**: Xiang Liu,Mingchen Li,Xia Li,Leigang Qu,Zifan Peng,Yijun Song,Zemin Liu,Linshan Jiang,Jialin Li
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 11 pages, 2 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Most pruning methods concentrate on unimportant filters of neural networks. However, they face the loss of statistical information due to a lack of consideration for class-wise data. In this paper, from the perspective of leveraging precise class-wise information for model pruning, we utilize structured lasso with guidance from Information Bottleneck theory. Our approach ensures that statistical information is retained during the pruning process. With these techniques, we introduce two innovative adaptive network pruning schemes: sparse graph-structured lasso pruning with Information Bottleneck (\textbf{sGLP-IB}) and sparse tree-guided lasso pruning with Information Bottleneck (\textbf{sTLP-IB}). The key aspect is pruning model filters using sGLP-IB and sTLP-IB to better capture class-wise relatedness. Compared to multiple state-of-the-art methods, our approaches demonstrate superior performance across three datasets and six model architectures in extensive experiments. For instance, using the VGG16 model on the CIFAR-10 dataset, we achieve a parameter reduction of 85%, a decrease in FLOPs by 61%, and maintain an accuracy of 94.10% (0.14% higher than the original model); we reduce the parameters by 55% with the accuracy at 76.12% using the ResNet architecture on ImageNet (only drops 0.03%). In summary, we successfully reduce model size and computational resource usage while maintaining accuracy. Our codes are at https://anonymous.4open.science/r/IJCAI-8104.

### Pulling Back the Curtain: Unsupervised Adversarial Detection via Contrastive Auxiliary Networks 
[[arxiv](https://arxiv.org/abs/2502.09110)] [[cool](https://papers.cool/arxiv/2502.09110)] [[pdf](https://arxiv.org/pdf/2502.09110)]
> **Authors**: Eylon Mizrahi,Raz Lapid,Moshe Sipper
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Deep learning models are widely employed in safety-critical applications yet remain susceptible to adversarial attacks -- imperceptible perturbations that can significantly degrade model performance. Conventional defense mechanisms predominantly focus on either enhancing model robustness or detecting adversarial inputs independently. In this work, we propose an Unsupervised adversarial detection via Contrastive Auxiliary Networks (U-CAN) to uncover adversarial behavior within auxiliary feature representations, without the need for adversarial examples. U-CAN is embedded within selected intermediate layers of the target model. These auxiliary networks, comprising projection layers and ArcFace-based linear layers, refine feature representations to more effectively distinguish between benign and adversarial inputs. Comprehensive experiments across multiple datasets (CIFAR-10, Mammals, and a subset of ImageNet) and architectures (ResNet-50, VGG-16, and ViT) demonstrate that our method surpasses existing unsupervised adversarial detection techniques, achieving superior F1 scores against four distinct attack methods. The proposed framework provides a scalable and effective solution for enhancing the security and reliability of deep learning systems.

### Unsupervised Anomaly Detection on Implicit Shape representations for Sarcopenia Detection 
[[arxiv](https://arxiv.org/abs/2502.09088)] [[cool](https://papers.cool/arxiv/2502.09088)] [[pdf](https://arxiv.org/pdf/2502.09088)]
> **Authors**: Louise Piecuch,Jeremie Huet,Antoine Frouin,Antoine Nordez,Anne-Sophie Boureau,Diana Mateus
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: ef:2025 IEEE International Symposium on Biomedical Imaging
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Sarcopenia is an age-related progressive loss of muscle mass and strength that significantly impacts daily life. A commonly studied criterion for characterizing the muscle mass has been the combination of 3D imaging and manual segmentations. In this paper, we instead study the muscles' shape. We rely on an implicit neural representation (INR) to model normal muscle shapes. We then introduce an unsupervised anomaly detection method to identify sarcopenic muscles based on the reconstruction error of the implicit model. Relying on a conditional INR with an auto-decoding strategy, we also learn a latent representation of the muscles that clearly separates normal from abnormal muscles in an unsupervised fashion. Experimental results on a dataset of 103 segmented volumes indicate that our double anomaly detection strategy effectively discriminates sarcopenic and non-sarcopenic muscles.

### BevSplat: Resolving Height Ambiguity via Feature-Based Gaussian Primitives for Weakly-Supervised Cross-View Localization 
[[arxiv](https://arxiv.org/abs/2502.09080)] [[cool](https://papers.cool/arxiv/2502.09080)] [[pdf](https://arxiv.org/pdf/2502.09080)]
> **Authors**: Qiwei Wang,Shaoxun Wu,Yujiao Shi
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: This paper addresses the problem of weakly supervised cross-view localization, where the goal is to estimate the pose of a ground camera relative to a satellite image with noisy ground truth annotations. A common approach to bridge the cross-view domain gap for pose estimation is Bird's-Eye View (BEV) synthesis. However, existing methods struggle with height ambiguity due to the lack of depth information in ground images and satellite height maps. Previous solutions either assume a flat ground plane or rely on complex models, such as cross-view transformers. We propose BevSplat, a novel method that resolves height ambiguity by using feature-based Gaussian primitives. Each pixel in the ground image is represented by a 3D Gaussian with semantic and spatial features, which are synthesized into a BEV feature map for relative pose estimation. Additionally, to address challenges with panoramic query images, we introduce an icosphere-based supervision strategy for the Gaussian primitives. We validate our method on the widely used KITTI and VIGOR datasets, which include both pinhole and panoramic query images. Experimental results show that BevSplat significantly improves localization accuracy over prior approaches.

### Vision-Language In-Context Learning Driven Few-Shot Visual Inspection Model 
[[arxiv](https://arxiv.org/abs/2502.09057)] [[cool](https://papers.cool/arxiv/2502.09057)] [[pdf](https://arxiv.org/pdf/2502.09057)]
> **Authors**: Shiryu Ueno,Yoshikazu Hayashi,Shunsuke Nakatsuka,Yusei Yamada,Hiroaki Aizawa,Kunihito Kato
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: VISAPP 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: We propose general visual inspection model using Vision-Language Model~(VLM) with few-shot images of non-defective or defective products, along with explanatory texts that serve as inspection criteria. Although existing VLM exhibit high performance across various tasks, they are not trained on specific tasks such as visual inspection. Thus, we construct a dataset consisting of diverse images of non-defective and defective products collected from the web, along with unified formatted output text, and fine-tune VLM. For new products, our method employs In-Context Learning, which allows the model to perform inspections with an example of non-defective or defective image and the corresponding explanatory texts with visual prompts. This approach eliminates the need to collect a large number of training samples and re-train the model for each product. The experimental results show that our method achieves high performance, with MCC of 0.804 and F1-score of 0.950 on MVTec AD in a one-shot manner. Our code is available at~https://github.com/ia-gu/Vision-Language-In-Context-Learning-Driven-Few-Shot-Visual-Inspection-Model.

### AIDE: Agentically Improve Visual Language Model with Domain Experts 
[[arxiv](https://arxiv.org/abs/2502.09051)] [[cool](https://papers.cool/arxiv/2502.09051)] [[pdf](https://arxiv.org/pdf/2502.09051)]
> **Authors**: Ming-Chang Chiu,Fuxiao Liu,Karan Sapra,Andrew Tao,Yaser Jacoob,Xuezhe Ma,Zhiding Yu,Guilin Liu
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 6 pages, 4 figures, 2 tables
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,多代理系统
- **Abstract**: The enhancement of Visual Language Models (VLMs) has traditionally relied on knowledge distillation from larger, more capable models. This dependence creates a fundamental bottleneck for improving state-of-the-art systems, particularly when no superior models exist. We introduce AIDE (Agentic Improvement through Domain Experts), a novel framework that enables VLMs to autonomously enhance their capabilities by leveraging specialized domain expert models. AIDE operates through a four-stage process: (1) identifying instances for refinement, (2) engaging domain experts for targeted analysis, (3) synthesizing expert outputs with existing data, and (4) integrating enhanced instances into the training pipeline. Experiments on multiple benchmarks, including MMMU, MME, MMBench, etc., demonstrate AIDE's ability to achieve notable performance gains without relying on larger VLMs nor human supervision. Our framework provides a scalable, resource-efficient approach to continuous VLM improvement, addressing critical limitations in current methodologies, particularly valuable when larger models are unavailable to access.

### Evolution of Data-driven Single- and Multi-Hazard Susceptibility Mapping and Emergence of Deep Learning Methods 
[[arxiv](https://arxiv.org/abs/2502.09045)] [[cool](https://papers.cool/arxiv/2502.09045)] [[pdf](https://arxiv.org/pdf/2502.09045)]
> **Authors**: Jaya Sreevalsan-Nair,Aswathi Mundayatt
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Data-driven susceptibility mapping of natural hazards has harnessed the advances in classification methods used on heterogeneous sources represented as raster images. Susceptibility mapping is an important step towards risk assessment for any natural hazard. Increasingly, multiple hazards co-occur spatially, temporally, or both, which calls for an in-depth study on multi-hazard susceptibility mapping. In recent years, single-hazard susceptibility mapping algorithms have become well-established and have been extended to multi-hazard susceptibility mapping. Deep learning is also emerging as a promising method for single-hazard susceptibility mapping. Here, we discuss the evolution of methods for a single hazard, their extensions to multi-hazard maps as a late fusion of decisions, and the use of deep learning methods in susceptibility mapping. We finally propose a vision for adapting data fusion strategies in multimodal deep learning to multi-hazard susceptibility mapping. From the background study of susceptibility methods, we demonstrate that deep learning models are promising, untapped methods for multi-hazard susceptibility mapping. Data fusion strategies provide a larger space of deep learning models applicable to multi-hazard susceptibility mapping.

### EventSTR: A Benchmark Dataset and Baselines for Event Stream based Scene Text Recognition 
[[arxiv](https://arxiv.org/abs/2502.09020)] [[cool](https://papers.cool/arxiv/2502.09020)] [[pdf](https://arxiv.org/pdf/2502.09020)]
> **Authors**: Xiao Wang,Jingtao Jiang,Dong Li,Futian Wang,Lin Zhu,Yaowei Wang,Yongyong Tian,Jin Tang
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: In Peer Review
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Mainstream Scene Text Recognition (STR) algorithms are developed based on RGB cameras which are sensitive to challenging factors such as low illumination, motion blur, and cluttered backgrounds. In this paper, we propose to recognize the scene text using bio-inspired event cameras by collecting and annotating a large-scale benchmark dataset, termed EventSTR. It contains 9,928 high-definition (1280 * 720) event samples and involves both Chinese and English characters. We also benchmark multiple STR algorithms as the baselines for future works to compare. In addition, we propose a new event-based scene text recognition framework, termed SimC-ESTR. It first extracts the event features using a visual encoder and projects them into tokens using a Q-former module. More importantly, we propose to augment the vision tokens based on a memory mechanism before feeding into the large language models. A similarity-based error correction mechanism is embedded within the large language model to correct potential minor errors fundamentally based on contextual information. Extensive experiments on the newly proposed EventSTR dataset and two simulation STR datasets fully demonstrate the effectiveness of our proposed model. We believe that the dataset and algorithmic model can innovatively propose an event-based STR task and are expected to accelerate the application of event cameras in various industries. The source code and pre-trained models will be released on https://github.com/Event-AHU/EventSTR

### Residual Transformer Fusion Network for Salt and Pepper Image Denoising 
[[arxiv](https://arxiv.org/abs/2502.09000)] [[cool](https://papers.cool/arxiv/2502.09000)] [[pdf](https://arxiv.org/pdf/2502.09000)]
> **Authors**: Bintang Pradana Erlangga Putra,Heri Prasetyo,Esti Suryani
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 8 pages, 17 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Convolutional Neural Network (CNN) has been widely used in unstructured datasets, one of which is image denoising. Image denoising is a noisy image reconstruction process that aims to reduce additional noise that occurs from the noisy image with various strategies. Image denoising has a problem, namely that some image denoising methods require some prior knowledge of information about noise. To overcome this problem, a combined architecture of Convolutional Vision Transformer (CvT) and Residual Networks (ResNet) is used which is called the Residual Transformer Fusion Network (RTF-Net). In general, the process in this architecture can be divided into two parts, Noise Suppression Network (NSN) and Structure Enhancement Network (SEN). Residual Block is used in the Noise Suppression Network and is used to learn the noise map in the image, while the CvT is used in the Structure Enhancement Network and is used to learn the details that need to be added to the image processed by the Noise Suppression Network. The model was trained using the DIV2K Training Set dataset, and validation using the DIV2K Validation Set. After doing the training, the model was tested using Lena, Bridge, Pepper, and BSD300 images with noise levels ranging from 30%, 50%, and 70% and the PSNR results were compared with the DBA, NASNLM, PARIGI, NLSF, NLSF-MLP and NLSF-CNN methods. The test results show that the proposed method is superior in all cases except for Pepper's image with a noise level of 30%, where NLSF-CNN is superior with a PSNR value of 32.99 dB, while the proposed method gets a PSNR value of 31.70 dB.

### Hierarchical Vision Transformer with Prototypes for Interpretable Medical Image Classification 
[[arxiv](https://arxiv.org/abs/2502.08997)] [[cool](https://papers.cool/arxiv/2502.08997)] [[pdf](https://arxiv.org/pdf/2502.08997)]
> **Authors**: Luisa Gallée,Catharina Silvia Lisson,Meinrad Beer,Michael Götz
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Explainability is a highly demanded requirement for applications in high-risk areas such as medicine. Vision Transformers have mainly been limited to attention extraction to provide insight into the model's reasoning. Our approach combines the high performance of Vision Transformers with the introduction of new explainability capabilities. We present HierViT, a Vision Transformer that is inherently interpretable and adapts its reasoning to that of humans. A hierarchical structure is used to process domain-specific features for prediction. It is interpretable by design, as it derives the target output with human-defined features that are visualized by exemplary images (prototypes). By incorporating domain knowledge about these decisive features, the reasoning is semantically similar to human reasoning and therefore intuitive. Moreover, attention heatmaps visualize the crucial regions for identifying each feature, thereby providing HierViT with a versatile tool for validating predictions. Evaluated on two medical benchmark datasets, LIDC-IDRI for lung nodule assessment and derm7pt for skin lesion classification, HierViT achieves superior and comparable prediction accuracy, respectively, while offering explanations that align with human reasoning.

### Topo2Seq: Enhanced Topology Reasoning via Topology Sequence Learning 
[[arxiv](https://arxiv.org/abs/2502.08974)] [[cool](https://papers.cool/arxiv/2502.08974)] [[pdf](https://arxiv.org/pdf/2502.08974)]
> **Authors**: Yiming Yang,Yueru Luo,Bingkun He,Erlong Li,Zhipeng Cao,Chao Zheng,Shuqi Mei,Zhen Li
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Extracting lane topology from perspective views (PV) is crucial for planning and control in autonomous driving. This approach extracts potential drivable trajectories for self-driving vehicles without relying on high-definition (HD) maps. However, the unordered nature and weak long-range perception of the DETR-like framework can result in misaligned segment endpoints and limited topological prediction capabilities. Inspired by the learning of contextual relationships in language models, the connectivity relations in roads can be characterized as explicit topology sequences. In this paper, we introduce Topo2Seq, a novel approach for enhancing topology reasoning via topology sequences learning. The core concept of Topo2Seq is a randomized order prompt-to-sequence learning between lane segment decoder and topology sequence decoder. The dual-decoder branches simultaneously learn the lane topology sequences extracted from the Directed Acyclic Graph (DAG) and the lane graph containing geometric information. Randomized order prompt-to-sequence learning extracts unordered key points from the lane graph predicted by the lane segment decoder, which are then fed into the prompt design of the topology sequence decoder to reconstruct an ordered and complete lane graph. In this way, the lane segment decoder learns powerful long-range perception and accurate topological reasoning from the topology sequence decoder. Notably, topology sequence decoder is only introduced during training and does not affect the inference efficiency. Experimental evaluations on the OpenLane-V2 dataset demonstrate the state-of-the-art performance of Topo2Seq in topology reasoning.

## 计算机与社会(cs.CY:Computers and Society)

### Meta-Cultural Competence: Climbing the Right Hill of Cultural Awareness 
[[arxiv](https://arxiv.org/abs/2502.09637)] [[cool](https://papers.cool/arxiv/2502.09637)] [[pdf](https://arxiv.org/pdf/2502.09637)]
> **Authors**: Sougata Saha,Saurabh Kumar Pandey,Monojit Choudhury
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,人工智能,计算语言学
- **Abstract**: Numerous recent studies have shown that Large Language Models (LLMs) are biased towards a Western and Anglo-centric worldview, which compromises their usefulness in non-Western cultural settings. However, "culture" is a complex, multifaceted topic, and its awareness, representation, and modeling in LLMs and LLM-based applications can be defined and measured in numerous ways. In this position paper, we ask what does it mean for an LLM to possess "cultural awareness", and through a thought experiment, which is an extension of the Octopus test proposed by Bender and Koller (2020), we argue that it is not cultural awareness or knowledge, rather meta-cultural competence, which is required of an LLM and LLM-based AI system that will make it useful across various, including completely unseen, cultures. We lay out the principles of meta-cultural competence AI systems, and discuss ways to measure and model those.

## 数据结构和算法(cs.DS:Data Structures and Algorithms)

### Fast Tensor Completion via Approximate Richardson Iteration 
[[arxiv](https://arxiv.org/abs/2502.09534)] [[cool](https://papers.cool/arxiv/2502.09534)] [[pdf](https://arxiv.org/pdf/2502.09534)]
> **Authors**: Mehrdad Ghadiri,Matthew Fahrbach,Yunbum Kook,Ali Jadbabaie
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 20 pages, 4 figures
- **标题**: None
- **领域**: 数据结构和算法,机器学习,统计理论
- **Abstract**: We study tensor completion (TC) through the lens of low-rank tensor decomposition (TD). Many TD algorithms use fast alternating minimization methods, which solve highly structured linear regression problems at each step (e.g., for CP, Tucker, and tensor-train decompositions). However, such algebraic structure is lost in TC regression problems, making direct extensions unclear. To address this, we propose a lifting approach that approximately solves TC regression problems using structured TD regression algorithms as blackbox subroutines, enabling sublinear-time methods. We theoretically analyze the convergence rate of our approximate Richardson iteration based algorithm, and we demonstrate on real-world tensors that its running time can be 100x faster than direct methods for CP completion.

## 人机交互(cs.HC:Human-Computer Interaction)

### A Taxonomy of Linguistic Expressions That Contribute To Anthropomorphism of Language Technologies 
[[arxiv](https://arxiv.org/abs/2502.09870)] [[cool](https://papers.cool/arxiv/2502.09870)] [[pdf](https://arxiv.org/pdf/2502.09870)]
> **Authors**: Alicia DeVrio,Myra Cheng,Lisa Egede,Alexandra Olteanu,Su Lin Blodgett
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 18 pages, 1 figure, to appear at CHI 2025
- **标题**: None
- **领域**: 人机交互,人工智能,计算语言学
- **Abstract**: Recent attention to anthropomorphism -- the attribution of human-like qualities to non-human objects or entities -- of language technologies like LLMs has sparked renewed discussions about potential negative impacts of anthropomorphism. To productively discuss the impacts of this anthropomorphism and in what contexts it is appropriate, we need a shared vocabulary for the vast variety of ways that language can be anthropomorphic. In this work, we draw on existing literature and analyze empirical cases of user interactions with language technologies to develop a taxonomy of textual expressions that can contribute to anthropomorphism. We highlight challenges and tensions involved in understanding linguistic anthropomorphism, such as how all language is fundamentally human and how efforts to characterize and shift perceptions of humanness in machines can also dehumanize certain humans. We discuss ways that our taxonomy supports more precise and effective discussions of and decisions about anthropomorphism of language technologies.

### How Users Who are Blind or Low Vision Play Mobile Games: Perceptions, Challenges, and Strategies 
[[arxiv](https://arxiv.org/abs/2502.09866)] [[cool](https://papers.cool/arxiv/2502.09866)] [[pdf](https://arxiv.org/pdf/2502.09866)]
> **Authors**: Zihe Ran,Xiyu Li,Qing Xiao,Xianzhe Fan,Franklin Mingzhe Li,Yanyun Wang,Zhicong Lu
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 18 pages, 3 figures, Accepted by CHI '25
- **标题**: None
- **领域**: 人机交互,人工智能,计算机与社会,机器学习
- **Abstract**: As blind and low-vision (BLV) players engage more deeply with games, accessibility features have become essential. While some research has explored tools and strategies to enhance game accessibility, the specific experiences of these players with mobile games remain underexamined. This study addresses this gap by investigating how BLV users experience mobile games with varying accessibility levels. Through interviews with 32 experienced BLV mobile players, we explore their perceptions, challenges, and strategies for engaging with mobile games. Our findings reveal that BLV players turn to mobile games to alleviate boredom, achieve a sense of accomplishment, and build social connections, but face barriers depending on the game's accessibility level. We also compare mobile games to other forms of gaming, highlighting the relative advantages of mobile games, such as the inherent accessibility of smartphones. This study contributes to understanding BLV mobile gaming experiences and provides insights for enhancing accessible mobile game design.

### Co-designing Large Language Model Tools for Project-Based Learning with K12 Educators 
[[arxiv](https://arxiv.org/abs/2502.09799)] [[cool](https://papers.cool/arxiv/2502.09799)] [[pdf](https://arxiv.org/pdf/2502.09799)]
> **Authors**: Prerna Ravi,John Masla,Gisella Kakoti,Grace Lin,Emma Anderson,Matt Taylor,Anastasia Ostrowski,Cynthia Breazeal,Eric Klopfer,Hal Abelson
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 25 pages
- **标题**: None
- **领域**: 人机交互,人工智能,计算机与社会
- **Abstract**: The emergence of generative AI, particularly large language models (LLMs), has opened the door for student-centered and active learning methods like project-based learning (PBL). However, PBL poses practical implementation challenges for educators around project design and management, assessment, and balancing student guidance with student autonomy. The following research documents a co-design process with interdisciplinary K-12 teachers to explore and address the current PBL challenges they face. Through teacher-driven interviews, collaborative workshops, and iterative design of wireframes, we gathered evidence for ways LLMs can support teachers in implementing high-quality PBL pedagogy by automating routine tasks and enhancing personalized learning. Teachers in the study advocated for supporting their professional growth and augmenting their current roles without replacing them. They also identified affordances and challenges around classroom integration, including resource requirements and constraints, ethical concerns, and potential immediate and long-term impacts. Drawing on these, we propose design guidelines for future deployment of LLM tools in PBL.

### The AI-Therapist Duo: Exploring the Potential of Human-AI Collaboration in Personalized Art Therapy for PICS Intervention 
[[arxiv](https://arxiv.org/abs/2502.09757)] [[cool](https://papers.cool/arxiv/2502.09757)] [[pdf](https://arxiv.org/pdf/2502.09757)]
> **Authors**: Bereket A. Yilma,Chan Mi Kim,Geke Ludden,Thomas van Rompay,Luis A. Leiva
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 人机交互,人工智能
- **Abstract**: Post-intensive care syndrome (PICS) is a multifaceted condition that arises from prolonged stays in an intensive care unit (ICU). While preventing PICS among ICU patients is becoming increasingly important, interventions remain limited. Building on evidence supporting the effectiveness of art exposure in addressing the psychological aspects of PICS, we propose a novel art therapy solution through a collaborative Human-AI approach that enhances personalized therapeutic interventions using state-of-the-art Visual Art Recommendation Systems. We developed two Human-in-the-Loop (HITL) personalization methods and assessed their impact through a large-scale user study (N=150). Our findings demonstrate that this Human-AI collaboration not only enhances the personalization and effectiveness of art therapy but also supports therapists by streamlining their workload. While our study centres on PICS intervention, the results suggest that human-AI collaborative Art therapy could potentially benefit other areas where emotional support is critical, such as cases of anxiety and depression.

### Revisiting Euclidean Alignment for Transfer Learning in EEG-Based Brain-Computer Interfaces 
[[arxiv](https://arxiv.org/abs/2502.09203)] [[cool](https://papers.cool/arxiv/2502.09203)] [[pdf](https://arxiv.org/pdf/2502.09203)]
> **Authors**: Dongrui Wu
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 人机交互,机器学习
- **Abstract**: Due to the non-stationarity and large individual differences of EEG signals, EEG-based brain-computer interfaces (BCIs) usually need subject-specific calibration to tailor the decoding algorithm for each new subject, which is time-consuming and user-unfriendly, hindering their real-world applications. Transfer learning (TL) has been extensively used to expedite the calibration, by making use of EEG data from other subjects/sessions. An important consideration in TL for EEG-based BCIs is to reduce the data distribution discrepancies among different subjects/session, to avoid negative transfer. Euclidean alignment (EA) was proposed in 2020 to address this challenge. Numerous experiments from 10 different BCI paradigms demonstrated its effectiveness and efficiency. This paper revisits the EA, explaining its procedure and correct usage, introducing its applications and extensions, and pointing out potential new research directions. It should be very helpful to BCI researchers, especially those who are working on EEG signal decoding.

### Show Me the Work: Fact-Checkers' Requirements for Explainable Automated Fact-Checking 
[[arxiv](https://arxiv.org/abs/2502.09083)] [[cool](https://papers.cool/arxiv/2502.09083)] [[pdf](https://arxiv.org/pdf/2502.09083)]
> **Authors**: Greta Warren,Irina Shklovski,Isabelle Augenstein
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: Conditionally accepted to CHI'25
- **标题**: None
- **领域**: 人机交互,人工智能,计算语言学
- **Abstract**: The pervasiveness of large language models and generative AI in online media has amplified the need for effective automated fact-checking to assist fact-checkers in tackling the increasing volume and sophistication of misinformation. The complex nature of fact-checking demands that automated fact-checking systems provide explanations that enable fact-checkers to scrutinise their outputs. However, it is unclear how these explanations should align with the decision-making and reasoning processes of fact-checkers to be effectively integrated into their workflows. Through semi-structured interviews with fact-checking professionals, we bridge this gap by: (i) providing an account of how fact-checkers assess evidence, make decisions, and explain their processes; (ii) examining how fact-checkers use automated tools in practice; and (iii) identifying fact-checker explanation requirements for automated fact-checking tools. The findings show unmet explanation needs and identify important criteria for replicable fact-checking explanations that trace the model's reasoning path, reference specific evidence, and highlight uncertainty and information gaps.

### Exploring the Needs of Practising Musicians in Co-Creative AI Through Co-Design 
[[arxiv](https://arxiv.org/abs/2502.09055)] [[cool](https://papers.cool/arxiv/2502.09055)] [[pdf](https://arxiv.org/pdf/2502.09055)]
> **Authors**: Stephen James Krol,Maria Teresa Llano Rodriguez,Miguel Loor Paredes
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: Paper accepted into CHI 2025, Yokohama Japan, April 26th - May 1st
- **标题**: None
- **领域**: 人机交互,人工智能
- **Abstract**: Recent advances in generative AI music have resulted in new technologies that are being framed as co-creative tools for musicians with early work demonstrating their potential to add to music practice. While the field has seen many valuable contributions, work that involves practising musicians in the design and development of these tools is limited, with the majority of work including them only once a tool has been developed. In this paper, we present a case study that explores the needs of practising musicians through the co-design of a musical variation system, highlighting the importance of involving a diverse range of musicians throughout the design process and uncovering various design insights. This was achieved through two workshops and a two week ecological evaluation, where musicians from different musical backgrounds offered valuable insights not only on a musical system's design but also on how a musical AI could be integrated into their musical practices.

## 信息检索(cs.IR:Information Retrieval)

### ArchRAG: Attributed Community-based Hierarchical Retrieval-Augmented Generation 
[[arxiv](https://arxiv.org/abs/2502.09891)] [[cool](https://papers.cool/arxiv/2502.09891)] [[pdf](https://arxiv.org/pdf/2502.09891)]
> **Authors**: Shu Wang,Yixiang Fang,Yingli Zhou,Xilin Liu,Yuchi Ma
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,人工智能
- **Abstract**: Retrieval-Augmented Generation (RAG) has proven effective in integrating external knowledge into large language models (LLMs) for question-answer (QA) tasks. The state-of-the-art RAG approaches often use the graph data as the external data since they capture the rich semantic information and link relationships between entities. However, existing graph-based RAG approaches cannot accurately identify the relevant information from the graph and also consume large numbers of tokens in the online retrieval process. To address these issues, we introduce a novel graph-based RAG approach, called Attributed Community-based Hierarchical RAG (ArchRAG), by augmenting the question using attributed communities, and also introducing a novel LLM-based hierarchical clustering method. To retrieve the most relevant information from the graph for the question, we build a novel hierarchical index structure for the attributed communities and develop an effective online retrieval method. Experimental results demonstrate that ArchRAG outperforms existing methods in terms of both accuracy and token cost.

### A Survey on LLM-based News Recommender Systems 
[[arxiv](https://arxiv.org/abs/2502.09797)] [[cool](https://papers.cool/arxiv/2502.09797)] [[pdf](https://arxiv.org/pdf/2502.09797)]
> **Authors**: Rongyao Wang,Veronica Liesaputra,Zhiyi Huang
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 20 pages
- **标题**: None
- **领域**: 信息检索,人工智能
- **Abstract**: News recommender systems play a critical role in mitigating the information overload problem. In recent years, due to the successful applications of large language model technologies, researchers have utilized Discriminative Large Language Models (DLLMs) or Generative Large Language Models (GLLMs) to improve the performance of news recommender systems. Although several recent surveys review significant challenges for deep learning-based news recommender systems, such as fairness, privacy-preserving, and responsibility, there is a lack of a systematic survey on Large Language Model (LLM)-based news recommender systems. In order to review different core methodologies and explore potential issues systematically, we categorize DLLM-based and GLLM-based news recommender systems under the umbrella of LLM-based news recommender systems. In this survey, we first overview the development of deep learning-based news recommender systems. Then, we review LLM-based news recommender systems based on three aspects: news-oriented modeling, user-oriented modeling, and prediction-oriented modeling. Next, we examine the challenges from various perspectives, including datasets, benchmarking tools, and methodologies. Furthermore, we conduct extensive experiments to analyze how large language model technologies affect the performance of different news recommender systems. Finally, we comprehensively explore the future directions for LLM-based news recommendations in the era of LLMs.

### Bridging Jensen Gap for Max-Min Group Fairness Optimization in Recommendation 
[[arxiv](https://arxiv.org/abs/2502.09319)] [[cool](https://papers.cool/arxiv/2502.09319)] [[pdf](https://arxiv.org/pdf/2502.09319)]
> **Authors**: Chen Xu,Yuxin Li,Wenjie Wang,Liang Pang,Jun Xu,Tat-Seng Chua
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: Accepted in ICLR 2025
- **标题**: None
- **领域**: 信息检索,机器学习
- **Abstract**: Group max-min fairness (MMF) is commonly used in fairness-aware recommender systems (RS) as an optimization objective, as it aims to protect marginalized item groups and ensures a fair competition platform. However, our theoretical analysis indicates that integrating MMF constraint violates the assumption of sample independence during optimization, causing the loss function to deviate from linear additivity. Such nonlinearity property introduces the Jensen gap between the model's convergence point and the optimal point if mini-batch sampling is applied. Both theoretical and empirical studies show that as the mini-batch size decreases and the group size increases, the Jensen gap will widen accordingly. Some methods using heuristic re-weighting or debiasing strategies have the potential to bridge the Jensen gap. However, they either lack theoretical guarantees or suffer from heavy computational costs. To overcome these limitations, we first theoretically demonstrate that the MMF-constrained objective can be essentially reformulated as a group-weighted optimization objective. Then we present an efficient and effective algorithm named FairDual, which utilizes a dual optimization technique to minimize the Jensen gap. Our theoretical analysis demonstrates that FairDual can achieve a sub-linear convergence rate to the globally optimal solution and the Jensen gap can be well bounded under a mini-batch sampling strategy with random shuffle. Extensive experiments conducted using six large-scale RS backbone models on three publicly available datasets demonstrate that FairDual outperforms all baselines in terms of both accuracy and fairness. Our data and codes are shared at https://github.com/XuChen0427/FairDual.

### Leveraging Member-Group Relations via Multi-View Graph Filtering for Effective Group Recommendation 
[[arxiv](https://arxiv.org/abs/2502.09050)] [[cool](https://papers.cool/arxiv/2502.09050)] [[pdf](https://arxiv.org/pdf/2502.09050)]
> **Authors**: Chae-Hyun Kim,Yoon-Ryung Choi,Jin-Duk Park,Won-Yong Shin
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 5 pages, 3 figures, 4 tables; ACM Web Conference (WWW 2025) (to appear) (Please cite our conference version.)
- **标题**: None
- **领域**: 信息检索,人工智能,信息论,机器学习,社交和信息网络
- **Abstract**: Group recommendation aims at providing optimized recommendations tailored to diverse groups, enabling groups to enjoy appropriate items. On the other hand, most existing group recommendation methods are built upon deep neural network (DNN) architectures designed to capture the intricate relationships between member-level and group-level interactions. While these DNN-based approaches have proven their effectiveness, they require complex and expensive training procedures to incorporate group-level interactions in addition to member-level interactions. To overcome such limitations, we introduce Group-GF, a new approach for extremely fast recommendations of items to each group via multi-view graph filtering (GF) that offers a holistic view of complex member-group dynamics, without the need for costly model training. Specifically, in Group-GF, we first construct three item similarity graphs manifesting different viewpoints for GF. Then, we discover a distinct polynomial graph filter for each similarity graph and judiciously aggregate the three graph filters. Extensive experiments demonstrate the effectiveness of Group-GF in terms of significantly reducing runtime and achieving state-of-the-art recommendation accuracy.

### Criteria-Aware Graph Filtering: Extremely Fast Yet Accurate Multi-Criteria Recommendation 
[[arxiv](https://arxiv.org/abs/2502.09046)] [[cool](https://papers.cool/arxiv/2502.09046)] [[pdf](https://arxiv.org/pdf/2502.09046)]
> **Authors**: Jin-Duk Park,Jaemin Yoo,Won-Yong Shin
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 12 pages, 8 figures, 7 tables; ACM Web Conference (WWW 2025) (to appear) (Please cite our conference version.)
- **标题**: None
- **领域**: 信息检索,人工智能,信息论,机器学习,社交和信息网络
- **Abstract**: Multi-criteria (MC) recommender systems, which utilize MC rating information for recommendation, are increasingly widespread in various e-commerce domains. However, the MC recommendation using training-based collaborative filtering, requiring consideration of multiple ratings compared to single-criterion counterparts, often poses practical challenges in achieving state-of-the-art performance along with scalable model training. To solve this problem, we propose CA-GF, a training-free MC recommendation method, which is built upon criteria-aware graph filtering for efficient yet accurate MC recommendations. Specifically, first, we construct an item-item similarity graph using an MC user-expansion graph. Next, we design CA-GF composed of the following key components, including 1) criterion-specific graph filtering where the optimal filter for each criterion is found using various types of polynomial low-pass filters and 2) criteria preference-infused aggregation where the smoothed signals from each criterion are aggregated. We demonstrate that CA-GF is (a) efficient: providing the computational efficiency, offering the extremely fast runtime of less than 0.2 seconds even on the largest benchmark dataset, (b) accurate: outperforming benchmark MC recommendation methods, achieving substantial accuracy gains up to 24% compared to the best competitor, and (c) interpretable: providing interpretations for the contribution of each criterion to the model prediction based on visualizations.

## 机器学习(cs.LG:Machine Learning)

### Thompson Sampling for Repeated Newsvendor 
[[arxiv](https://arxiv.org/abs/2502.09900)] [[cool](https://papers.cool/arxiv/2502.09900)] [[pdf](https://arxiv.org/pdf/2502.09900)]
> **Authors**: Weizhou Zhang,Chen Li,Hanzhang Qin,Yunbei Xu,Ruihao Zhu
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: In this paper, we investigate the performance of Thompson Sampling (TS) for online learning with censored feedback, focusing primarily on the classic repeated newsvendor model--a foundational framework in inventory management--and demonstrating how our techniques can be naturally extended to a broader class of problems. We model demand using a Weibull distribution and initialize TS with a Gamma prior to dynamically adjust order quantities. Our analysis establishes optimal (up to logarithmic factors) frequentist regret bounds for TS without imposing restrictive prior assumptions. More importantly, it yields novel and highly interpretable insights on how TS addresses the exploration-exploitation trade-off in the repeated newsvendor setting. Specifically, our results show that when past order quantities are sufficiently large to overcome censoring, TS accurately estimates the unknown demand parameters, leading to near-optimal ordering decisions. Conversely, when past orders are relatively small, TS automatically increases future order quantities to gather additional demand information. Extensive numerical simulations further demonstrate that TS outperforms more conservative and widely-used approaches such as online convex optimization, upper confidence bounds, and myopic Bayesian dynamic programming. This study also lays the foundation for exploring general online learning problems with censored feedback.

### Optimal lower Lipschitz bounds for ReLU layers, saturation, and phase retrieval 
[[arxiv](https://arxiv.org/abs/2502.09898)] [[cool](https://papers.cool/arxiv/2502.09898)] [[pdf](https://arxiv.org/pdf/2502.09898)]
> **Authors**: Daniel Freeman,Daniel Haider
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 22 pages
- **标题**: None
- **领域**: 机器学习,泛函分析,数值分析
- **Abstract**: The injectivity of ReLU layers in neural networks, the recovery of vectors from clipped or saturated measurements, and (real) phase retrieval in $\mathbb{R}^n$ allow for a similar problem formulation and characterization using frame theory. In this paper, we revisit all three problems with a unified perspective and derive lower Lipschitz bounds for ReLU layers and clipping which are analogous to the previously known result for phase retrieval and are optimal up to a constant factor.

### Symmetry-Preserving Diffusion Models via Target Symmetrization 
[[arxiv](https://arxiv.org/abs/2502.09890)] [[cool](https://papers.cool/arxiv/2502.09890)] [[pdf](https://arxiv.org/pdf/2502.09890)]
> **Authors**: Vinh Tong,Yun Ye,Trung-Dung Hoang,Anji Liu,Guy Van den Broeck,Mathias Niepert
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Diffusion models are powerful tools for capturing complex distributions, but modeling data with inherent symmetries, such as molecular structures, remains challenging. Equivariant denoisers are commonly used to address this, but they introduce architectural complexity and optimization challenges, including noisy gradients and convergence issues. We propose a novel approach that enforces equivariance through a symmetrized loss function, which applies a time-dependent weighted averaging operation over group actions to the model's prediction target. This ensures equivariance without explicit architectural constraints and reduces gradient variance, leading to more stable and efficient optimization. Our method uses Monte Carlo sampling to estimate the average, incurring minimal computational overhead. We provide theoretical guarantees of equivariance for the minimizer of our loss function and demonstrate its effectiveness on synthetic datasets and the molecular conformation generation task using the GEOM-QM9 dataset. Experiments show improved sample quality compared to existing methods, highlighting the potential of our approach to enhance the scalability and practicality of equivariant diffusion models in generative tasks.

### Comprehensive Review of Neural Differential Equations for Time Series Analysis 
[[arxiv](https://arxiv.org/abs/2502.09885)] [[cool](https://papers.cool/arxiv/2502.09885)] [[pdf](https://arxiv.org/pdf/2502.09885)]
> **Authors**: YongKyung Oh,Seungsu Kam,Jonghun Lee,Dong-Young Lim,Sungil Kim,Alex Bui
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Time series modeling and analysis has become critical in various domains. Conventional methods such as RNNs and Transformers, while effective for discrete-time and regularly sampled data, face significant challenges in capturing the continuous dynamics and irregular sampling patterns inherent in real-world scenarios. Neural Differential Equations (NDEs) represent a paradigm shift by combining the flexibility of neural networks with the mathematical rigor of differential equations. This paper presents a comprehensive review of NDE-based methods for time series analysis, including neural ordinary differential equations, neural controlled differential equations, and neural stochastic differential equations. We provide a detailed discussion of their mathematical formulations, numerical methods, and applications, highlighting their ability to model continuous-time dynamics. Furthermore, we address key challenges and future research directions. This survey serves as a foundation for researchers and practitioners seeking to leverage NDEs for advanced time series analysis.

### Nonasymptotic CLT and Error Bounds for Two-Time-Scale Stochastic Approximation 
[[arxiv](https://arxiv.org/abs/2502.09884)] [[cool](https://papers.cool/arxiv/2502.09884)] [[pdf](https://arxiv.org/pdf/2502.09884)]
> **Authors**: Seo Taek Kong,Sihan Zeng,Thinh T. Doan,R. Srikant
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: We consider linear two-time-scale stochastic approximation algorithms driven by martingale noise. Recent applications in machine learning motivate the need to understand finite-time error rates, but conventional stochastic approximation analysis focus on either asymptotic convergence in distribution or finite-time bounds that are far from optimal. Prior work on asymptotic central limit theorems (CLTs) suggest that two-time-scale algorithms may be able to achieve $1/\sqrt{n}$ error in expectation, with a constant given by the expected norm of the limiting Gaussian vector. However, the best known finite-time rates are much slower. We derive the first non-asymptotic central limit theorem with respect to the Wasserstein-1 distance for two-time-scale stochastic approximation with Polyak-Ruppert averaging. As a corollary, we show that expected error achieved by Polyak-Ruppert averaging decays at rate $1/\sqrt{n}$, which significantly improves on the rates of convergence in prior works.

### Solvable Dynamics of Self-Supervised Word Embeddings and the Emergence of Analogical Reasoning 
[[arxiv](https://arxiv.org/abs/2502.09863)] [[cool](https://papers.cool/arxiv/2502.09863)] [[pdf](https://arxiv.org/pdf/2502.09863)]
> **Authors**: Dhruva Karkada,James B. Simon,Yasaman Bahri,Michael R. DeWeese
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 26 pages, 10 figures
- **标题**: None
- **领域**: 机器学习,计算语言学,机器学习
- **Abstract**: The remarkable success of large language models relies on their ability to implicitly learn structured latent representations from the pretraining corpus. As a simpler surrogate for representation learning in language modeling, we study a class of solvable contrastive self-supervised algorithms which we term quadratic word embedding models. These models resemble the word2vec algorithm and perform similarly on downstream tasks. Our main contributions are analytical solutions for both the training dynamics (under certain hyperparameter choices) and the final word embeddings, given in terms of only the corpus statistics. Our solutions reveal that these models learn orthogonal linear subspaces one at a time, each one incrementing the effective rank of the embeddings until model capacity is saturated. Training on WikiText, we find that the top subspaces represent interpretable concepts. Finally, we use our dynamical theory to predict how and when models acquire the ability to complete analogies.

### Automated Hypothesis Validation with Agentic Sequential Falsifications 
[[arxiv](https://arxiv.org/abs/2502.09858)] [[cool](https://papers.cool/arxiv/2502.09858)] [[pdf](https://arxiv.org/pdf/2502.09858)]
> **Authors**: Kexin Huang,Ying Jin,Ryan Li,Michael Y. Li,Emmanuel Candès,Jure Leskovec
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,定量方法
- **Abstract**: Hypotheses are central to information acquisition, decision-making, and discovery. However, many real-world hypotheses are abstract, high-level statements that are difficult to validate directly. This challenge is further intensified by the rise of hypothesis generation from Large Language Models (LLMs), which are prone to hallucination and produce hypotheses in volumes that make manual validation impractical. Here we propose Popper, an agentic framework for rigorous automated validation of free-form hypotheses. Guided by Karl Popper's principle of falsification, Popper validates a hypothesis using LLM agents that design and execute falsification experiments targeting its measurable implications. A novel sequential testing framework ensures strict Type-I error control while actively gathering evidence from diverse observations, whether drawn from existing data or newly conducted procedures. We demonstrate Popper on six domains including biology, economics, and sociology. Popper delivers robust error control, high power, and scalability. Furthermore, compared to human scientists, Popper achieved comparable performance in validating complex biological hypotheses while reducing time by 10 folds, providing a scalable, rigorous solution for hypothesis validation.

### Elastic Representation: Mitigating Spurious Correlations for Group Robustness 
[[arxiv](https://arxiv.org/abs/2502.09850)] [[cool](https://papers.cool/arxiv/2502.09850)] [[pdf](https://arxiv.org/pdf/2502.09850)]
> **Authors**: Tao Wen,Zihan Wang,Quan Zhang,Qi Lei
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: Accepted at AISTATS 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Deep learning models can suffer from severe performance degradation when relying on spurious correlations between input features and labels, making the models perform well on training data but have poor prediction accuracy for minority groups. This problem arises especially when training data are limited or imbalanced. While most prior work focuses on learning invariant features (with consistent correlations to y), it overlooks the potential harm of spurious correlations between features. We hereby propose Elastic Representation (ElRep) to learn features by imposing Nuclear- and Frobenius-norm penalties on the representation from the last layer of a neural network. Similar to the elastic net, ElRep enjoys the benefits of learning important features without losing feature diversity. The proposed method is simple yet effective. It can be integrated into many deep learning approaches to mitigate spurious correlations and improve group robustness. Moreover, we theoretically show that ElRep has minimum negative impacts on in-distribution predictions. This is a remarkable advantage over approaches that prioritize minority groups at the cost of overall performance.

### A Survey on Human-Centered Evaluation of Explainable AI Methods in Clinical Decision Support Systems 
[[arxiv](https://arxiv.org/abs/2502.09849)] [[cool](https://papers.cool/arxiv/2502.09849)] [[pdf](https://arxiv.org/pdf/2502.09849)]
> **Authors**: Alessandro Gambetti,Qiwei Han,Hong Shen,Claudia Soares
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 10 pages, 1 table
- **标题**: None
- **领域**: 机器学习,人机交互
- **Abstract**: Explainable AI (XAI) has become a crucial component of Clinical Decision Support Systems (CDSS) to enhance transparency, trust, and clinical adoption. However, while many XAI methods have been proposed, their effectiveness in real-world medical settings remains underexplored. This paper provides a survey of human-centered evaluations of Explainable AI methods in Clinical Decision Support Systems. By categorizing existing works based on XAI methodologies, evaluation frameworks, and clinical adoption challenges, we offer a structured understanding of the landscape. Our findings reveal key challenges in the integration of XAI into healthcare workflows and propose a structured framework to align the evaluation methods of XAI with the clinical needs of stakeholders.

### Solving Empirical Bayes via Transformers 
[[arxiv](https://arxiv.org/abs/2502.09844)] [[cool](https://papers.cool/arxiv/2502.09844)] [[pdf](https://arxiv.org/pdf/2502.09844)]
> **Authors**: Anzo Teh,Mark Jabbour,Yury Polyanskiy
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 27 pages, 14 figures, 11 tables
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: This work applies modern AI tools (transformers) to solving one of the oldest statistical problems: Poisson means under empirical Bayes (Poisson-EB) setting. In Poisson-EB a high-dimensional mean vector $θ$ (with iid coordinates sampled from an unknown prior $π$) is estimated on the basis of $X=\mathrm{Poisson}(θ)$. A transformer model is pre-trained on a set of synthetically generated pairs $(X,θ)$ and learns to do in-context learning (ICL) by adapting to unknown $π$. Theoretically, we show that a sufficiently wide transformer can achieve vanishing regret with respect to an oracle estimator who knows $π$ as dimension grows to infinity. Practically, we discover that already very small models (100k parameters) are able to outperform the best classical algorithm (non-parametric maximum likelihood, or NPMLE) both in runtime and validation loss, which we compute on out-of-distribution synthetic data as well as real-world datasets (NHL hockey, MLB baseball, BookCorpusOpen). Finally, by using linear probes, we confirm that the transformer's EB estimator appears to internally work differently from either NPMLE or Robbins' estimators.

### Learning Fair Policies for Infectious Diseases Mitigation using Path Integral Control 
[[arxiv](https://arxiv.org/abs/2502.09831)] [[cool](https://papers.cool/arxiv/2502.09831)] [[pdf](https://arxiv.org/pdf/2502.09831)]
> **Authors**: Zhuangzhuang Jia,Hyuk Park,Gökçe Dayanıklı,Grani A. Hanasusanto
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,优化与控制
- **Abstract**: Infectious diseases pose major public health challenges to society, highlighting the importance of designing effective policies to reduce economic loss and mortality. In this paper, we propose a framework for sequential decision-making under uncertainty to design fairness-aware disease mitigation policies that incorporate various measures of unfairness. Specifically, our approach learns equitable vaccination and lockdown strategies based on a stochastic multi-group SIR model. To address the challenges of solving the resulting sequential decision-making problem, we adopt the path integral control algorithm as an efficient solution scheme. Through a case study, we demonstrate that our approach effectively improves fairness compared to conventional methods and provides valuable insights for policymakers.

### ATM-Net: Adaptive Termination and Multi-Precision Neural Networks for Energy-Harvested Edge Intelligence 
[[arxiv](https://arxiv.org/abs/2502.09822)] [[cool](https://papers.cool/arxiv/2502.09822)] [[pdf](https://arxiv.org/pdf/2502.09822)]
> **Authors**: Neeraj Solanki,Sepehr Tabrizchi,Samin Sohrabi,Jason Schmidt,Arman Roohi
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: ATM-Net is a novel neural network architecture tailored for energy-harvested IoT devices, integrating adaptive termination points with multi-precision computing. It dynamically adjusts computational precision (32/8/4-bit) and network depth based on energy availability via early exit points. An energy-aware task scheduler optimizes the energy-accuracy trade-off. Experiments on CIFAR-10, PlantVillage, and TissueMNIST show ATM-Net achieves up to 96.93% accuracy while reducing power consumption by 87.5% with Q4 quantization compared to 32-bit operations. The power-delay product improves from 13.6J to 0.141J for DenseNet-121 and from 10.3J to 0.106J for ResNet-18, demonstrating its suitability for energy-harvesting systems.

### Improving Acoustic Side-Channel Attacks on Keyboards Using Transformers and Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.09782)] [[cool](https://papers.cool/arxiv/2502.09782)] [[pdf](https://arxiv.org/pdf/2502.09782)]
> **Authors**: Jin Hyun Park,Seyyed Ali Ayati,Yichen Cai
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: We would like to withdraw our paper due to a significant error in the experimental methodology, which impacts the validity of our results. The error specifically affects the analysis presented in Section 4, where an incorrect dataset preprocessing step led to misleading conclusions
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,音频和语音处理
- **Abstract**: The increasing prevalence of microphones in everyday devices and the growing reliance on online services have amplified the risk of acoustic side-channel attacks (ASCAs) targeting keyboards. This study explores deep learning techniques, specifically vision transformers (VTs) and large language models (LLMs), to enhance the effectiveness and applicability of such attacks. We present substantial improvements over prior research, with the CoAtNet model achieving state-of-the-art performance. Our CoAtNet shows a 5.0% improvement for keystrokes recorded via smartphone (Phone) and 5.9% for those recorded via Zoom compared to previous benchmarks. We also evaluate transformer architectures and language models, with the best VT model matching CoAtNet's performance. A key advancement is the introduction of a noise mitigation method for real-world scenarios. By using LLMs for contextual understanding, we detect and correct erroneous keystrokes in noisy environments, enhancing ASCA performance. Additionally, fine-tuned lightweight language models with Low-Rank Adaptation (LoRA) deliver comparable performance to heavyweight models with 67X more parameters. This integration of VTs and LLMs improves the practical applicability of ASCA mitigation, marking the first use of these technologies to address ASCAs and error correction in real-world scenarios.

### Medical Applications of Graph Convolutional Networks Using Electronic Health Records: A Survey 
[[arxiv](https://arxiv.org/abs/2502.09781)] [[cool](https://papers.cool/arxiv/2502.09781)] [[pdf](https://arxiv.org/pdf/2502.09781)]
> **Authors**: Garrik Hoyt,Noyonica Chatterjee,Fortunato Battaglia,Paramita Basu
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 5 pages, 4 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Graph Convolutional Networks (GCNs) have emerged as a promising approach to machine learning on Electronic Health Records (EHRs). By constructing a graph representation of patient data and performing convolutions on neighborhoods of nodes, GCNs can capture complex relationships and extract meaningful insights to support medical decision making. This survey provides an overview of the current research in applying GCNs to EHR data. We identify the key medical domains and prediction tasks where these models are being utilized, common benchmark datasets, and architectural patterns to provide a comprehensive survey of this field. While this is a nascent area of research, GCNs demonstrate strong potential to leverage the complex information hidden in EHRs. Challenges and opportunities for future work are also discussed.

### Incentivize without Bonus: Provably Efficient Model-based Online Multi-agent RL for Markov Games 
[[arxiv](https://arxiv.org/abs/2502.09780)] [[cool](https://papers.cool/arxiv/2502.09780)] [[pdf](https://arxiv.org/pdf/2502.09780)]
> **Authors**: Tong Yang,Bo Dai,Lin Xiao,Yuejie Chi
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机科学与博弈论,优化与控制
- **Abstract**: Multi-agent reinforcement learning (MARL) lies at the heart of a plethora of applications involving the interaction of a group of agents in a shared unknown environment. A prominent framework for studying MARL is Markov games, with the goal of finding various notions of equilibria in a sample-efficient manner, such as the Nash equilibrium (NE) and the coarse correlated equilibrium (CCE). However, existing sample-efficient approaches either require tailored uncertainty estimation under function approximation, or careful coordination of the players. In this paper, we propose a novel model-based algorithm, called VMG, that incentivizes exploration via biasing the empirical estimate of the model parameters towards those with a higher collective best-response values of all the players when fixing the other players' policies, thus encouraging the policy to deviate from its current equilibrium for more exploration. VMG is oblivious to different forms of function approximation, and permits simultaneous and uncoupled policy updates of all players. Theoretically, we also establish that VMG achieves a near-optimal regret for finding both the NEs of two-player zero-sum Markov games and CCEs of multi-player general-sum Markov games under linear function approximation in an online environment, which nearly match their counterparts with sophisticated uncertainty quantification.

### Non-Markovian Discrete Diffusion with Causal Language Models 
[[arxiv](https://arxiv.org/abs/2502.09767)] [[cool](https://papers.cool/arxiv/2502.09767)] [[pdf](https://arxiv.org/pdf/2502.09767)]
> **Authors**: Yangtian Zhang,Sizhuang He,Daniel Levine,Lawrence Zhao,David Zhang,Syed A Rizvi,Emanuele Zappala,Rex Ying,David van Dijk
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: Under Review
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Discrete diffusion models have emerged as a flexible and controllable paradigm for structured sequence modeling, yet they still lag behind causal language models in expressiveness. To bridge the gap between two paradigms, we introduce CaDDi, a causal discrete diffusion model that unifies sequential and temporal modeling within a non-Markovian diffusion framework. Unlike conventional diffusion models that operate step by step with no access to prior states, CaDDi integrates the temporal trajectory, enabling more expressive and controllable generation. Our approach also treats causal language models as a special case, allowing seamless adoption of pretrained large language models (LLMs) for discrete diffusion without the need for architectural modifications. Empirically, we demonstrate that CaDDi outperforms state-of-the-art discrete diffusion models on both natural language and biological sequence tasks, narrowing the gap between diffusion-based methods and large-scale autoregressive transformers.

### Differential Adjusted Parity for Learning Fair Representations 
[[arxiv](https://arxiv.org/abs/2502.09765)] [[cool](https://papers.cool/arxiv/2502.09765)] [[pdf](https://arxiv.org/pdf/2502.09765)]
> **Authors**: Bucher Sahyouni,Matthew Vowels,Liqun Chen,Simon Hadfield
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: The development of fair and unbiased machine learning models remains an ongoing objective for researchers in the field of artificial intelligence. We introduce the Differential Adjusted Parity (DAP) loss to produce unbiased informative representations. It utilises a differentiable variant of the adjusted parity metric to create a unified objective function. By combining downstream task classification accuracy and its inconsistency across sensitive feature domains, it provides a single tool to increase performance and mitigate bias. A key element in this approach is the use of soft balanced accuracies. In contrast to previous non-adversarial approaches, DAP does not suffer a degeneracy where the metric is satisfied by performing equally poorly across all sensitive domains. It outperforms several adversarial models on downstream task accuracy and fairness in our analysis. Specifically, it improves the demographic parity, equalized odds and sensitive feature accuracy by as much as 22.5\%, 44.1\% and 40.1\%, respectively, when compared to the best performing adversarial approaches on these metrics. Overall, the DAP loss and its associated metric can play a significant role in creating more fair machine learning models.

### Fine-Tuning Foundation Models with Federated Learning for Privacy Preserving Medical Time Series Forecasting 
[[arxiv](https://arxiv.org/abs/2502.09744)] [[cool](https://papers.cool/arxiv/2502.09744)] [[pdf](https://arxiv.org/pdf/2502.09744)]
> **Authors**: Mahad Ali,Curtis Lisle,Patrick W. Moore,Tammer Barkouki,Brian J. Kirkwood,Laura J. Brattain
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: submitted to IEEE EMBC 2025; 7 pages, 4 figures
- **标题**: None
- **领域**: 机器学习,密码学和安全
- **Abstract**: Federated Learning (FL) provides a decentralized machine learning approach, where multiple devices or servers collaboratively train a model without sharing their raw data, thus enabling data privacy. This approach has gained significant interest in academia and industry due to its privacy-preserving properties, which are particularly valuable in the medical domain where data availability is often protected under strict regulations. A relatively unexplored area is the use of FL to fine-tune Foundation Models (FMs) for time series forecasting, potentially enhancing model efficacy by overcoming data limitation while maintaining privacy. In this paper, we fine-tuned time series FMs with Electrocardiogram (ECG) and Impedance Cardiography (ICG) data using different FL techniques. We then examined various scenarios and discussed the challenges FL faces under different data heterogeneity configurations. Our empirical results demonstrated that while FL can be effective for fine-tuning FMs on time series forecasting tasks, its benefits depend on the data distribution across clients. We highlighted the trade-offs in applying FL to FM fine-tuning.

### Navigating the Social Welfare Frontier: Portfolios for Multi-objective Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.09724)] [[cool](https://papers.cool/arxiv/2502.09724)] [[pdf](https://arxiv.org/pdf/2502.09724)]
> **Authors**: Cheol Woo Kim,Jai Moondra,Shresth Verma,Madeleine Pollack,Lingkai Kong,Milind Tambe,Swati Gupta
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: In many real-world applications of reinforcement learning (RL), deployed policies have varied impacts on different stakeholders, creating challenges in reaching consensus on how to effectively aggregate their preferences. Generalized $p$-means form a widely used class of social welfare functions for this purpose, with broad applications in fair resource allocation, AI alignment, and decision-making. This class includes well-known welfare functions such as Egalitarian, Nash, and Utilitarian welfare. However, selecting the appropriate social welfare function is challenging for decision-makers, as the structure and outcomes of optimal policies can be highly sensitive to the choice of $p$. To address this challenge, we study the concept of an $α$-approximate portfolio in RL, a set of policies that are approximately optimal across the family of generalized $p$-means for all $p \in [-\infty, 1]$. We propose algorithms to compute such portfolios and provide theoretical guarantees on the trade-offs among approximation factor, portfolio size, and computational efficiency. Experimental results on synthetic and real-world datasets demonstrate the effectiveness of our approach in summarizing the policy space induced by varying $p$ values, empowering decision-makers to navigate this landscape more effectively.

### NestQuant: Nested Lattice Quantization for Matrix Products and LLMs 
[[arxiv](https://arxiv.org/abs/2502.09720)] [[cool](https://papers.cool/arxiv/2502.09720)] [[pdf](https://arxiv.org/pdf/2502.09720)]
> **Authors**: Semyon Savkin,Eitan Porat,Or Ordentlich,Yury Polyanskiy
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 16 pages
- **标题**: None
- **领域**: 机器学习,人工智能,信息论
- **Abstract**: Post-training quantization (PTQ) has emerged as a critical technique for efficient deployment of large language models (LLMs). This work proposes NestQuant, a novel PTQ scheme for weights and activations that is based on self-similar nested lattices. Recent work have mathematically shown such quantizers to be information-theoretically optimal for low-precision matrix multiplication. We implement a practical low-complexity version of NestQuant based on Gosset lattice, making it a drop-in quantizer for any matrix multiplication step (e.g., in self-attention, MLP etc). For example, NestQuant quantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving perplexity of 6.6 on wikitext2. This represents more than 55% reduction in perplexity gap with respect to unquantized model (perplexity of 6.14) compared to state-of-the-art Meta's SpinQuant (perplexity 7.3). Comparisons on various LLM evaluation benchmarks also show a reduction in performance degradation induced by quantization.

### Evaluating GPT's Capability in Identifying Stages of Cognitive Impairment from Electronic Health Data 
[[arxiv](https://arxiv.org/abs/2502.09715)] [[cool](https://papers.cool/arxiv/2502.09715)] [[pdf](https://arxiv.org/pdf/2502.09715)]
> **Authors**: Yu Leng,Yingnan He,Colin Magdamo,Ana-Maria Vranceanu,Christine S. Ritchie,Shibani S. Mukerji,Lidia M. V. R. Moura,John R. Dickson,Deborah Blacker,Sudeshna Das
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: Findings paper presented atMachineLearningfor Health (ML4H) symposium 2024, December 15-16, 2024, Vancouver, Canada, 7 pages
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Identifying cognitive impairment within electronic health records (EHRs) is crucial not only for timely diagnoses but also for facilitating research. Information about cognitive impairment often exists within unstructured clinician notes in EHRs, but manual chart reviews are both time-consuming and error-prone. To address this issue, our study evaluates an automated approach using zero-shot GPT-4o to determine stage of cognitive impairment in two different tasks. First, we evaluated the ability of GPT-4o to determine the global Clinical Dementia Rating (CDR) on specialist notes from 769 patients who visited the memory clinic at Massachusetts General Hospital (MGH), and achieved a weighted kappa score of 0.83. Second, we assessed GPT-4o's ability to differentiate between normal cognition, mild cognitive impairment (MCI), and dementia on all notes in a 3-year window from 860 Medicare patients. GPT-4o attained a weighted kappa score of 0.91 in comparison to specialist chart reviews and 0.96 on cases that the clinical adjudicators rated with high confidence. Our findings demonstrate GPT-4o's potential as a scalable chart review tool for creating research datasets and assisting diagnosis in clinical settings in the future.

### NeuralCFD: Deep Learning on High-Fidelity Automotive Aerodynamics Simulations 
[[arxiv](https://arxiv.org/abs/2502.09692)] [[cool](https://papers.cool/arxiv/2502.09692)] [[pdf](https://arxiv.org/pdf/2502.09692)]
> **Authors**: Maurits Bleeker,Matthias Dorfer,Tobias Kronlachner,Reinhard Sonnleitner,Benedikt Alkin,Johannes Brandstetter
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: Preprint
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Recent advancements in neural operator learning are paving the way for transformative innovations in fields such as automotive aerodynamics. However, key challenges must be overcome before neural network-based simulation surrogates can be implemented at an industry scale. First, surrogates must become scalable to large surface and volume meshes, especially when using raw geometry inputs only, i.e., without relying on the simulation mesh. Second, surrogates must be trainable with a limited number of high-fidelity numerical simulation samples while still reaching the required performance levels. To this end, we introduce Geometry-preserving Universal Physics Transformer (GP-UPT), which separates geometry encoding and physics predictions, ensuring flexibility with respect to geometry representations and surface sampling strategies. GP-UPT enables independent scaling of the respective parts of the model according to practical requirements, offering scalable solutions to open challenges. GP-UPT circumvents the creation of high-quality simulation meshes, enables accurate 3D velocity field predictions at 20 million mesh cells, and excels in transfer learning from low-fidelity to high-fidelity simulation datasets, requiring less than half of the high-fidelity data to match the performance of models trained from scratch.

### Leveraging Machine Learning and Deep Learning Techniques for Improved Pathological Staging of Prostate Cancer 
[[arxiv](https://arxiv.org/abs/2502.09686)] [[cool](https://papers.cool/arxiv/2502.09686)] [[pdf](https://arxiv.org/pdf/2502.09686)]
> **Authors**: Raziehsadat Ghalamkarian,Marziehsadat Ghalamkarian,MortezaAli Ahmadi,Sayed Mohammad Ahmadi,Abolfazl Diyanat
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Prostate cancer (Pca) continues to be a leading cause of cancer-related mortality in men, and the limitations in precision of traditional diagnostic methods such as the Digital Rectal Exam (DRE), Prostate-Specific Antigen (PSA) testing, and biopsies underscore the critical importance of accurate staging detection in enhancing treatment outcomes and improving patient prognosis. This study leverages machine learning and deep learning approaches, along with feature selection and extraction methods, to enhance PCa pathological staging predictions using RNA sequencing data from The Cancer Genome Atlas (TCGA). Gene expression profiles from 486 tumors were analyzed using advanced algorithms, including Random Forest (RF), Logistic Regression (LR), Extreme Gradient Boosting (XGB), and Support Vector Machine (SVM). The performance of the study is measured with respect to the F1-score, as well as precision and recall, all of which are calculated as weighted averages. The results reveal that the highest test F1-score, approximately 83%, was achieved by the Random Forest algorithm, followed by Logistic Regression at 80%, while both Extreme Gradient Boosting (XGB) and Support Vector Machine (SVM) scored around 79%. Furthermore, deep learning models with data augmentation achieved an accuracy of 71. 23%, while PCA-based dimensionality reduction reached an accuracy of 69.86%. This research highlights the potential of AI-driven approaches in clinical oncology, paving the way for more reliable diagnostic tools that can ultimately improve patient outcomes.

### A Novel Hybrid Approach to Contraceptive Demand Forecasting: Integrating Point Predictions with Probabilistic Distributions 
[[arxiv](https://arxiv.org/abs/2502.09685)] [[cool](https://papers.cool/arxiv/2502.09685)] [[pdf](https://arxiv.org/pdf/2502.09685)]
> **Authors**: Harsha Chamara Hewage,Bahman Rostami-Tabar,Aris Syntetos,Federico Liberatore,Glenn Milano
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: :90-05
- **标题**: None
- **领域**: 机器学习,应用领域,方法论
- **Abstract**: Accurate demand forecasting is vital for ensuring reliable access to contraceptive products, supporting key processes like procurement, inventory, and distribution. However, forecasting contraceptive demand in developing countries presents challenges, including incomplete data, poor data quality, and the need to account for multiple geographical and product factors. Current methods often rely on simple forecasting techniques, which fail to capture demand uncertainties arising from these factors, warranting expert involvement. Our study aims to improve contraceptive demand forecasting by combining probabilistic forecasting methods with expert knowledge. We developed a hybrid model that combines point forecasts from domain-specific model with probabilistic distributions from statistical and machine learning approaches, enabling human input to fine-tune and enhance the system-generated forecasts. This approach helps address the uncertainties in demand and is particularly useful in resource-limited settings. We evaluate different forecasting methods, including time series, Bayesian, machine learning, and foundational time series methods alongside our new hybrid approach. By comparing these methods, we provide insights into their strengths, weaknesses, and computational requirements. Our research fills a gap in forecasting contraceptive demand and offers a practical framework that combines algorithmic and human expertise. Our proposed model can also be generalized to other humanitarian contexts with similar data patterns.

### Channel Dependence, Limited Lookback Windows, and the Simplicity of Datasets: How Biased is Time Series Forecasting? 
[[arxiv](https://arxiv.org/abs/2502.09683)] [[cool](https://papers.cool/arxiv/2502.09683)] [[pdf](https://arxiv.org/pdf/2502.09683)]
> **Authors**: Ibram Abdelmalak,Kiran Madhusudhanan,Jungmin Choi,Maximilian Stubbemann,Lars Schmidt-Thieme
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Time-series forecasting research has converged to a small set of datasets and a standardized collection of evaluation scenarios. Such a standardization is to a specific extent needed for comparable research. However, the underlying assumption is, that the considered setting is a representative for the problem as a whole. In this paper, we challenge this assumption and show that the current scenario gives a strongly biased perspective on the state of time-series forecasting research. To be more detailed, we show that the current evaluation scenario is heavily biased by the simplicity of the current datasets. We furthermore emphasize, that when the lookback-window is properly tuned, current models usually do not need any information flow across channels. However, when using more complex benchmark data, the situation changes: Here, modeling channel-interactions in a sophisticated manner indeed enhances performances. Furthermore, in this complex evaluation scenario, Crossformer, a method regularly neglected as an important baseline, is the SOTA method for time series forecasting. Based on this, we present the Fast Channel-dependent Transformer (FaCT), a simplified version of Crossformer which closes the runtime gap between Crossformer and TimeMixer, leading to an efficient model for complex forecasting datasets.

### Theoretical Benefit and Limitation of Diffusion Language Model 
[[arxiv](https://arxiv.org/abs/2502.09622)] [[cool](https://papers.cool/arxiv/2502.09622)] [[pdf](https://arxiv.org/pdf/2502.09622)]
> **Authors**: Guhao Feng,Yihan Geng,Jian Guan,Wei Wu,Liwei Wang,Di He
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 32 pages, 3 figures
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,机器学习
- **Abstract**: Diffusion language models have emerged as a promising approach for text generation. One would naturally expect this method to be an efficient replacement for autoregressive models since multiple tokens can be sampled in parallel during each diffusion step. However, its efficiency-accuracy trade-off is not yet well understood. In this paper, we present a rigorous theoretical analysis of a widely used type of diffusion language model, the Masked Diffusion Model (MDM), and find that its effectiveness heavily depends on the target evaluation metric. Under mild conditions, we prove that when using perplexity as the metric, MDMs can achieve near-optimal perplexity in sampling steps regardless of sequence length, demonstrating that efficiency can be achieved without sacrificing performance. However, when using the sequence error rate--which is important for understanding the "correctness" of a sequence, such as a reasoning chain--we show that the required sampling steps must scale linearly with sequence length to obtain "correct" sequences, thereby eliminating MDM's efficiency advantage over autoregressive models. Our analysis establishes the first theoretical foundation for understanding the benefits and limitations of MDMs. All theoretical findings are supported by empirical studies.

### Can this Model Also Recognize Dogs? Zero-Shot Model Search from Weights 
[[arxiv](https://arxiv.org/abs/2502.09619)] [[cool](https://papers.cool/arxiv/2502.09619)] [[pdf](https://arxiv.org/pdf/2502.09619)]
> **Authors**: Jonathan Kahana,Or Nathan,Eliahu Horwitz,Yedid Hoshen
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: With the increasing numbers of publicly available models, there are probably pretrained, online models for most tasks users require. However, current model search methods are rudimentary, essentially a text-based search in the documentation, thus users cannot find the relevant models. This paper presents ProbeLog, a method for retrieving classification models that can recognize a target concept, such as "Dog", without access to model metadata or training data. Differently from previous probing methods, ProbeLog computes a descriptor for each output dimension (logit) of each model, by observing its responses on a fixed set of inputs (probes). Our method supports both logit-based retrieval ("find more logits like this") and zero-shot, text-based retrieval ("find all logits corresponding to dogs"). As probing-based representations require multiple costly feedforward passes through the model, we develop a method, based on collaborative filtering, that reduces the cost of encoding repositories by 3x. We demonstrate that ProbeLog achieves high retrieval accuracy, both in real-world and fine-grained search tasks and is scalable to full-size repositories.

### Variational Rectified Flow Matching 
[[arxiv](https://arxiv.org/abs/2502.09616)] [[cool](https://papers.cool/arxiv/2502.09616)] [[pdf](https://arxiv.org/pdf/2502.09616)]
> **Authors**: Pengsheng Guo,Alexander G. Schwing
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: We study Variational Rectified Flow Matching, a framework that enhances classic rectified flow matching by modeling multi-modal velocity vector-fields. At inference time, classic rectified flow matching 'moves' samples from a source distribution to the target distribution by solving an ordinary differential equation via integration along a velocity vector-field. At training time, the velocity vector-field is learnt by linearly interpolating between coupled samples one drawn from the source and one drawn from the target distribution randomly. This leads to ''ground-truth'' velocity vector-fields that point in different directions at the same location, i.e., the velocity vector-fields are multi-modal/ambiguous. However, since training uses a standard mean-squared-error loss, the learnt velocity vector-field averages ''ground-truth'' directions and isn't multi-modal. In contrast, variational rectified flow matching learns and samples from multi-modal flow directions. We show on synthetic data, MNIST, CIFAR-10, and ImageNet that variational rectified flow matching leads to compelling results.

### Designing a Conditional Prior Distribution for Flow-Based Generative Models 
[[arxiv](https://arxiv.org/abs/2502.09611)] [[cool](https://papers.cool/arxiv/2502.09611)] [[pdf](https://arxiv.org/pdf/2502.09611)]
> **Authors**: Noam Issachar,Mohammad Salama,Raanan Fattal,Sagie Benaim
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: Flow-based generative models have recently shown impressive performance for conditional generation tasks, such as text-to-image generation. However, current methods transform a general unimodal noise distribution to a specific mode of the target data distribution. As such, every point in the initial source distribution can be mapped to every point in the target distribution, resulting in long average paths. To this end, in this work, we tap into a non-utilized property of conditional flow-based models: the ability to design a non-trivial prior distribution. Given an input condition, such as a text prompt, we first map it to a point lying in data space, representing an ``average" data point with the minimal average distance to all data points of the same conditional mode (e.g., class). We then utilize the flow matching formulation to map samples from a parametric distribution centered around this point to the conditional target distribution. Experimentally, our method significantly improves training times and generation efficiency (FID, KID and CLIP alignment scores) compared to baselines, producing high quality samples using fewer sampling steps.

### Score-of-Mixture Training: Training One-Step Generative Models Made Simple via Score Estimation of Mixture Distributions 
[[arxiv](https://arxiv.org/abs/2502.09609)] [[cool](https://papers.cool/arxiv/2502.09609)] [[pdf](https://arxiv.org/pdf/2502.09609)]
> **Authors**: Tejas Jayashankar,J. Jon Ryu,Gregory Wornell
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 27 pages, 9 figures. Title updated to match the title of the manuscript, otherwise identical to v1
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: We propose Score-of-Mixture Training (SMT), a novel framework for training one-step generative models by minimizing a class of divergences called the $α$-skew Jensen-Shannon divergence. At its core, SMT estimates the score of mixture distributions between real and fake samples across multiple noise levels. Similar to consistency models, our approach supports both training from scratch (SMT) and distillation using a pretrained diffusion model, which we call Score-of-Mixture Distillation (SMD). It is simple to implement, requires minimal hyperparameter tuning, and ensures stable training. Experiments on CIFAR-10 and ImageNet 64x64 show that SMT/SMD are competitive with and can even outperform existing methods.

### Do LLMs Recognize Your Preferences? Evaluating Personalized Preference Following in LLMs 
[[arxiv](https://arxiv.org/abs/2502.09597)] [[cool](https://papers.cool/arxiv/2502.09597)] [[pdf](https://arxiv.org/pdf/2502.09597)]
> **Authors**: Siyan Zhao,Mingyi Hong,Yang Liu,Devamanyu Hazarika,Kaixiang Lin
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: Accepted at ICLR 2025 as oral presentation. Code and data at: https://prefeval.github.io/
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Large Language Models (LLMs) are increasingly used as chatbots, yet their ability to personalize responses to user preferences remains limited. We introduce PrefEval, a benchmark for evaluating LLMs' ability to infer, memorize and adhere to user preferences in a long-context conversational setting. PrefEval comprises 3,000 manually curated user preference and query pairs spanning 20 topics. PrefEval contains user personalization or preference information in both explicit and implicit forms, and evaluates LLM performance using a generation and a classification task. With PrefEval, we evaluated the aforementioned preference following capabilities of 10 open-source and proprietary LLMs in multi-session conversations with varying context lengths up to 100k tokens. We benchmark with various prompting, iterative feedback, and retrieval-augmented generation methods. Our benchmarking effort reveals that state-of-the-art LLMs face significant challenges in proactively following users' preferences during conversations. In particular, in zero-shot settings, preference following accuracy falls below 10% at merely 10 turns (~3k tokens) across most evaluated models. Even with advanced prompting and retrieval methods, preference following still deteriorates in long-context conversations. Furthermore, we show that fine-tuning on PrefEval significantly improves performance. We believe PrefEval serves as a valuable resource for measuring, understanding, and enhancing LLMs' preference following abilities, paving the way for personalized conversational agents. Our code and dataset are available at https://prefeval.github.io/.

### Censor Dependent Variational Inference 
[[arxiv](https://arxiv.org/abs/2502.09591)] [[cool](https://papers.cool/arxiv/2502.09591)] [[pdf](https://arxiv.org/pdf/2502.09591)]
> **Authors**: Chuanhui Liu,Xiao Wang
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: This paper provides a comprehensive analysis of variational inference in latent variable models for survival analysis, emphasizing the distinctive challenges associated with applying variational methods to survival data. We identify a critical weakness in the existing methodology, demonstrating how a poorly designed variational distribution may hinder the objective of survival analysis tasks--modeling time-to-event distributions. We prove that the optimal variational distribution, which perfectly bounds the log-likelihood, may depend on the censoring mechanism. To address this issue, we propose censor-dependent variational inference (CDVI), tailored for latent variable models in survival analysis. More practically, we introduce CD-CVAE, a V-structure Variational Autoencoder (VAE) designed for the scalable implementation of CDVI. Further discussion extends some existing theories and training techniques to survival analysis. Extensive experiments validate our analysis and demonstrate significant improvements in the estimation of individual survival distributions.

### Rolling Ahead Diffusion for Traffic Scene Simulation 
[[arxiv](https://arxiv.org/abs/2502.09587)] [[cool](https://papers.cool/arxiv/2502.09587)] [[pdf](https://arxiv.org/pdf/2502.09587)]
> **Authors**: Yunpeng Liu,Matthew Niedoba,William Harvey,Adam Scibior,Berend Zwartsenberg,Frank Wood
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: Accepted to Workshop onMachineLearningfor Autonomous Driving at AAAI 2025
- **标题**: None
- **领域**: 机器学习,机器人技术
- **Abstract**: Realistic driving simulation requires that NPCs not only mimic natural driving behaviors but also react to the behavior of other simulated agents. Recent developments in diffusion-based scenario generation focus on creating diverse and realistic traffic scenarios by jointly modelling the motion of all the agents in the scene. However, these traffic scenarios do not react when the motion of agents deviates from their modelled trajectories. For example, the ego-agent can be controlled by a stand along motion planner. To produce reactive scenarios with joint scenario models, the model must regenerate the scenario at each timestep based on new observations in a Model Predictive Control (MPC) fashion. Although reactive, this method is time-consuming, as one complete possible future for all NPCs is generated per simulation step. Alternatively, one can utilize an autoregressive model (AR) to predict only the immediate next-step future for all NPCs. Although faster, this method lacks the capability for advanced planning. We present a rolling diffusion based traffic scene generation model which mixes the benefits of both methods by predicting the next step future and simultaneously predicting partially noised further future steps at the same time. We show that such model is efficient compared to diffusion model based AR, achieving a beneficial compromise between reactivity and computational efficiency.

### Learning to Coordinate with Experts 
[[arxiv](https://arxiv.org/abs/2502.09583)] [[cool](https://papers.cool/arxiv/2502.09583)] [[pdf](https://arxiv.org/pdf/2502.09583)]
> **Authors**: Mohamad H. Danesh,Tu Trinh,Benjamin Plaut,Nguyen X. Khanh
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: When deployed in dynamic environments, AI agents will inevitably encounter challenges that exceed their individual capabilities. Leveraging assistance from expert agents-whether human or AI-can significantly enhance safety and performance in such situations. However, querying experts is often costly, necessitating the development of agents that can efficiently request and utilize expert guidance. In this paper, we introduce a fundamental coordination problem called Learning to Yield and Request Control (YRC), where the objective is to learn a strategy that determines when to act autonomously and when to seek expert assistance. We consider a challenging practical setting in which an agent does not interact with experts during training but must adapt to novel environmental changes and expert interventions at test time. To facilitate empirical research, we introduce YRC-Bench, an open-source benchmark featuring diverse domains. YRC-Bench provides a standardized Gym-like API, simulated experts, evaluation pipeline, and implementation of competitive baselines. Towards tackling the YRC problem, we propose a novel validation approach and investigate the performance of various learning methods across diverse environments, yielding insights that can guide future research.

### DiffMS: Diffusion Generation of Molecules Conditioned on Mass Spectra 
[[arxiv](https://arxiv.org/abs/2502.09571)] [[cool](https://papers.cool/arxiv/2502.09571)] [[pdf](https://arxiv.org/pdf/2502.09571)]
> **Authors**: Montgomery Bohde,Mrunali Manjrekar,Runzhong Wang,Shuiwang Ji,Connor W. Coley
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: Preprint
- **标题**: None
- **领域**: 机器学习,定量方法
- **Abstract**: Mass spectrometry plays a fundamental role in elucidating the structures of unknown molecules and subsequent scientific discoveries. One formulation of the structure elucidation task is the conditional $\textit{de novo}$ generation of molecular structure given a mass spectrum. Toward a more accurate and efficient scientific discovery pipeline for small molecules, we present DiffMS, a formula-restricted encoder-decoder generative network that achieves state-of-the-art performance on this task. The encoder utilizes a transformer architecture and models mass spectra domain knowledge such as peak formulae and neutral losses, and the decoder is a discrete graph diffusion model restricted by the heavy-atom composition of a known chemical formula. To develop a robust decoder that bridges latent embeddings and molecular structures, we pretrain the diffusion decoder with fingerprint-structure pairs, which are available in virtually infinite quantities, compared to structure-spectrum pairs that number in the tens of thousands. Extensive experiments on established benchmarks show that DiffMS outperforms existing models on $\textit{de novo}$ molecule generation. We provide several ablations to demonstrate the effectiveness of our diffusion and pretraining approaches and show consistent performance scaling with increasing pretraining dataset size. DiffMS code is publicly available at https://github.com/coleygroup/DiffMS.

### Enhancing the Utility of Higher-Order Information in Relational Learning 
[[arxiv](https://arxiv.org/abs/2502.09570)] [[cool](https://papers.cool/arxiv/2502.09570)] [[pdf](https://arxiv.org/pdf/2502.09570)]
> **Authors**: Raphael Pellegrin,Lukas Fesser,Melanie Weber
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Higher-order information is crucial for relational learning in many domains where relationships extend beyond pairwise interactions. Hypergraphs provide a natural framework for modeling such relationships, which has motivated recent extensions of graph neural network architectures to hypergraphs. However, comparisons between hypergraph architectures and standard graph-level models remain limited. In this work, we systematically evaluate a selection of hypergraph-level and graph-level architectures, to determine their effectiveness in leveraging higher-order information in relational learning. Our results show that graph-level architectures applied to hypergraph expansions often outperform hypergraph-level ones, even on inputs that are naturally parametrized as hypergraphs. As an alternative approach for leveraging higher-order information, we propose hypergraph-level encodings based on classical hypergraph characteristics. While these encodings do not significantly improve hypergraph architectures, they yield substantial performance gains when combined with graph-level models. Our theoretical analysis shows that hypergraph-level encodings provably increase the representational power of message-passing graph neural networks beyond that of their graph-level counterparts.

### Diffusing DeBias: a Recipe for Turning a Bug into a Feature 
[[arxiv](https://arxiv.org/abs/2502.09564)] [[cool](https://papers.cool/arxiv/2502.09564)] [[pdf](https://arxiv.org/pdf/2502.09564)]
> **Authors**: Massimiliano Ciranni,Vito Paolo Pastore,Roberto Di Via,Enzo Tartaglione,Francesca Odone,Vittorio Murino
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 29 Pages, 12 Figures
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: Deep learning model effectiveness in classification tasks is often challenged by the quality and quantity of training data which, whenever containing strong spurious correlations between specific attributes and target labels, can result in unrecoverable biases in model predictions. Tackling these biases is crucial in improving model generalization and trust, especially in real-world scenarios. This paper presents Diffusing DeBias (DDB), a novel approach acting as a plug-in for common methods in model debiasing while exploiting the inherent bias-learning tendency of diffusion models. Our approach leverages conditional diffusion models to generate synthetic bias-aligned images, used to train a bias amplifier model, to be further employed as an auxiliary method in different unsupervised debiasing approaches. Our proposed method, which also tackles the common issue of training set memorization typical of this type of techniques, beats current state-of-the-art in multiple benchmark datasets by significant margins, demonstrating its potential as a versatile and effective tool for tackling dataset bias in deep learning applications.

### Robust Learning of Multi-index Models via Iterative Subspace Approximation 
[[arxiv](https://arxiv.org/abs/2502.09525)] [[cool](https://papers.cool/arxiv/2502.09525)] [[pdf](https://arxiv.org/pdf/2502.09525)]
> **Authors**: Ilias Diakonikolas,Giannis Iakovidis,Daniel M. Kane,Nikos Zarifis
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,数据结构和算法,统计理论,机器学习
- **Abstract**: We study the task of learning Multi-Index Models (MIMs) with label noise under the Gaussian distribution. A $K$-MIM is any function $f$ that only depends on a $K$-dimensional subspace. We focus on well-behaved MIMs with finite ranges that satisfy certain regularity properties. Our main contribution is a general robust learner that is qualitatively optimal in the Statistical Query (SQ) model. Our algorithm iteratively constructs better approximations to the defining subspace by computing low-degree moments conditional on the projection to the subspace computed thus far, and adding directions with relatively large empirical moments. This procedure efficiently finds a subspace $V$ so that $f(\mathbf{x})$ is close to a function of the projection of $\mathbf{x}$ onto $V$. Conversely, for functions for which these conditional moments do not help, we prove an SQ lower bound suggesting that no efficient learner exists. As applications, we provide faster robust learners for the following concept classes: * {\bf Multiclass Linear Classifiers} We give a constant-factor approximate agnostic learner with sample complexity $N = O(d) 2^{\mathrm{poly}(K/ε)}$ and computational complexity $\mathrm{poly}(N ,d)$. This is the first constant-factor agnostic learner for this class whose complexity is a fixed-degree polynomial in $d$. * {\bf Intersections of Halfspaces} We give an approximate agnostic learner for this class achieving 0-1 error $K \tilde{O}(\mathrm{OPT}) + ε$ with sample complexity $N=O(d^2) 2^{\mathrm{poly}(K/ε)}$ and computational complexity $\mathrm{poly}(N ,d)$. This is the first agnostic learner for this class with near-linear error dependence and complexity a fixed-degree polynomial in $d$. Furthermore, we show that in the presence of random classification noise, the complexity of our algorithm scales polynomially with $1/ε$.

### Diffusion Models for Molecules: A Survey of Methods and Tasks 
[[arxiv](https://arxiv.org/abs/2502.09511)] [[cool](https://papers.cool/arxiv/2502.09511)] [[pdf](https://arxiv.org/pdf/2502.09511)]
> **Authors**: Liang Wang,Chao Song,Zhiyuan Liu,Yu Rong,Qiang Liu,Shu Wu,Liang Wang
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算工程、金融和科学
- **Abstract**: Generative tasks about molecules, including but not limited to molecule generation, are crucial for drug discovery and material design, and have consistently attracted significant attention. In recent years, diffusion models have emerged as an impressive class of deep generative models, sparking extensive research and leading to numerous studies on their application to molecular generative tasks. Despite the proliferation of related work, there remains a notable lack of up-to-date and systematic surveys in this area. Particularly, due to the diversity of diffusion model formulations, molecular data modalities, and generative task types, the research landscape is challenging to navigate, hindering understanding and limiting the area's growth. To address this, this paper conducts a comprehensive survey of diffusion model-based molecular generative methods. We systematically review the research from the perspectives of methodological formulations, data modalities, and task types, offering a novel taxonomy. This survey aims to facilitate understanding and further flourishing development in this area. The relevant papers are summarized at: https://github.com/AzureLeon1/awesome-molecular-diffusion-models.

### EQ-VAE: Equivariance Regularized Latent Space for Improved Generative Image Modeling 
[[arxiv](https://arxiv.org/abs/2502.09509)] [[cool](https://papers.cool/arxiv/2502.09509)] [[pdf](https://arxiv.org/pdf/2502.09509)]
> **Authors**: Theodoros Kouzelis,Ioannis Kakogeorgiou,Spyros Gidaris,Nikos Komodakis
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: Preprint
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Latent generative models have emerged as a leading approach for high-quality image synthesis. These models rely on an autoencoder to compress images into a latent space, followed by a generative model to learn the latent distribution. We identify that existing autoencoders lack equivariance to semantic-preserving transformations like scaling and rotation, resulting in complex latent spaces that hinder generative performance. To address this, we propose EQ-VAE, a simple regularization approach that enforces equivariance in the latent space, reducing its complexity without degrading reconstruction quality. By finetuning pre-trained autoencoders with EQ-VAE, we enhance the performance of several state-of-the-art generative models, including DiT, SiT, REPA and MaskGIT, achieving a 7 speedup on DiT-XL/2 with only five epochs of SD-VAE fine-tuning. EQ-VAE is compatible with both continuous and discrete autoencoders, thus offering a versatile enhancement for a wide range of latent generative models. Project page and code: https://eq-vae.github.io/.

### When and How Does CLIP Enable Domain and Compositional Generalization? 
[[arxiv](https://arxiv.org/abs/2502.09507)] [[cool](https://papers.cool/arxiv/2502.09507)] [[pdf](https://arxiv.org/pdf/2502.09507)]
> **Authors**: Elias Kempf,Simon Schrodi,Max Argus,Thomas Brox
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: The remarkable generalization performance of contrastive vision-language models like CLIP is often attributed to the diversity of their training distributions. However, key questions remain unanswered: Can CLIP generalize to an entirely unseen domain when trained on a diverse mixture of domains (domain generalization)? Can it generalize to unseen classes within partially seen domains (compositional generalization)? What factors affect such generalization? To answer these questions, we trained CLIP models on systematically constructed training distributions with controlled domain diversity and object class exposure. Our experiments show that domain diversity is essential for both domain and compositional generalization, yet compositional generalization can be surprisingly weaker than domain generalization when the training distribution contains a suboptimal subset of the test domain. Through data-centric and mechanistic analyses, we find that successful generalization requires learning of shared representations already in intermediate layers and shared circuitry.

### AttentionSmithy: A Modular Framework for Rapid Transformer Development and Customization 
[[arxiv](https://arxiv.org/abs/2502.09503)] [[cool](https://papers.cool/arxiv/2502.09503)] [[pdf](https://arxiv.org/pdf/2502.09503)]
> **Authors**: Caleb Cranney,Jesse G. Meyer
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Transformer architectures have transformed AI applications but remain complex to customize for domain experts lacking low-level implementation expertise. We introduce AttentionSmithy, a modular software package that simplifies transformer innovation by breaking down key components into reusable building blocks: attention modules, feed-forward networks, normalization layers, and positional encodings. Users can rapidly prototype and evaluate transformer variants without extensive coding. Our framework supports four positional encoding strategies and integrates with neural architecture search for automated design. We validate AttentionSmithy by replicating the original transformer under resource constraints and optimizing translation performance by combining positional encodings. Additionally, we demonstrate its adaptability in gene-specific modeling, achieving over 95% accuracy in cell type classification. These case studies highlight AttentionSmithy's potential to accelerate research across diverse fields by removing framework implementation barriers.

### Scalable First-order Method for Certifying Optimal k-Sparse GLMs 
[[arxiv](https://arxiv.org/abs/2502.09502)] [[cool](https://papers.cool/arxiv/2502.09502)] [[pdf](https://arxiv.org/pdf/2502.09502)]
> **Authors**: Jiachang Liu,Soroosh Shafiee,Andrea Lodi
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,优化与控制
- **Abstract**: This paper investigates the problem of certifying optimality for sparse generalized linear models (GLMs), where sparsity is enforced through an $\ell_0$ cardinality constraint. While branch-and-bound (BnB) frameworks can certify optimality by pruning nodes using dual bounds, existing methods for computing these bounds are either computationally intensive or exhibit slow convergence, limiting their scalability to large-scale problems. To address this challenge, we propose a first-order proximal gradient algorithm designed to solve the perspective relaxation of the problem within a BnB framework. Specifically, we formulate the relaxed problem as a composite optimization problem and demonstrate that the proximal operator of the non-smooth component can be computed exactly in log-linear time complexity, eliminating the need to solve a computationally expensive second-order cone program. Furthermore, we introduce a simple restart strategy that enhances convergence speed while maintaining low per-iteration complexity. Extensive experiments on synthetic and real-world datasets show that our approach significantly accelerates dual bound computations and is highly effective in providing optimality certificates for large-scale problems.

### Eidetic Learning: an Efficient and Provable Solution to Catastrophic Forgetting 
[[arxiv](https://arxiv.org/abs/2502.09500)] [[cool](https://papers.cool/arxiv/2502.09500)] [[pdf](https://arxiv.org/pdf/2502.09500)]
> **Authors**: Nicholas Dronen,Randall Balestriero
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 16 pages, 6 figures; code is available at https://github.com/amazon-science/eideticnet-training
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Catastrophic forgetting -- the phenomenon of a neural network learning a task t1 and losing the ability to perform it after being trained on some other task t2 -- is a long-standing problem for neural networks [McCloskey and Cohen, 1989]. We present a method, Eidetic Learning, that provably solves catastrophic forgetting. A network trained with Eidetic Learning -- here, an EideticNet -- requires no rehearsal or replay. We consider successive discrete tasks and show how at inference time an EideticNet automatically routes new instances without auxiliary task information. An EideticNet bears a family resemblance to the sparsely-gated Mixture-of-Experts layer Shazeer et al. [2016] in that network capacity is partitioned across tasks and the network itself performs data-conditional routing. An EideticNet is easy to implement and train, is efficient, and has time and space complexity linear in the number of parameters. The guarantee of our method holds for normalization layers of modern neural networks during both pre-training and fine-tuning. We show with a variety of network architectures and sets of tasks that EideticNets are immune to forgetting. While the practical benefits of EideticNets are substantial, we believe they can be benefit practitioners and theorists alike. The code for training EideticNets is available at https://github.com/amazon-science/eideticnet-training.

### On Agnostic PAC Learning in the Small Error Regime 
[[arxiv](https://arxiv.org/abs/2502.09496)] [[cool](https://papers.cool/arxiv/2502.09496)] [[pdf](https://arxiv.org/pdf/2502.09496)]
> **Authors**: Julian Asilis,Mikael Møller Høgsgaard,Grigoris Velegkas
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 44 pages
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Binary classification in the classic PAC model exhibits a curious phenomenon: Empirical Risk Minimization (ERM) learners are suboptimal in the realizable case yet optimal in the agnostic case. Roughly speaking, this owes itself to the fact that non-realizable distributions $\mathcal{D}$ are simply more difficult to learn than realizable distributions -- even when one discounts a learner's error by $\mathrm{err}(h^*_{\mathcal{D}})$, the error of the best hypothesis in $\mathcal{H}$ for $\mathcal{D}$. Thus, optimal agnostic learners are permitted to incur excess error on (easier-to-learn) distributions $\mathcal{D}$ for which $τ= \mathrm{err}(h^*_{\mathcal{D}})$ is small. Recent work of Hanneke, Larsen, and Zhivotovskiy (FOCS `24) addresses this shortcoming by including $τ$ itself as a parameter in the agnostic error term. In this more fine-grained model, they demonstrate tightness of the error lower bound $τ+ Ω\left(\sqrt{\frac{τ(d + \log(1 / δ))}{m}} + \frac{d + \log(1 / δ)}{m} \right)$ in a regime where $τ> d/m$, and leave open the question of whether there may be a higher lower bound when $τ\approx d/m$, with $d$ denoting $\mathrm{VC}(\mathcal{H})$. In this work, we resolve this question by exhibiting a learner which achieves error $c \cdot τ+ O \left(\sqrt{\frac{τ(d + \log(1 / δ))}{m}} + \frac{d + \log(1 / δ)}{m} \right)$ for a constant $c \leq 2.1$, thus matching the lower bound when $τ\approx d/m$. Further, our learner is computationally efficient and is based upon careful aggregations of ERM classifiers, making progress on two other questions of Hanneke, Larsen, and Zhivotovskiy (FOCS `24). We leave open the interesting question of whether our approach can be refined to lower the constant from 2.1 to 1, which would completely settle the complexity of agnostic learning.

### Inverse Design with Dynamic Mode Decomposition 
[[arxiv](https://arxiv.org/abs/2502.09490)] [[cool](https://papers.cool/arxiv/2502.09490)] [[pdf](https://arxiv.org/pdf/2502.09490)]
> **Authors**: Yunpeng Zhu,Liangliang Cheng,Anping Jing,Hanyu Huo,Ziqiang Lang,Bo Zhang,J. Nathan Kutz
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 29 pages, 19 figures
- **标题**: None
- **领域**: 机器学习,系统与控制,动力系统,优化与控制,流体动力学
- **Abstract**: We introduce a computationally efficient method for the automation of inverse design in science and engineering. Based on simple least-square regression, the underlying dynamic mode decomposition algorithm can be used to construct a low-rank subspace spanning multiple experiments in parameter space. The proposed inverse design dynamic mode composition (ID-DMD) algorithm leverages the computed low-dimensional subspace to enable fast digital design and optimization on laptop-level computing, including the potential to prescribe the dynamics themselves. Moreover, the method is robust to noise, physically interpretable, and can provide uncertainty quantification metrics. The architecture can also efficiently scale to large-scale design problems using randomized algorithms in the ID-DMD. The simplicity of the method and its implementation are highly attractive in practice, and the ID-DMD has been demonstrated to be an order of magnitude more accurate than competing methods while simultaneously being 3-5 orders faster on challenging engineering design problems ranging from structural vibrations to fluid dynamics. Due to its speed, robustness, interpretability, and ease-of-use, ID-DMD in comparison with other leading machine learning methods represents a significant advancement in data-driven methods for inverse design and optimization, promising a paradigm shift in how to approach inverse design in practice.

### Learning to Predict Global Atrial Fibrillation Dynamics from Sparse Measurements 
[[arxiv](https://arxiv.org/abs/2502.09473)] [[cool](https://papers.cool/arxiv/2502.09473)] [[pdf](https://arxiv.org/pdf/2502.09473)]
> **Authors**: Alexander Jenkins,Andrea Cini,Joseph Barker,Alexander Sharp,Arunashis Sau,Varun Valentine,Srushti Valasang,Xinyang Li,Tom Wong,Timothy Betts,Danilo Mandic,Cesare Alippi,Fu Siong Ng
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: Under review
- **标题**: None
- **领域**: 机器学习,信号处理
- **Abstract**: Catheter ablation of Atrial Fibrillation (AF) consists of a one-size-fits-all treatment with limited success in persistent AF. This may be due to our inability to map the dynamics of AF with the limited resolution and coverage provided by sequential contact mapping catheters, preventing effective patient phenotyping for personalised, targeted ablation. Here we introduce FibMap, a graph recurrent neural network model that reconstructs global AF dynamics from sparse measurements. Trained and validated on 51 non-contact whole atria recordings, FibMap reconstructs whole atria dynamics from 10% surface coverage, achieving a 210% lower mean absolute error and an order of magnitude higher performance in tracking phase singularities compared to baseline methods. Clinical utility of FibMap is demonstrated on real-world contact mapping recordings, achieving reconstruction fidelity comparable to non-contact mapping. FibMap's state-spaces and patient-specific parameters offer insights for electrophenotyping AF. Integrating FibMap into clinical practice could enable personalised AF care and improve outcomes.

### Relational Conformal Prediction for Correlated Time Series 
[[arxiv](https://arxiv.org/abs/2502.09443)] [[cool](https://papers.cool/arxiv/2502.09443)] [[pdf](https://arxiv.org/pdf/2502.09443)]
> **Authors**: Andrea Cini,Alexander Jenkins,Danilo Mandic,Cesare Alippi,Filippo Maria Bianchi
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: We address the problem of uncertainty quantification in time series forecasting by exploiting observations at correlated sequences. Relational deep learning methods leveraging graph representations are among the most effective tools for obtaining point estimates from spatiotemporal data and correlated time series. However, the problem of exploiting relational structures to estimate the uncertainty of such predictions has been largely overlooked in the same context. To this end, we propose a novel distribution-free approach based on the conformal prediction framework and quantile regression. Despite the recent applications of conformal prediction to sequential data, existing methods operate independently on each target time series and do not account for relationships among them when constructing the prediction interval. We fill this void by introducing a novel conformal prediction method based on graph deep learning operators. Our method, named Conformal Relational Prediction (CoRel), does not require the relational structure (graph) to be known as a prior and can be applied on top of any pre-trained time series predictor. Additionally, CoRel includes an adaptive component to handle non-exchangeable data and changes in the input time series. Our approach provides accurate coverage and archives state-of-the-art uncertainty quantification in relevant benchmarks.

### A Survey of Reinforcement Learning for Optimization in Automation 
[[arxiv](https://arxiv.org/abs/2502.09417)] [[cool](https://papers.cool/arxiv/2502.09417)] [[pdf](https://arxiv.org/pdf/2502.09417)]
> **Authors**: Ahmad Farooq,Kamran Iqbal
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 8 pages, 4 tables, and 1 figure. Accepted at IEEE 20th International Conference on Automation Science and Engineering (CASE) 2024
- **标题**: None
- **领域**: 机器学习,人工智能,神经和进化计算,机器人技术,系统与控制
- **Abstract**: Reinforcement Learning (RL) has become a critical tool for optimization challenges within automation, leading to significant advancements in several areas. This review article examines the current landscape of RL within automation, with a particular focus on its roles in manufacturing, energy systems, and robotics. It discusses state-of-the-art methods, major challenges, and upcoming avenues of research within each sector, highlighting RL's capacity to solve intricate optimization challenges. The paper reviews the advantages and constraints of RL-driven optimization methods in automation. It points out prevalent challenges encountered in RL optimization, including issues related to sample efficiency and scalability; safety and robustness; interpretability and trustworthiness; transfer learning and meta-learning; and real-world deployment and integration. It further explores prospective strategies and future research pathways to navigate these challenges. Additionally, the survey includes a comprehensive list of relevant research papers, making it an indispensable guide for scholars and practitioners keen on exploring this domain.

### A hierarchical approach for assessing the vulnerability of tree-based classification models to membership inference attack 
[[arxiv](https://arxiv.org/abs/2502.09396)] [[cool](https://papers.cool/arxiv/2502.09396)] [[pdf](https://arxiv.org/pdf/2502.09396)]
> **Authors**: Richard J. Preen,Jim Smith
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,密码学和安全
- **Abstract**: Machine learning models can inadvertently expose confidential properties of their training data, making them vulnerable to membership inference attacks (MIA). While numerous evaluation methods exist, many require computationally expensive processes, such as training multiple shadow models. This article presents two new complementary approaches for efficiently identifying vulnerable tree-based models: an ante-hoc analysis of hyperparameter choices and a post-hoc examination of trained model structure. While these new methods cannot certify whether a model is safe from MIA, they provide practitioners with a means to significantly reduce the number of models that need to undergo expensive MIA assessment through a hierarchical filtering approach. More specifically, it is shown that the rank order of disclosure risk for different hyperparameter combinations remains consistent across datasets, enabling the development of simple, human-interpretable rules for identifying relatively high-risk models before training. While this ante-hoc analysis cannot determine absolute safety since this also depends on the specific dataset, it allows the elimination of unnecessarily risky configurations during hyperparameter tuning. Additionally, computationally inexpensive structural metrics serve as indicators of MIA vulnerability, providing a second filtering stage to identify risky models after training but before conducting expensive attacks. Empirical results show that hyperparameter-based risk prediction rules can achieve high accuracy in predicting the most at risk combinations of hyperparameters across different tree-based model types, while requiring no model training. Moreover, target model accuracy is not seen to correlate with privacy risk, suggesting opportunities to optimise model configurations for both performance and privacy.

### LoRA Training Provably Converges to a Low-Rank Global Minimum or It Fails Loudly (But it Probably Won't Fail) 
[[arxiv](https://arxiv.org/abs/2502.09376)] [[cool](https://papers.cool/arxiv/2502.09376)] [[pdf](https://arxiv.org/pdf/2502.09376)]
> **Authors**: Junsu Kim,Jaeyeon Kim,Ernest K. Ryu
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Low-rank adaptation (LoRA) has become a standard approach for fine-tuning large foundation models. However, our theoretical understanding of LoRA remains limited as prior analyses of LoRA's training dynamics either rely on linearization arguments or consider highly simplified setups. In this work, we analyze the LoRA loss landscape without such restrictive assumptions. We define two regimes: a ``special regime'', which includes idealized setups where linearization arguments hold, and a ``generic regime'' representing more realistic setups where linearization arguments do not hold. In the generic regime, we show that LoRA training converges to a global minimizer with low rank and small magnitude, or a qualitatively distinct solution with high rank and large magnitude. Finally, we argue that the zero-initialization and weight decay in LoRA training induce an implicit bias toward the low-rank, small-magnitude region of the parameter space -- where global minima lie -- thus shedding light on why LoRA training usually succeeds in finding global minima.

### Mitigating multiple single-event upsets during deep neural network inference using fault-aware training 
[[arxiv](https://arxiv.org/abs/2502.09374)] [[cool](https://papers.cool/arxiv/2502.09374)] [[pdf](https://arxiv.org/pdf/2502.09374)]
> **Authors**: Toon Vinck,Naïn Jonckers,Gert Dekkers,Jeffrey Prinzie,Peter Karsmakers
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 7 pages, 4 figures, Topical Workshop on Electronics for Particle Physics
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Deep neural networks (DNNs) are increasingly used in safety-critical applications. Reliable fault analysis and mitigation are essential to ensure their functionality in harsh environments that contain high radiation levels. This study analyses the impact of multiple single-bit single-event upsets in DNNs by performing fault injection at the level of a DNN model. Additionally, a fault aware training (FAT) methodology is proposed that improves the DNNs' robustness to faults without any modification to the hardware. Experimental results show that the FAT methodology improves the tolerance to faults up to a factor 3.

### Language Agents as Digital Representatives in Collective Decision-Making 
[[arxiv](https://arxiv.org/abs/2502.09369)] [[cool](https://papers.cool/arxiv/2502.09369)] [[pdf](https://arxiv.org/pdf/2502.09369)]
> **Authors**: Daniel Jarrett,Miruna Pîslar,Michiel A. Bakker,Michael Henry Tessler,Raphael Köster,Jan Balaguer,Romuald Elie,Christopher Summerfield,Andrea Tacchetti
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,计算机与社会
- **Abstract**: Consider the process of collective decision-making, in which a group of individuals interactively select a preferred outcome from among a universe of alternatives. In this context, "representation" is the activity of making an individual's preferences present in the process via participation by a proxy agent -- i.e. their "representative". To this end, learned models of human behavior have the potential to fill this role, with practical implications for multi-agent scenario studies and mechanism design. In this work, we investigate the possibility of training \textit{language agents} to behave in the capacity of representatives of human agents, appropriately expressing the preferences of those individuals whom they stand for. First, we formalize the setting of \textit{collective decision-making} -- as the episodic process of interaction between a group of agents and a decision mechanism. On this basis, we then formalize the problem of \textit{digital representation} -- as the simulation of an agent's behavior to yield equivalent outcomes from the mechanism. Finally, we conduct an empirical case study in the setting of \textit{consensus-finding} among diverse humans, and demonstrate the feasibility of fine-tuning large language models to act as digital representatives.

### Simple Path Structural Encoding for Graph Transformers 
[[arxiv](https://arxiv.org/abs/2502.09365)] [[cool](https://papers.cool/arxiv/2502.09365)] [[pdf](https://arxiv.org/pdf/2502.09365)]
> **Authors**: Louis Airale,Antonio Longa,Mattia Rigon,Andrea Passerini,Roberto Passerone
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Graph transformers extend global self-attention to graph-structured data, achieving notable success in graph learning. Recently, random walk structural encoding (RWSE) has been found to further enhance their predictive power by encoding both structural and positional information into the edge representation. However, RWSE cannot always distinguish between edges that belong to different local graph patterns, which reduces its ability to capture the full structural complexity of graphs. This work introduces Simple Path Structural Encoding (SPSE), a novel method that utilizes simple path counts for edge encoding. We show theoretically and experimentally that SPSE overcomes the limitations of RWSE, providing a richer representation of graph structures, particularly for capturing local cyclic patterns. To make SPSE computationally tractable, we propose an efficient approximate algorithm for simple path counting. SPSE demonstrates significant performance improvements over RWSE on various benchmarks, including molecular and long-range graph datasets, achieving statistically significant gains in discriminative tasks. These results pose SPSE as a powerful edge encoding alternative for enhancing the expressivity of graph transformers.

### The Accuracy Cost of Weakness: A Theoretical Analysis of Fixed-Segment Weak Labeling for Events in Time 
[[arxiv](https://arxiv.org/abs/2502.09363)] [[cool](https://papers.cool/arxiv/2502.09363)] [[pdf](https://arxiv.org/pdf/2502.09363)]
> **Authors**: John Martinsson,Olof Mogren,Tuomas Virtanen,Maria Sandsten
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: Submitted to TMLR
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Accurate labels are critical for deriving robust machine learning models. Labels are used to train supervised learning models and to evaluate most machine learning paradigms. In this paper, we model the accuracy and cost of a common weak labeling process where annotators assign presence or absence labels to fixed-length data segments for a given event class. The annotator labels a segment as "present" if it sufficiently covers an event from that class, e.g., a birdsong sound event in audio data. We analyze how the segment length affects the label accuracy and the required number of annotations, and compare this fixed-length labeling approach with an oracle method that uses the true event activations to construct the segments. Furthermore, we quantify the gap between these methods and verify that in most realistic scenarios the oracle method is better than the fixed-length labeling method in both accuracy and cost. Our findings provide a theoretical justification for adaptive weak labeling strategies that mimic the oracle process, and a foundation for optimizing weak labeling processes in sequence labeling tasks.

### Wasserstein distributional adversarial training for deep neural networks 
[[arxiv](https://arxiv.org/abs/2502.09352)] [[cool](https://papers.cool/arxiv/2502.09352)] [[pdf](https://arxiv.org/pdf/2502.09352)]
> **Authors**: Xingjian Bai,Guangyi He,Yifan Jiang,Jan Obloj
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 15 pages, 4 figures
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别,优化与控制
- **Abstract**: Design of adversarial attacks for deep neural networks, as well as methods of adversarial training against them, are subject of intense research. In this paper, we propose methods to train against distributional attack threats, extending the TRADES method used for pointwise attacks. Our approach leverages recent contributions and relies on sensitivity analysis for Wasserstein distributionally robust optimization problems. We introduce an efficient fine-tuning method which can be deployed on a previously trained model. We test our methods on a range of pre-trained models on RobustBench. These experimental results demonstrate the additional training enhances Wasserstein distributional robustness, while maintaining original levels of pointwise robustness, even for already very successful networks. The improvements are less marked for models pre-trained using huge synthetic datasets of 20-100M images. However, remarkably, sometimes our methods are still able to improve their performance even when trained using only the original training dataset (50k images).

### Machine learning for modelling unstructured grid data in computational physics: a review 
[[arxiv](https://arxiv.org/abs/2502.09346)] [[cool](https://papers.cool/arxiv/2502.09346)] [[pdf](https://arxiv.org/pdf/2502.09346)]
> **Authors**: Sibo Cheng,Marc Bocquet,Weiping Ding,Tobias Sebastian Finn,Rui Fu,Jinlong Fu,Yike Guo,Eleda Johnson,Siyi Li,Che Liu,Eric Newton Moro,Jie Pan,Matthew Piggott,Cesar Quilodran,Prakhar Sharma,Kun Wang,Dunhui Xiao,Xiao Xue,Yong Zeng,Mingrui Zhang,Hao Zhou,Kewei Zhu,Rossella Arcucci
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算工程、金融和科学,数据分析、统计和概率,流体动力学
- **Abstract**: Unstructured grid data are essential for modelling complex geometries and dynamics in computational physics. Yet, their inherent irregularity presents significant challenges for conventional machine learning (ML) techniques. This paper provides a comprehensive review of advanced ML methodologies designed to handle unstructured grid data in high-dimensional dynamical systems. Key approaches discussed include graph neural networks, transformer models with spatial attention mechanisms, interpolation-integrated ML methods, and meshless techniques such as physics-informed neural networks. These methodologies have proven effective across diverse fields, including fluid dynamics and environmental simulations. This review is intended as a guidebook for computational scientists seeking to apply ML approaches to unstructured grid data in their domains, as well as for ML researchers looking to address challenges in computational physics. It places special focus on how ML methods can overcome the inherent limitations of traditional numerical techniques and, conversely, how insights from computational physics can inform ML development. To support benchmarking, this review also provides a summary of open-access datasets of unstructured grid data in computational physics. Finally, emerging directions such as generative models with unstructured data, reinforcement learning for mesh generation, and hybrid physics-data-driven paradigms are discussed to inspire future advancements in this evolving field.

### Neural Spatiotemporal Point Processes: Trends and Challenges 
[[arxiv](https://arxiv.org/abs/2502.09341)] [[cool](https://papers.cool/arxiv/2502.09341)] [[pdf](https://arxiv.org/pdf/2502.09341)]
> **Authors**: Sumantrak Mukherjee,Mouad Elhamdi,George Mohler,David A. Selby,Yao Xie,Sebastian Vollmer,Gerrit Grossmann
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Spatiotemporal point processes (STPPs) are probabilistic models for events occurring in continuous space and time. Real-world event data often exhibit intricate dependencies and heterogeneous dynamics. By incorporating modern deep learning techniques, STPPs can model these complexities more effectively than traditional approaches. Consequently, the fusion of neural methods with STPPs has become an active and rapidly evolving research area. In this review, we categorize existing approaches, unify key design choices, and explain the challenges of working with this data modality. We further highlight emerging trends and diverse application domains. Finally, we identify open challenges and gaps in the literature.

### This looks like what? Challenges and Future Research Directions for Part-Prototype Models 
[[arxiv](https://arxiv.org/abs/2502.09340)] [[cool](https://papers.cool/arxiv/2502.09340)] [[pdf](https://arxiv.org/pdf/2502.09340)]
> **Authors**: Khawla Elhadri,Tomasz Michalski,Adam Wróbel,Jörg Schlötterer,Bartosz Zieliński,Christin Seifert
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The growing interest in eXplainable Artificial Intelligence (XAI) has prompted research into models with built-in interpretability, the most prominent of which are part-prototype models. Part-Prototype Models (PPMs) make decisions by comparing an input image to a set of learned prototypes, providing human-understandable explanations in the form of ``this looks like that''. Despite their inherent interpretability, PPMS are not yet considered a valuable alternative to post-hoc models. In this survey, we investigate the reasons for this and provide directions for future research. We analyze papers from 2019 to 2024, and derive a taxonomy of the challenges that current PPMS face. Our analysis shows that the open challenges are quite diverse. The main concern is the quality and quantity of prototypes. Other concerns are the lack of generalization to a variety of tasks and contexts, and general methodological issues, including non-standardized evaluation. We provide ideas for future research in five broad directions: improving predictive performance, developing novel architectures grounded in theory, establishing frameworks for human-AI collaboration, aligning models with humans, and establishing metrics and benchmarks for evaluation. We hope that this survey will stimulate research and promote intrinsically interpretable models for application domains. Our list of surveyed papers is available at https://github.com/aix-group/ppm-survey.

### Graph Diffusion Network for Drug-Gene Prediction 
[[arxiv](https://arxiv.org/abs/2502.09335)] [[cool](https://papers.cool/arxiv/2502.09335)] [[pdf](https://arxiv.org/pdf/2502.09335)]
> **Authors**: Jiayang Wu,Wensheng Gan,Philip S. Yu
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: IEEE/ACM TCBB. 14 pages
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Predicting drug-gene associations is crucial for drug development and disease treatment. While graph neural networks (GNN) have shown effectiveness in this task, they face challenges with data sparsity and efficient contrastive learning implementation. We introduce a graph diffusion network for drug-gene prediction (GDNDGP), a framework that addresses these limitations through two key innovations. First, it employs meta-path-based homogeneous graph learning to capture drug-drug and gene-gene relationships, ensuring similar entities share embedding spaces. Second, it incorporates a parallel diffusion network that generates hard negative samples during training, eliminating the need for exhaustive negative sample retrieval. Our model achieves superior performance on the DGIdb 4.0 dataset and demonstrates strong generalization capability on tripartite drug-gene-disease networks. Results show significant improvements over existing methods in drug-gene prediction tasks, particularly in handling complex heterogeneous relationships. The source code is publicly available at https://github.com/csjywu1/GDNDGP.

### Full Swap Regret and Discretized Calibration 
[[arxiv](https://arxiv.org/abs/2502.09332)] [[cool](https://papers.cool/arxiv/2502.09332)] [[pdf](https://arxiv.org/pdf/2502.09332)]
> **Authors**: Maxwell Fishelson,Robert Kleinberg,Princewill Okoroafor,Renato Paes Leme,Jon Schneider,Yifeng Teng
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机科学与博弈论
- **Abstract**: We study the problem of minimizing swap regret in structured normal-form games. Players have a very large (potentially infinite) number of pure actions, but each action has an embedding into $d$-dimensional space and payoffs are given by bilinear functions of these embeddings. We provide an efficient learning algorithm for this setting that incurs at most $\tilde{O}(T^{(d+1)/(d+3)})$ swap regret after $T$ rounds. To achieve this, we introduce a new online learning problem we call \emph{full swap regret minimization}. In this problem, a learner repeatedly takes a (randomized) action in a bounded convex $d$-dimensional action set $\mathcal{K}$ and then receives a loss from the adversary, with the goal of minimizing their regret with respect to the \emph{worst-case} swap function mapping $\mathcal{K}$ to $\mathcal{K}$. For varied assumptions about the convexity and smoothness of the loss functions, we design algorithms with full swap regret bounds ranging from $O(T^{d/(d+2)})$ to $O(T^{(d+1)/(d+2)})$. Finally, we apply these tools to the problem of online forecasting to minimize calibration error, showing that several notions of calibration can be viewed as specific instances of full swap regret. In particular, we design efficient algorithms for online forecasting that guarantee at most $O(T^{1/3})$ $\ell_2$-calibration error and $O(\max(\sqrt{εT}, T^{1/3}))$ \emph{discretized-calibration} error (when the forecaster is restricted to predicting multiples of $ε$).

### Bayesian Optimization for Simultaneous Selection of Machine Learning Algorithms and Hyperparameters on Shared Latent Space 
[[arxiv](https://arxiv.org/abs/2502.09329)] [[cool](https://papers.cool/arxiv/2502.09329)] [[pdf](https://arxiv.org/pdf/2502.09329)]
> **Authors**: Kazuki Ishikawa,Ryota Ozaki,Yohei Kanzaki,Ichiro Takeuchi,Masayuki Karasuyama
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Selecting the optimal combination of a machine learning (ML) algorithm and its hyper-parameters is crucial for the development of high-performance ML systems. However, since the combination of ML algorithms and hyper-parameters is enormous, the exhaustive validation requires a significant amount of time. Many existing studies use Bayesian optimization (BO) for accelerating the search. On the other hand, a significant difficulty is that, in general, there exists a different hyper-parameter space for each one of candidate ML algorithms. BO-based approaches typically build a surrogate model independently for each hyper-parameter space, by which sufficient observations are required for all candidate ML algorithms. In this study, our proposed method embeds different hyper-parameter spaces into a shared latent space, in which a surrogate multi-task model for BO is estimated. This approach can share information of observations from different ML algorithms by which efficient optimization is expected with a smaller number of total observations. We further propose the pre-training of the latent space embedding with an adversarial regularization, and a ranking model for selecting an effective pre-trained embedding for a given target dataset. Our empirical study demonstrates effectiveness of the proposed method through datasets from OpenML.

### Depth-Bounds for Neural Networks via the Braid Arrangement 
[[arxiv](https://arxiv.org/abs/2502.09324)] [[cool](https://papers.cool/arxiv/2502.09324)] [[pdf](https://arxiv.org/pdf/2502.09324)]
> **Authors**: Moritz Grillo,Christoph Hertrich,Georg Loho
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,离散数学,神经和进化计算,组合学
- **Abstract**: We contribute towards resolving the open question of how many hidden layers are required in ReLU networks for exactly representing all continuous and piecewise linear functions on $\mathbb{R}^d$. While the question has been resolved in special cases, the best known lower bound in general is still 2. We focus on neural networks that are compatible with certain polyhedral complexes, more precisely with the braid fan. For such neural networks, we prove a non-constant lower bound of $Ω(\log\log d)$ hidden layers required to exactly represent the maximum of $d$ numbers. Additionally, under our assumption, we provide a combinatorial proof that 3 hidden layers are necessary to compute the maximum of 5 numbers; this had only been verified with an excessive computation so far. Finally, we show that a natural generalization of the best known upper bound to maxout networks is not tight, by demonstrating that a rank-3 maxout layer followed by a rank-2 maxout layer is sufficient to represent the maximum of 7 numbers.

### SigGate: Enhancing Recurrent Neural Networks with Signature-Based Gating Mechanisms 
[[arxiv](https://arxiv.org/abs/2502.09318)] [[cool](https://papers.cool/arxiv/2502.09318)] [[pdf](https://arxiv.org/pdf/2502.09318)]
> **Authors**: Rémi Genet,Hugo Inzirillo
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: In this paper, we propose a novel approach that enhances recurrent neural networks (RNNs) by incorporating path signatures into their gating mechanisms. Our method modifies both Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) architectures by replacing their forget and reset gates, respectively, with learnable path signatures. These signatures, which capture the geometric features of the entire path history, provide a richer context for controlling information flow through the network's memory. This modification allows the networks to make memory decisions based on the full historical context rather than just the current input and state. Through experimental studies, we demonstrate that our Signature-LSTM (SigLSTM) and Signature-GRU (SigGRU) models outperform their traditional counterparts across various sequential learning tasks. By leveraging path signatures in recurrent architectures, this method offers new opportunities to enhance performance in time series analysis and forecasting applications.

### Towards Seamless Hierarchical Federated Learning under Intermittent Client Participation: A Stagewise Decision-Making Methodology 
[[arxiv](https://arxiv.org/abs/2502.09303)] [[cool](https://papers.cool/arxiv/2502.09303)] [[pdf](https://arxiv.org/pdf/2502.09303)]
> **Authors**: Minghong Wu,Minghui Liwang,Yuhan Su,Li Li,Seyyedali Hosseinalipour,Xianbin Wang,Huaiyu Dai,Zhenzhen Jiao
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 20 pages, 8 figures,5 tables
- **标题**: None
- **领域**: 机器学习,分布式、并行和集群计算
- **Abstract**: Federated Learning (FL) offers a pioneering distributed learning paradigm that enables devices/clients to build a shared global model. This global model is obtained through frequent model transmissions between clients and a central server, which may cause high latency, energy consumption, and congestion over backhaul links. To overcome these drawbacks, Hierarchical Federated Learning (HFL) has emerged, which organizes clients into multiple clusters and utilizes edge nodes (e.g., edge servers) for intermediate model aggregations between clients and the central server. Current research on HFL mainly focus on enhancing model accuracy, latency, and energy consumption in scenarios with a stable/fixed set of clients. However, addressing the dynamic availability of clients -- a critical aspect of real-world scenarios -- remains underexplored. This study delves into optimizing client selection and client-to-edge associations in HFL under intermittent client participation so as to minimize overall system costs (i.e., delay and energy), while achieving fast model convergence. We unveil that achieving this goal involves solving a complex NP-hard problem. To tackle this, we propose a stagewise methodology that splits the solution into two stages, referred to as Plan A and Plan B. Plan A focuses on identifying long-term clients with high chance of participation in subsequent model training rounds. Plan B serves as a backup, selecting alternative clients when long-term clients are unavailable during model training rounds. This stagewise methodology offers a fresh perspective on client selection that can enhance both HFL and conventional FL via enabling low-overhead decision-making processes. Through evaluations on MNIST and CIFAR-10 datasets, we show that our methodology outperforms existing benchmarks in terms of model accuracy and system costs.

### Convex Is Back: Solving Belief MDPs With Convexity-Informed Deep Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.09298)] [[cool](https://papers.cool/arxiv/2502.09298)] [[pdf](https://arxiv.org/pdf/2502.09298)]
> **Authors**: Daniel Koutas,Daniel Hettegger,Kostas G. Papakonstantinou,Daniel Straub
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We present a novel method for Deep Reinforcement Learning (DRL), incorporating the convex property of the value function over the belief space in Partially Observable Markov Decision Processes (POMDPs). We introduce hard- and soft-enforced convexity as two different approaches, and compare their performance against standard DRL on two well-known POMDP environments, namely the Tiger and FieldVisionRockSample problems. Our findings show that including the convexity feature can substantially increase performance of the agents, as well as increase robustness over the hyperparameter space, especially when testing on out-of-distribution domains. The source code for this work can be found at https://github.com/Dakout/Convex_DRL.

### When do neural networks learn world models? 
[[arxiv](https://arxiv.org/abs/2502.09297)] [[cool](https://papers.cool/arxiv/2502.09297)] [[pdf](https://arxiv.org/pdf/2502.09297)]
> **Authors**: Tianren Zhang,Guanyu Chen,Feng Chen
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 28 pages, 9 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Humans develop world models that capture the underlying generation process of data. Whether neural networks can learn similar world models remains an open problem. In this work, we provide the first theoretical results for this problem, showing that in a multi-task setting, models with a low-degree bias provably recover latent data-generating variables under mild assumptions -- even if proxy tasks involve complex, non-linear functions of the latents. However, such recovery is also sensitive to model architecture. Our analysis leverages Boolean models of task solutions via the Fourier-Walsh transform and introduces new techniques for analyzing invertible Boolean transforms, which may be of independent interest. We illustrate the algorithmic implications of our results and connect them to related research areas, including self-supervised learning, out-of-distribution generalization, and the linear representation hypothesis in large language models.

### An Uncertainty Principle for Linear Recurrent Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.09287)] [[cool](https://papers.cool/arxiv/2502.09287)] [[pdf](https://arxiv.org/pdf/2502.09287)]
> **Authors**: Alexandre François,Antonio Orvieto,Francis Bach
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We consider linear recurrent neural networks, which have become a key building block of sequence modeling due to their ability for stable and effective long-range modeling. In this paper, we aim at characterizing this ability on a simple but core copy task, whose goal is to build a linear filter of order $S$ that approximates the filter that looks $K$ time steps in the past (which we refer to as the shift-$K$ filter), where $K$ is larger than $S$. Using classical signal models and quadratic cost, we fully characterize the problem by providing lower bounds of approximation, as well as explicit filters that achieve this lower bound up to constants. The optimal performance highlights an uncertainty principle: the optimal filter has to average values around the $K$-th time step in the past with a range~(width) that is proportional to $K/S$.

### LiSA: Leveraging Link Recommender to Attack Graph Neural Networks via Subgraph Injection 
[[arxiv](https://arxiv.org/abs/2502.09271)] [[cool](https://papers.cool/arxiv/2502.09271)] [[pdf](https://arxiv.org/pdf/2502.09271)]
> **Authors**: Wenlun Zhang,Enyan Dai,Kentaro Yoshioka
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: PAKDD 2025
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Graph Neural Networks (GNNs) have demonstrated remarkable proficiency in modeling data with graph structures, yet recent research reveals their susceptibility to adversarial attacks. Traditional attack methodologies, which rely on manipulating the original graph or adding links to artificially created nodes, often prove impractical in real-world settings. This paper introduces a novel adversarial scenario involving the injection of an isolated subgraph to deceive both the link recommender and the node classifier within a GNN system. Specifically, the link recommender is mislead to propose links between targeted victim nodes and the subgraph, encouraging users to unintentionally establish connections and that would degrade the node classification accuracy, thereby facilitating a successful attack. To address this, we present the LiSA framework, which employs a dual surrogate model and bi-level optimization to simultaneously meet two adversarial objectives. Extensive experiments on real-world datasets demonstrate the effectiveness of our method.

### Unlocking the Potential of Classic GNNs for Graph-level Tasks: Simple Architectures Meet Excellence 
[[arxiv](https://arxiv.org/abs/2502.09263)] [[cool](https://papers.cool/arxiv/2502.09263)] [[pdf](https://arxiv.org/pdf/2502.09263)]
> **Authors**: Yuankai Luo,Lei Shi,Xiao-Ming Wu
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Message-passing Graph Neural Networks (GNNs) are often criticized for their limited expressiveness, issues like over-smoothing and over-squashing, and challenges in capturing long-range dependencies, while Graph Transformers (GTs) are considered superior due to their global attention mechanisms. Literature frequently suggests that GTs outperform GNNs, particularly in graph-level tasks such as graph classification and regression. In this study, we explore the untapped potential of GNNs through an enhanced framework, GNN+, which integrates six widely used techniques: edge feature integration, normalization, dropout, residual connections, feed-forward networks, and positional encoding, to effectively tackle graph-level tasks. We conduct a systematic evaluation of three classic GNNs, namely GCN, GIN, and GatedGCN, enhanced by the GNN+ framework across 14 well-known graph-level datasets. Our results show that, contrary to the prevailing belief, classic GNNs excel in graph-level tasks, securing top three rankings across all datasets and achieving first place in eight, while also demonstrating greater efficiency than GTs. This highlights the potential of simple GNN architectures, challenging the belief that complex mechanisms in GTs are essential for superior graph-level performance.

### Bandit Multiclass List Classification 
[[arxiv](https://arxiv.org/abs/2502.09257)] [[cool](https://papers.cool/arxiv/2502.09257)] [[pdf](https://arxiv.org/pdf/2502.09257)]
> **Authors**: Liad Erez,Tomer Koren
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: We study the problem of multiclass list classification with (semi-)bandit feedback, where input examples are mapped into subsets of size $m$ of a collection of $K$ possible labels, and the feedback consists of the predicted labels which lie in the set of true labels of the given example. Our main result is for the $(\varepsilon,δ)$-PAC variant of the problem for which we design an algorithm that returns an $\varepsilon$-optimal hypothesis with high probability using a sample complexity of $O \big( (\mathrm{poly}(K/m) + sm / \varepsilon^2) \log (|H|/δ) \big)$ where $H$ is the underlying (finite) hypothesis class and $s$ is an upper bound on the number of true labels for a given example. This bound improves upon known bounds for combinatorial semi-bandits whenever $s \ll K$. Moreover, in the regime where $s = O(1)$ the leading terms in our bound match the corresponding full-information rates, implying that bandit feedback essentially comes at no cost. Our PAC learning algorithm is also computationally efficient given access to an ERM oracle for $H$. Additionally, we consider the regret minimization setting where data can be generated adversarially, and establish a regret bound of $\widetilde O(|H| + \sqrt{smT \log |H|})$. Our results generalize and extend those of Erez et al. (2024) who consider the simpler single-label setting corresponding to $s=m=1$, and in fact hold for the more general contextual combinatorial semi-bandit problem with $s$-sparse rewards.

### AnomalyGFM: Graph Foundation Model for Zero/Few-shot Anomaly Detection 
[[arxiv](https://arxiv.org/abs/2502.09254)] [[cool](https://papers.cool/arxiv/2502.09254)] [[pdf](https://arxiv.org/pdf/2502.09254)]
> **Authors**: Hezhe Qiao,Chaoxi Niu,Ling Chen,Guansong Pang
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 14 pages
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Graph anomaly detection (GAD) aims to identify abnormal nodes that differ from the majority of the nodes in a graph, which has been attracting significant attention in recent years. Existing generalist graph models have achieved remarkable success in different graph tasks but struggle to generalize to the GAD task. This limitation arises from their difficulty in learning generalized knowledge for capturing the inherently infrequent, irregular and heterogeneous abnormality patterns in graphs from different domains. To address this challenge, we propose AnomalyGFM, a GAD-oriented graph foundation model that supports zero-shot inference and few-shot prompt tuning for GAD in diverse graph datasets. One key insight is that graph-agnostic representations for normal and abnormal classes are required to support effective zero/few-shot GAD across different graphs. Motivated by this, AnomalyGFM is pre-trained to align data-independent, learnable normal and abnormal class prototypes with node representation residuals (i.e., representation deviation of a node from its neighbors). The residual features essentially project the node information into a unified feature space where we can effectively measure the abnormality of nodes from different graphs in a consistent way. This provides a driving force for the learning of graph-agnostic, discriminative prototypes for the normal and abnormal classes, which can be used to enable zero-shot GAD on new graphs, including very large-scale graphs. If there are few-shot labeled normal nodes available in the new graphs, AnomalyGFM can further support prompt tuning to leverage these nodes for better adaptation. Comprehensive experiments on 11 widely-used GAD datasets with real anomalies, demonstrate that AnomalyGFM significantly outperforms state-of-the-art competing methods under both zero- and few-shot GAD settings.

### On the Importance of Embedding Norms in Self-Supervised Learning 
[[arxiv](https://arxiv.org/abs/2502.09252)] [[cool](https://papers.cool/arxiv/2502.09252)] [[pdf](https://arxiv.org/pdf/2502.09252)]
> **Authors**: Andrew Draganov,Sharvaree Vadgama,Sebastian Damrich,Jan Niklas Böhm,Lucas Maes,Dmitry Kobak,Erik Bekkers
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Self-supervised learning (SSL) allows training data representations without a supervised signal and has become an important paradigm in machine learning. Most SSL methods employ the cosine similarity between embedding vectors and hence effectively embed data on a hypersphere. While this seemingly implies that embedding norms cannot play any role in SSL, a few recent works have suggested that embedding norms have properties related to network convergence and confidence. In this paper, we resolve this apparent contradiction and systematically establish the embedding norm's role in SSL training. Using theoretical analysis, simulations, and experiments, we show that embedding norms (i) govern SSL convergence rates and (ii) encode network confidence, with smaller norms corresponding to unexpected samples. Additionally, we show that manipulating embedding norms can have large effects on convergence speed. Our findings demonstrate that SSL embedding norms are integral to understanding and optimizing network behavior.

### You Do Not Fully Utilize Transformer's Representation Capacity 
[[arxiv](https://arxiv.org/abs/2502.09245)] [[cool](https://papers.cool/arxiv/2502.09245)] [[pdf](https://arxiv.org/pdf/2502.09245)]
> **Authors**: Gleb Gerasimov,Yaroslav Aksenov,Nikita Balagansky,Viacheslav Sinii,Daniil Gavrilov
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: In contrast to RNNs, which compress previous tokens into a single hidden state, Transformers can attend to all previous tokens directly. However, standard Transformers only use representations from the immediately preceding layer. In this paper, we show that this design choice causes representation collapse and leads to suboptimal performance. To address this issue, we introduce Layer-Integrated Memory (LIMe), a simple yet powerful approach that preserves the model's overall memory footprint while expanding its representational capacity by allowing access to hidden states from earlier layers. Through extensive experiments across various architectures and different lookup mechanisms, we demonstrate consistent performance improvements on a wide range of tasks. Moreover, our analysis of the learned representation dynamics and our exploration of depthwise circuits reveal how LIMe integrates information across layers, pointing to promising directions for future research.

### Neuro-Symbolic Contrastive Learning for Cross-domain Inference 
[[arxiv](https://arxiv.org/abs/2502.09213)] [[cool](https://papers.cool/arxiv/2502.09213)] [[pdf](https://arxiv.org/pdf/2502.09213)]
> **Authors**: Mingyue Liu,Ryo Ueda,Zhen Wan,Katsumi Inoue,Chris G. Willcocks
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: In Proceedings ICLP 2024, arXiv:2502.08453
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Pre-trained language models (PLMs) have made significant advances in natural language inference (NLI) tasks, however their sensitivity to textual perturbations and dependence on large datasets indicate an over-reliance on shallow heuristics. In contrast, inductive logic programming (ILP) excels at inferring logical relationships across diverse, sparse and limited datasets, but its discrete nature requires the inputs to be precisely specified, which limits their application. This paper proposes a bridge between the two approaches: neuro-symbolic contrastive learning. This allows for smooth and differentiable optimisation that improves logical accuracy across an otherwise discrete, noisy, and sparse topological space of logical functions. We show that abstract logical relationships can be effectively embedded within a neuro-symbolic paradigm, by representing data as logic programs and sets of logic rules. The embedding space captures highly varied textual information with similar semantic logical relations, but can also separate similar textual relations that have dissimilar logical relations. Experimental results demonstrate that our approach significantly improves the inference capabilities of the models in terms of generalisation and reasoning.

### Understanding High-Dimensional Bayesian Optimization 
[[arxiv](https://arxiv.org/abs/2502.09198)] [[cool](https://papers.cool/arxiv/2502.09198)] [[pdf](https://arxiv.org/pdf/2502.09198)]
> **Authors**: Leonard Papenmeier,Matthias Poloczek,Luigi Nardi
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 19 pages, 20 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Recent work reported that simple Bayesian optimization methods perform well for high-dimensional real-world tasks, seemingly contradicting prior work and tribal knowledge. This paper investigates the 'why'. We identify fundamental challenges that arise in high-dimensional Bayesian optimization and explain why recent methods succeed. Our analysis shows that vanishing gradients caused by Gaussian process initialization schemes play a major role in the failures of high-dimensional Bayesian optimization and that methods that promote local search behaviors are better suited for the task. We find that maximum likelihood estimation of Gaussian process length scales suffices for state-of-the-art performance. Based on this, we propose a simple variant of maximum likelihood estimation called MSR that leverages these findings to achieve state-of-the-art performance on a comprehensive set of real-world applications. We also present targeted experiments to illustrate and confirm our findings.

### Generalizability through Explainability: Countering Overfitting with Counterfactual Examples 
[[arxiv](https://arxiv.org/abs/2502.09193)] [[cool](https://papers.cool/arxiv/2502.09193)] [[pdf](https://arxiv.org/pdf/2502.09193)]
> **Authors**: Flavio Giorgi,Fabiano Veglianti,Fabrizio Silvestri,Gabriele Tolomei
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Overfitting is a well-known issue in machine learning that occurs when a model struggles to generalize its predictions to new, unseen data beyond the scope of its training set. Traditional techniques to mitigate overfitting include early stopping, data augmentation, and regularization. In this work, we demonstrate that the degree of overfitting of a trained model is correlated with the ability to generate counterfactual examples. The higher the overfitting, the easier it will be to find a valid counterfactual example for a randomly chosen input data point. Therefore, we introduce CF-Reg, a novel regularization term in the training loss that controls overfitting by ensuring enough margin between each instance and its corresponding counterfactual. Experiments conducted across multiple datasets and models show that our counterfactual regularizer generally outperforms existing regularization techniques.

### Two-Stage Representation Learning for Analyzing Movement Behavior Dynamics in People Living with Dementia 
[[arxiv](https://arxiv.org/abs/2502.09173)] [[cool](https://papers.cool/arxiv/2502.09173)] [[pdf](https://arxiv.org/pdf/2502.09173)]
> **Authors**: Jin Cui,Alexander Capstick,Payam Barnaghi,Gregory Scott
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: AAAI 2025 Workshop on LargeLanguageModels and GenerativeAIfor Health
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: In remote healthcare monitoring, time series representation learning reveals critical patient behavior patterns from high-frequency data. This study analyzes home activity data from individuals living with dementia by proposing a two-stage, self-supervised learning approach tailored to uncover low-rank structures. The first stage converts time-series activities into text sequences encoded by a pre-trained language model, providing a rich, high-dimensional latent state space using a PageRank-based method. This PageRank vector captures latent state transitions, effectively compressing complex behaviour data into a succinct form that enhances interpretability. This low-rank representation not only enhances model interpretability but also facilitates clustering and transition analysis, revealing key behavioral patterns correlated with clinicalmetrics such as MMSE and ADAS-COG scores. Our findings demonstrate the framework's potential in supporting cognitive status prediction, personalized care interventions, and large-scale health monitoring.

### LOB-Bench: Benchmarking Generative AI for Finance -- an Application to Limit Order Book Data 
[[arxiv](https://arxiv.org/abs/2502.09172)] [[cool](https://papers.cool/arxiv/2502.09172)] [[pdf](https://arxiv.org/pdf/2502.09172)]
> **Authors**: Peer Nagy,Sascha Frey,Kang Li,Bidipta Sarkar,Svitlana Vyetrenko,Stefan Zohren,Ani Calinescu,Jakob Foerster
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算工程、金融和科学,计算金融,交易和市场微观结构
- **Abstract**: While financial data presents one of the most challenging and interesting sequence modelling tasks due to high noise, heavy tails, and strategic interactions, progress in this area has been hindered by the lack of consensus on quantitative evaluation paradigms. To address this, we present LOB-Bench, a benchmark, implemented in python, designed to evaluate the quality and realism of generative message-by-order data for limit order books (LOB) in the LOBSTER format. Our framework measures distributional differences in conditional and unconditional statistics between generated and real LOB data, supporting flexible multivariate statistical evaluation. The benchmark also includes features commonly used LOB statistics such as spread, order book volumes, order imbalance, and message inter-arrival times, along with scores from a trained discriminator network. Lastly, LOB-Bench contains "market impact metrics", i.e. the cross-correlations and price response functions for specific events in the data. We benchmark generative autoregressive state-space models, a (C)GAN, as well as a parametric LOB model and find that the autoregressive GenAI approach beats traditional model classes.

### Vertical Federated Continual Learning via Evolving Prototype Knowledge 
[[arxiv](https://arxiv.org/abs/2502.09152)] [[cool](https://papers.cool/arxiv/2502.09152)] [[pdf](https://arxiv.org/pdf/2502.09152)]
> **Authors**: Shuo Wang,Keke Gai,Jing Yu,Liehuang Zhu,Qi Wu
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,神经和进化计算
- **Abstract**: Vertical Federated Learning (VFL) has garnered significant attention as a privacy-preserving machine learning framework for sample-aligned feature federation. However, traditional VFL approaches do not address the challenges of class and feature continual learning, resulting in catastrophic forgetting of knowledge from previous tasks. To address the above challenge, we propose a novel vertical federated continual learning method, named Vertical Federated Continual Learning via Evolving Prototype Knowledge (V-LETO), which primarily facilitates the transfer of knowledge from previous tasks through the evolution of prototypes. Specifically, we propose an evolving prototype knowledge method, enabling the global model to retain both previous and current task knowledge. Furthermore, we introduce a model optimization technique that mitigates the forgetting of previous task knowledge by restricting updates to specific parameters of the local model, thereby enhancing overall performance. Extensive experiments conducted in both CIL and FIL settings demonstrate that our method, V-LETO, outperforms the other state-of-the-art methods. For example, our method outperforms the state-of-the-art method by 10.39% and 35.15% for CIL and FIL tasks, respectively. Our code is available at https://anonymous.4open.science/r/V-LETO-0108/README.md.

### Regularization can make diffusion models more efficient 
[[arxiv](https://arxiv.org/abs/2502.09151)] [[cool](https://papers.cool/arxiv/2502.09151)] [[pdf](https://arxiv.org/pdf/2502.09151)]
> **Authors**: Mahsa Taheri,Johannes Lederer
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,统计理论,机器学习
- **Abstract**: Diffusion models are one of the key architectures of generative AI. Their main drawback, however, is the computational costs. This study indicates that the concept of sparsity, well known especially in statistics, can provide a pathway to more efficient diffusion pipelines. Our mathematical guarantees prove that sparsity can reduce the input dimension's influence on the computational complexity to that of a much smaller intrinsic dimension of the data. Our empirical findings confirm that inducing sparsity can indeed lead to better samples at a lower cost.

### Shortcut Learning Susceptibility in Vision Classifiers 
[[arxiv](https://arxiv.org/abs/2502.09150)] [[cool](https://papers.cool/arxiv/2502.09150)] [[pdf](https://arxiv.org/pdf/2502.09150)]
> **Authors**: Pirzada Suhail,Amit Sethi
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: Shortcut learning, where machine learning models exploit spurious correlations in data instead of capturing meaningful features, poses a significant challenge to building robust and generalizable models. This phenomenon is prevalent across various machine learning applications, including vision, natural language processing, and speech recognition, where models may find unintended cues that minimize training loss but fail to capture the underlying structure of the data. Vision classifiers such as Convolutional Neural Networks (CNNs), Multi-Layer Perceptrons (MLPs), and Vision Transformers (ViTs) leverage distinct architectural principles to process spatial and structural information, making them differently susceptible to shortcut learning. In this study, we systematically evaluate these architectures by introducing deliberate shortcuts into the dataset that are positionally correlated with class labels, creating a controlled setup to assess whether models rely on these artificial cues or learn actual distinguishing features. We perform both quantitative evaluation by training on the shortcut-modified dataset and testing them on two different test sets -- one containing the same shortcuts and another without them -- to determine the extent of reliance on shortcuts. Additionally, qualitative evaluation is performed by using network inversion-based reconstruction techniques to analyze what the models internalize in their weights, aiming to reconstruct the training data as perceived by the classifiers. We evaluate shortcut learning behavior across multiple benchmark datasets, including MNIST, Fashion-MNIST, SVHN, and CIFAR-10, to compare the susceptibility of different vision classifier architectures to shortcut reliance and assess their varying degrees of sensitivity to spurious correlations.

### Replay-free Online Continual Learning with Self-Supervised MultiPatches 
[[arxiv](https://arxiv.org/abs/2502.09140)] [[cool](https://papers.cool/arxiv/2502.09140)] [[pdf](https://arxiv.org/pdf/2502.09140)]
> **Authors**: Giacomo Cignoni,Andrea Cossu,Alex Gomez-Villa,Joost van de Weijer,Antonio Carta
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: Accepted at ESANN 2025
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: Online Continual Learning (OCL) methods train a model on a non-stationary data stream where only a few examples are available at a time, often leveraging replay strategies. However, usage of replay is sometimes forbidden, especially in applications with strict privacy regulations. Therefore, we propose Continual MultiPatches (CMP), an effective plug-in for existing OCL self-supervised learning strategies that avoids the use of replay samples. CMP generates multiple patches from a single example and projects them into a shared feature space, where patches coming from the same example are pushed together without collapsing into a single point. CMP surpasses replay and other SSL-based strategies on OCL streams, challenging the role of replay as a go-to solution for self-supervised OCL.

### Trust Me, I Know the Way: Predictive Uncertainty in the Presence of Shortcut Learning 
[[arxiv](https://arxiv.org/abs/2502.09137)] [[cool](https://papers.cool/arxiv/2502.09137)] [[pdf](https://arxiv.org/pdf/2502.09137)]
> **Authors**: Lisa Wimmer,Bernd Bischl,Ludwig Bothmann
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: Preprint. Under review
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The correct way to quantify predictive uncertainty in neural networks remains a topic of active discussion. In particular, it is unclear whether the state-of-the art entropy decomposition leads to a meaningful representation of model, or epistemic, uncertainty (EU) in the light of a debate that pits ignorance against disagreement perspectives. We aim to reconcile the conflicting viewpoints by arguing that both are valid but arise from different learning situations. Notably, we show that the presence of shortcuts is decisive for EU manifesting as disagreement.

### Interpreting and Steering Protein Language Models through Sparse Autoencoders 
[[arxiv](https://arxiv.org/abs/2502.09135)] [[cool](https://papers.cool/arxiv/2502.09135)] [[pdf](https://arxiv.org/pdf/2502.09135)]
> **Authors**: Edith Natalia Villegas Garcia,Alessio Ansuini
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 11 pages, 6 figures
- **标题**: None
- **领域**: 机器学习,生物分子
- **Abstract**: The rapid advancements in transformer-based language models have revolutionized natural language processing, yet understanding the internal mechanisms of these models remains a significant challenge. This paper explores the application of sparse autoencoders (SAE) to interpret the internal representations of protein language models, specifically focusing on the ESM-2 8M parameter model. By performing a statistical analysis on each latent component's relevance to distinct protein annotations, we identify potential interpretations linked to various protein characteristics, including transmembrane regions, binding sites, and specialized motifs. We then leverage these insights to guide sequence generation, shortlisting the relevant latent components that can steer the model towards desired targets such as zinc finger domains. This work contributes to the emerging field of mechanistic interpretability in biological sequence models, offering new perspectives on model steering for sequence design.

### Finite-Time Analysis of Discrete-Time Stochastic Interpolants 
[[arxiv](https://arxiv.org/abs/2502.09130)] [[cool](https://papers.cool/arxiv/2502.09130)] [[pdf](https://arxiv.org/pdf/2502.09130)]
> **Authors**: Yuhao Liu,Yu Chen,Rui Hu,Longbo Huang
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The stochastic interpolant framework offers a powerful approach for constructing generative models based on ordinary differential equations (ODEs) or stochastic differential equations (SDEs) to transform arbitrary data distributions. However, prior analyses of this framework have primarily focused on the continuous-time setting, assuming a perfect solution of the underlying equations. In this work, we present the first discrete-time analysis of the stochastic interpolant framework, where we introduce an innovative discrete-time sampler and derive a finite-time upper bound on its distribution estimation error. Our result provides a novel quantification of how different factors, including the distance between source and target distributions and estimation accuracy, affect the convergence rate and also offers a new principled way to design efficient schedules for convergence acceleration. Finally, numerical experiments are conducted on the discrete-time sampler to corroborate our theoretical findings.

### Improving Deep Regression with Tightness 
[[arxiv](https://arxiv.org/abs/2502.09122)] [[cool](https://papers.cool/arxiv/2502.09122)] [[pdf](https://arxiv.org/pdf/2502.09122)]
> **Authors**: Shihao Zhang,Yuguang Yan,Angela Yao
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: ICLR 2025, Code: https://github.com/needylove/Regression_tightness
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: For deep regression, preserving the ordinality of the targets with respect to the feature representation improves performance across various tasks. However, a theoretical explanation for the benefits of ordinality is still lacking. This work reveals that preserving ordinality reduces the conditional entropy $H(Z|Y)$ of representation $Z$ conditional on the target $Y$. However, our findings reveal that typical regression losses do little to reduce $H(Z|Y)$, even though it is vital for generalization performance. With this motivation, we introduce an optimal transport-based regularizer to preserve the similarity relationships of targets in the feature space to reduce $H(Z|Y)$. Additionally, we introduce a simple yet efficient strategy of duplicating the regressor targets, also with the aim of reducing $H(Z|Y)$. Experiments on three real-world regression tasks verify the effectiveness of our strategies to improve deep regression. Code: https://github.com/needylove/Regression_tightness.

### Scaling Law for Stochastic Gradient Descent in Quadratically Parameterized Linear Regression 
[[arxiv](https://arxiv.org/abs/2502.09106)] [[cool](https://papers.cool/arxiv/2502.09106)] [[pdf](https://arxiv.org/pdf/2502.09106)]
> **Authors**: Shihong Ding,Haihan Zhang,Hanzhen Zhao,Cong Fang
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: In machine learning, the scaling law describes how the model performance improves with the model and data size scaling up. From a learning theory perspective, this class of results establishes upper and lower generalization bounds for a specific learning algorithm. Here, the exact algorithm running using a specific model parameterization often offers a crucial implicit regularization effect, leading to good generalization. To characterize the scaling law, previous theoretical studies mainly focus on linear models, whereas, feature learning, a notable process that contributes to the remarkable empirical success of neural networks, is regretfully vacant. This paper studies the scaling law over a linear regression with the model being quadratically parameterized. We consider infinitely dimensional data and slope ground truth, both signals exhibiting certain power-law decay rates. We study convergence rates for Stochastic Gradient Descent and demonstrate the learning rates for variables will automatically adapt to the ground truth. As a result, in the canonical linear regression, we provide explicit separations for generalization curves between SGD with and without feature learning, and the information-theoretical lower bound that is agnostic to parametrization method and the algorithm. Our analysis for decaying ground truth provides a new characterization for the learning dynamic of the model.

### One-shot Federated Learning Methods: A Practical Guide 
[[arxiv](https://arxiv.org/abs/2502.09104)] [[cool](https://papers.cool/arxiv/2502.09104)] [[pdf](https://arxiv.org/pdf/2502.09104)]
> **Authors**: Xiang Liu,Zhenheng Tang,Xia Li,Yijun Song,Sijie Ji,Zemin Liu,Bo Han,Linshan Jiang,Jialin Li
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 10 pages, 1 figure
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: One-shot Federated Learning (OFL) is a distributed machine learning paradigm that constrains client-server communication to a single round, addressing privacy and communication overhead issues associated with multiple rounds of data exchange in traditional Federated Learning (FL). OFL demonstrates the practical potential for integration with future approaches that require collaborative training models, such as large language models (LLMs). However, current OFL methods face two major challenges: data heterogeneity and model heterogeneity, which result in subpar performance compared to conventional FL methods. Worse still, despite numerous studies addressing these limitations, a comprehensive summary is still lacking. To address these gaps, this paper presents a systematic analysis of the challenges faced by OFL and thoroughly reviews the current methods. We also offer an innovative categorization method and analyze the trade-offs of various techniques. Additionally, we discuss the most promising future directions and the technologies that should be integrated into the OFL field. This work aims to provide guidance and insights for future research.

### FlowAR: une plateforme uniformisée pour la reconnaissance des activités humaines à partir de capteurs binaires 
[[arxiv](https://arxiv.org/abs/2502.09067)] [[cool](https://papers.cool/arxiv/2502.09067)] [[pdf](https://arxiv.org/pdf/2502.09067)]
> **Authors**: Ali Ncibi,Luc Bouganim,Philippe Pucheral
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: in Frenchlanguagehttps://editions-rnti.fr/?inprocid=1003044
- **标题**: None
- **领域**: 机器学习
- **Abstract**: This demo showcases a platform for developing human activity recognition (AR) systems, focusing on daily activities using sensor data, like binary sensors. With a data-driven approach, this platform, named FlowAR, features a three-step pipeline (flow): data cleaning, segmentation, and personalized classification. Its modularity allows flexibility to test methods, datasets, and ensure rigorous evaluations. A concrete use case demonstrates its effectiveness.

### Zero-shot Concept Bottleneck Models 
[[arxiv](https://arxiv.org/abs/2502.09018)] [[cool](https://papers.cool/arxiv/2502.09018)] [[pdf](https://arxiv.org/pdf/2502.09018)]
> **Authors**: Shin'ya Yamaguchi,Kosuke Nishida,Daiki Chijiwa,Yasutoshi Ida
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 14 pages, 8 figures
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: Concept bottleneck models (CBMs) are inherently interpretable and intervenable neural network models, which explain their final label prediction by the intermediate prediction of high-level semantic concepts. However, they require target task training to learn input-to-concept and concept-to-label mappings, incurring target dataset collections and training resources. In this paper, we present \textit{zero-shot concept bottleneck models} (Z-CBMs), which predict concepts and labels in a fully zero-shot manner without training neural networks. Z-CBMs utilize a large-scale concept bank, which is composed of millions of vocabulary extracted from the web, to describe arbitrary input in various domains. For the input-to-concept mapping, we introduce concept retrieval, which dynamically finds input-related concepts by the cross-modal search on the concept bank. In the concept-to-label inference, we apply concept regression to select essential concepts from the retrieved concepts by sparse linear regression. Through extensive experiments, we confirm that our Z-CBMs provide interpretable and intervenable concepts without any additional training. Code will be available at https://github.com/yshinya6/zcbm.

### RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach for Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.09003)] [[cool](https://papers.cool/arxiv/2502.09003)] [[pdf](https://arxiv.org/pdf/2502.09003)]
> **Authors**: Quan Wei,Chung-Yiu Yau,Hoi-To Wai,Yang,Zhao,Dongyeop Kang,Youngsuk Park,Mingyi Hong
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 18 pages, 6 figures
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Supervised fine-tuning is a standard method for adapting pre-trained large language models (LLMs) to downstream tasks. Quantization has been recently studied as a post-training technique for efficient LLM deployment. To obtain quantized fine-tuned LLMs, conventional pipelines would first fine-tune the pre-trained models, followed by post-training quantization. This often yields suboptimal performance as it fails to leverage the synergy between fine-tuning and quantization. To effectively realize low-bit quantization of weights, activations, and KV caches in LLMs, we propose an algorithm named Rotated Straight-Through-Estimator (RoSTE), which combines quantization-aware supervised fine-tuning (QA-SFT) with an adaptive rotation strategy that identifies an effective rotation configuration to reduce activation outliers. We provide theoretical insights on RoSTE by analyzing its prediction error when applied to an overparameterized least square quantized training problem. Our findings reveal that the prediction error is directly proportional to the quantization error of the converged weights, which can be effectively managed through an optimized rotation configuration. Experiments on Pythia and Llama models of different sizes demonstrate the effectiveness of RoSTE. Compared to existing post-SFT quantization baselines, our method consistently achieves superior performances across various tasks and different LLM architectures.

### End-to-End triplet loss based fine-tuning for network embedding in effective PII detection 
[[arxiv](https://arxiv.org/abs/2502.09002)] [[cool](https://papers.cool/arxiv/2502.09002)] [[pdf](https://arxiv.org/pdf/2502.09002)]
> **Authors**: Rishika Kohli,Shaifu Gupta,Manoj Singh Gaur
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 13 pages, 10 figures, 5 tables
- **标题**: None
- **领域**: 机器学习
- **Abstract**: There are many approaches in mobile data ecosystem that inspect network traffic generated by applications running on user's device to detect personal data exfiltration from the user's device. State-of-the-art methods rely on features extracted from HTTP requests and in this context, machine learning involves training classifiers on these features and making predictions using labelled packet traces. However, most of these methods include external feature selection before model training. Deep learning, on the other hand, typically does not require such techniques, as it can autonomously learn and identify patterns in the data without external feature extraction or selection algorithms. In this article, we propose a novel deep learning based end-to-end learning framework for prediction of exposure of personally identifiable information (PII) in mobile packets. The framework employs a pre-trained large language model (LLM) and an autoencoder to generate embedding of network packets and then uses a triplet-loss based fine-tuning method to train the model, increasing detection effectiveness using two real-world datasets. We compare our proposed detection framework with other state-of-the-art works in detecting PII leaks from user's device.

### Privacy-Preserving Hybrid Ensemble Model for Network Anomaly Detection: Balancing Security and Data Protection 
[[arxiv](https://arxiv.org/abs/2502.09001)] [[cool](https://papers.cool/arxiv/2502.09001)] [[pdf](https://arxiv.org/pdf/2502.09001)]
> **Authors**: Shaobo Liu,Zihao Zhao,Weijie He,Jiren Wang,Jing Peng,Haoyuan Ma
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: Accepted by 2024 5th International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering(ICBAIE 2024)
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Privacy-preserving network anomaly detection has become an essential area of research due to growing concerns over the protection of sensitive data. Traditional anomaly detection models often prioritize accuracy while neglecting the critical aspect of privacy. In this work, we propose a hybrid ensemble model that incorporates privacy-preserving techniques to address both detection accuracy and data protection. Our model combines the strengths of several machine learning algorithms, including K-Nearest Neighbors (KNN), Support Vector Machines (SVM), XGBoost, and Artificial Neural Networks (ANN), to create a robust system capable of identifying network anomalies while ensuring privacy. The proposed approach integrates advanced preprocessing techniques that enhance data quality and address the challenges of small sample sizes and imbalanced datasets. By embedding privacy measures into the model design, our solution offers a significant advancement over existing methods, ensuring both enhanced detection performance and strong privacy safeguards.

### Task Generalization With AutoRegressive Compositional Structure: Can Learning From $\d$ Tasks Generalize to $\d^{T}$ Tasks? 
[[arxiv](https://arxiv.org/abs/2502.08991)] [[cool](https://papers.cool/arxiv/2502.08991)] [[pdf](https://arxiv.org/pdf/2502.08991)]
> **Authors**: Amirhesam Abedsoltan,Huaqing Zhang,Kaiyue Wen,Hongzhou Lin,Jingzhao Zhang,Mikhail Belkin
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Large language models (LLMs) exhibit remarkable task generalization, solving tasks they were never explicitly trained on with only a few demonstrations. This raises a fundamental question: When can learning from a small set of tasks generalize to a large task family? In this paper, we investigate task generalization through the lens of AutoRegressive Compositional (ARC) structure, where each task is a composition of $T$ operations, and each operation is among a finite family of $\d$ subtasks. This yields a total class of size~\( \d^\TT \). We first show that generalization to all \( \d^\TT \) tasks is theoretically achievable by training on only \( \tilde{O}(\d) \) tasks. Empirically, we demonstrate that Transformers achieve such exponential task generalization on sparse parity functions via in-context learning (ICL) and Chain-of-Thought (CoT) reasoning. We further demonstrate this generalization in arithmetic and language translation, extending beyond parity functions.

### Neural Force Field: Learning Generalized Physical Representation from a Few Examples 
[[arxiv](https://arxiv.org/abs/2502.08987)] [[cool](https://papers.cool/arxiv/2502.08987)] [[pdf](https://arxiv.org/pdf/2502.08987)]
> **Authors**: Shiqian Li,Ruihong Shen,Chi Zhang,Yixin Zhu
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 20 pages
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Physical reasoning is a remarkable human ability that enables rapid learning and generalization from limited experience. Current AI models, despite extensive training, still struggle to achieve similar generalization, especially in Out-of-distribution (OOD) settings. This limitation stems from their inability to abstract core physical principles from observations. A key challenge is developing representations that can efficiently learn and generalize physical dynamics from minimal data. Here we present Neural Force Field (NFF) a modeling framework built on Neural Ordinary Differential Equation (NODE) that learns interpretable force field representations which can be efficiently integrated through an Ordinary Differential Equation ( ODE) solver to predict object trajectories. Unlike existing approaches that rely on high-dimensional latent spaces, NFF captures fundamental physical concepts such as gravity, support, and collision in an interpretable manner. Experiments on two challenging physical reasoning tasks demonstrate that NFF, trained with only a few examples, achieves strong generalization to unseen scenarios. This physics-grounded representation enables efficient forward-backward planning and rapid adaptation through interactive refinement. Our work suggests that incorporating physics-inspired representations into learning systems can help bridge the gap between artificial and human physical reasoning capabilities.

### Few is More: Task-Efficient Skill-Discovery for Multi-Task Offline Multi-Agent Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.08985)] [[cool](https://papers.cool/arxiv/2502.08985)] [[pdf](https://arxiv.org/pdf/2502.08985)]
> **Authors**: Xun Wang,Zhuoran Li,Hai Zhong,Longbo Huang
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,多代理系统
- **Abstract**: As a data-driven approach, offline MARL learns superior policies solely from offline datasets, ideal for domains rich in historical data but with high interaction costs and risks. However, most existing methods are task-specific, requiring retraining for new tasks, leading to redundancy and inefficiency. To address this issue, in this paper, we propose a task-efficient multi-task offline MARL algorithm, Skill-Discovery Conservative Q-Learning (SD-CQL). Unlike existing offline skill-discovery methods, SD-CQL discovers skills by reconstructing the next observation. It then evaluates fixed and variable actions separately and employs behavior-regularized conservative Q-learning to execute the optimal action for each skill. This approach eliminates the need for local-global alignment and enables strong multi-task generalization from limited small-scale source tasks. Substantial experiments on StarCraftII demonstrates the superior generalization performance and task-efficiency of SD-CQL. It achieves the best performance on $\textbf{10}$ out of $14$ task sets, with up to $\textbf{65%}$ improvement on individual task sets, and is within $4\%$ of the best baseline on the remaining four.

### What exactly has TabPFN learned to do? 
[[arxiv](https://arxiv.org/abs/2502.08978)] [[cool](https://papers.cool/arxiv/2502.08978)] [[pdf](https://arxiv.org/pdf/2502.08978)]
> **Authors**: Calvin McCarter
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: Originally published in Blogposts Track at ICLR 2024. Appendix contains re-analysis on TabPFN-v2 [Hollmann et al., 2025]
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: TabPFN [Hollmann et al., 2023], a Transformer model pretrained to perform in-context learning on fresh tabular classification problems, was presented at the last ICLR conference. To better understand its behavior, we treat it as a black-box function approximator generator and observe its generated function approximations on a varied selection of training datasets. Exploring its learned inductive biases in this manner, we observe behavior that is at turns either brilliant or baffling. We conclude this post with thoughts on how these results might inform the development, evaluation, and application of prior-data fitted networks (PFNs) in the future.

### Small Molecule Drug Discovery Through Deep Learning:Progress, Challenges, and Opportunities 
[[arxiv](https://arxiv.org/abs/2502.08975)] [[cool](https://papers.cool/arxiv/2502.08975)] [[pdf](https://arxiv.org/pdf/2502.08975)]
> **Authors**: Kun Li,Yida Xiong,Hongzhi Zhang,Xiantao Cai,Bo Du,Wenbin Hu
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 9 pages, 1 figures, 8 tables
- **标题**: None
- **领域**: 机器学习,生物分子
- **Abstract**: Due to their excellent drug-like and pharmacokinetic properties, small molecule drugs are widely used to treat various diseases, making them a critical component of drug discovery. In recent years, with the rapid development of deep learning (DL) techniques, DL-based small molecule drug discovery methods have achieved excellent performance in prediction accuracy, speed, and complex molecular relationship modeling compared to traditional machine learning approaches. These advancements enhance drug screening efficiency and optimization, and they provide more precise and effective solutions for various drug discovery tasks. Contributing to this field's development, this paper aims to systematically summarize and generalize the recent key tasks and representative techniques in DL-based small molecule drug discovery in recent years. Specifically, we provide an overview of the major tasks in small molecule drug discovery and their interrelationships. Next, we analyze the six core tasks, summarizing the related methods, commonly used datasets, and technological development trends. Finally, we discuss key challenges, such as interpretability and out-of-distribution generalization, and offer our insights into future research directions for DL-assisted small molecule drug discovery.

## 计算机科学中的逻辑(cs.LO:Logic in Computer Science)

### Reliable Conversational Agents under ASP Control that Understand Natural Language 
[[arxiv](https://arxiv.org/abs/2502.09237)] [[cool](https://papers.cool/arxiv/2502.09237)] [[pdf](https://arxiv.org/pdf/2502.09237)]
> **Authors**: Yankai Zeng
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: In Proceedings ICLP 2024, arXiv:2502.08453
- **标题**: None
- **领域**: 计算机科学中的逻辑,计算语言学
- **Abstract**: Efforts have been made to make machines converse like humans in the past few decades. The recent techniques of Large Language Models (LLMs) make it possible to have human-like conversations with machines, but LLM's flaws of lacking understanding and reliability are well documented. We believe that the best way to eliminate this problem is to use LLMs only as parsers to translate text to knowledge and vice versa and carry out the conversation by reasoning over this knowledge using the answer set programming. I have been developing a framework based on LLMs and ASP to realize reliable chatbots that "understand" human conversation. This framework has been used to develop task-specific chatbots as well as socialbots. My future research is focused on making these chatbots scalable and trainable.

### Logical foundations of Smart Contracts 
[[arxiv](https://arxiv.org/abs/2502.09232)] [[cool](https://papers.cool/arxiv/2502.09232)] [[pdf](https://arxiv.org/pdf/2502.09232)]
> **Authors**: Kalonji Kalala
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: In Proceedings ICLP 2024, arXiv:2502.08453
- **标题**: None
- **领域**: 计算机科学中的逻辑,人工智能
- **Abstract**: Nowadays, sophisticated domains are emerging which require appropriate formalisms to be specified accurately in order to reason about them. One such domain is constituted of smart contracts that have emerged in cyber physical systems as a way of enforcing formal agreements between components of these systems. Smart contracts self-execute to run and share business processes through blockchain, in decentralized systems, with many different participants. Legal contracts are in many cases complex documents, with a number of exceptions, and many subcontracts. The implementation of smart contracts based on legal contracts is a long and laborious task, that needs to include all actions, procedures, and the effects of actions related to the execution of the contract. An ongoing open problem in this area is to formally account for smart contracts using a uniform and somewhat universal formalism. This thesis proposes logical foundations to smart contracts using the Situation Calculus, a logic for reasoning about actions. Situation Calculus is one of the prominent logic-based artificial intelligence approaches that provides enough logical mechanism to specify and implement dynamic and complex systems such as contracts. Situation Calculus is suitable to show how worlds dynamically change. Smart contracts are going to be implement with Golog (written en Prolog), a Situation Calculus-based programming language for modeling complex and dynamic behaviors.

### Abduction of Domain Relationships from Data for VQA 
[[arxiv](https://arxiv.org/abs/2502.09219)] [[cool](https://papers.cool/arxiv/2502.09219)] [[pdf](https://arxiv.org/pdf/2502.09219)]
> **Authors**: Al Mehdi Saadat Chowdhury,Paulo Shakarian,Gerardo I. Simari
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: In Proceedings ICLP 2024, arXiv:2502.08453
- **标题**: None
- **领域**: 计算机科学中的逻辑,人工智能,机器学习
- **Abstract**: In this paper, we study the problem of visual question answering (VQA) where the image and query are represented by ASP programs that lack domain data. We provide an approach that is orthogonal and complementary to existing knowledge augmentation techniques where we abduce domain relationships of image constructs from past examples. After framing the abduction problem, we provide a baseline approach, and an implementation that significantly improves the accuracy of query answering yet requires few examples.

### Data2Concept2Text: An Explainable Multilingual Framework for Data Analysis Narration 
[[arxiv](https://arxiv.org/abs/2502.09218)] [[cool](https://papers.cool/arxiv/2502.09218)] [[pdf](https://arxiv.org/pdf/2502.09218)]
> **Authors**: Flavio Bertini,Alessandro Dal Palù,Federica Zaglio,Francesco Fabiano,Andrea Formisano
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: In Proceedings ICLP 2024, arXiv:2502.08453
- **标题**: None
- **领域**: 计算机科学中的逻辑,人工智能
- **Abstract**: This paper presents a complete explainable system that interprets a set of data, abstracts the underlying features and describes them in a natural language of choice. The system relies on two crucial stages: (i) identifying emerging properties from data and transforming them into abstract concepts, and (ii) converting these concepts into natural language. Despite the impressive natural language generation capabilities demonstrated by Large Language Models, their statistical nature and the intricacy of their internal mechanism still force us to employ these techniques as black boxes, forgoing trustworthiness. Developing an explainable pipeline for data interpretation would allow facilitating its use in safety-critical environments like processing medical information and allowing non-experts and visually impaired people to access narrated information. To this end, we believe that the fields of knowledge representation and automated reasoning research could present a valid alternative. Expanding on prior research that tackled the first stage (i), we focus on the second stage, named Concept2Text. Being explainable, data translation is easily modeled through logic-based rules, once again emphasizing the role of declarative programming in achieving AI explainability. This paper explores a Prolog/CLP-based rewriting system to interpret concepts-articulated in terms of classes and relations, plus common knowledge-derived from a generic ontology, generating natural language text. Its main features include hierarchical tree rewritings, modular multilingual generation, support for equivalent variants across semantic, grammar, and lexical levels, and a transparent rule-based system. We outline the architecture and demonstrate its flexibility through some examples capable of generating numerous diverse and equivalent rewritings based on the input concept.

### Efficient OWL2QL Meta-reasoning Using ASP-based Hybrid Knowledge Bases 
[[arxiv](https://arxiv.org/abs/2502.09206)] [[cool](https://papers.cool/arxiv/2502.09206)] [[pdf](https://arxiv.org/pdf/2502.09206)]
> **Authors**: Haya Majid Qureshi,Wolfgang Faber
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: In Proceedings ICLP 2024, arXiv:2502.08453
- **标题**: None
- **领域**: 计算机科学中的逻辑,人工智能,符号计算
- **Abstract**: Metamodeling refers to scenarios in ontologies in which classes and roles can be members of classes or occur in roles. This is a desirable modelling feature in several applications, but allowing it without restrictions is problematic for several reasons, mainly because it causes undecidability. Therefore, practical languages either forbid metamodeling explicitly or treat occurrences of classes as instances to be semantically different from other occurrences, thereby not allowing metamodeling semantically. Several extensions have been proposed to provide metamodeling to some extent. Building on earlier work that reduces metamodeling query answering to Datalog query answering, recently reductions to query answering over hybrid knowledge bases were proposed with the aim of using the Datalog transformation only where necessary. Preliminary work showed that the approach works, but the hoped-for performance improvements were not observed yet. In this work we expand on this body of work by improving the theoretical basis of the reductions and by using alternative tools that show competitive performance.

## 多代理系统(cs.MA:Multiagent Systems)

### Evaluating and Improving Graph-based Explanation Methods for Multi-Agent Coordination 
[[arxiv](https://arxiv.org/abs/2502.09889)] [[cool](https://papers.cool/arxiv/2502.09889)] [[pdf](https://arxiv.org/pdf/2502.09889)]
> **Authors**: Siva Kailas,Shalin Jain,Harish Ravichandar
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 19 pages, 8 figures, 6 tables
- **标题**: None
- **领域**: 多代理系统,人工智能,机器学习,机器人技术
- **Abstract**: Graph Neural Networks (GNNs), developed by the graph learning community, have been adopted and shown to be highly effective in multi-robot and multi-agent learning. Inspired by this successful cross-pollination, we investigate and characterize the suitability of existing GNN explanation methods for explaining multi-agent coordination. We find that these methods have the potential to identify the most-influential communication channels that impact the team's behavior. Informed by our initial analyses, we propose an attention entropy regularization term that renders GAT-based policies more amenable to existing graph-based explainers. Intuitively, minimizing attention entropy incentivizes agents to limit their attention to the most influential or impactful agents, thereby easing the challenge faced by the explainer. We theoretically ground this intuition by showing that minimizing attention entropy increases the disparity between the explainer-generated subgraph and its complement. Evaluations across three tasks and three team sizes i) provides insights into the effectiveness of existing explainers, and ii) demonstrates that our proposed regularization consistently improves explanation quality without sacrificing task performance.

## 表现(cs.PF:Performance)

### PixLift: Accelerating Web Browsing via AI Upscaling 
[[arxiv](https://arxiv.org/abs/2502.08995)] [[cool](https://papers.cool/arxiv/2502.08995)] [[pdf](https://arxiv.org/pdf/2502.08995)]
> **Authors**: Yonas Atinafu,Sarthak Malla,HyunSeok Daniel Jang,Nouar Aldahoul,Matteo Varvello,Yasir Zaki
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 9 pages, 2 figures
- **标题**: None
- **领域**: 表现,人工智能
- **Abstract**: Accessing the internet in regions with expensive data plans and limited connectivity poses significant challenges, restricting information access and economic growth. Images, as a major contributor to webpage sizes, exacerbate this issue, despite advances in compression formats like WebP and AVIF. The continued growth of complex and curated web content, coupled with suboptimal optimization practices in many regions, has prevented meaningful reductions in web page sizes. This paper introduces PixLift, a novel solution to reduce webpage sizes by downscaling their images during transmission and leveraging AI models on user devices to upscale them. By trading computational resources for bandwidth, PixLift enables more affordable and inclusive web access. We address key challenges, including the feasibility of scaled image requests on popular websites, the implementation of PixLift as a browser extension, and its impact on user experience. Through the analysis of 71.4k webpages, evaluations of three mainstream upscaling models, and a user study, we demonstrate PixLift's ability to significantly reduce data usage without compromising image quality, fostering a more equitable internet.

## 编程语言(cs.PL:Programming Languages)

### CRANE: Reasoning with constrained LLM generation 
[[arxiv](https://arxiv.org/abs/2502.09061)] [[cool](https://papers.cool/arxiv/2502.09061)] [[pdf](https://arxiv.org/pdf/2502.09061)]
> **Authors**: Debangshu Banerjee,Tarun Suresh,Shubham Ugare,Sasa Misailovic,Gagandeep Singh
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 编程语言,机器学习
- **Abstract**: Code generation, symbolic math reasoning, and other tasks require LLMs to produce outputs that are both syntactically and semantically correct. Constrained LLM generation is a promising direction to enforce adherence to formal grammar, but prior works have empirically observed that strict enforcement of formal constraints often diminishes the reasoning capabilities of LLMs. In this work, we first provide a theoretical explanation for why constraining LLM outputs to very restrictive grammars that only allow syntactically valid final answers reduces the reasoning capabilities of the model. Second, we demonstrate that by augmenting the output grammar with carefully designed additional rules, it is always possible to preserve the reasoning capabilities of the LLM while ensuring syntactic and semantic correctness in its outputs. Building on these theoretical insights, we propose a reasoning-augmented constrained decoding algorithm, CRANE, which effectively balances the correctness of constrained generation with the flexibility of unconstrained generation. Experiments on multiple open-source LLMs and benchmarks show that CRANE significantly outperforms both state-of-the-art constrained decoding strategies and standard unconstrained decoding, showing up to 10% points accuracy improvement over baselines on challenging symbolic reasoning benchmarks GSM-symbolic and FOLIO.

## 机器人技术(cs.RO:Robotics)

### Video2Policy: Scaling up Manipulation Tasks in Simulation through Internet Videos 
[[arxiv](https://arxiv.org/abs/2502.09886)] [[cool](https://papers.cool/arxiv/2502.09886)] [[pdf](https://arxiv.org/pdf/2502.09886)]
> **Authors**: Weirui Ye,Fangchen Liu,Zheng Ding,Yang Gao,Oleh Rybkin,Pieter Abbeel
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,人工智能,机器学习
- **Abstract**: Simulation offers a promising approach for cheaply scaling training data for generalist policies. To scalably generate data from diverse and realistic tasks, existing algorithms either rely on large language models (LLMs) that may hallucinate tasks not interesting for robotics; or digital twins, which require careful real-to-sim alignment and are hard to scale. To address these challenges, we introduce Video2Policy, a novel framework that leverages internet RGB videos to reconstruct tasks based on everyday human behavior. Our approach comprises two phases: (1) task generation in simulation from videos; and (2) reinforcement learning utilizing in-context LLM-generated reward functions iteratively. We demonstrate the efficacy of Video2Policy by reconstructing over 100 videos from the Something-Something-v2 (SSv2) dataset, which depicts diverse and complex human behaviors on 9 different tasks. Our method can successfully train RL policies on such tasks, including complex and challenging tasks such as throwing. Finally, we show that the generated simulation data can be scaled up for training a general policy, and it can be transferred back to the real robot in a Real2Sim2Real way.

### Efficient Evaluation of Multi-Task Robot Policies With Active Experiment Selection 
[[arxiv](https://arxiv.org/abs/2502.09829)] [[cool](https://papers.cool/arxiv/2502.09829)] [[pdf](https://arxiv.org/pdf/2502.09829)]
> **Authors**: Abrar Anwar,Rohan Gupta,Zain Merchant,Sayan Ghosh,Willie Neiswanger,Jesse Thomason
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,人工智能,机器学习
- **Abstract**: Evaluating learned robot control policies to determine their physical task-level capabilities costs experimenter time and effort. The growing number of policies and tasks exacerbates this issue. It is impractical to test every policy on every task multiple times; each trial requires a manual environment reset, and each task change involves re-arranging objects or even changing robots. Naively selecting a random subset of tasks and policies to evaluate is a high-cost solution with unreliable, incomplete results. In this work, we formulate robot evaluation as an active testing problem. We propose to model the distribution of robot performance across all tasks and policies as we sequentially execute experiments. Tasks often share similarities that can reveal potential relationships in policy behavior, and we show that natural language is a useful prior in modeling these relationships between tasks. We then leverage this formulation to reduce the experimenter effort by using a cost-aware expected information gain heuristic to efficiently select informative trials. Our framework accommodates both continuous and discrete performance outcomes. We conduct experiments on existing evaluation data from real robots and simulations. By prioritizing informative trials, our framework reduces the cost of calculating evaluation metrics for robot policies across many tasks.

### Vote-Tree-Planner: Optimizing Execution Order in LLM-based Task Planning Pipeline via Voting 
[[arxiv](https://arxiv.org/abs/2502.09749)] [[cool](https://papers.cool/arxiv/2502.09749)] [[pdf](https://arxiv.org/pdf/2502.09749)]
> **Authors**: Chaoyuan Zhang,Zhaowei Li,Wentao Yuan
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: Accepted to RSS24-W: TaskSpec
- **标题**: None
- **领域**: 机器人技术,人工智能
- **Abstract**: Integrating large language models (LLMs) into closed-loop robotic task planning has become increasingly popular within embodied artificial intelligence. Previous efforts mainly focused on leveraging the strong reasoning abilities of LLMs to enhance task planning performance while often overlooking task planning efficiency and executability due to repetitive queries to LLMs. This paper addresses the synergy between LLMs and task planning systems, aiming to minimize redundancy while enhancing planning effectiveness. Specifically, building upon Prog-Prompt and the high-level concept of Tree-Planner, we propose Vote-Tree-Planner. This sampling strategy utilizes votes to guide plan traversal during the decision-making process. Our approach is motivated by a straightforward observation: assigning weights to agents during decision-making enables the evaluation of critical paths before execution. With this simple vote-tree construction, our method further improves the success rate and reduces the number of queries to LLMs. The experimental results highlight that our Vote-Tree-Planner demonstrates greater stability and shows a higher average success rate and goal condition recall on the unseen dataset compared with previous baseline methods. These findings underscore the potential of the Vote-Tree-Planner to enhance planning accuracy, reliability, and efficiency in LLM-based planning systems.

### DexTrack: Towards Generalizable Neural Tracking Control for Dexterous Manipulation from Human References 
[[arxiv](https://arxiv.org/abs/2502.09614)] [[cool](https://papers.cool/arxiv/2502.09614)] [[pdf](https://arxiv.org/pdf/2502.09614)]
> **Authors**: Xueyi Liu,Jianibieke Adalibieke,Qianwei Han,Yuzhe Qin,Li Yi
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: Accepted to ICLR 2025. Website: https://meowuu7.github.io/DexTrack/ Code: https://github.com/Meowuu7/DexTrack/ Video: https://youtu.be/zru1Z-DaiWE
- **标题**: None
- **领域**: 机器人技术,人工智能,计算机视觉和模式识别,机器学习
- **Abstract**: We address the challenge of developing a generalizable neural tracking controller for dexterous manipulation from human references. This controller aims to manage a dexterous robot hand to manipulate diverse objects for various purposes defined by kinematic human-object interactions. Developing such a controller is complicated by the intricate contact dynamics of dexterous manipulation and the need for adaptivity, generalizability, and robustness. Current reinforcement learning and trajectory optimization methods often fall short due to their dependence on task-specific rewards or precise system models. We introduce an approach that curates large-scale successful robot tracking demonstrations, comprising pairs of human references and robot actions, to train a neural controller. Utilizing a data flywheel, we iteratively enhance the controller's performance, as well as the number and quality of successful tracking demonstrations. We exploit available tracking demonstrations and carefully integrate reinforcement learning and imitation learning to boost the controller's performance in dynamic environments. At the same time, to obtain high-quality tracking demonstrations, we individually optimize per-trajectory tracking by leveraging the learned tracking controller in a homotopy optimization method. The homotopy optimization, mimicking chain-of-thought, aids in solving challenging trajectory tracking problems to increase demonstration diversity. We showcase our success by training a generalizable neural controller and evaluating it in both simulation and real world. Our method achieves over a 10% improvement in success rates compared to leading baselines. The project website with animated results is available at https://meowuu7.github.io/DexTrack/.

### Robot Pouring: Identifying Causes of Spillage and Selecting Alternative Action Parameters Using Probabilistic Actual Causation 
[[arxiv](https://arxiv.org/abs/2502.09395)] [[cool](https://papers.cool/arxiv/2502.09395)] [[pdf](https://arxiv.org/pdf/2502.09395)]
> **Authors**: Jaime Maldonado,Jonas Krumme,Christoph Zetzsche,Vanessa Didelez,Kerstin Schill
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 20 pages, 13 figures
- **标题**: None
- **领域**: 机器人技术,机器学习
- **Abstract**: In everyday life, we perform tasks (e.g., cooking or cleaning) that involve a large variety of objects and goals. When confronted with an unexpected or unwanted outcome, we take corrective actions and try again until achieving the desired result. The reasoning performed to identify a cause of the observed outcome and to select an appropriate corrective action is a crucial aspect of human reasoning for successful task execution. Central to this reasoning is the assumption that a factor is responsible for producing the observed outcome. In this paper, we investigate the use of probabilistic actual causation to determine whether a factor is the cause of an observed undesired outcome. Furthermore, we show how the actual causation probabilities can be used to find alternative actions to change the outcome. We apply the probabilistic actual causation analysis to a robot pouring task. When spillage occurs, the analysis indicates whether a task parameter is the cause and how it should be changed to avoid spillage. The analysis requires a causal graph of the task and the corresponding conditional probability distributions. To fulfill these requirements, we perform a complete causal modeling procedure (i.e., task analysis, definition of variables, determination of the causal graph structure, and estimation of conditional probability distributions) using data from a realistic simulation of the robot pouring task, covering a large combinatorial space of task parameters. Based on the results, we discuss the implications of the variables' representation and how the alternative actions suggested by the actual causation analysis would compare to the alternative solutions proposed by a human observer. The practical use of the analysis of probabilistic actual causation to select alternative action parameters is demonstrated.

### TRIFFID: Autonomous Robotic Aid For Increasing First Responders Efficiency 
[[arxiv](https://arxiv.org/abs/2502.09379)] [[cool](https://papers.cool/arxiv/2502.09379)] [[pdf](https://arxiv.org/pdf/2502.09379)]
> **Authors**: Jorgen Cani,Panagiotis Koletsis,Konstantinos Foteinos,Ioannis Kefaloukos,Lampros Argyriou,Manolis Falelakis,Iván Del Pino,Angel Santamaria-Navarro,Martin Čech,Ondřej Severa,Alessandro Umbrico,Francesca Fracasso,AndreA Orlandini,Dimitrios Drakoulis,Evangelos Markakis,Iraklis Varlamis,Georgios Th. Papadopoulos
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,人工智能
- **Abstract**: The increasing complexity of natural disaster incidents demands innovative technological solutions to support first responders in their efforts. This paper introduces the TRIFFID system, a comprehensive technical framework that integrates unmanned ground and aerial vehicles with advanced artificial intelligence functionalities to enhance disaster response capabilities across wildfires, urban floods, and post-earthquake search and rescue missions. By leveraging state-of-the-art autonomous navigation, semantic perception, and human-robot interaction technologies, TRIFFID provides a sophisticated system composed of the following key components: hybrid robotic platform, centralized ground station, custom communication infrastructure, and smartphone application. The defined research and development activities demonstrate how deep neural networks, knowledge graphs, and multimodal information fusion can enable robots to autonomously navigate and analyze disaster environments, reducing personnel risks and accelerating response times. The proposed system enhances emergency response teams by providing advanced mission planning, safety monitoring, and adaptive task execution capabilities. Moreover, it ensures real-time situational awareness and operational support in complex and risky situations, facilitating rapid and precise information collection and coordinated actions.

### GEVRM: Goal-Expressive Video Generation Model For Robust Visual Manipulation 
[[arxiv](https://arxiv.org/abs/2502.09268)] [[cool](https://papers.cool/arxiv/2502.09268)] [[pdf](https://arxiv.org/pdf/2502.09268)]
> **Authors**: Hongyin Zhang,Pengxiang Ding,Shangke Lyu,Ying Peng,Donglin Wang
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: Published as a conference paper at ICLR 2025
- **标题**: None
- **领域**: 机器人技术,机器学习
- **Abstract**: With the rapid development of embodied artificial intelligence, significant progress has been made in vision-language-action (VLA) models for general robot decision-making. However, the majority of existing VLAs fail to account for the inevitable external perturbations encountered during deployment. These perturbations introduce unforeseen state information to the VLA, resulting in inaccurate actions and consequently, a significant decline in generalization performance. The classic internal model control (IMC) principle demonstrates that a closed-loop system with an internal model that includes external input signals can accurately track the reference input and effectively offset the disturbance. We propose a novel closed-loop VLA method GEVRM that integrates the IMC principle to enhance the robustness of robot visual manipulation. The text-guided video generation model in GEVRM can generate highly expressive future visual planning goals. Simultaneously, we evaluate perturbations by simulating responses, which are called internal embeddings and optimized through prototype contrastive learning. This allows the model to implicitly infer and distinguish perturbations from the external environment. The proposed GEVRM achieves state-of-the-art performance on both standard and perturbed CALVIN benchmarks and shows significant improvements in realistic robot tasks.

### SkyRover: A Modular Simulator for Cross-Domain Pathfinding 
[[arxiv](https://arxiv.org/abs/2502.08969)] [[cool](https://papers.cool/arxiv/2502.08969)] [[pdf](https://arxiv.org/pdf/2502.08969)]
> **Authors**: Wenhui Ma,Wenhao Li,Bo Jin,Changhong Lu,Xiangfeng Wang
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 9 pages
- **标题**: None
- **领域**: 机器人技术,人工智能,机器学习,多代理系统
- **Abstract**: Unmanned Aerial Vehicles (UAVs) and Automated Guided Vehicles (AGVs) increasingly collaborate in logistics, surveillance, inspection tasks and etc. However, existing simulators often focus on a single domain, limiting cross-domain study. This paper presents the SkyRover, a modular simulator for UAV-AGV multi-agent pathfinding (MAPF). SkyRover supports realistic agent dynamics, configurable 3D environments, and convenient APIs for external solvers and learning methods. By unifying ground and aerial operations, it facilitates cross-domain algorithm design, testing, and benchmarking. Experiments highlight SkyRover's capacity for efficient pathfinding and high-fidelity simulations in UAV-AGV coordination. Project is available at https://sites.google.com/view/mapf3d/home.

## 软件工程(cs.SE:Software Engineering)

### TableTalk: Scaffolding Spreadsheet Development with a Language Agent 
[[arxiv](https://arxiv.org/abs/2502.09787)] [[cool](https://papers.cool/arxiv/2502.09787)] [[pdf](https://arxiv.org/pdf/2502.09787)]
> **Authors**: Jenny T. Liang,Aayush Kumar,Yasharth Bajpai,Sumit Gulwani,Vu Le,Chris Parnin,Arjun Radhakrishna,Ashish Tiwari,Emerson Murphy-Hill,Guastavo Soares
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,人工智能,人机交互
- **Abstract**: Despite its ubiquity in the workforce, spreadsheet programming remains challenging as programmers need both spreadsheet-specific knowledge (e.g., APIs to write formulas) and problem-solving skills to create complex spreadsheets. Large language models (LLMs) can help automate aspects of this process, and recent advances in planning and reasoning have enabled language agents, which dynamically plan, use tools, and take iterative actions to complete complex tasks. These agents observe, plan, and act, making them well-suited to scaffold spreadsheet programming by following expert processes. We present TableTalk, a language agent that helps programmers build spreadsheets conversationally. Its design reifies three design principles -- scaffolding, flexibility, and incrementality -- which we derived from two studies of seven programmers and 62 Excel templates. TableTalk structures spreadsheet development by generating step-by-step plans and suggesting three next steps users can choose from. It also integrates tools that enable incremental spreadsheet construction. A user study with 20 programmers shows that TableTalk produces spreadsheets 2.3 times more likely to be preferred over a baseline agent, while reducing cognitive load and time spent reasoning about spreadsheet actions by 12.6%. TableTalk's approach has implications for human-agent collaboration. This includes providing persistent direct manipulation interfaces for stopping or undoing agent actions, while ensuring that such interfaces for accepting actions can be deactivated.

## 普通经济学(econ.GN:General Economics)

### Cracking the Code: Enhancing Development finance understanding with artificial intelligence 
[[arxiv](https://arxiv.org/abs/2502.09495)] [[cool](https://papers.cool/arxiv/2502.09495)] [[pdf](https://arxiv.org/pdf/2502.09495)]
> **Authors**: Pierre Beaucoral
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 普通经济学,人工智能,机器学习
- **Abstract**: Analyzing development projects is crucial for understanding donors aid strategies, recipients priorities, and to assess development finance capacity to adress development issues by on-the-ground actions. In this area, the Organisation for Economic Co-operation and Developments (OECD) Creditor Reporting System (CRS) dataset is a reference data source. This dataset provides a vast collection of project narratives from various sectors (approximately 5 million projects). While the OECD CRS provides a rich source of information on development strategies, it falls short in informing project purposes due to its reporting process based on donors self-declared main objectives and pre-defined industrial sectors. This research employs a novel approach that combines Machine Learning (ML) techniques, specifically Natural Language Processing (NLP), an innovative Python topic modeling technique called BERTopic, to categorise (cluster) and label development projects based on their narrative descriptions. By revealing existing yet hidden topics of development finance, this application of artificial intelligence enables a better understanding of donor priorities and overall development funding and provides methods to analyse public and private projects narratives.

## 图像和视频处理(eess.IV:Image and Video Processing)

### Towards Patient-Specific Surgical Planning for Bicuspid Aortic Valve Repair: Fully Automated Segmentation of the Aortic Valve in 4D CT 
[[arxiv](https://arxiv.org/abs/2502.09805)] [[cool](https://papers.cool/arxiv/2502.09805)] [[pdf](https://arxiv.org/pdf/2502.09805)]
> **Authors**: Zaiyang Guo,Ningjun J Dong,Harold Litt,Natalie Yushkevich,Melanie Freas,Jessica Nunez,Victor Ferrari,Jilei Hao,Shir Goldfinger,Matthew A. Jolley,Joseph Bavaria,Nimesh Desai,Alison M. Pouch
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: The bicuspid aortic valve (BAV) is the most prevalent congenital heart defect and may require surgery for complications such as stenosis, regurgitation, and aortopathy. BAV repair surgery is effective but challenging due to the heterogeneity of BAV morphology. Multiple imaging modalities can be employed to assist the quantitative assessment of BAVs for surgical planning. Contrast-enhanced 4D computed tomography (CT) produces volumetric temporal sequences with excellent contrast and spatial resolution. Segmentation of the aortic cusps and root in these images is an essential step in creating patient specific models for visualization and quantification. While deep learning-based methods are capable of fully automated segmentation, no BAV-specific model exists. Among valve segmentation studies, there has been limited quantitative assessment of the clinical usability of the segmentation results. In this work, we developed a fully automated multi-label BAV segmentation pipeline based on nnU-Net. The predicted segmentations were used to carry out surgically relevant morphological measurements including geometric cusp height, commissural angle and annulus diameter, and the results were compared against manual segmentation. Automated segmentation achieved average Dice scores of over 0.7 and symmetric mean distance below 0.7 mm for all three aortic cusps and the root wall. Clinically relevant benchmarks showed good consistency between manual and predicted segmentations. Overall, fully automated BAV segmentation of 3D frames in 4D CT can produce clinically usable measurements for surgical risk stratification, but the temporal consistency of segmentations needs to be improved.

### Acute Lymphoblastic Leukemia Diagnosis Employing YOLOv11, YOLOv8, ResNet50, and Inception-ResNet-v2 Deep Learning Models 
[[arxiv](https://arxiv.org/abs/2502.09804)] [[cool](https://papers.cool/arxiv/2502.09804)] [[pdf](https://arxiv.org/pdf/2502.09804)]
> **Authors**: Alaa Awad,Salah A. Aly
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 12 pages, 28 figures, 5 tables
- **标题**: None
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别,机器学习
- **Abstract**: Thousands of individuals succumb annually to leukemia alone. As artificial intelligence-driven technologies continue to evolve and advance, the question of their applicability and reliability remains unresolved. This study aims to utilize image processing and deep learning methodologies to achieve state-of-the-art results for the detection of Acute Lymphoblastic Leukemia (ALL) using data that best represents real-world scenarios. ALL is one of several types of blood cancer, and it is an aggressive form of leukemia. In this investigation, we examine the most recent advancements in ALL detection, as well as the latest iteration of the YOLO series and its performance. We address the question of whether white blood cells are malignant or benign. Additionally, the proposed models can identify different ALL stages, including early stages. Furthermore, these models can detect hematogones despite their frequent misclassification as ALL. By utilizing advanced deep learning models, namely, YOLOv8, YOLOv11, ResNet50 and Inception-ResNet-v2, the study achieves accuracy rates as high as 99.7%, demonstrating the effectiveness of these algorithms across multiple datasets and various real-world situations.

### Lifespan tree of brain anatomy: diagnostic values for motor and cognitive neurodegenerative diseases 
[[arxiv](https://arxiv.org/abs/2502.09682)] [[cool](https://papers.cool/arxiv/2502.09682)] [[pdf](https://arxiv.org/pdf/2502.09682)]
> **Authors**: Pierrick Coupé,Boris Mansencal,José V. Manjón,Patrice Péran,Wassilios G. Meissner,Thomas Tourdias,Vincent Planche
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,机器学习
- **Abstract**: The differential diagnosis of neurodegenerative diseases, characterized by overlapping symptoms, may be challenging. Brain imaging coupled with artificial intelligence has been previously proposed for diagnostic support, but most of these methods have been trained to discriminate only isolated diseases from controls. Here, we develop a novel machine learning framework, named lifespan tree of brain anatomy, dedicated to the differential diagnosis between multiple diseases simultaneously. It integrates the modeling of volume changes for 124 brain structures during the lifespan with non-linear dimensionality reduction and synthetic sampling techniques to create easily interpretable representations of brain anatomy over the course of disease progression. As clinically relevant proof-of-concept applications, we constructed a cognitive lifespan tree of brain anatomy for the differential diagnosis of six causes of neurodegenerative dementia and a motor lifespan tree of brain anatomy for the differential diagnosis of four causes of parkinsonism using 37594 MRI as a training dataset. This original approach enhanced significantly the efficiency of differential diagnosis in the external validation cohort of 1754 cases, outperforming existing state-of-the art machine learning techniques. Lifespan tree holds promise as a valuable tool for differential diagnostic in relevant clinical conditions, especially for diseases still lacking effective biological markers.

## 信号处理(eess.SP:Signal Processing)

### On the Bias, Fairness, and Bias Mitigation for a Wearable-based Freezing of Gait Detection in Parkinson's Disease 
[[arxiv](https://arxiv.org/abs/2502.09626)] [[cool](https://papers.cool/arxiv/2502.09626)] [[pdf](https://arxiv.org/pdf/2502.09626)]
> **Authors**: Timothy Odonga,Christine D. Esper,Stewart A. Factor,J. Lucas McKay,Hyeokhyen Kwon
> **First submission**: 2025-01-29
> **First announcement**: 2025-02-14
> **comment**: Submitted to IMWUT 2025
- **标题**: None
- **领域**: 信号处理,机器学习
- **Abstract**: Freezing of gait (FOG) is a debilitating feature of Parkinson's disease (PD), which is a cause of injurious falls among PD patients. Recent advances in wearable-based human activity recognition (HAR) technology have enabled the detection of FOG subtypes across benchmark datasets. Since FOG manifestation is heterogeneous, developing models that quantify FOG consistently across patients with varying demographics, FOG types, and PD conditions is important. Bias and fairness in FOG models remain understudied in HAR, with research focused mainly on FOG detection using single benchmark datasets. We evaluated the bias and fairness of HAR models for wearable-based FOG detection across demographics and PD conditions using multiple datasets and the effectiveness of transfer learning as a potential bias mitigation approach. Our evaluation using demographic parity ratio (DPR) and equalized odds ratio (EOR) showed model bias (DPR & EOR < 0.8) for all stratified demographic variables, including age, sex, and disease duration. Our experiments demonstrated that transfer learning from multi-site datasets and generic human activity representations significantly improved fairness (average change in DPR +0.027, +0.039, respectively) and performance (average change in F1-score +0.026, +0.018, respectively) across attributes, supporting the hypothesis that generic human activity representations learn fairer representations applicable to health analytics.

### Joint Attention Mechanism Learning to Facilitate Opto-physiological Monitoring during Physical Activity 
[[arxiv](https://arxiv.org/abs/2502.09291)] [[cool](https://papers.cool/arxiv/2502.09291)] [[pdf](https://arxiv.org/pdf/2502.09291)]
> **Authors**: Xiaoyu Zheng,Sijung Hu,Vincent Dwyer,Mahsa Derakhshani,Laura Barrett
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 信号处理,机器学习
- **Abstract**: Opto-physiological monitoring is a non-contact technique for measuring cardiac signals, i.e., photoplethysmography (PPG). Quality PPG signals directly lead to reliable physiological readings. However, PPG signal acquisition procedures are often accompanied by spurious motion artefacts (MAs), especially during low-to-high-intensity physical activity. This study proposes a practical adversarial learning approach for opto-physiological monitoring by using a generative adversarial network with an attention mechanism (AM-GAN) to model motion noise and to allow MA removal. The AM-GAN learns an MA-resistant mapping from raw and noisy signals to clear PPG signals in an adversarial manner, guided by an attention mechanism to directly translate the motion reference of triaxial acceleration to the MAs appearing in the raw signal. The AM-GAN was experimented with three various protocols engaged with 39 subjects in various physical activities. The average absolute error for heart rate (HR) derived from the MA-free PPG signal via the AM-GAN, is 1.81 beats/min for the IEEE-SPC dataset and 3.86 beats/min for the PPGDalia dataset. The same procedure applied to an in-house LU dataset resulted in average absolute errors for HR and respiratory rate (RR) of less than 1.37 beats/min and 2.49 breaths/min, respectively. The study demonstrates the robustness and resilience of AM-GAN, particularly during low-to-high-intensity physical activities.

## 高能物理-现象学(hep-ph:High Energy Physics - Phenomenology)

### Communicating Likelihoods with Normalising Flows 
[[arxiv](https://arxiv.org/abs/2502.09494)] [[cool](https://papers.cool/arxiv/2502.09494)] [[pdf](https://arxiv.org/pdf/2502.09494)]
> **Authors**: Jack Y. Araz,Anja Beck,Méril Reboud,Michael Spannowsky,Danny van Dyk
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 4 pages + references, 1 figure
- **标题**: None
- **领域**: 高能物理-现象学,机器学习,高能物理-实验,数据分析、统计和概率
- **Abstract**: We present a machine-learning-based workflow to model an unbinned likelihood from its samples. A key advancement over existing approaches is the validation of the learned likelihood using rigorous statistical tests of the joint distribution, such as the Kolmogorov-Smirnov test of the joint distribution. Our method enables the reliable communication of experimental and phenomenological likelihoods for subsequent analyses. We demonstrate its effectiveness through three case studies in high-energy physics. To support broader adoption, we provide an open-source reference implementation, nabu.

## 经典分析和常微分方程(math.CA:Classical Analysis and ODEs)

### Reconstruction of frequency-localized functions from pointwise samples via least squares and deep learning 
[[arxiv](https://arxiv.org/abs/2502.09794)] [[cool](https://papers.cool/arxiv/2502.09794)] [[pdf](https://arxiv.org/pdf/2502.09794)]
> **Authors**: A. Martina Neuman,Andres Felipe Lerma Pineda,Jason J. Bramburger,Simone Brugiapaglia
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 经典分析和常微分方程,机器学习,数值分析
- **Abstract**: Recovering frequency-localized functions from pointwise data is a fundamental task in signal processing. We examine this problem from an approximation-theoretic perspective, focusing on least squares and deep learning-based methods. First, we establish a novel recovery theorem for least squares approximations using the Slepian basis from uniform random samples in low dimensions, explicitly tracking the dependence of the bandwidth on the sampling complexity. Building on these results, we then present a recovery guarantee for approximating bandlimited functions via deep learning from pointwise data. This result, framed as a practical existence theorem, provides conditions on the network architecture, training procedure, and data acquisition sufficient for accurate approximation. To complement our theoretical findings, we perform numerical comparisons between least squares and deep learning for approximating one- and two-dimensional functions. We conclude with a discussion of the theoretical limitations and the practical gaps between theory and implementation.

## 优化与控制(math.OC:Optimization and Control)

### Dynamic Rolling Horizon Optimization for Network-Constrained V2X Value Stacking of Electric Vehicles Under Uncertainties 
[[arxiv](https://arxiv.org/abs/2502.09290)] [[cool](https://papers.cool/arxiv/2502.09290)] [[pdf](https://arxiv.org/pdf/2502.09290)]
> **Authors**: Canchen Jiang,Ariel Liebman,Bo Jie,Hao Wang
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 20 pages, Renewable Energy
- **标题**: None
- **领域**: 优化与控制,机器学习,系统与控制
- **Abstract**: Electric vehicle (EV) coordination can provide significant benefits through vehicle-to-everything (V2X) by interacting with the grid, buildings, and other EVs. This work aims to develop a V2X value-stacking framework, including vehicle-to-building (V2B), vehicle-to-grid (V2G), and energy trading, to maximize economic benefits for residential communities while maintaining distribution voltage. This work also seeks to quantify the impact of prediction errors related to building load, renewable energy, and EV arrivals. A dynamic rolling-horizon optimization (RHO) method is employed to leverage multiple revenue streams and maximize the potential of EV coordination. To address energy uncertainties, including hourly local building load, local photovoltaic (PV) generation, and EV arrivals, this work develops a Transformer-based forecasting model named Gated Recurrent Units-Encoder-Temporal Fusion Decoder (GRU-EN-TFD). The simulation results, using real data from Australia's National Electricity Market, and the Independent System Operators in New England and New York in the US, reveal that V2X value stacking can significantly reduce energy costs. The proposed GRU-EN-TFD model outperforms the benchmark forecast model. Uncertainties in EV arrivals have a more substantial impact on value-stacking performance, highlighting the significance of its accurate forecast. This work provides new insights into the dynamic interactions among residential communities, unlocking the full potential of EV batteries.

## 医学物理(physics.med-ph:Medical Physics)

### Dynamic-Computed Tomography Angiography for Cerebral Vessel Templates and Segmentation 
[[arxiv](https://arxiv.org/abs/2502.09893)] [[cool](https://papers.cool/arxiv/2502.09893)] [[pdf](https://arxiv.org/pdf/2502.09893)]
> **Authors**: Shrikanth Yadav,Jisoo Kim,Geoffrey Young,Lei Qin
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 医学物理,计算机视觉和模式识别
- **Abstract**: Background: Computed Tomography Angiography (CTA) is crucial for cerebrovascular disease diagnosis. Dynamic CTA is a type of imaging that captures temporal information about the We aim to develop and evaluate two segmentation techniques to segment vessels directly on CTA images: (1) creating and registering population-averaged vessel atlases and (2) using deep learning (DL). Methods: We retrieved 4D-CT of the head from our institutional research database, with bone and soft tissue subtracted from post-contrast images. An Advanced Normalization Tools pipeline was used to create angiographic atlases from 25 patients. Then, atlas-driven ROIs were identified by a CT attenuation threshold to generate segmentation of the arteries and veins using non-linear registration. To create DL vessel segmentations, arterial and venous structures were segmented using the MRA vessel segmentation tool, iCafe, in 29 patients. These were then used to train a DL model, with bone-in CT images as input. Multiple phase images in the 4D-CT were used to increase the training and validation dataset. Both segmentation approaches were evaluated on a test 4D-CT dataset of 11 patients which were also processed by iCafe and validated by a neuroradiologist. Specifically, branch-wise segmentation accuracy was quantified with 20 labels for arteries and one for veins. DL outperformed the atlas-based segmentation models for arteries (average modified dice coefficient (amDC) 0.856 vs. 0.324) and veins (amDC 0.743 vs. 0.495) overall. For ICAs, vertebral and basilar arteries, DL and atlas -based segmentation had an amDC of 0.913 and 0.402, respectively. The amDC for MCA-M1, PCA-P1, and ACA-A1 segments were 0.932 and 0.474, respectively. Conclusion: Angiographic CT templates are developed for the first time in literature. Using 4D-CTA enables the use of tools like iCafe, lessening the burden of manual annotation.

## 物理与社会(physics.soc-ph:Physics and Society)

### Interpretable Early Warnings using Machine Learning in an Online Game-experiment 
[[arxiv](https://arxiv.org/abs/2502.09880)] [[cool](https://papers.cool/arxiv/2502.09880)] [[pdf](https://arxiv.org/pdf/2502.09880)]
> **Authors**: Guillaume Falmagne,Anna B. Stephenson,Simon A. Levin
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 物理与社会,机器学习,社交和信息网络,适应和自组织系统,机器学习
- **Abstract**: Stemming from physics and later applied to other fields such as ecology, the theory of critical transitions suggests that some regime shifts are preceded by statistical early warning signals. Reddit's r/place experiment, a large-scale social game, provides a unique opportunity to test these signals consistently across thousands of subsystems undergoing critical transitions. In r/place, millions of users collaboratively created compositions, or pixel-art drawings, in which transitions occur when one composition rapidly replaces another. We develop a machine-learning-based early warning system that combines the predictive power of multiple system-specific time series via gradient-boosted decision trees with memory-retaining features. Our method significantly outperforms standard early warning indicators. Trained on the 2022 r/place data, our algorithm detects half of the transitions occurring within 20 minutes at a false positive rate of just 3.7%. Its performance remains robust when tested on the 2023 r/place event, demonstrating generalizability across different contexts. Using SHapley Additive exPlanations (SHAP) for interpreting the predictions, we investigate the underlying drivers of warnings, which could be relevant to other complex systems, especially online social systems. We reveal an interplay of patterns preceding transitions, such as critical slowing down or speeding up, a lack of innovation or coordination, turbulent histories, and a lack of image complexity. These findings show the potential of machine learning indicators in socio-ecological systems for predicting regime shifts and understanding their dynamics.

## 生物分子(q-bio.BM:Biomolecules)

### Gradient GA: Gradient Genetic Algorithm for Drug Molecular Design 
[[arxiv](https://arxiv.org/abs/2502.09860)] [[cool](https://papers.cool/arxiv/2502.09860)] [[pdf](https://arxiv.org/pdf/2502.09860)]
> **Authors**: Chris Zhuang,Debadyuti Mukherjee,Yingzhou Lu,Tianfan Fu,Ruqi Zhang
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 生物分子,计算工程、金融和科学,机器学习,机器学习
- **Abstract**: Molecular discovery has brought great benefits to the chemical industry. Various molecule design techniques are developed to identify molecules with desirable properties. Traditional optimization methods, such as genetic algorithms, continue to achieve state-of-the-art results across multiple molecular design benchmarks. However, these techniques rely solely on random walk exploration, which hinders both the quality of the final solution and the convergence speed. To address this limitation, we propose a novel approach called Gradient Genetic Algorithm (Gradient GA), which incorporates gradient information from the objective function into genetic algorithms. Instead of random exploration, each proposed sample iteratively progresses toward an optimal solution by following the gradient direction. We achieve this by designing a differentiable objective function parameterized by a neural network and utilizing the Discrete Langevin Proposal to enable gradient guidance in discrete molecular spaces. Experimental results demonstrate that our method significantly improves both convergence speed and solution quality, outperforming cutting-edge techniques. For example, it achieves up to a 25% improvement in the top-10 score over the vanilla genetic algorithm. The code is publicly available at https://github.com/debadyuti23/GradientGA.

## 定量方法(q-bio.QM:Quantitative Methods)

### CellFlow: Simulating Cellular Morphology Changes via Flow Matching 
[[arxiv](https://arxiv.org/abs/2502.09775)] [[cool](https://papers.cool/arxiv/2502.09775)] [[pdf](https://arxiv.org/pdf/2502.09775)]
> **Authors**: Yuhui Zhang,Yuchang Su,Chenyu Wang,Tianhong Li,Zoe Wefers,Jeffrey Nirschl,James Burgess,Daisy Ding,Alejandro Lozano,Emma Lundberg,Serena Yeung-Levy
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 定量方法,计算机视觉和模式识别,机器学习,生物分子,细胞行为
- **Abstract**: Building a virtual cell capable of accurately simulating cellular behaviors in silico has long been a dream in computational biology. We introduce CellFlow, an image-generative model that simulates cellular morphology changes induced by chemical and genetic perturbations using flow matching. Unlike prior methods, CellFlow models distribution-wise transformations from unperturbed to perturbed cell states, effectively distinguishing actual perturbation effects from experimental artifacts such as batch effects -- a major challenge in biological data. Evaluated on chemical (BBBC021), genetic (RxRx1), and combined perturbation (JUMP) datasets, CellFlow generates biologically meaningful cell images that faithfully capture perturbation-specific morphological changes, achieving a 35% improvement in FID scores and a 12% increase in mode-of-action prediction accuracy over existing methods. Additionally, CellFlow enables continuous interpolation between cellular states, providing a potential tool for studying perturbation dynamics. These capabilities mark a significant step toward realizing virtual cell modeling for biomedical research.

### Generalizable Cervical Cancer Screening via Large-scale Pretraining and Test-Time Adaptation 
[[arxiv](https://arxiv.org/abs/2502.09662)] [[cool](https://papers.cool/arxiv/2502.09662)] [[pdf](https://arxiv.org/pdf/2502.09662)]
> **Authors**: Hao Jiang,Cheng Jin,Huangjing Lin,Yanning Zhou,Xi Wang,Jiabo Ma,Li Ding,Jun Hou,Runsheng Liu,Zhizhong Chai,Luyang Luo,Huijuan Shi,Yinling Qian,Qiong Wang,Changzhong Li,Anjia Han,Ronald Cheong Kin Chan,Hao Chen
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 定量方法,计算机视觉和模式识别,图像和视频处理
- **Abstract**: Cervical cancer is a leading malignancy in female reproductive system. While AI-assisted cytology offers a cost-effective and non-invasive screening solution, current systems struggle with generalizability in complex clinical scenarios. To address this issue, we introduced Smart-CCS, a generalizable Cervical Cancer Screening paradigm based on pretraining and adaptation to create robust and generalizable screening systems. To develop and validate Smart-CCS, we first curated a large-scale, multi-center dataset named CCS-127K, which comprises a total of 127,471 cervical cytology whole-slide images collected from 48 medical centers. By leveraging large-scale self-supervised pretraining, our CCS models are equipped with strong generalization capability, potentially generalizing across diverse scenarios. Then, we incorporated test-time adaptation to specifically optimize the trained CCS model for complex clinical settings, which adapts and refines predictions, improving real-world applicability. We conducted large-scale system evaluation among various cohorts. In retrospective cohorts, Smart-CCS achieved an overall area under the curve (AUC) value of 0.965 and sensitivity of 0.913 for cancer screening on 11 internal test datasets. In external testing, system performance maintained high at 0.950 AUC across 6 independent test datasets. In prospective cohorts, our Smart-CCS achieved AUCs of 0.947, 0.924, and 0.986 in three prospective centers, respectively. Moreover, the system demonstrated superior sensitivity in diagnosing cervical cancer, confirming the accuracy of our cancer screening results by using histology findings for validation. Interpretability analysis with cell and slide predictions further indicated that the system's decision-making aligns with clinical practice. Smart-CCS represents a significant advancement in cancer screening across diverse clinical contexts.

## 计算金融(q-fin.CP:Computational Finance)

### Transformer Based Time-Series Forecasting for Stock 
[[arxiv](https://arxiv.org/abs/2502.09625)] [[cool](https://papers.cool/arxiv/2502.09625)] [[pdf](https://arxiv.org/pdf/2502.09625)]
> **Authors**: Shuozhe Li,Zachery B Schulwol,Risto Miikkulainen
> **First submission**: 2025-01-28
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 计算金融,机器学习
- **Abstract**: To the naked eye, stock prices are considered chaotic, dynamic, and unpredictable. Indeed, it is one of the most difficult forecasting tasks that hundreds of millions of retail traders and professional traders around the world try to do every second even before the market opens. With recent advances in the development of machine learning and the amount of data the market generated over years, applying machine learning techniques such as deep learning neural networks is unavoidable. In this work, we modeled the task as a multivariate forecasting problem, instead of a naive autoregression problem. The multivariate analysis is done using the attention mechanism via applying a mutated version of the Transformer, "Stockformer", which we created.

## 一般财务(q-fin.GN:General Finance)

### Assessing Generative AI value in a public sector context: evidence from a field experiment 
[[arxiv](https://arxiv.org/abs/2502.09479)] [[cool](https://papers.cool/arxiv/2502.09479)] [[pdf](https://arxiv.org/pdf/2502.09479)]
> **Authors**: Trevor Fitzpatrick,Seamus Kelly,Patrick Carey,David Walsh,Ruairi Nugent
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 一般财务,机器学习,普通经济学
- **Abstract**: The emergence of Generative AI (Gen AI) has motivated an interest in understanding how it could be used to enhance productivity across various tasks. We add to research results for the performance impact of Gen AI on complex knowledge-based tasks in a public sector setting. In a pre-registered experiment, after establishing a baseline level of performance, we find mixed evidence for two types of composite tasks related to document understanding and data analysis. For the Documents task, the treatment group using Gen AI had a 17% improvement in answer quality scores (as judged by human evaluators) and a 34% improvement in task completion time compared to a control group. For the Data task, we find the Gen AI treatment group experienced a 12% reduction in quality scores and no significant difference in mean completion time compared to the control group. These results suggest that the benefits of Gen AI may be task and potentially respondent dependent. We also discuss field notes and lessons learned, as well as supplementary insights from a post-trial survey and feedback workshop with participants.

## 统计金融(q-fin.ST:Statistical Finance)

### Quantifying Cryptocurrency Unpredictability: A Comprehensive Study of Complexity and Forecasting 
[[arxiv](https://arxiv.org/abs/2502.09079)] [[cool](https://papers.cool/arxiv/2502.09079)] [[pdf](https://arxiv.org/pdf/2502.09079)]
> **Authors**: Francesco Puoti,Fabrizio Pittorino,Manuel Roveri
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: This is the author's accepted manuscript, modified per ACM self-archiving policy. The definitive Version of Record is available at https://doi.org/10.1145/3703412.3703420
- **标题**: None
- **领域**: 统计金融,机器学习,计算金融
- **Abstract**: This paper offers a thorough examination of the univariate predictability in cryptocurrency time-series. By exploiting a combination of complexity measure and model predictions we explore the cryptocurrencies time-series forecasting task focusing on the exchange rate in USD of Litecoin, Binance Coin, Bitcoin, Ethereum, and XRP. On one hand, to assess the complexity and the randomness of these time-series, a comparative analysis has been performed using Brownian and colored noises as a benchmark. The results obtained from the Complexity-Entropy causality plane and power density spectrum analysis reveal that cryptocurrency time-series exhibit characteristics closely resembling those of Brownian noise when analyzed in a univariate context. On the other hand, the application of a wide range of statistical, machine and deep learning models for time-series forecasting demonstrates the low predictability of cryptocurrencies. Notably, our analysis reveals that simpler models such as Naive models consistently outperform the more complex machine and deep learning ones in terms of forecasting accuracy across different forecast horizons and time windows. The combined study of complexity and forecasting accuracies highlights the difficulty of predicting the cryptocurrency market. These findings provide valuable insights into the inherent characteristics of the cryptocurrency data and highlight the need to reassess the challenges associated with predicting cryptocurrency's price movements.

## 量子物理学(quant-ph:Quantum Physics)

### Iterative quantum optimisation with a warm-started quantum state 
[[arxiv](https://arxiv.org/abs/2502.09704)] [[cool](https://papers.cool/arxiv/2502.09704)] [[pdf](https://arxiv.org/pdf/2502.09704)]
> **Authors**: Haomu Yuan,Songqinghao Yang,Crispin H. W. Barnes
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: feedback welcome, 13 pages, 12 figures
- **标题**: None
- **领域**: 量子物理学,无序系统和神经网络,机器学习,优化与控制,计算物理
- **Abstract**: We provide a method to prepare a warm-started quantum state from measurements with an iterative framework to enhance the quantum approximate optimisation algorithm (QAOA). The numerical simulations show the method can effectively address the "stuck issue" of the standard QAOA using a single-string warm-started initial state described in [Cain et al., 2023]. When applied to the $3$-regular MaxCut problem, our approach achieves an improved approximation ratio, with a lower bound that iteratively converges toward the best classical algorithms for $p=1$ standard QAOA. Additionally, in the context of the discrete global minimal variance portfolio (DGMVP) model, simulations reveal a more favourable scaling of identifying the global minimal compared to the QAOA standalone, the single-string warm-started QAOA and a classical constrained sampling approach.

## 机器学习(stat.ML:Machine Learning)

### Algorithmic contiguity from low-degree conjecture and applications in correlated random graphs 
[[arxiv](https://arxiv.org/abs/2502.09832)] [[cool](https://papers.cool/arxiv/2502.09832)] [[pdf](https://arxiv.org/pdf/2502.09832)]
> **Authors**: Zhangsong Li
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 40 pages. arXiv admin note: text overlap with arXiv:2311.00289 by other authors
- **标题**: None
- **领域**: 机器学习,数据结构和算法,机器学习,可能性,统计理论
- **Abstract**: In this paper, assuming a natural strengthening of the low-degree conjecture, we provide evidence of computational hardness for two problems: (1) the (partial) matching recovery problem in the sparse correlated Erdős-Rényi graphs $\mathcal G(n,q;ρ)$ when the edge-density $q=n^{-1+o(1)}$ and the correlation $ρ<\sqrtα$ lies below the Otter's threshold, solving a remaining problem in \cite{DDL23+}; (2) the detection problem between the correlated sparse stochastic block model $\mathcal S(n,\tfracλ{n};k,ε;s)$ and a pair of independent stochastic block models $\mathcal S(n,\tfrac{λs}{n};k,ε)$ when $ε^2 λs<1$ lies below the Kesten-Stigum (KS) threshold and $s<\sqrtα$ lies below the Otter's threshold, solving a remaining problem in \cite{CDGL24+}. One of the main ingredient in our proof is to derive certain forms of \emph{algorithmic contiguity} between two probability measures based on bounds on their low-degree advantage. To be more precise, consider the high-dimensional hypothesis testing problem between two probability measures $\mathbb{P}$ and $\mathbb{Q}$ based on the sample $\mathsf Y$. We show that if the low-degree advantage $\mathsf{Adv}_{\leq D} \big( \frac{\mathrm{d}\mathbb{P}}{\mathrm{d}\mathbb{Q}} \big)=O(1)$, then (assuming the low-degree conjecture) there is no efficient algorithm $\mathcal A$ such that $\mathbb{Q}(\mathcal A(\mathsf Y)=0)=1-o(1)$ and $\mathbb{P}(\mathcal A(\mathsf Y)=1)=Ω(1)$. This framework provides a useful tool for performing reductions between different inference tasks.

### A Differentiable Rank-Based Objective For Better Feature Learning 
[[arxiv](https://arxiv.org/abs/2502.09445)] [[cool](https://papers.cool/arxiv/2502.09445)] [[pdf](https://arxiv.org/pdf/2502.09445)]
> **Authors**: Krunoslav Lehman Pavasovic,David Lopez-Paz,Giulio Biroli,Levent Sagun
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: In this paper, we leverage existing statistical methods to better understand feature learning from data. We tackle this by modifying the model-free variable selection method, Feature Ordering by Conditional Independence (FOCI), which is introduced in \cite{azadkia2021simple}. While FOCI is based on a non-parametric coefficient of conditional dependence, we introduce its parametric, differentiable approximation. With this approximate coefficient of correlation, we present a new algorithm called difFOCI, which is applicable to a wider range of machine learning problems thanks to its differentiable nature and learnable parameters. We present difFOCI in three contexts: (1) as a variable selection method with baseline comparisons to FOCI, (2) as a trainable model parametrized with a neural network, and (3) as a generic, widely applicable neural network regularizer, one that improves feature learning with better management of spurious correlations. We evaluate difFOCI on increasingly complex problems ranging from basic variable selection in toy examples to saliency map comparisons in convolutional networks. We then show how difFOCI can be incorporated in the context of fairness to facilitate classifications without relying on sensitive data.

### Non-asymptotic Analysis of Diffusion Annealed Langevin Monte Carlo for Generative Modelling 
[[arxiv](https://arxiv.org/abs/2502.09306)] [[cool](https://papers.cool/arxiv/2502.09306)] [[pdf](https://arxiv.org/pdf/2502.09306)]
> **Authors**: Paula Cordero-Encinar,O. Deniz Akyildiz,Andrew B. Duncan
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习,可能性,计算
- **Abstract**: We investigate the theoretical properties of general diffusion (interpolation) paths and their Langevin Monte Carlo implementation, referred to as diffusion annealed Langevin Monte Carlo (DALMC), under weak conditions on the data distribution. Specifically, we analyse and provide non-asymptotic error bounds for the annealed Langevin dynamics where the path of distributions is defined as Gaussian convolutions of the data distribution as in diffusion models. We then extend our results to recently proposed heavy-tailed (Student's t) diffusion paths, demonstrating their theoretical properties for heavy-tailed data distributions for the first time. Our analysis provides theoretical guarantees for a class of score-based generative models that interpolate between a simple distribution (Gaussian or Student's t) and the data distribution in finite time. This approach offers a broader perspective compared to standard score-based diffusion approaches, which are typically based on a forward Ornstein-Uhlenbeck (OU) noising process.

### Optimal Algorithms in Linear Regression under Covariate Shift: On the Importance of Precondition 
[[arxiv](https://arxiv.org/abs/2502.09047)] [[cool](https://papers.cool/arxiv/2502.09047)] [[pdf](https://arxiv.org/pdf/2502.09047)]
> **Authors**: Yuanshi Liu,Haihan Zhang,Qian Chen,Cong Fang
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: A common pursuit in modern statistical learning is to attain satisfactory generalization out of the source data distribution (OOD). In theory, the challenge remains unsolved even under the canonical setting of covariate shift for the linear model. This paper studies the foundational (high-dimensional) linear regression where the ground truth variables are confined to an ellipse-shape constraint and addresses two fundamental questions in this regime: (i) given the target covariate matrix, what is the min-max \emph{optimal} algorithm under covariate shift? (ii) for what kinds of target classes, the commonly-used SGD-type algorithms achieve optimality? Our analysis starts with establishing a tight lower generalization bound via a Bayesian Cramer-Rao inequality. For (i), we prove that the optimal estimator can be simply a certain linear transformation of the best estimator for the source distribution. Given the source and target matrices, we show that the transformation can be efficiently computed via a convex program. The min-max optimal analysis for SGD leverages the idea that we recognize both the accumulated updates of the applied algorithms and the ideal transformation as preconditions on the learning variables. We provide sufficient conditions when SGD with its acceleration variants attain optimality.

### Off-Policy Evaluation for Recommendations with Missing-Not-At-Random Rewards 
[[arxiv](https://arxiv.org/abs/2502.08993)] [[cool](https://papers.cool/arxiv/2502.08993)] [[pdf](https://arxiv.org/pdf/2502.08993)]
> **Authors**: Tatsuki Takahashi,Chihiro Maru,Hiroko Shoji
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-14
> **comment**: 4pages
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Unbiased recommender learning (URL) and off-policy evaluation/learning (OPE/L) techniques are effective in addressing the data bias caused by display position and logging policies, thereby consistently improving the performance of recommendations. However, when both bias exits in the logged data, these estimators may suffer from significant bias. In this study, we first analyze the position bias of the OPE estimator when rewards are missing not at random. To mitigate both biases, we propose a novel estimator that leverages two probabilities of logging policies and reward observations as propensity scores. Our experiments demonstrate that the proposed estimator achieves superior performance compared to other estimators, even as the levels of bias in reward observations increases.

## 其他论文

- [A Deep Learning Approach to Interface Color Quality Assessment in HCI](https://arxiv.org/abs/2502.09914)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [ChatIoT: Large Language Model-based Security Assistant for Internet of Things with Retrieval-Augmented Generation](https://arxiv.org/abs/2502.09896)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [An Efficient Large Recommendation Model: Towards a Resource-Optimal Scaling Law](https://arxiv.org/abs/2502.09888)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [DesignWeaver: Dimensional Scaffolding for Text-to-Image Product Design](https://arxiv.org/abs/2502.09867)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [VIRGOS: Secure Graph Convolutional Network on Vertically Split Data from Sparse Matrix Decomposition](https://arxiv.org/abs/2502.09808)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [Unit Testing Past vs. Present: Examining LLMs' Impact on Defect Detection and Efficiency](https://arxiv.org/abs/2502.09801)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Accelerator-assisted Floating-point ASIP for Communication and Positioning in Massive MIMO Systems](https://arxiv.org/abs/2502.09785)
  - **标题**: None
  - **Filtered Reason**: none of cs.AR in whitelist
- [SLICES, a scientific instrument for the networking community](https://arxiv.org/abs/2502.09783)
  - **标题**: None
  - **Filtered Reason**: none of cs.NI in whitelist
- [Knowledge-Enhanced Program Repair for Data Science Code](https://arxiv.org/abs/2502.09771)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [LLM-Generated Microservice Implementations from RESTful API Definitions](https://arxiv.org/abs/2502.09766)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Genetic Data Governance in Crisis: Policy Recommendations for Safeguarding Privacy and Preventing Discrimination](https://arxiv.org/abs/2502.09716)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- ["Ronaldo's a poser!": How the Use of Generative AI Shapes Debates in Online Forums](https://arxiv.org/abs/2502.09693)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Pitfalls of Evidence-Based AI Policy](https://arxiv.org/abs/2502.09618)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [Polymind: Parallel Visual Diagramming with Large Language Models to Support Prewriting Through Microtasks](https://arxiv.org/abs/2502.09577)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Vortex: Overcoming Memory Capacity Limitations in GPU-Accelerated Large-Scale Data Analytics](https://arxiv.org/abs/2502.09541)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC,cs.DB in whitelist
- [Spiking Neural Networks for Temporal Processing: Status Quo and Future Prospects](https://arxiv.org/abs/2502.09449)
  - **标题**: None
  - **Filtered Reason**: none of cs.NE in whitelist
- [Generalizable Reinforcement Learning with Biologically Inspired Hyperdimensional Occupancy Grid Maps for Exploration and Goal-Directed Path Planning](https://arxiv.org/abs/2502.09393)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO,cs.NE in whitelist
- [APT-LLM: Embedding-Based Anomaly Detection of Cyber Advanced Persistent Threats Using Large Language Models](https://arxiv.org/abs/2502.09385)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [FARM: Frequency-Aware Model for Cross-Domain Live-Streaming Recommendation](https://arxiv.org/abs/2502.09375)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [Revisiting Topological Interference Management: A Learning-to-Code on Graphs Perspective](https://arxiv.org/abs/2502.09344)
  - **标题**: None
  - **Filtered Reason**: none of cs.IT in whitelist
- [ThunderServe: High-performance and Cost-efficient LLM Serving in Cloud Environments](https://arxiv.org/abs/2502.09334)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC in whitelist
- [Copilot Arena: A Platform for Code LLM Evaluation in the Wild](https://arxiv.org/abs/2502.09328)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [KET-RAG: A Cost-Efficient Multi-Granular Indexing Framework for Graph-RAG](https://arxiv.org/abs/2502.09304)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [AI Safety for Everyone](https://arxiv.org/abs/2502.09288)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [Oracle Separations for RPH](https://arxiv.org/abs/2502.09279)
  - **标题**: None
  - **Filtered Reason**: none of cs.CC in whitelist
- [Properties of Path-Independent Choice Correspondences and Their Applications to Efficient and Stable Matchings](https://arxiv.org/abs/2502.09265)
  - **标题**: None
  - **Filtered Reason**: none of cs.GT,econ.TH in whitelist
- [Recipe: Hardware-Accelerated Replication Protocols](https://arxiv.org/abs/2502.09251)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [Memristor-Based Meta-Learning for Fast mmWave Beam Prediction in Non-Stationary Environments](https://arxiv.org/abs/2502.09244)
  - **标题**: None
  - **Filtered Reason**: none of cs.IT in whitelist
- [OpenBench: A New Benchmark and Baseline for Semantic Navigation in Smart Logistics](https://arxiv.org/abs/2502.09238)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [Early Validation of High-level Requirements on Cyber-Physical Systems](https://arxiv.org/abs/2502.09236)
  - **标题**: None
  - **Filtered Reason**: none of cs.LO,cs.SE in whitelist
- [Bridging Logic Programming and Deep Learning for Explainability through ILASP](https://arxiv.org/abs/2502.09227)
  - **标题**: None
  - **Filtered Reason**: none of cs.LO in whitelist
- [A Prolog Program for Bottom-up Evaluation](https://arxiv.org/abs/2502.09223)
  - **标题**: None
  - **Filtered Reason**: none of cs.PL,cs.LO,cs.DB in whitelist
- [Modular Stochastic Rewritable Petri Nets](https://arxiv.org/abs/2502.09217)
  - **标题**: None
  - **Filtered Reason**: none of cs.SC,cs.PF in whitelist
- [Autonomous Task Completion Based on Goal-directed Answer Set Programming](https://arxiv.org/abs/2502.09208)
  - **标题**: None
  - **Filtered Reason**: none of cs.LO,cs.SE in whitelist
- [XAInomaly: Explainable and Interpretable Deep Contractive Autoencoder for O-RAN Traffic Anomaly Detection](https://arxiv.org/abs/2502.09194)
  - **标题**: None
  - **Filtered Reason**: none of cs.IT in whitelist
- [A Machine Learning Approach to Sensor Substitution for Non-Prehensile Manipulation](https://arxiv.org/abs/2502.09180)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [Modeling in Jjodel: Bridging Complexity and Usability in Model-Driven Engineering](https://arxiv.org/abs/2502.09146)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [LLM-Driven Augmented Reality Puppeteer: Controller-Free Voice-Commanded Robot Teleoperation](https://arxiv.org/abs/2502.09142)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO,cs.HC in whitelist
- [Bridging the Gap Between LLMs and Human Intentions: Progresses and Challenges in Instruction Understanding, Intention Reasoning, and Reliable Generation](https://arxiv.org/abs/2502.09101)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Semantic Ads Retrieval at Walmart eCommerce with Language Models Progressively Trained on Multiple Knowledge Domains](https://arxiv.org/abs/2502.09089)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [Lowering the Error Floor of Error Correction Code Transformer](https://arxiv.org/abs/2502.09065)
  - **标题**: None
  - **Filtered Reason**: none of cs.IT in whitelist
- [Anchor Sponsor Firms in Open Source Software Ecosystems](https://arxiv.org/abs/2502.09060)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY,cs.SE,cs.SI in whitelist
- [Unleashing the Power of Large Language Model for Denoising Recommendation](https://arxiv.org/abs/2502.09058)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [The Datafication of Care in Public Homelessness Services](https://arxiv.org/abs/2502.09043)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Implementation of a Fuzzy Relational Database. Case Study: Chilean Cardboard Industry in the Maule Region](https://arxiv.org/abs/2502.09035)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [MTDP: Modulated Transformer Diffusion Policy Model](https://arxiv.org/abs/2502.09029)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [From Occupations to Tasks: A New Perspective on Automatability Prediction Using BERT](https://arxiv.org/abs/2502.09021)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [Data-Driven Discovery of Population Balance Equations for the Particulate Sciences](https://arxiv.org/abs/2502.09010)
  - **标题**: None
  - **Filtered Reason**: none of cs.CE in whitelist
- [RED: Energy Optimization Framework for eDRAM-based PIM with Reconfigurable Voltage Swing and Retention-aware Scheduling](https://arxiv.org/abs/2502.09007)
  - **标题**: None
  - **Filtered Reason**: none of cs.AR in whitelist
- [Semantic Communication Meets Heterogeneous Network: Emerging Trends, Opportunities, and Challenges](https://arxiv.org/abs/2502.08999)
  - **标题**: None
  - **Filtered Reason**: none of cs.NI,eess.SP in whitelist
- [Quantum Approaches for Dysphonia Assessment in Small Speech Datasets](https://arxiv.org/abs/2502.08968)
  - **标题**: None
  - **Filtered Reason**: none of cs.SD,cs.ET in whitelist
