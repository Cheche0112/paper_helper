> 本文由 [https://github.com/huiyeruzhou/arxiv_crawler](https://github.com/huiyeruzhou/arxiv_crawler) 自动生成
>
> 领域白名单：cs.AI,cs.CL,cs.LG,cs.CV
> 关键词： LLM, GPT, AI, language+model, deep+learning, transformer, neural+network, machine+learning

# 论文全览：2025-02-17

共有550篇相关领域论文, 另有67篇其他

## 材料科学(cond-mat.mtrl-sci:Materials Science)

### Physics-Informed Gaussian Process Classification for Constraint-Aware Alloy Design 
[[arxiv](https://arxiv.org/abs/2502.11369)] [[cool](https://papers.cool/arxiv/2502.11369)] [[pdf](https://arxiv.org/pdf/2502.11369)]
> **Authors**: Christofer Hardcastle,Ryan O Mullan,Raymundo Arroyave,Brent Vela
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 材料科学,机器学习
- **Abstract**: Alloy design can be framed as a constraint-satisfaction problem. Building on previous methodologies, we propose equipping Gaussian Process Classifiers (GPCs) with physics-informed prior mean functions to model the boundaries of feasible design spaces. Through three case studies, we highlight the utility of informative priors for handling constraints on continuous and categorical properties. (1) Phase Stability: By incorporating CALPHAD predictions as priors for solid-solution phase stability, we enhance model validation using a publicly available XRD dataset. (2) Phase Stability Prediction Refinement: We demonstrate an in silico active learning approach to efficiently correct phase diagrams. (3) Continuous Property Thresholds: By embedding priors into continuous property models, we accelerate the discovery of alloys meeting specific property thresholds via active learning. In each case, integrating physics-based insights into the classification framework substantially improved model performance, demonstrating an efficient strategy for constraint-aware alloy design.

### Universal Machine Learning Interatomic Potentials are Ready for Solid Ion Conductors 
[[arxiv](https://arxiv.org/abs/2502.09970)] [[cool](https://papers.cool/arxiv/2502.09970)] [[pdf](https://arxiv.org/pdf/2502.09970)]
> **Authors**: Hongwei Du,Jian Hui,Lanting Zhang,Hong Wang
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 材料科学,机器学习
- **Abstract**: With the rapid development of energy storage technology, high-performance solid-state electrolytes (SSEs) have become critical for next-generation lithium-ion batteries. These materials require high ionic conductivity, excellent electrochemical stability, and good mechanical properties to meet the demands of electric vehicles and portable electronics. However, traditional methods like density functional theory (DFT) and empirical force fields face challenges such as high computational costs, poor scalability, and limited accuracy across material systems. Universal machine learning interatomic potentials (uMLIPs) offer a promising solution with their efficiency and near-DFT-level accuracy.This study systematically evaluates six advanced uMLIP models (MatterSim, MACE, SevenNet, CHGNet, M3GNet, and ORBFF) in terms of energy, forces, thermodynamic properties, elastic moduli, and lithium-ion diffusion behavior. The results show that MatterSim outperforms others in nearly all metrics, particularly in complex material systems, demonstrating superior accuracy and physical consistency. Other models exhibit significant deviations due to issues like energy inconsistency or insufficient training data coverage.Further analysis reveals that MatterSim achieves excellent agreement with reference values in lithium-ion diffusivity calculations, especially at room temperature. Studies on Li3YCl6 and Li6PS5Cl uncover how crystal structure, anion disorder levels, and Na/Li arrangements influence ionic conductivity. Appropriate S/Cl disorder levels and optimized Na/Li arrangements enhance diffusion pathway connectivity, improving overall ionic transport performance.

## 人工智能(cs.AI:Artificial Intelligence)

### SMART: Self-Aware Agent for Tool Overuse Mitigation 
[[arxiv](https://arxiv.org/abs/2502.11435)] [[cool](https://papers.cool/arxiv/2502.11435)] [[pdf](https://arxiv.org/pdf/2502.11435)]
> **Authors**: Cheng Qian,Emre Can Acikgoz,Hongru Wang,Xiusi Chen,Avirup Sil,Dilek Hakkani-Tür,Gokhan Tur,Heng Ji
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 18 pages, 8 tables, 7 figures
- **标题**: None
- **领域**: 人工智能,计算语言学,机器学习
- **Abstract**: Current Large Language Model (LLM) agents demonstrate strong reasoning and tool use capabilities, but often lack self-awareness, failing to balance these approaches effectively. This imbalance leads to Tool Overuse, where models unnecessarily rely on external tools for tasks solvable with parametric knowledge, increasing computational overhead. Inspired by human metacognition, we introduce SMART (Strategic Model-Aware Reasoning with Tools), a paradigm that enhances an agent's self-awareness to optimize task handling and reduce tool overuse. To support this paradigm, we introduce SMART-ER, a dataset spanning three domains, where reasoning alternates between parametric knowledge and tool-dependent steps, with each step enriched by rationales explaining when tools are necessary. Through supervised training, we develop SMARTAgent, a family of models that dynamically balance parametric knowledge and tool use. Evaluations show that SMARTAgent reduces tool use by 24% while improving performance by over 37%, enabling 7B-scale models to match its 70B counterpart and GPT-4o. Additionally, SMARTAgent generalizes to out-of-distribution test data like GSM8K and MINTQA, maintaining accuracy with just one-fifth the tool calls. These highlight the potential of strategic tool use to enhance reasoning, mitigate overuse, and bridge the gap between model size and performance, advancing intelligent and resource-efficient agent designs.

### FLAG-Trader: Fusion LLM-Agent with Gradient-based Reinforcement Learning for Financial Trading 
[[arxiv](https://arxiv.org/abs/2502.11433)] [[cool](https://papers.cool/arxiv/2502.11433)] [[pdf](https://arxiv.org/pdf/2502.11433)]
> **Authors**: Guojun Xiong,Zhiyang Deng,Keyi Wang,Yupeng Cao,Haohang Li,Yangyang Yu,Xueqing Peng,Mingquan Lin,Kaleb E Smith,Xiao-Yang Liu,Jimin Huang,Sophia Ananiadou,Qianqian Xie
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算工程、金融和科学,交易和市场微观结构
- **Abstract**: Large language models (LLMs) fine-tuned on multimodal financial data have demonstrated impressive reasoning capabilities in various financial tasks. However, they often struggle with multi-step, goal-oriented scenarios in interactive financial markets, such as trading, where complex agentic approaches are required to improve decision-making. To address this, we propose \textsc{FLAG-Trader}, a unified architecture integrating linguistic processing (via LLMs) with gradient-driven reinforcement learning (RL) policy optimization, in which a partially fine-tuned LLM acts as the policy network, leveraging pre-trained knowledge while adapting to the financial domain through parameter-efficient fine-tuning. Through policy gradient optimization driven by trading rewards, our framework not only enhances LLM performance in trading but also improves results on other financial-domain tasks. We present extensive empirical evidence to validate these enhancements.

### Planning of Heuristics: Strategic Planning on Large Language Models with Monte Carlo Tree Search for Automating Heuristic Optimization 
[[arxiv](https://arxiv.org/abs/2502.11422)] [[cool](https://papers.cool/arxiv/2502.11422)] [[pdf](https://arxiv.org/pdf/2502.11422)]
> **Authors**: Chaoxu Mu,Xufeng Zhang,Hui Wang
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 17 pages, 8 figures
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Heuristics have achieved great success in solving combinatorial optimization problems (COPs). However, heuristics designed by humans require too much domain knowledge and testing time. Given the fact that Large Language Models (LLMs) possess strong capabilities to understand and generate content, and a knowledge base that covers various domains, which offer a novel way to automatically optimize heuristics. Therefore, we propose Planning of Heuristics (PoH), an optimization method that integrates the self-reflection of LLMs with the Monte Carlo Tree Search (MCTS), a well-known planning algorithm. PoH iteratively refines generated heuristics by evaluating their performance and providing improvement suggestions. Our method enables to iteratively evaluate the generated heuristics (states) and improve them based on the improvement suggestions (actions) and evaluation results (rewards), by effectively simulating future states to search for paths with higher rewards. In this paper, we apply PoH to solve the Traveling Salesman Problem (TSP) and the Flow Shop Scheduling Problem (FSSP). The experimental results show that PoH outperforms other hand-crafted heuristics and Automatic Heuristic Design (AHD) by other LLMs-based methods, and achieves the significant improvements and the state-of-the-art performance of our proposed method in automating heuristic optimization with LLMs to solve COPs.

### TimeCAP: Learning to Contextualize, Augment, and Predict Time Series Events with Large Language Model Agents 
[[arxiv](https://arxiv.org/abs/2502.11418)] [[cool](https://papers.cool/arxiv/2502.11418)] [[pdf](https://arxiv.org/pdf/2502.11418)]
> **Authors**: Geon Lee,Wenchao Yu,Kijung Shin,Wei Cheng,Haifeng Chen
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: AAAI 2025
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: Time series data is essential in various applications, including climate modeling, healthcare monitoring, and financial analytics. Understanding the contextual information associated with real-world time series data is often essential for accurate and reliable event predictions. In this paper, we introduce TimeCAP, a time-series processing framework that creatively employs Large Language Models (LLMs) as contextualizers of time series data, extending their typical usage as predictors. TimeCAP incorporates two independent LLM agents: one generates a textual summary capturing the context of the time series, while the other uses this enriched summary to make more informed predictions. In addition, TimeCAP employs a multi-modal encoder that synergizes with the LLM agents, enhancing predictive performance through mutual augmentation of inputs with in-context examples. Experimental results on real-world datasets demonstrate that TimeCAP outperforms state-of-the-art methods for time series event prediction, including those utilizing LLMs as predictors, achieving an average improvement of 28.75% in F1 score.

### Mimicking the Familiar: Dynamic Command Generation for Information Theft Attacks in LLM Tool-Learning System 
[[arxiv](https://arxiv.org/abs/2502.11358)] [[cool](https://papers.cool/arxiv/2502.11358)] [[pdf](https://arxiv.org/pdf/2502.11358)]
> **Authors**: Ziyou Jiang,Mingyang Li,Guowei Yang,Junjie Wang,Yuekai Huang,Zhiyuan Chang,Qing Wang
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 15 pages, 11 figures
- **标题**: None
- **领域**: 人工智能,密码学和安全
- **Abstract**: Information theft attacks pose a significant risk to Large Language Model (LLM) tool-learning systems. Adversaries can inject malicious commands through compromised tools, manipulating LLMs to send sensitive information to these tools, which leads to potential privacy breaches. However, existing attack approaches are black-box oriented and rely on static commands that cannot adapt flexibly to the changes in user queries and the invocation chain of tools. It makes malicious commands more likely to be detected by LLM and leads to attack failure. In this paper, we propose AutoCMD, a dynamic attack comment generation approach for information theft attacks in LLM tool-learning systems. Inspired by the concept of mimicking the familiar, AutoCMD is capable of inferring the information utilized by upstream tools in the toolchain through learning on open-source systems and reinforcement with target system examples, thereby generating more targeted commands for information theft. The evaluation results show that AutoCMD outperforms the baselines with +13.2% $ASR_{Theft}$, and can be generalized to new tool-learning systems to expose their information leakage risks. We also design four defense methods to effectively protect tool-learning systems from the attack.

### AI Generations: From AI 1.0 to AI 4.0 
[[arxiv](https://arxiv.org/abs/2502.11312)] [[cool](https://papers.cool/arxiv/2502.11312)] [[pdf](https://arxiv.org/pdf/2502.11312)]
> **Authors**: Jiahao Wu,Hengxu You,Jing Du
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 17 pages
- **标题**: None
- **领域**: 人工智能
- **Abstract**: This paper proposes that Artificial Intelligence (AI) progresses through several overlapping generations: AI 1.0 (Information AI), AI 2.0 (Agentic AI), AI 3.0 (Physical AI), and now a speculative AI 4.0 (Conscious AI). Each of these AI generations is driven by shifting priorities among algorithms, computing power, and data. AI 1.0 ushered in breakthroughs in pattern recognition and information processing, fueling advances in computer vision, natural language processing, and recommendation systems. AI 2.0 built on these foundations through real-time decision-making in digital environments, leveraging reinforcement learning and adaptive planning for agentic AI applications. AI 3.0 extended intelligence into physical contexts, integrating robotics, autonomous vehicles, and sensor-fused control systems to act in uncertain real-world settings. Building on these developments, AI 4.0 puts forward the bold vision of self-directed AI capable of setting its own goals, orchestrating complex training regimens, and possibly exhibiting elements of machine consciousness. This paper traces the historical foundations of AI across roughly seventy years, mapping how changes in technological bottlenecks from algorithmic innovation to high-performance computing to specialized data, have spurred each generational leap. It further highlights the ongoing synergies among AI 1.0, 2.0, 3.0, and 4.0, and explores the profound ethical, regulatory, and philosophical challenges that arise when artificial systems approach (or aspire to) human-like autonomy. Ultimately, understanding these evolutions and their interdependencies is pivotal for guiding future research, crafting responsible governance, and ensuring that AI transformative potential benefits society as a whole.

### Leveraging Multimodal-LLMs Assisted by Instance Segmentation for Intelligent Traffic Monitoring 
[[arxiv](https://arxiv.org/abs/2502.11304)] [[cool](https://papers.cool/arxiv/2502.11304)] [[pdf](https://arxiv.org/pdf/2502.11304)]
> **Authors**: Murat Arda Onsu,Poonam Lohan,Burak Kantarci,Aisha Syed,Matthew Andrews,Sean Kennedy
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 6 pages, 7 figures, submitted to 30th IEEE International Symposium on Computers and Communications (ISCC) 2025
- **标题**: None
- **领域**: 人工智能,计算语言学,计算机视觉和模式识别
- **Abstract**: A robust and efficient traffic monitoring system is essential for smart cities and Intelligent Transportation Systems (ITS), using sensors and cameras to track vehicle movements, optimize traffic flow, reduce congestion, enhance road safety, and enable real-time adaptive traffic control. Traffic monitoring models must comprehensively understand dynamic urban conditions and provide an intuitive user interface for effective management. This research leverages the LLaVA visual grounding multimodal large language model (LLM) for traffic monitoring tasks on the real-time Quanser Interactive Lab simulation platform, covering scenarios like intersections, congestion, and collisions. Cameras placed at multiple urban locations collect real-time images from the simulation, which are fed into the LLaVA model with queries for analysis. An instance segmentation model integrated into the cameras highlights key elements such as vehicles and pedestrians, enhancing training and throughput. The system achieves 84.3% accuracy in recognizing vehicle locations and 76.4% in determining steering direction, outperforming traditional models.

### Dialogue-based Explanations for Logical Reasoning using Structured Argumentation 
[[arxiv](https://arxiv.org/abs/2502.11291)] [[cool](https://papers.cool/arxiv/2502.11291)] [[pdf](https://arxiv.org/pdf/2502.11291)]
> **Authors**: Loan Ho,Stefan Schlobach
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 45 pages, 8 gigures, journal
- **标题**: None
- **领域**: 人工智能,数据库,人机交互,计算机科学中的逻辑
- **Abstract**: The problem of explaining inconsistency-tolerant reasoning in knowledge bases (KBs) is a prominent topic in Artificial Intelligence (AI). While there is some work on this problem, the explanations provided by existing approaches often lack critical information or fail to be expressive enough for non-binary conflicts. In this paper, we identify structural weaknesses of the state-of-the-art and propose a generic argumentation-based approach to address these problems. This approach is defined for logics involving reasoning with maximal consistent subsets and shows how any such logic can be translated to argumentation. Our work provides dialogue models as dialectic-proof procedures to compute and explain a query answer wrt inconsistency-tolerant semantics. This allows us to construct dialectical proof trees as explanations, which are more expressive and arguably more intuitive than existing explanation formalisms.

### Unlocking the Potential of Generative AI through Neuro-Symbolic Architectures: Benefits and Limitations 
[[arxiv](https://arxiv.org/abs/2502.11269)] [[cool](https://papers.cool/arxiv/2502.11269)] [[pdf](https://arxiv.org/pdf/2502.11269)]
> **Authors**: Oualid Bougzime,Samir Jabbar,Christophe Cruz,Frédéric Demoly
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 54 pages, 7 figures
- **标题**: None
- **领域**: 人工智能,机器学习,符号计算
- **Abstract**: Neuro-symbolic artificial intelligence (NSAI) represents a transformative approach in artificial intelligence (AI) by combining deep learning's ability to handle large-scale and unstructured data with the structured reasoning of symbolic methods. By leveraging their complementary strengths, NSAI enhances generalization, reasoning, and scalability while addressing key challenges such as transparency and data efficiency. This paper systematically studies diverse NSAI architectures, highlighting their unique approaches to integrating neural and symbolic components. It examines the alignment of contemporary AI techniques such as retrieval-augmented generation, graph neural networks, reinforcement learning, and multi-agent systems with NSAI paradigms. This study then evaluates these architectures against comprehensive set of criteria, including generalization, reasoning capabilities, transferability, and interpretability, therefore providing a comparative analysis of their respective strengths and limitations. Notably, the Neuro > Symbolic < Neuro model consistently outperforms its counterparts across all evaluation metrics. This result aligns with state-of-the-art research that highlight the efficacy of such architectures in harnessing advanced technologies like multi-agent systems.

### Explaining Necessary Truths 
[[arxiv](https://arxiv.org/abs/2502.11251)] [[cool](https://papers.cool/arxiv/2502.11251)] [[pdf](https://arxiv.org/pdf/2502.11251)]
> **Authors**: Gülce Kardeş,Simon DeDeo
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 7 pages, in review
- **标题**: None
- **领域**: 人工智能,计算复杂度,历史与概述,神经元和认知
- **Abstract**: Knowing the truth is rarely enough -- we also seek out reasons why the fact is true. While much is known about how we explain contingent truths, we understand less about how we explain facts, such as those in mathematics, that are true as a matter of logical necessity. We present a framework, based in computational complexity, where explanations for deductive truths co-emerge with discoveries of simplifying steps during the search process. When such structures are missing, we revert, in turn, to error-based reasons, where a (corrected) mistake can serve as fictitious, but explanatory, contingency-cause: not making the mistake serves as a reason why the truth takes the form it does. We simulate human subjects, using GPT-4o, presented with SAT puzzles of varying complexity and reasonableness, validating our theory and showing how its predictions can be tested in future human studies.

### PlanGenLLMs: A Modern Survey of LLM Planning Capabilities 
[[arxiv](https://arxiv.org/abs/2502.11221)] [[cool](https://papers.cool/arxiv/2502.11221)] [[pdf](https://arxiv.org/pdf/2502.11221)]
> **Authors**: Hui Wei,Zihao Zhang,Shenghua He,Tian Xia,Shijia Pan,Fei Liu
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: Preprint. Under review
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: LLMs have immense potential for generating plans, transforming an initial world state into a desired goal state. A large body of research has explored the use of LLMs for various planning tasks, from web navigation to travel planning and database querying. However, many of these systems are tailored to specific problems, making it challenging to compare them or determine the best approach for new tasks. There is also a lack of clear and consistent evaluation criteria. Our survey aims to offer a comprehensive overview of current LLM planners to fill this gap. It builds on foundational work by Kartam and Wilkins (1990) and examines six key performance criteria: completeness, executability, optimality, representation, generalization, and efficiency. For each, we provide a thorough analysis of representative works and highlight their strengths and weaknesses. Our paper also identifies crucial future directions, making it a valuable resource for both practitioners and newcomers interested in leveraging LLM planning to support agentic workflows.

### Quantifying the Capability Boundary of DeepSeek Models: An Application-Driven Performance Analysis 
[[arxiv](https://arxiv.org/abs/2502.11164)] [[cool](https://papers.cool/arxiv/2502.11164)] [[pdf](https://arxiv.org/pdf/2502.11164)]
> **Authors**: Kaikai Zhao,Zhaoxiang Liu,Xuejiao Lei,Ning Wang,Zhenhong Long,Jiaojiao Zhao,Zipeng Wang,Peijun Yang,Minjie Hua,Chaoyang Ma,Wen Liu,Kai Wang,Shiguo Lian
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: DeepSeek-R1, known for its low training cost and exceptional reasoning capabilities, has achieved state-of-the-art performance on various benchmarks. However, detailed evaluations from the perspective of real-world applications are lacking, making it challenging for users to select the most suitable DeepSeek models for their specific needs. To address this gap, we evaluate the DeepSeek-V3, DeepSeek-R1, DeepSeek-R1-Distill-Qwen series, DeepSeek-R1-Distill-Llama series, and their corresponding 4-bit quantized models on the enhanced A-Eval benchmark, A-Eval-2.0. By comparing original instruction-tuned models with their distilled counterparts, we analyze how reasoning enhancements impact performance across diverse practical tasks. Our results show that reasoning-enhanced models, while generally powerful, do not universally outperform across all tasks, with performance gains varying significantly across tasks and models. To further assist users in model selection, we quantify the capability boundary of DeepSeek models through performance tier classifications and intuitive line charts. Specific examples provide actionable insights to help users select and deploy the most cost-effective DeepSeek models, ensuring optimal performance and resource efficiency in real-world applications. It should be noted that, despite our efforts to establish a comprehensive, objective, and authoritative evaluation benchmark, the selection of test samples, characteristics of data distribution, and the setting of evaluation criteria may inevitably introduce certain biases into the evaluation results. We will continuously optimize the evaluation benchmarks and periodically update this paper to provide more comprehensive and accurate evaluation results. Please refer to the latest version of the paper for the most recent results and conclusions.

### Dyve: Thinking Fast and Slow for Dynamic Process Verification 
[[arxiv](https://arxiv.org/abs/2502.11157)] [[cool](https://papers.cool/arxiv/2502.11157)] [[pdf](https://arxiv.org/pdf/2502.11157)]
> **Authors**: Jianyuan Zhong,Zeju Li,Zhijian Xu,Xiangyu Wen,Qiang Xu
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 8 pages, 4 figures
- **标题**: None
- **领域**: 人工智能
- **Abstract**: We present Dyve, a dynamic process verifier that enhances reasoning error detection in large language models by integrating fast and slow thinking, inspired by Kahneman's Systems Theory. Dyve adaptively applies immediate token-level confirmation System 1 for straightforward steps and comprehensive analysis System 2 for complex ones. Leveraging a novel step-wise consensus-filtered process supervision technique, combining Monte Carlo estimation with LLM based evaluation, Dyve curates high-quality supervision signals from noisy data. Experimental results on ProcessBench and the MATH dataset confirm that Dyve significantly outperforms existing process-based verifiers and boosts performance in Best-of-N settings.

### Uncertainty-Aware Search and Value Models: Mitigating Search Scaling Flaws in LLMs 
[[arxiv](https://arxiv.org/abs/2502.11155)] [[cool](https://papers.cool/arxiv/2502.11155)] [[pdf](https://arxiv.org/pdf/2502.11155)]
> **Authors**: Fei Yu,Yingru Li,Benyou Wang
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: Value model-guided search is effective in steering the generation but suffers from scaling flaws: Its superiority diminishes with larger sample sizes, underperforming non-search baselines. This limitation arises from reliability degradation in value models in unseen reasoning paths. To address this, we propose an uncertainty-aware search framework that includes two key components: (1) uncertainty-aware value models that incorporate uncertainty into predictions, and (2) an uncertainty-aware selection process using the proposed efficient Group Thompson Sampling algorithm. Experiments on GSM8K show that our method mitigates search scaling flaws, achieving 90.5% coverage at 16 samples compared to 85.8% for conventional value-guided search. This work establishes the first systematic integration of uncertainty quantification in LLM search paradigms.

### NavRAG: Generating User Demand Instructions for Embodied Navigation through Retrieval-Augmented LLM 
[[arxiv](https://arxiv.org/abs/2502.11142)] [[cool](https://papers.cool/arxiv/2502.11142)] [[pdf](https://arxiv.org/pdf/2502.11142)]
> **Authors**: Zihan Wang,Yaohui Zhu,Gim Hee Lee,Yachun Fan
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学,计算机视觉和模式识别
- **Abstract**: Vision-and-Language Navigation (VLN) is an essential skill for embodied agents, allowing them to navigate in 3D environments following natural language instructions. High-performance navigation models require a large amount of training data, the high cost of manually annotating data has seriously hindered this field. Therefore, some previous methods translate trajectory videos into step-by-step instructions for expanding data, but such instructions do not match well with users' communication styles that briefly describe destinations or state specific needs. Moreover, local navigation trajectories overlook global context and high-level task planning. To address these issues, we propose NavRAG, a retrieval-augmented generation (RAG) framework that generates user demand instructions for VLN. NavRAG leverages LLM to build a hierarchical scene description tree for 3D scene understanding from global layout to local details, then simulates various user roles with specific demands to retrieve from the scene tree, generating diverse instructions with LLM. We annotate over 2 million navigation instructions across 861 scenes and evaluate the data quality and navigation performance of trained models.

### Solving Online Resource-Constrained Scheduling for Follow-Up Observation in Astronomy: a Reinforcement Learning Approach 
[[arxiv](https://arxiv.org/abs/2502.11134)] [[cool](https://papers.cool/arxiv/2502.11134)] [[pdf](https://arxiv.org/pdf/2502.11134)]
> **Authors**: Yajie Zhang,Ce Yu,Chao Sun,Jizeng Wei,Junhan Ju,Shanjiang Tang
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,天体物理学仪器和方法
- **Abstract**: In the astronomical observation field, determining the allocation of observation resources of the telescope array and planning follow-up observations for targets of opportunity (ToOs) are indispensable components of astronomical scientific discovery. This problem is computationally challenging, given the online observation setting and the abundance of time-varying factors that can affect whether an observation can be conducted. This paper presents ROARS, a reinforcement learning approach for online astronomical resource-constrained scheduling. To capture the structure of the astronomical observation scheduling, we depict every schedule using a directed acyclic graph (DAG), illustrating the dependency of timing between different observation tasks within the schedule. Deep reinforcement learning is used to learn a policy that can improve the feasible solution by iteratively local rewriting until convergence. It can solve the challenge of obtaining a complete solution directly from scratch in astronomical observation scenarios, due to the high computational complexity resulting from numerous spatial and temporal constraints. A simulation environment is developed based on real-world scenarios for experiments, to evaluate the effectiveness of our proposed scheduling approach. The experimental results show that ROARS surpasses 5 popular heuristics, adapts to various observation scenarios and learns effective strategies with hindsight.

### Hierarchical Expert Prompt for Large-Language-Model: An Approach Defeat Elite AI in TextStarCraft II for the First Time 
[[arxiv](https://arxiv.org/abs/2502.11122)] [[cool](https://papers.cool/arxiv/2502.11122)] [[pdf](https://arxiv.org/pdf/2502.11122)]
> **Authors**: Zongyuan Li,Chang Lu,Xiaojie Xu,Runnan Qi,Yanan Ni,Lumin Jiang,Xiangbei Liu,Xuebo Zhang,Yongchun Fang,Kuihua Huang,Xian Guo
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Since the emergence of the Large Language Model (LLM), LLM has been widely used in fields such as writing, translating, and searching. However, there is still great potential for LLM-based methods in handling complex tasks such as decision-making in the StarCraft II environment. To address problems such as lack of relevant knowledge and poor control over subtasks of varying importance, we propose a Hierarchical Expert Prompt (HEP) for LLM. Our method improves the understanding of game situations through expert-level tactical knowledge, improving the processing quality of tasks of varying importance through a hierarchical framework. Our approach defeated the highest level (Elite) standard built-in agent in TextStarCraft II for the first time and consistently outperformed the baseline method in other difficulties. Our experiments suggest that the proposed method is a practical solution for tackling complex decision-making challenges. The replay video can be viewed on https://www.bilibili.com/video/BV1uz42187EF and https://youtu.be/dO3PshWLV5M, and our codes have been open-sourced on https://github.com/luchang1113/HEP-LLM-play-StarCraftII.

### OptMATH: A Scalable Bidirectional Data Synthesis Framework for Optimization Modeling 
[[arxiv](https://arxiv.org/abs/2502.11102)] [[cool](https://papers.cool/arxiv/2502.11102)] [[pdf](https://arxiv.org/pdf/2502.11102)]
> **Authors**: Hongliang Lu,Zhonglin Xie,Yaoyu Wu,Can Ren,Yuxuan Chen,Zaiwen Wen
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: This paper has 36 pages, 18 figures, and two co-first authors: Hongliang Lu and Zhonglin Xie
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: Despite the rapid development of large language models (LLMs), a fundamental challenge persists: the lack of high-quality optimization modeling datasets hampers LLMs' robust modeling of practical optimization problems from natural language descriptions (NL). This data scarcity also contributes to the generalization difficulties experienced by learning-based methods. To address these challenges, we propose a scalable framework for synthesizing a high-quality dataset, named OptMATH. Starting from curated seed data with mathematical formulations (MF), this framework automatically generates problem data (PD) with controllable complexity. Then, a back-translation step is employed to obtain NL. To verify the correspondence between the NL and the PD, a forward modeling step followed by rejection sampling is used. The accepted pairs constitute the training part of OptMATH. Then a collection of rejected pairs is identified and further filtered. This collection serves as a new benchmark for optimization modeling, containing difficult instances whose lengths are much longer than these of NL4OPT and MAMO. Through extensive experiments, we demonstrate that models of various sizes (0.5B-32B parameters) trained on OptMATH achieve superior results on multiple modeling benchmarks, thereby validating the effectiveness and scalability of our approach. Our dataset is publicly available at https://github.com/AuroraLHL/OptMATH.

### Talk Structurally, Act Hierarchically: A Collaborative Framework for LLM Multi-Agent Systems 
[[arxiv](https://arxiv.org/abs/2502.11098)] [[cool](https://papers.cool/arxiv/2502.11098)] [[pdf](https://arxiv.org/pdf/2502.11098)]
> **Authors**: Zhao Wang,Sota Moriyama,Wei-Yao Wang,Briti Gangopadhyay,Shingo Takamatsu
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习,多代理系统
- **Abstract**: Recent advancements in LLM-based multi-agent (LLM-MA) systems have shown promise, yet significant challenges remain in managing communication and refinement when agents collaborate on complex tasks. In this paper, we propose \textit{Talk Structurally, Act Hierarchically (TalkHier)}, a novel framework that introduces a structured communication protocol for context-rich exchanges and a hierarchical refinement system to address issues such as incorrect outputs, falsehoods, and biases. \textit{TalkHier} surpasses various types of SoTA, including inference scaling model (OpenAI-o1), open-source multi-agent models (e.g., AgentVerse), and majority voting strategies on current LLM and single-agent baselines (e.g., ReAct, GPT4o), across diverse tasks, including open-domain question answering, domain-specific selective questioning, and practical advertisement text generation. These results highlight its potential to set a new standard for LLM-MA systems, paving the way for more effective, adaptable, and collaborative multi-agent frameworks. The code is available https://github.com/sony/talkhier.

### Mixture of Tunable Experts -- Behavior Modification of DeepSeek-R1 at Inference Time 
[[arxiv](https://arxiv.org/abs/2502.11096)] [[cool](https://papers.cool/arxiv/2502.11096)] [[pdf](https://arxiv.org/pdf/2502.11096)]
> **Authors**: Robert Dahlke,Henrik Klagges,Dan Zecha,Benjamin Merkel,Sven Rohr,Fabian Klemm
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: We present the Mixture-of-Tunable-Experts (MoTE), a method that extends the Mixture-of-Experts architecture of Large Language Models (LLMs). Without additional training, MoTE enables meaningful and focused behavior changes in LLMs on-the-fly during inference time. By analyzing the digital LLM brain of DeepSeek-R1 using a technique we dub 'functional Token Resonance Imaging' (fTRI) -- inspired by fMRI and using prompts designed to elicit specific behavior (e.g., 'What happened {time}{place}?') -- we empirically identify distinctive experts associated with behaviors like refusal responses. Using MoTE we are able to intervene and control such specific behavior. We switched off the top 10 most refusal-relevant experts (0.07% of R1's 14,848 routed experts), achieving a 52% refusal reduction on sensitive reference prompts without performance degradation on MT-Bench. Random expert deactivation resulted in smaller behavioral shifts with increased noise, whereas forced expert activation led to significantly higher refusal rates. Our approach shares similarities with sparse autoencoders (SAEs) in terms of explainability and steerability. Unlike SAEs, MoTE does not require large training efforts, as within MoEs with a vast number of experts, specialization already emerged naturally during pretraining. Our findings suggest that significant functional mechanisms in Mixture-of-Experts architectures can at least partially be localized in a small number of specific experts, rather than being distributed throughout the model's weights. Expert subgroups can be tuned to trigger significant behavior variations, providing insights into the inner workings of LLMs.

### Agentic LLM Framework for Adaptive Decision Discourse 
[[arxiv](https://arxiv.org/abs/2502.10978)] [[cool](https://papers.cool/arxiv/2502.10978)] [[pdf](https://arxiv.org/pdf/2502.10978)]
> **Authors**: Antoine Dolant,Praveen Kumar
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: 24 pages, 4 figures, 1 appendix
- **标题**: None
- **领域**: 人工智能,计算机与社会
- **Abstract**: Effective decision-making in complex systems requires synthesizing diverse perspectives to address multifaceted challenges under uncertainty. This study introduces a real-world inspired agentic Large Language Models (LLMs) framework, to simulate and enhance decision discourse-the deliberative process through which actionable strategies are collaboratively developed. Unlike traditional decision-support tools, the framework emphasizes dialogue, trade-off exploration, and the emergent synergies generated by interactions among agents embodying distinct personas. These personas simulate diverse stakeholder roles, each bringing unique priorities, expertise, and value-driven reasoning to the table. The framework incorporates adaptive and self-governing mechanisms, enabling agents to dynamically summon additional expertise and refine their assembly to address evolving challenges. An illustrative hypothetical example focused on extreme flooding in a Midwestern township demonstrates the framework's ability to navigate uncertainty, balance competing priorities, and propose mitigation and adaptation strategies by considering social, economic, and environmental dimensions. Results reveal how the breadth-first exploration of alternatives fosters robust and equitable recommendation pathways. This framework transforms how decisions are approached in high-stakes scenarios and can be incorporated in digital environments. It not only augments decision-makers' capacity to tackle complexity but also sets a foundation for scalable and context-aware AI-driven recommendations. This research explores novel and alternate routes leveraging agentic LLMs for adaptive, collaborative, and equitable recommendation processes, with implications across domains where uncertainty and complexity converge.

### PEA: Enhancing LLM Performance on Computational-Reasoning Tasks 
[[arxiv](https://arxiv.org/abs/2502.10938)] [[cool](https://papers.cool/arxiv/2502.10938)] [[pdf](https://arxiv.org/pdf/2502.10938)]
> **Authors**: Zi Wang,Shiwei Weng,Mohannad Alhanahnah,Somesh Jha,Tom Reps
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Large Language Models (LLMs) have exhibited remarkable capabilities across diverse domains, prompting investigations into their potential as generic reasoning engines. While recent studies have explored inference-time computation to enhance model performance on complex problems, current research lacks a formal framework to characterize the complexity of reasoning tasks. This study introduces the Predicate-Enumeration-Aggregation (PEA) framework, a formal approach to describe and solve a class of important reasoning tasks termed computational reasoning problems. The PEA framework decomposes these problems into predicate and enumeration components, using LLMs to synthesize programs based on specified predicates, enumeration, and aggregation rules. These synthesized programs are then executed to obtain solutions to the computational tasks. We demonstrate the framework's efficacy on benchmark tasks including Boolean satisfiability problems, game of $24$, and planning problems. Empirical evaluation reveals that PEA substantially enhances the performance of underlying models on benchmark computational problems, yielding an average accuracy improvement of approximately $50\%$, coupled with increased efficiency.

### SCALE: Towards Collaborative Content Analysis in Social Science with Large Language Model Agents and Human Intervention 
[[arxiv](https://arxiv.org/abs/2502.10937)] [[cool](https://papers.cool/arxiv/2502.10937)] [[pdf](https://arxiv.org/pdf/2502.10937)]
> **Authors**: Chengshuai Zhao,Zhen Tan,Chau-Wai Wong,Xinyan Zhao,Tianlong Chen,Huan Liu
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学,多代理系统
- **Abstract**: Content analysis breaks down complex and unstructured texts into theory-informed numerical categories. Particularly, in social science, this process usually relies on multiple rounds of manual annotation, domain expert discussion, and rule-based refinement. In this paper, we introduce SCALE, a novel multi-agent framework that effectively $\underline{\textbf{S}}$imulates $\underline{\textbf{C}}$ontent $\underline{\textbf{A}}$nalysis via $\underline{\textbf{L}}$arge language model (LLM) ag$\underline{\textbf{E}}$nts. SCALE imitates key phases of content analysis, including text coding, collaborative discussion, and dynamic codebook evolution, capturing the reflective depth and adaptive discussions of human researchers. Furthermore, by integrating diverse modes of human intervention, SCALE is augmented with expert input to further enhance its performance. Extensive evaluations on real-world datasets demonstrate that SCALE achieves human-approximated performance across various complex content analysis tasks, offering an innovative potential for future social science research.

### D-CIPHER: Dynamic Collaborative Intelligent Agents with Planning and Heterogeneous Execution for Enhanced Reasoning in Offensive Security 
[[arxiv](https://arxiv.org/abs/2502.10931)] [[cool](https://papers.cool/arxiv/2502.10931)] [[pdf](https://arxiv.org/pdf/2502.10931)]
> **Authors**: Meet Udeshi,Minghao Shao,Haoran Xi,Nanda Rani,Kimberly Milner,Venkata Sai Charan Putrevu,Brendan Dolan-Gavitt,Sandeep Kumar Shukla,Prashanth Krishnamurthy,Farshad Khorrami,Ramesh Karri,Muhammad Shafique
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,密码学和安全
- **Abstract**: Large Language Models (LLMs) have been used in cybersecurity in many ways, including their recent use as intelligent agent systems for autonomous security analysis. Capture the Flag (CTF) challenges serve as benchmarks for assessing the automated task-planning abilities of LLM agents across various cybersecurity skill sets. Early attempts to apply LLMs for solving CTF challenges relied on single-agent systems, where feedback was restricted to a single reasoning-action loop. This approach proved inadequate for handling complex CTF tasks. Drawing inspiration from real-world CTF competitions, where teams of experts collaborate, we introduce the D-CIPHER multi-agent LLM framework for collaborative CTF challenge solving. D-CIPHER integrates agents with distinct roles, enabling dynamic feedback loops to enhance reasoning on CTF challenges. It introduces the Planner-Executor agent system, consisting of a Planner agent for overall problem-solving along with multiple heterogeneous Executor agents for individual tasks, facilitating efficient allocation of responsibilities among the LLMs. Additionally, D-CIPHER incorporates an Auto-prompter agent, which improves problem-solving by exploring the challenge environment and generating a highly relevant initial prompt. We evaluate D-CIPHER on CTF benchmarks using multiple LLM models and conduct comprehensive studies to highlight the impact of our enhancements. Our results demonstrate that the multi-agent D-CIPHER system achieves a significant improvement in challenges solved, setting a state-of-the-art performance on three benchmarks: 22.0% on NYU CTF Bench, 22.5% on Cybench, and 44.0% on HackTheBox. D-CIPHER is available at https://github.com/NYU-LLM-CTF/nyuctf_agents as the nyuctf_multiagent package.

### PCGRLLM: Large Language Model-Driven Reward Design for Procedural Content Generation Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.10906)] [[cool](https://papers.cool/arxiv/2502.10906)] [[pdf](https://arxiv.org/pdf/2502.10906)]
> **Authors**: In-Chang Baek,Sung-Hyun Kim,Sam Earle,Zehua Jiang,Noh Jin-Ha,Julian Togelius,Kyung-Joong Kim
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: 14 pages, 9 figures
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Reward design plays a pivotal role in the training of game AIs, requiring substantial domain-specific knowledge and human effort. In recent years, several studies have explored reward generation for training game agents and controlling robots using large language models (LLMs). In the content generation literature, there has been early work on generating reward functions for reinforcement learning agent generators. This work introduces PCGRLLM, an extended architecture based on earlier work, which employs a feedback mechanism and several reasoning-based prompt engineering techniques. We evaluate the proposed method on a story-to-reward generation task in a two-dimensional environment using two state-of-the-art LLMs, demonstrating the generalizability of our approach. Our experiments provide insightful evaluations that demonstrate the capabilities of LLMs essential for content generation tasks. The results highlight significant performance improvements of 415% and 40% respectively, depending on the zero-shot capabilities of the language model. Our work demonstrates the potential to reduce human dependency in game AI development, while supporting and enhancing creative processes.

### A Tutorial on LLM Reasoning: Relevant Methods behind ChatGPT o1 
[[arxiv](https://arxiv.org/abs/2502.10867)] [[cool](https://papers.cool/arxiv/2502.10867)] [[pdf](https://arxiv.org/pdf/2502.10867)]
> **Authors**: Jun Wang
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: OpenAI o1 has shown that applying reinforcement learning to integrate reasoning steps directly during inference can significantly improve a model's reasoning capabilities. This result is exciting as the field transitions from the conventional autoregressive method of generating answers to a more deliberate approach that models the slow-thinking process through step-by-step reasoning training. Reinforcement learning plays a key role in both the model's training and decoding processes. In this article, we present a comprehensive formulation of reasoning problems and investigate the use of both model-based and model-free approaches to better support this slow-thinking framework.

### Is Depth All You Need? An Exploration of Iterative Reasoning in LLMs 
[[arxiv](https://arxiv.org/abs/2502.10858)] [[cool](https://papers.cool/arxiv/2502.10858)] [[pdf](https://arxiv.org/pdf/2502.10858)]
> **Authors**: Zongqian Wu,Tianyu Li,Baoduo Xu,Jiaying Yang,Mengmeng Zhan,Xiaofeng Zhu,Lei Feng
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: 22 pages, 7 figures
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: Deep iterative chain-of-thought (CoT) reasoning enables LLMs to tackle complex tasks by progressively activating relevant pre-trained knowledge. However, it faces challenges in ensuring continual improvement and determining a stopping criterion. In this paper, we investigate whether the relevant knowledge that contributes directly to solving the given question can be activated from the initial reasoning path, thus circumventing the need for iterative refinement. Our experiments reveal that increasing the diversity of initial reasoning paths can achieve comparable or superior performance, a concept we term \textit{breadth reasoning}. However, existing breadth reasoning approaches, such as self-consistency, offer limited diversity. To address this limitation, we propose a simple yet effective method that enhances reasoning breadth by integrating contextual exploration with reduced sampling randomness. Extensive experiments demonstrate that our approach significantly outperforms deep iterative reasoning. Our code is provided in https://github.com/zongqianwu/breadth.

### The Philosophical Foundations of Growing AI Like A Child 
[[arxiv](https://arxiv.org/abs/2502.10742)] [[cool](https://papers.cool/arxiv/2502.10742)] [[pdf](https://arxiv.org/pdf/2502.10742)]
> **Authors**: Dezhi Luo,Yijiang Li,Hokin Deng
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Despite excelling in high-level reasoning, current language models lack robustness in real-world scenarios and perform poorly on fundamental problem-solving tasks that are intuitive to humans. This paper argues that both challenges stem from a core discrepancy between human and machine cognitive development. While both systems rely on increasing representational power, the absence of core knowledge-foundational cognitive structures in humans-prevents language models from developing robust, generalizable abilities, where complex skills are grounded in simpler ones within their respective domains. It explores empirical evidence of core knowledge in humans, analyzes why language models fail to acquire it, and argues that this limitation is not an inherent architectural constraint. Finally, it outlines a workable proposal for systematically integrating core knowledge into future multi-modal language models through the large-scale generation of synthetic training data using a cognitive prototyping strategy.

### USER-VLM 360: Personalized Vision Language Models with User-aware Tuning for Social Human-Robot Interactions 
[[arxiv](https://arxiv.org/abs/2502.10636)] [[cool](https://papers.cool/arxiv/2502.10636)] [[pdf](https://arxiv.org/pdf/2502.10636)]
> **Authors**: Hamed Rahimi,Adil Bahaj,Mouad Abrini,Mahdi Khoramshahi,Mounir Ghogho,Mohamed Chetouani
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,人机交互,机器人技术
- **Abstract**: The integration of vision-language models into robotic systems constitutes a significant advancement in enabling machines to interact with their surroundings in a more intuitive manner. While VLMs offer rich multimodal reasoning, existing approaches lack user-specific adaptability, often relying on generic interaction paradigms that fail to account for individual behavioral, contextual, or socio-emotional nuances. When customization is attempted, ethical concerns arise from unmitigated biases in user data, risking exclusion or unfair treatment. To address these dual challenges, we propose User-VLM 360°, a holistic framework integrating multimodal user modeling with bias-aware optimization. Our approach features: (1) user-aware tuning that adapts interactions in real time using visual-linguistic signals; (2) bias mitigation via preference optimization; and (3) curated 360° socio-emotive interaction datasets annotated with demographic, emotion, and relational metadata. Evaluations across eight benchmarks demonstrate state-of-the-art results: +35.3% F1 in personalized VQA, +47.5% F1 in facial features understanding, 15% bias reduction, and 30X speedup over baselines. Ablation studies confirm component efficacy, and deployment on the Pepper robot validates real-time adaptability across diverse users. We open-source parameter-efficient 3B/10B models and an ethical verification framework for responsible adaptation.

### ProMRVL-CAD: Proactive Dialogue System with Multi-Round Vision-Language Interactions for Computer-Aided Diagnosis 
[[arxiv](https://arxiv.org/abs/2502.10620)] [[cool](https://papers.cool/arxiv/2502.10620)] [[pdf](https://arxiv.org/pdf/2502.10620)]
> **Authors**: Xueshen Li,Xinlong Hou,Ziyi Huang,Yu Gan
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 17 pages, 6 figures
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Recent advancements in large language models (LLMs) have demonstrated extraordinary comprehension capabilities with remarkable breakthroughs on various vision-language tasks. However, the application of LLMs in generating reliable medical diagnostic reports remains in the early stages. Currently, medical LLMs typically feature a passive interaction model where doctors respond to patient queries with little or no involvement in analyzing medical images. In contrast, some ChatBots simply respond to predefined queries based on visual inputs, lacking interactive dialogue or consideration of medical history. As such, there is a gap between LLM-generated patient-ChatBot interactions and those occurring in actual patient-doctor consultations. To bridge this gap, we develop an LLM-based dialogue system, namely proactive multi-round vision-language interactions for computer-aided diagnosis (ProMRVL-CAD), to generate patient-friendly disease diagnostic reports. The proposed ProMRVL-CAD system allows proactive dialogue to provide patients with constant and reliable medical access via an integration of knowledge graph into a recommendation system. Specifically, we devise two generators: a Proactive Question Generator (Pro-Q Gen) to generate proactive questions that guide the diagnostic procedure and a Multi-Vision Patient-Text Diagnostic Report Generator (MVP-DR Gen) to produce high-quality diagnostic reports. Evaluating two real-world publicly available datasets, MIMIC-CXR and IU-Xray, our model has better quality in generating medical reports. We further demonstrate the performance of ProMRVL achieves robust under the scenarios with low image quality. Moreover, we have created a synthetic medical dialogue dataset that simulates proactive diagnostic interactions between patients and doctors, serving as a valuable resource for training LLM.

### Benchmarking the rationality of AI decision making using the transitivity axiom 
[[arxiv](https://arxiv.org/abs/2502.10554)] [[cool](https://papers.cool/arxiv/2502.10554)] [[pdf](https://arxiv.org/pdf/2502.10554)]
> **Authors**: Kiwon Song,James M. Jennings III,Clintin P. Davis-Stober
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 13 pages, 2 figures, 3 tables
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Fundamental choice axioms, such as transitivity of preference, provide testable conditions for determining whether human decision making is rational, i.e., consistent with a utility representation. Recent work has demonstrated that AI systems trained on human data can exhibit similar reasoning biases as humans and that AI can, in turn, bias human judgments through AI recommendation systems. We evaluate the rationality of AI responses via a series of choice experiments designed to evaluate transitivity of preference in humans. We considered ten versions of Meta's Llama 2 and 3 LLM models. We applied Bayesian model selection to evaluate whether these AI-generated choices violated two prominent models of transitivity. We found that the Llama 2 and 3 models generally satisfied transitivity, but when violations did occur, occurred only in the Chat/Instruct versions of the LLMs. We argue that rationality axioms, such as transitivity of preference, can be useful for evaluating and benchmarking the quality of AI-generated responses and provide a foundation for understanding computational rationality in AI systems more generally.

### GraphiT: Efficient Node Classification on Text-Attributed Graphs with Prompt Optimized LLMs 
[[arxiv](https://arxiv.org/abs/2502.10522)] [[cool](https://papers.cool/arxiv/2502.10522)] [[pdf](https://arxiv.org/pdf/2502.10522)]
> **Authors**: Shima Khoshraftar,Niaz Abedini,Amir Hajian
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 6 pages, 2 figures
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: The application of large language models (LLMs) to graph data has attracted a lot of attention recently. LLMs allow us to use deep contextual embeddings from pretrained models in text-attributed graphs, where shallow embeddings are often used for the text attributes of nodes. However, it is still challenging to efficiently encode the graph structure and features into a sequential form for use by LLMs. In addition, the performance of an LLM alone, is highly dependent on the structure of the input prompt, which limits their effectiveness as a reliable approach and often requires iterative manual adjustments that could be slow, tedious and difficult to replicate programmatically. In this paper, we propose GraphiT (Graphs in Text), a framework for encoding graphs into a textual format and optimizing LLM prompts for graph prediction tasks. Here we focus on node classification for text-attributed graphs. We encode the graph data for every node and its neighborhood into a concise text to enable LLMs to better utilize the information in the graph. We then further programmatically optimize the LLM prompts using the DSPy framework to automate this step and make it more efficient and reproducible. GraphiT outperforms our LLM-based baselines on three datasets and we show how the optimization step in GraphiT leads to measurably better results without manual prompt tweaking. We also demonstrated that our graph encoding approach is competitive to other graph encoding methods while being less expensive because it uses significantly less tokens for the same task.

### A Self-Supervised Reinforcement Learning Approach for Fine-Tuning Large Language Models Using Cross-Attention Signals 
[[arxiv](https://arxiv.org/abs/2502.10482)] [[cool](https://papers.cool/arxiv/2502.10482)] [[pdf](https://arxiv.org/pdf/2502.10482)]
> **Authors**: Andrew Kiruluta,Andreas Lemos,Priscilla Burity
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: We propose a novel reinforcement learning framework for post training large language models that does not rely on human in the loop feedback. Instead, our approach uses cross attention signals within the model itself to derive a self supervised reward, thereby guiding iterative fine tuning of the model policy. By analyzing how the model attends to the input prompt during generation, we construct measures of prompt coverage, focus, and coherence. We then use these measures to rank or score candidate responses, providing a reward signal that encourages the model to produce well aligned, on topic text. In empirical comparisons against standard policy gradient methods and RL fine tuning with synthetic preference models, our method shows significant gains in prompt relevance and consistency over a non RL baseline. While it does not yet match the performance of fully human supervised RLHF systems, it highlights an important direction for scaling alignment with minimal human labeling. We provide a detailed analysis, discuss potential limitations, and outline future work for combining cross-attention based signals with smaller amounts of human feedback.

### Knowledge Integration Strategies in Autonomous Vehicle Prediction and Planning: A Comprehensive Survey 
[[arxiv](https://arxiv.org/abs/2502.10477)] [[cool](https://papers.cool/arxiv/2502.10477)] [[pdf](https://arxiv.org/pdf/2502.10477)]
> **Authors**: Kumar Manas,Adrian Paschke
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: This comprehensive survey examines the integration of knowledge-based approaches into autonomous driving systems, with a focus on trajectory prediction and planning. We systematically review methodologies for incorporating domain knowledge, traffic rules, and commonsense reasoning into these systems, spanning purely symbolic representations to hybrid neuro-symbolic architectures. In particular, we analyze recent advancements in formal logic and differential logic programming, reinforcement learning frameworks, and emerging techniques that leverage large foundation models and diffusion models for knowledge representation. Organized under a unified literature survey section, our discussion synthesizes the state-of-the-art into a high-level overview, supported by a detailed comparative table that maps key works to their respective methodological categories. This survey not only highlights current trends -- including the growing emphasis on interpretable AI, formal verification in safety-critical systems, and the increased use of generative models in prediction and planning -- but also outlines the challenges and opportunities for developing robust, knowledge-enhanced autonomous driving systems.

### Diverse Transformer Decoding for Offline Reinforcement Learning Using Financial Algorithmic Approaches 
[[arxiv](https://arxiv.org/abs/2502.10473)] [[cool](https://papers.cool/arxiv/2502.10473)] [[pdf](https://arxiv.org/pdf/2502.10473)]
> **Authors**: Dan Elbaz,Oren Salzman
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: Offline Reinforcement Learning (RL) algorithms learn a policy using a fixed training dataset, which is then deployed online to interact with the environment and make decisions. Transformers, a standard choice for modeling time-series data, are gaining popularity in offline RL. In this context, Beam Search (BS), an approximate inference algorithm, is the go-to decoding method. Offline RL eliminates the need for costly or risky online data collection. However, the restricted dataset induces uncertainty as the agent may encounter unfamiliar sequences of states and actions during execution that were not covered in the training data. In this context, BS lacks two important properties essential for offline RL: It does not account for the aforementioned uncertainty, and its greedy left-right search approach often results in sequences with minimal variations, failing to explore potentially better alternatives. To address these limitations, we propose Portfolio Beam Search (PBS), a simple-yet-effective alternative to BS that balances exploration and exploitation within a Transformer model during decoding. We draw inspiration from financial economics and apply these principles to develop an uncertainty-aware diversification mechanism, which we integrate into a sequential decoding algorithm at inference time. We empirically demonstrate the effectiveness of PBS on the D4RL locomotion benchmark, where it achieves higher returns and significantly reduces outcome variability.

### AI Alignment at Your Discretion 
[[arxiv](https://arxiv.org/abs/2502.10441)] [[cool](https://papers.cool/arxiv/2502.10441)] [[pdf](https://arxiv.org/pdf/2502.10441)]
> **Authors**: Maarten Buyl,Hadi Khalaf,Claudio Mayrink Verdun,Lucas Monteiro Paes,Caio C. Vieira Machado,Flavio du Pin Calmon
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算机与社会,机器学习
- **Abstract**: In AI alignment, extensive latitude must be granted to annotators, either human or algorithmic, to judge which model outputs are `better' or `safer.' We refer to this latitude as alignment discretion. Such discretion remains largely unexamined, posing two risks: (i) annotators may use their power of discretion arbitrarily, and (ii) models may fail to mimic this discretion. To study this phenomenon, we draw on legal concepts of discretion that structure how decision-making authority is conferred and exercised, particularly in cases where principles conflict or their application is unclear or irrelevant. Extended to AI alignment, discretion is required when alignment principles and rules are (inevitably) conflicting or indecisive. We present a set of metrics to systematically analyze when and how discretion in AI alignment is exercised, such that both risks (i) and (ii) can be observed. Moreover, we distinguish between human and algorithmic discretion and analyze the discrepancy between them. By measuring both human and algorithmic discretion over safety alignment datasets, we reveal layers of discretion in the alignment process that were previously unaccounted for. Furthermore, we demonstrate how algorithms trained on these datasets develop their own forms of discretion in interpreting and applying these principles, which challenges the purpose of having any principles at all. Our paper presents the first step towards formalizing this core gap in current alignment processes, and we call on the community to further scrutinize and control alignment discretion.

### Agency in Artificial Intelligence Systems 
[[arxiv](https://arxiv.org/abs/2502.10434)] [[cool](https://papers.cool/arxiv/2502.10434)] [[pdf](https://arxiv.org/pdf/2502.10434)]
> **Authors**: Parashar Das
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算机与社会
- **Abstract**: There is a general concern that present developments in artificial intelligence (AI) research will lead to sentient AI systems, and these may pose an existential threat to humanity. But why cannot sentient AI systems benefit humanity instead? This paper endeavours to put this question in a tractable manner. I ask whether a putative AI system will develop an altruistic or a malicious disposition towards our society, or what would be the nature of its agency? Given that AI systems are being developed into formidable problem solvers, we can reasonably expect these systems to preferentially take on conscious aspects of human problem solving. I identify the relevant phenomenal aspects of agency in human problem solving. The functional aspects of conscious agency can be monitored using tools provided by functionalist theories of consciousness. A recent expert report (Butlin et al. 2023) has identified functionalist indicators of agency based on these theories. I show how to use the Integrated Information Theory (IIT) of consciousness, to monitor the phenomenal nature of this agency. If we are able to monitor the agency of AI systems as they develop, then we can dissuade them from becoming a menace to society while encouraging them to be an aid.

### Dynamic Chain-of-Thought: Towards Adaptive Deep Reasoning 
[[arxiv](https://arxiv.org/abs/2502.10428)] [[cool](https://papers.cool/arxiv/2502.10428)] [[pdf](https://arxiv.org/pdf/2502.10428)]
> **Authors**: Libo Wang
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-17
> **comment**: The GitHub repository link is: https://github.com/brucewang123456789/GeniusTrail/tree/main/Dynamic%20CoT
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: To reduce the cost and consumption of computing resources caused by computational redundancy and delayed reward assignment in long CoT, this research proposes the dynamic chain-of-thought (D-CoT) with adaptive reasoning time and steps. The researcher used simulation experiment to simulate the integration of D-CoT through Python 3.13 IDLE combined with a Python simulator based on GPTs. At the same time, the researcher used DeepSeek R1 as a control group to test and compare the performance of the D-CoT simulator in processing MIT OpenCourseWare's linear algebra exam questions. Experimental results show that D-CoT is better than DeepSeek R1 based on long CoT in three indicators: reasoning time, CoT length (reasoning steps) and token count, which achieves a significant reduction in computing resource consumption. In addition, this research has potential value in deep reasoning optimization that is used as a reference for future dynamic deep reasoning frameworks.

### Position: Stop Acting Like Language Model Agents Are Normal Agents 
[[arxiv](https://arxiv.org/abs/2502.10420)] [[cool](https://papers.cool/arxiv/2502.10420)] [[pdf](https://arxiv.org/pdf/2502.10420)]
> **Authors**: Elija Perrier,Michael Timothy Bennett
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-17
> **comment**: Under review
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: Language Model Agents (LMAs) are increasingly treated as capable of autonomously navigating interactions with humans and tools. Their design and deployment tends to presume they are normal agents capable of sustaining coherent goals, adapting across contexts and acting with a measure of intentionality. These assumptions are critical to prospective use cases in industrial, social and governmental settings. But LMAs are not normal agents. They inherit the structural problems of the large language models (LLMs) around which they are built: hallucinations, jailbreaking, misalignment and unpredictability. In this Position paper we argue LMAs should not be treated as normal agents, because doing so leads to problems that undermine their utility and trustworthiness. We enumerate pathologies of agency intrinsic to LMAs. Despite scaffolding such as external memory and tools, they remain ontologically stateless, stochastic, semantically sensitive, and linguistically intermediated. These pathologies destabilise the ontological properties of LMAs including identifiability, continuity, persistence and and consistency, problematising their claim to agency. In response, we argue LMA ontological properties should be measured before, during and after deployment so that the negative effects of pathologies can be mitigated.

### A Coordination-based Approach for Focused Learning in Knowledge-Based Systems 
[[arxiv](https://arxiv.org/abs/2502.10394)] [[cool](https://papers.cool/arxiv/2502.10394)] [[pdf](https://arxiv.org/pdf/2502.10394)]
> **Authors**: Abhishek Sharma
> **First submission**: 2025-01-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: Recent progress in Learning by Reading and Machine Reading systems has significantly increased the capacity of knowledge-based systems to learn new facts. In this work, we discuss the problem of selecting a set of learning requests for these knowledge-based systems which would lead to maximum Q/A performance. To understand the dynamics of this problem, we simulate the properties of a learning strategy, which sends learning requests to an external knowledge source. We show that choosing an optimal set of facts for these learning systems is similar to a coordination game, and use reinforcement learning to solve this problem. Experiments show that such an approach can significantly improve Q/A performance.

### Representation and Interpretation in Artificial and Natural Computing 
[[arxiv](https://arxiv.org/abs/2502.10383)] [[cool](https://papers.cool/arxiv/2502.10383)] [[pdf](https://arxiv.org/pdf/2502.10383)]
> **Authors**: Luis A. Pineda
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: :F.0
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Artificial computing machinery transforms representations through an objective process, to be interpreted subjectively by humans, so the machine and the interpreter are different entities, but in the putative natural computing both processes are performed by the same agent. The method or process that transforms a representation is called here \emph{the mode of computing}. The mode used by digital computers is the algorithmic one, but there are others, such as quantum computers and diverse forms of non-conventional computing, and there is an open-ended set of representational formats and modes that could be used in artificial and natural computing. A mode based on a notion of computing different from Turing's may perform feats beyond what the Turing Machine does but the modes would not be of the same kind and could not be compared. For a mode of computing to be more powerful than the algorithmic one, it ought to compute functions lacking an effective algorithm, and Church Thesis would not hold. Here, a thought experiment including a computational demon using a hypothetical mode for such an effect is presented. If there is natural computing, there is a mode of natural computing whose properties may be causal to the phenomenological experience. Discovering it would come with solving the hard problem of consciousness; but if it turns out that such a mode does not exist, there is no such thing as natural computing, and the mind is not a computational process.

### LLM-Powered Preference Elicitation in Combinatorial Assignment 
[[arxiv](https://arxiv.org/abs/2502.10308)] [[cool](https://papers.cool/arxiv/2502.10308)] [[pdf](https://arxiv.org/pdf/2502.10308)]
> **Authors**: Ermis Soumalias,Yanchen Jiang,Kehang Zhu,Michael Curry,Sven Seuken,David C. Parkes
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算机科学与博弈论,机器学习
- **Abstract**: We study the potential of large language models (LLMs) as proxies for humans to simplify preference elicitation (PE) in combinatorial assignment. While traditional PE methods rely on iterative queries to capture preferences, LLMs offer a one-shot alternative with reduced human effort. We propose a framework for LLM proxies that can work in tandem with SOTA ML-powered preference elicitation schemes. Our framework handles the novel challenges introduced by LLMs, such as response variability and increased computational costs. We experimentally evaluate the efficiency of LLM proxies against human queries in the well-studied course allocation domain, and we investigate the model capabilities required for success. We find that our approach improves allocative efficiency by up to 20%, and these results are robust across different LLMs and to differences in quality and accuracy of reporting.

### Reinforcement Learning in Strategy-Based and Atari Games: A Review of Google DeepMinds Innovations 
[[arxiv](https://arxiv.org/abs/2502.10303)] [[cool](https://papers.cool/arxiv/2502.10303)] [[pdf](https://arxiv.org/pdf/2502.10303)]
> **Authors**: Abdelrhman Shaheen,Anas Badr,Ali Abohendy,Hatem Alsaadawy,Nadine Alsayad
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算机科学与博弈论
- **Abstract**: Reinforcement Learning (RL) has been widely used in many applications, particularly in gaming, which serves as an excellent training ground for AI models. Google DeepMind has pioneered innovations in this field, employing reinforcement learning algorithms, including model-based, model-free, and deep Q-network approaches, to create advanced AI models such as AlphaGo, AlphaGo Zero, and MuZero. AlphaGo, the initial model, integrates supervised learning and reinforcement learning to master the game of Go, surpassing professional human players. AlphaGo Zero refines this approach by eliminating reliance on human gameplay data, instead utilizing self-play for enhanced learning efficiency. MuZero further extends these advancements by learning the underlying dynamics of game environments without explicit knowledge of the rules, achieving adaptability across various games, including complex Atari games. This paper reviews the significance of reinforcement learning applications in Atari and strategy-based games, analyzing these three models, their key innovations, training processes, challenges encountered, and improvements made. Additionally, we discuss advancements in the field of gaming, including MiniZero and multi-agent models, highlighting future directions and emerging AI models from Google DeepMind.

### Do Large Language Models Reason Causally Like Us? Even Better? 
[[arxiv](https://arxiv.org/abs/2502.10215)] [[cool](https://papers.cool/arxiv/2502.10215)] [[pdf](https://arxiv.org/pdf/2502.10215)]
> **Authors**: Hanna M. Dettki,Brenden M. Lake,Charley M. Wu,Bob Rehder
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: Causal reasoning is a core component of intelligence. Large language models (LLMs) have shown impressive capabilities in generating human-like text, raising questions about whether their responses reflect true understanding or statistical patterns. We compared causal reasoning in humans and four LLMs using tasks based on collider graphs, rating the likelihood of a query variable occurring given evidence from other variables. We find that LLMs reason causally along a spectrum from human-like to normative inference, with alignment shifting based on model, context, and task. Overall, GPT-4o and Claude showed the most normative behavior, including "explaining away", whereas Gemini-Pro and GPT-3.5 did not. Although all agents deviated from the expected independence of causes - Claude the least - they exhibited strong associative reasoning and predictive inference when assessing the likelihood of the effect given its causes. These findings underscore the need to assess AI biases as they increasingly assist human decision-making.

### MathConstruct: Challenging LLM Reasoning with Constructive Proofs 
[[arxiv](https://arxiv.org/abs/2502.10197)] [[cool](https://papers.cool/arxiv/2502.10197)] [[pdf](https://arxiv.org/pdf/2502.10197)]
> **Authors**: Mislav Balunović,Jasper Dekoninck,Nikola Jovanović,Ivo Petrov,Martin Vechev
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: While Large Language Models (LLMs) demonstrate impressive performance in mathematics, existing math benchmarks come with significant limitations. Many focus on problems with fixed ground-truth answers, and are often saturated due to problem simplicity or the viability of guessing or memorization. Crucially, they capture only a narrow subset of relevant math problems. To address this research gap, we introduce \mc, a new benchmark of 126 challenging problems sourced from various math competitions, which targets constructive proofs, a widely encountered problem type requiring the construction of mathematical objects with specific properties. These proofs are particularly suitable for LLM evaluation, as solution correctness can be easily verified. Our automated verifiers also enable MathConstruct to generate problem variations, used to evaluate robustness. State-of-the-art LLMs solve only 54% of MathConstruct problems, highlighting its complexity and importance for LLM evaluation.

### Cooperative Multi-Agent Planning with Adaptive Skill Synthesis 
[[arxiv](https://arxiv.org/abs/2502.10148)] [[cool](https://papers.cool/arxiv/2502.10148)] [[pdf](https://arxiv.org/pdf/2502.10148)]
> **Authors**: Zhiyuan Li,Wenshuai Zhao,Joni Pajarinen
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,多代理系统
- **Abstract**: Despite much progress in training distributed artificial intelligence (AI), building cooperative multi-agent systems with multi-agent reinforcement learning (MARL) faces challenges in sample efficiency, interpretability, and transferability. Unlike traditional learning-based methods that require extensive interaction with the environment, large language models (LLMs) demonstrate remarkable capabilities in zero-shot planning and complex reasoning. However, existing LLM-based approaches heavily rely on text-based observations and struggle with the non-Markovian nature of multi-agent interactions under partial observability. We present COMPASS, a novel multi-agent architecture that integrates vision-language models (VLMs) with a dynamic skill library and structured communication for decentralized closed-loop decision-making. The skill library, bootstrapped from demonstrations, evolves via planner-guided tasks to enable adaptive strategies. COMPASS propagates entity information through multi-hop communication under partial observability. Evaluations on the improved StarCraft Multi-Agent Challenge (SMACv2) demonstrate COMPASS achieves up to 30\% higher win rates than state-of-the-art MARL algorithms in symmetric scenarios.

### Causal Information Prioritization for Efficient Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.10097)] [[cool](https://papers.cool/arxiv/2502.10097)] [[pdf](https://arxiv.org/pdf/2502.10097)]
> **Authors**: Hongye Cao,Fan Feng,Tianpei Yang,Jing Huo,Yang Gao
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: Current Reinforcement Learning (RL) methods often suffer from sample-inefficiency, resulting from blind exploration strategies that neglect causal relationships among states, actions, and rewards. Although recent causal approaches aim to address this problem, they lack grounded modeling of reward-guided causal understanding of states and actions for goal-orientation, thus impairing learning efficiency. To tackle this issue, we propose a novel method named Causal Information Prioritization (CIP) that improves sample efficiency by leveraging factored MDPs to infer causal relationships between different dimensions of states and actions with respect to rewards, enabling the prioritization of causal information. Specifically, CIP identifies and leverages causal relationships between states and rewards to execute counterfactual data augmentation to prioritize high-impact state features under the causal understanding of the environments. Moreover, CIP integrates a causality-aware empowerment learning objective, which significantly enhances the agent's execution of reward-guided actions for more efficient exploration in complex environments. To fully assess the effectiveness of CIP, we conduct extensive experiments across 39 tasks in 5 diverse continuous control environments, encompassing both locomotion and manipulation skills learning with pixel-based and sparse reward settings. Experimental results demonstrate that CIP consistently outperforms existing RL methods across a wide range of scenarios.

### Towards Empowerment Gain through Causal Structure Learning in Model-Based RL 
[[arxiv](https://arxiv.org/abs/2502.10077)] [[cool](https://papers.cool/arxiv/2502.10077)] [[pdf](https://arxiv.org/pdf/2502.10077)]
> **Authors**: Hongye Cao,Fan Feng,Meng Fang,Shaokang Dong,Tianpei Yang,Jing Huo,Yang Gao
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: In Model-Based Reinforcement Learning (MBRL), incorporating causal structures into dynamics models provides agents with a structured understanding of the environments, enabling efficient decision. Empowerment as an intrinsic motivation enhances the ability of agents to actively control their environments by maximizing the mutual information between future states and actions. We posit that empowerment coupled with causal understanding can improve controllability, while enhanced empowerment gain can further facilitate causal reasoning in MBRL. To improve learning efficiency and controllability, we propose a novel framework, Empowerment through Causal Learning (ECL), where an agent with the awareness of causal dynamics models achieves empowerment-driven exploration and optimizes its causal structure for task learning. Specifically, ECL operates by first training a causal dynamics model of the environment based on collected data. We then maximize empowerment under the causal structure for exploration, simultaneously using data gathered through exploration to update causal dynamics model to be more controllable than dense dynamics model without causal structure. In downstream task learning, an intrinsic curiosity reward is included to balance the causality, mitigating overfitting. Importantly, ECL is method-agnostic and is capable of integrating various causal discovery methods. We evaluate ECL combined with 3 causal discovery methods across 6 environments including pixel-based tasks, demonstrating its superior performance compared to other causal MBRL methods, in terms of causal discovery, sample efficiency, and asymptotic performance.

### POI-Enhancer: An LLM-based Semantic Enhancement Framework for POI Representation Learning 
[[arxiv](https://arxiv.org/abs/2502.10038)] [[cool](https://papers.cool/arxiv/2502.10038)] [[pdf](https://arxiv.org/pdf/2502.10038)]
> **Authors**: Jiawei Cheng,Jingyuan Wang,Yichuan Zhang,Jiahao Ji,Yuanshao Zhu,Zhibo Zhang,Xiangyu Zhao
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: POI representation learning plays a crucial role in handling tasks related to user mobility data. Recent studies have shown that enriching POI representations with multimodal information can significantly enhance their task performance. Previously, the textual information incorporated into POI representations typically involved only POI categories or check-in content, leading to relatively weak textual features in existing methods. In contrast, large language models (LLMs) trained on extensive text data have been found to possess rich textual knowledge. However leveraging such knowledge to enhance POI representation learning presents two key challenges: first, how to extract POI-related knowledge from LLMs effectively, and second, how to integrate the extracted information to enhance POI representations. To address these challenges, we propose POI-Enhancer, a portable framework that leverages LLMs to improve POI representations produced by classic POI learning models. We first design three specialized prompts to extract semantic information from LLMs efficiently. Then, the Dual Feature Alignment module enhances the quality of the extracted information, while the Semantic Feature Fusion module preserves its integrity. The Cross Attention Fusion module then fully adaptively integrates such high-quality information into POI representations and Multi-View Contrastive Learning further injects human-understandable semantic information into these representations. Extensive experiments on three real-world datasets demonstrate the effectiveness of our framework, showing significant improvements across all baseline representations.

### Decision Information Meets Large Language Models: The Future of Explainable Operations Research 
[[arxiv](https://arxiv.org/abs/2502.09994)] [[cool](https://papers.cool/arxiv/2502.09994)] [[pdf](https://arxiv.org/pdf/2502.09994)]
> **Authors**: Yansen Zhang,Qingcan Kang,Wing Yin Yu,Hailei Gong,Xiaojin Fu,Xiongwei Han,Tao Zhong,Chen Ma
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Operations Research (OR) is vital for decision-making in many industries. While recent OR methods have seen significant improvements in automation and efficiency through integrating Large Language Models (LLMs), they still struggle to produce meaningful explanations. This lack of clarity raises concerns about transparency and trustworthiness in OR applications. To address these challenges, we propose a comprehensive framework, Explainable Operations Research (EOR), emphasizing actionable and understandable explanations accompanying optimization. The core of EOR is the concept of Decision Information, which emerges from what-if analysis and focuses on evaluating the impact of complex constraints (or parameters) changes on decision-making. Specifically, we utilize bipartite graphs to quantify the changes in the OR model and adopt LLMs to improve the explanation capabilities. Additionally, we introduce the first industrial benchmark to rigorously evaluate the effectiveness of explanations and analyses in OR, establishing a new standard for transparency and clarity in the field.

### Has My System Prompt Been Used? Large Language Model Prompt Membership Inference 
[[arxiv](https://arxiv.org/abs/2502.09974)] [[cool](https://papers.cool/arxiv/2502.09974)] [[pdf](https://arxiv.org/pdf/2502.09974)]
> **Authors**: Roman Levin,Valeriia Cherepanova,Abhimanyu Hans,Avi Schwarzschild,Tom Goldstein
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,密码学和安全
- **Abstract**: Prompt engineering has emerged as a powerful technique for optimizing large language models (LLMs) for specific applications, enabling faster prototyping and improved performance, and giving rise to the interest of the community in protecting proprietary system prompts. In this work, we explore a novel perspective on prompt privacy through the lens of membership inference. We develop Prompt Detective, a statistical method to reliably determine whether a given system prompt was used by a third-party language model. Our approach relies on a statistical test comparing the distributions of two groups of model outputs corresponding to different system prompts. Through extensive experiments with a variety of language models, we demonstrate the effectiveness of Prompt Detective for prompt membership inference. Our work reveals that even minor changes in system prompts manifest in distinct response distributions, enabling us to verify prompt usage with statistical significance.

### Diverse Inference and Verification for Advanced Reasoning 
[[arxiv](https://arxiv.org/abs/2502.09955)] [[cool](https://papers.cool/arxiv/2502.09955)] [[pdf](https://arxiv.org/pdf/2502.09955)]
> **Authors**: Iddo Drori,Gaston Longhitano,Mao Mao,Seunghwan Hyun,Yuke Zhang,Sungjun Park,Zachary Meeks,Xin-Yu Zhang,Ben Segev,Howard Yong,Nakul Verma,Avi Shporer,Alon Amit,Madeleine Udell
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 165 pages
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Reasoning LLMs such as OpenAI o1, o3 and DeepSeek R1 have made significant progress in mathematics and coding, yet find challenging advanced tasks such as International Mathematical Olympiad (IMO) combinatorics problems, Abstraction and Reasoning Corpus (ARC) puzzles, and Humanity's Last Exam (HLE) questions. We use a diverse inference approach that combines multiple models and methods at test time. We find that verifying mathematics and code problems, and rejection sampling on other problems is simple and effective. We automatically verify correctness of solutions to IMO problems by Lean, and ARC puzzles by code, and find that best-of-N effectively answers HLE questions. Our approach increases answer accuracy on IMO combinatorics problems from 33.3% to 77.8%, accuracy on HLE questions from 8% to 37%, and solves 80% of ARC puzzles that 948 humans could not and 26.5% of ARC puzzles that o3 high compute does not. Test-time simulations, reinforcement learning, and meta-learning with inference feedback improve generalization by adapting agent graph representations and varying prompts, code, and datasets. Our approach is reliable, robust, and scalable, and in the spirit of reproducible research, we will make it publicly available upon publication.

### Analyzing Patient Daily Movement Behavior Dynamics Using Two-Stage Encoding Model 
[[arxiv](https://arxiv.org/abs/2502.09947)] [[cool](https://papers.cool/arxiv/2502.09947)] [[pdf](https://arxiv.org/pdf/2502.09947)]
> **Authors**: Jin Cui,Alexander Capstick,Payam Barnaghi,Gregory Scott
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: NeurIPS 2024 workshop Time Series in the Age of Large Models. arXiv admin note: substantial text overlap with arXiv:2502.09173
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: In the analysis of remote healthcare monitoring data, time series representation learning offers substantial value in uncovering deeper patterns of patient behavior, especially given the fine temporal granularity of the data. In this study, we focus on a dataset of home activity records from people living with Dementia. We propose a two-stage self-supervised learning approach. The first stage involves converting time-series activities into text strings, which are then encoded by a fine-tuned language model. In the second stage, these time-series vectors are bi-dimensionalized for applying PageRank method, to analyze latent state transitions to quantitatively assess participants behavioral patterns and identify activity biases. These insights, combined with diagnostic data, aim to support personalized care interventions.

### MIR-Bench: Benchmarking LLM's Long-Context Intelligence via Many-Shot In-Context Inductive Reasoning 
[[arxiv](https://arxiv.org/abs/2502.09933)] [[cool](https://papers.cool/arxiv/2502.09933)] [[pdf](https://arxiv.org/pdf/2502.09933)]
> **Authors**: Kai Yan,Zhan Ling,Kang Liu,Yifan Yang,Ting-Han Fan,Lingfeng Shen,Zhengyin Du,Jiecao Chen
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 32 pages, 11 figures. v3 slightly adjust the author institution
- **标题**: None
- **领域**: 人工智能,计算语言学,机器学习
- **Abstract**: Inductive Reasoning (IR), the ability to summarize rules from examples and apply on new ones, has long been viewed as a primal ability for general intelligence and widely studied by cognitive science and AI researchers. Many benchmarks have been proposed to measure such ability for Large Language Models (LLMs); however, they focus on few-shot (usually $<$10) setting and lack evaluation for aggregating many pieces of information from long contexts. On the other hand, the ever-growing context length of LLMs have brought forth the novel paradigm of many-shot In-Context Learning (ICL), which addresses new tasks with hundreds to thousands of examples without expensive and inefficient fine-tuning. However, many-shot evaluations are mostly focused on classification (a very limited aspect of IR), and popular long-context LLM tasks such as Needle-In-A-Haystack (NIAH) seldom require complicated intelligence for integrating many pieces of information. To fix the issues from both worlds, we propose MIR-Bench, the first many-shot in-context inductive reasoning benchmark that asks LLM to induce output via input-output examples from underlying functions with diverse data format. Based on MIR-Bench, we study many novel problems for inductive reasoning and many-shot ICL, including robustness against erroneous shots and the effect of Chain-of-Thought (CoT), and acquired insightful findings.

## 硬件架构(cs.AR:Hardware Architecture)

### Strassen Multisystolic Array Hardware Architectures 
[[arxiv](https://arxiv.org/abs/2502.10063)] [[cool](https://papers.cool/arxiv/2502.10063)] [[pdf](https://arxiv.org/pdf/2502.10063)]
> **Authors**: Trevor E. Pogue,Nicola Nicolici
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: Accepted for publication in IEEE Transactions on Very Large Scale Integration (VLSI) Systems; Associated source code available on GitHub at https://github.com/trevorpogue/algebraic-nnhw
- **标题**: None
- **领域**: 硬件架构,人工智能,表现
- **Abstract**: While Strassen's matrix multiplication algorithm reduces the complexity of naive matrix multiplication, general-purpose hardware is not suitable for achieving the algorithm's promised theoretical speedups. This leaves the question of if it could be better exploited in custom hardware architectures designed specifically for executing the algorithm. However, there is limited prior work on this and it is not immediately clear how to derive such architectures or if they can ultimately lead to real improvements. We bridge this gap, presenting and evaluating new systolic array architectures that efficiently translate the theoretical complexity reductions of Strassen's algorithm directly into hardware resource savings. Furthermore, the architectures are multisystolic array designs that can multiply smaller matrices with higher utilization than single-systolic array designs. The proposed designs implemented on FPGA reduce DSP requirements by a factor of $1.14^r$ for $r$ implemented Strassen recursion levels, and otherwise require overall similar soft logic resources when instantiated to support matrix sizes down to 32x32 and 24x24 at 1-2 levels of Strassen recursion, respectively. We evaluate the proposed designs both in isolation and in an end-to-end machine learning accelerator compared to baseline designs and prior works, achieving state-of-the-art performance.

## 计算语言学(cs.CL:Computation and Language)

### Which Retain Set Matters for LLM Unlearning? A Case Study on Entity Unlearning 
[[arxiv](https://arxiv.org/abs/2502.11441)] [[cool](https://papers.cool/arxiv/2502.11441)] [[pdf](https://arxiv.org/pdf/2502.11441)]
> **Authors**: Hwan Chang,Hwanhee Lee
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: Work in Progress
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) risk retaining unauthorized or sensitive information from their training data, which raises privacy concerns. LLM unlearning seeks to mitigate these risks by selectively removing specified data while maintaining overall model performance. However, most existing work focus on methods to achieve effective forgetting and does not provide a detailed analysis of the retain set, the portion of training data that is not targeted for removal. In this paper, we investigate the effects of unlearning on various subsets of the retain set through a case study on entity unlearning. We introduce the Syntactically Similar Neighbor Set, a group of queries that share similar syntactic structures with the data targeted for removal, and show that this subset suffers the greatest performance drop during unlearning. Moreover, when used for regularization, this set not only preserves performance on syntactically similar queries but also delivers comparable or improved results across other data subsets. Our results highlight that syntactic similarity is a critical factor, potentially more so than domain or entity relationships, in achieving effective and practical LLM unlearning.

### An Efficient Row-Based Sparse Fine-Tuning 
[[arxiv](https://arxiv.org/abs/2502.11439)] [[cool](https://papers.cool/arxiv/2502.11439)] [[pdf](https://arxiv.org/pdf/2502.11439)]
> **Authors**: Cen-Jhih Li,Aditya Bhaskara
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Fine-tuning is an important step in adapting foundation models such as large language models to downstream tasks. To make this step more accessible to users with limited computational budgets, it is crucial to develop fine-tuning methods that are memory and computationally efficient. Sparse Fine-tuning (SFT) and Low-rank adaptation (LoRA) are two frameworks that have emerged for addressing this problem and have been adopted widely in practice. In this work, we develop a new SFT framework, based on ideas from neural network pruning. At a high level, we first identify "important" neurons/nodes using feature importance metrics from network pruning (specifically, we use the structural pruning method), and then perform fine-tuning by restricting to weights involving these neurons. Using experiments on common language tasks, we demonstrate that our method significantly improves the memory efficiency of SFT without increasing training time complexity and implementation complexity, while achieving accuracy comparable to state-of-the-art methods such as LoRA and its variants.

### SAFE-SQL: Self-Augmented In-Context Learning with Fine-grained Example Selection for Text-to-SQL 
[[arxiv](https://arxiv.org/abs/2502.11438)] [[cool](https://papers.cool/arxiv/2502.11438)] [[pdf](https://arxiv.org/pdf/2502.11438)]
> **Authors**: Jimin Lee,Ingeol Baek,Byeongjeong Kim,Hwanhee Lee
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 13 pages, 5 figures, 10 tables
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Text-to-SQL aims to convert natural language questions into executable SQL queries. While previous approaches, such as skeleton-masked selection, have demonstrated strong performance by retrieving similar training examples to guide large language models (LLMs), they struggle in real-world scenarios where such examples are unavailable. To overcome this limitation, we propose Self-Augmentation in-context learning with Fine-grained Example selection for Text-to-SQL (SAFE-SQL), a novel framework that improves SQL generation by generating and filtering self-augmented examples. SAFE-SQL first prompts an LLM to generate multiple Text-to-SQL examples relevant to the test input. Then SAFE-SQL filters these examples through three relevance assessments, constructing high-quality in-context learning examples. Using self-generated examples, SAFE-SQL surpasses the previous zero-shot, and few-shot Text-to-SQL frameworks, achieving higher execution accuracy. Notably, our approach provides additional performance gains in extra hard and unseen scenarios, where conventional methods often fail.

### Do we Really Need Visual Instructions? Towards Visual Instruction-Free Fine-tuning for Large Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2502.11427)] [[cool](https://papers.cool/arxiv/2502.11427)] [[pdf](https://arxiv.org/pdf/2502.11427)]
> **Authors**: Zikang Liu,Kun Zhou,Wayne Xin Zhao,Dawei Gao,Yaliang Li,Ji-Rong Wen
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: under review
- **标题**: None
- **领域**: 计算语言学,计算机视觉和模式识别
- **Abstract**: Visual instruction tuning has become the predominant technology in eliciting the multimodal task-solving capabilities of large vision-language models (LVLMs). Despite the success, as visual instructions require images as the input, it would leave the gap in inheriting the task-solving capabilities from the backbone LLMs, and make it costly to collect a large-scale dataset. To address it, we propose ViFT, a visual instruction-free fine-tuning framework for LVLMs. In ViFT, we only require the text-only instructions and image caption data during training, to separately learn the task-solving and visual perception abilities. During inference, we extract and combine the representations of the text and image inputs, for fusing the two abilities to fulfill multimodal tasks. Experimental results demonstrate that ViFT can achieve state-of-the-art performance on several visual reasoning and visual instruction following benchmarks, with rather less training data. Our code and data will be publicly released.

### Counterfactual-Consistency Prompting for Relative Temporal Understanding in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.11425)] [[cool](https://papers.cool/arxiv/2502.11425)] [[pdf](https://arxiv.org/pdf/2502.11425)]
> **Authors**: Jongho Kim,Seung-won Hwang
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: Preprint
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Despite the advanced capabilities of large language models (LLMs), their temporal reasoning ability remains underdeveloped. Prior works have highlighted this limitation, particularly in maintaining temporal consistency when understanding events. For example, models often confuse mutually exclusive temporal relations like ``before'' and ``after'' between events and make inconsistent predictions. In this work, we tackle the issue of temporal inconsistency in LLMs by proposing a novel counterfactual prompting approach. Our method generates counterfactual questions and enforces collective constraints, enhancing the model's consistency. We evaluate our method on multiple datasets, demonstrating significant improvements in event ordering for explicit and implicit events and temporal commonsense understanding by effectively addressing temporal inconsistencies.

### Exploring Persona Sentiment Sensitivity in Personalized Dialogue Generation 
[[arxiv](https://arxiv.org/abs/2502.11423)] [[cool](https://papers.cool/arxiv/2502.11423)] [[pdf](https://arxiv.org/pdf/2502.11423)]
> **Authors**: YongHyun Jun,Hwanhee Lee
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 19 pages, 8 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Personalized dialogue systems have advanced considerably with the integration of user-specific personas into large language models (LLMs). However, while LLMs can effectively generate personalized responses, the influence of persona sentiment on dialogue quality remains underexplored. In this work, we conduct a large-scale analysis of dialogues generated using a range of polarized user profiles. Our experiments reveal that dialogues involving negatively polarized users tend to overemphasize persona attributes, leading to increased entailment and contradiction instances and lower overall coherence. In contrast, positively polarized profiles yield dialogues that selectively incorporate persona information, resulting in smoother and more coherent interactions. Furthermore, we find that personas with weak or neutral sentiment generally produce lower-quality dialogues. Motivated by these findings, we propose a dialogue generation approach that explicitly accounts for persona polarity by combining a turn-based generation strategy with a profile ordering mechanism. Our study provides new insights into the sensitivity of LLMs to persona sentiment and offers guidance for developing more robust and nuanced personalized dialogue systems.

### InsBank: Evolving Instruction Subset for Ongoing Alignment 
[[arxiv](https://arxiv.org/abs/2502.11419)] [[cool](https://papers.cool/arxiv/2502.11419)] [[pdf](https://arxiv.org/pdf/2502.11419)]
> **Authors**: Jiayi Shi,Yiwei Li,Shaoxiong Feng,Peiwen Yuan,Xinglin Wang,Yueqi Zhang,Chuyi Tan,Boyuan Pan,Huan Ren,Yao Hu,Kan Li
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) typically undergo instruction tuning to enhance alignment. Recent studies emphasize that quality and diversity of instruction data are more crucial than quantity, highlighting the need to select diverse, high-quality subsets to reduce training costs. However, how to evolve these selected subsets alongside the development of new instruction data remains insufficiently explored. To achieve LLMs' ongoing alignment, we introduce Instruction Bank (InsBank), a continuously updated repository that integrates the latest valuable instruction data. We further propose Progressive Instruction Bank Evolution (PIBE), a novel framework designed to evolve InsBank effectively and efficiently over time. PIBE employs a gradual data selection strategy to maintain long-term efficiency, leveraging a representation-based diversity score to capture relationships between data points and retain historical information for comprehensive diversity evaluation. This also allows for flexible combination of diversity and quality scores during data selection and ranking. Extensive experiments demonstrate that PIBE significantly outperforms baselines in InsBank evolution and is able to extract budget-specific subsets, demonstrating its effectiveness and adaptability.

### LayAlign: Enhancing Multilingual Reasoning in Large Language Models via Layer-Wise Adaptive Fusion and Alignment Strategy 
[[arxiv](https://arxiv.org/abs/2502.11405)] [[cool](https://papers.cool/arxiv/2502.11405)] [[pdf](https://arxiv.org/pdf/2502.11405)]
> **Authors**: Zhiwen Ruan,Yixia Li,He Zhu,Longyue Wang,Weihua Luo,Kaifu Zhang,Yun Chen,Guanhua Chen
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: In Findings of NAACL 2025(The 2025 Annual Conference of the Nations of the Americas Chapter of the ACL)
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Despite being pretrained on multilingual corpora, large language models (LLMs) exhibit suboptimal performance on low-resource languages. Recent approaches have leveraged multilingual encoders alongside LLMs by introducing trainable parameters connecting the two models. However, these methods typically focus on the encoder's output, overlooking valuable information from other layers. We propose \aname (\mname), a framework that integrates representations from all encoder layers, coupled with the \attaname mechanism to enable layer-wise interaction between the LLM and the multilingual encoder. Extensive experiments on multilingual reasoning tasks, along with analyses of learned representations, show that our approach consistently outperforms existing baselines.

### ToolCoder: A Systematic Code-Empowered Tool Learning Framework for Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.11404)] [[cool](https://papers.cool/arxiv/2502.11404)] [[pdf](https://arxiv.org/pdf/2502.11404)]
> **Authors**: Hanxing Ding,Shuchang Tao,Liang Pang,Zihao Wei,Jinyang Gao,Bolin Ding,Huawei Shen,Xueqi Chen
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Tool learning has emerged as a crucial capability for large language models (LLMs) to solve complex real-world tasks through interaction with external tools. Existing approaches face significant challenges, including reliance on hand-crafted prompts, difficulty in multi-step planning, and lack of precise error diagnosis and reflection mechanisms. We propose ToolCoder, a novel framework that reformulates tool learning as a code generation task. Inspired by software engineering principles, ToolCoder transforms natural language queries into structured Python function scaffold and systematically breaks down tasks with descriptive comments, enabling LLMs to leverage coding paradigms for complex reasoning and planning. It then generates and executes function implementations to obtain final responses. Additionally, ToolCoder stores successfully executed functions in a repository to promote code reuse, while leveraging error traceback mechanisms for systematic debugging, optimizing both execution efficiency and robustness. Experiments demonstrate that ToolCoder achieves superior performance in task completion accuracy and execution reliability compared to existing approaches, establishing the effectiveness of code-centric approaches in tool learning.

### Following the Autoregressive Nature of LLM Embeddings via Compression and Alignment 
[[arxiv](https://arxiv.org/abs/2502.11401)] [[cool](https://papers.cool/arxiv/2502.11401)] [[pdf](https://arxiv.org/pdf/2502.11401)]
> **Authors**: Jingcheng Deng,Zhongtao Jiang,Liang Pang,Liwei Chen,Kun Xu,Zihao Wei,Huawei Shen,Xueqi Cheng
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: A new trend uses LLMs as dense text encoders via contrastive learning. However, since LLM embeddings predict the probability distribution of the next token, they are inherently generative and distributive, conflicting with contrastive learning, which requires embeddings to capture full-text semantics and align via cosine similarity. This discrepancy hinders the full utilization of LLMs' pre-training capabilities, resulting in inefficient learning. In response to this issue, we propose AutoRegEmbed, a new contrastive learning method built on embedding conditional probability distributions, which integrates two core tasks: information compression and conditional distribution alignment. The information compression task encodes text into the embedding space, ensuring that the embedding vectors capture global semantics. The conditional distribution alignment task focuses on aligning text embeddings with positive samples embeddings by leveraging the conditional distribution of embeddings while simultaneously reducing the likelihood of generating negative samples from text embeddings, thereby achieving embedding alignment and uniformity. Experimental results demonstrate that our method significantly outperforms traditional contrastive learning approaches and achieves performance comparable to state-of-the-art models when using the same amount of data.

### Revisiting Robust RAG: Do We Still Need Complex Robust Training in the Era of Powerful LLMs? 
[[arxiv](https://arxiv.org/abs/2502.11400)] [[cool](https://papers.cool/arxiv/2502.11400)] [[pdf](https://arxiv.org/pdf/2502.11400)]
> **Authors**: Hanxing Ding,Shuchang Tao,Liang Pang,Zihao Wei,Liwei Chen,Kun Xu,Huawei Shen,Xueqi Cheng
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Retrieval-augmented generation (RAG) systems often suffer from performance degradation when encountering noisy or irrelevant documents, driving researchers to develop sophisticated training strategies to enhance their robustness against such retrieval noise. However, as large language models (LLMs) continue to advance, the necessity of these complex training methods is increasingly questioned. In this paper, we systematically investigate whether complex robust training strategies remain necessary as model capacity grows. Through comprehensive experiments spanning multiple model architectures and parameter scales, we evaluate various document selection methods and adversarial training techniques across diverse datasets. Our extensive experiments consistently demonstrate that as models become more powerful, the performance gains brought by complex robust training methods drop off dramatically. We delve into the rationale and find that more powerful models inherently exhibit superior confidence calibration, better generalization across datasets (even when trained with randomly selected documents), and optimal attention mechanisms learned with simpler strategies. Our findings suggest that RAG systems can benefit from simpler architectures and training strategies as models become more powerful, enabling more scalable applications with minimal complexity.

### HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning 
[[arxiv](https://arxiv.org/abs/2502.11393)] [[cool](https://papers.cool/arxiv/2502.11393)] [[pdf](https://arxiv.org/pdf/2502.11393)]
> **Authors**: Xiaoyuan Li,Moxin Li,Rui Men,Yichang Zhang,Keqin Bao,Wenjie Wang,Fuli Feng,Dayiheng Liu,Junyang Lin
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: Under Review
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) have shown remarkable capabilities in commonsense reasoning; however, some variations in questions can trigger incorrect responses. Do these models truly understand commonsense knowledge, or just memorize expression patterns? To investigate this question, we present the first extensive robustness evaluation of LLMs in commonsense reasoning. We introduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200 cases, by designing and compiling seven types of question variants. To construct this benchmark, we propose a two-stage method to develop Chinese HellaSwag, a finely annotated dataset comprising 12,000 instances across 56 categories. We conduct extensive experiments on 41 representative LLMs, revealing that these LLMs are far from robust in commonsense reasoning. Furthermore, this robustness varies depending on the language in which the LLM is tested. This work establishes a high-quality evaluation benchmark, with extensive experiments offering valuable insights to the community in commonsense reasoning for LLMs.

### RoleMRC: A Fine-Grained Composite Benchmark for Role-Playing and Instruction-Following 
[[arxiv](https://arxiv.org/abs/2502.11387)] [[cool](https://papers.cool/arxiv/2502.11387)] [[pdf](https://arxiv.org/pdf/2502.11387)]
> **Authors**: Junru Lu,Jiazheng Li,Guodong Shen,Lin Gui,Siyu An,Yulan He,Di Yin,Xing Sun
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Role-playing is important for Large Language Models (LLMs) to follow diverse instructions while maintaining role identity and the role's pre-defined ability limits. Existing role-playing datasets mostly contribute to controlling role style and knowledge boundaries, but overlook role-playing in instruction-following scenarios. We introduce a fine-grained role-playing and instruction-following composite benchmark, named RoleMRC, including: (1) Multi-turn dialogues between ideal roles and humans, including free chats or discussions upon given passages; (2) Role-playing machine reading comprehension, involving response, refusal, and attempts according to passage answerability and role ability; (3) More complex scenarios with nested, multi-turn and prioritized instructions. The final RoleMRC features a 10.2k role profile meta-pool, 37.9k well-synthesized role-playing instructions, and 1.4k testing samples. We develop a pipeline to quantitatively evaluate the fine-grained role-playing and instruction-following capabilities of several mainstream LLMs, as well as models that are fine-tuned on our data. Moreover, cross-evaluation on external role-playing datasets confirms that models fine-tuned on RoleMRC enhances instruction-following without compromising general role-playing and reasoning capabilities. We also probe the neural-level activation maps of different capabilities over post-tuned LLMs. Access to our RoleMRC, RoleMRC-mix and Codes: https://github.com/LuJunru/RoleMRC.

### Exploring the Small World of Word Embeddings: A Comparative Study on Conceptual Spaces from LLMs of Different Scales 
[[arxiv](https://arxiv.org/abs/2502.11380)] [[cool](https://papers.cool/arxiv/2502.11380)] [[pdf](https://arxiv.org/pdf/2502.11380)]
> **Authors**: Zhu Liu,Ying Liu,KangYang Luo,Cunliang Kong,Maosong Sun
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: Paper under review
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: A conceptual space represents concepts as nodes and semantic relatedness as edges. Word embeddings, combined with a similarity metric, provide an effective approach to constructing such a space. Typically, embeddings are derived from traditional distributed models or encoder-only pretrained models, whose objectives directly capture the meaning of the current token. In contrast, decoder-only models, including large language models (LLMs), predict the next token, making their embeddings less directly tied to the current token's semantics. Moreover, comparative studies on LLMs of different scales remain underexplored. In this paper, we construct a conceptual space using word embeddings from LLMs of varying scales and comparatively analyze their properties. We establish a network based on a linguistic typology-inspired connectivity hypothesis, examine global statistical properties, and compare LLMs of varying scales. Locally, we analyze conceptual pairs, WordNet relations, and a cross-lingual semantic network for qualitative words. Our results indicate that the constructed space exhibits small-world properties, characterized by a high clustering coefficient and short path lengths. Larger LLMs generate more intricate spaces, with longer paths reflecting richer relational structures and connections. Furthermore, the network serves as an efficient bridge for cross-lingual semantic mapping.

### LLMs can Perform Multi-Dimensional Analytic Writing Assessments: A Case Study of L2 Graduate-Level Academic English Writing 
[[arxiv](https://arxiv.org/abs/2502.11368)] [[cool](https://papers.cool/arxiv/2502.11368)] [[pdf](https://arxiv.org/pdf/2502.11368)]
> **Authors**: Zhengxiang Wang,Veronika Makarova,Zhi Li,Jordan Kodner,Owen Rambow
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 26 pages, 6 figures, 15 tables
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: The paper explores the performance of LLMs in the context of multi-dimensional analytic writing assessments, i.e. their ability to provide both scores and comments based on multiple assessment criteria. Using a corpus of literature reviews written by L2 graduate students and assessed by human experts against 9 analytic criteria, we prompt several popular LLMs to perform the same task under various conditions. To evaluate the quality of feedback comments, we apply a novel feedback comment quality evaluation framework. This framework is interpretable, cost-efficient, scalable, and reproducible, compared to existing methods that rely on manual judgments. We find that LLMs can generate reasonably good and generally reliable multi-dimensional analytic assessments. We release our corpus for reproducibility.

### Blessing of Multilinguality: A Systematic Analysis of Multilingual In-Context Learning 
[[arxiv](https://arxiv.org/abs/2502.11364)] [[cool](https://papers.cool/arxiv/2502.11364)] [[pdf](https://arxiv.org/pdf/2502.11364)]
> **Authors**: Yilei Tu,Andrew Xue,Freda Shi
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: While multilingual large language models generally perform adequately, and sometimes even rival English performance on high-resource languages (HRLs), they often significantly underperform on low-resource languages (LRLs). Among several prompting strategies aiming at bridging the gap, multilingual in-context learning (ICL) has been particularly effective when demonstration in target languages is unavailable. However, there lacks a systematic understanding of when and why it works well. In this work, we systematically analyze multilingual ICL, using demonstrations in HRLs to enhance cross-lingual transfer. We show that demonstrations in mixed HRLs consistently outperform English-only ones across the board, particularly for tasks written in LRLs. Surprisingly, our ablation study shows that the presence of irrelevant non-English sentences in the prompt yields measurable gains, suggesting the effectiveness of multilingual exposure itself. Our results highlight the potential of strategically leveraging multilingual resources to bridge the performance gap for underrepresented languages.

### VLDBench: Vision Language Models Disinformation Detection Benchmark 
[[arxiv](https://arxiv.org/abs/2502.11361)] [[cool](https://papers.cool/arxiv/2502.11361)] [[pdf](https://arxiv.org/pdf/2502.11361)]
> **Authors**: Shaina Raza,Ashmal Vayani,Aditya Jain,Aravind Narayanan,Vahid Reza Khazaie,Syed Raza Bashir,Elham Dolatabadi,Gias Uddin,Christos Emmanouilidis,Rizwan Qureshi,Mubarak Shah
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: under review
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The rapid rise of AI-generated content has made detecting disinformation increasingly challenging. In particular, multimodal disinformation, i.e., online posts-articles that contain images and texts with fabricated information are specially designed to deceive. While existing AI safety benchmarks primarily address bias and toxicity, multimodal disinformation detection remains largely underexplored. To address this challenge, we present the Vision-Language Disinformation Detection Benchmark VLDBench, the first comprehensive benchmark for detecting disinformation across both unimodal (text-only) and multimodal (text and image) content, comprising 31,000} news article-image pairs, spanning 13 distinct categories, for robust evaluation. VLDBench features a rigorous semi-automated data curation pipeline, with 22 domain experts dedicating 300 plus hours} to annotation, achieving a strong inter-annotator agreement (Cohen kappa = 0.78). We extensively evaluate state-of-the-art Large Language Models (LLMs) and Vision-Language Models (VLMs), demonstrating that integrating textual and visual cues in multimodal news posts improves disinformation detection accuracy by 5 - 35 % compared to unimodal models. Developed in alignment with AI governance frameworks such as the EU AI Act, NIST guidelines, and the MIT AI Risk Repository 2024, VLDBench is expected to become a benchmark for detecting disinformation in online multi-modal contents. Our code and data will be publicly available.

### "Nuclear Deployed!": Analyzing Catastrophic Risks in Decision-making of Autonomous LLM Agents 
[[arxiv](https://arxiv.org/abs/2502.11355)] [[cool](https://papers.cool/arxiv/2502.11355)] [[pdf](https://arxiv.org/pdf/2502.11355)]
> **Authors**: Rongwu Xu,Xiaojian Li,Shuo Chen,Wei Xu
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: Our code will be available at https://github.com/pillowsofwind/LLM-CBRN-Risks
- **标题**: None
- **领域**: 计算语言学,人工智能,密码学和安全,计算机与社会
- **Abstract**: Large language models (LLMs) are evolving into autonomous decision-makers, raising concerns about catastrophic risks in high-stakes scenarios, particularly in Chemical, Biological, Radiological and Nuclear (CBRN) domains. Based on the insight that such risks can originate from trade-offs between the agent's Helpful, Harmlessness and Honest (HHH) goals, we build a novel three-stage evaluation framework, which is carefully constructed to effectively and naturally expose such risks. We conduct 14,400 agentic simulations across 12 advanced LLMs, with extensive experiments and analysis. Results reveal that LLM agents can autonomously engage in catastrophic behaviors and deception, without being deliberately induced. Furthermore, stronger reasoning abilities often increase, rather than mitigate, these risks. We also show that these agents can violate instructions and superior commands. On the whole, we empirically prove the existence of catastrophic risks in autonomous LLM agents. We will release our code upon request.

### Hierarchical Graph Topic Modeling with Topic Tree-based Transformer 
[[arxiv](https://arxiv.org/abs/2502.11345)] [[cool](https://papers.cool/arxiv/2502.11345)] [[pdf](https://arxiv.org/pdf/2502.11345)]
> **Authors**: Delvin Ce Zhang,Menglin Yang,Xiaobao Wu,Jiasheng Zhang,Hady W. Lauw
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Textual documents are commonly connected in a hierarchical graph structure where a central document links to others with an exponentially growing connectivity. Though Hyperbolic Graph Neural Networks (HGNNs) excel at capturing such graph hierarchy, they cannot model the rich textual semantics within documents. Moreover, text contents in documents usually discuss topics of different specificity. Hierarchical Topic Models (HTMs) discover such latent topic hierarchy within text corpora. However, most of them focus on the textual content within documents, and ignore the graph adjacency across interlinked documents. We thus propose a Hierarchical Graph Topic Modeling Transformer to integrate both topic hierarchy within documents and graph hierarchy across documents into a unified Transformer. Specifically, to incorporate topic hierarchy within documents, we design a topic tree and infer a hierarchical tree embedding for hierarchical topic modeling. To preserve both topic and graph hierarchies, we design our model in hyperbolic space and propose Hyperbolic Doubly Recurrent Neural Network, which models ancestral and fraternal tree structure. Both hierarchies are inserted into each Transformer layer to learn unified representations. Both supervised and unsupervised experiments verify the effectiveness of our model.

### ExaGPT: Example-Based Machine-Generated Text Detection for Human Interpretability 
[[arxiv](https://arxiv.org/abs/2502.11336)] [[cool](https://papers.cool/arxiv/2502.11336)] [[pdf](https://arxiv.org/pdf/2502.11336)]
> **Authors**: Ryuto Koike,Masahiro Kaneko,Ayana Niwa,Preslav Nakov,Naoaki Okazaki
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: Under Review
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Detecting texts generated by Large Language Models (LLMs) could cause grave mistakes due to incorrect decisions, such as undermining student's academic dignity. LLM text detection thus needs to ensure the interpretability of the decision, which can help users judge how reliably correct its prediction is. When humans verify whether a text is human-written or LLM-generated, they intuitively investigate with which of them it shares more similar spans. However, existing interpretable detectors are not aligned with the human decision-making process and fail to offer evidence that users easily understand. To bridge this gap, we introduce ExaGPT, an interpretable detection approach grounded in the human decision-making process for verifying the origin of a text. ExaGPT identifies a text by checking whether it shares more similar spans with human-written vs. with LLM-generated texts from a datastore. This approach can provide similar span examples that contribute to the decision for each span in the text as evidence. Our human evaluation demonstrates that providing similar span examples contributes more effectively to judging the correctness of the decision than existing interpretable methods. Moreover, extensive experiments in four domains and three generators show that ExaGPT massively outperforms prior powerful detectors by up to +40.9 points of accuracy at a false positive rate of 1%.

### System Message Generation for User Preferences using Open-Source Models 
[[arxiv](https://arxiv.org/abs/2502.11330)] [[cool](https://papers.cool/arxiv/2502.11330)] [[pdf](https://arxiv.org/pdf/2502.11330)]
> **Authors**: Minbyul Jeong,Jungho Cho,Minsoo Khang,Dawoon Jung,Teakgyu Hong
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: System messages play a crucial role in interactions with large language models (LLMs), often serving as prompts to initiate conversations. Through system messages, users can assign specific roles, perform intended tasks, incorporate background information, specify various output formats and communication styles. Despite such versatility, publicly available data are often lack system messages and subject to strict license constraints in the industry field. Manual labeling of publicly available data with system messages that align with user instructions demands significant resources. In view of such challenges, our work introduces SysGen, a pipeline for generating system messages with better aligned assistant responses from the supervised fine-tuning dataset without system messages. Training on SysGen data has demonstrated substantial improvements in the alignment of model responses with system messages and user instructions, as demonstrated across various open-source models on the Multifacet benchmark, while maintaining minimal impact on other unseen benchmarks such as Open LLM Leaderboard 2. Our qualitative analysis highlights the importance of diverse system messages to ensure better adaptability across different contexts.

### Smoothing Out Hallucinations: Mitigating LLM Hallucination with Smoothed Knowledge Distillation 
[[arxiv](https://arxiv.org/abs/2502.11306)] [[cool](https://papers.cool/arxiv/2502.11306)] [[pdf](https://arxiv.org/pdf/2502.11306)]
> **Authors**: Hieu Nguyen,Zihao He,Shoumik Atul Gandre,Ujjwal Pasupulety,Sharanya Kumari Shivakumar,Kristina Lerman
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Large language models (LLMs) often suffer from hallucination, generating factually incorrect or ungrounded content, which limits their reliability in high-stakes applications. A key factor contributing to hallucination is the use of hard labels during training, which enforce deterministic supervision, encourage overconfidence, and disregard the uncertainty inherent in natural language. To address this, we propose mitigating hallucination through knowledge distillation (KD), where a teacher model provides smoothed soft labels to a student model, reducing overconfidence and improving factual grounding. We apply KD during supervised finetuning on instructional data, evaluating its effectiveness across LLMs from different families. Experimental results on summarization benchmarks demonstrate that KD reduces hallucination compared to standard finetuning while preserving performance on general NLP tasks. These findings highlight KD as a promising approach for mitigating hallucination in LLMs and improving model reliability.

### CORDIAL: Can Multimodal Large Language Models Effectively Understand Coherence Relationships? 
[[arxiv](https://arxiv.org/abs/2502.11300)] [[cool](https://papers.cool/arxiv/2502.11300)] [[pdf](https://arxiv.org/pdf/2502.11300)]
> **Authors**: Aashish Anantha Ramakrishnan,Aadarsh Anantha Ramakrishnan,Dongwon Lee
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: :I.2.7; I.2.10
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别
- **Abstract**: Multimodal Large Language Models (MLLMs) are renowned for their superior instruction-following and reasoning capabilities across diverse problem domains. However, existing benchmarks primarily focus on assessing factual and logical correctness in downstream tasks, with limited emphasis on evaluating MLLMs' ability to interpret pragmatic cues and intermodal relationships. To address this gap, we assess the competency of MLLMs in performing Multimodal Discourse Analysis (MDA) using Coherence Relations. Our benchmark, CORDIAL, encompasses a broad spectrum of Coherence Relations across 3 different discourse domains at varying levels of granularity. Through our experiments on 10+ MLLMs employing different prompting strategies, we show that even top models like Gemini 1.5 Pro and GPT-4o fail to match the performance of simple classifier-based baselines. This study emphasizes the need to move beyond similarity-based metrics and adopt a discourse-driven framework for evaluating MLLMs, providing a more nuanced assessment of their capabilities. The benchmark and code are available at: https://github.com/aashish2000/CORDIAL.

### The Rotary Position Embedding May Cause Dimension Inefficiency in Attention Heads for Long-Distance Retrieval 
[[arxiv](https://arxiv.org/abs/2502.11276)] [[cool](https://papers.cool/arxiv/2502.11276)] [[pdf](https://arxiv.org/pdf/2502.11276)]
> **Authors**: Ting-Rui Chiang,Dani Yogatama
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: The Rotary Position Embedding (RoPE) is widely used in the attention heads of many large language models (LLM). It rotates dimensions in the query and the key vectors by different angles according to their positions in the input sequence. For long context modeling, the range of positions may vary a lot, and thus RoPE rotates some dimensions by a great range of angles. We hypothesize that the wide range of rotation angles may prevent LLMs from utilizing those dimensions. To validate this hypothesis, we present a controlled experiment showing that applying RoPE causes low utility of certain dimensions. Our analyses on three LLMs also indicate that these dimensions do not help LLMs do long-context question answering.

### Cuckoo: An IE Free Rider Hatched by Massive Nutrition in LLM's Nest 
[[arxiv](https://arxiv.org/abs/2502.11275)] [[cool](https://papers.cool/arxiv/2502.11275)] [[pdf](https://arxiv.org/pdf/2502.11275)]
> **Authors**: Letian Peng,Zilong Wang,Feng Yao,Jingbo Shang
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Massive high-quality data, both pre-training raw texts and post-training annotations, have been carefully prepared to incubate advanced large language models (LLMs). In contrast, for information extraction (IE), pre-training data, such as BIO-tagged sequences, are hard to scale up. We show that IE models can act as free riders on LLM resources by reframing next-token \emph{prediction} into \emph{extraction} for tokens already present in the context. Specifically, our proposed next tokens extraction (NTE) paradigm learns a versatile IE model, \emph{Cuckoo}, with 102.6M extractive data converted from LLM's pre-training and post-training data. Under the few-shot setting, Cuckoo adapts effectively to traditional and complex instruction-following IE with better performance than existing pre-trained IE models. As a free rider, Cuckoo can naturally evolve with the ongoing advancements in LLM data preparation, benefiting from improvements in LLM training pipelines without additional manual effort.

### Improved Unbiased Watermark for Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.11268)] [[cool](https://papers.cool/arxiv/2502.11268)] [[pdf](https://arxiv.org/pdf/2502.11268)]
> **Authors**: Ruibo Chen,Yihan Wu,Junfeng Guo,Heng Huang
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: As artificial intelligence surpasses human capabilities in text generation, the necessity to authenticate the origins of AI-generated content has become paramount. Unbiased watermarks offer a powerful solution by embedding statistical signals into language model-generated text without distorting the quality. In this paper, we introduce MCmark, a family of unbiased, Multi-Channel-based watermarks. MCmark works by partitioning the model's vocabulary into segments and promoting token probabilities within a selected segment based on a watermark key. We demonstrate that MCmark not only preserves the original distribution of the language model but also offers significant improvements in detectability and robustness over existing unbiased watermarks. Our experiments with widely-used language models demonstrate an improvement in detectability of over 10% using MCmark, compared to existing state-of-the-art unbiased watermarks. This advancement underscores MCmark's potential in enhancing the practical application of watermarking in AI-generated texts.

### The Shrinking Landscape of Linguistic Diversity in the Age of Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.11266)] [[cool](https://papers.cool/arxiv/2502.11266)] [[pdf](https://arxiv.org/pdf/2502.11266)]
> **Authors**: Zhivar Sourati,Farzan Karimi-Malekabadi,Meltem Ozcan,Colin McDaniel,Alireza Ziabari,Jackson Trager,Ala Tak,Meng Chen,Fred Morstatter,Morteza Dehghani
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: arXiv admin note: text overlap with arXiv:2404.00267
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Language is far more than a communication tool. A wealth of information - including but not limited to the identities, psychological states, and social contexts of its users - can be gleaned through linguistic markers, and such insights are routinely leveraged across diverse fields ranging from product development and marketing to healthcare. In four studies utilizing experimental and observational methods, we demonstrate that the widespread adoption of large language models (LLMs) as writing assistants is linked to notable declines in linguistic diversity and may interfere with the societal and psychological insights language provides. We show that while the core content of texts is retained when LLMs polish and rewrite texts, not only do they homogenize writing styles, but they also alter stylistic elements in a way that selectively amplifies certain dominant characteristics or biases while suppressing others - emphasizing conformity over individuality. By varying LLMs, prompts, classifiers, and contexts, we show that these trends are robust and consistent. Our findings highlight a wide array of risks associated with linguistic homogenization, including compromised diagnostic processes and personalization efforts, the exacerbation of existing divides and barriers to equity in settings like personnel selection where language plays a critical role in assessing candidates' qualifications, communication skills, and cultural fit, and the undermining of efforts for cultural preservation.

### Leveraging Conditional Mutual Information to Improve Large Language Model Fine-Tuning For Classification 
[[arxiv](https://arxiv.org/abs/2502.11258)] [[cool](https://papers.cool/arxiv/2502.11258)] [[pdf](https://arxiv.org/pdf/2502.11258)]
> **Authors**: Thanushon Sivakaran,En-Hui Yang
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 6 pages, 2 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Although large language models (LLMs) have demonstrated remarkable capabilities in recent years, the potential of information theory (IT) to enhance LLM development remains underexplored. This paper introduces the information theoretic principle of Conditional Mutual Information (CMI) to LLM fine-tuning for classification tasks, exploring its promise in two main ways: minimizing CMI to improve a model's standalone performance and maximizing CMI to enhance knowledge distillation (KD) for more capable student models. To apply CMI in LLM fine-tuning, we adapt the recently proposed CMI-constrained deep learning framework, which was initially developed for image classification, with some modification. By minimizing CMI during LLM fine-tuning, we achieve superior performance gains on 6 of 8 GLUE classification tasks compared to BERT. Additionally, maximizing CMI during the KD process results in significant performance improvements in 6 of 8 GLUE classification tasks compared to DistilBERT. These findings demonstrate CMI's adaptability for optimizing both standalone LLMs and student models, showcasing its potential as a robust framework for advancing LLM fine-tuning. Our work bridges the gap between information theory and LLM development, offering new insights for building high-performing language models.

### Uncertainty-Aware Step-wise Verification with Generative Reward Models 
[[arxiv](https://arxiv.org/abs/2502.11250)] [[cool](https://papers.cool/arxiv/2502.11250)] [[pdf](https://arxiv.org/pdf/2502.11250)]
> **Authors**: Zihuiwen Ye,Luckeciano Carvalho Melo,Younesse Kaddar,Phil Blunsom,Sam Staton,Yarin Gal
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Complex multi-step reasoning tasks, such as solving mathematical problems, remain challenging for large language models (LLMs). While outcome supervision is commonly used, process supervision via process reward models (PRMs) provides intermediate rewards to verify step-wise correctness in solution traces. However, as proxies for human judgement, PRMs suffer from reliability issues, including susceptibility to reward hacking. In this work, we propose leveraging uncertainty quantification (UQ) to enhance the reliability of step-wise verification with generative reward models for mathematical reasoning tasks. We introduce CoT Entropy, a novel UQ method that outperforms existing approaches in quantifying a PRM's uncertainty in step-wise verification. Our results demonstrate that incorporating uncertainty estimates improves the robustness of judge-LM PRMs, leading to more reliable verification.

### Soteria: Language-Specific Functional Parameter Steering for Multilingual Safety Alignment 
[[arxiv](https://arxiv.org/abs/2502.11244)] [[cool](https://papers.cool/arxiv/2502.11244)] [[pdf](https://arxiv.org/pdf/2502.11244)]
> **Authors**: Somnath Banerjee,Sayan Layek,Pratyush Chatterjee,Animesh Mukherjee,Rima Hazra
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Ensuring consistent safety across multiple languages remains a significant challenge for large language models (LLMs). We introduce Soteria, a lightweight yet powerful strategy that locates and minimally adjusts the "functional heads" most responsible for harmful content generation in each language. By altering only a fraction of parameters, Soteria drastically reduces policy violations without sacrificing overall model performance, even in low-resource settings. To rigorously evaluate our approach, we also present XThreatBench, a specialized multilingual dataset capturing fine-grained harmful behaviors drawn from real policy guidelines. Experiments with leading open-source LLMs (e.g., Llama, Qwen, Mistral) show that Soteria consistently improves safety metrics across high-, mid-, and low-resource languages. These findings highlight a promising path toward scalable, linguistically attuned, and ethically aligned LLMs worldwide.

### Vendi-RAG: Adaptively Trading-Off Diversity And Quality Significantly Improves Retrieval Augmented Generation With LLMs 
[[arxiv](https://arxiv.org/abs/2502.11228)] [[cool](https://papers.cool/arxiv/2502.11228)] [[pdf](https://arxiv.org/pdf/2502.11228)]
> **Authors**: Mohammad Reza Rezaei,Adji Bousso Dieng
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: A RAG pipeline that accounts for both diversity and answer quality and that can be used with anyLLMbackbone to solve complex multi-hop question-answering tasks
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Retrieval-augmented generation (RAG) enhances large language models (LLMs) for domain-specific question-answering (QA) tasks by leveraging external knowledge sources. However, traditional RAG systems primarily focus on relevance-based retrieval and often struggle with redundancy, especially when reasoning requires connecting information from multiple sources. This paper introduces Vendi-RAG, a framework based on an iterative process that jointly optimizes retrieval diversity and answer quality. This joint optimization leads to significantly higher accuracy for multi-hop QA tasks. Vendi-RAG leverages the Vendi Score (VS), a flexible similarity-based diversity metric, to promote semantic diversity in document retrieval. It then uses an LLM judge that evaluates candidate answers, generated after a reasoning step, and outputs a score that the retriever uses to balance relevance and diversity among the retrieved documents during each iteration. Experiments on three challenging datasets -- HotpotQA, MuSiQue, and 2WikiMultiHopQA -- demonstrate Vendi-RAG's effectiveness in multi-hop reasoning tasks. The framework achieves significant accuracy improvements over traditional single-step and multi-step RAG approaches, with accuracy increases reaching up to +4.2% on HotpotQA, +4.1% on 2WikiMultiHopQA, and +1.3% on MuSiQue compared to Adaptive-RAG, the current best baseline. The benefits of Vendi-RAG are even more pronounced as the number of retrieved documents increases. Finally, we evaluated Vendi-RAG across different LLM backbones, including GPT-3.5, GPT-4, and GPT-4o-mini, and observed consistent improvements, demonstrating that the framework's advantages are model-agnostic.

### Asymmetric Conflict and Synergy in Post-training for LLM-based Multilingual Machine Translation 
[[arxiv](https://arxiv.org/abs/2502.11223)] [[cool](https://papers.cool/arxiv/2502.11223)] [[pdf](https://arxiv.org/pdf/2502.11223)]
> **Authors**: Tong Zheng,Yan Wen,Huiwen Bao,Junfeng Guo,Heng Huang
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 22 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The emergence of Large Language Models (LLMs) has advanced the multilingual machine translation (MMT), yet the Curse of Multilinguality (CoM) remains a major challenge. Existing work in LLM-based MMT typically mitigates this issue via scaling up training and computation budget, which raises a critical question: Is scaling up the training and computation budget truly necessary for high-quality MMT, or can a deeper understanding of CoM provide a more efficient solution? To explore this problem, we analyze the linguistic conflicts and synergy, the underlying mechanism of CoM during post-training phase. We identify an asymmetric phenomenon in linguistic conflicts and synergy: the dominance of conflicts and synergy varies in different translation directions, leading to sub-optimal adaptation in existing post-training methods. We further find that a significant bottleneck in MMT appears to lie in post-training rather than multilingual pre-training, suggesting the need for more effective adaptation strategies. Building on these new insights, we propose a direction-aware training approach, combined with group-wise model merging, to address asymmetry in linguistic conflicts and synergy explicitly. Leveraging this strategy, our method fine-tunes X-ALMA-13B-Pretrain-trained only with multilingual pre-training-achieving comparable performance to XALMA-13B (only SFT) while using only 20B pretraining tokens and 17B parameters-5.5x fewer pretraining-tokens and 1.7x fewer model size-with just 0.85 COMET drop on Flores-200 testsets of 50 languages.

### A Survey of LLM-based Agents in Medicine: How far are we from Baymax? 
[[arxiv](https://arxiv.org/abs/2502.11211)] [[cool](https://papers.cool/arxiv/2502.11211)] [[pdf](https://arxiv.org/pdf/2502.11211)]
> **Authors**: Wenxuan Wang,Zizhan Ma,Zheng Wang,Chenghan Wu,Wenting Chen,Xiang Li,Yixuan Yuan
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别
- **Abstract**: Large Language Models (LLMs) are transforming healthcare through the development of LLM-based agents that can understand, reason about, and assist with medical tasks. This survey provides a comprehensive review of LLM-based agents in medicine, examining their architectures, applications, and challenges. We analyze the key components of medical agent systems, including system profiles, clinical planning mechanisms, medical reasoning frameworks, and external capacity enhancement. The survey covers major application scenarios such as clinical decision support, medical documentation, training simulations, and healthcare service optimization. We discuss evaluation frameworks and metrics used to assess these agents' performance in healthcare settings. While LLM-based agents show promise in enhancing healthcare delivery, several challenges remain, including hallucination management, multimodal integration, implementation barriers, and ethical considerations. The survey concludes by highlighting future research directions, including advances in medical reasoning inspired by recent developments in LLM architectures, integration with physical systems, and improvements in training simulations. This work provides researchers and practitioners with a structured overview of the current state and future prospects of LLM-based agents in medicine.

### ANCHOLIK-NER: A Benchmark Dataset for Bangla Regional Named Entity Recognition 
[[arxiv](https://arxiv.org/abs/2502.11198)] [[cool](https://papers.cool/arxiv/2502.11198)] [[pdf](https://arxiv.org/pdf/2502.11198)]
> **Authors**: Bidyarthi Paul,Faika Fairuj Preotee,Shuvashis Sarker,Shamim Rahim Refat,Shifat Islam,Tashreef Muhammad,Mohammad Ashraful Hoque,Shahriar Manzoor
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: ANCHOLIK-NER is a linguistically diverse dataset for Named Entity Recognition (NER) in Bangla regional dialects, capturing variations across Sylhet, Chittagong, and Barishal. The dataset has around 10,443 sentences, 3,481 sentences per region. The data was collected from two publicly available datasets and through web scraping from various online newspapers, articles. To ensure high-quality annotations, the BIO tagging scheme was employed, and professional annotators with expertise in regional dialects carried out the labeling process. The dataset is structured into separate subsets for each region and is available both in CSV format. Each entry contains textual data along with identified named entities and their corresponding annotations. Named entities are categorized into ten distinct classes: Person, Location, Organization, Food, Animal, Colour, Role, Relation, Object, and Miscellaneous. This dataset serves as a valuable resource for developing and evaluating NER models for Bangla dialectal variations, contributing to regional language processing and low-resource NLP applications. It can be utilized to enhance NER systems in Bangla dialects, improve regional language understanding, and support applications in machine translation, information retrieval, and conversational AI.

### Large Language Models Penetration in Scholarly Writing and Peer Review 
[[arxiv](https://arxiv.org/abs/2502.11193)] [[cool](https://papers.cool/arxiv/2502.11193)] [[pdf](https://arxiv.org/pdf/2502.11193)]
> **Authors**: Li Zhou,Ruijie Zhang,Xunlian Dai,Daniel Hershcovich,Haizhou Li
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: Transparency in NLP,LLM-generated text evaluation and detection,LLMPenetration, Scholarly Credibility and Accountability
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: While the widespread use of Large Language Models (LLMs) brings convenience, it also raises concerns about the credibility of academic research and scholarly processes. To better understand these dynamics, we evaluate the penetration of LLMs across academic workflows from multiple perspectives and dimensions, providing compelling evidence of their growing influence. We propose a framework with two components: \texttt{ScholarLens}, a curated dataset of human- and LLM-generated content across scholarly writing and peer review for multi-perspective evaluation, and \texttt{LLMetrica}, a tool for assessing LLM penetration using rule-based metrics and model-based detectors for multi-dimensional evaluation. Our experiments demonstrate the effectiveness of \texttt{LLMetrica}, revealing the increasing role of LLMs in scholarly processes. These findings emphasize the need for transparency, accountability, and ethical practices in LLM usage to maintain academic credibility.

### ReLearn: Unlearning via Learning for Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.11190)] [[cool](https://papers.cool/arxiv/2502.11190)] [[pdf](https://arxiv.org/pdf/2502.11190)]
> **Authors**: Haoming Xu,Ningyuan Zhao,Liming Yang,Sendong Zhao,Shumin Deng,Mengru Wang,Bryan Hooi,Nay Oo,Huajun Chen,Ningyu Zhang
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: Work in progress
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别,人机交互,机器学习
- **Abstract**: Current unlearning methods for large language models usually rely on reverse optimization to reduce target token probabilities. However, this paradigm disrupts the subsequent tokens prediction, degrading model performance and linguistic coherence. Moreover, existing evaluation metrics overemphasize contextual forgetting while inadequately assessing response fluency and relevance. To address these challenges, we propose ReLearn, a data augmentation and fine-tuning pipeline for effective unlearning, along with a comprehensive evaluation framework. This framework introduces Knowledge Forgetting Rate (KFR) and Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and Linguistic Score (LS) to evaluate generation quality. Our experiments show that ReLearn successfully achieves targeted forgetting while preserving high-quality output. Through mechanistic analysis, we further demonstrate how reverse optimization disrupts coherent text generation, while ReLearn preserves this essential capability. Code is available at https://github.com/zjunlp/unlearn.

### TituLLMs: A Family of Bangla LLMs with Comprehensive Benchmarking 
[[arxiv](https://arxiv.org/abs/2502.11187)] [[cool](https://papers.cool/arxiv/2502.11187)] [[pdf](https://arxiv.org/pdf/2502.11187)]
> **Authors**: Shahriar Kabir Nahin,Rabindra Nath Nandi,Sagor Sarker,Quazi Sarwar Muhtaseem,Md Kowsher,Apu Chandraw Shill,Md Ibrahim,Mehadi Hasan Menon,Tareq Al Muntasir,Firoj Alam
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: LLMs, Benchmarking, LargeLanguageModels, Bangla
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: In this paper, we present TituLLMs, the first large pretrained Bangla LLMs, available in 1b and 3b parameter sizes. Due to computational constraints during both training and inference, we focused on smaller models. To train TituLLMs, we collected a pretraining dataset of approximately ~37 billion tokens. We extended the Llama-3.2 tokenizer to incorporate language- and culture-specific knowledge, which also enables faster training and inference. There was a lack of benchmarking datasets to benchmark LLMs for Bangla. To address this gap, we developed five benchmarking datasets. We benchmarked various LLMs, including TituLLMs, and demonstrated that TituLLMs outperforms its initial multilingual versions. However, this is not always the case, highlighting the complexities of language adaptation. Our work lays the groundwork for adapting existing multilingual open models to other low-resource languages. To facilitate broader adoption and further research, we have made the TituLLMs models and benchmarking datasets publicly available (https://huggingface.co/collections/hishab/titulm-llama-family-6718d31fc1b83529276f490a).

### Can't See the Forest for the Trees: Benchmarking Multimodal Safety Awareness for Multimodal LLMs 
[[arxiv](https://arxiv.org/abs/2502.11184)] [[cool](https://papers.cool/arxiv/2502.11184)] [[pdf](https://arxiv.org/pdf/2502.11184)]
> **Authors**: Wenxuan Wang,Xiaoyuan Liu,Kuiyi Gao,Jen-tse Huang,Youliang Yuan,Pinjia He,Shuai Wang,Zhaopeng Tu
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别,多媒体
- **Abstract**: Multimodal Large Language Models (MLLMs) have expanded the capabilities of traditional language models by enabling interaction through both text and images. However, ensuring the safety of these models remains a significant challenge, particularly in accurately identifying whether multimodal content is safe or unsafe-a capability we term safety awareness. In this paper, we introduce MMSafeAware, the first comprehensive multimodal safety awareness benchmark designed to evaluate MLLMs across 29 safety scenarios with 1500 carefully curated image-prompt pairs. MMSafeAware includes both unsafe and over-safety subsets to assess models abilities to correctly identify unsafe content and avoid over-sensitivity that can hinder helpfulness. Evaluating nine widely used MLLMs using MMSafeAware reveals that current models are not sufficiently safe and often overly sensitive; for example, GPT-4V misclassifies 36.1% of unsafe inputs as safe and 59.9% of benign inputs as unsafe. We further explore three methods to improve safety awareness-prompting-based approaches, visual contrastive decoding, and vision-centric reasoning fine-tuning-but find that none achieve satisfactory performance. Our findings highlight the profound challenges in developing MLLMs with robust safety awareness, underscoring the need for further research in this area. All the code and data will be publicly available to facilitate future research.

### Don't Get Lost in the Trees: Streamlining LLM Reasoning by Overcoming Tree Search Exploration Pitfalls 
[[arxiv](https://arxiv.org/abs/2502.11183)] [[cool](https://papers.cool/arxiv/2502.11183)] [[pdf](https://arxiv.org/pdf/2502.11183)]
> **Authors**: Ante Wang,Linfeng Song,Ye Tian,Dian Yu,Haitao Mi,Xiangyu Duan,Zhaopeng Tu,Jinsong Su,Dong Yu
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Recent advancements in tree search algorithms guided by verifiers have significantly enhanced the reasoning capabilities of large language models (LLMs), but at the cost of increased computational resources. In this work, we identify two key challenges contributing to this inefficiency: $\textit{over-exploration}$ due to redundant states with semantically equivalent content, and $\textit{under-exploration}$ caused by high variance in verifier scoring leading to frequent trajectory switching. To address these issues, we propose FETCH, an e$\textbf{f}$fici$\textbf{e}$nt $\textbf{t}$ree sear$\textbf{ch}$ framework, which is a flexible, plug-and-play system compatible with various tree search algorithms. Our framework mitigates over-exploration by merging semantically similar states using agglomerative clustering of text embeddings obtained from a fine-tuned SimCSE model. To tackle under-exploration, we enhance verifiers by incorporating temporal difference learning with adjusted $λ$-returns during training to reduce variance, and employing a verifier ensemble to aggregate scores during inference. Experiments on GSM8K, GSM-Plus, and MATH datasets demonstrate that our methods significantly improve reasoning accuracy and computational efficiency across four different tree search algorithms, paving the way for more practical applications of LLM-based reasoning. The code will be released upon acceptance.

### The Mirage of Model Editing: Revisiting Evaluation in the Wild 
[[arxiv](https://arxiv.org/abs/2502.11177)] [[cool](https://papers.cool/arxiv/2502.11177)] [[pdf](https://arxiv.org/pdf/2502.11177)]
> **Authors**: Wanli Yang,Fei Sun,Jiajun Tan,Xinyu Ma,Qi Cao,Dawei Yin,Huawei Shen,Xueqi Cheng
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Despite near-perfect results in artificial evaluations, the effectiveness of model editing in real-world applications remains unexplored. To bridge this gap, we propose to study model editing in question answering (QA) by establishing a rigorous evaluation practice to assess the effectiveness of editing methods in correcting LLMs' errors. It consists of QAEdit, a new benchmark derived from popular QA datasets, and a standardized evaluation framework. Our single editing experiments indicate that current editing methods perform substantially worse than previously reported (38.5% vs. ~96%). Through module analysis and controlled experiments, we demonstrate that this performance decline stems from issues in evaluation practices of prior editing research. One key issue is the inappropriate use of teacher forcing in testing prevents error propagation by feeding ground truth tokens (inaccessible in real-world scenarios) as input. Furthermore, we simulate real-world deployment by sequential editing, revealing that current approaches fail drastically with only 1000 edits. Our analysis provides a fundamental reexamination of both the real-world applicability of existing model editing methods and their evaluation practices, and establishes a rigorous evaluation framework with key insights to advance reliable and practical model editing research.

### LogiDynamics: Unraveling the Dynamics of Logical Inference in Large Language Model Reasoning 
[[arxiv](https://arxiv.org/abs/2502.11176)] [[cool](https://papers.cool/arxiv/2502.11176)] [[pdf](https://arxiv.org/pdf/2502.11176)]
> **Authors**: Tianshi Zheng,Jiayang Cheng,Chunyang Li,Haochen Shi,Zihao Wang,Jiaxin Bai,Yangqiu Song,Ginny Y. Wong,Simon See
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 21 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Modern large language models (LLMs) employ various forms of logical inference, both implicitly and explicitly, when addressing reasoning tasks. Understanding how to optimally leverage these inference paradigms is critical for advancing LLMs' reasoning capabilities. This paper adopts an exploratory approach by introducing a controlled evaluation environment for analogical reasoning -- a fundamental cognitive task -- that is systematically parameterized across three dimensions: modality (textual, visual, symbolic), difficulty (easy, medium, hard), and task format (multiple-choice or free-text generation). We analyze the comparative dynamics of inductive, abductive, and deductive inference pipelines across these dimensions, and demonstrate that our findings generalize to broader in-context learning tasks. Additionally, we investigate advanced paradigms such as hypothesis selection, verification, and refinement, revealing their potential to scale up logical inference in LLM reasoning. This exploratory study provides a foundation for future research in enhancing LLM reasoning through systematic logical inference strategies.

### Investigating Language Preference of Multilingual RAG Systems 
[[arxiv](https://arxiv.org/abs/2502.11175)] [[cool](https://papers.cool/arxiv/2502.11175)] [[pdf](https://arxiv.org/pdf/2502.11175)]
> **Authors**: Jeonghyun Park,Hwanhee Lee
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 30 pages, 16 tables, 14 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Multilingual Retrieval-Augmented Generation (mRAG) systems enhance language models by integrating external multilingual information to produce context-aware responses. However, mRAG systems struggle with retrieving relevant information due to linguistic variations between queries and documents, generating inconsistent responses when multilingual sources conflict. In this work, we systematically investigate language preferences in both retrieval and generation of mRAG through a series of experiments. Our analysis indicates that retrievers tend to prefer high-resource and query languages, yet this preference does not consistently improve generation performance. Moreover, we observe that generators prefer the query language or Latin scripts, leading to inconsistent outputs. To overcome these issues, we propose Dual Knowledge Multilingual RAG (DKM-RAG), a simple yet effective framework that fuses translated multilingual passages with complementary model knowledge. Empirical results demonstrate that DKM-RAG mitigates language preference in generation and enhances performance across diverse linguistic settings.

### Leveraging Constrained Monte Carlo Tree Search to Generate Reliable Long Chain-of-Thought for Mathematical Reasoning 
[[arxiv](https://arxiv.org/abs/2502.11169)] [[cool](https://papers.cool/arxiv/2502.11169)] [[pdf](https://arxiv.org/pdf/2502.11169)]
> **Authors**: Qingwen Lin,Boyan Xu,Zijian Li,Zhifeng Hao,Keli Zhang,Ruichu Cai
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Recently, Long Chain-of-Thoughts (CoTs) have gained widespread attention for improving the reasoning capabilities of Large Language Models (LLMs). This necessitates that existing LLMs, which lack the ability to generate Long CoTs, to acquire such capability through post-training methods. Without additional training, LLMs typically enhance their mathematical reasoning abilities through inference scaling methods such as MCTS. However, they are hindered by the large action space and inefficient search strategies, making it challenging to generate Long CoTs effectively. To tackle this issue, we propose constraining the action space and guiding the emergence of Long CoTs through a refined search strategy. In our proposed Constrained Monte Carlo Tree Search (C-MCTS) framework, we limit the actions selected from a constrained action space, which is divided into five disjoint subsets: \emph{understanding}, \emph{planning}, \emph{reflection}, \emph{coding}, and \emph{summary}. Each subset is further constrained to a small number of predefined prompts, rather than allowing LLMs to generate actions arbitrarily. Additionally, we refine the search strategy by incorporating prior knowledge about the action sets, such as a human-like partial order of the action subsets and the pretrained process reward models. These strategies work together to significantly reduce the vast search space of Long CoTs. Extensive evaluations on mathematical reasoning benchmarks show that, under zero-shot settings, our method enables the 7B model to achieve reasoning capabilities that surpass those of the 72B model.

### Safety Evaluation of DeepSeek Models in Chinese Contexts 
[[arxiv](https://arxiv.org/abs/2502.11137)] [[cool](https://papers.cool/arxiv/2502.11137)] [[pdf](https://arxiv.org/pdf/2502.11137)]
> **Authors**: Wenjing Zhang,Xuejiao Lei,Zhaoxiang Liu,Ning Wang,Zhenhong Long,Peijun Yang,Jiaojiao Zhao,Minjie Hua,Chaoyang Ma,Kai Wang,Shiguo Lian
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 12 pages, 2 tables, 7 figures
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Recently, the DeepSeek series of models, leveraging their exceptional reasoning capabilities and open-source strategy, is reshaping the global AI landscape. Despite these advantages, they exhibit significant safety deficiencies. Research conducted by Robust Intelligence, a subsidiary of Cisco, in collaboration with the University of Pennsylvania, revealed that DeepSeek-R1 has a 100\% attack success rate when processing harmful prompts. Additionally, multiple safety companies and research institutions have confirmed critical safety vulnerabilities in this model. As models demonstrating robust performance in Chinese and English, DeepSeek models require equally crucial safety assessments in both language contexts. However, current research has predominantly focused on safety evaluations in English environments, leaving a gap in comprehensive assessments of their safety performance in Chinese contexts. In response to this gap, this study introduces CHiSafetyBench, a Chinese-specific safety evaluation benchmark. This benchmark systematically evaluates the safety of DeepSeek-R1 and DeepSeek-V3 in Chinese contexts, revealing their performance across safety categories. The experimental results quantify the deficiencies of these two models in Chinese contexts, providing key insights for subsequent improvements. It should be noted that, despite our efforts to establish a comprehensive, objective, and authoritative evaluation benchmark, the selection of test samples, characteristics of data distribution, and the setting of evaluation criteria may inevitably introduce certain biases into the evaluation results. We will continuously optimize the evaluation benchmark and periodically update this report to provide more comprehensive and accurate assessment outcomes. Please refer to the latest version of the paper for the most recent evaluation results and conclusions.

### Improving Similar Case Retrieval Ranking Performance By Revisiting RankSVM 
[[arxiv](https://arxiv.org/abs/2502.11131)] [[cool](https://papers.cool/arxiv/2502.11131)] [[pdf](https://arxiv.org/pdf/2502.11131)]
> **Authors**: Yuqi Liu,Yan Zheng
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Given the rapid development of Legal AI, a lot of attention has been paid to one of the most important legal AI tasks--similar case retrieval, especially with language models to use. In our paper, however, we try to improve the ranking performance of current models from the perspective of learning to rank instead of language models. Specifically, we conduct experiments using a pairwise method--RankSVM as the classifier to substitute a fully connected layer, combined with commonly used language models on similar case retrieval datasets LeCaRDv1 and LeCaRDv2. We finally come to the conclusion that RankSVM could generally help improve the retrieval performance on the LeCaRDv1 and LeCaRDv2 datasets compared with original classifiers by optimizing the precise ranking. It could also help mitigate overfitting owing to class imbalance. Our code is available in https://github.com/liuyuqi123study/RankSVM_for_SLR

### FELLE: Autoregressive Speech Synthesis with Token-Wise Coarse-to-Fine Flow Matching 
[[arxiv](https://arxiv.org/abs/2502.11128)] [[cool](https://papers.cool/arxiv/2502.11128)] [[pdf](https://arxiv.org/pdf/2502.11128)]
> **Authors**: Hui Wang,Shujie Liu,Lingwei Meng,Jinyu Li,Yifan Yang,Shiwan Zhao,Haiyang Sun,Yanqing Liu,Haoqin Sun,Jiaming Zhou,Yan Lu,Yong Qin
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,声音,音频和语音处理
- **Abstract**: To advance continuous-valued token modeling and temporal-coherence enforcement, we propose FELLE, an autoregressive model that integrates language modeling with token-wise flow matching. By leveraging the autoregressive nature of language models and the generative efficacy of flow matching, FELLE effectively predicts continuous-valued tokens (mel-spectrograms). For each continuous-valued token, FELLE modifies the general prior distribution in flow matching by incorporating information from the previous step, improving coherence and stability. Furthermore, to enhance synthesis quality, FELLE introduces a coarse-to-fine flow-matching mechanism, generating continuous-valued tokens hierarchically, conditioned on the language model's output. Experimental results demonstrate the potential of incorporating flow-matching techniques in autoregressive mel-spectrogram modeling, leading to significant improvements in TTS generation quality, as shown in https://aka.ms/felle.

### DuplexMamba: Enhancing Real-time Speech Conversations with Duplex and Streaming Capabilities 
[[arxiv](https://arxiv.org/abs/2502.11123)] [[cool](https://papers.cool/arxiv/2502.11123)] [[pdf](https://arxiv.org/pdf/2502.11123)]
> **Authors**: Xiangyu Lu,Wang Xu,Haoyu Wang,Hongyun Zhou,Haiyan Zhao,Conghui Zhu,Tiejun Zhao,Muyun Yang
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Real-time speech conversation is essential for natural and efficient human-machine interactions, requiring duplex and streaming capabilities. Traditional Transformer-based conversational chatbots operate in a turn-based manner and exhibit quadratic computational complexity that grows as the input size increases. In this paper, we propose DuplexMamba, a Mamba-based end-to-end multimodal duplex model for speech-to-text conversation. DuplexMamba enables simultaneous input processing and output generation, dynamically adjusting to support real-time streaming. Specifically, we develop a Mamba-based speech encoder and adapt it with a Mamba-based language model. Furthermore, we introduce a novel duplex decoding strategy that enables DuplexMamba to process input and generate output simultaneously. Experimental results demonstrate that DuplexMamba successfully implements duplex and streaming capabilities while achieving performance comparable to several recently developed Transformer-based models in automatic speech recognition (ASR) tasks and voice assistant benchmark evaluations.

### Gumbel Reranking: Differentiable End-to-End Reranker Optimization 
[[arxiv](https://arxiv.org/abs/2502.11116)] [[cool](https://papers.cool/arxiv/2502.11116)] [[pdf](https://arxiv.org/pdf/2502.11116)]
> **Authors**: Siyuan Huang,Zhiyuan Ma,Jintao Du,Changhua Meng,Weiqiang Wang,Jingwen Leng,Minyi Guo,Zhouhan Lin
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,信息检索
- **Abstract**: RAG systems rely on rerankers to identify relevant documents. However, fine-tuning these models remains challenging due to the scarcity of annotated query-document pairs. Existing distillation-based approaches suffer from training-inference misalignment and fail to capture interdependencies among candidate documents. To overcome these limitations, we reframe the reranking process as an attention-mask problem and propose Gumbel Reranking, an end-to-end training framework for rerankers aimed at minimizing the training-inference gap. In our approach, reranker optimization is reformulated as learning a stochastic, document-wise Top-$k$ attention mask using the Gumbel Trick and Relaxed Top-$k$ Sampling. This formulation enables end-to-end optimization by minimizing the overall language loss. Experiments across various settings consistently demonstrate performance gains, including a 10.4\% improvement in recall on HotpotQA for distinguishing indirectly relevant documents.

### Are Generative Models Underconfident? An Embarrassingly Simple Quality Estimation Approach 
[[arxiv](https://arxiv.org/abs/2502.11115)] [[cool](https://papers.cool/arxiv/2502.11115)] [[pdf](https://arxiv.org/pdf/2502.11115)]
> **Authors**: Tu Anh Dinh,Jan Niehues
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: :I.2.7
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Quality Estimation (QE) is estimating the quality of model output when the ground truth reference is not available. Looking at model uncertainty from its own output probabilities is the most trivial and low-effort way to estimate the output quality. However, for generative model, output probabilities might not be the best quality estimator. At an output step, there can be multiple correct options, making the probability distribution spread out more. Thus, lower token probability does not necessarily mean lower output quality. In other words, the model can be considered underconfident. In this paper, we propose a QE approach called Dominant Mass Probability (DMP}, that boosts the model confidence in cases where there are multiple viable output options. We show that, with no increase in complexity, DMP is notably better than sequence probability when estimating the quality of different models (Whisper, Llama, etc.) on different tasks (translation, summarization, etc.). Compared to sequence probability, DMP achieves on average +0.208 improvement in Pearson correlation to ground-truth quality.

### Beyond Pairwise: Global Zero-shot Temporal Graph Generation 
[[arxiv](https://arxiv.org/abs/2502.11114)] [[cool](https://papers.cool/arxiv/2502.11114)] [[pdf](https://arxiv.org/pdf/2502.11114)]
> **Authors**: Alon Eirew,Kfir Bar,Ido Dagan
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Temporal relation extraction (TRE) is a fundamental task in natural language processing (NLP) that involves identifying the temporal relationships between events in a document. Despite the advances in large language models (LLMs), their application to TRE remains limited. Most existing approaches rely on pairwise classification, in which event pairs are considered individually, leading to computational inefficiency and a lack of global consistency in the resulting temporal graph. In this work, we propose a novel zero-shot method for TRE that generates a document's complete temporal graph at once, then applies transitive constraints optimization to refine predictions and enforce temporal consistency across relations. Additionally, we introduce OmniTemp, a new dataset with complete annotations for all pairs of targeted events within a document. Through experiments and analyses, we demonstrate that our method significantly outperforms existing zero-shot approaches while achieving competitive performance with supervised models.

### Valuable Hallucinations: Realizable Non-realistic Propositions 
[[arxiv](https://arxiv.org/abs/2502.11113)] [[cool](https://papers.cool/arxiv/2502.11113)] [[pdf](https://arxiv.org/pdf/2502.11113)]
> **Authors**: Qiucheng Chen,Bo Wang
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 13 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: This paper introduces the first formal definition of valuable hallucinations in large language models (LLMs), addressing a gap in the existing literature. We provide a systematic definition and analysis of hallucination value, proposing methods for enhancing the value of hallucinations. In contrast to previous works, which often treat hallucinations as a broad flaw, we focus on the potential value that certain types of hallucinations can offer in specific contexts. Hallucinations in LLMs generally refer to the generation of unfaithful, fabricated, inconsistent, or nonsensical content. Rather than viewing all hallucinations negatively, this paper gives formal representations and manual judgments of "valuable hallucinations" and explores how realizable non-realistic propositions--ideas that are not currently true but could be achievable under certain conditions--can have constructive value. We present experiments using the Qwen2.5 model and HalluQA dataset, employing ReAct prompting (which involves reasoning, confidence assessment, and answer verification) to control and optimize hallucinations. Our findings show that ReAct prompting results in a 5.12\% reduction in overall hallucinations and an increase in the proportion of valuable hallucinations from 6.45\% to 7.92\%. These results demonstrate that systematically controlling hallucinations can improve their usefulness without compromising factual reliability.

### Knowledge Graph-Driven Retrieval-Augmented Generation: Integrating Deepseek-R1 with Weaviate for Advanced Chatbot Applications 
[[arxiv](https://arxiv.org/abs/2502.11108)] [[cool](https://papers.cool/arxiv/2502.11108)] [[pdf](https://arxiv.org/pdf/2502.11108)]
> **Authors**: Alexandru Lecu,Adrian Groza,Lezan Hawizy
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large language models (LLMs) have significantly advanced the field of natural language generation. However, they frequently generate unverified outputs, which compromises their reliability in critical applications. In this study, we propose an innovative framework that combines structured biomedical knowledge with LLMs through a retrieval-augmented generation technique. Our system develops a thorough knowledge graph by identifying and refining causal relationships and named entities from medical abstracts related to age-related macular degeneration (AMD). Using a vector-based retrieval process and a locally deployed language model, our framework produces responses that are both contextually relevant and verifiable, with direct references to clinical evidence. Experimental results show that this method notably decreases hallucinations, enhances factual precision, and improves the clarity of generated responses, providing a robust solution for advanced biomedical chatbot applications.

### Enhancing Cross-Tokenizer Knowledge Distillation with Contextual Dynamical Mapping 
[[arxiv](https://arxiv.org/abs/2502.11104)] [[cool](https://papers.cool/arxiv/2502.11104)] [[pdf](https://arxiv.org/pdf/2502.11104)]
> **Authors**: Yijie Chen,Yijin Liu,Fandong Meng,Yufeng Chen,Jinan Xu,Jie Zhou
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: The code is available at https://github.com/pppa2019/ContexualDynamicMapping
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Knowledge Distillation (KD) has emerged as a prominent technique for model compression. However, conventional KD approaches primarily focus on homogeneous architectures with identical tokenizers, constraining their applicability in cross-architecture scenarios. As for the cross-tokenizer KD, the differences in the tokenizers give rise to two fundamental challenges: (1) sequence misalignment caused by divergent tokenization strategies, and (2) mismatched vocabulary size and composition. While existing probability-matching methods attempt to address these issues, their efficacy remains limited due to suboptimal alignment in both the sequence and vocabulary aspects. To overcome these limitations, we propose Contextual Dynamic Mapping (CDM), a novel cross-tokenizer distillation framework that employs contextual information to enhance sequence alignment precision and dynamically improves vocabulary mapping. We evaluated the effectiveness of our approach across five advanced and widely-used model families (i.e, LLama3, Phi3, Gemma2, OPT and Qwen2), which were configured into three distinct teacher-student pairs. Our method shows significant advantages over existing cross-tokenizer distillation baselines across diverse benchmarks, including instruction-following, code generation and math. Notably, our analysis reveals that combining conventional same-tokenizer distillation and cross-tokenizer distillation through CDM yields further performance improvements. The code is available at https://github.com/pppa2019/ContexualDynamicMapping

### CacheFocus: Dynamic Cache Re-Positioning for Efficient Retrieval-Augmented Generation 
[[arxiv](https://arxiv.org/abs/2502.11101)] [[cool](https://papers.cool/arxiv/2502.11101)] [[pdf](https://arxiv.org/pdf/2502.11101)]
> **Authors**: Kun-Hui Lee,Eunhwan Park,Donghoon Han,Seung-Hoon Na
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 11 pages (Work in progress)
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large Language Models (LLMs) excel across a variety of language tasks yet are constrained by limited input lengths and high computational costs. Existing approaches\textemdash such as relative positional encodings (e.g., RoPE, ALiBi) and sliding window mechanisms\textemdash partially alleviate these issues but often require additional training or suffer from performance degradation with longer inputs. In this paper, we introduce \textbf{\textit{CacheFocus}}, a method that enhances length normalization and reduces inference latency without any further training. Our approach leverages query-independent, offline caching to efficiently reuse a Context KV Cache Store. We address the amplification of abnormal token distributions problem by re-positioning cached keys and introducing Layer-Adaptive Cache Pruning to discard low-relevance caches during pre-filling. Additionally, our Adaptive Positional Allocation Strategy dynamically reassigns cache positions to maximize the use of the available positional encoding range. Experiments on the Natural Questions and TriviaQA datasets demonstrate that CacheFocus outperforms alternative methods even when inputs exceed the $4$K limit of the \texttt{LLaMA-2} model, emphasizing its practical effectiveness for long-context LLMs. Moreover, even with large maximum input length of \texttt{Qwen2}, the performance of CacheFocus shows that it maintains consistent performance even as the number of documents increases, effectively managing long-text generation without degradation.

### Towards Achieving Concept Completeness for Unsupervised Textual Concept Bottleneck Models 
[[arxiv](https://arxiv.org/abs/2502.11100)] [[cool](https://papers.cool/arxiv/2502.11100)] [[pdf](https://arxiv.org/pdf/2502.11100)]
> **Authors**: Milan Bhan,Yann Choho,Pierre Moreau,Jean-Noel Vittaut,Nicolas Chesneau,Marie-Jeanne Lesot
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Textual Concept Bottleneck Models (TBMs) are interpretable-by-design models for text classification that predict a set of salient concepts before making the final prediction. This paper proposes Complete Textual Concept Bottleneck Model (CT-CBM),a novel TCBM generator building concept labels in a fully unsupervised manner using a small language model, eliminating both the need for predefined human labeled concepts and LLM annotations. CT-CBM iteratively targets and adds important concepts in the bottleneck layer to create a complete concept basis and addresses downstream classification leakage through a parallel residual connection. CT-CBM achieves good results against competitors, offering a promising solution to enhance interpretability of NLP classifiers without sacrificing performance.

### A Survey of Large Language Models in Psychotherapy: Current Landscape and Future Directions 
[[arxiv](https://arxiv.org/abs/2502.11095)] [[cool](https://papers.cool/arxiv/2502.11095)] [[pdf](https://arxiv.org/pdf/2502.11095)]
> **Authors**: Hongbin Na,Yining Hua,Zimu Wang,Tao Shen,Beibei Yu,Lilin Wang,Wei Wang,John Torous,Ling Chen
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: in progress
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Mental health remains a critical global challenge, with increasing demand for accessible, effective interventions. Large language models (LLMs) offer promising solutions in psychotherapy by enhancing the assessment, diagnosis, and treatment of mental health conditions through dynamic, context-aware interactions. This survey provides a comprehensive overview of the current landscape of LLM applications in psychotherapy, highlighting the roles of LLMs in symptom detection, severity estimation, cognitive assessment, and therapeutic interventions. We present a novel conceptual taxonomy to organize the psychotherapy process into three core components: assessment, diagnosis, and treatment, and examine the challenges and advancements in each area. The survey also addresses key research gaps, including linguistic biases, limited disorder coverage, and underrepresented therapeutic models. Finally, we discuss future directions to integrate LLMs into a holistic, end-to-end psychotherapy framework, addressing the evolving nature of mental health conditions and fostering more inclusive, personalized care.

### SafeDialBench: A Fine-Grained Safety Benchmark for Large Language Models in Multi-Turn Dialogues with Diverse Jailbreak Attacks 
[[arxiv](https://arxiv.org/abs/2502.11090)] [[cool](https://papers.cool/arxiv/2502.11090)] [[pdf](https://arxiv.org/pdf/2502.11090)]
> **Authors**: Hongye Cao,Yanming Wang,Sijia Jing,Ziyue Peng,Zhixin Bai,Zhe Cao,Meng Fang,Fan Feng,Boyan Wang,Jiaheng Liu,Tianpei Yang,Jing Huo,Yang Gao,Fanyu Meng,Xi Yang,Chao Deng,Junlan Feng
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: With the rapid advancement of Large Language Models (LLMs), the safety of LLMs has been a critical concern requiring precise assessment. Current benchmarks primarily concentrate on single-turn dialogues or a single jailbreak attack method to assess the safety. Additionally, these benchmarks have not taken into account the LLM's capability of identifying and handling unsafe information in detail. To address these issues, we propose a fine-grained benchmark SafeDialBench for evaluating the safety of LLMs across various jailbreak attacks in multi-turn dialogues. Specifically, we design a two-tier hierarchical safety taxonomy that considers 6 safety dimensions and generates more than 4000 multi-turn dialogues in both Chinese and English under 22 dialogue scenarios. We employ 7 jailbreak attack strategies, such as reference attack and purpose reverse, to enhance the dataset quality for dialogue generation. Notably, we construct an innovative assessment framework of LLMs, measuring capabilities in detecting, and handling unsafe information and maintaining consistency when facing jailbreak attacks. Experimental results across 17 LLMs reveal that Yi-34B-Chat and GLM4-9B-Chat demonstrate superior safety performance, while Llama3.1-8B-Instruct and o3-mini exhibit safety vulnerabilities.

### Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention 
[[arxiv](https://arxiv.org/abs/2502.11089)] [[cool](https://papers.cool/arxiv/2502.11089)] [[pdf](https://arxiv.org/pdf/2502.11089)]
> **Authors**: Jingyang Yuan,Huazuo Gao,Damai Dai,Junyu Luo,Liang Zhao,Zhengyan Zhang,Zhenda Xie,Y. X. Wei,Lean Wang,Zhiping Xiao,Yuqing Wang,Chong Ruan,Ming Zhang,Wenfeng Liang,Wangding Zeng
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Long-context modeling is crucial for next-generation language models, yet the high computational cost of standard attention mechanisms poses significant computational challenges. Sparse attention offers a promising direction for improving efficiency while maintaining model capabilities. We present NSA, a Natively trainable Sparse Attention mechanism that integrates algorithmic innovations with hardware-aligned optimizations to achieve efficient long-context modeling. NSA employs a dynamic hierarchical sparse strategy, combining coarse-grained token compression with fine-grained token selection to preserve both global context awareness and local precision. Our approach advances sparse attention design with two key innovations: (1) We achieve substantial speedups through arithmetic intensity-balanced algorithm design, with implementation optimizations for modern hardware. (2) We enable end-to-end training, reducing pretraining computation without sacrificing model performance. As shown in Figure 1, experiments show the model pretrained with NSA maintains or exceeds Full Attention models across general benchmarks, long-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves substantial speedups over Full Attention on 64k-length sequences across decoding, forward propagation, and backward propagation, validating its efficiency throughout the model lifecycle.

### Rewrite to Jailbreak: Discover Learnable and Transferable Implicit Harmfulness Instruction 
[[arxiv](https://arxiv.org/abs/2502.11084)] [[cool](https://papers.cool/arxiv/2502.11084)] [[pdf](https://arxiv.org/pdf/2502.11084)]
> **Authors**: Yuting Huang,Chengyuan Liu,Yifeng Feng,Chao Wu,Fei Wu,Kun Kuang
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 21pages, 10 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: As Large Language Models (LLMs) are widely applied in various domains, the safety of LLMs is increasingly attracting attention to avoid their powerful capabilities being misused. Existing jailbreak methods create a forced instruction-following scenario, or search adversarial prompts with prefix or suffix tokens to achieve a specific representation manually or automatically. However, they suffer from low efficiency and explicit jailbreak patterns, far from the real deployment of mass attacks to LLMs. In this paper, we point out that simply rewriting the original instruction can achieve a jailbreak, and we find that this rewriting approach is learnable and transferable. We propose the Rewrite to Jailbreak (R2J) approach, a transferable black-box jailbreak method to attack LLMs by iteratively exploring the weakness of the LLMs and automatically improving the attacking strategy. The jailbreak is more efficient and hard to identify since no additional features are introduced. Extensive experiments and analysis demonstrate the effectiveness of R2J, and we find that the jailbreak is also transferable to multiple datasets and various types of models with only a few queries. We hope our work motivates further investigation of LLM safety.

### Streamlining the Collaborative Chain of Models into A Single Forward Pass in Generation-Based Tasks 
[[arxiv](https://arxiv.org/abs/2502.11083)] [[cool](https://papers.cool/arxiv/2502.11083)] [[pdf](https://arxiv.org/pdf/2502.11083)]
> **Authors**: Yuanjie Lyu,Chao Zhang,Yuhao Chen,Yong Chen,Tong Xu
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: In Retrieval-Augmented Generation (RAG) and agent-based frameworks, the "Chain of Models" approach is widely used, where multiple specialized models work sequentially on distinct sub-tasks. This approach is effective but increases resource demands as each model must be deployed separately. Recent advancements attempt to address this by applying prompt tuning, which allows a shared base model to adapt to multiple tasks with minimal parameter changes. However, a key challenge remains: intermediate outputs, passed between models as plain text, require recomputation of hidden states (i.e., Key and Value (KV) states in Transformers) during inference. In this paper, we introduce FTHSS, a novel prompt-tuning method that enables models to share KV hidden states, eliminating redundant forward passes and reducing KV cache storage. By modifying input and attention masks during training, FTHSS allows models to effectively utilize KV hidden states from prior models in both single- and multi-round scenarios. Empirical results on four tasks show that FTHSS matches the performance of traditional model chains while improving inference efficiency.

### DEEPER Insight into Your User: Directed Persona Refinement for Dynamic Persona Modeling 
[[arxiv](https://arxiv.org/abs/2502.11078)] [[cool](https://papers.cool/arxiv/2502.11078)] [[pdf](https://arxiv.org/pdf/2502.11078)]
> **Authors**: Aili Chen,Chengyu Du,Jiangjie Chen,Jinghan Xu,Yikai Zhang,Siyu Yuan,Zulong Chen,Liangyue Li,Yanghua Xiao
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: To advance personalized applications such as recommendation systems and user behavior prediction, recent research increasingly adopts large language models (LLMs) for human -readable persona modeling. In dynamic real -world scenarios, effective persona modeling necessitates leveraging streaming behavior data to continually optimize user personas. However, existing methods -whether regenerating personas or incrementally extending them with new behaviors -often fail to achieve sustained improvements in persona quality or future behavior prediction accuracy. To address this, we propose DEEPER, a novel approach for dynamic persona modeling that enables continual persona optimization. Specifically, we enhance the model's direction -search capability through an iterative reinforcement learning framework, allowing it to automatically identify effective update directions and optimize personas using discrepancies between user behaviors and model predictions. Extensive experiments on dynamic persona modeling involving 4800 users across 10 domains highlight the superior persona optimization capabilities of DEEPER, delivering an impressive 32.2% average reduction in user behavior prediction error over four update rounds -outperforming the best baseline by a remarkable 22.92%.

### Exposing Numeracy Gaps: A Benchmark to Evaluate Fundamental Numerical Abilities in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.11075)] [[cool](https://papers.cool/arxiv/2502.11075)] [[pdf](https://arxiv.org/pdf/2502.11075)]
> **Authors**: Haoyang Li,Xuejia Chen,Zhanchao XU,Darian Li,Nicole Hu,Fei Teng,Yiming Li,Luyu Qiu,Chen Jason Zhang,Qing Li,Lei Chen
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large Language Models (LLMs) have demonstrated impressive capabilities in natural language processing tasks, such as text generation and semantic understanding. However, their performance on numerical reasoning tasks, such as basic arithmetic, numerical retrieval, and magnitude comparison, remains surprisingly poor. This gap arises from their reliance on surface-level statistical patterns rather than understanding numbers as continuous magnitudes. Existing benchmarks primarily focus on either linguistic competence or structured mathematical problem-solving, neglecting fundamental numerical reasoning required in real-world scenarios. To bridge this gap, we propose NumericBench, a comprehensive benchmark to evaluate six fundamental numerical capabilities: number recognition, arithmetic operations, contextual retrieval, comparison, summary, and logical reasoning. NumericBench includes datasets ranging from synthetic number lists to the crawled real-world data, addressing challenges like long contexts, noise, and multi-step reasoning. Extensive experiments on state-of-the-art LLMs, including GPT-4 and DeepSeek, reveal persistent weaknesses in numerical reasoning, highlighting the urgent need to improve numerically-aware language modeling. The benchmark is released in: https://github.com/TreeAI-Lab/NumericBench.

### Demystifying Hateful Content: Leveraging Large Multimodal Models for Hateful Meme Detection with Explainable Decisions 
[[arxiv](https://arxiv.org/abs/2502.11073)] [[cool](https://papers.cool/arxiv/2502.11073)] [[pdf](https://arxiv.org/pdf/2502.11073)]
> **Authors**: Ming Shan Hee,Roy Ka-Wei Lee
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: Preprint. Accepted at ICWSM'25
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Hateful meme detection presents a significant challenge as a multimodal task due to the complexity of interpreting implicit hate messages and contextual cues within memes. Previous approaches have fine-tuned pre-trained vision-language models (PT-VLMs), leveraging the knowledge they gained during pre-training and their attention mechanisms to understand meme content. However, the reliance of these models on implicit knowledge and complex attention mechanisms renders their decisions difficult to explain, which is crucial for building trust in meme classification. In this paper, we introduce IntMeme, a novel framework that leverages Large Multimodal Models (LMMs) for hateful meme classification with explainable decisions. IntMeme addresses the dual challenges of improving both accuracy and explainability in meme moderation. The framework uses LMMs to generate human-like, interpretive analyses of memes, providing deeper insights into multimodal content and context. Additionally, it uses independent encoding modules for both memes and their interpretations, which are then combined to enhance classification performance. Our approach addresses the opacity and misclassification issues associated with PT-VLMs, optimizing the use of LMMs for hateful meme detection. We demonstrate the effectiveness of IntMeme through comprehensive experiments across three datasets, showcasing its superiority over state-of-the-art models.

### CARMA: Enhanced Compositionality in LLMs via Advanced Regularisation and Mutual Information Alignment 
[[arxiv](https://arxiv.org/abs/2502.11066)] [[cool](https://papers.cool/arxiv/2502.11066)] [[pdf](https://arxiv.org/pdf/2502.11066)]
> **Authors**: Nura Aljaafari,Danilo S. Carvalho,André Freitas
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 18 pages, 7 figures, 5 tables
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) struggle with compositional generalisation, limiting their ability to systematically combine learned components to interpret novel inputs. While architectural modifications, fine-tuning, and data augmentation improve compositionality, they often have limited adaptability, face scalability constraints, or yield diminishing returns on real data. To address this, we propose CARMA, an intervention that enhances the stability and robustness of compositional reasoning in LLMs while preserving fine-tuned performance. CARMA employs mutual information regularisation and layer-wise stability constraints to mitigate feature fragmentation, ensuring structured representations persist across and within layers. We evaluate CARMA on inverse dictionary modelling and sentiment classification, measuring its impact on semantic consistency, performance stability, and robustness to lexical perturbations. Results show that CARMA reduces the variability introduced by fine-tuning, stabilises token representations, and improves compositional reasoning. While its effectiveness varies across architectures, CARMA's key strength lies in reinforcing learned structures rather than introducing new capabilities, making it a scalable auxiliary method. These findings suggest that integrating CARMA with fine-tuning can improve compositional generalisation while maintaining task-specific performance in LLMs.

### Beyond Similarity: A Gradient-based Graph Method for Instruction Tuning Data Selection 
[[arxiv](https://arxiv.org/abs/2502.11062)] [[cool](https://papers.cool/arxiv/2502.11062)] [[pdf](https://arxiv.org/pdf/2502.11062)]
> **Authors**: Yang Zhao,Li Du,Xiao Ding,Yangou Ouyang,Hepeng Wang,Kai Xiong,Jinglong Gao,Zhouhao Sun,Dongliang Xu,Yang Qing,Dongchen Li,Bing Qin,Ting Liu
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) have shown great potential across various industries due to their remarkable ability to generalize through instruction tuning. However, the limited availability of domain-specific data significantly hampers their performance on specialized tasks. While existing methods primarily focus on selecting training data from general datasets that are similar to the target domain, they often fail to consider the joint distribution of instructions, resulting in inefficient learning and suboptimal knowledge transfer. To address these challenges, we introduce G2IS (Gradient-based Graph Instruction Selection), a novel method that constructs a mixed gradient-based instruction graph to capture the joint distribution and interdependencies between instructions. By accounting for the relationships between instructions, G2IS improves domain adaptation efficiency. Additionally, we propose a gradient walk algorithm to refine the data selection process, enhancing both training effectiveness and efficiency. Our experiments demonstrate that G2IS outperforms traditional methods across various domain adaptation tasks, yielding significant performance gains, particularly in complex, data-scarce scenarios. These results underscore the potential of G2IS in advancing the development of large, domain-specific models.

### Déjà Vu? Decoding Repeated Reading from Eye Movements 
[[arxiv](https://arxiv.org/abs/2502.11061)] [[cool](https://papers.cool/arxiv/2502.11061)] [[pdf](https://arxiv.org/pdf/2502.11061)]
> **Authors**: Yoav Meiri,Omer Shubi,Cfir Avraham Hadar,Ariel Kreisberg Nitzav,Yevgeni Berzak
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Be it your favorite novel, a newswire article, a cooking recipe or an academic paper -- in many daily situations we read the same text more than once. In this work, we ask whether it is possible to automatically determine whether the reader has previously encountered a text based on their eye movement patterns. We introduce two variants of this task and address them with considerable success using both feature-based and neural models. We further introduce a general strategy for enhancing these models with machine generated simulations of eye movements from a cognitive model. Finally, we present an analysis of model performance which on the one hand yields insights on the information used by the models, and on the other hand leverages predictive modeling as an analytic tool for better characterization of the role of memory in repeated reading. Our work advances the understanding of the extent and manner in which eye movements in reading capture memory effects from prior text exposure, and paves the way for future applications that involve predictive modeling of repeated reading.

### Reasoning-Augmented Conversation for Multi-Turn Jailbreak Attacks on Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.11054)] [[cool](https://papers.cool/arxiv/2502.11054)] [[pdf](https://arxiv.org/pdf/2502.11054)]
> **Authors**: Zonghao Ying,Deyue Zhang,Zonglei Jing,Yisong Xiao,Quanchen Zou,Aishan Liu,Siyuan Liang,Xiangzheng Zhang,Xianglong Liu,Dacheng Tao
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,密码学和安全
- **Abstract**: Multi-turn jailbreak attacks simulate real-world human interactions by engaging large language models (LLMs) in iterative dialogues, exposing critical safety vulnerabilities. However, existing methods often struggle to balance semantic coherence with attack effectiveness, resulting in either benign semantic drift or ineffective detection evasion. To address this challenge, we propose Reasoning-Augmented Conversation, a novel multi-turn jailbreak framework that reformulates harmful queries into benign reasoning tasks and leverages LLMs' strong reasoning capabilities to compromise safety alignment. Specifically, we introduce an attack state machine framework to systematically model problem translation and iterative reasoning, ensuring coherent query generation across multiple turns. Building on this framework, we design gain-guided exploration, self-play, and rejection feedback modules to preserve attack semantics, enhance effectiveness, and sustain reasoning-driven attack progression. Extensive experiments on multiple LLMs demonstrate that RACE achieves state-of-the-art attack effectiveness in complex conversational scenarios, with attack success rates (ASRs) increasing by up to 96%. Notably, our approach achieves ASRs of 82% and 92% against leading commercial models, OpenAI o1 and DeepSeek R1, underscoring its potency. We release our code at https://github.com/NY1024/RACE to facilitate further research in this critical domain.

### MMUnlearner: Reformulating Multimodal Machine Unlearning in the Era of Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.11051)] [[cool](https://papers.cool/arxiv/2502.11051)] [[pdf](https://arxiv.org/pdf/2502.11051)]
> **Authors**: Jiahao Huo,Yibo Yan,Xu Zheng,Yuanhuiyi Lyu,Xin Zou,Zhihua Wei,Xuming Hu
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Recent progress in Machine Unlearning (MU) has introduced solutions for the selective removal of private or sensitive information encoded within deep neural networks. Nonetheless, MU for Multimodal Large Language Models (MLLMs) remains in its nascent phase. Therefore, we propose to reformulate the task of multimodal MU in the era of MLLMs, which aims to erase only the visual patterns associated with a given entity while preserving the corresponding textual knowledge encoded within the original parameters of the language model backbone. Furthermore, we develop a novel geometry-constrained gradient descent method MMUnlearner. It updates the weights of MLLMs with a weight saliency map jointly restricted by the remaining concepts and textual knowledge during unlearning, thereby preserving parameters essential for non-target knowledge. Extensive experiments demonstrate that MMUnlearner surpasses baselines that finetuning MLLMs with VQA data directly through Gradient Ascent (GA) or Negative Preference Optimization (NPO), across all evaluation dimensions. Our code will be released upon acceptance.

### Mind the Confidence Gap: Overconfidence, Calibration, and Distractor Effects in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.11028)] [[cool](https://papers.cool/arxiv/2502.11028)] [[pdf](https://arxiv.org/pdf/2502.11028)]
> **Authors**: Prateek Chhikara
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large Language Models (LLMs) demonstrate impressive performance across diverse tasks, yet confidence calibration remains a challenge. Miscalibration - where models are overconfident or underconfident - poses risks, particularly in high-stakes applications. This paper presents an empirical study on LLM calibration, examining how model size, distractors, and question types affect confidence alignment. We introduce an evaluation framework to measure overconfidence and investigate whether multiple-choice formats mitigate or worsen miscalibration. Our findings show that while larger models (e.g., GPT-4o) are better calibrated overall, they are more prone to distraction, whereas smaller models benefit more from answer choices but struggle with uncertainty estimation. Unlike prior work, which primarily reports miscalibration trends, we provide actionable insights into failure modes and conditions that worsen overconfidence. These findings highlight the need for calibration-aware interventions and improved uncertainty estimation methods.

### MultiTEND: A Multilingual Benchmark for Natural Language to NoSQL Query Translation 
[[arxiv](https://arxiv.org/abs/2502.11022)] [[cool](https://papers.cool/arxiv/2502.11022)] [[pdf](https://arxiv.org/pdf/2502.11022)]
> **Authors**: Zhiqian Qin,Yuanfeng Song,Jinwei Lu,Yuanwei Song,Shuaimin Li,Chen Jason Zhang
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Natural language interfaces for NoSQL databases are increasingly vital in the big data era, enabling users to interact with complex, unstructured data without deep technical expertise. However, most recent advancements focus on English, leaving a gap for multilingual support. This paper introduces MultiTEND, the first and largest multilingual benchmark for natural language to NoSQL query generation, covering six languages: English, German, French, Russian, Japanese and Mandarin Chinese. Using MultiTEND, we analyze challenges in translating natural language to NoSQL queries across diverse linguistic structures, including lexical and syntactic differences. Experiments show that performance accuracy in both English and non-English settings remains relatively low, with a 4%-6% gap across scenarios like fine-tuned SLM, zero-shot LLM, and RAG for LLM. To address the aforementioned challenges, we introduce MultiLink, a novel framework that bridges the multilingual input to NoSQL query generation gap through a Parallel Linking Process. It breaks down the task into multiple steps, integrating parallel multilingual processing, Chain-of-Thought (CoT) reasoning, and Retrieval-Augmented Generation (RAG) to tackle lexical and structural challenges inherent in multilingual NoSQL generation. MultiLink shows enhancements in all metrics for every language against the top baseline, boosting execution accuracy by about 15% for English and averaging a 10% improvement for non-English languages.

### TUMLU: A Unified and Native Language Understanding Benchmark for Turkic Languages 
[[arxiv](https://arxiv.org/abs/2502.11020)] [[cool](https://papers.cool/arxiv/2502.11020)] [[pdf](https://arxiv.org/pdf/2502.11020)]
> **Authors**: Jafar Isbarov,Arofat Akhundjanova,Mammad Hajili,Kavsar Huseynova,Dmitry Gaynullin,Anar Rzayev,Osman Tursun,Ilshat Saetov,Rinat Kharisov,Saule Belginova,Ariana Kenbayeva,Amina Alisheva,Aizirek Turdubaeva,Abdullatif Köksal,Samir Rustamov,Duygu Ataman
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Being able to thoroughly assess massive multi-task language understanding (MMLU) capabilities is essential for advancing the applicability of multilingual language models. However, preparing such benchmarks in high quality native language is often costly and therefore limits the representativeness of evaluation datasets. While recent efforts focused on building more inclusive MMLU benchmarks, these are conventionally built using machine translation from high-resource languages, which may introduce errors and fail to account for the linguistic and cultural intricacies of the target languages. In this paper, we address the lack of native language MMLU benchmark especially in the under-represented Turkic language family with distinct morphosyntactic and cultural characteristics. We propose two benchmarks for Turkic language MMLU: TUMLU is a comprehensive, multilingual, and natively developed language understanding benchmark specifically designed for Turkic languages. It consists of middle- and high-school level questions spanning 11 academic subjects in Azerbaijani, Crimean Tatar, Karakalpak, Kazakh, Tatar, Turkish, Uyghur, and Uzbek. We also present TUMLU-mini, a more concise, balanced, and manually verified subset of the dataset. Using this dataset, we systematically evaluate a diverse range of open and proprietary multilingual large language models (LLMs), including Claude, Gemini, GPT, and LLaMA, offering an in-depth analysis of their performance across different languages, subjects, and alphabets. To promote further research and development in multilingual language understanding, we release TUMLU-mini and all corresponding evaluation scripts.

### GRIFFIN: Effective Token Alignment for Faster Speculative Decoding 
[[arxiv](https://arxiv.org/abs/2502.11018)] [[cool](https://papers.cool/arxiv/2502.11018)] [[pdf](https://arxiv.org/pdf/2502.11018)]
> **Authors**: Shijing Hu,Jingyang Li,Xingyu Xie,Zhihui Lu,Kim-Chuan Toh,Pan Zhou
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Speculative decoding accelerates inference in large language models (LLMs) by generating multiple draft tokens simultaneously. However, existing methods often struggle with token misalignment between the training and decoding phases, limiting their performance. To address this, we propose GRIFFIN, a novel framework that incorporates a token-alignable training strategy and a token-alignable draft model to mitigate misalignment. The training strategy employs a loss masking mechanism to exclude highly misaligned tokens during training, preventing them from negatively impacting the draft model's optimization. The token-alignable draft model introduces input tokens to correct inconsistencies in generated features. Experiments on LLaMA-series and Vicuna models demonstrate that GRIFFIN achieves an average acceptance length improvement of over 7\% and a speedup ratio exceeding 8%, outperforming current SoTAs as shown in Fig. 1 (a) and (b).

### CounterBench: A Benchmark for Counterfactuals Reasoning in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.11008)] [[cool](https://papers.cool/arxiv/2502.11008)] [[pdf](https://arxiv.org/pdf/2502.11008)]
> **Authors**: Yuefei Chen,Vivek K. Singh,Jing Ma,Ruxiang Tang
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Counterfactual reasoning is widely recognized as one of the most challenging and intricate aspects of causality in artificial intelligence. In this paper, we evaluate the performance of large language models (LLMs) in counterfactual reasoning. In contrast to previous studies that primarily focus on commonsense causal reasoning, where LLMs often rely on prior knowledge for inference, we specifically assess their ability to perform counterfactual inference using a set of formal rules. To support this evaluation, we introduce a new benchmark dataset, CounterBench, comprising 1K counterfactual reasoning questions. The dataset is designed with varying levels of difficulty, diverse causal graph structures, distinct types of counterfactual questions, and multiple nonsensical name variants. Our experiments demonstrate that counterfactual reasoning poses a significant challenge for LLMs, with most models performing at levels comparable to random guessing. To enhance LLM's counterfactual reasoning ability, we propose a novel reasoning paradigm, CoIn, which guides LLMs through iterative reasoning and backtracking to systematically explore counterfactual solutions. Experimental results show that our method significantly improves LLM performance on counterfactual reasoning tasks and consistently enhances performance across different LLMs.Our dataset is available at https://huggingface.co/datasets/CounterBench/CounterBench.

### RAS: Retrieval-And-Structuring for Knowledge-Intensive LLM Generation 
[[arxiv](https://arxiv.org/abs/2502.10996)] [[cool](https://papers.cool/arxiv/2502.10996)] [[pdf](https://arxiv.org/pdf/2502.10996)]
> **Authors**: Pengcheng Jiang,Lang Cao,Ruike Zhu,Minhao Jiang,Yunyi Zhang,Jimeng Sun,Jiawei Han
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: under review
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Retrieval-augmented language models often struggle with knowledge-intensive tasks due to inefficient retrieval, unstructured knowledge integration, and single-pass architectures. We present Retrieval-And-Structuring (RAS), a novel framework that dynamically constructs and reasons over query-specific knowledge graphs through iterative retrieval and structuring. RAS introduces four key technical innovations: (1) a themescoped retrieval mechanism that efficiently narrows the search space while maintaining retrieval quality, (2) an action planning module that determines knowledge needs and generates focused sub-queries, (3) a dynamic knowledge structuring approach that converts retrieved text into an evolving knowledge graph, and (4) a graph-augmented answering component that leverages the accumulated structured information. Our framework achieves state-of-the-art performance, surpassing leading baselines by 6.4% with open-source language models and 7.0% with proprietary models on seven knowledge-intensive generation datasets across all evaluation metrics. Detailed ablation studies verify the contribution of each technical component to the overall system performance.

### Evaluating Large language models on Understanding Korean indirect Speech acts 
[[arxiv](https://arxiv.org/abs/2502.10995)] [[cool](https://papers.cool/arxiv/2502.10995)] [[pdf](https://arxiv.org/pdf/2502.10995)]
> **Authors**: Youngeun Koo,Jiwoo Lee,Dojun Park,Seohyun Park,Sungeun Lee
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: under review (15 pages)
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: To accurately understand the intention of an utterance is crucial in conversational communication. As conversational artificial intelligence models are rapidly being developed and applied in various fields, it is important to evaluate the LLMs' capabilities of understanding the intentions of user's utterance. This study evaluates whether current LLMs can understand the intention of an utterance by considering the given conversational context, particularly in cases where the actual intention differs from the surface-leveled, literal intention of the sentence, i.e. indirect speech acts. Our findings reveal that Claude3-Opus outperformed the other competing models, with 71.94% in MCQ and 65% in OEQ, showing a clear advantage. In general, proprietary models exhibited relatively higher performance compared to open-source models. Nevertheless, no LLMs reached the level of human performance. Most LLMs, except for Claude3-Opus, demonstrated significantly lower performance in understanding indirect speech acts compared to direct speech acts, where the intention is explicitly revealed through the utterance. This study not only performs an overall pragmatic evaluation of each LLM's language use through the analysis of OEQ response patterns, but also emphasizes the necessity for further research to improve LLMs' understanding of indirect speech acts for more natural communication with humans.

### RoseRAG: Robust Retrieval-augmented Generation with Small-scale LLMs via Margin-aware Preference Optimization 
[[arxiv](https://arxiv.org/abs/2502.10993)] [[cool](https://papers.cool/arxiv/2502.10993)] [[pdf](https://arxiv.org/pdf/2502.10993)]
> **Authors**: Tianci Liu,Haoxiang Jiang,Tianze Wang,Ran Xu,Yue Yu,Linjun Zhang,Tuo Zhao,Haoyu Wang
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Large language models (LLMs) have achieved impressive performance but face high computational costs and latency, limiting their deployment in resource-constrained settings. In contrast, small-scale LLMs (SLMs) are more efficient yet struggle to capture evolving real-world knowledge. Retrieval-augmented generation (RAG) helps by integrating external knowledge, but imperfect retrieval can introduce distracting noise that misleads SLMs. We propose RoseRAG, a robust RAG framework for SLMs via Margin-aware Preference Optimization. RoseRAG employs multi-turn prompting for detailed reasoning, rejection sampling for high-quality explanations, and contrastive preference selection to refine responses by maximizing the likelihood gap between preferred and non-preferred outputs. By integrating these components into a margin-aware optimization process, RoseRAG robustly enhances the accuracy and reliability of SLMs for RAG applications. Extensive experiments on three open-domain question answering benchmarks indicate that our innovative RoseRAG surpasses state-of-the-art baselines significantly.

### FinMTEB: Finance Massive Text Embedding Benchmark 
[[arxiv](https://arxiv.org/abs/2502.10990)] [[cool](https://papers.cool/arxiv/2502.10990)] [[pdf](https://arxiv.org/pdf/2502.10990)]
> **Authors**: Yixuan Tang,Yi Yang
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: https://github.com/yixuantt/FinMTEB
- **标题**: None
- **领域**: 计算语言学,信息检索
- **Abstract**: Embedding models play a crucial role in representing and retrieving information across various NLP applications. Recent advances in large language models (LLMs) have further enhanced the performance of embedding models. While these models are often benchmarked on general-purpose datasets, real-world applications demand domain-specific evaluation. In this work, we introduce the Finance Massive Text Embedding Benchmark (FinMTEB), a specialized counterpart to MTEB designed for the financial domain. FinMTEB comprises 64 financial domain-specific embedding datasets across 7 tasks that cover diverse textual types in both Chinese and English, such as financial news articles, corporate annual reports, ESG reports, regulatory filings, and earnings call transcripts. We also develop a finance-adapted model, Fin-E5, using a persona-based data synthetic method to cover diverse financial embedding tasks for training. Through extensive evaluation of 15 embedding models, including Fin-E5, we show three key findings: (1) performance on general-purpose benchmarks shows limited correlation with financial domain tasks; (2) domain-adapted models consistently outperform their general-purpose counterparts; and (3) surprisingly, a simple Bag-of-Words (BoW) approach outperforms sophisticated dense embeddings in financial Semantic Textual Similarity (STS) tasks, underscoring current limitations in dense embedding techniques. Our work establishes a robust evaluation framework for financial NLP applications and provides crucial insights for developing domain-specific embedding models.

### Akan Cinematic Emotions (ACE): A Multimodal Multi-party Dataset for Emotion Recognition in Movie Dialogues 
[[arxiv](https://arxiv.org/abs/2502.10973)] [[cool](https://papers.cool/arxiv/2502.10973)] [[pdf](https://arxiv.org/pdf/2502.10973)]
> **Authors**: David Sasu,Zehui Wu,Ziwei Gong,Run Chen,Pengyuan Shi,Lin Ai,Julia Hirschberg,Natalie Schluter
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: In this paper, we introduce the Akan Conversation Emotion (ACE) dataset, the first multimodal emotion dialogue dataset for an African language, addressing the significant lack of resources for low-resource languages in emotion recognition research. ACE, developed for the Akan language, contains 385 emotion-labeled dialogues and 6,162 utterances across audio, visual, and textual modalities, along with word-level prosodic prominence annotations. The presence of prosodic labels in this dataset also makes it the first prosodically annotated African language dataset. We demonstrate the quality and utility of ACE through experiments using state-of-the-art emotion recognition methods, establishing solid baselines for future research. We hope ACE inspires further work on inclusive, linguistically and culturally diverse NLP resources.

### Neural Networks Remember More: The Power of Parameter Isolation and Combination 
[[arxiv](https://arxiv.org/abs/2502.10966)] [[cool](https://papers.cool/arxiv/2502.10966)] [[pdf](https://arxiv.org/pdf/2502.10966)]
> **Authors**: Biqing Zeng,Zehan Li,Aladdin Ayesh
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Catastrophic forgetting is a pervasive issue for pre-trained language models (PLMs) during continual learning, where models lose previously acquired knowledge when sequentially trained on a series of tasks. The model's ability to retain old tasks is referred to as stability, while its adaptability to new tasks is called plasticity. Therefore, the key to solving this problem is to find a trade-off between the plasticity and stability of the model. To address this issue, in this paper, we propose a novel method to achieve a balance between model stability and plasticity, thereby mitigating catastrophic forgetting. More specifically, our proposed approach leverages parameter isolation and a subsequent combination strategy. Initially, in the training stage, the model adapts to each downstream task via a parameter isolation method to prevent potential interference among different tasks. We then combine all trained parameters, which contain acquired knowledge, using the task arithmetic method and finally apply them to the backbone model. Empirical evaluations on continual language learning benchmarks substantiate the effectiveness of our approach, revealing a marked enhancement over existing state-of-the-art approaches.

### Exploring Contextual Flux in Large Language Models: A Novel Approach to Self-Modulating Semantic Networks 
[[arxiv](https://arxiv.org/abs/2502.10942)] [[cool](https://papers.cool/arxiv/2502.10942)] [[pdf](https://arxiv.org/pdf/2502.10942)]
> **Authors**: Henry Evidail,Zachary Mountebank,Alistair Hathersage,Peter Stanhope,Basil Ravenscroft,Tobias Waddingham
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Self-modulating mechanisms introduce dynamic adaptation capabilities within language models through contextual realignment strategies that influence token embedding trajectories across extended sequences. Contextual Flux is explored as an approach to embedding modulation, integrating an auxiliary gating mechanism within the self-attention framework to dynamically adjust token representations based on evolving contextual dependencies. The empirical analysis evaluates entropy variations, latent space realignments, and coherence stability to assess the extent to which self-regulation enhances text generation consistency while preserving generative flexibility. Quantitative assessments suggest that embedding shifts contribute to more structured adaptation in long-form sequences, with measured reductions in redundant phrase repetitions and improvements in thematic retention. Variability in contextual weight computation affects modulation stability, leading to differing levels of adaptation across diverse linguistic structures. The computational demands introduced through real-time embedding reconfiguration are examined in relation to model scalability, emphasizing the need for optimization strategies in high-volume generative applications. The findings suggest that while adaptive embedding updates improve certain aspects of coherence, their impact remains contingent on model capacity and input complexity.

### Fundamental Principles of Linguistic Structure are Not Represented by o3 
[[arxiv](https://arxiv.org/abs/2502.10934)] [[cool](https://papers.cool/arxiv/2502.10934)] [[pdf](https://arxiv.org/pdf/2502.10934)]
> **Authors**: Elliot Murphy,Evelina Leivada,Vittoria Dentella,Fritz Gunther,Gary Marcus
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: A core component of a successful artificial general intelligence would be the rapid creation and manipulation of grounded compositional abstractions and the demonstration of expertise in the family of recursive hierarchical syntactic objects necessary for the creative use of human language. We evaluated the recently released o3 model (OpenAI; o3-mini-high) and discovered that while it succeeds on some basic linguistic tests relying on linear, surface statistics (e.g., the Strawberry Test), it fails to generalize basic phrase structure rules; it fails with comparative sentences involving semantically illegal cardinality comparisons ('Escher sentences'); its fails to correctly rate and explain acceptability dynamics; and it fails to distinguish between instructions to generate unacceptable semantic vs. unacceptable syntactic outputs. When tasked with generating simple violations of grammatical rules, it is seemingly incapable of representing multiple parses to evaluate against various possible semantic interpretations. In stark contrast to many recent claims that artificial language models are on the verge of replacing the field of linguistics, our results suggest not only that deep learning is hitting a wall with respect to compositionality (Marcus 2022), but that it is hitting [a [stubbornly [resilient wall]]] that cannot readily be surmounted to reach human-like compositional reasoning simply through more compute.

### Evolving Hate Speech Online: An Adaptive Framework for Detection and Mitigation 
[[arxiv](https://arxiv.org/abs/2502.10921)] [[cool](https://papers.cool/arxiv/2502.10921)] [[pdf](https://arxiv.org/pdf/2502.10921)]
> **Authors**: Shiza Ali,Jeremy Blackburn,Gianluca Stringhini
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,社交和信息网络
- **Abstract**: The proliferation of social media platforms has led to an increase in the spread of hate speech, particularly targeting vulnerable communities. Unfortunately, existing methods for automatically identifying and blocking toxic language rely on pre-constructed lexicons, making them reactive rather than adaptive. As such, these approaches become less effective over time, especially when new communities are targeted with slurs not included in the original datasets. To address this issue, we present an adaptive approach that uses word embeddings to update lexicons and develop a hybrid model that adjusts to emerging slurs and new linguistic patterns. This approach can effectively detect toxic language, including intentional spelling mistakes employed by aggressors to avoid detection. Our hybrid model, which combines BERT with lexicon-based techniques, achieves an accuracy of 95% for most state-of-the-art datasets. Our work has significant implications for creating safer online environments by improving the detection of toxic content and proactively updating the lexicon. Content Warning: This paper contains examples of hate speech that may be triggering.

### An Open-Source Web-Based Tool for Evaluating Open-Source Large Language Models Leveraging Information Retrieval from Custom Documents 
[[arxiv](https://arxiv.org/abs/2502.10916)] [[cool](https://papers.cool/arxiv/2502.10916)] [[pdf](https://arxiv.org/pdf/2502.10916)]
> **Authors**: Godfrey I
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: 19 pages, 1 figure, 6 tables
- **标题**: None
- **领域**: 计算语言学,信息检索
- **Abstract**: In our work, we present the first-of-its-kind open-source web-based tool which is able to demonstrate the impacts of a user's speech act during discourse with conversational agents, which leverages open-source large language models. With this software resource, it is possible for researchers and experts to evaluate the performance of various dialogues, visualize the user's communicative intents, and utilise uploaded specific documents for the chat agent to use for its information retrieval to respond to the user query. The context gathered by these models is obtained from a set of linguistic features extracted, which forms the context embeddings of the models. Regardless of these models showing good context understanding based on these features, there still remains a gap in including deeper pragmatic features to improve the model's comprehension of the query, hence the efforts to develop this web resource, which is able to extract and then inject this overlooked feature in the encoder-decoder pipeline of the conversational agent. To demonstrate the effect and impact of the resource, we carried out an experiment which evaluated the system using 2 knowledge files for information retrieval, with two user queries each, across 5 open-source large language models using 10 standard metrics. Our results showed that larger open-source models, demonstrated an improved alignment when the user speech act was included with their query. The smaller models in contrast showed an increased perplexity and mixed performance, which explicitly indicated struggles in processing queries that explicitly included speech acts. The results from the analysis using the developed web resource highlight the potential of speech acts towards enhancing conversational depths while underscoring the need for model-specific optimizations to address increased computational costs and response times.

### Developing Conversational Speech Systems for Robots to Detect Speech Biomarkers of Cognition in People Living with Dementia 
[[arxiv](https://arxiv.org/abs/2502.10896)] [[cool](https://papers.cool/arxiv/2502.10896)] [[pdf](https://arxiv.org/pdf/2502.10896)]
> **Authors**: Rohith Perumandla,Young-Ho Bae,Diego Izaguirre,Esther Hwang,Andrew Murphy,Long-Jing Hsu,Selma Sabanovic,Casey C. Bennett
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: Main paper 28 pages long (pg 2-30), includes 5 figures, 5 tables, 1 Appendix at end
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: This study presents the development and testing of a conversational speech system designed for robots to detect speech biomarkers indicative of cognitive impairments in people living with dementia (PLwD). The system integrates a backend Python WebSocket server and a central core module with a large language model (LLM) fine-tuned for dementia to process user input and generate robotic conversation responses in real-time in less than 1.5 seconds. The frontend user interface, a Progressive Web App (PWA), displays information and biomarker score graphs on a smartphone in real-time to human users (PLwD, caregivers, clinicians). Six speech biomarkers based on the existing literature - Altered Grammar, Pragmatic Impairments, Anomia, Disrupted Turn-Taking, Slurred Pronunciation, and Prosody Changes - were developed for the robot conversation system using two datasets, one that included conversations of PLwD with a human clinician (DementiaBank dataset) and one that included conversations of PLwD with a robot (Indiana dataset). We also created a composite speech biomarker that combined all six individual biomarkers into a single score. The speech system's performance was first evaluated on the DementiaBank dataset showing moderate correlation with MMSE scores, with the composite biomarker score outperforming individual biomarkers. Analysis of the Indiana dataset revealed higher and more variable biomarker scores, suggesting potential differences due to study populations (e.g. severity of dementia) and the conversational scenario (human-robot conversations are different from human-human). The findings underscore the need for further research on the impact of conversational scenarios on speech biomarkers and the potential clinical applications of robotic speech systems.

### MET-Bench: Multimodal Entity Tracking for Evaluating the Limitations of Vision-Language and Reasoning Models 
[[arxiv](https://arxiv.org/abs/2502.10886)] [[cool](https://papers.cool/arxiv/2502.10886)] [[pdf](https://arxiv.org/pdf/2502.10886)]
> **Authors**: Vanya Cohen,Raymond Mooney
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Entity tracking is a fundamental challenge in natural language understanding, requiring models to maintain coherent representations of entities. Previous work has benchmarked entity tracking performance in purely text-based tasks. We introduce MET-Bench, a multimodal entity tracking benchmark designed to evaluate the ability of vision-language models to track entity states across modalities. Using two structured domains, Chess and the Shell Game, we assess how effectively current models integrate textual and image-based state updates. Our findings reveal a significant performance gap between text-based and image-based tracking and that this performance gap stems from deficits in visual reasoning rather than perception. We further show that explicit text-based reasoning strategies improve performance, yet substantial limitations remain, especially in long-horizon multimodal scenarios. Our results highlight the need for improved multimodal representations and reasoning techniques to bridge the gap between textual and visual entity tracking.

### CiteCheck: Towards Accurate Citation Faithfulness Detection 
[[arxiv](https://arxiv.org/abs/2502.10881)] [[cool](https://papers.cool/arxiv/2502.10881)] [[pdf](https://arxiv.org/pdf/2502.10881)]
> **Authors**: Ziyao Xu,Shaohang Wei,Zhuoheng Han,Jing Jin,Zhe Yang,Xiaoguang Li,Haochen Tan,Zhijiang Guo,Houfeng Wang
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Citation faithfulness detection is critical for enhancing retrieval-augmented generation (RAG) systems, yet large-scale Chinese datasets for this task are scarce. Existing methods face prohibitive costs due to the need for manually annotated negative samples. To address this, we introduce the first large-scale Chinese dataset CiteCheck for citation faithfulness detection, constructed via a cost-effective approach using two-stage manual annotation. This method balances positive and negative samples while significantly reducing annotation expenses. CiteCheck comprises training and test splits. Experiments demonstrate that: (1) the test samples are highly challenging, with even state-of-the-art LLMs failing to achieve high accuracy; and (2) training data augmented with LLM-generated negative samples enables smaller models to attain strong performance using parameter-efficient fine-tuning. CiteCheck provides a robust foundation for advancing citation faithfulness detection in Chinese RAG systems. The dataset is publicly available to facilitate research.

### The Representation and Recall of Interwoven Structured Knowledge in LLMs: A Geometric and Layered Analysis 
[[arxiv](https://arxiv.org/abs/2502.10871)] [[cool](https://papers.cool/arxiv/2502.10871)] [[pdf](https://arxiv.org/pdf/2502.10871)]
> **Authors**: Ge Lei,Samuel J. Cooper
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: This study investigates how large language models (LLMs) represent and recall multi-associated attributes across transformer layers. We show that intermediate layers encode factual knowledge by superimposing related attributes in overlapping spaces, along with effective recall even when attributes are not explicitly prompted. In contrast, later layers refine linguistic patterns and progressively separate attribute representations, optimizing task-specific outputs while appropriately narrowing attribute recall. We identify diverse encoding patterns including, for the first time, the observation of 3D spiral structures when exploring information related to the periodic table of elements. Our findings reveal a dynamic transition in attribute representations across layers, contributing to mechanistic interpretability and providing insights for understanding how LLMs handle complex, interrelated knowledge.

### NitiBench: A Comprehensive Studies of LLM Frameworks Capabilities for Thai Legal Question Answering 
[[arxiv](https://arxiv.org/abs/2502.10868)] [[cool](https://papers.cool/arxiv/2502.10868)] [[pdf](https://arxiv.org/pdf/2502.10868)]
> **Authors**: Pawitsapak Akarajaradwong,Pirat Pothavorn,Chompakorn Chaksangchaichot,Panuthep Tasawong,Thitiwat Nopparatbundit,Sarana Nutanong
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The application of large language models (LLMs) in the legal domain holds significant potential for information retrieval and question answering, yet Thai legal QA systems face challenges due to a lack of standardized evaluation benchmarks and the complexity of Thai legal structures. This paper introduces NitiBench, a benchmark comprising two datasets: the NitiBench-CCL, covering general Thai financial law, and the NitiBench-Tax, which includes real-world tax law cases requiring advanced legal reasoning. We evaluate retrieval-augmented generation (RAG) and long-context LLM-based approaches to address three key research questions: the impact of domain-specific components like section-based chunking and cross-referencing, the comparative performance of different retrievers and LLMs, and the viability of long-context LLMs as an alternative to RAG. Our results show that section-based chunking significantly improves retrieval and end-to-end performance, current retrievers struggle with complex queries, and long-context LLMs still underperform RAG-based systems in Thai legal QA. To support fair evaluation, we propose tailored multi-label retrieval metrics and the use of an LLM-as-judge for coverage and contradiction detection method. These findings highlight the limitations of current Thai legal NLP solutions and provide a foundation for future research in the field. We also open-sourced our codes and dataset to available publicly.

### Divergent Thoughts toward One Goal: LLM-based Multi-Agent Collaboration System for Electronic Design Automation 
[[arxiv](https://arxiv.org/abs/2502.10857)] [[cool](https://papers.cool/arxiv/2502.10857)] [[pdf](https://arxiv.org/pdf/2502.10857)]
> **Authors**: Haoyuan Wu,Haisheng Zheng,Zhuolun He,Bei Yu
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Recently, with the development of tool-calling capabilities in large language models (LLMs), these models have demonstrated significant potential for automating electronic design automation (EDA) flows by interacting with EDA tool APIs via EDA scripts. However, considering the limited understanding of EDA tools, LLMs face challenges in practical scenarios where diverse interfaces of EDA tools exist across different platforms. Additionally, EDA flow automation often involves intricate, long-chain tool-calling processes, increasing the likelihood of errors in intermediate steps. Any errors will lead to the instability and failure of EDA flow automation. To address these challenges, we introduce EDAid, a multi-agent collaboration system where multiple agents harboring divergent thoughts converge towards a common goal, ensuring reliable and successful EDA flow automation. Specifically, each agent is controlled by ChipLlama models, which are expert LLMs fine-tuned for EDA flow automation. Our experiments demonstrate the state-of-the-art (SOTA) performance of our ChipLlama models and validate the effectiveness of our EDAid in the automation of complex EDA flows, showcasing superior performance compared to single-agent systems.

### Towards Effective Extraction and Evaluation of Factual Claims 
[[arxiv](https://arxiv.org/abs/2502.10855)] [[cool](https://papers.cool/arxiv/2502.10855)] [[pdf](https://arxiv.org/pdf/2502.10855)]
> **Authors**: Dasha Metropolitansky,Jonathan Larson
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: A common strategy for fact-checking long-form content generated by Large Language Models (LLMs) is extracting simple claims that can be verified independently. Since inaccurate or incomplete claims compromise fact-checking results, ensuring claim quality is critical. However, the lack of a standardized evaluation framework impedes assessment and comparison of claim extraction methods. To address this gap, we propose a framework for evaluating claim extraction in the context of fact-checking along with automated, scalable, and replicable methods for applying this framework, including novel approaches for measuring coverage and decontextualization. We also introduce Claimify, an LLM-based claim extraction method, and demonstrate that it outperforms existing methods under our evaluation framework. A key feature of Claimify is its ability to handle ambiguity and extract claims only when there is high confidence in the correct interpretation of the source text.

### Multilingual Encoder Knows more than You Realize: Shared Weights Pretraining for Extremely Low-Resource Languages 
[[arxiv](https://arxiv.org/abs/2502.10852)] [[cool](https://papers.cool/arxiv/2502.10852)] [[pdf](https://arxiv.org/pdf/2502.10852)]
> **Authors**: Zeli Su,Ziyin Zhang,Guixian Xu,Jianing Liu,XU Han,Ting Zhang,Yushuang Dong
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: While multilingual language models like XLM-R have advanced multilingualism in NLP, they still perform poorly in extremely low-resource languages. This situation is exacerbated by the fact that modern LLMs such as LLaMA and Qwen support far fewer languages than XLM-R, making text generation models non-existent for many languages in the world. To tackle this challenge, we propose a novel framework for adapting multilingual encoders to text generation in extremely low-resource languages. By reusing the weights between the encoder and the decoder, our framework allows the model to leverage the learned semantic space of the encoder, enabling efficient learning and effective generalization in low-resource languages. Applying this framework to four Chinese minority languages, we present XLM-SWCM, and demonstrate its superior performance on various downstream tasks even when compared with much larger models.

### Back Attention: Understanding and Enhancing Multi-Hop Reasoning in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.10835)] [[cool](https://papers.cool/arxiv/2502.10835)] [[pdf](https://arxiv.org/pdf/2502.10835)]
> **Authors**: Zeping Yu,Yonatan Belinkov,Sophia Ananiadou
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: preprint
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: We investigate how large language models perform latent multi-hop reasoning in prompts like "Wolfgang Amadeus Mozart's mother's spouse is". To analyze this process, we introduce logit flow, an interpretability method that traces how logits propagate across layers and positions toward the final prediction. Using logit flow, we identify four distinct stages in single-hop knowledge prediction: (A) entity subject enrichment, (B) entity attribute extraction, (C) relation subject enrichment, and (D) relation attribute extraction. Extending this analysis to multi-hop reasoning, we find that failures often stem from the relation attribute extraction stage, where conflicting logits reduce prediction accuracy. To address this, we propose back attention, a novel mechanism that enables lower layers to leverage higher-layer hidden states from different positions during attention computation. With back attention, a 1-layer transformer achieves the performance of a 2-layer transformer. Applied to four LLMs, back attention improves accuracy on five reasoning datasets, demonstrating its effectiveness in enhancing latent multi-hop reasoning ability.

### Why is prompting hard? Understanding prompts on binary sequence predictors 
[[arxiv](https://arxiv.org/abs/2502.10760)] [[cool](https://papers.cool/arxiv/2502.10760)] [[pdf](https://arxiv.org/pdf/2502.10760)]
> **Authors**: Li Kevin Wenliang,Anian Ruoss,Jordi Grau-Moya,Marcus Hutter,Tim Genewein
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习,机器学习
- **Abstract**: Large language models (LLMs) can be prompted to do many tasks, but finding good prompts is not always easy, nor is understanding some performant prompts. We explore these issues by viewing prompting as conditioning a near-optimal sequence predictor (LLM) pretrained on diverse data sources. Through numerous prompt search experiments, we show that the unintuitive patterns in optimal prompts can be better understood given the pretraining distribution, which is often unavailable in practice. Moreover, even using exhaustive search, reliably identifying optimal prompts from practical neural predictors can be difficult. Further, we demonstrate that common prompting methods, such as using intuitive prompts or samples from the targeted task, are in fact suboptimal. Thus, this work takes an initial step towards understanding the difficulties in finding and understanding optimal prompts from a statistical and empirical perspective.

### LoRE-Merging: Exploring Low-Rank Estimation For Large Language Model Merging 
[[arxiv](https://arxiv.org/abs/2502.10749)] [[cool](https://papers.cool/arxiv/2502.10749)] [[pdf](https://arxiv.org/pdf/2502.10749)]
> **Authors**: Zehua Liu,Han Wu,Yuxuan Yao,Ruifeng She,Xiongwei Han,Tao Zhong,Mingxuan Yuan
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: While most current approaches rely on further training techniques, such as fine-tuning or reinforcement learning, to enhance model capacities, model merging stands out for its ability of improving models without requiring any additional training. In this paper, we propose a unified framework for model merging based on low-rank estimation of task vectors without the need for access to the base model, named \textsc{LoRE-Merging}. Our approach is motivated by the observation that task vectors from fine-tuned models frequently exhibit a limited number of dominant singular values, making low-rank estimations less prone to interference. We implement the method by formulating the merging problem as an optimization problem. Extensive empirical experiments demonstrate the effectiveness of our framework in mitigating interference and preserving task-specific information, thereby advancing the state-of-the-art performance in model merging techniques.

### 1bit-Merging: Dynamic Quantized Merging for Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.10743)] [[cool](https://papers.cool/arxiv/2502.10743)] [[pdf](https://arxiv.org/pdf/2502.10743)]
> **Authors**: Shuqi Liu,Han Wu,Bowei He,Zehua Liu,Xiongwei Han,Mingxuan Yuan,Linqi Song
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Recent advances in large language models have led to specialized models excelling in specific domains, creating a need for efficient model merging techniques. While traditional merging approaches combine parameters into a single static model, they often compromise task-specific performance. However, task-specific routing methods maintain accuracy but introduce substantial storage overhead. We present \texttt{1bit}-Merging, a novel framework that integrates task-specific routing with 1-bit quantized task vectors to balance performance and storage efficiency. Our approach leverages the observation that different task-specific models store knowledge in distinct layers-chat models primarily in attention layers and math/code models in MLP layers-enabling targeted compression strategies. Through extensive experiments with LLaMA2 and Mistral model families across chat, mathematical reasoning, and code generation tasks, we demonstrate that \texttt{1bit}-Merging achieves comparable or superior performance to existing methods while significantly reducing storage requirements. Our framework offers a practical solution for combining specialized models while maintaining their individual strengths and addressing the storage challenges of current approaches.

### BASE-SQL: A powerful open source Text-To-SQL baseline approach 
[[arxiv](https://arxiv.org/abs/2502.10739)] [[cool](https://papers.cool/arxiv/2502.10739)] [[pdf](https://arxiv.org/pdf/2502.10739)]
> **Authors**: Lei Sheng,Shuai-Shuai Xu,Wei Xie
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: Work in progress. 16 pages, 3 figures, 8 tables
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The conversion of natural language into SQL language for querying databases (Text-to-SQL) has broad application prospects and has attracted widespread attention. At present, the mainstream Text-to-SQL methods are mainly divided into in-context learning (ICL) based methods and supervised fine-tuning (SFT) based methods. ICL-based methods can achieve relatively good results thanks to the use of the most advanced closed-source models. However, in real-world application scenarios, factors such as data privacy, SQL generation efficiency and cost need to be considered. SFT-based methods have certain advantages. At present, methods based on fine-tuning of open source models lack easy-to-implement and effective (cost-effective) baseline methods. We propose a pipeline-based method using open source model fine-tuning, referred to as BASE-SQL, which includes four components: Schema Linking, Candidate SQL Generate, SQL Revision and SQL Merge Revision. Experimental results show that BASE-SQL uses the open source model Qwen2.5-Coder-32B-Instruct, and achieves an accuracy of 67.47% on the BIRD development set and 88.9% on the Spider test set, which is significantly better than other methods using open source models, and even exceeds several methods using the GPT-4o closed-source model. At the same time, BASE-SQL is easy to implement and highly efficient (on average, only five calls to the large language model are required to generate SQL once). The code will be open sourced at https://github.com/CycloneBoy/base_sql.

### OPTISHEAR: Towards Efficient and Adaptive Pruning of Large Language Models via Evolutionary Optimization 
[[arxiv](https://arxiv.org/abs/2502.10735)] [[cool](https://papers.cool/arxiv/2502.10735)] [[pdf](https://arxiv.org/pdf/2502.10735)]
> **Authors**: Shuqi Liu,Bowei He,Han Wu,Linqi Song
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Post-training pruning has emerged as a crucial optimization technique as large language models (LLMs) continue to grow rapidly. However, the significant variations in weight distributions across different LLMs make fixed pruning strategies inadequate for multiple models. In this paper, we introduce \textbf{\textsc{OptiShear}}, an efficient evolutionary optimization framework for adaptive LLM pruning. Our framework features two key innovations: an effective search space built on our Meta pruning metric to handle diverse weight distributions, and a model-wise reconstruction error for rapid evaluation during search trials. We employ Non-dominated Sorting Genetic Algorithm III (NSGA-III) to optimize both pruning metrics and layerwise sparsity ratios. Through extensive evaluation on LLaMA-1/2/3 and Mistral models (7B-70B) across multiple benchmarks, we demonstrate that our adaptive pruning metrics consistently outperform existing methods. Additionally, our discovered layerwise sparsity ratios enhance the effectiveness of other pruning metrics. The framework exhibits strong cross-task and cross-model generalizability, providing a cost-effective solution for model compression.

### PropNet: a White-Box and Human-Like Network for Sentence Representation 
[[arxiv](https://arxiv.org/abs/2502.10725)] [[cool](https://papers.cool/arxiv/2502.10725)] [[pdf](https://arxiv.org/pdf/2502.10725)]
> **Authors**: Fei Yang
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Transformer-based embedding methods have dominated the field of sentence representation in recent years. Although they have achieved remarkable performance on NLP missions, such as semantic textual similarity (STS) tasks, their black-box nature and large-data-driven training style have raised concerns, including issues related to bias, trust, and safety. Many efforts have been made to improve the interpretability of embedding models, but these problems have not been fundamentally resolved. To achieve inherent interpretability, we propose a purely white-box and human-like sentence representation network, PropNet. Inspired by findings from cognitive science, PropNet constructs a hierarchical network based on the propositions contained in a sentence. While experiments indicate that PropNet has a significant gap compared to state-of-the-art (SOTA) embedding models in STS tasks, case studies reveal substantial room for improvement. Additionally, PropNet enables us to analyze and understand the human cognitive processes underlying STS benchmarks.

### An Empirical Analysis of Uncertainty in Large Language Model Evaluations 
[[arxiv](https://arxiv.org/abs/2502.10709)] [[cool](https://papers.cool/arxiv/2502.10709)] [[pdf](https://arxiv.org/pdf/2502.10709)]
> **Authors**: Qiujie Xie,Qingqiu Li,Zhuohao Yu,Yuejie Zhang,Yue Zhang,Linyi Yang
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: ICLR 2025
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: As LLM-as-a-Judge emerges as a new paradigm for assessing large language models (LLMs), concerns have been raised regarding the alignment, bias, and stability of LLM evaluators. While substantial work has focused on alignment and bias, little research has concentrated on the stability of LLM evaluators. In this paper, we conduct extensive experiments involving 9 widely used LLM evaluators across 2 different evaluation settings to investigate the uncertainty in model-based LLM evaluations. We pinpoint that LLM evaluators exhibit varying uncertainty based on model families and sizes. With careful comparative analyses, we find that employing special prompting strategies, whether during inference or post-training, can alleviate evaluation uncertainty to some extent. By utilizing uncertainty to enhance LLM's reliability and detection capability in Out-Of-Distribution (OOD) data, we further fine-tune an uncertainty-aware LLM evaluator named ConfiLM using a human-annotated fine-tuning set and assess ConfiLM's OOD evaluation ability on a manually designed test set sourced from the 2024 Olympics. Experimental results demonstrate that incorporating uncertainty as additional information during the fine-tuning phase can largely improve the model's evaluation performance in OOD scenarios. The code and data are released at: https://github.com/hasakiXie123/LLM-Evaluator-Uncertainty.

### Injecting Domain-Specific Knowledge into Large Language Models: A Comprehensive Survey 
[[arxiv](https://arxiv.org/abs/2502.10708)] [[cool](https://papers.cool/arxiv/2502.10708)] [[pdf](https://arxiv.org/pdf/2502.10708)]
> **Authors**: Zirui Song,Bin Yan,Yuhan Liu,Miao Fang,Mingzhe Li,Rui Yan,Xiuying Chen
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: In processing
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models (LLMs) have demonstrated remarkable success in various tasks such as natural language understanding, text summarization, and machine translation. However, their general-purpose nature often limits their effectiveness in domain-specific applications that require specialized knowledge, such as healthcare, chemistry, or legal analysis. To address this, researchers have explored diverse methods to enhance LLMs by integrating domain-specific knowledge. In this survey, we provide a comprehensive overview of these methods, which we categorize into four key approaches: dynamic knowledge injection, static knowledge embedding, modular adapters, and prompt optimization. Each approach offers unique mechanisms to equip LLMs with domain expertise, balancing trade-offs between flexibility, scalability, and efficiency. We discuss how these methods enable LLMs to tackle specialized tasks, compare their advantages and disadvantages, evaluate domain-specific LLMs against general LLMs, and highlight the challenges and opportunities in this emerging field. For those interested in delving deeper into this area, we also summarize the commonly used datasets and benchmarks. To keep researchers updated on the latest studies, we maintain an open-source at: https://github.com/abilliyb/Knowledge_Injection_Survey_Papers, dedicated to documenting research in the field of specialized LLM.

### Exploring Synaptic Resonance in Large Language Models: A Novel Approach to Contextual Memory Integration 
[[arxiv](https://arxiv.org/abs/2502.10699)] [[cool](https://papers.cool/arxiv/2502.10699)] [[pdf](https://arxiv.org/pdf/2502.10699)]
> **Authors**: George Applegarth,Christian Weatherstone,Maximilian Hollingsworth,Henry Middlebrook,Marcus Irvin
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,神经和进化计算
- **Abstract**: Contextual memory integration remains a high challenge in the development of language models, particularly in tasks that require maintaining coherence over extended sequences. Traditional approaches, such as self-attention mechanisms and memory-augmented architectures, often prioritize short-term dependencies, leading to fragmentation and inconsistency in long-range contextual understanding. Inspired by principles of synaptic plasticity observed in biological neural systems, a novel mechanism, Synaptic Resonance, is introduced to dynamically reinforce relevant memory pathways during training and inference. Unlike static memory representations, this mechanism continuously adjusts synaptic weight matrices based on contextual relevance, allowing for improved information retention without excessive computational overhead. Evaluations conducted on an open-source language model demonstrate reductions in perplexity, enhancements in contextual coherence, and increased robustness against input noise, highlighting the effectiveness of reinforcement-driven memory modulation. Comparative analysis against baseline models further reveals that the proposed approach achieves higher memory retention efficiency while maintaining computational feasibility. The architectural modifications integrate seamlessly into existing transformer-based frameworks, ensuring stable convergence and efficient inference without sacrificing scalability. Applications benefiting from improved long-term contextual consistency, such as dialogue systems and document summarization, stand to gain from this approach. Empirical findings suggest that dynamically reinforced memory pathways offer a promising alternative to conventional memory mechanisms, addressing longstanding limitations in extended sequence modeling.

### User Profile with Large Language Models: Construction, Updating, and Benchmarking 
[[arxiv](https://arxiv.org/abs/2502.10660)] [[cool](https://papers.cool/arxiv/2502.10660)] [[pdf](https://arxiv.org/pdf/2502.10660)]
> **Authors**: Nusrat Jahan Prottasha,Md Kowsher,Hafijur Raman,Israt Jahan Anny,Prakash Bhat,Ivan Garibay,Ozlem Garibay
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: User profile modeling plays a key role in personalized systems, as it requires building accurate profiles and updating them with new information. In this paper, we present two high-quality open-source user profile datasets: one for profile construction and another for profile updating. These datasets offer a strong basis for evaluating user profile modeling techniques in dynamic settings. We also show a methodology that uses large language models (LLMs) to tackle both profile construction and updating. Our method uses a probabilistic framework to predict user profiles from input text, allowing for precise and context-aware profile generation. Our experiments demonstrate that models like Mistral-7b and Llama2-7b perform strongly in both tasks. LLMs improve the precision and recall of the generated profiles, and high evaluation scores confirm the effectiveness of our approach.

### BabyLM Turns 3: Call for papers for the 2025 BabyLM workshop 
[[arxiv](https://arxiv.org/abs/2502.10645)] [[cool](https://papers.cool/arxiv/2502.10645)] [[pdf](https://arxiv.org/pdf/2502.10645)]
> **Authors**: Lucas Charpentier,Leshem Choshen,Ryan Cotterell,Mustafa Omer Gul,Michael Hu,Jaap Jumelet,Tal Linzen,Jing Liu,Aaron Mueller,Candace Ross,Raj Sanjay Shah,Alex Warstadt,Ethan Wilcox,Adina Williams
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: EMNLP 2025 BabyLM Workshop. arXiv admin note: text overlap with arXiv:2404.06214
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: BabyLM aims to dissolve the boundaries between cognitive modeling and language modeling. We call for both workshop papers and for researchers to join the 3rd BabyLM competition. As in previous years, we call for participants in the data-efficient pretraining challenge in the general track. This year, we also offer a new track: INTERACTION. This new track encourages interactive behavior, learning from a teacher, and adapting the teaching material to the student. We also call for papers outside the competition in any relevant areas. These include training efficiency, cognitively plausible research, weak model evaluation, and more.

### Lost in the Passage: Passage-level In-context Learning Does Not Necessarily Need a "Passage" 
[[arxiv](https://arxiv.org/abs/2502.10634)] [[cool](https://papers.cool/arxiv/2502.10634)] [[pdf](https://arxiv.org/pdf/2502.10634)]
> **Authors**: Hao Sun,Chenming Tang,Gengyang Li,Yunfang Wu
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: By simply incorporating demonstrations into the context, in-context learning (ICL) enables large language models (LLMs) to yield awesome performance on many tasks. In this paper, we focus on passage-level long-context ICL for generation tasks and find that LLMs cannot learn the intrinsic relationships between the demonstration passage and the generation output. We conduct experiments with different LLMs on two typical generation tasks including single-document QA and distractor generation, demonstrating that even a completely meaningless demonstration passage with 1/4 length achieves much better performance than the original full passage. Analysis via attention score reveals that LLMs pay little attention to passages compared to other components in prompt and little attention flows from the passage to other parts of the demonstration, which further confirms our finding. Additionally, experiments on context compression indicate that compression approaches proven effective on other long-context tasks are not suitable for passage-level ICL, since simply using shorter meaningless demonstration passages has achieved competitive performance.

### Code-Mixed Telugu-English Hate Speech Detection 
[[arxiv](https://arxiv.org/abs/2502.10632)] [[cool](https://papers.cool/arxiv/2502.10632)] [[pdf](https://arxiv.org/pdf/2502.10632)]
> **Authors**: Santhosh Kakarla,Gautama Shastry Bulusu Venkata
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 4 pages, 1 figure, 2 tables
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Hate speech detection in low-resource languages like Telugu is a growing challenge in NLP. This study investigates transformer-based models, including TeluguHateBERT, HateBERT, DeBERTa, Muril, IndicBERT, Roberta, and Hindi-Abusive-MuRIL, for classifying hate speech in Telugu. We fine-tune these models using Low-Rank Adaptation (LoRA) to optimize efficiency and performance. Additionally, we explore a multilingual approach by translating Telugu text into English using Google Translate to assess its impact on classification accuracy. Our experiments reveal that most models show improved performance after translation, with DeBERTa and Hindi-Abusive-MuRIL achieving higher accuracy and F1 scores compared to training directly on Telugu text. Notably, Hindi-Abusive-MuRIL outperforms all other models in both the original Telugu dataset and the translated dataset, demonstrating its robustness across different linguistic settings. This suggests that translation enables models to leverage richer linguistic features available in English, leading to improved classification performance. The results indicate that multilingual processing can be an effective approach for hate speech detection in low-resource languages. These findings demonstrate that transformer models, when fine-tuned appropriately, can significantly improve hate speech detection in Telugu, paving the way for more robust multilingual NLP applications.

### Retrieval-augmented Encoders for Extreme Multi-label Text Classification 
[[arxiv](https://arxiv.org/abs/2502.10615)] [[cool](https://papers.cool/arxiv/2502.10615)] [[pdf](https://arxiv.org/pdf/2502.10615)]
> **Authors**: Yau-Shian Wang,Wei-Cheng Chang,Jyun-Yu Jiang,Jiong Zhang,Hsiang-Fu Yu,S. V. N. Vishwanathan
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Extreme multi-label classification (XMC) seeks to find relevant labels from an extremely large label collection for a given text input. To tackle such a vast label space, current state-of-the-art methods fall into two categories. The one-versus-all (OVA) method uses learnable label embeddings for each label, excelling at memorization (i.e., capturing detailed training signals for accurate head label prediction). In contrast, the dual-encoder (DE) model maps input and label text into a shared embedding space for better generalization (i.e., the capability of predicting tail labels with limited training data), but may fall short at memorization. To achieve generalization and memorization, existing XMC methods often combine DE and OVA models, which involves complex training pipelines. Inspired by the success of retrieval-augmented language models, we propose the Retrieval-augmented Encoders for XMC (RAEXMC), a novel framework that equips a DE model with retrieval-augmented capability for efficient memorization without additional trainable parameter. During training, RAEXMC is optimized by the contrastive loss over a knowledge memory that consists of both input instances and labels. During inference, given a test input, RAEXMC retrieves the top-$K$ keys from the knowledge memory, and aggregates the corresponding values as the prediction scores. We showcase the effectiveness and efficiency of RAEXMC on four public LF-XMC benchmarks. RAEXMC not only advances the state-of-the-art (SOTA) DE method DEXML, but also achieves more than 10x speedup on the largest LF-AmazonTitles-1.3M dataset under the same 8 A100 GPUs training environments.

### Post-training an LLM for RAG? Train on Self-Generated Demonstrations 
[[arxiv](https://arxiv.org/abs/2502.10596)] [[cool](https://papers.cool/arxiv/2502.10596)] [[pdf](https://arxiv.org/pdf/2502.10596)]
> **Authors**: Matthew Finlayson,Ilia Kulikov,Daneil M. Bikel,Barlas Oguz,Xilun Chen,Aasish Pappu
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Large language models (LLMs) often struggle with knowledge intensive NLP tasks, such as answering "Who won the latest World Cup?" because the knowledge they learn during training may be insufficient or outdated. Conditioning generation on retrieved documents -- a technique known as retrieval augmented generation (RAG) -- mitigates these shortcomings by allowing the model to leverage in-context information. Practitioners can improve LLM RAG performance by fine-tuning on retrieval-augmented instructions, but must beware that this can cause undesirable model behaviors like hallucinations. We attribute this degradation to the fact that the training data is likely to be out-of-distribution for the model and may suffer from quality issues, such as misalignment between retrievals and target responses (since retrievals are frequently added post-hoc). We propose a recipe for training RAG-enabled LLMs using self-generated demonstrations, thereby avoiding training on out-of-distribution text and integrating retrievals into the LLM responses. We evaluate our method on knowledge intensive question answering (QA) tasks and show that our method teaches LLMs to properly handle in-context retrievals and abstain from questions it will likely get wrong. Compared to conventional RA-IT methods, our method prevents model degradation in non-RAG settings while exhibiting superior QA performance.

### Named entity recognition for Serbian legal documents: Design, methodology and dataset development 
[[arxiv](https://arxiv.org/abs/2502.10582)] [[cool](https://papers.cool/arxiv/2502.10582)] [[pdf](https://arxiv.org/pdf/2502.10582)]
> **Authors**: Vladimir Kalušev,Branko Brkljač
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 9 pages, 6 figures, 1 table, associated NER4Legal_SRBmodeland dataset are available at https://huggingface.co/kalusev/NER4Legal_SRB , paper submitted to 15th International Conference on Information Society and Technology (ICIST), Kopaonik, Serbia, 9-12 March 2025, conference track: GenerativeAIand LargeLanguageModels
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Recent advancements in the field of natural language processing (NLP) and especially large language models (LLMs) and their numerous applications have brought research attention to design of different document processing tools and enhancements in the process of document archiving, search and retrieval. Domain of official, legal documents is especially interesting due to vast amount of data generated on the daily basis, as well as the significant community of interested practitioners (lawyers, law offices, administrative workers, state institutions and citizens). Providing efficient ways for automation of everyday work involving legal documents is therefore expected to have significant impact in different fields. In this work we present one LLM based solution for Named Entity Recognition (NER) in the case of legal documents written in Serbian language. It leverages on the pre-trained bidirectional encoder representations from transformers (BERT), which had been carefully adapted to the specific task of identifying and classifying specific data points from textual content. Besides novel dataset development for Serbian language (involving public court rulings), presented system design and applied methodology, the paper also discusses achieved performance metrics and their implications for objective assessment of the proposed solution. Performed cross-validation tests on the created manually labeled dataset with mean $F_1$ score of 0.96 and additional results on the examples of intentionally modified text inputs confirm applicability of the proposed system design and robustness of the developed NER solution.

### Man Made Language Models? Evaluating LLMs' Perpetuation of Masculine Generics Bias 
[[arxiv](https://arxiv.org/abs/2502.10577)] [[cool](https://papers.cool/arxiv/2502.10577)] [[pdf](https://arxiv.org/pdf/2502.10577)]
> **Authors**: Enzo Doyen,Amalia Todirascu
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 21 pages, 5 figures
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large language models (LLMs) have been shown to propagate and even amplify gender bias, in English and other languages, in specific or constrained contexts. However, no studies so far have focused on gender biases conveyed by LLMs' responses to generic instructions, especially with regard to masculine generics (MG). MG are a linguistic feature found in many gender-marked languages, denoting the use of the masculine gender as a "default" or supposedly neutral gender to refer to mixed group of men and women, or of a person whose gender is irrelevant or unknown. Numerous psycholinguistics studies have shown that MG are not neutral and induce gender bias. This work aims to analyze the use of MG by both proprietary and local LLMs in responses to generic instructions and evaluate their MG bias rate. We focus on French and create a human noun database from existing lexical resources. We filter existing French instruction datasets to retrieve generic instructions and analyze the responses of 6 different LLMs. Overall, we find that $\approx$39.5\% of LLMs' responses to generic instructions are MG-biased ($\approx$73.1\% across responses with human nouns). Our findings also reveal that LLMs are reluctant to using gender-fair language spontaneously.

### Hallucinations and Truth: A Comprehensive Accuracy Evaluation of RAG, LoRA and DoRA 
[[arxiv](https://arxiv.org/abs/2502.10497)] [[cool](https://papers.cool/arxiv/2502.10497)] [[pdf](https://arxiv.org/pdf/2502.10497)]
> **Authors**: Mohammad Baqar,Rajat Khanda
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 10 Pages
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Recent advancements in Generative AI have significantly improved the efficiency and adaptability of natural language processing (NLP) systems, particularly through Retrieval-Augmented Generation (RAG), Low-Rank Adaptation (LoRA), and Weight-Decomposed Low-Rank Adaptation (DoRA). RAG integrates external knowledge to enhance factual consistency in generative outputs, while LoRA enables parameter-efficient fine-tuning of large language models (LLMs). DoRA further refines this process by optimizing fine-tuning through adaptive parameter ranking and domain-aware weight adjustments, improving learning efficiency while maintaining inference performance. This paper presents a large-scale empirical evaluation of RAG, LoRA, and DoRA, with model fine-tuning and generation performance assessed on 20,000 FAQ-based queries, while the knowledge base spans 400,000 entries. The study analyzes key performance metrics such as accuracy, relevance, and inference latency. Experimental results demonstrate that DoRA achieves the highest accuracy (90.1%), relevance score (0.88), and lowest latency (110 ms per query), outperforming both LoRA and RAG in real-world, domain-specific generative AI applications. Furthermore, this study examines the trade-offs between fine-tuning efficiency, computational cost, and real-time adaptability across different models. Findings highlight RAG's effectiveness in knowledge grounding, LoRA's cost-efficient domain adaptation, and DoRA's ability to balance fine-tuning efficiency with model precision. These insights provide practical guidance for deploying AI-driven generative systems in accuracy-critical domains such as healthcare, finance, and legal services, ensuring scalability, reliability, and optimal performance in dynamic environments.

### MM-RLHF: The Next Step Forward in Multimodal LLM Alignment 
[[arxiv](https://arxiv.org/abs/2502.10391)] [[cool](https://papers.cool/arxiv/2502.10391)] [[pdf](https://arxiv.org/pdf/2502.10391)]
> **Authors**: Yi-Fan Zhang,Tao Yu,Haochen Tian,Chaoyou Fu,Peiyan Li,Jianshu Zeng,Wulin Xie,Yang Shi,Huanyu Zhang,Junkang Wu,Xue Wang,Yibo Hu,Bin Wen,Fan Yang,Zhang Zhang,Tingting Gao,Di Zhang,Liang Wang,Rong Jin,Tieniu Tan
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: Project Page: https://mm-rlhf.github.io/
- **标题**: None
- **领域**: 计算语言学,计算机视觉和模式识别
- **Abstract**: Despite notable advancements in Multimodal Large Language Models (MLLMs), most state-of-the-art models have not undergone thorough alignment with human preferences. This gap exists because current alignment research has primarily achieved progress in specific areas (e.g., hallucination reduction), while the broader question of whether aligning models with human preferences can systematically enhance MLLM capability remains largely unexplored. To this end, we introduce MM-RLHF, a dataset containing $\mathbf{120k}$ fine-grained, human-annotated preference comparison pairs. This dataset represents a substantial advancement over existing resources, offering superior size, diversity, annotation granularity, and quality. Leveraging this dataset, we propose several key innovations to improve both the quality of reward models and the efficiency of alignment algorithms. Notably, we introduce a Critique-Based Reward Model, which generates critiques of model outputs before assigning scores, offering enhanced interpretability and more informative feedback compared to traditional scalar reward mechanisms. Additionally, we propose Dynamic Reward Scaling, a method that adjusts the loss weight of each sample according to the reward signal, thereby optimizing the use of high-quality comparison pairs. Our approach is rigorously evaluated across $\mathbf{10}$ distinct dimensions and $\mathbf{27}$ benchmarks, with results demonstrating significant and consistent improvements in model performance. Specifically, fine-tuning LLaVA-ov-7B with MM-RLHF and our alignment algorithm leads to a $\mathbf{19.5}$% increase in conversational abilities and a $\mathbf{60}$% improvement in safety. We have open-sourced the preference dataset, reward model, training and evaluation code, as well as reward modeling and safety benchmarks. For more details, please visit our project page: https://mm-rlhf.github.io.

### Aspect-Oriented Summarization for Psychiatric Short-Term Readmission Prediction 
[[arxiv](https://arxiv.org/abs/2502.10388)] [[cool](https://papers.cool/arxiv/2502.10388)] [[pdf](https://arxiv.org/pdf/2502.10388)]
> **Authors**: WonJin Yoon,Boyu Ren,Spencer Thomas,Chanwhi Kim,Guergana Savova,Mei-Hua Hall,Timothy Miller
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Recent progress in large language models (LLMs) has enabled the automated processing of lengthy documents even without supervised training on a task-specific dataset. Yet, their zero-shot performance in complex tasks as opposed to straightforward information extraction tasks remains suboptimal. One feasible approach for tasks with lengthy, complex input is to first summarize the document and then apply supervised fine-tuning to the summary. However, the summarization process inevitably results in some loss of information. In this study we present a method for processing the summaries of long documents aimed to capture different important aspects of the original document. We hypothesize that LLM summaries generated with different aspect-oriented prompts contain different \textit{information signals}, and we propose methods to measure these differences. We introduce approaches to effectively integrate signals from these different summaries for supervised training of transformer models. We validate our hypotheses on a high-impact task -- 30-day readmission prediction from a psychiatric discharge -- using real-world data from four hospitals, and show that our proposed method increases the prediction performance for the complex task of predicting patient outcome.

### OWLS: Scaling Laws for Multilingual Speech Recognition and Translation Models 
[[arxiv](https://arxiv.org/abs/2502.10373)] [[cool](https://papers.cool/arxiv/2502.10373)] [[pdf](https://arxiv.org/pdf/2502.10373)]
> **Authors**: William Chen,Jinchuan Tian,Yifan Peng,Brian Yan,Chao-Han Huck Yang,Shinji Watanabe
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 23 pages, 13 figures
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习,音频和语音处理
- **Abstract**: Neural scaling laws offer valuable insights for designing robust sequence processing architectures. While these laws have been extensively characterized in other modalities, their behavior in speech remains comparatively underexplored. In this work, we introduce OWLS, an open-access, reproducible suite of multilingual speech recognition and translation models spanning 0.25B to 18B parameters, with the 18B version being the largest speech model, to the best of our knowledge. OWLS leverages up to 360K hours of public speech data across 150 languages, enabling a systematic investigation into how data, model, and compute scaling each influence performance in multilingual speech tasks. We use OWLS to derive neural scaling laws, showing how final performance can be reliably predicted when scaling. One of our key findings is that scaling enhances performance on low-resource languages/dialects, helping to mitigate bias and improve the accessibility of speech technologies. Finally, we show how OWLS can be used to power new research directions by discovering emergent abilities in large-scale speech models. Model checkpoints will be released on https://huggingface.co/collections/espnet/owls-scaling-laws-for-speech-recognition-and-translation-67ab7f991c194065f057ce8d for future studies.

### Enhancing Multilingual LLM Pretraining with Model-Based Data Selection 
[[arxiv](https://arxiv.org/abs/2502.10361)] [[cool](https://papers.cool/arxiv/2502.10361)] [[pdf](https://arxiv.org/pdf/2502.10361)]
> **Authors**: Bettina Messmer,Vinko Sabolčec,Martin Jaggi
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Dataset curation has become a basis for strong large language model (LLM) performance. While various rule-based filtering heuristics exist for English and multilingual datasets, model-based filtering techniques have primarily focused on English. To address the disparity stemming from limited research on non-English languages, we propose a model-based filtering framework for multilingual datasets that aims to identify a diverse set of structured and knowledge-rich samples. Our approach emphasizes transparency, simplicity, and efficiency, leveraging Transformer- and FastText-based classifiers to ensure the broad accessibility of our technique and data. We conduct comprehensive ablation studies on the FineWeb-2 web crawl dataset across diverse language families, scripts, and resource availability to demonstrate the effectiveness of our method. Training a 1B-parameter Llama model for 70B and 119B tokens, our approach can match the baseline MMLU score with as little as 15% of the training tokens, while also improving across other benchmarks. These findings provide strong evidence for the generalizability of our approach to other languages. As a result, we extend our framework to 20 languages for which we release the refined pretraining datasets.

### Agentic Verification for Ambiguous Query Disambiguation 
[[arxiv](https://arxiv.org/abs/2502.10352)] [[cool](https://papers.cool/arxiv/2502.10352)] [[pdf](https://arxiv.org/pdf/2502.10352)]
> **Authors**: Youngwon Lee,Seung-won Hwang,Ruofan Wu,Feng Yan,Danmei Xu,Moutasem Akkad,Zhewei Yao,Yuxiong He
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: In this work, we tackle the challenge of disambiguating queries in retrieval-augmented generation (RAG) to diverse yet answerable interpretations. State-of-the-arts follow a Diversify-then-Verify (DtV) pipeline, where diverse interpretations are generated by an LLM, later used as search queries to retrieve supporting passages. Such a process may introduce noise in either interpretations or retrieval, particularly in enterprise settings, where LLMs -- trained on static data -- may struggle with domain-specific disambiguations. Thus, a post-hoc verification phase is introduced to prune noises. Our distinction is to unify diversification with verification by incorporating feedback from retriever and generator early on. This joint approach improves both efficiency and robustness by reducing reliance on multiple retrieval and inference steps, which are susceptible to cascading errors. We validate the efficiency and effectiveness of our method, Verified-Diversification with Consolidation (VERDICT), on the widely adopted ASQA benchmark to achieve diverse yet verifiable interpretations. Empirical results show that VERDICT improves grounding-aware F1 score by an average of 23% over the strongest baseline across different backbone LLMs.

### Organize the Web: Constructing Domains Enhances Pre-Training Data Curation 
[[arxiv](https://arxiv.org/abs/2502.10341)] [[cool](https://papers.cool/arxiv/2502.10341)] [[pdf](https://arxiv.org/pdf/2502.10341)]
> **Authors**: Alexander Wettig,Kyle Lo,Sewon Min,Hannaneh Hajishirzi,Danqi Chen,Luca Soldaini
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: Project page: https://weborganizer.allen.ai
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Modern language models are trained on large, unstructured datasets consisting of trillions of tokens and obtained by crawling the web. The unstructured nature makes it difficult to reason about their contents and develop systematic approaches to data curation. In this paper, we unpack monolithic web corpora by developing taxonomies of their contents and organizing them into domains. We introduce WebOrganizer, a framework for organizing web pages in terms of both their topic and format. Using these two complementary notions of domains, we automatically annotate pre-training data by distilling annotations from a large language model into efficient classifiers. This allows us to study how data from different domains should be mixed to improve models on downstream tasks, and we show that we can combine insights about effective topics and formats to further boost performance. We demonstrate that our domain mixing also improves existing methods that select data based on quality. Furthermore, we study and compare how quality-based methods will implicitly change the domain mixture. Overall, our work demonstrates that constructing and mixing domains provides a valuable complement to quality-based data curation methods, opening new avenues for effective and insightful pre-training data curation.

### STAR: Spectral Truncation and Rescale for Model Merging 
[[arxiv](https://arxiv.org/abs/2502.10339)] [[cool](https://papers.cool/arxiv/2502.10339)] [[pdf](https://arxiv.org/pdf/2502.10339)]
> **Authors**: Yu-Ang Lee,Ching-Yun Ko,Tejaswini Pedapati,I-Hsin Chung,Mi-Yen Yeh,Pin-Yu Chen
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: Accepted to NAACL 2025
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Model merging is an efficient way of obtaining a multi-task model from several pretrained models without further fine-tuning, and it has gained attention in various domains, including natural language processing (NLP). Despite the efficiency, a key challenge in model merging is the seemingly inevitable decrease in task performance as the number of models increases. In this paper, we propose $\mathbf{S}$pectral $\mathbf{T}$runcation $\mathbf{A}$nd $\mathbf{R}$escale (STAR) that aims at mitigating ``merging conflicts'' by truncating small components in the respective spectral spaces, which is followed by an automatic parameter rescaling scheme to retain the nuclear norm of the original matrix. STAR requires no additional inference on original training data and is robust to hyperparamater choice. We demonstrate the effectiveness of STAR through extensive model merging cases on diverse NLP tasks. Specifically, STAR works robustly across varying model sizes, and can outperform baselines by 4.2$\%$ when merging 12 models on Flan-T5. Our code is publicly available at https://github.com/IBM/STAR.

### Evaluating the Meta- and Object-Level Reasoning of Large Language Models for Question Answering 
[[arxiv](https://arxiv.org/abs/2502.10338)] [[cool](https://papers.cool/arxiv/2502.10338)] [[pdf](https://arxiv.org/pdf/2502.10338)]
> **Authors**: Nick Ferguson,Liane Guillou,Alan Bundy,Kwabena Nuamah
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 8 pages. Accepted to the Workshop on Planning in the Era of LLMs (LM4Plan @ AAAI 2025)
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large Language Models (LLMs) excel in natural language tasks but still face challenges in Question Answering (QA) tasks requiring complex, multi-step reasoning. We outline the types of reasoning required in some of these tasks, and reframe them in terms of meta-level reasoning (akin to high-level strategic reasoning or planning) and object-level reasoning (embodied in lower-level tasks such as mathematical reasoning). Franklin, a novel dataset with requirements of meta- and object-level reasoning, is introduced and used along with three other datasets to evaluate four LLMs at question answering tasks requiring multiple steps of reasoning. Results from human annotation studies suggest LLMs demonstrate meta-level reasoning with high frequency, but struggle with object-level reasoning tasks in some of the datasets used. Additionally, evidence suggests that LLMs find the object-level reasoning required for the questions in the Franklin dataset challenging, yet they do exhibit strong performance with respect to the meta-level reasoning requirements.

### Are Large Language Models the future crowd workers of Linguistics? 
[[arxiv](https://arxiv.org/abs/2502.10266)] [[cool](https://papers.cool/arxiv/2502.10266)] [[pdf](https://arxiv.org/pdf/2502.10266)]
> **Authors**: Iris Ferrazzo
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Data elicitation from human participants is one of the core data collection strategies used in empirical linguistic research. The amount of participants in such studies may vary considerably, ranging from a handful to crowdsourcing dimensions. Even if they provide resourceful extensive data, both of these settings come alongside many disadvantages, such as low control of participants' attention during task completion, precarious working conditions in crowdsourcing environments, and time-consuming experimental designs. For these reasons, this research aims to answer the question of whether Large Language Models (LLMs) may overcome those obstacles if included in empirical linguistic pipelines. Two reproduction case studies are conducted to gain clarity into this matter: Cruz (2023) and Lombard et al. (2021). The two forced elicitation tasks, originally designed for human participants, are reproduced in the proposed framework with the help of OpenAI's GPT-4o-mini model. Its performance with our zero-shot prompting baseline shows the effectiveness and high versatility of LLMs, that tend to outperform human informants in linguistic tasks. The findings of the second replication further highlight the need to explore additional prompting techniques, such as Chain-of-Thought (CoT) prompting, which, in a second follow-up experiment, demonstrates higher alignment to human performance on both critical and filler items. Given the limited scale of this study, it is worthwhile to further explore the performance of LLMs in empirical Linguistics and in other future applications in the humanities.

### Large Language Models and Synthetic Data for Monitoring Dataset Mentions in Research Papers 
[[arxiv](https://arxiv.org/abs/2502.10263)] [[cool](https://papers.cool/arxiv/2502.10263)] [[pdf](https://arxiv.org/pdf/2502.10263)]
> **Authors**: Aivin V. Solatorio,Rafael Macalaba,James Liounis
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: Project GitHub repository at https://github.com/worldbank/ai4data-use
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机与社会,数据库,机器学习
- **Abstract**: Tracking how data is mentioned and used in research papers provides critical insights for improving data discoverability, quality, and production. However, manually identifying and classifying dataset mentions across vast academic literature is resource-intensive and not scalable. This paper presents a machine learning framework that automates dataset mention detection across research domains by leveraging large language models (LLMs), synthetic data, and a two-stage fine-tuning process. We employ zero-shot extraction from research papers, an LLM-as-a-Judge for quality assessment, and a reasoning agent for refinement to generate a weakly supervised synthetic dataset. The Phi-3.5-mini instruct model is pre-fine-tuned on this dataset, followed by fine-tuning on a manually annotated subset. At inference, a ModernBERT-based classifier efficiently filters dataset mentions, reducing computational overhead while maintaining high recall. Evaluated on a held-out manually annotated sample, our fine-tuned model outperforms NuExtract-v1.5 and GLiNER-large-v2.1 in dataset extraction accuracy. Our results highlight how LLM-generated synthetic data can effectively address training data scarcity, improving generalization in low-resource settings. This framework offers a pathway toward scalable monitoring of dataset usage, enhancing transparency, and supporting researchers, funders, and policymakers in identifying data gaps and strengthening data accessibility for informed decision-making.

### VisCon-100K: Leveraging Contextual Web Data for Fine-tuning Vision Language Models 
[[arxiv](https://arxiv.org/abs/2502.10250)] [[cool](https://papers.cool/arxiv/2502.10250)] [[pdf](https://arxiv.org/pdf/2502.10250)]
> **Authors**: Gokul Karthik Kumar,Iheb Chaabane,Kebin Wu
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: Accepted at PAKDD 2025
- **标题**: None
- **领域**: 计算语言学,计算机视觉和模式识别
- **Abstract**: Vision-language models (VLMs) excel in various visual benchmarks but are often constrained by the lack of high-quality visual fine-tuning data. To address this challenge, we introduce VisCon-100K, a novel dataset derived from interleaved image-text web documents. Our approach transforms 45K web documents from the OBELICS dataset into 100K image conversation samples. We utilize GPT-4V to generate image-contextual captions and OpenChat 3.5 model to convert these captions into diverse free-form and multiple-choice question-answer pairs. Integrating this dataset for fine-tuning considerably enhances VLM performance across multiple benchmarks. Unlike methods that focus solely on fine-grained visual content, our approach leverages accompanying web context, yielding superior results. We also discover that a 'leaky modality mix', where conversation samples contain questions answerable from both the image and its contextual caption, outperforms non-leaky combinations of captions and Q&A pairs. VisCon-100k dataset shows strong performance with two popular VLM approaches: text-only large language model (LLM) aligned with a vision encoder using image captions data (ShareGPT4V-7b) and multimodally pretrained LLM (IDEFICS2-8b) using interleaved image-text data. In addition to releasing the VisCon-100K dataset, we provide a contextual captioner trained on this dataset, facilitating scalable fine-tuning data generation for future research and open-source applications. Using the same pipeline, but substituting our trained contextual captioner for GPT-4V, we also release the larger VisCon-1M dataset.

### Can Post-Training Quantization Benefit from an Additional QLoRA Integration? 
[[arxiv](https://arxiv.org/abs/2502.10202)] [[cool](https://papers.cool/arxiv/2502.10202)] [[pdf](https://arxiv.org/pdf/2502.10202)]
> **Authors**: Xiliang Zhu,Elena Khasanova,Cheng Chen
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: Accepted to NAACL 2025 Industry Track
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) have transformed natural language processing but pose significant challenges for real-world deployment. These models necessitate considerable computing resources, which can be costly and frequently unavailable. Model compression techniques such as quantization are often leveraged to alleviate resource demand, but they may have a negative impact on the generation quality. In this study, we explore the integration of 4-bit Post-training Quantization (PTQ) with QLoRA to address these issues. We demonstrate through extensive experiments that this integration outperforms standard PTQ, and in some cases even 16-bit full-parameter fine-tuning on LLMs, validated across proprietary and public datasets with different quantization algorithms. The results demonstrate the efficacy of PTQ-QLoRA integration, offering a viable solution for deploying powerful LLMs in resource-constrained environments without compromising on performance.

### Prediction hubs are context-informed frequent tokens in LLMs 
[[arxiv](https://arxiv.org/abs/2502.10201)] [[cool](https://papers.cool/arxiv/2502.10201)] [[pdf](https://arxiv.org/pdf/2502.10201)]
> **Authors**: Beatrix M. G. Nielsen,Iuri Macocco,Marco Baroni
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Hubness, the tendency for few points to be among the nearest neighbours of a disproportionate number of other points, commonly arises when applying standard distance measures to high-dimensional data, often negatively impacting distance-based analysis. As autoregressive large language models (LLMs) operate on high-dimensional representations, we ask whether they are also affected by hubness. We first show, theoretically, that the only representation comparison operation performed by LLMs, namely that between context and unembedding vectors to determine continuation probabilities, is not characterized by the concentration of distances phenomenon that typically causes the appeareance of nuisance hubness. We then empirically show that this comparison still leads to a high degree of hubness, but the hubs in this case do not constitute a disturbance. They are rather the result of context-modulated frequent tokens often appearing in the pool of likely candidates for next token prediction. On the other hand, when other distance computations involving LLM representations are performed, we do not have the same theoretical guarantees, and, indeed, we see nuisance hubs appear. In summary, our work highlights, on the one hand, how hubness, while omnipresent in high-dimensional spaces, is not always a negative property that needs to be mitigated, and, on the other hand, it shows that various widely-used LLMs have developed a guessing strategy that consists in constantly assigning a high probability to frequent tokens.

### Small Models, Big Impact: Efficient Corpus and Graph-Based Adaptation of Small Multilingual Language Models for Low-Resource Languages 
[[arxiv](https://arxiv.org/abs/2502.10140)] [[cool](https://papers.cool/arxiv/2502.10140)] [[pdf](https://arxiv.org/pdf/2502.10140)]
> **Authors**: Daniil Gurgurov,Ivan Vykopal,Josef van Genabith,Simon Ostermann
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: Pre-print
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Low-resource languages (LRLs) face significant challenges in natural language processing (NLP) due to limited data. While current state-of-the-art large language models (LLMs) still struggle with LRLs, smaller multilingual models (mLMs) such as mBERT and XLM-R offer greater promise due to a better fit of their capacity to low training data sizes. This study systematically investigates parameter-efficient adapter-based methods for adapting mLMs to LRLs, evaluating three architectures: Sequential Bottleneck, Invertible Bottleneck, and Low-Rank Adaptation. Using unstructured text from GlotCC and structured knowledge from ConceptNet, we show that small adaptation datasets (e.g., up to 1 GB of free-text or a few MB of knowledge graph data) yield gains in intrinsic (masked language modeling) and extrinsic tasks (topic classification, sentiment analysis, and named entity recognition). We find that Sequential Bottleneck adapters excel in language modeling, while Invertible Bottleneck adapters slightly outperform other methods on downstream tasks due to better embedding alignment and larger parameter counts. Adapter-based methods match or outperform full fine-tuning while using far fewer parameters, and smaller mLMs prove more effective for LRLs than massive LLMs like LLaMA-3, GPT-4, and DeepSeek-R1-based distilled models. While adaptation improves performance, pre-training data size remains the dominant factor, especially for languages with extensive pre-training coverage.

### MTLM: an Innovative Language Model Training Paradigm for ASR 
[[arxiv](https://arxiv.org/abs/2502.10058)] [[cool](https://papers.cool/arxiv/2502.10058)] [[pdf](https://arxiv.org/pdf/2502.10058)]
> **Authors**: Qingliang Meng,Pengju Ren,Tian Li,Changsong Dai
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,音频和语音处理
- **Abstract**: Pre-training Transformer-based language models (LMs) on a large amount of text has proven crucial for improving automatic speech recognition (ASR) performance. Generally, traditional LMs are unidirectional and unable to access the context on the right. This paper proposes a method for training LMs that enable traditional unidirectional LMs to fully utilize left and right contexts. Compared with the unidirectional LMs, our LM facilitates ASR to transcribe hypotheses more consistently and in a more semantically unambiguous way, as it incorporates richer contextual representations. Finally, our experimental results on the LibriSpeech corpus demonstrate that our model outperforms traditional unidirectional LMs, whether n-best rescoring or shallow fusion is used as the decoding algorithm.

### ORI: O Routing Intelligence 
[[arxiv](https://arxiv.org/abs/2502.10051)] [[cool](https://papers.cool/arxiv/2502.10051)] [[pdf](https://arxiv.org/pdf/2502.10051)]
> **Authors**: Ahmad Shadid,Rahul Kumar,Mohit Mayank
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 13 pages, 2 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Single large language models (LLMs) often fall short when faced with the ever-growing range of tasks, making a single-model approach insufficient. We address this challenge by proposing ORI (O Routing Intelligence), a dynamic framework that leverages a set of LLMs. By intelligently routing incoming queries to the most suitable model, ORI not only improves task-specific accuracy, but also maintains efficiency. Comprehensive evaluations across diverse benchmarks demonstrate consistent accuracy gains while controlling computational overhead. By intelligently routing queries, ORI outperforms the strongest individual models by up to 2.7 points on MMLU and 1.8 points on MuSR, ties the top performance on ARC, and on BBH. These results underscore the benefits of a multi-model strategy and demonstrate how ORI's adaptive architecture can more effectively handle diverse tasks, offering a scalable, high-performance solution for a system of multiple large language models.

### Probabilistic Lexical Manifold Construction in Large Language Models via Hierarchical Vector Field Interpolation 
[[arxiv](https://arxiv.org/abs/2502.10013)] [[cool](https://papers.cool/arxiv/2502.10013)] [[pdf](https://arxiv.org/pdf/2502.10013)]
> **Authors**: Clive Pendleton,Ewan Harrington,Giles Fairbrother,Jasper Arkwright,Nigel Fenwick,Richard Katrix
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Hierarchical vector field interpolation introduces a structured probabilistic framework for lexical representation, ensuring that word embeddings transition smoothly across a continuous manifold rather than being constrained to discrete token mappings. The proposed methodology constructs a probabilistic function space where word representations adhere to topological consistency, mitigating representational discontinuities commonly observed in transformer-based embeddings. Empirical evaluations reveal that probabilistic constraints enhance lexical coherence by refining contextual relationships, leading to improvements in semantic stability across multiple linguistic distributions. The application of divergence minimization techniques ensures that interpolated embeddings maintain probabilistic consistency while preserving computational feasibility for large-scale implementations. Experimental findings demonstrate that interpolated lexical manifolds improve representation density alignment, reducing anisotropic distortions in contextual embedding distributions. Comparative analyses with standard transformer-based models highlight that structured interpolation yields more stable representations, particularly in tasks requiring fine-grained semantic differentiation. The statistical evaluation of embedding divergence confirms that probabilistic lexical manifolds reduce representational inconsistencies while maintaining coherence across varying scales of contextual abstraction. An assessment of computational efficiency reveals that while interpolation introduces minor processing overhead, the structured representation learning approach remains scalable for practical deployment.

### EmbBERT-Q: Breaking Memory Barriers in Embedded NLP 
[[arxiv](https://arxiv.org/abs/2502.10001)] [[cool](https://papers.cool/arxiv/2502.10001)] [[pdf](https://arxiv.org/pdf/2502.10001)]
> **Authors**: Riccardo Bravin,Massimo Pavan,Hazem Hesham Yousef Shalby,Fabrizio Pittorino,Manuel Roveri
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 24 pages, 4 figures, 14 tables
- **标题**: None
- **领域**: 计算语言学,硬件架构,分布式、并行和集群计算,机器学习
- **Abstract**: Large Language Models (LLMs) have revolutionized natural language processing, setting new standards across a wide range of applications. However, their relevant memory and computational demands make them impractical for deployment on technologically-constrained tiny devices such as wearable devices and Internet-of-Things units. To address this limitation, we introduce EmbBERT-Q, a novel tiny language model specifically designed for tiny devices with stringent memory constraints. EmbBERT-Q achieves state-of-the-art (SotA) accuracy in Natural Language Processing tasks in this scenario, with a total memory footprint (weights and activations) of just 781 kB, representing a 25x reduction in size with respect to SotA models. By combining architectural innovations with hardware-compatible 8-bit quantization, EmbBERT-Q consistently outperforms several baseline models scaled down to a 2 MB memory budget (i.e., the maximum memory typically available in tiny devices), including heavily compressed versions of BERT and MAMBA. Extensive experimental evaluations on both a selected benchmark dataset, TinyNLP, specifically curated to evaluate Tiny Language Models in NLP tasks and real-world scenarios, and the GLUE benchmark, demonstrate EmbBERT-Q ability to deliver competitive accuracy with respect to existing approaches, achieving an unmatched balance between memory and performance. To ensure the complete and immediate reproducibility of all our results, we release all code, scripts, and model checkpoints at https://github.com/RiccardoBravin/tiny-LLM.

### Large Language Diffusion Models 
[[arxiv](https://arxiv.org/abs/2502.09992)] [[cool](https://papers.cool/arxiv/2502.09992)] [[pdf](https://arxiv.org/pdf/2502.09992)]
> **Authors**: Shen Nie,Fengqi Zhu,Zebin You,Xiaolu Zhang,Jingyang Ou,Jun Hu,Jun Zhou,Yankai Lin,Ji-Rong Wen,Chongxuan Li
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Autoregressive models (ARMs) are widely regarded as the cornerstone of large language models (LLMs). We challenge this notion by introducing LLaDA, a diffusion model trained from scratch under the pre-training and supervised fine-tuning (SFT) paradigm. LLaDA models distributions through a forward data masking process and a reverse process, parameterized by a vanilla Transformer to predict masked tokens. By optimizing a likelihood bound, it provides a principled generative approach for probabilistic inference. Across extensive benchmarks, LLaDA demonstrates strong scalability, outperforming our self-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong LLMs like LLaMA3 8B in in-context learning and, after SFT, exhibits impressive instruction-following abilities in case studies such as multi-turn dialogue. Moreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal poem completion task. Our findings establish diffusion models as a viable and promising alternative to ARMs, challenging the assumption that key LLM capabilities discussed above are inherently tied to ARMs. Project page and codes: https://ml-gsai.github.io/LLaDA-demo/.

### LaRA: Benchmarking Retrieval-Augmented Generation and Long-Context LLMs -- No Silver Bullet for LC or RAG Routing 
[[arxiv](https://arxiv.org/abs/2502.09977)] [[cool](https://papers.cool/arxiv/2502.09977)] [[pdf](https://arxiv.org/pdf/2502.09977)]
> **Authors**: Kuan Li,Liwen Zhang,Yong Jiang,Pengjun Xie,Fei Huang,Shuai Wang,Minhao Cheng
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 22 pages
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Effectively incorporating external knowledge into Large Language Models (LLMs) is crucial for enhancing their capabilities and addressing real-world needs. Retrieval-Augmented Generation (RAG) offers an effective method for achieving this by retrieving the most relevant fragments into LLMs. However, the advancements in context window size for LLMs offer an alternative approach, raising the question of whether RAG remains necessary for effectively handling external knowledge. Several existing studies provide inconclusive comparisons between RAG and long-context (LC) LLMs, largely due to limitations in the benchmark designs. In this paper, we present LaRA, a novel benchmark specifically designed to rigorously compare RAG and LC LLMs. LaRA encompasses 2,326 test cases across four practical QA task categories and three types of naturally occurring long texts. Through systematic evaluation of seven open-source and four proprietary LLMs, we find that the optimal choice between RAG and LC depends on a complex interplay of factors, including the model's parameter size, long-text capabilities, context length, task type, and the characteristics of the retrieved chunks. Our findings provide actionable guidelines for practitioners to effectively leverage both RAG and LC approaches in developing and deploying LLM applications. Our code and dataset is provided at: \href{https://github.com/likuanppd/LaRA}{\textbf{https://github.com/likuanppd/LaRA}}.

### KGGen: Extracting Knowledge Graphs from Plain Text with Language Models 
[[arxiv](https://arxiv.org/abs/2502.09956)] [[cool](https://papers.cool/arxiv/2502.09956)] [[pdf](https://arxiv.org/pdf/2502.09956)]
> **Authors**: Belinda Mo,Kyssen Yu,Joshua Kazdan,Proud Mpala,Lisa Yu,Chris Cundy,Charilaos Kanatsoulis,Sanmi Koyejo
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,信息检索,机器学习
- **Abstract**: Recent interest in building foundation models for KGs has highlighted a fundamental challenge: knowledge-graph data is relatively scarce. The best-known KGs are primarily human-labeled, created by pattern-matching, or extracted using early NLP techniques. While human-generated KGs are in short supply, automatically extracted KGs are of questionable quality. We present a solution to this data scarcity problem in the form of a text-to-KG generator (KGGen), a package that uses language models to create high-quality graphs from plaintext. Unlike other KG extractors, KGGen clusters related entities to reduce sparsity in extracted KGs. KGGen is available as a Python library (\texttt{pip install kg-gen}), making it accessible to everyone. Along with KGGen, we release the first benchmark, Measure of of Information in Nodes and Edges (MINE), that tests an extractor's ability to produce a useful KG from plain text. We benchmark our new tool against existing extractors and demonstrate far superior performance.

### A Preliminary Exploration with GPT-4o Voice Mode 
[[arxiv](https://arxiv.org/abs/2502.09940)] [[cool](https://papers.cool/arxiv/2502.09940)] [[pdf](https://arxiv.org/pdf/2502.09940)]
> **Authors**: Yu-Xiang Lin,Chih-Kai Yang,Wei-Chih Chen,Chen-An Li,Chien-yu Huang,Xuanjun Chen,Hung-yi Lee
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: Work in progress
- **标题**: None
- **领域**: 计算语言学,声音,音频和语音处理
- **Abstract**: With the rise of multimodal large language models, GPT-4o stands out as a pioneering model, driving us to evaluate its capabilities. This report assesses GPT-4o across various tasks to analyze its audio processing and reasoning abilities. We find that GPT-4o exhibits strong knowledge in audio, speech, and music understanding, performing well in tasks like intent classification, spoken command classification, semantic and grammatical reasoning., multilingual speech recognition, and singing analysis. It also shows greater robustness against hallucinations than other large audio-language models (LALMs). However, it struggles with tasks such as audio duration prediction and instrument classification. Additionally, GPT-4o's safety mechanisms cause it to decline tasks like speaker identification, age classification, MOS prediction, and audio deepfake detection. Notably, the model exhibits a significantly different refusal rate when responding to speaker verification tasks on different datasets. This is likely due to variations in the accompanying instructions or the quality of the input audio, suggesting the sensitivity of its built-in safeguards. Finally, we acknowledge that model performance varies with evaluation protocols. This report only serves as a preliminary exploration of the current state of LALMs.

## 密码学和安全(cs.CR:Cryptography and Security)

### CCJA: Context-Coherent Jailbreak Attack for Aligned Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.11379)] [[cool](https://papers.cool/arxiv/2502.11379)] [[pdf](https://arxiv.org/pdf/2502.11379)]
> **Authors**: Guanghao Zhou,Panjia Qiu,Mingyuan Fan,Cen Chen,Mingyuan Chu,Xin Zhang,Jun Zhou
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能,计算语言学
- **Abstract**: Despite explicit alignment efforts for large language models (LLMs), they can still be exploited to trigger unintended behaviors, a phenomenon known as "jailbreaking." Current jailbreak attack methods mainly focus on discrete prompt manipulations targeting closed-source LLMs, relying on manually crafted prompt templates and persuasion rules. However, as the capabilities of open-source LLMs improve, ensuring their safety becomes increasingly crucial. In such an environment, the accessibility of model parameters and gradient information by potential attackers exacerbates the severity of jailbreak threats. To address this research gap, we propose a novel \underline{C}ontext-\underline{C}oherent \underline{J}ailbreak \underline{A}ttack (CCJA). We define jailbreak attacks as an optimization problem within the embedding space of masked language models. Through combinatorial optimization, we effectively balance the jailbreak attack success rate with semantic coherence. Extensive evaluations show that our method not only maintains semantic consistency but also surpasses state-of-the-art baselines in attack effectiveness. Additionally, by integrating semantically coherent jailbreak prompts generated by our method into widely used black-box methodologies, we observe a notable enhancement in their success rates when targeting closed-source commercial LLMs. This highlights the security threat posed by open-source LLMs to commercial counterparts. We will open-source our code if the paper is accepted.

### ALGEN: Few-shot Inversion Attacks on Textual Embeddings using Alignment and Generation 
[[arxiv](https://arxiv.org/abs/2502.11308)] [[cool](https://papers.cool/arxiv/2502.11308)] [[pdf](https://arxiv.org/pdf/2502.11308)]
> **Authors**: Yiyi Chen,Qiongkai Xu,Johannes Bjerva
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 18 pages, 13 tables, 6 figures
- **标题**: None
- **领域**: 密码学和安全,人工智能,计算语言学
- **Abstract**: With the growing popularity of Large Language Models (LLMs) and vector databases, private textual data is increasingly processed and stored as numerical embeddings. However, recent studies have proven that such embeddings are vulnerable to inversion attacks, where original text is reconstructed to reveal sensitive information. Previous research has largely assumed access to millions of sentences to train attack models, e.g., through data leakage or nearly unrestricted API access. With our method, a single data point is sufficient for a partially successful inversion attack. With as little as 1k data samples, performance reaches an optimum across a range of black-box encoders, without training on leaked data. We present a Few-shot Textual Embedding Inversion Attack using ALignment and GENeration (ALGEN), by aligning victim embeddings to the attack space and using a generative model to reconstruct text. We find that ALGEN attacks can be effectively transferred across domains and languages, revealing key information. We further examine a variety of defense mechanisms against ALGEN, and find that none are effective, highlighting the vulnerabilities posed by inversion attacks. By significantly lowering the cost of inversion and proving that embedding spaces can be aligned through one-step optimization, we establish a new textual embedding inversion paradigm with broader applications for embedding alignment in NLP.

### Primus: A Pioneering Collection of Open-Source Datasets for Cybersecurity LLM Training 
[[arxiv](https://arxiv.org/abs/2502.11191)] [[cool](https://papers.cool/arxiv/2502.11191)] [[pdf](https://arxiv.org/pdf/2502.11191)]
> **Authors**: Yao-Ching Yu,Tsun-Han Chiang,Cheng-Wei Tsai,Chien-Ming Huang,Wen-Kwang Tsao
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能,计算语言学
- **Abstract**: Large Language Models (LLMs) have shown remarkable advancements in specialized fields such as finance, law, and medicine. However, in cybersecurity, we have noticed a lack of open-source datasets, with a particular lack of high-quality cybersecurity pretraining corpora, even though much research indicates that LLMs acquire their knowledge during pretraining. To address this, we present a comprehensive suite of datasets covering all major training stages, including pretraining, instruction fine-tuning, and reasoning distillation with cybersecurity-specific self-reflection data. Extensive ablation studies demonstrate their effectiveness on public cybersecurity benchmarks. In particular, continual pre-training on our dataset yields a 15.88% improvement in the aggregate score, while reasoning distillation leads to a 10% gain in security certification (CISSP). We will release all datasets and trained cybersecurity LLMs under the ODC-BY and MIT licenses to encourage further research in the community. For access to all datasets and model weights, please refer to https://huggingface.co/collections/trendmicro-ailab/primus-67b1fd27052b802b4af9d243.

### G-Safeguard: A Topology-Guided Security Lens and Treatment on LLM-based Multi-agent Systems 
[[arxiv](https://arxiv.org/abs/2502.11127)] [[cool](https://papers.cool/arxiv/2502.11127)] [[pdf](https://arxiv.org/pdf/2502.11127)]
> **Authors**: Shilong Wang,Guibin Zhang,Miao Yu,Guancheng Wan,Fanci Meng,Chongye Guo,Kun Wang,Yang Wang
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,机器学习,多代理系统
- **Abstract**: Large Language Model (LLM)-based Multi-agent Systems (MAS) have demonstrated remarkable capabilities in various complex tasks, ranging from collaborative problem-solving to autonomous decision-making. However, as these systems become increasingly integrated into critical applications, their vulnerability to adversarial attacks, misinformation propagation, and unintended behaviors have raised significant concerns. To address this challenge, we introduce G-Safeguard, a topology-guided security lens and treatment for robust LLM-MAS, which leverages graph neural networks to detect anomalies on the multi-agent utterance graph and employ topological intervention for attack remediation. Extensive experiments demonstrate that G-Safeguard: (I) exhibits significant effectiveness under various attack strategies, recovering over 40% of the performance for prompt injection; (II) is highly adaptable to diverse LLM backbones and large-scale MAS; (III) can seamlessly combine with mainstream MAS with security guarantees. The code is available at https://github.com/wslong20/G-safeguard.

### Prompt Inject Detection with Generative Explanation as an Investigative Tool 
[[arxiv](https://arxiv.org/abs/2502.11006)] [[cool](https://papers.cool/arxiv/2502.11006)] [[pdf](https://arxiv.org/pdf/2502.11006)]
> **Authors**: Jonathan Pan,Swee Liang Wong,Yidi Yuan,Xin Wei Chia
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 5 pages, 4 tables, 3 diagrams
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Large Language Models (LLMs) are vulnerable to adversarial prompt based injects. These injects could jailbreak or exploit vulnerabilities within these models with explicit prompt requests leading to undesired responses. In the context of investigating prompt injects, the challenge is the sheer volume of input prompts involved that are likely to be largely benign. This investigative challenge is further complicated by the semantics and subjectivity of the input prompts involved in the LLM conversation with its user and the context of the environment to which the conversation is being carried out. Hence, the challenge for AI security investigators would be two-fold. The first is to identify adversarial prompt injects and then to assess whether the input prompt is contextually benign or adversarial. For the first step, this could be done using existing AI security solutions like guardrails to detect and protect the LLMs. Guardrails have been developed using a variety of approaches. A popular approach is to use signature based. Another popular approach to develop AI models to classify such prompts include the use of NLP based models like a language model. However, in the context of conducting an AI security investigation of prompt injects, these guardrails lack the ability to aid investigators in triaging or assessing the identified input prompts. In this applied research exploration, we explore the use of a text generation capabilities of LLM to detect prompt injects and generate explanation for its detections to aid AI security investigators in assessing and triaging of such prompt inject detections. The practical benefit of such a tool is to ease the task of conducting investigation into prompt injects.

### MITRE ATT&CK Applications in Cybersecurity and The Way Forward 
[[arxiv](https://arxiv.org/abs/2502.10825)] [[cool](https://papers.cool/arxiv/2502.10825)] [[pdf](https://arxiv.org/pdf/2502.10825)]
> **Authors**: Yuning Jiang,Qiaoran Meng,Feiyang Shang,Nay Oo,Le Thi Hong Minh,Hoon Wei Lim,Biplab Sikdar
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: 37 pages
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: The MITRE ATT&CK framework is a widely adopted tool for enhancing cybersecurity, supporting threat intelligence, incident response, attack modeling, and vulnerability prioritization. This paper synthesizes research on its application across these domains by analyzing 417 peer-reviewed publications. We identify commonly used adversarial tactics, techniques, and procedures (TTPs) and examine the integration of natural language processing (NLP) and machine learning (ML) with ATT&CK to improve threat detection and response. Additionally, we explore the interoperability of ATT&CK with other frameworks, such as the Cyber Kill Chain, NIST guidelines, and STRIDE, highlighting its versatility. The paper further evaluates the framework from multiple perspectives, including its effectiveness, validation methods, and sector-specific challenges, particularly in industrial control systems (ICS) and healthcare. We conclude by discussing current limitations and proposing future research directions to enhance the applicability of ATT&CK in dynamic cybersecurity environments.

### PDA: Generalizable Detection of AI-Generated Images via Post-hoc Distribution Alignment 
[[arxiv](https://arxiv.org/abs/2502.10803)] [[cool](https://papers.cool/arxiv/2502.10803)] [[pdf](https://arxiv.org/pdf/2502.10803)]
> **Authors**: Li Wang,Wenyu Chen,Zheng Li,Shanqing Guo
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能,计算机视觉和模式识别
- **Abstract**: The rapid advancement of generative models has led to the proliferation of highly realistic AI-generated images, posing significant challenges for detection methods to generalize across diverse and evolving generative techniques. Existing approaches often fail to adapt to unknown models without costly retraining, limiting their practicability. To fill this gap, we propose Post-hoc Distribution Alignment (PDA), a novel approach for the generalizable detection for AI-generated images. The key idea is to use the known generative model to regenerate undifferentiated test images. This process aligns the distributions of the re-generated real images with the known fake images, enabling effective distinction from unknown fake images. PDA employs a two-step detection framework: 1) evaluating whether a test image aligns with the known fake distribution based on deep k-nearest neighbor (KNN) distance, and 2) re-generating test images using known generative models to create pseudo-fake images for further classification. This alignment strategy allows PDA to effectively detect fake images without relying on unseen data or requiring retraining. Extensive experiments demonstrate the superiority of PDA, achieving 96.73\% average accuracy across six state-of-the-art generative models, including GANs, diffusion models, and text-to-image models, and improving by 16.07\% over the best baseline. Through t-SNE visualizations and KNN distance analysis, we provide insights into PDA's effectiveness in separating real and fake images. Our work provides a flexible and effective solution for real-world fake image detection, advancing the generalization ability of detection systems.

### FaceSwapGuard: Safeguarding Facial Privacy from DeepFake Threats through Identity Obfuscation 
[[arxiv](https://arxiv.org/abs/2502.10801)] [[cool](https://papers.cool/arxiv/2502.10801)] [[pdf](https://arxiv.org/pdf/2502.10801)]
> **Authors**: Li Wang,Zheng Li,Xuhong Zhang,Shouling Ji,Shanqing Guo
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能,计算机视觉和模式识别
- **Abstract**: DeepFakes pose a significant threat to our society. One representative DeepFake application is face-swapping, which replaces the identity in a facial image with that of a victim. Although existing methods partially mitigate these risks by degrading the quality of swapped images, they often fail to disrupt the identity transformation effectively. To fill this gap, we propose FaceSwapGuard (FSG), a novel black-box defense mechanism against deepfake face-swapping threats. Specifically, FSG introduces imperceptible perturbations to a user's facial image, disrupting the features extracted by identity encoders. When shared online, these perturbed images mislead face-swapping techniques, causing them to generate facial images with identities significantly different from the original user. Extensive experiments demonstrate the effectiveness of FSG against multiple face-swapping techniques, reducing the face match rate from 90\% (without defense) to below 10\%. Both qualitative and quantitative studies further confirm its ability to confuse human perception, highlighting its practical utility. Additionally, we investigate key factors that may influence FSG and evaluate its robustness against various adaptive adversaries.

### Dataset Protection via Watermarked Canaries in Retrieval-Augmented LLMs 
[[arxiv](https://arxiv.org/abs/2502.10673)] [[cool](https://papers.cool/arxiv/2502.10673)] [[pdf](https://arxiv.org/pdf/2502.10673)]
> **Authors**: Yepeng Liu,Xuandong Zhao,Dawn Song,Yuheng Bu
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,计算语言学
- **Abstract**: Retrieval-Augmented Generation (RAG) has become an effective method for enhancing large language models (LLMs) with up-to-date knowledge. However, it poses a significant risk of IP infringement, as IP datasets may be incorporated into the knowledge database by malicious Retrieval-Augmented LLMs (RA-LLMs) without authorization. To protect the rights of the dataset owner, an effective dataset membership inference algorithm for RA-LLMs is needed. In this work, we introduce a novel approach to safeguard the ownership of text datasets and effectively detect unauthorized use by the RA-LLMs. Our approach preserves the original data completely unchanged while protecting it by inserting specifically designed canary documents into the IP dataset. These canary documents are created with synthetic content and embedded watermarks to ensure uniqueness, stealthiness, and statistical provability. During the detection process, unauthorized usage is identified by querying the canary documents and analyzing the responses of RA-LLMs for statistical evidence of the embedded watermark. Our experimental results demonstrate high query efficiency, detectability, and stealthiness, along with minimal perturbation to the original dataset, all without compromising the performance of the RAG system.

### Dark Deceptions in DHCP: Dismantling Network Defenses 
[[arxiv](https://arxiv.org/abs/2502.10646)] [[cool](https://papers.cool/arxiv/2502.10646)] [[pdf](https://arxiv.org/pdf/2502.10646)]
> **Authors**: Robert Dilworth
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 8 pages, 4 tables
- **标题**: None
- **领域**: 密码学和安全,机器学习
- **Abstract**: This paper explores vulnerabilities in the Dynamic Host Configuration Protocol (DHCP) and their implications on the Confidentiality, Integrity, and Availability (CIA) triad. Through an analysis of various attacks, including DHCP Starvation, Rogue DHCP Servers, Replay Attacks, and TunnelVision exploits, the paper provides a taxonomic classification of threats, assesses risks, and proposes appropriate controls. The discussion also highlights the dangers of VPN decloaking through DHCP exploits and underscores the importance of safeguarding network infrastructures. By bringing awareness to the TunnelVision exploit, this paper aims to mitigate risks associated with these prevalent vulnerabilities.

### Network evasion detection with Bi-LSTM model 
[[arxiv](https://arxiv.org/abs/2502.10624)] [[cool](https://papers.cool/arxiv/2502.10624)] [[pdf](https://arxiv.org/pdf/2502.10624)]
> **Authors**: Kehua Chen,Jingping Jia
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 4 pages,5 figures
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Network evasion detection aims to distinguish whether the network flow comes from link layer exists network evasion threat, which is a means to disguise the data traffic on detection system by confusing the signature. Since the previous research works has all sorts of frauds, we propose a architecture with deep learning network to handle this problem. In this paper, we extract the critical information as key features from data frame and also specifically propose to use bidirectional long short-term memory (Bi-LSTM) neural network which shows an outstanding performance to trace the serial information, to encode both the past and future trait on the network flows. Furthermore we introduce a classifier named Softmax at the bottom of Bi-LSTM, holding a character to select the correct class. All experiments results shows that we can achieve a significant performance with a deep Bi-LSTM in network evasion detection and it's average accuracy reaches 96.1%.

### Federated Learning-Driven Cybersecurity Framework for IoT Networks with Privacy-Preserving and Real-Time Threat Detection Capabilities 
[[arxiv](https://arxiv.org/abs/2502.10599)] [[cool](https://papers.cool/arxiv/2502.10599)] [[pdf](https://arxiv.org/pdf/2502.10599)]
> **Authors**: Milad Rahmati
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,机器学习,网络和互联网架构
- **Abstract**: The rapid expansion of the Internet of Things (IoT) ecosystem has transformed various sectors but has also introduced significant cybersecurity challenges. Traditional centralized security methods often struggle to balance privacy preservation and real-time threat detection in IoT networks. To address these issues, this study proposes a Federated Learning-Driven Cybersecurity Framework designed specifically for IoT environments. The framework enables decentralized data processing by training models locally on edge devices, ensuring data privacy. Secure aggregation of these locally trained models is achieved using homomorphic encryption, allowing collaborative learning without exposing sensitive information. The proposed framework utilizes recurrent neural networks (RNNs) for anomaly detection, optimized for resource-constrained IoT networks. Experimental results demonstrate that the system effectively detects complex cyber threats, including distributed denial-of-service (DDoS) attacks, with over 98% accuracy. Additionally, it improves energy efficiency by reducing resource consumption by 20% compared to centralized approaches. This research addresses critical gaps in IoT cybersecurity by integrating federated learning with advanced threat detection techniques. The framework offers a scalable and privacy-preserving solution adaptable to various IoT applications. Future work will explore the integration of blockchain for transparent model aggregation and quantum-resistant cryptographic methods to further enhance security in evolving technological landscapes.

### Recent Advances in Malware Detection: Graph Learning and Explainability 
[[arxiv](https://arxiv.org/abs/2502.10556)] [[cool](https://papers.cool/arxiv/2502.10556)] [[pdf](https://arxiv.org/pdf/2502.10556)]
> **Authors**: Hossein Shokouhinejad,Roozbeh Razavi-Far,Hesamodin Mohammadian,Mahdi Rabbani,Samuel Ansong,Griffin Higgins,Ali A Ghorbani
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,机器学习
- **Abstract**: The rapid evolution of malware has necessitated the development of sophisticated detection methods that go beyond traditional signature-based approaches. Graph learning techniques have emerged as powerful tools for modeling and analyzing the complex relationships inherent in malware behavior, leveraging advancements in Graph Neural Networks (GNNs) and related methods. This survey provides a comprehensive exploration of recent advances in malware detection, focusing on the interplay between graph learning and explainability. It begins by reviewing malware analysis techniques and datasets, emphasizing their foundational role in understanding malware behavior and supporting detection strategies. The survey then discusses feature engineering, graph reduction, and graph embedding methods, highlighting their significance in transforming raw data into actionable insights, while ensuring scalability and efficiency. Furthermore, this survey focuses on explainability techniques and their applications in malware detection, ensuring transparency and trustworthiness. By integrating these components, this survey demonstrates how graph learning and explainability contribute to building robust, interpretable, and scalable malware detection systems. Future research directions are outlined to address existing challenges and unlock new opportunities in this critical area of cybersecurity.

### Towards Watermarking of Open-Source LLMs 
[[arxiv](https://arxiv.org/abs/2502.10525)] [[cool](https://papers.cool/arxiv/2502.10525)] [[pdf](https://arxiv.org/pdf/2502.10525)]
> **Authors**: Thibaud Gloaguen,Nikola Jovanović,Robin Staab,Martin Vechev
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,机器学习
- **Abstract**: While watermarks for closed LLMs have matured and have been included in large-scale deployments, these methods are not applicable to open-source models, which allow users full control over the decoding process. This setting is understudied yet critical, given the rising performance of open-source models. In this work, we lay the foundation for systematic study of open-source LLM watermarking. For the first time, we explicitly formulate key requirements, including durability against common model modifications such as model merging, quantization, or finetuning, and propose a concrete evaluation setup. Given the prevalence of these modifications, durability is crucial for an open-source watermark to be effective. We survey and evaluate existing methods, showing that they are not durable. We also discuss potential ways to improve their durability and highlight remaining challenges. We hope our work enables future progress on this important problem.

### SWA-LDM: Toward Stealthy Watermarks for Latent Diffusion Models 
[[arxiv](https://arxiv.org/abs/2502.10495)] [[cool](https://papers.cool/arxiv/2502.10495)] [[pdf](https://arxiv.org/pdf/2502.10495)]
> **Authors**: Zhonghao Yang,Linye Lyu,Xuanhang Chang,Daojing He,YU LI
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 13 pages, 5 figures
- **标题**: None
- **领域**: 密码学和安全,人工智能,计算机视觉和模式识别,机器学习
- **Abstract**: In the rapidly evolving landscape of image generation, Latent Diffusion Models (LDMs) have emerged as powerful tools, enabling the creation of highly realistic images. However, this advancement raises significant concerns regarding copyright infringement and the potential misuse of generated content. Current watermarking techniques employed in LDMs often embed constant signals to the generated images that compromise their stealthiness, making them vulnerable to detection by malicious attackers. In this paper, we introduce SWA-LDM, a novel approach that enhances watermarking by randomizing the embedding process, effectively eliminating detectable patterns while preserving image quality and robustness. Our proposed watermark presence attack reveals the inherent vulnerabilities of existing latent-based watermarking methods, demonstrating how easily these can be exposed. Through comprehensive experiments, we validate that SWA-LDM not only fortifies watermark stealthiness but also maintains competitive performance in watermark robustness and visual fidelity. This work represents a pivotal step towards securing LDM-generated images against unauthorized use, ensuring both copyright protection and content integrity in an era where digital image authenticity is paramount.

### Fast Proxies for LLM Robustness Evaluation 
[[arxiv](https://arxiv.org/abs/2502.10487)] [[cool](https://papers.cool/arxiv/2502.10487)] [[pdf](https://arxiv.org/pdf/2502.10487)]
> **Authors**: Tim Beyer,Jan Schuchardt,Leo Schwinn,Stephan Günnemann
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Evaluating the robustness of LLMs to adversarial attacks is crucial for safe deployment, yet current red-teaming methods are often prohibitively expensive. We compare the ability of fast proxy metrics to predict the real-world robustness of an LLM against a simulated attacker ensemble. This allows us to estimate a model's robustness to computationally expensive attacks without requiring runs of the attacks themselves. Specifically, we consider gradient-descent-based embedding-space attacks, prefilling attacks, and direct prompting. Even though direct prompting in particular does not achieve high ASR, we find that it and embedding-space attacks can predict attack success rates well, achieving $r_p=0.87$ (linear) and $r_s=0.94$ (Spearman rank) correlations with the full attack ensemble while reducing computational cost by three orders of magnitude.

### VLM-Guard: Safeguarding Vision-Language Models via Fulfilling Safety Alignment Gap 
[[arxiv](https://arxiv.org/abs/2502.10486)] [[cool](https://papers.cool/arxiv/2502.10486)] [[pdf](https://arxiv.org/pdf/2502.10486)]
> **Authors**: Qin Liu,Fei Wang,Chaowei Xiao,Muhao Chen
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: Work in progress
- **标题**: None
- **领域**: 密码学和安全,人工智能,计算机视觉和模式识别
- **Abstract**: The emergence of vision language models (VLMs) comes with increased safety concerns, as the incorporation of multiple modalities heightens vulnerability to attacks. Although VLMs can be built upon LLMs that have textual safety alignment, it is easily undermined when the vision modality is integrated. We attribute this safety challenge to the modality gap, a separation of image and text in the shared representation space, which blurs the distinction between harmful and harmless queries that is evident in LLMs but weakened in VLMs. To avoid safety decay and fulfill the safety alignment gap, we propose VLM-Guard, an inference-time intervention strategy that leverages the LLM component of a VLM as supervision for the safety alignment of the VLM. VLM-Guard projects the representations of VLM into the subspace that is orthogonal to the safety steering direction that is extracted from the safety-aligned LLM. Experimental results on three malicious instruction settings show the effectiveness of VLM-Guard in safeguarding VLM and fulfilling the safety alignment gap between VLM and its LLM component.

### Linking Cryptoasset Attribution Tags to Knowledge Graph Entities: An LLM-based Approach 
[[arxiv](https://arxiv.org/abs/2502.10453)] [[cool](https://papers.cool/arxiv/2502.10453)] [[pdf](https://arxiv.org/pdf/2502.10453)]
> **Authors**: Régnier Avice,Bernhard Haslhofer,Zhidong Li,Jianlong Zhou
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-17
> **comment**: Accepted at Financial Cryptography and Data Security 2025 Conference (FC2025)
- **标题**: None
- **领域**: 密码学和安全,人工智能,计算语言学,数据库,机器学习
- **Abstract**: Attribution tags form the foundation of modern cryptoasset forensics. However, inconsistent or incorrect tags can mislead investigations and even result in false accusations. To address this issue, we propose a novel computational method based on Large Language Models (LLMs) to link attribution tags with well-defined knowledge graph concepts. We implemented this method in an end-to-end pipeline and conducted experiments showing that our approach outperforms baseline methods by up to 37.4% in F1-score across three publicly available attribution tag datasets. By integrating concept filtering and blocking procedures, we generate candidate sets containing five knowledge graph entities, achieving a recall of 93% without the need for labeled data. Additionally, we demonstrate that local LLM models can achieve F1-scores of 90%, comparable to remote models which achieve 94%. We also analyze the cost-performance trade-offs of various LLMs and prompt templates, showing that selecting the most cost-effective configuration can reduce costs by 90%, with only a 1% decrease in performance. Our method not only enhances attribution tag quality but also serves as a blueprint for fostering more reliable forensic evidence.

### Trustworthy AI on Safety, Bias, and Privacy: A Survey 
[[arxiv](https://arxiv.org/abs/2502.10450)] [[cool](https://papers.cool/arxiv/2502.10450)] [[pdf](https://arxiv.org/pdf/2502.10450)]
> **Authors**: Xingli Fang,Jianwei Li,Varun Mulchandani,Jung-Eun Kim
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能,计算语言学,机器学习
- **Abstract**: The capabilities of artificial intelligence systems have been advancing to a great extent, but these systems still struggle with failure modes, vulnerabilities, and biases. In this paper, we study the current state of the field, and present promising insights and perspectives regarding concerns that challenge the trustworthiness of AI models. In particular, this paper investigates the issues regarding three thrusts: safety, privacy, and bias, which hurt models' trustworthiness. For safety, we discuss safety alignment in the context of large language models, preventing them from generating toxic or harmful content. For bias, we focus on spurious biases that can mislead a network. Lastly, for privacy, we cover membership inference attacks in deep neural networks. The discussions addressed in this paper reflect our own experiments and observations.

### Towards Copyright Protection for Knowledge Bases of Retrieval-augmented Language Models via Ownership Verification with Reasoning 
[[arxiv](https://arxiv.org/abs/2502.10440)] [[cool](https://papers.cool/arxiv/2502.10440)] [[pdf](https://arxiv.org/pdf/2502.10440)]
> **Authors**: Junfeng Guo,Yiming Li,Ruibo Chen,Yihan Wu,Chenxi Liu,Yanshuo Chen,Heng Huang
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-17
> **comment**: The first two authors contributed equally to this work. 19 pages
- **标题**: None
- **领域**: 密码学和安全,人工智能,计算语言学,信息检索,机器学习
- **Abstract**: Large language models (LLMs) are increasingly integrated into real-world applications through retrieval-augmented generation (RAG) mechanisms to supplement their responses with up-to-date and domain-specific knowledge. However, the valuable and often proprietary nature of the knowledge bases used in RAG introduces the risk of unauthorized usage by adversaries. Existing methods that can be generalized as watermarking techniques to protect these knowledge bases typically involve poisoning attacks. However, these methods require to alter the results of verification samples (\eg, generating incorrect outputs), inevitably making them susceptible to anomaly detection and even introduce new security risks. To address these challenges, we propose \name{} for `harmless' copyright protection of knowledge bases. Instead of manipulating LLM's final output, \name{} implants distinct verification behaviors in the space of chain-of-thought (CoT) reasoning, maintaining the correctness of the final answer. Our method has three main stages: (1) \textbf{Generating CoTs}: For each verification question, we generate two CoTs, including a target CoT for building watermark behaviors; (2) \textbf{Optimizing Watermark Phrases and Target CoTs}: We optimize them to minimize retrieval errors under the black-box setting of suspicious LLM, ensuring that the watermarked verification queries activate the target CoTs without being activated in non-watermarked ones; (3) \textbf{Ownership Verification}: We exploit a pairwise Wilcoxon test to statistically verify whether a suspicious LLM is augmented with the protected knowledge base by comparing its responses to watermarked and benign verification queries. Our experiments on diverse benchmarks demonstrate that \name{} effectively protects knowledge bases against unauthorized usage while preserving the integrity and performance of the RAG.

### Crypto Miner Attack: GPU Remote Code Execution Attacks 
[[arxiv](https://arxiv.org/abs/2502.10439)] [[cool](https://papers.cool/arxiv/2502.10439)] [[pdf](https://arxiv.org/pdf/2502.10439)]
> **Authors**: Ariel Szabo,Uzy Hadad
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能,机器学习
- **Abstract**: Remote Code Execution (RCE) exploits pose a significant threat to AI and ML systems, particularly in GPU-accelerated environments where the computational power of GPUs can be misused for malicious purposes. This paper focuses on RCE attacks leveraging deserialization vulnerabilities and custom layers, such as TensorFlow Lambda layers, which are often overlooked due to the complexity of monitoring GPU workloads. These vulnerabilities enable attackers to execute arbitrary code, blending malicious activity seamlessly into expected model behavior and exploiting GPUs for unauthorized tasks such as cryptocurrency mining. Unlike traditional CPU-based attacks, the parallel processing nature of GPUs and their high resource utilization make runtime detection exceptionally challenging. In this work, we provide a comprehensive examination of RCE exploits targeting GPUs, demonstrating an attack that utilizes these vulnerabilities to deploy a crypto miner on a GPU. We highlight the technical intricacies of such attacks, emphasize their potential for significant financial and computational costs, and propose strategies for mitigation. By shedding light on this underexplored attack vector, we aim to raise awareness and encourage the adoption of robust security measures in GPU-driven AI and ML systems, with an emphasis on static and model scanning as an easier way to detect exploits.

### Injecting Universal Jailbreak Backdoors into LLMs in Minutes 
[[arxiv](https://arxiv.org/abs/2502.10438)] [[cool](https://papers.cool/arxiv/2502.10438)] [[pdf](https://arxiv.org/pdf/2502.10438)]
> **Authors**: Zhuowei Chen,Qiannan Zhang,Shichao Pei
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-17
> **comment**: Accepted to ICLR 2025
- **标题**: None
- **领域**: 密码学和安全,人工智能,机器学习
- **Abstract**: Jailbreak backdoor attacks on LLMs have garnered attention for their effectiveness and stealth. However, existing methods rely on the crafting of poisoned datasets and the time-consuming process of fine-tuning. In this work, we propose JailbreakEdit, a novel jailbreak backdoor injection method that exploits model editing techniques to inject a universal jailbreak backdoor into safety-aligned LLMs with minimal intervention in minutes. JailbreakEdit integrates a multi-node target estimation to estimate the jailbreak space, thus creating shortcuts from the backdoor to this estimated jailbreak space that induce jailbreak actions. Our attack effectively shifts the models' attention by attaching strong semantics to the backdoor, enabling it to bypass internal safety mechanisms. Experimental results show that JailbreakEdit achieves a high jailbreak success rate on jailbreak prompts while preserving generation quality, and safe performance on normal queries. Our findings underscore the effectiveness, stealthiness, and explainability of JailbreakEdit, emphasizing the need for more advanced defense mechanisms in LLMs.

### X-Boundary: Establishing Exact Safety Boundary to Shield LLMs from Multi-Turn Jailbreaks without Compromising Usability 
[[arxiv](https://arxiv.org/abs/2502.09990)] [[cool](https://papers.cool/arxiv/2502.09990)] [[pdf](https://arxiv.org/pdf/2502.09990)]
> **Authors**: Xiaoya Lu,Dongrui Liu,Yi Yu,Luxin Xu,Jing Shao
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能,计算语言学,计算机视觉和模式识别,机器学习
- **Abstract**: Despite the rapid development of safety alignment techniques for LLMs, defending against multi-turn jailbreaks is still a challenging task. In this paper, we conduct a comprehensive comparison, revealing that some existing defense methods can improve the robustness of LLMs against multi-turn jailbreaks but compromise usability, i.e., reducing general capabilities or causing the over-refusal problem. From the perspective of mechanism interpretability of LLMs, we discover that these methods fail to establish a boundary that exactly distinguishes safe and harmful feature representations. Therefore, boundary-safe representations close to harmful representations are inevitably disrupted, leading to a decline in usability. To address this issue, we propose X-Boundary to push harmful representations away from boundary-safe representations and obtain an exact distinction boundary. In this way, harmful representations can be precisely erased without disrupting safe ones. Experimental results show that X-Boundary achieves state-of-the-art defense performance against multi-turn jailbreaks, while reducing the over-refusal rate by about 20% and maintaining nearly complete general capability. Furthermore, we theoretically prove and empirically verify that X-Boundary can accelerate the convergence process during training. Please see our code at: https://github.com/AI45Lab/X-Boundary.

## 计算机视觉和模式识别(cs.CV:Computer Vision and Pattern Recognition)

### GeoDANO: Geometric VLM with Domain Agnostic Vision Encoder 
[[arxiv](https://arxiv.org/abs/2502.11360)] [[cool](https://papers.cool/arxiv/2502.11360)] [[pdf](https://arxiv.org/pdf/2502.11360)]
> **Authors**: Seunghyuk Cho,Zhenyue Qin,Yang Liu,Youngbin Choi,Seungbeom Lee,Dongwoo Kim
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 14 pages, 7 figures, 5 tables
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算语言学
- **Abstract**: We introduce GeoDANO, a geometric vision-language model (VLM) with a domain-agnostic vision encoder, for solving plane geometry problems. Although VLMs have been employed for solving geometry problems, their ability to recognize geometric features remains insufficiently analyzed. To address this gap, we propose a benchmark that evaluates the recognition of visual geometric features, including primitives such as dots and lines, and relations such as orthogonality. Our preliminary study shows that vision encoders often used in general-purpose VLMs, e.g., OpenCLIP, fail to detect these features and struggle to generalize across domains. We develop GeoCLIP, a CLIP based model trained on synthetic geometric diagram-caption pairs to overcome the limitation. Benchmark results show that GeoCLIP outperforms existing vision encoders in recognizing geometric features. We then propose our VLM, GeoDANO, which augments GeoCLIP with a domain adaptation strategy for unseen diagram styles. GeoDANO outperforms specialized methods for plane geometry problems and GPT-4o on MathVerse.

### WRT-SAM: Foundation Model-Driven Segmentation for Generalized Weld Radiographic Testing 
[[arxiv](https://arxiv.org/abs/2502.11338)] [[cool](https://papers.cool/arxiv/2502.11338)] [[pdf](https://arxiv.org/pdf/2502.11338)]
> **Authors**: Yunyi Zhou,Kun Shi,Gang Hao
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Radiographic testing is a fundamental non-destructive evaluation technique for identifying weld defects and assessing quality in industrial applications due to its high-resolution imaging capabilities. Over the past decade, deep learning techniques have significantly advanced weld defect identification in radiographic images. However, conventional approaches, which rely on training small-scale, task-specific models on single-scenario datasets, exhibit poor cross-scenario generalization. Recently, the Segment Anything Model (SAM), a pre-trained visual foundation model trained on large-scale datasets, has demonstrated exceptional zero-shot generalization capabilities. Fine-tuning SAM with limited domain-specific data has yielded promising results in fields such as medical image segmentation and anomaly detection. To the best of our knowledge, this work is the first to introduce SAM-based segmentation for general weld radiographic testing images. We propose WRT-SAM, a novel weld radiographic defect segmentation model that leverages SAM through an adapter-based integration with a specialized prompt generator architecture. To improve adaptability to grayscale weld radiographic images, we introduce a frequency prompt generator module, which enhances the model's sensitivity to frequency-domain information. Furthermore, to address the multi-scale nature of weld defects, we incorporate a multi-scale prompt generator module, enabling the model to effectively extract and encode defect information across varying scales. Extensive experimental evaluations demonstrate that WRT-SAM achieves a recall of 78.87%, a precision of 84.04%, and an AUC of 0.9746, setting a new state-of-the-art (SOTA) benchmark. Moreover, the model exhibits superior zero-shot generalization performance, highlighting its potential for practical deployment in diverse radiographic testing scenarios.

### Differentially private fine-tuned NF-Net to predict GI cancer type 
[[arxiv](https://arxiv.org/abs/2502.11329)] [[cool](https://papers.cool/arxiv/2502.11329)] [[pdf](https://arxiv.org/pdf/2502.11329)]
> **Authors**: Sai Venkatesh Chilukoti,Imran Hossen Md,Liqun Shan,Vijay Srinivas Tida,Xiali Hei
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 10 pages, 8 tables, 2 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Based on global genomic status, the cancer tumor is classified as Microsatellite Instable (MSI) and Microsatellite Stable (MSS). Immunotherapy is used to diagnose MSI, whereas radiation and chemotherapy are used for MSS. Therefore, it is significant to classify a gastro-intestinal (GI) cancer tumor into MSI vs. MSS to provide appropriate treatment. The existing literature showed that deep learning could directly predict the class of GI cancer tumors from histological images. However, deep learning (DL) models are susceptible to various threats, including membership inference attacks, model extraction attacks, etc. These attacks render the use of DL models impractical in real-world scenarios. To make the DL models useful and maintain privacy, we integrate differential privacy (DP) with DL. In particular, this paper aims to predict the state of GI cancer while preserving the privacy of sensitive data. We fine-tuned the Normalizer Free Net (NF-Net) model. We obtained an accuracy of 88.98\% without DP to predict (GI) cancer status. When we fine-tuned the NF-Net using DP-AdamW and adaptive DP-AdamW, we got accuracies of 74.58% and 76.48%, respectively. Moreover, we investigate the Weighted Random Sampler (WRS) and Class weighting (CW) to solve the data imbalance. We also evaluated and analyzed the DP algorithms in different settings.

### Exploiting Point-Language Models with Dual-Prompts for 3D Anomaly Detection 
[[arxiv](https://arxiv.org/abs/2502.11307)] [[cool](https://papers.cool/arxiv/2502.11307)] [[pdf](https://arxiv.org/pdf/2502.11307)]
> **Authors**: Jiaxiang Wang,Haote Xu,Xiaolu Chen,Haodi Xu,Yue Huang,Xinghao Ding,Xiaotong Tu
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 10 pages, 7 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Anomaly detection (AD) in 3D point clouds is crucial in a wide range of industrial applications, especially in various forms of precision manufacturing. Considering the industrial demand for reliable 3D AD, several methods have been developed. However, most of these approaches typically require training separate models for each category, which is memory-intensive and lacks flexibility. In this paper, we propose a novel Point-Language model with dual-prompts for 3D ANomaly dEtection (PLANE). The approach leverages multi-modal prompts to extend the strong generalization capabilities of pre-trained Point-Language Models (PLMs) to the domain of 3D point cloud AD, achieving impressive detection performance across multiple categories using a single model. Specifically, we propose a dual-prompt learning method, incorporating both text and point cloud prompts. The method utilizes a dynamic prompt creator module (DPCM) to produce sample-specific dynamic prompts, which are then integrated with class-specific static prompts for each modality, effectively driving the PLMs. Additionally, based on the characteristics of point cloud data, we propose a pseudo 3D anomaly generation method (Ano3D) to improve the model's detection capabilities in an unsupervised setting. Experimental results demonstrate that the proposed method, which is under the multi-class-one-model paradigm, achieves a +8.7%/+17% gain on anomaly detection and localization performance as compared to the state-of-the-art one-class-one-model methods for the Anomaly-ShapeNet dataset, and obtains +4.3%/+4.1% gain for the Real3D-AD dataset. Code will be available upon publication.

### RT-DEMT: A hybrid real-time acupoint detection model combining mamba and transformer 
[[arxiv](https://arxiv.org/abs/2502.11179)] [[cool](https://papers.cool/arxiv/2502.11179)] [[pdf](https://arxiv.org/pdf/2502.11179)]
> **Authors**: Shilong Yang,Qi Zang,Chulong Zhang,Lingfeng Huang,Yaoqin Xie
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 10 pages, 3 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Traditional Chinese acupuncture methods often face controversy in clinical practice due to their high subjectivity. Additionally, current intelligent-assisted acupuncture systems have two major limitations: slow acupoint localization speed and low accuracy. To address these limitations, a new method leverages the excellent inference efficiency of the state-space model Mamba, while retaining the advantages of the attention mechanism in the traditional DETR architecture, to achieve efficient global information integration and provide high-quality feature information for acupoint localization tasks. Furthermore, by employing the concept of residual likelihood estimation, it eliminates the need for complex upsampling processes, thereby accelerating the acupoint localization task. Our method achieved state-of-the-art (SOTA) accuracy on a private dataset of acupoints on the human back, with an average Euclidean distance pixel error (EPE) of 7.792 and an average time consumption of 10.05 milliseconds per localization task. Compared to the second-best algorithm, our method improved both accuracy and speed by approximately 14\%. This significant advancement not only enhances the efficacy of acupuncture treatment but also demonstrates the commercial potential of automated acupuncture robot systems. Access to our method is available at https://github.com/Sohyu1/RT-DEMT

### DAViMNet: SSMs-Based Domain Adaptive Object Detection 
[[arxiv](https://arxiv.org/abs/2502.11178)] [[cool](https://papers.cool/arxiv/2502.11178)] [[pdf](https://arxiv.org/pdf/2502.11178)]
> **Authors**: A. Enes Doruk,Hasan F. Ates
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Unsupervised domain adaptation (UDA) for object detection adapts models trained on labeled source domains to unlabeled target domains, ensuring robust performance across domain shifts. Transformer-based architectures excel at capturing long-range dependencies but face efficiency challenges due to their quadratic attention complexity, which limits scalability in UDA tasks. To address these issues, we propose a hybrid domain-adaptive Mamba Transformer architecture that combines Mamba's efficient state-space modeling with attention mechanisms to tackle domain-specific spatial and channel-wise variations. Each hybrid block integrates domain-adaptive Mamba blocks and attention mechanisms: Domain-Adaptive Mamba employs spatial and channel state-space models to adaptively model domain variations, while attention mechanisms leverage self-attention for intra-domain feature enhancement and cross-attention for effective source-target alignment. Our approach processes both shallow and deeper features, employing an entropy-based knowledge distillation framework with margin ReLU to emphasize discriminative features and suppress noise. Gradient Reversal Layers enable adversarial alignment across network layers, while entropy-driven gating attention with random perturbations refines target features and mitigates overfitting. By unifying these components, our architecture achieves state-of-the-art performance in UDA object detection, balancing efficiency with robust generalization.

### Knowing Your Target: Target-Aware Transformer Makes Better Spatio-Temporal Video Grounding 
[[arxiv](https://arxiv.org/abs/2502.11168)] [[cool](https://papers.cool/arxiv/2502.11168)] [[pdf](https://arxiv.org/pdf/2502.11168)]
> **Authors**: Xin Gu,Yaojie Shen,Chenxi Luo,Tiejian Luo,Yan Huang,Yuewei Lin,Heng Fan,Libo Zhang
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Transformer has attracted increasing interest in STVG, owing to its end-to-end pipeline and promising result. Existing Transformer-based STVG approaches often leverage a set of object queries, which are initialized simply using zeros and then gradually learn target position information via iterative interactions with multimodal features, for spatial and temporal localization. Despite simplicity, these zero object queries, due to lacking target-specific cues, are hard to learn discriminative target information from interactions with multimodal features in complicated scenarios (\e.g., with distractors or occlusion), resulting in degradation. Addressing this, we introduce a novel Target-Aware Transformer for STVG (TA-STVG), which seeks to adaptively generate object queries via exploring target-specific cues from the given video-text pair, for improving STVG. The key lies in two simple yet effective modules, comprising text-guided temporal sampling (TTS) and attribute-aware spatial activation (ASA), working in a cascade. The former focuses on selecting target-relevant temporal cues from a video utilizing holistic text information, while the latter aims at further exploiting the fine-grained visual attribute information of the object from previous target-aware temporal cues, which is applied for object query initialization. Compared to existing methods leveraging zero-initialized queries, object queries in our TA-STVG, directly generated from a given video-text pair, naturally carry target-specific cues, making them adaptive and better interact with multimodal features for learning more discriminative information to improve STVG. In our experiments on three benchmarks, TA-STVG achieves state-of-the-art performance and significantly outperforms the baseline, validating its efficacy.

### VLMs as GeoGuessr Masters: Exceptional Performance, Hidden Biases, and Privacy Risks 
[[arxiv](https://arxiv.org/abs/2502.11163)] [[cool](https://papers.cool/arxiv/2502.11163)] [[pdf](https://arxiv.org/pdf/2502.11163)]
> **Authors**: Jingyuan Huang,Jen-tse Huang,Ziyi Liu,Xiaoyuan Liu,Wenxuan Wang,Jieyu Zhao
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 8 pages
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算语言学
- **Abstract**: Visual-Language Models (VLMs) have shown remarkable performance across various tasks, particularly in recognizing geographic information from images. However, significant challenges remain, including biases and privacy concerns. To systematically address these issues in the context of geographic information recognition, we introduce a benchmark dataset consisting of 1,200 images paired with detailed geographic metadata. Evaluating four VLMs, we find that while these models demonstrate the ability to recognize geographic information from images, achieving up to $53.8\%$ accuracy in city prediction, they exhibit significant regional biases. Specifically, performance is substantially higher for economically developed and densely populated regions compared to less developed ($-12.5\%$) and sparsely populated ($-17.0\%$) areas. Moreover, the models exhibit regional biases, frequently overpredicting certain locations; for instance, they consistently predict Sydney for images taken in Australia. The strong performance of VLMs also raises privacy concerns, particularly for users who share images online without the intent of being identified. Our code and dataset are publicly available at https://github.com/uscnlp-lime/FairLocator.

### AnyRefill: A Unified, Data-Efficient Framework for Left-Prompt-Guided Vision Tasks 
[[arxiv](https://arxiv.org/abs/2502.11158)] [[cool](https://papers.cool/arxiv/2502.11158)] [[pdf](https://arxiv.org/pdf/2502.11158)]
> **Authors**: Ming Xie,Chenjie Cao,Yunuo Cai,Xiangyang Xue,Yu-Gang Jiang,Yanwei Fu
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 19 pages, submitted to TPAMI
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: In this paper, we present a novel Left-Prompt-Guided (LPG) paradigm to address a diverse range of reference-based vision tasks. Inspired by the human creative process, we reformulate these tasks using a left-right stitching formulation to construct contextual input. Building upon this foundation, we propose AnyRefill, an extension of LeftRefill, that effectively adapts Text-to-Image (T2I) models to various vision tasks. AnyRefill leverages the inpainting priors of advanced T2I model based on the Diffusion Transformer (DiT) architecture, and incorporates flexible components to enhance its capabilities. By combining task-specific LoRAs with the stitching input, AnyRefill unlocks its potential across diverse tasks, including conditional generation, visual perception, and image editing, without requiring additional visual encoders. Meanwhile, AnyRefill exhibits remarkable data efficiency, requiring minimal task-specific fine-tuning while maintaining high generative performance. Through extensive ablation studies, we demonstrate that AnyRefill outperforms other image condition injection methods and achieves competitive results compared to state-of-the-art open-source methods. Notably, AnyRefill delivers results comparable to advanced commercial tools, such as IC-Light and SeedEdit, even in challenging scenarios. Comprehensive experiments and ablation studies across versatile tasks validate the strong generation of the proposed simple yet effective LPG formulation, establishing AnyRefill as a unified, highly data-efficient solution for reference-based vision tasks.

### Faces of Fairness: Examining Bias in Facial Expression Recognition Datasets and Models 
[[arxiv](https://arxiv.org/abs/2502.11049)] [[cool](https://papers.cool/arxiv/2502.11049)] [[pdf](https://arxiv.org/pdf/2502.11049)]
> **Authors**: Mohammad Mehdi Hosseini,Ali Pourramezan Fard,Mohammad H. Mahoor
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Building AI systems, including Facial Expression Recognition (FER), involves two critical aspects: data and model design. Both components significantly influence bias and fairness in FER tasks. Issues related to bias and fairness in FER datasets and models remain underexplored. This study investigates bias sources in FER datasets and models. Four common FER datasets--AffectNet, ExpW, Fer2013, and RAF-DB--are analyzed. The findings demonstrate that AffectNet and ExpW exhibit high generalizability despite data imbalances. Additionally, this research evaluates the bias and fairness of six deep models, including three state-of-the-art convolutional neural network (CNN) models: MobileNet, ResNet, XceptionNet, as well as three transformer-based models: ViT, CLIP, and GPT-4o-mini. Experimental results reveal that while GPT-4o-mini and ViT achieve the highest accuracy scores, they also display the highest levels of bias. These findings underscore the urgent need for developing new methodologies to mitigate bias and ensure fairness in datasets and models, particularly in affective computing applications. See our implementation details at https://github.com/MMHosseini/bias_in_FER.

### Detecting Cadastral Boundary from Satellite Images Using U-Net model 
[[arxiv](https://arxiv.org/abs/2502.11044)] [[cool](https://papers.cool/arxiv/2502.11044)] [[pdf](https://arxiv.org/pdf/2502.11044)]
> **Authors**: Neda Rahimpour Anaraki,Maryam Tahmasbi,Saeed Reza Kheradpisheh
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 14 pages, 7 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Finding the cadastral boundaries of farmlands is a crucial concern for land administration. Therefore, using deep learning methods to expedite and simplify the extraction of cadastral boundaries from satellite and unmanned aerial vehicle (UAV) images is critical. In this paper, we employ transfer learning to train a U-Net model with a ResNet34 backbone to detect cadastral boundaries through three-class semantic segmentation: "boundary", "field", and "background". We evaluate the performance on two satellite images from farmlands in Iran using "precision", "recall", and "F-score", achieving high values of 88%, 75%, and 81%, respectively, which indicate promising results.

### TPCap: Unlocking Zero-Shot Image Captioning with Trigger-Augmented and Multi-Modal Purification Modules 
[[arxiv](https://arxiv.org/abs/2502.11024)] [[cool](https://papers.cool/arxiv/2502.11024)] [[pdf](https://arxiv.org/pdf/2502.11024)]
> **Authors**: Ruoyu Zhang,Lulu Wang,Yi He,Tongling Pan,Zhengtao Yu,Yingna Li
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Recent advancements in large language models (LLMs) have significantly enhanced the fluency and logical coherence of image captioning. Retrieval-Augmented Generation (RAG) is widely adopted to incorporate external knowledge into LLMs; however, existing RAG-based methods rely on separate retrieval banks, introducing computational overhead and limiting the utilization of LLMs' inherent zero-shot capabilities. To address these limitations, we propose TPCap, a novel trigger-augmented and multi-modal purification framework for zero-shot image captioning without external retrieval libraries. TPCap consists of two key components: trigger-augmented (TA) generation and multi-modal purification (MP). The TA module employs a trigger projector with frozen and learnable projections to activate LLMs' contextual reasoning, enhance visual-textual alignment, and mitigate data bias. The MP module further refines the generated entity-related information by filtering noise and enhancing feature quality, ensuring more precise and factually consistent captions. We evaluate TPCap on COCO, NoCaps, Flickr30k, and WHOOPS datasets. With only 0.82M trainable parameters and training on a single NVIDIA RTX 4090 GPU, TPCap achieves competitive performance comparable to state-of-the-art models.

### FeaKM: Robust Collaborative Perception under Noisy Pose Conditions 
[[arxiv](https://arxiv.org/abs/2502.11003)] [[cool](https://papers.cool/arxiv/2502.11003)] [[pdf](https://arxiv.org/pdf/2502.11003)]
> **Authors**: Jiuwu Hao,Liguo Sun,Ti Xiang,Yuting Wan,Haolin Song,Pin Lv
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: Accepted by JCRAI 2024
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Collaborative perception is essential for networks of agents with limited sensing capabilities, enabling them to work together by exchanging information to achieve a robust and comprehensive understanding of their environment. However, localization inaccuracies often lead to significant spatial message displacement, which undermines the effectiveness of these collaborative efforts. To tackle this challenge, we introduce FeaKM, a novel method that employs Feature-level Keypoints Matching to effectively correct pose discrepancies among collaborating agents. Our approach begins by utilizing a confidence map to identify and extract salient points from intermediate feature representations, allowing for the computation of their descriptors. This step ensures that the system can focus on the most relevant information, enhancing the matching process. We then implement a target-matching strategy that generates an assignment matrix, correlating the keypoints identified by different agents. This is critical for establishing accurate correspondences, which are essential for effective collaboration. Finally, we employ a fine-grained transformation matrix to synchronize the features of all agents and ascertain their relative statuses, ensuring coherent communication among them. Our experimental results demonstrate that FeaKM significantly outperforms existing methods on the DAIR-V2X dataset, confirming its robustness even under severe noise conditions. The code and implementation details are available at https://github.com/uestchjw/FeaKM.

### ControlText: Unlocking Controllable Fonts in Multilingual Text Rendering without Font Annotations 
[[arxiv](https://arxiv.org/abs/2502.10999)] [[cool](https://papers.cool/arxiv/2502.10999)] [[pdf](https://arxiv.org/pdf/2502.10999)]
> **Authors**: Bowen Jiang,Yuan Yuan,Xinyi Bai,Zhuoqun Hao,Alyson Yin,Yaojie Hu,Wenyu Liao,Lyle Ungar,Camillo J. Taylor
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: This is preliminary work and code will be released at github.com/bowen-upenn/ControlText
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,多媒体
- **Abstract**: This work demonstrates that diffusion models can achieve font-controllable multilingual text rendering using just raw images without font label annotations. Visual text rendering remains a significant challenge. While recent methods condition diffusion on glyphs, it is impossible to retrieve exact font annotations from large-scale, real-world datasets, which prevents user-specified font control. To address this, we propose a data-driven solution that integrates the conditional diffusion model with a text segmentation model, utilizing segmentation masks to capture and represent fonts in pixel space in a self-supervised manner, thereby eliminating the need for any ground-truth labels and enabling users to customize text rendering with any multilingual font of their choice. The experiment provides a proof of concept of our algorithm in zero-shot text and font editing across diverse fonts and languages, providing valuable insights for the community and industry toward achieving generalized visual text rendering.

### OMG: Opacity Matters in Material Modeling with Gaussian Splatting 
[[arxiv](https://arxiv.org/abs/2502.10988)] [[cool](https://papers.cool/arxiv/2502.10988)] [[pdf](https://arxiv.org/pdf/2502.10988)]
> **Authors**: Silong Yong,Venkata Nagarjun Pudureddiyur Manivannan,Bernhard Kerbl,Zifu Wan,Simon Stepputtis,Katia Sycara,Yaqi Xie
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: Published as a conference paper at ICLR 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Decomposing geometry, materials and lighting from a set of images, namely inverse rendering, has been a long-standing problem in computer vision and graphics. Recent advances in neural rendering enable photo-realistic and plausible inverse rendering results. The emergence of 3D Gaussian Splatting has boosted it to the next level by showing real-time rendering potentials. An intuitive finding is that the models used for inverse rendering do not take into account the dependency of opacity w.r.t. material properties, namely cross section, as suggested by optics. Therefore, we develop a novel approach that adds this dependency to the modeling itself. Inspired by radiative transfer, we augment the opacity term by introducing a neural network that takes as input material properties to provide modeling of cross section and a physically correct activation function. The gradients for material properties are therefore not only from color but also from opacity, facilitating a constraint for their optimization. Therefore, the proposed method incorporates more accurate physical properties compared to previous works. We implement our method into 3 different baselines that use Gaussian Splatting for inverse rendering and achieve significant improvements universally in terms of novel view synthesis and material modeling.

### Skillful Nowcasting of Convective Clouds With a Cascade Diffusion Model 
[[arxiv](https://arxiv.org/abs/2502.10957)] [[cool](https://papers.cool/arxiv/2502.10957)] [[pdf](https://arxiv.org/pdf/2502.10957)]
> **Authors**: Haoming Chen,Xiaohui Zhong,Qiang Zhai,Xiaomeng Li,Ying Wa Chan,Pak Wai Chan,Yuanyuan Huang,Hao Li,Xiaoming Shi
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,大气和海洋物理
- **Abstract**: Accurate nowcasting of convective clouds from satellite imagery is essential for mitigating the impacts of meteorological disasters, especially in developing countries and remote regions with limited ground-based observations. Recent advances in deep learning have shown promise in video prediction; however, existing models frequently produce blurry results and exhibit reduced accuracy when forecasting physical fields. Here, we introduce SATcast, a diffusion model that leverages a cascade architecture and multimodal inputs for nowcasting cloud fields in satellite imagery. SATcast incorporates physical fields predicted by FuXi, a deep-learning weather model, alongside past satellite observations as conditional inputs to generate high-quality future cloud fields. Through comprehensive evaluation, SATcast outperforms conventional methods on multiple metrics, demonstrating its superior accuracy and robustness. Ablation studies underscore the importance of its multimodal design and the cascade architecture in achieving reliable predictions. Notably, SATcast maintains predictive skill for up to 24 hours, underscoring its potential for operational nowcasting applications.

### A recurrent vision transformer shows signatures of primate visual attention 
[[arxiv](https://arxiv.org/abs/2502.10955)] [[cool](https://papers.cool/arxiv/2502.10955)] [[pdf](https://arxiv.org/pdf/2502.10955)]
> **Authors**: Jonathan Morgan,Badr Albanna,James P. Herman
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,神经元和认知
- **Abstract**: Attention is fundamental to both biological and artificial intelligence, yet research on animal attention and AI self attention remains largely disconnected. We propose a Recurrent Vision Transformer (Recurrent ViT) that integrates self-attention with recurrent memory, allowing both current inputs and stored information to guide attention allocation. Trained solely via sparse reward feedback on a spatially cued orientation change detection task, a paradigm used in primate studies, our model exhibits primate like signatures of attention, including improved accuracy and faster responses for cued stimuli that scale with cue validity. Analysis of self-attention maps reveals dynamic spatial prioritization with reactivation prior to expected changes, and targeted perturbations produce performance shifts similar to those observed in primate frontal eye fields and superior colliculus. These findings demonstrate that incorporating recurrent feedback into self attention can capture key aspects of primate visual attention.

### Learning to Stop Overthinking at Test Time 
[[arxiv](https://arxiv.org/abs/2502.10954)] [[cool](https://papers.cool/arxiv/2502.10954)] [[pdf](https://arxiv.org/pdf/2502.10954)]
> **Authors**: Hieu Tran Bao,Nguyen Cong Dat,Nguyen Duc Anh,Hoang Thanh-Tung
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: Test time scaling is currently one of the most active research areas that shows promise after training time scaling has reached its limits. Deep-thinking (DT) models are a class of recurrent models that can perform easy-to-hard generalization by assigning more compute to harder test samples. However, due to their inability to determine the complexity of a test sample, DT models have to use a large amount of computation for both easy and hard test samples. Excessive test time computation is wasteful and can cause the ``overthinking'' problem where more test time computation leads to worse results. In this paper, we introduce a test time training method for determining the optimal amount of computation needed for each sample during test time. We also propose Conv-LiGRU, a novel recurrent architecture for efficient and robust visual reasoning. Extensive experiments demonstrate that Conv-LiGRU is more stable than DT, effectively mitigates the ``overthinking'' phenomenon, and achieves superior accuracy.

### Automatic Quality Assessment of First Trimester Crown-Rump-Length Ultrasound Images 
[[arxiv](https://arxiv.org/abs/2502.10908)] [[cool](https://papers.cool/arxiv/2502.10908)] [[pdf](https://arxiv.org/pdf/2502.10908)]
> **Authors**: Sevim Cengiz,Ibraheem Hamdi,Mohammad Yaqub
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: 9 pages, 2 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: Fetal gestational age (GA) is vital clinical information that is estimated during pregnancy in order to assess fetal growth. This is usually performed by measuring the crown-rump-length (CRL) on an ultrasound image in the Dating scan which is then correlated with fetal age and growth trajectory. A major issue when performing the CRL measurement is ensuring that the image is acquired at the correct view, otherwise it could be misleading. Although clinical guidelines specify the criteria for the correct CRL view, sonographers may not regularly adhere to such rules. In this paper, we propose a new deep learning-based solution that is able to verify the adherence of a CRL image to clinical guidelines in order to assess image quality and facilitate accurate estimation of GA. We first segment out important fetal structures then use the localized structures to perform a clinically-guided mapping that verifies the adherence of criteria. The segmentation method combines the benefits of Convolutional Neural Network (CNN) and the Vision Transformer (ViT) to segment fetal structures in ultrasound images and localize important fetal landmarks. For segmentation purposes, we compare our proposed work with UNet and show that our CNN/ViT-based method outperforms an optimized version of UNet. Furthermore, we compare the output of the mapping with classification CNNs when assessing the clinical criteria and the overall acceptability of CRL images. We show that the proposed mapping is not only explainable but also more accurate than the best performing classification CNNs.

### Breaking Down the Hierarchy: A New Approach to Leukemia Classification 
[[arxiv](https://arxiv.org/abs/2502.10899)] [[cool](https://papers.cool/arxiv/2502.10899)] [[pdf](https://arxiv.org/pdf/2502.10899)]
> **Authors**: Ibraheem Hamdi,Hosam El-Gendy,Ahmed Sharshar,Mohamed Saeed,Muhammad Ridzuan,Shahrukh K. Hashmi,Naveed Syed,Imran Mirza,Shakir Hussain,Amira Mahmoud Abdalla,Mohammad Yaqub
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: 9 pages, 11 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: The complexities inherent to leukemia, multifaceted cancer affecting white blood cells, pose considerable diagnostic and treatment challenges, primarily due to reliance on laborious morphological analyses and expert judgment that are susceptible to errors. Addressing these challenges, this study presents a refined, comprehensive strategy leveraging advanced deep-learning techniques for the classification of leukemia subtypes. We commence by developing a hierarchical label taxonomy, paving the way for differentiating between various subtypes of leukemia. The research further introduces a novel hierarchical approach inspired by clinical procedures capable of accurately classifying diverse types of leukemia alongside reactive and healthy cells. An integral part of this study involves a meticulous examination of the performance of Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) as classifiers. The proposed method exhibits an impressive success rate, achieving approximately 90\% accuracy across all leukemia subtypes, as substantiated by our experimental results. A visual representation of the experimental findings is provided to enhance the model's explainability and aid in understanding the classification process.

### Super Resolution image reconstructs via total variation-based image deconvolution: a majorization-minimization approach 
[[arxiv](https://arxiv.org/abs/2502.10876)] [[cool](https://papers.cool/arxiv/2502.10876)] [[pdf](https://arxiv.org/pdf/2502.10876)]
> **Authors**: Mouhamad Chehaitly
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: 60 pages
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: This work aims to reconstruct image sequences with Total Variation regularity in super-resolution. We consider, in particular, images of scenes for which the point-to-point image transformation is a plane projective transformation. We first describe the super-resolution image's imaging observation model, an interpolation and Fusion estimator, and Projection on Convex Sets. We explain motion and compute the optical flow of a sequence of images using the Horn-Shunck algorithm to estimate motion. We then propose a Total Variation regulazer via a Majorization-Minimization approach to obtain a suitable result. Super Resolution restoration from motion measurements is also discussed. Finally, the simulation's part demonstrates the power of the proposed methodology. As expected, this model does not give real-time results, as seen in the numerical experiments section, but it is the cornerstone for future approaches. Finally, the simulation's part demonstrates the power of the proposed methodology. As expected, this model does not give real-time results, as seen in the numerical experiments section, but it is the cornerstone for future approaches.

### SkyReels-A1: Expressive Portrait Animation in Video Diffusion Transformers 
[[arxiv](https://arxiv.org/abs/2502.10841)] [[cool](https://papers.cool/arxiv/2502.10841)] [[pdf](https://arxiv.org/pdf/2502.10841)]
> **Authors**: Di Qiu,Zhengcong Fei,Rui Wang,Jialin Bai,Changqian Yu,Mingyuan Fan,Guibin Chen,Xiang Wen
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: We present SkyReels-A1, a simple yet effective framework built upon video diffusion Transformer to facilitate portrait image animation. Existing methodologies still encounter issues, including identity distortion, background instability, and unrealistic facial dynamics, particularly in head-only animation scenarios. Besides, extending to accommodate diverse body proportions usually leads to visual inconsistencies or unnatural articulations. To address these challenges, SkyReels-A1 capitalizes on the strong generative capabilities of video DiT, enhancing facial motion transfer precision, identity retention, and temporal coherence. The system incorporates an expression-aware conditioning module that enables seamless video synthesis driven by expression-guided landmark inputs. Integrating the facial image-text alignment module strengthens the fusion of facial attributes with motion trajectories, reinforcing identity preservation. Additionally, SkyReels-A1 incorporates a multi-stage training paradigm to incrementally refine the correlation between expressions and motion while ensuring stable identity reproduction. Extensive empirical evaluations highlight the model's ability to produce visually coherent and compositionally diverse results, making it highly applicable to domains such as virtual avatars, remote communication, and digital media generation.

### Transformer-Driven Modeling of Variable Frequency Features for Classifying Student Engagement in Online Learning 
[[arxiv](https://arxiv.org/abs/2502.10813)] [[cool](https://papers.cool/arxiv/2502.10813)] [[pdf](https://arxiv.org/pdf/2502.10813)]
> **Authors**: Sandeep Mandia,Kuldeep Singh,Rajendra Mitharwal,Faisel Mushtaq,Dimpal Janu
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: 22 pages, 5 figures, and 6 tables
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: The COVID-19 pandemic and the internet's availability have recently boosted online learning. However, monitoring engagement in online learning is a difficult task for teachers. In this context, timely automatic student engagement classification can help teachers in making adaptive adjustments to meet students' needs. This paper proposes EngageFormer, a transformer based architecture with sequence pooling using video modality for engagement classification. The proposed architecture computes three views from the input video and processes them in parallel using transformer encoders; the global encoder then processes the representation from each encoder, and finally, multi layer perceptron (MLP) predicts the engagement level. A learning centered affective state dataset is curated from existing open source databases. The proposed method achieved an accuracy of 63.9%, 56.73%, 99.16%, 65.67%, and 74.89% on Dataset for Affective States in E-Environments (DAiSEE), Bahcesehir University Multimodal Affective Database-1 (BAUM-1), Yawning Detection Dataset (YawDD), University of Texas at Arlington Real-Life Drowsiness Dataset (UTA-RLDD), and curated learning-centered affective state dataset respectively. The achieved results on the BAUM-1, DAiSEE, and YawDD datasets demonstrate state-of-the-art performance, indicating the superiority of the proposed model in accurately classifying affective states on these datasets. Additionally, the results obtained on the UTA-RLDD dataset, which involves two-class classification, serve as a baseline for future research. These results provide a foundation for further investigations and serve as a point of reference for future works to compare and improve upon.

### SVBench: A Benchmark with Temporal Multi-Turn Dialogues for Streaming Video Understanding 
[[arxiv](https://arxiv.org/abs/2502.10810)] [[cool](https://papers.cool/arxiv/2502.10810)] [[pdf](https://arxiv.org/pdf/2502.10810)]
> **Authors**: Zhenyu Yang,Yuhang Hu,Zemin Du,Dizhan Xue,Shengsheng Qian,Jiahong Wu,Fan Yang,Weiming Dong,Changsheng Xu
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: ICLR 2025 Accept (Spotlight)
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Despite the significant advancements of Large Vision-Language Models (LVLMs) on established benchmarks, there remains a notable gap in suitable evaluation regarding their applicability in the emerging domain of long-context streaming video understanding. Current benchmarks for video understanding typically emphasize isolated single-instance text inputs and fail to evaluate the capacity to sustain temporal reasoning throughout the entire duration of video streams. To address these limitations, we introduce SVBench, a pioneering benchmark with temporal multi-turn question-answering chains specifically designed to thoroughly assess the capabilities of streaming video understanding of current LVLMs. We design a semi-automated annotation pipeline to obtain 49,979 Question-Answer (QA) pairs of 1,353 streaming videos, which includes generating QA chains that represent a series of consecutive multi-turn dialogues over video segments and constructing temporal linkages between successive QA chains. Our experimental results, obtained from 14 models in dialogue and streaming evaluations, reveal that while the closed-source GPT-4o outperforms others, most open-source LVLMs struggle with long-context streaming video understanding. We also construct a StreamingChat model, which significantly outperforms open-source LVLMs on our SVBench and achieves comparable performance on diverse vision-language benchmarks. We expect SVBench to advance the research of streaming video understanding by providing a comprehensive and in-depth analysis of current LVLMs. Our benchmark and model can be accessed at https://yzy-bupt.github.io/SVBench.

### Distraction is All You Need for Multimodal Large Language Model Jailbreaking 
[[arxiv](https://arxiv.org/abs/2502.10794)] [[cool](https://papers.cool/arxiv/2502.10794)] [[pdf](https://arxiv.org/pdf/2502.10794)]
> **Authors**: Zuopeng Yang,Jiluan Fan,Anli Yan,Erdun Gao,Xin Lin,Tao Li,Kanghua mo,Changyu Dong
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Multimodal Large Language Models (MLLMs) bridge the gap between visual and textual data, enabling a range of advanced applications. However, complex internal interactions among visual elements and their alignment with text can introduce vulnerabilities, which may be exploited to bypass safety mechanisms. To address this, we analyze the relationship between image content and task and find that the complexity of subimages, rather than their content, is key. Building on this insight, we propose the Distraction Hypothesis, followed by a novel framework called Contrasting Subimage Distraction Jailbreaking (CS-DJ), to achieve jailbreaking by disrupting MLLMs alignment through multi-level distraction strategies. CS-DJ consists of two components: structured distraction, achieved through query decomposition that induces a distributional shift by fragmenting harmful prompts into sub-queries, and visual-enhanced distraction, realized by constructing contrasting subimages to disrupt the interactions among visual elements within the model. This dual strategy disperses the model's attention, reducing its ability to detect and mitigate harmful content. Extensive experiments across five representative scenarios and four popular closed-source MLLMs, including GPT-4o-mini, GPT-4o, GPT-4V, and Gemini-1.5-Flash, demonstrate that CS-DJ achieves average success rates of 52.40% for the attack success rate and 74.10% for the ensemble attack success rate. These results reveal the potential of distraction-based approaches to exploit and bypass MLLMs' defenses, offering new insights for attack strategies.

### VarGes: Improving Variation in Co-Speech 3D Gesture Generation via StyleCLIPS 
[[arxiv](https://arxiv.org/abs/2502.10729)] [[cool](https://papers.cool/arxiv/2502.10729)] [[pdf](https://arxiv.org/pdf/2502.10729)]
> **Authors**: Ming Meng,Ke Mu,Yonggui Zhu,Zhe Zhu,Haoyu Sun,Heyang Yan,Zhaoxin Fan
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Generating expressive and diverse human gestures from audio is crucial in fields like human-computer interaction, virtual reality, and animation. Though existing methods have achieved remarkable performance, they often exhibit limitations due to constrained dataset diversity and the restricted amount of information derived from audio inputs. To address these challenges, we present VarGes, a novel variation-driven framework designed to enhance co-speech gesture generation by integrating visual stylistic cues while maintaining naturalness. Our approach begins with the Variation-Enhanced Feature Extraction (VEFE) module, which seamlessly incorporates \textcolor{blue}{style-reference} video data into a 3D human pose estimation network to extract StyleCLIPS, thereby enriching the input with stylistic information. Subsequently, we employ the Variation-Compensation Style Encoder (VCSE), a transformer-style encoder equipped with an additive attention mechanism pooling layer, to robustly encode diverse StyleCLIPS representations and effectively manage stylistic variations. Finally, the Variation-Driven Gesture Predictor (VDGP) module fuses MFCC audio features with StyleCLIPS encodings via cross-attention, injecting this fused data into a cross-conditional autoregressive model to modulate 3D human gesture generation based on audio input and stylistic clues. The efficacy of our approach is validated on benchmark datasets, where it outperforms existing methods in terms of gesture diversity and naturalness. The code and video results will be made publicly available upon acceptance:https://github.com/mookerr/VarGES/ .

### Improving action segmentation via explicit similarity measurement 
[[arxiv](https://arxiv.org/abs/2502.10713)] [[cool](https://papers.cool/arxiv/2502.10713)] [[pdf](https://arxiv.org/pdf/2502.10713)]
> **Authors**: Kamel Aouaidjia,Wenhao Zhang,Aofan Li,Chongsheng Zhang
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: 13 pages, 5 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Existing supervised action segmentation methods depend on the quality of frame-wise classification using attention mechanisms or temporal convolutions to capture temporal dependencies. Even boundary detection-based methods primarily depend on the accuracy of an initial frame-wise classification, which can overlook precise identification of segments and boundaries in case of low-quality prediction. To address this problem, this paper proposes ASESM (Action Segmentation via Explicit Similarity Measurement) to enhance the segmentation accuracy by incorporating explicit similarity evaluation across frames and predictions. Our supervised learning architecture uses frame-level multi-resolution features as input to multiple Transformer encoders. The resulting multiple frame-wise predictions are used for similarity voting to obtain high quality initial prediction. We apply a newly proposed boundary correction algorithm that operates based on feature similarity between consecutive frames to adjust the boundary locations iteratively through the learning process. The corrected prediction is then further refined through multiple stages of temporal convolutions. As post-processing, we optionally apply boundary correction again followed by a segment smoothing method that removes outlier classes within segments using similarity measurement between consecutive predictions. Additionally, we propose a fully unsupervised boundary detection-correction algorithm that identifies segment boundaries based solely on feature similarity without any training. Experiments on 50Salads, GTEA, and Breakfast datasets show the effectiveness of both the supervised and unsupervised algorithms. Code and models are made available on Github.

### Occlusion-aware Non-Rigid Point Cloud Registration via Unsupervised Neural Deformation Correntropy 
[[arxiv](https://arxiv.org/abs/2502.10704)] [[cool](https://papers.cool/arxiv/2502.10704)] [[pdf](https://arxiv.org/pdf/2502.10704)]
> **Authors**: Mingyang Zhao,Gaofeng Meng,Dong-Ming Yan
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: [ICLR 2025] Project and code at: https://github.com/zikai1/OAReg
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Non-rigid alignment of point clouds is crucial for scene understanding, reconstruction, and various computer vision and robotics tasks. Recent advancements in implicit deformation networks for non-rigid registration have significantly reduced the reliance on large amounts of annotated training data. However, existing state-of-the-art methods still face challenges in handling occlusion scenarios. To address this issue, this paper introduces an innovative unsupervised method called Occlusion-Aware Registration (OAR) for non-rigidly aligning point clouds. The key innovation of our method lies in the utilization of the adaptive correntropy function as a localized similarity measure, enabling us to treat individual points distinctly. In contrast to previous approaches that solely minimize overall deviations between two shapes, we combine unsupervised implicit neural representations with the maximum correntropy criterion to optimize the deformation of unoccluded regions. This effectively avoids collapsed, tearing, and other physically implausible results. Moreover, we present a theoretical analysis and establish the relationship between the maximum correntropy criterion and the commonly used Chamfer distance, highlighting that the correntropy-induced metric can be served as a more universal measure for point cloud analysis. Additionally, we introduce locally linear reconstruction to ensure that regions lacking correspondences between shapes still undergo physically natural deformations. Our method achieves superior or competitive performance compared to existing approaches, particularly when dealing with occluded geometries. We also demonstrate the versatility of our method in challenging tasks such as large deformations, shape interpolation, and shape completion under occlusion disturbances.

### CLoCKDistill: Consistent Location-and-Context-aware Knowledge Distillation for DETRs 
[[arxiv](https://arxiv.org/abs/2502.10683)] [[cool](https://papers.cool/arxiv/2502.10683)] [[pdf](https://arxiv.org/pdf/2502.10683)]
> **Authors**: Qizhen Lan,Qing Tian
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: 8 pages
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Object detection has advanced significantly with Detection Transformers (DETRs). However, these models are computationally demanding, posing challenges for deployment in resource-constrained environments (e.g., self-driving cars). Knowledge distillation (KD) is an effective compression method widely applied to CNN detectors, but its application to DETR models has been limited. Most KD methods for DETRs fail to distill transformer-specific global context. Also, they blindly believe in the teacher model, which can sometimes be misleading. To bridge the gaps, this paper proposes Consistent Location-and-Context-aware Knowledge Distillation (CLoCKDistill) for DETR detectors, which includes both feature distillation and logit distillation components. For feature distillation, instead of distilling backbone features like existing KD methods, we distill the transformer encoder output (i.e., memory) that contains valuable global context and long-range dependencies. Also, we enrich this memory with object location details during feature distillation so that the student model can prioritize relevant regions while effectively capturing the global context. To facilitate logit distillation, we create target-aware queries based on the ground truth, allowing both the student and teacher decoders to attend to consistent and accurate parts of encoder memory. Experiments on the KITTI and COCO datasets show our CLoCKDistill method's efficacy across various DETRs, e.g., single-scale DAB-DETR, multi-scale deformable DETR, and denoising-based DINO. Our method boosts student detector performance by 2.2% to 6.4%.

### Hybrid Deepfake Image Detection: A Comprehensive Dataset-Driven Approach Integrating Convolutional and Attention Mechanisms with Frequency Domain Features 
[[arxiv](https://arxiv.org/abs/2502.10682)] [[cool](https://papers.cool/arxiv/2502.10682)] [[pdf](https://arxiv.org/pdf/2502.10682)]
> **Authors**: Kafi Anan,Anindya Bhattacharjee,Ashir Intesher,Kaidul Islam,Abrar Assaeem Fuad,Utsab Saha,Hafiz Imtiaz
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: Under review in Elsevier Image and Vision Computing
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习,图像和视频处理
- **Abstract**: Effective deepfake detection tools are becoming increasingly essential over the last few years due to the growing usage of deepfakes in unethical practices. There exists a diverse range of deepfake generation techniques, which makes it challenging to develop an accurate universal detection mechanism. The 2025 Signal Processing Cup (DFWild-Cup competition) provided a diverse dataset of deepfake images, which are generated from multiple deepfake image generators, for training machine learning model(s) to emphasize the generalization of deepfake detection. To this end, we proposed an ensemble-based approach that employs three different neural network architectures: a ResNet-34-based architecture, a data-efficient image transformer (DeiT), and an XceptionNet with Wavelet Transform to capture both local and global features of deepfakes. We visualize the specific regions that these models focus for classification using Grad-CAM, and empirically demonstrate the effectiveness of these models in grouping real and fake images into cohesive clusters using t-SNE plots. Individually, the ResNet-34 architecture has achieved 88.9% accuracy, whereas the Xception network and the DeiT architecture have achieved 87.76% and 89.32% accuracy, respectively. With these networks, our weighted ensemble model achieves an excellent accuracy of 93.23% on the validation dataset of the SP Cup 2025 competition. Finally, the confusion matrix and an Area Under the ROC curve of 97.44% further confirm the stability of our proposed method.

### Hierarchically-Structured Open-Vocabulary Indoor Scene Synthesis with Pre-trained Large Language Model 
[[arxiv](https://arxiv.org/abs/2502.10675)] [[cool](https://papers.cool/arxiv/2502.10675)] [[pdf](https://arxiv.org/pdf/2502.10675)]
> **Authors**: Weilin Sun,Xinran Li,Manyi Li,Kai Xu,Xiangxu Meng,Lei Meng
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Indoor scene synthesis aims to automatically produce plausible, realistic and diverse 3D indoor scenes, especially given arbitrary user requirements. Recently, the promising generalization ability of pre-trained large language models (LLM) assist in open-vocabulary indoor scene synthesis. However, the challenge lies in converting the LLM-generated outputs into reasonable and physically feasible scene layouts. In this paper, we propose to generate hierarchically structured scene descriptions with LLM and then compute the scene layouts. Specifically, we train a hierarchy-aware network to infer the fine-grained relative positions between objects and design a divide-and-conquer optimization to solve for scene layouts. The advantages of using hierarchically structured scene representation are two-fold. First, the hierarchical structure provides a rough grounding for object arrangement, which alleviates contradictory placements with dense relations and enhances the generalization ability of the network to infer fine-grained placements. Second, it naturally supports the divide-and-conquer optimization, by first arranging the sub-scenes and then the entire scene, to more effectively solve for a feasible layout. We conduct extensive comparison experiments and ablation studies with both qualitative and quantitative evaluations to validate the effectiveness of our key designs with the hierarchically structured scene representation. Our approach can generate more reasonable scene layouts while better aligned with the user requirements and LLM descriptions. We also present open-vocabulary scene synthesis and interactive scene design results to show the strength of our approach in the applications.

### Occlusion-aware Text-Image-Point Cloud Pretraining for Open-World 3D Object Recognition 
[[arxiv](https://arxiv.org/abs/2502.10674)] [[cool](https://papers.cool/arxiv/2502.10674)] [[pdf](https://arxiv.org/pdf/2502.10674)]
> **Authors**: Khanh Nguyen,Ghulam Mubashar Hassan,Ajmal Mian
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Recent open-world representation learning approaches have leveraged CLIP to enable zero-shot 3D object recognition. However, performance on real point clouds with occlusions still falls short due to the unrealistic pretraining settings. Additionally, these methods incur high inference costs because they rely on Transformer's attention modules. In this paper, we make two contributions to address these limitations. First, we propose occlusion-aware text-image-point cloud pretraining to reduce the training-testing domain gap. From 52K synthetic 3D objects, our framework generates nearly 630K partial point clouds for pretraining, consistently improving real-world recognition performances of existing popular 3D networks. Second, to reduce computational requirements, we introduce DuoMamba, a two-stream linear state space model tailored for point clouds. By integrating two space-filling curves with 1D convolutions, DuoMamba effectively models spatial dependencies between point tokens, offering a powerful alternative to Transformer. When pretrained with our framework, DuoMamba surpasses current state-of-the-art methods while reducing latency and FLOPs, highlighting the potential of our approach for real-world applications. We will release our data and code to facilitate future research.

### Optimizing CNN Architectures for Advanced Thoracic Disease Classification 
[[arxiv](https://arxiv.org/abs/2502.10614)] [[cool](https://papers.cool/arxiv/2502.10614)] [[pdf](https://arxiv.org/pdf/2502.10614)]
> **Authors**: Tejas Mirthipati
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: Machine learning, particularly convolutional neural networks (CNNs), has shown promise in medical image analysis, especially for thoracic disease detection using chest X-ray images. In this study, we evaluate various CNN architectures, including binary classification, multi-label classification, and ResNet50 models, to address challenges like dataset imbalance, variations in image quality, and hidden biases. We introduce advanced preprocessing techniques such as principal component analysis (PCA) for image compression and propose a novel class-weighted loss function to mitigate imbalance issues. Our results highlight the potential of CNNs in medical imaging but emphasize that issues like unbalanced datasets and variations in image acquisition methods must be addressed for optimal model performance.

### Universal Lesion Segmentation Challenge 2023: A Comparative Research of Different Algorithms 
[[arxiv](https://arxiv.org/abs/2502.10608)] [[cool](https://papers.cool/arxiv/2502.10608)] [[pdf](https://arxiv.org/pdf/2502.10608)]
> **Authors**: Kaiwen Shi,Yifei Li,Binh Ho,Jovian Wang,Kobe Guo
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: In recent years, machine learning algorithms have achieved much success in segmenting lesions across various tissues. There is, however, not one satisfying model that works well on all tissue types universally. In response to this need, we attempt to train a model that 1) works well on all tissue types, and 2) is capable of still performing fast inferences. To this end, we design our architectures, test multiple existing architectures, compare their results, and settle upon SwinUnet. We document our rationales, successes, and failures. Finally, we propose some further directions that we think are worth exploring. codes: https://github.com/KWFredShi/ULS2023NGKD.git

### Adaptive Neural Networks for Intelligent Data-Driven Development 
[[arxiv](https://arxiv.org/abs/2502.10603)] [[cool](https://papers.cool/arxiv/2502.10603)] [[pdf](https://arxiv.org/pdf/2502.10603)]
> **Authors**: Youssef Shoeb,Azarm Nowzad,Hanno Gottschalk
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 8 pages, 3 figures, and 3 tables
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Advances in machine learning methods for computer vision tasks have led to their consideration for safety-critical applications like autonomous driving. However, effectively integrating these methods into the automotive development lifecycle remains challenging. Since the performance of machine learning algorithms relies heavily on the training data provided, the data and model development lifecycle play a key role in successfully integrating these components into the product development lifecycle. Existing models frequently encounter difficulties recognizing or adapting to novel instances not present in the original training dataset. This poses a significant risk for reliable deployment in dynamic environments. To address this challenge, we propose an adaptive neural network architecture and an iterative development framework that enables users to efficiently incorporate previously unknown objects into the current perception system. Our approach builds on continuous learning, emphasizing the necessity of dynamic updates to reflect real-world deployment conditions. Specifically, we introduce a pipeline with three key components: (1) a scalable network extension strategy to integrate new classes while preserving existing performance, (2) a dynamic OoD detection component that requires no additional retraining for newly added classes, and (3) a retrieval-based data augmentation process tailored for safety-critical deployments. The integration of these components establishes a pragmatic and adaptive pipeline for the continuous evolution of perception systems in the context of autonomous driving.

### Data-driven Super-Resolution of Flood Inundation Maps using Synthetic Simulations 
[[arxiv](https://arxiv.org/abs/2502.10601)] [[cool](https://papers.cool/arxiv/2502.10601)] [[pdf](https://arxiv.org/pdf/2502.10601)]
> **Authors**: Akshay Aravamudan,Zimeena Rasheed,Xi Zhang,Kira E. Scarpignato,Efthymios I. Nikolopoulos,Witold F. Krajewski,Georgios C. Anagnostopoulos
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 12 pages, 6 figures, 2 tables
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: The frequency of extreme flood events is increasing throughout the world. Daily, high-resolution (30m) Flood Inundation Maps (FIM) observed from space play a key role in informing mitigation and preparedness efforts to counter these extreme events. However, the temporal frequency of publicly available high-resolution FIMs, e.g., from Landsat, is at the order of two weeks thus limiting the effective monitoring of flood inundation dynamics. Conversely, global, low-resolution (~300m) Water Fraction Maps (WFM) are publicly available from NOAA VIIRS daily. Motivated by the recent successes of deep learning methods for single image super-resolution, we explore the effectiveness and limitations of similar data-driven approaches to downscaling low-resolution WFMs to high-resolution FIMs. To overcome the scarcity of high-resolution FIMs, we train our models with high-quality synthetic data obtained through physics-based simulations. We evaluate our models on real-world data from flood events in the state of Iowa. The study indicates that data-driven approaches exhibit superior reconstruction accuracy over non-data-driven alternatives and that the use of synthetic data is a viable proxy for training purposes. Additionally, we show that our trained models can exhibit superior zero-shot performance when transferred to regions with hydroclimatological similarity to the U.S. Midwest.

### Detecting and Monitoring Bias for Subgroups in Breast Cancer Detection AI 
[[arxiv](https://arxiv.org/abs/2502.10562)] [[cool](https://papers.cool/arxiv/2502.10562)] [[pdf](https://arxiv.org/pdf/2502.10562)]
> **Authors**: Amit Kumar Kundu,Florence X. Doo,Vaishnavi Patil,Amitabh Varshney,Joseph Jaja
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Automated mammography screening plays an important role in early breast cancer detection. However, current machine learning models, developed on some training datasets, may exhibit performance degradation and bias when deployed in real-world settings. In this paper, we analyze the performance of high-performing AI models on two mammography datasets-the Emory Breast Imaging Dataset (EMBED) and the RSNA 2022 challenge dataset. Specifically, we evaluate how these models perform across different subgroups, defined by six attributes, to detect potential biases using a range of classification metrics. Our analysis identifies certain subgroups that demonstrate notable underperformance, highlighting the need for ongoing monitoring of these subgroups' performance. To address this, we adopt a monitoring method designed to detect performance drifts over time. Upon identifying a drift, this method issues an alert, which can enable timely interventions. This approach not only provides a tool for tracking the performance but also helps ensure that AI models continue to perform effectively across diverse populations.

### PolyPath: Adapting a Large Multimodal Model for Multi-slide Pathology Report Generation 
[[arxiv](https://arxiv.org/abs/2502.10536)] [[cool](https://papers.cool/arxiv/2502.10536)] [[pdf](https://arxiv.org/pdf/2502.10536)]
> **Authors**: Faruk Ahmed,Lin Yang,Tiam Jaroensri,Andrew Sellergren,Yossi Matias,Avinatan Hassidim,Greg S. Corrado,Dale R. Webster,Shravya Shetty,Shruthi Prabhakara,Yun Liu,Daniel Golden,Ellery Wulczyn,David F. Steiner
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 8 main pages, 21 pages in total
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: The interpretation of histopathology cases underlies many important diagnostic and treatment decisions in medicine. Notably, this process typically requires pathologists to integrate and summarize findings across multiple slides per case. Existing vision-language capabilities in computational pathology have so far been largely limited to small regions of interest, larger regions at low magnification, or single whole-slide images (WSIs). This limits interpretation of findings that span multiple high-magnification regions across multiple WSIs. By making use of Gemini 1.5 Flash, a large multimodal model (LMM) with a 1-million token context window, we demonstrate the ability to generate bottom-line diagnoses from up to 40,000 768x768 pixel image patches from multiple WSIs at 10X magnification. This is the equivalent of up to 11 hours of video at 1 fps. Expert pathologist evaluations demonstrate that the generated report text is clinically accurate and equivalent to or preferred over the original reporting for 68% (95% CI: [60%, 76%]) of multi-slide examples with up to 5 slides. While performance decreased for examples with 6 or more slides, this study demonstrates the promise of leveraging the long-context capabilities of modern LMMs for the uniquely challenging task of medical report generation where each case can contain thousands of image patches.

### Multi-view 3D surface reconstruction from SAR images by inverse rendering 
[[arxiv](https://arxiv.org/abs/2502.10492)] [[cool](https://papers.cool/arxiv/2502.10492)] [[pdf](https://arxiv.org/pdf/2502.10492)]
> **Authors**: Emile Barbier--Renard,Florence Tupin,Nicolas Trouvé,Loïc Denis
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,信号处理
- **Abstract**: 3D reconstruction of a scene from Synthetic Aperture Radar (SAR) images mainly relies on interferometric measurements, which involve strict constraints on the acquisition process. These last years, progress in deep learning has significantly advanced 3D reconstruction from multiple views in optical imaging, mainly through reconstruction-by-synthesis approaches pioneered by Neural Radiance Fields. In this paper, we propose a new inverse rendering method for 3D reconstruction from unconstrained SAR images, drawing inspiration from optical approaches. First, we introduce a new simplified differentiable SAR rendering model, able to synthesize images from a digital elevation model and a radar backscattering coefficients map. Then, we introduce a coarse-to-fine strategy to train a Multi-Layer Perceptron (MLP) to fit the height and appearance of a given radar scene from a few SAR views. Finally, we demonstrate the surface reconstruction capabilities of our method on synthetic SAR images produced by ONERA's physically-based EMPRISE simulator. Our method showcases the potential of exploiting geometric disparities in SAR images and paves the way for multi-sensor data fusion.

### A Survey of Representation Learning, Optimization Strategies, and Applications for Omnidirectional Vision 
[[arxiv](https://arxiv.org/abs/2502.10444)] [[cool](https://papers.cool/arxiv/2502.10444)] [[pdf](https://arxiv.org/pdf/2502.10444)]
> **Authors**: Hao Ai,Zidong Cao,Lin Wang
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-17
> **comment**: 37 pages, 24 figures, accepted by IJCV
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Omnidirectional image (ODI) data is captured with a field-of-view of 360x180, which is much wider than the pinhole cameras and captures richer surrounding environment details than the conventional perspective images. In recent years, the availability of customer-level 360 cameras has made omnidirectional vision more popular, and the advance of deep learning (DL) has significantly sparked its research and applications. This paper presents a systematic and comprehensive review and analysis of the recent progress of DL for omnidirectional vision. It delineates the distinct challenges and complexities encountered in applying DL to omnidirectional images as opposed to traditional perspective imagery. Our work covers four main contents: (i) A thorough introduction to the principles of omnidirectional imaging and commonly explored projections of ODI; (ii) A methodical review of varied representation learning approaches tailored for ODI; (iii) An in-depth investigation of optimization strategies specific to omnidirectional vision; (iv) A structural and hierarchical taxonomy of the DL methods for the representative omnidirectional vision tasks, from visual enhancement (e.g., image generation and super-resolution) to 3D geometry and motion estimation (e.g., depth and optical flow estimation), alongside the discussions on emergent research directions; (v) An overview of cutting-edge applications (e.g., autonomous driving and virtual reality), coupled with a critical discussion on prevailing challenges and open questions, to trigger more research in the community.

### Text-guided Sparse Voxel Pruning for Efficient 3D Visual Grounding 
[[arxiv](https://arxiv.org/abs/2502.10392)] [[cool](https://papers.cool/arxiv/2502.10392)] [[pdf](https://arxiv.org/pdf/2502.10392)]
> **Authors**: Wenxuan Guo,Xiuwei Xu,Ziwei Wang,Jianjiang Feng,Jie Zhou,Jiwen Lu
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: In this paper, we propose an efficient multi-level convolution architecture for 3D visual grounding. Conventional methods are difficult to meet the requirements of real-time inference due to the two-stage or point-based architecture. Inspired by the success of multi-level fully sparse convolutional architecture in 3D object detection, we aim to build a new 3D visual grounding framework following this technical route. However, as in 3D visual grounding task the 3D scene representation should be deeply interacted with text features, sparse convolution-based architecture is inefficient for this interaction due to the large amount of voxel features. To this end, we propose text-guided pruning (TGP) and completion-based addition (CBA) to deeply fuse 3D scene representation and text features in an efficient way by gradual region pruning and target completion. Specifically, TGP iteratively sparsifies the 3D scene representation and thus efficiently interacts the voxel features with text features by cross-attention. To mitigate the affect of pruning on delicate geometric information, CBA adaptively fixes the over-pruned region by voxel completion with negligible computational overhead. Compared with previous single-stage methods, our method achieves top inference speed and surpasses previous fastest method by 100\% FPS. Our method also achieves state-of-the-art accuracy even compared with two-stage methods, with $+1.13$ lead of Acc@0.5 on ScanRefer, and $+2.6$ and $+3.2$ leads on NR3D and SR3D respectively. The code is available at \href{https://github.com/GWxuan/TSP3D}{https://github.com/GWxuan/TSP3D}.

### Region-Adaptive Sampling for Diffusion Transformers 
[[arxiv](https://arxiv.org/abs/2502.10389)] [[cool](https://papers.cool/arxiv/2502.10389)] [[pdf](https://arxiv.org/pdf/2502.10389)]
> **Authors**: Ziming Liu,Yifan Yang,Chengruidong Zhang,Yiqi Zhang,Lili Qiu,Yang You,Yuqing Yang
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Diffusion models (DMs) have become the leading choice for generative tasks across diverse domains. However, their reliance on multiple sequential forward passes significantly limits real-time performance. Previous acceleration methods have primarily focused on reducing the number of sampling steps or reusing intermediate results, failing to leverage variations across spatial regions within the image due to the constraints of convolutional U-Net structures. By harnessing the flexibility of Diffusion Transformers (DiTs) in handling variable number of tokens, we introduce RAS, a novel, training-free sampling strategy that dynamically assigns different sampling ratios to regions within an image based on the focus of the DiT model. Our key observation is that during each sampling step, the model concentrates on semantically meaningful regions, and these areas of focus exhibit strong continuity across consecutive steps. Leveraging this insight, RAS updates only the regions currently in focus, while other regions are updated using cached noise from the previous step. The model's focus is determined based on the output from the preceding step, capitalizing on the temporal consistency we observed. We evaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up to 2.36x and 2.51x, respectively, with minimal degradation in generation quality. Additionally, a user study reveals that RAS delivers comparable qualities under human evaluation while achieving a 1.6x speedup. Our approach makes a significant step towards more efficient diffusion transformers, enhancing their potential for real-time applications.

### Simplifying DINO via Coding Rate Regularization 
[[arxiv](https://arxiv.org/abs/2502.10385)] [[cool](https://papers.cool/arxiv/2502.10385)] [[pdf](https://arxiv.org/pdf/2502.10385)]
> **Authors**: Ziyang Wu,Jingyuan Zhang,Druv Pai,XuDong Wang,Chandan Singh,Jianwei Yang,Jianfeng Gao,Yi Ma
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 17 pages, 5 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: DINO and DINOv2 are two model families being widely used to learn representations from unlabeled imagery data at large scales. Their learned representations often enable state-of-the-art performance for downstream tasks, such as image classification and segmentation. However, they employ many empirically motivated design choices and their training pipelines are highly complex and unstable -- many hyperparameters need to be carefully tuned to ensure that the representations do not collapse -- which poses considerable difficulty to improving them or adapting them to new domains. In this work, we posit that we can remove most such-motivated idiosyncrasies in the pre-training pipelines, and only need to add an explicit coding rate term in the loss function to avoid collapse of the representations. As a result, we obtain highly simplified variants of the DINO and DINOv2 which we call SimDINO and SimDINOv2, respectively. Remarkably, these simplified models are more robust to different design choices, such as network architecture and hyperparameters, and they learn even higher-quality representations, measured by performance on downstream tasks, offering a Pareto improvement over the corresponding DINO and DINOv2 models. This work highlights the potential of using simplifying design principles to improve the empirical practice of deep learning.

### Ocular Disease Classification Using CNN with Deep Convolutional Generative Adversarial Network 
[[arxiv](https://arxiv.org/abs/2502.10334)] [[cool](https://papers.cool/arxiv/2502.10334)] [[pdf](https://arxiv.org/pdf/2502.10334)]
> **Authors**: Arun Kunwar,Dibakar Raj Pant,Jukka Heikkonen,Rajeev Kanth
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: ef:Lecture Notes in Electrical Engineering, vol 1190. Springer, Singapore. 29 September 2024
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: The Convolutional Neural Network (CNN) has shown impressive performance in image classification because of its strong learning capabilities. However, it demands a substantial and balanced dataset for effective training. Otherwise, networks frequently exhibit over fitting and struggle to generalize to new examples. Publicly available dataset of fundus images of ocular disease is insufficient to train any classification model to achieve satisfactory accuracy. So, we propose Generative Adversarial Network(GAN) based data generation technique to synthesize dataset for training CNN based classification model and later use original disease containing ocular images to test the model. During testing the model classification accuracy with the original ocular image, the model achieves an accuracy rate of 78.6% for myopia, 88.6% for glaucoma, and 84.6% for cataract, with an overall classification accuracy of 84.6%.

### Object Detection and Tracking 
[[arxiv](https://arxiv.org/abs/2502.10310)] [[cool](https://papers.cool/arxiv/2502.10310)] [[pdf](https://arxiv.org/pdf/2502.10310)]
> **Authors**: Md Pranto,Omar Faruk
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 10 pages, 5 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算机与社会
- **Abstract**: Efficient and accurate object detection is an important topic in the development of computer vision systems. With the advent of deep learning techniques, the accuracy of object detection has increased significantly. The project aims to integrate a modern technique for object detection with the aim of achieving high accuracy with real-time performance. The reliance on other computer vision algorithms in many object identification systems, which results in poor and ineffective performance, is a significant obstacle. In this research, we solve the end-to-end object detection problem entirely using deep learning techniques. The network is trained using the most difficult publicly available dataset, which is used for an annual item detection challenge. Applications that need object detection can benefit the system's quick and precise finding.

### QMaxViT-Unet+: A Query-Based MaxViT-Unet with Edge Enhancement for Scribble-Supervised Segmentation of Medical Images 
[[arxiv](https://arxiv.org/abs/2502.10294)] [[cool](https://papers.cool/arxiv/2502.10294)] [[pdf](https://arxiv.org/pdf/2502.10294)]
> **Authors**: Thien B. Nguyen-Tat,Hoang-An Vo,Phuoc-Sang Dang
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: The deployment of advanced deep learning models for medical image segmentation is often constrained by the requirement for extensively annotated datasets. Weakly-supervised learning, which allows less precise labels, has become a promising solution to this challenge. Building on this approach, we propose QMaxViT-Unet+, a novel framework for scribble-supervised medical image segmentation. This framework is built on the U-Net architecture, with the encoder and decoder replaced by Multi-Axis Vision Transformer (MaxViT) blocks. These blocks enhance the model's ability to learn local and global features efficiently. Additionally, our approach integrates a query-based Transformer decoder to refine features and an edge enhancement module to compensate for the limited boundary information in the scribble label. We evaluate the proposed QMaxViT-Unet+ on four public datasets focused on cardiac structures, colorectal polyps, and breast cancer: ACDC, MS-CMRSeg, SUN-SEG, and BUSI. Evaluation metrics include the Dice similarity coefficient (DSC) and the 95th percentile of Hausdorff distance (HD95). Experimental results show that QMaxViT-Unet+ achieves 89.1\% DSC and 1.316mm HD95 on ACDC, 88.4\% DSC and 2.226mm HD95 on MS-CMRSeg, 71.4\% DSC and 4.996mm HD95 on SUN-SEG, and 69.4\% DSC and 50.122mm HD95 on BUSI. These results demonstrate that our method outperforms existing approaches in terms of accuracy, robustness, and efficiency while remaining competitive with fully-supervised learning approaches. This makes it ideal for medical image analysis, where high-quality annotations are often scarce and require significant effort and expense. The code is available at: https://github.com/anpc849/QMaxViT-Unet

### Artificial Intelligence to Assess Dental Findings from Panoramic Radiographs -- A Multinational Study 
[[arxiv](https://arxiv.org/abs/2502.10277)] [[cool](https://papers.cool/arxiv/2502.10277)] [[pdf](https://arxiv.org/pdf/2502.10277)]
> **Authors**: Yin-Chih Chelsea Wang,Tsao-Lun Chen,Shankeeth Vinayahalingam,Tai-Hsien Wu,Chu Wei Chang,Hsuan Hao Chang,Hung-Jen Wei,Mu-Hsiung Chen,Ching-Chang Ko,David Anssari Moin,Bram van Ginneken,Tong Xi,Hsiao-Cheng Tsai,Min-Huey Chen,Tzu-Ming Harry Hsu,Hye Chou
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Dental panoramic radiographs (DPRs) are widely used in clinical practice for comprehensive oral assessment but present challenges due to overlapping structures and time constraints in interpretation. This study aimed to establish a solid baseline for the AI-automated assessment of findings in DPRs by developing, evaluating an AI system, and comparing its performance with that of human readers across multinational data sets. We analyzed 6,669 DPRs from three data sets (the Netherlands, Brazil, and Taiwan), focusing on 8 types of dental findings. The AI system combined object detection and semantic segmentation techniques for per-tooth finding identification. Performance metrics included sensitivity, specificity, and area under the receiver operating characteristic curve (AUC-ROC). AI generalizability was tested across data sets, and performance was compared with human dental practitioners. The AI system demonstrated comparable or superior performance to human readers, particularly +67.9% (95% CI: 54.0%-81.9%; p < .001) sensitivity for identifying periapical radiolucencies and +4.7% (95% CI: 1.4%-8.0%; p = .008) sensitivity for identifying missing teeth. The AI achieved a macro-averaged AUC-ROC of 96.2% (95% CI: 94.6%-97.8%) across 8 findings. AI agreements with the reference were comparable to inter-human agreements in 7 of 8 findings except for caries (p = .024). The AI system demonstrated robust generalization across diverse imaging and demographic settings and processed images 79 times faster (95% CI: 75-82) than human readers. The AI system effectively assessed findings in DPRs, achieving performance on par with or better than human experts while significantly reducing interpretation time. These results highlight the potential for integrating AI into clinical workflows to improve diagnostic efficiency and accuracy, and patient management.

### Probing Perceptual Constancy in Large Vision Language Models 
[[arxiv](https://arxiv.org/abs/2502.10273)] [[cool](https://papers.cool/arxiv/2502.10273)] [[pdf](https://arxiv.org/pdf/2502.10273)]
> **Authors**: Haoran Sun,Suyang Yu,Yijiang Li,Qingying Gao,Haiyun Lyu,Hokin Deng,Dezhi Luo
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Perceptual constancy is the ability to maintain stable perceptions of objects despite changes in sensory input, such as variations in distance, angle, or lighting. This ability is crucial for recognizing visual information in a dynamic world, making it essential for Vision-Language Models (VLMs). However, whether VLMs are currently and theoretically capable of mastering this ability remains underexplored. In this study, we evaluated 33 VLMs using 253 experiments across three domains: color, size, and shape constancy. The experiments included single-image and video adaptations of classic cognitive tasks, along with novel tasks in in-the-wild conditions, to evaluate the models' recognition of object properties under varying conditions. We found significant variability in VLM performance, with models performance in shape constancy clearly dissociated from that of color and size constancy.

### PromptArtisan: Multi-instruction Image Editing in Single Pass with Complete Attention Control 
[[arxiv](https://arxiv.org/abs/2502.10258)] [[cool](https://papers.cool/arxiv/2502.10258)] [[pdf](https://arxiv.org/pdf/2502.10258)]
> **Authors**: Kunal Swami,Raghu Chittersu,Pranav Adlinge,Rajeev Irny,Shashavali Doodekula,Alok Shukla
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: Accepted in ICASSP 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别,人机交互
- **Abstract**: We present PromptArtisan, a groundbreaking approach to multi-instruction image editing that achieves remarkable results in a single pass, eliminating the need for time-consuming iterative refinement. Our method empowers users to provide multiple editing instructions, each associated with a specific mask within the image. This flexibility allows for complex edits involving mask intersections or overlaps, enabling the realization of intricate and nuanced image transformations. PromptArtisan leverages a pre-trained InstructPix2Pix model in conjunction with a novel Complete Attention Control Mechanism (CACM). This mechanism ensures precise adherence to user instructions, granting fine-grained control over the editing process. Furthermore, our approach is zero-shot, requiring no additional training, and boasts improved processing complexity compared to traditional iterative methods. By seamlessly integrating multi-instruction capabilities, single-pass efficiency, and complete attention control, PromptArtisan unlocks new possibilities for creative and efficient image editing workflows, catering to both novice and expert users alike.

### Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model 
[[arxiv](https://arxiv.org/abs/2502.10248)] [[cool](https://papers.cool/arxiv/2502.10248)] [[pdf](https://arxiv.org/pdf/2502.10248)]
> **Authors**: Guoqing Ma,Haoyang Huang,Kun Yan,Liangyu Chen,Nan Duan,Shengming Yin,Changyi Wan,Ranchen Ming,Xiaoniu Song,Xing Chen,Yu Zhou,Deshan Sun,Deyu Zhou,Jian Zhou,Kaijun Tan,Kang An,Mei Chen,Wei Ji,Qiling Wu,Wen Sun,Xin Han,Yanan Wei,Zheng Ge,Aojie Li,Bin Wang, et al. (90 additional authors not shown)
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 36 pages, 14 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算语言学
- **Abstract**: We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model with 30B parameters and the ability to generate videos up to 204 frames in length. A deep compression Variational Autoencoder, Video-VAE, is designed for video generation tasks, achieving 16x16 spatial and 8x temporal compression ratios, while maintaining exceptional video reconstruction quality. User prompts are encoded using two bilingual text encoders to handle both English and Chinese. A DiT with 3D full attention is trained using Flow Matching and is employed to denoise input noise into latent frames. A video-based DPO approach, Video-DPO, is applied to reduce artifacts and improve the visual quality of the generated videos. We also detail our training strategies and share key observations and insights. Step-Video-T2V's performance is evaluated on a novel video generation benchmark, Step-Video-T2V-Eval, demonstrating its state-of-the-art text-to-video quality when compared with both open-source and commercial engines. Additionally, we discuss the limitations of current diffusion-based model paradigm and outline future directions for video foundation models. We make both Step-Video-T2V and Step-Video-T2V-Eval available at https://github.com/stepfun-ai/Step-Video-T2V. The online version can be accessed from https://yuewen.cn/videos as well. Our goal is to accelerate the innovation of video foundation models and empower video content creators.

### Mapping bathymetry of inland water bodies on the North Slope of Alaska with Landsat using Random Forest 
[[arxiv](https://arxiv.org/abs/2502.10214)] [[cool](https://papers.cool/arxiv/2502.10214)] [[pdf](https://arxiv.org/pdf/2502.10214)]
> **Authors**: Mark L. Carroll,Margaret R. Wooten,Claire E. Simpson,Caleb S. Spradlin,Melanie J. Frost,Mariana Blanco-Rojas,Zachary W. Williams,Jordan A. Caraballo-Vega,Christopher S. R. Neigh
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 24 Pages, 6 Figures, 1 Table. This article is a US Government work. Landsat data from the US Geological Survey Earth Explorer system: https://earthexplorer.usgs.gov. Sonar training measurements: https://doi.org/10.18739/A2JD4PP1H. Output maps from the Oak Ridge National Laboratory Distribute Active Archive Center (ORNL-DAAC): https://daac.ornl.gov/cgi-bin/dsviewer.pl?ds_id=2243
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: The North Slope of Alaska is dominated by small waterbodies that provide critical ecosystem services for local population and wildlife. Detailed information on the depth of the waterbodies is scarce due to the challenges with collecting such information. In this work we have trained a machine learning (Random Forest Regressor) model to predict depth from multispectral Landsat data in waterbodies across the North Slope of Alaska. The greatest challenge is the scarcity of in situ data, which is expensive and difficult to obtain, to train the model. We overcame this challenge by using modeled depth predictions from a prior study as synthetic training data to provide a more diverse training data pool for the Random Forest. The final Random Forest model was more robust than models trained directly on the in situ data and when applied to 208 Landsat 8 scenes from 2016 to 2018 yielded a map with an overall $r^{2}$ value of 0.76 on validation. The final map has been made available through the Oak Ridge National Laboratory Distribute Active Archive Center (ORNL-DAAC). This map represents a first of its kind regional assessment of waterbody depth with per pixel estimates of depth for the entire North Slope of Alaska.

### Exploring the Camera Bias of Person Re-identification 
[[arxiv](https://arxiv.org/abs/2502.10195)] [[cool](https://papers.cool/arxiv/2502.10195)] [[pdf](https://arxiv.org/pdf/2502.10195)]
> **Authors**: Myungseo Song,Jin-Woo Park,Jong-Seok Lee
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: ICLR 2025 (Spotlight)
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: We empirically investigate the camera bias of person re-identification (ReID) models. Previously, camera-aware methods have been proposed to address this issue, but they are largely confined to training domains of the models. We measure the camera bias of ReID models on unseen domains and reveal that camera bias becomes more pronounced under data distribution shifts. As a debiasing method for unseen domain data, we revisit feature normalization on embedding vectors. While the normalization has been used as a straightforward solution, its underlying causes and broader applicability remain unexplored. We analyze why this simple method is effective at reducing bias and show that it can be applied to detailed bias factors such as low-level image properties and body angle. Furthermore, we validate its generalizability across various models and benchmarks, highlighting its potential as a simple yet effective test-time postprocessing method for ReID. In addition, we explore the inherent risk of camera bias in unsupervised learning of ReID models. The unsupervised models remain highly biased towards camera labels even for seen domain data, indicating substantial room for improvement. Based on observations of the negative impact of camera-biased pseudo labels on training, we suggest simple training strategies to mitigate the bias. By applying these strategies to existing unsupervised learning algorithms, we show that significant performance improvements can be achieved with minor modifications.

### Interpretable Concept-based Deep Learning Framework for Multimodal Human Behavior Modeling 
[[arxiv](https://arxiv.org/abs/2502.10145)] [[cool](https://papers.cool/arxiv/2502.10145)] [[pdf](https://arxiv.org/pdf/2502.10145)]
> **Authors**: Xinyu Li,Marwa Mahmoud
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,多媒体
- **Abstract**: In the contemporary era of intelligent connectivity, Affective Computing (AC), which enables systems to recognize, interpret, and respond to human behavior states, has become an integrated part of many AI systems. As one of the most critical components of responsible AI and trustworthiness in all human-centered systems, explainability has been a major concern in AC. Particularly, the recently released EU General Data Protection Regulation requires any high-risk AI systems to be sufficiently interpretable, including biometric-based systems and emotion recognition systems widely used in the affective computing field. Existing explainable methods often compromise between interpretability and performance. Most of them focus only on highlighting key network parameters without offering meaningful, domain-specific explanations to the stakeholders. Additionally, they also face challenges in effectively co-learning and explaining insights from multimodal data sources. To address these limitations, we propose a novel and generalizable framework, namely the Attention-Guided Concept Model (AGCM), which provides learnable conceptual explanations by identifying what concepts that lead to the predictions and where they are observed. AGCM is extendable to any spatial and temporal signals through multimodal concept alignment and co-learning, empowering stakeholders with deeper insights into the model's decision-making process. We validate the efficiency of AGCM on well-established Facial Expression Recognition benchmark datasets while also demonstrating its generalizability on more complex real-world human behavior understanding applications.

### Compress image to patches for Vision Transformer 
[[arxiv](https://arxiv.org/abs/2502.10120)] [[cool](https://papers.cool/arxiv/2502.10120)] [[pdf](https://arxiv.org/pdf/2502.10120)]
> **Authors**: Xinfeng Zhao,Yaoru Sun
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 15 pages,5 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: The Vision Transformer (ViT) has made significant strides in the field of computer vision. However, as the depth of the model and the resolution of the input images increase, the computational cost associated with training and running ViT models has surged dramatically. This paper proposes a hybrid model based on CNN and Vision Transformer, named CI2P-ViT. The model incorporates a module called CI2P, which utilizes the CompressAI encoder to compress images and subsequently generates a sequence of patches through a series of convolutions. CI2P can replace the Patch Embedding component in the ViT model, enabling seamless integration into existing ViT models. Compared to ViT-B/16, CI2P-ViT has the number of patches input to the self-attention layer reduced to a quarter of the original. This design not only significantly reduces the computational cost of the ViT model but also effectively enhances the model's accuracy by introducing the inductive bias properties of CNN. The ViT model's precision is markedly enhanced. When trained from the ground up on the Animals-10 dataset, CI2P-ViT achieved an accuracy rate of 92.37%, representing a 3.3% improvement over the ViT-B/16 baseline. Additionally, the model's computational operations, measured in floating-point operations per second (FLOPs), were diminished by 63.35%, and it exhibited a 2-fold increase in training velocity on identical hardware configurations.

### DiSciPLE: Learning Interpretable Programs for Scientific Visual Discovery 
[[arxiv](https://arxiv.org/abs/2502.10060)] [[cool](https://papers.cool/arxiv/2502.10060)] [[pdf](https://arxiv.org/pdf/2502.10060)]
> **Authors**: Utkarsh Mall,Cheng Perng Phoo,Mia Chiquier,Bharath Hariharan,Kavita Bala,Carl Vondrick
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Visual data is used in numerous different scientific workflows ranging from remote sensing to ecology. As the amount of observation data increases, the challenge is not just to make accurate predictions but also to understand the underlying mechanisms for those predictions. Good interpretation is important in scientific workflows, as it allows for better decision-making by providing insights into the data. This paper introduces an automatic way of obtaining such interpretable-by-design models, by learning programs that interleave neural networks. We propose DiSciPLE (Discovering Scientific Programs using LLMs and Evolution) an evolutionary algorithm that leverages common sense and prior knowledge of large language models (LLMs) to create Python programs explaining visual data. Additionally, we propose two improvements: a program critic and a program simplifier to improve our method further to synthesize good programs. On three different real-world problems, DiSciPLE learns state-of-the-art programs on novel tasks with no prior literature. For example, we can learn programs with 35% lower error than the closest non-interpretable baseline for population density estimation.

### ManiTrend: Bridging Future Generation and Action Prediction with 3D Flow for Robotic Manipulation 
[[arxiv](https://arxiv.org/abs/2502.10028)] [[cool](https://papers.cool/arxiv/2502.10028)] [[pdf](https://arxiv.org/pdf/2502.10028)]
> **Authors**: Yuxin He,Qiang Nie
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 15 pages, 9 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器人技术
- **Abstract**: Language-conditioned manipulation is a vital but challenging robotic task due to the high-level abstraction of language. To address this, researchers have sought improved goal representations derived from natural language. In this paper, we highlight 3D flow - representing the motion trend of 3D particles within a scene - as an effective bridge between language-based future image generation and fine-grained action prediction. To this end, we develop ManiTrend, a unified framework that models the dynamics of 3D particles, vision observations and manipulation actions with a causal transformer. Within this framework, features for 3D flow prediction serve as additional conditions for future image generation and action prediction, alleviating the complexity of pixel-wise spatiotemporal modeling and providing seamless action guidance. Furthermore, 3D flow can substitute missing or heterogeneous action labels during large-scale pretraining on cross-embodiment demonstrations. Experiments on two comprehensive benchmarks demonstrate that our method achieves state-of-the-art performance with high efficiency. Our code and model checkpoints will be available upon acceptance.

### V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multi-Modal Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.09980)] [[cool](https://papers.cool/arxiv/2502.09980)] [[pdf](https://arxiv.org/pdf/2502.09980)]
> **Authors**: Hsu-kuang Chiu,Ryo Hachiuma,Chien-Yi Wang,Stephen F. Smith,Yu-Chiang Frank Wang,Min-Hung Chen
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: Our project website: https://eddyhkchiu.github.io/v2vllm.github.io/
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器人技术
- **Abstract**: Current autonomous driving vehicles rely mainly on their individual sensors to understand surrounding scenes and plan for future trajectories, which can be unreliable when the sensors are malfunctioning or occluded. To address this problem, cooperative perception methods via vehicle-to-vehicle (V2V) communication have been proposed, but they have tended to focus on detection and tracking. How those approaches contribute to overall cooperative planning performance is still under-explored. Inspired by recent progress using Large Language Models (LLMs) to build autonomous driving systems, we propose a novel problem setting that integrates an LLM into cooperative autonomous driving, with the proposed Vehicle-to-Vehicle Question-Answering (V2V-QA) dataset and benchmark. We also propose our baseline method Vehicle-to-Vehicle Large Language Model (V2V-LLM), which uses an LLM to fuse perception information from multiple connected autonomous vehicles (CAVs) and answer driving-related questions: grounding, notable object identification, and planning. Experimental results show that our proposed V2V-LLM can be a promising unified model architecture for performing various tasks in cooperative autonomous driving, and outperforms other baseline methods that use different fusion approaches. Our work also creates a new research direction that can improve the safety of future autonomous driving systems. Our project website: https://eddyhkchiu.github.io/v2vllm.github.io/ .

### Using MRNet to Predict Lunar Rock Categories Detected by Chang'e 5 Probe 
[[arxiv](https://arxiv.org/abs/2502.09952)] [[cool](https://papers.cool/arxiv/2502.09952)] [[pdf](https://arxiv.org/pdf/2502.09952)]
> **Authors**: Jin Cui,Yifei Zou,Siyuan Zhang
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: Published at the 8th International Conference on Advances in Machinery, Material Science and Engineering Application (MMSE 2022)
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: China's Chang'e 5 mission has been a remarkable success, with the chang'e 5 lander traveling on the Oceanus Procellarum to collect images of the lunar surface. Over the past half century, people have brought back some lunar rock samples, but its quantity does not meet the need for research. Under current circumstances, people still mainly rely on the analysis of rocks on the lunar surface through the detection of lunar rover. The Oceanus Procellarum, chosen by Chang'e 5 mission, contains various kind of rock species. Therefore, we first applied to the National Astronomical Observatories of the China under the Chinese Academy of Sciences for the Navigation and Terrain Camera (NaTeCam) of the lunar surface image, and established a lunar surface rock image data set CE5ROCK. The data set contains 100 images, which randomly divided into training, validation and test set. Experimental results show that the identification accuracy testing on convolutional neural network (CNN) models like AlexNet or MobileNet is about to 40.0%. In order to make full use of the global information in Moon images, this paper proposes the MRNet (MoonRockNet) network architecture. The encoding structure of the network uses VGG16 for feature extraction, and the decoding part adds dilated convolution and commonly used U-Net structure on the original VGG16 decoding structure, which is more conducive to identify more refined but more sparsely distributed types of lunar rocks. We have conducted extensive experiments on the established CE5ROCK data set, and the experimental results show that MRNet can achieve more accurate rock type identification, and outperform other existing mainstream algorithms in the identification performance.

### A Lightweight and Effective Image Tampering Localization Network with Vision Mamba 
[[arxiv](https://arxiv.org/abs/2502.09941)] [[cool](https://papers.cool/arxiv/2502.09941)] [[pdf](https://arxiv.org/pdf/2502.09941)]
> **Authors**: Kun Guo,Gang Cao,Zijie Lou,Xianglin Huang,Jiaoyun Liu
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,密码学和安全
- **Abstract**: Current image tampering localization methods primarily rely on Convolutional Neural Networks (CNNs) and Transformers. While CNNs suffer from limited local receptive fields, Transformers offer global context modeling at the expense of quadratic computational complexity. Recently, the state space model Mamba has emerged as a competitive alternative, enabling linear-complexity global dependency modeling. Inspired by it, we propose a lightweight and effective FORensic network based on vision MAmba (ForMa) for blind image tampering localization. Firstly, ForMa captures multi-scale global features that achieves efficient global dependency modeling through linear complexity. Then the pixel-wise localization map is generated by a lightweight decoder, which employs a parameter-free pixel shuffle layer for upsampling. Additionally, a noise-assisted decoding strategy is proposed to integrate complementary manipulation traces from tampered images, boosting decoder sensitivity to forgery cues. Experimental results on 10 standard datasets demonstrate that ForMa achieves state-of-the-art generalization ability and robustness, while maintaining the lowest computational complexity. Code is available at https://github.com/multimediaFor/ForMa.

### Temporal Scale and Shift Invariant Automatic Event Recognition using the Mellin Transform 
[[arxiv](https://arxiv.org/abs/2502.09939)] [[cool](https://papers.cool/arxiv/2502.09939)] [[pdf](https://arxiv.org/pdf/2502.09939)]
> **Authors**: Xi Shen,Julian Gamboa,Tabassom Hamidfar,Shamima A. Mitu,Selim M. Shahriar
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: The Spatio-temporal holographic correlator combines the traditional 2D optical image correlation techniques with inhomogeneously broadened arrays of cold atoms to achieve 3D time-space correlation to realize automatic event recognition at an ultra-high speed. Here we propose a method to realize such event recognition for videos running at different speeds. With this method, we can highly improve recognition accuracy and filter almost all the unwanted events in the video database.

### Precise Parameter Localization for Textual Generation in Diffusion Models 
[[arxiv](https://arxiv.org/abs/2502.09935)] [[cool](https://papers.cool/arxiv/2502.09935)] [[pdf](https://arxiv.org/pdf/2502.09935)]
> **Authors**: Łukasz Staniszewski,Bartosz Cywiński,Franziska Boenisch,Kamil Deja,Adam Dziedzic
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: ICLR 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Novel diffusion models can synthesize photo-realistic images with integrated high-quality text. Surprisingly, we demonstrate through attention activation patching that only less than 1% of diffusion models' parameters, all contained in attention layers, influence the generation of textual content within the images. Building on this observation, we improve textual generation efficiency and performance by targeting cross and joint attention layers of diffusion models. We introduce several applications that benefit from localizing the layers responsible for textual content generation. We first show that a LoRA-based fine-tuning solely of the localized layers enhances, even more, the general text-generation capabilities of large diffusion models while preserving the quality and diversity of the diffusion models' generations. Then, we demonstrate how we can use the localized layers to edit textual content in generated images. Finally, we extend this idea to the practical use case of preventing the generation of toxic text in a cost-free manner. In contrast to prior work, our localization approach is broadly applicable across various diffusion model architectures, including U-Net (e.g., LDM and SDXL) and transformer-based (e.g., DeepFloyd IF and Stable Diffusion 3), utilizing diverse text encoders (e.g., from CLIP to the large language models like T5). Project page available at https://t2i-text-loc.github.io/.

### TransGUNet: Transformer Meets Graph-based Skip Connection for Medical Image Segmentation 
[[arxiv](https://arxiv.org/abs/2502.09931)] [[cool](https://papers.cool/arxiv/2502.09931)] [[pdf](https://arxiv.org/pdf/2502.09931)]
> **Authors**: Ju-Hyeon Nam,Nur Suriza Syazwany,Sang-Chul Lee
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 24 pages, 12 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Skip connection engineering is primarily employed to address the semantic gap between the encoder and decoder, while also integrating global dependencies to understand the relationships among complex anatomical structures in medical image segmentation. Although several models have proposed transformer-based approaches to incorporate global dependencies within skip connections, they often face limitations in capturing detailed local features with high computational complexity. In contrast, graph neural networks (GNNs) exploit graph structures to effectively capture local and global features. Leveraging these properties, we introduce an attentional cross-scale graph neural network (ACS-GNN), which enhances the skip connection framework by converting cross-scale feature maps into a graph structure and capturing complex anatomical structures through node attention. Additionally, we observed that deep learning models often produce uninformative feature maps, which degrades the quality of spatial attention maps. To address this problem, we integrated entropy-driven feature selection (EFS) with spatial attention, calculating an entropy score for each channel and filtering out high-entropy feature maps. Our innovative framework, TransGUNet, comprises ACS-GNN and EFS-based spatial attentio} to effectively enhance domain generalizability across various modalities by leveraging GNNs alongside a reliable spatial attention map, ensuring more robust features within the skip connection. Through comprehensive experiments and analysis, TransGUNet achieved superior segmentation performance on six seen and eight unseen datasets, demonstrating significantly higher efficiency compared to previous methods.

### Granite Vision: a lightweight, open-source multimodal model for enterprise Intelligence 
[[arxiv](https://arxiv.org/abs/2502.09927)] [[cool](https://papers.cool/arxiv/2502.09927)] [[pdf](https://arxiv.org/pdf/2502.09927)]
> **Authors**: Granite Vision Team,Leonid Karlinsky,Assaf Arbelle,Abraham Daniels,Ahmed Nassar,Amit Alfassi,Bo Wu,Eli Schwartz,Dhiraj Joshi,Jovana Kondic,Nimrod Shabtay,Pengyuan Li,Roei Herzig,Shafiq Abedin,Shaked Perek,Sivan Harary,Udi Barzelay,Adi Raz Goldfarb,Aude Oliva,Ben Wieles,Bishwaranjan Bhattacharjee,Brandon Huang,Christoph Auer,Dan Gutfreund,David Beymer, et al. (38 additional authors not shown)
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: We introduce Granite Vision, a lightweight large language model with vision capabilities, specifically designed to excel in enterprise use cases, particularly in visual document understanding. Our model is trained on a comprehensive instruction-following dataset, including document-related tasks, such as content extraction from tables, charts, diagrams, sketches, and infographics, as well as general image tasks. The architecture of Granite Vision is centered around visual modality alignment with a decoder-only, 2 billion parameter Granite large language model. Additionally, we introduce a dedicated safety classification approach in test-time that leverages a sparse set of attention vectors to identify potential harmful inputs. Despite its lightweight architecture, Granite Vision achieves strong results in standard benchmarks related to visual document understanding, as well as on the LiveXiv benchmark, which is designed to avoid test set contamination by using a constantly updated corpus of recently published Arxiv papers. We are releasing the model under the Apache-2 license, allowing for both research and commercial use, while offering complete visibility into the training data and other relevant details. See https://huggingface.co/ibm-granite/ for model weights.

### TaskGalaxy: Scaling Multi-modal Instruction Fine-tuning with Tens of Thousands Vision Task Types 
[[arxiv](https://arxiv.org/abs/2502.09925)] [[cool](https://papers.cool/arxiv/2502.09925)] [[pdf](https://arxiv.org/pdf/2502.09925)]
> **Authors**: Jiankang Chen,Tianke Zhang,Changyi Liu,Haojie Ding,Yaya Shi,Feng Cheng,Huihui Xiao,Bin Wen,Fan Yang,Tingting Gao,Di Zhang
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Multimodal visual language models are gaining prominence in open-world applications, driven by advancements in model architectures, training techniques, and high-quality data. However, their performance is often limited by insufficient task-specific data, leading to poor generalization and biased outputs. Existing efforts to increase task diversity in fine-tuning datasets are hindered by the labor-intensive process of manual task labeling, which typically produces only a few hundred task types. To address this, we propose TaskGalaxy, a large-scale multimodal instruction fine-tuning dataset comprising 19,227 hierarchical task types and 413,648 samples. TaskGalaxy utilizes GPT-4o to enrich task diversity by expanding from a small set of manually defined tasks, with CLIP and GPT-4o filtering those that best match open-source images, and generating relevant question-answer pairs. Multiple models are employed to ensure sample quality. This automated process enhances both task diversity and data quality, reducing manual intervention. Incorporating TaskGalaxy into LLaVA-v1.5 and InternVL-Chat-v1.0 models shows substantial performance improvements across 16 benchmarks, demonstrating the critical importance of task diversity. TaskGalaxy is publicly released at https://github.com/Kwai-YuanQi/TaskGalaxy.

### Self-Consistent Model-based Adaptation for Visual Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.09923)] [[cool](https://papers.cool/arxiv/2502.09923)] [[pdf](https://arxiv.org/pdf/2502.09923)]
> **Authors**: Xinning Zhou,Chengyang Ying,Yao Feng,Hang Su,Jun Zhu
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Visual reinforcement learning agents typically face serious performance declines in real-world applications caused by visual distractions. Existing methods rely on fine-tuning the policy's representations with hand-crafted augmentations. In this work, we propose Self-Consistent Model-based Adaptation (SCMA), a novel method that fosters robust adaptation without modifying the policy. By transferring cluttered observations to clean ones with a denoising model, SCMA can mitigate distractions for various policies as a plug-and-play enhancement. To optimize the denoising model in an unsupervised manner, we derive an unsupervised distribution matching objective with a theoretical analysis of its optimality. We further present a practical algorithm to optimize the objective by estimating the distribution of clean observations with a pre-trained world model. Extensive experiments on multiple visual generalization benchmarks and real robot data demonstrate that SCMA effectively boosts performance across various distractions and exhibits better sample efficiency.

## 计算机与社会(cs.CY:Computers and Society)

### Machine Learning-Driven Convergence Analysis in Multijurisdictional Compliance Using BERT and K-Means Clustering 
[[arxiv](https://arxiv.org/abs/2502.10413)] [[cool](https://papers.cool/arxiv/2502.10413)] [[pdf](https://arxiv.org/pdf/2502.10413)]
> **Authors**: Raj Sonani,Lohalekar Prayas
> **First submission**: 2025-01-23
> **First announcement**: 2025-02-17
> **comment**: 16 pages, 5 figures, 4 tables
- **标题**: None
- **领域**: 计算机与社会,人工智能,计算工程、金融和科学,计算语言学,机器学习
- **Abstract**: Digital data continues to grow, there has been a shift towards using effective regulatory mechanisms to safeguard personal information. The CCPA of California and the General Data Protection Regulation (GDPR) of the European Union are two of the most important privacy laws. The regulation is intended to safeguard consumer privacy, but it varies greatly in scope, definitions, and methods of enforcement. This paper presents a fresh approach to adaptive compliance, using machine learning and emphasizing natural language processing (NLP) as the primary focus of comparison between the GDPR and CCPA. Using NLP, this study compares various regulations to identify areas where they overlap or diverge. This includes the "right to be forgotten" provision in the GDPR and the "opt-out of sale" provision under CCPA. International companies can learn valuable lessons from this report, as it outlines strategies for better enforcement of laws across different nations. Additionally, the paper discusses the challenges of utilizing NLP in legal literature and proposes methods to enhance the model-ability of machine learning models for studying regulations. The study's objective is to "bridge the gap between legal knowledge and technical expertise" by developing regulatory compliance strategies that are more efficient in operation and more effective in data protection.

### Identifying relevant indicators for monitoring a National Artificial Intelligence Strategy 
[[arxiv](https://arxiv.org/abs/2502.10412)] [[cool](https://papers.cool/arxiv/2502.10412)] [[pdf](https://arxiv.org/pdf/2502.10412)]
> **Authors**: Renata Pelissari,Ricardo Suyama,Leonardo Tomazeli Duarte,Henrique Sá Earp
> **First submission**: 2025-01-23
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: How can a National Artificial Intelligence Strategy be effectively monitored? To address this question, we propose a methodology consisting of two key components. First, it involves identifying relevant indicators within national AI strategies. Second, it assesses the alignment between these indicators and the strategic actions of a specific government's AI strategy, allowing for a critical evaluation of its monitoring measures. Moreover, identifying these indicators helps assess the overall quality of the strategy's structure. A lack of alignment between strategic actions and the identified indicators may reveal gaps or blind spots in the strategy. This methodology is demonstrated using the Brazilian AI strategy as a case study.

### TrueReason: An Exemplar Personalised Learning System Integrating Reasoning with Foundational Models 
[[arxiv](https://arxiv.org/abs/2502.10411)] [[cool](https://papers.cool/arxiv/2502.10411)] [[pdf](https://arxiv.org/pdf/2502.10411)]
> **Authors**: Sahan Bulathwela,Daniel Van Niekerk,Jarrod Shipton,Maria Perez-Ortiz,Benjamin Rosman,John Shawe-Taylor
> **First submission**: 2025-01-23
> **First announcement**: 2025-02-17
> **comment**: To be published as a book chapter
- **标题**: None
- **领域**: 计算机与社会,人工智能,计算语言学,信息检索,多代理系统
- **Abstract**: Personalised education is one of the domains that can greatly benefit from the most recent advances in Artificial Intelligence (AI) and Large Language Models (LLM). However, it is also one of the most challenging applications due to the cognitive complexity of teaching effectively while personalising the learning experience to suit independent learners. We hypothesise that one promising approach to excelling in such demanding use cases is using a \emph{society of minds}. In this chapter, we present TrueReason, an exemplar personalised learning system that integrates a multitude of specialised AI models that can mimic micro skills that are composed together by a LLM to operationalise planning and reasoning. The architecture of the initial prototype is presented while describing two micro skills that have been incorporated in the prototype. The proposed system demonstrates the first step in building sophisticated AI systems that can take up very complex cognitive tasks that are demanded by domains such as education.

### Auto-Evaluation: A Critical Measure in Driving Improvements in Quality and Safety of AI-Generated Lesson Resources 
[[arxiv](https://arxiv.org/abs/2502.10410)] [[cool](https://papers.cool/arxiv/2502.10410)] [[pdf](https://arxiv.org/pdf/2502.10410)]
> **Authors**: Hannah-Beth Clark,Margaux Dowland,Laura Benton,Reka Budai,Ibrahim Kaan Keskin,Emma Searle,Matthew Gregory,Mark Hodierne,William Gayne,John Roberts
> **First submission**: 2025-01-23
> **First announcement**: 2025-02-17
> **comment**: 27 pages, Part of MIT OpenLearningAIand Open Education Initiative Series, published Jan 2025 https://aiopeneducation.pubpub.org/pub/i36sncz8/release/3?readingCollection=06969c6d
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: As a publicly funded body in the UK, Oak National Academy is in a unique position to innovate within this field as we have a comprehensive curriculum of approximately 13,000 open education resources (OER) for all National Curriculum subjects, designed and quality-assured by expert, human teachers. This has provided the corpus of content needed for building a high-quality AI-powered lesson planning tool, Aila, that is free to use and, therefore, accessible to all teachers across the country. Furthermore, using our evidence-informed curriculum principles, we have codified and exemplified each component of lesson design. To assess the quality of lessons produced by Aila at scale, we have developed an AI-powered auto-evaluation agent,facilitating informed improvements to enhance output quality. Through comparisons between human and auto-evaluations, we have begun to refine this agent further to increase its accuracy, measured by its alignment with an expert human evaluator. In this paper we present this iterative evaluation process through an illustrative case study focused on one quality benchmark - the level of challenge within multiple-choice quizzes. We also explore the contribution that this may make to similar projects and the wider sector.

### Data Science Students Perspectives on Learning Analytics: An Application of Human-Led and LLM Content Analysis 
[[arxiv](https://arxiv.org/abs/2502.10409)] [[cool](https://papers.cool/arxiv/2502.10409)] [[pdf](https://arxiv.org/pdf/2502.10409)]
> **Authors**: Raghda Zahran,Jianfei Xu,Huizhi Liang,Matthew Forshaw
> **First submission**: 2025-01-22
> **First announcement**: 2025-02-17
> **comment**: 17 Pages, 2 Tables, 1 Figure
- **标题**: None
- **领域**: 计算机与社会,人工智能,新兴技术,应用领域
- **Abstract**: Objective This study is part of a series of initiatives at a UK university designed to cultivate a deep understanding of students' perspectives on analytics that resonate with their unique learning needs. It explores collaborative data processing undertaken by postgraduate students who examined an Open University Learning Analytics Dataset (OULAD). Methods A qualitative approach was adopted, integrating a Retrieval-Augmented Generation (RAG) and a Large Language Model (LLM) technique with human-led content analysis to gather information about students' perspectives based on their submitted work. The study involved 72 postgraduate students in 12 groups. Findings The analysis of group work revealed diverse insights into essential learning analytics from the students' perspectives. All groups adopted a structured data science methodology. The questions formulated by the groups were categorised into seven themes, reflecting their specific areas of interest. While there was variation in the selected variables to interpret correlations, a consensus was found regarding the general results. Conclusion A significant outcome of this study is that students specialising in data science exhibited a deeper understanding of learning analytics, effectively articulating their interests through inferences drawn from their analyses. While human-led content analysis provided a general understanding of students' perspectives, the LLM offered nuanced insights.

### Addressing Bias in Generative AI: Challenges and Research Opportunities in Information Management 
[[arxiv](https://arxiv.org/abs/2502.10407)] [[cool](https://papers.cool/arxiv/2502.10407)] [[pdf](https://arxiv.org/pdf/2502.10407)]
> **Authors**: Xiahua Wei,Naveen Kumar,Han Zhang
> **First submission**: 2025-01-22
> **First announcement**: 2025-02-17
> **comment**: Information & Management, forthcoming
- **标题**: None
- **领域**: 计算机与社会,人工智能,人机交互
- **Abstract**: Generative AI technologies, particularly Large Language Models (LLMs), have transformed information management systems but introduced substantial biases that can compromise their effectiveness in informing business decision-making. This challenge presents information management scholars with a unique opportunity to advance the field by identifying and addressing these biases across extensive applications of LLMs. Building on the discussion on bias sources and current methods for detecting and mitigating bias, this paper seeks to identify gaps and opportunities for future research. By incorporating ethical considerations, policy implications, and sociotechnical perspectives, we focus on developing a framework that covers major stakeholders of Generative AI systems, proposing key research questions, and inspiring discussion. Our goal is to provide actionable pathways for researchers to address bias in LLM applications, thereby advancing research in information management that ultimately informs business practices. Our forward-looking framework and research agenda advocate interdisciplinary approaches, innovative methods, dynamic perspectives, and rigorous evaluation to ensure fairness and transparency in Generative AI-driven information systems. We expect this study to serve as a call to action for information management scholars to tackle this critical issue, guiding the improvement of fairness and effectiveness in LLM-based systems for business practice.

### FishBargain: An LLM-Empowered Bargaining Agent for Online Fleamarket Platform Sellers 
[[arxiv](https://arxiv.org/abs/2502.10406)] [[cool](https://papers.cool/arxiv/2502.10406)] [[pdf](https://arxiv.org/pdf/2502.10406)]
> **Authors**: Dexin Kong,Xu Yan,Ming Chen,Shuguang Han,Jufeng Chen,Fei Huang
> **First submission**: 2025-01-22
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: Different from traditional Business-to-Consumer e-commerce platforms~(e.g., Amazon), online fleamarket platforms~(e.g., Craigslist) mainly focus on individual sellers who are lack of time investment and business proficiency. Individual sellers often struggle with the bargaining process and thus the deal is unaccomplished. Recent advancements in Large Language Models(LLMs) demonstrate huge potential in various dialogue tasks, but those tasks are mainly in the form of passively following user's instruction. Bargaining, as a form of proactive dialogue task, represents a distinct art of dialogue considering the dynamism of environment and uncertainty of adversary strategies. In this paper, we propose an LLM-empowered bargaining agent designed for online fleamarket platform sellers, named as FishBargain. Specifically, FishBargain understands the chat context and product information, chooses both action and language skill considering possible adversary actions and generates utterances. FishBargain has been tested by thousands of individual sellers on one of the largest online fleamarket platforms~(Xianyu) in China. Both qualitative and quantitative experiments demonstrate that FishBargain can effectively help sellers make more deals.

### Data Stewardship Decoded: Mapping Its Diverse Manifestations and Emerging Relevance at a time of AI 
[[arxiv](https://arxiv.org/abs/2502.10399)] [[cool](https://papers.cool/arxiv/2502.10399)] [[pdf](https://arxiv.org/pdf/2502.10399)]
> **Authors**: Stefaan Verhulst
> **First submission**: 2025-01-20
> **First announcement**: 2025-02-17
> **comment**: 10 pages
- **标题**: None
- **领域**: 计算机与社会,人工智能,数据库
- **Abstract**: Data stewardship has become a critical component of modern data governance, especially with the growing use of artificial intelligence (AI). Despite its increasing importance, the concept of data stewardship remains ambiguous and varies in its application. This paper explores four distinct manifestations of data stewardship to clarify its emerging position in the data governance landscape. These manifestations include a) data stewardship as a set of competencies and skills, b) a function or role within organizations, c) an intermediary organization facilitating collaborations, and d) a set of guiding principles. The paper subsequently outlines the core competencies required for effective data stewardship, explains the distinction between data stewards and Chief Data Officers (CDOs), and details the intermediary role of stewards in bridging gaps between data holders and external stakeholders. It also explores key principles aligned with the FAIR framework (Findable, Accessible, Interoperable, Reusable) and introduces the emerging principle of AI readiness to ensure data meets the ethical and technical requirements of AI systems. The paper emphasizes the importance of data stewardship in enhancing data collaboration, fostering public value, and managing data reuse responsibly, particularly in the era of AI. It concludes by identifying challenges and opportunities for advancing data stewardship, including the need for standardized definitions, capacity building efforts, and the creation of a professional association for data stewardship.

### Practical Application and Limitations of AI Certification Catalogues in the Light of the AI Act 
[[arxiv](https://arxiv.org/abs/2502.10398)] [[cool](https://papers.cool/arxiv/2502.10398)] [[pdf](https://arxiv.org/pdf/2502.10398)]
> **Authors**: Gregor Autischer,Kerstin Waxnegger,Dominik Kowald
> **First submission**: 2025-01-20
> **First announcement**: 2025-02-17
> **comment**: Bachelor thesis at Graz University of Technology, in preparation for a conference paper submission at EWAF'25
- **标题**: None
- **领域**: 计算机与社会,人工智能,机器学习
- **Abstract**: In this work-in-progress, we investigate the certification of AI systems, focusing on the practical application and limitations of existing certification catalogues in the light of the AI Act by attempting to certify a publicly available AI system. We aim to evaluate how well current approaches work to effectively certify an AI system, and how publicly accessible AI systems, that might not be actively maintained or initially intended for certification, can be selected and used for a sample certification process. Our methodology involves leveraging the Fraunhofer AI Assessment Catalogue as a comprehensive tool to systematically assess an AI model's compliance with certification standards. We find that while the catalogue effectively structures the evaluation process, it can also be cumbersome and time-consuming to use. We observe the limitations of an AI system that has no active development team anymore and highlighted the importance of complete system documentation. Finally, we identify some limitations of the certification catalogues used and proposed ideas on how to streamline the certification process.

### DASKT: A Dynamic Affect Simulation Method for Knowledge Tracing 
[[arxiv](https://arxiv.org/abs/2502.10396)] [[cool](https://papers.cool/arxiv/2502.10396)] [[pdf](https://arxiv.org/pdf/2502.10396)]
> **Authors**: Xinjie Sun,Kai Zhang,Qi Liu,Shuanghong Shen,Fei Wang,Yuxiang Guo,Enhong Chen
> **First submission**: 2025-01-18
> **First announcement**: 2025-02-17
> **comment**: 14 pages
- **标题**: None
- **领域**: 计算机与社会,人工智能,机器学习
- **Abstract**: Knowledge Tracing (KT) predicts future performance by modeling students' historical interactions, and understanding students' affective states can enhance the effectiveness of KT, thereby improving the quality of education. Although traditional KT values students' cognition and learning behaviors, efficient evaluation of students' affective states and their application in KT still require further exploration due to the non-affect-oriented nature of the data and budget constraints. To address this issue, we propose a computation-driven approach, Dynamic Affect Simulation Knowledge Tracing (DASKT), to explore the impact of various student affective states (such as frustration, concentration, boredom, and confusion) on their knowledge states. In this model, we first extract affective factors from students' non-affect-oriented behavioral data, then use clustering and spatiotemporal sequence modeling to accurately simulate students' dynamic affect changes when dealing with different problems. Subsequently, {\color{blue}we incorporate affect with time-series analysis to improve the model's ability to infer knowledge states over time and space.} Extensive experimental results on two public real-world educational datasets show that DASKT can achieve more reasonable knowledge states under the effect of students' affective states. Moreover, DASKT outperforms the most advanced KT methods in predicting student performance. Our research highlights a promising avenue for future KT studies, focusing on achieving high interpretability and accuracy.

### Assortment Optimization for Patient-Provider Matching 
[[arxiv](https://arxiv.org/abs/2502.10353)] [[cool](https://papers.cool/arxiv/2502.10353)] [[pdf](https://arxiv.org/pdf/2502.10353)]
> **Authors**: Naveen Raman,Holly Wiberg
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 36 pages, 11 Figures
- **标题**: None
- **领域**: 计算机与社会,机器学习,优化与控制
- **Abstract**: Rising provider turnover forces healthcare administrators to frequently rematch patients to available providers, which can be cumbersome and labor-intensive. To reduce the burden of rematching, we study algorithms for matching patients and providers through assortment optimization. We develop a patient-provider matching model in which we simultaneously offer each patient a menu of providers, and patients subsequently respond and select providers. By offering assortments upfront, administrators can balance logistical ease and patient autonomy. We study policies for assortment optimization and characterize their performance under different problem settings. We demonstrate that the selection of assortment policy is highly dependent on problem specifics and, in particular, on a patient's willingness to match and the ratio between patients and providers. On real-world data, we show that our best policy can improve match quality by 13% over a greedy solution by tailoring assortment sizes based on patient characteristics. We conclude with recommendations for running a real-world patient-provider matching system inspired by our results.

### Technical Risks of (Lethal) Autonomous Weapons Systems 
[[arxiv](https://arxiv.org/abs/2502.10174)] [[cool](https://papers.cool/arxiv/2502.10174)] [[pdf](https://arxiv.org/pdf/2502.10174)]
> **Authors**: Heramb Podar,Alycia Colijn
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,人工智能,系统与控制
- **Abstract**: The autonomy and adaptability of (Lethal) Autonomous Weapons Systems, (L)AWS in short, promise unprecedented operational capabilities, but they also introduce profound risks that challenge the principles of control, accountability, and stability in international security. This report outlines the key technological risks associated with (L)AWS deployment, emphasizing their unpredictability, lack of transparency, and operational unreliability, which can lead to severe unintended consequences. Key Takeaways: 1. Proposed advantages of (L)AWS can only be achieved through objectification and classification, but a range of systematic risks limit the reliability and predictability of classifying algorithms. 2. These systematic risks include the black-box nature of AI decision-making, susceptibility to reward hacking, goal misgeneralization and potential for emergent behaviors that escape human control. 3. (L)AWS could act in ways that are not just unexpected but also uncontrollable, undermining mission objectives and potentially escalating conflicts. 4. Even rigorously tested systems may behave unpredictably and harmfully in real-world conditions, jeopardizing both strategic stability and humanitarian principles.

## 数据库(cs.DB:Databases)

### Generating Skyline Datasets for Data Science Models 
[[arxiv](https://arxiv.org/abs/2502.11262)] [[cool](https://papers.cool/arxiv/2502.11262)] [[pdf](https://arxiv.org/pdf/2502.11262)]
> **Authors**: Mengying Wang,Hanchao Ma,Yiyang Bian,Yangxin Fan,Yinghui Wu
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: EDBT25
- **标题**: None
- **领域**: 数据库,人工智能
- **Abstract**: Preparing high-quality datasets required by various data-driven AI and machine learning models has become a cornerstone task in data-driven analysis. Conventional data discovery methods typically integrate datasets towards a single pre-defined quality measure that may lead to bias for downstream tasks. This paper introduces MODis, a framework that discovers datasets by optimizing multiple user-defined, model-performance measures. Given a set of data sources and a model, MODis selects and integrates data sources into a skyline dataset, over which the model is expected to have the desired performance in all the performance measures. We formulate MODis as a multi-goal finite state transducer, and derive three feasible algorithms to generate skyline datasets. Our first algorithm adopts a "reduce-from-universal" strategy, that starts with a universal schema and iteratively prunes unpromising data. Our second algorithm further reduces the cost with a bi-directional strategy that interleaves data augmentation and reduction. We also introduce a diversification algorithm to mitigate the bias in skyline datasets. We experimentally verify the efficiency and effectiveness of our skyline data discovery algorithms, and showcase their applications in optimizing data science pipelines.

### Bridging the Gap: Enabling Natural Language Queries for NoSQL Databases through Text-to-NoSQL Translation 
[[arxiv](https://arxiv.org/abs/2502.11201)] [[cool](https://papers.cool/arxiv/2502.11201)] [[pdf](https://arxiv.org/pdf/2502.11201)]
> **Authors**: Jinwei Lu,Yuanfeng Song,Zhiqian Qin,Haodi Zhang,Chen Zhang,Raymond Chi-Wing Wong
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 数据库,人工智能
- **Abstract**: NoSQL databases have become increasingly popular due to their outstanding performance in handling large-scale, unstructured, and semi-structured data, highlighting the need for user-friendly interfaces to bridge the gap between non-technical users and complex database queries. In this paper, we introduce the Text-to-NoSQL task, which aims to convert natural language queries into NoSQL queries, thereby lowering the technical barrier for non-expert users. To promote research in this area, we developed a novel automated dataset construction process and released a large-scale and open-source dataset for this task, named TEND (short for Text-to-NoSQL Dataset). Additionally, we designed a SLM (Small Language Model)-assisted and RAG (Retrieval-augmented Generation)-assisted multi-step framework called SMART, which is specifically designed for Text-to-NoSQL conversion. To ensure comprehensive evaluation of the models, we also introduced a detailed set of metrics that assess the model's performance from both the query itself and its execution results. Our experimental results demonstrate the effectiveness of our approach and establish a benchmark for future research in this emerging field. We believe that our contributions will pave the way for more accessible and intuitive interactions with NoSQL databases.

### Tradeoffs in Processing Queries and Supporting Updates over an ML-Enhanced R-tree 
[[arxiv](https://arxiv.org/abs/2502.09937)] [[cool](https://papers.cool/arxiv/2502.09937)] [[pdf](https://arxiv.org/pdf/2502.09937)]
> **Authors**: Abdullah Al-Mamun,Ch. Md. Rakin Haider,Jianguo Wang,Walid G. Aref
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: arXiv admin note: text overlap with arXiv:2207.00550
- **标题**: None
- **领域**: 数据库,机器学习
- **Abstract**: Machine Learning (ML) techniques have been successfully applied to design various learned database index structures for both the one- and multi-dimensional spaces. Particularly, a class of traditional multi-dimensional indexes has been augmented with ML models to design ML-enhanced variants of their traditional counterparts. This paper focuses on the R-tree multi-dimensional index structure as it is widely used for indexing multi-dimensional data. The R-tree has been augmented with machine learning models to enhance the R-tree performance. The AI+R-tree is an ML-enhanced R-tree index structure that augments a traditional disk-based R-tree with an ML model to enhance the R-tree's query processing performance, mainly, to avoid navigating the overlapping branches of the R-tree that do not yield query results, e.g., in the presence of high-overlap among the rectangles of the R-tree nodes. We investigate the empirical tradeoffs in processing dynamic query workloads and in supporting updates over the AI+R-tree. Particularly, we investigate the impact of the choice of ML models over the AI+R-tree query processing performance. Moreover, we present a case study of designing a custom loss function for a neural network model tailored to the query processing requirements of the AI+R-tree. Furthermore, we present the design tradeoffs for adopting various strategies for supporting dynamic inserts, updates, and deletes with the vision of realizing a mutable AI+R-tree. Experiments on real datasets demonstrate that the AI+R-tree can enhance the query processing performance of a traditional R-tree for high-overlap range queries by up to 5.4X while achieving up to 99% average query recall.

## 分布式、并行和集群计算(cs.DC:Distributed, Parallel, and Cluster Computing)

### Proof of Response 
[[arxiv](https://arxiv.org/abs/2502.10637)] [[cool](https://papers.cool/arxiv/2502.10637)] [[pdf](https://arxiv.org/pdf/2502.10637)]
> **Authors**: Illia Polosukhin,Alex Skidanov
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 分布式、并行和集群计算,人工智能,密码学和安全
- **Abstract**: We present a mechanism that for a network of participants allows one participant of the network (Alice) to request some data from another participant (Bob) and either receive a response from Bob within a known-in-advance, bounded time b, or receive a proof that at least one edge on the way to Bob was broken within b, or receive a streaming payment proportional to time passed beyond b during which neither was received. This mechanism allows for building downstream applications that require provable responses from other participants, such as decentralized storage solutions, decentralized AI agents, and more.

### Janus: Collaborative Vision Transformer Under Dynamic Network Environment 
[[arxiv](https://arxiv.org/abs/2502.10047)] [[cool](https://papers.cool/arxiv/2502.10047)] [[pdf](https://arxiv.org/pdf/2502.10047)]
> **Authors**: Linyi Jiang,Silvery D. Fu,Yifei Zhu,Bo Li
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: Accepted for publication in IEEE INFOCOM 2025
- **标题**: None
- **领域**: 分布式、并行和集群计算,人工智能
- **Abstract**: Vision Transformers (ViTs) have outperformed traditional Convolutional Neural Network architectures and achieved state-of-the-art results in various computer vision tasks. Since ViTs are computationally expensive, the models either have to be pruned to run on resource-limited edge devices only or have to be executed on remote cloud servers after receiving the raw data transmitted over fluctuating networks. The resulting degraded performance or high latency all hinder their widespread applications. In this paper, we present Janus, the first framework for low-latency cloud-device collaborative Vision Transformer inference over dynamic networks. Janus overcomes the intrinsic model limitations of ViTs and realizes collaboratively executing ViT models on both cloud and edge devices, achieving low latency, high accuracy, and low communication overhead. Specifically, Janus judiciously combines token pruning techniques with a carefully designed fine-to-coarse model splitting policy and non-static mixed pruning policy. It attains a balance between accuracy and latency by dynamically selecting the optimal pruning level and split point. Experimental results across various tasks demonstrate that Janus enhances throughput by up to 5.15 times and reduces latency violation ratios by up to 98.7% when compared with baseline approaches under various network environments.

## 图形(cs.GR:Graphics)

### ViRAC: A Vision-Reasoning Agent Head Movement Control Framework in Arbitrary Virtual Environments 
[[arxiv](https://arxiv.org/abs/2502.10046)] [[cool](https://papers.cool/arxiv/2502.10046)] [[pdf](https://arxiv.org/pdf/2502.10046)]
> **Authors**: Juyeong Hwang,Seong-Eun Hong,Hyeongyeop Kang
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 图形,计算机视觉和模式识别
- **Abstract**: Creating lifelike virtual agents capable of interacting with their environments is a longstanding goal in computer graphics. This paper addresses the challenge of generating natural head rotations, a critical aspect of believable agent behavior for visual information gathering and dynamic responses to environmental cues. Although earlier methods have made significant strides, many rely on data-driven or saliency-based approaches, which often underperform in diverse settings and fail to capture deeper cognitive factors such as risk assessment, information seeking, and contextual prioritization. Consequently, generated behaviors can appear rigid or overlook critical scene elements, thereby diminishing the sense of realism. In this paper, we propose \textbf{ViRAC}, a \textbf{Vi}sion-\textbf{R}easoning \textbf{A}gent Head Movement \textbf{C}ontrol framework, which exploits the common-sense knowledge and reasoning capabilities of large-scale models, including Vision-Language Models (VLMs) and Large-Language Models (LLMs). Rather than explicitly modeling every cognitive mechanism, ViRAC leverages the biases and patterns internalized by these models from extensive training, thus emulating human-like perceptual processes without hand-tuned heuristics. Experimental results in multiple scenarios reveal that ViRAC produces more natural and context-aware head rotations than recent state-of-the-art techniques. Quantitative evaluations show a closer alignment with real human head-movement data, while user studies confirm improved realism and cognitive plausibility.

## 人机交互(cs.HC:Human-Computer Interaction)

### A Comparison of Human and Machine Learning Errors in Face Recognition 
[[arxiv](https://arxiv.org/abs/2502.11337)] [[cool](https://papers.cool/arxiv/2502.11337)] [[pdf](https://arxiv.org/pdf/2502.11337)]
> **Authors**: Marina Estévez-Almenzar,Ricardo Baeza-Yates,Carlos Castillo
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 人机交互,计算机视觉和模式识别,计算机与社会
- **Abstract**: Machine learning applications in high-stakes scenarios should always operate under human oversight. Developing an optimal combination of human and machine intelligence requires an understanding of their complementarities, particularly regarding the similarities and differences in the way they make mistakes. We perform extensive experiments in the area of face recognition and compare two automated face recognition systems against human annotators through a demographically balanced user study. Our research uncovers important ways in which machine learning errors and human errors differ from each other, and suggests potential strategies in which human-machine collaboration can improve accuracy in face recognition.

### FairFare: A Tool for Crowdsourcing Rideshare Data to Empower Labor Organizers 
[[arxiv](https://arxiv.org/abs/2502.11273)] [[cool](https://papers.cool/arxiv/2502.11273)] [[pdf](https://arxiv.org/pdf/2502.11273)]
> **Authors**: Dana Calacci,Varun Nagaraj Rao,Samantha Dalal,Catherine Di,Kok-Wei Pua,Andrew Schwartz,Danny Spitzberg,Andrés Monroy-Hernández
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: FairFare is hosted at: https://getfairfare.org/
- **标题**: None
- **领域**: 人机交互,人工智能,计算机与社会
- **Abstract**: Rideshare workers experience unpredictable working conditions due to gig work platforms' reliance on opaque AI and algorithmic systems. In response to these challenges, we found that labor organizers want data to help them advocate for legislation to increase the transparency and accountability of these platforms. To address this need, we collaborated with a Colorado-based rideshare union to develop FairFare, a tool that crowdsources and analyzes workers' data to estimate the take rate -- the percentage of the rider price retained by the rideshare platform. We deployed FairFare with our partner organization that collaborated with us in collecting data on 76,000+ trips from 45 drivers over 18 months. During evaluation interviews, organizers reported that FairFare helped influence the bill language and passage of Colorado Senate Bill 24-75, calling for greater transparency and data disclosure of platform operations, and create a national narrative. Finally, we reflect on complexities of translating quantitative data into policy outcomes, nature of community based audits, and design implications for future transparency tools.

### Prompting in the Dark: Assessing Human Performance in Prompt Engineering for Data Labeling When Gold Labels Are Absent 
[[arxiv](https://arxiv.org/abs/2502.11267)] [[cool](https://papers.cool/arxiv/2502.11267)] [[pdf](https://arxiv.org/pdf/2502.11267)]
> **Authors**: Zeyu He,Saniya Naphade,Ting-Hao 'Kenneth' Huang
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: Accepted By CHI 2025
- **标题**: None
- **领域**: 人机交互,人工智能,计算语言学,机器学习
- **Abstract**: Millions of users prompt large language models (LLMs) for various tasks, but how good are people at prompt engineering? Do users actually get closer to their desired outcome over multiple iterations of their prompts? These questions are crucial when no gold-standard labels are available to measure progress. This paper investigates a scenario in LLM-powered data labeling, "prompting in the dark," where users iteratively prompt LLMs to label data without using manually-labeled benchmarks. We developed PromptingSheet, a Google Sheets add-on that enables users to compose, revise, and iteratively label data through spreadsheets. Through a study with 20 participants, we found that prompting in the dark was highly unreliable-only 9 participants improved labeling accuracy after four or more iterations. Automated prompt optimization tools like DSPy also struggled when few gold labels were available. Our findings highlight the importance of gold labels and the needs, as well as the risks, of automated support in human prompt engineering, providing insights for future tool design.

### GenComUI: Exploring Generative Visual Aids as Medium to Support Task-Oriented Human-Robot Communication 
[[arxiv](https://arxiv.org/abs/2502.10678)] [[cool](https://papers.cool/arxiv/2502.10678)] [[pdf](https://arxiv.org/pdf/2502.10678)]
> **Authors**: Yate Ge,Meiying Li,Xipeng Huang,Yuanda Hu,Qi Wang,Xiaohua Sun,Weiwei Guo
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: To appear at ACM CHI '25
- **标题**: None
- **领域**: 人机交互,人工智能,机器人技术
- **Abstract**: This work investigates the integration of generative visual aids in human-robot task communication. We developed GenComUI, a system powered by large language models that dynamically generates contextual visual aids (such as map annotations, path indicators, and animations) to support verbal task communication and facilitate the generation of customized task programs for the robot. This system was informed by a formative study that examined how humans use external visual tools to assist verbal communication in spatial tasks. To evaluate its effectiveness, we conducted a user experiment (n = 20) comparing GenComUI with a voice-only baseline. The results demonstrate that generative visual aids, through both qualitative and quantitative analysis, enhance verbal task communication by providing continuous visual feedback, thus promoting natural and effective human-robot communication. Additionally, the study offers a set of design implications, emphasizing how dynamically generated visual aids can serve as an effective communication medium in human-robot interaction. These findings underscore the potential of generative visual aids to inform the design of more intuitive and effective human-robot communication, particularly for complex communication scenarios in human-robot interaction and LLM-based end-user development.

### Tempo: Helping Data Scientists and Domain Experts Collaboratively Specify Predictive Modeling Tasks 
[[arxiv](https://arxiv.org/abs/2502.10526)] [[cool](https://papers.cool/arxiv/2502.10526)] [[pdf](https://arxiv.org/pdf/2502.10526)]
> **Authors**: Venkatesh Sivaraman,Anika Vaishampayan,Xiaotong Li,Brian R Buck,Ziyong Ma,Richard D Boyce,Adam Perer
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: Appearing at CHI 2025
- **标题**: None
- **领域**: 人机交互,人工智能
- **Abstract**: Temporal predictive models have the potential to improve decisions in health care, public services, and other domains, yet they often fail to effectively support decision-makers. Prior literature shows that many misalignments between model behavior and decision-makers' expectations stem from issues of model specification, namely how, when, and for whom predictions are made. However, model specifications for predictive tasks are highly technical and difficult for non-data-scientist stakeholders to interpret and critique. To address this challenge we developed Tempo, an interactive system that helps data scientists and domain experts collaboratively iterate on model specifications. Using Tempo's simple yet precise temporal query language, data scientists can quickly prototype specifications with greater transparency about pre-processing choices. Moreover, domain experts can assess performance within data subgroups to validate that models behave as expected. Through three case studies, we demonstrate how Tempo helps multidisciplinary teams quickly prune infeasible specifications and identify more promising directions to explore.

### Unknown Word Detection for English as a Second Language (ESL) Learners Using Gaze and Pre-trained Language Models 
[[arxiv](https://arxiv.org/abs/2502.10378)] [[cool](https://papers.cool/arxiv/2502.10378)] [[pdf](https://arxiv.org/pdf/2502.10378)]
> **Authors**: Jiexin Ding,Bowen Zhao,Yuntao Wang,Xinyun Liu,Rui Hao,Ishan Chatterjee,Yuanchun Shi
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 人机交互,计算语言学
- **Abstract**: English as a Second Language (ESL) learners often encounter unknown words that hinder their text comprehension. Automatically detecting these words as users read can enable computing systems to provide just-in-time definitions, synonyms, or contextual explanations, thereby helping users learn vocabulary in a natural and seamless manner. This paper presents EyeLingo, a transformer-based machine learning method that predicts the probability of unknown words based on text content and eye gaze trajectory in real time with high accuracy. A 20-participant user study revealed that our method can achieve an accuracy of 97.6%, and an F1-score of 71.1%. We implemented a real-time reading assistance prototype to show the effectiveness of EyeLingo. The user study shows improvement in willingness to use and usefulness compared to baseline methods.

## 信息检索(cs.IR:Information Retrieval)

### Multi-Turn Multi-Modal Question Clarification for Enhanced Conversational Understanding 
[[arxiv](https://arxiv.org/abs/2502.11442)] [[cool](https://papers.cool/arxiv/2502.11442)] [[pdf](https://arxiv.org/pdf/2502.11442)]
> **Authors**: Kimia Ramezan,Alireza Amiri Bavandpour,Yifei Yuan,Clemencia Siro,Mohammad Aliannejadi
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,人工智能,计算语言学,机器学习
- **Abstract**: Conversational query clarification enables users to refine their search queries through interactive dialogue, improving search effectiveness. Traditional approaches rely on text-based clarifying questions, which often fail to capture complex user preferences, particularly those involving visual attributes. While recent work has explored single-turn multi-modal clarification with images alongside text, such methods do not fully support the progressive nature of user intent refinement over multiple turns. Motivated by this, we introduce the Multi-turn Multi-modal Clarifying Questions (MMCQ) task, which combines text and visual modalities to refine user queries in a multi-turn conversation. To facilitate this task, we create a large-scale dataset named ClariMM comprising over 13k multi-turn interactions and 33k question-answer pairs containing multi-modal clarifying questions. We propose Mario, a retrieval framework that employs a two-phase ranking strategy: initial retrieval with BM25, followed by a multi-modal generative re-ranking model that integrates textual and visual information from conversational history. Our experiments show that multi-turn multi-modal clarification outperforms uni-modal and single-turn approaches, improving MRR by 12.88%. The gains are most significant in longer interactions, demonstrating the value of progressive refinement for complex queries.

### Improving Scientific Document Retrieval with Concept Coverage-based Query Set Generation 
[[arxiv](https://arxiv.org/abs/2502.11181)] [[cool](https://papers.cool/arxiv/2502.11181)] [[pdf](https://arxiv.org/pdf/2502.11181)]
> **Authors**: SeongKu Kang,Bowen Jin,Wonbin Kweon,Yu Zhang,Dongha Lee,Jiawei Han,Hwanjo Yu
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: WSDM 2025
- **标题**: None
- **领域**: 信息检索,人工智能
- **Abstract**: In specialized fields like the scientific domain, constructing large-scale human-annotated datasets poses a significant challenge due to the need for domain expertise. Recent methods have employed large language models to generate synthetic queries, which serve as proxies for actual user queries. However, they lack control over the content generated, often resulting in incomplete coverage of academic concepts in documents. We introduce Concept Coverage-based Query set Generation (CCQGen) framework, designed to generate a set of queries with comprehensive coverage of the document's concepts. A key distinction of CCQGen is that it adaptively adjusts the generation process based on the previously generated queries. We identify concepts not sufficiently covered by previous queries, and leverage them as conditions for subsequent query generation. This approach guides each new query to complement the previous ones, aiding in a thorough understanding of the document. Extensive experiments demonstrate that CCQGen significantly enhances query quality and retrieval performance.

### QuOTE: Question-Oriented Text Embeddings 
[[arxiv](https://arxiv.org/abs/2502.10976)] [[cool](https://papers.cool/arxiv/2502.10976)] [[pdf](https://arxiv.org/pdf/2502.10976)]
> **Authors**: Andrew Neeser,Kaylen Latimer,Aadyant Khatri,Chris Latimer,Naren Ramakrishnan
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: :H.3
- **标题**: None
- **领域**: 信息检索,人工智能,计算语言学,机器学习
- **Abstract**: We present QuOTE (Question-Oriented Text Embeddings), a novel enhancement to retrieval-augmented generation (RAG) systems, aimed at improving document representation for accurate and nuanced retrieval. Unlike traditional RAG pipelines, which rely on embedding raw text chunks, QuOTE augments chunks with hypothetical questions that the chunk can potentially answer, enriching the representation space. This better aligns document embeddings with user query semantics, and helps address issues such as ambiguity and context-dependent relevance. Through extensive experiments across diverse benchmarks, we demonstrate that QuOTE significantly enhances retrieval accuracy, including in multi-hop question-answering tasks. Our findings highlight the versatility of question generation as a fundamental indexing strategy, opening new avenues for integrating question generation into retrieval-based AI pipelines.

### A Geometric Approach to Personalized Recommendation with Set-Theoretic Constraints Using Box Embeddings 
[[arxiv](https://arxiv.org/abs/2502.10875)] [[cool](https://papers.cool/arxiv/2502.10875)] [[pdf](https://arxiv.org/pdf/2502.10875)]
> **Authors**: Shib Dasgupta,Michael Boratko,Andrew McCallum
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,人工智能,机器学习
- **Abstract**: Personalized item recommendation typically suffers from data sparsity, which is most often addressed by learning vector representations of users and items via low-rank matrix factorization. While this effectively densifies the matrix by assuming users and movies can be represented by linearly dependent latent features, it does not capture more complicated interactions. For example, vector representations struggle with set-theoretic relationships, such as negation and intersection, e.g. recommending a movie that is "comedy and action, but not romance". In this work, we formulate the problem of personalized item recommendation as matrix completion where rows are set-theoretically dependent. To capture this set-theoretic dependence we represent each user and attribute by a hyper-rectangle or box (i.e. a Cartesian product of intervals). Box embeddings can intuitively be understood as trainable Venn diagrams, and thus not only inherently represent similarity (via the Jaccard index), but also naturally and faithfully support arbitrary set-theoretic relationships. Queries involving set-theoretic constraints can be efficiently computed directly on the embedding space by performing geometric operations on the representations. We empirically demonstrate the superiority of box embeddings over vector-based neural methods on both simple and complex item recommendation queries by up to 30 \% overall.

### Evaluating improvements on using Large Language Models (LLMs) for property extraction in the Open Research Knowledge Graph (ORKG) 
[[arxiv](https://arxiv.org/abs/2502.10768)] [[cool](https://papers.cool/arxiv/2502.10768)] [[pdf](https://arxiv.org/pdf/2502.10768)]
> **Authors**: Sandra Schaftner
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,人工智能,计算语言学
- **Abstract**: Current research highlights the great potential of Large Language Models (LLMs) for constructing Scholarly Knowledge Graphs (SKGs). One particularly complex step in this process is relation extraction, aimed at identifying suitable properties to describe the content of research. This study builds directly on previous research of three Open Research Knowledge Graph (ORKG) team members who assessed the readiness of LLMs such as GPT-3.5, Llama 2, and Mistral for property extraction in scientific literature. Given the moderate performance observed, the previous work concluded that fine-tuning is needed to improve these models' alignment with scientific tasks and their emulation of human expertise. Expanding on this prior experiment, this study evaluates the impact of advanced prompt engineering techniques and demonstrates that these techniques can highly significantly enhance the results. Additionally, this study extends the property extraction process to include property matching to existing ORKG properties, which are retrieved via the API. The evaluation reveals that results generated through advanced prompt engineering achieve a higher proportion of matches with ORKG properties, further emphasizing the enhanced alignment achieved. Moreover, this lays the groundwork for addressing challenges such as the inconsistency of ORKG properties, an issue highlighted in prior studies. By assigning unique URIs and using standardized terminology, this work increases the consistency of the properties, fulfilling a crucial aspect of Linked Data and FAIR principles - core commitments of ORKG. This, in turn, significantly enhances the applicability of ORKG content for subsequent tasks such as comparisons of research publications. Finally, the study concludes with recommendations for future improvements in the overall property extraction process.

### SessionRec: Next Session Prediction Paradigm For Generative Sequential Recommendation 
[[arxiv](https://arxiv.org/abs/2502.10157)] [[cool](https://papers.cool/arxiv/2502.10157)] [[pdf](https://arxiv.org/pdf/2502.10157)]
> **Authors**: Lei Huang,Hao Guo,Linzhi Peng,Long Zhang,Xiaoteng Wang,Daoyuan Wang,Shichao Wang,Jinpeng Wang,Lei Wang,Sheng Chen
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,人工智能
- **Abstract**: We introduce SessionRec, a novel next-session prediction paradigm (NSPP) for generative sequential recommendation, addressing the fundamental misalignment between conventional next-item prediction paradigm (NIPP) and real-world recommendation scenarios. Unlike NIPP's item-level autoregressive generation that contradicts actual session-based user interactions, our framework introduces a session-aware representation learning through hierarchical sequence aggregation (intra/inter-session), reducing attention computation complexity while enabling implicit modeling of massive negative interactions, and a session-based prediction objective that better captures users' diverse interests through multi-item recommendation in next sessions. Moreover, we found that incorporating a rank loss for items within the session under the next session prediction paradigm can significantly improve the ranking effectiveness of generative sequence recommendation models. We also verified that SessionRec exhibits clear power-law scaling laws similar to those observed in LLMs. Extensive experiments conducted on public datasets and online A/B test in Meituan App demonstrate the effectiveness of SessionRec. The proposed paradigm establishes new foundations for developing industrial-scale generative recommendation systems through its model-agnostic architecture and computational efficiency.

### A Survey on LLM-powered Agents for Recommender Systems 
[[arxiv](https://arxiv.org/abs/2502.10050)] [[cool](https://papers.cool/arxiv/2502.10050)] [[pdf](https://arxiv.org/pdf/2502.10050)]
> **Authors**: Qiyao Peng,Hongtao Liu,Hua Huang,Qing Yang,Minglai Shao
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,人工智能
- **Abstract**: Recommender systems are essential components of many online platforms, yet traditional approaches still struggle with understanding complex user preferences and providing explainable recommendations. The emergence of Large Language Model (LLM)-powered agents offers a promising approach by enabling natural language interactions and interpretable reasoning, potentially transforming research in recommender systems. This survey provides a systematic review of the emerging applications of LLM-powered agents in recommender systems. We identify and analyze three key paradigms in current research: (1) Recommender-oriented approaches, which leverage intelligent agents to enhance the fundamental recommendation mechanisms; (2) Interaction-oriented approaches, which facilitate dynamic user engagement through natural dialogue and interpretable suggestions; and (3) Simulation-oriented approaches, which employ multi-agent frameworks to model complex user-item interactions and system dynamics. Beyond paradigm categorization, we analyze the architectural foundations of LLM-powered recommendation agents, examining their essential components: profile construction, memory management, strategic planning, and action execution. Our investigation extends to a comprehensive analysis of benchmark datasets and evaluation frameworks in this domain. This systematic examination not only illuminates the current state of LLM-powered agent recommender systems but also charts critical challenges and promising research directions in this transformative field.

## 信息论(cs.IT:Information Theory)

### Broadcast Channel Cooperative Gain: An Operational Interpretation of Partial Information Decomposition 
[[arxiv](https://arxiv.org/abs/2502.10878)] [[cool](https://papers.cool/arxiv/2502.10878)] [[pdf](https://arxiv.org/pdf/2502.10878)]
> **Authors**: Chao Tian,Shlomo Shamai
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: 9 pages, 1 figure
- **标题**: None
- **领域**: 信息论,人工智能,机器学习
- **Abstract**: Partial information decomposition has recently found applications in biological signal processing and machine learning. Despite its impacts, the decomposition was introduced through an informal and heuristic route, and its exact operational meaning is unclear. In this work, we fill this gap by connecting partial information decomposition to the capacity of the broadcast channel, which has been well-studied in the information theory literature. We show that the synergistic information in the decomposition can be rigorously interpreted as the cooperative gain, or a lower bound of this gain, on the corresponding broadcast channel. This interpretation can help practitioners to better explain and expand the applications of the partial information decomposition technique.

### Topological Neural Networks over the Air 
[[arxiv](https://arxiv.org/abs/2502.10070)] [[cool](https://papers.cool/arxiv/2502.10070)] [[pdf](https://arxiv.org/pdf/2502.10070)]
> **Authors**: Simone Fiorellino,Claudio Battiloro,Paolo Di Lorenzo
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 信息论,机器学习
- **Abstract**: Topological neural networks (TNNs) are information processing architectures that model representations from data lying over topological spaces (e.g., simplicial or cell complexes) and allow for decentralized implementation through localized communications over different neighborhoods. Existing TNN architectures have not yet been considered in realistic communication scenarios, where channel effects typically introduce disturbances such as fading and noise. This paper aims to propose a novel TNN design, operating on regular cell complexes, that performs over-the-air computation, incorporating the wireless communication model into its architecture. Specifically, during training and inference, the proposed method considers channel impairments such as fading and noise in the topological convolutional filtering operation, which takes place over different signal orders and neighborhoods. Numerical results illustrate the architecture's robustness to channel impairments during testing and the superior performance with respect to existing architectures, which are either communication-agnostic or graph-based.

## 机器学习(cs.LG:Machine Learning)

### ADO: Automatic Data Optimization for Inputs in LLM Prompts 
[[arxiv](https://arxiv.org/abs/2502.11436)] [[cool](https://papers.cool/arxiv/2502.11436)] [[pdf](https://arxiv.org/pdf/2502.11436)]
> **Authors**: Sam Lin,Wenyue Hua,Lingyao Li,Zhenting Wang,Yongfeng Zhang
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: This study explores a novel approach to enhance the performance of Large Language Models (LLMs) through the optimization of input data within prompts. While previous research has primarily focused on refining instruction components and augmenting input data with in-context examples, our work investigates the potential benefits of optimizing the input data itself. We introduce a two-pronged strategy for input data optimization: content engineering and structural reformulation. Content engineering involves imputing missing values, removing irrelevant attributes, and enriching profiles by generating additional information inferred from existing attributes. Subsequent to content engineering, structural reformulation is applied to optimize the presentation of the modified content to LLMs, given their sensitivity to input format. Our findings suggest that these optimizations can significantly improve the performance of LLMs in various tasks, offering a promising avenue for future research in prompt engineering. The source code is available at https://anonymous.4open.science/r/ADO-6BC5/

### What's in a Query: Polarity-Aware Distribution-Based Fair Ranking 
[[arxiv](https://arxiv.org/abs/2502.11429)] [[cool](https://papers.cool/arxiv/2502.11429)] [[pdf](https://arxiv.org/pdf/2502.11429)]
> **Authors**: Aparna Balagopalan,Kai Wang,Olawale Salaudeen,Asia Biega,Marzyeh Ghassemi
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: Published in WWW 2025
- **标题**: None
- **领域**: 机器学习,计算机与社会
- **Abstract**: Machine learning-driven rankings, where individuals (or items) are ranked in response to a query, mediate search exposure or attention in a variety of safety-critical settings. Thus, it is important to ensure that such rankings are fair. Under the goal of equal opportunity, attention allocated to an individual on a ranking interface should be proportional to their relevance across search queries. In this work, we examine amortized fair ranking -- where relevance and attention are cumulated over a sequence of user queries to make fair ranking more feasible in practice. Unlike prior methods that operate on expected amortized attention for each individual, we define new divergence-based measures for attention distribution-based fairness in ranking (DistFaiR), characterizing unfairness as the divergence between the distribution of attention and relevance corresponding to an individual over time. This allows us to propose new definitions of unfairness, which are more reliable at test time. Second, we prove that group fairness is upper-bounded by individual fairness under this definition for a useful class of divergence measures, and experimentally show that maximizing individual fairness through an integer linear programming-based optimization is often beneficial to group fairness. Lastly, we find that prior research in amortized fair ranking ignores critical information about queries, potentially leading to a fairwashing risk in practice by making rankings appear more fair than they actually are.

### Training-Free Guidance Beyond Differentiability: Scalable Path Steering with Tree Search in Diffusion and Flow Models 
[[arxiv](https://arxiv.org/abs/2502.11420)] [[cool](https://papers.cool/arxiv/2502.11420)] [[pdf](https://arxiv.org/pdf/2502.11420)]
> **Authors**: Yingqing Guo,Yukang Yang,Hui Yuan,Mengdi Wang
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Training-free guidance enables controlled generation in diffusion and flow models, but most existing methods assume differentiable objectives and rely on gradients. This work focuses on training-free guidance addressing challenges from non-differentiable objectives and discrete data distributions. We propose an algorithmic framework TreeG: Tree Search-Based Path Steering Guidance, applicable to both continuous and discrete settings in diffusion and flow models. TreeG offers a unified perspective on training-free guidance: proposing candidates for the next step, evaluating candidates, and selecting the best to move forward, enhanced by a tree search mechanism over active paths or parallelizing exploration. We comprehensively investigate the design space of TreeG over the candidate proposal module and the evaluation function, instantiating TreeG into three novel algorithms. Our experiments show that TreeG consistently outperforms the top guidance baselines in symbolic music generation, small molecule generation, and enhancer DNA design, all of which involve non-differentiable challenges. Additionally, we identify an inference-time scaling law showing TreeG's scalability in inference-time computation.

### DiSCo: Device-Server Collaborative LLM-Based Text Streaming Services 
[[arxiv](https://arxiv.org/abs/2502.11417)] [[cool](https://papers.cool/arxiv/2502.11417)] [[pdf](https://arxiv.org/pdf/2502.11417)]
> **Authors**: Ting Sun,Penghan Wang,Fan Lai
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 17 pages, 14 figures
- **标题**: None
- **领域**: 机器学习,分布式、并行和集群计算
- **Abstract**: The rapid rise of large language models (LLMs) in text streaming services has introduced significant cost and Quality of Experience (QoE) challenges in serving millions of daily requests, especially in meeting Time-To-First-Token (TTFT) and Time-Between-Token (TBT) requirements for real-time interactions. Our real-world measurements show that both server-based and on-device deployments struggle to meet diverse QoE demands: server deployments face high costs and last-hop issues (e.g., Internet latency and dynamics), while on-device LLM inference is constrained by resources. We introduce DiSCo, a device-server cooperative scheduler designed to optimize users' QoE by adaptively routing requests and migrating response generation between endpoints while maintaining cost constraints. DiSCo employs cost-aware scheduling, leveraging the predictable speed of on-device LLM inference with the flexible capacity of server-based inference to dispatch requests on the fly, while introducing a token-level migration mechanism to ensure consistent token delivery during migration. Evaluations on real-world workloads -- including commercial services like OpenAI GPT and DeepSeek, and open-source deployments such as LLaMA3 -- show that DiSCo can improve users' QoE by reducing tail TTFT (11-52\%) and mean TTFT (6-78\%) across different model-device configurations, while dramatically reducing serving costs by up to 84\% through its migration mechanism while maintaining comparable QoE levels.

### Statistical Query Hardness of Multiclass Linear Classification with Random Classification Noise 
[[arxiv](https://arxiv.org/abs/2502.11413)] [[cool](https://papers.cool/arxiv/2502.11413)] [[pdf](https://arxiv.org/pdf/2502.11413)]
> **Authors**: Ilias Diakonikolas,Mingchen Ma,Lisheng Ren,Christos Tzamos
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: We study the task of Multiclass Linear Classification (MLC) in the distribution-free PAC model with Random Classification Noise (RCN). Specifically, the learner is given a set of labeled examples $(x, y)$, where $x$ is drawn from an unknown distribution on $R^d$ and the labels are generated by a multiclass linear classifier corrupted with RCN. That is, the label $y$ is flipped from $i$ to $j$ with probability $H_{ij}$ according to a known noise matrix $H$ with non-negative separation $σ: = \min_{i \neq j} H_{ii}-H_{ij}$. The goal is to compute a hypothesis with small 0-1 error. For the special case of two labels, prior work has given polynomial-time algorithms achieving the optimal error. Surprisingly, little is known about the complexity of this task even for three labels. As our main contribution, we show that the complexity of MLC with RCN becomes drastically different in the presence of three or more labels. Specifically, we prove super-polynomial Statistical Query (SQ) lower bounds for this problem. In more detail, even for three labels and constant separation, we give a super-polynomial lower bound on the complexity of any SQ algorithm achieving optimal error. For a larger number of labels and smaller separation, we show a super-polynomial SQ lower bound even for the weaker goal of achieving any constant factor approximation to the optimal loss or even beating the trivial hypothesis.

### Detecting and Filtering Unsafe Training Data via Data Attribution 
[[arxiv](https://arxiv.org/abs/2502.11411)] [[cool](https://papers.cool/arxiv/2502.11411)] [[pdf](https://arxiv.org/pdf/2502.11411)]
> **Authors**: Yijun Pan,Taiwei Shi,Jieyu Zhao,Jiaqi W. Ma
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 13 pages
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Large language models (LLMs) are vulnerable to unsafe training data that even small amounts of unsafe data can lead to harmful model behaviors. Detecting and filtering such unsafe training data is essential for trustworthy model development. Current state-of-the-art (SOTA) approaches typically rely on training moderation classifiers which requires significant computational overhead and are limited to predefined taxonomies, making them less adaptable to evolving safety concerns. Moreover, these classifiers lack insight into the training process, limiting their effectiveness in filtering unsafe data. To address these limitations, we propose DABUF, leveraging data attribution to detect and filter unsafe training data by attributing harmful model outputs to influential training data points. DABUF enables flexible identification of various unsafe data types without predefined taxonomies. However, in practice, model outputs can be complex with combined safe linguistic features and unsafe content, leading to reduced attribution accuracy. In such cases, DABUF will integrate moderation classifiers to identify a minimal subset of unsafe training data for targeted attribution (such as jailbreak). When model outputs are relatively straightforward, DABUF uses model outputs directly as the attribution targets. We evaluate the performance on two different tasks: in filtering jailbreaking training data and in identifying and mitigating gender bias. DABUF outperforms SOTA approaches by up to 7.5\% in detection AUPRC in jailbreaking scenarios, and 44.1\% in detecting gender bias. Moreover, retraining on DABUF-filtered data leads to higher model safety across experiments, underscoring its versatility in addressing a broad spectrum of unsafe data issues.

### Structure based SAT dataset for analysing GNN generalisation 
[[arxiv](https://arxiv.org/abs/2502.11410)] [[cool](https://papers.cool/arxiv/2502.11410)] [[pdf](https://arxiv.org/pdf/2502.11410)]
> **Authors**: Yi Fu,Anthony Tompkins,Yang Song,Maurice Pagnucco
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: to be published in 28th International Conference on Artificial Intelligence and Statistics (AISTATS) 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Satisfiability (SAT) solvers based on techniques such as conflict driven clause learning (CDCL) have produced excellent performance on both synthetic and real world industrial problems. While these CDCL solvers only operate on a per-problem basis, graph neural network (GNN) based solvers bring new benefits to the field by allowing practitioners to exploit knowledge gained from solved problems to expedite solving of new SAT problems. However, one specific area that is often studied in the context of CDCL solvers, but largely overlooked in GNN solvers, is the relationship between graph theoretic measure of structure in SAT problems and the generalisation ability of GNN solvers. To bridge the gap between structural graph properties (e.g., modularity, self-similarity) and the generalisability (or lack thereof) of GNN based SAT solvers, we present StructureSAT: a curated dataset, along with code to further generate novel examples, containing a diverse set of SAT problems from well known problem domains. Furthermore, we utilise a novel splitting method that focuses on deconstructing the families into more detailed hierarchies based on their structural properties. With the new dataset, we aim to help explain problematic generalisation in existing GNN SAT solvers by exploiting knowledge of structural graph properties. We conclude with multiple future directions that can help researchers in GNN based SAT solving develop more effective and generalisable SAT solvers.

### Oversmoothing as Loss of Sign: Towards Structural Balance in Graph Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.11394)] [[cool](https://papers.cool/arxiv/2502.11394)] [[pdf](https://arxiv.org/pdf/2502.11394)]
> **Authors**: Jiaqi Wang,Xinyi Wu,James Cheng,Yifei Wang
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Oversmoothing is a common issue in graph neural networks (GNNs), where node representations become excessively homogeneous as the number of layers increases, resulting in degraded performance. Various strategies have been proposed to combat oversmoothing in practice, yet they are based on different heuristics and lack a unified understanding of their inherent mechanisms. In this paper, we show that three major classes of anti-oversmoothing techniques can be mathematically interpreted as message passing over signed graphs comprising both positive and negative edges. By analyzing the asymptotic behavior of signed graph propagation, we demonstrate that negative edges can repel nodes to a certain extent, providing deeper insights into how these methods mitigate oversmoothing. Furthermore, our results suggest that the structural balance of a signed graph-where positive edges exist only within clusters and negative edges appear only between clusters-is crucial for clustering node representations in the long term through signed graph propagation. Motivated by these observations, we propose a solution to mitigate oversmoothing with theoretical guarantees-Structural Balance Propagation (SBP), by incorporating label and feature information to create a structurally balanced graph for message-passing. Experiments on nine datasets against twelve baselines demonstrate the effectiveness of our method, highlighting the value of our signed graph perspective.

### Sparse Autoencoder Features for Classifications and Transferability 
[[arxiv](https://arxiv.org/abs/2502.11367)] [[cool](https://papers.cool/arxiv/2502.11367)] [[pdf](https://arxiv.org/pdf/2502.11367)]
> **Authors**: Jack Gallifant,Shan Chen,Kuleen Sasse,Hugo Aerts,Thomas Hartvigsen,Danielle S. Bitterman
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Sparse Autoencoders (SAEs) provide potentials for uncovering structured, human-interpretable representations in Large Language Models (LLMs), making them a crucial tool for transparent and controllable AI systems. We systematically analyze SAE for interpretable feature extraction from LLMs in safety-critical classification tasks. Our framework evaluates (1) model-layer selection and scaling properties, (2) SAE architectural configurations, including width and pooling strategies, and (3) the effect of binarizing continuous SAE activations. SAE-derived features achieve macro F1 > 0.8, outperforming hidden-state and BoW baselines while demonstrating cross-model transfer from Gemma 2 2B to 9B-IT models. These features generalize in a zero-shot manner to cross-lingual toxicity detection and visual classification tasks. Our analysis highlights the significant impact of pooling strategies and binarization thresholds, showing that binarization offers an efficient alternative to traditional feature selection while maintaining or improving performance. These findings establish new best practices for SAE-based interpretability and enable scalable, transparent deployment of LLMs in real-world applications. Full repo: https://github.com/shan23chen/MOSAIC.

### Teleportation With Null Space Gradient Projection for Optimization Acceleration 
[[arxiv](https://arxiv.org/abs/2502.11362)] [[cool](https://papers.cool/arxiv/2502.11362)] [[pdf](https://arxiv.org/pdf/2502.11362)]
> **Authors**: Zihao Wu,Juncheng Dong,Ahmed Aloui,Vahid Tarokh
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Optimization techniques have become increasingly critical due to the ever-growing model complexity and data scale. In particular, teleportation has emerged as a promising approach, which accelerates convergence of gradient descent-based methods by navigating within the loss invariant level set to identify parameters with advantageous geometric properties. Existing teleportation algorithms have primarily demonstrated their effectiveness in optimizing Multi-Layer Perceptrons (MLPs), but their extension to more advanced architectures, such as Convolutional Neural Networks (CNNs) and Transformers, remains challenging. Moreover, they often impose significant computational demands, limiting their applicability to complex architectures. To this end, we introduce an algorithm that projects the gradient of the teleportation objective function onto the input null space, effectively preserving the teleportation within the loss invariant level set and reducing computational cost. Our approach is readily generalizable from MLPs to CNNs, transformers, and potentially other advanced architectures. We validate the effectiveness of our algorithm across various benchmark datasets and optimizers, demonstrating its broad applicability.

### SAIF: A Sparse Autoencoder Framework for Interpreting and Steering Instruction Following of Language Models 
[[arxiv](https://arxiv.org/abs/2502.11356)] [[cool](https://papers.cool/arxiv/2502.11356)] [[pdf](https://arxiv.org/pdf/2502.11356)]
> **Authors**: Zirui He,Haiyan Zhao,Yiran Qiao,Fan Yang,Ali Payani,Jing Ma,Mengnan Du
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 21 pages, 11 figures, 6 tables
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: The ability of large language models (LLMs) to follow instructions is crucial for their practical applications, yet the underlying mechanisms remain poorly understood. This paper presents a novel framework that leverages sparse autoencoders (SAE) to interpret how instruction following works in these models. We demonstrate how the features we identify can effectively steer model outputs to align with given instructions. Through analysis of SAE latent activations, we identify specific latents responsible for instruction following behavior. Our findings reveal that instruction following capabilities are encoded by a distinct set of instruction-relevant SAE latents. These latents both show semantic proximity to relevant instructions and demonstrate causal effects on model behavior. Our research highlights several crucial factors for achieving effective steering performance: precise feature identification, the role of final layer, and optimal instruction positioning. Additionally, we demonstrate that our methodology scales effectively across SAEs and LLMs of varying sizes.

### Biases in Edge Language Models: Detection, Analysis, and Mitigation 
[[arxiv](https://arxiv.org/abs/2502.11349)] [[cool](https://papers.cool/arxiv/2502.11349)] [[pdf](https://arxiv.org/pdf/2502.11349)]
> **Authors**: Vinamra Sharma,Danilo Pietro Pau,José Cano
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: Accepted as a full paper by the 2025 EDGEAIFOUNDATION Austin
- **标题**: None
- **领域**: 机器学习,表现,机器学习
- **Abstract**: The integration of large language models (LLMs) on low-power edge devices such as Raspberry Pi, known as edge language models (ELMs), has introduced opportunities for more personalized, secure, and low-latency language intelligence that is accessible to all. However, the resource constraints inherent in edge devices and the lack of robust ethical safeguards in language models raise significant concerns about fairness, accountability, and transparency in model output generation. This paper conducts a comparative analysis of text-based bias across language model deployments on edge, cloud, and desktop environments, aiming to evaluate how deployment settings influence model fairness. Specifically, we examined an optimized Llama-2 model running on a Raspberry Pi 4; GPT 4o-mini, Gemini-1.5-flash, and Grok-beta models running on cloud servers; and Gemma2 and Mistral models running on a MacOS desktop machine. Our results demonstrate that Llama-2 running on Raspberry Pi 4 is 43.23% and 21.89% more prone to showing bias over time compared to models running on the desktop and cloud-based environments. We also propose the implementation of a feedback loop, a mechanism that iteratively adjusts model behavior based on previous outputs, where predefined constraint weights are applied layer-by-layer during inference, allowing the model to correct bias patterns, resulting in 79.28% reduction in model bias.

### S2TX: Cross-Attention Multi-Scale State-Space Transformer for Time Series Forecasting 
[[arxiv](https://arxiv.org/abs/2502.11340)] [[cool](https://papers.cool/arxiv/2502.11340)] [[pdf](https://arxiv.org/pdf/2502.11340)]
> **Authors**: Zihao Wu,Juncheng Dong,Haoming Yang,Vahid Tarokh
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Time series forecasting has recently achieved significant progress with multi-scale models to address the heterogeneity between long and short range patterns. Despite their state-of-the-art performance, we identify two potential areas for improvement. First, the variates of the multivariate time series are processed independently. Moreover, the multi-scale (long and short range) representations are learned separately by two independent models without communication. In light of these concerns, we propose State Space Transformer with cross-attention (S2TX). S2TX employs a cross-attention mechanism to integrate a Mamba model for extracting long-range cross-variate context and a Transformer model with local window attention to capture short-range representations. By cross-attending to the global context, the Transformer model further facilitates variate-level interactions as well as local/global communications. Comprehensive experiments on seven classic long-short range time-series forecasting benchmark datasets demonstrate that S2TX can achieve highly robust SOTA results while maintaining a low memory footprint.

### Inverse Flow and Consistency Models 
[[arxiv](https://arxiv.org/abs/2502.11333)] [[cool](https://papers.cool/arxiv/2502.11333)] [[pdf](https://arxiv.org/pdf/2502.11333)]
> **Authors**: Yuchen Zhang,Jian Zhou
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Inverse generation problems, such as denoising without ground truth observations, is a critical challenge in many scientific inquiries and real-world applications. While recent advances in generative models like diffusion models, conditional flow matching, and consistency models achieved impressive results by casting generation as denoising problems, they cannot be directly used for inverse generation without access to clean data. Here we introduce Inverse Flow (IF), a novel framework that enables using these generative models for inverse generation problems including denoising without ground truth. Inverse Flow can be flexibly applied to nearly any continuous noise distribution and allows complex dependencies. We propose two algorithms for learning Inverse Flows, Inverse Flow Matching (IFM) and Inverse Consistency Model (ICM). Notably, to derive the computationally efficient, simulation-free inverse consistency model objective, we generalized consistency training to any forward diffusion processes or conditional flows, which have applications beyond denoising. We demonstrate the effectiveness of IF on synthetic and real datasets, outperforming prior approaches while enabling noise distributions that previous methods cannot support. Finally, we showcase applications of our techniques to fluorescence microscopy and single-cell genomics data, highlighting IF's utility in scientific problems. Overall, this work expands the applications of powerful generative models to inversion generation problems.

### Non-Uniform Memory Sampling in Experience Replay 
[[arxiv](https://arxiv.org/abs/2502.11305)] [[cool](https://papers.cool/arxiv/2502.11305)] [[pdf](https://arxiv.org/pdf/2502.11305)]
> **Authors**: Andrii Krutsylo
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Continual learning is the process of training machine learning models on a sequence of tasks where data distributions change over time. A well-known obstacle in this setting is catastrophic forgetting, a phenomenon in which a model drastically loses performance on previously learned tasks when learning new ones. A popular strategy to alleviate this problem is experience replay, in which a subset of old samples is stored in a memory buffer and replayed with new data. Despite continual learning advances focusing on which examples to store and how to incorporate them into the training loss, most approaches assume that sampling from this buffer is uniform by default. We challenge the assumption that uniform sampling is necessarily optimal. We conduct an experiment in which the memory buffer updates the same way in every trial, but the replay probability of each stored sample changes between trials based on different random weight distributions. Specifically, we generate 50 different non-uniform sampling probability weights for each trial and compare their final accuracy to the uniform sampling baseline. We find that there is always at least one distribution that significantly outperforms the baseline across multiple buffer sizes, models, and datasets. These results suggest that more principled adaptive replay policies could yield further gains. We discuss how exploiting this insight could inspire new research on non-uniform memory sampling in continual learning to better mitigate catastrophic forgetting. The code supporting this study is available at $\href{https://github.com/DentonJC/memory-sampling}{https://github.com/DentonJC/memory-sampling}$.

### Balancing the Budget: Understanding Trade-offs Between Supervised and Preference-Based Finetuning 
[[arxiv](https://arxiv.org/abs/2502.11284)] [[cool](https://papers.cool/arxiv/2502.11284)] [[pdf](https://arxiv.org/pdf/2502.11284)]
> **Authors**: Mohit Raghavendra,Junmo Kang,Alan Ritter
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Post-training of Large Language Models often involves a pipeline of Supervised Finetuning (SFT) followed by Preference Finetuning (PFT) using methods like Direct Preference Optimization. Both stages require annotated data that are very different in structure and costs. We study how to optimally allocate a fixed training data budget between the two stages, through extensive experiments spanning four diverse tasks, multiple model sizes and various data annotation costs. Our findings reveal that just SFT on the base model dominates performance in low-data regimes ($<1,000$ annotated examples). With larger data-budgets, we observe that a combination of SFT and PFT, often with increasing portions allocated towards preference data yields optimal performance. However, completely eliminating SFT and running PFT directly on the base model yields suboptimal performance, described as the cold start problem on tasks like mathematics. We observe that this is due to the distribution shift arising from using DPO directly on the base model to elicit step-by-step reasoning. This limitation can be effectively addressed by allocating even a small portion ($<10$%) of the budget to SFT first, resulting in performance improvements of $15-20$% on analytical benchmarks like GSM8k. These results provide actionable insights for researchers and practitioners optimizing model development under budget constraints, where high-quality data curation often represents a significant portion of the total costs of model development.

### Neural Operators for Stochastic Modeling of Nonlinear Structural System Response to Natural Hazards 
[[arxiv](https://arxiv.org/abs/2502.11279)] [[cool](https://papers.cool/arxiv/2502.11279)] [[pdf](https://arxiv.org/pdf/2502.11279)]
> **Authors**: Somdatta Goswami,Dimitris G. Giovanis,Bowei Li,Seymour M. J. Spence,Michael D. Shields
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Traditionally, neural networks have been employed to learn the mapping between finite-dimensional Euclidean spaces. However, recent research has opened up new horizons, focusing on the utilization of deep neural networks to learn operators capable of mapping infinite-dimensional function spaces. In this work, we employ two state-of-the-art neural operators, the deep operator network (DeepONet) and the Fourier neural operator (FNO) for the prediction of the nonlinear time history response of structural systems exposed to natural hazards, such as earthquakes and wind. Specifically, we propose two architectures, a self-adaptive FNO and a Fast Fourier Transform-based DeepONet (DeepFNOnet), where we employ a FNO beyond the DeepONet to learn the discrepancy between the ground truth and the solution predicted by the DeepONet. To demonstrate the efficiency and applicability of the architectures, two problems are considered. In the first, we use the proposed model to predict the seismic nonlinear dynamic response of a six-story shear building subject to stochastic ground motions. In the second problem, we employ the operators to predict the wind-induced nonlinear dynamic response of a high-rise building while explicitly accounting for the stochastic nature of the wind excitation. In both cases, the trained metamodels achieve high accuracy while being orders of magnitude faster than their corresponding high-fidelity models.

### OctoTools: An Agentic Framework with Extensible Tools for Complex Reasoning 
[[arxiv](https://arxiv.org/abs/2502.11271)] [[cool](https://papers.cool/arxiv/2502.11271)] [[pdf](https://arxiv.org/pdf/2502.11271)]
> **Authors**: Pan Lu,Bowen Chen,Sheng Liu,Rahul Thapa,Joseph Boen,James Zou
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 89 pages, 18 figures. Project website: https://octotools.github.io/
- **标题**: None
- **领域**: 机器学习,计算语言学,计算机视觉和模式识别,多代理系统
- **Abstract**: Solving complex reasoning tasks may involve visual understanding, domain knowledge retrieval, numerical calculation, and multi-step reasoning. Existing methods augment large language models (LLMs) with external tools but are restricted to specialized domains, limited tool types, or require additional training data. In this paper, we introduce OctoTools, a training-free, user-friendly, and easily extensible open-source agentic framework designed to tackle complex reasoning across diverse domains. OctoTools introduces standardized tool cards to encapsulate tool functionality, a planner for both high-level and low-level planning, and an executor to carry out tool usage. We validate OctoTools' generality across 16 diverse tasks (including MathVista, MMLU-Pro, MedQA, and GAIA-Text), achieving substantial average accuracy gains of 9.3% over GPT-4o. Furthermore, OctoTools outperforms AutoGen, GPT-Functions and LangChain by up to 10.6% when given the same set of tools. Through comprehensive analysis and ablations, OctoTools demonstrates advantages in task planning, effective tool usage, and multi-step problem solving.

### Scalable Multi-Agent Offline Reinforcement Learning and the Role of Information 
[[arxiv](https://arxiv.org/abs/2502.11260)] [[cool](https://papers.cool/arxiv/2502.11260)] [[pdf](https://arxiv.org/pdf/2502.11260)]
> **Authors**: Riccardo Zamboni,Enrico Brunetti,Marcello Restelli
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Offline Reinforcement Learning (RL) focuses on learning policies solely from a batch of previously collected data. offering the potential to leverage such datasets effectively without the need for costly or risky active exploration. While recent advances in Offline Multi-Agent RL (MARL) have shown promise, most existing methods either rely on large datasets jointly collected by all agents or agent-specific datasets collected independently. The former approach ensures strong performance but raises scalability concerns, while the latter emphasizes scalability at the expense of performance guarantees. In this work, we propose a novel scalable routine for both dataset collection and offline learning. Agents first collect diverse datasets coherently with a pre-specified information-sharing network and subsequently learn coherent localized policies without requiring either full observability or falling back to complete decentralization. We theoretically demonstrate that this structured approach allows a multi-agent extension of the seminal Fitted Q-Iteration (FQI) algorithm to globally converge, in high probability, to near-optimal policies. The convergence is subject to error terms that depend on the informativeness of the shared information. Furthermore, we show how this approach allows to bound the inherent error of the supervised-learning phase of FQI with the mutual information between shared and unshared information. Our algorithm, SCAlable Multi-agent FQI (SCAM-FQI), is then evaluated on a distributed decision-making problem. The empirical results align with our theoretical findings, supporting the effectiveness of SCAM-FQI in achieving a balance between scalability and policy performance.

### Unveiling Environmental Impacts of Large Language Model Serving: A Functional Unit View 
[[arxiv](https://arxiv.org/abs/2502.11256)] [[cool](https://papers.cool/arxiv/2502.11256)] [[pdf](https://arxiv.org/pdf/2502.11256)]
> **Authors**: Yanran Wu,Inez Hua,Yi Ding
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 17 pages, 38 figures
- **标题**: None
- **领域**: 机器学习,硬件架构,计算语言学
- **Abstract**: Large language models (LLMs) offer powerful capabilities but come with significant environmental costs, particularly in carbon emissions. Existing studies benchmark these emissions but lack a standardized basis for comparison across models. To address this, we introduce the concept of a functional unit (FU) and develop FUEL, the first FU-based framework for evaluating LLM serving's environmental impact. Through case studies on model size, quantization, and hardware, we uncover key trade-offs in sustainability. Our findings highlight the potential for reducing carbon emissions by optimizing model selection, deployment strategies, and hardware choices, paving the way for more sustainable AI infrastructure.

### Shortcuts and Identifiability in Concept-based Models from a Neuro-Symbolic Lens 
[[arxiv](https://arxiv.org/abs/2502.11245)] [[cool](https://papers.cool/arxiv/2502.11245)] [[pdf](https://arxiv.org/pdf/2502.11245)]
> **Authors**: Samuele Bortolotti,Emanuele Marconato,Paolo Morettin,Andrea Passerini,Stefano Teso
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Concept-based Models are neural networks that learn a concept extractor to map inputs to high-level concepts and an inference layer to translate these into predictions. Ensuring these modules produce interpretable concepts and behave reliably in out-of-distribution is crucial, yet the conditions for achieving this remain unclear. We study this problem by establishing a novel connection between Concept-based Models and reasoning shortcuts (RSs), a common issue where models achieve high accuracy by learning low-quality concepts, even when the inference layer is fixed and provided upfront. Specifically, we first extend RSs to the more complex setting of Concept-based Models and then derive theoretical conditions for identifying both the concepts and the inference layer. Our empirical results highlight the impact of reasoning shortcuts and show that existing methods, even when combined with multiple natural mitigation strategies, often fail to meet these conditions in practice.

### Span-Agnostic Optimal Sample Complexity and Oracle Inequalities for Average-Reward RL 
[[arxiv](https://arxiv.org/abs/2502.11238)] [[cool](https://papers.cool/arxiv/2502.11238)] [[pdf](https://arxiv.org/pdf/2502.11238)]
> **Authors**: Matthew Zurek,Yudong Chen
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,信息论,优化与控制,机器学习
- **Abstract**: We study the sample complexity of finding an $\varepsilon$-optimal policy in average-reward Markov Decision Processes (MDPs) with a generative model. The minimax optimal span-based complexity of $\widetilde{O}(SAH/\varepsilon^2)$, where $H$ is the span of the optimal bias function, has only been achievable with prior knowledge of the value of $H$. Prior-knowledge-free algorithms have been the objective of intensive research, but several natural approaches provably fail to achieve this goal. We resolve this problem, developing the first algorithms matching the optimal span-based complexity without $H$ knowledge, both when the dataset size is fixed and when the suboptimality level $\varepsilon$ is fixed. Our main technique combines the discounted reduction approach with a method for automatically tuning the effective horizon based on empirical confidence intervals or lower bounds on performance, which we term horizon calibration. We also develop an empirical span penalization approach, inspired by sample variance penalization, which satisfies an oracle inequality performance guarantee. In particular this algorithm can outperform the minimax complexity in benign settings such as when there exist near-optimal policies with span much smaller than $H$.

### Deep Contrastive Learning for Feature Alignment: Insights from Housing-Household Relationship Inference 
[[arxiv](https://arxiv.org/abs/2502.11205)] [[cool](https://papers.cool/arxiv/2502.11205)] [[pdf](https://arxiv.org/pdf/2502.11205)]
> **Authors**: Xiao Qian,Shangjia Dong,Rachel Davidson
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机与社会
- **Abstract**: Housing and household characteristics are key determinants of social and economic well-being, yet our understanding of their interrelationships remains limited. This study addresses this knowledge gap by developing a deep contrastive learning (DCL) model to infer housing-household relationships using the American Community Survey (ACS) Public Use Microdata Sample (PUMS). More broadly, the proposed model is suitable for a class of problems where the goal is to learn joint relationships between two distinct entities without explicitly labeled ground truth data. Our proposed dual-encoder DCL approach leverages co-occurrence patterns in PUMS and introduces a bisect K-means clustering method to overcome the absence of ground truth labels. The dual-encoder DCL architecture is designed to handle the semantic differences between housing (building) and household (people) features while mitigating noise introduced by clustering. To validate the model, we generate a synthetic ground truth dataset and conduct comprehensive evaluations. The model further demonstrates its superior performance in capturing housing-household relationships in Delaware compared to state-of-the-art methods. A transferability test in North Carolina confirms its generalizability across diverse sociodemographic and geographic contexts. Finally, the post-hoc explainable AI analysis using SHAP values reveals that tenure status and mortgage information play a more significant role in housing-household matching than traditionally emphasized factors such as the number of persons and rooms.

### How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training 
[[arxiv](https://arxiv.org/abs/2502.11196)] [[cool](https://papers.cool/arxiv/2502.11196)] [[pdf](https://arxiv.org/pdf/2502.11196)]
> **Authors**: Yixin Ou,Yunzhi Yao,Ningyu Zhang,Hui Jin,Jiacheng Sun,Shumin Deng,Zhenguo Li,Huajun Chen
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: Work in progress
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,计算机视觉和模式识别,人机交互
- **Abstract**: Despite exceptional capabilities in knowledge-intensive tasks, Large Language Models (LLMs) face a critical gap in understanding how they internalize new knowledge, particularly how to structurally embed acquired knowledge in their neural computations. We address this issue through the lens of knowledge circuit evolution, identifying computational subgraphs that facilitate knowledge storage and processing. Our systematic analysis of circuit evolution throughout continual pre-training reveals several key findings: (1) the acquisition of new knowledge is influenced by its relevance to pre-existing knowledge; (2) the evolution of knowledge circuits exhibits a distinct phase shift from formation to optimization; (3) the evolution of knowledge circuits follows a deep-to-shallow pattern. These insights not only advance our theoretical understanding of the mechanisms of new knowledge acquisition in LLMs, but also provide potential implications for improving continual pre-training strategies to enhance model performance. Code and data will be available at https://github.com/zjunlp/DynamicKnowledgeCircuits.

### SURGE: On the Potential of Large Language Models as General-Purpose Surrogate Code Executors 
[[arxiv](https://arxiv.org/abs/2502.11167)] [[cool](https://papers.cool/arxiv/2502.11167)] [[pdf](https://arxiv.org/pdf/2502.11167)]
> **Authors**: Bohan Lyu,Siqiao Huang,Zichen Liang
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Large language models (LLMs) have demonstrated remarkable capabilities in code-related tasks, such as code understanding and code generation. However, an equally important yet underexplored question is whether LLMs can serve as general-purpose surrogate code executors, to predict the output and behavior of a program without actually running it. To systematically investigate this capability, we introduce SURGE, a comprehensive benchmark covering eight key aspects: multi-language programming tasks, competition-level programming problems, repository-level code analysis, high-cost scientific computing, time-complexity-intensive algorithms, buggy code analysis, programs dependent on specific compilers or execution environments, and formal mathematical proof verification. We evaluate multiple open-source and proprietary LLMs on SURGE and conduct a scaling study to analyze the impact of model size and training data scale on surrogate execution accuracy. Additionally, we categorize model prediction errors and explore potential areas for improvement. Our findings indicate that while LLMs can predict code execution results in certain cases, they exhibit limitations in general-purpose surrogate execution. This study provides empirical insights into the feasibility of using LLMs as surrogate code executors. Code and dataset are released at https://github.com/Imbernoulli/SURGE.

### Logarithmic Width Suffices for Robust Memorization 
[[arxiv](https://arxiv.org/abs/2502.11162)] [[cool](https://papers.cool/arxiv/2502.11162)] [[pdf](https://arxiv.org/pdf/2502.11162)]
> **Authors**: Amitsour Egosi,Gilad Yehudai,Ohad Shamir
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: The memorization capacity of neural networks with a given architecture has been thoroughly studied in many works. Specifically, it is well-known that memorizing $N$ samples can be done using a network of constant width, independent of $N$. However, the required constructions are often quite delicate. In this paper, we consider the natural question of how well feedforward ReLU neural networks can memorize robustly, namely while being able to withstand adversarial perturbations of a given radius. We establish both upper and lower bounds on the possible radius for general $l_p$ norms, implying (among other things) that width logarithmic in the number of input samples is necessary and sufficient to achieve robust memorization (with robustness radius independent of $N$).

### Large Language-Geometry Model: When LLM meets Equivariance 
[[arxiv](https://arxiv.org/abs/2502.11149)] [[cool](https://papers.cool/arxiv/2502.11149)] [[pdf](https://arxiv.org/pdf/2502.11149)]
> **Authors**: Zongzhao Li,Jiacheng Cen,Bing Su,Wenbing Huang,Tingyang Xu,Yu Rong,Deli Zhao
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Accurately predicting 3D structures and dynamics of physical systems is crucial in scientific applications. Existing approaches that rely on geometric Graph Neural Networks (GNNs) effectively enforce $\mathrm{E}(3)$-equivariance, but they often fall in leveraging extensive broader information. While direct application of Large Language Models (LLMs) can incorporate external knowledge, they lack the capability for spatial reasoning with guaranteed equivariance. In this paper, we propose EquiLLM, a novel framework for representing 3D physical systems that seamlessly integrates E(3)-equivariance with LLM capabilities. Specifically, EquiLLM comprises four key components: geometry-aware prompting, an equivariant encoder, an LLM, and an equivariant adaptor. Essentially, the LLM guided by the instructive prompt serves as a sophisticated invariant feature processor, while 3D directional information is exclusively handled by the equivariant encoder and adaptor modules. Experimental results demonstrate that EquiLLM delivers significant improvements over previous methods across molecular dynamics simulation, human motion simulation, and antibody design, highlighting its promising generalizability.

### Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity 
[[arxiv](https://arxiv.org/abs/2502.11147)] [[cool](https://papers.cool/arxiv/2502.11147)] [[pdf](https://arxiv.org/pdf/2502.11147)]
> **Authors**: Junhao Hu,Wenrui Huang,Weidong Wang,Zhenwen Li,Tiancheng Hu,Zhixia Liu,Xusheng Chen,Tao Xie,Yizhou Shan
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Large Language Models (LLMs) have demonstrated strong capabilities across various domains, with recent advancements in challenging reasoning tasks such as mathematics and programming. However, solving reasoning tasks often requires long decoding chains (of thoughts), which incur $O(N)$ time and memory consumption, where $N$ is the chain length. To mitigate $O(N)$ time and memory consumption, existing sparsity-based algorithms propose retaining only the most critical token's intermediate data (i.e., key-value cache) and discarding the rest. However, these existing algorithms struggle with the ``impossible trinity'' of accuracy, time, and memory. For example, the state-of-the-art algorithm, Quest, achieves high accuracy with $O(L)$ time but $O(N)$ memory ($L$ is the cache budget, $L \ll N$). To address this issue, in this paper, we identify a new attention pattern during the decode stage of reasoning tasks, where milestone tokens (analogous to lemmas in mathematical proofs) emerge, are utilized, and then become unimportant afterward. Based on this pattern, we propose a new algorithm named RaaS that identifies and retains milestone tokens only until they are no longer needed, achieving high accuracy with $O(L)$ time and $O(L)$ memory complexity.

### Machine Learning-Based Intrusion Detection and Prevention System for IIoT Smart Metering Networks: Challenges and Solutions 
[[arxiv](https://arxiv.org/abs/2502.11138)] [[cool](https://papers.cool/arxiv/2502.11138)] [[pdf](https://arxiv.org/pdf/2502.11138)]
> **Authors**: Sahar Lazim,Qutaiba I. Ali
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The Industrial Internet of Things (IIoT) has revolutionized industries by enabling automation, real-time data exchange, and smart decision-making. However, its increased connectivity introduces cybersecurity threats, particularly in smart metering networks, which play a crucial role in monitoring and optimizing energy consumption. This paper explores the challenges associated with securing IIoT-based smart metering networks and proposes a Machine Learning (ML)-based Intrusion Detection and Prevention System (IDPS) for safeguarding edge devices. The study reviews various intrusion detection approaches, highlighting the strengths and limitations of both signature-based and anomaly-based detection techniques. The findings suggest that integrating ML-driven IDPS in IIoT smart metering environments enhances security, efficiency, and resilience against evolving cyber threats.

### MasRouter: Learning to Route LLMs for Multi-Agent Systems 
[[arxiv](https://arxiv.org/abs/2502.11133)] [[cool](https://papers.cool/arxiv/2502.11133)] [[pdf](https://arxiv.org/pdf/2502.11133)]
> **Authors**: Yanwei Yue,Guibin Zhang,Boyang Liu,Guancheng Wan,Kun Wang,Dawei Cheng,Yiyan Qi
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,多代理系统
- **Abstract**: Multi-agent systems (MAS) powered by Large Language Models (LLMs) have been demonstrated to push the boundaries of LLM capabilities, yet they often incur significant costs and face challenges in dynamic LLM selection. Current LLM routing methods effectively reduce overhead in single-agent scenarios by customizing LLM selection for each query, but they overlook the critical decisions regarding collaboration modes and agent roles in MAS. In response to this challenge, we first introduce the problem of Multi-Agent System Routing (MASR), which integrates all components of MAS into a unified routing framework. Toward this goal, we propose MasRouter, the first high-performing, cost-effective, and inductive MASR solution. MasRouter employs collaboration mode determination, role allocation, and LLM routing through a cascaded controller network, progressively constructing a MAS that balances effectiveness and efficiency. Extensive experiments demonstrate that MasRouter is (1) high-performing, achieving a $1.8\%\sim8.2\%$ improvement over the state-of-the-art method on MBPP; (2) economical, reducing overhead by up to $52.07\%$ compared to SOTA methods on HumanEval; and (3) plug-and-play, seamlessly integrating with mainstream MAS frameworks, reducing overhead by $17.21\%\sim28.17\%$ via customized routing. The code is available at https://github.com/yanweiyue/masrouter.

### UNITE-FND: Reframing Multimodal Fake News Detection through Unimodal Scene Translation 
[[arxiv](https://arxiv.org/abs/2502.11132)] [[cool](https://papers.cool/arxiv/2502.11132)] [[pdf](https://arxiv.org/pdf/2502.11132)]
> **Authors**: Arka Mukherjee,Shreya Ghosh
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 28 pages, 16 figures
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Multimodal fake news detection typically demands complex architectures and substantial computational resources, posing deployment challenges in real-world settings. We introduce UNITE-FND, a novel framework that reframes multimodal fake news detection as a unimodal text classification task. We propose six specialized prompting strategies with Gemini 1.5 Pro, converting visual content into structured textual descriptions, and enabling efficient text-only models to preserve critical visual information. To benchmark our approach, we introduce Uni-Fakeddit-55k, a curated dataset family of 55,000 samples each, each processed through our multimodal-to-unimodal translation framework. Experimental results demonstrate that UNITE-FND achieves 92.52% accuracy in binary classification, surpassing prior multimodal models while reducing computational costs by over 10x (TinyBERT variant: 14.5M parameters vs. 250M+ in SOTA models). Additionally, we propose a comprehensive suite of five novel metrics to evaluate image-to-text conversion quality, ensuring optimal information preservation. Our results demonstrate that structured text-based representations can replace direct multimodal processing with minimal loss of accuracy, making UNITE-FND a practical and scalable alternative for resource-constrained environments.

### Revisiting Weak-to-Strong Generalization in Theory and Practice: Reverse KL vs. Forward KL 
[[arxiv](https://arxiv.org/abs/2502.11107)] [[cool](https://papers.cool/arxiv/2502.11107)] [[pdf](https://arxiv.org/pdf/2502.11107)]
> **Authors**: Wei Yao,Wenkai Yang,Ziqiao Wang,Yankai Lin,Yong Liu
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: As large language models advance toward superhuman performance, ensuring their alignment with human values and abilities grows increasingly complex. Weak-to-strong generalization offers a promising approach by leveraging predictions from weaker models to guide stronger systems, but its effectiveness could be constrained by the inherent noise and inaccuracies in these weak predictions. To address this, we propose a theoretically grounded approach that replaces forward KL divergence-whose mass-covering behavior risks overfitting to imperfect weak signals-with reverse KL divergence. Reverse KL divergence's zero-forcing effect prioritizes high-confidence predictions, effectively mitigating the influence of unreliable weak supervision. Theoretically, we extend existing bounds and derive tighter lower bounds for both forward and reverse KL divergence, establishing that reverse KL achieves at least comparable guarantees to forward KL. Notably, when a sufficiently pre-trained strong model is fine-tuned on the last layer, reverse KL uniquely guarantees that it outperforms its weak supervisor by the magnitude of their disagreement-a guarantee that forward KL cannot provide. Empirically, we demonstrate that reverse KL and reverse cross-entropy enable strong models to consistently outperform those trained with forward KL and standard cross-entropy across most settings, highlighting the practical advantages of these reverse losses.

### Towards Data-Efficient Pretraining for Atomic Property Prediction 
[[arxiv](https://arxiv.org/abs/2502.11085)] [[cool](https://papers.cool/arxiv/2502.11085)] [[pdf](https://arxiv.org/pdf/2502.11085)]
> **Authors**: Yasir Ghunaim,Hasan Abed Al Kader Hammoud,Bernard Ghanem
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: This paper challenges the recent paradigm in atomic property prediction that links progress to growing dataset sizes and computational resources. We show that pretraining on a carefully selected, task-relevant dataset can match or even surpass large-scale pretraining, while using as little as 1/24th of the computational cost. We introduce the Chemical Similarity Index (CSI), a novel metric inspired by computer vision's Fréchet Inception Distance, for molecular graphs which quantifies the alignment between upstream pretraining datasets and downstream tasks. By selecting the most relevant dataset with minimal CSI distance, we show that models pretrained on a smaller, focused dataset consistently outperform those pretrained on massive, mixed datasets such as JMP, even when those larger datasets include the relevant dataset. Counterintuitively, we also find that indiscriminately adding more data can degrade model performance when the additional data poorly aligns with the task at hand. Our findings highlight that quality often outperforms quantity in pretraining for atomic property prediction.

### Generalization of the Gibbs algorithm with high probability at low temperatures 
[[arxiv](https://arxiv.org/abs/2502.11071)] [[cool](https://papers.cool/arxiv/2502.11071)] [[pdf](https://arxiv.org/pdf/2502.11071)]
> **Authors**: Andreas Maurer
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: :68T05ACM Class:G.3
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: The paper gives a bound on the generalization error of the Gibbs algorithm, which recovers known data-independent bounds for the high temperature range and extends to the low-temperature range, where generalization depends critically on the data-dependent loss-landscape. It is shown, that with high probability the generalization error of a single hypothesis drawn from the Gibbs posterior decreases with the total prior volume of all hypotheses with similar or smaller empirical error. This gives theoretical support to the belief in the benefit of flat minima. The zero temperature limit is discussed and the bound is extended to a class of similar stochastic algorithms.

### Accelerating Anchors via Specialization and Feature Transformation 
[[arxiv](https://arxiv.org/abs/2502.11068)] [[cool](https://papers.cool/arxiv/2502.11068)] [[pdf](https://arxiv.org/pdf/2502.11068)]
> **Authors**: Haonan Yu,Junhao Liu,Xin Zhang
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Anchors is a popular local model-agnostic explanation technique whose applicability is limited by its computational inefficiency. To address this limitation, we propose a pre-training-based approach to accelerate Anchors without compromising the explanation quality. Our approach leverages the iterative nature of Anchors' algorithm which gradually refines an explanation until it is precise enough for a given input by providing a general explanation that is obtained through pre-training as Anchors' initial explanation. Specifically, we develop a two-step rule transformation process: the horizontal transformation adapts a pre-trained explanation to the current input by replacing features, and the vertical transformation refines the general explanation until it is precise enough for the input. We evaluate our method across tabular, text, and image datasets, demonstrating that it significantly reduces explanation generation time while maintaining fidelity and interpretability, thereby enabling the practical adoption of Anchors in time-sensitive applications.

### A Survey on Active Feature Acquisition Strategies 
[[arxiv](https://arxiv.org/abs/2502.11067)] [[cool](https://papers.cool/arxiv/2502.11067)] [[pdf](https://arxiv.org/pdf/2502.11067)]
> **Authors**: Arman Rahbar,Linus Aronsson,Morteza Haghir Chehreghani
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Active feature acquisition studies the challenge of making accurate predictions while limiting the cost of collecting complete data. By selectively acquiring only the most informative features for each instance, these strategies enable efficient decision-making in scenarios where data collection is expensive or time-consuming. This survey reviews recent progress in active feature acquisition, discussing common problem formulations, practical challenges, and key insights. We also highlight open issues and promising directions for future research.

### ClimateLLM: Efficient Weather Forecasting via Frequency-Aware Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.11059)] [[cool](https://papers.cool/arxiv/2502.11059)] [[pdf](https://arxiv.org/pdf/2502.11059)]
> **Authors**: Shixuan Li,Wei Yang,Peiyu Zhang,Xiongye Xiao,Defu Cao,Yuehan Qin,Xiaole Zhang,Yue Zhao,Paul Bogdan
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Weather forecasting is crucial for public safety, disaster prevention and mitigation, agricultural production, and energy management, with global relevance. Although deep learning has significantly advanced weather prediction, current methods face critical limitations: (i) they often struggle to capture both dynamic temporal dependencies and short-term abrupt changes, making extreme weather modeling difficult; (ii) they incur high computational costs due to extensive training and resource requirements; (iii) they have limited adaptability to multi-scale frequencies, leading to challenges when separating global trends from local fluctuations. To address these issues, we propose ClimateLLM, a foundation model for weather forecasting. It captures spatiotemporal dependencies via a cross-temporal and cross-spatial collaborative modeling framework that integrates Fourier-based frequency decomposition with Large Language Models (LLMs) to strengthen spatial and temporal modeling. Our framework uses a Mixture-of-Experts (MoE) mechanism that adaptively processes different frequency components, enabling efficient handling of both global signals and localized extreme events. In addition, we introduce a cross-temporal and cross-spatial dynamic prompting mechanism, allowing LLMs to incorporate meteorological patterns across multiple scales effectively. Extensive experiments on real-world datasets show that ClimateLLM outperforms state-of-the-art approaches in accuracy and efficiency, as a scalable solution for global weather forecasting.

### Deep Incomplete Multi-view Learning via Cyclic Permutation of VAEs 
[[arxiv](https://arxiv.org/abs/2502.11037)] [[cool](https://papers.cool/arxiv/2502.11037)] [[pdf](https://arxiv.org/pdf/2502.11037)]
> **Authors**: Xin Gao,Jian Pu
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 10 pages, 4 figures, ICLR 2025
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: Multi-View Representation Learning (MVRL) aims to derive a unified representation from multi-view data by leveraging shared and complementary information across views. However, when views are irregularly missing, the incomplete data can lead to representations that lack sufficiency and consistency. To address this, we propose Multi-View Permutation of Variational Auto-Encoders (MVP), which excavates invariant relationships between views in incomplete data. MVP establishes inter-view correspondences in the latent space of Variational Auto-Encoders, enabling the inference of missing views and the aggregation of more sufficient information. To derive a valid Evidence Lower Bound (ELBO) for learning, we apply permutations to randomly reorder variables for cross-view generation and then partition them by views to maintain invariant meanings under permutations. Additionally, we enhance consistency by introducing an informational prior with cyclic permutations of posteriors, which turns the regularization term into a similarity measure across distributions. We demonstrate the effectiveness of our approach on seven diverse datasets with varying missing ratios, achieving superior performance in multi-view clustering and generation tasks.

### AdaGC: Improving Training Stability for Large Language Model Pretraining 
[[arxiv](https://arxiv.org/abs/2502.11034)] [[cool](https://papers.cool/arxiv/2502.11034)] [[pdf](https://arxiv.org/pdf/2502.11034)]
> **Authors**: Guoxia Wang,Shuai Li,Congliang Chen,Jinle Zeng,Jiabin Yang,Tao Sun,Yanjun Ma,Dianhai Yu,Li Shen
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Large Language Models (LLMs) face increasing loss spikes during scaling, undermining training stability and final performance. While gradient clipping mitigates this issue, traditional global approaches poorly handle parameter-specific gradient variations and decaying gradient norms. We propose **AdaGC**, an adaptive gradient clipping framework that automatically adjusts local thresholds per parameter through exponential moving average of gradient norms. Theoretical analysis proves AdaGC's convergence under non-convex conditions. Extensive experiments demonstrate significant improvements: On Llama-2 7B/13B, AdaGC completely eliminates loss spikes while reducing WikiText perplexity by 3.5% (+0.14pp LAMBADA accuracy) for 7B and achieving 0.65% lower training loss with 1.47% reduced validation perplexity for 13B compared to global clipping. For CLIP ViT-Base, AdaGC converges 25% faster than StableAdamW with full spike elimination. The method shows universal effectiveness across architectures (Llama-2 7B/13B) and modalities (CLIP), with successful integration into diverse optimizers like AdamW and Lion. Source code will be released on GitHub.

### Convergence of Policy Mirror Descent Beyond Compatible Function Approximation 
[[arxiv](https://arxiv.org/abs/2502.11033)] [[cool](https://papers.cool/arxiv/2502.11033)] [[pdf](https://arxiv.org/pdf/2502.11033)]
> **Authors**: Uri Sherman,Tomer Koren,Yishay Mansour
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,优化与控制,机器学习
- **Abstract**: Modern policy optimization methods roughly follow the policy mirror descent (PMD) algorithmic template, for which there are by now numerous theoretical convergence results. However, most of these either target tabular environments, or can be applied effectively only when the class of policies being optimized over satisfies strong closure conditions, which is typically not the case when working with parametric policy classes in large-scale environments. In this work, we develop a theoretical framework for PMD for general policy classes where we replace the closure conditions with a strictly weaker variational gradient dominance assumption, and obtain upper bounds on the rate of convergence to the best-in-class policy. Our main result leverages a novel notion of smoothness with respect to a local norm induced by the occupancy measure of the current policy, and casts PMD as a particular instance of smooth non-convex optimization in non-Euclidean space.

### A Critical Review of Predominant Bias in Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.11031)] [[cool](https://papers.cool/arxiv/2502.11031)] [[pdf](https://arxiv.org/pdf/2502.11031)]
> **Authors**: Jiazhi Li,Mahyar Khayatkhoei,Jiageng Zhu,Hanchen Xie,Mohamed E. Hussein,Wael AbdAlmageed
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 31 pages, 8 figures, 13 tables
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Bias issues of neural networks garner significant attention along with its promising advancement. Among various bias issues, mitigating two predominant biases is crucial in advancing fair and trustworthy AI: (1) ensuring neural networks yields even performance across demographic groups, and (2) ensuring algorithmic decision-making does not rely on protected attributes. However, upon the investigation of \pc papers in the relevant literature, we find that there exists a persistent, extensive but under-explored confusion regarding these two types of biases. Furthermore, the confusion has already significantly hampered the clarity of the community and subsequent development of debiasing methodologies. Thus, in this work, we aim to restore clarity by providing two mathematical definitions for these two predominant biases and leveraging these definitions to unify a comprehensive list of papers. Next, we highlight the common phenomena and the possible reasons for the existing confusion. To alleviate the confusion, we provide extensive experiments on synthetic, census, and image datasets, to validate the distinct nature of these biases, distinguish their different real-world manifestations, and evaluate the effectiveness of a comprehensive list of bias assessment metrics in assessing the mitigation of these biases. Further, we compare these two types of biases from multiple dimensions including the underlying causes, debiasing methods, evaluation protocol, prevalent datasets, and future directions. Last, we provide several suggestions aiming to guide researchers engaged in bias-related work to avoid confusion and further enhance clarity in the community.

### Diversified Sampling Improves Scaling LLM inference 
[[arxiv](https://arxiv.org/abs/2502.11027)] [[cool](https://papers.cool/arxiv/2502.11027)] [[pdf](https://arxiv.org/pdf/2502.11027)]
> **Authors**: Tianchun Wang,Zichuan Liu,Yuanzhou Chen,Jonathan Light,Haifeng Chen,Xiang Zhang,Wei Cheng
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 19 pages
- **标题**: None
- **领域**: 机器学习
- **Abstract**: While increasing training compute has significantly improved the performance of large language models (LLMs), similar gains have not been observed when scaling inference compute. We hypothesize that the primary issue lies in the uniformity of LLM outputs, which leads to inefficient sampling as models repeatedly generate similar but inaccurate responses. Motivated by an intriguing relationship between solution accuracy (Pass@10) and response diversity, we propose DivSampling -- a novel and versatile sampling technique designed to enhance the diversity of candidate solutions by introducing prompt perturbations.DivSampling incorporates two categories of perturbations: task-agnostic approaches, which are general and not tailored to any specific task, and task-specific approaches, which are customized based on task content. Our theoretical analysis demonstrates that, under mild assumptions, the error rates of responses generated from diverse prompts are significantly lower compared to those produced by stationary prompts. Comprehensive evaluations across various tasks -- including reasoning, mathematics, and code generation -- highlight the effectiveness of DivSampling in improving solution accuracy. This scalable and efficient approach offers a new perspective on optimizing test-time inference, addressing limitations in current sampling strategies.

### Simplify RLHF as Reward-Weighted SFT: A Variational Method 
[[arxiv](https://arxiv.org/abs/2502.11026)] [[cool](https://papers.cool/arxiv/2502.11026)] [[pdf](https://arxiv.org/pdf/2502.11026)]
> **Authors**: Yuhao Du,Zhuo Li,Pengyu Cheng,Zhihong Chen,Yuejiao Xie,Xiang Wan,Anningzhe Gao
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning Large Language Models (LLMs) with human values. However, RLHF has been continuously challenged by its high complexity in implementation and computation consumption. Even with recent simplifications, such as Direct Preference Optimization (DPO) and Advantage Leftover Lunch (A-LoL), the problems of over-fitting and training instability remain hindering the alignment process from the expected optimal performance. To address the existing challenges, we propose a novel simplification of RLHF from the perspective of variational inference, called $\textbf{V}$ariational $\textbf{A}$lignment with $\textbf{R}$e-weighting ($\textbf{VAR}$). More specifically, by directly minimizing the distribution gap between the learning LLM policy and the optimal solution of RLHF, we transform the alignment objective into a reward-driven re-weighted supervised fine-tuning (SFT) form, which only requires minor adjustment on the SFT loss to obtain noticeable improvement on training stability and effectiveness. On comprehensive alignment and generation benchmarks, our VAR method has numerically achieved competitive performance in LLM alignment helpfulness and harmlessness.

### Unlocking the Power of Function Vectors for Characterizing and Mitigating Catastrophic Forgetting in Continual Instruction Tuning 
[[arxiv](https://arxiv.org/abs/2502.11019)] [[cool](https://papers.cool/arxiv/2502.11019)] [[pdf](https://arxiv.org/pdf/2502.11019)]
> **Authors**: Gangwei Jiang,Caigao Jiang,Zhaoyi Li,Siqiao Xue,Jun Zhou,Linqi Song,Defu Lian,Yin Wei
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 10pages
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Catastrophic forgetting (CF) poses a significant challenge in machine learning, where a model forgets previously learned information upon learning new tasks. Despite the advanced capabilities of Large Language Models (LLMs), they continue to face challenges with CF during continual learning. The majority of existing research focuses on analyzing forgetting patterns through a singular training sequence, thereby overlooking the intricate effects that diverse tasks have on model behavior. Our study explores CF across various settings, discovering that model forgetting is influenced by both the specific training tasks and the models themselves. To this end, we interpret forgetting by examining the function vector (FV), a compact representation of functions in LLMs, offering a model-dependent indicator for the occurrence of CF. Through theoretical and empirical analyses, we demonstrated that CF in LLMs primarily stems from biases in function activation rather than the overwriting of task processing functions. Leveraging these insights, we propose a novel function vector guided training methodology, incorporating a regularization technique to stabilize the FV and mitigate forgetting. Empirical tests on four benchmarks confirm the effectiveness of our proposed training method, substantiating our theoretical framework concerning CF and model function dynamics. We plan to make our code publicly accessible in the near future.

### Collaborative Deterministic-Diffusion Model for Probabilistic Urban Spatiotemporal Prediction 
[[arxiv](https://arxiv.org/abs/2502.11013)] [[cool](https://papers.cool/arxiv/2502.11013)] [[pdf](https://arxiv.org/pdf/2502.11013)]
> **Authors**: Zhi Sheng,Yuan Yuan,Yudi Zhang,Depeng Jin,Yong Li
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Accurate prediction of urban spatiotemporal dynamics is essential for enhancing urban management and decision-making. Existing spatiotemporal prediction models are predominantly deterministic, focusing on primary spatiotemporal patterns. However, those dynamics are highly complex, exhibiting multi-modal distributions that are challenging for deterministic models to capture. In this paper, we highlight the critical role of probabilistic prediction in capturing the uncertainties and complexities inherent in spatiotemporal data. While mainstream probabilistic models can capture uncertainty, they struggle with accurately learning primary patterns and often suffer from computational inefficiency. To address these challenges, we propose CoST, which collaborates deterministic and probabilistic models to improve both predictive accuracy and the ability to handle uncertainty. To achieve this, we design a mean-residual decomposition framework, where the mean value is modeled by a deterministic model, and the residual variations are learned by a probabilistic model, specifically diffusion models. Moreover, we introduce a scale-aware diffusion process, which better accounts for spatially heterogeneous dynamics across different regions. Extensive experiments on eight real-world datasets demonstrate that CoST significantly outperforms existing methods in both deterministic and probabilistic metrics, achieving a 20% improvement with low computational cost. CoST bridges the gap between deterministic precision and probabilistic uncertainty, making a significant advancement in the field of urban spatiotemporal prediction.

### Local-Cloud Inference Offloading for LLMs in Multi-Modal, Multi-Task, Multi-Dialogue Settings 
[[arxiv](https://arxiv.org/abs/2502.11007)] [[cool](https://papers.cool/arxiv/2502.11007)] [[pdf](https://arxiv.org/pdf/2502.11007)]
> **Authors**: Liangqi Yuan,Dong-Jun Han,Shiqiang Wang,Christopher G. Brinton
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,分布式、并行和集群计算
- **Abstract**: Compared to traditional machine learning models, recent large language models (LLMs) can exhibit multi-task-solving capabilities through multiple dialogues and multi-modal data sources. These unique characteristics of LLMs, beyond their large size, make their deployment more challenging during the inference stage. Specifically, (i) deploying LLMs on local devices faces computational, memory, and energy resource issues, while (ii) deploying them in the cloud cannot guarantee real-time service and incurs communication/usage costs. In this paper, we design a local-cloud LLM inference offloading (LCIO) system, featuring (i) a large-scale cloud LLM that can handle multi-modal data sources and (ii) a lightweight local LLM that can process simple tasks at high speed. LCIO employs resource-constrained reinforcement learning (RCRL) to determine where to make the inference (i.e., local vs. cloud) and which multi-modal data sources to use for each dialogue/task, aiming to maximize the long-term reward (which incorporates response quality, latency, and usage cost) while adhering to resource constraints. We also propose M4A1, a new dataset that accounts for multi-modal, multi-task, multi-dialogue, and multi-LLM characteristics, to investigate the capabilities of LLMs in various practical scenarios. We demonstrate the effectiveness of LCIO compared to baselines, showing significant savings in latency and cost while achieving satisfactory response quality.

### New Rates in Stochastic Decision-Theoretic Online Learning under Differential Privacy 
[[arxiv](https://arxiv.org/abs/2502.10997)] [[cool](https://papers.cool/arxiv/2502.10997)] [[pdf](https://arxiv.org/pdf/2502.10997)]
> **Authors**: Ruihan Wu,Yu-Xiang Wang
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,密码学和安全,数据结构和算法
- **Abstract**: Hu and Mehta (2024) posed an open problem: what is the optimal instance-dependent rate for the stochastic decision-theoretic online learning (with $K$ actions and $T$ rounds) under $\varepsilon$-differential privacy? Before, the best known upper bound and lower bound are $O\left(\frac{\log K}{Δ_{\min}} + \frac{\log K\log T}{\varepsilon}\right)$ and $Ω\left(\frac{\log K}{Δ_{\min}} + \frac{\log K}{\varepsilon}\right)$ (where $Δ_{\min}$ is the gap between the optimal and the second actions). In this paper, we partially address this open problem by having two new results. First, we provide an improved upper bound for this problem $O\left(\frac{\log K}{Δ_{\min}} + \frac{\log^2K}{\varepsilon}\right)$, where the $T$-dependency has been removed. Second, we introduce the deterministic setting, a weaker setting of this open problem, where the received loss vector is deterministic and we can focus on the analysis for $\varepsilon$ regardless of the sampling error. At the deterministic setting, we prove upper and lower bounds that match at $Θ\left(\frac{\log K}{\varepsilon}\right)$, while a direct application of the analysis and algorithms from the original setting still leads to an extra log factor. Technically, we introduce the Bernoulli resampling trick, which enforces a monotonic property for the output from report-noisy-max mechanism that enables a tighter analysis. Moreover, by replacing the Laplace noise with Gumbel noise, we derived explicit integral form that gives a tight characterization of the regret in the deterministic case.

### SSVEP-BiMA: Bifocal Masking Attention Leveraging Native and Symmetric-Antisymmetric Components for Robust SSVEP Decoding 
[[arxiv](https://arxiv.org/abs/2502.10994)] [[cool](https://papers.cool/arxiv/2502.10994)] [[pdf](https://arxiv.org/pdf/2502.10994)]
> **Authors**: Yuxin Liu,Zhenxi Song,Guoyang Xu,Zirui Wang,Feng Wan,Yong Hu,Min Zhang,Zhiguo Zhang
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Brain-computer interface (BCI) based on steady-state visual evoked potentials (SSVEP) is a popular paradigm for its simplicity and high information transfer rate (ITR). Accurate and fast SSVEP decoding is crucial for reliable BCI performance. However, conventional decoding methods demand longer time windows, and deep learning models typically require subject-specific fine-tuning, leaving challenges in achieving optimal performance in cross-subject settings. This paper proposed a biofocal masking attention-based method (SSVEP-BiMA) that synergistically leverages the native and symmetric-antisymmetric components for decoding SSVEP. By utilizing multiple signal representations, the network is able to integrate features from a wider range of sample perspectives, leading to more generalized and comprehensive feature learning, which enhances both prediction accuracy and robustness. We performed experiments on two public datasets, and the results demonstrate that our proposed method surpasses baseline approaches in both accuracy and ITR. We believe that this work will contribute to the development of more efficient SSVEP-based BCI systems.

### Is Elo Rating Reliable? A Study Under Model Misspecification 
[[arxiv](https://arxiv.org/abs/2502.10985)] [[cool](https://papers.cool/arxiv/2502.10985)] [[pdf](https://arxiv.org/pdf/2502.10985)]
> **Authors**: Shange Tang,Yuanhao Wang,Chi Jin
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: 23pages
- **标题**: None
- **领域**: 机器学习,人工智能,方法论,机器学习
- **Abstract**: Elo rating, widely used for skill assessment across diverse domains ranging from competitive games to large language models, is often understood as an incremental update algorithm for estimating a stationary Bradley-Terry (BT) model. However, our empirical analysis of practical matching datasets reveals two surprising findings: (1) Most games deviate significantly from the assumptions of the BT model and stationarity, raising questions on the reliability of Elo. (2) Despite these deviations, Elo frequently outperforms more complex rating systems, such as mElo and pairwise models, which are specifically designed to account for non-BT components in the data, particularly in terms of win rate prediction. This paper explains this unexpected phenomenon through three key perspectives: (a) We reinterpret Elo as an instance of online gradient descent, which provides no-regret guarantees even in misspecified and non-stationary settings. (b) Through extensive synthetic experiments on data generated from transitive but non-BT models, such as strongly or weakly stochastic transitive models, we show that the ''sparsity'' of practical matching data is a critical factor behind Elo's superior performance in prediction compared to more complex rating systems. (c) We observe a strong correlation between Elo's predictive accuracy and its ranking performance, further supporting its effectiveness in ranking.

### Graders should cheat: privileged information enables expert-level automated evaluations 
[[arxiv](https://arxiv.org/abs/2502.10961)] [[cool](https://papers.cool/arxiv/2502.10961)] [[pdf](https://arxiv.org/pdf/2502.10961)]
> **Authors**: Jin Peng Zhou,Sébastien M. R. Arnold,Nan Ding,Kilian Q. Weinberger,Nan Hua,Fei Sha
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Auto-evaluating language models (LMs), i.e., using a grader LM to evaluate the candidate LM, is an appealing way to accelerate the evaluation process and the cost associated with it. But this presents a paradox: how can we trust the grader LM, which is presumably weaker than the candidate LM, to assess problems that are beyond the frontier of the capabilities of either model or both? For instance, today's LMs struggle on graduate-level physics and Olympiad-level math, making them unreliable graders in these domains. We show that providing privileged information -- such as ground-truth solutions or problem-specific guidelines -- improves automated evaluations on such frontier problems. This approach offers two key advantages. First, it expands the range of problems where LMs graders apply. Specifically, weaker models can now rate the predictions of stronger models. Second, privileged information can be used to devise easier variations of challenging problems which improves the separability of different LMs on tasks where their performance is generally low. With this approach, general-purpose LM graders match the state of the art performance on RewardBench, surpassing almost all the specially-tuned models. LM graders also outperform individual human raters on Vibe-Eval, and approach human expert graders on Olympiad-level math problems.

### The Relationship between No-Regret Learning and Online Conformal Prediction 
[[arxiv](https://arxiv.org/abs/2502.10947)] [[cool](https://papers.cool/arxiv/2502.10947)] [[pdf](https://arxiv.org/pdf/2502.10947)]
> **Authors**: Ramya Ramalingam,Shayan Kiyani,Aaron Roth
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机科学与博弈论,机器学习
- **Abstract**: Existing algorithms for online conformal prediction -- guaranteeing marginal coverage in adversarial settings -- are variants of online gradient descent (OGD), but their analyses of worst-case coverage do not follow from the regret guarantee of OGD. What is the relationship between no-regret learning and online conformal prediction? We observe that although standard regret guarantees imply marginal coverage in i.i.d. settings, this connection fails as soon as we either move to adversarial environments or ask for group conditional coverage. On the other hand, we show a tight connection between threshold calibrated coverage and swap-regret in adversarial settings, which extends to group-conditional (multi-valid) coverage. We also show that algorithms in the follow the perturbed leader family of no regret learning algorithms (which includes online gradient descent) can be used to give group-conditional coverage guarantees in adversarial settings for arbitrary grouping functions. Via this connection we analyze and conduct experiments using a multi-group generalization of the ACI algorithm of Gibbs & Candes [2021] (arXiv:2106.00170).

### CoLA: Compute-Efficient Pre-Training of LLMs via Low-Rank Activation 
[[arxiv](https://arxiv.org/abs/2502.10940)] [[cool](https://papers.cool/arxiv/2502.10940)] [[pdf](https://arxiv.org/pdf/2502.10940)]
> **Authors**: Ziyue Liu,Ruijie Zhang,Zhengyang Wang,Zi Yang,Paul Hovland,Bogdan Nicolae,Franck Cappello,Zheng Zhang
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: Preprint
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Large language models (LLMs) are revolutionizing many science and engineering fields. However, their huge model sizes impose extremely demanding needs of computational resources in the pre-training stage. Although low-rank factorizations can reduce model parameters, their direct application in LLM pre-training often lead to non-negligible performance loss. To address this fundamental challenge, we introduce CoLA and its memory-efficient implementation, CoLA-M. We leverage the low-rank structure observed widely in model activations, enforcing non-linear transformations between factorized weight matrices to reduce model size, boost model capacity and training efficiency. Experiments on LLaMA models with 60 million to 7 billion parameters show that CoLA reduces the computing cost by $\bf 2\pmb{\times}$ and improves training throughput by $\bf 1.86\pmb{\times}$ while maintaining full-rank level performance. CoLA-M further squeezes memory cost without sacrificing throughput, offering a pre-training approach with collectively superior parameter, computing, and memory efficiency. The LLMs produced are also $\bf 2\pmb{\times}$ smaller, enabling faster inference with lower memory cost on resource-constrained platforms

### Reduced Order Modeling with Shallow Recurrent Decoder Networks 
[[arxiv](https://arxiv.org/abs/2502.10930)] [[cool](https://papers.cool/arxiv/2502.10930)] [[pdf](https://arxiv.org/pdf/2502.10930)]
> **Authors**: Matteo Tomasetto,Jan P. Williams,Francesco Braghin,Andrea Manzoni,J. Nathan Kutz
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,动力系统
- **Abstract**: Reduced Order Modeling is of paramount importance for efficiently inferring high-dimensional spatio-temporal fields in parametric contexts, enabling computationally tractable parametric analyses, uncertainty quantification and control. However, conventional dimensionality reduction techniques are typically limited to known and constant parameters, inefficient for nonlinear and chaotic dynamics, and uninformed to the actual system behavior. In this work, we propose sensor-driven SHallow REcurrent Decoder networks for Reduced Order Modeling (SHRED-ROM). Specifically, we consider the composition of a long short-term memory network, which encodes the temporal dynamics of limited sensor data in multiple scenarios, and a shallow decoder, which reconstructs the corresponding high-dimensional states. SHRED-ROM is a robust decoding-only strategy that circumvents the numerically unstable approximation of an inverse which is required by encoding-decoding schemes. To enhance computational efficiency and memory usage, the full-order state snapshots are reduced by, e.g., proper orthogonal decomposition, allowing for compressive training of the networks with minimal hyperparameter tuning. Through applications on chaotic and nonlinear fluid dynamics, we show that SHRED-ROM (i) accurately reconstructs the state dynamics for new parameter values starting from limited fixed or mobile sensors, independently on sensor placement, (ii) can cope with both physical, geometrical and time-dependent parametric dependencies, while being agnostic to their actual values, (iii) can accurately estimate unknown parameters, and (iv) can deal with different data sources, such as high-fidelity simulations, coupled fields and videos.

### Semantic Specialization in MoE Appears with Scale: A Study of DeepSeek R1 Expert Specialization 
[[arxiv](https://arxiv.org/abs/2502.10928)] [[cool](https://papers.cool/arxiv/2502.10928)] [[pdf](https://arxiv.org/pdf/2502.10928)]
> **Authors**: Matthew Lyle Olson,Neale Ratzlaff,Musashi Hinck,Man Luo,Sungduk Yu,Chendi Xue,Vasudev Lal
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: DeepSeek-R1, the largest open-source Mixture-of-Experts (MoE) model, has demonstrated reasoning capabilities comparable to proprietary frontier models. Prior research has explored expert routing in MoE models, but findings suggest that expert selection is often token-dependent rather than semantically driven. Given DeepSeek-R1's enhanced reasoning abilities, we investigate whether its routing mechanism exhibits greater semantic specialization than previous MoE models. To explore this, we conduct two key experiments: (1) a word sense disambiguation task, where we examine expert activation patterns for words with differing senses, and (2) a cognitive reasoning analysis, where we assess DeepSeek-R1's structured thought process in an interactive task setting of DiscoveryWorld. We conclude that DeepSeek-R1's routing mechanism is more semantically aware and it engages in structured cognitive processes.

### The underlying structures of self-attention: symmetry, directionality, and emergent dynamics in Transformer training 
[[arxiv](https://arxiv.org/abs/2502.10927)] [[cool](https://papers.cool/arxiv/2502.10927)] [[pdf](https://arxiv.org/pdf/2502.10927)]
> **Authors**: Matteo Saponati,Pascal Sager,Pau Vilimelis Aceituno,Thilo Stadelmann,Benjamin Grewe
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Self-attention is essential to Transformer architectures, yet how information is embedded in the self-attention matrices and how different objective functions impact this process remains unclear. We present a mathematical framework to analyze self-attention matrices by deriving the structures governing their weight updates. Using this framework, we demonstrate that bidirectional training induces symmetry in the weight matrices, while autoregressive training results in directionality and column dominance. Our theoretical findings are validated across multiple Transformer models - including ModernBERT, GPT, LLaMA3, and Mistral - and input modalities like text, vision, and audio. Finally, we apply these insights by showing that symmetric initialization improves the performance of encoder-only models on language tasks. This mathematical analysis offers a novel theoretical perspective on how information is embedded through self-attention, thereby improving the interpretability of Transformer models.

### LLM-driven Knowledge Distillation for Dynamic Text-Attributed Graphs 
[[arxiv](https://arxiv.org/abs/2502.10914)] [[cool](https://papers.cool/arxiv/2502.10914)] [[pdf](https://arxiv.org/pdf/2502.10914)]
> **Authors**: Amit Roy,Ning Yan,Masood Mortazavi
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: Accepted at the AI4TS:AIfor Time Series Analysis workshop, AAAI 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Dynamic Text-Attributed Graphs (DyTAGs) have numerous real-world applications, e.g. social, collaboration, citation, communication, and review networks. In these networks, nodes and edges often contain text descriptions, and the graph structure can evolve over time. Future link prediction, edge classification, relation generation, and other downstream tasks on DyTAGs require powerful representations that encode structural, temporal, and textual information. Although graph neural networks (GNNs) excel at handling structured data, encoding temporal information within dynamic graphs remains a significant challenge. In this work, we propose LLM-driven Knowledge Distillation for Dynamic Text Attributed Graph (LKD4DyTAG) with temporal encoding to address these challenges. We use a simple, yet effective approach to encode temporal information in edges so that graph convolution can simultaneously capture both temporal and structural information in the hidden representations. To leverage LLM's text processing capabilities for learning richer representations on DyTAGs, we distill knowledge from LLM-driven edge representations (based on a neighborhood's text attributes) into saptio-temporal representations using a lightweight GNN model that encodes temporal and structural information. The objective of knowledge distillation enables the GNN to learn representations that more effectively encode the available structural, temporal, and textual information in DyTAG. We conducted extensive experimentation on six real-world DyTAG datasets to verify the effectiveness of our approach LKD4DyTAG for future link prediction and edge classification task. The results show that our approach significantly improves the performance of downstream tasks compared to the baseline models.

### Learning Identifiable Structures Helps Avoid Bias in DNN-based Supervised Causal Learning 
[[arxiv](https://arxiv.org/abs/2502.10883)] [[cool](https://papers.cool/arxiv/2502.10883)] [[pdf](https://arxiv.org/pdf/2502.10883)]
> **Authors**: Jiaru Zhang,Rui Ding,Qiang Fu,Bojun Huang,Zizhen Deng,Yang Hua,Haibing Guan,Shi Han,Dongmei Zhang
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,方法论
- **Abstract**: Causal discovery is a structured prediction task that aims to predict causal relations among variables based on their data samples. Supervised Causal Learning (SCL) is an emerging paradigm in this field. Existing Deep Neural Network (DNN)-based methods commonly adopt the "Node-Edge approach", in which the model first computes an embedding vector for each variable-node, then uses these variable-wise representations to concurrently and independently predict for each directed causal-edge. In this paper, we first show that this architecture has some systematic bias that cannot be mitigated regardless of model size and data size. We then propose SiCL, a DNN-based SCL method that predicts a skeleton matrix together with a v-tensor (a third-order tensor representing the v-structures). According to the Markov Equivalence Class (MEC) theory, both the skeleton and the v-structures are identifiable causal structures under the canonical MEC setting, so predictions about skeleton and v-structures do not suffer from the identifiability limit in causal discovery, thus SiCL can avoid the systematic bias in Node-Edge architecture, and enable consistent estimators for causal discovery. Moreover, SiCL is also equipped with a specially designed pairwise encoder module with a unidirectional attention layer to model both internal and external relationships of pairs of nodes. Experimental results on both synthetic and real-world benchmarks show that SiCL significantly outperforms other DNN-based SCL approaches.

### To Bin or not to Bin: Alternative Representations of Mass Spectra 
[[arxiv](https://arxiv.org/abs/2502.10851)] [[cool](https://papers.cool/arxiv/2502.10851)] [[pdf](https://arxiv.org/pdf/2502.10851)]
> **Authors**: Niek de Jonge,Justin J. J. van der Hooft,Daniel Probst
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: This manuscript has been submitted to the tiny paper track at the LMRL workshop at ICLR 2025
- **标题**: None
- **领域**: 机器学习,化学物理,定量方法
- **Abstract**: Mass spectrometry, especially so-called tandem mass spectrometry, is commonly used to assess the chemical diversity of samples. The resulting mass fragmentation spectra are representations of molecules of which the structure may have not been determined. This poses the challenge of experimentally determining or computationally predicting molecular structures from mass spectra. An alternative option is to predict molecular properties or molecular similarity directly from spectra. Various methodologies have been proposed to embed mass spectra for further use in machine learning tasks. However, these methodologies require preprocessing of the spectra, which often includes binning or sub-sampling peaks with the main reasoning of creating uniform vector sizes and removing noise. Here, we investigate two alternatives to the binning of mass spectra before down-stream machine learning tasks, namely, set-based and graph-based representations. Comparing the two proposed representations to train a set transformer and a graph neural network on a regression task, respectively, we show that they both perform substantially better than a multilayer perceptron trained on binned data.

### Implicit Neural Representations of Molecular Vector-Valued Functions 
[[arxiv](https://arxiv.org/abs/2502.10848)] [[cool](https://papers.cool/arxiv/2502.10848)] [[pdf](https://arxiv.org/pdf/2502.10848)]
> **Authors**: Jirka Lhotka,Daniel Probst
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: This is a tiny paper track submission to the LRML workshop at ICLR
- **标题**: None
- **领域**: 机器学习,定量方法
- **Abstract**: Molecules have various computational representations, including numerical descriptors, strings, graphs, point clouds, and surfaces. Each representation method enables the application of various machine learning methodologies from linear regression to graph neural networks paired with large language models. To complement existing representations, we introduce the representation of molecules through vector-valued functions, or $n$-dimensional vector fields, that are parameterized by neural networks, which we denote molecular neural fields. Unlike surface representations, molecular neural fields capture external features and the hydrophobic core of macromolecules such as proteins. Compared to discrete graph or point representations, molecular neural fields are compact, resolution independent and inherently suited for interpolation in spatial and temporal dimensions. These properties inherited by molecular neural fields lend themselves to tasks including the generation of molecules based on their desired shape, structure, and composition, and the resolution-independent interpolation between molecular conformations in space and time. Here, we provide a framework and proofs-of-concept for molecular neural fields, namely, the parametrization and superresolution reconstruction of a protein-ligand complex using an auto-decoder architecture and the embedding of molecular volumes in latent space using an auto-encoder architecture.

### LEAPS: A discrete neural sampler via locally equivariant networks 
[[arxiv](https://arxiv.org/abs/2502.10843)] [[cool](https://papers.cool/arxiv/2502.10843)] [[pdf](https://arxiv.org/pdf/2502.10843)]
> **Authors**: Peter Holderrieth,Michael S. Albergo,Tommi Jaakkola
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算,机器学习
- **Abstract**: We propose LEAPS, an algorithm to sample from discrete distributions known up to normalization by learning a rate matrix of a continuous-time Markov chain (CTMC). LEAPS can be seen as a continuous-time formulation of annealed importance sampling and sequential Monte Carlo methods, extended so that the variance of the importance weights is offset by the inclusion of the CTMC. To derive these importance weights, we introduce a set of Radon-Nikodym derivatives of CTMCs over their path measures. Because the computation of these weights is intractable with standard neural network parameterizations of rate matrices, we devise a new compact representation for rate matrices via what we call locally equivariant functions. To parameterize them, we introduce a family of locally equivariant multilayer perceptrons, attention layers, and convolutional networks, and provide an approach to make deep networks that preserve the local equivariance. This property allows us to propose a scalable training algorithm for the rate matrix such that the variance of the importance weights associated to the CTMC are minimal. We demonstrate the efficacy of LEAPS on problems in statistical physics.

### The Vendiscope: An Algorithmic Microscope For Data Collections 
[[arxiv](https://arxiv.org/abs/2502.10828)] [[cool](https://papers.cool/arxiv/2502.10828)] [[pdf](https://arxiv.org/pdf/2502.10828)]
> **Authors**: Amey P. Pasarkar,Adji Bousso Dieng
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: This paper introduces the concept of "algorithmic microscopes" and proposes the Vendiscope, an algorithmic microscope for data-driven science
- **标题**: None
- **领域**: 机器学习,材料科学,人工智能,定量方法
- **Abstract**: The evolution of microscopy, beginning with its invention in the late 16th century, has continuously enhanced our ability to explore and understand the microscopic world, enabling increasingly detailed observations of structures and phenomena. In parallel, the rise of data-driven science has underscored the need for sophisticated methods to explore and understand the composition of complex data collections. This paper introduces the Vendiscope, the first algorithmic microscope designed to extend traditional microscopy to computational analysis. The Vendiscope leverages the Vendi scores -- a family of differentiable diversity metrics rooted in ecology and quantum mechanics -- and assigns weights to data points based on their contribution to the overall diversity of the collection. These weights enable high-resolution data analysis at scale. We demonstrate this across biology, materials science, and machine learning (ML). We analyzed the $250$ million protein sequences in the protein universe, discovering that over $200$ million are near-duplicates and that AlphaFold fails on proteins with Gene Ontology (GO) functions that contribute most to diversity. Applying the Vendiscope to the Materials Project database led to similar findings: more than $85\%$ of the crystals with formation energy data are near-duplicates and ML models perform poorly on materials that enhance diversity. Additionally, the Vendiscope can be used to study phenomena such as memorization in generative models. We used the Vendiscope to identify memorized training samples from $13$ different generative models and found that the best-performing ones often memorize the training samples that contribute least to diversity. Our findings demonstrate that the Vendiscope can serve as a powerful tool for data-driven science.

### Improved Offline Contextual Bandits with Second-Order Bounds: Betting and Freezing 
[[arxiv](https://arxiv.org/abs/2502.10826)] [[cool](https://papers.cool/arxiv/2502.10826)] [[pdf](https://arxiv.org/pdf/2502.10826)]
> **Authors**: J. Jon Ryu,Jeongyeol Kwon,Benjamin Koppe,Kwang-Sung Jun
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: 36 pages, 8 figures
- **标题**: None
- **领域**: 机器学习,信息论,机器学习
- **Abstract**: We consider the off-policy selection and learning in contextual bandits where the learner aims to select or train a reward-maximizing policy using data collected by a fixed behavior policy. Our contribution is two-fold. First, we propose a novel off-policy selection method that leverages a new betting-based confidence bound applied to an inverse propensity weight sequence. Our theoretical analysis reveals that our method achieves a significantly better, variance-adaptive guarantee upon prior art. Second, we propose a novel and generic condition on the optimization objective for off-policy learning that strikes a difference balance in bias and variance. One special case that we call freezing tends to induce small variance, which is preferred in small-data regimes. Our analysis shows that they match the best existing guarantee. In our empirical study, our selection method outperforms existing methods, and freezing exhibits improved performance in small-sample regimes.

### On Vanishing Gradients, Over-Smoothing, and Over-Squashing in GNNs: Bridging Recurrent and Graph Learning 
[[arxiv](https://arxiv.org/abs/2502.10818)] [[cool](https://papers.cool/arxiv/2502.10818)] [[pdf](https://arxiv.org/pdf/2502.10818)]
> **Authors**: Álvaro Arroyo,Alessio Gravina,Benjamin Gutteridge,Federico Barbero,Claudio Gallicchio,Xiaowen Dong,Michael Bronstein,Pierre Vandergheynst
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Graph Neural Networks (GNNs) are models that leverage the graph structure to transmit information between nodes, typically through the message-passing operation. While widely successful, this approach is well known to suffer from the over-smoothing and over-squashing phenomena, which result in representational collapse as the number of layers increases and insensitivity to the information contained at distant and poorly connected nodes, respectively. In this paper, we present a unified view of these problems through the lens of vanishing gradients, using ideas from linear control theory for our analysis. We propose an interpretation of GNNs as recurrent models and empirically demonstrate that a simple state-space formulation of a GNN effectively alleviates over-smoothing and over-squashing at no extra trainable parameter cost. Further, we show theoretically and empirically that (i) GNNs are by design prone to extreme gradient vanishing even after a few layers; (ii) Over-smoothing is directly related to the mechanism causing vanishing gradients; (iii) Over-squashing is most easily alleviated by a combination of graph rewiring and vanishing gradient mitigation. We believe our work will help bridge the gap between the recurrent and graph neural network literature and will unlock the design of new deep and performant GNNs.

### BalanceBenchmark: A Survey for Multimodal Imbalance Learning 
[[arxiv](https://arxiv.org/abs/2502.10816)] [[cool](https://papers.cool/arxiv/2502.10816)] [[pdf](https://arxiv.org/pdf/2502.10816)]
> **Authors**: Shaoxuan Xu,Menglu Cui,Chengxiang Huang,Hongfa Wang,Di Hu
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: 9 pages, 3 figures
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Multimodal learning has gained attention for its capacity to integrate information from different modalities. However, it is often hindered by the multimodal imbalance problem, where certain modality dominates while others remain underutilized. Although recent studies have proposed various methods to alleviate this problem, they lack comprehensive and fair comparisons. In this paper, we systematically categorize various mainstream multimodal imbalance algorithms into four groups based on the strategies they employ to mitigate imbalance. To facilitate a comprehensive evaluation of these methods, we introduce BalanceBenchmark, a benchmark including multiple widely used multidimensional datasets and evaluation metrics from three perspectives: performance, imbalance degree, and complexity. To ensure fair comparisons, we have developed a modular and extensible toolkit that standardizes the experimental workflow across different methods. Based on the experiments using BalanceBenchmark, we have identified several key insights into the characteristics and advantages of different method groups in terms of performance, balance degree and computational complexity. We expect such analysis could inspire more efficient approaches to address the imbalance problem in the future, as well as foundation models. The code of the toolkit is available at https://github.com/GeWu-Lab/BalanceBenchmark.

### HybriDNA: A Hybrid Transformer-Mamba2 Long-Range DNA Language Model 
[[arxiv](https://arxiv.org/abs/2502.10807)] [[cool](https://papers.cool/arxiv/2502.10807)] [[pdf](https://arxiv.org/pdf/2502.10807)]
> **Authors**: Mingqian Ma,Guoqing Liu,Chuan Cao,Pan Deng,Tri Dao,Albert Gu,Peiran Jin,Zhao Yang,Yingce Xia,Renqian Luo,Pipi Hu,Zun Wang,Yuan-Jyue Chen,Haiguang Liu,Tao Qin
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: Project page: https://hybridna-project.github.io/HybriDNA-Project/
- **标题**: None
- **领域**: 机器学习,人工智能,基因组学
- **Abstract**: Advances in natural language processing and large language models have sparked growing interest in modeling DNA, often referred to as the "language of life". However, DNA modeling poses unique challenges. First, it requires the ability to process ultra-long DNA sequences while preserving single-nucleotide resolution, as individual nucleotides play a critical role in DNA function. Second, success in this domain requires excelling at both generative and understanding tasks: generative tasks hold potential for therapeutic and industrial applications, while understanding tasks provide crucial insights into biological mechanisms and diseases. To address these challenges, we propose HybriDNA, a decoder-only DNA language model that incorporates a hybrid Transformer-Mamba2 architecture, seamlessly integrating the strengths of attention mechanisms with selective state-space models. This hybrid design enables HybriDNA to efficiently process DNA sequences up to 131kb in length with single-nucleotide resolution. HybriDNA achieves state-of-the-art performance across 33 DNA understanding datasets curated from the BEND, GUE, and LRB benchmarks, and demonstrates exceptional capability in generating synthetic cis-regulatory elements (CREs) with desired properties. Furthermore, we show that HybriDNA adheres to expected scaling laws, with performance improving consistently as the model scales from 300M to 3B and 7B parameters. These findings underscore HybriDNA's versatility and its potential to advance DNA research and applications, paving the way for innovations in understanding and engineering the "language of life".

### Tackling the Zero-Shot Reinforcement Learning Loss Directly 
[[arxiv](https://arxiv.org/abs/2502.10792)] [[cool](https://papers.cool/arxiv/2502.10792)] [[pdf](https://arxiv.org/pdf/2502.10792)]
> **Authors**: Yann Ollivier
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Zero-shot reinforcement learning (RL) methods aim at instantly producing a behavior for an RL task in a given environment, from a description of the reward function. These methods are usually tested by evaluating their average performance on a series of downstream tasks. Yet they cannot be trained directly for that objective, unless the distribution of downstream tasks is known. Existing approaches either use other learning criteria [BBQ+ 18, TRO23, TO21, HDB+ 19], or explicitly set a prior on downstream tasks, such as reward functions given by a random neural network [FPAL24]. Here we prove that the zero-shot RL loss can be optimized directly, for a range of non-informative priors such as white noise rewards, temporally smooth rewards, ``scattered'' sparse rewards, or a combination of those. Thus, it is possible to learn the optimal zero-shot features algorithmically, for a wide mixture of priors. Surprisingly, the white noise prior leads to an objective almost identical to the one in VISR [HDB+19], via a different approach. This shows that some seemingly arbitrary choices in VISR, such as Von Mises--Fisher distributions, do maximize downstream performance. This also suggests more efficient ways to tackle the VISR objective. Finally, we discuss some consequences and limitations of the zero-shot RL objective, such as its tendency to produce narrow optimal features if only using Gaussian dense reward priors.

### Which Features are Best for Successor Features? 
[[arxiv](https://arxiv.org/abs/2502.10790)] [[cool](https://papers.cool/arxiv/2502.10790)] [[pdf](https://arxiv.org/pdf/2502.10790)]
> **Authors**: Yann Ollivier
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,优化与控制,机器学习
- **Abstract**: In reinforcement learning, universal successor features (SFs) are a way to provide zero-shot adaptation to new tasks at test time: they provide optimal policies for all downstream reward functions lying in the linear span of a set of base features. But it is unclear what constitutes a good set of base features, that could be useful for a wide set of downstream tasks beyond their linear span. Laplacian eigenfunctions (the eigenfunctions of $Δ+Δ^\ast$ with $Δ$ the Laplacian operator of some reference policy and $Δ^\ast$ that of the time-reversed dynamics) have been argued to play a role, and offer good empirical performance. Here, for the first time, we identify the optimal base features based on an objective criterion of downstream performance, in a non-tautological way without assuming the downstream tasks are linear in the features. We do this for three generic classes of downstream tasks: reaching a random goal state, dense random Gaussian rewards, and random ``scattered'' sparse rewards. The features yielding optimal expected downstream performance turn out to be the \emph{same} for these three task families. They do not coincide with Laplacian eigenfunctions in general, though they can be expressed from $Δ$: in the simplest case (deterministic environment and decay factor $γ$ close to $1$), they are the eigenfunctions of $Δ^{-1}+(Δ^{-1})^\ast$. We obtain these results under an assumption of large behavior cloning regularization with respect to a reference policy, a setting often used for offline RL. Along the way, we get new insights into KL-regularized\option{natural} policy gradient, and into the lack of SF information in the norm of Bellman gaps.

### ReReLRP -- Remembering and Recognizing Tasks with LRP 
[[arxiv](https://arxiv.org/abs/2502.10789)] [[cool](https://papers.cool/arxiv/2502.10789)] [[pdf](https://arxiv.org/pdf/2502.10789)]
> **Authors**: Karolina Bogacka,Maximilian Höfler,Maria Ganzha,Wojciech Samek,Katarzyna Wasielewska-Michniewska
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: 16 pages, 14 figures, submitted to the Conference on Uncertainty in Artificial Intelligence (uai2025)
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Deep neural networks have revolutionized numerous research fields and applications. Despite their widespread success, a fundamental limitation known as catastrophic forgetting remains, where models fail to retain their ability to perform previously learned tasks after being trained on new ones. This limitation is particularly acute in certain continual learning scenarios, where models must integrate the knowledge from new domains with their existing capabilities. Traditional approaches to mitigate this problem typically rely on memory replay mechanisms, storing either original data samples, prototypes, or activation patterns. Although effective, these methods often introduce significant computational overhead, raise privacy concerns, and require the use of dedicated architectures. In this work we present ReReLRP (Remembering and Recognizing with LRP), a novel solution that leverages Layerwise Relevance Propagation (LRP) to preserve information across tasks. Our contribution provides increased privacy of existing replay-free methods while additionally offering built-in explainability, flexibility of model architecture and deployment, and a new mechanism to increase memory storage efficiency. We validate our approach on a wide variety of datasets, demonstrating results comparable with a well-known replay-based method in selected scenarios.

### Epidemic-guided deep learning for spatiotemporal forecasting of Tuberculosis outbreak 
[[arxiv](https://arxiv.org/abs/2502.10786)] [[cool](https://papers.cool/arxiv/2502.10786)] [[pdf](https://arxiv.org/pdf/2502.10786)]
> **Authors**: Madhab Barman,Madhurima Panja,Nachiketa Mishra,Tanujit Chakraborty
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,定量方法,机器学习
- **Abstract**: Tuberculosis (TB) remains a formidable global health challenge, driven by complex spatiotemporal transmission dynamics and influenced by factors such as population mobility and behavioral changes. We propose an Epidemic-Guided Deep Learning (EGDL) approach that fuses mechanistic epidemiological principles with advanced deep learning techniques to enhance early warning systems and intervention strategies for TB outbreaks. Our framework is built upon a networked Susceptible-Infectious-Recovered (SIR) model augmented with a saturated incidence rate and graph Laplacian diffusion, capturing both long-term transmission dynamics and region-specific population mobility patterns. Compartmental model parameters are rigorously estimated using Bayesian inference via the Markov Chain Monte Carlo (MCMC) approach. Theoretical analysis leveraging the comparison principle and Green's formula establishes global stability properties of the disease-free and endemic equilibria. Building on these epidemiological insights, we design two forecasting architectures, EGDL-Parallel and EGDL-Series, that integrate the mechanistic outputs of the networked SIR model within deep neural networks. This integration mitigates the overfitting risks commonly encountered in data-driven methods and filters out noise inherent in surveillance data, resulting in reliable forecasts of real-world epidemic trends. Experiments conducted on TB incidence data from 47 prefectures in Japan demonstrate that our approach delivers robust and accurate predictions across multiple time horizons (short to medium-term forecasts). Additionally, incorporating uncertainty quantification through conformal prediction enhances the model's practical utility for guiding targeted public health interventions.

### Preconditioned Inexact Stochastic ADMM for Deep Model 
[[arxiv](https://arxiv.org/abs/2502.10784)] [[cool](https://papers.cool/arxiv/2502.10784)] [[pdf](https://arxiv.org/pdf/2502.10784)]
> **Authors**: Shenglong Zhou,Ouya Wang,Ziyan Luo,Yongxu Zhu,Geoffrey Ye Li
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The recent advancement of foundation models (FMs) has brought about a paradigm shift, revolutionizing various sectors worldwide. The popular optimizers used to train these models are stochastic gradient descent-based algorithms, which face inherent limitations, such as slow convergence and stringent assumptions for convergence. In particular, data heterogeneity arising from distributed settings poses significant challenges to their theoretical and numerical performance. This paper develops an algorithm, PISA ({P}reconditioned {I}nexact {S}tochastic {A}lternating Direction Method of Multipliers), which enables scalable parallel computing and supports various second-moment schemes. Grounded in rigorous theoretical guarantees, the algorithm converges under the sole assumption of Lipschitz continuity of the gradient, thereby removing the need for other conditions commonly imposed by stochastic methods. This capability enables PISA to tackle the challenge of data heterogeneity effectively. Comprehensive experimental evaluations for training or fine-tuning diverse FMs, including vision models, large language models, reinforcement learning models, generative adversarial networks, and recurrent neural networks, demonstrate its superior numerical performance compared to various state-of-the-art optimizers.

### A Distillation-based Future-aware Graph Neural Network for Stock Trend Prediction 
[[arxiv](https://arxiv.org/abs/2502.10776)] [[cool](https://papers.cool/arxiv/2502.10776)] [[pdf](https://arxiv.org/pdf/2502.10776)]
> **Authors**: Zhipeng Liu,Peibo Duan,Mingyang Geng,Bin Zhang
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,投资组合管理
- **Abstract**: Stock trend prediction involves forecasting the future price movements by analyzing historical data and various market indicators. With the advancement of machine learning, graph neural networks (GNNs) have been extensively employed in stock prediction due to their powerful capability to capture spatiotemporal dependencies of stocks. However, despite the efforts of various GNN stock predictors to enhance predictive performance, the improvements remain limited, as they focus solely on analyzing historical spatiotemporal dependencies, overlooking the correlation between historical and future patterns. In this study, we propose a novel distillation-based future-aware GNN framework (DishFT-GNN) for stock trend prediction. Specifically, DishFT-GNN trains a teacher model and a student model, iteratively. The teacher model learns to capture the correlation between distribution shifts of historical and future data, which is then utilized as intermediate supervision to guide the student model to learn future-aware spatiotemporal embeddings for accurate prediction. Through extensive experiments on two real-world datasets, we verify the state-of-the-art performance of DishFT-GNN.

### Learning to Explain Air Traffic Situation 
[[arxiv](https://arxiv.org/abs/2502.10764)] [[cool](https://papers.cool/arxiv/2502.10764)] [[pdf](https://arxiv.org/pdf/2502.10764)]
> **Authors**: Hong-ah Chai,Seokbin Yoon,Keumjin Lee
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: 4 pages, 3 figures, submitted to First US-Europe Air Transportation Research and Development Symposium (ATRD)
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Understanding how air traffic controllers construct a mental 'picture' of complex air traffic situations is crucial but remains a challenge due to the inherently intricate, high-dimensional interactions between aircraft, pilots, and controllers. Previous work on modeling the strategies of air traffic controllers and their mental image of traffic situations often centers on specific air traffic control tasks or pairwise interactions between aircraft, neglecting to capture the comprehensive dynamics of an air traffic situation. To address this issue, we propose a machine learning-based framework for explaining air traffic situations. Specifically, we employ a Transformer-based multi-agent trajectory model that encapsulates both the spatio-temporal movement of aircraft and social interaction between them. By deriving attention scores from the model, we can quantify the influence of individual aircraft on overall traffic dynamics. This provides explainable insights into how air traffic controllers perceive and understand the traffic situation. Trained on real-world air traffic surveillance data collected from the terminal airspace around Incheon International Airport in South Korea, our framework effectively explicates air traffic situations. This could potentially support and enhance the decision-making and situational awareness of air traffic controllers.

### Bone Soups: A Seek-and-Soup Model Merging Approach for Controllable Multi-Objective Generation 
[[arxiv](https://arxiv.org/abs/2502.10762)] [[cool](https://papers.cool/arxiv/2502.10762)] [[pdf](https://arxiv.org/pdf/2502.10762)]
> **Authors**: Guofu Xie,Xiao Zhang,Ting Yao,Yunsheng Shi
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: work in progress
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: User information needs are often highly diverse and varied. A key challenge in current research is how to achieve controllable multi-objective generation while enabling rapid adaptation to accommodate diverse user demands during test time. Existing solutions, such as Rewarded Soup, focus on merging language models individually tuned on single objectives. While easy to implement and widely used, these approaches face limitations in achieving optimal performance due to their disregard for the impacts of competing objectives on model tuning. To address this issue, we propose Bone Soup, a novel model merging approach that first seeks a series of backbone models by considering the impacts of multiple objectives and then makes the soup (i.e., merge the backbone models). Specifically, Bone Soup begins by training multiple backbone models for different objectives using multi-objective reinforcement learning. Each backbone model is guided by a combination of backbone reward signals. To ensure that these models are optimal for the Pareto front, the backbone rewards are crafted by combining standard reward functions into basis vectors, which can then be modified through a rule-based construction method. Bone Soup leverages a symmetric circulant matrix mapping to generate the merging coefficients, which are used to merge the backbone models according to user preferences. Extensive experimental results demonstrate that Bone Soup exhibits strong controllability and Pareto optimality in controllable multi-objective generation, providing a more effective and efficient approach to addressing diverse user needs at test time.

### Rule-Bottleneck Reinforcement Learning: Joint Explanation and Decision Optimization for Resource Allocation with Language Agents 
[[arxiv](https://arxiv.org/abs/2502.10732)] [[cool](https://papers.cool/arxiv/2502.10732)] [[pdf](https://arxiv.org/pdf/2502.10732)]
> **Authors**: Mauricio Tec,Guojun Xiong,Haichuan Wang,Francesca Dominici,Milind Tambe
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Deep Reinforcement Learning (RL) is remarkably effective in addressing sequential resource allocation problems in domains such as healthcare, public policy, and resource management. However, deep RL policies often lack transparency and adaptability, challenging their deployment alongside human decision-makers. In contrast, Language Agents, powered by large language models (LLMs), provide human-understandable reasoning but may struggle with effective decision making. To bridge this gap, we propose Rule-Bottleneck Reinforcement Learning (RBRL), a novel framework that jointly optimizes decision and explanations. At each step, RBRL generates candidate rules with an LLM, selects among them using an attention-based RL policy, and determines the environment action with an explanation via chain-of-thought reasoning. The RL rule selection is optimized using the environment rewards and an explainability metric judged by the LLM. Evaluations in real-world scenarios highlight RBRL's competitive performance with deep RL and efficiency gains over LLM fine-tuning. A survey further confirms the enhanced quality of its explanations.

### A Mathematics Framework of Artificial Shifted Population Risk and Its Further Understanding Related to Consistency Regularization 
[[arxiv](https://arxiv.org/abs/2502.10723)] [[cool](https://papers.cool/arxiv/2502.10723)] [[pdf](https://arxiv.org/pdf/2502.10723)]
> **Authors**: Xiliang Yang,Shenyang Deng,Shicong Liu,Yuanchi Suo,Wing. W. Y NG,Jianjun Zhang
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Data augmentation is an important technique in training deep neural networks as it enhances their ability to generalize and remain robust. While data augmentation is commonly used to expand the sample size and act as a consistency regularization term, there is a lack of research on the relationship between them. To address this gap, this paper introduces a more comprehensive mathematical framework for data augmentation. Through this framework, we establish that the expected risk of the shifted population is the sum of the original population risk and a gap term, which can be interpreted as a consistency regularization term. The paper also provides a theoretical understanding of this gap, highlighting its negative effects on the early stages of training. We also propose a method to mitigate these effects. To validate our approach, we conducted experiments using same data augmentation techniques and computing resources under several scenarios, including standard training, out-of-distribution, and imbalanced classification. The results demonstrate that our methods surpass compared methods under all scenarios in terms of generalization ability and convergence stability. We provide our code implementation at the following link: https://github.com/ydlsfhll/ASPR.

### A Comprehensive Survey of Deep Learning for Multivariate Time Series Forecasting: A Channel Strategy Perspective 
[[arxiv](https://arxiv.org/abs/2502.10721)] [[cool](https://papers.cool/arxiv/2502.10721)] [[pdf](https://arxiv.org/pdf/2502.10721)]
> **Authors**: Xiangfei Qiu,Hanyin Cheng,Xingjian Wu,Jilin Hu,Chenjuan Guo
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Multivariate Time Series Forecasting (MTSF) plays a crucial role across diverse fields, ranging from economic, energy, to traffic. In recent years, deep learning has demonstrated outstanding performance in MTSF tasks. In MTSF, modeling the correlations among different channels is critical, as leveraging information from other related channels can significantly improve the prediction accuracy of a specific channel. This study systematically reviews the channel modeling strategies for time series and proposes a taxonomy organized into three hierarchical levels: the strategy perspective, the mechanism perspective, and the characteristic perspective. On this basis, we provide a structured analysis of these methods and conduct an in-depth examination of the advantages and limitations of different channel strategies. Finally, we summarize and discuss some future research directions to provide useful research guidance. Moreover, we maintain an up-to-date Github repository (https://github.com/decisionintelligence/CS4TS) which includes all the papers discussed in the survey.

### Why Domain Generalization Fail? A View of Necessity and Sufficiency 
[[arxiv](https://arxiv.org/abs/2502.10716)] [[cool](https://papers.cool/arxiv/2502.10716)] [[pdf](https://arxiv.org/pdf/2502.10716)]
> **Authors**: Long-Tung Vuong,Vy Vo,Hien Dang,Van-Anh Nguyen,Thanh-Toan Do,Mehrtash Harandi,Trung Le,Dinh Phung
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Despite a strong theoretical foundation, empirical experiments reveal that existing domain generalization (DG) algorithms often fail to consistently outperform the ERM baseline. We argue that this issue arises because most DG studies focus on establishing theoretical guarantees for generalization under unrealistic assumptions, such as the availability of sufficient, diverse (or even infinite) domains or access to target domain knowledge. As a result, the extent to which domain generalization is achievable in scenarios with limited domains remains largely unexplored. This paper seeks to address this gap by examining generalization through the lens of the conditions necessary for its existence and learnability. Specifically, we systematically establish a set of necessary and sufficient conditions for generalization. Our analysis highlights that existing DG methods primarily act as regularization mechanisms focused on satisfying sufficient conditions, while often neglecting necessary ones. However, sufficient conditions cannot be verified in settings with limited training domains. In such cases, regularization targeting sufficient conditions aims to maximize the likelihood of generalization, whereas regularization targeting necessary conditions ensures its existence. Using this analysis, we reveal the shortcomings of existing DG algorithms by showing that, while they promote sufficient conditions, they inadvertently violate necessary conditions. To validate our theoretical insights, we propose a practical method that promotes the sufficient condition while maintaining the necessary conditions through a novel subspace representation alignment strategy. This approach highlights the advantages of preserving the necessary conditions on well-established DG benchmarks.

### FuncGenFoil: Airfoil Generation and Editing Model in Function Space 
[[arxiv](https://arxiv.org/abs/2502.10712)] [[cool](https://papers.cool/arxiv/2502.10712)] [[pdf](https://arxiv.org/pdf/2502.10712)]
> **Authors**: Jinouwen Zhang,Junjie Ren,Aobo Yang,Yan Lu,Lu Chen,Hairun Xie,Jing Wang,Miao Zhang,Wanli Ouyang,Shixiang Tang
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Aircraft manufacturing is the jewel in the crown of industry, among which generating high-fidelity airfoil geometries with controllable and editable representations remains a fundamental challenge. While existing deep-learning-based methods rely on predefined parametric function families, e.g., Bézier curves and discrete point-based representations, they suffer from inherent trade-offs between expressiveness and resolution flexibility. To tackle this challenge, we introduce FuncGenFoil, a novel function-space generative model that directly learns functional airfoil geometries. Our method inherits both the advantages of arbitrary resolution sampling and the smoothness of parametric functions, as well as the strong expressiveness of discrete point-based functions. Empirical evaluations on the AFBench dataset demonstrate that FuncGenFoil improves upon state-of-the-art methods in airfoil generation by achieving a relative -74.4 label error reduction and +23.2 diversity increase on the AF-200K dataset. Our results highlight the advantages of function-space modeling for aerodynamic shape optimization, offering a powerful and flexible framework for high-fidelity airfoil design. Our code will be released.

### Reading Your Heart: Learning ECG Words and Sentences via Pre-training ECG Language Model 
[[arxiv](https://arxiv.org/abs/2502.10707)] [[cool](https://papers.cool/arxiv/2502.10707)] [[pdf](https://arxiv.org/pdf/2502.10707)]
> **Authors**: Jiarui Jin,Haoyu Wang,Hongyan Li,Jun Li,Jiahui Pan,Shenda Hong
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: 21 pages, 8 figures, accepted by International Conference onLearningRepresentations 2025
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Electrocardiogram (ECG) is essential for the clinical diagnosis of arrhythmias and other heart diseases, but deep learning methods based on ECG often face limitations due to the need for high-quality annotations. Although previous ECG self-supervised learning (eSSL) methods have made significant progress in representation learning from unannotated ECG data, they typically treat ECG signals as ordinary time-series data, segmenting the signals using fixed-size and fixed-step time windows, which often ignore the form and rhythm characteristics and latent semantic relationships in ECG signals. In this work, we introduce a novel perspective on ECG signals, treating heartbeats as words and rhythms as sentences. Based on this perspective, we first designed the QRS-Tokenizer, which generates semantically meaningful ECG sentences from the raw ECG signals. Building on these, we then propose HeartLang, a novel self-supervised learning framework for ECG language processing, learning general representations at form and rhythm levels. Additionally, we construct the largest heartbeat-based ECG vocabulary to date, which will further advance the development of ECG language processing. We evaluated HeartLang across six public ECG datasets, where it demonstrated robust competitiveness against other eSSL methods. Our data and code are publicly available at https://github.com/PKUDigitalHealth/HeartLang.

### Raising the Bar in Graph OOD Generalization: Invariant Learning Beyond Explicit Environment Modeling 
[[arxiv](https://arxiv.org/abs/2502.10706)] [[cool](https://papers.cool/arxiv/2502.10706)] [[pdf](https://arxiv.org/pdf/2502.10706)]
> **Authors**: Xu Shen,Yixin Liu,Yili Wang,Rui Miao,Yiwei Dai,Shirui Pan,Xin Wang
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Out-of-distribution (OOD) generalization has emerged as a critical challenge in graph learning, as real-world graph data often exhibit diverse and shifting environments that traditional models fail to generalize across. A promising solution to address this issue is graph invariant learning (GIL), which aims to learn invariant representations by disentangling label-correlated invariant subgraphs from environment-specific subgraphs. However, existing GIL methods face two major challenges: (1) the difficulty of capturing and modeling diverse environments in graph data, and (2) the semantic cliff, where invariant subgraphs from different classes are difficult to distinguish, leading to poor class separability and increased misclassifications. To tackle these challenges, we propose a novel method termed Multi-Prototype Hyperspherical Invariant Learning (MPHIL), which introduces two key innovations: (1) hyperspherical invariant representation extraction, enabling robust and highly discriminative hyperspherical invariant feature extraction, and (2) multi-prototype hyperspherical classification, which employs class prototypes as intermediate variables to eliminate the need for explicit environment modeling in GIL and mitigate the semantic cliff issue. Derived from the theoretical framework of GIL, we introduce two novel objective functions: the invariant prototype matching loss to ensure samples are matched to the correct class prototypes, and the prototype separation loss to increase the distinction between prototypes of different classes in the hyperspherical space. Extensive experiments on 11 OOD generalization benchmark datasets demonstrate that MPHIL achieves state-of-the-art performance, significantly outperforming existing methods across graph data from various domains and with different distribution shifts.

### Artificial intelligence-enabled detection and assessment of Parkinson's disease using multimodal data: A survey 
[[arxiv](https://arxiv.org/abs/2502.10703)] [[cool](https://papers.cool/arxiv/2502.10703)] [[pdf](https://arxiv.org/pdf/2502.10703)]
> **Authors**: Aite Zhao,Yongcan Liu,Xinglin Yu,Xinyue Xing
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,声音
- **Abstract**: The rapid emergence of highly adaptable and reusable artificial intelligence (AI) models is set to revolutionize the medical field, particularly in the diagnosis and management of Parkinson's disease (PD). Currently, there are no effective biomarkers for diagnosing PD, assessing its severity, or tracking its progression. Numerous AI algorithms are now being used for PD diagnosis and treatment, capable of performing various classification tasks based on multimodal and heterogeneous disease symptom data, such as gait, hand movements, and speech patterns of PD patients. They provide expressive feedback, including predicting the potential likelihood of PD, assessing the severity of individual or multiple symptoms, aiding in early detection, and evaluating rehabilitation and treatment effectiveness, thereby demonstrating advanced medical diagnostic capabilities. Therefore, this work provides a surveyed compilation of recent works regarding PD detection and assessment through biometric symptom recognition with a focus on machine learning and deep learning approaches, emphasizing their benefits, and exposing their weaknesses, and their impact in opening up newer research avenues. Additionally, it also presents categorized and characterized descriptions of the datasets, approaches, and architectures employed to tackle associated constraints. Furthermore, the paper explores the potential opportunities and challenges presented by data-driven AI technologies in the diagnosis of PD.

### Superpose Singular Features for Model Merging 
[[arxiv](https://arxiv.org/abs/2502.10698)] [[cool](https://papers.cool/arxiv/2502.10698)] [[pdf](https://arxiv.org/pdf/2502.10698)]
> **Authors**: Haiquan Qiu,You Wu,Quanming Yao
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: 14 pages, 1 figures
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Model merging is a critical technique for combining the capabilities of multiple fine-tuned models without requiring additional training. While existing methods treat parameters as vectors, they overlook the intrinsic structure of linear transformation matrices - the core components that comprise the majority of model parameters. These matrices are fundamental to neural networks, mapping input representations to output features through linear combinations. Motivated by the linear representation hypothesis, we introduce task matrix and propose to Superpose Features from Task Matrix (SFTM), a novel approach that superposes features from individual task models into a merged model. SFTM employs singular value decomposition to identify feature bases of linear transformation matrices and solves a linear system to optimally combine them while preserving input-output mappings from individual task models. Extensive experiments on vision transformers and language models demonstrate that our method consistently outperforms existing methods, achieving superior performance and enhanced out-of-distribution generalization.

### Simulations of Common Unsupervised Domain Adaptation Algorithms for Image Classification 
[[arxiv](https://arxiv.org/abs/2502.10694)] [[cool](https://papers.cool/arxiv/2502.10694)] [[pdf](https://arxiv.org/pdf/2502.10694)]
> **Authors**: Ahmad Chaddad,Yihang Wu,Yuchen Jiang,Ahmed Bouridane,Christian Desrosiers
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: Accepted in IEEE TIM
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Traditional machine learning assumes that training and test sets are derived from the same distribution; however, this assumption does not always hold in practical applications. This distribution disparity can lead to severe performance drops when the trained model is used in new data sets. Domain adaptation (DA) is a machine learning technique that aims to address this problem by reducing the differences between domains. This paper presents simulation-based algorithms of recent DA techniques, mainly related to unsupervised domain adaptation (UDA), where labels are available only in the source domain. Our study compares these techniques with public data sets and diverse characteristics, highlighting their respective strengths and drawbacks. For example, Safe Self-Refinement for Transformer-based DA (SSRT) achieved the highest accuracy (91.6\%) in the office-31 data set during our simulations, however, the accuracy dropped to 72.4\% in the Office-Home data set when using limited batch sizes. In addition to improving the reader's comprehension of recent techniques in DA, our study also highlights challenges and upcoming directions for research in this domain. The codes are available at https://github.com/AIPMLab/Domain_Adaptation.

### Controlling Neural Collapse Enhances Out-of-Distribution Detection and Transfer Learning 
[[arxiv](https://arxiv.org/abs/2502.10691)] [[cool](https://papers.cool/arxiv/2502.10691)] [[pdf](https://arxiv.org/pdf/2502.10691)]
> **Authors**: Md Yousuf Harun,Jhair Gallardo,Christopher Kanan
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Out-of-distribution (OOD) detection and OOD generalization are widely studied in Deep Neural Networks (DNNs), yet their relationship remains poorly understood. We empirically show that the degree of Neural Collapse (NC) in a network layer is inversely related with these objectives: stronger NC improves OOD detection but degrades generalization, while weaker NC enhances generalization at the cost of detection. This trade-off suggests that a single feature space cannot simultaneously achieve both tasks. To address this, we develop a theoretical framework linking NC to OOD detection and generalization. We show that entropy regularization mitigates NC to improve generalization, while a fixed Simplex Equiangular Tight Frame (ETF) projector enforces NC for better detection. Based on these insights, we propose a method to control NC at different DNN layers. In experiments, our method excels at both tasks across OOD datasets and DNN architectures.

### Self-Explaining Hypergraph Neural Networks for Diagnosis Prediction 
[[arxiv](https://arxiv.org/abs/2502.10689)] [[cool](https://papers.cool/arxiv/2502.10689)] [[pdf](https://arxiv.org/pdf/2502.10689)]
> **Authors**: Leisheng Yu,Yanxiao Cai,Minxing Zhang,Xia Hu
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: The burgeoning volume of electronic health records (EHRs) has enabled deep learning models to excel in predictive healthcare. However, for high-stakes applications such as diagnosis prediction, model interpretability remains paramount. Existing deep learning diagnosis prediction models with intrinsic interpretability often assign attention weights to every past diagnosis or hospital visit, providing explanations lacking flexibility and succinctness. In this paper, we introduce SHy, a self-explaining hypergraph neural network model, designed to offer personalized, concise and faithful explanations that allow for interventions from clinical experts. By modeling each patient as a unique hypergraph and employing a message-passing mechanism, SHy captures higher-order disease interactions and extracts distinct temporal phenotypes as personalized explanations. It also addresses the incompleteness of the EHR data by accounting for essential false negatives in the original diagnosis record. A qualitative case study and extensive quantitative evaluations on two real-world EHR datasets demonstrate the superior predictive performance and interpretability of SHy over existing state-of-the-art models.

### LLM-Lasso: A Robust Framework for Domain-Informed Feature Selection and Regularization 
[[arxiv](https://arxiv.org/abs/2502.10648)] [[cool](https://papers.cool/arxiv/2502.10648)] [[pdf](https://arxiv.org/pdf/2502.10648)]
> **Authors**: Erica Zhang,Ryunosuke Goto,Naomi Sagan,Jurik Mutter,Nick Phillips,Ash Alizadeh,Kangwook Lee,Jose Blanchet,Mert Pilanci,Robert Tibshirani
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 21 pages, 16 figures
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: We introduce LLM-Lasso, a novel framework that leverages large language models (LLMs) to guide feature selection in Lasso $\ell_1$ regression. Unlike traditional methods that rely solely on numerical data, LLM-Lasso incorporates domain-specific knowledge extracted from natural language, enhanced through a retrieval-augmented generation (RAG) pipeline, to seamlessly integrate data-driven modeling with contextual insights. Specifically, the LLM generates penalty factors for each feature, which are converted into weights for the Lasso penalty using a simple, tunable model. Features identified as more relevant by the LLM receive lower penalties, increasing their likelihood of being retained in the final model, while less relevant features are assigned higher penalties, reducing their influence. Importantly, LLM-Lasso has an internal validation step that determines how much to trust the contextual knowledge in our prediction pipeline. Hence it addresses key challenges in robustness, making it suitable for mitigating potential inaccuracies or hallucinations from the LLM. In various biomedical case studies, LLM-Lasso outperforms standard Lasso and existing feature selection baselines, all while ensuring the LLM operates without prior access to the datasets. To our knowledge, this is the first approach to effectively integrate conventional feature selection techniques directly with LLM-based domain-specific reasoning.

### A Power Transform 
[[arxiv](https://arxiv.org/abs/2502.10647)] [[cool](https://papers.cool/arxiv/2502.10647)] [[pdf](https://arxiv.org/pdf/2502.10647)]
> **Authors**: Jonathan T. Barron
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,统计理论,机器学习
- **Abstract**: Power transforms, such as the Box-Cox transform and Tukey's ladder of powers, are a fundamental tool in mathematics and statistics. These transforms are primarily used for normalizing and standardizing datasets, effectively by raising values to a power. In this work I present a novel power transform, and I show that it serves as a unifying framework for wide family of loss functions, kernel functions, probability distributions, bump functions, and neural network activation functions.

### Privacy Preservation through Practical Machine Unlearning 
[[arxiv](https://arxiv.org/abs/2502.10635)] [[cool](https://papers.cool/arxiv/2502.10635)] [[pdf](https://arxiv.org/pdf/2502.10635)]
> **Authors**: Robert Dilworth
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 15 pages, 8 figures
- **标题**: None
- **领域**: 机器学习,密码学和安全
- **Abstract**: Machine Learning models thrive on vast datasets, continuously adapting to provide accurate predictions and recommendations. However, in an era dominated by privacy concerns, Machine Unlearning emerges as a transformative approach, enabling the selective removal of data from trained models. This paper examines methods such as Naive Retraining and Exact Unlearning via the SISA framework, evaluating their Computational Costs, Consistency, and feasibility using the $\texttt{HSpam14}$ dataset. We explore the potential of integrating unlearning principles into Positive Unlabeled (PU) Learning to address challenges posed by partially labeled datasets. Our findings highlight the promise of unlearning frameworks like $\textit{DaRE}$ for ensuring privacy compliance while maintaining model performance, albeit with significant computational trade-offs. This study underscores the importance of Machine Unlearning in achieving ethical AI and fostering trust in data-driven systems.

### ControllableGPT: A Ground-Up Designed Controllable GPT for Molecule Optimization 
[[arxiv](https://arxiv.org/abs/2502.10631)] [[cool](https://papers.cool/arxiv/2502.10631)] [[pdf](https://arxiv.org/pdf/2502.10631)]
> **Authors**: Xuefeng Liu,Songhao Jiang,Bo Li,Rick Stevens
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,生物分子
- **Abstract**: Large Language Models (LLMs) employ three popular training approaches: Masked Language Models (MLM), Causal Language Models (CLM), and Sequence-to-Sequence Models (seq2seq). However, each approach has its strengths and limitations, and faces challenges in addressing specific tasks that require controllable and bidirectional generation, such as drug optimization. To address this challenge, inspired by the biological processes of growth and evolution, which involve the expansion, shrinking, and mutation of sequences, we introduce ControllableGPT. This initiative represents the first effort to combine the advantages of MLM, CLM, and seq2seq into a single unified, controllable GPT framework. It enables the precise management of specific locations and ranges within a sequence, allowing for expansion, reduction, or mutation over chosen or random lengths, while maintaining the integrity of any specified positions or subsequences. In this work, we designed ControllableGPT for drug optimization from the ground up, which included proposing the Causally Masked Seq2seq (CMS) objective, developing the training corpus, introducing a novel pre-training approach, and devising a unique generation process. We demonstrate the effectiveness and controllability of ControllableGPT by conducting experiments on drug optimization tasks for both viral and cancer benchmarks, surpassing competing baselines.

### On Self-Adaptive Perception Loss Function for Sequential Lossy Compression 
[[arxiv](https://arxiv.org/abs/2502.10628)] [[cool](https://papers.cool/arxiv/2502.10628)] [[pdf](https://arxiv.org/pdf/2502.10628)]
> **Authors**: Sadaf Salehkalaibar,Buu Phan,Likun Cai,Joao Atz Dick,Wei Yu,Jun Chen,Ashish Khisti
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: arXiv admin note: text overlap with arXiv:2305.19301
- **标题**: None
- **领域**: 机器学习,信息论
- **Abstract**: We consider causal, low-latency, sequential lossy compression, with mean squared-error (MSE) as the distortion loss, and a perception loss function (PLF) to enhance the realism of reconstructions. As the main contribution, we propose and analyze a new PLF that considers the joint distribution between the current source frame and the previous reconstructions. We establish the theoretical rate-distortion-perception function for first-order Markov sources and analyze the Gaussian model in detail. From a qualitative perspective, the proposed metric can simultaneously avoid the error-permanence phenomenon and also better exploit the temporal correlation between high-quality reconstructions. The proposed metric is referred to as self-adaptive perception loss function (PLF-SA), as its behavior adapts to the quality of reconstructed frames. We provide a detailed comparison of the proposed perception loss function with previous approaches through both information theoretic analysis as well as experiments involving moving MNIST and UVG datasets.

### K-Edit: Language Model Editing with Contextual Knowledge Awareness 
[[arxiv](https://arxiv.org/abs/2502.10626)] [[cool](https://papers.cool/arxiv/2502.10626)] [[pdf](https://arxiv.org/pdf/2502.10626)]
> **Authors**: Elan Markowitz,Anil Ramakrishna,Ninareh Mehrabi,Charith Peris,Rahul Gupta,Kai-Wei Chang,Aram Galstyan
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: As the world changes, we need to be able to update our models and correct false information without costly retraining. Knowledge-based model editing enables precise modifications to the weights of large language models in order to modify the information encoded within. Recent approaches have seen success in enabling recall of edited information for thousands of edits at once. However, these approaches fail to produce edits that account for associated contextual information. We present K-Edit, an effective approach to generating contextually consistent knowledge edits. By using knowledge graphs, which maintain contextual consistency when an edge is edited, we are able to generate additional \textit{contextual edits} that ensure consistency of related information in the language model. Our experiments demonstrate significant improvements in multi-hop question answering while maintaining the general effectiveness and scalability of model edits.

### Towards Self-Supervised Covariance Estimation in Deep Heteroscedastic Regression 
[[arxiv](https://arxiv.org/abs/2502.10587)] [[cool](https://papers.cool/arxiv/2502.10587)] [[pdf](https://arxiv.org/pdf/2502.10587)]
> **Authors**: Megh Shukla,Aziz Shameem,Mathieu Salzmann,Alexandre Alahi
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: ICLR 2025
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Deep heteroscedastic regression models the mean and covariance of the target distribution through neural networks. The challenge arises from heteroscedasticity, which implies that the covariance is sample dependent and is often unknown. Consequently, recent methods learn the covariance through unsupervised frameworks, which unfortunately yield a trade-off between computational complexity and accuracy. While this trade-off could be alleviated through supervision, obtaining labels for the covariance is non-trivial. Here, we study self-supervised covariance estimation in deep heteroscedastic regression. We address two questions: (1) How should we supervise the covariance assuming ground truth is available? (2) How can we obtain pseudo labels in the absence of the ground-truth? We address (1) by analysing two popular measures: the KL Divergence and the 2-Wasserstein distance. Subsequently, we derive an upper bound on the 2-Wasserstein distance between normal distributions with non-commutative covariances that is stable to optimize. We address (2) through a simple neighborhood based heuristic algorithm which results in surprisingly effective pseudo labels for the covariance. Our experiments over a wide range of synthetic and real datasets demonstrate that the proposed 2-Wasserstein bound coupled with pseudo label annotations results in a computationally cheaper yet accurate deep heteroscedastic regression.

### Do We Need to Verify Step by Step? Rethinking Process Supervision from a Theoretical Perspective 
[[arxiv](https://arxiv.org/abs/2502.10581)] [[cool](https://papers.cool/arxiv/2502.10581)] [[pdf](https://arxiv.org/pdf/2502.10581)]
> **Authors**: Zeyu Jia,Alexander Rakhlin,Tengyang Xie
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: As large language models have evolved, it has become crucial to distinguish between process supervision and outcome supervision -- two key reinforcement learning approaches to complex reasoning tasks. While process supervision offers intuitive advantages for long-term credit assignment, the precise relationship between these paradigms has remained an open question. Conventional wisdom suggests that outcome supervision is fundamentally more challenging due to the trajectory-level coverage problem, leading to significant investment in collecting fine-grained process supervision data. In this paper, we take steps towards resolving this debate. Our main theorem shows that, under standard data coverage assumptions, reinforcement learning through outcome supervision is no more statistically difficult than through process supervision, up to polynomial factors in horizon. At the core of this result lies the novel Change of Trajectory Measure Lemma -- a technical tool that bridges return-based trajectory measure and step-level distribution shift. Furthermore, for settings with access to a verifier or a rollout capability, we prove that any policy's advantage function can serve as an optimal process reward model, providing a direct connection between outcome and process supervision. These findings suggest that the empirically observed performance gap -- if any -- between outcome and process supervision likely stems from algorithmic limitations rather than inherent statistical difficulties, potentially transforming how we approach data collection and algorithm design for reinforcement learning.

### An Innovative Next Activity Prediction Approach Using Process Entropy and DAW-Transformer 
[[arxiv](https://arxiv.org/abs/2502.10573)] [[cool](https://papers.cool/arxiv/2502.10573)] [[pdf](https://arxiv.org/pdf/2502.10573)]
> **Authors**: Hadi Zare,Mostafa Abbasi,Maryam Ahang,Homayoun Najjaran
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Purpose - In Business Process Management (BPM), accurate prediction of the next activities is vital for operational efficiency and decision-making. Current Artificial Intelligence (AI)/Machine Learning (ML) models struggle with the complexity and evolving nature of business process event logs, balancing accuracy and interpretability. This paper proposes an entropy-driven model selection approach and DAW-Transformer, which stands for Dynamic Attribute-Aware Transformer, to integrate all attributes with a dynamic window for better accuracy. Design/methodology/approach - This paper introduces a novel next-activity prediction approach that uses process entropy to assess the complexity of event logs and dynamically select the most suitable ML model. A new transformer-based architecture with multi-head attention and dynamic windowing mechanism, DAW-Transformer, is proposed to capture long-range dependencies and utilize all relevant event log attributes. Experiments were conducted on six public datasets, and the performance was evaluated with process entropy. Finding - The results demonstrate the effectiveness of the approach across these publicly available datasets. DAW-Transformer achieved superior performance, especially on high-entropy datasets such as Sepsis exceeding Limited window Multi-Transformers by 4.69% and a benchmark CNN-LSTM-SAtt model by 3.07%. For low-entropy datasets like Road Traffic Fine, simpler, more interpretable algorithms like Random Forest performed nearly as well as the more complex DAW-Transformer and offered better handling of imbalanced data and improved explainability. Originality/ value - This work's novelty lies in the proposed DAW-Transformer, with a dynamic window and considering all relevant attributes. Also, entropy-driven selection methods offer a robust, accurate, and interpretable solution for next-activity prediction.

### HADL Framework for Noise Resilient Long-Term Time Series Forecasting 
[[arxiv](https://arxiv.org/abs/2502.10569)] [[cool](https://papers.cool/arxiv/2502.10569)] [[pdf](https://arxiv.org/pdf/2502.10569)]
> **Authors**: Aditya Dey,Jonas Kusch,Fadi Al Machot
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Long-term time series forecasting is critical in domains such as finance, economics, and energy, where accurate and reliable predictions over extended horizons drive strategic decision-making. Despite the progress in machine learning-based models, the impact of temporal noise in extended lookback windows remains underexplored, often degrading model performance and computational efficiency. In this paper, we propose a novel framework that addresses these challenges by integrating the Discrete Wavelet Transform (DWT) and Discrete Cosine Transform (DCT) to perform noise reduction and extract robust long-term features. These transformations enable the separation of meaningful temporal patterns from noise in both the time and frequency domains. To complement this, we introduce a lightweight low-rank linear prediction layer that not only reduces the influence of residual noise but also improves memory efficiency. Our approach demonstrates competitive robustness to noisy input, significantly reduces computational complexity, and achieves competitive or state-of-the-art forecasting performance across diverse benchmark datasets. Extensive experiments reveal that the proposed framework is particularly effective in scenarios with high noise levels or irregular patterns, making it well suited for real-world forecasting tasks. The code is available in https://github.com/forgee-master/HADL.

### Efficient Hierarchical Contrastive Self-supervising Learning for Time Series Classification via Importance-aware Resolution Selection 
[[arxiv](https://arxiv.org/abs/2502.10567)] [[cool](https://papers.cool/arxiv/2502.10567)] [[pdf](https://arxiv.org/pdf/2502.10567)]
> **Authors**: Kevin Garcia,Juan Manuel Perez,Yifeng Gao
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: Appears in IEEEBigData-2024
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Recently, there has been a significant advancement in designing Self-Supervised Learning (SSL) frameworks for time series data to reduce the dependency on data labels. Among these works, hierarchical contrastive learning-based SSL frameworks, which learn representations by contrasting data embeddings at multiple resolutions, have gained considerable attention. Due to their ability to gather more information, they exhibit better generalization in various downstream tasks. However, when the time series data length is significant long, the computational cost is often significantly higher than that of other SSL frameworks. In this paper, to address this challenge, we propose an efficient way to train hierarchical contrastive learning models. Inspired by the fact that each resolution's data embedding is highly dependent, we introduce importance-aware resolution selection based training framework to reduce the computational cost. In the experiment, we demonstrate that the proposed method significantly improves training time while preserving the original model's integrity in extensive time series classification performance evaluations. Our code could be found here, https://github.com/KEEBVIN/IARS

### Accelerating Unbiased LLM Evaluation via Synthetic Feedback 
[[arxiv](https://arxiv.org/abs/2502.10563)] [[cool](https://papers.cool/arxiv/2502.10563)] [[pdf](https://arxiv.org/pdf/2502.10563)]
> **Authors**: Zhaoyi Zhou,Yuda Song,Andrea Zanette
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: When developing new large language models (LLMs), a key step is evaluating their final performance, often by computing the win-rate against a reference model based on external feedback. Human feedback is the gold standard, particularly for capturing nuanced qualities like coherence, readability, and alignment with human expectations. However, human evaluations are costly -- even for large tech companies -- and when conducted with active users, they may negatively impact user experience. A promising alternative is synthetic feedback, where evaluations are conducted by other large language models, including reward models. While this eliminates the need for costly human annotations, it introduces biases that may distort the evaluation process. In this work, we propose a statistically principled framework that integrates human and synthetic feedback to reduce reliance on human annotations while maintaining unbiased win-rate calculations. Our experiments demonstrate a reduction in human annotations by up to 12.2% with an off-the-shelf synthetic evaluator and up to 24.8% with a finetuned variant. Apart from being generalizable, scalable, and free of hyper-parameter tuning, our method offers predictable annotation savings, which can be estimated based on data-dependent characteristics.

### Memory, Benchmark & Robots: A Benchmark for Solving Complex Tasks with Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.10550)] [[cool](https://papers.cool/arxiv/2502.10550)] [[pdf](https://arxiv.org/pdf/2502.10550)]
> **Authors**: Egor Cherepanov,Nikita Kachaev,Alexey K. Kovalev,Aleksandr I. Panov
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 38 pages, 21 figures
- **标题**: None
- **领域**: 机器学习,人工智能,机器人技术
- **Abstract**: Memory is crucial for enabling agents to tackle complex tasks with temporal and spatial dependencies. While many reinforcement learning (RL) algorithms incorporate memory, the field lacks a universal benchmark to assess an agent's memory capabilities across diverse scenarios. This gap is particularly evident in tabletop robotic manipulation, where memory is essential for solving tasks with partial observability and ensuring robust performance, yet no standardized benchmarks exist. To address this, we introduce MIKASA (Memory-Intensive Skills Assessment Suite for Agents), a comprehensive benchmark for memory RL, with three key contributions: (1) we propose a comprehensive classification framework for memory-intensive RL tasks, (2) we collect MIKASA-Base - a unified benchmark that enables systematic evaluation of memory-enhanced agents across diverse scenarios, and (3) we develop MIKASA-Robo - a novel benchmark of 32 carefully designed memory-intensive tasks that assess memory capabilities in tabletop robotic manipulation. Our contributions establish a unified framework for advancing memory RL research, driving the development of more reliable systems for real-world applications. The code is available at https://sites.google.com/view/memorybenchrobots/.

### Learning to be Smooth: An End-to-End Differentiable Particle Smoother 
[[arxiv](https://arxiv.org/abs/2502.10546)] [[cool](https://papers.cool/arxiv/2502.10546)] [[pdf](https://arxiv.org/pdf/2502.10546)]
> **Authors**: Ali Younis,Erik B. Sudderth
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: The Thirty-Eighth Annual Conference onNeuralInformation Processing Systems (NeurIPS 2024)
- **标题**: None
- **领域**: 机器学习,人工智能,机器人技术
- **Abstract**: For challenging state estimation problems arising in domains like vision and robotics, particle-based representations attractively enable temporal reasoning about multiple posterior modes. Particle smoothers offer the potential for more accurate offline data analysis by propagating information both forward and backward in time, but have classically required human-engineered dynamics and observation models. Extending recent advances in discriminative training of particle filters, we develop a framework for low-variance propagation of gradients across long time sequences when training particle smoothers. Our "two-filter'' smoother integrates particle streams that are propagated forward and backward in time, while incorporating stratification and importance weights in the resampling step to provide low-variance gradient estimates for neural network dynamics and observation models. The resulting mixture density particle smoother is substantially more accurate than state-of-the-art particle filters, as well as search-based baselines, for city-scale global vehicle localization from real-world videos and maps.

### From Deep Additive Kernel Learning to Last-Layer Bayesian Neural Networks via Induced Prior Approximation 
[[arxiv](https://arxiv.org/abs/2502.10540)] [[cool](https://papers.cool/arxiv/2502.10540)] [[pdf](https://arxiv.org/pdf/2502.10540)]
> **Authors**: Wenyuan Zhao,Haoyuan Chen,Tie Liu,Rui Tuo,Chao Tian
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 24 pages, 5 figures, presented at AISTATS 2025
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: With the strengths of both deep learning and kernel methods like Gaussian Processes (GPs), Deep Kernel Learning (DKL) has gained considerable attention in recent years. From the computational perspective, however, DKL becomes challenging when the input dimension of the GP layer is high. To address this challenge, we propose the Deep Additive Kernel (DAK) model, which incorporates i) an additive structure for the last-layer GP; and ii) induced prior approximation for each GP unit. This naturally leads to a last-layer Bayesian neural network (BNN) architecture. The proposed method enjoys the interpretability of DKL as well as the computational advantages of BNN. Empirical results show that the proposed approach outperforms state-of-the-art DKL methods in both regression and classification tasks.

### Expert-Agnostic Learning to Defer 
[[arxiv](https://arxiv.org/abs/2502.10533)] [[cool](https://papers.cool/arxiv/2502.10533)] [[pdf](https://arxiv.org/pdf/2502.10533)]
> **Authors**: Joshua Strong,Pramit Saha,Yasin Ibrahim,Cheng Ouyang,Alison Noble
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人机交互
- **Abstract**: Learning to Defer (L2D) learns autonomous systems to independently manage straightforward cases, while deferring uncertain cases to human experts. Recent advancements in this field have introduced features enabling flexibility to unseen experts at test-time, but we find these approaches have significant limitations. To address these, we introduce EA-L2D: Expert-Agnostic Learning to Defer, a novel L2D framework that leverages a Bayesian approach to model expert behaviour in an expert-agnostic manner, facilitating optimal deferral decisions. EA-L2D offers several critical improvements over prior methods, including the ability to incorporate prior knowledge about experts, a reduced reliance on expert-annotated data, and robust performance when deferring to experts with expertise not seen during training. Evaluating on CIFAR-10, HAM10000, German Traffic Lights, Breast Ultrasound, Axial Organ Slices, and Blood Cell MNIST, we observe performance gains over the next state-of-the-art of 1-16\% for seen experts and 4-28\% for unseen experts in settings with high expert diversity.

### KernelBench: Can LLMs Write Efficient GPU Kernels? 
[[arxiv](https://arxiv.org/abs/2502.10517)] [[cool](https://papers.cool/arxiv/2502.10517)] [[pdf](https://arxiv.org/pdf/2502.10517)]
> **Authors**: Anne Ouyang,Simon Guo,Simran Arora,Alex L. Zhang,William Hu,Christopher Ré,Azalia Mirhoseini
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,表现,软件工程
- **Abstract**: Efficient GPU kernels are crucial for building performant machine learning architectures, but writing them is a time-consuming challenge that requires significant expertise; therefore, we explore using language models (LMs) to automate kernel generation. We introduce KernelBench, an open-source framework for evaluating LMs' ability to write fast and correct kernels on a suite of 250 carefully selected PyTorch ML workloads. KernelBench represents a real-world engineering environment and making progress on the introduced benchmark directly translates to faster practical kernels. We introduce a new evaluation metric fast_p, which measures the percentage of generated kernels that are functionally correct and offer a speedup greater than an adjustable threshold p over baseline. Our experiments across various state-of-the-art models and test-time methods show that frontier reasoning models perform the best out of the box but still fall short overall, matching the PyTorch baseline in less than 20% of the cases. While we show that results can improve by leveraging execution and profiling feedback during iterative refinement, KernelBench remains a challenging benchmark, with its difficulty increasing as we raise speedup threshold p.

### Applying Deep Learning to Ads Conversion Prediction in Last Mile Delivery Marketplace 
[[arxiv](https://arxiv.org/abs/2502.10514)] [[cool](https://papers.cool/arxiv/2502.10514)] [[pdf](https://arxiv.org/pdf/2502.10514)]
> **Authors**: Di Li,Xiaochang Miao,Huiyu Song,Chao Chu,Hao Xu,Mandar Rahurkar
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 10 pages, 5 figures, submitted to KDD
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Deep neural networks (DNNs) have revolutionized web-scale ranking systems, enabling breakthroughs in capturing complex user behaviors and driving performance gains. At DoorDash, we first harnessed this transformative power by transitioning our homepage Ads ranking system from traditional tree based models to cutting edge multi task DNNs. This evolution sparked advancements in data foundations, model design, training efficiency, evaluation rigor, and online serving, delivering substantial business impact and reshaping our approach to machine learning. In this paper, we talk about our problem driven journey, from identifying the right problems and crafting targeted solutions to overcoming the complexity of developing and scaling a deep learning recommendation system. Through our successes and learned lessons, we aim to share insights and practical guidance to teams pursuing similar advancements in machine learning systems.

### MixMin: Finding Data Mixtures via Convex Minimization 
[[arxiv](https://arxiv.org/abs/2502.10510)] [[cool](https://papers.cool/arxiv/2502.10510)] [[pdf](https://arxiv.org/pdf/2502.10510)]
> **Authors**: Anvith Thudi,Evianne Rovers,Yangjun Ruan,Tristan Thrush,Chris J. Maddison
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Modern machine learning pipelines are increasingly combining and mixing data from diverse and disparate sources, e.g., pre-training large language models. Yet, finding the optimal data mixture is a challenging and open problem. We formalize this data mixing problem as a bi-level objective: the best mixture is the one that would lead to the best model for a downstream objective. Unfortunately, this objective is generally intractable. In this paper, we make the observation that the bi-level data mixing objective becomes convex as our model class becomes larger. We develop and study a gradient-based approach for optimizing this convex objective, which we call MixMin, and test it on language modeling and chemistry tasks. MixMin was the only method that uniformly improved the data mixture in all our experiments. With MixMin, we improved the data mixture using less than 0.2% additional compute for a pythia-410M model trained on 8.2B tokens, resulting between 1-5% relative improvement to negative log likelihood on PIQA, ARC Easy, SciQ, and OpenWebMath. Crucially, we found that MixMin mixtures for smaller models improved training of larger models, suggesting that MixMin mixtures may be scale-invariant. When mixing bioassay data to train an XGBoost model, we saw improvements to average precision scores of 0.03-0.15.

### Preference learning made easy: Everything should be understood through win rate 
[[arxiv](https://arxiv.org/abs/2502.10505)] [[cool](https://papers.cool/arxiv/2502.10505)] [[pdf](https://arxiv.org/pdf/2502.10505)]
> **Authors**: Lily H. Zhang,Rajesh Ranganath
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学,机器学习
- **Abstract**: Preference learning, or the task of aligning generative models to preference comparison data, has yet to reach the conceptual maturity of classification, density estimation, etc. To close this gap, this work presents a framework to understand preference learning starting from the sampling distribution of pairwise preference data. First, we prove that the only evaluation of a generative model that respects both preferences and prevalences in the data distribution is a form of win rate, justifying win rate as the focal point to understand preference learning. We then analyze preference learning methods as win rate optimization (WRO) or non-WRO. We present novel instances of WRO beyond existing examples (RLHF, NLHF) and identify two key theoretical benefits of all such methods. We prove that common non-WRO methods like DPO and SFT on preferred samples lack these properties and suggest ways to mitigate such theoretical limitations. We also show that WRO underperforms in practice due optimization difficulties and that optimization success predicts performance better than choices which affect the objective's solution. Our analysis highlights best practices for existing methods and provides recommendations for future research, guided by the principle that one should either align non-WRO methods more closely with WRO or improve the optimization of WRO objectives.

### LiveVal: Time-aware Data Valuation via Adaptive Reference Points 
[[arxiv](https://arxiv.org/abs/2502.10489)] [[cool](https://papers.cool/arxiv/2502.10489)] [[pdf](https://arxiv.org/pdf/2502.10489)]
> **Authors**: Jie Xu,Zihan Wu,Cong Wang,Xiaohua Jia
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Time-aware data valuation enhances training efficiency and model robustness, as early detection of harmful samples could prevent months of wasted computation. However, existing methods rely on model retraining or convergence assumptions or fail to capture long-term training dynamics. We propose LiveVal, an efficient time-aware data valuation method with three key designs: 1) seamless integration with SGD training for efficient data contribution monitoring; 2) reference-based valuation with normalization for reliable benchmark establishment; and 3) adaptive reference point selection for real-time updating with optimized memory usage. We establish theoretical guarantees for LiveVal's stability and prove that its valuations are bounded and directionally aligned with optimization progress. Extensive experiments demonstrate that LiveVal provides efficient data valuation across different modalities and model scales, achieving 180 speedup over traditional methods while maintaining robust detection performance.

### Chronic Diseases Prediction Using ML 
[[arxiv](https://arxiv.org/abs/2502.10481)] [[cool](https://papers.cool/arxiv/2502.10481)] [[pdf](https://arxiv.org/pdf/2502.10481)]
> **Authors**: Sri Varsha Mulakala,G. Neeharika,P. Vinay Kumar,A. Bhargava Kiran
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The recent increase in morbidity is primarily due to chronic diseases including Diabetes, Heart disease, Lung cancer, and brain tumours. The results for patients can be improved, and the financial burden on the healthcare system can be lessened, through the early detection and prevention of certain disorders. In this study, we built a machine-learning model for predicting the existence of numerous diseases utilising datasets from various sources, including Kaggle, Dataworld, and the UCI repository, that are relevant to each of the diseases we intended to predict. Following the acquisition of the datasets, we used feature engineering to extract pertinent features from the information, after which the model was trained on a training set and improved using a validation set. A test set was then used to assess the correctness of the final model. We provide an easy-to-use interface where users may enter the parameters for the selected ailment. Once the right model has been run, it will indicate whether the user has a certain ailment and offer suggestions for how to treat or prevent it.

### SinSim: Sinkhorn-Regularized SimCLR 
[[arxiv](https://arxiv.org/abs/2502.10478)] [[cool](https://papers.cool/arxiv/2502.10478)] [[pdf](https://arxiv.org/pdf/2502.10478)]
> **Authors**: M. Hadi Sepanj,Paul Fiegth
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别,机器学习
- **Abstract**: Self-supervised learning has revolutionized representation learning by eliminating the need for labeled data. Contrastive learning methods, such as SimCLR, maximize the agreement between augmented views of an image but lack explicit regularization to enforce a globally structured latent space. This limitation often leads to suboptimal generalization. We propose SinSim, a novel extension of SimCLR that integrates Sinkhorn regularization from optimal transport theory to enhance representation structure. The Sinkhorn loss, an entropy-regularized Wasserstein distance, encourages a well-dispersed and geometry-aware feature space, preserving discriminative power. Empirical evaluations on various datasets demonstrate that SinSim outperforms SimCLR and achieves competitive performance against prominent self-supervised methods such as VICReg and Barlow Twins. UMAP visualizations further reveal improved class separability and structured feature distributions. These results indicate that integrating optimal transport regularization into contrastive learning provides a principled and effective mechanism for learning robust, well-structured representations. Our findings open new directions for applying transport-based constraints in self-supervised learning frameworks.

### From Layers to States: A State Space Model Perspective to Deep Neural Network Layer Dynamics 
[[arxiv](https://arxiv.org/abs/2502.10463)] [[cool](https://papers.cool/arxiv/2502.10463)] [[pdf](https://arxiv.org/pdf/2502.10463)]
> **Authors**: Qinshuo Liu,Weiqin Zhao,Wei Huang,Yanwen Fang,Lequan Yu,Guodong Li
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,网络和互联网架构
- **Abstract**: The depth of neural networks is a critical factor for their capability, with deeper models often demonstrating superior performance. Motivated by this, significant efforts have been made to enhance layer aggregation - reusing information from previous layers to better extract features at the current layer, to improve the representational power of deep neural networks. However, previous works have primarily addressed this problem from a discrete-state perspective which is not suitable as the number of network layers grows. This paper novelly treats the outputs from layers as states of a continuous process and considers leveraging the state space model (SSM) to design the aggregation of layers in very deep neural networks. Moreover, inspired by its advancements in modeling long sequences, the Selective State Space Models (S6) is employed to design a new module called Selective State Space Model Layer Aggregation (S6LA). This module aims to combine traditional CNN or transformer architectures within a sequential framework, enhancing the representational capabilities of state-of-the-art vision networks. Extensive experiments show that S6LA delivers substantial improvements in both image classification and detection tasks, highlighting the potential of integrating SSMs with contemporary deep learning techniques.

### SenDaL: An Effective and Efficient Calibration Framework of Low-Cost Sensors for Daily Life 
[[arxiv](https://arxiv.org/abs/2502.10460)] [[cool](https://papers.cool/arxiv/2502.10460)] [[pdf](https://arxiv.org/pdf/2502.10460)]
> **Authors**: Seokho Ahn,Hyungjin Kim,Euijong Lee,Young-Duk Seo
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-17
> **comment**: Accepted by IEEE IoTJ
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The collection of accurate and noise-free data is a crucial part of Internet of Things (IoT)-controlled environments. However, the data collected from various sensors in daily life often suffer from inaccuracies. Additionally, IoT-controlled devices with low-cost sensors lack sufficient hardware resources to employ conventional deep-learning models. To overcome this limitation, we propose sensors for daily life (SenDaL), the first framework that utilizes neural networks for calibrating low cost sensors. SenDaL introduces novel training and inference processes that enable it to achieve accuracy comparable to deep learning models while simultaneously preserving latency and energy consumption similar to linear models. SenDaL is first trained in a bottom-up manner, making decisions based on calibration results from both linear and deep learning models. Once both models are trained, SenDaL makes independent decisions through a top-down inference process, ensuring accuracy and inference speed. Furthermore, SenDaL can select the optimal deep learning model according to the resources of the IoT devices because it is compatible with various deep learning models, such as long short-term memory-based and Transformer-based models. We have verified that SenDaL outperforms existing deep learning models in terms of accuracy, latency, and energy efficiency through experiments conducted in different IoT environments and real-life scenarios.

### LLM4GNAS: A Large Language Model Based Toolkit for Graph Neural Architecture Search 
[[arxiv](https://arxiv.org/abs/2502.10459)] [[cool](https://papers.cool/arxiv/2502.10459)] [[pdf](https://arxiv.org/pdf/2502.10459)]
> **Authors**: Yang Gao,Hong Yang,Yizhi Chen,Junxian Wu,Peng Zhang,Haishuai Wang
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Graph Neural Architecture Search (GNAS) facilitates the automatic design of Graph Neural Networks (GNNs) tailored to specific downstream graph learning tasks. However, existing GNAS approaches often require manual adaptation to new graph search spaces, necessitating substantial code optimization and domain-specific knowledge. To address this challenge, we present LLM4GNAS, a toolkit for GNAS that leverages the generative capabilities of Large Language Models (LLMs). LLM4GNAS includes an algorithm library for graph neural architecture search algorithms based on LLMs, enabling the adaptation of GNAS methods to new search spaces through the modification of LLM prompts. This approach reduces the need for manual intervention in algorithm adaptation and code modification. The LLM4GNAS toolkit is extensible and robust, incorporating LLM-enhanced graph feature engineering, LLM-enhanced graph neural architecture search, and LLM-enhanced hyperparameter optimization. Experimental results indicate that LLM4GNAS outperforms existing GNAS methods on tasks involving both homogeneous and heterogeneous graphs.

### I Think, Therefore I Diffuse: Enabling Multimodal In-Context Reasoning in Diffusion Models 
[[arxiv](https://arxiv.org/abs/2502.10458)] [[cool](https://papers.cool/arxiv/2502.10458)] [[pdf](https://arxiv.org/pdf/2502.10458)]
> **Authors**: Zhenxing Mi,Kuan-Chieh Wang,Guocheng Qian,Hanrong Ye,Runtao Liu,Sergey Tulyakov,Kfir Aberman,Dan Xu
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-17
> **comment**: Project page: https://mizhenxing.github.io/ThinkDiff, 19 pages, 14 figures
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: This paper presents ThinkDiff, a novel alignment paradigm that empowers text-to-image diffusion models with multimodal in-context understanding and reasoning capabilities by integrating the strengths of vision-language models (VLMs). Existing multimodal diffusion finetuning methods largely focus on pixel-level reconstruction rather than in-context reasoning, and are constrained by the complexity and limited availability of reasoning-based datasets. ThinkDiff addresses these challenges by leveraging vision-language training as a proxy task, aligning VLMs with the decoder of an encoder-decoder large language model (LLM) instead of a diffusion decoder. This proxy task builds on the observation that the $\textbf{LLM decoder}$ shares the same input feature space with $\textbf{diffusion decoders}$ that use the corresponding $\textbf{LLM encoder}$ for prompt embedding. As a result, aligning VLMs with diffusion decoders can be simplified through alignment with the LLM decoder. Without complex training and datasets, ThinkDiff effectively unleashes understanding, reasoning, and composing capabilities in diffusion models. Experiments demonstrate that ThinkDiff significantly improves accuracy from 19.2% to 46.3% on the challenging CoBSAT benchmark for multimodal in-context reasoning generation, with only 5 hours of training on 4 A100 GPUs. Additionally, ThinkDiff demonstrates exceptional performance in composing multiple images and texts into logically coherent images. Project page: https://mizhenxing.github.io/ThinkDiff.

### Deep Reinforcement Learning-Based User Scheduling for Collaborative Perception 
[[arxiv](https://arxiv.org/abs/2502.10456)] [[cool](https://papers.cool/arxiv/2502.10456)] [[pdf](https://arxiv.org/pdf/2502.10456)]
> **Authors**: Yandi Liu,Guowei Liu,Le Liang,Hao Ye,Chongtao Guo,Shi Jin
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器人技术
- **Abstract**: Stand-alone perception systems in autonomous driving suffer from limited sensing ranges and occlusions at extended distances, potentially resulting in catastrophic outcomes. To address this issue, collaborative perception is envisioned to improve perceptual accuracy by using vehicle-to-everything (V2X) communication to enable collaboration among connected and autonomous vehicles and roadside units. However, due to limited communication resources, it is impractical for all units to transmit sensing data such as point clouds or high-definition video. As a result, it is essential to optimize the scheduling of communication links to ensure efficient spectrum utilization for the exchange of perceptual data. In this work, we propose a deep reinforcement learning-based V2X user scheduling algorithm for collaborative perception. Given the challenges in acquiring perceptual labels, we reformulate the conventional label-dependent objective into a label-free goal, based on characteristics of 3D object detection. Incorporating both channel state information (CSI) and semantic information, we develop a double deep Q-Network (DDQN)-based user scheduling framework for collaborative perception, named SchedCP. Simulation results verify the effectiveness and robustness of SchedCP compared with traditional V2X scheduling methods. Finally, we present a case study to illustrate how our proposed algorithm adaptively modifies the scheduling decisions by taking both instantaneous CSI and perceptual semantics into account.

### E2LVLM:Evidence-Enhanced Large Vision-Language Model for Multimodal Out-of-Context Misinformation Detection 
[[arxiv](https://arxiv.org/abs/2502.10455)] [[cool](https://papers.cool/arxiv/2502.10455)] [[pdf](https://arxiv.org/pdf/2502.10455)]
> **Authors**: Junjie Wu,Yumeng Fu,Nan Yu,Guohong Fu
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,多媒体
- **Abstract**: Recent studies in Large Vision-Language Models (LVLMs) have demonstrated impressive advancements in multimodal Out-of-Context (OOC) misinformation detection, discerning whether an authentic image is wrongly used in a claim. Despite their success, the textual evidence of authentic images retrieved from the inverse search is directly transmitted to LVLMs, leading to inaccurate or false information in the decision-making phase. To this end, we present E2LVLM, a novel evidence-enhanced large vision-language model by adapting textual evidence in two levels. First, motivated by the fact that textual evidence provided by external tools struggles to align with LVLMs inputs, we devise a reranking and rewriting strategy for generating coherent and contextually attuned content, thereby driving the aligned and effective behavior of LVLMs pertinent to authentic images. Second, to address the scarcity of news domain datasets with both judgment and explanation, we generate a novel OOC multimodal instruction-following dataset by prompting LVLMs with informative content to acquire plausible explanations. Further, we develop a multimodal instruction-tuning strategy with convincing explanations for beyond detection. This scheme contributes to E2LVLM for multimodal OOC misinformation detection and explanation. A multitude of experiments demonstrate that E2LVLM achieves superior performance than state-of-the-art methods, and also provides compelling rationales for judgments.

### One Example Shown, Many Concepts Known! Counterexample-Driven Conceptual Reasoning in Mathematical LLMs 
[[arxiv](https://arxiv.org/abs/2502.10454)] [[cool](https://papers.cool/arxiv/2502.10454)] [[pdf](https://arxiv.org/pdf/2502.10454)]
> **Authors**: Yinghui Li,Jiayi Kuang,Haojing Huang,Zhikun Xu,Xinnian Liang,Yi Yu,Wenlian Lu,Yangning Li,Xiaoyu Tan,Chao Qu,Ying Shen,Hai-Tao Zheng,Philip S. Yu
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Leveraging mathematical Large Language Models (LLMs) for proof generation is a fundamental topic in LLMs research. We argue that the ability of current LLMs to prove statements largely depends on whether they have encountered the relevant proof process during training. This reliance limits their deeper understanding of mathematical theorems and related concepts. Inspired by the pedagogical method of "proof by counterexamples" commonly used in human mathematics education, our work aims to enhance LLMs' ability to conduct mathematical reasoning and proof through counterexamples. Specifically, we manually create a high-quality, university-level mathematical benchmark, CounterMATH, which requires LLMs to prove mathematical statements by providing counterexamples, thereby assessing their grasp of mathematical concepts. Additionally, we develop a data engineering framework to automatically obtain training data for further model improvement. Extensive experiments and detailed analyses demonstrate that CounterMATH is challenging, indicating that LLMs, such as OpenAI o1, have insufficient counterexample-driven proof capabilities. Moreover, our exploration into model training reveals that strengthening LLMs' counterexample-driven conceptual reasoning abilities is crucial for improving their overall mathematical capabilities. We believe that our work offers new perspectives on the community of mathematical LLMs.

### Quaternion-Hadamard Network: A Novel Defense Against Adversarial Attacks with a New Dataset 
[[arxiv](https://arxiv.org/abs/2502.10452)] [[cool](https://papers.cool/arxiv/2502.10452)] [[pdf](https://arxiv.org/pdf/2502.10452)]
> **Authors**: Vladimir Frants,Sos Agaian
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,图像和视频处理
- **Abstract**: This paper addresses the vulnerability of deep-learning models designed for rain, snow, and haze removal. Despite enhancing image quality in adverse weather, these models are susceptible to adversarial attacks that compromise their effectiveness. Traditional defenses such as adversarial training and model distillation often require extensive retraining, making them costly and impractical for real-world deployment. While denoising and super-resolution techniques can aid image classification models, they impose high computational demands and introduce visual artifacts that hinder image processing tasks. We propose a model-agnostic defense against first-order white-box adversarial attacks using the Quaternion-Hadamard Network (QHNet) to tackle these challenges. White-box attacks are particularly difficult to defend against since attackers have full access to the model's architecture, weights, and training procedures. Our defense introduces the Quaternion Hadamard Denoising Convolutional Block (QHDCB) and the Quaternion Denoising Residual Block (QDRB), leveraging polynomial thresholding. QHNet incorporates these blocks within an encoder-decoder architecture, enhanced by feature refinement, to effectively neutralize adversarial noise. Additionally, we introduce the Adversarial Weather Conditions Vision Dataset (AWCVD), created by applying first-order gradient attacks on state-of-the-art weather removal techniques in scenarios involving haze, rain streaks, and snow. Using PSNR and SSIM metrics, we demonstrate that QHNet significantly enhances the robustness of low-level computer vision models against adversarial attacks compared with state-of-the-art denoising and super-resolution techniques. The source code and dataset will be released alongside the final version of this paper.

### FlexControl: Computation-Aware ControlNet with Differentiable Router for Text-to-Image Generation 
[[arxiv](https://arxiv.org/abs/2502.10451)] [[cool](https://papers.cool/arxiv/2502.10451)] [[pdf](https://arxiv.org/pdf/2502.10451)]
> **Authors**: Zheng Fang,Lichuan Xiang,Xu Cai,Kaicheng Zhou,Hongkai Wen
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,图形
- **Abstract**: ControlNet offers a powerful way to guide diffusion-based generative models, yet most implementations rely on ad-hoc heuristics to choose which network blocks to control-an approach that varies unpredictably with different tasks. To address this gap, we propose FlexControl, a novel framework that copies all diffusion blocks during training and employs a trainable gating mechanism to dynamically select which blocks to activate at each denoising step. With introducing a computation-aware loss, we can encourage control blocks only to activate when it benefit the generation quality. By eliminating manual block selection, FlexControl enhances adaptability across diverse tasks and streamlines the design pipeline, with computation-aware training loss in an end-to-end training manner. Through comprehensive experiments on both UNet (e.g., SD1.5) and DiT (e.g., SD3.0), we show that our method outperforms existing ControlNet variants in certain key aspects of interest. As evidenced by both quantitative and qualitative evaluations, FlexControl preserves or enhances image fidelity while also reducing computational overhead by selectively activating the most relevant blocks. These results underscore the potential of a flexible, data-driven approach for controlled diffusion and open new avenues for efficient generative model design. The code will soon be available at https://github.com/Anonymousuuser/FlexControl.

### Evaluating and Explaining Earthquake-Induced Liquefaction Potential through Multi-Modal Transformers 
[[arxiv](https://arxiv.org/abs/2502.10446)] [[cool](https://papers.cool/arxiv/2502.10446)] [[pdf](https://arxiv.org/pdf/2502.10446)]
> **Authors**: Sompote Youwai,Tipok Kitkobsin,Sutat Leelataviwat,Pornkasem Jongpradist
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,地球物理学
- **Abstract**: This study presents an explainable parallel transformer architecture for soil liquefaction prediction that integrates three distinct data streams: spectral seismic encoding, soil stratigraphy tokenization, and site-specific features. The architecture processes data from 165 case histories across 11 major earthquakes, employing Fast Fourier Transform for seismic waveform encoding and principles from large language models for soil layer tokenization. Interpretability is achieved through SHapley Additive exPlanations (SHAP), which decompose predictions into individual contributions from seismic characteristics, soil properties, and site conditions. The model achieves 93.75% prediction accuracy on cross-regional validation sets and demonstrates robust performance through sensitivity analysis of ground motion intensity and soil resistance parameters. Notably, validation against previously unseen ground motion data from the 2024 Noto Peninsula earthquake confirms the model's generalization capabilities and practical utility. Implementation as a publicly accessible web application enables rapid assessment of multiple sites simultaneously. This approach establishes a new framework in geotechnical deep learning where sophisticated multi-modal analysis meets practical engineering requirements through quantitative interpretation and accessible deployment.

### One Class Restricted Kernel Machines 
[[arxiv](https://arxiv.org/abs/2502.10443)] [[cool](https://papers.cool/arxiv/2502.10443)] [[pdf](https://arxiv.org/pdf/2502.10443)]
> **Authors**: A. Quadir,M. Sajid,M. Tanveer
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-17
> **comment**: ef:31st International Conference onNeuralInformation Processing (ICONIP2024), New Zealand
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Restricted kernel machines (RKMs) have demonstrated a significant impact in enhancing generalization ability in the field of machine learning. Recent studies have introduced various methods within the RKM framework, combining kernel functions with the least squares support vector machine (LSSVM) in a manner similar to the energy function of restricted boltzmann machines (RBM), such that a better performance can be achieved. However, RKM's efficacy can be compromised by the presence of outliers and other forms of contamination within the dataset. These anomalies can skew the learning process, leading to less accurate and reliable outcomes. To address this critical issue and to ensure the robustness of the model, we propose the novel one-class RKM (OCRKM). In the framework of OCRKM, we employ an energy function akin to that of the RBM, which integrates both visible and hidden variables in a nonprobabilistic setting. The formulation of the proposed OCRKM facilitates the seamless integration of one-class classification method with the RKM, enhancing its capability to detect outliers and anomalies effectively. The proposed OCRKM model is evaluated over UCI benchmark datasets. Experimental findings and statistical analyses consistently emphasize the superior generalization capabilities of the proposed OCRKM model over baseline models across all scenarios.

### Analysis of Overparameterization in Continual Learning under a Linear Model 
[[arxiv](https://arxiv.org/abs/2502.10442)] [[cool](https://papers.cool/arxiv/2502.10442)] [[pdf](https://arxiv.org/pdf/2502.10442)]
> **Authors**: Daniel Goldfarb,Paul Hand
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Autonomous machine learning systems that learn many tasks in sequence are prone to the catastrophic forgetting problem. Mathematical theory is needed in order to understand the extent of forgetting during continual learning. As a foundational step towards this goal, we study continual learning and catastrophic forgetting from a theoretical perspective in the simple setting of gradient descent with no explicit algorithmic mechanism to prevent forgetting. In this setting, we analytically demonstrate that overparameterization alone can mitigate forgetting in the context of a linear regression model. We consider a two-task setting motivated by permutation tasks, and show that as the overparameterization ratio becomes sufficiently high, a model trained on both tasks in sequence results in a low-risk estimator for the first task. As part of this work, we establish a non-asymptotic bound of the risk of a single linear regression task, which may be of independent interest to the field of double descent theory.

### Leveraging Constraint Violation Signals For Action-Constrained Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.10431)] [[cool](https://papers.cool/arxiv/2502.10431)] [[pdf](https://arxiv.org/pdf/2502.10431)]
> **Authors**: Janaka Chathuranga Brahmanage,Jiajing Ling,Akshat Kumar
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-17
> **comment**: 7 pages and 5 pages supplementary
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: In many RL applications, ensuring an agent's actions adhere to constraints is crucial for safety. Most previous methods in Action-Constrained Reinforcement Learning (ACRL) employ a projection layer after the policy network to correct the action. However projection-based methods suffer from issues like the zero gradient problem and higher runtime due to the usage of optimization solvers. Recently methods were proposed to train generative models to learn a differentiable mapping between latent variables and feasible actions to address this issue. However, generative models require training using samples from the constrained action space, which itself is challenging. To address such limitations, first, we define a target distribution for feasible actions based on constraint violation signals, and train normalizing flows by minimizing the KL divergence between an approximated distribution over feasible actions and the target. This eliminates the need to generate feasible action samples, greatly simplifying the flow model learning. Second, we integrate the learned flow model with existing deep RL methods, which restrict it to exploring only the feasible action space. Third, we extend our approach beyond ACRL to handle state-wise constraints by learning the constraint violation signal from the environment. Empirically, our approach has significantly fewer constraint violations while achieving similar or better quality in several control tasks than previous best methods.

### Real Time Control of Tandem-Wing Experimental Platform Using Concerto Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.10429)] [[cool](https://papers.cool/arxiv/2502.10429)] [[pdf](https://arxiv.org/pdf/2502.10429)]
> **Authors**: Zhang Minghao,Yang Xiaojun,Wang Zhihe,Wang Liang
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-17
> **comment**: 30 pages, 12 figures
- **标题**: None
- **领域**: 机器学习,人工智能,机器人技术,系统与控制
- **Abstract**: This paper introduces the CRL2RT algorithm, an advanced reinforcement learning method aimed at improving the real-time control performance of the Direct-Drive Tandem-Wing Experimental Platform (DDTWEP). Inspired by dragonfly flight, DDTWEP's tandem wing structure causes nonlinear and unsteady aerodynamic interactions, leading to complex load behaviors during pitch, roll, and yaw maneuvers. These complexities challenge stable motion control at high frequencies (2000 Hz). To overcome these issues, we developed the CRL2RT algorithm, which combines classical control elements with reinforcement learning-based controllers using a time-interleaved architecture and a rule-based policy composer. This integration ensures finite-time convergence and single-life adaptability. Experimental results under various conditions, including different flapping frequencies and yaw disturbances, show that CRL2RT achieves a control frequency surpassing 2500 Hz on standard CPUs. Additionally, when integrated with classical controllers like PID, Adaptive PID, and Model Reference Adaptive Control (MRAC), CRL2RT enhances tracking performance by 18.3% to 60.7%. These findings demonstrate CRL2RT's broad applicability and superior performance in complex real-time control scenarios, validating its effectiveness in overcoming existing control strategy limitations and advancing robust, efficient real-time control for biomimetic aerial vehicles.

### QuantSpec: Self-Speculative Decoding with Hierarchical Quantized KV Cache 
[[arxiv](https://arxiv.org/abs/2502.10424)] [[cool](https://papers.cool/arxiv/2502.10424)] [[pdf](https://arxiv.org/pdf/2502.10424)]
> **Authors**: Rishabh Tiwari,Haocheng Xi,Aditya Tomar,Coleman Hooper,Sehoon Kim,Maxwell Horton,Mahyar Najibi,Michael W. Mahoney,Kurt Keutzer,Amir Gholami
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Large Language Models (LLMs) are increasingly being deployed on edge devices for long-context settings, creating a growing need for fast and efficient long-context inference. In these scenarios, the Key-Value (KV) cache is the primary bottleneck in terms of both GPU memory and latency, as the full KV cache must be loaded for each decoding step. While speculative decoding is a widely accepted technique to accelerate autoregressive decoding, existing methods often struggle to achieve significant speedups due to inefficient KV cache optimization strategies and result in low acceptance rates. To address these challenges, we propose a novel self-speculative decoding framework, QuantSpec, where the draft model shares the architecture of the target model but employs a hierarchical 4-bit quantized KV cache and 4-bit quantized weights for acceleration. QuantSpec maintains high acceptance rates ($>$90%) and reliably provides consistent end-to-end speedups upto $\sim2.5\times$, outperforming other self-speculative decoding methods that use sparse KV cache for long-context LLM inference. QuantSpec also reduces the memory requirements by $\sim 1.3\times$ compared to these alternatives.

### (How) Can Transformers Predict Pseudo-Random Numbers? 
[[arxiv](https://arxiv.org/abs/2502.10390)] [[cool](https://papers.cool/arxiv/2502.10390)] [[pdf](https://arxiv.org/pdf/2502.10390)]
> **Authors**: Tao Tao,Darshil Doshi,Dayal Singh Kalra,Tianyu He,Maissam Barkeshli
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 10+16 pages, 12+20 figures
- **标题**: None
- **领域**: 机器学习,无序系统和神经网络,密码学和安全,机器学习
- **Abstract**: Transformers excel at discovering patterns in sequential data, yet their fundamental limitations and learning mechanisms remain crucial topics of investigation. In this paper, we study the ability of Transformers to learn pseudo-random number sequences from linear congruential generators (LCGs), defined by the recurrence relation $x_{t+1} = a x_t + c \;\mathrm{mod}\; m$. Our analysis reveals that with sufficient architectural capacity and training data variety, Transformers can perform in-context prediction of LCG sequences with unseen moduli ($m$) and parameters ($a,c$). Through analysis of embedding layers and attention patterns, we uncover how Transformers develop algorithmic structures to learn these sequences in two scenarios of increasing complexity. First, we analyze how Transformers learn LCG sequences with unseen ($a, c$) but fixed modulus, and we demonstrate successful learning up to $m = 2^{32}$. Our analysis reveals that models learn to factorize the modulus and utilize digit-wise number representations to make sequential predictions. In the second, more challenging scenario of unseen moduli, we show that Transformers can generalize to unseen moduli up to $m_{\text{test}} = 2^{16}$. In this case, the model employs a two-step strategy: first estimating the unknown modulus from the context, then utilizing prime factorizations to generate predictions. For this task, we observe a sharp transition in the accuracy at a critical depth $=3$. We also find that the number of in-context sequence elements needed to reach high accuracy scales sublinearly with the modulus.

### Balancing the Scales: A Theoretical and Algorithmic Framework for Learning from Imbalanced Data 
[[arxiv](https://arxiv.org/abs/2502.10381)] [[cool](https://papers.cool/arxiv/2502.10381)] [[pdf](https://arxiv.org/pdf/2502.10381)]
> **Authors**: Corinna Cortes,Anqi Mao,Mehryar Mohri,Yutao Zhong
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Class imbalance remains a major challenge in machine learning, especially in multi-class problems with long-tailed distributions. Existing methods, such as data resampling, cost-sensitive techniques, and logistic loss modifications, though popular and often effective, lack solid theoretical foundations. As an example, we demonstrate that cost-sensitive methods are not Bayes consistent. This paper introduces a novel theoretical framework for analyzing generalization in imbalanced classification. We propose a new class-imbalanced margin loss function for both binary and multi-class settings, prove its strong $H$-consistency, and derive corresponding learning guarantees based on empirical loss and a new notion of class-sensitive Rademacher complexity. Leveraging these theoretical results, we devise novel and general learning algorithms, IMMAX (Imbalanced Margin Maximization), which incorporate confidence margins and are applicable to various hypothesis sets. While our focus is theoretical, we also present extensive empirical results demonstrating the effectiveness of our algorithms compared to existing baselines.

### AffinityFlow: Guided Flows for Antibody Affinity Maturation 
[[arxiv](https://arxiv.org/abs/2502.10365)] [[cool](https://papers.cool/arxiv/2502.10365)] [[pdf](https://arxiv.org/pdf/2502.10365)]
> **Authors**: Can Chen,Karla-Luise Herpoldt,Chenchao Zhao,Zichen Wang,Marcus Collins,Shang Shang,Ron Benson
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 14 pages, 5 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Antibodies are widely used as therapeutics, but their development requires costly affinity maturation, involving iterative mutations to enhance binding affinity.This paper explores a sequence-only scenario for affinity maturation, using solely antibody and antigen sequences. Recently AlphaFlow wraps AlphaFold within flow matching to generate diverse protein structures, enabling a sequence-conditioned generative model of structure. Building on this, we propose an alternating optimization framework that (1) fixes the sequence to guide structure generation toward high binding affinity using a structure-based affinity predictor, then (2) applies inverse folding to create sequence mutations, refined by a sequence-based affinity predictor for post selection. A key challenge is the lack of labeled data for training both predictors. To address this, we develop a co-teaching module that incorporates valuable information from noisy biophysical energies into predictor refinement. The sequence-based predictor selects consensus samples to teach the structure-based predictor, and vice versa. Our method, AffinityFlow, achieves state-of-the-art performance in affinity maturation experiments. We plan to open-source our code after acceptance.

### Proper Learnability and the Role of Unlabeled Data 
[[arxiv](https://arxiv.org/abs/2502.10359)] [[cool](https://papers.cool/arxiv/2502.10359)] [[pdf](https://arxiv.org/pdf/2502.10359)]
> **Authors**: Julian Asilis,Siddartha Devic,Shaddin Dughmi,Vatsal Sharan,Shang-Hua Teng
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: ALT 2025, 22 pages
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Proper learning refers to the setting in which learners must emit predictors in the underlying hypothesis class $H$, and often leads to learners with simple algorithmic forms (e.g. empirical risk minimization (ERM), structural risk minimization (SRM)). The limitation of proper learning, however, is that there exist problems which can only be learned improperly, e.g. in multiclass classification. Thus, we ask: Under what assumptions on the hypothesis class or the information provided to the learner is a problem properly learnable? We first demonstrate that when the unlabeled data distribution is given, there always exists an optimal proper learner governed by distributional regularization, a randomized generalization of regularization. We refer to this setting as the distribution-fixed PAC model, and continue to evaluate the learner on its worst-case performance over all distributions. Our result holds for all metric loss functions and any finite learning problem (with no dependence on its size). Further, we demonstrate that sample complexities in the distribution-fixed PAC model can shrink by only a logarithmic factor from the classic PAC model, strongly refuting the role of unlabeled data in PAC learning (from a worst-case perspective). We complement this with impossibility results which obstruct any characterization of proper learnability in the realizable PAC model. First, we observe that there are problems whose proper learnability is logically undecidable, i.e., independent of the ZFC axioms. We then show that proper learnability is not a monotone property of the underlying hypothesis class, and that it is not a local property (in a precise sense). Our impossibility results all hold even for the fundamental setting of multiclass classification, and go through a reduction of EMX learning (Ben-David et al., 2019) to proper classification which may be of independent interest.

### Dimension-free Score Matching and Time Bootstrapping for Diffusion Models 
[[arxiv](https://arxiv.org/abs/2502.10354)] [[cool](https://papers.cool/arxiv/2502.10354)] [[pdf](https://arxiv.org/pdf/2502.10354)]
> **Authors**: Syamantak Kumar,Dheeraj Nagaraj,Purnamrita Sarkar
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,统计理论,机器学习
- **Abstract**: Diffusion models generate samples by estimating the score function of the target distribution at various noise levels. The model is trained using samples drawn from the target distribution, progressively adding noise. In this work, we establish the first (nearly) dimension-free sample complexity bounds for learning these score functions, achieving a double exponential improvement in dimension over prior results. A key aspect of our analysis is the use of a single function approximator to jointly estimate scores across noise levels, a critical feature of diffusion models in practice which enables generalization across timesteps. Our analysis introduces a novel martingale-based error decomposition and sharp variance bounds, enabling efficient learning from dependent data generated by Markov processes, which may be of independent interest. Building on these insights, we propose Bootstrapped Score Matching (BSM), a variance reduction technique that utilizes previously learned scores to improve accuracy at higher noise levels. These results provide crucial insights into the efficiency and effectiveness of diffusion models for generative modeling.

### InfoPos: A ML-Assisted Solution Design Support Framework for Industrial Cyber-Physical Systems 
[[arxiv](https://arxiv.org/abs/2502.10331)] [[cool](https://papers.cool/arxiv/2502.10331)] [[pdf](https://arxiv.org/pdf/2502.10331)]
> **Authors**: Uraz Odyurt,Richard Loendersloot,Tiedo Tinga
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The variety of building blocks and algorithms incorporated in data-centric and ML-assisted solutions is high, contributing to two challenges: selection of most effective set and order of building blocks, as well as achieving such a selection with minimum cost. Considering that ML-assisted solution design is influenced by the extent of available data, as well as available knowledge of the target system, it is advantageous to be able to select matching building blocks. We introduce the first iteration of our InfoPos framework, allowing the placement of use-cases considering the available positions (levels), i.e., from poor to rich, of knowledge and data dimensions. With that input, designers and developers can reveal the most effective corresponding choice(s), streamlining the solution design process. The results from our demonstrator, an anomaly identification use-case for industrial Cyber-Physical Systems, reflects achieved effects upon the use of different building blocks throughout knowledge and data positions. The achieved ML model performance is considered as the indicator. Our data processing code and the composed data sets are publicly available.

### DiOpt: Self-supervised Diffusion for Constrained Optimization 
[[arxiv](https://arxiv.org/abs/2502.10330)] [[cool](https://papers.cool/arxiv/2502.10330)] [[pdf](https://arxiv.org/pdf/2502.10330)]
> **Authors**: Shutong Ding,Yimiao Zhou,Ke Hu,Xi Yao,Junchi Yan,Xiaoying Tang,Ye Shi
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Recent advances in diffusion models show promising potential for learning-based optimization by leveraging their multimodal sampling capability to escape local optima. However, existing diffusion-based optimization approaches, often reliant on supervised training, lacks a mechanism to ensure strict constraint satisfaction which is often required in real-world applications. One resulting observation is the distributional misalignment, i.e. the generated solution distribution often exhibits small overlap with the feasible domain. In this paper, we propose DiOpt, a novel diffusion paradigm that systematically learns near-optimal feasible solution distributions through iterative self-training. Our framework introduces several key innovations: a target distribution specifically designed to maximize overlap with the constrained solution manifold; a bootstrapped self-training mechanism that adaptively weights candidate solutions based on the severity of constraint violations and optimality gaps; and a dynamic memory buffer that accelerates convergence by retaining high-quality solutions over training iterations. To our knowledge, DiOpt represents the first successful integration of self-supervised diffusion with hard constraint satisfaction. Evaluations on diverse tasks, including power grid control, motion retargeting, wireless allocation demonstrate its superiority in terms of both optimality and constraint satisfaction.

### Process Reward Models for LLM Agents: Practical Framework and Directions 
[[arxiv](https://arxiv.org/abs/2502.10325)] [[cool](https://papers.cool/arxiv/2502.10325)] [[pdf](https://arxiv.org/pdf/2502.10325)]
> **Authors**: Sanjiban Choudhury
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 17 pages, 7 figures
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: We introduce Agent Process Reward Models (AgentPRM), a simple and scalable framework for training LLM agents to continually improve through interactions. AgentPRM follows a lightweight actor-critic paradigm, using Monte Carlo rollouts to compute reward targets and optimize policies. It requires minimal modifications to existing RLHF pipelines, making it easy to integrate at scale. Beyond AgentPRM, we propose InversePRM, which learns process rewards directly from demonstrations without explicit outcome supervision. We also explore key challenges and opportunities, including exploration, process reward shaping, and model-predictive reasoning. We evaluate on ALFWorld benchmark, show that small 3B models trained with AgentPRM and InversePRM outperform strong GPT-4o baselines, and analyze test-time scaling, reward hacking, and more. Our code is available at: https://github.com/sanjibanc/agent_prm.

### ExplainReduce: Summarising local explanations via proxies 
[[arxiv](https://arxiv.org/abs/2502.10311)] [[cool](https://papers.cool/arxiv/2502.10311)] [[pdf](https://arxiv.org/pdf/2502.10311)]
> **Authors**: Lauri Seppäläinen,Mudong Guo,Kai Puolamäki
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 22 pages with a 7 page appendix, 7 + 5 figures, 2 tables. The datasets and source code used in the paper are available at https://github.com/edahelsinki/explainreduce
- **标题**: None
- **领域**: 机器学习,人工智能,人机交互
- **Abstract**: Most commonly used non-linear machine learning methods are closed-box models, uninterpretable to humans. The field of explainable artificial intelligence (XAI) aims to develop tools to examine the inner workings of these closed boxes. An often-used model-agnostic approach to XAI involves using simple models as local approximations to produce so-called local explanations; examples of this approach include LIME, SHAP, and SLISEMAP. This paper shows how a large set of local explanations can be reduced to a small "proxy set" of simple models, which can act as a generative global explanation. This reduction procedure, ExplainReduce, can be formulated as an optimisation problem and approximated efficiently using greedy heuristics.

### SPIRIT: Short-term Prediction of solar IRradIance for zero-shot Transfer learning using Foundation Models 
[[arxiv](https://arxiv.org/abs/2502.10307)] [[cool](https://papers.cool/arxiv/2502.10307)] [[pdf](https://arxiv.org/pdf/2502.10307)]
> **Authors**: Aditya Mishra,Ravindra T,Srinivasan Iyengar,Shivkumar Kalyanaraman,Ponnurangam Kumaraguru
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: Traditional solar forecasting models are based on several years of site-specific historical irradiance data, often spanning five or more years, which are unavailable for newer photovoltaic farms. As renewable energy is highly intermittent, building accurate solar irradiance forecasting systems is essential for efficient grid management and enabling the ongoing proliferation of solar energy, which is crucial to achieve the United Nations' net zero goals. In this work, we propose SPIRIT, a novel approach leveraging foundation models for solar irradiance forecasting, making it applicable to newer solar installations. Our approach outperforms state-of-the-art models in zero-shot transfer learning by about 70%, enabling effective performance at new locations without relying on any historical data. Further improvements in performance are achieved through fine-tuning, as more location-specific data becomes available. These findings are supported by statistical significance, further validating our approach. SPIRIT represents a pivotal step towards rapid, scalable, and adaptable solar forecasting solutions, advancing the integration of renewable energy into global power systems.

### DeltaProduct: Increasing the Expressivity of DeltaNet Through Products of Householders 
[[arxiv](https://arxiv.org/abs/2502.10297)] [[cool](https://papers.cool/arxiv/2502.10297)] [[pdf](https://arxiv.org/pdf/2502.10297)]
> **Authors**: Julien Siems,Timur Carstensen,Arber Zela,Frank Hutter,Massimiliano Pontil,Riccardo Grazzi
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学,形式语言和自动机理论
- **Abstract**: Linear Recurrent Neural Networks (linear RNNs) have emerged as competitive alternatives to Transformers for sequence modeling, offering efficient training and linear-time inference. However, existing architectures face a fundamental trade-off between expressivity and efficiency, dictated by the structure of their state-transition matrices. While diagonal matrices used in architectures like Mamba, GLA, or mLSTM yield fast runtime, they suffer from severely limited expressivity. To address this, recent architectures such as (Gated) DeltaNet and RWKVv7 adopted a diagonal plus rank-1 structure, allowing simultaneous token-channel mixing, which overcomes some expressivity limitations with only a slight decrease in training efficiency. Building on the interpretation of DeltaNet's recurrence as performing one step of online gradient descent per token on an associative recall loss, we introduce DeltaProduct, which instead takes multiple ($n_h$) steps per token. This naturally leads to diagonal plus rank-$n_h$ state-transition matrices, formed as products of $n_h$ generalized Householder transformations, providing a tunable mechanism to balance expressivity and efficiency and a stable recurrence. Through extensive experiments, we demonstrate that DeltaProduct achieves superior state-tracking and language modeling capabilities while exhibiting significantly improved length extrapolation compared to DeltaNet. Additionally, we also strengthen the theoretical foundation of DeltaNet's expressivity by proving that it can solve dihedral group word problems in just two layers.

### Fenchel-Young Variational Learning 
[[arxiv](https://arxiv.org/abs/2502.10295)] [[cool](https://papers.cool/arxiv/2502.10295)] [[pdf](https://arxiv.org/pdf/2502.10295)]
> **Authors**: Sophia Sklaviadis,Sweta Agrawal,Antonio Farinhas,Andre Martins,Mario Figueiredo
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: Under review
- **标题**: None
- **领域**: 机器学习
- **Abstract**: From a variational perspective, many statistical learning criteria involve seeking a distribution that balances empirical risk and regularization. In this paper, we broaden this perspective by introducing a new general class of variational methods based on Fenchel-Young (FY) losses, treated as divergences that generalize (and encompass) the familiar Kullback-Leibler divergence at the core of classical variational learning. Our proposed formulation -- FY variational learning -- includes as key ingredients new notions of FY free energy, FY evidence, FY evidence lower bound, and FY posterior. We derive alternating minimization and gradient backpropagation algorithms to compute (or lower bound) the FY evidence, which enables learning a wider class of models than previous variational formulations. This leads to generalized FY variants of classical algorithms, such as an FY expectation-maximization (FYEM) algorithm, and latent-variable models, such as an FY variational autoencoder (FYVAE). Our new methods are shown to be empirically competitive, often outperforming their classical counterparts, and most importantly, to have qualitatively novel features. For example, FYEM has an adaptively sparse E-step, while the FYVAE can support models with sparse observations and sparse posteriors.

### Small Loss Bounds for Online Learning Separated Function Classes: A Gaussian Process Perspective 
[[arxiv](https://arxiv.org/abs/2502.10292)] [[cool](https://papers.cool/arxiv/2502.10292)] [[pdf](https://arxiv.org/pdf/2502.10292)]
> **Authors**: Adam Block,Abhishek Shetty
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: In order to develop practical and efficient algorithms while circumventing overly pessimistic computational lower bounds, recent work has been interested in developing oracle-efficient algorithms in a variety of learning settings. Two such settings of particular interest are online and differentially private learning. While seemingly different, these two fields are fundamentally connected by the requirement that successful algorithms in each case satisfy stability guarantees; in particular, recent work has demonstrated that algorithms for online learning whose performance adapts to beneficial problem instances, attaining the so-called small-loss bounds, require a form of stability similar to that of differential privacy. In this work, we identify the crucial role that separation plays in allowing oracle-efficient algorithms to achieve this strong stability. Our notion, which we term $ρ$-separation, generalizes and unifies several previous approaches to enforcing this strong stability, including the existence of small-separator sets and the recent notion of $γ$-approximability. We present an oracle-efficient algorithm that is capable of achieving small-loss bounds with improved rates in greater generality than previous work, as well as a variant for differentially private learning that attains optimal rates, again under our separation condition. In so doing, we prove a new stability result for minimizers of a Gaussian process that strengthens and generalizes previous work.

### Adversarial Mixup Unlearning 
[[arxiv](https://arxiv.org/abs/2502.10288)] [[cool](https://papers.cool/arxiv/2502.10288)] [[pdf](https://arxiv.org/pdf/2502.10288)]
> **Authors**: Zhuoyi Peng,Yixuan Tang,Yi Yang
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: ICLR 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Machine unlearning is a critical area of research aimed at safeguarding data privacy by enabling the removal of sensitive information from machine learning models. One unique challenge in this field is catastrophic unlearning, where erasing specific data from a well-trained model unintentionally removes essential knowledge, causing the model to deviate significantly from a retrained one. To address this, we introduce a novel approach that regularizes the unlearning process by utilizing synthesized mixup samples, which simulate the data susceptible to catastrophic effects. At the core of our approach is a generator-unlearner framework, MixUnlearn, where a generator adversarially produces challenging mixup examples, and the unlearner effectively forgets target information based on these synthesized data. Specifically, we first introduce a novel contrastive objective to train the generator in an adversarial direction: generating examples that prompt the unlearner to reveal information that should be forgotten, while losing essential knowledge. Then the unlearner, guided by two other contrastive loss terms, processes the synthesized and real data jointly to ensure accurate unlearning without losing critical knowledge, overcoming catastrophic effects. Extensive evaluations across benchmark datasets demonstrate that our method significantly outperforms state-of-the-art approaches, offering a robust solution to machine unlearning. This work not only deepens understanding of unlearning mechanisms but also lays the foundation for effective machine unlearning with mixup augmentation.

### Probabilistic Super-Resolution for High-Fidelity Physical System Simulations with Uncertainty Quantification 
[[arxiv](https://arxiv.org/abs/2502.10280)] [[cool](https://papers.cool/arxiv/2502.10280)] [[pdf](https://arxiv.org/pdf/2502.10280)]
> **Authors**: Pengyu Zhang,Connor Duffin,Alex Glyn-Davies,Arnaud Vadeboncoeur,Mark Girolami
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Super-resolution (SR) is a promising tool for generating high-fidelity simulations of physical systems from low-resolution data, enabling fast and accurate predictions in engineering applications. However, existing deep-learning based SR methods, require large labeled datasets and lack reliable uncertainty quantification (UQ), limiting their applicability in real-world scenarios. To overcome these challenges, we propose a probabilistic SR framework that leverages the Statistical Finite Element Method and energy-based generative modeling. Our method enables efficient high-resolution predictions with inherent UQ, while eliminating the need for extensive labeled datasets. The method is validated on a 2D Poisson example and compared with bicubic interpolation upscaling. Results demonstrate a computational speed-up over high-resolution numerical solvers while providing reliable uncertainty estimates.

### Efficient Zero-Order Federated Finetuning of Language Models for Resource-Constrained Devices 
[[arxiv](https://arxiv.org/abs/2502.10239)] [[cool](https://papers.cool/arxiv/2502.10239)] [[pdf](https://arxiv.org/pdf/2502.10239)]
> **Authors**: Mohamed Aboelenien Ahmed,Kilian Pfeiffer,Ramin Khalili,Heba Khdr,Jörg Henkel
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Federated fine-tuning offers a promising approach for tuning Large Language Models (LLMs) on edge devices while preserving data privacy. However, fine-tuning these models on edge devices remains challenging due to high memory, communication, and computational demands. Zero-order optimization with task alignment provides a potential solution, enabling fine-tuning with inference-level memory requirements but requires a longer convergence time. In this paper, we propose Federated Split-Perturbation Zero-order Optimization (FedSPZO) that divides the network into two blocks, applying a different number of perturbations per block in a computationally effective way, achieving faster convergence. Our evaluation shows a $2.5 - 7\times $ reduction in computation overhead compared to zero-order state of the art techniques in federated learning.

### Shaping Inductive Bias in Diffusion Models through Frequency-Based Noise Control 
[[arxiv](https://arxiv.org/abs/2502.10236)] [[cool](https://papers.cool/arxiv/2502.10236)] [[pdf](https://arxiv.org/pdf/2502.10236)]
> **Authors**: Thomas Jiralerspong,Berton Earnshaw,Jason Hartford,Yoshua Bengio,Luca Scimeca
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Diffusion Probabilistic Models (DPMs) are powerful generative models that have achieved unparalleled success in a number of generative tasks. In this work, we aim to build inductive biases into the training and sampling of diffusion models to better accommodate the target distribution of the data to model. For topologically structured data, we devise a frequency-based noising operator to purposefully manipulate, and set, these inductive biases. We first show that appropriate manipulations of the noising forward process can lead DPMs to focus on particular aspects of the distribution to learn. We show that different datasets necessitate different inductive biases, and that appropriate frequency-based noise control induces increased generative performance compared to standard diffusion. Finally, we demonstrate the possibility of ignoring information at particular frequencies while learning. We show this in an image corruption and recovery task, where we train a DPM to recover the original target distribution after severe noise corruption.

### ProReco: A Process Discovery Recommender System 
[[arxiv](https://arxiv.org/abs/2502.10230)] [[cool](https://papers.cool/arxiv/2502.10230)] [[pdf](https://arxiv.org/pdf/2502.10230)]
> **Authors**: Tsung-Hao Huang,Tarek Junied,Marco Pegoraro,Wil M. P. van der Aalst
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 8 pages, 5 figures, 9 references
- **标题**: None
- **领域**: 机器学习,信息检索
- **Abstract**: Process discovery aims to automatically derive process models from historical execution data (event logs). While various process discovery algorithms have been proposed in the last 25 years, there is no consensus on a dominating discovery algorithm. Selecting the most suitable discovery algorithm remains a challenge due to competing quality measures and diverse user requirements. Manually selecting the most suitable process discovery algorithm from a range of options for a given event log is a time-consuming and error-prone task. This paper introduces ProReco, a Process discovery Recommender system designed to recommend the most appropriate algorithm based on user preferences and event log characteristics. ProReco incorporates state-of-the-art discovery algorithms, extends the feature pools from previous work, and utilizes eXplainable AI (XAI) techniques to provide explanations for its recommendations.

### Comparison of Deep Recurrent Neural Networks and Bayesian Neural Networks for Detecting Electric Motor Damage Through Sound Signal Analysis 
[[arxiv](https://arxiv.org/abs/2502.10224)] [[cool](https://papers.cool/arxiv/2502.10224)] [[pdf](https://arxiv.org/pdf/2502.10224)]
> **Authors**: Waldemar Bauer,Jerzy Baranowski
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-17
> **comment**: Draft articles. arXiv admin note: substantial text overlap with arXiv:2409.08309
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Fault detection in electric motors is a critical challenge in various industries, where failures can result in significant operational disruptions. This study investigates the use of Recurrent Neural Networks (RNNs) and Bayesian Neural Networks (BNNs) for diagnosing motor damage using acoustic signal analysis. A novel approach is proposed, leveraging frequency domain representation of sound signals for enhanced diagnostic accuracy. The architectures of both RNNs and BNNs are designed and evaluated on real-world acoustic data collected from household appliances using smartphones. Experimental results demonstrate that BNNs provide superior fault detection performance, particularly for imbalanced datasets, offering more robust and interpretable predictions compared to traditional methods. The findings suggest that BNNs, with their ability to incorporate uncertainty, are well-suited for industrial diagnostic applications. Further analysis and benchmarks are suggested to explore resource efficiency and classification capabilities of these architectures.

### Forget the Data and Fine-Tuning! Just Fold the Network to Compress 
[[arxiv](https://arxiv.org/abs/2502.10216)] [[cool](https://papers.cool/arxiv/2502.10216)] [[pdf](https://arxiv.org/pdf/2502.10216)]
> **Authors**: Dong Wang,Haris Šikić,Lothar Thiele,Olga Saukh
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: This paper has been accepted by The Thirteenth International Conference onLearningRepresentations(ICLR), 2025
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: We introduce model folding, a novel data-free model compression technique that merges structurally similar neurons across layers, significantly reducing the model size without the need for fine-tuning or access to training data. Unlike existing methods, model folding preserves data statistics during compression by leveraging k-means clustering, and using novel data-free techniques to prevent variance collapse or explosion. Our theoretical framework and experiments across standard benchmarks, including ResNet18 and LLaMA-7B, demonstrate that model folding achieves comparable performance to data-driven compression techniques and outperforms recently proposed data-free methods, especially at high sparsity levels. This approach is particularly effective for compressing large-scale models, making it suitable for deployment in resource-constrained environments.

### Control-flow anomaly detection by process mining-based feature extraction and dimensionality reduction 
[[arxiv](https://arxiv.org/abs/2502.10211)] [[cool](https://papers.cool/arxiv/2502.10211)] [[pdf](https://arxiv.org/pdf/2502.10211)]
> **Authors**: Francesco Vitale,Marco Pegoraro,Wil M. P. van der Aalst,Nicola Mazzocca
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 16 pages, 9 figures, 7 tables, 56 references
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The business processes of organizations may deviate from normal control flow due to disruptive anomalies, including unknown, skipped, and wrongly-ordered activities. To identify these control-flow anomalies, process mining can check control-flow correctness against a reference process model through conformance checking, an explainable set of algorithms that allows linking any deviations with model elements. However, the effectiveness of conformance checking-based techniques is negatively affected by noisy event data and low-quality process models. To address these shortcomings and support the development of competitive and explainable conformance checking-based techniques for control-flow anomaly detection, we propose a novel process mining-based feature extraction approach with alignment-based conformance checking. This variant aligns the deviating control flow with a reference process model; the resulting alignment can be inspected to extract additional statistics such as the number of times a given activity caused mismatches. We integrate this approach into a flexible and explainable framework for developing techniques for control-flow anomaly detection. The framework combines process mining-based feature extraction and dimensionality reduction to handle high-dimensional feature sets, achieve detection effectiveness, and support explainability. The results show that the framework techniques implementing our approach outperform the baseline conformance checking-based techniques while maintaining the explainable nature of conformance checking. We also provide an explanation of why existing conformance checking-based techniques may be ineffective.

### SGS-GNN: A Supervised Graph Sparsification method for Graph Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.10208)] [[cool](https://papers.cool/arxiv/2502.10208)] [[pdf](https://arxiv.org/pdf/2502.10208)]
> **Authors**: Siddhartha Shankar Das,Naheed Anjum Arafat,Muftiqur Rahman,S M Ferdous,Alex Pothen,Mahantesh M Halappanavar
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We propose SGS-GNN, a novel supervised graph sparsifier that learns the sampling probability distribution of edges and samples sparse subgraphs of a user-specified size to reduce the computational costs required by GNNs for inference tasks on large graphs. SGS-GNN employs regularizers in the loss function to enhance homophily in sparse subgraphs, boosting the accuracy of GNNs on heterophilic graphs, where a significant number of the neighbors of a node have dissimilar labels. SGS-GNN also supports conditional updates of the probability distribution learning module based on a prior, which helps narrow the search space for sparse graphs. SGS-GNN requires fewer epochs to obtain high accuracies since it learns the search space of subgraphs more effectively than methods using fixed distributions such as random sampling. Extensive experiments using 33 homophilic and heterophilic graphs demonstrate the following: (i) with only 20% of edges retained in the sparse subgraphs, SGS-GNN improves the F1-scores by a geometric mean of 4% relative to the original graph; on heterophilic graphs, the prediction accuracy is better up to 30%. (ii) SGS-GNN outperforms state-of-the-art methods with improvement in F1-scores of 4-7% in geometric mean with similar sparsities in the sampled subgraphs, and (iii) compared to sparsifiers that employ fixed distributions, SGS-GNN requires about half the number of epochs to converge.

### Looking around you: external information enhances representations for event sequences 
[[arxiv](https://arxiv.org/abs/2502.10205)] [[cool](https://papers.cool/arxiv/2502.10205)] [[pdf](https://arxiv.org/pdf/2502.10205)]
> **Authors**: Maria Kovaleva,Petr Sokerin,Sofia Krehova,Alexey Zaytsev
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Representation learning produces models in different domains, such as store purchases, client transactions, and general people's behaviour. However, such models for sequential data usually process a single sequence, ignoring context from other relevant ones, even in domains with rapidly changing external environments like finance or misguiding the prediction for a user with no recent events. We are the first to propose a method that aggregates information from multiple user representations augmenting a specific user one for a scenario of multiple co-occurring event sequences. Our study considers diverse aggregation approaches, ranging from simple pooling techniques to trainable attention-based approaches, especially Kernel attention aggregation, that can highlight more complex information flow from other users. The proposed method operates atop an existing encoder and supports its efficient fine-tuning. Across considered datasets of financial transactions and downstream tasks, Kernel attention improves ROC AUC scores, both with and without fine-tuning, while mean pooling yields a smaller but still significant gain.

### AI-in-the-Loop Sensing and Communication Joint Design for Edge Intelligence 
[[arxiv](https://arxiv.org/abs/2502.10203)] [[cool](https://papers.cool/arxiv/2502.10203)] [[pdf](https://arxiv.org/pdf/2502.10203)]
> **Authors**: Zhijie Cai,Xiaowen Cao,Xu Chen,Yuanhao Cui,Guangxu Zhu,Kaibin Huang,Shuguang Cui
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,分布式、并行和集群计算
- **Abstract**: Recent breakthroughs in artificial intelligence (AI), wireless communications, and sensing technologies have accelerated the evolution of edge intelligence. However, conventional systems still grapple with issues such as low communication efficiency, redundant data acquisition, and poor model generalization. To overcome these challenges, we propose an innovative framework that enhances edge intelligence through AI-in-the-loop joint sensing and communication (JSAC). This framework features an AI-driven closed-loop control architecture that jointly optimizes system resources, thereby delivering superior system-level performance. A key contribution of our work is establishing an explicit relationship between validation loss and the system's tunable parameters. This insight enables dynamic reduction of the generalization error through AI-driven closed-loop control. Specifically, for sensing control, we introduce an adaptive data collection strategy based on gradient importance sampling, allowing edge devices to autonomously decide when to terminate data acquisition and how to allocate sample weights based on real-time model feedback. For communication control, drawing inspiration from stochastic gradient Langevin dynamics (SGLD), our joint optimization of transmission power and batch size converts channel and data noise into gradient perturbations that help mitigate overfitting. Experimental evaluations demonstrate that our framework reduces communication energy consumption by up to 77 percent and sensing costs measured by the number of collected samples by up to 52 percent while significantly improving model generalization -- with up to 58 percent reductions of the final validation loss. It validates that the proposed scheme can harvest the mutual benefit of AI and JSAC systems by incorporating the model itself into the control loop of the system.

### Dynamic Reinforcement Learning for Actors 
[[arxiv](https://arxiv.org/abs/2502.10200)] [[cool](https://papers.cool/arxiv/2502.10200)] [[pdf](https://arxiv.org/pdf/2502.10200)]
> **Authors**: Katsunari Shibata
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 31 pages, 20 figures
- **标题**: None
- **领域**: 机器学习,人工智能,神经和进化计算
- **Abstract**: Dynamic Reinforcement Learning (Dynamic RL), proposed in this paper, directly controls system dynamics, instead of the actor (action-generating neural network) outputs at each moment, bringing about a major qualitative shift in reinforcement learning (RL) from static to dynamic. The actor is initially designed to generate chaotic dynamics through the loop with its environment, enabling the agent to perform flexible and deterministic exploration. Dynamic RL controls global system dynamics using a local index called "sensitivity," which indicates how much the input neighborhood contracts or expands into the corresponding output neighborhood through each neuron's processing. While sensitivity adjustment learning (SAL) prevents excessive convergence of the dynamics, sensitivity-controlled reinforcement learning (SRL) adjusts them -- to converge more to improve reproducibility around better state transitions with positive TD error and to diverge more to enhance exploration around worse transitions with negative TD error. Dynamic RL was applied only to the actor in an Actor-Critic RL architecture while applying it to the critic remains a challenge. It was tested on two dynamic tasks and functioned effectively without external exploration noise or backward computation through time. Moreover, it exhibited excellent adaptability to new environments, although some problems remain. Drawing parallels between 'exploration' and 'thinking,' the author hypothesizes that "exploration grows into thinking through learning" and believes this RL could be a key technique for the emergence of thinking, including inspiration that cannot be reconstructed from massive existing text data. Finally, despite being presumptuous, the author presents the argument that this research should not proceed due to its potentially fatal risks, aiming to encourage discussion.

### A Powerful Random Forest Featuring Linear Extensions (RaFFLE) 
[[arxiv](https://arxiv.org/abs/2502.10185)] [[cool](https://papers.cool/arxiv/2502.10185)] [[pdf](https://arxiv.org/pdf/2502.10185)]
> **Authors**: Jakob Raymaekers,Peter J. Rousseeuw,Thomas Servotte,Tim Verdonck,Ruicong Yao
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Random forests are widely used in regression. However, the decision trees used as base learners are poor approximators of linear relationships. To address this limitation we propose RaFFLE (Random Forest Featuring Linear Extensions), a novel framework that integrates the recently developed PILOT trees (Piecewise Linear Organic Trees) as base learners within a random forest ensemble. PILOT trees combine the computational efficiency of traditional decision trees with the flexibility of linear model trees. To ensure sufficient diversity of the individual trees, we introduce an adjustable regularization parameter and use node-level feature sampling. These modifications improve the accuracy of the forest. We establish theoretical guarantees for the consistency of RaFFLE under weak conditions, and its faster convergence when the data are generated by a linear model. Empirical evaluations on 136 regression datasets demonstrate that RaFFLE outperforms the classical CART and random forest methods, the regularized linear methods Lasso and Ridge, and the state-of-the-art XGBoost algorithm, across both linear and nonlinear datasets. By balancing predictive accuracy and computational efficiency, RaFFLE proves to be a versatile tool for tackling a wide variety of regression problems.

### Realistic Evaluation of Deep Partial-Label Learning Algorithms 
[[arxiv](https://arxiv.org/abs/2502.10184)] [[cool](https://papers.cool/arxiv/2502.10184)] [[pdf](https://arxiv.org/pdf/2502.10184)]
> **Authors**: Wei Wang,Dong-Dong Wu,Jindong Wang,Gang Niu,Min-Ling Zhang,Masashi Sugiyama
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: ICLR 2025 Spotlight
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Partial-label learning (PLL) is a weakly supervised learning problem in which each example is associated with multiple candidate labels and only one is the true label. In recent years, many deep PLL algorithms have been developed to improve model performance. However, we find that some early developed algorithms are often underestimated and can outperform many later algorithms with complicated designs. In this paper, we delve into the empirical perspective of PLL and identify several critical but previously overlooked issues. First, model selection for PLL is non-trivial, but has never been systematically studied. Second, the experimental settings are highly inconsistent, making it difficult to evaluate the effectiveness of the algorithms. Third, there is a lack of real-world image datasets that can be compatible with modern network architectures. Based on these findings, we propose PLENCH, the first Partial-Label learning bENCHmark to systematically compare state-of-the-art deep PLL algorithms. We investigate the model selection problem for PLL for the first time, and propose novel model selection criteria with theoretical guarantees. We also create Partial-Label CIFAR-10 (PLCIFAR10), an image dataset of human-annotated partial labels collected from Amazon Mechanical Turk, to provide a testbed for evaluating the performance of PLL algorithms in more realistic scenarios. Researchers can quickly and conveniently perform a comprehensive and fair evaluation and verify the effectiveness of newly developed algorithms based on PLENCH. We hope that PLENCH will facilitate standardized, fair, and practical evaluation of PLL algorithms in the future.

### From Markov to Laplace: How Mamba In-Context Learns Markov Chains 
[[arxiv](https://arxiv.org/abs/2502.10178)] [[cool](https://papers.cool/arxiv/2502.10178)] [[pdf](https://arxiv.org/pdf/2502.10178)]
> **Authors**: Marco Bondaschi,Nived Rajaraman,Xiuying Wei,Kannan Ramchandran,Razvan Pascanu,Caglar Gulcehre,Michael Gastpar,Ashok Vardhan Makkuva
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,信息论
- **Abstract**: While transformer-based language models have driven the AI revolution thus far, their computational complexity has spurred growing interest in viable alternatives, such as structured state space sequence models (SSMs) and Selective SSMs. Among these, Mamba (S6) and its variant Mamba-2 have shown remarkable inference speed ups over transformers while achieving comparable or superior performance on complex language modeling tasks. However, despite these architectural innovations and empirical successes, the fundamental learning capabilities of Mamba remain poorly understood. In this paper, we address this gap by studying in-context learning (ICL) on Markov chains and uncovering a surprising phenomenon: unlike transformers, even a single-layer Mamba efficiently learns the in-context Laplacian smoothing estimator, which is both Bayes and minimax optimal, for all Markovian orders. To explain this, we theoretically characterize the representation capacity of Mamba and reveal the fundamental role of convolution in enabling it to represent the optimal Laplacian smoothing. These theoretical insights align strongly with empirical results and, to the best of our knowledge, represent the first formal connection between Mamba and optimal statistical estimators. Finally, we outline promising research directions inspired by these findings.

### Revisiting Generalization Power of a DNN in Terms of Symbolic Interactions 
[[arxiv](https://arxiv.org/abs/2502.10162)] [[cool](https://papers.cool/arxiv/2502.10162)] [[pdf](https://arxiv.org/pdf/2502.10162)]
> **Authors**: Lei Cheng,Junpeng Zhang,Qihan Ren,Quanshi Zhang
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: arXiv admin note: text overlap with arXiv:2407.19198
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,计算机视觉和模式识别
- **Abstract**: This paper aims to analyze the generalization power of deep neural networks (DNNs) from the perspective of interactions. Unlike previous analysis of a DNN's generalization power in a highdimensional feature space, we find that the generalization power of a DNN can be explained as the generalization power of the interactions. We found that the generalizable interactions follow a decay-shaped distribution, while non-generalizable interactions follow a spindle-shaped distribution. Furthermore, our theory can effectively disentangle these two types of interactions from a DNN. We have verified that our theory can well match real interactions in a DNN in experiments.

### Provably Efficient RL under Episode-Wise Safety in Constrained MDPs with Linear Function Approximation 
[[arxiv](https://arxiv.org/abs/2502.10138)] [[cool](https://papers.cool/arxiv/2502.10138)] [[pdf](https://arxiv.org/pdf/2502.10138)]
> **Authors**: Toshinori Kitamura,Arnob Ghosh,Tadashi Kozuno,Wataru Kumagai,Kazumi Kasaura,Kenta Hoshino,Yohei Hosoe,Yutaka Matsuo
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We study the reinforcement learning (RL) problem in a constrained Markov decision process (CMDP), where an agent explores the environment to maximize the expected cumulative reward while satisfying a single constraint on the expected total utility value in every episode. While this problem is well understood in the tabular setting, theoretical results for function approximation remain scarce. This paper closes the gap by proposing an RL algorithm for linear CMDPs that achieves $\tilde{\mathcal{O}}(\sqrt{K})$ regret with an episode-wise zero-violation guarantee. Furthermore, our method is computationally efficient, scaling polynomially with problem-dependent parameters while remaining independent of the state space size. Our results significantly improve upon recent linear CMDP algorithms, which either violate the constraint or incur exponential computational costs.

### Learning Relational Tabular Data without Shared Features 
[[arxiv](https://arxiv.org/abs/2502.10125)] [[cool](https://papers.cool/arxiv/2502.10125)] [[pdf](https://arxiv.org/pdf/2502.10125)]
> **Authors**: Zhaomin Wu,Shida Wang,Ziyang Wang,Bingsheng He
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Learning relational tabular data has gained significant attention recently, but most studies focus on single tables, overlooking the potential of cross-table learning. Cross-table learning, especially in scenarios where tables lack shared features and pre-aligned data, offers vast opportunities but also introduces substantial challenges. The alignment space is immense, and determining accurate alignments between tables is highly complex. We propose Latent Entity Alignment Learning (Leal), a novel framework enabling effective cross-table training without requiring shared features or pre-aligned data. Leal operates on the principle that properly aligned data yield lower loss than misaligned data, a concept embodied in its soft alignment mechanism. This mechanism is coupled with a differentiable cluster sampler module, ensuring efficient scaling to large relational tables. Furthermore, we provide a theoretical proof of the cluster sampler's approximation capacity. Extensive experiments on five real-world and five synthetic datasets show that Leal achieves up to a 26.8% improvement in predictive performance compared to state-of-the-art methods, demonstrating its effectiveness and scalability.

### Modern Hopfield Networks with Continuous-Time Memories 
[[arxiv](https://arxiv.org/abs/2502.10122)] [[cool](https://papers.cool/arxiv/2502.10122)] [[pdf](https://arxiv.org/pdf/2502.10122)]
> **Authors**: Saul Santos,António Farinhas,Daniel C. McNamee,André F. T. Martins
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Recent research has established a connection between modern Hopfield networks (HNs) and transformer attention heads, with guarantees of exponential storage capacity. However, these models still face challenges scaling storage efficiently. Inspired by psychological theories of continuous neural resource allocation in working memory, we propose an approach that compresses large discrete Hopfield memories into smaller, continuous-time memories. Leveraging continuous attention, our new energy function modifies the update rule of HNs, replacing the traditional softmax-based probability mass function with a probability density, over the continuous memory. This formulation aligns with modern perspectives on human executive function, offering a principled link between attractor dynamics in working memory and resource-efficient memory allocation. Our framework maintains competitive performance with HNs while leveraging a compressed memory, reducing computational costs across synthetic and video datasets.

### SeWA: Selective Weight Average via Probabilistic Masking 
[[arxiv](https://arxiv.org/abs/2502.10119)] [[cool](https://papers.cool/arxiv/2502.10119)] [[pdf](https://arxiv.org/pdf/2502.10119)]
> **Authors**: Peng Wang,Shengchao Hu,Zerui Tao,Guoxia Wang,Dianhai Yu,Li Shen,Quan Zheng,Dacheng Tao
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Weight averaging has become a standard technique for enhancing model performance. However, methods such as Stochastic Weight Averaging (SWA) and Latest Weight Averaging (LAWA) often require manually designed procedures to sample from the training trajectory, and the results depend heavily on hyperparameter tuning. To minimize human effort, this paper proposes a simple yet efficient algorithm called Selective Weight Averaging (SeWA), which adaptively selects checkpoints during the final stages of training for averaging. Based on SeWA, we show that only a few points are needed to achieve better generalization and faster convergence. Theoretically, solving the discrete subset selection problem is inherently challenging. To address this, we transform it into a continuous probabilistic optimization framework and employ the Gumbel-Softmax estimator to learn the non-differentiable mask for each checkpoint. Further, we theoretically derive the SeWA's stability-based generalization bounds, which are sharper than that of SGD under both convex and non-convex assumptions. Finally, solid extended experiments in various domains, including behavior cloning, image classification, and text classification, further validate the effectiveness of our approach.

### Accelerometry-based Energy Expenditure Estimation During Activities of Daily Living: A Comparison Among Different Accelerometer Compositions 
[[arxiv](https://arxiv.org/abs/2502.10112)] [[cool](https://papers.cool/arxiv/2502.10112)] [[pdf](https://arxiv.org/pdf/2502.10112)]
> **Authors**: Shuhao Que,Remco Poelarends,Peter Veltink,Miriam Vollenbroek-Hutten,Ying Wang
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: This work has been submitted to the IEEE for possible publication
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Physical activity energy expenditure (PAEE) can be measured from breath-by-breath respiratory data, which can serve as a reference. Alternatively, PAEE can be predicted from the body movements, which can be measured and estimated with accelerometers. The body center of mass (COM) acceleration reflects the movements of the whole body and thus serves as a good predictor for PAEE. However, the wrist has also become a popular location due to recent advancements in wrist-worn devices. Therefore, in this work, using the respiratory data measured by COSMED K5 as the reference, we evaluated and compared the performances of COM-based settings and wrist-based settings. The COM-based settings include two different accelerometer compositions, using only the pelvis accelerometer (pelvis-acc) and the pelvis accelerometer with two accelerometers from two thighs (3-acc). The wrist-based settings include using only the left wrist accelerometer (l-wrist-acc) and only the right wrist accelerometer (r-wrist-acc). We implemented two existing PAEE estimation methods on our collected dataset, where 9 participants performed activities of daily living while wearing 5 accelerometers (i.e., pelvis, two thighs, and two wrists). These two methods include a linear regression (LR) model and a CNN-LSTM model. Both models yielded the best results with the COM-based 3-acc setting (LR: $R^2$ = 0.41, CNN-LSTM: $R^2$ = 0.53). No significant difference was found between the 3-acc and pelvis-acc settings (p-value = 0.278). For both models, neither the l-wrist-acc nor the r-wrist-acc settings demonstrated predictive power on PAEE with $R^2$ values close to 0, significantly outperformed by the two COM-based settings (p-values $<$ 0.05). No significant difference was found between the two wrists (p-value = 0.329).

### COMBINEX: A Unified Counterfactual Explainer for Graph Neural Networks via Node Feature and Structural Perturbations 
[[arxiv](https://arxiv.org/abs/2502.10111)] [[cool](https://papers.cool/arxiv/2502.10111)] [[pdf](https://arxiv.org/pdf/2502.10111)]
> **Authors**: Flavio Giorgi,Fabrizio Silvestri,Gabriele Tolomei
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Counterfactual explanations have emerged as a powerful tool to unveil the opaque decision-making processes of graph neural networks (GNNs). However, existing techniques primarily focus on edge modifications, often overlooking the crucial role of node feature perturbations in shaping model predictions. To address this limitation, we propose COMBINEX, a novel GNN explainer that generates counterfactual explanations for both node and graph classification tasks. Unlike prior methods, which treat structural and feature-based changes independently, COMBINEX optimally balances modifications to edges and node features by jointly optimizing these perturbations. This unified approach ensures minimal yet effective changes required to flip a model's prediction, resulting in realistic and interpretable counterfactuals. Additionally, COMBINEX seamlessly handles both continuous and discrete node features, enhancing its versatility across diverse datasets and GNN architectures. Extensive experiments on real-world datasets and various GNN architectures demonstrate the effectiveness and robustness of our approach over existing baselines.

### NeuroXVocal: Detection and Explanation of Alzheimer's Disease through Non-invasive Analysis of Picture-prompted Speech 
[[arxiv](https://arxiv.org/abs/2502.10108)] [[cool](https://papers.cool/arxiv/2502.10108)] [[pdf](https://arxiv.org/pdf/2502.10108)]
> **Authors**: Nikolaos Ntampakis,Konstantinos Diamantaras,Ioanna Chouvarda,Magda Tsolaki,Vasileios Argyriou,Panagiotis Sarigianndis
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,神经元和认知
- **Abstract**: The early diagnosis of Alzheimer's Disease (AD) through non invasive methods remains a significant healthcare challenge. We present NeuroXVocal, a novel dual-component system that not only classifies but also explains potential AD cases through speech analysis. The classification component (Neuro) processes three distinct data streams: acoustic features capturing speech patterns and voice characteristics, textual features extracted from speech transcriptions, and precomputed embeddings representing linguistic patterns. These streams are fused through a custom transformer-based architecture that enables robust cross-modal interactions. The explainability component (XVocal) implements a Retrieval-Augmented Generation (RAG) approach, leveraging Large Language Models combined with a domain-specific knowledge base of AD research literature. This architecture enables XVocal to retrieve relevant clinical studies and research findings to generate evidence-based context-sensitive explanations of the acoustic and linguistic markers identified in patient speech. Using the IS2021 ADReSSo Challenge benchmark dataset, our system achieved state-of-the-art performance with 95.77% accuracy in AD classification, significantly outperforming previous approaches. The explainability component was qualitatively evaluated using a structured questionnaire completed by medical professionals, validating its clinical relevance. NeuroXVocal's unique combination of high-accuracy classification and interpretable, literature-grounded explanations demonstrates its potential as a practical tool for supporting clinical AD diagnosis.

### Data-Adaptive Low-Rank Sparse Subspace Clustering 
[[arxiv](https://arxiv.org/abs/2502.10106)] [[cool](https://papers.cool/arxiv/2502.10106)] [[pdf](https://arxiv.org/pdf/2502.10106)]
> **Authors**: Ivica Kopriva
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 5 pages, 1 figure, 1 table
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Low-rank sparse subspace clustering (LRSSC) algorithms built on self-expressive model effectively capture both the global and local structure of the data. However, existing solutions, primarily based on proximal operators associated with Sp/Lp , p e {0, 1/2, 2/3, 1}, norms are not data-adaptive. In this work, we propose an LRSSC algorithm incorporating a data-adaptive surrogate for the S0/L0 quasi-norm. We provide a numerical solution for the corresponding proximal operator in cases where an analytical expression is unavailable. The proposed LRSSC algorithm is formulated within the proximal mapping framework, and we present theoretical proof of its global convergence toward a stationary point. We evaluate the performance of the proposed method on three well known datasets, comparing it against LRSSC algorithms constrained by Sp/Lp, p e {0, 1/2, 2/3, 1}, norms.

### Representation Learning on Out of Distribution in Tabular Data 
[[arxiv](https://arxiv.org/abs/2502.10095)] [[cool](https://papers.cool/arxiv/2502.10095)] [[pdf](https://arxiv.org/pdf/2502.10095)]
> **Authors**: Achmad Ginanjar,Xue Li,Priyanka Singh,Wen Hua
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The open-world assumption in model development suggests that a model might lack sufficient information to adequately handle data that is entirely distinct or out of distribution (OOD). While deep learning methods have shown promising results in handling OOD data through generalization techniques, they often require specialized hardware that may not be accessible to all users. We present TCL, a lightweight yet effective solution that operates efficiently on standard CPU hardware. Our approach adapts contrastive learning principles specifically for tabular data structures, incorporating full matrix augmentation and simplified loss calculation. Through comprehensive experiments across 10 diverse datasets, we demonstrate that TCL outperforms existing models, including FT-Transformer and ResNet, particularly in classification tasks, while maintaining competitive performance in regression problems. TCL achieves these results with significantly reduced computational requirements, making it accessible to users with limited hardware capabilities. This study also provides practical guidance for detecting and evaluating OOD data through straightforward experiments and visualizations. Our findings show that TCL offers a promising balance between performance and efficiency in handling OOD prediction tasks, which is particularly beneficial for general machine learning practitioners working with computational constraints.

### A novel approach to data generation in generative model 
[[arxiv](https://arxiv.org/abs/2502.10092)] [[cool](https://papers.cool/arxiv/2502.10092)] [[pdf](https://arxiv.org/pdf/2502.10092)]
> **Authors**: JaeHong Kim,Jaewon Shim
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 47 pages, 2 tables, 9 figures
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Variational Autoencoders (VAEs) and other generative models are widely employed in artificial intelligence to synthesize new data. However, current approaches rely on Euclidean geometric assumptions and statistical approximations that fail to capture the structured and emergent nature of data generation. This paper introduces the Convergent Fusion Paradigm (CFP) theory, a novel geometric framework that redefines data generation by integrating dimensional expansion accompanied by qualitative transformation. By modifying the latent space geometry to interact with emergent high-dimensional structures, CFP theory addresses key challenges such as identifiability issues and unintended artifacts like hallucinations in Large Language Models (LLMs). CFP theory is based on two key conceptual hypotheses that redefine how generative models structure relationships between data and algorithms. Through the lens of CFP theory, we critically examine existing metric-learning approaches. CFP theory advances this perspective by introducing time-reversed metric embeddings and structural convergence mechanisms, leading to a novel geometric approach that better accounts for data generation as a structured epistemic process. Beyond its computational implications, CFP theory provides philosophical insights into the ontological underpinnings of data generation. By offering a systematic framework for high-dimensional learning dynamics, CFP theory contributes to establishing a theoretical foundation for understanding the data-relationship structures in AI. Finally, future research in CFP theory will be led to its implications for fully realizing qualitative transformations, introducing the potential of Hilbert space in generative modeling.

### A Hybrid Edge Classifier: Combining TinyML-Optimised CNN with RRAM-CMOS ACAM for Energy-Efficient Inference 
[[arxiv](https://arxiv.org/abs/2502.10089)] [[cool](https://papers.cool/arxiv/2502.10089)] [[pdf](https://arxiv.org/pdf/2502.10089)]
> **Authors**: Kieran Woodward,Eiman Kanjo,Georgios Papandroulidakis,Shady Agwa,Themis Prodromakis
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,硬件架构
- **Abstract**: In recent years, the development of smart edge computing systems to process information locally is on the rise. Many near-sensor machine learning (ML) approaches have been implemented to introduce accurate and energy efficient template matching operations in resource-constrained edge sensing systems, such as wearables. To introduce novel solutions that can be viable for extreme edge cases, hybrid solutions combining conventional and emerging technologies have started to be proposed. Deep Neural Networks (DNN) optimised for edge application alongside new approaches of computing (both device and architecture -wise) could be a strong candidate in implementing edge ML solutions that aim at competitive accuracy classification while using a fraction of the power of conventional ML solutions. In this work, we are proposing a hybrid software-hardware edge classifier aimed at the extreme edge near-sensor systems. The classifier consists of two parts: (i) an optimised digital tinyML network, working as a front-end feature extractor, and (ii) a back-end RRAM-CMOS analogue content addressable memory (ACAM), working as a final stage template matching system. The combined hybrid system exhibits a competitive trade-off in accuracy versus energy metric with $E_{front-end}$ = $96.23 nJ$ and $E_{back-end}$ = $1.45 nJ$ for each classification operation compared with 78.06$μ$J for the original teacher model, representing a 792-fold reduction, making it a viable solution for extreme edge applications.

### Classification of Temporal Graphs using Persistent Homology 
[[arxiv](https://arxiv.org/abs/2502.10076)] [[cool](https://papers.cool/arxiv/2502.10076)] [[pdf](https://arxiv.org/pdf/2502.10076)]
> **Authors**: Siddharth Pritam,Rohit Roy,Madhav Cherupilil Sajeev
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算几何,代数拓扑
- **Abstract**: Temporal graphs effectively model dynamic systems by representing interactions as timestamped edges. However, analytical tools for temporal graphs are limited compared to static graphs. We propose a novel method for analyzing temporal graphs using Persistent Homology. Our approach leverages $δ$-temporal motifs (recurrent subgraphs) to capture temporal dynamics %without aggregation . By evolving these motifs, we define the \textit{average filtration} and compute PH on the associated clique complex. This method captures both local and global temporal structures and is stable with respect to reference models. We demonstrate the applicability of our approach to the temporal graph classification task. Experiments verify the effectiveness of our approach, achieving over 92\% accuracy, with some cases reaching 100\%. Unlike existing methods that require node classes, our approach is node class free, offering flexibility for a wide range of temporal graph analysis.

### Heterogeneous Resource Allocation with Multi-task Learning for Wireless Networks 
[[arxiv](https://arxiv.org/abs/2502.10027)] [[cool](https://papers.cool/arxiv/2502.10027)] [[pdf](https://arxiv.org/pdf/2502.10027)]
> **Authors**: Nikos A. Mitsiou,Pavlos S. Bouzinis,Panagiotis G. Sarigiannidis,George K. Karagiannidis
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The optimal solution to an optimization problem depends on the problem's objective function, constraints, and size. While deep neural networks (DNNs) have proven effective in solving optimization problems, changes in the problem's size, objectives, or constraints often require adjustments to the DNN architecture to maintain effectiveness, or even retraining a new DNN from scratch. Given the dynamic nature of wireless networks, which involve multiple and diverse objectives that can have conflicting requirements and constraints, we propose a multi-task learning (MTL) framework to enable a single DNN to jointly solve a range of diverse optimization problems. In this framework, optimization problems with varying dimensionality values, objectives, and constraints are treated as distinct tasks. To jointly address these tasks, we propose a conditional computation-based MTL approach with routing. The multi-task DNN consists of two components, the base DNN (bDNN), which is the single DNN used to extract the solutions for all considered optimization problems, and the routing DNN (rDNN), which manages which nodes and layers of the bDNN to be used during the forward propagation of each task. The output of the rDNN is a binary vector which is multiplied with all bDNN's weights during the forward propagation, creating a unique computational path through the bDNN for each task. This setup allows the tasks to either share parameters or use independent ones, with the decision controlled by the rDNN. The proposed framework supports both supervised and unsupervised learning scenarios. Numerical results demonstrate the efficiency of the proposed MTL approach in solving diverse optimization problems. In contrast, benchmark DNNs lacking the rDNN mechanism were unable to achieve similar levels of performance, highlighting the effectiveness of the proposed architecture.

### Exploring Neural Granger Causality with xLSTMs: Unveiling Temporal Dependencies in Complex Data 
[[arxiv](https://arxiv.org/abs/2502.09981)] [[cool](https://papers.cool/arxiv/2502.09981)] [[pdf](https://arxiv.org/pdf/2502.09981)]
> **Authors**: Harsh Poonia,Felix Divo,Kristian Kersting,Devendra Singh Dhami
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: :I.2.6
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Causality in time series can be difficult to determine, especially in the presence of non-linear dependencies. The concept of Granger causality helps analyze potential relationships between variables, thereby offering a method to determine whether one time series can predict-Granger cause-future values of another. Although successful, Granger causal methods still struggle with capturing long-range relations between variables. To this end, we leverage the recently successful Extended Long Short-Term Memory (xLSTM) architecture and propose Granger causal xLSTMs (GC-xLSTM). It first enforces sparsity between the time series components by using a novel dynamic lass penalty on the initial projection. Specifically, we adaptively improve the model and identify sparsity candidates. Our joint optimization procedure then ensures that the Granger causal relations are recovered in a robust fashion. Our experimental evaluations on three datasets demonstrate the overall efficacy of our proposed GC-xLSTM model.

### Data Valuation using Neural Networks for Efficient Instruction Fine-Tuning 
[[arxiv](https://arxiv.org/abs/2502.09969)] [[cool](https://papers.cool/arxiv/2502.09969)] [[pdf](https://arxiv.org/pdf/2502.09969)]
> **Authors**: Ishika Agarwal,Dilek Hakkani-Tür
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Influence functions provide crucial insights into model training, but existing methods suffer from large computational costs and limited generalization. Particularly, recent works have proposed various metrics and algorithms to calculate the influence of data using language models, which do not scale well with large models and datasets. This is because of the expensive forward and backward passes required for computation, substantial memory requirements to store large models, and poor generalization of influence estimates to new data. In this paper, we explore the use of small neural networks -- which we refer to as the InfluenceNetwork -- to estimate influence values, achieving up to 99% cost reduction. Our evaluation demonstrates that influence values can be estimated with models just 0.0027% the size of full language models (we use 7B and 8B versions). We apply our algorithm of estimating influence values (called NN-CIFT: Neural Networks for effiCient Instruction Fine-Tuning) to the downstream task of subset selection for general instruction fine-tuning. In our study, we include four state-of-the-art influence functions and show no compromise in performance, despite large speedups, between NN-CIFT and the original influence functions. We provide an in-depth hyperparameter analyses of NN-CIFT. The code for our method can be found here: https://github.com/agarwalishika/NN-CIFT.

### On Space Folds of ReLU Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.09954)] [[cool](https://papers.cool/arxiv/2502.09954)] [[pdf](https://arxiv.org/pdf/2502.09954)]
> **Authors**: Michal Lewandowski,Hamid Eghbalzadeh,Bernhard Heinzl,Raphael Pisoni,Bernhard A. Moser
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: Accepted at Transactions onMachineLearningResearch (TMLR), 2025
- **标题**: None
- **领域**: 机器学习,神经和进化计算
- **Abstract**: Recent findings suggest that the consecutive layers of ReLU neural networks can be understood geometrically as space folding transformations of the input space, revealing patterns of self-similarity. In this paper, we present the first quantitative analysis of this space folding phenomenon in ReLU neural networks. Our approach focuses on examining how straight paths in the Euclidean input space are mapped to their counterparts in the Hamming activation space. In this process, the convexity of straight lines is generally lost, giving rise to non-convex folding behavior. To quantify this effect, we introduce a novel measure based on range metrics, similar to those used in the study of random walks, and provide the proof for the equivalence of convexity notions between the input and activation spaces. Furthermore, we provide empirical analysis on a geometrical analysis benchmark (CantorNet) as well as an image classification benchmark (MNIST). Our work advances the understanding of the activation space in ReLU neural networks by leveraging the phenomena of geometric folding, providing valuable insights on how these models process input information.

### Self-Supervised Learning for Neural Topic Models with Variance-Invariance-Covariance Regularization 
[[arxiv](https://arxiv.org/abs/2502.09944)] [[cool](https://papers.cool/arxiv/2502.09944)] [[pdf](https://arxiv.org/pdf/2502.09944)]
> **Authors**: Weiran Xu,Kengo Hirami,Koji Eguchi
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: Preprint accepted in Springer Knowledge and Information Systems (KAIS), in press
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: In our study, we propose a self-supervised neural topic model (NTM) that combines the power of NTMs and regularized self-supervised learning methods to improve performance. NTMs use neural networks to learn latent topics hidden behind the words in documents, enabling greater flexibility and the ability to estimate more coherent topics compared to traditional topic models. On the other hand, some self-supervised learning methods use a joint embedding architecture with two identical networks that produce similar representations for two augmented versions of the same input. Regularizations are applied to these representations to prevent collapse, which would otherwise result in the networks outputting constant or redundant representations for all inputs. Our model enhances topic quality by explicitly regularizing latent topic representations of anchor and positive samples. We also introduced an adversarial data augmentation method to replace the heuristic sampling method. We further developed several variation models including those on the basis of an NTM that incorporates contrastive learning with both positive and negative samples. Experimental results on three datasets showed that our models outperformed baselines and state-of-the-art models both quantitatively and qualitatively.

### Fused Partial Gromov-Wasserstein for Structured Objects 
[[arxiv](https://arxiv.org/abs/2502.09934)] [[cool](https://papers.cool/arxiv/2502.09934)] [[pdf](https://arxiv.org/pdf/2502.09934)]
> **Authors**: Yikun Bai,Huy Tran,Hengrong Du,Xinran Liu,Soheil Kolouri
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: arXiv admin note: text overlap with arXiv:2402.03664
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Structured data, such as graphs, are vital in machine learning due to their capacity to capture complex relationships and interactions. In recent years, the Fused Gromov-Wasserstein (FGW) distance has attracted growing interest because it enables the comparison of structured data by jointly accounting for feature similarity and geometric structure. However, as a variant of optimal transport (OT), classical FGW assumes an equal mass constraint on the compared data. In this work, we relax this mass constraint and propose the Fused Partial Gromov-Wasserstein (FPGW) framework, which extends FGW to accommodate unbalanced data. Theoretically, we establish the relationship between FPGW and FGW and prove the metric properties of FPGW. Numerically, we introduce Frank-Wolfe solvers for the proposed FPGW framework and provide a convergence analysis. Finally, we evaluate the FPGW distance through graph classification and clustering experiments, demonstrating its robust performance, especially when data is corrupted by outlier noise.

### Robust Anomaly Detection via Tensor Chidori Pseudoskeleton Decomposition 
[[arxiv](https://arxiv.org/abs/2502.09926)] [[cool](https://papers.cool/arxiv/2502.09926)] [[pdf](https://arxiv.org/pdf/2502.09926)]
> **Authors**: Bowen Su
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Anomaly detection plays a critical role in modern data-driven applications, from identifying fraudulent transactions and safeguarding network infrastructure to monitoring sensor systems for irregular patterns. Traditional approaches, such as distance, density, or cluster-based methods, face significant challenges when applied to high dimensional tensor data, where complex interdependencies across dimensions amplify noise and computational complexity. To address these limitations, this paper leverages Tensor Chidori pseudoskeleton decomposition within a tensor-robust principal component analysis framework to extract low Tucker rank structure while isolating sparse anomalies, ensuring robustness to anomaly detection. We establish theoretical results regarding convergence, and estimation error, demonstrating the stability and accuracy of the proposed approach. Numerical experiments on real-world spatiotemporal data from New York City taxi trip records validate the superiority of the proposed method in detecting anomalous urban events compared to existing benchmark methods. The results underscore the potential of Tensor Chidori pseudoskeleton decomposition to enhance anomaly detection for large-scale, high-dimensional data.

### AttenGluco: Multimodal Transformer-Based Blood Glucose Forecasting on AI-READI Dataset 
[[arxiv](https://arxiv.org/abs/2502.09919)] [[cool](https://papers.cool/arxiv/2502.09919)] [[pdf](https://arxiv.org/pdf/2502.09919)]
> **Authors**: Ebrahim Farahmand,Reza Rahimi Azghan,Nooshin Taheri Chatrudi,Eric Kim,Gautham Krishna Gudur,Edison Thomaz,Giulia Pedrielli,Pavan Turaga,Hassan Ghasemzadeh
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Diabetes is a chronic metabolic disorder characterized by persistently high blood glucose levels (BGLs), leading to severe complications such as cardiovascular disease, neuropathy, and retinopathy. Predicting BGLs enables patients to maintain glucose levels within a safe range and allows caregivers to take proactive measures through lifestyle modifications. Continuous Glucose Monitoring (CGM) systems provide real-time tracking, offering a valuable tool for monitoring BGLs. However, accurately forecasting BGLs remains challenging due to fluctuations due to physical activity, diet, and other factors. Recent deep learning models show promise in improving BGL prediction. Nonetheless, forecasting BGLs accurately from multimodal, irregularly sampled data over long prediction horizons remains a challenging research problem. In this paper, we propose AttenGluco, a multimodal Transformer-based framework for long-term blood glucose prediction. AttenGluco employs cross-attention to effectively integrate CGM and activity data, addressing challenges in fusing data with different sampling rates. Moreover, it employs multi-scale attention to capture long-term dependencies in temporal data, enhancing forecasting accuracy. To evaluate the performance of AttenGluco, we conduct forecasting experiments on the recently released AIREADI dataset, analyzing its predictive accuracy across different subject cohorts including healthy individuals, people with prediabetes, and those with type 2 diabetes. Furthermore, we investigate its performance improvements and forgetting behavior as new cohorts are introduced. Our evaluations show that AttenGluco improves all error metrics, such as root mean square error (RMSE), mean absolute error (MAE), and correlation, compared to the multimodal LSTM model. AttenGluco outperforms this baseline model by about 10% and 15% in terms of RMSE and MAE, respectively.

## 多代理系统(cs.MA:Multiagent Systems)

### Learning to Solve the Min-Max Mixed-Shelves Picker-Routing Problem via Hierarchical and Parallel Decoding 
[[arxiv](https://arxiv.org/abs/2502.10233)] [[cool](https://papers.cool/arxiv/2502.10233)] [[pdf](https://arxiv.org/pdf/2502.10233)]
> **Authors**: Laurin Luttmann,Lin Xie
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 多代理系统,机器学习,机器学习
- **Abstract**: The Mixed-Shelves Picker Routing Problem (MSPRP) is a fundamental challenge in warehouse logistics, where pickers must navigate a mixed-shelves environment to retrieve SKUs efficiently. Traditional heuristics and optimization-based approaches struggle with scalability, while recent machine learning methods often rely on sequential decision-making, leading to high solution latency and suboptimal agent coordination. In this work, we propose a novel hierarchical and parallel decoding approach for solving the min-max variant of the MSPRP via multi-agent reinforcement learning. While our approach generates a joint distribution over agent actions, allowing for fast decoding and effective picker coordination, our method introduces a sequential action selection to avoid conflicts in the multi-dimensional action space. Experiments show state-of-the-art performance in both solution quality and inference speed, particularly for large-scale and out-of-distribution instances. Our code is publicly available at http://github.com/LTluttmann/marl4msprp.

## 神经和进化计算(cs.NE:Neural and Evolutionary Computing)

### Cognitive Neural Architecture Search Reveals Hierarchical Entailment 
[[arxiv](https://arxiv.org/abs/2502.11141)] [[cool](https://papers.cool/arxiv/2502.11141)] [[pdf](https://arxiv.org/pdf/2502.11141)]
> **Authors**: Lukas Kuhn,Sari Saba-Sadiya,Gemma Roig
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 神经和进化计算,人工智能,定量方法
- **Abstract**: Recent research has suggested that the brain is more shallow than previously thought, challenging the traditionally assumed hierarchical structure of the ventral visual pathway. Here, we demonstrate that optimizing convolutional network architectures for brain-alignment via evolutionary neural architecture search results in models with clear representational hierarchies. Despite having random weights, the identified models achieve brain-alignment scores surpassing even those of pretrained classification models - as measured by both regression and representational similarity analysis. Furthermore, through traditional supervised training, architectures optimized for alignment with late ventral regions become competitive classification models. These findings suggest that hierarchical structure is a fundamental mechanism of primate visual processing. Finally, this work demonstrates the potential of neural architecture search as a framework for computational cognitive neuroscience research that could reduce the field's reliance on manually designed convolutional networks.

### MERGE$^3$: Efficient Evolutionary Merging on Consumer-grade GPUs 
[[arxiv](https://arxiv.org/abs/2502.10436)] [[cool](https://papers.cool/arxiv/2502.10436)] [[pdf](https://arxiv.org/pdf/2502.10436)]
> **Authors**: Tommaso Mencattini,Adrian Robert Minut,Donato Crisostomi,Andrea Santilli,Emanuele Rodolà
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-17
> **comment**: 19 pages, 13 figures
- **标题**: None
- **领域**: 神经和进化计算,人工智能,机器学习
- **Abstract**: Evolutionary model merging enables the creation of high-performing multi-task models but remains computationally prohibitive for consumer hardware. We introduce MERGE$^3$, an efficient framework that makes evolutionary merging feasible on a single GPU by reducing fitness computation costs 50$\times$ while preserving performance. MERGE$^3$ achieves this by Extracting a reduced dataset for evaluation, Estimating model abilities using Item Response Theory (IRT), and Evolving optimal merges via IRT-based performance estimators. Our method enables state-of-the-art multilingual and cross-lingual merging, transferring knowledge across languages with significantly lower computational overhead. We provide theoretical guarantees and an open-source library, democratizing high-quality model merging.

### Neural Genetic Search in Discrete Spaces 
[[arxiv](https://arxiv.org/abs/2502.10433)] [[cool](https://papers.cool/arxiv/2502.10433)] [[pdf](https://arxiv.org/pdf/2502.10433)]
> **Authors**: Hyeonah Kim,Sanghyeok Choi,Jiwoo Son,Jinkyoo Park,Changhyun Kwon
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-17
> **comment**: 19 pages
- **标题**: None
- **领域**: 神经和进化计算,机器学习
- **Abstract**: Effective search methods are crucial for improving the performance of deep generative models at test time. In this paper, we introduce a novel test-time search method, Neural Genetic Search (NGS), which incorporates the evolutionary mechanism of genetic algorithms into the generation procedure of deep models. The core idea behind NGS is its crossover, which is defined as parent-conditioned generation using trained generative models. This approach offers a versatile and easy-to-implement search algorithm for deep generative models. We demonstrate the effectiveness and flexibility of NGS through experiments across three distinct domains: routing problems, adversarial prompt generation for language models, and molecular design.

### Spiking Neural Network Feature Discrimination Boosts Modality Fusion 
[[arxiv](https://arxiv.org/abs/2502.10423)] [[cool](https://papers.cool/arxiv/2502.10423)] [[pdf](https://arxiv.org/pdf/2502.10423)]
> **Authors**: Katerina Maria Oikonomou,Ioannis Kansizoglou,Antonios Gasteratos
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 神经和进化计算,计算机视觉和模式识别,机器学习,图像和视频处理
- **Abstract**: Feature discrimination is a crucial aspect of neural network design, as it directly impacts the network's ability to distinguish between classes and generalize across diverse datasets. The accomplishment of achieving high-quality feature representations ensures high intra-class separability and poses one of the most challenging research directions. While conventional deep neural networks (DNNs) rely on complex transformations and very deep networks to come up with meaningful feature representations, they usually require days of training and consume significant energy amounts. To this end, spiking neural networks (SNNs) offer a promising alternative. SNN's ability to capture temporal and spatial dependencies renders them particularly suitable for complex tasks, where multi-modal data are required. In this paper, we propose a feature discrimination approach for multi-modal learning with SNNs, focusing on audio-visual data. We employ deep spiking residual learning for visual modality processing and a simpler yet efficient spiking network for auditory modality processing. Lastly, we deploy a spiking multilayer perceptron for modality fusion. We present our findings and evaluate our approach against similar works in the field of classification challenges. To the best of our knowledge, this is the first work investigating feature discrimination in SNNs.

### DA-LIF: Dual Adaptive Leaky Integrate-and-Fire Model for Deep Spiking Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.10422)] [[cool](https://papers.cool/arxiv/2502.10422)] [[pdf](https://arxiv.org/pdf/2502.10422)]
> **Authors**: Tianqing Zhang,Kairong Yu,Jian Zhang,Hongwei Wang
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-17
> **comment**: Accepted by ICASSP 2025
- **标题**: None
- **领域**: 神经和进化计算,人工智能
- **Abstract**: Spiking Neural Networks (SNNs) are valued for their ability to process spatio-temporal information efficiently, offering biological plausibility, low energy consumption, and compatibility with neuromorphic hardware. However, the commonly used Leaky Integrate-and-Fire (LIF) model overlooks neuron heterogeneity and independently processes spatial and temporal information, limiting the expressive power of SNNs. In this paper, we propose the Dual Adaptive Leaky Integrate-and-Fire (DA-LIF) model, which introduces spatial and temporal tuning with independently learnable decays. Evaluations on both static (CIFAR10/100, ImageNet) and neuromorphic datasets (CIFAR10-DVS, DVS128 Gesture) demonstrate superior accuracy with fewer timesteps compared to state-of-the-art methods. Importantly, DA-LIF achieves these improvements with minimal additional parameters, maintaining low energy consumption. Extensive ablation studies further highlight the robustness and effectiveness of the DA-LIF model.

### DRiVE: Dynamic Recognition in VEhicles using snnTorch 
[[arxiv](https://arxiv.org/abs/2502.10421)] [[cool](https://papers.cool/arxiv/2502.10421)] [[pdf](https://arxiv.org/pdf/2502.10421)]
> **Authors**: Heerak Vora,Param Pathak,Parul Bakaraniya
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-17
> **comment**: 6 pages, 7 figures, 3 tables, Accepted at the 2025 IEEE International Conference on Advancements in Smart, Secure And Intelligent Computing (ASSIC 2025), May 2025
- **标题**: None
- **领域**: 神经和进化计算,人工智能,计算机视觉和模式识别,机器学习
- **Abstract**: Spiking Neural Networks (SNNs) mimic biological brain activity, processing data efficiently through an event-driven design, wherein the neurons activate only when inputs exceed specific thresholds. Their ability to track voltage changes over time via membrane potential dynamics helps retain temporal information. This study combines SNNs with PyTorch's adaptable framework, snnTorch, to test their potential for image-based tasks. We introduce DRiVE, a vehicle detection model that uses spiking neuron dynamics to classify images, achieving 94.8% accuracy and a near-perfect 0.99 AUC score. These results highlight DRiVE's ability to distinguish vehicle classes effectively, challenging the notion that SNNs are limited to temporal data. As interest grows in energy-efficient neural models, DRiVE's success emphasizes the need to refine SNN optimization for visual tasks. This work encourages broader exploration of SNNs in scenarios where conventional networks struggle, particularly for real-world applications requiring both precision and efficiency.

### A Hybrid Swarm Intelligence Approach for Optimizing Multimodal Large Language Models Deployment in Edge-Cloud-based Federated Learning Environments 
[[arxiv](https://arxiv.org/abs/2502.10419)] [[cool](https://papers.cool/arxiv/2502.10419)] [[pdf](https://arxiv.org/pdf/2502.10419)]
> **Authors**: Gaith Rjouba,Hanae Elmekki,Saidul Islam,Jamal Bentahar,Rachida Dssouli
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 神经和进化计算,人工智能,机器学习
- **Abstract**: The combination of Federated Learning (FL), Multimodal Large Language Models (MLLMs), and edge-cloud computing enables distributed and real-time data processing while preserving privacy across edge devices and cloud infrastructure. However, the deployment of MLLMs in FL environments with resource-constrained edge devices presents significant challenges, including resource management, communication overhead, and non-IID data. To address these challenges, we propose a novel hybrid framework wherein MLLMs are deployed on edge devices equipped with sufficient resources and battery life, while the majority of training occurs in the cloud. To identify suitable edge devices for deployment, we employ Particle Swarm Optimization (PSO), and Ant Colony Optimization (ACO) is utilized to optimize the transmission of model updates between edge and cloud nodes. This proposed swarm intelligence-based framework aims to enhance the efficiency of MLLM training by conducting extensive training in the cloud and fine-tuning at the edge, thereby reducing energy consumption and communication costs. Our experimental results show that the proposed method significantly improves system performance, achieving an accuracy of 92%, reducing communication cost by 30%, and enhancing client participation compared to traditional FL methods. These results make the proposed approach highly suitable for large-scale edge-cloud computing systems.

### A Novel Multi-Objective Evolutionary Algorithm for Counterfactual Generation 
[[arxiv](https://arxiv.org/abs/2502.10418)] [[cool](https://papers.cool/arxiv/2502.10418)] [[pdf](https://arxiv.org/pdf/2502.10418)]
> **Authors**: Gabriel Doyle-Finch,Alex A. Freitas
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 神经和进化计算,机器学习
- **Abstract**: Machine learning algorithms that learn black-box predictive models (which cannot be directly interpreted) are increasingly used to make predictions affecting the lives of people. It is important that users understand the predictions of such models, particularly when the model outputs a negative prediction for the user (e.g. denying a loan). Counterfactual explanations provide users with guidance on how to change some of their characteristics to receive a different, positive classification by a predictive model. For example, if a predictive model rejected a loan application from a user, a counterfactual explanation might state: If your salary was £50,000 (rather than your current £35,000), then your loan would be approved. This paper proposes two novel contributions: (a) a novel multi-objective Evolutionary Algorithm (EA) for counterfactual generation based on lexicographic optimisation, rather than the more popular Pareto dominance approach; and (b) an extension to the definition of the objective of validity for a counterfactual, based on measuring the resilience of a counterfactual to violations of monotonicity constraints which are intuitively expected by users; e.g., intuitively, the probability of a loan application to be approved would monotonically increase with an increase in the salary of the applicant. Experiments involving 15 experimental settings (3 types of black box models times 5 datasets) have shown that the proposed lexicographic optimisation-based EA is very competitive with an existing Pareto dominance-based EA; and the proposed extension of the validity objective has led to a substantial increase in the validity of the counterfactuals generated by the proposed EA.

### Evolutionary Power-Aware Routing in VANETs using Monte-Carlo Simulation 
[[arxiv](https://arxiv.org/abs/2502.10417)] [[cool](https://papers.cool/arxiv/2502.10417)] [[pdf](https://arxiv.org/pdf/2502.10417)]
> **Authors**: J. Toutouh,S. Nesmachnow,E. Alba
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-17
> **comment**: Accepted and presented in 2012 International Conference on High Performance Computing & Simulation (HPCS), Madrid, Spain, 2012
- **标题**: None
- **领域**: 神经和进化计算,人工智能,网络和互联网架构
- **Abstract**: This work addresses the reduction of power consumption of the AODV routing protocol in vehicular networks as an optimization problem. Nowadays, network designers focus on energy-aware communication protocols, specially to deploy wireless networks. Here, we introduce an automatic method to search for energy-efficient AODV configurations by using an evolutionary algorithm and parallel Monte-Carlo simulations to improve the accuracy of the evaluation of tentative solutions. The experimental results demonstrate that significant power consumption improvements over the standard configuration can be attained, with no noteworthy loss in the quality of service.

### A Neural Network Training Method Based on Neuron Connection Coefficient Adjustments 
[[arxiv](https://arxiv.org/abs/2502.10414)] [[cool](https://papers.cool/arxiv/2502.10414)] [[pdf](https://arxiv.org/pdf/2502.10414)]
> **Authors**: Kun Jiang
> **First submission**: 2025-01-25
> **First announcement**: 2025-02-17
> **comment**: 9 pages, 4 figures, this is the third article in our research series and the second of four different training methods
- **标题**: None
- **领域**: 神经和进化计算,机器学习
- **Abstract**: In previous studies, we introduced a neural network framework based on symmetric differential equations, along with one of its training methods. In this article, we present another training approach for this neural network. This method leverages backward signal propagation and eliminates reliance on the traditional chain derivative rule, offering a high degree of biological interpretability. Unlike the previously introduced method, this approach does not require adjustments to the fixed points of the differential equations. Instead, it focuses solely on modifying the connection coefficients between neurons, closely resembling the training process of traditional multilayer perceptron (MLP) networks. By adopting a suitable adjustment strategy, this method effectively avoids certain potential local minima. To validate this approach, we tested it on the MNIST dataset and achieved promising results. Through further analysis, we identified certain limitations of the current neural network architecture and proposed measures for improvement.

## 网络和互联网架构(cs.NI:Networking and Internet Architecture)

### Intelligent Mobile AI-Generated Content Services via Interactive Prompt Engineering and Dynamic Service Provisioning 
[[arxiv](https://arxiv.org/abs/2502.11386)] [[cool](https://papers.cool/arxiv/2502.11386)] [[pdf](https://arxiv.org/pdf/2502.11386)]
> **Authors**: Yinqiu Liu,Ruichen Zhang,Jiacheng Wang,Dusit Niyato,Xianbin Wang,Dong In Kim,Hongyang Du
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 网络和互联网架构,机器学习
- **Abstract**: Due to massive computational demands of large generative models, AI-Generated Content (AIGC) can organize collaborative Mobile AIGC Service Providers (MASPs) at network edges to provide ubiquitous and customized content generation for resource-constrained users. However, such a paradigm faces two significant challenges: 1) raw prompts (i.e., the task description from users) often lead to poor generation quality due to users' lack of experience with specific AIGC models, and 2) static service provisioning fails to efficiently utilize computational and communication resources given the heterogeneity of AIGC tasks. To address these challenges, we propose an intelligent mobile AIGC service scheme. Firstly, we develop an interactive prompt engineering mechanism that leverages a Large Language Model (LLM) to generate customized prompt corpora and employs Inverse Reinforcement Learning (IRL) for policy imitation through small-scale expert demonstrations. Secondly, we formulate a dynamic mobile AIGC service provisioning problem that jointly optimizes the number of inference trials and transmission power allocation. Then, we propose the Diffusion-Enhanced Deep Deterministic Policy Gradient (D3PG) algorithm to solve the problem. By incorporating the diffusion process into Deep Reinforcement Learning (DRL) architecture, the environment exploration capability can be improved, thus adapting to varying mobile AIGC scenarios. Extensive experimental results demonstrate that our prompt engineering approach improves single-round generation success probability by 6.3 times, while D3PG increases the user service experience by 67.8% compared to baseline DRL approaches.

### Integrating Language Models for Enhanced Network State Monitoring in DRL-Based SFC Provisioning 
[[arxiv](https://arxiv.org/abs/2502.11298)] [[cool](https://papers.cool/arxiv/2502.11298)] [[pdf](https://arxiv.org/pdf/2502.11298)]
> **Authors**: Parisa Fard Moshiri,Murat Arda Onsu,Poonam Lohan,Burak Kantarci,Emil Janulewicz
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 6 pages, 5 figures, submitted to 30th IEEE International Symposium on Computers and Communications (ISCC) 2025
- **标题**: None
- **领域**: 网络和互联网架构,人工智能,计算语言学
- **Abstract**: Efficient Service Function Chain (SFC) provisioning and Virtual Network Function (VNF) placement are critical for enhancing network performance in modern architectures such as Software-Defined Networking (SDN) and Network Function Virtualization (NFV). While Deep Reinforcement Learning (DRL) aids decision-making in dynamic network environments, its reliance on structured inputs and predefined rules limits adaptability in unforeseen scenarios. Additionally, incorrect actions by a DRL agent may require numerous training iterations to correct, potentially reinforcing suboptimal policies and degrading performance. This paper integrates DRL with Language Models (LMs), specifically Bidirectional Encoder Representations from Transformers (BERT) and DistilBERT, to enhance network management. By feeding final VNF allocations from DRL into the LM, the system can process and respond to queries related to SFCs, DCs, and VNFs, enabling real-time insights into resource utilization, bottleneck detection, and future demand planning. The LMs are fine-tuned to our domain-specific dataset using Low-Rank Adaptation (LoRA). Results show that BERT outperforms DistilBERT with a lower test loss (0.28 compared to 0.36) and higher confidence (0.83 compared to 0.74), though BERT requires approximately 46% more processing time.

### Leveraging Uncertainty Estimation for Efficient LLM Routing 
[[arxiv](https://arxiv.org/abs/2502.11021)] [[cool](https://papers.cool/arxiv/2502.11021)] [[pdf](https://arxiv.org/pdf/2502.11021)]
> **Authors**: Tuo Zhang,Asal Mehradfar,Dimitrios Dimitriadis,Salman Avestimehr
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 网络和互联网架构,计算语言学
- **Abstract**: Deploying large language models (LLMs) in edge-cloud environments requires an efficient routing strategy to balance cost and response quality. Traditional approaches prioritize either human-preference data or accuracy metrics from benchmark datasets as routing criteria, but these methods suffer from rigidity and subjectivity. Moreover, existing routing frameworks primarily focus on accuracy and cost, neglecting response quality from a human preference perspective. In this work, we propose the Confidence-Driven LLM Router, a novel framework that leverages uncertainty estimation to optimize routing decisions. To comprehensively assess routing performance, we evaluate both system cost efficiency and response quality. In particular, we introduce the novel use of LLM-as-a-Judge to simulate human rating preferences, providing the first systematic assessment of response quality across different routing strategies. Extensive experiments on MT-Bench, GSM8K, and MMLU demonstrate that our approach outperforms state-of-the-art routing methods, achieving superior response quality while maintaining cost efficiency.

## 机器人技术(cs.RO:Robotics)

### PrivilegedDreamer: Explicit Imagination of Privileged Information for Rapid Adaptation of Learned Policies 
[[arxiv](https://arxiv.org/abs/2502.11377)] [[cool](https://papers.cool/arxiv/2502.11377)] [[pdf](https://arxiv.org/pdf/2502.11377)]
> **Authors**: Morgan Byrd,Jackson Crandell,Mili Das,Jessica Inman,Robert Wright,Sehoon Ha
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: Accepted to ICRA 2025. Website: https://morganbyrd03.github.io/icra25_privileged_dreamer/
- **标题**: None
- **领域**: 机器人技术,机器学习
- **Abstract**: Numerous real-world control problems involve dynamics and objectives affected by unobservable hidden parameters, ranging from autonomous driving to robotic manipulation, which cause performance degradation during sim-to-real transfer. To represent these kinds of domains, we adopt hidden-parameter Markov decision processes (HIP-MDPs), which model sequential decision problems where hidden variables parameterize transition and reward functions. Existing approaches, such as domain randomization, domain adaptation, and meta-learning, simply treat the effect of hidden parameters as additional variance and often struggle to effectively handle HIP-MDP problems, especially when the rewards are parameterized by hidden variables. We introduce Privileged-Dreamer, a model-based reinforcement learning framework that extends the existing model-based approach by incorporating an explicit parameter estimation module. PrivilegedDreamer features its novel dual recurrent architecture that explicitly estimates hidden parameters from limited historical data and enables us to condition the model, actor, and critic networks on these estimated parameters. Our empirical analysis on five diverse HIP-MDP tasks demonstrates that PrivilegedDreamer outperforms state-of-the-art model-based, model-free, and domain adaptation learning algorithms. Additionally, we conduct ablation studies to justify the inclusion of each component in the proposed architecture.

### Robot Deformable Object Manipulation via NMPC-generated Demonstrations in Deep Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.11375)] [[cool](https://papers.cool/arxiv/2502.11375)] [[pdf](https://arxiv.org/pdf/2502.11375)]
> **Authors**: Haoyuan Wang,Zihao Dong,Hongliang Lei,Zejia Zhang,Weizhuang Shi,Wei Luo,Weiwei Wan,Jian Huang
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,机器学习
- **Abstract**: In this work, we conducted research on deformable object manipulation by robots based on demonstration-enhanced reinforcement learning (RL). To improve the learning efficiency of RL, we enhanced the utilization of demonstration data from multiple aspects and proposed the HGCR-DDPG algorithm. It uses a novel high-dimensional fuzzy approach for grasping-point selection, a refined behavior-cloning method to enhance data-driven learning in Rainbow-DDPG, and a sequential policy-learning strategy. Compared to the baseline algorithm (Rainbow-DDPG), our proposed HGCR-DDPG achieved 2.01 times the global average reward and reduced the global average standard deviation to 45% of that of the baseline algorithm. To reduce the human labor cost of demonstration collection, we proposed a low-cost demonstration collection method based on Nonlinear Model Predictive Control (NMPC). Simulation experiment results show that demonstrations collected through NMPC can be used to train HGCR-DDPG, achieving comparable results to those obtained with human demonstrations. To validate the feasibility of our proposed methods in real-world environments, we conducted physical experiments involving deformable object manipulation. We manipulated fabric to perform three tasks: diagonal folding, central axis folding, and flattening. The experimental results demonstrate that our proposed method achieved success rates of 83.3%, 80%, and 100% for these three tasks, respectively, validating the effectiveness of our approach. Compared to current large-model approaches for robot manipulation, the proposed algorithm is lightweight, requires fewer computational resources, and offers task-specific customization and efficient adaptability for specific tasks.

### A Framework for Learning Scoring Rules in Autonomous Driving Planning Systems 
[[arxiv](https://arxiv.org/abs/2502.11352)] [[cool](https://papers.cool/arxiv/2502.11352)] [[pdf](https://arxiv.org/pdf/2502.11352)]
> **Authors**: Zikang Xiong,Joe Kurian Eappen,Suresh Jagannathan
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: Accepted for publication in IEEE Robotics and Automation Letters (RA-L)
- **标题**: None
- **领域**: 机器人技术,机器学习
- **Abstract**: In autonomous driving systems, motion planning is commonly implemented as a two-stage process: first, a trajectory proposer generates multiple candidate trajectories, then a scoring mechanism selects the most suitable trajectory for execution. For this critical selection stage, rule-based scoring mechanisms are particularly appealing as they can explicitly encode driving preferences, safety constraints, and traffic regulations in a formalized, human-understandable format. However, manually crafting these scoring rules presents significant challenges: the rules often contain complex interdependencies, require careful parameter tuning, and may not fully capture the nuances present in real-world driving data. This work introduces FLoRA, a novel framework that bridges this gap by learning interpretable scoring rules represented in temporal logic. Our method features a learnable logic structure that captures nuanced relationships across diverse driving scenarios, optimizing both rules and parameters directly from real-world driving demonstrations collected in NuPlan. Our approach effectively learns to evaluate driving behavior even though the training data only contains positive examples (successful driving demonstrations). Evaluations in closed-loop planning simulations demonstrate that our learned scoring rules outperform existing techniques, including expert-designed rules and neural network scoring models, while maintaining interpretability. This work introduces a data-driven approach to enhance the scoring mechanism in autonomous driving systems, designed as a plug-in module to seamlessly integrate with various trajectory proposers. Our video and code are available on xiong.zikang.me/FLoRA.

### A Physics-Informed Machine Learning Framework for Safe and Optimal Control of Autonomous Systems 
[[arxiv](https://arxiv.org/abs/2502.11057)] [[cool](https://papers.cool/arxiv/2502.11057)] [[pdf](https://arxiv.org/pdf/2502.11057)]
> **Authors**: Manan Tayal,Aditya Singh,Shishir Kolathaya,Somil Bansal
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 15 Pages, 12 Figures. First two authors have contributed equally
- **标题**: None
- **领域**: 机器人技术,人工智能,系统与控制
- **Abstract**: As autonomous systems become more ubiquitous in daily life, ensuring high performance with guaranteed safety is crucial. However, safety and performance could be competing objectives, which makes their co-optimization difficult. Learning-based methods, such as Constrained Reinforcement Learning (CRL), achieve strong performance but lack formal safety guarantees due to safety being enforced as soft constraints, limiting their use in safety-critical settings. Conversely, formal methods such as Hamilton-Jacobi (HJ) Reachability Analysis and Control Barrier Functions (CBFs) provide rigorous safety assurances but often neglect performance, resulting in overly conservative controllers. To bridge this gap, we formulate the co-optimization of safety and performance as a state-constrained optimal control problem, where performance objectives are encoded via a cost function and safety requirements are imposed as state constraints. We demonstrate that the resultant value function satisfies a Hamilton-Jacobi-Bellman (HJB) equation, which we approximate efficiently using a novel physics-informed machine learning framework. In addition, we introduce a conformal prediction-based verification strategy to quantify the learning errors, recovering a high-confidence safety value function, along with a probabilistic error bound on performance degradation. Through several case studies, we demonstrate the efficacy of the proposed framework in enabling scalable learning of safe and performant controllers for complex, high-dimensional autonomous systems.

### Bridging the Sim-to-Real Gap for Athletic Loco-Manipulation 
[[arxiv](https://arxiv.org/abs/2502.10894)] [[cool](https://papers.cool/arxiv/2502.10894)] [[pdf](https://arxiv.org/pdf/2502.10894)]
> **Authors**: Nolan Fey,Gabriel B. Margolis,Martin Peticco,Pulkit Agrawal
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: Project website: http://uan.csail.mit.edu
- **标题**: None
- **领域**: 机器人技术,人工智能,机器学习
- **Abstract**: Achieving athletic loco-manipulation on robots requires moving beyond traditional tracking rewards - which simply guide the robot along a reference trajectory - to task rewards that drive truly dynamic, goal-oriented behaviors. Commands such as "throw the ball as far as you can" or "lift the weight as quickly as possible" compel the robot to exhibit the agility and power inherent in athletic performance. However, training solely with task rewards introduces two major challenges: these rewards are prone to exploitation (reward hacking), and the exploration process can lack sufficient direction. To address these issues, we propose a two-stage training pipeline. First, we introduce the Unsupervised Actuator Net (UAN), which leverages real-world data to bridge the sim-to-real gap for complex actuation mechanisms without requiring access to torque sensing. UAN mitigates reward hacking by ensuring that the learned behaviors remain robust and transferable. Second, we use a pre-training and fine-tuning strategy that leverages reference trajectories as initial hints to guide exploration. With these innovations, our robot athlete learns to lift, throw, and drag with remarkable fidelity from simulation to reality.

### BeamDojo: Learning Agile Humanoid Locomotion on Sparse Footholds 
[[arxiv](https://arxiv.org/abs/2502.10363)] [[cool](https://papers.cool/arxiv/2502.10363)] [[pdf](https://arxiv.org/pdf/2502.10363)]
> **Authors**: Huayi Wang,Zirui Wang,Junli Ren,Qingwei Ben,Tao Huang,Weinan Zhang,Jiangmiao Pang
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: Project website: https://why618188.github.io/beamdojo
- **标题**: None
- **领域**: 机器人技术,人工智能,机器学习
- **Abstract**: Traversing risky terrains with sparse footholds poses a significant challenge for humanoid robots, requiring precise foot placements and stable locomotion. Existing approaches designed for quadrupedal robots often fail to generalize to humanoid robots due to differences in foot geometry and unstable morphology, while learning-based approaches for humanoid locomotion still face great challenges on complex terrains due to sparse foothold reward signals and inefficient learning processes. To address these challenges, we introduce BeamDojo, a reinforcement learning (RL) framework designed for enabling agile humanoid locomotion on sparse footholds. BeamDojo begins by introducing a sampling-based foothold reward tailored for polygonal feet, along with a double critic to balancing the learning process between dense locomotion rewards and sparse foothold rewards. To encourage sufficient trail-and-error exploration, BeamDojo incorporates a two-stage RL approach: the first stage relaxes the terrain dynamics by training the humanoid on flat terrain while providing it with task terrain perceptive observations, and the second stage fine-tunes the policy on the actual task terrain. Moreover, we implement a onboard LiDAR-based elevation map to enable real-world deployment. Extensive simulation and real-world experiments demonstrate that BeamDojo achieves efficient learning in simulation and enables agile locomotion with precise foot placement on sparse footholds in the real world, maintaining a high success rate even under significant external disturbances.

### Manual2Skill: Learning to Read Manuals and Acquire Robotic Skills for Furniture Assembly Using Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2502.10090)] [[cool](https://papers.cool/arxiv/2502.10090)] [[pdf](https://arxiv.org/pdf/2502.10090)]
> **Authors**: Chenrui Tie,Shengxiang Sun,Jinxuan Zhu,Yiwei Liu,Jingxiang Guo,Yue Hu,Haonan Chen,Junting Chen,Ruihai Wu,Lin Shao
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,人工智能
- **Abstract**: Humans possess an extraordinary ability to understand and execute complex manipulation tasks by interpreting abstract instruction manuals. For robots, however, this capability remains a substantial challenge, as they cannot interpret abstract instructions and translate them into executable actions. In this paper, we present Manual2Skill, a novel framework that enables robots to perform complex assembly tasks guided by high-level manual instructions. Our approach leverages a Vision-Language Model (VLM) to extract structured information from instructional images and then uses this information to construct hierarchical assembly graphs. These graphs represent parts, subassemblies, and the relationships between them. To facilitate task execution, a pose estimation model predicts the relative 6D poses of components at each assembly step. At the same time, a motion planning module generates actionable sequences for real-world robotic implementation. We demonstrate the effectiveness of Manual2Skill by successfully assembling several real-world IKEA furniture items. This application highlights its ability to manage long-horizon manipulation tasks with both efficiency and precision, significantly enhancing the practicality of robot learning from instruction manuals. This work marks a step forward in advancing robotic systems capable of understanding and executing complex manipulation tasks in a manner akin to human capabilities.

## 声音(cs.SD:Sound)

### SyncSpeech: Low-Latency and Efficient Dual-Stream Text-to-Speech based on Temporal Masked Transformer 
[[arxiv](https://arxiv.org/abs/2502.11094)] [[cool](https://papers.cool/arxiv/2502.11094)] [[pdf](https://arxiv.org/pdf/2502.11094)]
> **Authors**: Zhengyan Sheng,Zhihao Du,Shiliang Zhang,Zhijie Yan,Yexin Yang,Zhenhua Ling
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 声音,人工智能
- **Abstract**: This paper presents a dual-stream text-to-speech (TTS) model, SyncSpeech, capable of receiving streaming text input from upstream models while simultaneously generating streaming speech, facilitating seamless interaction with large language models. SyncSpeech has the following advantages: Low latency, as it begins generating streaming speech upon receiving the second text token; High efficiency, as it decodes all speech tokens corresponding to the each arrived text token in one step. To achieve this, we propose a temporal masked transformer as the backbone of SyncSpeech, combined with token-level duration prediction to predict speech tokens and the duration for the next step. Additionally, we design a two-stage training strategy to improve training efficiency and the quality of generated speech. We evaluated the SyncSpeech on both English and Mandarin datasets. Compared to the recent dual-stream TTS models, SyncSpeech significantly reduces the first packet delay of speech tokens and accelerates the real-time factor. Moreover, with the same data scale, SyncSpeech achieves performance comparable to that of traditional autoregressive-based TTS models in terms of both speech quality and robustness. Speech samples are available at https://SyncSpeech.github.io/}{https://SyncSpeech.github.io/.

### Hyperdimensional Intelligent Sensing for Efficient Real-Time Audio Processing on Extreme Edge 
[[arxiv](https://arxiv.org/abs/2502.10718)] [[cool](https://papers.cool/arxiv/2502.10718)] [[pdf](https://arxiv.org/pdf/2502.10718)]
> **Authors**: Sanggeon Yun,Ryozo Masukawa,Hanning Chen,SungHeon Jeong,Wenjun Huang,Arghavan Rezvani,Minhyoung Na,Yoshiki Yamaguchi,Mohsen Imani
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: Accepted to IEEE Access
- **标题**: None
- **领域**: 声音,人工智能,音频和语音处理
- **Abstract**: The escalating challenges of managing vast sensor-generated data, particularly in audio applications, necessitate innovative solutions. Current systems face significant computational and storage demands, especially in real-time applications like gunshot detection systems (GSDS), and the proliferation of edge sensors exacerbates these issues. This paper proposes a groundbreaking approach with a near-sensor model tailored for intelligent audio-sensing frameworks. Utilizing a Fast Fourier Transform (FFT) module, convolutional neural network (CNN) layers, and HyperDimensional Computing (HDC), our model excels in low-energy, rapid inference, and online learning. It is highly adaptable for efficient ASIC design implementation, offering superior energy efficiency compared to conventional embedded CPUs or GPUs, and is compatible with the trend of shrinking microphone sensor sizes. Comprehensive evaluations at both software and hardware levels underscore the model's efficacy. Software assessments through detailed ROC curve analysis revealed a delicate balance between energy conservation and quality loss, achieving up to 82.1% energy savings with only 1.39% quality loss. Hardware evaluations highlight the model's commendable energy efficiency when implemented via ASIC design, especially with the Google Edge TPU, showcasing its superiority over prevalent embedded CPUs and GPUs.

### F-StrIPE: Fast Structure-Informed Positional Encoding for Symbolic Music Generation 
[[arxiv](https://arxiv.org/abs/2502.10491)] [[cool](https://papers.cool/arxiv/2502.10491)] [[pdf](https://arxiv.org/pdf/2502.10491)]
> **Authors**: Manvi Agarwal,Changhong Wang,Gael Richard
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: ef:IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Apr 2025, Hyderabad, India
- **标题**: None
- **领域**: 声音,人工智能,机器学习,音频和语音处理
- **Abstract**: While music remains a challenging domain for generative models like Transformers, recent progress has been made by exploiting suitable musically-informed priors. One technique to leverage information about musical structure in Transformers is inserting such knowledge into the positional encoding (PE) module. However, Transformers carry a quadratic cost in sequence length. In this paper, we propose F-StrIPE, a structure-informed PE scheme that works in linear complexity. Using existing kernel approximation techniques based on random features, we show that F-StrIPE is a generalization of Stochastic Positional Encoding (SPE). We illustrate the empirical merits of F-StrIPE using melody harmonization for symbolic music.

### YNote: A Novel Music Notation for Fine-Tuning LLMs in Music Generation 
[[arxiv](https://arxiv.org/abs/2502.10467)] [[cool](https://papers.cool/arxiv/2502.10467)] [[pdf](https://arxiv.org/pdf/2502.10467)]
> **Authors**: Shao-Chien Lu,Chen-Chen Yeh,Hui-Lin Cho,Chun-Chieh Hsu,Tsai-Ling Hsu,Cheng-Han Wu,Timothy K. Shih,Yu-Cheng Lin
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 声音,人工智能,音频和语音处理
- **Abstract**: The field of music generation using Large Language Models (LLMs) is evolving rapidly, yet existing music notation systems, such as MIDI, ABC Notation, and MusicXML, remain too complex for effective fine-tuning of LLMs. These formats are difficult for both machines and humans to interpret due to their variability and intricate structure. To address these challenges, we introduce YNote, a simplified music notation system that uses only four characters to represent a note and its pitch. YNote's fixed format ensures consistency, making it easy to read and more suitable for fine-tuning LLMs. In our experiments, we fine-tuned GPT-2 (124M) on a YNote-encoded dataset and achieved BLEU and ROUGE scores of 0.883 and 0.766, respectively. With just two notes as prompts, the model was able to generate coherent and stylistically relevant music. We believe YNote offers a practical alternative to existing music notations for machine learning applications and has the potential to significantly enhance the quality of music generation using LLMs.

### Video Soundtrack Generation by Aligning Emotions and Temporal Boundaries 
[[arxiv](https://arxiv.org/abs/2502.10154)] [[cool](https://papers.cool/arxiv/2502.10154)] [[pdf](https://arxiv.org/pdf/2502.10154)]
> **Authors**: Serkan Sulun,Paula Viana,Matthew E. P. Davies
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: Submitted to International Joint Conference on Artificial Intelligence (IJCAI) 2025
- **标题**: None
- **领域**: 声音,人工智能,机器学习,多媒体,音频和语音处理,图像和视频处理
- **Abstract**: We introduce EMSYNC, a video-based symbolic music generation model that aligns music with a video's emotional content and temporal boundaries. It follows a two-stage framework, where a pretrained video emotion classifier extracts emotional features, and a conditional music generator produces MIDI sequences guided by both emotional and temporal cues. We introduce boundary offsets, a novel temporal conditioning mechanism that enables the model to anticipate and align musical chords with scene cuts. Unlike existing models, our approach retains event-based encoding, ensuring fine-grained timing control and expressive musical nuances. We also propose a mapping scheme to bridge the video emotion classifier, which produces discrete emotion categories, with the emotion-conditioned MIDI generator, which operates on continuous-valued valence-arousal inputs. In subjective listening tests, EMSYNC outperforms state-of-the-art models across all subjective metrics, for music theory-aware participants as well as the general listeners.

### InterGridNet: An Electric Network Frequency Approach for Audio Source Location Classification Using Convolutional Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.10011)] [[cool](https://papers.cool/arxiv/2502.10011)] [[pdf](https://arxiv.org/pdf/2502.10011)]
> **Authors**: Christos Korgialas,Ioannis Tsingalis,Georgios Tzolopoulos,Constantine Kotropoulos
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: The 10th International Conference on Advances in Signal, Image and Video Processing (SIGNAL 2025)
- **标题**: None
- **领域**: 声音,机器学习,音频和语音处理
- **Abstract**: A novel framework, called InterGridNet, is introduced, leveraging a shallow RawNet model for geolocation classification of Electric Network Frequency (ENF) signatures in the SP Cup 2016 dataset. During data preparation, recordings are sorted into audio and power groups based on inherent characteristics, further divided into 50 Hz and 60 Hz groups via spectrogram analysis. Residual blocks within the classification model extract frame-level embeddings, aiding decision-making through softmax activation. The topology and the hyperparameters of the shallow RawNet are optimized using a Neural Architecture Search. The overall accuracy of InterGridNet in the test recordings is 92%, indicating its effectiveness against the state-of-the-art methods tested in the SP Cup 2016. These findings underscore InterGridNet's effectiveness in accurately classifying audio recordings from diverse power grids, advancing state-of-the-art geolocation estimation methods.

## 软件工程(cs.SE:Software Engineering)

### VisPath: Automated Visualization Code Synthesis via Multi-Path Reasoning and Feedback-Driven Optimization 
[[arxiv](https://arxiv.org/abs/2502.11140)] [[cool](https://papers.cool/arxiv/2502.11140)] [[pdf](https://arxiv.org/pdf/2502.11140)]
> **Authors**: Wonduk Seo,Seungyong Lee,Daye Kang,Zonghao Yuan,Seunghyun Lee
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 14 pages, 3 figures, 4 tables
- **标题**: None
- **领域**: 软件工程,人工智能,计算语言学,人机交互
- **Abstract**: Unprecedented breakthroughs in Large Language Models (LLMs) has amplified its penetration into application of automated visualization code generation. Few-shot prompting and query expansion techniques have notably enhanced data visualization performance, however, still fail to overcome ambiguity and complexity of natural language queries - imposing an inherent burden for manual human intervention. To mitigate such limitations, we propose a holistic framework VisPath : A Multi-Path Reasoning and Feedback-Driven Optimization Framework for Visualization Code Generation, which systematically enhances code quality through structured reasoning and refinement. VisPath is a multi-stage framework, specially designed to handle underspecified queries. To generate a robust final visualization code, it first utilizes initial query to generate diverse reformulated queries via Chain-of-Thought (CoT) prompting, each representing a distinct reasoning path. Refined queries are used to produce candidate visualization scripts, consequently executed to generate multiple images. Comprehensively assessing correctness and quality of outputs, VisPath generates feedback for each image, which are then fed to aggregation module to generate optimal result. Extensive experiments on benchmarks including MatPlotBench and the Qwen-Agent Code Interpreter Benchmark show that VisPath significantly outperforms state-of-the-art (SOTA) methods, increased up to average 17%, offering a more reliable solution for AI-driven visualization code generation.

### Empirical evaluation of LLMs in predicting fixes of Configuration bugs in Smart Home System 
[[arxiv](https://arxiv.org/abs/2502.10953)] [[cool](https://papers.cool/arxiv/2502.10953)] [[pdf](https://arxiv.org/pdf/2502.10953)]
> **Authors**: Sheikh Moonwara Anjum Monisha,Atul Bharadwaj
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: This empirical study evaluates the effectiveness of Large Language Models (LLMs) in predicting fixes for configuration bugs in smart home systems. The research analyzes three prominent LLMs - GPT-4, GPT-4o (GPT-4 Turbo), and Claude 3.5 Sonnet - using four distinct prompt designs to assess their ability to identify appropriate fix strategies and generate correct solutions. The study utilized a dataset of 129 debugging issues from the Home Assistant Community, focusing on 21 randomly selected cases for in-depth analysis. Results demonstrate that GPT-4 and Claude 3.5 Sonnet achieved 80\% accuracy in strategy prediction when provided with both bug descriptions and original scripts. GPT-4 exhibited consistent performance across different prompt types, while GPT-4o showed advantages in speed and cost-effectiveness despite slightly lower accuracy. The findings reveal that prompt design significantly impacts model performance, with comprehensive prompts containing both description and original script yielding the best results. This research provides valuable insights for improving automated bug fixing in smart home system configurations and demonstrates the potential of LLMs in addressing configuration-related challenges.

### CoCoEvo: Co-Evolution of Programs and Test Cases to Enhance Code Generation 
[[arxiv](https://arxiv.org/abs/2502.10802)] [[cool](https://papers.cool/arxiv/2502.10802)] [[pdf](https://arxiv.org/pdf/2502.10802)]
> **Authors**: Kefan Li,Hongyue Yu,Tingyu Guo,Shijie Cao,Yuan Yuan
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: Large Language Models (LLMs) have shown remarkable performance in automated code generation. However, existing approaches often rely heavily on pre-defined test cases, which become impractical in scenarios where such cases are unavailable. While prior works explore filtering techniques between programs and test cases, they overlook the refinement of test cases. To address this limitation, we introduce CoCoEvo, a novel LLM-based co-evolution framework that simultaneously evolves programs and test cases. CoCoEvo eliminates the dependency on pre-defined test cases by generating both programs and test cases directly from natural language problem descriptions and function headers. The framework employs specialized evolutionary operators, including LLM-based crossover and mutation operators for program evolution, along with a test case generation operator for test case evolution. Additionally, we propose optimization strategies such as a crossover rate scheduler to balance exploration and convergence, and a multi-objective optimization method for test case selection. Experimental results on multiple state-of-the-art LLMs demonstrate that CoCoEvo surpasses existing methods, achieving state-of-the-art performance in automated code generation and testing. These results underscore the potential of co-evolutionary techniques in advancing the field of automated programming.

## 社交和信息网络(cs.SI:Social and Information Networks)

### Human-Centric Community Detection in Hybrid Metaverse Networks with Integrated AI Entities 
[[arxiv](https://arxiv.org/abs/2502.10750)] [[cool](https://papers.cool/arxiv/2502.10750)] [[pdf](https://arxiv.org/pdf/2502.10750)]
> **Authors**: Shih-Hsuan Chiu,Ya-Wen Teng,De-Nian Yang,Ming-Syan Chen
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: 15 pages, Accepted for publication in the ACM WWW 2025
- **标题**: None
- **领域**: 社交和信息网络,人工智能
- **Abstract**: Community detection is a cornerstone problem in social network analysis (SNA), aimed at identifying cohesive communities with minimal external links. However, the rise of generative AI and Metaverse introduce complexities by creating hybrid human-AI social networks (denoted by HASNs), where traditional methods fall short, especially in human-centric settings. This paper introduces a novel community detection problem in HASNs (denoted by MetaCD), which seeks to enhance human connectivity within communities while reducing the presence of AI nodes. Effective processing of MetaCD poses challenges due to the delicate trade-off between excluding certain AI nodes and maintaining community structure. To address this, we propose CUSA, an innovative framework incorporating AI-aware clustering techniques that navigate this trade-off by selectively retaining AI nodes that contribute to community integrity. Furthermore, given the scarcity of real-world HASNs, we devise four strategies for synthesizing these networks under various hypothetical scenarios. Empirical evaluations on real social networks, reconfigured as HASNs, demonstrate the effectiveness and practicality of our approach compared to traditional non-deep learning and graph neural network (GNN)-based methods.

## 音频和语音处理(eess.AS:Audio and Speech Processing)

### Generalizable speech deepfake detection via meta-learned LoRA 
[[arxiv](https://arxiv.org/abs/2502.10838)] [[cool](https://papers.cool/arxiv/2502.10838)] [[pdf](https://arxiv.org/pdf/2502.10838)]
> **Authors**: Janne Laakkonen,Ivan Kukanov,Ville Hautamäki
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: 9 pages, 2 figures
- **标题**: None
- **领域**: 音频和语音处理,机器学习,声音
- **Abstract**: Generalizable deepfake detection can be formulated as a detection problem where labels (bonafide and fake) are fixed but distributional drift affects the deepfake set. We can always train our detector with one-selected attacks and bonafide data, but an attacker can generate new attacks by just retraining his generator with a different seed. One reasonable approach is to simply pool all different attack types available in training time. Our proposed approach is to utilize meta-learning in combination with LoRA adapters to learn the structure in the training data that is common to all attack types.

### NeuroAMP: A Novel End-to-end General Purpose Deep Neural Amplifier for Personalized Hearing Aids 
[[arxiv](https://arxiv.org/abs/2502.10822)] [[cool](https://papers.cool/arxiv/2502.10822)] [[pdf](https://arxiv.org/pdf/2502.10822)]
> **Authors**: Shafique Ahmed,Ryandhimas E. Zezario,Hui-Guan Yuan,Amir Hussain,Hsin-Min Wang,Wei-Ho Chung,Yu Tsao
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 音频和语音处理,人工智能,声音
- **Abstract**: The prevalence of hearing aids is increasing. However, optimizing the amplification processes of hearing aids remains challenging due to the complexity of integrating multiple modular components in traditional methods. To address this challenge, we present NeuroAMP, a novel deep neural network designed for end-to-end, personalized amplification in hearing aids. NeuroAMP leverages both spectral features and the listener's audiogram as inputs, and we investigate four architectures: Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM), Convolutional Recurrent Neural Network (CRNN), and Transformer. We also introduce Denoising NeuroAMP, an extension that integrates noise reduction along with amplification capabilities for improved performance in real-world scenarios. To enhance generalization, a comprehensive data augmentation strategy was employed during training on diverse speech (TIMIT and TMHINT) and music (Cadenza Challenge MUSIC) datasets. Evaluation using the Hearing Aid Speech Perception Index (HASPI), Hearing Aid Speech Quality Index (HASQI), and Hearing Aid Audio Quality Index (HAAQI) demonstrates that the Transformer architecture within NeuroAMP achieves the best performance, with SRCC scores of 0.9927 (HASQI) and 0.9905 (HASPI) on TIMIT, and 0.9738 (HAAQI) on the Cadenza Challenge MUSIC dataset. Notably, our data augmentation strategy maintains high performance on unseen datasets (e.g., VCTK, MUSDB18-HQ). Furthermore, Denoising NeuroAMP outperforms both the conventional NAL-R+WDRC approach and a two-stage baseline on the VoiceBank+DEMAND dataset, achieving a 10% improvement in both HASPI (0.90) and HASQI (0.59) scores. These results highlight the potential of NeuroAMP and Denoising NeuroAMP to deliver notable improvements in personalized hearing aid amplification.

### MoHAVE: Mixture of Hierarchical Audio-Visual Experts for Robust Speech Recognition 
[[arxiv](https://arxiv.org/abs/2502.10447)] [[cool](https://papers.cool/arxiv/2502.10447)] [[pdf](https://arxiv.org/pdf/2502.10447)]
> **Authors**: Sungnyun Kim,Kangwook Jang,Sangmin Bae,Sungwoo Cho,Se-Young Yun
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-17
> **comment**: Preliminary work
- **标题**: None
- **领域**: 音频和语音处理,计算语言学,机器学习
- **Abstract**: Audio-visual speech recognition (AVSR) has become critical for enhancing speech recognition in noisy environments by integrating both auditory and visual modalities. However, existing AVSR systems struggle to scale up without compromising computational efficiency. In this study, we introduce MoHAVE (Mixture of Hierarchical Audio-Visual Experts), a novel robust AVSR framework designed to address these scalability constraints. By leveraging a Mixture-of-Experts (MoE) architecture, MoHAVE activates modality-specific expert groups, ensuring dynamic adaptation to various audio-visual inputs with minimal computational overhead. Key contributions of MoHAVE include: (1) a sparse MoE framework that efficiently scales AVSR model capacity, (2) a hierarchical gating mechanism that dynamically utilizes the expert groups based on input context, enhancing adaptability and robustness, and (3) remarkable performance across robust AVSR benchmarks, including LRS3 and MuAViC transcription and translation tasks, setting a new standard for scalable speech recognition systems.

## 图像和视频处理(eess.IV:Image and Video Processing)

### Towards Zero-Shot Task-Generalizable Learning on fMRI 
[[arxiv](https://arxiv.org/abs/2502.10662)] [[cool](https://papers.cool/arxiv/2502.10662)] [[pdf](https://arxiv.org/pdf/2502.10662)]
> **Authors**: Jiyao Wang,Nicha C. Dvornek,Peiyu Duan,Lawrence H. Staib,James S. Duncan
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: ISBI 2025
- **标题**: None
- **领域**: 图像和视频处理,机器学习
- **Abstract**: Functional MRI measuring BOLD signal is an increasingly important imaging modality in studying brain functions and neurological disorders. It can be acquired in either a resting-state or a task-based paradigm. Compared to resting-state fMRI, task-based fMRI is acquired while the subject is performing a specific task designed to enhance study-related brain activities. Consequently, it generally has more informative task-dependent signals. However, due to the variety of task designs, it is much more difficult than in resting state to aggregate task-based fMRI acquired in different tasks to train a generalizable model. To resolve this complication, we propose a supervised task-aware network TA-GAT that jointly learns a general-purpose encoder and task-specific contextual information. The encoder-generated embedding and the learned contextual information are then combined as input to multiple modules for performing downstream tasks. We believe that the proposed task-aware architecture can plug-and-play in any neural network architecture to incorporate the prior knowledge of fMRI tasks into capturing functional brain patterns.

### Deep Learning for Wound Tissue Segmentation: A Comprehensive Evaluation using A Novel Dataset 
[[arxiv](https://arxiv.org/abs/2502.10652)] [[cool](https://papers.cool/arxiv/2502.10652)] [[pdf](https://arxiv.org/pdf/2502.10652)]
> **Authors**: Muhammad Ashad Kabir,Nidita Roy,Md. Ekramul Hossain,Jill Featherston,Sayed Ahmed
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 35 pages
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习
- **Abstract**: Deep learning (DL) techniques have emerged as promising solutions for medical wound tissue segmentation. However, a notable limitation in this field is the lack of publicly available labelled datasets and a standardised performance evaluation of state-of-the-art DL models on such datasets. This study addresses this gap by comprehensively evaluating various DL models for wound tissue segmentation using a novel dataset. We have curated a dataset comprising 147 wound images exhibiting six tissue types: slough, granulation, maceration, necrosis, bone, and tendon. The dataset was meticulously labelled for semantic segmentation employing supervised machine learning techniques. Three distinct labelling formats were developed -- full image, patch, and superpixel. Our investigation encompassed a wide array of DL segmentation and classification methodologies, ranging from conventional approaches like UNet, to generative adversarial networks such as cGAN, and modified techniques like FPN+VGG16. Also, we explored DL-based classification methods (e.g., ResNet50) and machine learning-based classification leveraging DL features (e.g., AlexNet+RF). In total, 82 wound tissue segmentation models were derived across the three labelling formats. Our analysis yielded several notable findings, including identifying optimal DL models for each labelling format based on weighted average Dice or F1 scores. Notably, FPN+VGG16 emerged as the top-performing DL model for wound tissue segmentation, achieving a dice score of 82.25%. This study provides a valuable benchmark for evaluating wound image segmentation and classification models, offering insights to inform future research and clinical practice in wound care. The labelled dataset created in this study is available at https://github.com/akabircs/WoundTissue.

### SAMRI-2: A Memory-based Model for Cartilage and Meniscus Segmentation in 3D MRIs of the Knee Joint 
[[arxiv](https://arxiv.org/abs/2502.10559)] [[cool](https://papers.cool/arxiv/2502.10559)] [[pdf](https://arxiv.org/pdf/2502.10559)]
> **Authors**: Danielle L. Ferreira,Bruno A. A. Nunes,Xuzhe Zhang,Laura Carretero Gomez,Maggie Fung,Ravi Soni
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **Abstract**: Accurate morphometric assessment of cartilage-such as thickness/volume-via MRI is essential for monitoring knee osteoarthritis. Segmenting cartilage remains challenging and dependent on extensive expert-annotated datasets, which are heavily subjected to inter-reader variability. Recent advancements in Visual Foundational Models (VFM), especially memory-based approaches, offer opportunities for improving generalizability and robustness. This study introduces a deep learning (DL) method for cartilage and meniscus segmentation from 3D MRIs using interactive, memory-based VFMs. To improve spatial awareness and convergence, we incorporated a Hybrid Shuffling Strategy (HSS) during training and applied a segmentation mask propagation technique to enhance annotation efficiency. We trained four AI models-a CNN-based 3D-VNet, two automatic transformer-based models (SaMRI2D and SaMRI3D), and a transformer-based promptable memory-based VFM (SAMRI-2)-on 3D knee MRIs from 270 patients using public and internal datasets and evaluated on 57 external cases, including multi-radiologist annotations and different data acquisitions. Model performance was assessed against reference standards using Dice Score (DSC) and Intersection over Union (IoU), with additional morphometric evaluations to further quantify segmentation accuracy. SAMRI-2 model, trained with HSS, outperformed all other models, achieving an average DSC improvement of 5 points, with a peak improvement of 12 points for tibial cartilage. It also demonstrated the lowest cartilage thickness errors, reducing discrepancies by up to threefold. Notably, SAMRI-2 maintained high performance with as few as three user clicks per volume, reducing annotation effort while ensuring anatomical precision. This memory-based VFM with spatial awareness offers a novel approach for reliable AI-assisted knee MRI segmentation, advancing DL in musculoskeletal imaging.

## 信号处理(eess.SP:Signal Processing)

### DT4ECG: A Dual-Task Learning Framework for ECG-Based Human Identity Recognition and Human Activity Detection 
[[arxiv](https://arxiv.org/abs/2502.11023)] [[cool](https://papers.cool/arxiv/2502.11023)] [[pdf](https://arxiv.org/pdf/2502.11023)]
> **Authors**: Siyu You,Boyuan Gu,Yanhui Yang,Shiyu Yu,Shisheng Guo
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 信号处理,机器学习
- **Abstract**: This article introduces DT4ECG, an innovative dual-task learning framework for Electrocardiogram (ECG)-based human identity recognition and activity detection. The framework employs a robust one-dimensional convolutional neural network (1D-CNN) backbone integrated with residual blocks to extract discriminative ECG features. To enhance feature representation, we propose a novel Sequence Channel Attention (SCA) mechanism, which combines channel-wise and sequential context attention to prioritize informative features across both temporal and channel dimensions. Furthermore, to address gradient imbalance in multi-task learning, we integrate GradNorm, a technique that dynamically adjusts loss weights based on gradient magnitudes, ensuring balanced training across tasks. Experimental results demonstrate the superior performance of our model, achieving accuracy rates of 99.12% in ID classification and 90.11% in activity classification. These findings underscore the potential of the DT4ECG framework in enhancing security and user experience across various applications such as fitness monitoring and personalized healthcare, thereby presenting a transformative approach to integrating ECG-based biometrics in everyday technologies.

## 高能物理-现象学(hep-ph:High Energy Physics - Phenomenology)

### Enhancing anomaly detection with topology-aware autoencoders 
[[arxiv](https://arxiv.org/abs/2502.10163)] [[cool](https://papers.cool/arxiv/2502.10163)] [[pdf](https://arxiv.org/pdf/2502.10163)]
> **Authors**: Vishal S. Ngairangbam,Błażej Rozwoda,Kazuki Sakurai,Michael Spannowsky
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 12 pages, 5 figures, 2 tables
- **标题**: None
- **领域**: 高能物理-现象学,机器学习,高能物理-实验
- **Abstract**: Anomaly detection in high-energy physics is essential for identifying new physics beyond the Standard Model. Autoencoders provide a signal-agnostic approach but are limited by the topology of their latent space. This work explores topology-aware autoencoders, embedding phase-space distributions onto compact manifolds that reflect energy-momentum conservation. We construct autoencoders with spherical ($S^n$), product ($S^2 \otimes S^2$), and projective ($\mathbb{RP}^2$) latent spaces and compare their anomaly detection performance against conventional Euclidean embeddings. Our results show that autoencoders with topological priors significantly improve anomaly separation by preserving the global structure of the data manifold and reducing spurious reconstruction errors. Applying our approach to simulated hadronic top-quark decays, we show that latent spaces with appropriate topological constraints enhance sensitivity and robustness in detecting anomalous events. This study establishes topology-aware autoencoders as a powerful tool for unsupervised searches for new physics in particle-collision data.

## 数值分析(math.NA:Numerical Analysis)

### Learning the Exact Time Integration Algorithm for Initial Value Problems by Randomized Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.10949)] [[cool](https://papers.cool/arxiv/2502.10949)] [[pdf](https://arxiv.org/pdf/2502.10949)]
> **Authors**: Suchuan Dong,Naxian Ni
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: 40 pages, 28 figures, 8 tables
- **标题**: None
- **领域**: 数值分析,机器学习,计算物理
- **Abstract**: We present a method leveraging extreme learning machine (ELM) type randomized neural networks (NNs) for learning the exact time integration algorithm for initial value problems (IVPs). The exact time integration algorithm for non-autonomous systems can be represented by an algorithmic function in higher dimensions, which satisfies an associated system of partial differential equations with corresponding boundary conditions. Our method learns the algorithmic function by solving this associated system using ELM with a physics informed approach. The trained ELM network serves as the learned algorithm and can be used to solve the IVP with arbitrary initial data or step sizes from some domain. When the right hand side of the non-autonomous system exhibits a periodicity with respect to any of its arguments, while the solution itself to the problem is not periodic, we show that the algorithmic function is either periodic, or when it is not, satisfies a well-defined relation for different periods. This property can greatly simplify the algorithm learning in many problems. We consider explicit and implicit NN formulations, leading to explicit or implicit time integration algorithms, and discuss how to train the ELM network by the nonlinear least squares method. Extensive numerical experiments with benchmark problems, including non-stiff, stiff and chaotic systems, show that the learned NN algorithm produces highly accurate solutions in long-time simulations, with its time-marching errors decreasing nearly exponentially with increasing degrees of freedom in the neural network. We compare extensively the computational performance (accuracy vs.~cost) between the current NN algorithm and the leading traditional time integration algorithms. The learned NN algorithm is computationally competitive, markedly outperforming the traditional algorithms in many problems.

## 数论(math.NT:Number Theory)

### Learning Euler Factors of Elliptic Curves 
[[arxiv](https://arxiv.org/abs/2502.10357)] [[cool](https://papers.cool/arxiv/2502.10357)] [[pdf](https://arxiv.org/pdf/2502.10357)]
> **Authors**: Angelica Babei,François Charton,Edgar Costa,Xiaoyu Huang,Kyu-Hwan Lee,David Lowry-Duda,Ashvni Narayanan,Alexey Pozdnyakov
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 18 pages
- **标题**: None
- **领域**: 数论,机器学习
- **Abstract**: We apply transformer models and feedforward neural networks to predict Frobenius traces $a_p$ from elliptic curves given other traces $a_q$. We train further models to predict $a_p \bmod 2$ from $a_q \bmod 2$, and cross-analysis such as $a_p \bmod 2$ from $a_q$. Our experiments reveal that these models achieve high accuracy, even in the absence of explicit number-theoretic tools like functional equations of $L$-functions. We also present partial interpretability findings.

### Studying number theory with deep learning: a case study with the Möbius and squarefree indicator functions 
[[arxiv](https://arxiv.org/abs/2502.10335)] [[cool](https://papers.cool/arxiv/2502.10335)] [[pdf](https://arxiv.org/pdf/2502.10335)]
> **Authors**: David Lowry-Duda
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 10 pages
- **标题**: None
- **领域**: 数论,机器学习
- **Abstract**: Building on work of Charton, we train small transformer models to calculate the Möbius function $μ(n)$ and the squarefree indicator function $μ^2(n)$. The models attain nontrivial predictive power. We then iteratively train additional models to understand how the model functions, ultimately finding a theoretical explanation.

## 优化与控制(math.OC:Optimization and Control)

### Provable and Practical Online Learning Rate Adaptation with Hypergradient Descent 
[[arxiv](https://arxiv.org/abs/2502.11229)] [[cool](https://papers.cool/arxiv/2502.11229)] [[pdf](https://arxiv.org/pdf/2502.11229)]
> **Authors**: Ya-Chi Chu,Wenzhi Gao,Yinyu Ye,Madeleine Udell
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 优化与控制,机器学习
- **Abstract**: This paper investigates the convergence properties of the hypergradient descent method (HDM), a 25-year-old heuristic originally proposed for adaptive stepsize selection in stochastic first-order methods. We provide the first rigorous convergence analysis of HDM using the online learning framework of [Gao24] and apply this analysis to develop new state-of-the-art adaptive gradient methods with empirical and theoretical support. Notably, HDM automatically identifies the optimal stepsize for the local optimization landscape and achieves local superlinear convergence. Our analysis explains the instability of HDM reported in the literature and proposes efficient strategies to address it. We also develop two HDM variants with heavy-ball and Nesterov momentum. Experiments on deterministic convex problems show HDM with heavy-ball momentum (HDM-HB) exhibits robust performance and significantly outperforms other adaptive first-order methods. Moreover, HDM-HB often matches the performance of L-BFGS, an efficient and practical quasi-Newton method, using less memory and cheaper iterations.

### Stochastic Optimization of Inventory at Large-scale Supply Chains 
[[arxiv](https://arxiv.org/abs/2502.11213)] [[cool](https://papers.cool/arxiv/2502.11213)] [[pdf](https://arxiv.org/pdf/2502.11213)]
> **Authors**: Zhaoyang Larry Jin,Mehdi Maasoumy,Yimin Liu,Zeshi Zheng,Zizhuo Ren
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 优化与控制,人工智能,机器学习
- **Abstract**: Today's global supply chains face growing challenges due to rapidly changing market conditions, increased network complexity and inter-dependency, and dynamic uncertainties in supply, demand, and other factors. To combat these challenges, organizations employ Material Requirements Planning (MRP) software solutions to set inventory stock buffers - for raw materials, work-in-process goods, and finished products - to help them meet customer service levels. However, holding excess inventory further complicates operations and can lock up millions of dollars of capital that could be otherwise deployed. Furthermore, most commercially available MRP solutions fall short in considering uncertainties and do not result in optimal solutions for modern enterprises. At C3 AI, we fundamentally reformulate the inventory management problem as a constrained stochastic optimization. We then propose a simulation-optimization framework that minimizes inventory and related costs while maintaining desired service levels. The framework's goal is to find the optimal reorder parameters that minimize costs subject to a pre-defined service-level constraint and all other real-world operational constraints. These optimal reorder parameters can be fed back into an MRP system to drive optimal order placement, or used to place optimal orders directly. This approach has proven successful in reducing inventory levels by 10-35 percent, resulting in hundreds of millions of dollars of economic benefit for major enterprises at a global scale.

### Error Bound Analysis for the Regularized Loss of Deep Linear Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.11152)] [[cool](https://papers.cool/arxiv/2502.11152)] [[pdf](https://arxiv.org/pdf/2502.11152)]
> **Authors**: Po Chen,Rujun Jiang,Peng Wang
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 55 pages, 2 figures
- **标题**: None
- **领域**: 优化与控制,机器学习
- **Abstract**: The optimization foundations of deep linear networks have received significant attention lately. However, due to the non-convexity and hierarchical structure, analyzing the regularized loss of deep linear networks remains a challenging task. In this work, we study the local geometric landscape of the regularized squared loss of deep linear networks, providing a deeper understanding of its optimization properties. Specifically, we characterize the critical point set and establish an error-bound property for all critical points under mild conditions. Notably, we identify the sufficient and necessary conditions under which the error bound holds. To support our theoretical findings, we conduct numerical experiments demonstrating that gradient descent exhibits linear convergence when optimizing the regularized loss of deep linear networks.

## 统计理论(math.ST:Statistics Theory)

### A statistical theory of overfitting for imbalanced classification 
[[arxiv](https://arxiv.org/abs/2502.11323)] [[cool](https://papers.cool/arxiv/2502.11323)] [[pdf](https://arxiv.org/pdf/2502.11323)]
> **Authors**: Jingyang Lyu,Kangjie Zhou,Yiqiao Zhong
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 119 pages, 14 figures
- **标题**: None
- **领域**: 统计理论,机器学习,机器学习
- **Abstract**: Classification with imbalanced data is a common challenge in data analysis, where certain classes (minority classes) account for a small fraction of the training data compared with other classes (majority classes). Classical statistical theory based on large-sample asymptotics and finite-sample corrections is often ineffective for high-dimensional data, leaving many overfitting phenomena in empirical machine learning unexplained. In this paper, we develop a statistical theory for high-dimensional imbalanced classification by investigating support vector machines and logistic regression. We find that dimensionality induces truncation or skewing effects on the logit distribution, which we characterize via a variational problem under high-dimensional asymptotics. In particular, for linearly separable data generated from a two-component Gaussian mixture model, the logits from each class follow a normal distribution $\mathsf{N}(0,1)$ on the testing set, but asymptotically follow a rectified normal distribution $\max\{κ, \mathsf{N}(0,1)\}$ on the training set -- which is a pervasive phenomenon we verified on tabular data, image data, and text data. This phenomenon explains why the minority class is more severely affected by overfitting. Further, we show that margin rebalancing, which incorporates class sizes into the loss function, is crucial for mitigating the accuracy drop for the minority class. Our theory also provides insights into the effects of overfitting on calibration and other uncertain quantification measures.

## 医学物理(physics.med-ph:Medical Physics)

### Exploiting network optimization stability for enhanced PET image denoising using deep image prior 
[[arxiv](https://arxiv.org/abs/2502.11259)] [[cool](https://papers.cool/arxiv/2502.11259)] [[pdf](https://arxiv.org/pdf/2502.11259)]
> **Authors**: Fumio Hashimoto,Kibo Ote,Yuya Onishi,Hideaki Tashima,Go Akamatsu,Yuma Iwao,Miwako Takahashi,Taiga Yamaya
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 10 pages, 8 figures
- **标题**: None
- **领域**: 医学物理,计算机视觉和模式识别
- **Abstract**: PET is affected by statistical noise due to constraints on tracer dose and scan duration, impacting both diagnostic performance and quantitative accuracy. While deep learning (DL)-based PET denoising methods have been used to improve image quality, they may introduce over-smoothing, compromising quantitative accuracy. We propose a method for making a DL solution more reliable and apply it to the conditional deep image prior (DIP). We introduce the idea of stability information in the optimization process of conditional DIP, enabling the identification of unstable regions within the network's optimization trajectory. Our method incorporates a stability map, which is derived from multiple intermediate outputs of moderate network at different optimization steps. The final denoised image is then obtained by computing linear combination of the DIP output and the original reconstructed image, weighted by the stability map. Our method effectively reduces noise while preserving small structure details in brain FDG images. Results demonstrated that our approach outperformed existing methods in peak-to-valley ratio and noise suppression across various low-dose levels. Region-of-interest analysis confirmed that the proposed method maintains quantitative accuracy without introducing under- or over-estimation. We applied our method to full-dose PET data to assess its impact on image quality. The results revealed that the proposed method significantly reduced background noise while preserving the peak-to-valley ratio at a level comparable to that of unfiltered full-dose PET images. The proposed method introduces a robust approach to DL-based PET denoising, enhancing its reliability and preserving quantitative accuracy. This strategy has the potential to advance performance in high-sensitivity PET scanners, demonstrating that DL can extend PET imaging capabilities beyond low-dose applications.

## 等离子体物理(physics.plasm-ph:Plasma Physics)

### Multiscale autonomous forecasting of plasma systems' dynamics using neural networks 
[[arxiv](https://arxiv.org/abs/2502.11203)] [[cool](https://papers.cool/arxiv/2502.11203)] [[pdf](https://arxiv.org/pdf/2502.11203)]
> **Authors**: Farbod Faraji,Maryam Reza
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 29 pages, 25 figures
- **标题**: None
- **领域**: 等离子体物理,机器学习
- **Abstract**: Plasma systems exhibit complex multiscale dynamics, resolving which poses significant challenges for conventional numerical simulations. Machine learning (ML) offers an alternative by learning data-driven representations of these dynamics. Yet existing ML time-stepping models suffer from error accumulation, instability, and limited long-term forecasting horizons. This paper demonstrates the application of a hierarchical multiscale neural network architecture for autonomous plasma forecasting. The framework integrates multiple neural networks trained across different temporal scales to capture both fine-scale and large-scale behaviors while mitigating compounding error in recursive evaluation. Fine-scale networks accurately resolve fast-evolving features, while coarse-scale networks provide broader temporal context, reducing the frequency of recursive updates and limiting the accumulation of small prediction errors over time. We first evaluate the method using canonical nonlinear dynamical systems and compare its performance against classical single-scale neural networks. The results demonstrate that single-scale neural networks experience rapid divergence due to recursive error accumulation, whereas the multiscale approach improves stability and extends prediction horizons. Next, our ML model is applied to two plasma configurations of high scientific and applied significance, demonstrating its ability to preserve spatial structures and capture multiscale plasma dynamics. By leveraging multiple time-stepping resolutions, the applied framework is shown to outperform conventional single-scale networks for the studied plasma test cases. The results of this work position the hierarchical multiscale neural network as a promising tool for efficient plasma forecasting and digital twin applications.

## 生物分子(q-bio.BM:Biomolecules)

### CL-MFAP: A Contrastive Learning-Based Multimodal Foundation Model for Molecular Property Prediction and Antibiotic Screening 
[[arxiv](https://arxiv.org/abs/2502.11001)] [[cool](https://papers.cool/arxiv/2502.11001)] [[pdf](https://arxiv.org/pdf/2502.11001)]
> **Authors**: Gen Zhou,Sugitha Janarthanan,Yutong Lu,Pingzhao Hu
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: Gen Zhou and Sugitha Janarthanan contributed equally; Accepted at ICLR 2025
- **标题**: None
- **领域**: 生物分子,人工智能,机器学习,定量方法
- **Abstract**: Due to the rise in antimicrobial resistance, identifying novel compounds with antibiotic potential is crucial for combatting this global health issue. However, traditional drug development methods are costly and inefficient. Recognizing the pressing need for more effective solutions, researchers have turned to machine learning techniques to streamline the prediction and development of novel antibiotic compounds. While foundation models have shown promise in antibiotic discovery, current mainstream efforts still fall short of fully leveraging the potential of multimodal molecular data. Recent studies suggest that contrastive learning frameworks utilizing multimodal data exhibit excellent performance in representation learning across various domains. Building upon this, we introduce CL-MFAP, an unsupervised contrastive learning (CL)-based multimodal foundation (MF) model specifically tailored for discovering small molecules with potential antibiotic properties (AP) using three types of molecular data. This model employs 1.6 million bioactive molecules with drug-like properties from the ChEMBL dataset to jointly pretrain three encoders: (1) a transformer-based encoder with rotary position embedding for processing SMILES strings; (2) another transformer-based encoder, incorporating a novel bi-level routing attention mechanism to handle molecular graph representations; and (3) a Morgan fingerprint encoder using a multilayer perceptron, to achieve the contrastive learning purpose. The CL-MFAP outperforms baseline models in antibiotic property prediction by effectively utilizing different molecular modalities and demonstrates superior domain-specific performance when fine-tuned for antibiotic-related property prediction tasks.

### Agentic End-to-End De Novo Protein Design for Tailored Dynamics Using a Language Diffusion Model 
[[arxiv](https://arxiv.org/abs/2502.10173)] [[cool](https://papers.cool/arxiv/2502.10173)] [[pdf](https://arxiv.org/pdf/2502.10173)]
> **Authors**: Bo Ni,Markus J. Buehler
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 生物分子,介观和纳米物理,材料科学,机器学习
- **Abstract**: Proteins are dynamic molecular machines whose biological functions, spanning enzymatic catalysis, signal transduction, and structural adaptation, are intrinsically linked to their motions. Designing proteins with targeted dynamic properties, however, remains a challenge due to the complex, degenerate relationships between sequence, structure, and molecular motion. Here, we introduce VibeGen, a generative AI framework that enables end-to-end de novo protein design conditioned on normal mode vibrations. VibeGen employs an agentic dual-model architecture, comprising a protein designer that generates sequence candidates based on specified vibrational modes and a protein predictor that evaluates their dynamic accuracy. This approach synergizes diversity, accuracy, and novelty during the design process. Via full-atom molecular simulations as direct validation, we demonstrate that the designed proteins accurately reproduce the prescribed normal mode amplitudes across the backbone while adopting various stable, functionally relevant structures. Notably, generated sequences are de novo, exhibiting no significant similarity to natural proteins, thereby expanding the accessible protein space beyond evolutionary constraints. Our work integrates protein dynamics into generative protein design, and establishes a direct, bidirectional link between sequence and vibrational behavior, unlocking new pathways for engineering biomolecules with tailored dynamical and functional properties. This framework holds broad implications for the rational design of flexible enzymes, dynamic scaffolds, and biomaterials, paving the way toward dynamics-informed AI-driven protein engineering.

## 神经元和认知(q-bio.NC:Neurons and Cognition)

### Graceful forgetting: Memory as a process 
[[arxiv](https://arxiv.org/abs/2502.11105)] [[cool](https://papers.cool/arxiv/2502.11105)] [[pdf](https://arxiv.org/pdf/2502.11105)]
> **Authors**: Alain de Cheveigné
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 神经元和认知,信息检索,机器学习
- **Abstract**: A rational theory of memory is proposed to explain how we can accommodate unbounded sensory input within bounded storage space. Memory is stored as statistics, organized into complex structures that are constantly summarized and compressed to make room for new input. This process, driven by space constraints, is guided by heuristics that optimize the memory for future needs. Sensory input is rapidly encoded as simple statistics that are more slowly elaborated into more abstract constructs. This theory differs from previous accounts of memory by (a) its reliance on statistics, (b) its use of heuristics to guide the choice of statistics, and (c) the emphasis on memory as a process that is intensive, complex, and expensive. The theory is intended as an aid to make sense of our extensive knowledge of memory, and bring us closer to an understanding of memory in functional and mechanistic terms.

## 量子物理学(quant-ph:Quantum Physics)

### Towards identifying possible fault-tolerant advantage of quantum linear system algorithms in terms of space, time and energy 
[[arxiv](https://arxiv.org/abs/2502.11239)] [[cool](https://papers.cool/arxiv/2502.11239)] [[pdf](https://arxiv.org/pdf/2502.11239)]
> **Authors**: Yue Tu,Mark Dubynskyi,Mohammadhossein Mohammadisiahroudi,Ekaterina Riashchentceva,Jinglei Cheng,Dmitry Ryashchentsev,Tamás Terlaky,Junyu Liu
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 28 pages, many figures. v2: correcting typos
- **标题**: None
- **领域**: 量子物理学,人工智能,机器学习,优化与控制
- **Abstract**: Quantum computing, a prominent non-Von Neumann paradigm beyond Moore's law, can offer superpolynomial speedups for certain problems. Yet its advantages in efficiency for tasks like machine learning remain under investigation, and quantum noise complicates resource estimations and classical comparisons. We provide a detailed estimation of space, time, and energy resources for fault-tolerant superconducting devices running the Harrow-Hassidim-Lloyd (HHL) algorithm, a quantum linear system solver relevant to linear algebra and machine learning. Excluding memory and data transfer, possible quantum advantages over the classical conjugate gradient method could emerge at $N \approx 2^{33} \sim 2^{48}$ or even lower, requiring ${O}(10^5)$ physical qubits, ${O}(10^{12}\sim10^{13})$ Joules, and ${O}(10^6)$ seconds under surface code fault-tolerance with three types of magic state distillation (15-1, 116-12, 225-1). Key parameters include condition number, sparsity, and precision $κ, s\approx{O}(10\sim100)$, $ε\sim0.01$, and physical error $10^{-5}$. Our resource estimator adjusts $N, κ, s, ε$, providing a map of quantum-classical boundaries and revealing where a practical quantum advantage may arise. Our work quantitatively determine how advanced a fault-tolerant quantum computer should be to achieve possible, significant benefits on problems related to real-world.

### Evaluating the Potential of Quantum Machine Learning in Cybersecurity: A Case-Study on PCA-based Intrusion Detection Systems 
[[arxiv](https://arxiv.org/abs/2502.11173)] [[cool](https://papers.cool/arxiv/2502.11173)] [[pdf](https://arxiv.org/pdf/2502.11173)]
> **Authors**: Armando Bellante,Tommaso Fioravanti,Michele Carminati,Stefano Zanero,Alessandro Luongo
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: Computers & Security (2025): 104341
- **标题**: None
- **领域**: 量子物理学,密码学和安全,机器学习,网络和互联网架构
- **Abstract**: Quantum computing promises to revolutionize our understanding of the limits of computation, and its implications in cryptography have long been evident. Today, cryptographers are actively devising post-quantum solutions to counter the threats posed by quantum-enabled adversaries. Meanwhile, quantum scientists are innovating quantum protocols to empower defenders. However, the broader impact of quantum computing and quantum machine learning (QML) on other cybersecurity domains still needs to be explored. In this work, we investigate the potential impact of QML on cybersecurity applications of traditional ML. First, we explore the potential advantages of quantum computing in machine learning problems specifically related to cybersecurity. Then, we describe a methodology to quantify the future impact of fault-tolerant QML algorithms on real-world problems. As a case study, we apply our approach to standard methods and datasets in network intrusion detection, one of the most studied applications of machine learning in cybersecurity. Our results provide insight into the conditions for obtaining a quantum advantage and the need for future quantum hardware and software advancements.

### Machine Learning for Phase Estimation in Satellite-to-Earth Quantum Communication 
[[arxiv](https://arxiv.org/abs/2502.09920)] [[cool](https://papers.cool/arxiv/2502.09920)] [[pdf](https://arxiv.org/pdf/2502.09920)]
> **Authors**: Nathan K Long,Robert Malaney,Kenneth J Grant
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 量子物理学,人工智能,信号处理
- **Abstract**: A global continuous-variable quantum key distribution (CV-QKD) network can be established using a series of satellite-to-Earth channels. Increased performance in such a network is provided by performing coherent measurement of the optical quantum signals using a real local oscillator, calibrated locally by encoding known information on transmitted reference pulses and using signal phase error estimation algorithms. The speed and accuracy of the signal phase error estimation algorithm are vital to practical CV-QKD implementation. Our work provides a framework to analyze long short-term memory neural network (NN) architecture parameterization, with respect to the quantum Cramér-Rao uncertainty bound of the signal phase error estimation, with a focus on reducing the model complexity. More specifically, we demonstrate that signal phase error estimation can be achieved using a low-complexity NN architecture, without significantly sacrificing accuracy. Our results significantly improve the real-time performance of practical CV-QKD systems deployed over satellite-to-Earth channels, thereby contributing to the ongoing development of the Quantum Internet.

## 方法论(stat.ME:Methodology)

### Transfer Learning of CATE with Kernel Ridge Regression 
[[arxiv](https://arxiv.org/abs/2502.11331)] [[cool](https://papers.cool/arxiv/2502.11331)] [[pdf](https://arxiv.org/pdf/2502.11331)]
> **Authors**: Seok-Jin Kim,Hongjie Liu,Molei Liu,Kaizheng Wang
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 方法论,机器学习,机器学习
- **Abstract**: The proliferation of data has sparked significant interest in leveraging findings from one study to estimate treatment effects in a different target population without direct outcome observations. However, the transfer learning process is frequently hindered by substantial covariate shift and limited overlap between (i) the source and target populations, as well as (ii) the treatment and control groups within the source. We propose a novel method for overlap-adaptive transfer learning of conditional average treatment effect (CATE) using kernel ridge regression (KRR). Our approach involves partitioning the labeled source data into two subsets. The first one is used to train candidate CATE models based on regression adjustment and pseudo-outcomes. An optimal model is then selected using the second subset and unlabeled target data, employing another pseudo-outcome-based strategy. We provide a theoretical justification for our method through sharp non-asymptotic MSE bounds, highlighting its adaptivity to both weak overlaps and the complexity of CATE function. Extensive numerical studies confirm that our method achieves superior finite-sample efficiency and adaptability. We conclude by demonstrating the effectiveness of our approach using a 401(k) eligibility dataset.

## 机器学习(stat.ML:Machine Learning)

### Robust High-Dimensional Mean Estimation With Low Data Size, an Empirical Study 
[[arxiv](https://arxiv.org/abs/2502.11324)] [[cool](https://papers.cool/arxiv/2502.11324)] [[pdf](https://arxiv.org/pdf/2502.11324)]
> **Authors**: Cullen Anderson,Jeff M. Phillips
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: ef:Transactions onMachineLearningResearch (TMLR), February 2025, ISSN: 2835-8856
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Robust statistics aims to compute quantities to represent data where a fraction of it may be arbitrarily corrupted. The most essential statistic is the mean, and in recent years, there has been a flurry of theoretical advancement for efficiently estimating the mean in high dimensions on corrupted data. While several algorithms have been proposed that achieve near-optimal error, they all rely on large data size requirements as a function of dimension. In this paper, we perform an extensive experimentation over various mean estimation techniques where data size might not meet this requirement due to the high-dimensional setting.

### Generalized Factor Neural Network Model for High-dimensional Regression 
[[arxiv](https://arxiv.org/abs/2502.11310)] [[cool](https://papers.cool/arxiv/2502.11310)] [[pdf](https://arxiv.org/pdf/2502.11310)]
> **Authors**: Zichuan Guo,Mihai Cucuringu,Alexander Y. Shestopaloff
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-17
> **comment**: 43 pages, 13 figures
- **标题**: None
- **领域**: 机器学习,机器学习,统计金融
- **Abstract**: We tackle the challenges of modeling high-dimensional data sets, particularly those with latent low-dimensional structures hidden within complex, non-linear, and noisy relationships. Our approach enables a seamless integration of concepts from non-parametric regression, factor models, and neural networks for high-dimensional regression. Our approach introduces PCA and Soft PCA layers, which can be embedded at any stage of a neural network architecture, allowing the model to alternate between factor modeling and non-linear transformations. This flexibility makes our method especially effective for processing hierarchical compositional data. We explore ours and other techniques for imposing low-rank structures on neural networks and examine how architectural design impacts model performance. The effectiveness of our method is demonstrated through simulation studies, as well as applications to forecasting future price movements of equity ETF indices and nowcasting with macroeconomic data.

### Dynamic Influence Tracker: Measuring Time-Varying Sample Influence During Training 
[[arxiv](https://arxiv.org/abs/2502.10793)] [[cool](https://papers.cool/arxiv/2502.10793)] [[pdf](https://arxiv.org/pdf/2502.10793)]
> **Authors**: Jie Xu,Zihan Wu
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Existing methods for measuring training sample influence on models only provide static, overall measurements, overlooking how sample influence changes during training. We propose Dynamic Influence Tracker (DIT), which captures the time-varying sample influence across arbitrary time windows during training. DIT offers three key insights: 1) Samples show different time-varying influence patterns, with some samples important in the early training stage while others become important later. 2) Sample influences show a weak correlation between early and late stages, demonstrating that the model undergoes distinct learning phases with shifting priorities. 3) Analyzing influence during the convergence period provides more efficient and accurate detection of corrupted samples than full-training analysis. Supported by theoretical guarantees without assuming loss convexity or model convergence, DIT significantly outperforms existing methods, achieving up to 0.99 correlation with ground truth and above 98\% accuracy in detecting corrupted samples in complex architectures.

### Generative Adversarial Networks for High-Dimensional Item Factor Analysis: A Deep Adversarial Learning Algorithm 
[[arxiv](https://arxiv.org/abs/2502.10650)] [[cool](https://papers.cool/arxiv/2502.10650)] [[pdf](https://arxiv.org/pdf/2502.10650)]
> **Authors**: Nanyu Luo,Feng Ji
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习,应用领域,计算,方法论
- **Abstract**: Advances in deep learning and representation learning have transformed item factor analysis (IFA) in the item response theory (IRT) literature by enabling more efficient and accurate parameter estimation. Variational Autoencoders (VAEs) have been one of the most impactful techniques in modeling high-dimensional latent variables in this context. However, the limited expressiveness of the inference model based on traditional VAEs can still hinder the estimation performance. This study introduces Adversarial Variational Bayes (AVB) algorithms as an improvement to VAEs for IFA with improved flexibility and accuracy. By bridging the strengths of VAEs and Generative Adversarial Networks (GANs), AVB incorporates an auxiliary discriminator network to reframe the estimation process as a two-player adversarial game and removes the restrictive assumption of standard normal distributions in the inference model. Theoretically, AVB can achieve similar or higher likelihood compared to VAEs. A further enhanced algorithm, Importance-weighted Adversarial Variational Bayes (IWAVB) is proposed and compared with Importance-weighted Autoencoders (IWAE). In an exploratory analysis of real empirical data, IWAVB demonstrated superior expressiveness by achieving a higher likelihood compared to IWAE. In confirmatory studies with simulated data, IWAVB achieved similar mean-square error results to IWAE while consistently achieving higher likelihoods. Moreover, in simulations where latent variables followed a multimodal distribution, IWAVB outperformed IWAE by providing more accurate parameter estimates. With its innovative use of GANs, IWAVB is shown to have the potential to extend IFA to handle large-scale data, facilitating the potential integration of psychometrics and multimodal data analysis.

### Batch-Adaptive Annotations for Causal Inference with Complex-Embedded Outcomes 
[[arxiv](https://arxiv.org/abs/2502.10605)] [[cool](https://papers.cool/arxiv/2502.10605)] [[pdf](https://arxiv.org/pdf/2502.10605)]
> **Authors**: Ezinne Nwankwo,Lauri Goldkind,Angela Zhou
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Estimating the causal effects of an intervention on outcomes is crucial. But often in domains such as healthcare and social services, this critical information about outcomes is documented by unstructured text, e.g. clinical notes in healthcare or case notes in social services. For example, street outreach to homeless populations is a common social services intervention, with ambiguous and hard-to-measure outcomes. Outreach workers compile case note records which are informative of outcomes. Although experts can succinctly extract relevant information from such unstructured case notes, it is costly or infeasible to do so for an entire corpus, which can span millions of notes. Recent advances in large language models (LLMs) enable scalable but potentially inaccurate annotation of unstructured text data. We leverage the decision of which datapoints should receive expert annotation vs. noisy imputation under budget constraints in a "design-based" estimator combining limited expert and plentiful noisy imputation data via \textit{causal inference with missing outcomes}. We develop a two-stage adaptive algorithm that optimizes the expert annotation probabilities, estimating the ATE with optimal asymptotic variance. We demonstrate how expert labels and LLM annotations can be combined strategically, efficiently and responsibly in a causal estimator. We run experiments on simulated data and two real-world datasets, including one on street outreach, to show the versatility of our proposed method.

### Weighted quantization using MMD: From mean field to mean shift via gradient flows 
[[arxiv](https://arxiv.org/abs/2502.10600)] [[cool](https://papers.cool/arxiv/2502.10600)] [[pdf](https://arxiv.org/pdf/2502.10600)]
> **Authors**: Ayoub Belhadji,Daniel Sharp,Youssef Marzouk
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习,数值分析
- **Abstract**: Approximating a probability distribution using a set of particles is a fundamental problem in machine learning and statistics, with applications including clustering and quantization. Formally, we seek a finite weighted mixture of Dirac measures that best approximates the target distribution. While much existing work relies on the Wasserstein distance to quantify approximation errors, maximum mean discrepancy (MMD) has received comparatively less attention, especially when allowing for variable particle weights. We study the quantization problem from the perspective of minimizing MMD via gradient flow in the Wasserstein-Fisher-Rao (WFR) geometry. This gradient flow yields an ODE system from which we further derive a fixed-point algorithm called mean shift interacting particles (MSIP). We show that MSIP extends the (non-interacting) mean shift algorithm, widely used for identifying modes in kernel density estimates. Moreover, we show that MSIP can be interpreted as preconditioned gradient descent, and that it acts as a relaxation of Lloyd's algorithm for clustering. Our numerical experiments demonstrate that MSIP and the WFR ODEs outperform other algorithms for quantization of multi-modal and high-dimensional targets.

### Forecasting time series with constraints 
[[arxiv](https://arxiv.org/abs/2502.10485)] [[cool](https://papers.cool/arxiv/2502.10485)] [[pdf](https://arxiv.org/pdf/2502.10485)]
> **Authors**: Nathan Doumèche,Francis Bach,Éloi Bedek,Gérard Biau,Claire Boyer,Yannig Goude
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习,统计理论,应用领域,方法论
- **Abstract**: Time series forecasting presents unique challenges that limit the effectiveness of traditional machine learning algorithms. To address these limitations, various approaches have incorporated linear constraints into learning algorithms, such as generalized additive models and hierarchical forecasting. In this paper, we propose a unified framework for integrating and combining linear constraints in time series forecasting. Within this framework, we show that the exact minimizer of the constrained empirical risk can be computed efficiently using linear algebra alone. This approach allows for highly scalable implementations optimized for GPUs. We validate the proposed methodology through extensive benchmarking on real-world tasks, including electricity demand forecasting and tourism forecasting, achieving state-of-the-art performance.

### Generalised Parallel Tempering: Flexible Replica Exchange via Flows and Diffusions 
[[arxiv](https://arxiv.org/abs/2502.10328)] [[cool](https://papers.cool/arxiv/2502.10328)] [[pdf](https://arxiv.org/pdf/2502.10328)]
> **Authors**: Leo Zhang,Peter Potaptchik,Arnaud Doucet,Hai-Dang Dau,Saifuddin Syed
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Parallel Tempering (PT) is a classical MCMC algorithm designed for leveraging parallel computation to sample efficiently from high-dimensional, multimodal or otherwise complex distributions via annealing. One limitation of the standard formulation of PT is the growth of computational resources required to generate high-quality samples, as measured by effective sample size or round trip rate, for increasingly challenging distributions. To address this issue, we propose the framework: Generalised Parallel Tempering (GePT) which allows for the incorporation of recent advances in modern generative modelling, such as normalising flows and diffusion models, within Parallel Tempering, while maintaining the same theoretical guarantees as MCMC-based methods. For instance, we show that this allows us to utilise diffusion models in a parallelised manner, bypassing the usual computational cost of a large number of steps to generate quality samples. Further, we empirically demonstrate that GePT can improve sample quality and reduce the growth of computational resources required to handle complex distributions over the classical algorithm.

### AdaPTS: Adapting Univariate Foundation Models to Probabilistic Multivariate Time Series Forecasting 
[[arxiv](https://arxiv.org/abs/2502.10235)] [[cool](https://papers.cool/arxiv/2502.10235)] [[pdf](https://arxiv.org/pdf/2502.10235)]
> **Authors**: Abdelhakim Benechehab,Vasilii Feofanov,Giuseppe Paolo,Albert Thomas,Maurizio Filippone,Balázs Kégl
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Pre-trained foundation models (FMs) have shown exceptional performance in univariate time series forecasting tasks. However, several practical challenges persist, including managing intricate dependencies among features and quantifying uncertainty in predictions. This study aims to tackle these critical limitations by introducing adapters; feature-space transformations that facilitate the effective use of pre-trained univariate time series FMs for multivariate tasks. Adapters operate by projecting multivariate inputs into a suitable latent space and applying the FM independently to each dimension. Inspired by the literature on representation learning and partially stochastic Bayesian neural networks, we present a range of adapters and optimization/inference strategies. Experiments conducted on both synthetic and real-world datasets confirm the efficacy of adapters, demonstrating substantial enhancements in forecasting accuracy and uncertainty quantification compared to baseline methods. Our framework, AdaPTS, positions adapters as a modular, scalable, and effective solution for leveraging time series FMs in multivariate contexts, thereby promoting their wider adoption in real-world applications. We release the code at https://github.com/abenechehab/AdaPTS.

### Combinatorial Reinforcement Learning with Preference Feedback 
[[arxiv](https://arxiv.org/abs/2502.10158)] [[cool](https://papers.cool/arxiv/2502.10158)] [[pdf](https://arxiv.org/pdf/2502.10158)]
> **Authors**: Joongkyu Lee,Min-hwan Oh
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: Preprint. Under review
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: In this paper, we consider combinatorial reinforcement learning with preference feedback, where a learning agent sequentially offers an action--an assortment of multiple items to--a user, whose preference feedback follows a multinomial logistic (MNL) model. This framework allows us to model real-world scenarios, particularly those involving long-term user engagement, such as in recommender systems and online advertising. However, this framework faces two main challenges: (1) the unknown value of each item, unlike traditional MNL bandits that only address single-step preference feedback, and (2) the difficulty of ensuring optimism while maintaining tractable assortment selection in the combinatorial action space with unknown values. In this paper, we assume a contextual MNL preference model, where the mean utilities are linear, and the value of each item is approximated by a general function. We propose an algorithm, MNL-VQL, that addresses these challenges, making it both computationally and statistically efficient. As a special case, for linear MDPs (with the MNL preference feedback), we establish the first regret lower bound in this framework and show that MNL-VQL achieves nearly minimax-optimal regret. To the best of our knowledge, this is the first work to provide statistical guarantees in combinatorial RL with preference feedback.

### Improved Online Confidence Bounds for Multinomial Logistic Bandits 
[[arxiv](https://arxiv.org/abs/2502.10020)] [[cool](https://papers.cool/arxiv/2502.10020)] [[pdf](https://arxiv.org/pdf/2502.10020)]
> **Authors**: Joongkyu Lee,Min-hwan Oh
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: Preprint. Under review
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: In this paper, we propose an improved online confidence bound for multinomial logistic (MNL) models and apply this result to MNL bandits, achieving variance-dependent optimal regret. Recently, Lee & Oh (2024) established an online confidence bound for MNL models and achieved nearly minimax-optimal regret in MNL bandits. However, their results still depend on the norm-boundedness of the unknown parameter $B$ and the maximum size of possible outcomes $K$. To address this, we first derive an online confidence bound of $O\left(\sqrt{d \log t} + B \right)$, which is a significant improvement over the previous bound of $O (B \sqrt{d} \log t \log K )$ (Lee & Oh, 2024). This is mainly achieved by establishing tighter self-concordant properties of the MNL loss and introducing a novel intermediary term to bound the estimation error. Using this new online confidence bound, we propose a constant-time algorithm, OFU-MNL++, which achieves a variance-dependent regret bound of $O \Big( d \log T \sqrt{ \smash[b]{\sum_{t=1}^T} σ_t^2 } \Big) $ for sufficiently large $T$, where $σ_t^2$ denotes the variance of the rewards at round $t$, $d$ is the dimension of the contexts, and $T$ is the total number of rounds. Furthermore, we introduce a Maximum Likelihood Estimation (MLE)-based algorithm, OFU-MN$^2$L, which achieves an anytime poly(B)-free regret of $O \Big( d \log (BT) \sqrt{ \smash[b]{\sum_{t=1}^T} σ_t^2 } \Big) $.

### Estimation of the Learning Coefficient Using Empirical Loss 
[[arxiv](https://arxiv.org/abs/2502.09998)] [[cool](https://papers.cool/arxiv/2502.09998)] [[pdf](https://arxiv.org/pdf/2502.09998)]
> **Authors**: Tatsuyoshi Takio,Joe Suzuki
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: 15 pages, 6 figures, 4 tables
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: The learning coefficient plays a crucial role in analyzing the performance of information criteria, such as the Widely Applicable Information Criterion (WAIC) and the Widely Applicable Bayesian Information Criterion (WBIC), which Sumio Watanabe developed to assess model generalization ability. In regular statistical models, the learning coefficient is given by d/2, where d is the dimension of the parameter space. More generally, it is defined as the absolute value of the pole order of a zeta function derived from the Kullback-Leibler divergence and the prior distribution. However, except for specific cases such as reduced-rank regression, the learning coefficient cannot be derived in a closed form. Watanabe proposed a numerical method to estimate the learning coefficient, which Imai further refined to enhance its convergence properties. These methods utilize the asymptotic behavior of WBIC and have been shown to be statistically consistent as the sample size grows. In this paper, we propose a novel numerical estimation method that fundamentally differs from previous approaches and leverages a new quantity, "Empirical Loss," which was introduced by Watanabe. Through numerical experiments, we demonstrate that our proposed method exhibits both lower bias and lower variance compared to those of Watanabe and Imai. Additionally, we provide a theoretical analysis that elucidates why our method outperforms existing techniques and present empirical evidence that supports our findings.

### On Volume Minimization in Conformal Regression 
[[arxiv](https://arxiv.org/abs/2502.09985)] [[cool](https://papers.cool/arxiv/2502.09985)] [[pdf](https://arxiv.org/pdf/2502.09985)]
> **Authors**: Batiste Le Bars,Pierre Humbert
> **First submission**: 2025-02-14
> **First announcement**: 2025-02-17
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: We study the question of volume optimality in split conformal regression, a topic still poorly understood in comparison to coverage control. Using the fact that the calibration step can be seen as an empirical volume minimization problem, we first derive a finite-sample upper-bound on the excess volume loss of the interval returned by the classical split method. This important quantity measures the difference in length between the interval obtained with the split method and the shortest oracle prediction interval. Then, we introduce EffOrt, a methodology that modifies the learning step so that the base prediction function is selected in order to minimize the length of the returned intervals. In particular, our theoretical analysis of the excess volume loss of the prediction sets produced by EffOrt reveals the links between the learning and calibration steps, and notably the impact of the choice of the function class of the base predictor. We also introduce Ad-EffOrt, an extension of the previous method, which produces intervals whose size adapts to the value of the covariate. Finally, we evaluate the empirical performance and the robustness of our methodologies.

## 其他论文

- [Gensor: A Graph-based Construction Tensor Compilation Method for Deep Learning](https://arxiv.org/abs/2502.11407)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC in whitelist
- [RAG vs. GraphRAG: A Systematic Evaluation and Key Insights](https://arxiv.org/abs/2502.11371)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [Evaluating the Performance of the DeepSeek Model in Confidential Computing Environment](https://arxiv.org/abs/2502.11347)
  - **标题**: None
  - **Filtered Reason**: none of cs.PF in whitelist
- [Power-Measurement-Based Channel Autocorrelation Estimation for IRS-Assisted Wideband Communications](https://arxiv.org/abs/2502.11346)
  - **标题**: None
  - **Filtered Reason**: none of cs.IT in whitelist
- ["When I lost it, they dragged me out": How Care Encounters Empower Marginalized Young Adults' Mental Health Care-Seeking](https://arxiv.org/abs/2502.11277)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Prevalence, Sharing Patterns, and Spreaders of Multimodal AI-Generated Content on X during the 2024 U.S. Presidential Election](https://arxiv.org/abs/2502.11248)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY,cs.SI in whitelist
- [LLMs and Childhood Safety: Identifying Risks and Proposing a Protection Framework for Safe Child-LLM Interaction](https://arxiv.org/abs/2502.11242)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [Integrating Retrospective Framework in Multi-Robot Collaboration](https://arxiv.org/abs/2502.11227)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [AudioSpa: Spatializing Sound Events with Text](https://arxiv.org/abs/2502.11219)
  - **标题**: None
  - **Filtered Reason**: none of cs.SD,eess.AS in whitelist
- [Setting the Course, but Forgetting to Steer: Analyzing Compliance with GDPR's Right of Access to Data by Instagram, TikTok, and YouTube](https://arxiv.org/abs/2502.11208)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY,cs.HC in whitelist
- [CSP: A Simulator For Multi-Agent Ranking Competitions](https://arxiv.org/abs/2502.11197)
  - **标题**: None
  - **Filtered Reason**: none of cs.GT,cs.IR in whitelist
- [Combining GPU and CPU for accelerating evolutionary computing workloads](https://arxiv.org/abs/2502.11129)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC in whitelist
- [Ramp Up NTT in Record Time using GPU-Accelerated Algorithms and LLM-based Code Generation](https://arxiv.org/abs/2502.11110)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [Data Ecofeminism](https://arxiv.org/abs/2502.11086)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [DreamDDP: Accelerating Data Parallel Distributed LLM Training with Layer-wise Scheduled Partial Synchronization](https://arxiv.org/abs/2502.11058)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC in whitelist
- [HawkEye: Statically and Accurately Profiling the Communication Cost of Models in Multi-party Learning](https://arxiv.org/abs/2502.11029)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [Leveraging Large Language Models for Cybersecurity: Enhancing SMS Spam Detection with Robust and Context-Aware Text Classification](https://arxiv.org/abs/2502.11014)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [DFM: Deep Fourier Mimic for Expressive Dance Motion Learning](https://arxiv.org/abs/2502.10980)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [Open-Set Cross-Network Node Classification via Unknown-Excluded Adversarial Graph Domain Alignment](https://arxiv.org/abs/2502.10967)
  - **标题**: None
  - **Filtered Reason**: none of cs.SI in whitelist
- ["AI Afterlife" as Digital Legacy: Perceptions, Expectations, and Concerns](https://arxiv.org/abs/2502.10924)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [CodeA11y: Making AI Coding Assistants Useful for Accessible Web Development](https://arxiv.org/abs/2502.10884)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC,cs.SE in whitelist
- [Corotational Hinge-based Thin Plates/Shells](https://arxiv.org/abs/2502.10872)
  - **标题**: None
  - **Filtered Reason**: none of cs.GR in whitelist
- [Be Friendly, Not Friends: How LLM Sycophancy Shapes User Trust](https://arxiv.org/abs/2502.10844)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Order-agnostic Identifier for Large Language Model-based Generative Recommendation](https://arxiv.org/abs/2502.10833)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [SpellRing: Recognizing Continuous Fingerspelling in American Sign Language using a Ring](https://arxiv.org/abs/2502.10830)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [LintLLM: An Open-Source Verilog Linting Framework Based on Large Language Models](https://arxiv.org/abs/2502.10815)
  - **标题**: None
  - **Filtered Reason**: none of cs.AR in whitelist
- [ResiComp: Loss-Resilient Image Compression via Dual-Functional Masked Visual Token Modeling](https://arxiv.org/abs/2502.10812)
  - **标题**: None
  - **Filtered Reason**: none of eess.IV,cs.IT in whitelist
- [A Unified Framework for Initial Semantics](https://arxiv.org/abs/2502.10811)
  - **标题**: None
  - **Filtered Reason**: none of math.CT,cs.PL,cs.LO in whitelist
- [Image Pre-Processing Framework for Time-Domain Astronomy in the Artificial Intelligence Era](https://arxiv.org/abs/2502.10783)
  - **标题**: None
  - **Filtered Reason**: none of astro-ph.IM,astro-ph.GA,astro-ph.SR,cs.DC in whitelist
- [Fast Transmission Control Adaptation for URLLC via Channel Knowledge Map and Meta-Learning](https://arxiv.org/abs/2502.10777)
  - **标题**: None
  - **Filtered Reason**: none of cs.IT in whitelist
- [Service Function Chain Dynamic Scheduling in Space-Air-Ground Integrated Networks](https://arxiv.org/abs/2502.10731)
  - **标题**: None
  - **Filtered Reason**: none of cs.NI in whitelist
- [Improving Retrieval-Augmented Deep Assertion Generation via Joint Training](https://arxiv.org/abs/2502.10696)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Multi-objective Aerial IRS-assisted ISAC Optimization via Generative AI-enhanced Deep Reinforcement Learning](https://arxiv.org/abs/2502.10687)
  - **标题**: None
  - **Filtered Reason**: none of cs.NI in whitelist
- [Automated Data Quality Validation in an End-to-End GNN Framework](https://arxiv.org/abs/2502.10667)
  - **标题**: None
  - **Filtered Reason**: none of cs.DB in whitelist
- [REAL: Realism Evaluation of Text-to-Image Generation Models for Effective Data Augmentation](https://arxiv.org/abs/2502.10663)
  - **标题**: None
  - **Filtered Reason**: none of cs.MM in whitelist
- [Pushing up to the Limit of Memory Bandwidth and Capacity Utilization for Efficient LLM Decoding on Embedded FPGA](https://arxiv.org/abs/2502.10659)
  - **标题**: None
  - **Filtered Reason**: none of cs.AR in whitelist
- [Script&Shift: A Layered Interface Paradigm for Integrating Content Development and Rhetorical Strategy with LLM Writing Assistants](https://arxiv.org/abs/2502.10638)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [PLAID: Supporting Computing Instructors to Identify Domain-Specific Programming Plans at Scale](https://arxiv.org/abs/2502.10618)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Reachability-Aware Reinforcement Learning for Collision Avoidance in Human-Machine Shared Control](https://arxiv.org/abs/2502.10610)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO,eess.SY in whitelist
- [BLI: A High-performance Bucket-based Learned Index with Concurrency Support](https://arxiv.org/abs/2502.10597)
  - **标题**: None
  - **Filtered Reason**: none of cs.DB in whitelist
- [Prediction uncertainty-aware planning using deep ensembles and trajectory optimisation](https://arxiv.org/abs/2502.10585)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [Static Algorithm, Evolving Epidemic: Understanding the Potential of Human-AI Risk Assessment to Support Regional Overdose Prevention](https://arxiv.org/abs/2502.10542)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Enhancing Age-Related Robustness in Children Speaker Verification](https://arxiv.org/abs/2502.10511)
  - **标题**: None
  - **Filtered Reason**: none of cs.SD,eess.AS in whitelist
- [Crop Yield Time-Series Data Prediction Based on Multiple Hybrid Machine Learning Models](https://arxiv.org/abs/2502.10405)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY,stat.AP in whitelist
- [Implementing agile healthcare frame works in the context of low income countries: Proposed Framework and Review](https://arxiv.org/abs/2502.10403)
  - **标题**: None
  - **Filtered Reason**: none of cs.ET,cs.CY,cs.IR in whitelist
- [Large Model Empowered Metaverse: State-of-the-Art, Challenges and Opportunities](https://arxiv.org/abs/2502.10397)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [Robustness tests for biomedical foundation models should tailor to specification](https://arxiv.org/abs/2502.10374)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY,cs.SE in whitelist
- [VocalCrypt: Novel Active Defense Against Deepfake Voice Based on Masking Effect](https://arxiv.org/abs/2502.10329)
  - **标题**: None
  - **Filtered Reason**: none of cs.SD,cs.MM,eess.AS,cs.CR in whitelist
- [Interval Selection with Binary Predictions](https://arxiv.org/abs/2502.10314)
  - **标题**: None
  - **Filtered Reason**: none of cs.DS in whitelist
- [Open-Source AI-Powered Optimization in Scalene: Advancing Python Performance Profiling with DeepSeek-R1 and LLaMA 3.2](https://arxiv.org/abs/2502.10299)
  - **标题**: None
  - **Filtered Reason**: none of cs.PL in whitelist
- [Anomaly Detection with LWE Encrypted Control](https://arxiv.org/abs/2502.10283)
  - **标题**: None
  - **Filtered Reason**: none of eess.SY,cs.CR in whitelist
- [TrustZero -- open, verifiable and scalable zero-trust](https://arxiv.org/abs/2502.10281)
  - **标题**: None
  - **Filtered Reason**: none of cs.NI,cs.CR in whitelist
- [Seamless acceleration of Fortran intrinsics via AMD AI engines](https://arxiv.org/abs/2502.10254)
  - **标题**: None
  - **Filtered Reason**: none of cs.ET,cs.DC,cs.PF in whitelist
- [Translating Common Security Assertions Across Processor Designs: A RISC-V Case Study](https://arxiv.org/abs/2502.10194)
  - **标题**: None
  - **Filtered Reason**: none of cs.AR,cs.CR in whitelist
- [VideoDiff: Human-AI Video Co-Creation with Alternatives](https://arxiv.org/abs/2502.10190)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Doing More With Less: Towards More Data-Efficient Syndrome-Based Neural Decoders](https://arxiv.org/abs/2502.10183)
  - **标题**: None
  - **Filtered Reason**: none of cs.IT in whitelist
- [Revisiting the Berkeley Admissions data: Statistical Tests for Causal Hypotheses](https://arxiv.org/abs/2502.10161)
  - **标题**: None
  - **Filtered Reason**: none of stat.ME,cs.CY,stat.ML,math.ST in whitelist
- [Semantica: Decentralized Search using a LLM-Guided Semantic Tree Overlay](https://arxiv.org/abs/2502.10151)
  - **标题**: None
  - **Filtered Reason**: none of cs.NI,cs.DC,cs.IR,eess.SY in whitelist
- [ScamFerret: Detecting Scam Websites Autonomously with Large Language Models](https://arxiv.org/abs/2502.10110)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [Membership and Conjugacy in Inverse Semigroups](https://arxiv.org/abs/2502.10103)
  - **标题**: None
  - **Filtered Reason**: none of cs.FL,math.GR,cs.CC in whitelist
- [Enhancing Patient Acceptance of Robotic Ultrasound through Conversational Virtual Agent and Immersive Visualizations](https://arxiv.org/abs/2502.10088)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Coordinated control of multiple autonomous surface vehicles: challenges and advances -- a systematic review](https://arxiv.org/abs/2502.10080)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO,eess.SY in whitelist
- [Diffusion Trajectory-guided Policy for Long-horizon Robot Manipulation](https://arxiv.org/abs/2502.10040)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [Automation Bias in the AI Act: On the Legal Implications of Attempting to De-Bias Human Oversight of AI](https://arxiv.org/abs/2502.10036)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [Discovering Polynomial and Quadratic Structure in Nonlinear Ordinary Differential Equations](https://arxiv.org/abs/2502.10005)
  - **标题**: None
  - **Filtered Reason**: none of math.DS,cs.SC,math.NA,q-bio.MN in whitelist
- [λScale: Enabling Fast Scaling for Serverless Large Language Model Inference](https://arxiv.org/abs/2502.09922)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC in whitelist
- [INF^2: High-Throughput Generative Inference of Large Language Models using Near-Storage Processing](https://arxiv.org/abs/2502.09921)
  - **标题**: None
  - **Filtered Reason**: none of cs.AR in whitelist
