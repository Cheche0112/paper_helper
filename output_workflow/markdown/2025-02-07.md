> 本文由 [https://github.com/huiyeruzhou/arxiv_crawler](https://github.com/huiyeruzhou/arxiv_crawler) 自动生成
>
> 领域白名单：cs.AI,cs.CL,cs.LG,cs.CV
> 关键词： LLM, GPT, AI, language+model, deep+learning, transformer, neural+network, machine+learning

# 论文全览：2025-02-07

共有305篇相关领域论文, 另有27篇其他

## 人工智能(cs.AI:Artificial Intelligence)

### Agentic Reasoning: Reasoning LLMs with Tools for the Deep Research 
[[arxiv](https://arxiv.org/abs/2502.04644)] [[cool](https://papers.cool/arxiv/2502.04644)] [[pdf](https://arxiv.org/pdf/2502.04644)]
> **Authors**: Junde Wu,Jiayuan Zhu,Yuyuan Liu
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: work in progress
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: We introduce Agentic Reasoning, a framework that enhances large language model (LLM) reasoning by integrating external tool-using agents. Unlike conventional LLM-based reasoning approaches, which rely solely on internal inference, Agentic Reasoning dynamically engages web search, code execution, and structured reasoning-context memory to solve complex problems requiring deep research and multi-step logical deduction. Our framework introduces the Mind Map agent, which constructs a structured knowledge graph to track logical relationships, improving deductive reasoning. Additionally, the integration of web-search and coding agents enables real-time retrieval and computational analysis, enhancing reasoning accuracy and decision-making. Evaluations on PhD-level scientific reasoning (GPQA) and domain-specific deep research tasks demonstrate that our approach significantly outperforms existing models, including leading retrieval-augmented generation (RAG) systems and closed-source LLMs. Moreover, our results indicate that agentic reasoning improves expert-level knowledge synthesis, test-time scalability, and structured problem-solving. The code is at: https://github.com/theworldofagents/Agentic-Reasoning.

### Preference Optimization via Contrastive Divergence: Your Reward Model is Secretly an NLL Estimator 
[[arxiv](https://arxiv.org/abs/2502.04567)] [[cool](https://papers.cool/arxiv/2502.04567)] [[pdf](https://arxiv.org/pdf/2502.04567)]
> **Authors**: Zhuotong Chen,Fang Liu,Xuan Zhu,Yanjun Qi,Mohammad Ghavamzadeh
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Existing studies on preference optimization (PO) have centered on constructing pairwise preference data following simple heuristics, such as maximizing the margin between preferred and dispreferred completions based on human (or AI) ranked scores. However, none of these heuristics has a full theoretical justification. In this work, we develop a novel PO framework that provides theoretical guidance to effectively sample dispreferred completions. To achieve this, we formulate PO as minimizing the negative log-likelihood (NLL) of a probability model and propose to estimate its normalization constant via a sampling strategy. As we will demonstrate, these estimative samples can act as dispreferred completions in PO. We then select contrastive divergence (CD) as the sampling strategy, and propose a novel MC-PO algorithm that applies the Monte Carlo (MC) kernel from CD to sample hard negatives w.r.t. the parameterized reward model. Finally, we propose the OnMC-PO algorithm, an extension of MC-PO to the online setting. On popular alignment benchmarks, MC-PO outperforms existing SOTA baselines, and OnMC-PO leads to further improvement.

### Robust Probabilistic Model Checking with Continuous Reward Domains 
[[arxiv](https://arxiv.org/abs/2502.04530)] [[cool](https://papers.cool/arxiv/2502.04530)] [[pdf](https://arxiv.org/pdf/2502.04530)]
> **Authors**: Xiaotong Ji,Hanchun Wang,Antonio Filieri,Ilenia Epifani
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: Accepted by the 20th International Conference on Software Engineering for Adaptive and Self-Managing Systems 2025
- **标题**: None
- **领域**: 人工智能,形式语言和自动机理论,机器学习
- **Abstract**: Probabilistic model checking traditionally verifies properties on the expected value of a measure of interest. This restriction may fail to capture the quality of service of a significant proportion of a system's runs, especially when the probability distribution of the measure of interest is poorly represented by its expected value due to heavy-tail behaviors or multiple modalities. Recent works inspired by distributional reinforcement learning use discrete histograms to approximate integer reward distribution, but they struggle with continuous reward space and present challenges in balancing accuracy and scalability. We propose a novel method for handling both continuous and discrete reward distributions in Discrete Time Markov Chains using moment matching with Erlang mixtures. By analytically deriving higher-order moments through Moment Generating Functions, our method approximates the reward distribution with theoretically bounded error while preserving the statistical properties of the true distribution. This detailed distributional insight enables the formulation and robust model checking of quality properties based on the entire reward distribution function, rather than restricting to its expected value. We include a theoretical foundation ensuring bounded approximation errors, along with an experimental evaluation demonstrating our method's accuracy and scalability in practical model-checking problems.

### Safety is Essential for Responsible Open-Ended Systems 
[[arxiv](https://arxiv.org/abs/2502.04512)] [[cool](https://papers.cool/arxiv/2502.04512)] [[pdf](https://arxiv.org/pdf/2502.04512)]
> **Authors**: Ivaxi Sheth,Jan Wehner,Sahar Abdelnabi,Ruta Binkyte,Mario Fritz
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 12 pages
- **标题**: None
- **领域**: 人工智能
- **Abstract**: AI advancements have been significantly driven by a combination of foundation models and curiosity-driven learning aimed at increasing capability and adaptability. A growing area of interest within this field is Open-Endedness - the ability of AI systems to continuously and autonomously generate novel and diverse artifacts or solutions. This has become relevant for accelerating scientific discovery and enabling continual adaptation in AI agents. This position paper argues that the inherently dynamic and self-propagating nature of Open-Ended AI introduces significant, underexplored risks, including challenges in maintaining alignment, predictability, and control. This paper systematically examines these challenges, proposes mitigation strategies, and calls for action for different stakeholders to support the safe, responsible and successful development of Open-Ended AI.

### PerPO: Perceptual Preference Optimization via Discriminative Rewarding 
[[arxiv](https://arxiv.org/abs/2502.04371)] [[cool](https://papers.cool/arxiv/2502.04371)] [[pdf](https://arxiv.org/pdf/2502.04371)]
> **Authors**: Zining Zhu,Liang Zhao,Kangheng Lin,Jinze Yang,En Yu,Chenglong Liu,Haoran Wei,Jianjian Sun,Zheng Ge,Xiangyu Zhang
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学,机器学习
- **Abstract**: This paper presents Perceptual Preference Optimization (PerPO), a perception alignment method aimed at addressing the visual discrimination challenges in generative pre-trained multimodal large language models (MLLMs). To align MLLMs with human visual perception process, PerPO employs discriminative rewarding to gather diverse negative samples, followed by listwise preference optimization to rank them.By utilizing the reward as a quantitative margin for ranking, our method effectively bridges generative preference optimization and discriminative empirical risk minimization. PerPO significantly enhances MLLMs' visual discrimination capabilities while maintaining their generative strengths, mitigates image-unconditional reward hacking, and ensures consistent performance across visual tasks. This work marks a crucial step towards more perceptually aligned and versatile MLLMs. We also hope that PerPO will encourage the community to rethink MLLM alignment strategies.

### Free Energy Risk Metrics for Systemically Safe AI: Gatekeeping Multi-Agent Study 
[[arxiv](https://arxiv.org/abs/2502.04249)] [[cool](https://papers.cool/arxiv/2502.04249)] [[pdf](https://arxiv.org/pdf/2502.04249)]
> **Authors**: Michael Walters,Rafael Kaufmann,Justice Sefas,Thomas Kopinski
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 9 pages, 1 figure
- **标题**: None
- **领域**: 人工智能,机器学习,多代理系统,数据分析、统计和概率,机器学习
- **Abstract**: We investigate the Free Energy Principle as a foundation for measuring risk in agentic and multi-agent systems. From these principles we introduce a Cumulative Risk Exposure metric that is flexible to differing contexts and needs. We contrast this to other popular theories for safe AI that hinge on massive amounts of data or describing arbitrarily complex world models. In our framework, stakeholders need only specify their preferences over system outcomes, providing straightforward and transparent decision rules for risk governance and mitigation. This framework naturally accounts for uncertainty in both world model and preference model, allowing for decision-making that is epistemically and axiologically humble, parsimonious, and future-proof. We demonstrate this novel approach in a simplified autonomous vehicle environment with multi-agent vehicles whose driving policies are mediated by gatekeepers that evaluate, in an online fashion, the risk to the collective safety in their neighborhood, and intervene through each vehicle's policy when appropriate. We show that the introduction of gatekeepers in an AV fleet, even at low penetration, can generate significant positive externalities in terms of increased system safety.

### Fine, I'll Merge It Myself: A Multi-Fidelity Framework for Automated Model Merging 
[[arxiv](https://arxiv.org/abs/2502.04030)] [[cool](https://papers.cool/arxiv/2502.04030)] [[pdf](https://arxiv.org/pdf/2502.04030)]
> **Authors**: Guinan Su,Jonas Geiping
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: Reasoning capabilities represent a critical frontier for large language models (LLMs), but developing them requires extensive proprietary datasets and computational resources. One way to efficiently supplement capabilities with is by model merging, which offers a promising alternative by combining multiple models without retraining. However, current merging approaches rely on manually-designed strategies for merging hyperparameters, limiting the exploration of potential model combinations and requiring significant human effort. We propose an Automated Model Merging Framework that enables fine-grained exploration of merging strategies while reducing costs through multi-fidelity approximations. We support both single and multi-objective optimization and introduce two novel search spaces: layerwise fusion (LFS) and depth-wise integration (DIS). Evaluating across a number of benchmarks, we find that the search autonomously finds 1) Merges that further boost single-objective performance, even on tasks the model has already been finetuned on, and 2) Merges that optimize multi-objective frontiers across tasks. Effective merges are found with limited compute, e.g. within less than 500 search steps.

## 计算语言学(cs.CL:Computation and Language)

### Before It's Too Late: A State Space Model for the Early Prediction of Misinformation and Disinformation Engagement 
[[arxiv](https://arxiv.org/abs/2502.04655)] [[cool](https://papers.cool/arxiv/2502.04655)] [[pdf](https://arxiv.org/pdf/2502.04655)]
> **Authors**: Lin Tian,Emily Booth,Francesco Bailo,Julian Droogan,Marian-Andrei Rizoiu
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 11 pages, 5 figures, 10 tables, Accepted by the Web Conference 2025 (WWW2025)
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: In today's digital age, conspiracies and information campaigns can emerge rapidly and erode social and democratic cohesion. While recent deep learning approaches have made progress in modeling engagement through language and propagation models, they struggle with irregularly sampled data and early trajectory assessment. We present IC-Mamba, a novel state space model that forecasts social media engagement by modeling interval-censored data with integrated temporal embeddings. Our model excels at predicting engagement patterns within the crucial first 15-30 minutes of posting (RMSE 0.118-0.143), enabling rapid assessment of content reach. By incorporating interval-censored modeling into the state space framework, IC-Mamba captures fine-grained temporal dynamics of engagement growth, achieving a 4.72% improvement over state-of-the-art across multiple engagement metrics (likes, shares, comments, and emojis). Our experiments demonstrate IC-Mamba's effectiveness in forecasting both post-level dynamics and broader narrative patterns (F1 0.508-0.751 for narrative-level predictions). The model maintains strong predictive performance across extended time horizons, successfully forecasting opinion-level engagement up to 28 days ahead using observation windows of 3-10 days. These capabilities enable earlier identification of potentially problematic content, providing crucial lead time for designing and implementing countermeasures. Code is available at: https://github.com/ltian678/ic-mamba. An interactive dashboard demonstrating our results is available at: https://ic-mamba.behavioral-ds.science.

### Extracting and Understanding the Superficial Knowledge in Alignment 
[[arxiv](https://arxiv.org/abs/2502.04602)] [[cool](https://papers.cool/arxiv/2502.04602)] [[pdf](https://arxiv.org/pdf/2502.04602)]
> **Authors**: Runjin Chen,Gabriel Jacob Perin,Xuxi Chen,Xilun Chen,Yan Han,Nina S. T. Hirata,Junyuan Hong,Bhavya Kailkhura
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Alignment of large language models (LLMs) with human values and preferences, often achieved through fine-tuning based on human feedback, is essential for ensuring safe and responsible AI behaviors. However, the process typically requires substantial data and computation resources. Recent studies have revealed that alignment might be attainable at lower costs through simpler methods, such as in-context learning. This leads to the question: Is alignment predominantly superficial? In this paper, we delve into this question and provide a quantitative analysis. We formalize the concept of superficial knowledge, defining it as knowledge that can be acquired through easily token restyling, without affecting the model's ability to capture underlying causal relationships between tokens. We propose a method to extract and isolate superficial knowledge from aligned models, focusing on the shallow modifications to the final token selection process. By comparing models augmented only with superficial knowledge to fully aligned models, we quantify the superficial portion of alignment. Our findings reveal that while superficial knowledge constitutes a significant portion of alignment, particularly in safety and detoxification tasks, it is not the whole story. Tasks requiring reasoning and contextual understanding still rely on deeper knowledge. Additionally, we demonstrate two practical advantages of isolated superficial knowledge: (1) it can be transferred between models, enabling efficient offsite alignment of larger models using extracted superficial knowledge from smaller models, and (2) it is recoverable, allowing for the restoration of alignment in compromised models without sacrificing performance.

### My LLM might Mimic AAE -- But When Should it? 
[[arxiv](https://arxiv.org/abs/2502.04564)] [[cool](https://papers.cool/arxiv/2502.04564)] [[pdf](https://arxiv.org/pdf/2502.04564)]
> **Authors**: Sandra C. Sandoval,Christabel Acquaye,Kwesi Cobbina,Mohammad Nayeem Teli,Hal Daumé III
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: Accepted to NAACL 2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: We examine the representation of African American English (AAE) in large language models (LLMs), exploring (a) the perceptions Black Americans have of how effective these technologies are at producing authentic AAE, and (b) in what contexts Black Americans find this desirable. Through both a survey of Black Americans ($n=$ 104) and annotation of LLM-produced AAE by Black Americans ($n=$ 228), we find that Black Americans favor choice and autonomy in determining when AAE is appropriate in LLM output. They tend to prefer that LLMs default to communicating in Mainstream U.S. English in formal settings, with greater interest in AAE production in less formal settings. When LLMs were appropriately prompted and provided in context examples, our participants found their outputs to have a level of AAE authenticity on par with transcripts of Black American speech. Select code and data for our project can be found here: https://github.com/smelliecat/AAEMime.git

### TruthFlow: Truthful LLM Generation via Representation Flow Correction 
[[arxiv](https://arxiv.org/abs/2502.04556)] [[cool](https://papers.cool/arxiv/2502.04556)] [[pdf](https://arxiv.org/pdf/2502.04556)]
> **Authors**: Hanyu Wang,Bochuan Cao,Yuanpu Cao,Jinghui Chen
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Large language models (LLMs) are known to struggle with consistently generating truthful responses. While various representation intervention techniques have been proposed, these methods typically apply a universal representation correction vector to all input queries, limiting their effectiveness against diverse queries in practice. In this study, we introduce TruthFlow, a novel method that leverages the Flow Matching technique for query-specific truthful representation correction. Specifically, TruthFlow first uses a flow model to learn query-specific correction vectors that transition representations from hallucinated to truthful states. Then, during inference, the trained flow model generates these correction vectors to enhance the truthfulness of LLM outputs. Experimental results demonstrate that TruthFlow significantly improves performance on open-ended generation tasks across various advanced LLMs evaluated on TruthfulQA. Moreover, the trained TruthFlow model exhibits strong transferability, performing effectively on other unseen hallucination benchmarks.

### Contextual Gradient Flow Modeling for Large Language Model Generalization in Multi-Scale Feature Spaces 
[[arxiv](https://arxiv.org/abs/2502.04548)] [[cool](https://papers.cool/arxiv/2502.04548)] [[pdf](https://arxiv.org/pdf/2502.04548)]
> **Authors**: Daphne Quillington,Kingsley Fairbrother,Xavier Tattershall,Irin Kabakum
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Optimization methodologies for training large-scale neural architectures often rely on uniform gradient propagation mechanisms that fail to align with hierarchical linguistic structures, limiting their capacity to generalize across diverse language distributions. A structured gradient refinement framework was introduced to incorporate multi-scale contextual adjustments, improving parameter adaptation through dynamic weighting strategies that enhanced representation coherence. Empirical evaluations demonstrated that structured propagation mechanisms contributed to reductions in gradient oscillations, resulting in more stable training dynamics and improved optimization efficiency. The comparative performance assessment indicated that models incorporating hierarchical propagation strategies exhibited greater robustness in long-range dependency retention and cross-domain adaptation. The hierarchical adjustment of weight updates provided an alternative to conventional backpropagation, reducing sensitivity to initialization conditions while improving overall convergence efficiency. The experimental results confirmed that structured gradient propagation influenced representation learning trajectories, aligning parameter updates with broader linguistic dependencies rather than isolated token-level relationships. Statistical evaluations indicated that structured optimization strategies mitigated overfitting while preserving adaptability across heterogeneous text distributions. The findings established that structured gradient propagation provided an empirically validated framework for refining hierarchical representation learning, supporting more effective integration of linguistic dependencies into optimization dynamics.

### Multilingual Non-Autoregressive Machine Translation without Knowledge Distillation 
[[arxiv](https://arxiv.org/abs/2502.04537)] [[cool](https://papers.cool/arxiv/2502.04537)] [[pdf](https://arxiv.org/pdf/2502.04537)]
> **Authors**: Chenyang Huang,Fei Huang,Zaixiang Zheng,Osmar R. Zaïane,Hao Zhou,Lili Mou
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: In Findings of the Association for Computational Linguistics: IJCNLP-AACL 2023
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Multilingual neural machine translation (MNMT) aims at using one single model for multiple translation directions. Recent work applies non-autoregressive Transformers to improve the efficiency of MNMT, but requires expensive knowledge distillation (KD) processes. To this end, we propose an M-DAT approach to non-autoregressive multilingual machine translation. Our system leverages the recent advance of the directed acyclic Transformer (DAT), which does not require KD. We further propose a pivot back-translation (PivotBT) approach to improve the generalization to unseen translation directions. Experiments show that our M-DAT achieves state-of-the-art performance in non-autoregressive MNMT.

### A Decoding Algorithm for Length-Control Summarization Based on Directed Acyclic Transformers 
[[arxiv](https://arxiv.org/abs/2502.04535)] [[cool](https://papers.cool/arxiv/2502.04535)] [[pdf](https://arxiv.org/pdf/2502.04535)]
> **Authors**: Chenyang Huang,Hao Zhou,Cameron Jen,Kangjie Zheng,Osmar R. Zaïane,Lili Mou
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: Findings of the Association for Computational Linguistics: EMNLP 2024
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Length-control summarization aims to condense long texts into a short one within a certain length limit. Previous approaches often use autoregressive (AR) models and treat the length requirement as a soft constraint, which may not always be satisfied. In this study, we propose a novel length-control decoding algorithm based on the Directed Acyclic Transformer (DAT). Our approach allows for multiple plausible sequence fragments and predicts a \emph{path} to connect them. In addition, we propose a Sequence Maximum a Posteriori (SeqMAP) decoding algorithm that marginalizes different possible paths and finds the most probable summary satisfying the length budget. Our algorithm is based on beam search, which further facilitates a reranker for performance improvement. Experimental results on the Gigaword and DUC2004 datasets demonstrate our state-of-the-art performance for length-control summarization.

### Group-Adaptive Threshold Optimization for Robust AI-Generated Text Detection 
[[arxiv](https://arxiv.org/abs/2502.04528)] [[cool](https://papers.cool/arxiv/2502.04528)] [[pdf](https://arxiv.org/pdf/2502.04528)]
> **Authors**: Minseok Jung,Cynthia Fuertes Panizo,Liam Dugan,Yi R.,Fung,Pin-Yu Chen,Paul Pu Liang
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: The advancement of large language models (LLMs) has made it difficult to differentiate human-written text from AI-generated text. Several AI-text detectors have been developed in response, which typically utilize a fixed global threshold (e.g., θ = 0.5) to classify machine-generated text. However, we find that one universal threshold can fail to account for subgroup-specific distributional variations. For example, when using a fixed threshold, detectors make more false positive errors on shorter human-written text than longer, and more positive classifications on neurotic writing styles than open among long text. These discrepancies can lead to misclassification that disproportionately affects certain groups. We address this critical limitation by introducing FairOPT, an algorithm for group-specific threshold optimization in AI-generated content classifiers. Our approach partitions data into subgroups based on attributes (e.g., text length and writing style) and learns decision thresholds for each group, which enables careful balancing of performance and fairness metrics within each subgroup. In experiments with four AI text classifiers on three datasets, FairOPT enhances overall F1 score and decreases balanced error rate (BER) discrepancy across subgroups. Our framework paves the way for more robust and fair classification criteria in AI-generated output detection.

### Linear Correlation in LM's Compositional Generalization and Hallucination 
[[arxiv](https://arxiv.org/abs/2502.04520)] [[cool](https://papers.cool/arxiv/2502.04520)] [[pdf](https://arxiv.org/pdf/2502.04520)]
> **Authors**: Letian Peng,Chenyang An,Shibo Hao,Chengyu Dong,Jingbo Shang
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The generalization of language models (LMs) is undergoing active debates, contrasting their potential for general intelligence with their struggles with basic knowledge composition (e.g., reverse/transition curse). This paper uncovers the phenomenon of linear correlations in LMs during knowledge composition. For explanation, there exists a linear transformation between certain related knowledge that maps the next token prediction logits from one prompt to another, e.g., "X lives in the city of" $\rightarrow$ "X lives in the country of" for every given X. This mirrors the linearity in human knowledge composition, such as Paris $\rightarrow$ France. Our findings indicate that the linear transformation is resilient to large-scale fine-tuning, generalizing updated knowledge when aligned with real-world relationships, but causing hallucinations when it deviates. Empirical results suggest that linear correlation can serve as a potential identifier of LM's generalization. Finally, we show such linear correlations can be learned with a single feedforward network and pre-trained vocabulary representations, indicating LM generalization heavily relies on the latter.

### Beyond Sample-Level Feedback: Using Reference-Level Feedback to Guide Data Synthesis 
[[arxiv](https://arxiv.org/abs/2502.04511)] [[cool](https://papers.cool/arxiv/2502.04511)] [[pdf](https://arxiv.org/pdf/2502.04511)]
> **Authors**: Shuhaib Mehri,Xiusi Chen,Heng Ji,Dilek Hakkani-Tür
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: LLMs demonstrate remarkable capabilities in following natural language instructions, largely due to instruction-tuning on high-quality datasets. While synthetic data generation has emerged as a scalable approach for creating such datasets, maintaining consistent quality standards remains challenging. Recent approaches incorporate feedback to improve data quality, but typically operate at the sample level, generating and applying feedback for each response individually. In this work, we propose Reference-Level Feedback, a novel methodology that instead collects feedback based on high-quality reference samples from carefully curated seed data. We use this feedback to capture rich signals of desirable characteristics and propagate it throughout the data synthesis process. We present REFED, a dataset of 10K instruction-response pairs synthesized using such feedback. We demonstrate the effectiveness of our approach by showing that Llama-3.1-8B-Instruct finetuned on REFED achieves state-of-the-art performance among similar-sized SFT-based models on AlpacaEval 2.0 and strong results on Arena-Hard. Through extensive experiments, we show that our approach consistently outperforms traditional sample-level feedback methods with significantly fewer feedback collections and improves performance across different model architectures.

### Heterogeneous Swarms: Jointly Optimizing Model Roles and Weights for Multi-LLM Systems 
[[arxiv](https://arxiv.org/abs/2502.04510)] [[cool](https://papers.cool/arxiv/2502.04510)] [[pdf](https://arxiv.org/pdf/2502.04510)]
> **Authors**: Shangbin Feng,Zifeng Wang,Palash Goyal,Yike Wang,Weijia Shi,Huang Xia,Hamid Palangi,Luke Zettlemoyer,Yulia Tsvetkov,Chen-Yu Lee,Tomas Pfister
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: We propose Heterogeneous Swarms, an algorithm to design multi-LLM systems by jointly optimizing model roles and weights. We represent multi-LLM systems as directed acyclic graphs (DAGs) of LLMs with topological message passing for collaborative generation. Given a pool of LLM experts and a utility function, Heterogeneous Swarms employs two iterative steps: role-step and weight-step. For role-step, we interpret model roles as learning a DAG that specifies the flow of inputs and outputs between LLMs. Starting from a swarm of random continuous adjacency matrices, we decode them into discrete DAGs, call the LLMs in topological order, evaluate on the utility function (e.g. accuracy on a task), and optimize the adjacency matrices with particle swarm optimization based on the utility score. For weight-step, we assess the contribution of individual LLMs in the multi-LLM systems and optimize model weights with swarm intelligence. We propose JFK-score to quantify the individual contribution of each LLM in the best-found DAG of the role-step, then optimize model weights with particle swarm optimization based on the JFK-score. Experiments demonstrate that Heterogeneous Swarms outperforms 15 role- and/or weight-based baselines by 18.5% on average across 12 tasks. Further analysis reveals that Heterogeneous Swarms discovers multi-LLM systems with heterogeneous model roles and substantial collaborative gains, and benefits from the diversity of language models.

### When One LLM Drools, Multi-LLM Collaboration Rules 
[[arxiv](https://arxiv.org/abs/2502.04506)] [[cool](https://papers.cool/arxiv/2502.04506)] [[pdf](https://arxiv.org/pdf/2502.04506)]
> **Authors**: Shangbin Feng,Wenxuan Ding,Alisa Liu,Zifeng Wang,Weijia Shi,Yike Wang,Zejiang Shen,Xiaochuang Han,Hunter Lang,Chen-Yu Lee,Tomas Pfister,Yejin Choi,Yulia Tsvetkov
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: This position paper argues that in many realistic (i.e., complex, contextualized, subjective) scenarios, one LLM is not enough to produce a reliable output. We challenge the status quo of relying solely on a single general-purpose LLM and argue for multi-LLM collaboration to better represent the extensive diversity of data, skills, and people. We first posit that a single LLM underrepresents real-world data distributions, heterogeneous skills, and pluralistic populations, and that such representation gaps cannot be trivially patched by further training a single LLM. We then organize existing multi-LLM collaboration methods into a hierarchy, based on the level of access and information exchange, ranging from API-level, text-level, logit-level, to weight-level collaboration. Based on these methods, we highlight how multi-LLM collaboration addresses challenges that a single LLM struggles with, such as reliability, democratization, and pluralism. Finally, we identify the limitations of existing multi-LLM methods and motivate future work. We envision multi-LLM collaboration as an essential path toward compositional intelligence and collaborative AI development.

### ULPT: Prompt Tuning with Ultra-Low-Dimensional Optimization 
[[arxiv](https://arxiv.org/abs/2502.04501)] [[cool](https://papers.cool/arxiv/2502.04501)] [[pdf](https://arxiv.org/pdf/2502.04501)]
> **Authors**: Zijun Wu,Yongchang Hao,Lili Mou
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models achieve state-of-the-art performance but are costly to fine-tune due to their size. Parameter-efficient fine-tuning methods, such as prompt tuning, address this by reducing trainable parameters while maintaining strong performance. However, prior methods tie prompt embeddings to the model's dimensionality, which may not scale well with larger LLMs and more customized LLMs. In this paper, we propose Ultra-Low-dimensional Prompt Tuning (ULPT), which optimizes prompts in a low-dimensional space (e.g., 2D) and use a random but frozen matrix for the up-projection. To enhance alignment, we introduce learnable shift and scale embeddings. ULPT drastically reduces the trainable parameters, e.g., 2D only using 2% parameters compared with vanilla prompt tuning while retaining most of the performance across 21 NLP tasks. Our theoretical analysis shows that random projections can capture high-rank structures effectively, and experimental results demonstrate ULPT's competitive performance over existing parameter-efficient methods.

### Verifiable Format Control for Large Language Model Generations 
[[arxiv](https://arxiv.org/abs/2502.04498)] [[cool](https://papers.cool/arxiv/2502.04498)] [[pdf](https://arxiv.org/pdf/2502.04498)]
> **Authors**: Zhaoyang Wang,Jinqi Jiang,Huichi Zhou,Wenhao Zheng,Xuchao Zhang,Chetan Bansal,Huaxiu Yao
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: To appear at Findings of NAACL 2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Recent Large Language Models (LLMs) have demonstrated satisfying general instruction following ability. However, small LLMs with about 7B parameters still struggle fine-grained format following (e.g., JSON format), which seriously hinder the advancements of their applications. Most existing methods focus on benchmarking general instruction following while overlook how to improve the specific format following ability for small LLMs. Besides, these methods often rely on evaluations based on advanced LLMs (e.g., GPT-4), which can introduce the intrinsic bias of LLMs and be costly due to the API calls. In this paper, we first curate a fully verifiable format following dataset VFF. In contrast to existing works often adopting external LLMs for instruction-following validations, every sample of VFF can be easily validated with a Python function. Further, we propose to leverage this verifiable feature to synthesize massive data for progressively training small LLMs, in order to improve their format following abilities. Experimental results highlight the prevalent limitations in the format following capabilities of 7B level open-source LLMs and demonstrate the effectiveness of our method in enhancing this essential ability.

### Multi-Agent Reinforcement Learning with Focal Diversity Optimization 
[[arxiv](https://arxiv.org/abs/2502.04492)] [[cool](https://papers.cool/arxiv/2502.04492)] [[pdf](https://arxiv.org/pdf/2502.04492)]
> **Authors**: Selim Furkan Tekin,Fatih Ilhan,Tiansheng Huang,Sihao Hu,Zachary Yahn,Ling Liu
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The advancement of Large Language Models (LLMs) and their finetuning strategies has triggered the renewed interests in multi-agent reinforcement learning. In this paper, we introduce a focal diversity-optimized multi-agent reinforcement learning approach, coined as MARL-Focal, with three unique characteristics. First, we develop an agent-fusion framework for encouraging multiple LLM based agents to collaborate in producing the final inference output for each LLM query. Second, we develop a focal-diversity optimized agent selection algorithm that can choose a small subset of the available agents based on how well they can complement one another to generate the query output. Finally, we design a conflict-resolution method to detect output inconsistency among multiple agents and produce our MARL-Focal output through reward-aware and policy-adaptive inference fusion. Extensive evaluations on five benchmarks show that MARL-Focal is cost-efficient and adversarial-robust. Our multi-agent fusion model achieves performance improvement of 5.51\% compared to the best individual LLM-agent and offers stronger robustness over the TruthfulQA benchmark. Code is available at https://github.com/sftekin/rl-focal

### Building A Unified AI-centric Language System: analysis, framework and future work 
[[arxiv](https://arxiv.org/abs/2502.04488)] [[cool](https://papers.cool/arxiv/2502.04488)] [[pdf](https://arxiv.org/pdf/2502.04488)]
> **Authors**: Edward Hong Wang,Cynthia Xin Wen
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Recent advancements in large language models have demonstrated that extended inference through techniques can markedly improve performance, yet these gains come with increased computational costs and the propagation of inherent biases found in natural languages. This paper explores the design of a unified AI-centric language system that addresses these challenges by offering a more concise, unambiguous, and computationally efficient alternative to traditional human languages. We analyze the limitations of natural language such as gender bias, morphological irregularities, and contextual ambiguities and examine how these issues are exacerbated within current Transformer architectures, where redundant attention heads and token inefficiencies prevail. Drawing on insights from emergent artificial communication systems and constructed languages like Esperanto and Lojban, we propose a framework that translates diverse natural language inputs into a streamlined AI-friendly language, enabling more efficient model training and inference while reducing memory footprints. Finally, we outline a pathway for empirical validation through controlled experiments, paving the way for a universal interchange format that could revolutionize AI-to-AI and human-to-AI interactions by enhancing clarity, fairness, and overall performance.

### Active Task Disambiguation with LLMs 
[[arxiv](https://arxiv.org/abs/2502.04485)] [[cool](https://papers.cool/arxiv/2502.04485)] [[pdf](https://arxiv.org/pdf/2502.04485)]
> **Authors**: Katarzyna Kobalczyk,Nicolas Astorga,Tennison Liu,Mihaela van der Schaar
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Despite the impressive performance of large language models (LLMs) across various benchmarks, their ability to address ambiguously specified problems--frequent in real-world interactions--remains underexplored. To address this gap, we introduce a formal definition of task ambiguity and frame the problem of task disambiguation through the lens of Bayesian Experimental Design. By posing clarifying questions, LLM agents can acquire additional task specifications, progressively narrowing the space of viable solutions and reducing the risk of generating unsatisfactory outputs. Yet, generating effective clarifying questions requires LLM agents to engage in a form of meta-cognitive reasoning, an ability LLMs may presently lack. Our proposed approach of active task disambiguation enables LLM agents to generate targeted questions maximizing the information gain. Effectively, this approach shifts the load from implicit to explicit reasoning about the space of viable solutions. Empirical results demonstrate that this form of question selection leads to more effective task disambiguation in comparison to approaches relying on reasoning solely within the space of questions.

### Confident or Seek Stronger: Exploring Uncertainty-Based On-device LLM Routing From Benchmarking to Generalization 
[[arxiv](https://arxiv.org/abs/2502.04428)] [[cool](https://papers.cool/arxiv/2502.04428)] [[pdf](https://arxiv.org/pdf/2502.04428)]
> **Authors**: Yu-Neng Chuang,Leisheng Yu,Guanchu Wang,Lizhe Zhang,Zirui Liu,Xuanting Cai,Yang Sui,Vladimir Braverman,Xia Hu
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Large language models (LLMs) are increasingly deployed and democratized on edge devices. To improve the efficiency of on-device deployment, small language models (SLMs) are often adopted due to their efficient decoding latency and reduced energy consumption. However, these SLMs often generate inaccurate responses when handling complex queries. One promising solution is uncertainty-based SLM routing, offloading high-stakes queries to stronger LLMs when resulting in low-confidence responses on SLM. This follows the principle of "If you lack confidence, seek stronger support" to enhance reliability. Relying on more powerful LLMs is yet effective but increases invocation costs. Therefore, striking a routing balance between efficiency and efficacy remains a critical challenge. Additionally, efficiently generalizing the routing strategy to new datasets remains under-explored. In this paper, we conduct a comprehensive investigation into benchmarking and generalization of uncertainty-driven routing strategies from SLMs to LLMs over 1500+ settings. Our findings highlight: First, uncertainty-correctness alignment in different uncertainty quantification (UQ) methods significantly impacts routing performance. Second, uncertainty distributions depend more on both the specific SLM and the chosen UQ method, rather than downstream data. Building on the insight, we propose a calibration data construction instruction pipeline and open-source a constructed hold-out set to enhance routing generalization on new downstream scenarios. The experimental results indicate calibration data effectively bootstraps routing performance without any new data.

### Decoding AI Judgment: How LLMs Assess News Credibility and Bias 
[[arxiv](https://arxiv.org/abs/2502.04426)] [[cool](https://papers.cool/arxiv/2502.04426)] [[pdf](https://arxiv.org/pdf/2502.04426)]
> **Authors**: Edoardo Loru,Jacopo Nudo,Niccolò Di Marco,Matteo Cinelli,Walter Quattrociocchi
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机与社会
- **Abstract**: Large Language Models (LLMs) are increasingly used to assess news credibility, yet little is known about how they make these judgments. While prior research has examined political bias in LLM outputs or their potential for automated fact-checking, their internal evaluation processes remain largely unexamined. Understanding how LLMs assess credibility provides insights into AI behavior and how credibility is structured and applied in large-scale language models. This study benchmarks the reliability and political classifications of state-of-the-art LLMs - Gemini 1.5 Flash (Google), GPT-4o mini (OpenAI), and LLaMA 3.1 (Meta) - against structured, expert-driven rating systems such as NewsGuard and Media Bias Fact Check. Beyond assessing classification performance, we analyze the linguistic markers that shape LLM decisions, identifying which words and concepts drive their evaluations. We uncover patterns in how LLMs associate credibility with specific linguistic features by examining keyword frequency, contextual determinants, and rank distributions. Beyond static classification, we introduce a framework in which LLMs refine their credibility assessments by retrieving external information, querying other models, and adapting their responses. This allows us to investigate whether their assessments reflect structured reasoning or rely primarily on prior learned associations.

### EmoBench-M: Benchmarking Emotional Intelligence for Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.04424)] [[cool](https://papers.cool/arxiv/2502.04424)] [[pdf](https://arxiv.org/pdf/2502.04424)]
> **Authors**: He Hu,Yucheng Zhou,Lianzhong You,Hongbo Xu,Qianning Wang,Zheng Lian,Fei Richard Yu,Fei Ma,Laizhong Cui
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: With the integration of Multimodal large language models (MLLMs) into robotic systems and various AI applications, embedding emotional intelligence (EI) capabilities into these models is essential for enabling robots to effectively address human emotional needs and interact seamlessly in real-world scenarios. Existing static, text-based, or text-image benchmarks overlook the multimodal complexities of real-world interactions and fail to capture the dynamic, multimodal nature of emotional expressions, making them inadequate for evaluating MLLMs' EI. Based on established psychological theories of EI, we build EmoBench-M, a novel benchmark designed to evaluate the EI capability of MLLMs across 13 valuation scenarios from three key dimensions: foundational emotion recognition, conversational emotion understanding, and socially complex emotion analysis. Evaluations of both open-source and closed-source MLLMs on EmoBench-M reveal a significant performance gap between them and humans, highlighting the need to further advance their EI capabilities. All benchmark resources, including code and datasets, are publicly available at https://emo-gml.github.io/.

### MedRAG: Enhancing Retrieval-augmented Generation with Knowledge Graph-Elicited Reasoning for Healthcare Copilot 
[[arxiv](https://arxiv.org/abs/2502.04413)] [[cool](https://papers.cool/arxiv/2502.04413)] [[pdf](https://arxiv.org/pdf/2502.04413)]
> **Authors**: Xuejiao Zhao,Siyan Liu,Su-Yin Yang,Chunyan Miao
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,信息检索
- **Abstract**: Retrieval-augmented generation (RAG) is a well-suited technique for retrieving privacy-sensitive Electronic Health Records (EHR). It can serve as a key module of the healthcare copilot, helping reduce misdiagnosis for healthcare practitioners and patients. However, the diagnostic accuracy and specificity of existing heuristic-based RAG models used in the medical domain are inadequate, particularly for diseases with similar manifestations. This paper proposes MedRAG, a RAG model enhanced by knowledge graph (KG)-elicited reasoning for the medical domain that retrieves diagnosis and treatment recommendations based on manifestations. MedRAG systematically constructs a comprehensive four-tier hierarchical diagnostic KG encompassing critical diagnostic differences of various diseases. These differences are dynamically integrated with similar EHRs retrieved from an EHR database, and reasoned within a large language model. This process enables more accurate and specific decision support, while also proactively providing follow-up questions to enhance personalized medical decision-making. MedRAG is evaluated on both a public dataset DDXPlus and a private chronic pain diagnostic dataset (CPDD) collected from Tan Tock Seng Hospital, and its performance is compared against various existing RAG methods. Experimental results show that, leveraging the information integration and relational abilities of the KG, our MedRAG provides more specific diagnostic insights and outperforms state-of-the-art models in reducing misdiagnosis rates. Our code will be available at https://github.com/SNOWTEAM2023/MedRAG

### Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models 
[[arxiv](https://arxiv.org/abs/2502.04404)] [[cool](https://papers.cool/arxiv/2502.04404)] [[pdf](https://arxiv.org/pdf/2502.04404)]
> **Authors**: Xiao-Wen Yang,Xuan-Yi Zhu,Wen-Da Wei,Ding-Chu Zhang,Jie-Jing Shao,Zhi Zhou,Lan-Zhe Guo,Yu-Feng Li
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: This is a preprint under review, 15 pages, 13 figures
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: The integration of slow-thinking mechanisms into large language models (LLMs) offers a promising way toward achieving Level 2 AGI Reasoners, as exemplified by systems like OpenAI's o1. However, several significant challenges remain, including inefficient overthinking and an overreliance on auxiliary reward models. We point out that these limitations stem from LLMs' inability to internalize the search process, a key component of effective reasoning. A critical step toward addressing this issue is enabling LLMs to autonomously determine when and where to backtrack, a fundamental operation in traditional search algorithms. To this end, we propose a self-backtracking mechanism that equips LLMs with the ability to backtrack during both training and inference. This mechanism not only enhances reasoning ability but also efficiency by transforming slow-thinking processes into fast-thinking through self-improvement. Empirical evaluations demonstrate that our proposal significantly enhances the reasoning capabilities of LLMs, achieving a performance gain of over 40 percent compared to the optimal-path supervised fine-tuning method. We believe this study introduces a novel and promising pathway for developing more advanced and robust Reasoners.

### Multimodal Medical Code Tokenizer 
[[arxiv](https://arxiv.org/abs/2502.04397)] [[cool](https://papers.cool/arxiv/2502.04397)] [[pdf](https://arxiv.org/pdf/2502.04397)]
> **Authors**: Xiaorui Su,Shvat Messica,Yepeng Huang,Ruth Johnson,Lukas Fesser,Shanghua Gao,Faryad Sahneh,Marinka Zitnik
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: conference
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Foundation models trained on patient electronic health records (EHRs) require tokenizing medical data into sequences of discrete vocabulary items. Existing tokenizers treat medical codes from EHRs as isolated textual tokens. However, each medical code is defined by its textual description, its position in ontological hierarchies, and its relationships to other codes, such as disease co-occurrences and drug-treatment associations. Medical vocabularies contain more than 600,000 codes with critical information for clinical reasoning. We introduce MedTok, a multimodal medical code tokenizer that uses the text descriptions and relational context of codes. MedTok processes text using a language model encoder and encodes the relational structure with a graph encoder. It then quantizes both modalities into a unified token space, preserving modality-specific and cross-modality information. We integrate MedTok into five EHR models and evaluate it on operational and clinical tasks across in-patient and out-patient datasets, including outcome prediction, diagnosis classification, drug recommendation, and risk stratification. Swapping standard EHR tokenizers with MedTok improves AUPRC across all EHR models, by 4.10% on MIMIC-III, 4.78% on MIMIC-IV, and 11.30% on EHRShot, with the largest gains in drug recommendation. Beyond EHR modeling, we demonstrate using MedTok tokenizer with medical QA systems. Our results demonstrate the potential of MedTok as a unified tokenizer for medical codes, improving tokenization for medical foundation models.

### DECT: Harnessing LLM-assisted Fine-Grained Linguistic Knowledge and Label-Switched and Label-Preserved Data Generation for Diagnosis of Alzheimer's Disease 
[[arxiv](https://arxiv.org/abs/2502.04394)] [[cool](https://papers.cool/arxiv/2502.04394)] [[pdf](https://arxiv.org/pdf/2502.04394)]
> **Authors**: Tingyu Mo,Jacqueline C. K. Lam,Victor O. K. Li,Lawrence Y. L. Cheung
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Alzheimer's Disease (AD) is an irreversible neurodegenerative disease affecting 50 million people worldwide. Low-cost, accurate identification of key markers of AD is crucial for timely diagnosis and intervention. Language impairment is one of the earliest signs of cognitive decline, which can be used to discriminate AD patients from normal control individuals. Patient-interviewer dialogues may be used to detect such impairments, but they are often mixed with ambiguous, noisy, and irrelevant information, making the AD detection task difficult. Moreover, the limited availability of AD speech samples and variability in their speech styles pose significant challenges in developing robust speech-based AD detection models. To address these challenges, we propose DECT, a novel speech-based domain-specific approach leveraging large language models (LLMs) for fine-grained linguistic analysis and label-switched label-preserved data generation. Our study presents four novelties: We harness the summarizing capabilities of LLMs to identify and distill key Cognitive-Linguistic information from noisy speech transcripts, effectively filtering irrelevant information. We leverage the inherent linguistic knowledge of LLMs to extract linguistic markers from unstructured and heterogeneous audio transcripts. We exploit the compositional ability of LLMs to generate AD speech transcripts consisting of diverse linguistic patterns to overcome the speech data scarcity challenge and enhance the robustness of AD detection models. We use the augmented AD textual speech transcript dataset and a more fine-grained representation of AD textual speech transcript data to fine-tune the AD detection model. The results have shown that DECT demonstrates superior model performance with an 11% improvement in AD detection accuracy on the datasets from DementiaBank compared to the baselines.

### Division-of-Thoughts: Harnessing Hybrid Language Model Synergy for Efficient On-Device Agents 
[[arxiv](https://arxiv.org/abs/2502.04392)] [[cool](https://papers.cool/arxiv/2502.04392)] [[pdf](https://arxiv.org/pdf/2502.04392)]
> **Authors**: Chenyang Shao,Xinyuan Hu,Yutang Lin,Fengli Xu
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: The rapid expansion of web content has made on-device AI assistants indispensable for helping users manage the increasing complexity of online tasks. The emergent reasoning ability in large language models offer a promising path for next-generation on-device AI agents. However, deploying full-scale Large Language Models (LLMs) on resource-limited local devices is challenging. In this paper, we propose Division-of-Thoughts (DoT), a collaborative reasoning framework leveraging the synergy between locally deployed Smaller-scale Language Models (SLMs) and cloud-based LLMs. DoT leverages a Task Decomposer to elicit the inherent planning abilities in language models to decompose user queries into smaller sub-tasks, which allows hybrid language models to fully exploit their respective strengths. Besides, DoT employs a Task Scheduler to analyze the pair-wise dependency of sub-tasks and create a dependency graph, facilitating parallel reasoning of sub-tasks and the identification of key steps. To allocate the appropriate model based on the difficulty of sub-tasks, DoT leverages a Plug-and-Play Adapter, which is an additional task head attached to the SLM that does not alter the SLM's parameters. To boost adapter's task allocation capability, we propose a self-reinforced training method that relies solely on task execution feedback. Extensive experiments on various benchmarks demonstrate that our DoT significantly reduces LLM costs while maintaining competitive reasoning accuracy. Specifically, DoT reduces the average reasoning time and API costs by 66.12% and 83.57%, while achieving comparable reasoning accuracy with the best baseline methods.

### In Praise of Stubbornness: The Case for Cognitive-Dissonance-Aware Knowledge Updates in LLMs 
[[arxiv](https://arxiv.org/abs/2502.04390)] [[cool](https://papers.cool/arxiv/2502.04390)] [[pdf](https://arxiv.org/pdf/2502.04390)]
> **Authors**: Simone Clemente,Zied Ben Houidi,Alexis Huet,Dario Rossi,Giulio Franzese,Pietro Michiardi
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习,神经元和认知
- **Abstract**: Despite remarkable capabilities, large language models (LLMs) struggle to continually update their knowledge without catastrophic forgetting. In contrast, humans effortlessly integrate new information, detect conflicts with existing beliefs, and selectively update their mental models. This paper introduces a cognitive-inspired investigation paradigm to study continual knowledge updating in LLMs. We implement two key components inspired by human cognition: (1) Dissonance and Familiarity Awareness, analyzing model behavior to classify information as novel, familiar, or dissonant; and (2) Targeted Network Updates, which track neural activity to identify frequently used (stubborn) and rarely used (plastic) neurons. Through carefully designed experiments in controlled settings, we uncover a number of empirical findings demonstrating the potential of this approach. First, dissonance detection is feasible using simple activation and gradient features, suggesting potential for cognitive-inspired training. Second, we find that non-dissonant updates largely preserve prior knowledge regardless of targeting strategy, revealing inherent robustness in LLM knowledge integration. Most critically, we discover that dissonant updates prove catastrophically destructive to the model's knowledge base, indiscriminately affecting even information unrelated to the current updates. This suggests fundamental limitations in how neural networks handle contradictions and motivates the need for new approaches to knowledge updating that better mirror human cognitive mechanisms.

### FedP$^2$EFT: Federated Learning to Personalize Parameter Efficient Fine-Tuning for Multilingual LLMs 
[[arxiv](https://arxiv.org/abs/2502.04387)] [[cool](https://papers.cool/arxiv/2502.04387)] [[pdf](https://arxiv.org/pdf/2502.04387)]
> **Authors**: Royson Lee,Minyoung Kim,Fady Rezk,Rui Li,Stylianos I. Venieris,Timothy Hospedales
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-07
> **comment**: Preprint
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Federated learning (FL) has enabled the training of multilingual large language models (LLMs) on diverse and decentralized multilingual data, especially on low-resource languages. To improve client-specific performance, personalization via the use of parameter-efficient fine-tuning (PEFT) modules such as LoRA is common. This involves a personalization strategy (PS), such as the design of the PEFT adapter structures (e.g., in which layers to add LoRAs and what ranks) and choice of hyperparameters (e.g., learning rates) for fine-tuning. Instead of manual PS configuration, we propose FedP$^2$EFT, a federated learning-to-personalize method for multilingual LLMs in cross-device FL settings. Unlike most existing PEFT structure selection methods, which are prone to overfitting low-data regimes, FedP$^2$EFT collaboratively learns the optimal personalized PEFT structure for each client via Bayesian sparse rank selection. Evaluations on both simulated and real-world multilingual FL benchmarks demonstrate that FedP$^2$EFT largely outperforms existing personalized fine-tuning methods, while complementing a range of existing FL methods.

### Enhancing Reasoning to Adapt Large Language Models for Domain-Specific Applications 
[[arxiv](https://arxiv.org/abs/2502.04384)] [[cool](https://papers.cool/arxiv/2502.04384)] [[pdf](https://arxiv.org/pdf/2502.04384)]
> **Authors**: Bo Wen,Xin Zhang
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-07
> **comment**: NeurIPS 2024 Workshop AFM (Adaptive Foundation Models: EvolvingAIfor Personalized and EfficientLearning)
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习,系统与控制
- **Abstract**: This paper presents SOLOMON, a novel Neuro-inspired Large Language Model (LLM) Reasoning Network architecture that enhances the adaptability of foundation models for domain-specific applications. Through a case study in semiconductor layout design, we demonstrate how SOLOMON enables swift adaptation of general-purpose LLMs to specialized tasks by leveraging Prompt Engineering and In-Context Learning techniques. Our experiments reveal the challenges LLMs face in spatial reasoning and applying domain knowledge to practical problems. Results show that SOLOMON instances significantly outperform their baseline LLM counterparts and achieve performance comparable to state-of-the-art reasoning model, o1-preview. We discuss future research directions for developing more adaptive AI systems that can continually learn, adapt, and evolve in response to new information and changing requirements.

### Sparse Autoencoders for Hypothesis Generation 
[[arxiv](https://arxiv.org/abs/2502.04382)] [[cool](https://papers.cool/arxiv/2502.04382)] [[pdf](https://arxiv.org/pdf/2502.04382)]
> **Authors**: Rajiv Movva,Kenny Peng,Nikhil Garg,Jon Kleinberg,Emma Pierson
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-07
> **comment**: First two authors contributed equally; working paper
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机与社会
- **Abstract**: We describe HypotheSAEs, a general method to hypothesize interpretable relationships between text data (e.g., headlines) and a target variable (e.g., clicks). HypotheSAEs has three steps: (1) train a sparse autoencoder on text embeddings to produce interpretable features describing the data distribution, (2) select features that predict the target variable, and (3) generate a natural language interpretation of each feature (e.g., "mentions being surprised or shocked") using an LLM. Each interpretation serves as a hypothesis about what predicts the target variable. Compared to baselines, our method better identifies reference hypotheses on synthetic datasets (at least +0.06 in F1) and produces more predictive hypotheses on real datasets (~twice as many significant findings), despite requiring 1-2 orders of magnitude less compute than recent LLM-based methods. HypotheSAEs also produces novel discoveries on two well-studied tasks: explaining partisan differences in Congressional speeches and identifying drivers of engagement with online headlines.

### Limitations of Large Language Models in Clinical Problem-Solving Arising from Inflexible Reasoning 
[[arxiv](https://arxiv.org/abs/2502.04381)] [[cool](https://papers.cool/arxiv/2502.04381)] [[pdf](https://arxiv.org/pdf/2502.04381)]
> **Authors**: Jonathan Kim,Anna Podlasek,Kie Shidara,Feng Liu,Ahmed Alaa,Danilo Bernardo
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-07
> **comment**: 14 pages, 6 figures
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large Language Models (LLMs) have attained human-level accuracy on medical question-answer (QA) benchmarks. However, their limitations in navigating open-ended clinical scenarios have recently been shown, raising concerns about the robustness and generalizability of LLM reasoning across diverse, real-world medical tasks. To probe potential LLM failure modes in clinical problem-solving, we present the medical abstraction and reasoning corpus (M-ARC). M-ARC assesses clinical reasoning through scenarios designed to exploit the Einstellung effect -- the fixation of thought arising from prior experience, targeting LLM inductive biases toward inflexible pattern matching from their training data rather than engaging in flexible reasoning. We find that LLMs, including current state-of-the-art o1 and Gemini models, perform poorly compared to physicians on M-ARC, often demonstrating lack of commonsense medical reasoning and a propensity to hallucinate. In addition, uncertainty estimation analyses indicate that LLMs exhibit overconfidence in their answers, despite their limited accuracy. The failure modes revealed by M-ARC in LLM medical reasoning underscore the need to exercise caution when deploying these models in clinical settings.

### Diversity as a Reward: Fine-Tuning LLMs on a Mixture of Domain-Undetermined Data 
[[arxiv](https://arxiv.org/abs/2502.04380)] [[cool](https://papers.cool/arxiv/2502.04380)] [[pdf](https://arxiv.org/pdf/2502.04380)]
> **Authors**: Zhenqing Ling,Daoyuan Chen,Liuyi Yao,Yaliang Li,Ying Shen
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-07
> **comment**: 26 pages, 15 figures, 11 tables
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Fine-tuning large language models (LLMs) using diverse datasets is crucial for enhancing their overall performance across various domains. In practical scenarios, existing methods based on modeling the mixture proportions of data composition often struggle with data whose domain labels are missing, imprecise or non-normalized, while methods based on data selection usually encounter difficulties in balancing multi-domain performance. To address these challenges, in this paper, we study the role of data diversity in enhancing the overall abilities of LLMs by empirically constructing contrastive data pools and theoretically deriving explanations for both inter- and intra-diversity. Building upon the insights gained, we propose a new method that gives the LLM a dual identity: an output model to cognitively probe and select data based on diversity reward, as well as an input model to be tuned with the selected data. Extensive experiments show that the proposed method notably boosts performance across domain-undetermined data and a series of foundational downstream tasks when applied to various advanced LLMs. We release our code and hope this study can shed light on the understanding of data diversity and advance feedback-driven data-model co-development for LLMs.

### MEETING DELEGATE: Benchmarking LLMs on Attending Meetings on Our Behalf 
[[arxiv](https://arxiv.org/abs/2502.04376)] [[cool](https://papers.cool/arxiv/2502.04376)] [[pdf](https://arxiv.org/pdf/2502.04376)]
> **Authors**: Lingxiang Hu,Shurun Yuan,Xiaoting Qin,Jue Zhang,Qingwei Lin,Dongmei Zhang,Saravan Rajmohan,Qi Zhang
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: In contemporary workplaces, meetings are essential for exchanging ideas and ensuring team alignment but often face challenges such as time consumption, scheduling conflicts, and inefficient participation. Recent advancements in Large Language Models (LLMs) have demonstrated their strong capabilities in natural language generation and reasoning, prompting the question: can LLMs effectively delegate participants in meetings? To explore this, we develop a prototype LLM-powered meeting delegate system and create a comprehensive benchmark using real meeting transcripts. Our evaluation reveals that GPT-4/4o maintain balanced performance between active and cautious engagement strategies. In contrast, Gemini 1.5 Pro tends to be more cautious, while Gemini 1.5 Flash and Llama3-8B/70B display more active tendencies. Overall, about 60\% of responses address at least one key point from the ground-truth. However, improvements are needed to reduce irrelevant or repetitive content and enhance tolerance for transcription errors commonly found in real-world settings. Additionally, we implement the system in practical settings and collect real-world feedback from demos. Our findings underscore the potential and challenges of utilizing LLMs as meeting delegates, offering valuable insights into their practical application for alleviating the burden of meetings.

### An Analysis for Reasoning Bias of Language Models with Small Initialization 
[[arxiv](https://arxiv.org/abs/2502.04375)] [[cool](https://papers.cool/arxiv/2502.04375)] [[pdf](https://arxiv.org/pdf/2502.04375)]
> **Authors**: Junjie Yao,Zhongwang Zhang,Zhi-Qin John Xu
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-07
> **comment**: 30 pages, 14 figures
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Transformer-based Large Language Models (LLMs) have revolutionized Natural Language Processing by demonstrating exceptional performance across diverse tasks. This study investigates the impact of the parameter initialization scale on the training behavior and task preferences of LLMs. We discover that smaller initialization scales encourage models to favor reasoning tasks, whereas larger initialization scales lead to a preference for memorization tasks. We validate this reasoning bias via real datasets and meticulously designed anchor functions. Further analysis of initial training dynamics suggests that specific model components, particularly the embedding space and self-attention mechanisms, play pivotal roles in shaping these learning biases. We provide a theoretical framework from the perspective of model training dynamics to explain these phenomena. Additionally, experiments on real-world language tasks corroborate our theoretical insights. This work enhances our understanding of how initialization strategies influence LLM performance on reasoning tasks and offers valuable guidelines for training models.

### Mining Unstructured Medical Texts With Conformal Active Learning 
[[arxiv](https://arxiv.org/abs/2502.04372)] [[cool](https://papers.cool/arxiv/2502.04372)] [[pdf](https://arxiv.org/pdf/2502.04372)]
> **Authors**: Juliano Genari,Guilherme Tegoni Goedert
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习,机器学习
- **Abstract**: The extraction of relevant data from Electronic Health Records (EHRs) is crucial to identifying symptoms and automating epidemiological surveillance processes. By harnessing the vast amount of unstructured text in EHRs, we can detect patterns that indicate the onset of disease outbreaks, enabling faster, more targeted public health responses. Our proposed framework provides a flexible and efficient solution for mining data from unstructured texts, significantly reducing the need for extensive manual labeling by specialists. Experiments show that our framework achieving strong performance with as few as 200 manually labeled texts, even for complex classification problems. Additionally, our approach can function with simple lightweight models, achieving competitive and occasionally even better results compared to more resource-intensive deep learning models. This capability not only accelerates processing times but also preserves patient privacy, as the data can be processed on weaker on-site hardware rather than being transferred to external systems. Our methodology, therefore, offers a practical, scalable, and privacy-conscious approach to real-time epidemiological monitoring, equipping health institutions to respond rapidly and effectively to emerging health threats.

### DreamDPO: Aligning Text-to-3D Generation with Human Preferences via Direct Preference Optimization 
[[arxiv](https://arxiv.org/abs/2502.04370)] [[cool](https://papers.cool/arxiv/2502.04370)] [[pdf](https://arxiv.org/pdf/2502.04370)]
> **Authors**: Zhenglin Zhou,Xiaobo Xia,Fan Ma,Hehe Fan,Yi Yang,Tat-Seng Chua
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-07
> **comment**: 20 pages, 12 figures
- **标题**: None
- **领域**: 计算语言学,图形,机器学习
- **Abstract**: Text-to-3D generation automates 3D content creation from textual descriptions, which offers transformative potential across various fields. However, existing methods often struggle to align generated content with human preferences, limiting their applicability and flexibility. To address these limitations, in this paper, we propose DreamDPO, an optimization-based framework that integrates human preferences into the 3D generation process, through direct preference optimization. Practically, DreamDPO first constructs pairwise examples, then compare their alignment with human preferences using reward or large multimodal models, and lastly optimizes the 3D representation with a preference-driven loss function. By leveraging pairwise comparison to reflect preferences, DreamDPO reduces reliance on precise pointwise quality evaluations while enabling fine-grained controllability through preference-guided optimization. Experiments demonstrate that DreamDPO achieves competitive results, and provides higher-quality and more controllable 3D content compared to existing methods. The code and models will be open-sourced.

### Contrastive Token-level Explanations for Graph-based Rumour Detection 
[[arxiv](https://arxiv.org/abs/2502.04366)] [[cool](https://papers.cool/arxiv/2502.04366)] [[pdf](https://arxiv.org/pdf/2502.04366)]
> **Authors**: Daniel Wai Kit Chin,Roy Ka-Wei Lee
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-07
> **comment**: This work has been submitted to the IEEE for possible publication
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: The widespread use of social media has accelerated the dissemination of information, but it has also facilitated the spread of harmful rumours, which can disrupt economies, influence political outcomes, and exacerbate public health crises, such as the COVID-19 pandemic. While Graph Neural Network (GNN)-based approaches have shown significant promise in automated rumour detection, they often lack transparency, making their predictions difficult to interpret. Existing graph explainability techniques fall short in addressing the unique challenges posed by the dependencies among feature dimensions in high-dimensional text embeddings used in GNN-based models. In this paper, we introduce Contrastive Token Layerwise Relevance Propagation (CT-LRP), a novel framework designed to enhance the explainability of GNN-based rumour detection. CT-LRP extends current graph explainability methods by providing token-level explanations that offer greater granularity and interpretability. We evaluate the effectiveness of CT-LRP across multiple GNN models trained on three publicly available rumour detection datasets, demonstrating that it consistently produces high-fidelity, meaningful explanations, paving the way for more robust and trustworthy rumour detection systems.

### LLMs can be easily Confused by Instructional Distractions 
[[arxiv](https://arxiv.org/abs/2502.04362)] [[cool](https://papers.cool/arxiv/2502.04362)] [[pdf](https://arxiv.org/pdf/2502.04362)]
> **Authors**: Yerin Hwang,Yongil Kim,Jahyun Koo,Taegwan Kang,Hyunkyung Bae,Kyomin Jung
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-07
> **comment**: 8 pages
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Despite the fact that large language models (LLMs) show exceptional skill in instruction following tasks, this strength can turn into a vulnerability when the models are required to disregard certain instructions. Instruction-following tasks typically involve a clear task description and input text containing the target data to be processed. However, when the input itself resembles an instruction, confusion may arise, even if there is explicit prompting to distinguish between the task instruction and the input. We refer to this phenomenon as instructional distraction. In this paper, we introduce a novel benchmark, named DIM-Bench, specifically designed to assess LLMs' performance under instructional distraction. The benchmark categorizes real-world instances of instructional distraction and evaluates LLMs across four instruction tasks: rewriting, proofreading, translation, and style transfer -- alongside five input tasks: reasoning, code generation, mathematical reasoning, bias detection, and question answering. Our experimental results reveal that even the most advanced LLMs are susceptible to instructional distraction, often failing to accurately follow user intent in such cases.

### MARAGE: Transferable Multi-Model Adversarial Attack for Retrieval-Augmented Generation Data Extraction 
[[arxiv](https://arxiv.org/abs/2502.04360)] [[cool](https://papers.cool/arxiv/2502.04360)] [[pdf](https://arxiv.org/pdf/2502.04360)]
> **Authors**: Xiao Hu,Eric Liu,Weizhou Wang,Xiangyu Guo,David Lie
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,密码学和安全,机器学习
- **Abstract**: Retrieval-Augmented Generation (RAG) offers a solution to mitigate hallucinations in Large Language Models (LLMs) by grounding their outputs to knowledge retrieved from external sources. The use of private resources and data in constructing these external data stores can expose them to risks of extraction attacks, in which attackers attempt to steal data from these private databases. Existing RAG extraction attacks often rely on manually crafted prompts, which limit their effectiveness. In this paper, we introduce a framework called MARAGE for optimizing an adversarial string that, when appended to user queries submitted to a target RAG system, causes outputs containing the retrieved RAG data verbatim. MARAGE leverages a continuous optimization scheme that integrates gradients from multiple models with different architectures simultaneously to enhance the transferability of the optimized string to unseen models. Additionally, we propose a strategy that emphasizes the initial tokens in the target RAG data, further improving the attack's generalizability. Evaluations show that MARAGE consistently outperforms both manual and optimization-based baselines across multiple LLMs and RAG datasets, while maintaining robust transferability to previously unseen models. Moreover, we conduct probing tasks to shed light on the reasons why MARAGE is more effective compared to the baselines and to analyze the impact of our approach on the model's internal state.

### Exploring Spatial Language Grounding Through Referring Expressions 
[[arxiv](https://arxiv.org/abs/2502.04359)] [[cool](https://papers.cool/arxiv/2502.04359)] [[pdf](https://arxiv.org/pdf/2502.04359)]
> **Authors**: Akshar Tumu,Parisa Kordjamshidi
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别
- **Abstract**: Spatial Reasoning is an important component of human cognition and is an area in which the latest Vision-language models (VLMs) show signs of difficulty. The current analysis works use image captioning tasks and visual question answering. In this work, we propose using the Referring Expression Comprehension task instead as a platform for the evaluation of spatial reasoning by VLMs. This platform provides the opportunity for a deeper analysis of spatial comprehension and grounding abilities when there is 1) ambiguity in object detection, 2) complex spatial expressions with a longer sentence structure and multiple spatial relations, and 3) expressions with negation ('not'). In our analysis, we use task-specific architectures as well as large VLMs and highlight their strengths and weaknesses in dealing with these specific situations. While all these models face challenges with the task at hand, the relative behaviors depend on the underlying models and the specific categories of spatial semantics (topological, directional, proximal, etc.). Our results highlight these challenges and behaviors and provide insight into research gaps and future directions.

### Position: Scaling LLM Agents Requires Asymptotic Analysis with LLM Primitives 
[[arxiv](https://arxiv.org/abs/2502.04358)] [[cool](https://papers.cool/arxiv/2502.04358)] [[pdf](https://arxiv.org/pdf/2502.04358)]
> **Authors**: Elliot Meyerson,Xin Qiu
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-07
> **comment**: 12 pages including references
- **标题**: None
- **领域**: 计算语言学,人工智能,计算复杂度,机器学习,神经和进化计算
- **Abstract**: Decomposing hard problems into subproblems often makes them easier and more efficient to solve. With large language models (LLMs) crossing critical reliability thresholds for a growing slate of capabilities, there is an increasing effort to decompose systems into sets of LLM-based agents, each of whom can be delegated sub-tasks. However, this decomposition (even when automated) is often intuitive, e.g., based on how a human might assign roles to members of a human team. How close are these role decompositions to optimal? This position paper argues that asymptotic analysis with LLM primitives is needed to reason about the efficiency of such decomposed systems, and that insights from such analysis will unlock opportunities for scaling them. By treating the LLM forward pass as the atomic unit of computational cost, one can separate out the (often opaque) inner workings of a particular LLM from the inherent efficiency of how a set of LLMs are orchestrated to solve hard problems. In other words, if we want to scale the deployment of LLMs to the limit, instead of anthropomorphizing LLMs, asymptotic analysis with LLM primitives should be used to reason about and develop more powerful decompositions of large problems into LLM agents.

### Reusing Embeddings: Reproducible Reward Model Research in Large Language Model Alignment without GPUs 
[[arxiv](https://arxiv.org/abs/2502.04357)] [[cool](https://papers.cool/arxiv/2502.04357)] [[pdf](https://arxiv.org/pdf/2502.04357)]
> **Authors**: Hao Sun,Yunyi Shen,Jean-Francois Ton,Mihaela van der Schaar
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Large Language Models (LLMs) have made substantial strides in structured tasks through Reinforcement Learning (RL), demonstrating proficiency in mathematical reasoning and code generation. However, applying RL in broader domains like chatbots and content generation -- through the process known as Reinforcement Learning from Human Feedback (RLHF) -- presents unique challenges. Reward models in RLHF are critical, acting as proxies that evaluate the alignment of LLM outputs with human intent. Despite advancements, the development of reward models is hindered by challenges such as computational heavy training, costly evaluation, and therefore poor reproducibility. We advocate for using embedding-based input in reward model research as an accelerated solution to those challenges. By leveraging embeddings for reward modeling, we can enhance reproducibility, reduce computational demands on hardware, improve training stability, and significantly reduce training and evaluation costs, hence facilitating fair and efficient comparisons in this active research area. We then show a case study of reproducing existing reward model ensemble research using embedding-based reward models. We discussed future avenues for research, aiming to contribute to safer and more effective LLM deployments.

### Open Foundation Models in Healthcare: Challenges, Paradoxes, and Opportunities with GenAI Driven Personalized Prescription 
[[arxiv](https://arxiv.org/abs/2502.04356)] [[cool](https://papers.cool/arxiv/2502.04356)] [[pdf](https://arxiv.org/pdf/2502.04356)]
> **Authors**: Mahdi Alkaeed,Sofiat Abioye,Adnan Qayyum,Yosra Magdi Mekki,Ilhem Berrou,Mohamad Abdallah,Ala Al-Fuqaha,Muhammad Bilal,Junaid Qadir
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: In response to the success of proprietary Large Language Models (LLMs) such as OpenAI's GPT-4, there is a growing interest in developing open, non-proprietary LLMs and AI foundation models (AIFMs) for transparent use in academic, scientific, and non-commercial applications. Despite their inability to match the refined functionalities of their proprietary counterparts, open models hold immense potential to revolutionize healthcare applications. In this paper, we examine the prospects of open-source LLMs and AIFMs for developing healthcare applications and make two key contributions. Firstly, we present a comprehensive survey of the current state-of-the-art open-source healthcare LLMs and AIFMs and introduce a taxonomy of these open AIFMs, categorizing their utility across various healthcare tasks. Secondly, to evaluate the general-purpose applications of open LLMs in healthcare, we present a case study on personalized prescriptions. This task is particularly significant due to its critical role in delivering tailored, patient-specific medications that can greatly improve treatment outcomes. In addition, we compare the performance of open-source models with proprietary models in settings with and without Retrieval-Augmented Generation (RAG). Our findings suggest that, although less refined, open LLMs can achieve performance comparable to proprietary models when paired with grounding techniques such as RAG. Furthermore, to highlight the clinical significance of LLMs-empowered personalized prescriptions, we perform subjective assessment through an expert clinician. We also elaborate on ethical considerations and potential risks associated with the misuse of powerful LLMs and AIFMs, highlighting the need for a cautious and responsible implementation in healthcare.

### LLM-ProS: Analyzing Large Language Models' Performance in Competitive Problem Solving 
[[arxiv](https://arxiv.org/abs/2502.04355)] [[cool](https://papers.cool/arxiv/2502.04355)] [[pdf](https://arxiv.org/pdf/2502.04355)]
> **Authors**: Md Sifat Hossain,Anika Tabassum,Md. Fahim Arefin,Tarannum Shaila Zaman
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-07
> **comment**: To be published in LLM4Code 2025 workshop proceedings
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: The rapid advancement of large language models has opened new avenues for automating complex problem-solving tasks such as algorithmic coding and competitive programming. This paper introduces a novel evaluation technique, LLM-ProS, to assess the performance of state-of-the-art LLMs on International Collegiate Programming Contest (ICPC) problems. Using a curated dataset of 166 World Finals problems from 2011 to 2024, we benchmark the models' reasoning, accuracy, and efficiency. We evaluate the five models-GPT-4o, Mistral Large, Llama-3.1-405B, and the o1 family, consisting of o1-mini and o1-preview, across critical metrics like correctness, resource utilization, and response calibration. Our results reveal significant differences in the models' abilities to generalize, adapt, and solve novel problems. We also investigated the impact of training methodologies, dataset contamination, and chain-of-thought reasoning on model performance. The findings provide new insights into optimizing LLMs for algorithmic tasks, highlighting both strengths and limitations of current models.

### Reviving The Classics: Active Reward Modeling in Large Language Model Alignment 
[[arxiv](https://arxiv.org/abs/2502.04354)] [[cool](https://papers.cool/arxiv/2502.04354)] [[pdf](https://arxiv.org/pdf/2502.04354)]
> **Authors**: Yunyi Shen,Hao Sun,Jean-François Ton
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Building neural reward models from human preferences is a pivotal component in reinforcement learning from human feedback (RLHF) and large language model alignment research. Given the scarcity and high cost of human annotation, how to select the most informative pairs to annotate is an essential yet challenging open problem. In this work, we highlight the insight that an ideal comparison dataset for reward modeling should balance exploration of the representation space and make informative comparisons between pairs with moderate reward differences. Technically, challenges arise in quantifying the two objectives and efficiently prioritizing the comparisons to be annotated. To address this, we propose the Fisher information-based selection strategies, adapt theories from the classical experimental design literature, and apply them to the final linear layer of the deep neural network-based reward modeling tasks. Empirically, our method demonstrates remarkable performance, high computational efficiency, and stability compared to other selection methods from deep learning and classical statistical literature across multiple open-source LLMs and datasets. Further ablation studies reveal that incorporating cross-prompt comparisons in active reward modeling significantly enhances labeling efficiency, shedding light on the potential for improved annotation strategies in RLHF.

### CognArtive: Large Language Models for Automating Art Analysis and Decoding Aesthetic Elements 
[[arxiv](https://arxiv.org/abs/2502.04353)] [[cool](https://papers.cool/arxiv/2502.04353)] [[pdf](https://arxiv.org/pdf/2502.04353)]
> **Authors**: Afshin Khadangi,Amir Sartipi,Igor Tchappi,Gilbert Fridgen
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别
- **Abstract**: Art, as a universal language, can be interpreted in diverse ways, with artworks embodying profound meanings and nuances. The advent of Large Language Models (LLMs) and the availability of Multimodal Large Language Models (MLLMs) raise the question of how these transformative models can be used to assess and interpret the artistic elements of artworks. While research has been conducted in this domain, to the best of our knowledge, a deep and detailed understanding of the technical and expressive features of artworks using LLMs has not been explored. In this study, we investigate the automation of a formal art analysis framework to analyze a high-throughput number of artworks rapidly and examine how their patterns evolve over time. We explore how LLMs can decode artistic expressions, visual elements, composition, and techniques, revealing emerging patterns that develop across periods. Finally, we discuss the strengths and limitations of LLMs in this context, emphasizing their ability to process vast quantities of art-related data and generate insightful interpretations. Due to the exhaustive and granular nature of the results, we have developed interactive data visualizations, available online https://cognartive.github.io/, to enhance understanding and accessibility.

### Investigating the Robustness of Deductive Reasoning with Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.04352)] [[cool](https://papers.cool/arxiv/2502.04352)] [[pdf](https://arxiv.org/pdf/2502.04352)]
> **Authors**: Fabian Hoppe,Filip Ilievski,Jan-Christoph Kalo
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large Language Models (LLMs) have been shown to achieve impressive results for many reasoning-based Natural Language Processing (NLP) tasks, suggesting a degree of deductive reasoning capability. However, it remains unclear to which extent LLMs, in both informal and autoformalisation methods, are robust on logical deduction tasks. Moreover, while many LLM-based deduction methods have been proposed, there is a lack of a systematic study that analyses the impact of their design components. Addressing these two challenges, we propose the first study of the robustness of LLM-based deductive reasoning methods. We devise a framework with two families of perturbations: adversarial noise and counterfactual statements, which jointly generate seven perturbed datasets. We organize the landscape of LLM reasoners according to their reasoning format, formalisation syntax, and feedback for error recovery. The results show that adversarial noise affects autoformalisation, while counterfactual statements influence all approaches. Detailed feedback does not improve overall accuracy despite reducing syntax errors, pointing to the challenge of LLM-based methods to self-correct effectively.

### NER4all or Context is All You Need: Using LLMs for low-effort, high-performance NER on historical texts. A humanities informed approach 
[[arxiv](https://arxiv.org/abs/2502.04351)] [[cool](https://papers.cool/arxiv/2502.04351)] [[pdf](https://arxiv.org/pdf/2502.04351)]
> **Authors**: Torsten Hiltmann,Martin Dröge,Nicole Dresselhaus,Till Grallert,Melanie Althage,Paul Bayer,Sophie Eckenstaler,Koray Mendi,Jascha Marijn Schmitz,Philipp Schneider,Wiebke Sczeponik,Anica Skibba
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-07
> **comment**: :I.2.7
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Named entity recognition (NER) is a core task for historical research in automatically establishing all references to people, places, events and the like. Yet, do to the high linguistic and genre diversity of sources, only limited canonisation of spellings, the level of required historical domain knowledge, and the scarcity of annotated training data, established approaches to natural language processing (NLP) have been both extremely expensive and yielded only unsatisfactory results in terms of recall and precision. Our paper introduces a new approach. We demonstrate how readily-available, state-of-the-art LLMs significantly outperform two leading NLP frameworks, spaCy and flair, for NER in historical documents by seven to twentytwo percent higher F1-Scores. Our ablation study shows how providing historical context to the task and a bit of persona modelling that turns focus away from a purely linguistic approach are core to a successful prompting strategy. We also demonstrate that, contrary to our expectations, providing increasing numbers of examples in few-shot approaches does not improve recall or precision below a threshold of 16-shot. In consequence, our approach democratises access to NER for all historians by removing the barrier of scripting languages and computational skills required for established NLP tools and instead leveraging natural language prompts and consumer-grade tools and frontends.

### CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance 
[[arxiv](https://arxiv.org/abs/2502.04350)] [[cool](https://papers.cool/arxiv/2502.04350)] [[pdf](https://arxiv.org/pdf/2502.04350)]
> **Authors**: Yongchao Chen,Yilun Hao,Yueying Liu,Yang Zhang,Chuchu Fan
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-07
> **comment**: 27 pages, 12 figures
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习,符号计算,软件工程
- **Abstract**: Existing methods fail to effectively steer Large Language Models (LLMs) between textual reasoning and code generation, leaving symbolic computing capabilities underutilized. We introduce CodeSteer, an effective method for guiding LLM code/text generation. We construct a comprehensive benchmark SymBench comprising 37 symbolic tasks with adjustable complexity and also synthesize datasets of 12k multi-round guidance/generation trajectories and 5.5k guidance comparison pairs. We fine-tune the Llama-3-8B model with a newly designed multi-round supervised fine-tuning (SFT) and direct preference optimization (DPO). The resulting model, CodeSteerLLM, augmented with the proposed symbolic and self-answer checkers, effectively guides the code/text generation of larger models. Augmenting GPT-4o with CodeSteer raises its average performance score from 53.3 to 86.4, even outperforming the existing best LLM OpenAI o1 (82.7), o1-preview (74.8), and DeepSeek R1 (76.8) across all 37 tasks (28 seen, 9 unseen). Trained for GPT-4o, CodeSteer demonstrates superior generalizability, providing an average 41.8 performance boost on Claude, Mistral, and GPT-3.5. CodeSteer-guided LLMs fully harness symbolic computing to maintain strong performance on highly complex tasks. Models, Datasets, and Codes are available at https://github.com/yongchao98/CodeSteer-v1.0.

### Dynamic benchmarking framework for LLM-based conversational data capture 
[[arxiv](https://arxiv.org/abs/2502.04349)] [[cool](https://papers.cool/arxiv/2502.04349)] [[pdf](https://arxiv.org/pdf/2502.04349)]
> **Authors**: Pietro Alessandro Aluffi,Patrick Zietkiewicz,Marya Bazzi,Matt Arderne,Vladimirs Murevics
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: The rapid evolution of large language models (LLMs) has transformed conversational agents, enabling complex human-machine interactions. However, evaluation frameworks often focus on single tasks, failing to capture the dynamic nature of multi-turn dialogues. This paper introduces a dynamic benchmarking framework to assess LLM-based conversational agents through interactions with synthetic users. The framework integrates generative agent simulation to evaluate performance on key dimensions: information extraction, context awareness, and adaptive engagement. By simulating various aspects of user behavior, our work provides a scalable, automated, and flexible benchmarking approach. Experimental evaluation - within a loan application use case - demonstrates the framework's effectiveness under one-shot and few-shot extraction conditions. Results show that adaptive strategies improve data extraction accuracy, especially when handling ambiguous responses. Future work will extend its applicability to broader domains and incorporate additional metrics (e.g., conversational coherence, user engagement). This study contributes a structured, scalable approach to evaluating LLM-based conversational agents, facilitating real-world deployment.

### Prompt-based Depth Pruning of Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.04348)] [[cool](https://papers.cool/arxiv/2502.04348)] [[pdf](https://arxiv.org/pdf/2502.04348)]
> **Authors**: Juyun Wee,Minjae Park,Jaeho Lee
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-07
> **comment**: 13 pages, 5 figures
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Depth pruning aims to reduce the inference cost of a large language model without any hardware-specific complications, by simply removing several less important transformer blocks. However, our empirical findings suggest that the importance of a transformer block may be highly task-dependent -- a block that is crucial for a task can be removed without degrading the accuracy on another task. Based on this observation, we develop a dynamic depth pruning algorithm, coined PuDDing (Prompt-routed Dynamic Depth Pruning), which determines which blocks to omit from the model based on the input prompt. PuDDing operates by training a lightweight router to predict the best omission set among a set of options, where this option set has also been constructed in a data-driven manner. Empirical results on commonsense reasoning benchmarks demonstrate that PuDDing effectively accelerates the inference language models, and achieves better on-task performance than static depth pruning baselines.

### SCALM: Detecting Bad Practices in Smart Contracts Through LLMs 
[[arxiv](https://arxiv.org/abs/2502.04347)] [[cool](https://papers.cool/arxiv/2502.04347)] [[pdf](https://arxiv.org/pdf/2502.04347)]
> **Authors**: Zongwei Li,Xiaoqi Li,Wenkai Li,Xin Wang
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-07
> **comment**: 7 pages
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: As the Ethereum platform continues to mature and gain widespread usage, it is crucial to maintain high standards of smart contract writing practices. While bad practices in smart contracts may not directly lead to security issues, they do elevate the risk of encountering problems. Therefore, to understand and avoid these bad practices, this paper introduces the first systematic study of bad practices in smart contracts, delving into over 35 specific issues. Specifically, we propose a large language models (LLMs)-based framework, SCALM. It combines Step-Back Prompting and Retrieval-Augmented Generation (RAG) to identify and address various bad practices effectively. Our extensive experiments using multiple LLMs and datasets have shown that SCALM outperforms existing tools in detecting bad practices in smart contracts.

### Multi-Lingual Cyber Threat Detection in Tweets/X Using ML, DL, and LLM: A Comparative Analysis 
[[arxiv](https://arxiv.org/abs/2502.04346)] [[cool](https://papers.cool/arxiv/2502.04346)] [[pdf](https://arxiv.org/pdf/2502.04346)]
> **Authors**: Saydul Akbar Murad,Ashim Dahal,Nick Rahimi
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Cyber threat detection has become an important area of focus in today's digital age due to the growing spread of fake information and harmful content on social media platforms such as Twitter (now 'X'). These cyber threats, often disguised within tweets, pose significant risks to individuals, communities, and even nations, emphasizing the need for effective detection systems. While previous research has explored tweet-based threats, much of the work is limited to specific languages, domains, or locations, or relies on single-model approaches, reducing their applicability to diverse real-world scenarios. To address these gaps, our study focuses on multi-lingual tweet cyber threat detection using a variety of advanced models. The research was conducted in three stages: (1) We collected and labeled tweet datasets in four languages English, Chinese, Russian, and Arabic employing both manual and polarity-based labeling methods to ensure high-quality annotations. (2) Each dataset was analyzed individually using machine learning (ML) and deep learning (DL) models to assess their performance on distinct languages. (3) Finally, we combined all four datasets into a single multi-lingual dataset and applied DL and large language model (LLM) architectures to evaluate their efficacy in identifying cyber threats across various languages. Our results show that among machine learning models, Random Forest (RF) attained the highest performance; however, the Bi-LSTM architecture consistently surpassed other DL and LLM architectures across all datasets. These findings underline the effectiveness of Bi-LSTM in multilingual cyber threat detection. The code for this paper can be found at this link: https://github.com/Mmurrad/Tweet-Data-Classification.git.

### JingFang: A Traditional Chinese Medicine Large Language Model of Expert-Level Medical Diagnosis and Syndrome Differentiation-Based Treatment 
[[arxiv](https://arxiv.org/abs/2502.04345)] [[cool](https://papers.cool/arxiv/2502.04345)] [[pdf](https://arxiv.org/pdf/2502.04345)]
> **Authors**: Yehan Yan,Tianhao Ma,Ruotai Li,Xinhan Zheng,Guodong Shan,Chisheng Li
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Traditional Chinese medicine (TCM) plays a vital role in health protection and disease treatment, but its practical application requires extensive medical knowledge and clinical experience. Existing TCM Large Language Models (LLMs) exhibit critical limitations of uncomprehensive medical consultation and diagnoses, and inaccurate syndrome differentiation-based treatment. To address these issues, this study establishes JingFang (JF): a novel TCM Large Language Model that demonstrates the expert-level capability of medical diagnosis and syndrome differentiation-based treatment. We innovate a Multi-agent Dynamic Collaborative Chain-of-Thought Mechanism (MDCCTM) for medical consultation, enabling JF with effective and accurate diagnostic ability. In addition, a Syndrome Agent and a Dual-Stage Retrieval Scheme (DSRS) are developed to significantly enhance the capacity of JF for disease treatment based on syndrome differentiation. JingFang not only facilitates the application of LLMs but also promotes the effective practice of TCM in human health protection and disease treatment.

### Tutorial on Using Machine Learning and Deep Learning Models for Mental Illness Detection 
[[arxiv](https://arxiv.org/abs/2502.04342)] [[cool](https://papers.cool/arxiv/2502.04342)] [[pdf](https://arxiv.org/pdf/2502.04342)]
> **Authors**: Yeyubei Zhang,Zhongyan Wang,Zhanyi Ding,Yexin Tian,Jianglai Dai,Xiaorui Shen,Yunchong Liu,Yuchen Cao
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Social media has become an important source for understanding mental health, providing researchers with a way to detect conditions like depression from user-generated posts. This tutorial provides practical guidance to address common challenges in applying machine learning and deep learning methods for mental health detection on these platforms. It focuses on strategies for working with diverse datasets, improving text preprocessing, and addressing issues such as imbalanced data and model evaluation. Real-world examples and step-by-step instructions demonstrate how to apply these techniques effectively, with an emphasis on transparency, reproducibility, and ethical considerations. By sharing these approaches, this tutorial aims to help researchers build more reliable and widely applicable models for mental health research, contributing to better tools for early detection and intervention.

### Can Grammarly and ChatGPT accelerate language change? AI-powered technologies and their impact on the English language: wordiness vs. conciseness 
[[arxiv](https://arxiv.org/abs/2502.04324)] [[cool](https://papers.cool/arxiv/2502.04324)] [[pdf](https://arxiv.org/pdf/2502.04324)]
> **Authors**: Karolina Rudnicka
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 10 pages, article
- **标题**: None
- **领域**: 计算语言学,计算机与社会
- **Abstract**: The proliferation of NLP-powered language technologies, AI-based natural language generation models, and English as a mainstream means of communication among both native and non-native speakers make the output of AI-powered tools especially intriguing to linguists. This paper investigates how Grammarly and ChatGPT affect the English language regarding wordiness vs. conciseness. A case study focusing on the purpose subordinator in order to is presented to illustrate the way in which Grammarly and ChatGPT recommend shorter grammatical structures instead of longer and more elaborate ones. Although the analysed sentences were produced by native speakers, are perfectly correct, and were extracted from a language corpus of contemporary English, both Grammarly and ChatGPT suggest more conciseness and less verbosity, even for relatively short sentences. The present article argues that technologies such as Grammarly not only mirror language change but also have the potential to facilitate or accelerate it.

### ChameleonLLM: Batch-Aware Dynamic Low-Rank Adaptation via Inference-Time Clusters 
[[arxiv](https://arxiv.org/abs/2502.04315)] [[cool](https://papers.cool/arxiv/2502.04315)] [[pdf](https://arxiv.org/pdf/2502.04315)]
> **Authors**: Kamer Ali Yuksel,Hassan Sawaf
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Recent advances in large language models (LLMs) have shown remarkable performance across diverse tasks. However, these models are typically deployed with fixed weights, which limits their ability to adapt dynamically to the variability inherent in real-world data during inference. This paper introduces ChameleonLLM, a novel framework that enables inference-time adaptation of LLMs by leveraging batch-aware clustering and on-the-fly generation of low-rank updates. Unlike traditional fine-tuning approaches such as Low-Rank Adaptation (LoRA) or methods that rely on a fixed set of pre-learned uniforms (changeable masks), our method dynamically generates adaptive modifications to the decoder weights based on the aggregated statistics of clustered batches. By intelligently grouping similar inputs and computing context-aware low-rank updates via a hyper-network, ChameleonLLM achieves significant performance gains, outperforming conventional LoRA methods while eliminating the overhead of maintaining multiple expert models. Our experiments highlight the potential of our approach to serve as a versatile and highly adaptive solution for language model inference. ChameleonLLM is open-sourced to ensure the reproducibility of our experiments: https://anonymous.4open.science/r/ChamaleonLLM/

### ScoreFlow: Mastering LLM Agent Workflows via Score-based Preference Optimization 
[[arxiv](https://arxiv.org/abs/2502.04306)] [[cool](https://papers.cool/arxiv/2502.04306)] [[pdf](https://arxiv.org/pdf/2502.04306)]
> **Authors**: Yinjie Wang,Ling Yang,Guohao Li,Mengdi Wang,Bryon Aragam
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: Project: https://github.com/Gen-Verse/ScoreFlow
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Recent research has leveraged large language model multi-agent systems for complex problem-solving while trying to reduce the manual effort required to build them, driving the development of automated agent workflow optimization methods. However, existing methods remain inflexible due to representational limitations, a lack of adaptability, and poor scalability when relying on discrete optimization techniques. We address these challenges with ScoreFlow, a simple yet high-performance framework that leverages efficient gradient-based optimization in a continuous space. ScoreFlow incorporates Score-DPO, a novel variant of the direct preference optimization method that accounts for quantitative feedback. Across six benchmarks spanning question answering, coding, and mathematical reasoning, ScoreFlow achieves an 8.2% improvement over existing baselines. Moreover, it empowers smaller models to outperform larger ones with lower inference costs. Project: https://github.com/Gen-Verse/ScoreFlow

### Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization 
[[arxiv](https://arxiv.org/abs/2502.04295)] [[cool](https://papers.cool/arxiv/2502.04295)] [[pdf](https://arxiv.org/pdf/2502.04295)]
> **Authors**: Yuanye Liu,Jiahang Xu,Li Lyna Zhang,Qi Chen,Xuan Feng,Yang Chen,Zhongxin Guo,Yuqing Yang,Peng Cheng
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models (LLMs) have shown significant capability across various tasks, with their real-world effectiveness often driven by prompt design. While recent research has focused on optimizing prompt content, the role of prompt formatting, a critical but often overlooked dimension, has received limited systematic investigation. In this paper, we introduce Content-Format Integrated Prompt Optimization (CFPO), an innovative methodology that jointly optimizes both prompt content and formatting through an iterative refinement process. CFPO leverages natural language mutations to explore content variations and employs a dynamic format exploration strategy that systematically evaluates diverse format options. Our extensive evaluations across multiple tasks and open-source LLMs demonstrate that CFPO demonstrates measurable performance improvements compared to content-only optimization methods. This highlights the importance of integrated content-format optimization and offers a practical, model-agnostic approach to enhancing LLM performance. Code is available at https://github.com/HenryLau7/CFPO.

### How does a Multilingual LM Handle Multiple Languages? 
[[arxiv](https://arxiv.org/abs/2502.04269)] [[cool](https://papers.cool/arxiv/2502.04269)] [[pdf](https://arxiv.org/pdf/2502.04269)]
> **Authors**: Santhosh Kakarla,Gautama Shastry Bulusu Venkata,Aishwarya Gaddam
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 10 pages, 8 figures
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Multilingual language models have significantly advanced due to rapid progress in natural language processing. Models like BLOOM 1.7B, trained on diverse multilingual datasets, aim to bridge linguistic gaps. However, their effectiveness in capturing linguistic knowledge, particularly for low-resource languages, remains an open question. This study critically examines MLMs capabilities in multilingual understanding, semantic representation, and cross-lingual knowledge transfer. While these models perform well for high-resource languages, they struggle with less-represented ones. Additionally, traditional evaluation methods often overlook their internal syntactic and semantic encoding. This research addresses key limitations through three objectives. First, it assesses semantic similarity by analyzing multilingual word embeddings for consistency using cosine similarity. Second, it examines BLOOM-1.7B and Qwen2 through Named Entity Recognition and sentence similarity tasks to understand their linguistic structures. Third, it explores cross-lingual knowledge transfer by evaluating generalization from high-resource to low-resource languages in sentiment analysis and text classification. By leveraging linguistic probing, performance metrics, and visualizations, this study provides insights into the strengths and limitations of MLMs. The findings aim to enhance multilingual NLP models, ensuring better support for both high- and low-resource languages, thereby promoting inclusivity in language technologies.

### TriNER: A Series of Named Entity Recognition Models For Hindi, Bengali & Marathi 
[[arxiv](https://arxiv.org/abs/2502.04245)] [[cool](https://papers.cool/arxiv/2502.04245)] [[pdf](https://arxiv.org/pdf/2502.04245)]
> **Authors**: Mohammed Amaan Dhamaskar,Rasika Ransing
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: India's rich cultural and linguistic diversity poses various challenges in the domain of Natural Language Processing (NLP), particularly in Named Entity Recognition (NER). NER is a NLP task that aims to identify and classify tokens into different entity groups like Person, Location, Organization, Number, etc. This makes NER very useful for downstream tasks like context-aware anonymization. This paper details our work to build a multilingual NER model for the three most spoken languages in India - Hindi, Bengali & Marathi. We train a custom transformer model and fine tune a few pretrained models, achieving an F1 Score of 92.11 for a total of 6 entity groups. Through this paper, we aim to introduce a single model to perform NER and significantly reduce the inconsistencies in entity groups and tag names, across the three languages.

### MAGA: MAssive Genre-Audience Reformulation to Pretraining Corpus Expansion 
[[arxiv](https://arxiv.org/abs/2502.04235)] [[cool](https://papers.cool/arxiv/2502.04235)] [[pdf](https://arxiv.org/pdf/2502.04235)]
> **Authors**: Xintong Hao,Ke Shen,Chenggang Li
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: Dataset released url https://huggingface.co/datasets/bytedance-research/MAGACorpus
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Despite the remarkable capabilities of large language models across various tasks, their continued scaling faces a critical challenge: the scarcity of high-quality pretraining data. While model architectures continue to evolve, the natural language data struggles to scale up. To tackle this bottleneck, we propose \textbf{MA}ssive \textbf{G}enre-\textbf{A}udience~(MAGA) reformulation method, which systematic synthesizes diverse, contextually-rich pretraining data from existing corpus. This work makes three main contributions: (1) We propose MAGA reformulation method, a lightweight and scalable approach for pretraining corpus expansion, and build a 770B tokens MAGACorpus. (2) We evaluate MAGACorpus with different data budget scaling strategies, demonstrating consistent improvements across various model sizes (134M-13B), establishing the necessity for next-generation large-scale synthetic pretraining language models. (3) Through comprehensive analysis, we investigate prompt engineering's impact on synthetic training collapse and reveal limitations in conventional collapse detection metrics using validation losses. Our work shows that MAGA can substantially expand training datasets while maintaining quality, offering a reliably pathway for scaling models beyond data limitations.

### A Classification System Approach in Predicting Chinese Censorship 
[[arxiv](https://arxiv.org/abs/2502.04234)] [[cool](https://papers.cool/arxiv/2502.04234)] [[pdf](https://arxiv.org/pdf/2502.04234)]
> **Authors**: Matt Prodani,Tianchu Ze,Yushen Hu
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习,社交和信息网络
- **Abstract**: This paper is dedicated to using a classifier to predict whether a Weibo post would be censored under the Chinese internet. Through randomized sampling from \citeauthor{Fu2021} and Chinese tokenizing strategies, we constructed a cleaned Chinese phrase dataset with binary censorship markings. Utilizing various probability-based information retrieval methods on the data, we were able to derive 4 logistic regression models for classification. Furthermore, we experimented with pre-trained transformers to perform similar classification tasks. After evaluating both the macro-F1 and ROC-AUC metrics, we concluded that the Fined-Tuned BERT model exceeds other strategies in performance.

### Sports and Women's Sports: Gender Bias in Text Generation with Olympic Data 
[[arxiv](https://arxiv.org/abs/2502.04218)] [[cool](https://papers.cool/arxiv/2502.04218)] [[pdf](https://arxiv.org/pdf/2502.04218)]
> **Authors**: Laura Biester
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: NAACL 2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models (LLMs) have been shown to be biased in prior work, as they generate text that is in line with stereotypical views of the world or that is not representative of the viewpoints and values of historically marginalized demographic groups. In this work, we propose using data from parallel men's and women's events at the Olympic Games to investigate different forms of gender bias in language models. We define three metrics to measure bias, and find that models are consistently biased against women when the gender is ambiguous in the prompt. In this case, the model frequently retrieves only the results of the men's event with or without acknowledging them as such, revealing pervasive gender bias in LLMs in the context of athletics.

### The Best Instruction-Tuning Data are Those That Fit 
[[arxiv](https://arxiv.org/abs/2502.04194)] [[cool](https://papers.cool/arxiv/2502.04194)] [[pdf](https://arxiv.org/pdf/2502.04194)]
> **Authors**: Dylan Zhang,Qirun Dai,Hao Peng
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: High-quality supervised fine-tuning (SFT) data are crucial for eliciting strong capabilities from pretrained large language models (LLMs). Typically, instructions are paired with multiple responses sampled from other LLMs, which are often out of the distribution of the target model to be fine-tuned. This, at scale, can lead to diminishing returns and even hurt the models' performance and robustness. We propose **GRAPE**, a novel SFT framework that accounts for the unique characteristics of the target model. For each instruction, it gathers responses from various LLMs and selects the one with the highest probability measured by the target model, indicating that it aligns most closely with the target model's pretrained distribution; it then proceeds with standard SFT training. We first evaluate GRAPE with a controlled experiment, where we sample various solutions for each question in UltraInteract from multiple models and fine-tune commonly used LMs like LLaMA3.1-8B, Mistral-7B, and Qwen2.5-7B on GRAPE-selected data. GRAPE significantly outperforms strong baselines, including distilling from the strongest model with an absolute gain of up to 13.8%, averaged across benchmarks, and training on 3x more data with a maximum performance improvement of 17.3%. GRAPE's strong performance generalizes to realistic settings. We experiment with the post-training data used for Tulu3 and Olmo-2. GRAPE outperforms strong baselines trained on 4.5 times more data by 6.1% and a state-of-the-art data selection approach by 3% on average performance. Remarkably, using 1/3 of the data and half the number of epochs, GRAPE enables LLaMA3.1-8B to surpass the performance of Tulu3-SFT by 3.5%.

### Lexical Substitution is not Synonym Substitution: On the Importance of Producing Contextually Relevant Word Substitutes 
[[arxiv](https://arxiv.org/abs/2502.04173)] [[cool](https://papers.cool/arxiv/2502.04173)] [[pdf](https://arxiv.org/pdf/2502.04173)]
> **Authors**: Juraj Vladika,Stephen Meisenbacher,Florian Matthes
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: Accepted to ICAART 2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Lexical Substitution is the task of replacing a single word in a sentence with a similar one. This should ideally be one that is not necessarily only synonymous, but also fits well into the surrounding context of the target word, while preserving the sentence's grammatical structure. Recent advances in Lexical Substitution have leveraged the masked token prediction task of Pre-trained Language Models to generate replacements for a given word in a sentence. With this technique, we introduce ConCat, a simple augmented approach which utilizes the original sentence to bolster contextual information sent to the model. Compared to existing approaches, it proves to be very effective in guiding the model to make contextually relevant predictions for the target word. Our study includes a quantitative evaluation, measured via sentence similarity and task performance. In addition, we conduct a qualitative human analysis to validate that users prefer the substitutions proposed by our method, as opposed to previous methods. Finally, we test our approach on the prevailing benchmark for Lexical Substitution, CoInCo, revealing potential pitfalls of the benchmark. These insights serve as the foundation for a critical discussion on the way in which Lexical Substitution is evaluated.

### UltraIF: Advancing Instruction Following from the Wild 
[[arxiv](https://arxiv.org/abs/2502.04153)] [[cool](https://papers.cool/arxiv/2502.04153)] [[pdf](https://arxiv.org/pdf/2502.04153)]
> **Authors**: Kaikai An,Li Sheng,Ganqu Cui,Shuzheng Si,Ning Ding,Yu Cheng,Baobao Chang
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Instruction-following made modern large language models (LLMs) helpful assistants. However, the key to taming LLMs on complex instructions remains mysterious, for that there are huge gaps between models trained by open-source community and those trained by leading companies. To bridge the gap, we propose a simple and scalable approach UltraIF for building LLMs that can follow complex instructions with open-source data. UltraIF first decomposes real-world user prompts into simpler queries, constraints, and corresponding evaluation questions for the constraints. Then, we train an UltraComposer to compose constraint-associated prompts with evaluation questions. This prompt composer allows us to synthesize complicated instructions as well as filter responses with evaluation questions. In our experiment, for the first time, we successfully align LLaMA-3.1-8B-Base to catch up with its instruct version on 5 instruction-following benchmarks without any benchmark information, using only 8B model as response generator and evaluator. The aligned model also achieved competitive scores on other benchmarks. Moreover, we also show that UltraIF could further improve LLaMA-3.1-8B-Instruct through self-alignment, motivating broader use cases for the method. Our code will be available at https://github.com/kkk-an/UltraIF.

### The Order Effect: Investigating Prompt Sensitivity in Closed-Source LLMs 
[[arxiv](https://arxiv.org/abs/2502.04134)] [[cool](https://papers.cool/arxiv/2502.04134)] [[pdf](https://arxiv.org/pdf/2502.04134)]
> **Authors**: Bryan Guan,Tanya Roosta,Peyman Passban,Mehdi Rezagholizadeh
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: The first 3 authors have contributed equally
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: As large language models (LLMs) become integral to diverse applications, ensuring their reliability under varying input conditions is crucial. One key issue affecting this reliability is order sensitivity, wherein slight variations in input arrangement can lead to inconsistent or biased outputs. Although recent advances have reduced this sensitivity, the problem remains unresolved. This paper investigates the extent of order sensitivity in closed-source LLMs by conducting experiments across multiple tasks, including paraphrasing, relevance judgment, and multiple-choice questions. Our results show that input order significantly affects performance across tasks, with shuffled inputs leading to measurable declines in output accuracy. Few-shot prompting demonstrates mixed effectiveness and offers partial mitigation, however, fails to fully resolve the problem. These findings highlight persistent risks, particularly in high-stakes applications, and point to the need for more robust LLMs or improved input-handling techniques in future development.

### LLMs to Support a Domain Specific Knowledge Assistant 
[[arxiv](https://arxiv.org/abs/2502.04095)] [[cool](https://papers.cool/arxiv/2502.04095)] [[pdf](https://arxiv.org/pdf/2502.04095)]
> **Authors**: Maria-Flavia Lovin
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: This work presents a custom approach to developing a domain specific knowledge assistant for sustainability reporting using the International Financial Reporting Standards (IFRS). In this domain, there is no publicly available question-answer dataset, which has impeded the development of a high-quality chatbot to support companies with IFRS reporting. The two key contributions of this project therefore are: (1) A high-quality synthetic question-answer (QA) dataset based on IFRS sustainability standards, created using a novel generation and evaluation pipeline leveraging Large Language Models (LLMs). This comprises 1,063 diverse QA pairs that address a wide spectrum of potential user queries in sustainability reporting. Various LLM-based techniques are employed to create the dataset, including chain-of-thought reasoning and few-shot prompting. A custom evaluation framework is developed to assess question and answer quality across multiple dimensions, including faithfulness, relevance, and domain specificity. The dataset averages a score range of 8.16 out of 10 on these metrics. (2) Two architectures for question-answering in the sustainability reporting domain - a RAG pipeline and a fully LLM-based pipeline. The architectures are developed by experimenting, fine-tuning, and training on the QA dataset. The final pipelines feature an LLM fine-tuned on domain specific data and an industry classification component to improve the handling of complex queries. The RAG architecture achieves an accuracy of 85.32% on single-industry and 72.15% on cross-industry multiple-choice questions, outperforming the baseline approach by 4.67 and 19.21 percentage points, respectively. The LLM-based pipeline achieves an accuracy of 93.45% on single-industry and 80.30% on cross-industry multiple-choice questions, an improvement of 12.80 and 27.36 percentage points over the baseline, respectively.

### AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference 
[[arxiv](https://arxiv.org/abs/2502.04077)] [[cool](https://papers.cool/arxiv/2502.04077)] [[pdf](https://arxiv.org/pdf/2502.04077)]
> **Authors**: Qingyue Yang,Jie Wang,Xing Li,Zhihai Wang,Chen Chen,Lei Chen,Xianzhi Yu,Wulong Liu,Jianye Hao,Mingxuan Yuan,Bin Li
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: With the development of large language models (LLMs), efficient inference through Key-Value (KV) cache compression has attracted considerable attention, especially for long-context generation. To compress the KV cache, recent methods identify critical KV tokens through heuristic ranking with attention scores. However, these methods often struggle to accurately determine critical tokens as they neglect the \textit{temporal patterns} in attention scores, resulting in a noticeable degradation in LLM performance. To address this challenge, we propose AttentionPredictor, which is the first learning-based critical token identification approach. Specifically, AttentionPredictor learns a lightweight convolution model to capture spatiotemporal patterns and predict the next-token attention score. An appealing feature of AttentionPredictor is that it accurately predicts the attention score while consuming negligible memory. Moreover, we propose a cross-token critical cache prefetching framework that hides the token estimation time overhead to accelerate the decoding stage. By retaining most of the attention information, AttentionPredictor achieves 16$\times$ KV cache compression with comparable LLM performance, significantly outperforming the state-of-the-art.

### Controllable Emotion Generation with Emotion Vectors 
[[arxiv](https://arxiv.org/abs/2502.04075)] [[cool](https://papers.cool/arxiv/2502.04075)] [[pdf](https://arxiv.org/pdf/2502.04075)]
> **Authors**: Yurui Dong,Luozhijie Jin,Yao Yang,Bingjie Lu,Jiaxi Yang,Zhi Liu
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 15 pages, 5 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: In recent years, technologies based on large-scale language models (LLMs) have made remarkable progress in many fields, especially in customer service, content creation, and embodied intelligence, showing broad application potential. However, The LLM's ability to express emotions with proper tone, timing, and in both direct and indirect forms is still insufficient but significant. Few works have studied on how to build the controlable emotional expression capability of LLMs. In this work, we propose a method for emotion expression output by LLMs, which is universal, highly flexible, and well controllable proved with the extensive experiments and verifications. This method has broad application prospects in fields involving emotions output by LLMs, such as intelligent customer service, literary creation, and home companion robots. The extensive experiments on various LLMs with different model-scales and architectures prove the versatility and the effectiveness of the proposed method.

### Predicting Large Language Model Capabilities on Closed-Book QA Tasks Using Only Information Available Prior to Training 
[[arxiv](https://arxiv.org/abs/2502.04066)] [[cool](https://papers.cool/arxiv/2502.04066)] [[pdf](https://arxiv.org/pdf/2502.04066)]
> **Authors**: Changhao Jiang,Ming Zhang,Junjie Ye,Xiaoran Fan,Yifei Cao,Jiajun Sun,Zhiheng Xi,Shihan Dou,Yi Dong,Yujiong Shen,Jingqi Tong,Zhen Wang,Tao Liang,Zhihui Fei,Mingyang Wan,Guojun Ma,Qi Zhang,Tao Gui,Xuanjing Huang
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: The GPT-4 technical report from OpenAI suggests that model performance on specific tasks can be predicted prior to training, though methodologies remain unspecified. This approach is crucial for optimizing resource allocation and ensuring data alignment with target tasks. To achieve this vision, we focus on predicting performance on Closed-book Question Answering (CBQA) tasks, which are closely tied to pre-training data and knowledge retention. We address three major challenges: 1) mastering the entire pre-training process, especially data construction; 2) evaluating a model's knowledge retention; and 3) predicting task-specific knowledge retention using only information available prior to training. To tackle these challenges, we pre-train three large language models (i.e., 1.6B, 7B, and 13B) using 560k dollars and 520k GPU hours. We analyze the pre-training data with knowledge triples and assess knowledge retention using established methods. Additionally, we introduce the SMI metric, an information-theoretic measure that quantifies the relationship between pre-training data, model size, and task-specific knowledge retention. Our experiments reveal a strong linear correlation ($\text{R}^2 > 0.84$) between the SMI metric and the model's accuracy on CBQA tasks across models of varying sizes (i.e., 1.1B, 1.6B, 7B, and 13B). The dataset, model, and code are available at https://github.com/yuhui1038/SMI.

### Simulating the Emergence of Differential Case Marking with Communicating Neural-Network Agents 
[[arxiv](https://arxiv.org/abs/2502.04038)] [[cool](https://papers.cool/arxiv/2502.04038)] [[pdf](https://arxiv.org/pdf/2502.04038)]
> **Authors**: Yuchen Lian,Arianna Bisazza,Tessa Verhoef
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Differential Case Marking (DCM) refers to the phenomenon where grammatical case marking is applied selectively based on semantic, pragmatic, or other factors. The emergence of DCM has been studied in artificial language learning experiments with human participants, which were specifically aimed at disentangling the effects of learning from those of communication (Smith & Culbertson, 2020). Multi-agent reinforcement learning frameworks based on neural networks have gained significant interest to simulate the emergence of human-like linguistic phenomena. In this study, we employ such a framework in which agents first acquire an artificial language before engaging in communicative interactions, enabling direct comparisons to human result. Using a very generic communication optimization algorithm and neural-network learners that have no prior experience with language or semantic preferences, our results demonstrate that learning alone does not lead to DCM, but when agents communicate, differential use of markers arises. This supports Smith and Culbertson (2020)'s findings that highlight the critical role of communication in shaping DCM and showcases the potential of neural-agent models to complement experimental research on language evolution.

### Exploring Imbalanced Annotations for Effective In-Context Learning 
[[arxiv](https://arxiv.org/abs/2502.04037)] [[cool](https://papers.cool/arxiv/2502.04037)] [[pdf](https://arxiv.org/pdf/2502.04037)]
> **Authors**: Hongfu Gao,Feipeng Zhang,Hao Zeng,Deyu Meng,Bingyi Jing,Hongxin Wei
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Large language models (LLMs) have shown impressive performance on downstream tasks through in-context learning (ICL), which heavily relies on the demonstrations selected from annotated datasets. Existing selection methods may hinge on the distribution of annotated datasets, which can often be long-tailed in real-world scenarios. In this work, we show that imbalanced class distributions in annotated datasets significantly degrade the performance of ICL across various tasks and selection methods. Moreover, traditional rebalance methods fail to ameliorate the issue of class imbalance in ICL. Our method is motivated by decomposing the distributional differences between annotated and test datasets into two-component weights: class-wise weights and conditional bias. The key idea behind our method is to estimate the conditional bias by minimizing the empirical error on a balanced validation dataset and to employ the two-component weights to modify the original scoring functions during selection. Our approach can prevent selecting too many demonstrations from a single class while preserving the effectiveness of the original selection methods. Extensive experiments demonstrate the effectiveness of our method, improving the average accuracy by up to 5.46 on common benchmarks with imbalanced datasets.

### Quantification of Biodiversity from Historical Survey Text with LLM-based Best-Worst Scaling 
[[arxiv](https://arxiv.org/abs/2502.04022)] [[cool](https://papers.cool/arxiv/2502.04022)] [[pdf](https://arxiv.org/pdf/2502.04022)]
> **Authors**: Thomas Haider,Tobias Perschl,Malte Rehbein
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: NoDaLiDa 2025, EcoNLP Workshop
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: In this study, we evaluate methods to determine the frequency of species via quantity estimation from historical survey text. To that end, we formulate classification tasks and finally show that this problem can be adequately framed as a regression task using Best-Worst Scaling (BWS) with Large Language Models (LLMs). We test Ministral-8B, DeepSeek-V3, and GPT-4, finding that the latter two have reasonable agreement with humans and each other. We conclude that this approach is more cost-effective and similarly robust compared to a fine-grained multi-class approach, allowing automated quantity estimation across species.

### Ontology-Guided, Hybrid Prompt Learning for Generalization in Knowledge Graph Question Answering 
[[arxiv](https://arxiv.org/abs/2502.03992)] [[cool](https://papers.cool/arxiv/2502.03992)] [[pdf](https://arxiv.org/pdf/2502.03992)]
> **Authors**: Longquan Jiang,Junbo Huang,Cedric Möller,Ricardo Usbeck
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: Accepted By ICSC 2025
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Most existing Knowledge Graph Question Answering (KGQA) approaches are designed for a specific KG, such as Wikidata, DBpedia or Freebase. Due to the heterogeneity of the underlying graph schema, topology and assertions, most KGQA systems cannot be transferred to unseen Knowledge Graphs (KGs) without resource-intensive training data. We present OntoSCPrompt, a novel Large Language Model (LLM)-based KGQA approach with a two-stage architecture that separates semantic parsing from KG-dependent interactions. OntoSCPrompt first generates a SPARQL query structure (including SPARQL keywords such as SELECT, ASK, WHERE and placeholders for missing tokens) and then fills them with KG-specific information. To enhance the understanding of the underlying KG, we present an ontology-guided, hybrid prompt learning strategy that integrates KG ontology into the learning process of hybrid prompts (e.g., discrete and continuous vectors). We also present several task-specific decoding strategies to ensure the correctness and executability of generated SPARQL queries in both stages. Experimental results demonstrate that OntoSCPrompt performs as well as SOTA approaches without retraining on a number of KGQA datasets such as CWQ, WebQSP and LC-QuAD 1.0 in a resource-efficient manner and can generalize well to unseen domain-specific KGs like DBLP-QuAD and CoyPu KG Code: \href{https://github.com/LongquanJiang/OntoSCPrompt}{https://github.com/LongquanJiang/OntoSCPrompt}

### PGB: One-Shot Pruning for BERT via Weight Grouping and Permutation 
[[arxiv](https://arxiv.org/abs/2502.03984)] [[cool](https://papers.cool/arxiv/2502.03984)] [[pdf](https://arxiv.org/pdf/2502.03984)]
> **Authors**: Hyemin Lim,Jaeyeon Lee,Dong-Wan Choi
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large pretrained language models such as BERT suffer from slow inference and high memory usage, due to their huge size. Recent approaches to compressing BERT rely on iterative pruning and knowledge distillation, which, however, are often too complicated and computationally intensive. This paper proposes a novel semi-structured one-shot pruning method for BERT, called $\textit{Permutation and Grouping for BERT}$ (PGB), which achieves high compression efficiency and sparsity while preserving accuracy. To this end, PGB identifies important groups of individual weights by permutation and prunes all other weights as a structure in both multi-head attention and feed-forward layers. Furthermore, if no important group is formed in a particular layer, PGB drops the entire layer to produce an even more compact model. Our experimental results on BERT$_{\text{BASE}}$ demonstrate that PGB outperforms the state-of-the-art structured pruning methods in terms of computational cost and accuracy preservation.

### MAQInstruct: Instruction-based Unified Event Relation Extraction 
[[arxiv](https://arxiv.org/abs/2502.03954)] [[cool](https://papers.cool/arxiv/2502.03954)] [[pdf](https://arxiv.org/pdf/2502.03954)]
> **Authors**: Jun Xu,Mengshu Sun,Zhiqiang Zhang,Jun Zhou
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: Accepted by WWW 2025 short
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Extracting event relations that deviate from known schemas has proven challenging for previous methods based on multi-class classification, MASK prediction, or prototype matching. Recent advancements in large language models have shown impressive performance through instruction tuning. Nevertheless, in the task of event relation extraction, instruction-based methods face several challenges: there are a vast number of inference samples, and the relations between events are non-sequential. To tackle these challenges, we present an improved instruction-based event relation extraction framework named MAQInstruct. Firstly, we transform the task from extracting event relations using given event-event instructions to selecting events using given event-relation instructions, which reduces the number of samples required for inference. Then, by incorporating a bipartite matching loss, we reduce the dependency of the instruction-based method on the generation sequence. Our experimental results demonstrate that MAQInstruct significantly improves the performance of event relation extraction across multiple LLMs.

### Afrispeech-Dialog: A Benchmark Dataset for Spontaneous English Conversations in Healthcare and Beyond 
[[arxiv](https://arxiv.org/abs/2502.03945)] [[cool](https://papers.cool/arxiv/2502.03945)] [[pdf](https://arxiv.org/pdf/2502.03945)]
> **Authors**: Mardhiyah Sanni,Tassallah Abdullahi,Devendra D. Kayande,Emmanuel Ayodele,Naome A. Etori,Michael S. Mollel,Moshood Yekini,Chibuzor Okocha,Lukman E. Ismaila,Folafunmi Omofoye,Boluwatife A. Adewale,Tobi Olatunji
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 19 pages, 5 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Speech technologies are transforming interactions across various sectors, from healthcare to call centers and robots, yet their performance on African-accented conversations remains underexplored. We introduce Afrispeech-Dialog, a benchmark dataset of 50 simulated medical and non-medical African-accented English conversations, designed to evaluate automatic speech recognition (ASR) and related technologies. We assess state-of-the-art (SOTA) speaker diarization and ASR systems on long-form, accented speech, comparing their performance with native accents and discover a 10%+ performance degradation. Additionally, we explore medical conversation summarization capabilities of large language models (LLMs) to demonstrate the impact of ASR errors on downstream medical summaries, providing insights into the challenges and opportunities for speech technologies in the Global South. Our work highlights the need for more inclusive datasets to advance conversational AI in low-resource settings.

### Experiments with Large Language Models on Retrieval-Augmented Generation for Closed-Source Simulation Software 
[[arxiv](https://arxiv.org/abs/2502.03916)] [[cool](https://papers.cool/arxiv/2502.03916)] [[pdf](https://arxiv.org/pdf/2502.03916)]
> **Authors**: Andreas Baumann,Peter Eberhard
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 11 pages, 6 tables
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large Language Models (LLMs) are increasingly helpful in text generation, even writing code in programming languages based on user prompts written in natural language. They are even applied to generate simulation models for multibody systems from natural language. Research results suggest that LLMs surpass the mere replication of existing code examples, where some LLMs have been trained on an open-source multibody simulation code. However, for closed-source simulation software, such results are not to be expected as their ideas and concepts might differ from other publicly available ones. LLMs can hallucinate for knowledge-intensive tasks, such as model creation, which can lead to wrong responses. This is especially the case for the LLM unknown closed-source simulation software. The same applies to other internal knowledge kept private to protect intellectual property or data privacy. The Retrieval-Augmented Generation (RAG) approach might yield a solution for these knowledge-intensive tasks. This paper explores the application of RAG to closed-source simulation software and presents first experiments. After a brief introduction to LLMs, the RAG approach, and the simulation method applied by the close-source simulation software, several examples are provided to test LLMs' knowledge of the simulation software and the creation of simulation models using two RAG systems. The examples show promising results indicating the benefits of applying RAG systems to closed-source simulation software, helping to access their knowledge. Nevertheless, they also reveal gaps in the applied information and open questions for further research.

### BOLT: Bootstrap Long Chain-of-Thought in Language Models without Distillation 
[[arxiv](https://arxiv.org/abs/2502.03860)] [[cool](https://papers.cool/arxiv/2502.03860)] [[pdf](https://arxiv.org/pdf/2502.03860)]
> **Authors**: Bo Pang,Hanze Dong,Jiacheng Xu,Silvio Savarese,Yingbo Zhou,Caiming Xiong
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 36 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs), such as o1 from OpenAI, have demonstrated remarkable reasoning capabilities. o1 generates a long chain-of-thought (LongCoT) before answering a question. LongCoT allows LLMs to analyze problems, devise plans, reflect, and backtrack effectively. These actions empower LLM to solve complex problems. After the release of o1, many teams have attempted to replicate its LongCoT and reasoning capabilities. In terms of methods, they primarily rely on knowledge distillation with data from existing models with LongCoT capacities (e.g., OpenAI-o1, Qwen-QwQ, DeepSeek-R1-Preview), leaving significant uncertainties on systematically developing such reasoning abilities. In terms of data domains, these works focus narrowly on math while a few others include coding, limiting their generalizability. This paper introduces a novel approach to enable LLM's LongCoT capacity without distillation from o1-like models or expensive human annotations, where we bootstrap LongCoT (BOLT) from a standard instruct model. BOLT involves three stages: 1) LongCoT data bootstrapping with in-context learning on a standard instruct model; 2) LongCoT supervised finetuning; 3) online training to further refine LongCoT capacities. In BOLT, only a few in-context examples need to be constructed during the bootstrapping stage; in our experiments, we created 10 examples, demonstrating the feasibility of this approach. We use Llama-3.1-70B-Instruct to bootstrap LongCoT and apply our method to various model scales (7B, 8B, 70B). We achieve impressive performance on a variety of benchmarks, Arena-Hard, MT-Bench, WildBench, ZebraLogic, MATH500, which evaluate diverse task-solving and reasoning capabilities.

### Improving Natural Language Understanding for LLMs via Large-Scale Instruction Synthesis 
[[arxiv](https://arxiv.org/abs/2502.03843)] [[cool](https://papers.cool/arxiv/2502.03843)] [[pdf](https://arxiv.org/pdf/2502.03843)]
> **Authors**: Lin Yuan,Jun Xu,Honghao Gui,Mengshu Sun,Zhiqiang Zhang,Lei Liang,Jun Zhou
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: Accepted by AAAI 2025
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: High-quality, large-scale instructions are crucial for aligning large language models (LLMs), however, there is a severe shortage of instruction in the field of natural language understanding (NLU). Previous works on constructing NLU instructions mainly focus on information extraction (IE), neglecting tasks such as machine reading comprehension, question answering, and text classification. Furthermore, the lack of diversity in the data has led to a decreased generalization ability of trained LLMs in other NLU tasks and a noticeable decline in the fundamental model's general capabilities. To address this issue, we propose Hum, a large-scale, high-quality synthetic instruction corpus for NLU tasks, designed to enhance the NLU capabilities of LLMs. Specifically, Hum includes IE (either close IE or open IE), machine reading comprehension, text classification, and instruction generalist tasks, thereby enriching task diversity. Additionally, we introduce a human-LLMs collaborative mechanism to synthesize instructions, which enriches instruction diversity by incorporating guidelines, preference rules, and format variants. We conduct extensive experiments on 5 NLU tasks and 28 general capability evaluation datasets for LLMs. Experimental results show that Hum enhances the NLU capabilities of six LLMs by an average of 3.1\%, with no significant decline observed in other general capabilities.

### A comprehensive survey of contemporary Arabic sentiment analysis: Methods, Challenges, and Future Directions 
[[arxiv](https://arxiv.org/abs/2502.03827)] [[cool](https://papers.cool/arxiv/2502.03827)] [[pdf](https://arxiv.org/pdf/2502.03827)]
> **Authors**: Zhiqiang Shi,Ruchit Agrawal
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: Paper accepted to NAACL 2025
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Sentiment Analysis, a popular subtask of Natural Language Processing, employs computational methods to extract sentiment, opinions, and other subjective aspects from linguistic data. Given its crucial role in understanding human sentiment, research in sentiment analysis has witnessed significant growth in the recent years. However, the majority of approaches are aimed at the English language, and research towards Arabic sentiment analysis remains relatively unexplored. This paper presents a comprehensive and contemporary survey of Arabic Sentiment Analysis, identifies the challenges and limitations of existing literature in this field and presents avenues for future research. We present a systematic review of Arabic sentiment analysis methods, focusing specifically on research utilizing deep learning. We then situate Arabic Sentiment Analysis within the broader context, highlighting research gaps in Arabic sentiment analysis as compared to general sentiment analysis. Finally, we outline the main challenges and promising future directions for research in Arabic sentiment analysis.

### Syntriever: How to Train Your Retriever with Synthetic Data from LLMs 
[[arxiv](https://arxiv.org/abs/2502.03824)] [[cool](https://papers.cool/arxiv/2502.03824)] [[pdf](https://arxiv.org/pdf/2502.03824)]
> **Authors**: Minsang Kim,Seungjun Baek
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: the Nations of the Americas Chapter of the Association for Computational Linguistics (NAACL), Findings, Accepted
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: LLMs have boosted progress in many AI applications. Recently, there were attempts to distill the vast knowledge of LLMs into information retrieval systems. Those distillation methods mostly use output probabilities of LLMs which are unavailable in the latest black-box LLMs. We propose Syntriever, a training framework for retrievers using synthetic data from black-box LLMs. Syntriever consists of two stages. Firstly in the distillation stage, we synthesize relevant and plausibly irrelevant passages and augmented queries using chain-of-thoughts for the given queries. LLM is asked to self-verify the synthetic data for possible hallucinations, after which retrievers are trained with a loss designed to cluster the embeddings of relevant passages. Secondly in the alignment stage, we align the retriever with the preferences of LLMs. We propose a preference modeling called partial Plackett-Luce ranking to learn LLM preferences with regularization which prevents the model from deviating excessively from that trained in the distillation stage. Experiments show that Syntriever achieves state-of-the-art performances on benchmark datasets from various domains in nDCG@$K$. The code is available at \href{https://github.com/kmswin1/Syntriever}{https://github.com/kmswin1/Syntriever}.

### PsyPlay: Personality-Infused Role-Playing Conversational Agents 
[[arxiv](https://arxiv.org/abs/2502.03821)] [[cool](https://papers.cool/arxiv/2502.03821)] [[pdf](https://arxiv.org/pdf/2502.03821)]
> **Authors**: Tao Yang,Yuhua Zhu,Xiaojun Quan,Cong Liu,Qifan Wang
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The current research on Role-Playing Conversational Agents (RPCAs) with Large Language Models (LLMs) primarily focuses on imitating specific speaking styles and utilizing character backgrounds, neglecting the depiction of deeper personality traits.~In this study, we introduce personality-infused role-playing for LLM agents, which encourages agents to accurately portray their designated personality traits during dialogues. We then propose PsyPlay, a dialogue generation framework that facilitates the expression of rich personalities among multiple LLM agents. Specifically, PsyPlay enables agents to assume roles with distinct personality traits and engage in discussions centered around specific topics, consistently exhibiting their designated personality traits throughout the interactions. Validation on generated dialogue data demonstrates that PsyPlay can accurately portray the intended personality traits, achieving an overall success rate of 80.31% on GPT-3.5. Notably, we observe that LLMs aligned with positive values are more successful in portraying positive personality roles compared to negative ones. Moreover, we construct a dialogue corpus for personality-infused role-playing, called PsyPlay-Bench. The corpus, which consists of 4745 instances of correctly portrayed dialogues using PsyPlay, aims to further facilitate research in personalized role-playing and dialogue personality detection.

### Identify Critical KV Cache in LLM Inference from an Output Perturbation Perspective 
[[arxiv](https://arxiv.org/abs/2502.03805)] [[cool](https://papers.cool/arxiv/2502.03805)] [[pdf](https://arxiv.org/pdf/2502.03805)]
> **Authors**: Yuan Feng,Junlin Lv,Yukun Cao,Xike Xie,S Kevin Zhou
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models have revolutionized natural language processing but face significant challenges of high storage and runtime costs, due to the transformer architecture's reliance on self-attention, particularly the large Key-Value (KV) cache for long-sequence inference. Recent efforts to reduce KV cache size by pruning less critical entries based on attention weights remain empirical and lack formal grounding. This paper presents a formal study on identifying critical KV cache entries by analyzing attention output perturbation. Our analysis reveals that, beyond attention weights, the value states within KV entries and pretrained parameter matrices are also crucial. Based on this, we propose a perturbation-constrained selection algorithm that optimizes the worst-case output perturbation to identify critical entries. Evaluations on the Needle-in-a-Haystack test and Longbench benchmark show our algorithm enhances state-of-the-art cache eviction methods. Further empirical analysis confirms that our algorithm achieves lower output perturbations in over 92% attention heads in Llama model, thereby providing a significant improvement over existing methods.

### Enhancing Hallucination Detection through Noise Injection 
[[arxiv](https://arxiv.org/abs/2502.03799)] [[cool](https://papers.cool/arxiv/2502.03799)] [[pdf](https://arxiv.org/pdf/2502.03799)]
> **Authors**: Litian Liu,Reza Pourreza,Sunny Panchal,Apratim Bhattacharyya,Yao Qin,Roland Memisevic
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,系统与控制
- **Abstract**: Large Language Models (LLMs) are prone to generating plausible yet incorrect responses, known as hallucinations. Effectively detecting hallucinations is therefore crucial for the safe deployment of LLMs. Recent research has linked hallucinations to model uncertainty, suggesting that hallucinations can be detected by measuring dispersion over answer distributions obtained from a set of samples drawn from a model. While drawing from the distribution over tokens defined by the model is a natural way to obtain samples, in this work, we argue that it is sub-optimal for the purpose of detecting hallucinations. We show that detection can be improved significantly by taking into account model uncertainty in the Bayesian sense. To this end, we propose a very simple and efficient approach that perturbs an appropriate subset of model parameters, or equivalently hidden unit activations, during sampling. We demonstrate its effectiveness across a wide range of datasets and model architectures.

### It's All in The [MASK]: Simple Instruction-Tuning Enables BERT-like Masked Language Models As Generative Classifiers 
[[arxiv](https://arxiv.org/abs/2502.03793)] [[cool](https://papers.cool/arxiv/2502.03793)] [[pdf](https://arxiv.org/pdf/2502.03793)]
> **Authors**: Benjamin Clavié,Nathan Cooper,Benjamin Warner
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: While encoder-only models such as BERT and ModernBERT are ubiquitous in real-world NLP applications, their conventional reliance on task-specific classification heads can limit their applicability compared to decoder-based large language models (LLMs). In this work, we introduce ModernBERT-Large-Instruct, a 0.4B-parameter encoder model that leverages its masked language modelling (MLM) head for generative classification. Our approach employs an intentionally simple training loop and inference mechanism that requires no heavy pre-processing, heavily engineered prompting, or architectural modifications. ModernBERT-Large-Instruct exhibits strong zero-shot performance on both classification and knowledge-based tasks, outperforming similarly sized LLMs on MMLU and achieving 93% of Llama3-1B's MMLU performance with 60% less parameters. We also demonstrate that, when fine-tuned, the generative approach using the MLM head matches or even surpasses traditional classification-head methods across diverse NLU tasks.This capability emerges specifically in models trained on contemporary, diverse data mixes, with models trained on lower volume, less-diverse data yielding considerably weaker performance. Although preliminary, these results demonstrate the potential of using the original generative masked language modelling head over traditional task-specific heads for downstream tasks. Our work suggests that further exploration into this area is warranted, highlighting many avenues for future improvements.

## 密码学和安全(cs.CR:Cryptography and Security)

### LATTEO: A Framework to Support Learning Asynchronously Tempered with Trusted Execution and Obfuscation 
[[arxiv](https://arxiv.org/abs/2502.04601)] [[cool](https://papers.cool/arxiv/2502.04601)] [[pdf](https://arxiv.org/pdf/2502.04601)]
> **Authors**: Abhinav Kumar,George Torres,Noah Guzinski,Gaurav Panwar,Reza Tourani,Satyajayant Misra,Marcin Spoczynski,Mona Vij,Nageen Himayat
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,机器学习
- **Abstract**: The privacy vulnerabilities of the federated learning (FL) paradigm, primarily caused by gradient leakage, have prompted the development of various defensive measures. Nonetheless, these solutions have predominantly been crafted for and assessed in the context of synchronous FL systems, with minimal focus on asynchronous FL. This gap arises in part due to the unique challenges posed by the asynchronous setting, such as the lack of coordinated updates, increased variability in client participation, and the potential for more severe privacy risks. These concerns have stymied the adoption of asynchronous FL. In this work, we first demonstrate the privacy vulnerabilities of asynchronous FL through a novel data reconstruction attack that exploits gradient updates to recover sensitive client data. To address these vulnerabilities, we propose a privacy-preserving framework that combines a gradient obfuscation mechanism with Trusted Execution Environments (TEEs) for secure asynchronous FL aggregation at the network edge. To overcome the limitations of conventional enclave attestation, we introduce a novel data-centric attestation mechanism based on Multi-Authority Attribute-Based Encryption. This mechanism enables clients to implicitly verify TEE-based aggregation services, effectively handle on-demand client participation, and scale seamlessly with an increasing number of asynchronous connections. Our gradient obfuscation mechanism reduces the structural similarity index of data reconstruction by 85% and increases reconstruction error by 400%, while our framework improves attestation efficiency by lowering average latency by up to 1500% compared to RA-TLS, without additional overhead.

### Assessing and Prioritizing Ransomware Risk Based on Historical Victim Data 
[[arxiv](https://arxiv.org/abs/2502.04421)] [[cool](https://papers.cool/arxiv/2502.04421)] [[pdf](https://arxiv.org/pdf/2502.04421)]
> **Authors**: Spencer Massengale,Philip Huff
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能,机器学习
- **Abstract**: We present an approach to identifying which ransomware adversaries are most likely to target specific entities, thereby assisting these entities in formulating better protection strategies. Ransomware poses a formidable cybersecurity threat characterized by profit-driven motives, a complex underlying economy supporting criminal syndicates, and the overt nature of its attacks. This type of malware has consistently ranked among the most prevalent, with a rapid escalation in activity observed. Recent estimates indicate that approximately two-thirds of organizations experienced ransomware attacks in 2023 \cite{Sophos2023Ransomware}. A central tactic in ransomware campaigns is publicizing attacks to coerce victims into paying ransoms. Our study utilizes public disclosures from ransomware victims to predict the likelihood of an entity being targeted by a specific ransomware variant. We employ a Large Language Model (LLM) architecture that uses a unique chain-of-thought, multi-shot prompt methodology to define adversary SKRAM (Skills, Knowledge, Resources, Authorities, and Motivation) profiles from ransomware bulletins, threat reports, and news items. This analysis is enriched with publicly available victim data and is further enhanced by a heuristic for generating synthetic data that reflects victim profiles. Our work culminates in the development of a machine learning model that assists organizations in prioritizing ransomware threats and formulating defenses based on the tactics, techniques, and procedures (TTP) of the most likely attackers.

### SoK: Benchmarking Poisoning Attacks and Defenses in Federated Learning 
[[arxiv](https://arxiv.org/abs/2502.03801)] [[cool](https://papers.cool/arxiv/2502.03801)] [[pdf](https://arxiv.org/pdf/2502.03801)]
> **Authors**: Heyi Zhang,Yule Liu,Xinlei He,Jun Wu,Tianshuo Cong,Xinyi Huang
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能,机器学习
- **Abstract**: Federated learning (FL) enables collaborative model training while preserving data privacy, but its decentralized nature exposes it to client-side data poisoning attacks (DPAs) and model poisoning attacks (MPAs) that degrade global model performance. While numerous proposed defenses claim substantial effectiveness, their evaluation is typically done in isolation with limited attack strategies, raising concerns about their validity. Additionally, existing studies overlook the mutual effectiveness of defenses against both DPAs and MPAs, causing fragmentation in this field. This paper aims to provide a unified benchmark and analysis of defenses against DPAs and MPAs, clarifying the distinction between these two similar but slightly distinct domains. We present a systematic taxonomy of poisoning attacks and defense strategies, outlining their design, strengths, and limitations. Then, a unified comparative evaluation across FL algorithms and data heterogeneity is conducted to validate their individual and mutual effectiveness and derive key insights for design principles and future research. Along with the analysis, we frame our work to a unified benchmark, FLPoison, with high modularity and scalability to evaluate 15 representative poisoning attacks and 17 defense strategies, facilitating future research in this domain. Code is available at https://github.com/vio1etus/FLPoison.

## 计算机视觉和模式识别(cs.CV:Computer Vision and Pattern Recognition)

### AIQViT: Architecture-Informed Post-Training Quantization for Vision Transformers 
[[arxiv](https://arxiv.org/abs/2502.04628)] [[cool](https://papers.cool/arxiv/2502.04628)] [[pdf](https://arxiv.org/pdf/2502.04628)]
> **Authors**: Runqing Jiang,Ye Zhang,Longguang Wang,Pengpeng Yu,Yulan Guo
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Post-training quantization (PTQ) has emerged as a promising solution for reducing the storage and computational cost of vision transformers (ViTs). Recent advances primarily target at crafting quantizers to deal with peculiar activations characterized by ViTs. However, most existing methods underestimate the information loss incurred by weight quantization, resulting in significant performance deterioration, particularly in low-bit cases. Furthermore, a common practice in quantizing post-Softmax activations of ViTs is to employ logarithmic transformations, which unfortunately prioritize less informative values around zero. This approach introduces additional redundancies, ultimately leading to suboptimal quantization efficacy. To handle these, this paper proposes an innovative PTQ method tailored for ViTs, termed AIQViT (Architecture-Informed Post-training Quantization for ViTs). First, we design an architecture-informed low rank compensation mechanism, wherein learnable low-rank weights are introduced to compensate for the degradation caused by weight quantization. Second, we design a dynamic focusing quantizer to accommodate the unbalanced distribution of post-Softmax activations, which dynamically selects the most valuable interval for higher quantization resolution. Extensive experiments on five vision tasks, including image classification, object detection, instance segmentation, point cloud classification, and point cloud part segmentation, demonstrate the superiority of AIQViT over state-of-the-art PTQ methods.

### HetSSNet: Spatial-Spectral Heterogeneous Graph Learning Network for Panchromatic and Multispectral Images Fusion 
[[arxiv](https://arxiv.org/abs/2502.04623)] [[cool](https://papers.cool/arxiv/2502.04623)] [[pdf](https://arxiv.org/pdf/2502.04623)]
> **Authors**: Mengting Ma,Yizhen Jiang,Mengjiao Zhao,Jiaxin Li,Wei Zhang
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Remote sensing pansharpening aims to reconstruct spatial-spectral properties during the fusion of panchromatic (PAN) images and low-resolution multi-spectral (LR-MS) images, finally generating the high-resolution multi-spectral (HR-MS) images. In the mainstream modeling strategies, i.e., CNN and Transformer, the input images are treated as the equal-sized grid of pixels in the Euclidean space. They have limitations in facing remote sensing images with irregular ground objects. Graph is the more flexible structure, however, there are two major challenges when modeling spatial-spectral properties with graph: \emph{1) constructing the customized graph structure for spatial-spectral relationship priors}; \emph{2) learning the unified spatial-spectral representation through the graph}. To address these challenges, we propose the spatial-spectral heterogeneous graph learning network, named \textbf{HetSSNet}. Specifically, HetSSNet initially constructs the heterogeneous graph structure for pansharpening, which explicitly describes pansharpening-specific relationships. Subsequently, the basic relationship pattern generation module is designed to extract the multiple relationship patterns from the heterogeneous graph. Finally, relationship pattern aggregation module is exploited to collaboratively learn unified spatial-spectral representation across different relationships among nodes with adaptive importance learning from local and global perspectives. Extensive experiments demonstrate the significant superiority and generalization of HetSSNet.

### Neural Clustering for Prefractured Mesh Generation in Real-time Object Destruction 
[[arxiv](https://arxiv.org/abs/2502.04615)] [[cool](https://papers.cool/arxiv/2502.04615)] [[pdf](https://arxiv.org/pdf/2502.04615)]
> **Authors**: Seunghwan Kim,Sunha Park,Seungkyu Lee
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,图形
- **Abstract**: Prefracture method is a practical implementation for real-time object destruction that is hardly achievable within performance constraints, but can produce unrealistic results due to its heuristic nature. To mitigate it, we approach the clustering of prefractured mesh generation as an unordered segmentation on point cloud data, and propose leveraging the deep neural network trained on a physics-based dataset. Our novel paradigm successfully predicts the structural weakness of object that have been limited, exhibiting ready-to-use results with remarkable quality.

### An Optimized YOLOv5 Based Approach For Real-time Vehicle Detection At Road Intersections Using Fisheye Cameras 
[[arxiv](https://arxiv.org/abs/2502.04566)] [[cool](https://papers.cool/arxiv/2502.04566)] [[pdf](https://arxiv.org/pdf/2502.04566)]
> **Authors**: Md. Jahin Alam,Muhammad Zubair Hasan,Md Maisoon Rahman,Md Awsafur Rahman,Najibul Haque Sarker,Shariar Azad,Tasnim Nishat Islam,Bishmoy Paul,Tanvir Anjum,Barproda Halder,Shaikh Anowarul Fattah
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Real time vehicle detection is a challenging task for urban traffic surveillance. Increase in urbanization leads to increase in accidents and traffic congestion in junction areas resulting in delayed travel time. In order to solve these problems, an intelligent system utilizing automatic detection and tracking system is significant. But this becomes a challenging task at road intersection areas which require a wide range of field view. For this reason, fish eye cameras are widely used in real time vehicle detection purpose to provide large area coverage and 360 degree view at junctions. However, it introduces challenges such as light glare from vehicles and street lights, shadow, non-linear distortion, scaling issues of vehicles and proper localization of small vehicles. To overcome each of these challenges, a modified YOLOv5 object detection scheme is proposed. YOLOv5 is a deep learning oriented convolutional neural network (CNN) based object detection method. The proposed scheme for detecting vehicles in fish-eye images consists of a light-weight day-night CNN classifier so that two different solutions can be implemented to address the day-night detection issues. Furthurmore, challenging instances are upsampled in the dataset for proper localization of vehicles and later on the detection model is ensembled and trained in different combination of vehicle datasets for better generalization, detection and accuracy. For testing, a real world fisheye dataset provided by the Video and Image Processing (VIP) Cup organizer ISSD has been used which includes images from video clips of different fisheye cameras at junction of different cities during day and night time. Experimental results show that our proposed model has outperformed the YOLOv5 model on the dataset by 13.7% mAP @ 0.5.

### The Phantom of the Elytra -- Phylogenetic Trait Extraction from Images of Rove Beetles Using Deep Learning -- Is the Mask Enough? 
[[arxiv](https://arxiv.org/abs/2502.04541)] [[cool](https://papers.cool/arxiv/2502.04541)] [[pdf](https://arxiv.org/pdf/2502.04541)]
> **Authors**: Roberta Hunt,Kim Steenstrup Pedersen
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: Accepted at Imageomics Workshop at AAAI 2025 (not published in proceedings)
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Phylogenetic analysis traditionally relies on labor-intensive manual extraction of morphological traits, limiting its scalability for large datasets. Recent advances in deep learning offer the potential to automate this process, but the effectiveness of different morphological representations for phylogenetic trait extraction remains poorly understood. In this study, we compare the performance of deep learning models using three distinct morphological representations - full segmentations, binary masks, and Fourier descriptors of beetle outlines. We test this on the Rove-Tree-11 dataset, a curated collection of images from 215 rove beetle species. Our results demonstrate that the mask-based model outperformed the others, achieving a normalized Align Score of 0.33 plus/minus 0.02 on the test set, compared to 0.45 plus/minus 0.01 for the Fourier-based model and 0.39 plus/minus 0.07 for the segmentation-based model. The performance of the mask-based model likely reflects its ability to capture shape features while taking advantage of the depth and capacity of the ResNet50 architecture. These results also indicate that dorsal textural features, at least in this group of beetles, may be of lowered phylogenetic relevance, though further investigation is necessary to confirm this. In contrast, the Fourier-based model suffered from reduced capacity and occasional inaccuracies in outline approximations, particularly in fine structures like legs. These findings highlight the importance of selecting appropriate morphological representations for automated phylogenetic studies and the need for further research into explainability in automatic morphological trait extraction.

### Fast Video Generation with Sliding Tile Attention 
[[arxiv](https://arxiv.org/abs/2502.04507)] [[cool](https://papers.cool/arxiv/2502.04507)] [[pdf](https://arxiv.org/pdf/2502.04507)]
> **Authors**: Peiyuan Zhang,Yongqi Chen,Runlong Su,Hangliang Ding,Ion Stoica,Zhenghong Liu,Hao Zhang
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Diffusion Transformers (DiTs) with 3D full attention power state-of-the-art video generation, but suffer from prohibitive compute cost -- when generating just a 5-second 720P video, attention alone takes 800 out of 945 seconds of total inference time. This paper introduces sliding tile attention (STA) to address this challenge. STA leverages the observation that attention scores in pretrained video diffusion models predominantly concentrate within localized 3D windows. By sliding and attending over the local spatial-temporal region, STA eliminates redundancy from full attention. Unlike traditional token-wise sliding window attention (SWA), STA operates tile-by-tile with a novel hardware-aware sliding window design, preserving expressiveness while being hardware-efficient. With careful kernel-level optimizations, STA offers the first efficient 2D/3D sliding-window-like attention implementation, achieving 58.79% MFU. Precisely, STA accelerates attention by 2.8-17x over FlashAttention-2 (FA2) and 1.6-10x over FlashAttention-3 (FA3). On the leading video DiT, HunyuanVideo, STA reduces end-to-end latency from 945s (FA3) to 685s without quality degradation, requiring no training. Enabling finetuning further lowers latency to 268s with only a 0.09% drop on VBench.

### OneTrack-M: A multitask approach to transformer-based MOT models 
[[arxiv](https://arxiv.org/abs/2502.04478)] [[cool](https://papers.cool/arxiv/2502.04478)] [[pdf](https://arxiv.org/pdf/2502.04478)]
> **Authors**: Luiz C. S. de Araujo,Carlos M. S. Figueiredo
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 13 pages, 11 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Multi-Object Tracking (MOT) is a critical problem in computer vision, essential for understanding how objects move and interact in videos. This field faces significant challenges such as occlusions and complex environmental dynamics, impacting model accuracy and efficiency. While traditional approaches have relied on Convolutional Neural Networks (CNNs), introducing transformers has brought substantial advancements. This work introduces OneTrack-M, a transformer-based MOT model designed to enhance tracking computational efficiency and accuracy. Our approach simplifies the typical transformer-based architecture by eliminating the need for a decoder model for object detection and tracking. Instead, the encoder alone serves as the backbone for temporal data interpretation, significantly reducing processing time and increasing inference speed. Additionally, we employ innovative data pre-processing and multitask training techniques to address occlusion and diverse objective challenges within a single set of weights. Experimental results demonstrate that OneTrack-M achieves at least 25% faster inference times compared to state-of-the-art models in the literature while maintaining or improving tracking accuracy metrics. These improvements highlight the potential of the proposed solution for real-time applications such as autonomous vehicles, surveillance systems, and robotics, where rapid responses are crucial for system effectiveness.

### Augmented Conditioning Is Enough For Effective Training Image Generation 
[[arxiv](https://arxiv.org/abs/2502.04475)] [[cool](https://papers.cool/arxiv/2502.04475)] [[pdf](https://arxiv.org/pdf/2502.04475)]
> **Authors**: Jiahui Chen,Amy Zhang,Adriana Romero-Soriano
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: Image generation abilities of text-to-image diffusion models have significantly advanced, yielding highly photo-realistic images from descriptive text and increasing the viability of leveraging synthetic images to train computer vision models. To serve as effective training data, generated images must be highly realistic while also sufficiently diverse within the support of the target data distribution. Yet, state-of-the-art conditional image generation models have been primarily optimized for creative applications, prioritizing image realism and prompt adherence over conditional diversity. In this paper, we investigate how to improve the diversity of generated images with the goal of increasing their effectiveness to train downstream image classification models, without fine-tuning the image generation model. We find that conditioning the generation process on an augmented real image and text prompt produces generations that serve as effective synthetic datasets for downstream training. Conditioning on real training images contextualizes the generation process to produce images that are in-domain with the real image distribution, while data augmentations introduce visual diversity that improves the performance of the downstream classifier. We validate augmentation-conditioning on a total of five established long-tail and few-shot image classification benchmarks and show that leveraging augmentations to condition the generation process results in consistent improvements over the state-of-the-art on the long-tailed benchmark and remarkable gains in extreme few-shot regimes of the remaining four benchmarks. These results constitute an important step towards effectively leveraging synthetic data for downstream training.

### Color in Visual-Language Models: CLIP deficiencies 
[[arxiv](https://arxiv.org/abs/2502.04470)] [[cool](https://papers.cool/arxiv/2502.04470)] [[pdf](https://arxiv.org/pdf/2502.04470)]
> **Authors**: Guillem Arias,Ramon Baldrich,Maria Vanrell
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 6 pages, 10 figures, conference, Artificial Intelligence
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: This work explores how color is encoded in CLIP (Contrastive Language-Image Pre-training) which is currently the most influential VML (Visual Language model) in Artificial Intelligence. After performing different experiments on synthetic datasets created for this task, we conclude that CLIP is able to attribute correct color labels to colored visual stimulus, but, we come across two main deficiencies: (a) a clear bias on achromatic stimuli that are poorly related to the color concept, thus white, gray and black are rarely assigned as color labels; and (b) the tendency to prioritize text over other visual information. Here we prove it is highly significant in color labelling through an exhaustive Stroop-effect test. With the aim to find the causes of these color deficiencies, we analyse the internal representation at the neuron level. We conclude that CLIP presents an important amount of neurons selective to text, specially in deepest layers of the network, and a smaller amount of multi-modal color neurons which could be the key of understanding the concept of color properly. Our investigation underscores the necessity of refining color representation mechanisms in neural networks to foster a more comprehensive comprehension of colors as humans understand them, thereby advancing the efficacy and versatility of multimodal models like CLIP in real-world scenarios.

### Decoder-Only LLMs are Better Controllers for Diffusion Models 
[[arxiv](https://arxiv.org/abs/2502.04412)] [[cool](https://papers.cool/arxiv/2502.04412)] [[pdf](https://arxiv.org/pdf/2502.04412)]
> **Authors**: Ziyi Dong,Yao Xiao,Pengxu Wei,Liang Lin
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **Abstract**: Groundbreaking advancements in text-to-image generation have recently been achieved with the emergence of diffusion models. These models exhibit a remarkable ability to generate highly artistic and intricately detailed images based on textual prompts. However, obtaining desired generation outcomes often necessitates repetitive trials of manipulating text prompts just like casting spells on a magic mirror, and the reason behind that is the limited capability of semantic understanding inherent in current image generation models. Specifically, existing diffusion models encode the text prompt input with a pre-trained encoder structure, which is usually trained on a limited number of image-caption pairs. The state-of-the-art large language models (LLMs) based on the decoder-only structure have shown a powerful semantic understanding capability as their architectures are more suitable for training on very large-scale unlabeled data. In this work, we propose to enhance text-to-image diffusion models by borrowing the strength of semantic understanding from large language models, and devise a simple yet effective adapter to allow the diffusion models to be compatible with the decoder-only structure. Meanwhile, we also provide a supporting theoretical analysis with various architectures (e.g., encoder-only, encoder-decoder, and decoder-only), and conduct extensive empirical evaluations to verify its effectiveness. The experimental results show that the enhanced models with our adapter module are superior to the stat-of-the-art models in terms of text-to-image generation quality and reliability.

### Time-VLM: Exploring Multimodal Vision-Language Models for Augmented Time Series Forecasting 
[[arxiv](https://arxiv.org/abs/2502.04395)] [[cool](https://papers.cool/arxiv/2502.04395)] [[pdf](https://arxiv.org/pdf/2502.04395)]
> **Authors**: Siru Zhong,Weilin Ruan,Ming Jin,Huan Li,Qingsong Wen,Yuxuan Liang
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 19 pages
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Recent advancements in time series forecasting have explored augmenting models with text or vision modalities to improve accuracy. While text provides contextual understanding, it often lacks fine-grained temporal details. Conversely, vision captures intricate temporal patterns but lacks semantic context, limiting the complementary potential of these modalities. To address this, we propose Time-VLM, a novel multimodal framework that leverages pre-trained Vision-Language Models (VLMs) to bridge temporal, visual, and textual modalities for enhanced forecasting. Our framework comprises three key components: (1) a Retrieval-Augmented Learner, which extracts enriched temporal features through memory bank interactions; (2) a Vision-Augmented Learner, which encodes time series as informative images; and (3) a Text-Augmented Learner, which generates contextual textual descriptions. These components collaborate with frozen pre-trained VLMs to produce multimodal embeddings, which are then fused with temporal features for final prediction. Extensive experiments across diverse datasets demonstrate that Time-VLM achieves superior performance, particularly in few-shot and zero-shot scenarios, thereby establishing a new direction for multimodal time series forecasting.

### UniCP: A Unified Caching and Pruning Framework for Efficient Video Generation 
[[arxiv](https://arxiv.org/abs/2502.04393)] [[cool](https://papers.cool/arxiv/2502.04393)] [[pdf](https://arxiv.org/pdf/2502.04393)]
> **Authors**: Wenzhang Sun,Qirui Hou,Donglin Di,Jiahui Yang,Yongjia Ma,Jianxun Cui
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Diffusion Transformers (DiT) excel in video generation but encounter significant computational challenges due to the quadratic complexity of attention. Notably, attention differences between adjacent diffusion steps follow a U-shaped pattern. Current methods leverage this property by caching attention blocks, however, they still struggle with sudden error spikes and large discrepancies. To address these issues, we propose UniCP a unified caching and pruning framework for efficient video generation. UniCP optimizes both temporal and spatial dimensions through. Error Aware Dynamic Cache Window (EDCW): Dynamically adjusts cache window sizes for different blocks at various timesteps, adapting to abrupt error changes. PCA based Slicing (PCAS) and Dynamic Weight Shift (DWS): PCAS prunes redundant attention components, and DWS integrates caching and pruning by enabling dynamic switching between pruned and cached outputs. By adjusting cache windows and pruning redundant components, UniCP enhances computational efficiency and maintains video detail fidelity. Experimental results show that UniCP outperforms existing methods in both performance and efficiency.

### Towards Fair and Robust Face Parsing for Generative AI: A Multi-Objective Approach 
[[arxiv](https://arxiv.org/abs/2502.04391)] [[cool](https://papers.cool/arxiv/2502.04391)] [[pdf](https://arxiv.org/pdf/2502.04391)]
> **Authors**: Sophia J. Abraham,Jonathan D. Hauenstein,Walter J. Scheirer
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Face parsing is a fundamental task in computer vision, enabling applications such as identity verification, facial editing, and controllable image synthesis. However, existing face parsing models often lack fairness and robustness, leading to biased segmentation across demographic groups and errors under occlusions, noise, and domain shifts. These limitations affect downstream face synthesis, where segmentation biases can degrade generative model outputs. We propose a multi-objective learning framework that optimizes accuracy, fairness, and robustness in face parsing. Our approach introduces a homotopy-based loss function that dynamically adjusts the importance of these objectives during training. To evaluate its impact, we compare multi-objective and single-objective U-Net models in a GAN-based face synthesis pipeline (Pix2PixHD). Our results show that fairness-aware and robust segmentation improves photorealism and consistency in face generation. Additionally, we conduct preliminary experiments using ControlNet, a structured conditioning model for diffusion-based synthesis, to explore how segmentation quality influences guided image generation. Our findings demonstrate that multi-objective face parsing improves demographic consistency and robustness, leading to higher-quality GAN-based synthesis.

### Towards Fair Medical AI: Adversarial Debiasing of 3D CT Foundation Embeddings 
[[arxiv](https://arxiv.org/abs/2502.04386)] [[cool](https://papers.cool/arxiv/2502.04386)] [[pdf](https://arxiv.org/pdf/2502.04386)]
> **Authors**: Guangyao Zheng,Michael A. Jacobs,Vladimir Braverman,Vishwa S. Parekh
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: Self-supervised learning has revolutionized medical imaging by enabling efficient and generalizable feature extraction from large-scale unlabeled datasets. Recently, self-supervised foundation models have been extended to three-dimensional (3D) computed tomography (CT) data, generating compact, information-rich embeddings with 1408 features that achieve state-of-the-art performance on downstream tasks such as intracranial hemorrhage detection and lung cancer risk forecasting. However, these embeddings have been shown to encode demographic information, such as age, sex, and race, which poses a significant risk to the fairness of clinical applications. In this work, we propose a Variation Autoencoder (VAE) based adversarial debiasing framework to transform these embeddings into a new latent space where demographic information is no longer encoded, while maintaining the performance of critical downstream tasks. We validated our approach on the NLST lung cancer screening dataset, demonstrating that the debiased embeddings effectively eliminate multiple encoded demographic information and improve fairness without compromising predictive accuracy for lung cancer risk at 1-year and 2-year intervals. Additionally, our approach ensures the embeddings are robust against adversarial bias attacks. These results highlight the potential of adversarial debiasing techniques to ensure fairness and equity in clinical applications of self-supervised 3D CT embeddings, paving the way for their broader adoption in unbiased medical decision-making.

### TexLiDAR: Automated Text Understanding for Panoramic LiDAR Data 
[[arxiv](https://arxiv.org/abs/2502.04385)] [[cool](https://papers.cool/arxiv/2502.04385)] [[pdf](https://arxiv.org/pdf/2502.04385)]
> **Authors**: Naor Cohen,Roy Orfaig,Ben-Zion Bobrovsky
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Efforts to connect LiDAR data with text, such as LidarCLIP, have primarily focused on embedding 3D point clouds into CLIP text-image space. However, these approaches rely on 3D point clouds, which present challenges in encoding efficiency and neural network processing. With the advent of advanced LiDAR sensors like Ouster OS1, which, in addition to 3D point clouds, produce fixed resolution depth, signal, and ambient panoramic 2D images, new opportunities emerge for LiDAR based tasks. In this work, we propose an alternative approach to connect LiDAR data with text by leveraging 2D imagery generated by the OS1 sensor instead of 3D point clouds. Using the Florence 2 large model in a zero-shot setting, we perform image captioning and object detection. Our experiments demonstrate that Florence 2 generates more informative captions and achieves superior performance in object detection tasks compared to existing methods like CLIP. By combining advanced LiDAR sensor data with a large pre-trained model, our approach provides a robust and accurate solution for challenging detection scenarios, including real-time applications requiring high accuracy and robustness.

### Can Large Language Models Capture Video Game Engagement? 
[[arxiv](https://arxiv.org/abs/2502.04379)] [[cool](https://papers.cool/arxiv/2502.04379)] [[pdf](https://arxiv.org/pdf/2502.04379)]
> **Authors**: David Melhart,Matthew Barthet,Georgios N. Yannakakis
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-07
> **comment**: This work has been submitted to the IEEE for possible publication
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,人机交互
- **Abstract**: Can out-of-the-box pretrained Large Language Models (LLMs) detect human affect successfully when observing a video? To address this question, for the first time, we evaluate comprehensively the capacity of popular LLMs to annotate and successfully predict continuous affect annotations of videos when prompted by a sequence of text and video frames in a multimodal fashion. Particularly in this paper, we test LLMs' ability to correctly label changes of in-game engagement in 80 minutes of annotated videogame footage from 20 first-person shooter games of the GameVibe corpus. We run over 2,400 experiments to investigate the impact of LLM architecture, model size, input modality, prompting strategy, and ground truth processing method on engagement prediction. Our findings suggest that while LLMs rightfully claim human-like performance across multiple domains, they generally fall behind capturing continuous experience annotations provided by humans. We examine some of the underlying causes for the relatively poor overall performance, highlight the cases where LLMs exceed expectations, and draw a roadmap for the further exploration of automated emotion labelling via LLMs.

### DILLEMA: Diffusion and Large Language Models for Multi-Modal Augmentation 
[[arxiv](https://arxiv.org/abs/2502.04378)] [[cool](https://papers.cool/arxiv/2502.04378)] [[pdf](https://arxiv.org/pdf/2502.04378)]
> **Authors**: Luciano Baresi,Davide Yi Xian Hu,Muhammad Irfan Mas'udi,Giovanni Quattrocchi
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,图形,机器学习,软件工程
- **Abstract**: Ensuring the robustness of deep learning models requires comprehensive and diverse testing. Existing approaches, often based on simple data augmentation techniques or generative adversarial networks, are limited in producing realistic and varied test cases. To address these limitations, we present a novel framework for testing vision neural networks that leverages Large Language Models and control-conditioned Diffusion Models to generate synthetic, high-fidelity test cases. Our approach begins by translating images into detailed textual descriptions using a captioning model, allowing the language model to identify modifiable aspects of the image and generate counterfactual descriptions. These descriptions are then used to produce new test images through a text-to-image diffusion process that preserves spatial consistency and maintains the critical elements of the scene. We demonstrate the effectiveness of our method using two datasets: ImageNet1K for image classification and SHIFT for semantic segmentation in autonomous driving. The results show that our approach can generate significant test cases that reveal weaknesses and improve the robustness of the model through targeted retraining. We conducted a human assessment using Mechanical Turk to validate the generated images. The responses from the participants confirmed, with high agreement among the voters, that our approach produces valid and realistic images.

### MapFusion: A Novel BEV Feature Fusion Network for Multi-modal Map Construction 
[[arxiv](https://arxiv.org/abs/2502.04377)] [[cool](https://papers.cool/arxiv/2502.04377)] [[pdf](https://arxiv.org/pdf/2502.04377)]
> **Authors**: Xiaoshuai Hao,Yunfeng Diao,Mengchuan Wei,Yifan Yang,Peng Hao,Rong Yin,Hui Zhang,Weiming Li,Shu Zhao,Yu Liu
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Map construction task plays a vital role in providing precise and comprehensive static environmental information essential for autonomous driving systems. Primary sensors include cameras and LiDAR, with configurations varying between camera-only, LiDAR-only, or camera-LiDAR fusion, based on cost-performance considerations. While fusion-based methods typically perform best, existing approaches often neglect modality interaction and rely on simple fusion strategies, which suffer from the problems of misalignment and information loss. To address these issues, we propose MapFusion, a novel multi-modal Bird's-Eye View (BEV) feature fusion method for map construction. Specifically, to solve the semantic misalignment problem between camera and LiDAR BEV features, we introduce the Cross-modal Interaction Transform (CIT) module, enabling interaction between two BEV feature spaces and enhancing feature representation through a self-attention mechanism. Additionally, we propose an effective Dual Dynamic Fusion (DDF) module to adaptively select valuable information from different modalities, which can take full advantage of the inherent information between different modalities. Moreover, MapFusion is designed to be simple and plug-and-play, easily integrated into existing pipelines. We evaluate MapFusion on two map construction tasks, including High-definition (HD) map and BEV map segmentation, to show its versatility and effectiveness. Compared with the state-of-the-art methods, MapFusion achieves 3.6% and 6.2% absolute improvements on the HD map construction and BEV map segmentation tasks on the nuScenes dataset, respectively, demonstrating the superiority of our approach.

### HSI: A Holistic Style Injector for Arbitrary Style Transfer 
[[arxiv](https://arxiv.org/abs/2502.04369)] [[cool](https://papers.cool/arxiv/2502.04369)] [[pdf](https://arxiv.org/pdf/2502.04369)]
> **Authors**: Shuhao Zhang,Hui Kang,Yang Liu,Fang Mei,Hongjuan Li
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习,图像和视频处理
- **Abstract**: Attention-based arbitrary style transfer methods have gained significant attention recently due to their impressive ability to synthesize style details. However, the point-wise matching within the attention mechanism may overly focus on local patterns such that neglect the remarkable global features of style images. Additionally, when processing large images, the quadratic complexity of the attention mechanism will bring high computational load. To alleviate above problems, we propose Holistic Style Injector (HSI), a novel attention-style transformation module to deliver artistic expression of target style. Specifically, HSI performs stylization only based on global style representation that is more in line with the characteristics of style transfer, to avoid generating local disharmonious patterns in stylized images. Moreover, we propose a dual relation learning mechanism inside the HSI to dynamically render images by leveraging semantic similarity in content and style, ensuring the stylized images preserve the original content and improve style fidelity. Note that the proposed HSI achieves linear computational complexity because it establishes feature mapping through element-wise multiplication rather than matrix multiplication. Qualitative and quantitative results demonstrate that our method outperforms state-of-the-art approaches in both effectiveness and efficiency.

### AI-Based Thermal Video Analysis in Privacy-Preserving Healthcare: A Case Study on Detecting Time of Birth 
[[arxiv](https://arxiv.org/abs/2502.04365)] [[cool](https://papers.cool/arxiv/2502.04365)] [[pdf](https://arxiv.org/pdf/2502.04365)]
> **Authors**: Jorge García-Torres,Øyvind Meinich-Bache,Siren Rettedal,Kjersti Engan
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-07
> **comment**: Paper accepted in 2025 IEEE International Symposium on Biomedical Imaging (ISBI 2025)
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Approximately 10% of newborns need some assistance to start breathing and 5\% proper ventilation. It is crucial that interventions are initiated as soon as possible after birth. Accurate documentation of Time of Birth (ToB) is thereby essential for documenting and improving newborn resuscitation performance. However, current clinical practices rely on manual recording of ToB, typically with minute precision. In this study, we present an AI-driven, video-based system for automated ToB detection using thermal imaging, designed to preserve the privacy of healthcare providers and mothers by avoiding the use of identifiable visual data. Our approach achieves 91.4% precision and 97.4% recall in detecting ToB within thermal video clips during performance evaluation. Additionally, our system successfully identifies ToB in 96% of test cases with an absolute median deviation of 1 second compared to manual annotations. This method offers a reliable solution for improving ToB documentation and enhancing newborn resuscitation outcomes.

### Lost in Edits? A $λ$-Compass for AIGC Provenance 
[[arxiv](https://arxiv.org/abs/2502.04364)] [[cool](https://papers.cool/arxiv/2502.04364)] [[pdf](https://arxiv.org/pdf/2502.04364)]
> **Authors**: Wenhao You,Bryan Hooi,Yiwei Wang,Euijin Choo,Ming-Hsuan Yang,Junsong Yuan,Zi Huang,Yujun Cai
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,人机交互,机器学习
- **Abstract**: Recent advancements in diffusion models have driven the growth of text-guided image editing tools, enabling precise and iterative modifications of synthesized content. However, as these tools become increasingly accessible, they also introduce significant risks of misuse, emphasizing the critical need for robust attribution methods to ensure content authenticity and traceability. Despite the creative potential of such tools, they pose significant challenges for attribution, particularly in adversarial settings where edits can be layered to obscure an image's origins. We propose LambdaTracer, a novel latent-space attribution method that robustly identifies and differentiates authentic outputs from manipulated ones without requiring any modifications to generative or editing pipelines. By adaptively calibrating reconstruction losses, LambdaTracer remains effective across diverse iterative editing processes, whether automated through text-guided editing tools such as InstructPix2Pix and ControlNet or performed manually with editing software such as Adobe Photoshop. Extensive experiments reveal that our method consistently outperforms baseline approaches in distinguishing maliciously edited images, providing a practical solution to safeguard ownership, creativity, and credibility in the open, fast-evolving AI ecosystems.

### Predicting 3D Motion from 2D Video for Behavior-Based VR Biometrics 
[[arxiv](https://arxiv.org/abs/2502.04361)] [[cool](https://papers.cool/arxiv/2502.04361)] [[pdf](https://arxiv.org/pdf/2502.04361)]
> **Authors**: Mingjun Li,Natasha Kholgade Banerjee,Sean Banerjee
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-07
> **comment**: IEEE AIxVR 2025: 7th International Conference on Artificial Intelligence & extended and Virtual Reality
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,人机交互
- **Abstract**: Critical VR applications in domains such as healthcare, education, and finance that use traditional credentials, such as PIN, password, or multi-factor authentication, stand the chance of being compromised if a malicious person acquires the user credentials or if the user hands over their credentials to an ally. Recently, a number of approaches on user authentication have emerged that use motions of VR head-mounted displays (HMDs) and hand controllers during user interactions in VR to represent the user's behavior as a VR biometric signature. One of the fundamental limitations of behavior-based approaches is that current on-device tracking for HMDs and controllers lacks capability to perform tracking of full-body joint articulation, losing key signature data encapsulated by the user articulation. In this paper, we propose an approach that uses 2D body joints, namely shoulder, elbow, wrist, hip, knee, and ankle, acquired from the right side of the participants using an external 2D camera. Using a Transformer-based deep neural network, our method uses the 2D data of body joints that are not tracked by the VR device to predict past and future 3D tracks of the right controller, providing the benefit of augmenting 3D knowledge in authentication. Our approach provides a minimum equal error rate (EER) of 0.025, and a maximum EER drop of 0.040 over prior work that uses single-unit 3D trajectory as the input.

### Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive Modality Alignment 
[[arxiv](https://arxiv.org/abs/2502.04328)] [[cool](https://papers.cool/arxiv/2502.04328)] [[pdf](https://arxiv.org/pdf/2502.04328)]
> **Authors**: Zuyan Liu,Yuhao Dong,Jiahui Wang,Ziwei Liu,Winston Hu,Jiwen Lu,Yongming Rao
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算语言学,多媒体,声音,音频和语音处理,图像和视频处理
- **Abstract**: Recent advances in large language models, particularly following GPT-4o, have sparked increasing interest in developing omni-modal models capable of understanding more modalities. While some open-source alternatives have emerged, there is still a notable lag behind specialized single-modality models in performance. In this paper, we present Ola, an Omni-modal language model that achieves competitive performance across image, video, and audio understanding compared to specialized counterparts. The core design of Ola lies in its progressive modality alignment strategy that extends the supporting modality of the language model progressively. Our training pipeline begins with the most distinct modalities: image and text, then gradually expands the skill sets of the model using speech data that connects language and audio knowledge, and video data that connects all modalities. The progressive learning pipeline also enables us to maintain a relatively small size of the cross-modal alignment data, making developing omni-modal from existing vision-language models easy and less costly. Moreover, to unlock an advanced interactive experience like GPT-4o, we further design a sentence-wise decoding solution for streaming speech generation. Extensive experiments demonstrate that Ola surpasses existing open omni-modal LLMs across all modalities while achieving highly competitive performance compared to state-of-the-art specialized models of similar sizes. We aim to make Ola a fully open omni-modal understanding solution to advance future research in this emerging field. Model weights, code, and data are open-sourced at https://github.com/Ola-Omni/Ola.

### WorldSense: Evaluating Real-world Omnimodal Understanding for Multimodal LLMs 
[[arxiv](https://arxiv.org/abs/2502.04326)] [[cool](https://papers.cool/arxiv/2502.04326)] [[pdf](https://arxiv.org/pdf/2502.04326)]
> **Authors**: Jack Hong,Shilin Yan,Jiayin Cai,Xiaolong Jiang,Yao Hu,Weidi Xie
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: In this paper, we introduce WorldSense, the first benchmark to assess the multi-modal video understanding, that simultaneously encompasses visual, audio, and text inputs. In contrast to existing benchmarks, our WorldSense has several features: (i) collaboration of omni-modality, we design the evaluation tasks to feature a strong coupling of audio and video, requiring models to effectively utilize the synergistic perception of omni-modality; (ii) diversity of videos and tasks, WorldSense encompasses a diverse collection of 1,662 audio-visual synchronised videos, systematically categorized into 8 primary domains and 67 fine-grained subcategories to cover the broad scenarios, and 3,172 multi-choice QA pairs across 26 distinct tasks to enable the comprehensive evaluation; (iii) high-quality annotations, all the QA pairs are manually labeled by 80 expert annotators with multiple rounds of correction to ensure quality. Based on our WorldSense, we extensively evaluate various state-of-the-art models. The experimental results indicate that existing models face significant challenges in understanding real-world scenarios (48.0% best accuracy). We hope our WorldSense can provide a platform for evaluating the ability in constructing and understanding coherent contexts from omni-modality.

### ConceptAttention: Diffusion Transformers Learn Highly Interpretable Features 
[[arxiv](https://arxiv.org/abs/2502.04320)] [[cool](https://papers.cool/arxiv/2502.04320)] [[pdf](https://arxiv.org/pdf/2502.04320)]
> **Authors**: Alec Helbling,Tuna Han Salih Meral,Ben Hoover,Pinar Yanardag,Duen Horng Chau
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Do the rich representations of multi-modal diffusion transformers (DiTs) exhibit unique properties that enhance their interpretability? We introduce ConceptAttention, a novel method that leverages the expressive power of DiT attention layers to generate high-quality saliency maps that precisely locate textual concepts within images. Without requiring additional training, ConceptAttention repurposes the parameters of DiT attention layers to produce highly contextualized concept embeddings, contributing the major discovery that performing linear projections in the output space of DiT attention layers yields significantly sharper saliency maps compared to commonly used cross-attention mechanisms. Remarkably, ConceptAttention even achieves state-of-the-art performance on zero-shot image segmentation benchmarks, outperforming 11 other zero-shot interpretability methods on the ImageNet-Segmentation dataset and on a single-class subset of PascalVOC. Our work contributes the first evidence that the representations of multi-modal DiT models like Flux are highly transferable to vision tasks like segmentation, even outperforming multi-modal foundation models like CLIP.

### Factorized Implicit Global Convolution for Automotive Computational Fluid Dynamics Prediction 
[[arxiv](https://arxiv.org/abs/2502.04317)] [[cool](https://papers.cool/arxiv/2502.04317)] [[pdf](https://arxiv.org/pdf/2502.04317)]
> **Authors**: Chris Choy,Alexey Kamenev,Jean Kossaifi,Max Rietmann,Jan Kautz,Kamyar Azizzadenesheli
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Computational Fluid Dynamics (CFD) is crucial for automotive design, requiring the analysis of large 3D point clouds to study how vehicle geometry affects pressure fields and drag forces. However, existing deep learning approaches for CFD struggle with the computational complexity of processing high-resolution 3D data. We propose Factorized Implicit Global Convolution (FIGConv), a novel architecture that efficiently solves CFD problems for very large 3D meshes with arbitrary input and output geometries. FIGConv achieves quadratic complexity $O(N^2)$, a significant improvement over existing 3D neural CFD models that require cubic complexity $O(N^3)$. Our approach combines Factorized Implicit Grids to approximate high-resolution domains, efficient global convolutions through 2D reparameterization, and a U-shaped architecture for effective information gathering and integration. We validate our approach on the industry-standard Ahmed body dataset and the large-scale DrivAerNet dataset. In DrivAerNet, our model achieves an $R^2$ value of 0.95 for drag prediction, outperforming the previous state-of-the-art by a significant margin. This represents a 40% improvement in relative mean squared error and a 70% improvement in absolute mean squared error over previous methods.

### GCE-Pose: Global Context Enhancement for Category-level Object Pose Estimation 
[[arxiv](https://arxiv.org/abs/2502.04293)] [[cool](https://papers.cool/arxiv/2502.04293)] [[pdf](https://arxiv.org/pdf/2502.04293)]
> **Authors**: Weihang Li,Hongli Xu,Junwen Huang,Hyunjun Jung,Peter KT Yu,Nassir Navab,Benjamin Busam
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: A key challenge in model-free category-level pose estimation is the extraction of contextual object features that generalize across varying instances within a specific category. Recent approaches leverage foundational features to capture semantic and geometry cues from data. However, these approaches fail under partial visibility. We overcome this with a first-complete-then-aggregate strategy for feature extraction utilizing class priors. In this paper, we present GCE-Pose, a method that enhances pose estimation for novel instances by integrating category-level global context prior. GCE-Pose performs semantic shape reconstruction with a proposed Semantic Shape Reconstruction (SSR) module. Given an unseen partial RGB-D object instance, our SSR module reconstructs the instance's global geometry and semantics by deforming category-specific 3D semantic prototypes through a learned deep Linear Shape Model. We further introduce a Global Context Enhanced (GCE) feature fusion module that effectively fuses features from partial RGB-D observations and the reconstructed global context. Extensive experiments validate the impact of our global context prior and the effectiveness of the GCE fusion module, demonstrating that GCE-Pose significantly outperforms existing methods on challenging real-world datasets HouseCat6D and NOCS-REAL275. Our project page is available at https://colin-de.github.io/GCE-Pose/.

### Cross the Gap: Exposing the Intra-modal Misalignment in CLIP via Modality Inversion 
[[arxiv](https://arxiv.org/abs/2502.04263)] [[cool](https://papers.cool/arxiv/2502.04263)] [[pdf](https://arxiv.org/pdf/2502.04263)]
> **Authors**: Marco Mistretta,Alberto Baldrati,Lorenzo Agnolucci,Marco Bertini,Andrew D. Bagdanov
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: Accepted for publication at ICLR 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: Pre-trained multi-modal Vision-Language Models like CLIP are widely used off-the-shelf for a variety of applications. In this paper, we show that the common practice of individually exploiting the text or image encoders of these powerful multi-modal models is highly suboptimal for intra-modal tasks like image-to-image retrieval. We argue that this is inherently due to the CLIP-style inter-modal contrastive loss that does not enforce any intra-modal constraints, leading to what we call intra-modal misalignment. To demonstrate this, we leverage two optimization-based modality inversion techniques that map representations from their input modality to the complementary one without any need for auxiliary data or additional trained adapters. We empirically show that, in the intra-modal tasks of image-to-image and text-to-text retrieval, approaching these tasks inter-modally significantly improves performance with respect to intra-modal baselines on more than fifteen datasets. Additionally, we demonstrate that approaching a native inter-modal task (e.g. zero-shot image classification) intra-modally decreases performance, further validating our findings. Finally, we show that incorporating an intra-modal term in the pre-training objective or narrowing the modality gap between the text and image feature embedding spaces helps reduce the intra-modal misalignment. The code is publicly available at: https://github.com/miccunifi/Cross-the-Gap.

### Keep It Light! Simplifying Image Clustering Via Text-Free Adapters 
[[arxiv](https://arxiv.org/abs/2502.04226)] [[cool](https://papers.cool/arxiv/2502.04226)] [[pdf](https://arxiv.org/pdf/2502.04226)]
> **Authors**: Yicen Li,Haitz Sáez de Ocáriz Borde,Anastasis Kratsios,Paul D. McNicholas
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习,神经和进化计算,计算,机器学习
- **Abstract**: Many competitive clustering pipelines have a multi-modal design, leveraging large language models (LLMs) or other text encoders, and text-image pairs, which are often unavailable in real-world downstream applications. Additionally, such frameworks are generally complicated to train and require substantial computational resources, making widespread adoption challenging. In this work, we show that in deep clustering, competitive performance with more complex state-of-the-art methods can be achieved using a text-free and highly simplified training pipeline. In particular, our approach, Simple Clustering via Pre-trained models (SCP), trains only a small cluster head while leveraging pre-trained vision model feature representations and positive data pairs. Experiments on benchmark datasets including CIFAR-10, CIFAR-20, CIFAR-100, STL-10, ImageNet-10, and ImageNet-Dogs, demonstrate that SCP achieves highly competitive performance. Furthermore, we provide a theoretical result explaining why, at least under ideal conditions, additional text-based embeddings may not be necessary to achieve strong clustering performance in vision.

### Éclair -- Extracting Content and Layout with Integrated Reading Order for Documents 
[[arxiv](https://arxiv.org/abs/2502.04223)] [[cool](https://papers.cool/arxiv/2502.04223)] [[pdf](https://arxiv.org/pdf/2502.04223)]
> **Authors**: Ilia Karmanov,Amala Sanjay Deshmukh,Lukas Voegtle,Philipp Fischer,Kateryna Chumachenko,Timo Roman,Jarno Seppänen,Jupinder Parmar,Joseph Jennings,Andrew Tao,Karan Sapra
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Optical Character Recognition (OCR) technology is widely used to extract text from images of documents, facilitating efficient digitization and data retrieval. However, merely extracting text is insufficient when dealing with complex documents. Fully comprehending such documents requires an understanding of their structure -- including formatting, formulas, tables, and the reading order of multiple blocks and columns across multiple pages -- as well as semantic information for detecting elements like footnotes and image captions. This comprehensive understanding is crucial for downstream tasks such as retrieval, document question answering, and data curation for training Large Language Models (LLMs) and Vision Language Models (VLMs). To address this, we introduce Éclair, a general-purpose text-extraction tool specifically designed to process a wide range of document types. Given an image, Éclair is able to extract formatted text in reading order, along with bounding boxes and their corresponding semantic classes. To thoroughly evaluate these novel capabilities, we introduce our diverse human-annotated benchmark for document-level OCR and semantic classification. Éclair achieves state-of-the-art accuracy on this benchmark, outperforming other methods across key metrics. Additionally, we evaluate Éclair on established benchmarks, demonstrating its versatility and strength across several evaluation standards.

### Enhanced Feature-based Image Stitching for Endoscopic Videos in Pediatric Eosinophilic Esophagitis 
[[arxiv](https://arxiv.org/abs/2502.04207)] [[cool](https://papers.cool/arxiv/2502.04207)] [[pdf](https://arxiv.org/pdf/2502.04207)]
> **Authors**: Juming Xiong,Muyang Li,Ruining Deng,Tianyuan Yao,Shunxing Bao,Regina N Tyree,Girish Hiremath,Yuankai Huo
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Video endoscopy represents a major advance in the investigation of gastrointestinal diseases. Reviewing endoscopy videos often involves frequent adjustments and reorientations to piece together a complete view, which can be both time-consuming and prone to errors. Image stitching techniques address this issue by providing a continuous and complete visualization of the examined area. However, endoscopic images, particularly those of the esophagus, present unique challenges. The smooth surface, lack of distinct feature points, and non-horizontal orientation complicate the stitching process, rendering traditional feature-based methods often ineffective for these types of images. In this paper, we propose a novel preprocessing pipeline designed to enhance endoscopic image stitching through advanced computational techniques. Our approach converts endoscopic video data into continuous 2D images by following four key steps: (1) keyframe selection, (2) image rotation adjustment to correct distortions, (3) surface unwrapping using polar coordinate transformation to generate a flat image, and (4) feature point matching enhanced by Adaptive Histogram Equalization for improved feature detection. We evaluate stitching quality through the assessment of valid feature point match pairs. Experiments conducted on 20 pediatric endoscopy videos demonstrate that our method significantly improves image alignment and stitching quality compared to traditional techniques, laying a robust foundation for more effective panoramic image creation.

### PixFoundation: Are We Heading in the Right Direction with Pixel-level Vision Foundation Models? 
[[arxiv](https://arxiv.org/abs/2502.04192)] [[cool](https://papers.cool/arxiv/2502.04192)] [[pdf](https://arxiv.org/pdf/2502.04192)]
> **Authors**: Mennatullah Siam
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: Under Review
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Multiple works have emerged to push the boundaries on multi-modal large language models (MLLMs) towards pixel-level understanding. Such approaches have shown strong performance on benchmarks for referring expression segmentation and grounded conversation generation. The current trend in pixel-level MLLMs is to train with pixel-level grounding supervision on large-scale labelled data. However, we show that such MLLMs when evaluated on recent challenging vision centric benchmarks, exhibit a weak ability in visual question answering. Surprisingly, some of these methods even downgrade the grounding ability of MLLMs that were never trained with such supervision. In this work, we propose two novel challenging benchmarks and show that MLLMs without pixel-level grounding supervision can outperform the state of the art in such tasks when evaluating both the pixel-level grounding and visual question answering. We propose simple baselines to extract the grounding information that can be plugged into any MLLM, which we call as PixFoundation. More importantly, we study the research question of "When does grounding emerge in MLLMs that are not trained with pixel-level grounding supervision?" We show that grounding can coincide with object parts or location/appearance information. Code repository is at https://github.com/MSiam/PixFoundation/.

### Beyond the Final Layer: Hierarchical Query Fusion Transformer with Agent-Interpolation Initialization for 3D Instance Segmentation 
[[arxiv](https://arxiv.org/abs/2502.04139)] [[cool](https://papers.cool/arxiv/2502.04139)] [[pdf](https://arxiv.org/pdf/2502.04139)]
> **Authors**: Jiahao Lu,Jiacheng Deng,Tianzhu Zhang
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: Under review
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: 3D instance segmentation aims to predict a set of object instances in a scene and represent them as binary foreground masks with corresponding semantic labels. Currently, transformer-based methods are gaining increasing attention due to their elegant pipelines, reduced manual selection of geometric properties, and superior performance. However, transformer-based methods fail to simultaneously maintain strong position and content information during query initialization. Additionally, due to supervision at each decoder layer, there exists a phenomenon of object disappearance with the deepening of layers. To overcome these hurdles, we introduce Beyond the Final Layer: Hierarchical Query Fusion Transformer with Agent-Interpolation Initialization for 3D Instance Segmentation (BFL). Specifically, an Agent-Interpolation Initialization Module is designed to generate resilient queries capable of achieving a balance between foreground coverage and content learning. Additionally, a Hierarchical Query Fusion Decoder is designed to retain low overlap queries, mitigating the decrease in recall with the deepening of layers. Extensive experiments on ScanNetV2, ScanNet200, ScanNet++ and S3DIS datasets demonstrate the superior performance of BFL.

### Efficient Few-Shot Continual Learning in Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2502.04098)] [[cool](https://papers.cool/arxiv/2502.04098)] [[pdf](https://arxiv.org/pdf/2502.04098)]
> **Authors**: Aristeidis Panos,Rahaf Aljundi,Daniel Olmeda Reino,Richard E. Turner
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Vision-language models (VLMs) excel in tasks such as visual question answering and image captioning. However, VLMs are often limited by their use of pretrained image encoders, like CLIP, leading to image understanding errors that hinder overall performance. On top of that, real-world applications often require the model to be continuously adapted as new and often limited data continuously arrive. To address this, we propose LoRSU (Low-Rank Adaptation with Structured Updates), a robust and computationally efficient method for selectively updating image encoders within VLMs. LoRSU introduces structured and localized parameter updates, effectively correcting performance on previously error-prone data while preserving the model's general robustness. Our approach leverages theoretical insights to identify and update only the most critical parameters, achieving significant resource efficiency. Specifically, we demonstrate that LoRSU reduces computational overhead by over 25x compared to full VLM updates, without sacrificing performance. Experimental results on VQA tasks in the few-shot continual learning setting, validate LoRSU's scalability, efficiency, and effectiveness, making it a compelling solution for image encoder adaptation in resource-constrained environments.

### Automatic quantification of breast cancer biomarkers from multiple 18F-FDG PET image segmentation 
[[arxiv](https://arxiv.org/abs/2502.04083)] [[cool](https://papers.cool/arxiv/2502.04083)] [[pdf](https://arxiv.org/pdf/2502.04083)]
> **Authors**: Tewele W. Tareke,Neree Payan,Alexandre Cochet,Laurent Arnould,Benoit Presles,Jean-Marc Vrigneaud,Fabrice Meriaudeau,Alain Lalande
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: Submit soon to EJNMMI Research
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Neoadjuvant chemotherapy (NAC) has become a standard clinical practice for tumor downsizing in breast cancer with 18F-FDG Positron Emission Tomography (PET). Our work aims to leverage PET imaging for the segmentation of breast lesions. The focus is on developing an automated system that accurately segments primary tumor regions and extracts key biomarkers from these areas to provide insights into the evolution of breast cancer following the first course of NAC. 243 baseline 18F-FDG PET scans (PET_Bl) and 180 follow-up 18F-FDG PET scans (PET_Fu) were acquired before and after the first course of NAC, respectively. Firstly, a deep learning-based breast tumor segmentation method was developed. The optimal baseline model (model trained on baseline exams) was fine-tuned on 15 follow-up exams and adapted using active learning to segment tumor areas in PET_Fu. The pipeline computes biomarkers such as maximum standardized uptake value (SUVmax), metabolic tumor volume (MTV), and total lesion glycolysis (TLG) to evaluate tumor evolution between PET_Fu and PET_Bl. Quality control measures were employed to exclude aberrant outliers. The nnUNet deep learning model outperformed in tumor segmentation on PET_Bl, achieved a Dice similarity coefficient (DSC) of 0.89 and a Hausdorff distance (HD) of 3.52 mm. After fine-tuning, the model demonstrated a DSC of 0.78 and a HD of 4.95 mm on PET_Fu exams. Biomarkers analysis revealed very strong correlations whatever the biomarker between manually segmented and automatically predicted regions. The significant average decrease of SUVmax, MTV and TLG were 5.22, 11.79 cm3 and 19.23 cm3, respectively. The presented approach demonstrates an automated system for breast tumor segmentation from 18F-FDG PET. Thanks to the extracted biomarkers, our method enables the automatic assessment of cancer progression.

### Content-Rich AIGC Video Quality Assessment via Intricate Text Alignment and Motion-Aware Consistency 
[[arxiv](https://arxiv.org/abs/2502.04076)] [[cool](https://papers.cool/arxiv/2502.04076)] [[pdf](https://arxiv.org/pdf/2502.04076)]
> **Authors**: Shangkun Sun,Xiaoyu Liang,Bowen Qu,Wei Gao
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: The advent of next-generation video generation models like \textit{Sora} poses challenges for AI-generated content (AIGC) video quality assessment (VQA). These models substantially mitigate flickering artifacts prevalent in prior models, enable longer and complex text prompts and generate longer videos with intricate, diverse motion patterns. Conventional VQA methods designed for simple text and basic motion patterns struggle to evaluate these content-rich videos. To this end, we propose \textbf{CRAVE} (\underline{C}ontent-\underline{R}ich \underline{A}IGC \underline{V}ideo \underline{E}valuator), specifically for the evaluation of Sora-era AIGC videos. CRAVE proposes the multi-granularity text-temporal fusion that aligns long-form complex textual semantics with video dynamics. Additionally, CRAVE leverages the hybrid motion-fidelity modeling to assess temporal artifacts. Furthermore, given the straightforward prompts and content in current AIGC VQA datasets, we introduce \textbf{CRAVE-DB}, a benchmark featuring content-rich videos from next-generation models paired with elaborate prompts. Extensive experiments have shown that the proposed CRAVE achieves excellent results on multiple AIGC VQA benchmarks, demonstrating a high degree of alignment with human perception. All data and code will be publicly available at https://github.com/littlespray/CRAVE.

### 3D Prior is All You Need: Cross-Task Few-shot 2D Gaze Estimation 
[[arxiv](https://arxiv.org/abs/2502.04074)] [[cool](https://papers.cool/arxiv/2502.04074)] [[pdf](https://arxiv.org/pdf/2502.04074)]
> **Authors**: Yihua Cheng,Hengfei Wang,Zhongqun Zhang,Yang Yue,Bo Eun Kim,Feng Lu,Hyung Jin Chang
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: 3D and 2D gaze estimation share the fundamental objective of capturing eye movements but are traditionally treated as two distinct research domains. In this paper, we introduce a novel cross-task few-shot 2D gaze estimation approach, aiming to adapt a pre-trained 3D gaze estimation network for 2D gaze prediction on unseen devices using only a few training images. This task is highly challenging due to the domain gap between 3D and 2D gaze, unknown screen poses, and limited training data. To address these challenges, we propose a novel framework that bridges the gap between 3D and 2D gaze. Our framework contains a physics-based differentiable projection module with learnable parameters to model screen poses and project 3D gaze into 2D gaze. The framework is fully differentiable and can integrate into existing 3D gaze networks without modifying their original architecture. Additionally, we introduce a dynamic pseudo-labelling strategy for flipped images, which is particularly challenging for 2D labels due to unknown screen poses. To overcome this, we reverse the projection process by converting 2D labels to 3D space, where flipping is performed. Notably, this 3D space is not aligned with the camera coordinate system, so we learn a dynamic transformation matrix to compensate for this misalignment. We evaluate our method on MPIIGaze, EVE, and GazeCapture datasets, collected respectively on laptops, desktop computers, and mobile devices. The superior performance highlights the effectiveness of our approach, and demonstrates its strong potential for real-world applications.

### Inteligencia artificial para la multi-clasificación de fauna en fotografías automáticas utilizadas en investigación científica 
[[arxiv](https://arxiv.org/abs/2502.04064)] [[cool](https://papers.cool/arxiv/2502.04064)] [[pdf](https://arxiv.org/pdf/2502.04064)]
> **Authors**: Federico Gonzalez,Leonel Viera,Rosina Soler,Lucila Chiarvetto Peralta,Matias Gel,Gimena Bustamante,Abril Montaldo,Brian Rigoni,Ignacio Perez
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: in Spanishlanguage, XXIV Workshop de Investigadores en Ciencias de la Computación (WICC 2022, Mendoza)
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: The management of natural environments, whether for conservation or production, requires a deep understanding of wildlife. The number, location, and behavior of wild animals are among the main subjects of study in ecology and wildlife research. The use of camera traps offers the opportunity to quickly collect large quantities of photographs that capture wildlife in its natural habitat, avoiding factors that could alter their behavior. In Tierra del Fuego, Argentina, research is being conducted on forest use by different herbivores (guanacos, cows, sheep) to optimize management and protect these natural ecosystems. Although camera traps allow for the collection of millions of images, interpreting such photographs presents a scalability challenge for manual processing. As a result, much of the valuable knowledge stored in these vast data repositories remains untapped. Neural Networks and Deep Learning are areas of study within Artificial Intelligence. Over the past decade, these two disciplines have made significant contributions to image recognition on a global scale. Ecological and wildlife conservation studies can be combined with these new technologies to extract important information from the photographs obtained by camera traps, contributing to the understanding of various natural processes and improving the management of the involved wild areas. Our project aims to develop neural network models to classify animal species in photographs taken with camera traps, addressing large-scale challenges in scientific research.

### PartEdit: Fine-Grained Image Editing using Pre-Trained Diffusion Models 
[[arxiv](https://arxiv.org/abs/2502.04050)] [[cool](https://papers.cool/arxiv/2502.04050)] [[pdf](https://arxiv.org/pdf/2502.04050)]
> **Authors**: Aleksandar Cvejic,Abdelrahman Eldesokey,Peter Wonka
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: Project page: https://partedit.github.io/PartEdit/
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: We present the first text-based image editing approach for object parts based on pre-trained diffusion models. Diffusion-based image editing approaches capitalized on the deep understanding of diffusion models of image semantics to perform a variety of edits. However, existing diffusion models lack sufficient understanding of many object parts, hindering fine-grained edits requested by users. To address this, we propose to expand the knowledge of pre-trained diffusion models to allow them to understand various object parts, enabling them to perform fine-grained edits. We achieve this by learning special textual tokens that correspond to different object parts through an efficient token optimization process. These tokens are optimized to produce reliable localization masks at each inference step to localize the editing region. Leveraging these masks, we design feature-blending and adaptive thresholding strategies to execute the edits seamlessly. To evaluate our approach, we establish a benchmark and an evaluation protocol for part editing. Experiments show that our approach outperforms existing editing methods on all metrics and is preferred by users 77-90% of the time in conducted user studies.

### CAD-Editor: A Locate-then-Infill Framework with Automated Training Data Synthesis for Text-Based CAD Editing 
[[arxiv](https://arxiv.org/abs/2502.03997)] [[cool](https://papers.cool/arxiv/2502.03997)] [[pdf](https://arxiv.org/pdf/2502.03997)]
> **Authors**: Yu Yuan,Shizhao Sun,Qi Liu,Jiang Bian
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Computer Aided Design (CAD) is indispensable across various industries. \emph{Text-based CAD editing}, which automates the modification of CAD models based on textual instructions, holds great potential but remains underexplored. Existing methods primarily focus on design variation generation or text-based CAD generation, either lacking support for text-based control or neglecting existing CAD models as constraints. We introduce \emph{CAD-Editor}, the first framework for text-based CAD editing. To address the challenge of demanding triplet data with accurate correspondence for training, we propose an automated data synthesis pipeline. This pipeline utilizes design variation models to generate pairs of original and edited CAD models and employs Large Vision-Language Models (LVLMs) to summarize their differences into editing instructions. To tackle the composite nature of text-based CAD editing, we propose a locate-then-infill framework that decomposes the task into two focused sub-tasks: locating regions requiring modification and infilling these regions with appropriate edits. Large Language Models (LLMs) serve as the backbone for both sub-tasks, leveraging their capabilities in natural language understanding and CAD knowledge. Experiments show that CAD-Editor achieves superior performance both quantitatively and qualitatively.

### RWKV-UI: UI Understanding with Enhanced Perception and Reasoning 
[[arxiv](https://arxiv.org/abs/2502.03971)] [[cool](https://papers.cool/arxiv/2502.03971)] [[pdf](https://arxiv.org/pdf/2502.03971)]
> **Authors**: Jiaxi Yang,Haowen Hou
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 10 pages, 5figures, conference
- **标题**: None
- **领域**: 计算机视觉和模式识别,人机交互
- **Abstract**: Existing Visual Language Modelsoften struggle with information loss and limited reasoning abilities when handling high-resolution web interfaces that combine complex visual, textual, and interactive elements. These challenges are particularly evident in tasks requiring webpage layout comprehension and multi-step interactive reasoning. To address these challenges, we propose RWKV-UI, a Visual Language Model based on the RWKV architecture, specifically designed to handle high-resolution UI images. During model training, we introduce layout detection as a visual prompt to help the model better understand the webpage layout structures. Additionally, we design a visual prompt based on the Chain-of-Thought(CoT) mechanism, which enhances the model's ability to understand and reason about webpage content through reasoning chains. Experimental results show that RWKV-UI demonstrates significant performance improvements in high-resolution UI understanding and interactive reasoning tasks.

### MultiFloodSynth: Multi-Annotated Flood Synthetic Dataset Generation 
[[arxiv](https://arxiv.org/abs/2502.03966)] [[cool](https://papers.cool/arxiv/2502.03966)] [[pdf](https://arxiv.org/pdf/2502.03966)]
> **Authors**: YoonJe Kang,Yonghoon Jung,Wonseop Shin,Bumsoo Kim,Sanghyun Seo
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 6 pages, 6 figures. Accepted as Oral Presentation to AAAI 2025 Workshop on Good-Data
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: In this paper, we present synthetic data generation framework for flood hazard detection system. For high fidelity and quality, we characterize several real-world properties into virtual world and simulate the flood situation by controlling them. For the sake of efficiency, recent generative models in image-to-3D and urban city synthesis are leveraged to easily composite flood environments so that we avoid data bias due to the hand-crafted manner. Based on our framework, we build the flood synthetic dataset with 5 levels, dubbed MultiFloodSynth which contains rich annotation types like normal map, segmentation, 3D bounding box for a variety of downstream task. In experiments, our dataset demonstrate the enhanced performance of flood hazard detection with on-par realism compared with real dataset.

### LR0.FM: Low-Resolution Zero-shot Classification Benchmark For Foundation Models 
[[arxiv](https://arxiv.org/abs/2502.03950)] [[cool](https://papers.cool/arxiv/2502.03950)] [[pdf](https://arxiv.org/pdf/2502.03950)]
> **Authors**: Priyank Pathak,Shyam Marjit,Shruti Vyas,Yogesh S Rawat
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: Accepted to ICLR 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Visual-language foundation Models (FMs) exhibit remarkable zero-shot generalization across diverse tasks, largely attributed to extensive pre-training on largescale datasets. However, their robustness on low-resolution/pixelated (LR) images, a common challenge in real-world scenarios, remains underexplored. We introduce LR0.FM, a comprehensive benchmark evaluating the impact of low resolution on the zero-shot classification performance of 10 FM(s) across 66 backbones and 15 datasets. We propose a novel metric, Weighted Aggregated Robustness, to address the limitations of existing metrics and better evaluate model performance across resolutions and datasets. Our key findings show that: (i) model size positively correlates with robustness to resolution degradation, (ii) pre-training dataset quality is more important than its size, and (iii) fine-tuned and higher resolution models are less robust against LR. Our analysis further reveals that the model makes semantically reasonable predictions at LR, and the lack of fine-grained details in input adversely impacts the model's initial layers more than the deeper layers. We use these insights and introduce a simple strategy, LR-TK0, to enhance the robustness of models without compromising their pre-trained weights. We demonstrate the effectiveness of LR-TK0 for robustness against low-resolution across several datasets and its generalization capability across backbones and other approaches. Code is available at https://github.com/shyammarjit/LR0.FM

### Rule-Based Modeling of Low-Dimensional Data with PCA and Binary Particle Swarm Optimization (BPSO) in ANFIS 
[[arxiv](https://arxiv.org/abs/2502.03895)] [[cool](https://papers.cool/arxiv/2502.03895)] [[pdf](https://arxiv.org/pdf/2502.03895)]
> **Authors**: Afnan Al-Ali,Uvais Qidwai
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 41 pages, 9 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Fuzzy rule-based systems interpret data in low-dimensional domains, providing transparency and interpretability. In contrast, deep learning excels in complex tasks like image and speech recognition but is prone to overfitting in sparse, unstructured, or low-dimensional data. This interpretability is crucial in fields like healthcare and finance. Traditional rule-based systems, especially ANFIS with grid partitioning, suffer from exponential rule growth as dimensionality increases. We propose a strategic rule-reduction model that applies Principal Component Analysis (PCA) on normalized firing strengths to obtain linearly uncorrelated components. Binary Particle Swarm Optimization (BPSO) selectively refines these components, significantly reducing the number of rules while preserving precision in decision-making. A custom parameter update mechanism fine-tunes specific ANFIS layers by dynamically adjusting BPSO parameters, avoiding local minima. We validated our approach on standard UCI respiratory, keel classification, regression datasets, and a real-world ischemic stroke dataset, demonstrating adaptability and practicality. Results indicate fewer rules, shorter training, and high accuracy, underscoring the methods effectiveness for low-dimensional interpretability and complex data scenarios. This synergy of fuzzy logic and optimization fosters robust solutions. Our method contributes a powerful framework for interpretable AI in multiple domains. It addresses dimensionality, ensuring a rule base.

### Semi-rPPG: Semi-Supervised Remote Physiological Measurement with Curriculum Pseudo-Labeling 
[[arxiv](https://arxiv.org/abs/2502.03855)] [[cool](https://papers.cool/arxiv/2502.03855)] [[pdf](https://arxiv.org/pdf/2502.03855)]
> **Authors**: Bingjie Wu,Zitong Yu,Yiping Xie,Wei Liu,Chaoqi Luo,Yong Liu,Rick Siow Mong Goh
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: Accepted by IEEE Transactions on Instrumentation and Measurement (TIM)
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Remote Photoplethysmography (rPPG) is a promising technique to monitor physiological signals such as heart rate from facial videos. However, the labeled facial videos in this research are challenging to collect. Current rPPG research is mainly based on several small public datasets collected in simple environments, which limits the generalization and scale of the AI models. Semi-supervised methods that leverage a small amount of labeled data and abundant unlabeled data can fill this gap for rPPG learning. In this study, a novel semi-supervised learning method named Semi-rPPG that combines curriculum pseudo-labeling and consistency regularization is proposed to extract intrinsic physiological features from unlabelled data without impairing the model from noises. Specifically, a curriculum pseudo-labeling strategy with signal-to-noise ratio (SNR) criteria is proposed to annotate the unlabelled data while adaptively filtering out the low-quality unlabelled data. Besides, a novel consistency regularization term for quasi-periodic signals is proposed through weak and strong augmented clips. To benefit the research on semi-supervised rPPG measurement, we establish a novel semi-supervised benchmark for rPPG learning through intra-dataset and cross-dataset evaluation on four public datasets. The proposed Semi-rPPG method achieves the best results compared with three classical semi-supervised methods under different protocols. Ablation studies are conducted to prove the effectiveness of the proposed methods.

### Adapting Human Mesh Recovery with Vision-Language Feedback 
[[arxiv](https://arxiv.org/abs/2502.03836)] [[cool](https://papers.cool/arxiv/2502.03836)] [[pdf](https://arxiv.org/pdf/2502.03836)]
> **Authors**: Chongyang Xu,Buzhen Huang,Chengfang Zhang,Ziliang Feng,Yangang Wang
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 6 pages, 7 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Human mesh recovery can be approached using either regression-based or optimization-based methods. Regression models achieve high pose accuracy but struggle with model-to-image alignment due to the lack of explicit 2D-3D correspondences. In contrast, optimization-based methods align 3D models to 2D observations but are prone to local minima and depth ambiguity. In this work, we leverage large vision-language models (VLMs) to generate interactive body part descriptions, which serve as implicit constraints to enhance 3D perception and limit the optimization space. Specifically, we formulate monocular human mesh recovery as a distribution adaptation task by integrating both 2D observations and language descriptions. To bridge the gap between text and 3D pose signals, we first train a text encoder and a pose VQ-VAE, aligning texts to body poses in a shared latent space using contrastive learning. Subsequently, we employ a diffusion-based framework to refine the initial parameters guided by gradients derived from both 2D observations and text descriptions. Finally, the model can produce poses with accurate 3D perception and image consistency. Experimental results on multiple benchmarks validate its effectiveness. The code will be made publicly available.

### FE-UNet: Frequency Domain Enhanced U-Net with Segment Anything Capability for Versatile Image Segmentation 
[[arxiv](https://arxiv.org/abs/2502.03829)] [[cool](https://papers.cool/arxiv/2502.03829)] [[pdf](https://arxiv.org/pdf/2502.03829)]
> **Authors**: Guohao Huo,Ruiting Dai,Ling Shao,Hao Tang
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Image segmentation is a critical task in visual understanding. Convolutional Neural Networks (CNNs) are predisposed to capture high-frequency features in images, while Transformers exhibit a contrasting focus on low-frequency features. In this paper, we experimentally quantify the contrast sensitivity function of CNNs and compare it with that of the human visual system, informed by the seminal experiments of Mannos and Sakrison. Leveraging these insights, we propose the Wavelet-Guided Spectral Pooling Module (WSPM) to enhance and balance image features across the frequency domain. To further emulate the human visual system, we introduce the Frequency Domain Enhanced Receptive Field Block (FE-RFB), which integrates WSPM to extract enriched features from the frequency domain. Building on these innovations, we develop FE-UNet, a model that utilizes SAM2 as its backbone and incorporates Hiera-Large as a pre-trained block, designed to enhance generalization capabilities while ensuring high segmentation accuracy. Experimental results demonstrate that FE-UNet achieves state-of-the-art performance in diverse tasks, including marine animal and polyp segmentation, underscoring its versatility and effectiveness.

### FairT2I: Mitigating Social Bias in Text-to-Image Generation via Large Language Model-Assisted Detection and Attribute Rebalancing 
[[arxiv](https://arxiv.org/abs/2502.03826)] [[cool](https://papers.cool/arxiv/2502.03826)] [[pdf](https://arxiv.org/pdf/2502.03826)]
> **Authors**: Jinya Sakurai,Issei Sato
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: The proliferation of Text-to-Image (T2I) models has revolutionized content creation, providing powerful tools for diverse applications ranging from artistic expression to educational material development and marketing. Despite these technological advancements, significant ethical concerns arise from these models' reliance on large-scale datasets that often contain inherent societal biases. These biases are further amplified when AI-generated content is included in training data, potentially reinforcing and perpetuating stereotypes in the generated outputs. In this paper, we introduce FairT2I, a novel framework that harnesses large language models to detect and mitigate social biases in T2I generation. Our framework comprises two key components: (1) an LLM-based bias detection module that identifies potential social biases in generated images based on text prompts, and (2) an attribute rebalancing module that fine-tunes sensitive attributes within the T2I model to mitigate identified biases. Our extensive experiments across various T2I models and datasets show that FairT2I can significantly reduce bias while maintaining high-quality image generation. We conducted both qualitative user studies and quantitative non-parametric analyses in the generated image feature space, building upon the occupational dataset introduced in the Stable Bias study. Our results show that FairT2I successfully mitigates social biases and enhances the diversity of sensitive attributes in generated images. We further demonstrate, using the P2 dataset, that our framework can detect subtle biases that are challenging for human observers to perceive, extending beyond occupation-related prompts. On the basis of these findings, we introduce a new benchmark dataset for evaluating bias in T2I models.

## 数据结构和算法(cs.DS:Data Structures and Algorithms)

### Knowing When to Stop Matters: A Unified Algorithm for Online Conversion under Horizon Uncertainty 
[[arxiv](https://arxiv.org/abs/2502.03817)] [[cool](https://papers.cool/arxiv/2502.03817)] [[pdf](https://arxiv.org/pdf/2502.03817)]
> **Authors**: Yanzhao Wang,Hasti Nourmohammadi Sigaroudi,Bo Sun,Omid Ardakanian,Xiaoqi Tan
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 36 pages, 6 figures
- **标题**: None
- **领域**: 数据结构和算法,机器学习
- **Abstract**: This paper investigates the online conversion problem, which involves sequentially trading a divisible resource (e.g., energy) under dynamically changing prices to maximize profit. A key challenge in online conversion is managing decisions under horizon uncertainty, where the duration of trading is either known, revealed partway, or entirely unknown. We propose a unified algorithm that achieves optimal competitive guarantees across these horizon models, accounting for practical constraints such as box constraints, which limit the maximum allowable trade per step. Additionally, we extend the algorithm to a learning-augmented version, leveraging horizon predictions to adaptively balance performance: achieving near-optimal results when predictions are accurate while maintaining strong guarantees when predictions are unreliable. These results advance the understanding of online conversion under various degrees of horizon uncertainty and provide more practical strategies to address real world constraints.

## 人机交互(cs.HC:Human-Computer Interaction)

### Ancient Greek Technology: An Immersive Learning Use Case Described Using a Co-Intelligent Custom ChatGPT Assistant 
[[arxiv](https://arxiv.org/abs/2502.04110)] [[cool](https://papers.cool/arxiv/2502.04110)] [[pdf](https://arxiv.org/pdf/2502.04110)]
> **Authors**: Vlasis Kasapakis,Leonel Morgado
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 5 pages, presented at the 2024 IEEE 3rd International Conference on Intelligent Reality (ICIR 2024), 6th of December, 2024
- **标题**: None
- **领域**: 人机交互,人工智能
- **Abstract**: Achieving consistency in immersive learning case descriptions is essential but challenging due to variations in research focus, methodology, and researchers' background. We address these challenges by leveraging the Immersive Learning Case Sheet (ILCS), a methodological instrument to standardize case descriptions, that we applied to an immersive learning case on ancient Greek technology in VRChat. Research team members had differing levels of familiarity with the ILCS and the case content, so we developed a custom ChatGPT assistant to facilitate consistent terminology and process alignment across the team. This paper constitutes an example of how structured case reports can be a novel contribution to immersive learning literature. Our findings demonstrate how the ILCS supports structured reflection and interpretation of the case. Further we report that the use of a ChatGPT assistant significantly sup-ports the coherence and quality of the team members development of the final ILCS. This exposes the potential of employing AI-driven tools to enhance collaboration and standardization of research practices in qualitative educational research. However, we also discuss the limitations and challenges, including reliance on AI for interpretive tasks and managing varied levels of expertise within the team. This study thus provides insights into the practical application of AI in standardizing immersive learning research processes.

### VTutor: An Open-Source SDK for Generative AI-Powered Animated Pedagogical Agents with Multi-Media Output 
[[arxiv](https://arxiv.org/abs/2502.04103)] [[cool](https://papers.cool/arxiv/2502.04103)] [[pdf](https://arxiv.org/pdf/2502.04103)]
> **Authors**: Eason Chen,Chenyu Lin,Xinyi Tang,Aprille Xi,Canwen Wang,Jionghao Lin,Kenneth R Koedinger
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 人机交互,人工智能,软件工程
- **Abstract**: The rapid evolution of large language models (LLMs) has transformed human-computer interaction (HCI), but the interaction with LLMs is currently mainly focused on text-based interactions, while other multi-model approaches remain under-explored. This paper introduces VTutor, an open-source Software Development Kit (SDK) that combines generative AI with advanced animation technologies to create engaging, adaptable, and realistic APAs for human-AI multi-media interactions. VTutor leverages LLMs for real-time personalized feedback, advanced lip synchronization for natural speech alignment, and WebGL rendering for seamless web integration. Supporting various 2D and 3D character models, VTutor enables researchers and developers to design emotionally resonant, contextually adaptive learning agents. This toolkit enhances learner engagement, feedback receptivity, and human-AI interaction while promoting trustworthy AI principles in education. VTutor sets a new standard for next-generation APAs, offering an accessible, scalable solution for fostering meaningful and immersive human-AI interaction experiences. The VTutor project is open-sourced and welcomes community-driven contributions and showcases.

### Understanding and Supporting Formal Email Exchange by Answering AI-Generated Questions 
[[arxiv](https://arxiv.org/abs/2502.03804)] [[cool](https://papers.cool/arxiv/2502.03804)] [[pdf](https://arxiv.org/pdf/2502.03804)]
> **Authors**: Yusuke Miura,Chi-Lan Yang,Masaki Kuribayashi,Keigo Matsumoto,Hideaki Kuzuoka,Shigeo Morishima
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 人机交互,人工智能
- **Abstract**: Replying to formal emails is time-consuming and cognitively demanding, as it requires crafting polite phrasing and providing an adequate response to the sender's demands. Although systems with Large Language Models (LLMs) were designed to simplify the email replying process, users still need to provide detailed prompts to obtain the expected output. Therefore, we proposed and evaluated an LLM-powered question-and-answer (QA)-based approach for users to reply to emails by answering a set of simple and short questions generated from the incoming email. We developed a prototype system, ResQ, and conducted controlled and field experiments with 12 and 8 participants. Our results demonstrated that the QA-based approach improves the efficiency of replying to emails and reduces workload while maintaining email quality, compared to a conventional prompt-based approach that requires users to craft appropriate prompts to obtain email drafts. We discuss how the QA-based approach influences the email reply process and interpersonal relationship dynamics, as well as the opportunities and challenges associated with using a QA-based approach in AI-mediated communication.

## 信息检索(cs.IR:Information Retrieval)

### Cross-Encoder Rediscovers a Semantic Variant of BM25 
[[arxiv](https://arxiv.org/abs/2502.04645)] [[cool](https://papers.cool/arxiv/2502.04645)] [[pdf](https://arxiv.org/pdf/2502.04645)]
> **Authors**: Meng Lu,Catherine Chen,Carsten Eickhoff
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,人工智能
- **Abstract**: Neural Ranking Models (NRMs) have rapidly advanced state-of-the-art performance on information retrieval tasks. In this work, we investigate a Cross-Encoder variant of MiniLM to determine which relevance features it computes and where they are stored. We find that it employs a semantic variant of the traditional BM25 in an interpretable manner, featuring localized components: (1) Transformer attention heads that compute soft term frequency while controlling for term saturation and document length effects, and (2) a low-rank component of its embedding matrix that encodes inverse document frequency information for the vocabulary. This suggests that the Cross-Encoder uses the same fundamental mechanisms as BM25, but further leverages their capacity to capture semantics for improved retrieval performance. The granular understanding lays the groundwork for model editing to enhance model transparency, addressing safety concerns, and improving scalability in training and real-world applications.

## 机器学习(cs.LG:Machine Learning)

### Importance Sampling via Score-based Generative Models 
[[arxiv](https://arxiv.org/abs/2502.04646)] [[cool](https://papers.cool/arxiv/2502.04646)] [[pdf](https://arxiv.org/pdf/2502.04646)]
> **Authors**: Heasung Kim,Taekyun Lee,Hyeji Kim,Gustavo de Veciana
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 18 pages
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Importance sampling, which involves sampling from a probability density function (PDF) proportional to the product of an importance weight function and a base PDF, is a powerful technique with applications in variance reduction, biased or customized sampling, data augmentation, and beyond. Inspired by the growing availability of score-based generative models (SGMs), we propose an entirely training-free Importance sampling framework that relies solely on an SGM for the base PDF. Our key innovation is realizing the importance sampling process as a backward diffusion process, expressed in terms of the score function of the base PDF and the specified importance weight function--both readily available--eliminating the need for any additional training. We conduct a thorough analysis demonstrating the method's scalability and effectiveness across diverse datasets and tasks, including importance sampling for industrial and natural images with neural importance weight functions. The training-free aspect of our method is particularly compelling in real-world scenarios where a single base distribution underlies multiple biased sampling tasks, each requiring a different importance weight function. To the best of our knowledge our approach is the first importance sampling framework to achieve this.

### Confidence Elicitation: A New Attack Vector for Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.04643)] [[cool](https://papers.cool/arxiv/2502.04643)] [[pdf](https://arxiv.org/pdf/2502.04643)]
> **Authors**: Brian Formento,Chuan Sheng Foo,See-Kiong Ng
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: Published in ICLR 2025. The code is publicly available at https://github.com/Aniloid2/Confidence_Elicitation_Attacks
- **标题**: None
- **领域**: 机器学习,计算语言学,密码学和安全
- **Abstract**: A fundamental issue in deep learning has been adversarial robustness. As these systems have scaled, such issues have persisted. Currently, large language models (LLMs) with billions of parameters suffer from adversarial attacks just like their earlier, smaller counterparts. However, the threat models have changed. Previously, having gray-box access, where input embeddings or output logits/probabilities were visible to the user, might have been reasonable. However, with the introduction of closed-source models, no information about the model is available apart from the generated output. This means that current black-box attacks can only utilize the final prediction to detect if an attack is successful. In this work, we investigate and demonstrate the potential of attack guidance, akin to using output probabilities, while having only black-box access in a classification setting. This is achieved through the ability to elicit confidence from the model. We empirically show that the elicited confidence is calibrated and not hallucinated for current LLMs. By minimizing the elicited confidence, we can therefore increase the likelihood of misclassification. Our new proposed paradigm demonstrates promising state-of-the-art results on three datasets across two models (LLaMA-3-8B-Instruct and Mistral-7B-Instruct-V0.3) when comparing our technique to existing hard-label black-box attack methods that introduce word-level substitutions.

### The $α$-Alternator: Dynamic Adaptation To Varying Noise Levels In Sequences Using The Vendi Score For Improved Robustness and Performance 
[[arxiv](https://arxiv.org/abs/2502.04593)] [[cool](https://papers.cool/arxiv/2502.04593)] [[pdf](https://arxiv.org/pdf/2502.04593)]
> **Authors**: Mohammad Reza Rezaei,Adji Bousso Dieng
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: The codebase will be made available upon publication. This paper is dedicated to Patrice Lumumba
- **标题**: None
- **领域**: 机器学习,人工智能,神经和进化计算,机器学习
- **Abstract**: Current state-of-the-art dynamical models, such as Mamba, assume the same level of noisiness for all elements of a given sequence, which limits their performance on noisy temporal data. In this paper, we introduce the $α$-Alternator, a novel generative model for time-dependent data that dynamically adapts to the complexity introduced by varying noise levels in sequences. The $α$-Alternator leverages the Vendi Score (VS), a flexible similarity-based diversity metric, to adjust, at each time step $t$, the influence of the sequence element at time $t$ and the latent representation of the dynamics up to that time step on the predicted future dynamics. This influence is captured by a parameter that is learned and shared across all sequences in a given dataset. The sign of this parameter determines the direction of influence. A negative value indicates a noisy dataset, where a sequence element that increases the VS is considered noisy, and the model relies more on the latent history when processing that element. Conversely, when the parameter is positive, a sequence element that increases the VS is considered informative, and the $α$-Alternator relies more on this new input than on the latent history when updating its predicted latent dynamics. The $α$-Alternator is trained using a combination of observation masking and Alternator loss minimization. Masking simulates varying noise levels in sequences, enabling the model to be more robust to these fluctuations and improving its performance in trajectory prediction, imputation, and forecasting. Our experimental results demonstrate that the $α$-Alternator outperforms both Alternators and state-of-the-art state-space models across neural decoding and time-series forecasting benchmarks.

### CAMEF: Causal-Augmented Multi-Modality Event-Driven Financial Forecasting by Integrating Time Series Patterns and Salient Macroeconomic Announcements 
[[arxiv](https://arxiv.org/abs/2502.04592)] [[cool](https://papers.cool/arxiv/2502.04592)] [[pdf](https://arxiv.org/pdf/2502.04592)]
> **Authors**: Yang Zhang,Wenbo Yang,Jun Wang,Qiang Ma,Jie Xiong
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算工程、金融和科学
- **Abstract**: Accurately forecasting the impact of macroeconomic events is critical for investors and policymakers. Salient events like monetary policy decisions and employment reports often trigger market movements by shaping expectations of economic growth and risk, thereby establishing causal relationships between events and market behavior. Existing forecasting methods typically focus either on textual analysis or time-series modeling, but fail to capture the multi-modal nature of financial markets and the causal relationship between events and price movements. To address these gaps, we propose CAMEF (Causal-Augmented Multi-Modality Event-Driven Financial Forecasting), a multi-modality framework that effectively integrates textual and time-series data with a causal learning mechanism and an LLM-based counterfactual event augmentation technique for causal-enhanced financial forecasting. Our contributions include: (1) a multi-modal framework that captures causal relationships between policy texts and historical price data; (2) a new financial dataset with six types of macroeconomic releases from 2008 to April 2024, and high-frequency real trading data for five key U.S. financial assets; and (3) an LLM-based counterfactual event augmentation strategy. We compare CAMEF to state-of-the-art transformer-based time-series and multi-modal baselines, and perform ablation studies to validate the effectiveness of the causal learning mechanism and event types.

### Rethinking Oversmoothing in Graph Neural Networks: A Rank-Based Perspective 
[[arxiv](https://arxiv.org/abs/2502.04591)] [[cool](https://papers.cool/arxiv/2502.04591)] [[pdf](https://arxiv.org/pdf/2502.04591)]
> **Authors**: Kaicheng Zhang,Piero Deidda,Desmond Higham,Francesco Tudisco
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Oversmoothing is a fundamental challenge in graph neural networks (GNNs): as the number of layers increases, node embeddings become increasingly similar, and model performance drops sharply. Traditionally, oversmoothing has been quantified using metrics that measure the similarity of neighbouring node features, such as the Dirichlet energy. While these metrics are related to oversmoothing, we argue they have critical limitations and fail to reliably capture oversmoothing in realistic scenarios. For instance, they provide meaningful insights only for very deep networks and under somewhat strict conditions on the norm of network weights and feature representations. As an alternative, we propose measuring oversmoothing by examining the numerical or effective rank of the feature representations. We provide theoretical support for this approach, demonstrating that the numerical rank of feature representations converges to one for a broad family of nonlinear activation functions under the assumption of nonnegative trained weights. To the best of our knowledge, this is the first result that proves the occurrence of oversmoothing without assumptions on the boundedness of the weight matrices. Along with the theoretical findings, we provide extensive numerical evaluation across diverse graph architectures. Our results show that rank-based metrics consistently capture oversmoothing, whereas energy-based metrics often fail. Notably, we reveal that a significant drop in the rank aligns closely with performance degradation, even in scenarios where energy metrics remain unchanged.

### Overcoming Fake Solutions in Semi-Dual Neural Optimal Transport: A Smoothing Approach for Learning the Optimal Transport Plan 
[[arxiv](https://arxiv.org/abs/2502.04583)] [[cool](https://papers.cool/arxiv/2502.04583)] [[pdf](https://arxiv.org/pdf/2502.04583)]
> **Authors**: Jaemoo Choi,Jaewoong Choi,Dohyun Kwon
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 18 pages, 10 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We address the convergence problem in learning the Optimal Transport (OT) map, where the OT Map refers to a map from one distribution to another while minimizing the transport cost. Semi-dual Neural OT, a widely used approach for learning OT Maps with neural networks, often generates fake solutions that fail to transfer one distribution to another accurately. We identify a sufficient condition under which the max-min solution of Semi-dual Neural OT recovers the true OT Map. Moreover, to address cases when this sufficient condition is not satisfied, we propose a novel method, OTP, which learns both the OT Map and the Optimal Transport Plan, representing the optimal coupling between two distributions. Under sharp assumptions on the distributions, we prove that our model eliminates the fake solution issue and correctly solves the OT problem. Our experiments show that the OTP model recovers the optimal transport map where existing methods fail and outperforms current OT-based models in image-to-image translation tasks. Notably, the OTP model can learn stochastic transport maps when deterministic OT Maps do not exist, such as one-to-many tasks like colorization.

### Technical Debt in In-Context Learning: Diminishing Efficiency in Long Context 
[[arxiv](https://arxiv.org/abs/2502.04580)] [[cool](https://papers.cool/arxiv/2502.04580)] [[pdf](https://arxiv.org/pdf/2502.04580)]
> **Authors**: Taejong Joo,Diego Klabjan
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Transformers have demonstrated remarkable in-context learning (ICL) capabilities, adapting to new tasks by simply conditioning on demonstrations without parameter updates. Compelling empirical and theoretical evidence suggests that ICL, as a general-purpose learner, could outperform task-specific models. However, it remains unclear to what extent the transformers optimally learn in-context compared to principled learning algorithms. To bridge this gap, we introduce a new framework for quantifying optimality of ICL as a learning algorithm in stylized settings. Our findings reveal a striking dichotomy: while ICL initially matches the efficiency of a Bayes optimal estimator, its efficiency significantly deteriorates in long context. Through an information-theoretic analysis, we show that the diminishing efficiency is inherent to ICL. These results clarify the trade-offs in adopting ICL as a universal problem solver, motivating a new generation of on-the-fly adaptive methods without the diminishing efficiency.

### Position-aware Automatic Circuit Discovery 
[[arxiv](https://arxiv.org/abs/2502.04577)] [[cool](https://papers.cool/arxiv/2502.04577)] [[pdf](https://arxiv.org/pdf/2502.04577)]
> **Authors**: Tal Haklay,Hadas Orgad,David Bau,Aaron Mueller,Yonatan Belinkov
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: :68T50ACM Class:I.2.7
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: A widely used strategy to discover and understand language model mechanisms is circuit analysis. A circuit is a minimal subgraph of a model's computation graph that executes a specific task. We identify a gap in existing circuit discovery methods: they assume circuits are position-invariant, treating model components as equally relevant across input positions. This limits their ability to capture cross-positional interactions or mechanisms that vary across positions. To address this gap, we propose two improvements to incorporate positionality into circuits, even on tasks containing variable-length examples. First, we extend edge attribution patching, a gradient-based method for circuit discovery, to differentiate between token positions. Second, we introduce the concept of a dataset schema, which defines token spans with similar semantics across examples, enabling position-aware circuit discovery in datasets with variable length examples. We additionally develop an automated pipeline for schema generation and application using large language models. Our approach enables fully automated discovery of position-sensitive circuits, yielding better trade-offs between circuit size and faithfulness compared to prior work.

### Self-Regulation and Requesting Interventions 
[[arxiv](https://arxiv.org/abs/2502.04576)] [[cool](https://papers.cool/arxiv/2502.04576)] [[pdf](https://arxiv.org/pdf/2502.04576)]
> **Authors**: So Yeon Min,Yue Wu,Jimin Sun,Max Kaufmann,Fahim Tajwar,Yonatan Bisk,Ruslan Salakhutdinov
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Human intelligence involves metacognitive abilities like self-regulation, recognizing limitations, and seeking assistance only when needed. While LLM Agents excel in many domains, they often lack this awareness. Overconfident agents risk catastrophic failures, while those that seek help excessively hinder efficiency. A key challenge is enabling agents with a limited intervention budget $C$ is to decide when to request assistance. In this paper, we propose an offline framework that trains a "helper" policy to request interventions, such as more powerful models or test-time compute, by combining LLM-based process reward models (PRMs) with tabular reinforcement learning. Using state transitions collected offline, we score optimal intervention timing with PRMs and train the helper model on these labeled trajectories. This offline approach significantly reduces costly intervention calls during training. Furthermore, the integration of PRMs with tabular RL enhances robustness to off-policy data while avoiding the inefficiencies of deep RL. We empirically find that our method delivers optimal helper behavior.

### Zero-shot Meta-learning for Tabular Prediction Tasks with Adversarially Pre-trained Transformer 
[[arxiv](https://arxiv.org/abs/2502.04573)] [[cool](https://papers.cool/arxiv/2502.04573)] [[pdf](https://arxiv.org/pdf/2502.04573)]
> **Authors**: Yulun Wu,Doron L. Bergman
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: We present an Adversarially Pre-trained Transformer (APT) that is able to perform zero-shot meta-learning on tabular prediction tasks without pre-training on any real-world dataset, extending on the recent development of Prior-Data Fitted Networks (PFNs) and TabPFN. Specifically, APT is pre-trained with adversarial synthetic data agents, who continue to shift their underlying data generating distribution and deliberately challenge the model with different synthetic datasets. In addition, we propose a mixture block architecture that is able to handle classification tasks with arbitrary number of classes, addressing the class size limitation -- a crucial weakness of prior deep tabular zero-shot learners. In experiments, we show that our framework matches state-of-the-art performance on small classification tasks without filtering on dataset characteristics such as number of classes and number of missing values, while maintaining an average runtime under one second. On common benchmark dataset suites in both classification and regression, we show that adversarial pre-training was able to enhance TabPFN's performance. In our analysis, we demonstrate that the adversarial synthetic data agents were able to generate a more diverse collection of data compared to the ordinary random generator in TabPFN. In addition, we demonstrate that our mixture block neural design has improved generalizability and greatly accelerated pre-training.

### Learning Semantics-aware Search Operators for Genetic Programming 
[[arxiv](https://arxiv.org/abs/2502.04568)] [[cool](https://papers.cool/arxiv/2502.04568)] [[pdf](https://arxiv.org/pdf/2502.04568)]
> **Authors**: Piotr Wyrwiński,Krzysztof Krawiec
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: Submitted to GECCO 2025
- **标题**: None
- **领域**: 机器学习,神经和进化计算
- **Abstract**: Fitness landscapes in test-based program synthesis are known to be extremely rugged, with even minimal modifications of programs often leading to fundamental changes in their behavior and, consequently, fitness values. Relying on fitness as the only guidance in iterative search algorithms like genetic programming is thus unnecessarily limiting, especially when combined with purely syntactic search operators that are agnostic about their impact on program behavior. In this study, we propose a semantics-aware search operator that steers the search towards candidate programs that are valuable not only actually (high fitness) but also only potentially, i.e. are likely to be turned into high-quality solutions even if their current fitness is low. The key component of the method is a graph neural network that learns to model the interactions between program instructions and processed data, and produces a saliency map over graph nodes that represents possible search decisions. When applied to a suite of symbolic regression benchmarks, the proposed method outperforms conventional tree-based genetic programming and the ablated variant of the method.

### Private Federated Learning In Real World Application -- A Case Study 
[[arxiv](https://arxiv.org/abs/2502.04565)] [[cool](https://papers.cool/arxiv/2502.04565)] [[pdf](https://arxiv.org/pdf/2502.04565)]
> **Authors**: An Ji,Bortik Bandyopadhyay,Congzheng Song,Natarajan Krishnaswami,Prabal Vashisht,Rigel Smiroldo,Isabel Litton,Sayantan Mahinder,Mona Chitnis,Andrew W Hill
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,密码学和安全
- **Abstract**: This paper presents an implementation of machine learning model training using private federated learning (PFL) on edge devices. We introduce a novel framework that uses PFL to address the challenge of training a model using users' private data. The framework ensures that user data remain on individual devices, with only essential model updates transmitted to a central server for aggregation with privacy guarantees. We detail the architecture of our app selection model, which incorporates a neural network with attention mechanisms and ambiguity handling through uncertainty management. Experiments conducted through off-line simulations and on device training demonstrate the feasibility of our approach in real-world scenarios. Our results show the potential of PFL to improve the accuracy of an app selection model by adapting to changes in user behavior over time, while adhering to privacy standards. The insights gained from this study are important for industries looking to implement PFL, offering a robust strategy for training a predictive model directly on edge devices while ensuring user data privacy.

### WaferLLM: A Wafer-Scale LLM Inference System 
[[arxiv](https://arxiv.org/abs/2502.04563)] [[cool](https://papers.cool/arxiv/2502.04563)] [[pdf](https://arxiv.org/pdf/2502.04563)]
> **Authors**: Congjie He,Yeqi Huang,Pei Mu,Ziming Miao,Jilong Xue,Lingxiao Ma,Fan Yang,Luo Mai
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,硬件架构,分布式、并行和集群计算,新兴技术
- **Abstract**: Emerging AI accelerators increasingly adopt wafer-scale manufacturing technologies, integrating hundreds of thousands of AI cores in a mesh-based architecture with large distributed on-chip memory (tens of GB in total) and ultra-high on-chip memory bandwidth (tens of PB/s). However, current LLM inference systems, optimized for shared memory architectures like GPUs, fail to fully exploit these accelerators. We introduce WaferLLM, the first wafer-scale LLM inference system. WaferLLM is guided by a novel PLMR model (pronounced as "Plummer") that captures the unique hardware characteristics of wafer-scale architectures. Leveraging this model, WaferLLM pioneers wafer-scale LLM parallelism, optimizing the utilization of hundreds of thousands of on-chip cores. It also introduces MeshGEMM and MeshGEMV, the first GEMM and GEMV implementations designed to scale effectively on wafer-scale accelerators. Evaluations show that WaferLLM achieves 200$\times$ better wafer-scale accelerator utilization than state-of-the-art systems. On a commodity wafer-scale accelerator, WaferLLM delivers 606$\times$ faster and 22$\times$ more energy-efficient GEMV compared to an advanced GPU. For LLMs, based on 16-bit data type, WaferLLM achieves 2700 toks/sec/req decode speed on Llama3-8B model and 840 toks/sec/req decode speed on Qwen2-72B model, which enables 39$\times$ faster decoding with 1.7$\times$ better energy efficiency. We anticipate these numbers will grow significantly as wafer-scale AI models, software, and hardware continue to mature.

### Mixture of neural operator experts for learning boundary conditions and model selection 
[[arxiv](https://arxiv.org/abs/2502.04562)] [[cool](https://papers.cool/arxiv/2502.04562)] [[pdf](https://arxiv.org/pdf/2502.04562)]
> **Authors**: Dwyer Deighan,Jonas A. Actor,Ravi G. Patel
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,数值分析,流体动力学
- **Abstract**: While Fourier-based neural operators are best suited to learning mappings between functions on periodic domains, several works have introduced techniques for incorporating non trivial boundary conditions. However, all previously introduced methods have restrictions that limit their applicability. In this work, we introduce an alternative approach to imposing boundary conditions inspired by volume penalization from numerical methods and Mixture of Experts (MoE) from machine learning. By introducing competing experts, the approach additionally allows for model selection. To demonstrate the method, we combine a spatially conditioned MoE with the Fourier based, Modal Operator Regression for Physics (MOR-Physics) neural operator and recover a nonlinear operator on a disk and quarter disk. Next, we extract a large eddy simulation (LES) model from direct numerical simulation of channel flow and show the domain decomposition provided by our approach. Finally, we train our LES model with Bayesian variational inference and obtain posterior predictive samples of flow far past the DNS simulation time horizon.

### Speeding up Speculative Decoding via Approximate Verification 
[[arxiv](https://arxiv.org/abs/2502.04557)] [[cool](https://papers.cool/arxiv/2502.04557)] [[pdf](https://arxiv.org/pdf/2502.04557)]
> **Authors**: Meiyu Zhong,Noel Teku,Ravi Tandon
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,信息论
- **Abstract**: Speculative Decoding (SD) is a recently proposed technique for faster inference using Large Language Models (LLMs). SD operates by using a smaller draft LLM for autoregressively generating a sequence of tokens and a larger target LLM for parallel verification to ensure statistical consistency. However, periodic parallel calls to the target LLM for verification prevent SD from achieving even lower latencies. We propose SPRINTER, which utilizes a low-complexity verifier trained to predict if tokens generated from a draft LLM would be accepted by the target LLM. By performing approximate sequential verification, SPRINTER does not require verification by the target LLM and is only invoked when a token is deemed unacceptable. This leads to reducing the number of calls to the larger LLM and can achieve further speedups. We present a theoretical analysis of SPRINTER, examining the statistical properties of the generated tokens, as well as the expected reduction in latency as a function of the verifier. We evaluate SPRINTER on several datasets and model pairs, demonstrating that approximate verification can still maintain high quality generation while further reducing latency. For instance, on Wiki-Summaries dataset, SPRINTER achieves a 1.7x latency speedup and requires 8.3x fewer flops relative to SD, while still generating high-quality responses when using GPT2-Small and GPT2-XL as draft/target models.

### Mechanisms of Projective Composition of Diffusion Models 
[[arxiv](https://arxiv.org/abs/2502.04549)] [[cool](https://papers.cool/arxiv/2502.04549)] [[pdf](https://arxiv.org/pdf/2502.04549)]
> **Authors**: Arwen Bradley,Preetum Nakkiran,David Berthelot,James Thornton,Joshua M. Susskind
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 9 pages, 7 figures. The first two authors contributed equally
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We study the theoretical foundations of composition in diffusion models, with a particular focus on out-of-distribution extrapolation and length-generalization. Prior work has shown that composing distributions via linear score combination can achieve promising results, including length-generalization in some cases (Du et al., 2023; Liu et al., 2022). However, our theoretical understanding of how and why such compositions work remains incomplete. In fact, it is not even entirely clear what it means for composition to "work". This paper starts to address these fundamental gaps. We begin by precisely defining one possible desired result of composition, which we call projective composition. Then, we investigate: (1) when linear score combinations provably achieve projective composition, (2) whether reverse-diffusion sampling can generate the desired composition, and (3) the conditions under which composition fails. Finally, we connect our theoretical analysis to prior empirical observations where composition has either worked or failed, for reasons that were unclear at the time.

### Agricultural Field Boundary Detection through Integration of "Simple Non-Iterative Clustering (SNIC) Super Pixels" and "Canny Edge Detection Method" 
[[arxiv](https://arxiv.org/abs/2502.04529)] [[cool](https://papers.cool/arxiv/2502.04529)] [[pdf](https://arxiv.org/pdf/2502.04529)]
> **Authors**: Artughrul Gayibov
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 4 pages, 2 figures
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: Efficient use of cultivated areas is a necessary factor for sustainable development of agriculture and ensuring food security. Along with the rapid development of satellite technologies in developed countries, new methods are being searched for accurate and operational identification of cultivated areas. In this context, identification of cropland boundaries based on spectral analysis of data obtained from satellite images is considered one of the most optimal and accurate methods in modern agriculture. This article proposes a new approach to determine the suitability and green index of cultivated areas using satellite data obtained through the "Google Earth Engine" (GEE) platform. In this approach, two powerful algorithms, "SNIC (Simple Non-Iterative Clustering) Super Pixels" and "Canny Edge Detection Method", are combined. The SNIC algorithm combines pixels in a satellite image into larger regions (super pixels) with similar characteristics, thereby providing better image analysis. The Canny Edge Detection Method detects sharp changes (edges) in the image to determine the precise boundaries of agricultural fields. This study, carried out using high-resolution multispectral data from the Sentinel-2 satellite and the Google Earth Engine JavaScript API, has shown that the proposed method is effective in accurately and reliably classifying randomly selected agricultural fields. The combined use of these two tools allows for more accurate determination of the boundaries of agricultural fields by minimizing the effects of outliers in satellite images. As a result, more accurate and reliable maps can be created for agricultural monitoring and resource management over large areas based on the obtained data. By expanding the application capabilities of cloud-based platforms and artificial intelligence methods in the agricultural field.

### Towards Cost-Effective Reward Guided Text Generation 
[[arxiv](https://arxiv.org/abs/2502.04517)] [[cool](https://papers.cool/arxiv/2502.04517)] [[pdf](https://arxiv.org/pdf/2502.04517)]
> **Authors**: Ahmad Rashid,Ruotian Wu,Rongqi Fan,Hongliang Li,Agustinus Kristiadi,Pascal Poupart
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Reward-guided text generation (RGTG) has emerged as a viable alternative to offline reinforcement learning from human feedback (RLHF). RGTG methods can align baseline language models to human preferences without further training like in standard RLHF methods. However, they rely on a reward model to score each candidate token generated by the language model at inference, incurring significant test-time overhead. Additionally, the reward model is usually only trained to score full sequences, which can lead to sub-optimal choices for partial sequences. In this work, we present a novel reward model architecture that is trained, using a Bradley-Terry loss, to prefer the optimal expansion of a sequence with just a \emph{single call} to the reward model at each step of the generation process. That is, a score for all possible candidate tokens is generated simultaneously, leading to efficient inference. We theoretically analyze various RGTG reward models and demonstrate that prior techniques prefer sub-optimal sequences compared to our method during inference. Empirically, our reward model leads to significantly faster inference than other RGTG methods. It requires fewer calls to the reward model and performs competitively compared to previous RGTG and offline RLHF methods.

### MedGNN: Towards Multi-resolution Spatiotemporal Graph Learning for Medical Time Series Classification 
[[arxiv](https://arxiv.org/abs/2502.04515)] [[cool](https://papers.cool/arxiv/2502.04515)] [[pdf](https://arxiv.org/pdf/2502.04515)]
> **Authors**: Wei Fan,Jingru Fei,Dingyu Guo,Kun Yi,Xiaozhuang Song,Haolong Xiang,Hangting Ye,Min Li
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: Accepted by WWW 2025
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Medical time series has been playing a vital role in real-world healthcare systems as valuable information in monitoring health conditions of patients. Accurate classification for medical time series, e.g., Electrocardiography (ECG) signals, can help for early detection and diagnosis. Traditional methods towards medical time series classification rely on handcrafted feature extraction and statistical methods; with the recent advancement of artificial intelligence, the machine learning and deep learning methods have become more popular. However, existing methods often fail to fully model the complex spatial dynamics under different scales, which ignore the dynamic multi-resolution spatial and temporal joint inter-dependencies. Moreover, they are less likely to consider the special baseline wander problem as well as the multi-view characteristics of medical time series, which largely hinders their prediction performance. To address these limitations, we propose a Multi-resolution Spatiotemporal Graph Learning framework, MedGNN, for medical time series classification. Specifically, we first propose to construct multi-resolution adaptive graph structures to learn dynamic multi-scale embeddings. Then, to address the baseline wander problem, we propose Difference Attention Networks to operate self-attention mechanisms on the finite difference for temporal modeling. Moreover, to learn the multi-view characteristics, we utilize the Frequency Convolution Networks to capture complementary information of medical time series from the frequency domain. In addition, we introduce the Multi-resolution Graph Transformer architecture to model the dynamic dependencies and fuse the information from different resolutions. Finally, we have conducted extensive experiments on multiple medical real-world datasets that demonstrate the superior performance of our method. Our Code is available.

### Revisiting Intermediate-Layer Matching in Knowledge Distillation: Layer-Selection Strategy Doesn't Matter (Much) 
[[arxiv](https://arxiv.org/abs/2502.04499)] [[cool](https://papers.cool/arxiv/2502.04499)] [[pdf](https://arxiv.org/pdf/2502.04499)]
> **Authors**: Zony Yu,Yuqiao Wen,Lili Mou
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: :I.2.7; I.2.6
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Knowledge distillation (KD) is a popular method of transferring knowledge from a large "teacher" model to a small "student" model. KD can be divided into two categories: prediction matching and intermediate-layer matching. We explore an intriguing phenomenon: layer-selection strategy does not matter (much) in intermediate-layer matching. In this paper, we show that seemingly nonsensical matching strategies such as matching the teacher's layers in reverse still result in surprisingly good student performance. We provide an interpretation for this phenomenon by examining the angles between teacher layers viewed from the student's perspective.

### Discovering Physics Laws of Dynamical Systems via Invariant Function Learning 
[[arxiv](https://arxiv.org/abs/2502.04495)] [[cool](https://papers.cool/arxiv/2502.04495)] [[pdf](https://arxiv.org/pdf/2502.04495)]
> **Authors**: Shurui Gui,Xiner Li,Shuiwang Ji
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We consider learning underlying laws of dynamical systems governed by ordinary differential equations (ODE). A key challenge is how to discover intrinsic dynamics across multiple environments while circumventing environment-specific mechanisms. Unlike prior work, we tackle more complex environments where changes extend beyond function coefficients to entirely different function forms. For example, we demonstrate the discovery of ideal pendulum's natural motion $α^2 \sin{θ_t}$ by observing pendulum dynamics in different environments, such as the damped environment $α^2 \sin(θ_t) - ρω_t$ and powered environment $α^2 \sin(θ_t) + ρ\frac{ω_t}{\left|ω_t\right|}$. Here, we formulate this problem as an \emph{invariant function learning} task and propose a new method, known as \textbf{D}isentanglement of \textbf{I}nvariant \textbf{F}unctions (DIF), that is grounded in causal analysis. We propose a causal graph and design an encoder-decoder hypernetwork that explicitly disentangles invariant functions from environment-specific dynamics. The discovery of invariant functions is guaranteed by our information-based principle that enforces the independence between extracted invariant functions and environments. Quantitative comparisons with meta-learning and invariant learning baselines on three ODE systems demonstrate the effectiveness and efficiency of our method. Furthermore, symbolic regression explanation results highlight the ability of our framework to uncover intrinsic laws.

### Provable Sample-Efficient Transfer Learning Conditional Diffusion Models via Representation Learning 
[[arxiv](https://arxiv.org/abs/2502.04491)] [[cool](https://papers.cool/arxiv/2502.04491)] [[pdf](https://arxiv.org/pdf/2502.04491)]
> **Authors**: Ziheng Cheng,Tianyu Xie,Shiyue Zhang,Cheng Zhang
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,统计理论,机器学习
- **Abstract**: While conditional diffusion models have achieved remarkable success in various applications, they require abundant data to train from scratch, which is often infeasible in practice. To address this issue, transfer learning has emerged as an essential paradigm in small data regimes. Despite its empirical success, the theoretical underpinnings of transfer learning conditional diffusion models remain unexplored. In this paper, we take the first step towards understanding the sample efficiency of transfer learning conditional diffusion models through the lens of representation learning. Inspired by practical training procedures, we assume that there exists a low-dimensional representation of conditions shared across all tasks. Our analysis shows that with a well-learned representation from source tasks, the samplecomplexity of target tasks can be reduced substantially. In addition, we investigate the practical implications of our theoretical results in several real-world applications of conditional diffusion models. Numerical experiments are also conducted to verify our results.

### CNN Autoencoders for Hierarchical Feature Extraction and Fusion in Multi-sensor Human Activity Recognition 
[[arxiv](https://arxiv.org/abs/2502.04489)] [[cool](https://papers.cool/arxiv/2502.04489)] [[pdf](https://arxiv.org/pdf/2502.04489)]
> **Authors**: Saeed Arabzadeh,Farshad Almasganj,Mohammad Mahdi Ahmadi
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 10 pages, 9 figures
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Deep learning methods have been widely used for Human Activity Recognition (HAR) using recorded signals from Iner-tial Measurement Units (IMUs) sensors that are installed on various parts of the human body. For this type of HAR, sev-eral challenges exist, the most significant of which is the analysis of multivarious IMU sensors data. Here, we introduce a Hierarchically Unsupervised Fusion (HUF) model designed to extract, and fuse features from IMU sensors data via a hybrid structure of Convolutional Neural Networks (CNN)s and Autoencoders (AE)s. First, we design a stack CNN-AE to embed short-time signals into sets of high dimensional features. Second, we develop another CNN-AE network to locally fuse the extracted features from each sensor unit. Finally, we unify all the sensor features through a third CNN-AE architecture as globally feature fusion to create a unique feature set. Additionally, we analyze the effects of varying the model hyperparameters. The best results are achieved with eight convolutional layers in each AE. Furthermore, it is determined that an overcomplete AE with 256 kernels in the code layer is suitable for feature extraction in the first block of the proposed HUF model; this number reduces to 64 in the last block of the model to customize the size of the applied features to the classifier. The tuned model is applied to the UCI-HAR, DaLiAc, and Parkinson's disease gait da-tasets, achieving the classification accuracies of 97%, 97%, and 88%, respectively, which are nearly 3% better com-pared to the state-of-the-art supervised methods.

### Iterative Importance Fine-tuning of Diffusion Models 
[[arxiv](https://arxiv.org/abs/2502.04468)] [[cool](https://papers.cool/arxiv/2502.04468)] [[pdf](https://arxiv.org/pdf/2502.04468)]
> **Authors**: Alexander Denker,Shreyas Padhy,Francisco Vargas,Johannes Hertrich
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: :68T07ACM Class:I.4.9; I.2.6
- **标题**: None
- **领域**: 机器学习,图像和视频处理,可能性
- **Abstract**: Diffusion models are an important tool for generative modelling, serving as effective priors in applications such as imaging and protein design. A key challenge in applying diffusion models for downstream tasks is efficiently sampling from resulting posterior distributions, which can be addressed using the $h$-transform. This work introduces a self-supervised algorithm for fine-tuning diffusion models by estimating the $h$-transform, enabling amortised conditional sampling. Our method iteratively refines the $h$-transform using a synthetic dataset resampled with path-based importance weights. We demonstrate the effectiveness of this framework on class-conditional sampling and reward fine-tuning for text-to-image diffusion models.

### FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks 
[[arxiv](https://arxiv.org/abs/2502.04465)] [[cool](https://papers.cool/arxiv/2502.04465)] [[pdf](https://arxiv.org/pdf/2502.04465)]
> **Authors**: Luca Della Libera,Francesco Paissan,Cem Subakan,Mirco Ravanelli
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 18 pages
- **标题**: None
- **领域**: 机器学习,人工智能,声音,音频和语音处理
- **Abstract**: Large language models have revolutionized natural language processing through self-supervised pretraining on massive datasets. Inspired by this success, researchers have explored adapting these methods to speech by discretizing continuous audio into tokens using neural audio codecs. However, existing approaches face limitations, including high bitrates, the loss of either semantic or acoustic information, and the reliance on multi-codebook designs when trying to capture both, which increases architectural complexity for downstream tasks. To address these challenges, we introduce FocalCodec, an efficient low-bitrate codec based on focal modulation that utilizes a single binary codebook to compress speech between 0.16 and 0.65 kbps. FocalCodec delivers competitive performance in speech resynthesis and voice conversion at lower bitrates than the current state-of-the-art, while effectively handling multilingual speech and noisy environments. Evaluation on downstream tasks shows that FocalCodec successfully preserves sufficient semantic and acoustic information, while also being well-suited for generative modeling. Demo samples, code and checkpoints are available at https://lucadellalib.github.io/focalcodec-web/.

### Training Language Models to Reason Efficiently 
[[arxiv](https://arxiv.org/abs/2502.04463)] [[cool](https://papers.cool/arxiv/2502.04463)] [[pdf](https://arxiv.org/pdf/2502.04463)]
> **Authors**: Daman Arora,Andrea Zanette
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Scaling model size and training data has led to great advances in the performance of Large Language Models (LLMs). However, the diminishing returns of this approach necessitate alternative methods to improve model capabilities, particularly in tasks requiring advanced reasoning. Large reasoning models, which leverage long chain-of-thoughts, bring unprecedented breakthroughs in problem-solving capabilities but at a substantial deployment cost associated to longer generations. Reducing inference costs is crucial for the economic feasibility, user experience, and environmental sustainability of these models. In this work, we propose to train large reasoning models to reason efficiently. More precisely, we use reinforcement learning (RL) to train reasoning models to dynamically allocate inference-time compute based on task complexity. Our method incentivizes models to minimize unnecessary computational overhead while maintaining accuracy, thereby achieving substantial efficiency gains. It enables the derivation of a family of reasoning models with varying efficiency levels, controlled via a single hyperparameter. Experiments on two open-weight large reasoning models demonstrate significant reductions in inference cost while preserving most of the accuracy.

### Primary Care Diagnoses as a Reliable Predictor for Orthopedic Surgical Interventions 
[[arxiv](https://arxiv.org/abs/2502.04423)] [[cool](https://papers.cool/arxiv/2502.04423)] [[pdf](https://arxiv.org/pdf/2502.04423)]
> **Authors**: Khushboo Verma,Alan Michels,Ergi Gumusaneli,Shilpa Chitnis,Smita Sinha Kumar,Christopher Thompson,Lena Esmail,Guruprasath Srinivasan,Chandini Panchada,Sushovan Guha,Satwant Kumar
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: :I.2.6; I.2.7; J.3; H.2.8
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Referral workflow inefficiencies, including misaligned referrals and delays, contribute to suboptimal patient outcomes and higher healthcare costs. In this study, we investigated the possibility of predicting procedural needs based on primary care diagnostic entries, thereby improving referral accuracy, streamlining workflows, and providing better care to patients. A de-identified dataset of 2,086 orthopedic referrals from the University of Texas Health at Tyler was analyzed using machine learning models built on Base General Embeddings (BGE) for semantic extraction. To ensure real-world applicability, noise tolerance experiments were conducted, and oversampling techniques were employed to mitigate class imbalance. The selected optimum and parsimonious embedding model demonstrated high predictive accuracy (ROC-AUC: 0.874, Matthews Correlation Coefficient (MCC): 0.540), effectively distinguishing patients requiring surgical intervention. Dimensionality reduction techniques confirmed the model's ability to capture meaningful clinical relationships. A threshold sensitivity analysis identified an optimal decision threshold (0.30) to balance precision and recall, maximizing referral efficiency. In the predictive modeling analysis, the procedure rate increased from 11.27% to an optimal 60.1%, representing a 433% improvement with significant implications for operational efficiency and healthcare revenue. The results of our study demonstrate that referral optimization can enhance primary and surgical care integration. Through this approach, precise and timely predictions of procedural requirements can be made, thereby minimizing delays, improving surgical planning, and reducing administrative burdens. In addition, the findings highlight the potential of clinical decision support as a scalable solution for improving patient outcomes and the efficiency of the healthcare system.

### KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache Quantization for Efficient and Nearly Lossless LLM Inference 
[[arxiv](https://arxiv.org/abs/2502.04420)] [[cool](https://papers.cool/arxiv/2502.04420)] [[pdf](https://arxiv.org/pdf/2502.04420)]
> **Authors**: Xing Li,Zeyu Xing,Yiming Li,Linping Qu,Hui-Ling Zhen,Wulong Liu,Yiwu Yao,Sinno Jialin Pan,Mingxuan Yuan
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 36 pages. Code: https://github.com/cmd2001/KVTuner
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: KV cache quantization can improve Large Language Models (LLMs) inference throughput and latency in long contexts and large batch-size scenarios while preserving LLMs effectiveness. However, current methods have three unsolved issues: overlooking layer-wise sensitivity to KV cache quantization, high overhead of online fine-grained decision-making, and low flexibility to different LLMs and constraints. Therefore, we thoroughly analyze the inherent correlation of layer-wise transformer attention patterns to KV cache quantization errors and study why key cache is more important than value cache for quantization error reduction. We further propose a simple yet effective framework KVTuner to adaptively search for the optimal hardware-friendly layer-wise KV quantization precision pairs for coarse-grained KV cache with multi-objective optimization and directly utilize the offline searched configurations during online inference. To reduce the computational cost of offline calibration, we utilize the intra-layer KV precision pair pruning and inter-layer clustering to reduce the search space. Experimental results show that we can achieve nearly lossless 3.25-bit mixed precision KV cache quantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive models like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum inference throughput can be improved by 38.3% compared with KV8 quantization over various context lengths. Our code and searched configurations are available at https://github.com/cmd2001/KVTuner.

### Understanding and Mitigating the Bias Inheritance in LLM-based Data Augmentation on Downstream Tasks 
[[arxiv](https://arxiv.org/abs/2502.04419)] [[cool](https://papers.cool/arxiv/2502.04419)] [[pdf](https://arxiv.org/pdf/2502.04419)]
> **Authors**: Miaomiao Li,Hao Chen,Yang Wang,Tingyuan Zhu,Weijia Zhang,Kaijie Zhu,Kam-Fai Wong,Jindong Wang
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: Technical report; 31 pages
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Generating synthetic datasets via large language models (LLMs) themselves has emerged as a promising approach to improve LLM performance. However, LLMs inherently reflect biases present in their training data, leading to a critical challenge: when these models generate synthetic data for training, they may propagate and amplify their inherent biases that can significantly impact model fairness and robustness on downstream tasks--a phenomenon we term bias inheritance. This work presents the first systematic investigation in understanding, analyzing, and mitigating bias inheritance. We study this problem by fine-tuning LLMs with a combined dataset consisting of original and LLM-augmented data, where bias ratio represents the proportion of augmented data. Through systematic experiments across 10 classification and generation tasks, we analyze how 6 different types of biases manifest at varying bias ratios. Our results reveal that bias inheritance has nuanced effects on downstream tasks, influencing both classification tasks and generation tasks differently. Then, our analysis identifies three key misalignment factors: misalignment of values, group data, and data distributions. Based on these insights, we propose three mitigation strategies: token-based, mask-based, and loss-based approaches. Experiments demonstrate that these strategies also work differently on various tasks and bias, indicating the substantial challenges to fully mitigate bias inheritance. We hope this work can provide valuable insights to the research of LLM data augmentation.

### Autotelic Reinforcement Learning: Exploring Intrinsic Motivations for Skill Acquisition in Open-Ended Environments 
[[arxiv](https://arxiv.org/abs/2502.04418)] [[cool](https://papers.cool/arxiv/2502.04418)] [[pdf](https://arxiv.org/pdf/2502.04418)]
> **Authors**: Prakhar Srivastava,Jasmeet Singh
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 12 pages, 12 figures
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: This paper presents a comprehensive overview of autotelic Reinforcement Learning (RL), emphasizing the role of intrinsic motivations in the open-ended formation of skill repertoires. We delineate the distinctions between knowledge-based and competence-based intrinsic motivations, illustrating how these concepts inform the development of autonomous agents capable of generating and pursuing self-defined goals. The typology of Intrinsically Motivated Goal Exploration Processes (IMGEPs) is explored, with a focus on the implications for multi-goal RL and developmental robotics. The autotelic learning problem is framed within a reward-free Markov Decision Process (MDP), WHERE agents must autonomously represent, generate, and master their own goals. We address the unique challenges in evaluating such agents, proposing various metrics for measuring exploration, generalization, and robustness in complex environments. This work aims to advance the understanding of autotelic RL agents and their potential for enhancing skill acquisition in a diverse and dynamic setting.

### NeuralMOVES: A lightweight and microscopic vehicle emission estimation model based on reverse engineering and surrogate learning 
[[arxiv](https://arxiv.org/abs/2502.04417)] [[cool](https://papers.cool/arxiv/2502.04417)] [[pdf](https://arxiv.org/pdf/2502.04417)]
> **Authors**: Edgar Ramirez-Sanchez,Catherine Tang,Yaosheng Xu,Nrithya Renganathan,Vindula Jayawardana,Zhengbing He,Cathy Wu
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: The transportation sector significantly contributes to greenhouse gas emissions, necessitating accurate emission models to guide mitigation strategies. Despite its field validation and certification, the industry-standard Motor Vehicle Emission Simulator (MOVES) faces challenges related to complexity in usage, high computational demands, and its unsuitability for microscopic real-time applications. To address these limitations, we present NeuralMOVES, a comprehensive suite of high-performance, lightweight surrogate models for vehicle CO2 emissions. Developed based on reverse engineering and Neural Networks, NeuralMOVES achieves a remarkable 6.013% Mean Average Percentage Error relative to MOVES across extensive tests spanning over two million scenarios with diverse trajectories and the factors regarding environments and vehicles. NeuralMOVES is only 2.4 MB, largely condensing the original MOVES and the reverse engineered MOVES into a compact representation, while maintaining high accuracy. Therefore, NeuralMOVES significantly enhances accessibility while maintaining the accuracy of MOVES, simplifying CO2 evaluation for transportation analyses and enabling real-time, microscopic applications across diverse scenarios without reliance on complex software or extensive computational resources. Moreover, this paper provides, for the first time, a framework for reverse engineering industrial-grade software tailored specifically to transportation scenarios, going beyond MOVES. The surrogate models are available at https://github.com/edgar-rs/neuralMOVES.

### CMoE: Fast Carving of Mixture-of-Experts for Efficient LLM Inference 
[[arxiv](https://arxiv.org/abs/2502.04416)] [[cool](https://papers.cool/arxiv/2502.04416)] [[pdf](https://arxiv.org/pdf/2502.04416)]
> **Authors**: Zehua Pei,Lancheng Zou,Hui-Ling Zhen,Xianzhi Yu,Wulong Liu,Sinno Jialin Pan,Mingxuan Yuan,Bei Yu
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Large language models (LLMs) achieve impressive performance by scaling model parameters, but this comes with significant inference overhead. Feed-forward networks (FFNs), which dominate LLM parameters, exhibit high activation sparsity in hidden neurons. To exploit this, researchers have proposed using a mixture-of-experts (MoE) architecture, where only a subset of parameters is activated. However, existing approaches often require extensive training data and resources, limiting their practicality. We propose CMoE (Carved MoE), a novel framework to efficiently carve MoE models from dense models. CMoE achieves remarkable performance through efficient expert grouping and lightweight adaptation. First, neurons are grouped into shared and routed experts based on activation rates. Next, we construct a routing mechanism without training from scratch, incorporating a differentiable routing process and load balancing. Using modest data, CMoE produces a well-designed, usable MoE from a 7B dense model within five minutes. With lightweight fine-tuning, it achieves high-performance recovery in under an hour. We make our code publicly available at https://github.com/JarvisPei/CMoE.

### Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing 
[[arxiv](https://arxiv.org/abs/2502.04411)] [[cool](https://papers.cool/arxiv/2502.04411)] [[pdf](https://arxiv.org/pdf/2502.04411)]
> **Authors**: Kunfeng Lai,Zhenheng Tang,Xinglin Pan,Peijie Dong,Xiang Liu,Haolan Chen,Li Shen,Bo Li,Xiaowen Chu
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: work in progress. arXiv admin note: text overlap with arXiv:2405.09673 by other authors
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Model merging aggregates Large Language Models (LLMs) finetuned on different tasks into a stronger one. However, parameter conflicts between models leads to performance degradation in averaging. While model routing addresses this issue by selecting individual models during inference, it imposes excessive storage and compute costs, and fails to leverage the common knowledge from different models. In this work, we observe that different layers exhibit varying levels of parameter conflicts. Building on this insight, we average layers with minimal parameter conflicts and use a novel task-level expert routing for layers with significant conflicts. To further reduce storage costs, inspired by task arithmetic sparsity, we decouple multiple fine-tuned experts into a dense expert and several sparse experts. Considering the out-of-distribution samples, we select and merge appropriate experts based on the task uncertainty of the input data. We conduct extensive experiments on both LLaMA and Qwen with varying parameter scales, and evaluate on real-world reasoning tasks. Results demonstrate that our method consistently achieves significant performance improvements while requiring less system cost compared to existing methods.

### Learning low-dimensional representations of ensemble forecast fields using autoencoder-based methods 
[[arxiv](https://arxiv.org/abs/2502.04409)] [[cool](https://papers.cool/arxiv/2502.04409)] [[pdf](https://arxiv.org/pdf/2502.04409)]
> **Authors**: Jieyu Chen,Kevin Höhlein,Sebastian Lerch
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,大气和海洋物理
- **Abstract**: Large-scale numerical simulations often produce high-dimensional gridded data that is challenging to process for downstream applications. A prime example is numerical weather prediction, where atmospheric processes are modeled using discrete gridded representations of the physical variables and dynamics. Uncertainties are assessed by running the simulations multiple times, yielding ensembles of simulated fields as a high-dimensional stochastic representation of the forecast distribution. The high-dimensionality and large volume of ensemble datasets poses major computing challenges for subsequent forecasting stages. Data-driven dimensionality reduction techniques could help to reduce the data volume before further processing by learning meaningful and compact representations. However, existing dimensionality reduction methods are typically designed for deterministic and single-valued inputs, and thus cannot handle ensemble data from multiple randomized simulations. In this study, we propose novel dimensionality reduction approaches specifically tailored to the format of ensemble forecast fields. We present two alternative frameworks, which yield low-dimensional representations of ensemble forecasts while respecting their probabilistic character. The first approach derives a distribution-based representation of an input ensemble by applying standard dimensionality reduction techniques in a member-by-member fashion and merging the member representations into a joint parametric distribution model. The second approach achieves a similar representation by encoding all members jointly using a tailored variational autoencoder. We evaluate and compare both approaches in a case study using 10 years of temperature and wind speed forecasts over Europe. The approaches preserve key spatial and statistical characteristics of the ensemble and enable probabilistic reconstructions of the forecast fields.

### Transforming Multimodal Models into Action Models for Radiotherapy 
[[arxiv](https://arxiv.org/abs/2502.04408)] [[cool](https://papers.cool/arxiv/2502.04408)] [[pdf](https://arxiv.org/pdf/2502.04408)]
> **Authors**: Matteo Ferrante,Alessandra Carosi,Rolando Maria D Angelillo,Nicola Toschi
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Radiotherapy is a crucial cancer treatment that demands precise planning to balance tumor eradication and preservation of healthy tissue. Traditional treatment planning (TP) is iterative, time-consuming, and reliant on human expertise, which can potentially introduce variability and inefficiency. We propose a novel framework to transform a large multimodal foundation model (MLM) into an action model for TP using a few-shot reinforcement learning (RL) approach. Our method leverages the MLM's extensive pre-existing knowledge of physics, radiation, and anatomy, enhancing it through a few-shot learning process. This allows the model to iteratively improve treatment plans using a Monte Carlo simulator. Our results demonstrate that this method outperforms conventional RL-based approaches in both quality and efficiency, achieving higher reward scores and more optimal dose distributions in simulations on prostate cancer data. This proof-of-concept suggests a promising direction for integrating advanced AI models into clinical workflows, potentially enhancing the speed, quality, and standardization of radiotherapy treatment planning.

### Illuminating Spaces: Deep Reinforcement Learning and Laser-Wall Partitioning for Architectural Layout Generation 
[[arxiv](https://arxiv.org/abs/2502.04407)] [[cool](https://papers.cool/arxiv/2502.04407)] [[pdf](https://arxiv.org/pdf/2502.04407)]
> **Authors**: Reza Kakooee,Benjamin Dillenburger
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Space layout design (SLD), occurring in the early stages of the design process, nonetheless influences both the functionality and aesthetics of the ultimate architectural outcome. The complexity of SLD necessitates innovative approaches to efficiently explore vast solution spaces. While image-based generative AI has emerged as a potential solution, they often rely on pixel-based space composition methods that lack intuitive representation of architectural processes. This paper leverages deep Reinforcement Learning (RL), as it offers a procedural approach that intuitively mimics the process of human designers. Effectively using RL for SLD requires an explorative space composing method to generate desirable design solutions. We introduce "laser-wall", a novel space partitioning method that conceptualizes walls as emitters of imaginary light beams to partition spaces. This approach bridges vector-based and pixel-based partitioning methods, offering both flexibility and exploratory power in generating diverse layouts. We present two planning strategies: one-shot planning, which generates entire layouts in a single pass, and dynamic planning, which allows for adaptive refinement by continuously transforming laser-walls. Additionally, we introduce on-light and off-light wall transformations for smooth and fast layout refinement, as well as identity-less and identity-full walls for versatile room assignment. We developed SpaceLayoutGym, an open-source OpenAI Gym compatible simulator for generating and evaluating space layouts. The RL agent processes the input design scenarios and generates solutions following a reward function that balances geometrical and topological requirements. Our results demonstrate that the RL-based laser-wall approach can generate diverse and functional space layouts that satisfy both geometric constraints and topological requirements and is architecturally intuitive.

### Calibrated Physics-Informed Uncertainty Quantification 
[[arxiv](https://arxiv.org/abs/2502.04406)] [[cool](https://papers.cool/arxiv/2502.04406)] [[pdf](https://arxiv.org/pdf/2502.04406)]
> **Authors**: Vignesh Gopakumar,Ander Gray,Lorenzo Zanisi,Timothy Nunn,Stanislas Pamela,Daniel Giles,Matt J. Kusner,Marc Peter Deisenroth
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算物理
- **Abstract**: Neural PDEs offer efficient alternatives to computationally expensive numerical PDE solvers for simulating complex physical systems. However, their lack of robust uncertainty quantification (UQ) limits deployment in critical applications. We introduce a model-agnostic, physics-informed conformal prediction (CP) framework that provides guaranteed uncertainty estimates without requiring labelled data. By utilising a physics-based approach, we are able to quantify and calibrate the model's inconsistencies with the PDE rather than the uncertainty arising from the data. Our approach uses convolutional layers as finite-difference stencils and leverages physics residual errors as nonconformity scores, enabling data-free UQ with marginal and joint coverage guarantees across prediction domains for a range of complex PDEs. We further validate the efficacy of our method on neural PDE models for plasma modelling and shot design in fusion reactors.

### FAS: Fast ANN-SNN Conversion for Spiking Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.04405)] [[cool](https://papers.cool/arxiv/2502.04405)] [[pdf](https://arxiv.org/pdf/2502.04405)]
> **Authors**: Long Chen,Xiaotian Song,Andy Song,BaDong Chen,Jiancheng Lv,Yanan Sun
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Spiking Large Language Models have been shown as a good alternative to LLMs in various scenarios. Existing methods for creating Spiking LLMs, i.e., direct training and ANN-SNN conversion, often suffer from performance degradation and relatively high computational costs. To address these issues, we propose a novel Fast ANN-SNN conversion strategy (FAS) that transforms LLMs into spiking LLMs in two stages. The first stage employs a full-parameter fine-tuning of pre-trained models, so it does not need any direct training from scratch. The second stage introduces a coarse-to-fine calibration method to reduce conversion errors and improve accuracy. Our experiments on both language and vision-language tasks across four different scales of LLMs demonstrate that FAS can achieve state-of-the-art performance yet with significantly reduced inference latency and computational costs. For example, FAS only takes 8 timesteps to achieve an accuracy of 3% higher than that of the OPT-7B model, while reducing energy consumption by 96.63%.

### Beyond Interpolation: Extrapolative Reasoning with Reinforcement Learning and Graph Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.04402)] [[cool](https://papers.cool/arxiv/2502.04402)] [[pdf](https://arxiv.org/pdf/2502.04402)]
> **Authors**: Niccolò Grillo,Andrea Toccaceli,Joël Mathys,Benjamin Estermann,Stefania Fresca,Roger Wattenhofer
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: The first two authors contributed equally to this work. Accepted as workshop paper at NEURMAD@AAAI25
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Despite incredible progress, many neural architectures fail to properly generalize beyond their training distribution. As such, learning to reason in a correct and generalizable way is one of the current fundamental challenges in machine learning. In this respect, logic puzzles provide a great testbed, as we can fully understand and control the learning environment. Thus, they allow to evaluate performance on previously unseen, larger and more difficult puzzles that follow the same underlying rules. Since traditional approaches often struggle to represent such scalable logical structures, we propose to model these puzzles using a graph-based approach. Then, we investigate the key factors enabling the proposed models to learn generalizable solutions in a reinforcement learning setting. Our study focuses on the impact of the inductive bias of the architecture, different reward systems and the role of recurrent modeling in enabling sequential reasoning. Through extensive experiments, we demonstrate how these elements contribute to successful extrapolation on increasingly complex puzzles.These insights and frameworks offer a systematic way to design learning-based systems capable of generalizable reasoning beyond interpolation.

### Adaptive Prototype Knowledge Transfer for Federated Learning with Mixed Modalities and Heterogeneous Tasks 
[[arxiv](https://arxiv.org/abs/2502.04400)] [[cool](https://papers.cool/arxiv/2502.04400)] [[pdf](https://arxiv.org/pdf/2502.04400)]
> **Authors**: Keke Gai,Mohan Wang,Jing Yu,Dongjue Wang,Qi Wu
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,密码学和安全,多媒体
- **Abstract**: Multimodal Federated Learning (MFL) enables multiple clients to collaboratively train models on multimodal data while ensuring clients' privacy. However, modality and task heterogeneity hinder clients from learning a unified representation, weakening local model generalization, especially in MFL with mixed modalities where only some clients have multimodal data. In this work, we propose an Adaptive prototype-based Multimodal Federated Learning (AproMFL) framework for mixed modalities and heterogeneous tasks to address the aforementioned issues. Our AproMFL transfers knowledge through adaptively-constructed prototypes without a prior public dataset. Clients adaptively select prototype construction methods in line with tasks; server converts client prototypes into unified multimodal prototypes and aggregates them to form global prototypes, avoid clients keeping unified labels. We divide the model into various modules and only aggregate mapping modules to reduce communication and computation overhead. To address aggregation issues in heterogeneity, we develop a client relationship graph-based scheme to dynamically adjust aggregation weights. Extensive experiments on representative datasets evidence effectiveness of AproMFL.

### Online Location Planning for AI-Defined Vehicles: Optimizing Joint Tasks of Order Serving and Spatio-Temporal Heterogeneous Model Fine-Tuning 
[[arxiv](https://arxiv.org/abs/2502.04399)] [[cool](https://papers.cool/arxiv/2502.04399)] [[pdf](https://arxiv.org/pdf/2502.04399)]
> **Authors**: Bokeng Zheng,Bo Rao,Tianxiang Zhu,Chee Wei Tan,Jingpu Duan,Zhi Zhou,Xu Chen,Xiaoxi Zhang
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,系统与控制
- **Abstract**: Advances in artificial intelligence (AI) including foundation models (FMs), are increasingly transforming human society, with smart city driving the evolution of urban living.Meanwhile, vehicle crowdsensing (VCS) has emerged as a key enabler, leveraging vehicles' mobility and sensor-equipped capabilities. In particular, ride-hailing vehicles can effectively facilitate flexible data collection and contribute towards urban intelligence, despite resource limitations. Therefore, this work explores a promising scenario, where edge-assisted vehicles perform joint tasks of order serving and the emerging foundation model fine-tuning using various urban data. However, integrating the VCS AI task with the conventional order serving task is challenging, due to their inconsistent spatio-temporal characteristics: (i) The distributions of ride orders and data point-of-interests (PoIs) may not coincide in geography, both following a priori unknown patterns; (ii) they have distinct forms of temporal effects, i.e., prolonged waiting makes orders become instantly invalid while data with increased staleness gradually reduces its utility for model fine-tuning.To overcome these obstacles, we propose an online framework based on multi-agent reinforcement learning (MARL) with careful augmentation. A new quality-of-service (QoS) metric is designed to characterize and balance the utility of the two joint tasks, under the effects of varying data volumes and staleness. We also integrate graph neural networks (GNNs) with MARL to enhance state representations, capturing graph-structured, time-varying dependencies among vehicles and across locations. Extensive experiments on our testbed simulator, utilizing various real-world foundation model fine-tuning tasks and the New York City Taxi ride order dataset, demonstrate the advantage of our proposed method.

### XMTC: Explainable Early Classification of Multivariate Time Series in Reach-to-Grasp Hand Kinematics 
[[arxiv](https://arxiv.org/abs/2502.04398)] [[cool](https://papers.cool/arxiv/2502.04398)] [[pdf](https://arxiv.org/pdf/2502.04398)]
> **Authors**: Reyhaneh Sabbagh Gol,Dimitar Valkov,Lars Linsen
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,图形,人机交互
- **Abstract**: Hand kinematics can be measured in Human-Computer Interaction (HCI) with the intention to predict the user's intention in a reach-to-grasp action. Using multiple hand sensors, multivariate time series data are being captured. Given a number of possible actions on a number of objects, the goal is to classify the multivariate time series data, where the class shall be predicted as early as possible. Many machine-learning methods have been developed for such classification tasks, where different approaches produce favorable solutions on different data sets. We, therefore, employ an ensemble approach that includes and weights different approaches. To provide a trustworthy classification production, we present the XMTC tool that incorporates coordinated multiple-view visualizations to analyze the predictions. Temporal accuracy plots, confusion matrix heatmaps, temporal confidence heatmaps, and partial dependence plots allow for the identification of the best trade-off between early prediction and prediction quality, the detection and analysis of challenging classification conditions, and the investigation of the prediction evolution in an overview and detail manner. We employ XMTC to real-world HCI data in multiple scenarios and show that good classification predictions can be achieved early on with our classifier as well as which conditions are easy to distinguish, which multivariate time series measurements impose challenges, and which features have most impact.

### Value-Based Deep RL Scales Predictably 
[[arxiv](https://arxiv.org/abs/2502.04327)] [[cool](https://papers.cool/arxiv/2502.04327)] [[pdf](https://arxiv.org/pdf/2502.04327)]
> **Authors**: Oleh Rybkin,Michal Nauman,Preston Fu,Charlie Snell,Pieter Abbeel,Sergey Levine,Aviral Kumar
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Scaling data and compute is critical to the success of machine learning. However, scaling demands predictability: we want methods to not only perform well with more compute or data, but also have their performance be predictable from small-scale runs, without running the large-scale experiment. In this paper, we show that value-based off-policy RL methods are predictable despite community lore regarding their pathological behavior. First, we show that data and compute requirements to attain a given performance level lie on a Pareto frontier, controlled by the updates-to-data (UTD) ratio. By estimating this frontier, we can predict this data requirement when given more compute, and this compute requirement when given more data. Second, we determine the optimal allocation of a total resource budget across data and compute for a given performance and use it to determine hyperparameters that maximize performance for a given budget. Third, this scaling behavior is enabled by first estimating predictable relationships between hyperparameters, which is used to manage effects of overfitting and plasticity loss unique to RL. We validate our approach using three algorithms: SAC, BRO, and PQL on DeepMind Control, OpenAI gym, and IsaacGym, when extrapolating to higher levels of data, compute, budget, or performance.

### The Uniformly Rotated Mondrian Kernel 
[[arxiv](https://arxiv.org/abs/2502.04323)] [[cool](https://papers.cool/arxiv/2502.04323)] [[pdf](https://arxiv.org/pdf/2502.04323)]
> **Authors**: Calvin Osborne,Eliza O'Reilly
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 22 pages, 4 figures, postprint for 28th International Conference on Artificial Intelligence and Statistics (AISTATS) 2025
- **标题**: None
- **领域**: 机器学习,可能性
- **Abstract**: First proposed by Rahimi and Recht, random features are used to decrease the computational cost of kernel machines in large-scale problems. The Mondrian kernel is one such example of a fast random feature approximation of the Laplace kernel, generated by a computationally efficient hierarchical random partition of the input space known as the Mondrian process. In this work, we study a variation of this random feature map by using uniformly randomly rotated Mondrian processes to approximate a kernel that is invariant under rotations. We obtain a closed-form expression for this isotropic kernel, as well as a uniform convergence rate of the uniformly rotated Mondrian kernel to this limit. To this end, we utilize techniques from the theory of stationary random tessellations in stochastic geometry and prove a new result on the geometry of the typical cell of the superposition of uniformly random rotations of Mondrian tessellations. Finally, we test the empirical performance of this random feature map on both synthetic and real-world datasets, demonstrating its improved performance over the Mondrian kernel on a debiased dataset.

### Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions 
[[arxiv](https://arxiv.org/abs/2502.04322)] [[cool](https://papers.cool/arxiv/2502.04322)] [[pdf](https://arxiv.org/pdf/2502.04322)]
> **Authors**: Yik Siu Chan,Narutatsu Ri,Yuxin Xiao,Marzyeh Ghassemi
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,计算机与社会
- **Abstract**: Despite extensive safety alignment efforts, large language models (LLMs) remain vulnerable to jailbreak attacks that elicit harmful behavior. While existing studies predominantly focus on attack methods that require technical expertise, two critical questions remain underexplored: (1) Are jailbroken responses truly useful in enabling average users to carry out harmful actions? (2) Do safety vulnerabilities exist in more common, simple human-LLM interactions? In this paper, we demonstrate that LLM responses most effectively facilitate harmful actions when they are both actionable and informative--two attributes easily elicited in multi-step, multilingual interactions. Using this insight, we propose HarmScore, a jailbreak metric that measures how effectively an LLM response enables harmful actions, and Speak Easy, a simple multi-step, multilingual attack framework. Notably, by incorporating Speak Easy into direct request and jailbreak baselines, we see an average absolute increase of 0.319 in Attack Success Rate and 0.426 in HarmScore in both open-source and proprietary LLMs across four safety benchmarks. Our work reveals a critical yet often overlooked vulnerability: Malicious users can easily exploit common interaction patterns for harmful intentions.

### Great Models Think Alike and this Undermines AI Oversight 
[[arxiv](https://arxiv.org/abs/2502.04313)] [[cool](https://papers.cool/arxiv/2502.04313)] [[pdf](https://arxiv.org/pdf/2502.04313)]
> **Authors**: Shashwat Goel,Joschka Struber,Ilze Amanda Auzina,Karuna K Chandra,Ponnurangam Kumaraguru,Douwe Kiela,Ameya Prabhu,Matthias Bethge,Jonas Geiping
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 60 pages, 20 figures
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: As Language Model (LM) capabilities advance, evaluating and supervising them at scale is getting harder for humans. There is hope that other language models can automate both these tasks, which we refer to as "AI Oversight". We study how model similarity affects both aspects of AI oversight by proposing a probabilistic metric for LM similarity based on overlap in model mistakes. Using this metric, we first show that LLM-as-a-judge scores favor models similar to the judge, generalizing recent self-preference results. Then, we study training on LM annotations, and find complementary knowledge between the weak supervisor and strong student model plays a crucial role in gains from "weak-to-strong generalization". As model capabilities increase, it becomes harder to find their mistakes, and we might defer more to AI oversight. However, we observe a concerning trend -- model mistakes are becoming more similar with increasing capabilities, pointing to risks from correlated failures. Our work underscores the importance of reporting and correcting for model similarity, especially in the emerging paradigm of AI oversight.

### Consistency of augmentation graph and network approximability in contrastive learning 
[[arxiv](https://arxiv.org/abs/2502.04312)] [[cool](https://papers.cool/arxiv/2502.04312)] [[pdf](https://arxiv.org/pdf/2502.04312)]
> **Authors**: Chenghui Li,A. Martina Neuman
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,偏微分方程分析,谱理论
- **Abstract**: Contrastive learning leverages data augmentation to develop feature representation without relying on large labeled datasets. However, despite its empirical success, the theoretical foundations of contrastive learning remain incomplete, with many essential guarantees left unaddressed, particularly the realizability assumption concerning neural approximability of an optimal spectral contrastive loss solution. In this work, we overcome these limitations by analyzing the pointwise and spectral consistency of the augmentation graph Laplacian. We establish that, under specific conditions for data generation and graph connectivity, as the augmented dataset size increases, the augmentation graph Laplacian converges to a weighted Laplace-Beltrami operator on the natural data manifold. These consistency results ensure that the graph Laplacian spectrum effectively captures the manifold geometry. Consequently, they give way to a robust framework for establishing neural approximability, directly resolving the realizability assumption in a current paradigm.

### Finding Pegasus: Enhancing Unsupervised Anomaly Detection in High-Dimensional Data using a Manifold-Based Approach 
[[arxiv](https://arxiv.org/abs/2502.04310)] [[cool](https://papers.cool/arxiv/2502.04310)] [[pdf](https://arxiv.org/pdf/2502.04310)]
> **Authors**: R. P. Nathan,Nikolaos Nikolaou,Ofer Lahav
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 21 pages, 14 figures
- **标题**: None
- **领域**: 机器学习,宇宙学和非银河系天体物理学
- **Abstract**: Unsupervised machine learning methods are well suited to searching for anomalies at scale but can struggle with the high-dimensional representation of many modern datasets, hence dimensionality reduction (DR) is often performed first. In this paper we analyse unsupervised anomaly detection (AD) from the perspective of the manifold created in DR. We present an idealised illustration, "Finding Pegasus", and a novel formal framework with which we categorise AD methods and their results into "on manifold" and "off manifold". We define these terms and show how they differ. We then use this insight to develop an approach of combining AD methods which significantly boosts AD recall without sacrificing precision in situations employing high DR. When tested on MNIST data, our approach of combining AD methods improves recall by as much as 16 percent compared with simply combining with the best standalone AD method (Isolation Forest), a result which shows great promise for its application to real-world data.

### Targeted Learning for Data Fairness 
[[arxiv](https://arxiv.org/abs/2502.04309)] [[cool](https://papers.cool/arxiv/2502.04309)] [[pdf](https://arxiv.org/pdf/2502.04309)]
> **Authors**: Alexander Asemota,Giles Hooker
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Data and algorithms have the potential to produce and perpetuate discrimination and disparate treatment. As such, significant effort has been invested in developing approaches to defining, detecting, and eliminating unfair outcomes in algorithms. In this paper, we focus on performing statistical inference for fairness. Prior work in fairness inference has largely focused on inferring the fairness properties of a given predictive algorithm. Here, we expand fairness inference by evaluating fairness in the data generating process itself, referred to here as data fairness. We perform inference on data fairness using targeted learning, a flexible framework for nonparametric inference. We derive estimators demographic parity, equal opportunity, and conditional mutual information. Additionally, we find that our estimators for probabilistic metrics exploit double robustness. To validate our approach, we perform several simulations and apply our estimators to real data.

### HOG-Diff: Higher-Order Guided Diffusion for Graph Generation 
[[arxiv](https://arxiv.org/abs/2502.04308)] [[cool](https://papers.cool/arxiv/2502.04308)] [[pdf](https://arxiv.org/pdf/2502.04308)]
> **Authors**: Yiming Huang,Tolga Birdal
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,社交和信息网络,物理与社会
- **Abstract**: Graph generation is a critical yet challenging task as empirical analyses require a deep understanding of complex, non-Euclidean structures. Although diffusion models have recently made significant achievements in graph generation, these models typically adapt from the frameworks designed for image generation, making them ill-suited for capturing the topological properties of graphs. In this work, we propose a novel Higher-order Guided Diffusion (HOG-Diff) model that follows a coarse-to-fine generation curriculum and is guided by higher-order information, enabling the progressive generation of plausible graphs with inherent topological structures. We further prove that our model exhibits a stronger theoretical guarantee than classical diffusion frameworks. Extensive experiments on both molecular and generic graph generation tasks demonstrate that our method consistently outperforms or remains competitive with state-of-the-art baselines. Our code is available at https://github.com/Yiminghh/HOG-Diff.

### Statistical guarantees for continuous-time policy evaluation: blessing of ellipticity and new tradeoffs 
[[arxiv](https://arxiv.org/abs/2502.04297)] [[cool](https://papers.cool/arxiv/2502.04297)] [[pdf](https://arxiv.org/pdf/2502.04297)]
> **Authors**: Wenlong Mou
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,优化与控制,可能性,统计理论
- **Abstract**: We study the estimation of the value function for continuous-time Markov diffusion processes using a single, discretely observed ergodic trajectory. Our work provides non-asymptotic statistical guarantees for the least-squares temporal-difference (LSTD) method, with performance measured in the first-order Sobolev norm. Specifically, the estimator attains an $O(1 / \sqrt{T})$ convergence rate when using a trajectory of length $T$; notably, this rate is achieved as long as $T$ scales nearly linearly with both the mixing time of the diffusion and the number of basis functions employed. A key insight of our approach is that the ellipticity inherent in the diffusion process ensures robust performance even as the effective horizon diverges to infinity. Moreover, we demonstrate that the Markovian component of the statistical error can be controlled by the approximation error, while the martingale component grows at a slower rate relative to the number of basis functions. By carefully balancing these two sources of error, our analysis reveals novel trade-offs between approximation and statistical errors.

### Every Call is Precious: Global Optimization of Black-Box Functions with Unknown Lipschitz Constants 
[[arxiv](https://arxiv.org/abs/2502.04290)] [[cool](https://papers.cool/arxiv/2502.04290)] [[pdf](https://arxiv.org/pdf/2502.04290)]
> **Authors**: Fares Fourati,Salma Kharrat,Vaneet Aggarwal,Mohamed-Slim Alouini
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: Accepted at AISTATS 2025
- **标题**: None
- **领域**: 机器学习,人工智能,系统与控制,优化与控制,机器学习
- **Abstract**: Optimizing expensive, non-convex, black-box Lipschitz continuous functions presents significant challenges, particularly when the Lipschitz constant of the underlying function is unknown. Such problems often demand numerous function evaluations to approximate the global optimum, which can be prohibitive in terms of time, energy, or resources. In this work, we introduce Every Call is Precious (ECP), a novel global optimization algorithm that minimizes unpromising evaluations by strategically focusing on potentially optimal regions. Unlike previous approaches, ECP eliminates the need to estimate the Lipschitz constant, thereby avoiding additional function evaluations. ECP guarantees no-regret performance for infinite evaluation budgets and achieves minimax-optimal regret bounds within finite budgets. Extensive ablation studies validate the algorithm's robustness, while empirical evaluations show that ECP outperforms 10 benchmark algorithms including Lipschitz, Bayesian, bandits, and evolutionary methods across 30 multi-dimensional non-convex synthetic and real-world optimization problems, which positions ECP as a competitive approach for global optimization.

### Leveraging Geolocation in Clinical Records to Improve Alzheimer's Disease Diagnosis Using DMV Framework 
[[arxiv](https://arxiv.org/abs/2502.04288)] [[cool](https://papers.cool/arxiv/2502.04288)] [[pdf](https://arxiv.org/pdf/2502.04288)]
> **Authors**: Peng Zhang,Divya Chaudhary
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Alzheimer's Disease (AD) early detection is critical for enabling timely intervention and improving patient outcomes. This paper presents a DMV framework using Llama3-70B and GPT-4o as embedding models to analyze clinical notes and predict a continuous risk score associated with early AD onset. Framing the task as a regression problem, we model the relationship between linguistic features in clinical notes (inputs) and a target variable (data value) that answers specific questions related to AD risk within certain topic categories. By leveraging a multi-faceted feature set that includes geolocation data, we capture additional environmental context potentially linked to AD. Our results demonstrate that the integration of the geolocation information significantly decreases the error of predicting early AD risk scores over prior models by 28.57% (Llama3-70B) and 33.47% (GPT4-o). Our findings suggest that this combined approach can enhance the predictive accuracy of AD risk assessment, supporting early diagnosis and intervention in clinical settings. Additionally, the framework's ability to incorporate geolocation data provides a more comprehensive risk assessment model that could help healthcare providers better understand and address environmental factors contributing to AD development.

### DECAF: Learning to be Fair in Multi-agent Resource Allocation 
[[arxiv](https://arxiv.org/abs/2502.04281)] [[cool](https://papers.cool/arxiv/2502.04281)] [[pdf](https://arxiv.org/pdf/2502.04281)]
> **Authors**: Ashwin Kumar,William Yeoh
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机与社会,多代理系统
- **Abstract**: A wide variety of resource allocation problems operate under resource constraints that are managed by a central arbitrator, with agents who evaluate and communicate preferences over these resources. We formulate this broad class of problems as Distributed Evaluation, Centralized Allocation (DECA) problems and propose methods to learn fair and efficient policies in centralized resource allocation. Our methods are applied to learning long-term fairness in a novel and general framework for fairness in multi-agent systems. We show three different methods based on Double Deep Q-Learning: (1) A joint weighted optimization of fairness and utility, (2) a split optimization, learning two separate Q-estimators for utility and fairness, and (3) an online policy perturbation to guide existing black-box utility functions toward fair solutions. Our methods outperform existing fair MARL approaches on multiple resource allocation domains, even when evaluated using diverse fairness functions, and allow for flexible online trade-offs between utility and fairness.

### Orthogonal Representation Learning for Estimating Causal Quantities 
[[arxiv](https://arxiv.org/abs/2502.04274)] [[cool](https://papers.cool/arxiv/2502.04274)] [[pdf](https://arxiv.org/pdf/2502.04274)]
> **Authors**: Valentyn Melnychuk,Dennis Frauen,Jonas Schweisthal,Stefan Feuerriegel
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Representation learning is widely used for estimating causal quantities (e.g., the conditional average treatment effect) from observational data. While existing representation learning methods have the benefit of allowing for end-to-end learning, they do not have favorable theoretical properties of Neyman-orthogonal learners, such as double robustness and quasi-oracle efficiency. Also, such representation learning methods often employ additional constraints, like balancing, which may even lead to inconsistent estimation. In this paper, we propose a novel class of Neyman-orthogonal learners for causal quantities defined at the representation level, which we call OR-learners. Our OR-learners have several practical advantages: they allow for consistent estimation of causal quantities based on any learned representation, while offering favorable theoretical properties including double robustness and quasi-oracle efficiency. In multiple experiments, we show that, under certain regularity conditions, our OR-learners improve existing representation learning methods and achieve state-of-the-art performance. To the best of our knowledge, our OR-learners are the first work to offer a unified framework of representation learning methods and Neyman-orthogonal learners for causal quantities estimation.

### PILAF: Optimal Human Preference Sampling for Reward Modeling 
[[arxiv](https://arxiv.org/abs/2502.04270)] [[cool](https://papers.cool/arxiv/2502.04270)] [[pdf](https://arxiv.org/pdf/2502.04270)]
> **Authors**: Yunzhen Feng,Ariel Kwiatkowski,Kunhao Zheng,Julia Kempe,Yaqi Duan
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: As large language models increasingly drive real-world applications, aligning them with human values becomes paramount. Reinforcement Learning from Human Feedback (RLHF) has emerged as a key technique, translating preference data into reward models when oracle human values remain inaccessible. In practice, RLHF mostly relies on approximate reward models, which may not consistently guide the policy toward maximizing the underlying human values. We propose Policy-Interpolated Learning for Aligned Feedback (PILAF), a novel response sampling strategy for preference labeling that explicitly aligns preference learning with maximizing the underlying oracle reward. PILAF is theoretically grounded, demonstrating optimality from both an optimization and a statistical perspective. The method is straightforward to implement and demonstrates strong performance in iterative and online RLHF settings where feedback curation is critical.

### Efficient Randomized Experiments Using Foundation Models 
[[arxiv](https://arxiv.org/abs/2502.04262)] [[cool](https://papers.cool/arxiv/2502.04262)] [[pdf](https://arxiv.org/pdf/2502.04262)]
> **Authors**: Piersilvio De Bartolomeis,Javier Abad,Guanbo Wang,Konstantin Donhauser,Raymond M. Duch,Fanny Yang,Issa J. Dahabreh
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,方法论,机器学习
- **Abstract**: Randomized experiments are the preferred approach for evaluating the effects of interventions, but they are costly and often yield estimates with substantial uncertainty. On the other hand, in silico experiments leveraging foundation models offer a cost-effective alternative that can potentially attain higher statistical precision. However, the benefits of in silico experiments come with a significant risk: statistical inferences are not valid if the models fail to accurately predict experimental responses to interventions. In this paper, we propose a novel approach that integrates the predictions from multiple foundation models with experimental data while preserving valid statistical inference. Our estimator is consistent and asymptotically normal, with asymptotic variance no larger than the standard estimator based on experimental data alone. Importantly, these statistical properties hold even when model predictions are arbitrarily biased. Empirical results across several randomized experiments show that our estimator offers substantial precision gains, equivalent to a reduction of up to 20% in the sample size needed to match the same precision as the standard estimator based on experimental data alone.

### Realistic Image-to-Image Machine Unlearning via Decoupling and Knowledge Retention 
[[arxiv](https://arxiv.org/abs/2502.04260)] [[cool](https://papers.cool/arxiv/2502.04260)] [[pdf](https://arxiv.org/pdf/2502.04260)]
> **Authors**: Ayush K. Varshney,Vicenç Torra
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Machine Unlearning allows participants to remove their data from a trained machine learning model in order to preserve their privacy, and security. However, the machine unlearning literature for generative models is rather limited. The literature for image-to-image generative model (I2I model) considers minimizing the distance between Gaussian noise and the output of I2I model for forget samples as machine unlearning. However, we argue that the machine learning model performs fairly well on unseen data i.e., a retrained model will be able to catch generic patterns in the data and hence will not generate an output which is equivalent to Gaussian noise. In this paper, we consider that the model after unlearning should treat forget samples as out-of-distribution (OOD) data, i.e., the unlearned model should no longer recognize or encode the specific patterns found in the forget samples. To achieve this, we propose a framework which decouples the model parameters with gradient ascent, ensuring that forget samples are OOD for unlearned model with theoretical guarantee. We also provide $(ε, δ)$-unlearning guarantee for model updates with gradient ascent. The unlearned model is further fine-tuned on the remaining samples to maintain its performance. We also propose an attack model to ensure that the unlearned model has effectively removed the influence of forget samples. Extensive empirical evaluation on two large-scale datasets, ImageNet-1K and Places365 highlights the superiority of our approach. To show comparable performance with retrained model, we also show the comparison of a simple AutoEncoder on various baselines on CIFAR-10 dataset.

### Adapting to Evolving Adversaries with Regularized Continual Robust Training 
[[arxiv](https://arxiv.org/abs/2502.04248)] [[cool](https://papers.cool/arxiv/2502.04248)] [[pdf](https://arxiv.org/pdf/2502.04248)]
> **Authors**: Sihui Dai,Christian Cianfarani,Arjun Bhagoji,Vikash Sehwag,Prateek Mittal
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Robust training methods typically defend against specific attack types, such as Lp attacks with fixed budgets, and rarely account for the fact that defenders may encounter new attacks over time. A natural solution is to adapt the defended model to new adversaries as they arise via fine-tuning, a method which we call continual robust training (CRT). However, when implemented naively, fine-tuning on new attacks degrades robustness on previous attacks. This raises the question: how can we improve the initial training and fine-tuning of the model to simultaneously achieve robustness against previous and new attacks? We present theoretical results which show that the gap in a model's robustness against different attacks is bounded by how far each attack perturbs a sample in the model's logit space, suggesting that regularizing with respect to this logit space distance can help maintain robustness against previous attacks. Extensive experiments on 3 datasets (CIFAR-10, CIFAR-100, and ImageNette) and over 100 attack combinations demonstrate that the proposed regularization improves robust accuracy with little overhead in training time. Our findings and open-source code lay the groundwork for the deployment of models robust to evolving attacks.

### A Theoretical Framework for Data Efficient Multi-Source Transfer Learning Based on Cramér-Rao Bound 
[[arxiv](https://arxiv.org/abs/2502.04242)] [[cool](https://papers.cool/arxiv/2502.04242)] [[pdf](https://arxiv.org/pdf/2502.04242)]
> **Authors**: Qingyue Zhang,Haohao Fu,Guanbo Huang,Yaoyuan Liang,Chang Chu,Tianren Peng,Yanru Wu,Qi Li,Yang Li,Shao-Lun Huang
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Multi-source transfer learning provides an effective solution to data scarcity in real-world supervised learning scenarios by leveraging multiple source tasks. In this field, existing works typically use all available samples from sources in training, which constrains their training efficiency and may lead to suboptimal results. To address this, we propose a theoretical framework that answers the question: what is the optimal quantity of source samples needed from each source task to jointly train the target model? Specifically, we introduce a generalization error measure that aligns with cross-entropy loss, and minimize it based on the Cramér-Rao Bound to determine the optimal transfer quantity for each source task. Additionally, we develop an architecture-agnostic and data-efficient algorithm OTQMS to implement our theoretical results for training deep multi-source transfer learning models. Experimental studies on diverse architectures and two real-world benchmark datasets show that our proposed algorithm significantly outperforms state-of-the-art approaches in both accuracy and data efficiency. The code and supplementary materials are available in https://anonymous.4open.science/r/Materials.

### Graph machine learning for flight delay prediction due to holding manouver 
[[arxiv](https://arxiv.org/abs/2502.04233)] [[cool](https://papers.cool/arxiv/2502.04233)] [[pdf](https://arxiv.org/pdf/2502.04233)]
> **Authors**: Jorge L. Franco,Manoel V. Machado Neto,Filipe A. N. Verri,Diego R. Amancio
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,社交和信息网络
- **Abstract**: Flight delays due to holding maneuvers are a critical and costly phenomenon in aviation, driven by the need to manage air traffic congestion and ensure safety. Holding maneuvers occur when aircraft are instructed to circle in designated airspace, often due to factors such as airport congestion, adverse weather, or air traffic control restrictions. This study models the prediction of flight delays due to holding maneuvers as a graph problem, leveraging advanced Graph Machine Learning (Graph ML) techniques to capture complex interdependencies in air traffic networks. Holding maneuvers, while crucial for safety, cause increased fuel usage, emissions, and passenger dissatisfaction, making accurate prediction essential for operational efficiency. Traditional machine learning models, typically using tabular data, often overlook spatial-temporal relations within air traffic data. To address this, we model the problem of predicting holding as edge feature prediction in a directed (multi)graph where we apply both CatBoost, enriched with graph features capturing network centrality and connectivity, and Graph Attention Networks (GATs), which excel in relational data contexts. Our results indicate that CatBoost outperforms GAT in this imbalanced dataset, effectively predicting holding events and offering interpretability through graph-based feature importance. Additionally, we discuss the model's potential operational impact through a web-based tool that allows users to simulate real-time delay predictions. This research underscores the viability of graph-based approaches for predictive analysis in aviation, with implications for enhancing fuel efficiency, reducing delays, and improving passenger experience.

### Algorithmic causal structure emerging through compression 
[[arxiv](https://arxiv.org/abs/2502.04210)] [[cool](https://papers.cool/arxiv/2502.04210)] [[pdf](https://arxiv.org/pdf/2502.04210)]
> **Authors**: Liang Wendong,Simon Buchholz,Bernhard Schölkopf
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算复杂度,信息论
- **Abstract**: We explore the relationship between causality, symmetry, and compression. We build on and generalize the known connection between learning and compression to a setting where causal models are not identifiable. We propose a framework where causality emerges as a consequence of compressing data across multiple environments. We define algorithmic causality as an alternative definition of causality when traditional assumptions for causal identifiability do not hold. We demonstrate how algorithmic causal and symmetric structures can emerge from minimizing upper bounds on Kolmogorov complexity, without knowledge of intervention targets. We hypothesize that these insights may also provide a novel perspective on the emergence of causality in machine learning models, such as large language models, where causal relationships may not be explicitly identifiable.

### Ensuring Reliability via Hyperparameter Selection: Review and Advances 
[[arxiv](https://arxiv.org/abs/2502.04206)] [[cool](https://papers.cool/arxiv/2502.04206)] [[pdf](https://arxiv.org/pdf/2502.04206)]
> **Authors**: Amirmohammad Farzaneh,Osvaldo Simeone
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,信息论
- **Abstract**: Hyperparameter selection is a critical step in the deployment of artificial intelligence (AI) models, particularly in the current era of foundational, pre-trained, models. By framing hyperparameter selection as a multiple hypothesis testing problem, recent research has shown that it is possible to provide statistical guarantees on population risk measures attained by the selected hyperparameter. This paper reviews the Learn-Then-Test (LTT) framework, which formalizes this approach, and explores several extensions tailored to engineering-relevant scenarios. These extensions encompass different risk measures and statistical guarantees, multi-objective optimization, the incorporation of prior knowledge and dependency structures into the hyperparameter selection process, as well as adaptivity. The paper also includes illustrative applications for communication systems.

### "Short-length" Adversarial Training Helps LLMs Defend "Long-length" Jailbreak Attacks: Theoretical and Empirical Evidence 
[[arxiv](https://arxiv.org/abs/2502.04204)] [[cool](https://papers.cool/arxiv/2502.04204)] [[pdf](https://arxiv.org/pdf/2502.04204)]
> **Authors**: Shaopeng Fu,Liang Ding,Di Wang
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,密码学和安全,机器学习
- **Abstract**: Jailbreak attacks against large language models (LLMs) aim to induce harmful behaviors in LLMs through carefully crafted adversarial prompts. To mitigate attacks, one way is to perform adversarial training (AT)-based alignment, i.e., training LLMs on some of the most adversarial prompts to help them learn how to behave safely under attacks. During AT, the length of adversarial prompts plays a critical role in the robustness of aligned LLMs. This paper focuses on adversarial suffix jailbreak attacks and unveils that to defend against a jailbreak attack with an adversarial suffix of length $Θ(M)$, it is enough to align LLMs on prompts with adversarial suffixes of length $Θ(\sqrt{M})$. Theoretically, we analyze the adversarial in-context learning of linear transformers on linear regression tasks and prove a robust generalization bound for trained transformers. The bound depends on the term $Θ(\sqrt{M_{\text{test}}}/M_{\text{train}})$, where $M_{\text{train}}$ and $M_{\text{test}}$ are the number of adversarially perturbed in-context samples during training and testing. Empirically, we conduct AT on popular open-source LLMs and evaluate their robustness against jailbreak attacks of different adversarial suffix lengths. Results confirm a positive correlation between the attack success rate and the ratio of the square root of the adversarial suffix during jailbreaking to the length during AT. Our findings show that it is practical to defend "long-length" jailbreak attacks via efficient "short-length" AT. The code is available at https://github.com/fshp971/adv-icl.

### Multi-agent Architecture Search via Agentic Supernet 
[[arxiv](https://arxiv.org/abs/2502.04180)] [[cool](https://papers.cool/arxiv/2502.04180)] [[pdf](https://arxiv.org/pdf/2502.04180)]
> **Authors**: Guibin Zhang,Luyang Niu,Junfeng Fang,Kun Wang,Lei Bai,Xiang Wang
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学,多代理系统
- **Abstract**: Large Language Model (LLM)-empowered multi-agent systems extend the cognitive boundaries of individual agents through disciplined collaboration and interaction, while constructing these systems often requires labor-intensive manual designs. Despite the availability of methods to automate the design of agentic workflows, they typically seek to identify a static, complex, one-size-fits-all system, which, however, fails to dynamically allocate inference resources based on the difficulty and domain of each query. To address this challenge, we shift away from the pursuit of a monolithic agentic system, instead optimizing the \textbf{agentic supernet}, a probabilistic and continuous distribution of agentic architectures. We introduce MaAS, an automated framework that samples query-dependent agentic systems from the supernet, delivering high-quality solutions and tailored resource allocation (\textit{e.g.}, LLM calls, tool calls, token cost). Comprehensive evaluation across six benchmarks demonstrates that MaAS \textbf{(I)} requires only $6\sim45\%$ of the inference costs of existing handcrafted or automated multi-agent systems, \textbf{(II)} surpasses them by $0.54\%\sim11.82\%$, and \textbf{(III)} enjoys superior cross-dataset and cross-LLM-backbone transferability.

### MRAMG-Bench: A BeyondText Benchmark for Multimodal Retrieval-Augmented Multimodal Generation 
[[arxiv](https://arxiv.org/abs/2502.04176)] [[cool](https://papers.cool/arxiv/2502.04176)] [[pdf](https://arxiv.org/pdf/2502.04176)]
> **Authors**: Qinhan Yu,Zhiyou Xiao,Binghui Li,Zhengren Wang,Chong Chen,Wentao Zhang
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 11 pages
- **标题**: None
- **领域**: 机器学习,信息检索
- **Abstract**: Recent advancements in Retrieval-Augmented Generation (RAG) have shown remarkable performance in enhancing response accuracy and relevance by integrating external knowledge into generative models. However, existing RAG methods primarily focus on providing text-only answers, even in multimodal retrieval-augmented generation scenarios. In this work, we introduce the Multimodal Retrieval-Augmented Multimodal Generation (MRAMG) task, which aims to generate answers that combine both text and images, fully leveraging the multimodal data within a corpus. Despite the importance of this task, there is a notable absence of a comprehensive benchmark to effectively evaluate MRAMG performance. To bridge this gap, we introduce the MRAMG-Bench, a carefully curated, human-annotated dataset comprising 4,346 documents, 14,190 images, and 4,800 QA pairs, sourced from three categories: Web Data, Academic Papers, and Lifestyle. The dataset incorporates diverse difficulty levels and complex multi-image scenarios, providing a robust foundation for evaluating multimodal generation tasks. To facilitate rigorous evaluation, our MRAMG-Bench incorporates a comprehensive suite of both statistical and LLM-based metrics, enabling a thorough analysis of the performance of popular generative models in the MRAMG task. Besides, we propose an efficient multimodal answer generation framework that leverages both LLMs and MLLMs to generate multimodal responses. Our datasets are available at: https://huggingface.co/MRAMG.

### Archetypal Analysis for Binary Data 
[[arxiv](https://arxiv.org/abs/2502.04172)] [[cool](https://papers.cool/arxiv/2502.04172)] [[pdf](https://arxiv.org/pdf/2502.04172)]
> **Authors**: A. Emilie J. Wedenborg,Morten Mørup
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 5 pages, Accepted at ICASSP 2025
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Archetypal analysis (AA) is a matrix decomposition method that identifies distinct patterns using convex combinations of the data points denoted archetypes with each data point in turn reconstructed as convex combinations of the archetypes. AA thereby forms a polytope representing trade-offs of the distinct aspects in the data. Most existing methods for AA are designed for continuous data and do not exploit the structure of the data distribution. In this paper, we propose two new optimization frameworks for archetypal analysis for binary data. i) A second order approximation of the AA likelihood based on the Bernoulli distribution with efficient closed-form updates using an active set procedure for learning the convex combinations defining the archetypes, and a sequential minimal optimization strategy for learning the observation specific reconstructions. ii) A Bernoulli likelihood based version of the principal convex hull analysis (PCHA) algorithm originally developed for least squares optimization. We compare these approaches with the only existing binary AA procedure relying on multiplicative updates and demonstrate their superiority on both synthetic and real binary data. Notably, the proposed optimization frameworks for AA can easily be extended to other data distributions providing generic efficient optimization frameworks for AA based on tailored likelihood functions reflecting the underlying data distribution.

### Making Sense of Touch: Unsupervised Shapelet Learning in Bag-of-words Sense 
[[arxiv](https://arxiv.org/abs/2502.04167)] [[cool](https://papers.cool/arxiv/2502.04167)] [[pdf](https://arxiv.org/pdf/2502.04167)]
> **Authors**: Zhicong Xian,Tabish Chaudhary,Jürgen Bock
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: ef:ICRA 2020 Brain-Pil Workshop- New advances in brain-inspired perception, interaction andlearning
- **标题**: None
- **领域**: 机器学习,机器人技术
- **Abstract**: This paper introduces NN-STNE, a neural network using t-distributed stochastic neighbor embedding (t-SNE) as a hidden layer to reduce input dimensions by mapping long time-series data into shapelet membership probabilities. A Gaussian kernel-based mean square error preserves local data structure, while K-means initializes shapelet candidates due to the non-convex optimization challenge. Unlike existing methods, our approach uses t-SNE to address crowding in low-dimensional space and applies L1-norm regularization to optimize shapelet length. Evaluations on the UCR dataset and an electrical component manipulation task, like switching on, demonstrate improved clustering accuracy over state-of-the-art feature-learning methods in robotics.

### Efficient Distributed Optimization under Heavy-Tailed Noise 
[[arxiv](https://arxiv.org/abs/2502.04164)] [[cool](https://papers.cool/arxiv/2502.04164)] [[pdf](https://arxiv.org/pdf/2502.04164)]
> **Authors**: Su Hyeong Lee,Manzil Zaheer,Tian Li
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Distributed optimization has become the default training paradigm in modern machine learning due to the growing scale of models and datasets. To mitigate communication overhead, local updates are often applied before global aggregation, resulting in a nested optimization approach with inner and outer steps. However, heavy-tailed stochastic gradient noise remains a significant challenge, particularly in attention-based models, hindering effective training. In this work, we propose TailOPT, an efficient framework designed to address heavy-tailed noise by leveraging adaptive optimization or clipping techniques. We establish convergence guarantees for the TailOPT framework under heavy-tailed noise with potentially unbounded gradient variance and local updates. Among its variants, we highlight a memory and communication efficient instantiation which we call $Bi^2Clip$, which performs coordinate-wise clipping at both the inner and outer optimizers, achieving adaptive-like performance (e.g., Adam) without the cost of maintaining or transmitting additional gradient statistics. Empirically, TailOPT, including $Bi^2Clip$, demonstrates superior performance on several language tasks and models, outperforming state-of-the-art methods.

### Behavioral Entropy-Guided Dataset Generation for Offline Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.04141)] [[cool](https://papers.cool/arxiv/2502.04141)] [[pdf](https://arxiv.org/pdf/2502.04141)]
> **Authors**: Wesley A. Suttle,Aamodh Suresh,Carlos Nieto-Granda
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: Accepted to ICLR 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Entropy-based objectives are widely used to perform state space exploration in reinforcement learning (RL) and dataset generation for offline RL. Behavioral entropy (BE), a rigorous generalization of classical entropies that incorporates cognitive and perceptual biases of agents, was recently proposed for discrete settings and shown to be a promising metric for robotic exploration problems. In this work, we propose using BE as a principled exploration objective for systematically generating datasets that provide diverse state space coverage in complex, continuous, potentially high-dimensional domains. To achieve this, we extend the notion of BE to continuous settings, derive tractable $k$-nearest neighbor estimators, provide theoretical guarantees for these estimators, and develop practical reward functions that can be used with standard RL methods to learn BE-maximizing policies. Using standard MuJoCo environments, we experimentally compare the performance of offline RL algorithms for a variety of downstream tasks on datasets generated using BE, Rényi, and Shannon entropy-maximizing policies, as well as the SMM and RND algorithms. We find that offline RL algorithms trained on datasets collected using BE outperform those trained on datasets collected using Shannon entropy, SMM, and RND on all tasks considered, and on 80% of the tasks compared to datasets collected using Rényi entropy.

### Synthetic Datasets for Machine Learning on Spatio-Temporal Graphs using PDEs 
[[arxiv](https://arxiv.org/abs/2502.04140)] [[cool](https://papers.cool/arxiv/2502.04140)] [[pdf](https://arxiv.org/pdf/2502.04140)]
> **Authors**: Jost Arndt,Utku Isil,Michael Detzel,Wojciech Samek,Jackie Ma
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: Currently under review
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Many physical processes can be expressed through partial differential equations (PDEs). Real-world measurements of such processes are often collected at irregularly distributed points in space, which can be effectively represented as graphs; however, there are currently only a few existing datasets. Our work aims to make advancements in the field of PDE-modeling accessible to the temporal graph machine learning community, while addressing the data scarcity problem, by creating and utilizing datasets based on PDEs. In this work, we create and use synthetic datasets based on PDEs to support spatio-temporal graph modeling in machine learning for different applications. More precisely, we showcase three equations to model different types of disasters and hazards in the fields of epidemiology, atmospheric particles, and tsunami waves. Further, we show how such created datasets can be used by benchmarking several machine learning models on the epidemiological dataset. Additionally, we show how pre-training on this dataset can improve model performance on real-world epidemiological data. The presented methods enable others to create datasets and benchmarks customized to individual requirements. The source code for our methodology and the three created datasets can be found on https://github.com/github-usr-ano/Temporal_Graph_Data_PDEs.

### Transfer Learning for Covert Speech Classification Using EEG Hilbert Envelope and Temporal Fine Structure 
[[arxiv](https://arxiv.org/abs/2502.04132)] [[cool](https://papers.cool/arxiv/2502.04132)] [[pdf](https://arxiv.org/pdf/2502.04132)]
> **Authors**: Saravanakumar Duraisamy,Mateusz Dubiel,Maurice Rekrut,Luis A. Leiva
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: Accepted to ICASSP 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Brain-Computer Interfaces (BCIs) can decode imagined speech from neural activity. However, these systems typically require extensive training sessions where participants imaginedly repeat words, leading to mental fatigue and difficulties identifying the onset of words, especially when imagining sequences of words. This paper addresses these challenges by transferring a classifier trained in overt speech data to covert speech classification. We used electroencephalogram (EEG) features derived from the Hilbert envelope and temporal fine structure, and used them to train a bidirectional long-short-term memory (BiLSTM) model for classification. Our method reduces the burden of extensive training and achieves state-of-the-art classification accuracy: 86.44% for overt speech and 79.82% for covert speech using the overt speech classifier.

### On the importance of structural identifiability for machine learning with partially observed dynamical systems 
[[arxiv](https://arxiv.org/abs/2502.04131)] [[cool](https://papers.cool/arxiv/2502.04131)] [[pdf](https://arxiv.org/pdf/2502.04131)]
> **Authors**: Janis Norden,Elisa Oostwal,Michael Chappell,Peter Tino,Kerstin Bunte
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 15 pages, 18 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The successful application of modern machine learning for time series classification is often hampered by limitations in quality and quantity of available training data. To overcome these limitations, available domain expert knowledge in the form of parametrised mechanistic dynamical models can be used whenever it is available and time series observations may be represented as an element from a given class of parametrised dynamical models. This makes the learning process interpretable and allows the modeller to deal with sparsely and irregularly sampled data in a natural way. However, the internal processes of a dynamical model are often only partially observed. This can lead to ambiguity regarding which particular model realization best explains a given time series observation. This problem is well-known in the literature, and a dynamical model with this issue is referred to as structurally unidentifiable. Training a classifier that incorporates knowledge about a structurally unidentifiable dynamical model can negatively influence classification performance. To address this issue, we employ structural identifiability analysis to explicitly relate parameter configurations that are associated with identical system outputs. Using the derived relations in classifier training, we demonstrate that this method significantly improves the classifier's ability to generalize to unseen data on a number of example models from the biomedical domain. This effect is especially pronounced when the number of training instances is limited. Our results demonstrate the importance of accounting for structural identifiability, a topic that has received relatively little attention from the machine learning community.

### Optimizing Perturbations for Improved Training of Machine Learning Models 
[[arxiv](https://arxiv.org/abs/2502.04121)] [[cool](https://papers.cool/arxiv/2502.04121)] [[pdf](https://arxiv.org/pdf/2502.04121)]
> **Authors**: Sagi Meir,Tommer D. Keidar,Shlomi Reuveni,Barak Hirshberg
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,无序系统和神经网络,化学物理
- **Abstract**: Machine learning models have become indispensable tools in applications across the physical sciences. Their training is often time-consuming, vastly exceeding the inference timescales. Several protocols have been developed to perturb the learning process and improve the training, such as shrink and perturb, warm restarts, and stochastic resetting. For classifiers, these perturbations have been shown to result in enhanced speedups or improved generalization. However, the design of such perturbations is usually done \textit{ad hoc} by intuition and trial and error. To rationally optimize training protocols, we frame them as first-passage processes and consider their response to perturbations. We show that if the unperturbed learning process reaches a quasi-steady state, the response at a single perturbation frequency can predict the behavior at a wide range of frequencies. We demonstrate that this is the case when training a CIFAR-10 classifier using the ResNet-18 model and use this approach to identify an optimal perturbation and frequency. Our work allows optimization of training protocols of machine learning models using a statistical mechanical approach.

### Generative Adversarial Networks Bridging Art and Machine Intelligence 
[[arxiv](https://arxiv.org/abs/2502.04116)] [[cool](https://papers.cool/arxiv/2502.04116)] [[pdf](https://arxiv.org/pdf/2502.04116)]
> **Authors**: Junhao Song,Yichao Zhang,Ziqian Bi,Tianyang Wang,Keyu Chen,Ming Li,Qian Niu,Junyu Liu,Benji Peng,Sen Zhang,Ming Liu,Jiawei Xu,Xuanhe Pan,Jinlang Wang,Pohsun Feng,Yizhu Wen,Lawrence K. Q. Yan,Hong-Ming Tseng,Xinyuan Song,Jintao Ren,Silin Chen,Yunze Wang,Weiche Hsieh,Bowen Jing,Junjie Yang, et al. (3 additional authors not shown)
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: Generative Adversarial Networks (GAN) have greatly influenced the development of computer vision and artificial intelligence in the past decade and also connected art and machine intelligence together. This book begins with a detailed introduction to the fundamental principles and historical development of GANs, contrasting them with traditional generative models and elucidating the core adversarial mechanisms through illustrative Python examples. The text systematically addresses the mathematical and theoretical underpinnings including probability theory, statistics, and game theory providing a solid framework for understanding the objectives, loss functions, and optimisation challenges inherent to GAN training. Subsequent chapters review classic variants such as Conditional GANs, DCGANs, InfoGAN, and LAPGAN before progressing to advanced training methodologies like Wasserstein GANs, GANs with gradient penalty, least squares GANs, and spectral normalisation techniques. The book further examines architectural enhancements and task-specific adaptations in generators and discriminators, showcasing practical implementations in high resolution image generation, artistic style transfer, video synthesis, text to image generation and other multimedia applications. The concluding sections offer insights into emerging research trends, including self-attention mechanisms, transformer-based generative models, and a comparative analysis with diffusion models, thus charting promising directions for future developments in both academic and applied settings.

### Smart IoT Security: Lightweight Machine Learning Techniques for Multi-Class Attack Detection in IoT Networks 
[[arxiv](https://arxiv.org/abs/2502.04057)] [[cool](https://papers.cool/arxiv/2502.04057)] [[pdf](https://arxiv.org/pdf/2502.04057)]
> **Authors**: Shahran Rahman Alve,Muhammad Zawad Mahmud,Samiha Islam,Md. Asaduzzaman Chowdhury,Jahirul Islam
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: Accepted in an international conference
- **标题**: None
- **领域**: 机器学习
- **Abstract**: In the growing terrain of the Internet of Things (IoT), it is vital that networks are secure to protect against a range of cyber threats. Based on the strong machine learning framework, this study proposes novel lightweight ensemble approaches for improving multi-class attack detection of IoT devices. Using the large CICIoT 2023 dataset with 34 attack types distributed amongst 10 attack categories, we systematically evaluated the performance of a wide variety of modern machine learning methods with the aim of establishing the best-performing algorithmic choice to secure IoT applications. In particular, we explore approaches based on ML classifiers to tackle the biocharges characterized by the challenging and heterogeneous nature of attack vectors in IoT environments. The method that performed best was the Decision Tree, with an accuracy of 99.56% and an F1 score of 99.62%, showing that this model is capable of accurately and reliably detecting threats.The Random Forest model was the next best-performing model with 98.22% and an F1 score of 98.24%, suggesting that ML methods are quite effective in a situation of high-dimensional data. Our results highlight the potential for using ML classifiers in bolstering security for IoT devices and also serve as motivations for future investigations targeting scalable, keystroke-based attack detection systems. We believe that our method provides a new path to develop complex machine learning algorithms for low-resource IoT devices, balancing both accuracy and time efficiency needs. In summary, these contributions enrich the state of the art of the IoT security literature, laying down solid ground and guidelines for the deployment of smart, adaptive security in IoT settings.

### TQ-DiT: Efficient Time-Aware Quantization for Diffusion Transformers 
[[arxiv](https://arxiv.org/abs/2502.04056)] [[cool](https://papers.cool/arxiv/2502.04056)] [[pdf](https://arxiv.org/pdf/2502.04056)]
> **Authors**: Younghye Hwang,Hyojin Lee,Joonhyuk Kang
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 8 pages
- **标题**: None
- **领域**: 机器学习,信号处理
- **Abstract**: Diffusion transformers (DiTs) combine transformer architectures with diffusion models. However, their computational complexity imposes significant limitations on real-time applications and sustainability of AI systems. In this study, we aim to enhance the computational efficiency through model quantization, which represents the weights and activation values with lower precision. Multi-region quantization (MRQ) is introduced to address the asymmetric distribution of network values in DiT blocks by allocating two scaling parameters to sub-regions. Additionally, time-grouping quantization (TGQ) is proposed to reduce quantization error caused by temporal variation in activations. The experimental results show that the proposed algorithm achieves performance comparable to the original full-precision model with only a 0.29 increase in FID at W8A8. Furthermore, it outperforms other baselines at W6A6, thereby confirming its suitability for low-bit quantization. These results highlight the potential of our method to enable efficient real-time generative models.

### Evaluating Inter-Column Logical Relationships in Synthetic Tabular Data Generation 
[[arxiv](https://arxiv.org/abs/2502.04055)] [[cool](https://papers.cool/arxiv/2502.04055)] [[pdf](https://arxiv.org/pdf/2502.04055)]
> **Authors**: Yunbo Long,Liming Xu,Alexandra Brintrup
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Current evaluations of synthetic tabular data mainly focus on how well joint distributions are modeled, often overlooking the assessment of their effectiveness in preserving realistic event sequences and coherent entity relationships across columns.This paper proposes three evaluation metrics designed to assess the preservation of logical relationships among columns in synthetic tabular data. We validate these metrics by assessing the performance of both classical and state-of-the-art generation methods on a real-world industrial dataset.Experimental results reveal that existing methods often fail to rigorously maintain logical consistency (e.g., hierarchical relationships in geography or organization) and dependencies (e.g., temporal sequences or mathematical relationships), which are crucial for preserving the fine-grained realism of real-world tabular data. Building on these insights, this study also discusses possible pathways to better capture logical relationships while modeling the distribution of synthetic tabular data.

### Precision Agriculture Revolution: Integrating Digital Twins and Advanced Crop Recommendation for Optimal Yield 
[[arxiv](https://arxiv.org/abs/2502.04054)] [[cool](https://papers.cool/arxiv/2502.04054)] [[pdf](https://arxiv.org/pdf/2502.04054)]
> **Authors**: Sayan Banerjee,Aniruddha Mukherjee,Suket Kamboj
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: With the help of a digital twin structure, Agriculture 4.0 technologies like weather APIs (Application programming interface), GPS (Global Positioning System) modules, and NPK (Nitrogen, Phosphorus and Potassium) soil sensors and machine learning recommendation models, we seek to revolutionize agricultural production through this concept. In addition to providing precise crop growth forecasts, the combination of real-time data on soil composition, meteorological dynamics, and geographic coordinates aims to support crop recommendation models and simulate predictive scenarios for improved water and pesticide management.

### Decision Trees That Remember: Gradient-Based Learning of Recurrent Decision Trees with Memory 
[[arxiv](https://arxiv.org/abs/2502.04052)] [[cool](https://papers.cool/arxiv/2502.04052)] [[pdf](https://arxiv.org/pdf/2502.04052)]
> **Authors**: Sascha Marton,Moritz Schneider
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Neural architectures such as Recurrent Neural Networks (RNNs), Transformers, and State-Space Models have shown great success in handling sequential data by learning temporal dependencies. Decision Trees (DTs), on the other hand, remain a widely used class of models for structured tabular data but are typically not designed to capture sequential patterns directly. Instead, DT-based approaches for time-series data often rely on feature engineering, such as manually incorporating lag features, which can be suboptimal for capturing complex temporal dependencies. To address this limitation, we introduce ReMeDe Trees, a novel recurrent DT architecture that integrates an internal memory mechanism, similar to RNNs, to learn long-term dependencies in sequential data. Our model learns hard, axis-aligned decision rules for both output generation and state updates, optimizing them efficiently via gradient descent. We provide a proof-of-concept study on synthetic benchmarks to demonstrate the effectiveness of our approach.

### Comparing privacy notions for protection against reconstruction attacks in machine learning 
[[arxiv](https://arxiv.org/abs/2502.04045)] [[cool](https://papers.cool/arxiv/2502.04045)] [[pdf](https://arxiv.org/pdf/2502.04045)]
> **Authors**: Sayan Biswas,Mark Dras,Pedro Faustini,Natasha Fernandes,Annabelle McIver,Catuscia Palamidessi,Parastoo Sadeghi
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,密码学和安全,信息论
- **Abstract**: Within the machine learning community, reconstruction attacks are a principal concern and have been identified even in federated learning (FL), which was designed with privacy preservation in mind. In response to these threats, the privacy community recommends the use of differential privacy (DP) in the stochastic gradient descent algorithm, termed DP-SGD. However, the proliferation of variants of DP in recent years\textemdash such as metric privacy\textemdash has made it challenging to conduct a fair comparison between different mechanisms due to the different meanings of the privacy parameters $ε$ and $δ$ across different variants. Thus, interpreting the practical implications of $ε$ and $δ$ in the FL context and amongst variants of DP remains ambiguous. In this paper, we lay a foundational framework for comparing mechanisms with differing notions of privacy guarantees, namely $(ε,δ)$-DP and metric privacy. We provide two foundational means of comparison: firstly, via the well-established $(ε,δ)$-DP guarantees, made possible through the Rényi differential privacy framework; and secondly, via Bayes' capacity, which we identify as an appropriate measure for reconstruction threats.

### Probe-Free Low-Rank Activation Intervention 
[[arxiv](https://arxiv.org/abs/2502.04043)] [[cool](https://papers.cool/arxiv/2502.04043)] [[pdf](https://arxiv.org/pdf/2502.04043)]
> **Authors**: Chonghe Jiang,Bao Nguyen,Anthony Man-Cho So,Viet Anh Nguyen
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: Accepted by NAACL 2025
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Language models (LMs) can produce texts that appear accurate and coherent but contain untruthful or toxic content. Inference-time interventions that edit the hidden activations have shown promising results in steering the LMs towards desirable generations. Existing activation intervention methods often comprise an activation probe to detect undesirable generation, triggering the activation modification to steer subsequent generation. This paper proposes a probe-free intervention method FLORAIN for all attention heads in a specific activation layer. It eliminates the need to train classifiers for probing purposes. The intervention function is parametrized by a sample-wise nonlinear low-rank mapping, which is trained by minimizing the distance between the modified activations and their projection onto the manifold of desirable content. Under specific constructions of the manifold and projection distance, we show that the intervention strategy can be computed efficiently by solving a smooth optimization problem. The empirical results, benchmarked on multiple base models, demonstrate that FLORAIN consistently outperforms several baseline methods in enhancing model truthfulness and quality across generation and multiple-choice tasks.

### Leveraging Reasoning with Guidelines to Elicit and Utilize Knowledge for Enhancing Safety Alignment 
[[arxiv](https://arxiv.org/abs/2502.04040)] [[cool](https://papers.cool/arxiv/2502.04040)] [[pdf](https://arxiv.org/pdf/2502.04040)]
> **Authors**: Haoyu Wang,Zeyu Qin,Li Shen,Xueqian Wang,Minhao Cheng,Dacheng Tao
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: The first two authors contributed equally
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Training safe LLMs is one of the most critical research challenge. However, the commonly used method, Refusal Training (RT), struggles to generalize against various OOD jailbreaking attacks. Many safety training methods have been proposed to address this issue. While they offer valuable insights, we aim to complement this line of research by investigating whether OOD attacks truly exceed the capability of RT model. Conducting evaluation with BoN, we observe significant improvements on generalization as N increases. This underscores that the model possesses sufficient safety-related latent knowledge, but RT fails to consistently elicit this knowledge when addressing OOD attacks. Further analysis based on domain adaptation reveals that training with direct refusal causes model to rely on superficial shortcuts, resulting in learning of non-robust representation mappings. Based on our findings, we propose training model to perform safety reasoning for each query. Reasoning supervision encourages model to perform more computations, explicitly eliciting and using latent knowledge through reasoning. To achieve this, we synthesize reasoning supervision based on pre-guidelines, training the model to reason in alignment with them, thereby effectively eliciting and utilizing latent knowledge from diverse perspectives. Extensive experiments show that our method significantly improves generalization performance against OOD attacks.

### Generalize Drug Response Prediction by Latent Independent Projection for Asymmetric Constrained Domain Generalization 
[[arxiv](https://arxiv.org/abs/2502.04034)] [[cool](https://papers.cool/arxiv/2502.04034)] [[pdf](https://arxiv.org/pdf/2502.04034)]
> **Authors**: Ran Song,Yinpu Bai,Hui Liu
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: The accurate prediction of drug responses remains a formidable challenge, particularly at the single-cell level and in clinical treatment contexts. Some studies employ transfer learning techniques to predict drug responses in individual cells and patients, but they require access to target-domain data during training, which is often unavailable or only obtainable in future. In this study, we propose a novel domain generalization framework, termed panCancerDR, to address this challenge. We conceptualize each cancer type as a distinct source domain, with its cell lines serving as domain-specific samples. Our primary objective is to extract domain-invariant features from the expression profiles of cell lines across diverse cancer types, thereby generalize the predictive capacity to out-of-distribution samples. To enhance robustness, we introduce a latent independence projection (LIP) module that encourages the encoder to extract informative yet non-redundant features. Also, we propose an asymmetric adaptive clustering constraint, which clusters drug-sensitive samples into a compact group while drives resistant samples dispersed across separate clusters in the latent space. Our empirical experiments demonstrate that panCancerDR effectively learns task-relevant features from diverse source domains, and achieves accurate predictions of drug response for unseen cancer type during training. Furthermore, when evaluated on single-cell and patient-level prediction tasks, our model-trained solely on in vitro cell line data without access to target-domain information-consistently outperforms and matched current state-of-the-art methods. These findings highlights the potential of our method for real-world clinical applications.

### Deep Meta Coordination Graphs for Multi-agent Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.04028)] [[cool](https://papers.cool/arxiv/2502.04028)] [[pdf](https://arxiv.org/pdf/2502.04028)]
> **Authors**: Nikunj Gupta,James Zachary Hare,Rajgopal Kannan,Viktor Prasanna
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: This paper presents deep meta coordination graphs (DMCG) for learning cooperative policies in multi-agent reinforcement learning (MARL). Coordination graph formulations encode local interactions and accordingly factorize the joint value function of all agents to improve efficiency in MARL. However, existing approaches rely solely on pairwise relations between agents, which potentially oversimplifies complex multi-agent interactions. DMCG goes beyond these simple direct interactions by also capturing useful higher-order and indirect relationships among agents. It generates novel graph structures accommodating multiple types of interactions and arbitrary lengths of multi-hop connections in coordination graphs to model such interactions. It then employs a graph convolutional network module to learn powerful representations in an end-to-end manner. We demonstrate its effectiveness in multiple coordination problems in MARL where other state-of-the-art methods can suffer from sample inefficiency or fail entirely. All codes can be found here: https://github.com/Nikunj-Gupta/dmcg-marl.

### Variational Quantum Optimization with Continuous Bandits 
[[arxiv](https://arxiv.org/abs/2502.04021)] [[cool](https://papers.cool/arxiv/2502.04021)] [[pdf](https://arxiv.org/pdf/2502.04021)]
> **Authors**: Marc Wanner,Johan Jonasson,Emil Carlsson,Devdatt Dubhashi
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 8 pages, 3 Figures + 7-page appendix
- **标题**: None
- **领域**: 机器学习,量子物理学
- **Abstract**: We introduce a novel approach to variational Quantum algorithms (VQA) via continuous bandits. VQA are a class of hybrid Quantum-classical algorithms where the parameters of Quantum circuits are optimized by classical algorithms. Previous work has used zero and first order gradient based methods, however such algorithms suffer from the barren plateau (BP) problem where gradients and loss differences are exponentially small. We introduce an approach using bandits methods which combine global exploration with local exploitation. We show how VQA can be formulated as a best arm identification problem in a continuous space of arms with Lipschitz smoothness. While regret minimization has been addressed in this setting, existing methods for pure exploration only cover discrete spaces. We give the first results for pure exploration in a continuous setting and derive a fixed-confidence, information-theoretic, instance specific lower bound. Under certain assumptions on the expected payoff, we derive a simple algorithm, which is near-optimal with respect to our lower bound. Finally, we apply our continuous bandit algorithm to two VQA schemes: a PQC and a QAOA quantum circuit, showing that we significantly outperform the previously known state of the art methods (which used gradient based methods).

### PINT: Physics-Informed Neural Time Series Models with Applications to Long-term Inference on WeatherBench 2m-Temperature Data 
[[arxiv](https://arxiv.org/abs/2502.04018)] [[cool](https://papers.cool/arxiv/2502.04018)] [[pdf](https://arxiv.org/pdf/2502.04018)]
> **Authors**: Keon Vin Park,Jisu Kim,Jaemin Seo
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: This paper introduces PINT (Physics-Informed Neural Time Series Models), a framework that integrates physical constraints into neural time series models to improve their ability to capture complex dynamics. We apply PINT to the ERA5 WeatherBench dataset, focusing on long-term forecasting of 2m-temperature data. PINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed prior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures. This equation's analytical solutions (sine and cosine functions) facilitate rigorous evaluation of the benefits of incorporating physics-informed constraints. By benchmarking against a linear regression baseline derived from its exact solutions, we quantify the impact of embedding physical principles in data-driven models. Unlike traditional time series models that rely on future observations, PINT is designed for practical forecasting. Using only the first 90 days of observed data, it iteratively predicts the next two years, addressing challenges posed by limited real-time updates. Experiments on the WeatherBench dataset demonstrate PINT's ability to generalize, capture periodic trends, and align with physical principles. This study highlights the potential of physics-informed neural models in bridging machine learning and interpretable climate applications. Our models and datasets are publicly available on GitHub: https://github.com/KV-Park.

### Near-optimal Regret Using Policy Optimization in Online MDPs with Aggregate Bandit Feedback 
[[arxiv](https://arxiv.org/abs/2502.04004)] [[cool](https://papers.cool/arxiv/2502.04004)] [[pdf](https://arxiv.org/pdf/2502.04004)]
> **Authors**: Tal Lancewicki,Yishay Mansour
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We study online finite-horizon Markov Decision Processes with adversarially changing loss and aggregate bandit feedback (a.k.a full-bandit). Under this type of feedback, the agent observes only the total loss incurred over the entire trajectory, rather than the individual losses at each intermediate step within the trajectory. We introduce the first Policy Optimization algorithms for this setting. In the known-dynamics case, we achieve the first \textit{optimal} regret bound of $\tilde Θ(H^2\sqrt{SAK})$, where $K$ is the number of episodes, $H$ is the episode horizon, $S$ is the number of states, and $A$ is the number of actions. In the unknown dynamics case we establish regret bound of $\tilde O(H^3 S \sqrt{AK})$, significantly improving the best known result by a factor of $H^2 S^5 A^2$.

### Online Learning of Counter Categories and Ratings in PvP Games 
[[arxiv](https://arxiv.org/abs/2502.03998)] [[cool](https://papers.cool/arxiv/2502.03998)] [[pdf](https://arxiv.org/pdf/2502.03998)]
> **Authors**: Chiu-Chou Lin,I-Chen Wu
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机科学与博弈论,多代理系统
- **Abstract**: In competitive games, strength ratings like Elo are widely used to quantify player skill and support matchmaking by accounting for skill disparities better than simple win rate statistics. However, scalar ratings cannot handle complex intransitive relationships, such as counter strategies seen in Rock-Paper-Scissors. To address this, recent work introduced Neural Rating Table and Neural Counter Table, which combine scalar ratings with discrete counter categories to model intransitivity. While effective, these methods rely on neural network training and cannot perform real-time updates. In this paper, we propose an online update algorithm that extends Elo principles to incorporate real-time learning of counter categories. Our method dynamically adjusts both ratings and counter relationships after each match, preserving the explainability of scalar ratings while addressing intransitivity. Experiments on zero-sum competitive games demonstrate its practicality, particularly in scenarios without complex team compositions.

### Tight Bounds on Jensen's Gap: Novel Approach with Applications in Generative Modeling 
[[arxiv](https://arxiv.org/abs/2502.03988)] [[cool](https://papers.cool/arxiv/2502.03988)] [[pdf](https://arxiv.org/pdf/2502.03988)]
> **Authors**: Marcin Mazur,Piotr Kościelniak,Łukasz Struski
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Among various mathematical tools of particular interest are those that provide a common basis for researchers in different scientific fields. One of them is Jensen's inequality, which states that the expectation of a convex function is greater than or equal to the function evaluated at the expectation. The resulting difference, known as Jensen's gap, became the subject of investigation by both the statistical and machine learning communities. Among many related topics, finding lower and upper bounds on Jensen's gap (under different assumptions on the underlying function and distribution) has recently become a problem of particular interest. In our paper, we take another step in this direction by providing a novel general and mathematically rigorous technique, motivated by the recent results of Struski et al. (2023). In addition, by studying in detail the case of the logarithmic function and the log-normal distribution, we explore a method for tightly estimating the log-likelihood of generative models trained on real-world datasets. Furthermore, we present both analytical and experimental arguments in support of the superiority of our approach in comparison to existing state-of-the-art solutions, contingent upon fulfillment of the criteria set forth by theoretical studies and corresponding experiments on synthetic data.

### Temporal Distribution Shift in Real-World Pharmaceutical Data: Implications for Uncertainty Quantification in QSAR Models 
[[arxiv](https://arxiv.org/abs/2502.03982)] [[cool](https://papers.cool/arxiv/2502.03982)] [[pdf](https://arxiv.org/pdf/2502.03982)]
> **Authors**: Hannah Rosa Friesacher,Emma Svensson,Susanne Winiwarter,Lewis Mervin,Adam Arany,Ola Engkvist
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The estimation of uncertainties associated with predictions from quantitative structure-activity relationship (QSAR) models can accelerate the drug discovery process by identifying promising experiments and allowing an efficient allocation of resources. Several computational tools exist that estimate the predictive uncertainty in machine learning models. However, deviations from the i.i.d. setting have been shown to impair the performance of these uncertainty quantification methods. We use a real-world pharmaceutical dataset to address the pressing need for a comprehensive, large-scale evaluation of uncertainty estimation methods in the context of realistic distribution shifts over time. We investigate the performance of several uncertainty estimation methods, including ensemble-based and Bayesian approaches. Furthermore, we use this real-world setting to systematically assess the distribution shifts in label and descriptor space and their impact on the capability of the uncertainty estimation methods. Our study reveals significant shifts over time in both label and descriptor space and a clear connection between the magnitude of the shift and the nature of the assay. Moreover, we show that pronounced distribution shifts impair the performance of popular uncertainty estimation methods used in QSAR models. This work highlights the challenges of identifying uncertainty quantification methods that remain reliable under distribution shifts introduced by real-world data.

### Innovative Framework for Early Estimation of Mental Disorder Scores to Enable Timely Interventions 
[[arxiv](https://arxiv.org/abs/2502.03965)] [[cool](https://papers.cool/arxiv/2502.03965)] [[pdf](https://arxiv.org/pdf/2502.03965)]
> **Authors**: Himanshi Singh,Sadhana Tiwari,Sonali Agarwal,Ritesh Chandra,Sanjay Kumar Sonbhadra,Vrijendra Singh
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Individual's general well-being is greatly impacted by mental health conditions including depression and Post-Traumatic Stress Disorder (PTSD), underscoring the importance of early detection and precise diagnosis in order to facilitate prompt clinical intervention. An advanced multimodal deep learning system for the automated classification of PTSD and depression is presented in this paper. Utilizing textual and audio data from clinical interview datasets, the method combines features taken from both modalities by combining the architectures of LSTM (Long Short Term Memory) and BiLSTM (Bidirectional Long Short-Term Memory).Although text features focus on speech's semantic and grammatical components; audio features capture vocal traits including rhythm, tone, and pitch. This combination of modalities enhances the model's capacity to identify minute patterns connected to mental health conditions. Using test datasets, the proposed method achieves classification accuracies of 92% for depression and 93% for PTSD, outperforming traditional unimodal approaches and demonstrating its accuracy and robustness.

### AL-PINN: Active Learning-Driven Physics-Informed Neural Networks for Efficient Sample Selection in Solving Partial Differential Equations 
[[arxiv](https://arxiv.org/abs/2502.03963)] [[cool](https://papers.cool/arxiv/2502.03963)] [[pdf](https://arxiv.org/pdf/2502.03963)]
> **Authors**: Keon Vin Park
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Physics-Informed Neural Networks (PINNs) have emerged as a promising approach for solving Partial Differential Equations (PDEs) by incorporating physical constraints into deep learning models. However, standard PINNs often require a large number of training samples to achieve high accuracy, leading to increased computational costs. To address this issue, we propose Active Learning-Driven PINNs (AL-PINN), which integrates Uncertainty Quantification (UQ) and Active Learning (AL) strategies to optimize sample selection dynamically. AL-PINN utilizes Monte Carlo Dropout to estimate epistemic uncertainty in the model predictions, enabling the adaptive selection of high-uncertainty regions for additional training. This approach significantly enhances learning efficiency by focusing computational resources on the most informative data points. We evaluate AL-PINN on benchmark PDE problems with known analytical solutions and real-world WeatherBench climate data. Our results demonstrate that AL-PINN achieves comparable or superior accuracy compared to traditional PINNs while reducing the number of required training samples. The proposed framework is particularly beneficial for scientific and engineering applications where data collection is expensive or limited, such as climate modeling, medical simulations, and material science. Our findings highlight the potential of active learning in accelerating PINN-based PDE solvers while maintaining high accuracy and computational efficiency.

### Non-convex composite federated learning with heterogeneous data 
[[arxiv](https://arxiv.org/abs/2502.03958)] [[cool](https://papers.cool/arxiv/2502.03958)] [[pdf](https://arxiv.org/pdf/2502.03958)]
> **Authors**: Jiaojiao Zhang,Jiang Hu,Mikael Johansson
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,分布式、并行和集群计算
- **Abstract**: We propose an innovative algorithm for non-convex composite federated learning that decouples the proximal operator evaluation and the communication between server and clients. Moreover, each client uses local updates to communicate less frequently with the server, sends only a single d-dimensional vector per communication round, and overcomes issues with client drift. In the analysis, challenges arise from the use of decoupling strategies and local updates in the algorithm, as well as from the non-convex and non-smooth nature of the problem. We establish sublinear and linear convergence to a bounded residual error under general non-convexity and the proximal Polyak-Lojasiewicz inequality, respectively. In the numerical experiments, we demonstrate the superiority of our algorithm over state-of-the-art methods on both synthetic and real datasets.

### Bridging the inference gap in Mutimodal Variational Autoencoders 
[[arxiv](https://arxiv.org/abs/2502.03952)] [[cool](https://papers.cool/arxiv/2502.03952)] [[pdf](https://arxiv.org/pdf/2502.03952)]
> **Authors**: Agathe Senellart,Stéphanie Allassonnière
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: From medical diagnosis to autonomous vehicles, critical applications rely on the integration of multiple heterogeneous data modalities. Multimodal Variational Autoencoders offer versatile and scalable methods for generating unobserved modalities from observed ones. Recent models using mixturesof-experts aggregation suffer from theoretically grounded limitations that restrict their generation quality on complex datasets. In this article, we propose a novel interpretable model able to learn both joint and conditional distributions without introducing mixture aggregation. Our model follows a multistage training process: first modeling the joint distribution with variational inference and then modeling the conditional distributions with Normalizing Flows to better approximate true posteriors. Importantly, we also propose to extract and leverage the information shared between modalities to improve the conditional coherence of generated samples. Our method achieves state-of-the-art results on several benchmark datasets.

### CleanSurvival: Automated data preprocessing for time-to-event models using reinforcement learning 
[[arxiv](https://arxiv.org/abs/2502.03946)] [[cool](https://papers.cool/arxiv/2502.03946)] [[pdf](https://arxiv.org/pdf/2502.03946)]
> **Authors**: Yousef Koka,David Selby,Gerrit Großmann,Sebastian Vollmer
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Data preprocessing is a critical yet frequently neglected aspect of machine learning, often paid little attention despite its potentially significant impact on model performance. While automated machine learning pipelines are starting to recognize and integrate data preprocessing into their solutions for classification and regression tasks, this integration is lacking for more specialized tasks like survival or time-to-event models. As a result, survival analysis not only faces the general challenges of data preprocessing but also suffers from the lack of tailored, automated solutions in this area. To address this gap, this paper presents 'CleanSurvival', a reinforcement-learning-based solution for optimizing preprocessing pipelines, extended specifically for survival analysis. The framework can handle continuous and categorical variables, using Q-learning to select which combination of data imputation, outlier detection and feature extraction techniques achieves optimal performance for a Cox, random forest, neural network or user-supplied time-to-event model. The package is available on GitHub: https://github.com/datasciapps/CleanSurvival Experimental benchmarks on real-world datasets show that the Q-learning-based data preprocessing results in superior predictive performance to standard approaches, finding such a model up to 10 times faster than undirected random grid search. Furthermore, a simulation study demonstrates the effectiveness in different types and levels of missingness and noise in the data.

### Multimodal Data-Driven Classification of Mental Disorders: A Comprehensive Approach to Diagnosing Depression, Anxiety, and Schizophrenia 
[[arxiv](https://arxiv.org/abs/2502.03943)] [[cool](https://papers.cool/arxiv/2502.03943)] [[pdf](https://arxiv.org/pdf/2502.03943)]
> **Authors**: Himanshi Singh,Sadhana Tiwari,Sonali Agarwal,Ritesh Chandra,Sanjay Kumar Sonbhadra,Vrijendra Singh
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: This study investigates the potential of multimodal data integration, which combines electroencephalogram (EEG) data with sociodemographic characteristics like age, sex, education, and intelligence quotient (IQ), to diagnose mental diseases like schizophrenia, depression, and anxiety. Using Apache Spark and convolutional neural networks (CNNs), a data-driven classification pipeline has been developed for big data environment to effectively analyze massive datasets. In order to evaluate brain activity and connection patterns associated with mental disorders, EEG parameters such as power spectral density (PSD) and coherence are examined. The importance of coherence features is highlighted by comparative analysis, which shows significant improvement in classification accuracy and robustness. This study emphasizes the significance of holistic approaches for efficient diagnostic tools by integrating a variety of data sources. The findings open the door for creative, data-driven approaches to treating psychiatric diseases by demonstrating the potential of utilizing big data, sophisticated deep learning methods, and multimodal datasets to enhance the precision, usability, and comprehension of mental health diagnostics.

### Unravelling Causal Genetic Biomarkers of Alzheimer's Disease via Neuron to Gene-token Backtracking in Neural Architecture: A Groundbreaking Reverse-Gene-Finder Approach 
[[arxiv](https://arxiv.org/abs/2502.03938)] [[cool](https://papers.cool/arxiv/2502.03938)] [[pdf](https://arxiv.org/pdf/2502.03938)]
> **Authors**: Victor OK Li,Yang Han,Jacqueline CK Lam
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Alzheimer's Disease (AD) affects over 55 million people globally, yet the key genetic contributors remain poorly understood. Leveraging recent advancements in genomic foundation models, we present the innovative Reverse-Gene-Finder technology, a ground-breaking neuron-to-gene-token backtracking approach in a neural network architecture to elucidate the novel causal genetic biomarkers driving AD onset. Reverse-Gene-Finder comprises three key innovations. Firstly, we exploit the observation that genes with the highest probability of causing AD, defined as the most causal genes (MCGs), must have the highest probability of activating those neurons with the highest probability of causing AD, defined as the most causal neurons (MCNs). Secondly, we utilize a gene token representation at the input layer to allow each gene (known or novel to AD) to be represented as a discrete and unique entity in the input space. Lastly, in contrast to the existing neural network architectures, which track neuron activations from the input layer to the output layer in a feed-forward manner, we develop an innovative backtracking method to track backwards from the MCNs to the input layer, identifying the Most Causal Tokens (MCTs) and the corresponding MCGs. Reverse-Gene-Finder is highly interpretable, generalizable, and adaptable, providing a promising avenue for application in other disease scenarios.

### Quantifying Correlations of Machine Learning Models 
[[arxiv](https://arxiv.org/abs/2502.03937)] [[cool](https://papers.cool/arxiv/2502.03937)] [[pdf](https://arxiv.org/pdf/2502.03937)]
> **Authors**: Yuanyuan Li,Neeraj Sarna,Yang Lin
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,数值分析
- **Abstract**: Machine Learning models are being extensively used in safety critical applications where errors from these models could cause harm to the user. Such risks are amplified when multiple machine learning models, which are deployed concurrently, interact and make errors simultaneously. This paper explores three scenarios where error correlations between multiple models arise, resulting in such aggregated risks. Using real-world data, we simulate these scenarios and quantify the correlations in errors of different models. Our findings indicate that aggregated risks are substantial, particularly when models share similar algorithms, training datasets, or foundational models. Overall, we observe that correlations across models are pervasive and likely to intensify with increased reliance on foundational models and widely used public datasets, highlighting the need for effective mitigation strategies to address these challenges.

### HEP-JEPA: A foundation model for collider physics using joint embedding predictive architecture 
[[arxiv](https://arxiv.org/abs/2502.03933)] [[cool](https://papers.cool/arxiv/2502.03933)] [[pdf](https://arxiv.org/pdf/2502.03933)]
> **Authors**: Jai Bardhan,Radhikesh Agrawal,Abhiram Tilak,Cyrin Neeraj,Subhadip Mitra
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 11 pages, 3 figures, 8 tables. Project website: https://hep-jepa.github.io/
- **标题**: None
- **领域**: 机器学习,高能物理-实验,高能物理-现象学
- **Abstract**: We present a transformer architecture-based foundation model for tasks at high-energy particle colliders such as the Large Hadron Collider. We train the model to classify jets using a self-supervised strategy inspired by the Joint Embedding Predictive Architecture. We use the JetClass dataset containing 100M jets of various known particles to pre-train the model with a data-centric approach -- the model uses a fraction of the jet constituents as the context to predict the embeddings of the unseen target constituents. Our pre-trained model fares well with other datasets for standard classification benchmark tasks. We test our model on two additional downstream tasks: top tagging and differentiating light-quark jets from gluon jets. We also evaluate our model with task-specific metrics and baselines and compare it with state-of-the-art models in high-energy physics. Project site: https://hep-jepa.github.io/

### Rank Also Matters: Hierarchical Configuration for Mixture of Adapter Experts in LLM Fine-Tuning 
[[arxiv](https://arxiv.org/abs/2502.03884)] [[cool](https://papers.cool/arxiv/2502.03884)] [[pdf](https://arxiv.org/pdf/2502.03884)]
> **Authors**: Peizhuang Cong,Wenpu Liu,Wenhan Yu,Haochen Zhao,Tong Yang
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Large language models (LLMs) have demonstrated remarkable success across various tasks, accompanied by a continuous increase in their parameter size. Parameter-efficient fine-tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), address the challenges of fine-tuning LLMs by significantly reducing the number of trainable parameters. Recent studies have integrated LoRA with Mixture of Experts (MoE) architectures, leveraging multiple adapter experts and gating mechanisms to further improve fine-tuning performance. However, existing approaches primarily focus on adjusting the allocations of adapter experts per layer to optimize the introduced trainable parameter size, while neglecting a critical factor of adapters' rank. To this end, we propose a hierarchical scheme for expert allocation and rank configuration, HILO, which dynamically adjusts the number and rank of adapter experts across layers, matching the varying representational complexity of model layers in adapter-granularity. Extensive experiments on multiple benchmark tasks demonstrate that HILO outperforms existing methods in accuracy while introducing fewer trainable parameters, providing an efficient and practical solution for fine-tuning LLMs.

### Position: Untrained Machine Learning for Anomaly Detection 
[[arxiv](https://arxiv.org/abs/2502.03876)] [[cool](https://papers.cool/arxiv/2502.03876)] [[pdf](https://arxiv.org/pdf/2502.03876)]
> **Authors**: Juan Du,Dongheng Chen,Hao Yan
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 6 pages,0 figure
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Anomaly detection based on 3D point cloud data is an important research problem and receives more and more attention recently. Untrained anomaly detection based on only one sample is an emerging research problem motivated by real manufacturing industries such as personalized manufacturing that only one sample can be collected without any additional labels. How to accurately identify anomalies based on one 3D point cloud sample is a critical challenge in both industrial applications and the field of machine learning. This paper aims to provide a formal definition of untrained anomaly detection problem based on 3D point cloud data, discuss the differences between untrained anomaly detection and current unsupervised anomaly detection methods. Unlike unsupervised learning, untrained methods do not rely on any data, including unlabeled data. Instead, they leverage prior knowledge about the manufacturing surfaces and anomalies. Examples are used to illustrate these prior knowledge and untrained machine learning model. Afterwards, literature review on untrained anomaly detection based on 3D point cloud data is also provided, and the potential of untrained deep neural networks for anomaly detection is also discussed as outlooks.

### Mirror Descent Actor Critic via Bounded Advantage Learning 
[[arxiv](https://arxiv.org/abs/2502.03854)] [[cool](https://papers.cool/arxiv/2502.03854)] [[pdf](https://arxiv.org/pdf/2502.03854)]
> **Authors**: Ryo Iwaki
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Regularization is a core component of recent Reinforcement Learning (RL) algorithms. Mirror Descent Value Iteration (MDVI) uses both Kullback-Leibler divergence and entropy as regularizers in its value and policy updates. Despite its empirical success in discrete action domains and strong theoretical guarantees, the performance of a MDVI-based method does not surpass an entropy-only-regularized method in continuous action domains. In this study, we propose Mirror Descent Actor Critic (MDAC) as an actor-critic style instantiation of MDVI for continuous action domains, and show that its empirical performance is significantly boosted by bounding the actor's log-density terms in the critic's loss function, compared to a non-bounded naive instantiation. Further, we relate MDAC to Advantage Learning by recalling that the actor's log-probability is equal to the regularized advantage function in tabular cases, and theoretically discuss when and why bounding the advantage terms is validated and beneficial. We also empirically explore a good choice for the bounding function, and show that MDAC perfoms better than strong non-regularized and entropy-only-regularized methods with an appropriate choice of the bounding function.

### Graph Neural Network-Driven Hierarchical Mining for Complex Imbalanced Data 
[[arxiv](https://arxiv.org/abs/2502.03803)] [[cool](https://papers.cool/arxiv/2502.03803)] [[pdf](https://arxiv.org/pdf/2502.03803)]
> **Authors**: Yijiashun Qi,Quanchao Lu,Shiyu Dou,Xiaoxuan Sun,Muqing Li,Yankaiqi Li
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: This study presents a hierarchical mining framework for high-dimensional imbalanced data, leveraging a depth graph model to address the inherent performance limitations of conventional approaches in handling complex, high-dimensional data distributions with imbalanced sample representations. By constructing a structured graph representation of the dataset and integrating graph neural network (GNN) embeddings, the proposed method effectively captures global interdependencies among samples. Furthermore, a hierarchical strategy is employed to enhance the characterization and extraction of minority class feature patterns, thereby facilitating precise and robust imbalanced data mining. Empirical evaluations across multiple experimental scenarios validate the efficacy of the proposed approach, demonstrating substantial improvements over traditional methods in key performance metrics, including pattern discovery count, average support, and minority class coverage. Notably, the method exhibits superior capabilities in minority-class feature extraction and pattern correlation analysis. These findings underscore the potential of depth graph models, in conjunction with hierarchical mining strategies, to significantly enhance the efficiency and accuracy of imbalanced data analysis. This research contributes a novel computational framework for high-dimensional complex data processing and lays the foundation for future extensions to dynamically evolving imbalanced data and multi-modal data applications, thereby expanding the applicability of advanced data mining methodologies to more intricate analytical domains.

### MXMap: A Multivariate Cross Mapping Framework for Causal Discovery in Dynamical Systems 
[[arxiv](https://arxiv.org/abs/2502.03802)] [[cool](https://papers.cool/arxiv/2502.03802)] [[pdf](https://arxiv.org/pdf/2502.03802)]
> **Authors**: Elise Zhang,François Mirallès,Raphaël Rousseau-Rizzi,Arnaud Zinflou,Di Wu,Benoit Boulet
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: Accepted by CLeaR 2025; Main manuscript 18 pages, appendix 24 pages, 30 tables
- **标题**: None
- **领域**: 机器学习,动力系统,方法论
- **Abstract**: Convergent Cross Mapping (CCM) is a powerful method for detecting causality in coupled nonlinear dynamical systems, providing a model-free approach to capture dynamic causal interactions. Partial Cross Mapping (PCM) was introduced as an extension of CCM to address indirect causality in three-variable systems by comparing cross-mapping quality between direct cause-effect mapping and indirect mapping through an intermediate conditioning variable. However, PCM remains limited to univariate delay embeddings in its cross-mapping processes. In this work, we extend PCM to the multivariate setting, introducing multiPCM, which leverages multivariate embeddings to more effectively distinguish indirect causal relationships. We further propose a multivariate cross-mapping framework (MXMap) for causal discovery in dynamical systems. This two-phase framework combines (1) pairwise CCM tests to establish an initial causal graph and (2) multiPCM to refine the graph by pruning indirect causal connections. Through experiments on simulated data and the ERA5 Reanalysis weather dataset, we demonstrate the effectiveness of MXMap. Additionally, MXMap is compared against several baseline methods, showing advantages in accuracy and causal graph refinement.

### Network-Wide Traffic Flow Estimation Across Multiple Cities with Global Open Multi-Source Data: A Large-Scale Case Study in Europe and North America 
[[arxiv](https://arxiv.org/abs/2502.03798)] [[cool](https://papers.cool/arxiv/2502.03798)] [[pdf](https://arxiv.org/pdf/2502.03798)]
> **Authors**: Zijian Hu,Zhenjie Zheng,Monica Menendez,Wei Ma
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Network-wide traffic flow, which captures dynamic traffic volume on each link of a general network, is fundamental to smart mobility applications. However, the observed traffic flow from sensors is usually limited across the entire network due to the associated high installation and maintenance costs. To address this issue, existing research uses various supplementary data sources to compensate for insufficient sensor coverage and estimate the unobserved traffic flow. Although these studies have shown promising results, the inconsistent availability and quality of supplementary data across cities make their methods typically face a trade-off challenge between accuracy and generality. In this research, we first time advocate using the Global Open Multi-Source (GOMS) data within an advanced deep learning framework to break the trade-off. The GOMS data primarily encompass geographical and demographic information, including road topology, building footprints, and population density, which can be consistently collected across cities. More importantly, these GOMS data are either causes or consequences of transportation activities, thereby creating opportunities for accurate network-wide flow estimation. Furthermore, we use map images to represent GOMS data, instead of traditional tabular formats, to capture richer and more comprehensive geographical and demographic information. To address multi-source data fusion, we develop an attention-based graph neural network that effectively extracts and synthesizes information from GOMS maps while simultaneously capturing spatiotemporal traffic dynamics from observed traffic data. A large-scale case study across 15 cities in Europe and North America was conducted. The results demonstrate stable and satisfactory estimation accuracy across these cities, which suggests that the trade-off challenge can be successfully addressed using our approach.

### Distribution learning via neural differential equations: minimal energy regularization and approximation theory 
[[arxiv](https://arxiv.org/abs/2502.03795)] [[cool](https://papers.cool/arxiv/2502.03795)] [[pdf](https://arxiv.org/pdf/2502.03795)]
> **Authors**: Youssef Marzouk,Zhi Ren,Jakob Zech
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,经典分析和常微分方程,方法论,机器学习
- **Abstract**: Neural ordinary differential equations (ODEs) provide expressive representations of invertible transport maps that can be used to approximate complex probability distributions, e.g., for generative modeling, density estimation, and Bayesian inference. We show that for a large class of transport maps $T$, there exists a time-dependent ODE velocity field realizing a straight-line interpolation $(1-t)x + tT(x)$, $t \in [0,1]$, of the displacement induced by the map. Moreover, we show that such velocity fields are minimizers of a training objective containing a specific minimum-energy regularization. We then derive explicit upper bounds for the $C^k$ norm of the velocity field that are polynomial in the $C^k$ norm of the corresponding transport map $T$; in the case of triangular (Knothe--Rosenblatt) maps, we also show that these bounds are polynomial in the $C^k$ norms of the associated source and target densities. Combining these results with stability arguments for distribution approximation via ODEs, we show that Wasserstein or Kullback--Leibler approximation of the target distribution to any desired accuracy $ε> 0$ can be achieved by a deep neural network representation of the velocity field whose size is bounded explicitly in terms of $ε$, the dimension, and the smoothness of the source and target densities. The same neural network ansatz yields guarantees on the value of the regularized training objective.

### Iterate to Accelerate: A Unified Framework for Iterative Reasoning and Feedback Convergence 
[[arxiv](https://arxiv.org/abs/2502.03787)] [[cool](https://papers.cool/arxiv/2502.03787)] [[pdf](https://arxiv.org/pdf/2502.03787)]
> **Authors**: Jacob Fein-Ashley
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We introduce a unified framework for iterative reasoning that leverages non-Euclidean geometry via Bregman divergences, higher-order operator averaging, and adaptive feedback mechanisms. Our analysis establishes that, under mild smoothness and contractivity assumptions, a generalized update scheme not only unifies classical methods such as mirror descent and dynamic programming but also captures modern chain-of-thought reasoning processes in large language models. In particular, we prove that our accelerated iterative update achieves an $O(1/t^2)$ convergence rate in the absence of persistent perturbations, and we further demonstrate that feedback (iterative) architectures are necessary to approximate certain fixed-point functions efficiently. These theoretical insights bridge classical acceleration techniques with contemporary applications in neural computation and optimization.

## 多代理系统(cs.MA:Multiagent Systems)

### Position: Emergent Machina Sapiens Urge Rethinking Multi-Agent Paradigms 
[[arxiv](https://arxiv.org/abs/2502.04388)] [[cool](https://papers.cool/arxiv/2502.04388)] [[pdf](https://arxiv.org/pdf/2502.04388)]
> **Authors**: Hepeng Li,Yuhong Liu,Jun Yan
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 多代理系统,人工智能
- **Abstract**: Artificially intelligent (AI) agents that are capable of autonomous learning and independent decision-making hold great promise for addressing complex challenges across domains like transportation, energy systems, and manufacturing. However, the surge in AI systems' design and deployment driven by various stakeholders with distinct and unaligned objectives introduces a crucial challenge: how can uncoordinated AI systems coexist and evolve harmoniously in shared environments without creating chaos? To address this, we advocate for a fundamental rethinking of existing multi-agent frameworks, such as multi-agent systems and game theory, which are largely limited to predefined rules and static objective structures. We posit that AI agents should be empowered to dynamically adjust their objectives, make compromises, form coalitions, and safely compete or cooperate through evolving relationships and social feedback. Through this paper, we call for a shift toward the emergent, self-organizing, and context-aware nature of these systems.

### Fairness Aware Reinforcement Learning via Proximal Policy Optimization 
[[arxiv](https://arxiv.org/abs/2502.03953)] [[cool](https://papers.cool/arxiv/2502.03953)] [[pdf](https://arxiv.org/pdf/2502.03953)]
> **Authors**: Gabriele La Malfa,Jie M. Zhang,Michael Luck,Elizabeth Black
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 多代理系统,机器学习
- **Abstract**: Fairness in multi-agent systems (MAS) focuses on equitable reward distribution among agents in scenarios involving sensitive attributes such as race, gender, or socioeconomic status. This paper introduces fairness in Proximal Policy Optimization (PPO) with a penalty term derived from demographic parity, counterfactual fairness, and conditional statistical parity. The proposed method balances reward maximisation with fairness by integrating two penalty components: a retrospective component that minimises disparities in past outcomes and a prospective component that ensures fairness in future decision-making. We evaluate our approach in the Allelopathic Harvest game, a cooperative and competitive MAS focused on resource collection, where some agents possess a sensitive attribute. Experiments demonstrate that fair-PPO achieves fairer policies across all fairness metrics than classic PPO. Fairness comes at the cost of reduced rewards, namely the Price of Fairness, although agents with and without the sensitive attribute renounce comparable amounts of rewards. Additionally, the retrospective and prospective penalties effectively change the agents' behaviour and improve fairness. These findings underscore the potential of fair-PPO to address fairness challenges in MAS.

## 多媒体(cs.MM:Multimedia)

### UniForm: A Unified Diffusion Transformer for Audio-Video Generation 
[[arxiv](https://arxiv.org/abs/2502.03897)] [[cool](https://papers.cool/arxiv/2502.03897)] [[pdf](https://arxiv.org/pdf/2502.03897)]
> **Authors**: Lei Zhao,Linfeng Feng,Dongxu Ge,Fangqiu Yi,Chi Zhang,Xiao-Lei Zhang,Xuelong Li
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: Our demos are available at https://uniform-t2av.github.io/
- **标题**: None
- **领域**: 多媒体,人工智能,计算机视觉和模式识别,声音,音频和语音处理
- **Abstract**: As a natural multimodal content, audible video delivers an immersive sensory experience. Consequently, audio-video generation systems have substantial potential. However, existing diffusion-based studies mainly employ relatively independent modules for generating each modality, which lack exploration of shared-weight generative modules. This approach may under-use the intrinsic correlations between audio and visual modalities, potentially resulting in sub-optimal generation quality. To address this, we propose UniForm, a unified diffusion transformer designed to enhance cross-modal consistency. By concatenating auditory and visual information, UniForm learns to generate audio and video simultaneously within a unified latent space, facilitating the creation of high-quality and well-aligned audio-visual pairs. Extensive experiments demonstrate the superior performance of our method in joint audio-video generation, audio-guided video generation, and video-guided audio generation tasks. Our demos are available at https://uniform-t2av.github.io/.

## 网络和互联网架构(cs.NI:Networking and Internet Architecture)

### Technical Report: Generating the WEB-IDS23 Dataset 
[[arxiv](https://arxiv.org/abs/2502.03909)] [[cool](https://papers.cool/arxiv/2502.03909)] [[pdf](https://arxiv.org/pdf/2502.03909)]
> **Authors**: Eric Lanfer,Dominik Brockmann,Nils Aschenbruck
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 网络和互联网架构,密码学和安全,机器学习
- **Abstract**: Anomaly-based Network Intrusion Detection Systems (NIDS) require correctly labelled, representative and diverse datasets for an accurate evaluation and development. However, several widely used datasets do not include labels which are fine-grained enough and, together with small sample sizes, can lead to overfitting issues that also remain undetected when using test data. Additionally, the cybersecurity sector is evolving fast, and new attack mechanisms require the continuous creation of up-to-date datasets. To address these limitations, we developed a modular traffic generator that can simulate a wide variety of benign and malicious traffic. It incorporates multiple protocols, variability through randomization techniques and can produce attacks along corresponding benign traffic, as it occurs in real-world scenarios. Using the traffic generator, we create a dataset capturing over 12 million samples with 82 flow-level features and 21 fine-grained labels. Additionally, we include several web attack types which are often underrepresented in other datasets.

### InfinitePOD: Building Datacenter-Scale High-Bandwidth Domain for LLM with Optical Circuit Switching Transceivers 
[[arxiv](https://arxiv.org/abs/2502.03885)] [[cool](https://papers.cool/arxiv/2502.03885)] [[pdf](https://arxiv.org/pdf/2502.03885)]
> **Authors**: Chenchen Shou,Guyue Liu,Hao Nie,Huaiyu Meng,Yu Zhou,Yimin Jiang,Wenqing Lv,Yelong Xu,Yuanwei Lu,Zhang Chen,Yanbo Yu,Yichen Shen,Yibo Zhu,Daxin Jiang
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 网络和互联网架构,分布式、并行和集群计算,机器学习
- **Abstract**: Scaling Large Language Model (LLM) training relies on multi-dimensional parallelism, where High-Bandwidth Domains (HBDs) are critical for communication-intensive parallelism like Tensor Parallelism (TP) and Expert Parallelism (EP). However, existing HBD architectures face fundamental limitations in scalability, cost, and fault resiliency: switch-centric HBDs (e.g., NVL-72) incur prohibitive scaling costs, while GPU-centric HBDs (e.g., TPUv3/Dojo) suffer from severe fault propagation. Switch-GPU hybrid HBDs such as TPUv4 takes a middle-ground approach by leveraging Optical Circuit Switches, but the fault explosion radius remains large at the cube level (e.g., 64 TPUs). We propose InfinitePOD, a novel transceiver-centric HBD architecture that unifies connectivity and dynamic switching at the transceiver level using Optical Circuit Switching (OCS). By embedding OCS within each transceiver, InfinitePOD achieves reconfigurable point-to-multipoint connectivity, allowing the topology to adapt into variable-size rings. This design provides: i) datacenter-wide scalability without cost explosion; ii) fault resilience by isolating failures to a single node, and iii) full bandwidth utilization for fault-free GPUs. Key innovations include a Silicon Photonic (SiPh) based low-cost OCS transceiver (OCSTrx), a reconfigurable k-hop ring topology co-designed with intra-/inter-node communication, and an HBD-DCN orchestration algorithm maximizing GPU utilization while minimizing cross-ToR datacenter network traffic. The evaluation demonstrates that InfinitePOD achieves 31% of the cost of NVL-72, near-zero GPU waste ratio (over one order of magnitude lower than NVL-72 and TPUv4), near-zero cross-ToR traffic when node fault ratios under 7%, and improves Model FLOPs Utilization by 3.37x compared to NVIDIA DGX (8 GPUs per Node).

## 机器人技术(cs.RO:Robotics)

### Probing a Vision-Language-Action Model for Symbolic States and Integration into a Cognitive Architecture 
[[arxiv](https://arxiv.org/abs/2502.04558)] [[cool](https://papers.cool/arxiv/2502.04558)] [[pdf](https://arxiv.org/pdf/2502.04558)]
> **Authors**: Hong Lu,Hengxu Li,Prithviraj Singh Shahani,Stephanie Herbers,Matthias Scheutz
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 8 Pages, 4 Figures
- **标题**: None
- **领域**: 机器人技术,人工智能
- **Abstract**: Vision-language-action (VLA) models hold promise as generalist robotics solutions by translating visual and linguistic inputs into robot actions, yet they lack reliability due to their black-box nature and sensitivity to environmental changes. In contrast, cognitive architectures (CA) excel in symbolic reasoning and state monitoring but are constrained by rigid predefined execution. This work bridges these approaches by probing OpenVLA's hidden layers to uncover symbolic representations of object properties, relations, and action states, enabling integration with a CA for enhanced interpretability and robustness. Through experiments on LIBERO-spatial pick-and-place tasks, we analyze the encoding of symbolic states across different layers of OpenVLA's Llama backbone. Our probing results show consistently high accuracies (> 0.90) for both object and action states across most layers, though contrary to our hypotheses, we did not observe the expected pattern of object states being encoded earlier than action states. We demonstrate an integrated DIARC-OpenVLA system that leverages these symbolic representations for real-time state monitoring, laying the foundation for more interpretable and reliable robotic manipulation.

### AnyPlace: Learning Generalized Object Placement for Robot Manipulation 
[[arxiv](https://arxiv.org/abs/2502.04531)] [[cool](https://papers.cool/arxiv/2502.04531)] [[pdf](https://arxiv.org/pdf/2502.04531)]
> **Authors**: Yuchi Zhao,Miroslav Bogdanovic,Chengyuan Luo,Steven Tohme,Kourosh Darvish,Alán Aspuru-Guzik,Florian Shkurti,Animesh Garg
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,人工智能,计算机视觉和模式识别
- **Abstract**: Object placement in robotic tasks is inherently challenging due to the diversity of object geometries and placement configurations. To address this, we propose AnyPlace, a two-stage method trained entirely on synthetic data, capable of predicting a wide range of feasible placement poses for real-world tasks. Our key insight is that by leveraging a Vision-Language Model (VLM) to identify rough placement locations, we focus only on the relevant regions for local placement, which enables us to train the low-level placement-pose-prediction model to capture diverse placements efficiently. For training, we generate a fully synthetic dataset of randomly generated objects in different placement configurations (insertion, stacking, hanging) and train local placement-prediction models. We conduct extensive evaluations in simulation, demonstrating that our method outperforms baselines in terms of success rate, coverage of possible placement modes, and precision. In real-world experiments, we show how our approach directly transfers models trained purely on synthetic data to the real world, where it successfully performs placements in scenarios where other models struggle -- such as with varying object geometries, diverse placement modes, and achieving high precision for fine placement. More at: https://any-place.github.io.

### DexterityGen: Foundation Controller for Unprecedented Dexterity 
[[arxiv](https://arxiv.org/abs/2502.04307)] [[cool](https://papers.cool/arxiv/2502.04307)] [[pdf](https://arxiv.org/pdf/2502.04307)]
> **Authors**: Zhao-Heng Yin,Changhao Wang,Luis Pineda,Francois Hogan,Krishna Bodduluri,Akash Sharma,Patrick Lancaster,Ishita Prasad,Mrinal Kalakrishnan,Jitendra Malik,Mike Lambeta,Tingfan Wu,Pieter Abbeel,Mustafa Mukadam
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: Project: https://zhaohengyin.github.io/dexteritygen
- **标题**: None
- **领域**: 机器人技术,人工智能,机器学习,系统与控制
- **Abstract**: Teaching robots dexterous manipulation skills, such as tool use, presents a significant challenge. Current approaches can be broadly categorized into two strategies: human teleoperation (for imitation learning) and sim-to-real reinforcement learning. The first approach is difficult as it is hard for humans to produce safe and dexterous motions on a different embodiment without touch feedback. The second RL-based approach struggles with the domain gap and involves highly task-specific reward engineering on complex tasks. Our key insight is that RL is effective at learning low-level motion primitives, while humans excel at providing coarse motion commands for complex, long-horizon tasks. Therefore, the optimal solution might be a combination of both approaches. In this paper, we introduce DexterityGen (DexGen), which uses RL to pretrain large-scale dexterous motion primitives, such as in-hand rotation or translation. We then leverage this learned dataset to train a dexterous foundational controller. In the real world, we use human teleoperation as a prompt to the controller to produce highly dexterous behavior. We evaluate the effectiveness of DexGen in both simulation and real world, demonstrating that it is a general-purpose controller that can realize input dexterous manipulation commands and significantly improves stability by 10-100x measured as duration of holding objects across diverse tasks. Notably, with DexGen we demonstrate unprecedented dexterous skills including diverse object reorientation and dexterous tool use such as pen, syringe, and screwdriver for the first time.

### Learning Real-World Action-Video Dynamics with Heterogeneous Masked Autoregression 
[[arxiv](https://arxiv.org/abs/2502.04296)] [[cool](https://papers.cool/arxiv/2502.04296)] [[pdf](https://arxiv.org/pdf/2502.04296)]
> **Authors**: Lirui Wang,Kevin Zhao,Chaoqi Liu,Xinlei Chen
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: Website: https://liruiw.github.io/hma/
- **标题**: None
- **领域**: 机器人技术,计算机视觉和模式识别,机器学习
- **Abstract**: We propose Heterogeneous Masked Autoregression (HMA) for modeling action-video dynamics to generate high-quality data and evaluation in scaling robot learning. Building interactive video world models and policies for robotics is difficult due to the challenge of handling diverse settings while maintaining computational efficiency to run in real time. HMA uses heterogeneous pre-training from observations and action sequences across different robotic embodiments, domains, and tasks. HMA uses masked autoregression to generate quantized or soft tokens for video predictions. \ourshort achieves better visual fidelity and controllability than the previous robotic video generation models with 15 times faster speed in the real world. After post-training, this model can be used as a video simulator from low-level action inputs for evaluating policies and generating synthetic data. See this link https://liruiw.github.io/hma for more information.

### Large Language Models for Multi-Robot Systems: A Survey 
[[arxiv](https://arxiv.org/abs/2502.03814)] [[cool](https://papers.cool/arxiv/2502.03814)] [[pdf](https://arxiv.org/pdf/2502.03814)]
> **Authors**: Peihan Li,Zijian An,Shams Abrar,Lifeng Zhou
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,人工智能
- **Abstract**: The rapid advancement of Large Language Models (LLMs) has opened new possibilities in Multi-Robot Systems (MRS), enabling enhanced communication, task planning, and human-robot interaction. Unlike traditional single-robot and multi-agent systems, MRS poses unique challenges, including coordination, scalability, and real-world adaptability. This survey provides the first comprehensive exploration of LLM integration into MRS. It systematically categorizes their applications across high-level task allocation, mid-level motion planning, low-level action generation, and human intervention. We highlight key applications in diverse domains, such as household robotics, construction, formation control, target tracking, and robot games, showcasing the versatility and transformative potential of LLMs in MRS. Furthermore, we examine the challenges that limit adapting LLMs in MRS, including mathematical reasoning limitations, hallucination, latency issues, and the need for robust benchmarking systems. Finally, we outline opportunities for future research, emphasizing advancements in fine-tuning, reasoning techniques, and task-specific models. This survey aims to guide researchers in the intelligence and real-world deployment of MRS powered by LLMs. Based on the fast-evolving nature of research in the field, we keep updating the papers in the open-source Github repository.

## 声音(cs.SD:Sound)

### ImprovNet: Generating Controllable Musical Improvisations with Iterative Corruption Refinement 
[[arxiv](https://arxiv.org/abs/2502.04522)] [[cool](https://papers.cool/arxiv/2502.04522)] [[pdf](https://arxiv.org/pdf/2502.04522)]
> **Authors**: Keshav Bhandari,Sungkyun Chang,Tongyu Lu,Fareza R. Enus,Louis B. Bradshaw,Dorien Herremans,Simon Colton
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 10 pages, 6 figures
- **标题**: None
- **领域**: 声音,人工智能,音频和语音处理
- **Abstract**: Deep learning has enabled remarkable advances in style transfer across various domains, offering new possibilities for creative content generation. However, in the realm of symbolic music, generating controllable and expressive performance-level style transfers for complete musical works remains challenging due to limited datasets, especially for genres such as jazz, and the lack of unified models that can handle multiple music generation tasks. This paper presents ImprovNet, a transformer-based architecture that generates expressive and controllable musical improvisations through a self-supervised corruption-refinement training strategy. ImprovNet unifies multiple capabilities within a single model: it can perform cross-genre and intra-genre improvisations, harmonize melodies with genre-specific styles, and execute short prompt continuation and infilling tasks. The model's iterative generation framework allows users to control the degree of style transfer and structural similarity to the original composition. Objective and subjective evaluations demonstrate ImprovNet's effectiveness in generating musically coherent improvisations while maintaining structural relationships with the original pieces. The model outperforms Anticipatory Music Transformer in short continuation and infilling tasks and successfully achieves recognizable genre conversion, with 79\% of participants correctly identifying jazz-style improvisations. Our code and demo page can be found at https://github.com/keshavbhandari/improvnet.

### ADIFF: Explaining audio difference using natural language 
[[arxiv](https://arxiv.org/abs/2502.04476)] [[cool](https://papers.cool/arxiv/2502.04476)] [[pdf](https://arxiv.org/pdf/2502.04476)]
> **Authors**: Soham Deshmukh,Shuo Han,Rita Singh,Bhiksha Raj
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: Accepted at ICLR 2025. Dataset and checkpoints are available at: https://github.com/soham97/ADIFF
- **标题**: None
- **领域**: 声音,人工智能,音频和语音处理
- **Abstract**: Understanding and explaining differences between audio recordings is crucial for fields like audio forensics, quality assessment, and audio generation. This involves identifying and describing audio events, acoustic scenes, signal characteristics, and their emotional impact on listeners. This paper stands out as the first work to comprehensively study the task of explaining audio differences and then propose benchmark, baselines for the task. First, we present two new datasets for audio difference explanation derived from the AudioCaps and Clotho audio captioning datasets. Using Large Language Models (LLMs), we generate three levels of difference explanations: (1) concise descriptions of audio events and objects, (2) brief sentences about audio events, acoustic scenes, and signal properties, and (3) comprehensive explanations that include semantics and listener emotions. For the baseline, we use prefix tuning where audio embeddings from two audio files are used to prompt a frozen language model. Our empirical analysis and ablation studies reveal that the naive baseline struggles to distinguish perceptually similar sounds and generate detailed tier 3 explanations. To address these limitations, we propose ADIFF, which introduces a cross-projection module, position captioning, and a three-step training process to enhance the model's ability to produce detailed explanations. We evaluate our model using objective metrics and human evaluation and show our model enhancements lead to significant improvements in performance over naive baseline and SoTA Audio-Language Model (ALM) Qwen Audio. Lastly, we conduct multiple ablation studies to study the effects of cross-projection, language model parameters, position captioning, third stage fine-tuning, and present our findings. Our benchmarks, findings, and strong baseline pave the way for nuanced and human-like explanations of audio differences.

### XAttnMark: Learning Robust Audio Watermarking with Cross-Attention 
[[arxiv](https://arxiv.org/abs/2502.04230)] [[cool](https://papers.cool/arxiv/2502.04230)] [[pdf](https://arxiv.org/pdf/2502.04230)]
> **Authors**: Yixin Liu,Lie Lu,Jihui Jin,Lichao Sun,Andrea Fanelli
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 24 pages, 10 figures
- **标题**: None
- **领域**: 声音,人工智能,密码学和安全,机器学习,音频和语音处理
- **Abstract**: The rapid proliferation of generative audio synthesis and editing technologies has raised significant concerns about copyright infringement, data provenance, and the spread of misinformation through deepfake audio. Watermarking offers a proactive solution by embedding imperceptible, identifiable, and traceable marks into audio content. While recent neural network-based watermarking methods like WavMark and AudioSeal have improved robustness and quality, they struggle to achieve both robust detection and accurate attribution simultaneously. This paper introduces Cross-Attention Robust Audio Watermark (XAttnMark), which bridges this gap by leveraging partial parameter sharing between the generator and the detector, a cross-attention mechanism for efficient message retrieval, and a temporal conditioning module for improved message distribution. Additionally, we propose a psychoacoustic-aligned temporal-frequency masking loss that captures fine-grained auditory masking effects, enhancing watermark imperceptibility. Our approach achieves state-of-the-art performance in both detection and attribution, demonstrating superior robustness against a wide range of audio transformations, including challenging generative editing with strong editing strength. The project webpage is available at https://liuyixin-louis.github.io/xattnmark/.

### A data-driven two-microphone method for in-situ sound absorption measurements 
[[arxiv](https://arxiv.org/abs/2502.04143)] [[cool](https://papers.cool/arxiv/2502.04143)] [[pdf](https://arxiv.org/pdf/2502.04143)]
> **Authors**: Leon Emmerich,Patrik Aste,Eric Brandão,Mélanie Nolan,Jacques Cuenca,U. Peter Svensson,Marcus Maeder,Steffen Marburg,Elias Zea
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 41 pages, 8 figures
- **标题**: None
- **领域**: 声音,机器学习,音频和语音处理
- **Abstract**: This work presents a data-driven approach to estimating the sound absorption coefficient of an infinite porous slab using a neural network and a two-microphone measurement on a finite porous sample. A 1D-convolutional network predicts the sound absorption coefficient from the complex-valued transfer function between the sound pressure measured at the two microphone positions. The network is trained and validated with numerical data generated by a boundary element model using the Delany-Bazley-Miki model, demonstrating accurate predictions for various numerical samples. The method is experimentally validated with baffled rectangular samples of a fibrous material, where sample size and source height are varied. The results show that the neural network offers the possibility to reliably predict the in-situ sound absorption of a porous material using the traditional two-microphone method as if the sample were infinite. The normal-incidence sound absorption coefficient obtained by the network compares well with that obtained theoretically and in an impedance tube. The proposed method has promising perspectives for estimating the sound absorption coefficient of acoustic materials after installation and in realistic operational conditions.

## 软件工程(cs.SE:Software Engineering)

### The ML Supply Chain in the Era of Software 2.0: Lessons Learned from Hugging Face 
[[arxiv](https://arxiv.org/abs/2502.04484)] [[cool](https://papers.cool/arxiv/2502.04484)] [[pdf](https://arxiv.org/pdf/2502.04484)]
> **Authors**: Trevor Stalnaker,Nathan Wintersgill,Oscar Chaparro,Laura A. Heymann,Massimiliano Di Penta,Daniel M German,Denys Poshyvanyk
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,机器学习
- **Abstract**: The last decade has seen widespread adoption of Machine Learning (ML) components in software systems. This has occurred in nearly every domain, from natural language processing to computer vision. These ML components range from relatively simple neural networks to complex and resource-intensive large language models. However, despite this widespread adoption, little is known about the supply chain relationships that produce these models, which can have implications for compliance and security. In this work, we conduct an extensive analysis of 760,460 models and 175,000 datasets mined from the popular model-sharing site Hugging Face. First, we evaluate the current state of documentation in the Hugging Face supply chain, report real-world examples of shortcomings, and offer actionable suggestions for improvement. Next, we analyze the underlying structure of the extant supply chain. Finally, we explore the current licensing landscape against what was reported in prior work and discuss the unique challenges posed in this domain. Our results motivate multiple research avenues, including the need for better license management for ML models/datasets, better support for model documentation, and automated inconsistency checking and validation. We make our research infrastructure and dataset available to facilitate future research.

### Identifying Flaky Tests in Quantum Code: A Machine Learning Approach 
[[arxiv](https://arxiv.org/abs/2502.04471)] [[cool](https://papers.cool/arxiv/2502.04471)] [[pdf](https://arxiv.org/pdf/2502.04471)]
> **Authors**: Khushdeep Kaur,Dongchan Kim,Ainaz Jamshidi,Lei Zhang
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 8 pages, 1 figure, accepted by Q-SANER 2025
- **标题**: None
- **领域**: 软件工程,机器学习
- **Abstract**: Testing and debugging quantum software pose significant challenges due to the inherent complexities of quantum mechanics, such as superposition and entanglement. One challenge is indeterminacy, a fundamental characteristic of quantum systems, which increases the likelihood of flaky tests in quantum programs. To the best of our knowledge, there is a lack of comprehensive studies on quantum flakiness in the existing literature. In this paper, we present a novel machine learning platform that leverages multiple machine learning models to automatically detect flaky tests in quantum programs. Our evaluation shows that the extreme gradient boosting and decision tree-based models outperform other models (i.e., random forest, k-nearest neighbors, and support vector machine), achieving the highest F1 score and Matthews Correlation Coefficient in a balanced dataset and an imbalanced dataset, respectively. Furthermore, we expand the currently limited dataset for researchers interested in quantum flaky tests. In the future, we plan to explore the development of unsupervised learning techniques to detect and classify quantum flaky tests more effectively. These advancements aim to improve the reliability and robustness of quantum software testing.

### Overcoming Vision Language Model Challenges in Diagram Understanding: A Proof-of-Concept with XML-Driven Large Language Models Solutions 
[[arxiv](https://arxiv.org/abs/2502.04389)] [[cool](https://papers.cool/arxiv/2502.04389)] [[pdf](https://arxiv.org/pdf/2502.04389)]
> **Authors**: Shue Shiinoki,Ryo Koshihara,Hayato Motegi,Masumi Morishige
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-07
> **comment**: The related code is available at \url{https://github.com/galirage/spreadsheet-intelligence}, which provides the core library developed for this research. The experimental code using this library can be found at \url{https://github.com/galirage/XMLDriven-Diagram-Understanding}
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: Diagrams play a crucial role in visually conveying complex relationships and processes within business documentation. Despite recent advances in Vision-Language Models (VLMs) for various image understanding tasks, accurately identifying and extracting the structures and relationships depicted in diagrams continues to pose significant challenges. This study addresses these challenges by proposing a text-driven approach that bypasses reliance on VLMs' visual recognition capabilities. Instead, it utilizes the editable source files--such as xlsx, pptx or docx--where diagram elements (e.g., shapes, lines, annotations) are preserved as textual metadata. In our proof-of-concept, we extracted diagram information from xlsx-based system design documents and transformed the extracted shape data into textual input for Large Language Models (LLMs). This approach allowed the LLM to analyze relationships and generate responses to business-oriented questions without the bottleneck of image-based processing. Experimental comparisons with a VLM-based method demonstrated that the proposed text-driven framework yielded more accurate answers for questions requiring detailed comprehension of diagram structures.The results obtained in this study are not limited to the tested .xlsx files but can also be extended to diagrams in other documents with source files, such as Office pptx and docx formats. These findings highlight the feasibility of circumventing VLM constraints through direct textual extraction from original source files. By enabling robust diagram understanding through LLMs, our method offers a promising path toward enhanced workflow efficiency and information analysis in real-world business scenarios.

### Combining Language and App UI Analysis for the Automated Assessment of Bug Reproduction Steps 
[[arxiv](https://arxiv.org/abs/2502.04251)] [[cool](https://papers.cool/arxiv/2502.04251)] [[pdf](https://arxiv.org/pdf/2502.04251)]
> **Authors**: Junayed Mahmud,Antu Saha,Oscar Chaparro,Kevin Moran,Andrian Marcus
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 12 pages, to appear in the Proceedings of the 33rd IEEE/ACM International Conference on Program Comprehension (ICPC'25)
- **标题**: None
- **领域**: 软件工程,机器学习
- **Abstract**: Bug reports are essential for developers to confirm software problems, investigate their causes, and validate fixes. Unfortunately, reports often miss important information or are written unclearly, which can cause delays, increased issue resolution effort, or even the inability to solve issues. One of the most common components of reports that are problematic is the steps to reproduce the bug(s) (S2Rs), which are essential to replicate the described program failures and reason about fixes. Given the proclivity for deficiencies in reported S2Rs, prior work has proposed techniques that assist reporters in writing or assessing the quality of S2Rs. However, automated understanding of S2Rs is challenging, and requires linking nuanced natural language phrases with specific, semantically related program information. Prior techniques often struggle to form such language to program connections - due to issues in language variability and limitations of information gleaned from program analyses. To more effectively tackle the problem of S2R quality annotation, we propose a new technique called AstroBR, which leverages the language understanding capabilities of LLMs to identify and extract the S2Rs from bug reports and map them to GUI interactions in a program state model derived via dynamic analysis. We compared AstroBR to a related state-of-the-art approach and we found that AstroBR annotates S2Rs 25.2% better (in terms of F1 score) than the baseline. Additionally, AstroBR suggests more accurate missing S2Rs than the baseline (by 71.4% in terms of F1 score).

### NLP-Based .NET CLR Event Logs Analyzer 
[[arxiv](https://arxiv.org/abs/2502.04219)] [[cool](https://papers.cool/arxiv/2502.04219)] [[pdf](https://arxiv.org/pdf/2502.04219)]
> **Authors**: Maxim Stavtsev,Sergey Shershakov
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: In this paper, we present a tool for analyzing .NET CLR event logs based on a novel method inspired by Natural Language Processing (NLP) approach. Our research addresses the growing need for effective monitoring and optimization of software systems through detailed event log analysis. We utilize a BERT-based architecture with an enhanced tokenization process customized to event logs. The tool, developed using Python, its libraries, and an SQLite database, allows both conducting experiments for academic purposes and efficiently solving industry-emerging tasks. Our experiments demonstrate the efficacy of our approach in compressing event sequences, detecting recurring patterns, and identifying anomalies. The trained model shows promising results, with a high accuracy rate in anomaly detection, which demonstrates the potential of NLP methods to improve the reliability and stability of software systems.

### Automating a Complete Software Test Process Using LLMs: An Automotive Case Study 
[[arxiv](https://arxiv.org/abs/2502.04008)] [[cool](https://papers.cool/arxiv/2502.04008)] [[pdf](https://arxiv.org/pdf/2502.04008)]
> **Authors**: Shuai Wang,Yinan Yu,Robert Feldt,Dhasarathy Parthasarathy
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: Accepted by International Conference on Software Engineering (ICSE) 2025
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: Vehicle API testing verifies whether the interactions between a vehicle's internal systems and external applications meet expectations, ensuring that users can access and control various vehicle functions and data. However, this task is inherently complex, requiring the alignment and coordination of API systems, communication protocols, and even vehicle simulation systems to develop valid test cases. In practical industrial scenarios, inconsistencies, ambiguities, and interdependencies across various documents and system specifications pose significant challenges. This paper presents a system designed for the automated testing of in-vehicle APIs. By clearly defining and segmenting the testing process, we enable Large Language Models (LLMs) to focus on specific tasks, ensuring a stable and controlled testing workflow. Experiments conducted on over 100 APIs demonstrate that our system effectively automates vehicle API testing. The results also confirm that LLMs can efficiently handle mundane tasks requiring human judgment, making them suitable for complete automation in similar industrial contexts.

### Should Code Models Learn Pedagogically? A Preliminary Evaluation of Curriculum Learning for Real-World Software Engineering Tasks 
[[arxiv](https://arxiv.org/abs/2502.03806)] [[cool](https://papers.cool/arxiv/2502.03806)] [[pdf](https://arxiv.org/pdf/2502.03806)]
> **Authors**: Kyi Shin Khant,Hong Yi Lin,Patanamon Thongtanunam
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: Accepted by the 22nd International Conference on Mining Software Repositories (MSR 25)
- **标题**: None
- **领域**: 软件工程,机器学习
- **Abstract**: Learning-based techniques, especially advanced pre-trained models for code have demonstrated capabilities in code understanding and generation, solving diverse software engineering (SE) tasks. Despite the promising results, current training approaches may not fully optimize model performance, as they typically involve learning from randomly shuffled training data. Recent work shows that Curriculum Learning (CL) can improve performance on code-related tasks through incremental learning based on the difficulty of synthetic code. Yet, the effectiveness of CL with conventional difficulty measures in SE tasks remains largely unexplored. In this study, we explore two conventional code metrics: code length and cyclomatic complexity to determine the difficulty levels. We investigate how the pre-trained code model (CodeT5) learns under CL, through the tasks of code clone detection and code summarization. Our empirical study on the CodeXGLUE benchmark showed contrasting results to prior studies, where the model exhibited signs of catastrophic forgetting and shortcut learning. Surprisingly, model performance saturates after only the first quartile of training, potentially indicating a limit in the model's representation capacity and/or the task's inherent difficulty. Future work should further explore various CL strategies with different code models across a wider range of SE tasks for a more holistic understanding.

## 音频和语音处理(eess.AS:Audio and Speech Processing)

### GenVC: Self-Supervised Zero-Shot Voice Conversion 
[[arxiv](https://arxiv.org/abs/2502.04519)] [[cool](https://papers.cool/arxiv/2502.04519)] [[pdf](https://arxiv.org/pdf/2502.04519)]
> **Authors**: Zexin Cai,Henry Li Xinyuan,Ashi Garg,Leibny Paola García-Perera,Kevin Duh,Sanjeev Khudanpur,Matthew Wiesner,Nicholas Andrews
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 音频和语音处理,机器学习
- **Abstract**: Zero-shot voice conversion has recently made substantial progress, but many models still depend on external supervised systems to disentangle speaker identity and linguistic content. Furthermore, current methods often use parallel conversion, where the converted speech inherits the source utterance's temporal structure, restricting speaker similarity and privacy. To overcome these limitations, we introduce GenVC, a generative zero-shot voice conversion model. GenVC learns to disentangle linguistic content and speaker style in a self-supervised manner, eliminating the need for external models and enabling efficient training on large, unlabeled datasets. Experimental results show that GenVC achieves state-of-the-art speaker similarity while maintaining naturalness competitive with leading approaches. Its autoregressive generation also allows the converted speech to deviate from the source utterance's temporal structure. This feature makes GenVC highly effective for voice anonymization, as it minimizes the preservation of source prosody and speaker characteristics, enhancing privacy protection.

### Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis 
[[arxiv](https://arxiv.org/abs/2502.04128)] [[cool](https://papers.cool/arxiv/2502.04128)] [[pdf](https://arxiv.org/pdf/2502.04128)]
> **Authors**: Zhen Ye,Xinfa Zhu,Chi-Min Chan,Xinsheng Wang,Xu Tan,Jiahe Lei,Yi Peng,Haohe Liu,Yizhu Jin,Zheqi Dai,Hongzhan Lin,Jianyi Chen,Xingjian Du,Liumeng Xue,Yunlin Chen,Zhifei Li,Lei Xie,Qiuqiang Kong,Yike Guo,Wei Xue
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 音频和语音处理,人工智能,计算语言学,多媒体,声音
- **Abstract**: Recent advances in text-based large language models (LLMs), particularly in the GPT series and the o1 model, have demonstrated the effectiveness of scaling both training-time and inference-time compute. However, current state-of-the-art TTS systems leveraging LLMs are often multi-stage, requiring separate models (e.g., diffusion models after LLM), complicating the decision of whether to scale a particular model during training or testing. This work makes the following contributions: First, we explore the scaling of train-time and inference-time compute for speech synthesis. Second, we propose a simple framework Llasa for speech synthesis that employs a single-layer vector quantizer (VQ) codec and a single Transformer architecture to fully align with standard LLMs such as Llama. Our experiments reveal that scaling train-time compute for Llasa consistently improves the naturalness of synthesized speech and enables the generation of more complex and accurate prosody patterns. Furthermore, from the perspective of scaling inference-time compute, we employ speech understanding models as verifiers during the search, finding that scaling inference-time compute shifts the sampling modes toward the preferences of specific verifiers, thereby improving emotional expressiveness, timbre consistency, and content accuracy. In addition, we released the checkpoint and training code for our TTS model (1B, 3B, 8B) and codec model publicly available.

### DiTAR: Diffusion Transformer Autoregressive Modeling for Speech Generation 
[[arxiv](https://arxiv.org/abs/2502.03930)] [[cool](https://papers.cool/arxiv/2502.03930)] [[pdf](https://arxiv.org/pdf/2502.03930)]
> **Authors**: Dongya Jia,Zhuo Chen,Jiawei Chen,Chenpeng Du,Jian Wu,Jian Cong,Xiaobin Zhuang,Chumin Li,Zhen Wei,Yuping Wang,Yuxuan Wang
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 16 pages, 8 figures
- **标题**: None
- **领域**: 音频和语音处理,人工智能,计算语言学,机器学习,声音
- **Abstract**: Several recent studies have attempted to autoregressively generate continuous speech representations without discrete speech tokens by combining diffusion and autoregressive models, yet they often face challenges with excessive computational loads or suboptimal outcomes. In this work, we propose Diffusion Transformer Autoregressive Modeling (DiTAR), a patch-based autoregressive framework combining a language model with a diffusion transformer. This approach significantly enhances the efficacy of autoregressive models for continuous tokens and reduces computational demands. DiTAR utilizes a divide-and-conquer strategy for patch generation, where the language model processes aggregated patch embeddings and the diffusion transformer subsequently generates the next patch based on the output of the language model. For inference, we propose defining temperature as the time point of introducing noise during the reverse diffusion ODE to balance diversity and determinism. We also show in the extensive scaling analysis that DiTAR has superb scalability. In zero-shot speech generation, DiTAR achieves state-of-the-art performance in robustness, speaker similarity, and naturalness.

## 图像和视频处理(eess.IV:Image and Video Processing)

### Generative Autoregressive Transformers for Model-Agnostic Federated MRI Reconstruction 
[[arxiv](https://arxiv.org/abs/2502.04521)] [[cool](https://papers.cool/arxiv/2502.04521)] [[pdf](https://arxiv.org/pdf/2502.04521)]
> **Authors**: Valiyeh A. Nezhad,Gokberk Elmas,Bilal Kabas,Fuat Arslan,Tolga Çukur
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: Although learning-based models hold great promise for MRI reconstruction, single-site models built on limited local datasets often suffer from poor generalization. This challenge has spurred interest in collaborative model training on multi-site datasets via federated learning (FL) -- a privacy-preserving framework that aggregates model updates instead of sharing imaging data. Conventional FL builds a global model by aggregating locally trained model weights, inherently constraining all sites to a homogeneous model architecture. This rigid homogeneity requirement forces sites to forgo architectures tailored to their compute infrastructure and application-specific demands. Consequently, existing FL methods for MRI reconstruction fail to support model-heterogeneous settings, where individual sites are allowed to use distinct architectures. To overcome this fundamental limitation, here we introduce FedGAT, a novel model-agnostic FL technique based on generative autoregressive transformers. FedGAT decentralizes the training of a global generative prior that captures the distribution of multi-site MR images. For enhanced fidelity, we propose a novel site-prompted GAT prior that controllably synthesizes MR images from desired sites via autoregressive prediction across spatial scales. Each site then trains its site-specific reconstruction model -- using its preferred architecture -- on a hybrid dataset comprising the local MRI dataset and GAT-generated synthetic MRI datasets for other sites. Comprehensive experiments on multi-institutional datasets demonstrate that FedGAT supports flexible collaborations while enjoying superior within-site and across-site reconstruction performance compared to state-of-the-art FL baselines.

### Hybrid Deep Learning Framework for Classification of Kidney CT Images: Diagnosis of Stones, Cysts, and Tumors 
[[arxiv](https://arxiv.org/abs/2502.04367)] [[cool](https://papers.cool/arxiv/2502.04367)] [[pdf](https://arxiv.org/pdf/2502.04367)]
> **Authors**: Kiran Sharma,Ziya Uddin,Adarsh Wadal,Dhruv Gupta
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习
- **Abstract**: Medical image classification is a vital research area that utilizes advanced computational techniques to improve disease diagnosis and treatment planning. Deep learning models, especially Convolutional Neural Networks (CNNs), have transformed this field by providing automated and precise analysis of complex medical images. This study introduces a hybrid deep learning model that integrates a pre-trained ResNet101 with a custom CNN to classify kidney CT images into four categories: normal, stone, cyst, and tumor. The proposed model leverages feature fusion to enhance classification accuracy, achieving 99.73% training accuracy and 100% testing accuracy. Using a dataset of 12,446 CT images and advanced feature mapping techniques, the hybrid CNN model outperforms standalone ResNet101. This architecture delivers a robust and efficient solution for automated kidney disease diagnosis, providing improved precision, recall, and reduced testing time, making it highly suitable for clinical applications.

### Expanding Training Data for Endoscopic Phenotyping of Eosinophilic Esophagitis 
[[arxiv](https://arxiv.org/abs/2502.04199)] [[cool](https://papers.cool/arxiv/2502.04199)] [[pdf](https://arxiv.org/pdf/2502.04199)]
> **Authors**: Juming Xiong,Hou Xiong,Quan Liu,Ruining Deng,Regina N Tyree,Girish Hiremath,Yuankai Huo
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: Eosinophilic esophagitis (EoE) is a chronic esophageal disorder marked by eosinophil-dominated inflammation. Diagnosing EoE usually involves endoscopic inspection of the esophageal mucosa and obtaining esophageal biopsies for histologic confirmation. Recent advances have seen AI-assisted endoscopic imaging, guided by the EREFS system, emerge as a potential alternative to reduce reliance on invasive histological assessments. Despite these advancements, significant challenges persist due to the limited availability of data for training AI models - a common issue even in the development of AI for more prevalent diseases. This study seeks to improve the performance of deep learning-based EoE phenotype classification by augmenting our training data with a diverse set of images from online platforms, public datasets, and electronic textbooks increasing our dataset from 435 to 7050 images. We utilized the Data-efficient Image Transformer for image classification and incorporated attention map visualizations to boost interpretability. The findings show that our expanded dataset and model enhancements improved diagnostic accuracy, robustness, and comprehensive analysis, enhancing patient outcomes.

### DEALing with Image Reconstruction: Deep Attentive Least Squares 
[[arxiv](https://arxiv.org/abs/2502.04079)] [[cool](https://papers.cool/arxiv/2502.04079)] [[pdf](https://arxiv.org/pdf/2502.04079)]
> **Authors**: Mehrsa Pourya,Erich Kobler,Michael Unser,Sebastian Neumayer
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习
- **Abstract**: State-of-the-art image reconstruction often relies on complex, highly parameterized deep architectures. We propose an alternative: a data-driven reconstruction method inspired by the classic Tikhonov regularization. Our approach iteratively refines intermediate reconstructions by solving a sequence of quadratic problems. These updates have two key components: (i) learned filters to extract salient image features, and (ii) an attention mechanism that locally adjusts the penalty of filter responses. Our method achieves performance on par with leading plug-and-play and learned regularizer approaches while offering interpretability, robustness, and convergent behavior. In effect, we bridge traditional regularization and deep learning with a principled reconstruction approach.

### A Self-supervised Multimodal Deep Learning Approach to Differentiate Post-radiotherapy Progression from Pseudoprogression in Glioblastoma 
[[arxiv](https://arxiv.org/abs/2502.03999)] [[cool](https://papers.cool/arxiv/2502.03999)] [[pdf](https://arxiv.org/pdf/2502.03999)]
> **Authors**: Ahmed Gomaa,Yixing Huang,Pluvio Stephan,Katharina Breininger,Benjamin Frey,Arnd Dörfler,Oliver Schnell,Daniel Delev,Roland Coras,Charlotte Schmitter,Jenny Stritzelberger,Sabine Semrau,Andreas Maier,Siming Bayer,Stephan Schönecker,Dieter H Heiland,Peter Hau,Udo S. Gaipl,Christoph Bert,Rainer Fietkau,Manuel A. Schmidt,Florian Putz
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: Accurate differentiation of pseudoprogression (PsP) from True Progression (TP) following radiotherapy (RT) in glioblastoma (GBM) patients is crucial for optimal treatment planning. However, this task remains challenging due to the overlapping imaging characteristics of PsP and TP. This study therefore proposes a multimodal deep-learning approach utilizing complementary information from routine anatomical MR images, clinical parameters, and RT treatment planning information for improved predictive accuracy. The approach utilizes a self-supervised Vision Transformer (ViT) to encode multi-sequence MR brain volumes to effectively capture both global and local context from the high dimensional input. The encoder is trained in a self-supervised upstream task on unlabeled glioma MRI datasets from the open BraTS2021, UPenn-GBM, and UCSF-PDGM datasets to generate compact, clinically relevant representations from FLAIR and T1 post-contrast sequences. These encoded MR inputs are then integrated with clinical data and RT treatment planning information through guided cross-modal attention, improving progression classification accuracy. This work was developed using two datasets from different centers: the Burdenko Glioblastoma Progression Dataset (n = 59) for training and validation, and the GlioCMV progression dataset from the University Hospital Erlangen (UKER) (n = 20) for testing. The proposed method achieved an AUC of 75.3%, outperforming the current state-of-the-art data-driven approaches. Importantly, the proposed approach relies on readily available anatomical MRI sequences, clinical data, and RT treatment planning information, enhancing its clinical feasibility. The proposed approach addresses the challenge of limited data availability for PsP and TP differentiation and could allow for improved clinical decision-making and optimized treatment plans for GBM patients.

### Synthetic Poisoning Attacks: The Impact of Poisoned MRI Image on U-Net Brain Tumor Segmentation 
[[arxiv](https://arxiv.org/abs/2502.03825)] [[cool](https://papers.cool/arxiv/2502.03825)] [[pdf](https://arxiv.org/pdf/2502.03825)]
> **Authors**: Tianhao Li,Tianyu Zeng,Yujia Zheng,Chulong Zhang,Jingyu Lu,Haotian Huang,Chuangxin Chu,Fang-Fang Yin,Zhenyu Yang
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,密码学和安全,计算机视觉和模式识别
- **Abstract**: Deep learning-based medical image segmentation models, such as U-Net, rely on high-quality annotated datasets to achieve accurate predictions. However, the increasing use of generative models for synthetic data augmentation introduces potential risks, particularly in the absence of rigorous quality control. In this paper, we investigate the impact of synthetic MRI data on the robustness and segmentation accuracy of U-Net models for brain tumor segmentation. Specifically, we generate synthetic T1-contrast-enhanced (T1-Ce) MRI scans using a GAN-based model with a shared encoding-decoding framework and shortest-path regularization. To quantify the effect of synthetic data contamination, we train U-Net models on progressively "poisoned" datasets, where synthetic data proportions range from 16.67% to 83.33%. Experimental results on a real MRI validation set reveal a significant performance degradation as synthetic data increases, with Dice coefficients dropping from 0.8937 (33.33% synthetic) to 0.7474 (83.33% synthetic). Accuracy and sensitivity exhibit similar downward trends, demonstrating the detrimental effect of synthetic data on segmentation robustness. These findings underscore the importance of quality control in synthetic data integration and highlight the risks of unregulated synthetic augmentation in medical image analysis. Our study provides critical insights for the development of more reliable and trustworthy AI-driven medical imaging systems.

### UltraBones100k: A reliable automated labeling method and large-scale dataset for ultrasound-based bone surface extraction 
[[arxiv](https://arxiv.org/abs/2502.03783)] [[cool](https://papers.cool/arxiv/2502.03783)] [[pdf](https://arxiv.org/pdf/2502.03783)]
> **Authors**: Luohong Wu,Nicola A. Cavalcanti,Matthias Seibold,Giuseppe Loggia,Lisa Reissner,Jonas Hein,Silvan Beeler,Arnd Viehöfer,Stephan Wirth,Lilian Calvet,Philipp Fürnstahl
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: Ultrasound-based bone surface segmentation is crucial in computer-assisted orthopedic surgery. However, ultrasound images have limitations, including a low signal-to-noise ratio, and acoustic shadowing, which make interpretation difficult. Existing deep learning models for bone segmentation rely primarily on costly manual labeling by experts, limiting dataset size and model generalizability. Additionally, the complexity of ultrasound physics and acoustic shadow makes the images difficult for humans to interpret, leading to incomplete labels in anechoic regions and limiting model performance. To advance ultrasound bone segmentation and establish effective model benchmarks, larger and higher-quality datasets are needed. We propose a methodology for collecting ex-vivo ultrasound datasets with automatically generated bone labels, including anechoic regions. The proposed labels are derived by accurately superimposing tracked bone CT models onto the tracked ultrasound images. These initial labels are refined to account for ultrasound physics. A clinical evaluation is conducted by an expert physician specialized on orthopedic sonography to assess the quality of the generated bone labels. A neural network for bone segmentation is trained on the collected dataset and its predictions are compared to expert manual labels, evaluating accuracy, completeness, and F1-score. We collected the largest known dataset of 100k ultrasound images of human lower limbs with bone labels, called UltraBones100k. A Wilcoxon signed-rank test with Bonferroni correction confirmed that the bone alignment after our method significantly improved the quality of bone labeling (p < 0.001). The model trained on UltraBones100k consistently outperforms manual labeling in all metrics, particularly in low-intensity regions (320% improvement in completeness at a distance threshold of 0.5 mm).

## 系统与控制(eess.SY:Systems and Control)

### End-to-End Learning Framework for Solving Non-Markovian Optimal Control 
[[arxiv](https://arxiv.org/abs/2502.04649)] [[cool](https://papers.cool/arxiv/2502.04649)] [[pdf](https://arxiv.org/pdf/2502.04649)]
> **Authors**: Xiaole Zhang,Peiyu Zhang,Xiongye Xiao,Shixuan Li,Vasileios Tzoumas,Vijay Gupta,Paul Bogdan
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 系统与控制,机器学习,优化与控制
- **Abstract**: Integer-order calculus often falls short in capturing the long-range dependencies and memory effects found in many real-world processes. Fractional calculus addresses these gaps via fractional-order integrals and derivatives, but fractional-order dynamical systems pose substantial challenges in system identification and optimal control due to the lack of standard control methodologies. In this paper, we theoretically derive the optimal control via linear quadratic regulator (LQR) for fractional-order linear time-invariant (FOLTI) systems and develop an end-to-end deep learning framework based on this theoretical foundation. Our approach establishes a rigorous mathematical model, derives analytical solutions, and incorporates deep learning to achieve data-driven optimal control of FOLTI systems. Our key contributions include: (i) proposing an innovative system identification method control strategy for FOLTI systems, (ii) developing the first end-to-end data-driven learning framework, Fractional-Order Learning for Optimal Control (FOLOC), that learns control policies from observed trajectories, and (iii) deriving a theoretical analysis of sample complexity to quantify the number of samples required for accurate optimal control in complex real-world problems. Experimental results indicate that our method accurately approximates fractional-order system behaviors without relying on Gaussian noise assumptions, pointing to promising avenues for advanced optimal control.

## 数值分析(math.NA:Numerical Analysis)

### Electrical Impedance Tomography for Anisotropic Media: a Machine Learning Approach to Classify Inclusions 
[[arxiv](https://arxiv.org/abs/2502.04273)] [[cool](https://papers.cool/arxiv/2502.04273)] [[pdf](https://arxiv.org/pdf/2502.04273)]
> **Authors**: Romina Gaburro,Patrick Healy,Shraddha Naidu,Clifford Nolan
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 27 pages, 17 figures
- **标题**: None
- **领域**: 数值分析,机器学习
- **Abstract**: We consider the problem in Electrical Impedance Tomography (EIT) of identifying one or multiple inclusions in a background-conducting body $Ω\subset\mathbb{R}^2$, from the knowledge of a finite number of electrostatic measurements taken on its boundary $\partialΩ$ and modelled by the Dirichlet-to-Neumann (D-N) matrix. Once the presence of one inclusion in $Ω$ is established, our model, combined with the machine learning techniques of Artificial Neural Networks (ANN) and Support Vector Machines (SVM), may be used to determine the size of the inclusion, the presence of multiple inclusions, and also that of anisotropy within the inclusion(s). Utilising both real and simulated datasets within a 16-electrode setup, we achieve a high rate of inclusion detection and show that two measurements are sufficient to achieve a good level of accuracy when predicting the size of an inclusion. This underscores the substantial potential of integrating machine learning approaches with the more classical analysis of EIT and the inverse inclusion problem to extract critical insights, such as the presence of anisotropy.

## 优化与控制(math.OC:Optimization and Control)

### Blackwell's Approachability with Approximation Algorithms 
[[arxiv](https://arxiv.org/abs/2502.03919)] [[cool](https://papers.cool/arxiv/2502.03919)] [[pdf](https://arxiv.org/pdf/2502.03919)]
> **Authors**: Dan Garber,Mhna Massalha
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 优化与控制,机器学习
- **Abstract**: We revisit Blackwell's celebrated approachability problem which considers a repeated vector-valued game between a player and an adversary. Motivated by settings in which the action set of the player or adversary (or both) is difficult to optimize over, for instance when it corresponds to the set of all possible solutions to some NP-Hard optimization problem, we ask what can the player guarantee \textit{efficiently}, when only having access to these sets via approximation algorithms with ratios $α_{\mX} \geq 1$ and $ 1 \geq α_{\mY} > 0$, respectively. Assuming the player has monotone preferences, in the sense that he does not prefer a vector-valued loss $\ell_1$ over $\ell_2$ if $\ell_2 \leq \ell_1$, we establish that given a Blackwell instance with an approachable target set $S$, the downward closure of the appropriately-scaled set $α_{\mX}α_{\mY}^{-1}S$ is \textit{efficiently} approachable with optimal rate. In case only the player's or adversary's set is equipped with an approximation algorithm, we give simpler and more efficient algorithms.

## 统计理论(math.ST:Statistics Theory)

### Analysis of Diffusion Models for Manifold Data 
[[arxiv](https://arxiv.org/abs/2502.04339)] [[cool](https://papers.cool/arxiv/2502.04339)] [[pdf](https://arxiv.org/pdf/2502.04339)]
> **Authors**: Anand Jerry George,Rodrigo Veiga,Nicolas Macris
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 统计理论,无序系统和神经网络,信息论,机器学习,可能性
- **Abstract**: We analyze the time reversed dynamics of generative diffusion models. If the exact empirical score function is used in a regime of large dimension and exponentially large number of samples, these models are known to undergo transitions between distinct dynamical regimes. We extend this analysis and compute the transitions for an analytically tractable manifold model where the statistical model for the data is a mixture of lower dimensional Gaussians embedded in higher dimensional space. We compute the so-called speciation and collapse transition times, as a function of the ratio of manifold-to-ambient space dimensions, and other characteristics of the data model. An important tool used in our analysis is the exact formula for the mutual information (or free energy) of Generalized Linear Models.

## 化学物理(physics.chem-ph:Chemical Physics)

### Retro-Rank-In: A Ranking-Based Approach for Inorganic Materials Synthesis Planning 
[[arxiv](https://arxiv.org/abs/2502.04289)] [[cool](https://papers.cool/arxiv/2502.04289)] [[pdf](https://arxiv.org/pdf/2502.04289)]
> **Authors**: Thorben Prein,Elton Pan,Sami Haddouti,Marco Lorenz,Janik Jehkul,Tymoteusz Wilk,Cansu Moran,Menelaos Panagiotis Fotiadis,Artur P. Toshev,Elsa Olivetti,Jennifer L. M. Rupp
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 化学物理,机器学习
- **Abstract**: Retrosynthesis strategically plans the synthesis of a chemical target compound from simpler, readily available precursor compounds. This process is critical for synthesizing novel inorganic materials, yet traditional methods in inorganic chemistry continue to rely on trial-and-error experimentation. Emerging machine-learning approaches struggle to generalize to entirely new reactions due to their reliance on known precursors, as they frame retrosynthesis as a multi-label classification task. To address these limitations, we propose Retro-Rank-In, a novel framework that reformulates the retrosynthesis problem by embedding target and precursor materials into a shared latent space and learning a pairwise ranker on a bipartite graph of inorganic compounds. We evaluate Retro-Rank-In's generalizability on challenging retrosynthesis dataset splits designed to mitigate data duplicates and overlaps. For instance, for Cr2AlB2, it correctly predicts the verified precursor pair CrB + Al despite never seeing them in training, a capability absent in prior work. Extensive experiments show that Retro-Rank-In sets a new state-of-the-art, particularly in out-of-distribution generalization and candidate set ranking, offering a powerful tool for accelerating inorganic material synthesis.

## 医学物理(physics.med-ph:Medical Physics)

### LUND-PROBE -- LUND Prostate Radiotherapy Open Benchmarking and Evaluation dataset 
[[arxiv](https://arxiv.org/abs/2502.04493)] [[cool](https://papers.cool/arxiv/2502.04493)] [[pdf](https://arxiv.org/pdf/2502.04493)]
> **Authors**: Viktor Rogowski,Lars E Olsson,Jonas Scherman,Emilia Persson,Mustafa Kadhim,Sacha af Wetterstedt,Adalsteinn Gunnlaugsson,Martin P. Nilsson,Nandor Vass,Mathieu Moreau,Maria Gebre Medhin,Sven Bäck,Per Munck af Rosenschöld,Silke Engelholm,Christian Jamtheim Gustafsson
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 4 figures
- **标题**: None
- **领域**: 医学物理,计算机视觉和模式识别,图像和视频处理
- **Abstract**: Radiotherapy treatment for prostate cancer relies on computed tomography (CT) and/or magnetic resonance imaging (MRI) for segmentation of target volumes and organs at risk (OARs). Manual segmentation of these volumes is regarded as the gold standard for ground truth in machine learning applications but to acquire such data is tedious and time-consuming. A publicly available clinical dataset is presented, comprising MRI- and synthetic CT (sCT) images, target and OARs segmentations, and radiotherapy dose distributions for 432 prostate cancer patients treated with MRI-guided radiotherapy. An extended dataset with 35 patients is also included, with the addition of deep learning (DL)-generated segmentations, DL segmentation uncertainty maps, and DL segmentations manually adjusted by four radiation oncologists. The publication of these resources aims to aid research within the fields of automated radiotherapy treatment planning, segmentation, inter-observer analyses, and DL model uncertainty investigation. The dataset is hosted on the AIDA Data Hub and offers a free-to-use resource for the scientific community, valuable for the advancement of medical imaging and prostate cancer radiotherapy research.

## 神经元和认知(q-bio.NC:Neurons and Cognition)

### Shifting Attention to You: Personalized Brain-Inspired AI Models 
[[arxiv](https://arxiv.org/abs/2502.04658)] [[cool](https://papers.cool/arxiv/2502.04658)] [[pdf](https://arxiv.org/pdf/2502.04658)]
> **Authors**: Stephen Chong Zhao,Yang Hu,Jason Lee,Andrew Bender,Trisha Mazumdar,Mark Wallace,David A. Tovar
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 7 Figures, 3 Tables, 3 Supplemental Figures, 1 Supplemental Table
- **标题**: None
- **领域**: 神经元和认知,人工智能
- **Abstract**: The integration of human and artificial intelligence represents a scientific opportunity to advance our understanding of information processing, as each system offers unique computational insights that can enhance and inform the other. The synthesis of human cognitive principles with artificial intelligence has the potential to produce more interpretable and functionally aligned computational models, while simultaneously providing a formal framework for investigating the neural mechanisms underlying perception, learning, and decision-making through systematic model comparisons and representational analyses. In this study, we introduce personalized brain-inspired modeling that integrates human behavioral embeddings and neural data to align with cognitive processes. We took a stepwise approach, fine-tuning the Contrastive Language-Image Pre-training (CLIP) model with large-scale behavioral decisions, group-level neural data, and finally, participant-level neural data within a broader framework that we have named CLIP-Human-Based Analysis (CLIP-HBA). We found that fine-tuning on behavioral data enhances its ability to predict human similarity judgments while indirectly aligning it with dynamic representations captured via MEG. To further gain mechanistic insights into the temporal evolution of cognitive processes, we introduced a model specifically fine-tuned on millisecond-level MEG neural dynamics (CLIP-HBA-MEG). This model resulted in enhanced temporal alignment with human neural processing while still showing improvement on behavioral alignment. Finally, we trained individualized models on participant-specific neural data, effectively capturing individualized neural dynamics and highlighting the potential for personalized AI systems. These personalized systems have far-reaching implications for the fields of medicine, cognitive research, human-computer interfaces, and AI development.

## 量子物理学(quant-ph:Quantum Physics)

### Variational decision diagrams for quantum-inspired machine learning applications 
[[arxiv](https://arxiv.org/abs/2502.04271)] [[cool](https://papers.cool/arxiv/2502.04271)] [[pdf](https://arxiv.org/pdf/2502.04271)]
> **Authors**: Santiago Acevedo-Mancera,Vladimir Vargas-Calderón,Herbert Vinck-Posada
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 8 pages, 3 figures, presented at Quantum Information in Spain (ICE-9)
- **标题**: None
- **领域**: 量子物理学,机器学习
- **Abstract**: Decision diagrams (DDs) have emerged as an efficient tool for simulating quantum circuits due to their capacity to exploit data redundancies in quantum states and quantum operations, enabling the efficient computation of probability amplitudes. However, their application in quantum machine learning (QML) has remained unexplored. This paper introduces variational decision diagrams (VDDs), a novel graph structure that combines the structural benefits of DDs with the adaptability of variational methods for efficiently representing quantum states. We investigate the trainability of VDDs by applying them to the ground state estimation problem for transverse-field Ising and Heisenberg Hamiltonians. Analysis of gradient variance suggests that training VDDs is possible, as no signs of vanishing gradients--also known as barren plateaus--are observed. This work provides new insights into the use of decision diagrams in QML as an alternative to design and train variational ansätze.

## 应用领域(stat.AP:Applications)

### A Pseudo Markov-Chain Model and Time-Elapsed Measures of Mobility from Collective Data 
[[arxiv](https://arxiv.org/abs/2502.04162)] [[cool](https://papers.cool/arxiv/2502.04162)] [[pdf](https://arxiv.org/pdf/2502.04162)]
> **Authors**: Alisha Foster,David A. Meyer,Asif Shakeel
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 27 pages, 11 figures
- **标题**: None
- **领域**: 应用领域,机器学习,社交和信息网络,机器学习
- **Abstract**: In this paper we develop a pseudo Markov-chain model to understand time-elapsed flows, over multiple intervals, from time and space aggregated collective inter-location trip data, given as a time-series. Building on the model, we develop measures of mobility that parallel those known for individual mobility data, such as the radius of gyration. We apply these measures to the NetMob 2024 Data Challenge data, and obtain interesting results that are consistent with published statistics and commuting patterns in cities. Besides building a new framework, we foresee applications of this approach to an improved understanding of human mobility in the context of environmental changes and sustainable development.

## 机器学习(stat.ML:Machine Learning)

### Complexity Analysis of Normalizing Constant Estimation: from Jarzynski Equality to Annealed Importance Sampling and beyond 
[[arxiv](https://arxiv.org/abs/2502.04575)] [[cool](https://papers.cool/arxiv/2502.04575)] [[pdf](https://arxiv.org/pdf/2502.04575)]
> **Authors**: Wei Guo,Molei Tao,Yongxin Chen
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习,数值分析,计算物理,计算
- **Abstract**: Given an unnormalized probability density $π\propto\mathrm{e}^{-V}$, estimating its normalizing constant $Z=\int_{\mathbb{R}^d}\mathrm{e}^{-V(x)}\mathrm{d}x$ or free energy $F=-\log Z$ is a crucial problem in Bayesian statistics, statistical mechanics, and machine learning. It is challenging especially in high dimensions or when $π$ is multimodal. To mitigate the high variance of conventional importance sampling estimators, annealing-based methods such as Jarzynski equality and annealed importance sampling are commonly adopted, yet their quantitative complexity guarantees remain largely unexplored. We take a first step toward a non-asymptotic analysis of annealed importance sampling. In particular, we derive an oracle complexity of $\widetilde{O}\left(\frac{dβ^2{\mathcal{A}}^2}{\varepsilon^4}\right)$ for estimating $Z$ within $\varepsilon$ relative error with high probability, where $β$ is the smoothness of $V$ and $\mathcal{A}$ denotes the action of a curve of probability measures interpolating $π$ and a tractable reference distribution. Our analysis, leveraging Girsanov theorem and optimal transport, does not explicitly require isoperimetric assumptions on the target distribution. Finally, to tackle the large action of the widely used geometric interpolation of probability distributions, we propose a new normalizing constant estimation algorithm based on reverse diffusion samplers and establish a framework for analyzing its complexity.

### Sparsity-Based Interpolation of External, Internal and Swap Regret 
[[arxiv](https://arxiv.org/abs/2502.04543)] [[cool](https://papers.cool/arxiv/2502.04543)] [[pdf](https://arxiv.org/pdf/2502.04543)]
> **Authors**: Zhou Lu,Y. Jennifer Sun,Zhiyu Zhang
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: Equal contribution, alphabetical order
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Focusing on the expert problem in online learning, this paper studies the interpolation of several performance metrics via $φ$-regret minimization, which measures the performance of an algorithm by its regret with respect to an arbitrary action modification rule $φ$. With $d$ experts and $T\gg d$ rounds in total, we present a single algorithm achieving the instance-adaptive $φ$-regret bound \begin{equation*} \tilde O\left(\min\left\{\sqrt{d-d^{\mathrm{unif}}_φ+1},\sqrt{d-d^{\mathrm{self}}_φ}\right\}\cdot\sqrt{T}\right), \end{equation*} where $d^{\mathrm{unif}}_φ$ is the maximum amount of experts modified identically by $φ$, and $d^{\mathrm{self}}_φ$ is the amount of experts that $φ$ trivially modifies to themselves. By recovering the optimal $O(\sqrt{T\log d})$ external regret bound when $d^{\mathrm{unif}}_φ=d$, the standard $\tilde O(\sqrt{T})$ internal regret bound when $d^{\mathrm{self}}_φ=d-1$ and the optimal $\tilde O(\sqrt{dT})$ swap regret bound in the worst case, we improve existing results in the intermediate regimes. In addition, the same algorithm achieves the optimal quantile regret bound, which corresponds to even easier settings of $φ$ than the external regret. Building on the classical reduction from $φ$-regret minimization to external regret minimization on stochastic matrices, our main idea is to further convert the latter to online linear regression using Haar-wavelet-inspired matrix features. Then, we apply a particular $L_1$-version of comparator-adaptive online learning algorithms to exploit the sparsity in this regression subroutine.

### Prediction-Powered E-Values 
[[arxiv](https://arxiv.org/abs/2502.04294)] [[cool](https://papers.cool/arxiv/2502.04294)] [[pdf](https://arxiv.org/pdf/2502.04294)]
> **Authors**: Daniel Csillag,Claudio José Struchiner,Guilherme Tegoni Goedert
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习,方法论
- **Abstract**: Quality statistical inference requires a sufficient amount of data, which can be missing or hard to obtain. To this end, prediction-powered inference has risen as a promising methodology, but existing approaches are largely limited to Z-estimation problems such as inference of means and quantiles. In this paper, we apply ideas of prediction-powered inference to e-values. By doing so, we inherit all the usual benefits of e-values -- such as anytime-validity, post-hoc validity and versatile sequential inference -- as well as greatly expand the set of inferences achievable in a prediction-powered manner. In particular, we show that every inference procedure that can be framed in terms of e-values has a prediction-powered counterpart, given by our method. We showcase the effectiveness of our framework across a wide range of inference tasks, from simple hypothesis testing and confidence intervals to more involved procedures for change-point detection and causal discovery, which were out of reach of previous techniques. Our approach is modular and easily integrable into existing algorithms, making it a compelling choice for practical applications.

### Gaussian Process Regression for Inverse Problems in Linear PDEs 
[[arxiv](https://arxiv.org/abs/2502.04276)] [[cool](https://papers.cool/arxiv/2502.04276)] [[pdf](https://arxiv.org/pdf/2502.04276)]
> **Authors**: Xin Li,Markus Lange-Hegermann,Bogdan Raiţă
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习,交换代数
- **Abstract**: This paper introduces a computationally efficient algorithm in system theory for solving inverse problems governed by linear partial differential equations (PDEs). We model solutions of linear PDEs using Gaussian processes with priors defined based on advanced commutative algebra and algebraic analysis. The implementation of these priors is algorithmic and achieved using the Macaulay2 computer algebra software. An example application includes identifying the wave speed from noisy data for classical wave equations, which are widely used in physics. The method achieves high accuracy while enhancing computational efficiency.

### Student-t processes as infinite-width limits of posterior Bayesian neural networks 
[[arxiv](https://arxiv.org/abs/2502.04247)] [[cool](https://papers.cool/arxiv/2502.04247)] [[pdf](https://arxiv.org/pdf/2502.04247)]
> **Authors**: Francesco Caporali,Stefano Favaro,Dario Trevisan
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习,可能性
- **Abstract**: The asymptotic properties of Bayesian Neural Networks (BNNs) have been extensively studied, particularly regarding their approximations by Gaussian processes in the infinite-width limit. We extend these results by showing that posterior BNNs can be approximated by Student-t processes, which offer greater flexibility in modeling uncertainty. Specifically, we show that, if the parameters of a BNN follow a Gaussian prior distribution, and the variance of both the last hidden layer and the Gaussian likelihood function follows an Inverse-Gamma prior distribution, then the resulting posterior BNN converges to a Student-t process in the infinite-width limit. Our proof leverages the Wasserstein metric to establish control over the convergence rate of the Student-t process approximation.

### Multi-task Online Learning for Probabilistic Load Forecasting 
[[arxiv](https://arxiv.org/abs/2502.04163)] [[cool](https://papers.cool/arxiv/2502.04163)] [[pdf](https://arxiv.org/pdf/2502.04163)]
> **Authors**: Onintze Zaballa,Verónica Álvarez,Santiago Mazuelas
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 2024 IEEE Sustainable Power and Energy Conference
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Load forecasting is essential for the efficient, reliable, and cost-effective management of power systems. Load forecasting performance can be improved by learning the similarities among multiple entities (e.g., regions, buildings). Techniques based on multi-task learning obtain predictions by leveraging consumption patterns from the historical load demand of multiple entities and their relationships. However, existing techniques cannot effectively assess inherent uncertainties in load demand or account for dynamic changes in consumption patterns. This paper proposes a multi-task learning technique for online and probabilistic load forecasting. This technique provides accurate probabilistic predictions for the loads of multiple entities by leveraging their dynamic similarities. The method's performance is evaluated using datasets that register the load demand of multiple entities and contain diverse and dynamic consumption patterns. The experimental results show that the proposed method can significantly enhance the effectiveness of current multi-task learning approaches across a wide variety of load consumption scenarios.

### Guiding Two-Layer Neural Network Lipschitzness via Gradient Descent Learning Rate Constraints 
[[arxiv](https://arxiv.org/abs/2502.03792)] [[cool](https://papers.cool/arxiv/2502.03792)] [[pdf](https://arxiv.org/pdf/2502.03792)]
> **Authors**: Kyle Sung,Anastasis Kratsios,Noah Forman
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-07
> **comment**: 26 pages, 8 figures
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: We demonstrate that applying an eventual decay to the learning rate (LR) in empirical risk minimization (ERM), where the mean-squared-error loss is minimized using standard gradient descent (GD) for training a two-layer neural network with Lipschitz activation functions, ensures that the resulting network exhibits a high degree of Lipschitz regularity, that is, a small Lipschitz constant. Moreover, we show that this decay does not hinder the convergence rate of the empirical risk, now measured with the Huber loss, toward a critical point of the non-convex empirical risk. From these findings, we derive generalization bounds for two-layer neural networks trained with GD and a decaying LR with a sub-linear dependence on its number of trainable parameters, suggesting that the statistical behaviour of these networks is independent of overparameterization. We validate our theoretical results with a series of toy numerical experiments, where surprisingly, we observe that networks trained with constant step size GD exhibit similar learning and regularity properties to those trained with a decaying LR. This suggests that neural networks trained with standard GD may already be highly regular learners.

## 其他论文

- [Contrastive Learning-Enhanced Large Language Models for Monolith-to-Microservice Decomposition](https://arxiv.org/abs/2502.04604)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Cooperative Payload Estimation by a Team of Mocobots](https://arxiv.org/abs/2502.04600)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [Fuzzy Linkography: Automatic Graphical Summarization of Creative Activity Traces](https://arxiv.org/abs/2502.04599)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Reinforcement Learning Based Prediction of PID Controller Gains for Quadrotor UAVs](https://arxiv.org/abs/2502.04552)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO,eess.SY in whitelist
- [Idioms: Neural Decompilation With Joint Code and Type Prediction](https://arxiv.org/abs/2502.04536)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR,cs.SE in whitelist
- [Regulating Reality: Exploring Synthetic Media Through Multistakeholder AI Governance](https://arxiv.org/abs/2502.04526)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [All-in-One Analog AI Accelerator: On-Chip Training and Inference with Conductive-Metal-Oxide/HfOx ReRAM Devices](https://arxiv.org/abs/2502.04524)
  - **标题**: None
  - **Filtered Reason**: none of cs.ET,cs.AR in whitelist
- [Cognitive AI framework: advances in the simulation of human thought](https://arxiv.org/abs/2502.04259)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC,cs.CY in whitelist
- [Work in Progress: AI-Powered Engineering-Bridging Theory and Practice](https://arxiv.org/abs/2502.04256)
  - **标题**: None
  - **Filtered Reason**: none of eess.SY,cs.SE in whitelist
- [FedOptimus: Optimizing Vertical Federated Learning for Scalability and Efficiency](https://arxiv.org/abs/2502.04243)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC in whitelist
- [Saflo: eBPF-Based MPTCP Scheduler for Mitigating Traffic Analysis Attacks in Cellular Networks](https://arxiv.org/abs/2502.04236)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR,cs.NI in whitelist
- [Can LLMs Hack Enterprise Networks? Autonomous Assumed Breach Penetration-Testing Active Directory Networks](https://arxiv.org/abs/2502.04227)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [Provably Robust Explainable Graph Neural Networks against Graph Perturbation Attacks](https://arxiv.org/abs/2502.04224)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [Automated Microservice Pattern Instance Detection Using Infrastructure-as-Code Artifacts and Large Language Models](https://arxiv.org/abs/2502.04188)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Are the Majority of Public Computational Notebooks Pathologically Non-Executable?](https://arxiv.org/abs/2502.04184)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Fast In-Spectrum Graph Watermarks](https://arxiv.org/abs/2502.04182)
  - **标题**: None
  - **Filtered Reason**: none of cs.DS in whitelist
- [From Configuration-Space Clearance to Feature-Space Margin: Sample Complexity in Learning-Based Collision Detection](https://arxiv.org/abs/2502.04170)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [SPRINT: An Assistant for Issue Report Management](https://arxiv.org/abs/2502.04147)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Quantifying imperfect cognition via achieved information gain](https://arxiv.org/abs/2502.04088)
  - **标题**: None
  - **Filtered Reason**: none of cs.IT in whitelist
- [Echo-Teddy: Preliminary Design and Development of Large Language Model-based Social Robot for Autistic Students](https://arxiv.org/abs/2502.04029)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- ["It Warned Me Just at the Right Moment": Exploring LLM-based Real-time Detection of Phone Scams](https://arxiv.org/abs/2502.03964)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC,cs.CR in whitelist
- [POPACheck: a Model Checker for probabilistic Pushdown Automata](https://arxiv.org/abs/2502.03956)
  - **标题**: None
  - **Filtered Reason**: none of cs.LO in whitelist
- [Generative AI and Creative Work: Narratives, Values, and Impacts](https://arxiv.org/abs/2502.03940)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [Counterfactual Query Rewriting to Use Historical Relevance Feedback](https://arxiv.org/abs/2502.03891)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [Weyl symmetry of the gradient-flow in information geometry](https://arxiv.org/abs/2502.03866)
  - **标题**: None
  - **Filtered Reason**: none of math-ph,gr-qc,cs.IT in whitelist
- [Frontend Diffusion: Empowering Self-Representation of Junior Researchers and Designers Through Agentic Workflows](https://arxiv.org/abs/2502.03788)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [GistVis: Automatic Generation of Word-scale Visualizations from Data-rich Documents](https://arxiv.org/abs/2502.03784)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
