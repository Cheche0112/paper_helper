> 本文由 [https://github.com/huiyeruzhou/arxiv_crawler](https://github.com/huiyeruzhou/arxiv_crawler) 自动生成
>
> 领域白名单：cs.AI,cs.CL,cs.LG,cs.CV
> 关键词： LLM, GPT, AI, language+model, deep+learning, transformer, neural+network, machine+learning

# 论文全览：2025-02-25

共有376篇相关领域论文, 另有37篇其他

## 地球和行星天体物理学(astro-ph.EP:Earth and Planetary Astrophysics)

### Utilizing Machine Learning to Predict Host Stars and the Key Elemental Abundances of Small Planets 
[[arxiv](https://arxiv.org/abs/2502.17563)] [[cool](https://papers.cool/arxiv/2502.17563)] [[pdf](https://arxiv.org/pdf/2502.17563)]
> **Authors**: Amílcar R. Torres-Quijano,Natalie R. Hinkel,Caleb H. Wheeler III,Patrick A. Young,Luan Ghezzi,Augusto P. Baldo
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 22 pages, 9 figures, 3 tables, accepted to AJ
- **标题**: None
- **领域**: 地球和行星天体物理学,天体物理学仪器和方法,太阳和恒星天体物理学,机器学习
- **Abstract**: Stars and their associated planets originate from the same cloud of gas and dust, making a star's elemental composition a valuable indicator for indirectly studying planetary compositions. While the connection between a star's iron (Fe) abundance and the presence of giant exoplanets is established (e.g. Gonzalez 1997; Fischer & Valenti 2005), the relationship with small planets remains unclear. The elements Mg, Si, and Fe are important in forming small planets. Employing machine learning algorithms like XGBoost, trained on the abundances (e.g., the Hypatia Catalog, Hinkel et al. 2014) of known exoplanet-hosting stars (NASA Exoplanet Archive), allows us to determine significant "features" (abundances or molar ratios) that may indicate the presence of small planets. We test on three groups of exoplanets: (a) all small, R$_{P}$ $<$ 3.5 $R_{\oplus}$, (b) sub-Neptunes, 2.0 $R_{\oplus}$ $<$ R$_{P}$ $<$ 3.5 $R_{\oplus}$, and (c) super-Earths, 1.0 $R_{\oplus}$ $<$ R$_{P}$ $<$ 2.0 $R_{\oplus}$ -- each subdivided into 7 ensembles to test different combinations of features. We created a list of stars with $\geq90\%$ probability of hosting small planets across all ensembles and experiments ("overlap stars"). We found abundance trends for stars hosting small planets, possibly indicating star-planet chemical interplay during formation. We also found that Na and V are key features regardless of planetary radii. We expect our results to underscore the importance of elements in exoplanet formation and machine learning's role in target selection for future NASA missions: e.g., the James Webb Space Telescope (JWST), Nancy Grace Roman Space Telescope (NGRST), Habitable Worlds Observatory (HWO) -- all of which are aimed at small planet detection.

## 天体物理学仪器和方法(astro-ph.IM:Instrumentation and Methods for Astrophysics)

### Joint multi-band deconvolution for $Euclid$ and $Vera$ $C.$ $Rubin$ images 
[[arxiv](https://arxiv.org/abs/2502.17177)] [[cool](https://papers.cool/arxiv/2502.17177)] [[pdf](https://arxiv.org/pdf/2502.17177)]
> **Authors**: Utsav Akhaury,Pascale Jablonka,Frédéric Courbin,Jean-Luc Starck
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 12 pages, 12 figures
- **标题**: None
- **领域**: 天体物理学仪器和方法,计算机视觉和模式识别
- **Abstract**: With the advent of surveys like $Euclid$ and $Vera$ $C.$ $Rubin$, astrophysicists will have access to both deep, high-resolution images, and multi-band images. However, these two conditions are not simultaneously available in any single dataset. It is therefore vital to devise image deconvolution algorithms that exploit the best of the two worlds and that can jointly analyze datasets spanning a range of resolutions and wavelengths. In this work, we introduce a novel multi-band deconvolution technique aimed at improving the resolution of ground-based astronomical images by leveraging higher-resolution space-based observations. The method capitalizes on the fortunate fact that the $Vera$ $C.$ $Rubin$ $r$-, $i$-, and $z$-bands lie within the $Euclid$ $VIS$ band. The algorithm jointly deconvolves all the data to turn the $r$-, $i$-, and $z$-band $Vera$ $C.$ $Rubin$ images to the resolution of $Euclid$ by enabling us to leverage the correlations between the different bands. We also investigate the performance of deep learning-based denoising with DRUNet to further improve the results. We illustrate the effectiveness of our method in terms of resolution and morphology recovery, flux preservation, and generalization to different noise levels. This approach extends beyond the specific $Euclid$-$Rubin$ combination, offering a versatile solution to improve the resolution of ground-based images in multiple photometric bands by jointly using any space-based images with overlapping filters.

## 材料科学(cond-mat.mtrl-sci:Materials Science)

### An Explainable AI Model for Binary LJ Fluids 
[[arxiv](https://arxiv.org/abs/2502.17357)] [[cool](https://papers.cool/arxiv/2502.17357)] [[pdf](https://arxiv.org/pdf/2502.17357)]
> **Authors**: Israrul H Hashmi,Rahul Karmakar,Marripelli Maniteja,Kumar Ayush,Tarak K. Patra
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 材料科学,机器学习,化学物理
- **Abstract**: Lennard-Jones (LJ) fluids serve as an important theoretical framework for understanding molecular interactions. Binary LJ fluids, where two distinct species of particles interact based on the LJ potential, exhibit rich phase behavior and provide valuable insights of complex fluid mixtures. Here we report the construction and utility of an artificial intelligence (AI) model for binary LJ fluids, focusing on their effectiveness in predicting radial distribution functions (RDFs) across a range of conditions. The RDFs of a binary mixture with varying compositions and temperatures are collected from molecular dynamics (MD) simulations to establish and validate the AI model. In this AI pipeline, RDFs are discretized in order to reduce the output dimension of the model. This, in turn, improves the efficacy, and reduce the complexity of an AI RDF model. The model is shown to predict RDFs for many unknown mixtures very accurately, especially outside the training temperature range. Our analysis suggests that the particle size ratio has a higher order impact on the microstructure of a binary mixture. We also highlight the areas where the fidelity of the AI model is low when encountering new regimes with different underlying physics.

### Active Learning for Conditional Inverse Design with Crystal Generation and Foundation Atomic Models 
[[arxiv](https://arxiv.org/abs/2502.16984)] [[cool](https://papers.cool/arxiv/2502.16984)] [[pdf](https://arxiv.org/pdf/2502.16984)]
> **Authors**: Zhuoyuan Li,Siyu Liu,Beilin Ye,David J. Srolovitz,Tongqi Wen
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 材料科学,机器学习
- **Abstract**: Artificial intelligence (AI) is transforming materials science, enabling both theoretical advancements and accelerated materials discovery. Recent progress in crystal generation models, which design crystal structures for targeted properties, and foundation atomic models (FAMs), which capture interatomic interactions across the periodic table, has significantly improved inverse materials design. However, an efficient integration of these two approaches remains an open challenge. Here, we present an active learning framework that combines crystal generation models and foundation atomic models to enhance the accuracy and efficiency of inverse design. As a case study, we employ Con-CDVAE to generate candidate crystal structures and MACE-MP-0 FAM as one of the high-throughput screeners for bulk modulus evaluation. Through iterative active learning, we demonstrate that Con-CDVAE progressively improves its accuracy in generating crystals with target properties, highlighting the effectiveness of a property-driven fine-tuning process. Our framework is general to accommodate different crystal generation and foundation atomic models, and establishes a scalable approach for AI-driven materials discovery. By bridging generative modeling with atomic-scale simulations, this work paves the way for more accurate and efficient inverse materials design.

## 人工智能(cs.AI:Artificial Intelligence)

### A Combinatorial Identities Benchmark for Theorem Proving via Automated Theorem Generation 
[[arxiv](https://arxiv.org/abs/2502.17840)] [[cool](https://papers.cool/arxiv/2502.17840)] [[pdf](https://arxiv.org/pdf/2502.17840)]
> **Authors**: Beibei Xiong,Hangyu Lv,Haojia Shan,Jianlin Wang,Zhengfeng Yang,Lihong Zhi
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Large language models (LLMs) have significantly advanced formal theorem proving, yet the scarcity of high-quality training data constrains their capabilities in complex mathematical domains. Combinatorics, a cornerstone of mathematics, provides essential tools for analyzing discrete structures and solving optimization problems. However, its inherent complexity makes it particularly challenging for automated theorem proving (ATP) for combinatorial identities. To address this, we manually construct LeanComb, combinatorial identities benchmark in Lean, which is, to our knowledge, the first formalized theorem proving benchmark built for combinatorial identities. We develop an Automated Theorem Generator for Combinatorial Identities, ATG4CI, which combines candidate tactics suggested by a self-improving large language model with a Reinforcement Learning Tree Search approach for tactic prediction. By utilizing ATG4CI, we generate a LeanComb-Enhanced dataset comprising 260K combinatorial identities theorems, each with a complete formal proof in Lean, and experimental evaluations demonstrate that models trained on this dataset can generate more effective tactics, thereby improving success rates in automated theorem proving for combinatorial identities.

### DocPuzzle: A Process-Aware Benchmark for Evaluating Realistic Long-Context Reasoning Capabilities 
[[arxiv](https://arxiv.org/abs/2502.17807)] [[cool](https://papers.cool/arxiv/2502.17807)] [[pdf](https://arxiv.org/pdf/2502.17807)]
> **Authors**: Tianyi Zhuang,Chuqiao Kuang,Xiaoguang Li,Yihua Teng,Jihao Wu,Yasheng Wang,Lifeng Shang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: We present DocPuzzle, a rigorously constructed benchmark for evaluating long-context reasoning capabilities in large language models (LLMs). This benchmark comprises 100 expert-level QA problems requiring multi-step reasoning over long real-world documents. To ensure the task quality and complexity, we implement a human-AI collaborative annotation-validation pipeline. DocPuzzle introduces an innovative evaluation framework that mitigates guessing bias through checklist-guided process analysis, establishing new standards for assessing reasoning capacities in LLMs. Our evaluation results show that: 1)Advanced slow-thinking reasoning models like o1-preview(69.7%) and DeepSeek-R1(66.3%) significantly outperform best general instruct models like Claude 3.5 Sonnet(57.7%); 2)Distilled reasoning models like DeepSeek-R1-Distill-Qwen-32B(41.3%) falls far behind the teacher model, suggesting challenges to maintain the generalization of reasoning capabilities relying solely on distillation.

### Detection of LLM-Paraphrased Code and Identification of the Responsible LLM Using Coding Style Features 
[[arxiv](https://arxiv.org/abs/2502.17749)] [[cool](https://papers.cool/arxiv/2502.17749)] [[pdf](https://arxiv.org/pdf/2502.17749)]
> **Authors**: Shinwoo Park,Hyundong Jin,Jeong-won Cha,Yo-Sub Han
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Recent progress in large language models (LLMs) for code generation has raised serious concerns about intellectual property protection. Malicious users can exploit LLMs to produce paraphrased versions of proprietary code that closely resemble the original. While the potential for LLM-assisted code paraphrasing continues to grow, research on detecting it remains limited, underscoring an urgent need for detection system. We respond to this need by proposing two tasks. The first task is to detect whether code generated by an LLM is a paraphrased version of original human-written code. The second task is to identify which LLM is used to paraphrase the original code. For these tasks, we construct a dataset LPcode consisting of pairs of human-written code and LLM-paraphrased code using various LLMs. We statistically confirm significant differences in the coding styles of human-written and LLM-paraphrased code, particularly in terms of naming consistency, code structure, and readability. Based on these findings, we develop LPcodedec, a detection method that identifies paraphrase relationships between human-written and LLM-generated code, and discover which LLM is used for the paraphrasing. LPcodedec outperforms the best baselines in two tasks, improving F1 scores by 2.64% and 15.17% while achieving speedups of 1,343x and 213x, respectively.

### Mind the Gesture: Evaluating AI Sensitivity to Culturally Offensive Non-Verbal Gestures 
[[arxiv](https://arxiv.org/abs/2502.17710)] [[cool](https://papers.cool/arxiv/2502.17710)] [[pdf](https://arxiv.org/pdf/2502.17710)]
> **Authors**: Akhila Yerukola,Saadia Gabriel,Nanyun Peng,Maarten Sap
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 40 pages, 49 figures
- **标题**: None
- **领域**: 人工智能,计算语言学,计算机视觉和模式识别,机器学习
- **Abstract**: Gestures are an integral part of non-verbal communication, with meanings that vary across cultures, and misinterpretations that can have serious social and diplomatic consequences. As AI systems become more integrated into global applications, ensuring they do not inadvertently perpetuate cultural offenses is critical. To this end, we introduce Multi-Cultural Set of Inappropriate Gestures and Nonverbal Signs (MC-SIGNS), a dataset of 288 gesture-country pairs annotated for offensiveness, cultural significance, and contextual factors across 25 gestures and 85 countries. Through systematic evaluation using MC-SIGNS, we uncover critical limitations: text-to-image (T2I) systems exhibit strong US-centric biases, performing better at detecting offensive gestures in US contexts than in non-US ones; large language models (LLMs) tend to over-flag gestures as offensive; and vision-language models (VLMs) default to US-based interpretations when responding to universal concepts like wishing someone luck, frequently suggesting culturally inappropriate gestures. These findings highlight the urgent need for culturally-aware AI safety mechanisms to ensure equitable global deployment of AI technologies.

### From Perceptions to Decisions: Wildfire Evacuation Decision Prediction with Behavioral Theory-informed LLMs 
[[arxiv](https://arxiv.org/abs/2502.17701)] [[cool](https://papers.cool/arxiv/2502.17701)] [[pdf](https://arxiv.org/pdf/2502.17701)]
> **Authors**: Ruxiao Chen,Chenguang Wang,Yuran Sun,Xilei Zhao,Susu Xu
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 24 pages, 9 figures
- **标题**: None
- **领域**: 人工智能,计算语言学,计算机与社会,机器学习
- **Abstract**: Evacuation decision prediction is critical for efficient and effective wildfire response by helping emergency management anticipate traffic congestion and bottlenecks, allocate resources, and minimize negative impacts. Traditional statistical methods for evacuation decision prediction fail to capture the complex and diverse behavioral logic of different individuals. In this work, for the first time, we introduce FLARE, short for facilitating LLM for advanced reasoning on wildfire evacuation decision prediction, a Large Language Model (LLM)-based framework that integrates behavioral theories and models to streamline the Chain-of-Thought (CoT) reasoning and subsequently integrate with memory-based Reinforcement Learning (RL) module to provide accurate evacuation decision prediction and understanding. Our proposed method addresses the limitations of using existing LLMs for evacuation behavioral predictions, such as limited survey data, mismatching with behavioral theory, conflicting individual preferences, implicit and complex mental states, and intractable mental state-behavior mapping. Experiments on three post-wildfire survey datasets show an average of 20.47% performance improvement over traditional theory-informed behavioral models, with strong cross-event generalizability. Our complete code is publicly available at https://github.com/SusuXu-s-Lab/FLARE

### Socratic: Enhancing Human Teamwork via AI-enabled Coaching 
[[arxiv](https://arxiv.org/abs/2502.17643)] [[cool](https://papers.cool/arxiv/2502.17643)] [[pdf](https://arxiv.org/pdf/2502.17643)]
> **Authors**: Sangwon Seo,Bing Han,Rayan E. Harari,Roger D. Dias,Marco A. Zenati,Eduardo Salas,Vaibhav Unhelkar
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Extended version of an identically-titled paper accepted at AAMAS 2025
- **标题**: None
- **领域**: 人工智能,人机交互,机器学习,多代理系统
- **Abstract**: Coaches are vital for effective collaboration, but cost and resource constraints often limit their availability during real-world tasks. This limitation poses serious challenges in life-critical domains that rely on effective teamwork, such as healthcare and disaster response. To address this gap, we propose and realize an innovative application of AI: task-time team coaching. Specifically, we introduce Socratic, a novel AI system that complements human coaches by providing real-time guidance during task execution. Socratic monitors team behavior, detects misalignments in team members' shared understanding, and delivers automated interventions to improve team performance. We validated Socratic through two human subject experiments involving dyadic collaboration. The results demonstrate that the system significantly enhances team performance with minimal interventions. Participants also perceived Socratic as helpful and trustworthy, supporting its potential for adoption. Our findings also suggest promising directions both for AI research and its practical applications to enhance human teamwork.

### Representation Engineering for Large-Language Models: Survey and Research Challenges 
[[arxiv](https://arxiv.org/abs/2502.17601)] [[cool](https://papers.cool/arxiv/2502.17601)] [[pdf](https://arxiv.org/pdf/2502.17601)]
> **Authors**: Lukasz Bartoszcze,Sarthak Munshi,Bryan Sukidi,Jennifer Yen,Zejia Yang,David Williams-King,Linh Le,Kosi Asuzu,Carsten Maple
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Large-language models are capable of completing a variety of tasks, but remain unpredictable and intractable. Representation engineering seeks to resolve this problem through a new approach utilizing samples of contrasting inputs to detect and edit high-level representations of concepts such as honesty, harmfulness or power-seeking. We formalize the goals and methods of representation engineering to present a cohesive picture of work in this emerging discipline. We compare it with alternative approaches, such as mechanistic interpretability, prompt-engineering and fine-tuning. We outline risks such as performance decrease, compute time increases and steerability issues. We present a clear agenda for future research to build predictable, dynamic, safe and personalizable LLMs.

### Intention Recognition in Real-Time Interactive Navigation Maps 
[[arxiv](https://arxiv.org/abs/2502.17581)] [[cool](https://papers.cool/arxiv/2502.17581)] [[pdf](https://arxiv.org/pdf/2502.17581)]
> **Authors**: Peijie Zhao,Zunayed Arefin,Felipe Meneguzzi,Ramon Fraga Pereira
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: In this demonstration, we develop IntentRec4Maps, a system to recognise users' intentions in interactive maps for real-world navigation. IntentRec4Maps uses the Google Maps Platform as the real-world interactive map, and a very effective approach for recognising users' intentions in real-time. We showcase the recognition process of IntentRec4Maps using two different Path-Planners and a Large Language Model (LLM). GitHub: https://github.com/PeijieZ/IntentRec4Maps

### How Do Large Language Monkeys Get Their Power (Laws)? 
[[arxiv](https://arxiv.org/abs/2502.17578)] [[cool](https://papers.cool/arxiv/2502.17578)] [[pdf](https://arxiv.org/pdf/2502.17578)]
> **Authors**: Rylan Schaeffer,Joshua Kazdan,John Hughes,Jordan Juravsky,Sara Price,Aengus Lynch,Erik Jones,Robert Kirk,Azalia Mirhoseini,Sanmi Koyejo
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: Recent research across mathematical problem solving, proof assistant programming and multimodal jailbreaking documents a striking finding: when (multimodal) language model tackle a suite of tasks with multiple attempts per task -- succeeding if any attempt is correct -- then the negative log of the average success rate scales a power law in the number of attempts. In this work, we identify an apparent puzzle: a simple mathematical calculation predicts that on each problem, the failure rate should fall exponentially with the number of attempts. We confirm this prediction empirically, raising a question: from where does aggregate polynomial scaling emerge? We then answer this question by demonstrating per-problem exponential scaling can be made consistent with aggregate polynomial scaling if the distribution of single-attempt success probabilities is heavy tailed such that a small fraction of tasks with extremely low success probabilities collectively warp the aggregate success trend into a power law - even as each problem scales exponentially on its own. We further demonstrate that this distributional perspective explains previously observed deviations from power law scaling, and provides a simple method for forecasting the power law exponent with an order of magnitude lower relative error, or equivalently, ${\sim}2-4$ orders of magnitude less inference compute. Overall, our work contributes to a better understanding of how neural language model performance improves with scaling inference compute and the development of scaling-predictable evaluations of (multimodal) language models.

### Dataset Featurization: Uncovering Natural Language Features through Unsupervised Data Reconstruction 
[[arxiv](https://arxiv.org/abs/2502.17541)] [[cool](https://papers.cool/arxiv/2502.17541)] [[pdf](https://arxiv.org/pdf/2502.17541)]
> **Authors**: Michal Bravansky,Vaclav Kubon,Suhas Hariharan,Robert Kirk
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: Interpreting data is central to modern research. Large language models (LLMs) show promise in providing such natural language interpretations of data, yet simple feature extraction methods such as prompting often fail to produce accurate and versatile descriptions for diverse datasets and lack control over granularity and scale. To address these limitations, we propose a domain-agnostic method for dataset featurization that provides precise control over the number of features extracted while maintaining compact and descriptive representations comparable to human expert labeling. Our method optimizes the selection of informative binary features by evaluating the ability of an LLM to reconstruct the original data using those features. We demonstrate its effectiveness in dataset modeling tasks and through two case studies: (1) Constructing a feature representation of jailbreak tactics that compactly captures both the effectiveness and diversity of a larger set of human-crafted attacks; and (2) automating the discovery of features that align with human preferences, achieving accuracy and robustness comparable to expert-crafted features. Moreover, we show that the pipeline scales effectively, improving as additional features are sampled, making it suitable for large and diverse datasets.

### User Intent to Use DeekSeep for Healthcare Purposes and their Trust in the Large Language Model: Multinational Survey Study 
[[arxiv](https://arxiv.org/abs/2502.17487)] [[cool](https://papers.cool/arxiv/2502.17487)] [[pdf](https://arxiv.org/pdf/2502.17487)]
> **Authors**: Avishek Choudhury,Yeganeh Shahsavar,Hamid Shamszare
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算机与社会,人机交互
- **Abstract**: Large language models (LLMs) increasingly serve as interactive healthcare resources, yet user acceptance remains underexplored. This study examines how ease of use, perceived usefulness, trust, and risk perception interact to shape intentions to adopt DeepSeek, an emerging LLM-based platform, for healthcare purposes. A cross-sectional survey of 556 participants from India, the United Kingdom, and the United States was conducted to measure perceptions and usage patterns. Structural equation modeling assessed both direct and indirect effects, including potential quadratic relationships. Results revealed that trust plays a pivotal mediating role: ease of use exerts a significant indirect effect on usage intentions through trust, while perceived usefulness contributes to both trust development and direct adoption. By contrast, risk perception negatively affects usage intent, emphasizing the importance of robust data governance and transparency. Notably, significant non-linear paths were observed for ease of use and risk, indicating threshold or plateau effects. The measurement model demonstrated strong reliability and validity, supported by high composite reliabilities, average variance extracted, and discriminant validity measures. These findings extend technology acceptance and health informatics research by illuminating the multifaceted nature of user adoption in sensitive domains. Stakeholders should invest in trust-building strategies, user-centric design, and risk mitigation measures to encourage sustained and safe uptake of LLMs in healthcare. Future work can employ longitudinal designs or examine culture-specific variables to further clarify how user perceptions evolve over time and across different regulatory environments. Such insights are critical for harnessing AI to enhance outcomes.

### From System 1 to System 2: A Survey of Reasoning Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.17419)] [[cool](https://papers.cool/arxiv/2502.17419)] [[pdf](https://arxiv.org/pdf/2502.17419)]
> **Authors**: Zhong-Zhi Li,Duzhen Zhang,Ming-Liang Zhang,Jiaxin Zhang,Zengyan Liu,Yuxuan Yao,Haotian Xu,Junhao Zheng,Pei-Jie Wang,Xiuyi Chen,Yingying Zhang,Fei Yin,Jiahua Dong,Zhijiang Guo,Le Song,Cheng-Lin Liu
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Slow-thinking, LargeLanguageModels, Human-like Reasoning, Decision Making inAI, AGI
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Achieving human-level intelligence requires refining the transition from the fast, intuitive System 1 to the slower, more deliberate System 2 reasoning. While System 1 excels in quick, heuristic decisions, System 2 relies on logical reasoning for more accurate judgments and reduced biases. Foundational Large Language Models (LLMs) excel at fast decision-making but lack the depth for complex reasoning, as they have not yet fully embraced the step-by-step analysis characteristic of true System 2 thinking. Recently, reasoning LLMs like OpenAI's o1/o3 and DeepSeek's R1 have demonstrated expert-level performance in fields such as mathematics and coding, closely mimicking the deliberate reasoning of System 2 and showcasing human-like cognitive abilities. This survey begins with a brief overview of the progress in foundational LLMs and the early development of System 2 technologies, exploring how their combination has paved the way for reasoning LLMs. Next, we discuss how to construct reasoning LLMs, analyzing their features, the core methods enabling advanced reasoning, and the evolution of various reasoning LLMs. Additionally, we provide an overview of reasoning benchmarks, offering an in-depth comparison of the performance of representative reasoning LLMs. Finally, we explore promising directions for advancing reasoning LLMs and maintain a real-time \href{https://github.com/zzli2022/Awesome-Slow-Reason-System}{GitHub Repository} to track the latest developments. We hope this survey will serve as a valuable resource to inspire innovation and drive progress in this rapidly evolving field.

### Emoti-Attack: Zero-Perturbation Adversarial Attacks on NLP Systems via Emoji Sequences 
[[arxiv](https://arxiv.org/abs/2502.17392)] [[cool](https://papers.cool/arxiv/2502.17392)] [[pdf](https://arxiv.org/pdf/2502.17392)]
> **Authors**: Yangshijie Zhang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学,密码学和安全
- **Abstract**: Deep neural networks (DNNs) have achieved remarkable success in the field of natural language processing (NLP), leading to widely recognized applications such as ChatGPT. However, the vulnerability of these models to adversarial attacks remains a significant concern. Unlike continuous domains like images, text exists in a discrete space, making even minor alterations at the sentence, word, or character level easily perceptible to humans. This inherent discreteness also complicates the use of conventional optimization techniques, as text is non-differentiable. Previous research on adversarial attacks in text has focused on character-level, word-level, sentence-level, and multi-level approaches, all of which suffer from inefficiency or perceptibility issues due to the need for multiple queries or significant semantic shifts. In this work, we introduce a novel adversarial attack method, Emoji-Attack, which leverages the manipulation of emojis to create subtle, yet effective, perturbations. Unlike character- and word-level strategies, Emoji-Attack targets emojis as a distinct layer of attack, resulting in less noticeable changes with minimal disruption to the text. This approach has been largely unexplored in previous research, which typically focuses on emoji insertion as an extension of character-level attacks. Our experiments demonstrate that Emoji-Attack achieves strong attack performance on both large and small models, making it a promising technique for enhancing adversarial robustness in NLP systems.

### Benchmarking Retrieval-Augmented Generation in Multi-Modal Contexts 
[[arxiv](https://arxiv.org/abs/2502.17297)] [[cool](https://papers.cool/arxiv/2502.17297)] [[pdf](https://arxiv.org/pdf/2502.17297)]
> **Authors**: Zhenghao Liu,Xingsheng Zhu,Tianshuo Zhou,Xinyi Zhang,Xiaoyuan Yi,Yukun Yan,Yu Gu,Ge Yu,Maosong Sun
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: This paper introduces Multi-Modal Retrieval-Augmented Generation (M^2RAG), a benchmark designed to evaluate the effectiveness of Multi-modal Large Language Models (MLLMs) in leveraging knowledge from multi-modal retrieval documents. The benchmark comprises four tasks: image captioning, multi-modal question answering, multi-modal fact verification, and image reranking. All tasks are set in an open-domain setting, requiring RAG models to retrieve query-relevant information from a multi-modal document collection and use it as input context for RAG modeling. To enhance the context utilization capabilities of MLLMs, we also introduce Multi-Modal Retrieval-Augmented Instruction Tuning (MM-RAIT), an instruction tuning method that optimizes MLLMs within multi-modal contexts. Our experiments show that MM-RAIT improves the performance of RAG systems by enabling them to effectively learn from multi-modal contexts. All data and code are available at https://github.com/NEUIR/M2RAG.

### Making LLMs Reason? The Intermediate Language Problem in Neurosymbolic Approaches 
[[arxiv](https://arxiv.org/abs/2502.17216)] [[cool](https://papers.cool/arxiv/2502.17216)] [[pdf](https://arxiv.org/pdf/2502.17216)]
> **Authors**: Alexander Beiser,David Penz
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: Logical reasoning tasks manifest themselves as a challenge to Large Language Models (LLMs). Neurosymbolic approaches use LLMs to translate logical reasoning problems formulated in natural language into a formal intermediate language. Subsequently, the usage of symbolic reasoners yields reliable solving thereof. However, LLMs often fail in translation due to poorly chosen intermediate languages. We introduce the intermediate language problem, which is the problem of choosing a suitable formal language representation for neurosymbolic approaches. Theoretically, we argue that its origins lie in the inability of LLMs to distinguish syntax from semantics and the relative independence of the problem from its representation. We showcase its existence experimentally by contrasting two intermediate languages, Answer Set Programming and the Python Knowledge Engine. In addition, we demonstrate the effects of varying degrees of supplementary context information. Our results show a maximum difference in overall-accuracy of 53.20% and 49.26% in execution-accuracy. When using the GPT4o-mini LLM we beat the state-of-the-art in overall-accuracy on the ProntoQA dataset by 21.20% and by 50.50% on the ProofWriter dataset.

### CodeSwift: Accelerating LLM Inference for Efficient Code Generation 
[[arxiv](https://arxiv.org/abs/2502.17139)] [[cool](https://papers.cool/arxiv/2502.17139)] [[pdf](https://arxiv.org/pdf/2502.17139)]
> **Authors**: Qianhui Zhao,Li Zhang,Fang Liu,Xiaoli Lian,Qiaoyuanhe Meng,Ziqian Jiao,Zetong Zhou,Borui Zhang,Runlin Guo,Jia Li
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,软件工程
- **Abstract**: Code generation is a latency-sensitive task that demands high timeliness, but the autoregressive decoding mechanism of Large Language Models (LLMs) leads to poor inference efficiency. Existing LLM inference acceleration methods mainly focus on standalone functions using only built-in components. Moreover, they treat code like natural language sequences, ignoring its unique syntax and semantic characteristics. As a result, the effectiveness of these approaches in code generation tasks remains limited and fails to align with real-world programming scenarios. To alleviate this issue, we propose CodeSwift, a simple yet highly efficient inference acceleration approach specifically designed for code generation, without comprising the quality of the output. CodeSwift constructs a multi-source datastore, providing access to both general and project-specific knowledge, facilitating the retrieval of high-quality draft sequences. Moreover, CodeSwift reduces retrieval cost by controlling retrieval timing, and enhances efficiency through parallel retrieval and a context- and LLM preference-aware cache. Experimental results show that CodeSwift can reach up to 2.53x and 2.54x speedup compared to autoregressive decoding in repository-level and standalone code generation tasks, respectively, outperforming state-of-the-art inference acceleration approaches by up to 88%.

### Evaluating the Effectiveness of Large Language Models in Automated News Article Summarization 
[[arxiv](https://arxiv.org/abs/2502.17136)] [[cool](https://papers.cool/arxiv/2502.17136)] [[pdf](https://arxiv.org/pdf/2502.17136)]
> **Authors**: Lionel Richy Panlap Houamegni,Fatih Gedikli
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: :I.2.7; H.3.3
- **标题**: None
- **领域**: 人工智能,信息检索
- **Abstract**: The automation of news analysis and summarization presents a promising solution to the challenge of processing and analyzing vast amounts of information prevalent in today's information society. Large Language Models (LLMs) have demonstrated the capability to transform vast amounts of textual data into concise and easily comprehensible summaries, offering an effective solution to the problem of information overload and providing users with a quick overview of relevant information. A particularly significant application of this technology lies in supply chain risk analysis. Companies must monitor the news about their suppliers and respond to incidents for several critical reasons, including compliance with laws and regulations, risk management, and maintaining supply chain resilience. This paper develops an automated news summarization system for supply chain risk analysis using LLMs. The proposed solution aggregates news from various sources, summarizes them using LLMs, and presents the condensed information to users in a clear and concise format. This approach enables companies to optimize their information processing and make informed decisions. Our study addresses two main research questions: (1) Are LLMs effective in automating news summarization, particularly in the context of supply chain risk analysis? (2) How effective are various LLMs in terms of readability, duplicate detection, and risk identification in their summarization quality? In this paper, we conducted an offline study using a range of publicly available LLMs at the time and complemented it with a user study focused on the top performing systems of the offline experiments to evaluate their effectiveness further. Our results demonstrate that LLMs, particularly Few-Shot GPT-4o mini, offer significant improvements in summary quality and risk identification.

### Applications of Large Models in Medicine 
[[arxiv](https://arxiv.org/abs/2502.17132)] [[cool](https://papers.cool/arxiv/2502.17132)] [[pdf](https://arxiv.org/pdf/2502.17132)]
> **Authors**: YunHe Su,Zhengyang Lu,Junhui Liu,Ke Pang,Haoran Dai,Sa Liu Yuxin Jia,Lujia Ge,Jing-min Yang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: This paper explores the advancements and applications of large-scale models in the medical field, with a particular focus on Medical Large Models (MedLMs). These models, encompassing Large Language Models (LLMs), Vision Models, 3D Large Models, and Multimodal Models, are revolutionizing healthcare by enhancing disease prediction, diagnostic assistance, personalized treatment planning, and drug discovery. The integration of graph neural networks in medical knowledge graphs and drug discovery highlights the potential of Large Graph Models (LGMs) in understanding complex biomedical relationships. The study also emphasizes the transformative role of Vision-Language Models (VLMs) and 3D Large Models in medical image analysis, anatomical modeling, and prosthetic design. Despite the challenges, these technologies are setting new benchmarks in medical innovation, improving diagnostic accuracy, and paving the way for personalized healthcare solutions. This paper aims to provide a comprehensive overview of the current state and future directions of large models in medicine, underscoring their significance in advancing global health.

### Strength Estimation and Human-Like Strength Adjustment in Games 
[[arxiv](https://arxiv.org/abs/2502.17109)] [[cool](https://papers.cool/arxiv/2502.17109)] [[pdf](https://arxiv.org/pdf/2502.17109)]
> **Authors**: Chun Jung Chen,Chung-Chin Shih,Ti-Rong Wu
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Accepted by the Thirteenth International Conference onLearningRepresentations (ICLR 2025)
- **标题**: None
- **领域**: 人工智能,人机交互,机器学习
- **Abstract**: Strength estimation and adjustment are crucial in designing human-AI interactions, particularly in games where AI surpasses human players. This paper introduces a novel strength system, including a strength estimator (SE) and an SE-based Monte Carlo tree search, denoted as SE-MCTS, which predicts strengths from games and offers different playing strengths with human styles. The strength estimator calculates strength scores and predicts ranks from games without direct human interaction. SE-MCTS utilizes the strength scores in a Monte Carlo tree search to adjust playing strength and style. We first conduct experiments in Go, a challenging board game with a wide range of ranks. Our strength estimator significantly achieves over 80% accuracy in predicting ranks by observing 15 games only, whereas the previous method reached 49% accuracy for 100 games. For strength adjustment, SE-MCTS successfully adjusts to designated ranks while achieving a 51.33% accuracy in aligning to human actions, outperforming a previous state-of-the-art, with only 42.56% accuracy. To demonstrate the generality of our strength system, we further apply SE and SE-MCTS to chess and obtain consistent results. These results show a promising approach to strength estimation and adjustment, enhancing human-AI interactions in games. Our code is available at https://rlg.iis.sinica.edu.tw/papers/strength-estimator.

### TabulaTime: A Novel Multimodal Deep Learning Framework for Advancing Acute Coronary Syndrome Prediction through Environmental and Clinical Data Integration 
[[arxiv](https://arxiv.org/abs/2502.17049)] [[cool](https://papers.cool/arxiv/2502.17049)] [[pdf](https://arxiv.org/pdf/2502.17049)]
> **Authors**: Xin Zhang,Liangxiu Han,Stephen White,Saad Hassan,Philip A Kalra,James Ritchie,Carl Diver,Jennie Shorley
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: Acute Coronary Syndromes (ACS), including ST-segment elevation myocardial infarctions (STEMI) and non-ST-segment elevation myocardial infarctions (NSTEMI), remain a leading cause of mortality worldwide. Traditional cardiovascular risk scores rely primarily on clinical data, often overlooking environmental influences like air pollution that significantly impact heart health. Moreover, integrating complex time-series environmental data with clinical records is challenging. We introduce TabulaTime, a multimodal deep learning framework that enhances ACS risk prediction by combining clinical risk factors with air pollution data. TabulaTime features three key innovations: First, it integrates time-series air pollution data with clinical tabular data to improve prediction accuracy. Second, its PatchRWKV module automatically extracts complex temporal patterns, overcoming limitations of traditional feature engineering while maintaining linear computational complexity. Third, attention mechanisms enhance interpretability by revealing interactions between clinical and environmental factors. Experimental results show that TabulaTime improves prediction accuracy by over 20% compared to conventional models such as CatBoost, Random Forest, and LightGBM, with air pollution data alone contributing over a 10% improvement. Feature importance analysis identifies critical predictors including previous angina, systolic blood pressure, PM10, and NO2. Overall, TabulaTime bridges clinical and environmental insights, supporting personalized prevention strategies and informing public health policies to mitigate ACS risk.

### A Multi-LLM-Agent-Based Framework for Economic and Public Policy Analysis 
[[arxiv](https://arxiv.org/abs/2502.16879)] [[cool](https://papers.cool/arxiv/2502.16879)] [[pdf](https://arxiv.org/pdf/2502.16879)]
> **Authors**: Yuzhi Hao,Danyang Xie
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,普通经济学
- **Abstract**: This paper pioneers a novel approach to economic and public policy analysis by leveraging multiple Large Language Models (LLMs) as heterogeneous artificial economic agents. We first evaluate five LLMs' economic decision-making capabilities in solving two-period consumption allocation problems under two distinct scenarios: with explicit utility functions and based on intuitive reasoning. While previous research has often simulated heterogeneity by solely varying prompts, our approach harnesses the inherent variations in analytical capabilities across different LLMs to model agents with diverse cognitive traits. Building on these findings, we construct a Multi-LLM-Agent-Based (MLAB) framework by mapping these LLMs to specific educational groups and corresponding income brackets. Using interest-income taxation as a case study, we demonstrate how the MLAB framework can simulate policy impacts across heterogeneous agents, offering a promising new direction for economic and public policy analysis by leveraging LLMs' human-like reasoning capabilities and computational power.

## 计算复杂度(cs.CC:Computational Complexity)

### When Can We Solve the Weighted Low Rank Approximation Problem in Truly Subquadratic Time? 
[[arxiv](https://arxiv.org/abs/2502.16912)] [[cool](https://papers.cool/arxiv/2502.16912)] [[pdf](https://arxiv.org/pdf/2502.16912)]
> **Authors**: Chenyang Li,Yingyu Liang,Zhenmei Shi,Zhao Song
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: AIStats 2025
- **标题**: None
- **领域**: 计算复杂度,人工智能,机器学习
- **Abstract**: The weighted low-rank approximation problem is a fundamental numerical linear algebra problem and has many applications in machine learning. Given a $n \times n$ weight matrix $W$ and a $n \times n$ matrix $A$, the goal is to find two low-rank matrices $U, V \in \mathbb{R}^{n \times k}$ such that the cost of $\| W \circ (U V^\top - A) \|_F^2$ is minimized. Previous work has to pay $Ω(n^2)$ time when matrices $A$ and $W$ are dense, e.g., having $Ω(n^2)$ non-zero entries. In this work, we show that there is a certain regime, even if $A$ and $W$ are dense, we can still hope to solve the weighted low-rank approximation problem in almost linear $n^{1+o(1)}$ time.

## 计算语言学(cs.CL:Computation and Language)

### LR${}^{2}$Bench: Evaluating Long-chain Reflective Reasoning Capabilities of Large Language Models via Constraint Satisfaction Problems 
[[arxiv](https://arxiv.org/abs/2502.17848)] [[cool](https://papers.cool/arxiv/2502.17848)] [[pdf](https://arxiv.org/pdf/2502.17848)]
> **Authors**: Jianghao Chen,Zhenlin Wei,Zhenjiang Ren,Ziyong Li,Jiajun Zhang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Recent progress in o1-like models has significantly enhanced the reasoning abilities of Large Language Models (LLMs), empowering them to tackle increasingly complex tasks through reflection capabilities, such as making assumptions, backtracking, and self-refinement. However, effectively evaluating such reflection capabilities remains challenging due to the lack of appropriate benchmarks. To bridge this gap, we introduce LR${}^{2}$Bench, a novel benchmark designed to evaluate the Long-chain Reflective Reasoning capabilities of LLMs. LR${}^{2}$Bench comprises 850 samples across six Constraint Satisfaction Problems (CSPs) where reflective reasoning is crucial for deriving solutions that meet all given constraints. Each type of task focuses on distinct constraint patterns, such as knowledge-based, logical, and spatial constraints, providing a comprehensive evaluation of diverse problem-solving scenarios. We conduct extensive evaluation on both conventional models and o1-like models. Our experimental results reveal that even the most advanced reasoning-specific models, such as DeepSeek-R1 and OpenAI o1-preview, struggle with tasks in LR${}^{2}$Bench, achieving an average Exact Match score of only 20.0% and 23.6%, respectively. These findings underscore the significant room for improvement in the reflective reasoning capabilities of current LLMs. The leaderboard of our benchmark is available at https://huggingface.co/spaces/UltraRonin/LR2Bench

### Say Less, Mean More: Leveraging Pragmatics in Retrieval-Augmented Generation 
[[arxiv](https://arxiv.org/abs/2502.17839)] [[cool](https://papers.cool/arxiv/2502.17839)] [[pdf](https://arxiv.org/pdf/2502.17839)]
> **Authors**: Haris Riaz,Ellen Riloff,Mihai Surdeanu
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 16 pages, 2 figures, 8 tables. Preprint
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: We propose a simple, unsupervised method that injects pragmatic principles in retrieval-augmented generation (RAG) frameworks such as Dense Passage Retrieval to enhance the utility of retrieved contexts. Our approach first identifies which sentences in a pool of documents retrieved by RAG are most relevant to the question at hand, cover all the topics addressed in the input question and no more, and then highlights these sentences within their context, before they are provided to the LLM, without truncating or altering the context in any other way. We show that this simple idea brings consistent improvements in experiments on three question answering tasks (ARC-Challenge, PubHealth and PopQA) using five different LLMs. It notably enhances relative accuracy by up to 19.7% on PubHealth and 10% on ARC-Challenge compared to a conventional RAG system.

### Predicting Through Generation: Why Generation Is Better for Prediction 
[[arxiv](https://arxiv.org/abs/2502.17817)] [[cool](https://papers.cool/arxiv/2502.17817)] [[pdf](https://arxiv.org/pdf/2502.17817)]
> **Authors**: Md Kowsher,Nusrat Jahan Prottasha,Prakash Bhat,Chun-Nam Yu,Mojtaba Soltanalian,Ivan Garibay,Ozlem Garibay,Chen Chen,Niloofar Yousefi
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Preprint paper
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: This paper argues that generating output tokens is more effective than using pooled representations for prediction tasks because token-level generation retains more mutual information. Since LLMs are trained on massive text corpora using next-token prediction, generation aligns naturally with their learned behavior. Using the Data Processing Inequality (DPI), we provide both theoretical and empirical evidence supporting this claim. However, autoregressive models face two key challenges when used for prediction: (1) exposure bias, where the model sees ground truth tokens during training but relies on its own predictions during inference, leading to errors, and (2) format mismatch, where discrete tokens do not always align with the tasks required output structure. To address these challenges, we introduce PredGen(Predicting Through Generating), an end to end framework that (i) uses scheduled sampling to reduce exposure bias, and (ii) introduces a task adapter to convert the generated tokens into structured outputs. Additionally, we introduce Writer-Director Alignment Loss (WDAL), which ensures consistency between token generation and final task predictions, improving both text coherence and numerical accuracy. We evaluate PredGen on multiple classification and regression benchmarks. Our results show that PredGen consistently outperforms standard baselines, demonstrating its effectiveness in structured prediction tasks.

### Can Multimodal LLMs Perform Time Series Anomaly Detection? 
[[arxiv](https://arxiv.org/abs/2502.17812)] [[cool](https://papers.cool/arxiv/2502.17812)] [[pdf](https://arxiv.org/pdf/2502.17812)]
> **Authors**: Xiongxiao Xu,Haoran Wang,Yueqing Liang,Philip S. Yu,Yue Zhao,Kai Shu
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 9 pages for the main content; 32 pages for the full paper including the appendix. More resources on the intersection of multimodal LLMs and time series analysis are on the website https://mllm-ts.github.io
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Large language models (LLMs) have been increasingly used in time series analysis. However, the potential of multimodal LLMs (MLLMs), particularly vision-language models, for time series remains largely under-explored. One natural way for humans to detect time series anomalies is through visualization and textual description. Motivated by this, we raise a critical and practical research question: Can multimodal LLMs perform time series anomaly detection? To answer this, we propose VisualTimeAnomaly benchmark to evaluate MLLMs in time series anomaly detection (TSAD). Our approach transforms time series numerical data into the image format and feed these images into various MLLMs, including proprietary models (GPT-4o and Gemini-1.5) and open-source models (LLaVA-NeXT and Qwen2-VL), each with one larger and one smaller variant. In total, VisualTimeAnomaly contains 12.4k time series images spanning 3 scenarios and 3 anomaly granularities with 9 anomaly types across 8 MLLMs. Starting with the univariate case (point- and range-wise anomalies), we extend our evaluation to more practical scenarios, including multivariate and irregular time series scenarios, and variate-wise anomalies. Our study reveals several key insights: 1) MLLMs detect range- and variate-wise anomalies more effectively than point-wise anomalies. 2) MLLMs are highly robust to irregular time series, even with 25% of the data missing. 3) Open-source MLLMs perform comparably to proprietary models in TSAD. While open-source MLLMs excel on univariate time series, proprietary MLLMs demonstrate superior effectiveness on multivariate time series. To the best of our knowledge, this is the first work to comprehensively investigate MLLMs for TSAD, particularly for multivariate and irregular time series scenarios. We release our dataset and code at https://github.com/mllm-ts/VisualTimeAnomaly to support future research.

### URO-Bench: A Comprehensive Benchmark for End-to-End Spoken Dialogue Models 
[[arxiv](https://arxiv.org/abs/2502.17810)] [[cool](https://papers.cool/arxiv/2502.17810)] [[pdf](https://arxiv.org/pdf/2502.17810)]
> **Authors**: Ruiqi Yan,Xiquan Li,Wenxi Chen,Zhikang Niu,Chen Yang,Ziyang Ma,Kai Yu,Xie Chen
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,音频和语音处理
- **Abstract**: In recent years, with advances in large language models (LLMs), end-to-end spoken dialogue models (SDMs) have made significant strides. Compared to text-based LLMs, the evaluation of SDMs needs to take speech-related aspects into account, such as paralinguistic information and speech quality. However, there is still a lack of comprehensive evaluations for SDMs in speech-to-speech (S2S) scenarios. To address this gap, we propose URO-Bench, an extensive benchmark for SDMs. Notably, URO-Bench is the first S2S benchmark that covers evaluations about multilingualism, multi-round dialogues, and paralinguistics. Our benchmark is divided into two difficulty levels: basic track and pro track, consisting of 16 and 20 datasets respectively, evaluating the model's abilities in Understanding, Reasoning, and Oral conversation. Evaluations on our proposed benchmark reveal that current open-source SDMs perform rather well in daily QA tasks, but lag behind their backbone LLMs in terms of instruction-following ability and also suffer from catastrophic forgetting. Their performance in advanced evaluations of paralinguistic information and audio understanding remains subpar, highlighting the need for further research in this direction. We hope that URO-Bench can effectively facilitate the development of spoken dialogue models by providing a multifaceted evaluation of existing models and helping to track progress in this area.

### Your Language Model May Think Too Rigidly: Achieving Reasoning Consistency with Symmetry-Enhanced Training 
[[arxiv](https://arxiv.org/abs/2502.17800)] [[cool](https://papers.cool/arxiv/2502.17800)] [[pdf](https://arxiv.org/pdf/2502.17800)]
> **Authors**: Yihang Yao,Zhepeng Cen,Miao Li,William Han,Yuyou Zhang,Emerson Liu,Zuxin Liu,Chuang Gan,Ding Zhao
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models (LLMs) have demonstrated strong reasoning capabilities across various tasks. However, even minor variations in query phrasing, despite preserving the underlying semantic meaning, can significantly affect their performance. To address this, we focus on enhancing LLMs' awareness of symmetry in query variations and propose syMmetry-ENhanceD (MEND) Data Augmentation, a data-centric approach that improves the model's ability to extract useful information from context. Unlike existing methods that emphasize reasoning chain augmentation, our approach improves model robustness at the knowledge extraction stage through query augmentations, enabling more data-efficient training and stronger generalization to Out-of-Distribution (OOD) settings. Extensive experiments on both logical and arithmetic reasoning tasks show that MEND enhances reasoning performance across diverse query variations, providing new insight into improving LLM robustness through structured dataset curation.

### Enhancing Human Evaluation in Machine Translation with Comparative Judgment 
[[arxiv](https://arxiv.org/abs/2502.17797)] [[cool](https://papers.cool/arxiv/2502.17797)] [[pdf](https://arxiv.org/pdf/2502.17797)]
> **Authors**: Yixiao Song,Parker Riley,Daniel Deutsch,Markus Freitag
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Preprint, 15 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Human evaluation is crucial for assessing rapidly evolving language models but is influenced by annotator proficiency and task design. This study explores the integration of comparative judgment into human annotation for machine translation (MT) and evaluates three annotation setups-point-wise Multidimensional Quality Metrics (MQM), side-by-side (SxS) MQM, and its simplified version SxS relative ranking (RR). In MQM, annotators mark error spans with categories and severity levels. SxS MQM extends MQM to pairwise error annotation for two translations of the same input, while SxS RR focuses on selecting the better output without labeling errors. Key findings are: (1) the SxS settings achieve higher inter-annotator agreement than MQM; (2) SxS MQM enhances inter-translation error marking consistency compared to MQM by, on average, 38.5% for explicitly compared MT systems and 19.5% for others; (3) all annotation settings return stable system rankings, with SxS RR offering a more efficient alternative to (SxS) MQM; (4) the SxS settings highlight subtle errors overlooked in MQM without altering absolute system evaluations. To spur further research, we will release the triply annotated datasets comprising 377 ZhEn and 104 EnDe annotation examples.

### AIR: Complex Instruction Generation via Automatic Iterative Refinement 
[[arxiv](https://arxiv.org/abs/2502.17787)] [[cool](https://papers.cool/arxiv/2502.17787)] [[pdf](https://arxiv.org/pdf/2502.17787)]
> **Authors**: Wei Liu,Yancheng He,Hui Huang,Chengwei Hu,Jiaheng Liu,Shilong Li,Wenbo Su,Bo Zheng
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: The first three authors contributed equally, 20 pages
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: With the development of large language models, their ability to follow simple instructions has significantly improved. However, adhering to complex instructions remains a major challenge. Current approaches to generating complex instructions are often irrelevant to the current instruction requirements or suffer from limited scalability and diversity. Moreover, methods such as back-translation, while effective for simple instruction generation, fail to leverage the rich contents and structures in large web corpora. In this paper, we propose a novel automatic iterative refinement framework to generate complex instructions with constraints, which not only better reflects the requirements of real scenarios but also significantly enhances LLMs' ability to follow complex instructions. The AIR framework consists of two stages: (1)Generate an initial instruction from a document; (2)Iteratively refine instructions with LLM-as-judge guidance by comparing the model's output with the document to incorporate valuable constraints. Finally, we construct the AIR-10K dataset with 10K complex instructions and demonstrate that instructions generated with our approach significantly improve the model's ability to follow complex instructions, outperforming existing methods for instruction generation.

### Exploring the Potential of Large Language Models for Estimating the Reading Comprehension Question Difficulty 
[[arxiv](https://arxiv.org/abs/2502.17785)] [[cool](https://papers.cool/arxiv/2502.17785)] [[pdf](https://arxiv.org/pdf/2502.17785)]
> **Authors**: Yoshee Jain,John Hollander,Amber He,Sunny Tang,Liang Zhang,John Sabatini
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 13 pages, 2 figures
- **标题**: None
- **领域**: 计算语言学,人机交互
- **Abstract**: Reading comprehension is a key for individual success, yet the assessment of question difficulty remains challenging due to the extensive human annotation and large-scale testing required by traditional methods such as linguistic analysis and Item Response Theory (IRT). While these robust approaches provide valuable insights, their scalability is limited. There is potential for Large Language Models (LLMs) to automate question difficulty estimation; however, this area remains underexplored. Our study investigates the effectiveness of LLMs, specifically OpenAI's GPT-4o and o1, in estimating the difficulty of reading comprehension questions using the Study Aid and Reading Assessment (SARA) dataset. We evaluated both the accuracy of the models in answering comprehension questions and their ability to classify difficulty levels as defined by IRT. The results indicate that, while the models yield difficulty estimates that align meaningfully with derived IRT parameters, there are notable differences in their sensitivity to extreme item characteristics. These findings suggest that LLMs can serve as the scalable method for automated difficulty assessment, particularly in dynamic interactions between learners and Adaptive Instructional Systems (AIS), bridging the gap between traditional psychometric techniques and modern AIS for reading comprehension and paving the way for more adaptive and personalized educational assessments.

### FoREST: Frame of Reference Evaluation in Spatial Reasoning Tasks 
[[arxiv](https://arxiv.org/abs/2502.17775)] [[cool](https://papers.cool/arxiv/2502.17775)] [[pdf](https://arxiv.org/pdf/2502.17775)]
> **Authors**: Tanawan Premsri,Parisa Kordjamshidi
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 9 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Spatial reasoning is a fundamental aspect of human intelligence. One key concept in spatial cognition is the Frame of Reference (FoR), which identifies the perspective of spatial expressions. Despite its significance, FoR has received limited attention in AI models that need spatial intelligence. There is a lack of dedicated benchmarks and in-depth evaluation of large language models (LLMs) in this area. To address this issue, we introduce the Frame of Reference Evaluation in Spatial Reasoning Tasks (FoREST) benchmark, designed to assess FoR comprehension in LLMs. We evaluate LLMs on answering questions that require FoR comprehension and layout generation in text-to-image models using FoREST. Our results reveal a notable performance gap across different FoR classes in various LLMs, affecting their ability to generate accurate layouts for text-to-image generation. This highlights critical shortcomings in FoR comprehension. To improve FoR understanding, we propose Spatial-Guided prompting, which improves LLMs ability to extract essential spatial concepts. Our proposed method improves overall performance across spatial reasoning tasks.

### LLM Inference Acceleration via Efficient Operation Fusion 
[[arxiv](https://arxiv.org/abs/2502.17728)] [[cool](https://papers.cool/arxiv/2502.17728)] [[pdf](https://arxiv.org/pdf/2502.17728)]
> **Authors**: Mahsa Salmani,Ilya Soloveychik
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,硬件架构
- **Abstract**: The rapid development of the Transformer-based Large Language Models (LLMs) in recent years has been closely linked to their ever-growing and already enormous sizes. Many LLMs contain hundreds of billions of parameters and require dedicated hardware resources for training and inference. One of the key challenges inherent to the Transformer architecture is the requirement to support numerous non-linear transformations that involves normalization. For instance, each decoder block typically contains at least one Softmax operation and two Layernorms. The computation of the corresponding normalization scaling factors becomes a major bottleneck as it requires spatial collective operations. In other words, when it comes to the computation of denominators for Softmax and Layernorm, all vector elements must be aggregated into a single location, requiring significant communication. These collective operations slow down inference on Transformers by approximately 20%, defeating the whole purpose of distributed in-memory compute. In this work, we propose an extremely efficient technique that can completely hide the overhead caused by such collective operations. Note that each Softmax and Layernorm operation is typically followed by a linear layer. Since non-linear and linear operations are performed on different hardware engines, they can be easily parallelized once the algebra allows such commutation. By leveraging the inherent properties of linear operations, we can defer the normalization of the preceding Softmax and Layernorm until after the linear layer is computed. Now we can compute the collective scaling factors concurrently with the matrix multiplication and completely hide the latency of the former behind the latter. Such parallelization preserves the numerical accuracy while significantly improving the hardware utilization and reducing the overall latency.

### Spontaneous Giving and Calculated Greed in Language Models 
[[arxiv](https://arxiv.org/abs/2502.17720)] [[cool](https://papers.cool/arxiv/2502.17720)] [[pdf](https://arxiv.org/pdf/2502.17720)]
> **Authors**: Yuxuan Li,Hirokazu Shirado
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large language models, when trained with reinforcement learning, demonstrate advanced problem-solving capabilities through reasoning techniques like chain of thoughts and reflection. However, it is unclear how these reasoning capabilities extend to social intelligence. In this study, we investigate how reasoning influences model outcomes in social dilemmas. First, we examine the effects of chain-of-thought and reflection techniques in a public goods game. We then extend our analysis to six economic games on cooperation and punishment, comparing off-the-shelf non-reasoning and reasoning models. We find that reasoning models reduce cooperation and norm enforcement, prioritizing individual rationality. Consequently, groups with more reasoning models exhibit less cooperation and lower gains through repeated interactions. These behaviors parallel human tendencies of "spontaneous giving and calculated greed." Our results suggest the need for AI architectures that incorporate social intelligence alongside reasoning capabilities to ensure that AI supports, rather than disrupts, human cooperative intuition.

### Knowledge Distillation with Training Wheels 
[[arxiv](https://arxiv.org/abs/2502.17717)] [[cool](https://papers.cool/arxiv/2502.17717)] [[pdf](https://arxiv.org/pdf/2502.17717)]
> **Authors**: Guanlin Liu,Anand Ramachandran,Tanmay Gangwani,Yan Fu,Abhinav Sethy
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Knowledge distillation is used, in generative language modeling, to train a smaller student model using the help of a larger teacher model, resulting in improved capabilities for the student model. In this paper, we formulate a more general framework for knowledge distillation where the student learns from the teacher during training, and also learns to ask for the teacher's help at test-time following rules specifying test-time restrictions. Towards this, we first formulate knowledge distillation as an entropy-regularized value optimization problem. Adopting Path Consistency Learning to solve this, leads to a new knowledge distillation algorithm using on-policy and off-policy demonstrations. We extend this using constrained reinforcement learning to a framework that incorporates the use of the teacher model as a test-time reference, within constraints. In this situation, akin to a human learner, the model needs to learn not only the learning material, but also the relative difficulty of different sections to prioritize for seeking teacher help. We examine the efficacy of our method through experiments in translation and summarization tasks, observing trends in accuracy and teacher use, noting that our approach unlocks operating points not available to the popular Speculative Decoding approach.

### Bridging Information Gaps with Comprehensive Answers: Improving the Diversity and Informativeness of Follow-Up Questions 
[[arxiv](https://arxiv.org/abs/2502.17715)] [[cool](https://papers.cool/arxiv/2502.17715)] [[pdf](https://arxiv.org/pdf/2502.17715)]
> **Authors**: Zhe Liu,Taekyu Kang,Haoyu Wang,Seyed Hossein Alavi,Vered Shwartz
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 8 pages, 2 figures, submitted to ACL 2025
- **标题**: None
- **领域**: 计算语言学,人工智能,人机交互
- **Abstract**: Effective conversational systems are expected to dynamically generate contextual follow-up questions to elicit new information while maintaining the conversation flow. While humans excel at asking diverse and informative questions by intuitively assessing both obtained and missing information, existing models often fall short of human performance on this task. To mitigate this, we propose a method that generates diverse and informative questions based on targeting unanswered information using a hypothetical LLM-generated "comprehensive answer". Our method is applied to augment an existing follow-up questions dataset. The experimental results demonstrate that language models fine-tuned on the augmented datasets produce follow-up questions of significantly higher quality and diversity. This promising approach could be effectively adopted to future work to augment information-seeking dialogues for reducing ambiguities and improving the accuracy of LLM answers.

### Semantics drives analogical change in Germanic strong verb paradigms: a phylogenetic study 
[[arxiv](https://arxiv.org/abs/2502.17670)] [[cool](https://papers.cool/arxiv/2502.17670)] [[pdf](https://arxiv.org/pdf/2502.17670)]
> **Authors**: Alexandru Craevschi,Sarah Babinski,Chundra Cathcart
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: A large body of research on morphological paradigms makes the prediction that irregular morphological patterns of allomorphy are more likely to emerge and persist when they serve to mark important functional distinctions. More specifically, it has been observed that in some Germanic languages in which narrative past tense is expressed by the past participle, there is a greater affinity for stem allomorphy shared by preterite forms and past participles to the exclusion of present forms (the so-called ABB pattern), as it serves to enhance marking of the binary semantic opposition between present and past. Using data from 107 cognate verbs attested across 14 archaic and contemporary Germanic languages and a novel hierarchical phylogenetic model, we show that there is a greater long-term preference for this alternation pattern in situations where narrative past tense has been extended to the past participle, confirming this hypothesis. We further elucidate the mechanisms underlying this association, demonstrating that this association holds because verbs with the ABB pattern are more likely to preserve it in situations where it marks an important binary semantic opposition; however, there is less evidence that the ABB pattern is extended to verbs with different patterns under the same circumstances. These results bear on debate as to whether the distribution of irregularity we observe cross-linguistically is due primarily to (1) the preservation of irregular patterns or (2) an active drive toward irregularization in certain contexts, and are more in line with the first hypothesis.

### Towards Human Cognition: Visual Context Guides Syntactic Priming in Fusion-Encoded Models 
[[arxiv](https://arxiv.org/abs/2502.17669)] [[cool](https://papers.cool/arxiv/2502.17669)] [[pdf](https://arxiv.org/pdf/2502.17669)]
> **Authors**: Bushi Xiao,Michael Bennie,Jayetri Bardhan,Daisy Zhe Wang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 8 pages, 9 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: We introduced PRISMATIC, the first multimodal structural priming dataset, and proposed a reference-free evaluation metric that assesses priming effects without predefined target sentences. Using this metric, we constructed and tested models with different multimodal encoding architectures (dual encoder and fusion encoder) to investigate their structural preservation capabilities. Our findings show that models with both encoding methods demonstrate comparable syntactic priming effects. However, only fusion-encoded models exhibit robust positive correlations between priming effects and visual similarity, suggesting a cognitive process more aligned with human psycholinguistic patterns. This work provides new insights into evaluating and understanding how syntactic information is processed in multimodal language models.

### Towards Typologically Aware Rescoring to Mitigate Unfaithfulness in Lower-Resource Languages 
[[arxiv](https://arxiv.org/abs/2502.17664)] [[cool](https://papers.cool/arxiv/2502.17664)] [[pdf](https://arxiv.org/pdf/2502.17664)]
> **Authors**: Tsan Tsai Chan,Xin Tong,Thi Thu Uyen Hoang,Barbare Tepnadze,Wojciech Stempniak
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: ISCA/ITG Workshop on Diversity in Large Speech andLanguageModels
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Multilingual large language models (LLMs) are known to more frequently generate non-faithful output in resource-constrained languages (Guerreiro et al., 2023 - arXiv:2303.16104), potentially because these typologically diverse languages are underrepresented in their training data. To mitigate unfaithfulness in such settings, we propose using computationally light auxiliary models to rescore the outputs of larger architectures. As proof of the feasibility of such an approach, we show that monolingual 4-layer BERT models pretrained from scratch on less than 700 MB of data without fine-tuning are able to identify faithful summaries with a mean accuracy of 88.33% in three genetically unrelated languages that differ in their morphological complexity - Vietnamese, Polish and Georgian. The same hyperparameter combination moreover generalises well to three other tasks, suggesting applications for rescoring beyond improving faithfulness. In order to inform typologically aware model selection, we also investigate how morphological complexity interacts with regularisation, model depth and training objectives, ultimately demonstrating that morphologically complex languages are more likely to benefit from dropout, while across languages downstream performance is enhanced most by shallow architectures as well as training using the standard BERT objectives.

### Evaluating the Effect of Retrieval Augmentation on Social Biases 
[[arxiv](https://arxiv.org/abs/2502.17611)] [[cool](https://papers.cool/arxiv/2502.17611)] [[pdf](https://arxiv.org/pdf/2502.17611)]
> **Authors**: Tianhui Zhang,Yi Zhou,Danushka Bollegala
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 18 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Retrieval Augmented Generation (RAG) has gained popularity as a method for conveniently incorporating novel facts that were not seen during the pre-training stage in Large Language Model (LLM)-based Natural Language Generation (NLG) systems. However, LLMs are known to encode significant levels of unfair social biases. The modulation of these biases by RAG in NLG systems is not well understood. In this paper, we systematically study the relationship between the different components of a RAG system and the social biases presented in the text generated across three languages (i.e. English, Japanese and Chinese) and four social bias types (i.e. gender, race, age and religion). Specifically, using the Bias Question Answering (BBQ) benchmark datasets, we evaluate the social biases in RAG responses from document collections with varying levels of stereotypical biases, employing multiple LLMs used as generators. We find that the biases in document collections are often amplified in the generated responses, even when the generating LLM exhibits a low-level of bias. Our findings raise concerns about the use of RAG as a technique for injecting novel facts into NLG systems and call for careful evaluation of potential social biases in RAG applications before their real-world deployment.

### PICASO: Permutation-Invariant Context Composition with State Space Models 
[[arxiv](https://arxiv.org/abs/2502.17605)] [[cool](https://papers.cool/arxiv/2502.17605)] [[pdf](https://arxiv.org/pdf/2502.17605)]
> **Authors**: Tian Yu Liu,Alessandro Achille,Matthew Trager,Aditya Golatkar,Luca Zancato,Stefano Soatto
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Published in The Thirteenth International Conference onLearningRepresentations, ICLR 2025
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Providing Large Language Models with relevant contextual knowledge at inference time has been shown to greatly improve the quality of their generations. This is often achieved by prepending informative passages of text, or 'contexts', retrieved from external knowledge bases to their input. However, processing additional contexts online incurs significant computation costs that scale with their length. State Space Models (SSMs) offer a promising solution by allowing a database of contexts to be mapped onto fixed-dimensional states from which to start the generation. A key challenge arises when attempting to leverage information present across multiple contexts, since there is no straightforward way to condition generation on multiple independent states in existing SSMs. To address this, we leverage a simple mathematical relation derived from SSM dynamics to compose multiple states into one that efficiently approximates the effect of concatenating textual contexts. Since the temporal ordering of contexts can often be uninformative, we enforce permutation-invariance by efficiently averaging states obtained via our composition algorithm across all possible context orderings. We evaluate our resulting method on WikiText and MSMARCO in both zero-shot and fine-tuned settings, and show that we can match the strongest performing baseline while enjoying on average 5.4x speedup.

### MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context Inference 
[[arxiv](https://arxiv.org/abs/2502.17599)] [[cool](https://papers.cool/arxiv/2502.17599)] [[pdf](https://arxiv.org/pdf/2502.17599)]
> **Authors**: Zhongwei Wan,Hui Shen,Xin Wang,Che Liu,Zheda Mai,Mi Zhang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: NAACL 2025 Main
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Long-context Multimodal Large Language Models (MLLMs) that incorporate long text-image and text-video modalities, demand substantial resources as their multimodal Key-Value (KV) caches grow with increasing input lengths, challenging inference efficiency. Existing methods for KV cache compression, in both text-only and multimodal LLMs, have neglected attention density variations across layers, thus often adopting uniform or progressive reduction strategies for layer-wise cache allocation. In this work, we propose MEDA, a dynamic layer-wise KV cache allocation method for efficient multimodal long-context inference. As its core, MEDA utilizes cross-modal attention entropy to determine the KV cache size at each MLLMs layer. Given the dynamically allocated KV cache size at each layer, MEDA also employs a KV pair selection scheme to identify which KV pairs to select and a KV pair merging strategy that merges the selected and non-selected ones to preserve information from the entire context. MEDA achieves up to 72% KV cache memory reduction and 2.82 times faster decoding speed, while maintaining or enhancing performance on various multimodal tasks in long-context settings, including multi-images and long-video scenarios. Our code is released at https://github.com/AIoT-MLSys-Lab/MEDA.

### Proactive Privacy Amnesia for Large Language Models: Safeguarding PII with Negligible Impact on Model Utility 
[[arxiv](https://arxiv.org/abs/2502.17591)] [[cool](https://papers.cool/arxiv/2502.17591)] [[pdf](https://arxiv.org/pdf/2502.17591)]
> **Authors**: Martin Kuo,Jingyang Zhang,Jianyi Zhang,Minxue Tang,Louis DiValentin,Aolin Ding,Jingwei Sun,William Chen,Amin Hass,Tianlong Chen,Yiran Chen,Hai Li
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: ICLR'25 Poster. Project page and code is available at https://ppa-iclr2025.my.canva.site/
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: With the rise of large language models (LLMs), increasing research has recognized their risk of leaking personally identifiable information (PII) under malicious attacks. Although efforts have been made to protect PII in LLMs, existing methods struggle to balance privacy protection with maintaining model utility. In this paper, inspired by studies of amnesia in cognitive science, we propose a novel approach, Proactive Privacy Amnesia (PPA), to safeguard PII in LLMs while preserving their utility. This mechanism works by actively identifying and forgetting key memories most closely associated with PII in sequences, followed by a memory implanting using suitable substitute memories to maintain the LLM's functionality. We conduct evaluations across multiple models to protect common PII, such as phone numbers and physical addresses, against prevalent PII-targeted attacks, demonstrating the superiority of our method compared with other existing defensive techniques. The results show that our PPA method completely eliminates the risk of phone number exposure by 100% and significantly reduces the risk of physical address exposure by 9.8% - 87.6%, all while maintaining comparable model utility performance.

### End-to-End Chart Summarization via Visual Chain-of-Thought in Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2502.17589)] [[cool](https://papers.cool/arxiv/2502.17589)] [[pdf](https://arxiv.org/pdf/2502.17589)]
> **Authors**: Raymond Choi,Frank Burns,Chase Lawrence
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Automated chart summarization is crucial for enhancing data accessibility and enabling efficient information extraction from visual data. While recent advances in visual-language models (VLMs) have demonstrated promise, existing methods often suffer from limitations in matching the generated summary to the chart data and in reasoning about complex chart patterns. This paper introduces End-to-End Visual Chain-of-Thought (V-CoT) for chart summarization, a novel approach optimized for Large Vision-Language Models (LVLMs). Our method directly trains an LVLM to process chart images and generate textual summaries in an end-to-end fashion, eliminating the need for explicit chart parsing modules. We incorporate a visual Chain-of-Thought mechanism through instruction fine-tuning, implicitly guiding the LVLM to perform visual reasoning steps during summary generation. Evaluated on the large-scale Chart-Sum-QA dataset, our V-CoT method significantly outperforms state-of-the-art baselines across a range of automatic metrics, including BLEU, BLEURT, CIDEr, and CS, and demonstrates superior matching degree and reasoning correctness in human evaluations. Ablation studies and detailed analyses further validate the effectiveness and robustness of our proposed approach, establishing a new benchmark for end-to-end chart summarization.

### Towards Conditioning Clinical Text Generation for User Control 
[[arxiv](https://arxiv.org/abs/2502.17571)] [[cool](https://papers.cool/arxiv/2502.17571)] [[pdf](https://arxiv.org/pdf/2502.17571)]
> **Authors**: Osman Alperen Koraş,Rabi Bahnan,Jens Kleesiek,Amin Dada
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: :I.2.7; I.2.1
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Deploying natural language generation systems in clinical settings remains challenging despite advances in Large Language Models (LLMs), which continue to exhibit hallucinations and factual inconsistencies, necessitating human oversight. This paper explores automated dataset augmentation using LLMs as human proxies to condition LLMs for clinician control without increasing cognitive workload. On the BioNLP ACL'24 Discharge Me! Shared Task, we achieve new state-of-the-art results with simpler methods than prior submissions through more efficient training, yielding a 9\% relative improvement without augmented training and up to 34\% with dataset augmentation. Preliminary human evaluation further supports the effectiveness of our approach, highlighting the potential of augmenting clinical text generation for control to enhance relevance, accuracy, and factual consistency.

### Policy Learning with a Natural Language Action Space: A Causal Approach 
[[arxiv](https://arxiv.org/abs/2502.17538)] [[cool](https://papers.cool/arxiv/2502.17538)] [[pdf](https://arxiv.org/pdf/2502.17538)]
> **Authors**: Bohan Zhang,Yixin Wang,Paramveer S. Dhillon
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: This paper introduces a novel causal framework for multi-stage decision-making in natural language action spaces where outcomes are only observed after a sequence of actions. While recent approaches like Proximal Policy Optimization (PPO) can handle such delayed-reward settings in high-dimensional action spaces, they typically require multiple models (policy, value, and reward) and substantial training data. Our approach employs Q-learning to estimate Dynamic Treatment Regimes (DTR) through a single model, enabling data-efficient policy learning via gradient ascent on language embeddings. A key technical contribution of our approach is a decoding strategy that translates optimized embeddings back into coherent natural language. We evaluate our approach on mental health intervention, hate speech countering, and sentiment transfer tasks, demonstrating significant improvements over competitive baselines across multiple metrics. Notably, our method achieves superior transfer strength while maintaining content preservation and fluency, as validated through human evaluation. Our work provides a practical foundation for learning optimal policies in complex language tasks where training data is limited.

### LongSpec: Long-Context Speculative Decoding with Efficient Drafting and Verification 
[[arxiv](https://arxiv.org/abs/2502.17421)] [[cool](https://papers.cool/arxiv/2502.17421)] [[pdf](https://arxiv.org/pdf/2502.17421)]
> **Authors**: Penghui Yang,Cunxiao Du,Fengzhuo Zhang,Haonan Wang,Tianyu Pang,Chao Du,Bo An
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Speculative decoding has become a promising technique to mitigate the high inference latency of autoregressive decoding in Large Language Models (LLMs). Despite its promise, the effective application of speculative decoding in LLMs still confronts three key challenges: the increasing memory demands of the draft model, the distribution shift between the short-training corpora and long-context inference, and inefficiencies in attention implementation. In this work, we enhance the performance of speculative decoding in long-context settings by addressing these challenges. First, we propose a memory-efficient draft model with a constant-sized Key-Value (KV) cache. Second, we introduce novel position indices for short-training data, enabling seamless adaptation from short-context training to long-context inference. Finally, we present an innovative attention aggregation method that combines fast implementations for prefix computation with standard attention for tree mask handling, effectively resolving the latency and memory inefficiencies of tree decoding. Our approach achieves strong results on various long-context tasks, including repository-level code completion, long-context summarization, and o1-like long reasoning tasks, demonstrating significant improvements in latency reduction. The code is available at https://github.com/sail-sg/LongSpec.

### Reasoning with Latent Thoughts: On the Power of Looped Transformers 
[[arxiv](https://arxiv.org/abs/2502.17416)] [[cool](https://papers.cool/arxiv/2502.17416)] [[pdf](https://arxiv.org/pdf/2502.17416)]
> **Authors**: Nikunj Saunshi,Nishanth Dikkala,Zhiyuan Li,Sanjiv Kumar,Sashank J. Reddi
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: ICLR 2025
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Large language models have shown remarkable reasoning abilities and scaling laws suggest that large parameter count, especially along the depth axis, is the primary driver. In this work, we make a stronger claim -- many reasoning problems require a large depth but not necessarily many parameters. This unlocks a novel application of looped models for reasoning. Firstly, we show that for many synthetic reasoning problems like addition, $p$-hop induction, and math problems, a $k$-layer transformer looped $L$ times nearly matches the performance of a $kL$-layer non-looped model, and is significantly better than a $k$-layer model. This is further corroborated by theoretical results showing that many such reasoning problems can be solved via iterative algorithms, and thus, can be solved effectively using looped models with nearly optimal depth. Perhaps surprisingly, these benefits also translate to practical settings of language modeling -- on many downstream reasoning tasks, a language model with $k$-layers looped $L$ times can be competitive to, if not better than, a $kL$-layer language model. In fact, our empirical analysis reveals an intriguing phenomenon: looped and non-looped models exhibit scaling behavior that depends on their effective depth, akin to the inference-time scaling of chain-of-thought (CoT) reasoning. We further elucidate the connection to CoT reasoning by proving that looped models implicitly generate latent thoughts and can simulate $T$ steps of CoT with $T$ loops. Inspired by these findings, we also present an interesting dichotomy between reasoning and memorization, and design a looping-based regularization that is effective on both fronts.

### Linguistic Generalizability of Test-Time Scaling in Mathematical Reasoning 
[[arxiv](https://arxiv.org/abs/2502.17407)] [[cool](https://papers.cool/arxiv/2502.17407)] [[pdf](https://arxiv.org/pdf/2502.17407)]
> **Authors**: Guijin Son,Jiwoo Hong,Hyunwoo Ko,James Thorne
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: work in progress
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Scaling pre-training compute has proven effective for achieving mulitlinguality, but does the same hold for test-time scaling? In this work, we introduce MCLM, a multilingual math benchmark featuring competition-level problems in 55 languages. We test three test-time scaling methods-Outcome Reward Modeling (ORM), Process Reward Modeling (ORM), and Budget Forcing (BF)-on both Qwen2.5-1.5B Math and MR1-1.5B, a multilingual LLM we trained for extended reasoning. Our experiments show that using Qwen2.5-1.5B Math with ORM achieves a score of 35.8 on MCLM, while BF on MR1-1.5B attains 35.2. Although "thinking LLMs" have recently garnered significant attention, we find that their performance is comparable to traditional scaling methods like best-of-N once constrained to similar levels of inference FLOPs. Moreover, while BF yields a 20-point improvement on English AIME, it provides only a 1.94-point average gain across other languages-a pattern consistent across the other test-time scaling methods we studied-higlighting that test-time scaling may not generalize as effectively to multilingual tasks. To foster further research, we release MCLM, MR1-1.5B, and evaluation results.

### Mitigating Bias in RAG: Controlling the Embedder 
[[arxiv](https://arxiv.org/abs/2502.17390)] [[cool](https://papers.cool/arxiv/2502.17390)] [[pdf](https://arxiv.org/pdf/2502.17390)]
> **Authors**: Taeyoun Kim,Jacob Springer,Aditi Raghunathan,Maarten Sap
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 26 pages (8 main), 12 figures, 7 tables
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: In retrieval augmented generation (RAG) systems, each individual component -- the LLM, embedder, and corpus -- could introduce biases in the form of skews towards outputting certain perspectives or identities. In this work, we study the conflict between biases of each component and their relationship to the overall bias of the RAG system, which we call bias conflict. Examining both gender and political biases as case studies, we show that bias conflict can be characterized through a linear relationship among components despite its complexity in 6 different LLMs. Through comprehensive fine-tuning experiments creating 120 differently biased embedders, we demonstrate how to control bias while maintaining utility and reveal the importance of reverse-biasing the embedder to mitigate bias in the overall system. Additionally, we find that LLMs and tasks exhibit varying sensitivities to the embedder bias, a crucial factor to consider for debiasing. Our results underscore that a fair RAG system can be better achieved by carefully controlling the bias of the embedder rather than increasing its fairness.

### What is a Good Question? Utility Estimation with LLM-based Simulations 
[[arxiv](https://arxiv.org/abs/2502.17383)] [[cool](https://papers.cool/arxiv/2502.17383)] [[pdf](https://arxiv.org/pdf/2502.17383)]
> **Authors**: Dong-Ho Lee,Hyundong Cho,Jonathan May,Jay Pujara
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 18 pages, 5 figures, 6 tables
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Asking questions is a fundamental aspect of learning that facilitates deeper understanding. However, characterizing and crafting questions that effectively improve learning remains elusive. To address this gap, we propose QUEST (Question Utility Estimation with Simulated Tests). QUEST simulates a learning environment that enables the quantification of a question's utility based on its direct impact on improving learning outcomes. Furthermore, we can identify high-utility questions and use them to fine-tune question generation models with rejection sampling. We find that questions generated by models trained with rejection sampling based on question utility result in exam scores that are higher by at least 20% than those from specialized prompting grounded on educational objectives literature and models fine-tuned with indirect measures of question quality, such as saliency and expected information gain.

### Bridging Gaps in Natural Language Processing for Yorùbá: A Systematic Review of a Decade of Progress and Prospects 
[[arxiv](https://arxiv.org/abs/2502.17364)] [[cool](https://papers.cool/arxiv/2502.17364)] [[pdf](https://arxiv.org/pdf/2502.17364)]
> **Authors**: Toheeb A. Jimoh,Tabea De Wille,Nikola S. Nikolov
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Natural Language Processing (NLP) is becoming a dominant subset of artificial intelligence as the need to help machines understand human language looks indispensable. Several NLP applications are ubiquitous, partly due to the myriads of datasets being churned out daily through mediums like social networking sites. However, the growing development has not been evident in most African languages due to the persisting resource limitation, among other issues. Yorùbá language, a tonal and morphologically rich African language, suffers a similar fate, resulting in limited NLP usage. To encourage further research towards improving this situation, this systematic literature review aims to comprehensively analyse studies addressing NLP development for Yorùbá, identifying challenges, resources, techniques, and applications. A well-defined search string from a structured protocol was employed to search, select, and analyse 105 primary studies between 2014 and 2024 from reputable databases. The review highlights the scarcity of annotated corpora, limited availability of pre-trained language models, and linguistic challenges like tonal complexity and diacritic dependency as significant obstacles. It also revealed the prominent techniques, including rule-based methods, among others. The findings reveal a growing body of multilingual and monolingual resources, even though the field is constrained by socio-cultural factors such as code-switching and desertion of language for digital usage. This review synthesises existing research, providing a foundation for advancing NLP for Yorùbá and in African languages generally. It aims to guide future research by identifying gaps and opportunities, thereby contributing to the broader inclusion of Yorùbá and other under-resourced African languages in global NLP advancements.

### On Relation-Specific Neurons in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.17355)] [[cool](https://papers.cool/arxiv/2502.17355)] [[pdf](https://arxiv.org/pdf/2502.17355)]
> **Authors**: Yihong Liu,Runsheng Chen,Lea Hirlimann,Ahmad Dawar Hakimi,Mingyang Wang,Amir Hossein Kargaran,Sascha Rothe,François Yvon,Hinrich Schütze
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: preprint
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: In large language models (LLMs), certain neurons can store distinct pieces of knowledge learned during pretraining. While knowledge typically appears as a combination of relations and entities, it remains unclear whether some neurons focus on a relation itself -- independent of any entity. We hypothesize such neurons detect a relation in the input text and guide generation involving such a relation. To investigate this, we study the Llama-2 family on a chosen set of relations with a statistics-based method. Our experiments demonstrate the existence of relation-specific neurons. We measure the effect of selectively deactivating candidate neurons specific to relation $r$ on the LLM's ability to handle (1) facts whose relation is $r$ and (2) facts whose relation is a different relation $r' \neq r$. With respect to their capacity for encoding relation information, we give evidence for the following three properties of relation-specific neurons. $\textbf{(i) Neuron cumulativity.}$ The neurons for $r$ present a cumulative effect so that deactivating a larger portion of them results in the degradation of more facts in $r$. $\textbf{(ii) Neuron versatility.}$ Neurons can be shared across multiple closely related as well as less related relations. Some relation neurons transfer across languages. $\textbf{(iii) Neuron interference.}$ Deactivating neurons specific to one relation can improve LLM generation performance for facts of other relations. We will make our code publicly available at https://github.com/cisnlp/relation-specific-neurons.

### Mutual Reinforcement of LLM Dialogue Synthesis and Summarization Capabilities for Few-Shot Dialogue Summarization 
[[arxiv](https://arxiv.org/abs/2502.17328)] [[cool](https://papers.cool/arxiv/2502.17328)] [[pdf](https://arxiv.org/pdf/2502.17328)]
> **Authors**: Yen-Ju Lu,Ting-Yao Hu,Hema Swetha Koppula,Hadi Pouransari,Jen-Hao Rick Chang,Yin Xia,Xiang Kong,Qi Zhu,Simon Wang,Oncel Tuzel,Raviteja Vemulapalli
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: NAACL 2025 Findings
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: In this work, we propose Mutual Reinforcing Data Synthesis (MRDS) within LLMs to improve few-shot dialogue summarization task. Unlike prior methods that require external knowledge, we mutually reinforce the LLMś dialogue synthesis and summarization capabilities, allowing them to complement each other during training and enhance overall performances. The dialogue synthesis capability is enhanced by directed preference optimization with preference scoring from summarization capability. The summarization capability is enhanced by the additional high quality dialogue-summary paired data produced by the dialogue synthesis capability. By leveraging the proposed MRDS mechanism, we elicit the internal knowledge of LLM in the format of synthetic data, and use it to augment the few-shot real training dataset. Empirical results demonstrate that our method improves dialogue summarization, achieving a 1.5% increase in ROUGE scores and a 0.3% improvement in BERT scores in few-shot settings. Furthermore, our method attains the highest average scores in human evaluations, surpassing both the pre-trained models and the baselines fine-tuned solely for summarization tasks.

### Turning Conversations into Workflows: A Framework to Extract and Evaluate Dialog Workflows for Service AI Agents 
[[arxiv](https://arxiv.org/abs/2502.17321)] [[cool](https://papers.cool/arxiv/2502.17321)] [[pdf](https://arxiv.org/pdf/2502.17321)]
> **Authors**: Prafulla Kumar Choubey,Xiangyu Peng,Shilpa Bhagavath,Caiming Xiong,Shiva Kumar Pentyala,Chien-Sheng Wu
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Automated service agents require well-structured workflows to provide consistent and accurate responses to customer queries. However, these workflows are often undocumented, and their automatic extraction from conversations remains unexplored. In this work, we present a novel framework for extracting and evaluating dialog workflows from historical interactions. Our extraction process consists of two key stages: (1) a retrieval step to select relevant conversations based on key procedural elements, and (2) a structured workflow generation process using a question-answer-based chain-of-thought (QA-CoT) prompting. To comprehensively assess the quality of extracted workflows, we introduce an automated agent and customer bots simulation framework that measures their effectiveness in resolving customer issues. Extensive experiments on the ABCD and SynthABCD datasets demonstrate that our QA-CoT technique improves workflow extraction by 12.16\% in average macro accuracy over the baseline. Moreover, our evaluation method closely aligns with human assessments, providing a reliable and scalable framework for future research.

### HIPPO: Enhancing the Table Understanding Capability of Large Language Models through Hybrid-Modal Preference Optimization 
[[arxiv](https://arxiv.org/abs/2502.17315)] [[cool](https://papers.cool/arxiv/2502.17315)] [[pdf](https://arxiv.org/pdf/2502.17315)]
> **Authors**: Zhenghao Liu,Haolan Wang,Xinze Li,Qiushi Xiong,Xiaocui Yang,Yu Gu,Yukun Yan,Qi Shi,Fangfang Li,Ge Yu,Maosong Sun
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Tabular data contains rich structural semantics and plays a crucial role in organizing and manipulating information. To better capture these structural semantics, this paper introduces the HybrId-modal Preference oPtimizatiOn (HIPPO) model, which represents tables using both text and image, and optimizes MLLMs to effectively learn more comprehensive table information from these multiple modalities. Specifically, HIPPO samples model responses from hybrid-modal table representations and designs a modality-consistent sampling strategy to enhance response diversity and mitigate modality bias during DPO training. Experimental results on table question answering and table fact verification tasks demonstrate the effectiveness of HIPPO, achieving a 4% improvement over various table reasoning models. Further analysis reveals that HIPPO not only enhances reasoning abilities based on unimodal table representations but also facilitates the extraction of crucial and distinct semantics from different modal representations. All data and codes are available at https://github.com/NEUIR/HIPPO.

### Implicit Word Reordering with Knowledge Distillation for Cross-Lingual Dependency Parsing 
[[arxiv](https://arxiv.org/abs/2502.17308)] [[cool](https://papers.cool/arxiv/2502.17308)] [[pdf](https://arxiv.org/pdf/2502.17308)]
> **Authors**: Zhuoran Li,Chunming Hu,Junfan Chen,Zhijun Chen,Richong Zhang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 9 pages, 5 figures, 3 tables. Accepted by The 39th Annual AAAI Conference on Artificial Intelligence (AAAI 2025)
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Word order difference between source and target languages is a major obstacle to cross-lingual transfer, especially in the dependency parsing task. Current works are mostly based on order-agnostic models or word reordering to mitigate this problem. However, such methods either do not leverage grammatical information naturally contained in word order or are computationally expensive as the permutation space grows exponentially with the sentence length. Moreover, the reordered source sentence with an unnatural word order may be a form of noising that harms the model learning. To this end, we propose an Implicit Word Reordering framework with Knowledge Distillation (IWR-KD). This framework is inspired by that deep networks are good at learning feature linearization corresponding to meaningful data transformation, e.g. word reordering. To realize this idea, we introduce a knowledge distillation framework composed of a word-reordering teacher model and a dependency parsing student model. We verify our proposed method on Universal Dependency Treebanks across 31 different languages and show it outperforms a series of competitors, together with experimental analysis to illustrate how our method works towards training a robust parser.

### `Generalization is hallucination' through the lens of tensor completions 
[[arxiv](https://arxiv.org/abs/2502.17305)] [[cool](https://papers.cool/arxiv/2502.17305)] [[pdf](https://arxiv.org/pdf/2502.17305)]
> **Authors**: Liang Ze Wong
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: In this short position paper, we introduce tensor completions and artifacts and make the case that they are a useful theoretical framework for understanding certain types of hallucinations and generalizations in language models.

### Child vs. machine language learning: Can the logical structure of human language unleash LLMs? 
[[arxiv](https://arxiv.org/abs/2502.17304)] [[cool](https://papers.cool/arxiv/2502.17304)] [[pdf](https://arxiv.org/pdf/2502.17304)]
> **Authors**: Uli Sauerland,Celia Matthaei,Felix Salfner
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: ISCA/ITG Workshop on Diversity in Large Speech andLanguageModels
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: We argue that human language learning proceeds in a manner that is different in nature from current approaches to training LLMs, predicting a difference in learning biases. We then present evidence from German plural formation by LLMs that confirm our hypothesis that even very powerful implementations produce results that miss aspects of the logic inherent to language that humans have no problem with. We conclude that attention to the different structures of human language and artificial neural networks is likely to be an avenue to improve LLM performance.

### Improving the Inclusivity of Dutch Speech Recognition by Fine-tuning Whisper on the JASMIN-CGN Corpus 
[[arxiv](https://arxiv.org/abs/2502.17284)] [[cool](https://papers.cool/arxiv/2502.17284)] [[pdf](https://arxiv.org/pdf/2502.17284)]
> **Authors**: Golshid Shekoufandeh,Paul Boersma,Antal van den Bosch
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: ISCA/ITG Workshop on Diversity in Large Speech andLanguageModels
- **标题**: None
- **领域**: 计算语言学,声音,音频和语音处理
- **Abstract**: We test and study the variation in speech recognition of fine-tuned versions of the Whisper model on child, elderly and non-native Dutch speech from the JASMIN-CGN corpus. Our primary goal is to evaluate how speakers' age and linguistic background influence Whisper's performance. Whisper achieves varying Word Error Rates (WER) when fine-tuned on subpopulations of specific ages and linguistic backgrounds. Fine-tuned performance is remarkably better than zero-shot performance, achieving a relative reduction in WER of 81% for native children, 72% for non-native children, 67% for non-native adults, and 65% for native elderly people. Our findings underscore the importance of training speech recognition models like Whisper on underrepresented subpopulations such as children, the elderly, and non-native speakers.

### Capability Instruction Tuning: A New Paradigm for Dynamic LLM Routing 
[[arxiv](https://arxiv.org/abs/2502.17282)] [[cool](https://papers.cool/arxiv/2502.17282)] [[pdf](https://arxiv.org/pdf/2502.17282)]
> **Authors**: Yi-Kai Zhang,De-Chuan Zhan,Han-Jia Ye
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: AAAI 2025; Project Page: https://cit-llm-routing.github.io
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Large Language Models (LLMs) have demonstrated human-like instruction-following abilities, particularly those exceeding 100 billion parameters. The combined capability of some smaller, resource-friendly LLMs can address most of the instructions that larger LLMs excel at. In this work, we explore how to route the best-performing LLM for each instruction to achieve better overall performance. We develop a new paradigm, constructing capability instructions with model capability representation, user instruction, and performance inquiry prompts to assess the performance. To learn from capability instructions, we introduce a new end-to-end framework called Model Selection with Aptitude Test (Model-SAT), which generates positive and negative samples based on what different models perform well or struggle with. Model-SAT uses a model capability encoder that extends its model representation to a lightweight LLM. Our experiments show that Model-SAT understands the performance dimensions of candidate models and provides the probabilities of their capability to handle various instructions. Additionally, during deployment, a new model can quickly infer its aptitude test results across 50 tasks, each with 20 shots. Model-SAT performs state-of-the-art model routing without candidate inference and in real-world new model-released scenarios. The code is available at https://github.com/Now-Join-Us/CIT-LLM-Routing

### Extracting domain-specific terms using contextual word embeddings 
[[arxiv](https://arxiv.org/abs/2502.17278)] [[cool](https://papers.cool/arxiv/2502.17278)] [[pdf](https://arxiv.org/pdf/2502.17278)]
> **Authors**: Andraž Repar,Nada Lavrač,Senja Pollak
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Automated terminology extraction refers to the task of extracting meaningful terms from domain-specific texts. This paper proposes a novel machine learning approach to terminology extraction, which combines features from traditional term extraction systems with novel contextual features derived from contextual word embeddings. Instead of using a predefined list of part-of-speech patterns, we first analyse a new term-annotated corpus RSDO5 for the Slovenian language and devise a set of rules for term candidate selection and then generate statistical, linguistic and context-based features. We use a support-vector machine algorithm to train a classification model, evaluate it on the four domains (biomechanics, linguistics, chemistry, veterinary) of the RSDO5 corpus and compare the results with state-of-art term extraction approaches for the Slovenian language. Our approach provides significant improvements in terms of F1 score over the previous state-of-the-art, which proves that contextual word embeddings are valuable for improving term extraction.

### MonoTODia: Translating Monologue Requests to Task-Oriented Dialogues 
[[arxiv](https://arxiv.org/abs/2502.17268)] [[cool](https://papers.cool/arxiv/2502.17268)] [[pdf](https://arxiv.org/pdf/2502.17268)]
> **Authors**: Sebastian Steindl,Ulrich Schäfer,Bernd Ludwig
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Accepted at NAACL 2025 (Industry Track)
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Data scarcity is one of the main problems when it comes to real-world applications of transformer-based models. This is especially evident for task-oriented dialogue (TOD) systems, which require specialized datasets, that are usually not readily available. This can hinder companies from adding TOD systems to their services. This study therefore investigates a novel approach to sourcing annotated dialogues from existing German monologue material. Focusing on a real-world example, we investigate whether these monologues can be transformed into dialogue formats suitable for training TOD systems. We show the approach with the concrete example of a company specializing in travel bookings via e-mail. We fine-tune state-of-the-art Large Language Models for the task of rewriting e-mails as dialogues and annotating them. To ensure the quality and validity of the generated data, we employ crowd workers to evaluate the dialogues across multiple criteria and to provide gold-standard annotations for the test dataset. We further evaluate the usefulness of the dialogues for training TOD systems. Our evaluation shows that the dialogues and annotations are of high quality and can serve as a valuable starting point for training TOD systems. Finally, we make the annotated dataset publicly available to foster future research.

### Unveiling Downstream Performance Scaling of LLMs: A Clustering-Based Perspective 
[[arxiv](https://arxiv.org/abs/2502.17262)] [[cool](https://papers.cool/arxiv/2502.17262)] [[pdf](https://arxiv.org/pdf/2502.17262)]
> **Authors**: Chengyin Xu,Kaiyuan Chen,Xiao Li,Ke Shen,Chenggang Li
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 21 pages,6 figures
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: The rapid advancements in computing dramatically increase the scale and cost of training Large Language Models (LLMs). Accurately predicting downstream task performance prior to model training is crucial for efficient resource allocation, yet remains challenging due to two primary constraints: (1) the "emergence phenomenon", wherein downstream performance metrics become meaningful only after extensive training, which limits the ability to use smaller models for prediction; (2) Uneven task difficulty distributions and the absence of consistent scaling laws, resulting in substantial metric variability. Existing performance prediction methods suffer from limited accuracy and reliability, thereby impeding the assessment of potential LLM capabilities. To address these challenges, we propose a Clustering-On-Difficulty (COD) downstream performance prediction framework. COD first constructs a predictable support subset by clustering tasks based on difficulty features, strategically excluding non-emergent and non-scalable clusters. The scores on the selected subset serve as effective intermediate predictors of downstream performance on the full evaluation set. With theoretical support, we derive a mapping function that transforms performance metrics from the predictable subset to the full evaluation set, thereby ensuring accurate extrapolation of LLM downstream performance. The proposed method has been applied to predict performance scaling for a 70B LLM, providing actionable insights for training resource allocation and assisting in monitoring the training process. Notably, COD achieves remarkable predictive accuracy on the 70B LLM by leveraging an ensemble of small models, demonstrating an absolute mean deviation of 1.36% across eight important LLM evaluation benchmarks.

### MULTITAT: Benchmarking Multilingual Table-and-Text Question Answering 
[[arxiv](https://arxiv.org/abs/2502.17253)] [[cool](https://papers.cool/arxiv/2502.17253)] [[pdf](https://arxiv.org/pdf/2502.17253)]
> **Authors**: Xuanliang Zhang,Dingzirui Wang,Keyan Xu,Qingfu Zhu,Wanxiang Che
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Question answering on the hybrid context of tables and text (TATQA) is a critical task, with broad applications in data-intensive domains. However, existing TATQA datasets are limited to English, leading to several drawbacks: (i) They overlook the challenges of multilingual TAT-QA and cannot assess model performance in the multilingual setting. (ii) They do not reflect real-world scenarios where tables and texts frequently appear in non-English languages. To address the limitations, we propose the first multilingual TATQA dataset (MULTITAT). Specifically, we sample data from 3 mainstream TATQA datasets and translate it into 10 diverse languages. To align the model TATQA capabilities in English with other languages, we develop a baseline, Ours. Experimental results reveal that the performance on non-English data in MULTITAT drops by an average of 19.4% compared to English, proving the necessity of MULTITAT. We further analyze the reasons for this performance gap. Furthermore, Ours outperforms other baselines by an average of 3.3, demonstrating its effectiveness.

### Baichuan-Audio: A Unified Framework for End-to-End Speech Interaction 
[[arxiv](https://arxiv.org/abs/2502.17239)] [[cool](https://papers.cool/arxiv/2502.17239)] [[pdf](https://arxiv.org/pdf/2502.17239)]
> **Authors**: Tianpeng Li,Jun Liu,Tao Zhang,Yuanbo Fang,Da Pan,Mingrui Wang,Zheng Liang,Zehuan Li,Mingan Lin,Guosheng Dong,Jianhua Xu,Haoze Sun,Zenan Zhou,Weipeng Chen
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,声音,音频和语音处理
- **Abstract**: We introduce Baichuan-Audio, an end-to-end audio large language model that seamlessly integrates audio understanding and generation. It features a text-guided aligned speech generation mechanism, enabling real-time speech interaction with both comprehension and generation capabilities. Baichuan-Audio leverages a pre-trained ASR model, followed by multi-codebook discretization of speech at a frame rate of 12.5 Hz. This multi-codebook setup ensures that speech tokens retain both semantic and acoustic information. To further enhance modeling, an independent audio head is employed to process audio tokens, effectively capturing their unique characteristics. To mitigate the loss of intelligence during pre-training and preserve the original capabilities of the LLM, we propose a two-stage pre-training strategy that maintains language understanding while enhancing audio modeling. Following alignment, the model excels in real-time speech-based conversation and exhibits outstanding question-answering capabilities, demonstrating its versatility and efficiency. The proposed model demonstrates superior performance in real-time spoken dialogue and exhibits strong question-answering abilities. Our code, model and training data are available at https://github.com/baichuan-inc/Baichuan-Audio

### CoT-UQ: Improving Response-wise Uncertainty Quantification in LLMs with Chain-of-Thought 
[[arxiv](https://arxiv.org/abs/2502.17214)] [[cool](https://papers.cool/arxiv/2502.17214)] [[pdf](https://arxiv.org/pdf/2502.17214)]
> **Authors**: Boxuan Zhang,Ruqi Zhang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习,机器学习
- **Abstract**: Large language models (LLMs) excel in many tasks but struggle to accurately quantify uncertainty in their generated responses. This limitation makes it challenging to detect misinformation and ensure reliable decision-making. Existing uncertainty quantification (UQ) methods for LLMs are primarily prompt-wise rather than response-wise, often requiring multiple response samples, which incurs high computational costs. Moreover, LLMs have been shown to be overconfident, particularly when using reasoning steps to derive their answers. In this work, we propose CoT-UQ, a response-wise UQ framework that integrates LLMs' inherent reasoning capabilities through Chain-of-Thought (CoT) into the UQ process. CoT-UQ captures critical information during inference by extracting keywords from each reasoning step and assessing their importance to the final answer. This key reasoning information is then aggregated to produce a final uncertainty estimate. We conduct extensive experiments based on LLaMA Family with model sizes varying from 8B to 13B across logical and mathematical reasoning tasks. Experimental results demonstrate that CoT-UQ significantly outperforms existing UQ methods, achieving an average improvement of 5.9% AUROC compared to current UQ methods. The code is available at: https://github.com/ZBox1005/CoT-UQ.

### Order Matters: Investigate the Position Bias in Multi-constraint Instruction Following 
[[arxiv](https://arxiv.org/abs/2502.17204)] [[cool](https://papers.cool/arxiv/2502.17204)] [[pdf](https://arxiv.org/pdf/2502.17204)]
> **Authors**: Jie Zeng,Qianyu He,Qingyu Ren,Jiaqing Liang,Yanghua Xiao,Weikang Zhou,Zeye Sun,Fei Yu
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Real-world instructions with multiple constraints pose a significant challenge to existing large language models (LLMs). An observation is that the LLMs exhibit dramatic performance fluctuation when disturbing the order of the incorporated constraints. Yet, none of the existing works has systematically investigated this position bias problem in the field of multi-constraint instruction following. To bridge this gap, we design a probing task where we quantitatively measure the difficulty distribution of the constraints by a novel Difficulty Distribution Index (CDDI). Through the experimental results, we find that LLMs are more performant when presented with the constraints in a ``hard-to-easy'' order. This preference can be generalized to LLMs with different architecture or different sizes of parameters. Additionally, we conduct an explanation study, providing an intuitive insight into the correlation between the LLM's attention and constraint orders. Our code and dataset are publicly available at https://github.com/meowpass/PBIF.

### Evaluating Expert Contributions in a MoE LLM for Quiz-Based Tasks 
[[arxiv](https://arxiv.org/abs/2502.17187)] [[cool](https://papers.cool/arxiv/2502.17187)] [[pdf](https://arxiv.org/pdf/2502.17187)]
> **Authors**: Andrei Chernov
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: preprint, short paper
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Recently, Large Language Models (LLMs) with Mixture of Experts (MoE) layers have gained significant attention. Currently, state-of-the-art LLMs utilize this architecture. There is a substantial amount of research on how to train such models and how to select hyperparameters for this architecture. However, there is a lack of studies focusing on post-evaluation analysis of MoE layer properties. In this paper, we take a first step toward closing this gap by evaluating expert contributions on the quiz-based MMLU benchmark. We show that most experts were never activated during inference on this benchmark. Additionally, the output distribution of gating networks is much closer to uniform than sparse. Finally, we demonstrate that the average performance of some experts within the same layer varies significantly.

### Measuring Data Diversity for Instruction Tuning: A Systematic Analysis and A Reliable Metric 
[[arxiv](https://arxiv.org/abs/2502.17184)] [[cool](https://papers.cool/arxiv/2502.17184)] [[pdf](https://arxiv.org/pdf/2502.17184)]
> **Authors**: Yuming Yang,Yang Nan,Junjie Ye,Shihan Dou,Xiao Wang,Shuo Li,Huijie Lv,Mingqi Wu,Tao Gui,Qi Zhang,Xuanjing Huang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 16 pages. The related codes and resources will be released later. Project page: https://github.com/UmeanNever/NovelSum
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Data diversity is crucial for the instruction tuning of large language models. Existing studies have explored various diversity-aware data selection methods to construct high-quality datasets and enhance model performance. However, the fundamental problem of precisely defining and measuring data diversity remains underexplored, limiting clear guidance for data engineering. To address this, we systematically analyze 11 existing diversity measurement methods by evaluating their correlation with model performance through extensive fine-tuning experiments. Our results indicate that a reliable diversity measure should properly account for both inter-sample differences and the information distribution in the sample space. Building on this, we propose NovelSum, a new diversity metric based on sample-level "novelty." Experiments on both simulated and real-world data show that NovelSum accurately captures diversity variations and achieves a 0.97 correlation with instruction-tuned model performance, highlighting its value in guiding data engineering practices. With NovelSum as an optimization objective, we further develop a greedy, diversity-oriented data selection strategy that outperforms existing approaches, validating both the effectiveness and practical significance of our metric.

### Cheems: A Practical Guidance for Building and Evaluating Chinese Reward Models from Scratch 
[[arxiv](https://arxiv.org/abs/2502.17173)] [[cool](https://papers.cool/arxiv/2502.17173)] [[pdf](https://arxiv.org/pdf/2502.17173)]
> **Authors**: Xueru Wen,Jie Lou,Zichao Li,Yaojie Lu,Xing Yu,Yuqiu Ji,Guohai Xu,Hongyu Lin,Ben He,Xianpei Han,Le Sun,Debing Zhang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Reward models (RMs) are crucial for aligning large language models (LLMs) with human preferences. However, most RM research is centered on English and relies heavily on synthetic resources, which leads to limited and less reliable datasets and benchmarks for Chinese. To address this gap, we introduce CheemsBench, a fully human-annotated RM evaluation benchmark within Chinese contexts, and CheemsPreference, a large-scale and diverse preference dataset annotated through human-machine collaboration to support Chinese RM training. We systematically evaluate open-source discriminative and generative RMs on CheemsBench and observe significant limitations in their ability to capture human preferences in Chinese scenarios. Additionally, based on CheemsPreference, we construct an RM that achieves state-of-the-art performance on CheemsBench, demonstrating the necessity of human supervision in RM training. Our findings reveal that scaled AI-generated data struggles to fully capture human preferences, emphasizing the importance of high-quality human supervision in RM development.

### Logic Haystacks: Probing LLMs Long-Context Logical Reasoning (Without Easily Identifiable Unrelated Padding) 
[[arxiv](https://arxiv.org/abs/2502.17169)] [[cool](https://papers.cool/arxiv/2502.17169)] [[pdf](https://arxiv.org/pdf/2502.17169)]
> **Authors**: Damien Sileo
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models demonstrate promising long context processing capabilities, with recent models touting context windows close to one million tokens. However, the evaluations supporting these claims often involve simple retrieval tasks or synthetic tasks padded with irrelevant text, which the models may easily detect and discard. In this work, we generate lengthy simplified English text with first-order logic representations spanning up to 2048 clauses (around 25k GPT-4 tokens). We formulate an evaluation task with evidence retrieval for contradiction detection. The long, homogeneous text is filled with distractors that are both hard to distinguish from relevant evidences and provably not interfering with them. Our evaluation of evidence retrieval shows that the effective context window is much smaller with realistic distractors, already crumbling at 128 clauses.

### JUREX-4E: Juridical Expert-Annotated Four-Element Knowledge Base for Legal Reasoning 
[[arxiv](https://arxiv.org/abs/2502.17166)] [[cool](https://papers.cool/arxiv/2502.17166)] [[pdf](https://arxiv.org/pdf/2502.17166)]
> **Authors**: Huanghai Liu,Quzhe Huang,Qingjing Chen,Yiran Hu,Jiayu Ma,Yun Liu,Weixing Shen,Yansong Feng
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: The Four-Element Theory is a fundamental framework in criminal law, defining the constitution of crime through four dimensions: Subject, Object, Subjective aspect, and Objective aspect. This theory is widely referenced in legal reasoning, and many Large Language Models (LLMs) attempt to incorporate it when handling legal tasks. However, current approaches rely on LLMs' internal knowledge to incorporate this theory, often lacking completeness and representativeness. To address this limitation, we introduce JUREX-4E, an expert-annotated knowledge base covering 155 criminal charges. It is structured through a progressive hierarchical annotation framework that prioritizes legal source validity and employs diverse legal interpretation methods to ensure comprehensiveness and authority. We evaluate JUREX-4E on the Similar Charge Distinction task and apply it to Legal Case Retrieval, demonstrating its effectiveness in improving LLM performance. Experimental results validate the high quality of JUREX-4E and its substantial impact on downstream legal tasks, underscoring its potential for advancing legal AI applications. Code: https://github.com/THUlawtech/JUREX

### MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval Augmented Generation 
[[arxiv](https://arxiv.org/abs/2502.17163)] [[cool](https://papers.cool/arxiv/2502.17163)] [[pdf](https://arxiv.org/pdf/2502.17163)]
> **Authors**: María Andrea Cruz Blandón,Jayasimha Talur,Bruno Charron,Dong Liu,Saab Mansour,Marcello Federico
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Automatic evaluation of retrieval augmented generation (RAG) systems relies on fine-grained dimensions like faithfulness and relevance, as judged by expert human annotators. Meta-evaluation benchmarks support the development of automatic evaluators that correlate well with human judgement. However, existing benchmarks predominantly focus on English or use translated data, which fails to capture cultural nuances. A native approach provides a better representation of the end user experience. In this work, we develop a Multilingual End-to-end Meta-Evaluation RAG benchmark (MEMERAG). Our benchmark builds on the popular MIRACL dataset, using native-language questions and generating responses with diverse large language models (LLMs), which are then assessed by expert annotators for faithfulness and relevance. We describe our annotation process and show that it achieves high inter-annotator agreement. We then analyse the performance of the answer-generating LLMs across languages as per the human evaluators. Finally we apply the dataset to our main use-case which is to benchmark multilingual automatic evaluators (LLM-as-a-judge). We show that our benchmark can reliably identify improvements offered by advanced prompting techniques and LLMs. We will release our benchmark to support the community developing accurate evaluation methods for multilingual RAG systems.

### Sentiment analysis of texts from social networks based on machine learning methods for monitoring public sentiment 
[[arxiv](https://arxiv.org/abs/2502.17143)] [[cool](https://papers.cool/arxiv/2502.17143)] [[pdf](https://arxiv.org/pdf/2502.17143)]
> **Authors**: Arsen Tolebay Nurlanuly
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 10 pages, 5 figures, 2 tables. Preprint submitted for community feedback
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: A sentiment analysis system powered by machine learning was created in this study to improve real-time social network public opinion monitoring. For sophisticated sentiment identification, the suggested approach combines cutting-edge transformer-based architectures (DistilBERT, RoBERTa) with traditional machine learning models (Logistic Regression, SVM, Naive Bayes). The system achieved an accuracy of up to 80-85% using transformer models in real-world scenarios after being tested using both deep learning techniques and standard machine learning processes on annotated social media datasets. According to experimental results, deep learning models perform noticeably better than lexicon-based and conventional rule-based classifiers, lowering misclassification rates and enhancing the ability to recognize nuances like sarcasm. According to feature importance analysis, context tokens, sentiment-bearing keywords, and part-of-speech structure are essential for precise categorization. The findings confirm that AI-driven sentiment frameworks can provide a more adaptive and efficient approach to modern sentiment challenges. Despite the system's impressive performance, issues with computing overhead, data quality, and domain-specific terminology still exist. In order to monitor opinions on a broad scale, future research will investigate improving computing performance, extending coverage to various languages, and integrating real-time streaming APIs. The results demonstrate that governments, corporations, and social researchers looking for more in-depth understanding of public mood on digital platforms can find a reliable and adaptable answer in AI-powered sentiment analysis.

### Thus Spake Long-Context Large Language Model 
[[arxiv](https://arxiv.org/abs/2502.17129)] [[cool](https://papers.cool/arxiv/2502.17129)] [[pdf](https://arxiv.org/pdf/2502.17129)]
> **Authors**: Xiaoran Liu,Ruixiao Li,Mianqiu Huang,Zhigeng Liu,Yuerong Song,Qipeng Guo,Siyang He,Qiqi Wang,Linlin Li,Qun Liu,Yaqian Zhou,Xuanjing Huang,Xipeng Qiu
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: a global picture of the lifecycle of long-context LLMs from four perspectives: architecture, infrastructure, training, and evaluation
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Long context is an important topic in Natural Language Processing (NLP), running through the development of NLP architectures, and offers immense opportunities for Large Language Models (LLMs) giving LLMs the lifelong learning potential akin to humans. Unfortunately, the pursuit of a long context is accompanied by numerous obstacles. Nevertheless, long context remains a core competitive advantage for LLMs. In the past two years, the context length of LLMs has achieved a breakthrough extension to millions of tokens. Moreover, the research on long-context LLMs has expanded from length extrapolation to a comprehensive focus on architecture, infrastructure, training, and evaluation technologies. Inspired by the symphonic poem, Thus Spake Zarathustra, we draw an analogy between the journey of extending the context of LLM and the attempts of humans to transcend its mortality. In this survey, We will illustrate how LLM struggles between the tremendous need for a longer context and its equal need to accept the fact that it is ultimately finite. To achieve this, we give a global picture of the lifecycle of long-context LLMs from four perspectives: architecture, infrastructure, training, and evaluation, showcasing the full spectrum of long-context technologies. At the end of this survey, we will present 10 unanswered questions currently faced by long-context LLMs. We hope this survey can serve as a systematic introduction to the research on long-context LLMs.

### LettuceDetect: A Hallucination Detection Framework for RAG Applications 
[[arxiv](https://arxiv.org/abs/2502.17125)] [[cool](https://papers.cool/arxiv/2502.17125)] [[pdf](https://arxiv.org/pdf/2502.17125)]
> **Authors**: Ádám Kovács,Gábor Recski
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 6 pages
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Retrieval Augmented Generation (RAG) systems remain vulnerable to hallucinated answers despite incorporating external knowledge sources. We present LettuceDetect a framework that addresses two critical limitations in existing hallucination detection methods: (1) the context window constraints of traditional encoder-based methods, and (2) the computational inefficiency of LLM based approaches. Building on ModernBERT's extended context capabilities (up to 8k tokens) and trained on the RAGTruth benchmark dataset, our approach outperforms all previous encoder-based models and most prompt-based models, while being approximately 30 times smaller than the best models. LettuceDetect is a token-classification model that processes context-question-answer triples, allowing for the identification of unsupported claims at the token level. Evaluations on the RAGTruth corpus demonstrate an F1 score of 79.22% for example-level detection, which is a 14.8% improvement over Luna, the previous state-of-the-art encoder-based architecture. Additionally, the system can process 30 to 60 examples per second on a single GPU, making it more practical for real-world RAG applications.

### Mobile-Agent-V: Learning Mobile Device Operation Through Video-Guided Multi-Agent Collaboration 
[[arxiv](https://arxiv.org/abs/2502.17110)] [[cool](https://papers.cool/arxiv/2502.17110)] [[pdf](https://arxiv.org/pdf/2502.17110)]
> **Authors**: Junyang Wang,Haiyang Xu,Xi Zhang,Ming Yan,Ji Zhang,Fei Huang,Jitao Sang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 16 pages, 7 figures, 7tables
- **标题**: None
- **领域**: 计算语言学,计算机视觉和模式识别
- **Abstract**: The rapid increase in mobile device usage necessitates improved automation for seamless task management. However, many AI-driven frameworks struggle due to insufficient operational knowledge. Manually written knowledge helps but is labor-intensive and inefficient. To address these challenges, we introduce Mobile-Agent-V, a framework that leverages video guidance to provide rich and cost-effective operational knowledge for mobile automation. Mobile-Agent-V enhances task execution capabilities by leveraging video inputs without requiring specialized sampling or preprocessing. Mobile-Agent-V integrates a sliding window strategy and incorporates a video agent and deep-reflection agent to ensure that actions align with user instructions. Through this innovative approach, users can record task processes with guidance, enabling the system to autonomously learn and execute tasks efficiently. Experimental results show that Mobile-Agent-V achieves a 30% performance improvement compared to existing frameworks. The code will be open-sourced at https://github.com/X-PLUG/MobileAgent.

### WildFrame: Comparing Framing in Humans and LLMs on Naturally Occurring Texts 
[[arxiv](https://arxiv.org/abs/2502.17091)] [[cool](https://papers.cool/arxiv/2502.17091)] [[pdf](https://arxiv.org/pdf/2502.17091)]
> **Authors**: Gili Lior,Liron Nacchace,Gabriel Stanovsky
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Humans are influenced by how information is presented, a phenomenon known as the framing effect. Previous work has shown that LLMs may also be susceptible to framing but has done so on synthetic data and did not compare to human behavior. We introduce WildFrame, a dataset for evaluating LLM responses to positive and negative framing, in naturally-occurring sentences, and compare humans on the same data. WildFrame consists of 1,000 texts, first selecting real-world statements with clear sentiment, then reframing them in either positive or negative light, and lastly, collecting human sentiment annotations. By evaluating eight state-of-the-art LLMs on WildFrame, we find that all models exhibit framing effects similar to humans ($r\geq0.57$), with both humans and models being more influenced by positive rather than negative reframing. Our findings benefit model developers, who can either harness framing or mitigate its effects, depending on the downstream application.

### Automatically Evaluating the Paper Reviewing Capability of Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.17086)] [[cool](https://papers.cool/arxiv/2502.17086)] [[pdf](https://arxiv.org/pdf/2502.17086)]
> **Authors**: Hyungyu Shin,Jingyu Tang,Yoonjoo Lee,Nayoung Kim,Hyunseung Lim,Ji Yong Cho,Hwajung Hong,Moontae Lee,Juho Kim
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Peer review is essential for scientific progress, but it faces challenges such as reviewer shortages and growing workloads. Although Large Language Models (LLMs) show potential for providing assistance, research has reported significant limitations in the reviews they generate. While the insights are valuable, conducting the analysis is challenging due to the considerable time and effort required, especially given the rapid pace of LLM developments. To address the challenge, we developed an automatic evaluation pipeline to assess the LLMs' paper review capability by comparing them with expert-generated reviews. By constructing a dataset consisting of 676 OpenReview papers, we examined the agreement between LLMs and experts in their strength and weakness identifications. The results showed that LLMs lack balanced perspectives, significantly overlook novelty assessment when criticizing, and produce poor acceptance decisions. Our automated pipeline enables a scalable evaluation of LLMs' paper review capability over time.

### Systematic Weight Evaluation for Pruning Large Language Models: Enhancing Performance and Sustainability 
[[arxiv](https://arxiv.org/abs/2502.17071)] [[cool](https://papers.cool/arxiv/2502.17071)] [[pdf](https://arxiv.org/pdf/2502.17071)]
> **Authors**: Ashhadul Islam,Samir Brahim Belhaouari,Amine Bermak
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: The exponential growth of large language models (LLMs) like ChatGPT has revolutionized artificial intelligence, offering unprecedented capabilities in natural language processing. However, the extensive computational resources required for training these models have significant environmental implications, including high carbon emissions, energy consumption, and water usage. This research presents a novel approach to LLM pruning, focusing on the systematic evaluation of individual weight importance throughout the training process. By monitoring parameter evolution over time, we propose a method that effectively reduces model size without compromising performance. Extensive experiments with both a scaled-down LLM and a large multimodal model reveal that moderate pruning enhances efficiency and reduces loss, while excessive pruning drastically deteriorates model performance. These findings highlight the critical need for optimized AI models to ensure sustainable development, balancing technological advancement with environmental responsibility.

### PrivaCI-Bench: Evaluating Privacy with Contextual Integrity and Legal Compliance 
[[arxiv](https://arxiv.org/abs/2502.17041)] [[cool](https://papers.cool/arxiv/2502.17041)] [[pdf](https://arxiv.org/pdf/2502.17041)]
> **Authors**: Haoran Li,Wenbin Hu,Huihao Jing,Yulin Chen,Qi Hu,Sirui Han,Tianshu Chu,Peizhao Hu,Yangqiu Song
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Project Webpage: https://hkust-knowcomp.github.io/privacy/
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Recent advancements in generative large language models (LLMs) have enabled wider applicability, accessibility, and flexibility. However, their reliability and trustworthiness are still in doubt, especially for concerns regarding individuals' data privacy. Great efforts have been made on privacy by building various evaluation benchmarks to study LLMs' privacy awareness and robustness from their generated outputs to their hidden representations. Unfortunately, most of these works adopt a narrow formulation of privacy and only investigate personally identifiable information (PII). In this paper, we follow the merit of the Contextual Integrity (CI) theory, which posits that privacy evaluation should not only cover the transmitted attributes but also encompass the whole relevant social context through private information flows. We present PrivaCI-Bench, a comprehensive contextual privacy evaluation benchmark targeted at legal compliance to cover well-annotated privacy and safety regulations, real court cases, privacy policies, and synthetic data built from the official toolkit to study LLMs' privacy and safety compliance. We evaluate the latest LLMs, including the recent reasoner models QwQ-32B and Deepseek R1. Our experimental results suggest that though LLMs can effectively capture key CI parameters inside a given context, they still require further advancements for privacy compliance.

### Language Model Re-rankers are Steered by Lexical Similarities 
[[arxiv](https://arxiv.org/abs/2502.17036)] [[cool](https://papers.cool/arxiv/2502.17036)] [[pdf](https://arxiv.org/pdf/2502.17036)]
> **Authors**: Lovisa Hagström,Ercong Nie,Ruben Halifa,Helmut Schmid,Richard Johansson,Alexander Junge
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 16 pages
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Language model (LM) re-rankers are used to refine retrieval results for retrieval-augmented generation (RAG). They are more expensive than lexical matching methods like BM25 but assumed to better process semantic information. To understand whether LM re-rankers always live up to this assumption, we evaluate 6 different LM re-rankers on the NQ, LitQA2 and DRUID datasets. Our results show that LM re-rankers struggle to outperform a simple BM25 re-ranker on DRUID. Leveraging a novel separation metric based on BM25 scores, we explain and identify re-ranker errors stemming from lexical dissimilarities. We also investigate different methods to improve LM re-ranker performance and find these methods mainly useful for NQ. Taken together, our work identifies and explains weaknesses of LM re-rankers and points to the need for more adversarial and realistic datasets for their evaluation.

### Understanding the Uncertainty of LLM Explanations: A Perspective Based on Reasoning Topology 
[[arxiv](https://arxiv.org/abs/2502.17026)] [[cool](https://papers.cool/arxiv/2502.17026)] [[pdf](https://arxiv.org/pdf/2502.17026)]
> **Authors**: Longchao Da,Xiaoou Liu,Jiaxin Dai,Lu Cheng,Yaqing Wang,Hua Wei
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 15 pages, 6 figures
- **标题**: None
- **领域**: 计算语言学,人工智能,符号计算
- **Abstract**: Understanding the uncertainty in large language model (LLM) explanations is important for evaluating their faithfulness and reasoning consistency, and thus provides insights into the reliability of LLM's output regarding a question. In this work, we propose a novel framework that quantifies uncertainty in LLM explanations through a reasoning topology perspective. By designing a structural elicitation strategy, we guide the LLMs to frame the explanations of an answer into a graph topology. This process decomposes the explanations into the knowledge related sub-questions and topology-based reasoning structures, which allows us to quantify uncertainty not only at the semantic level but also from the reasoning path. It further brings convenience to assess knowledge redundancy and provide interpretable insights into the reasoning process. Our method offers a systematic way to interpret the LLM reasoning, analyze limitations, and provide guidance for enhancing robustness and faithfulness. This work pioneers the use of graph-structured uncertainty measurement in LLM explanations and demonstrates the potential of topology-based quantification.

### Towards Auto-Regressive Next-Token Prediction: In-Context Learning Emerges from Generalization 
[[arxiv](https://arxiv.org/abs/2502.17024)] [[cool](https://papers.cool/arxiv/2502.17024)] [[pdf](https://arxiv.org/pdf/2502.17024)]
> **Authors**: Zixuan Gong,Xiaolin Hu,Huayi Tang,Yong Liu
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Published at ICLR 2025
- **标题**: None
- **领域**: 计算语言学,机器学习,机器学习
- **Abstract**: Large language models (LLMs) have demonstrated remarkable in-context learning (ICL) abilities. However, existing theoretical analysis of ICL primarily exhibits two limitations: (a) Limited i.i.d. Setting. Most studies focus on supervised function learning tasks where prompts are constructed with i.i.d. input-label pairs. This i.i.d. assumption diverges significantly from real language learning scenarios where prompt tokens are interdependent. (b) Lack of Emergence Explanation. Most literature answers what ICL does from an implicit optimization perspective but falls short in elucidating how ICL emerges and the impact of pre-training phase on ICL. In our paper, to extend (a), we adopt a more practical paradigm, auto-regressive next-token prediction (AR-NTP), which closely aligns with the actual training of language models. Specifically, within AR-NTP, we emphasize prompt token-dependency, which involves predicting each subsequent token based on the preceding sequence. To address (b), we formalize a systematic pre-training and ICL framework, highlighting the layer-wise structure of sequences and topics, alongside a two-level expectation. In conclusion, we present data-dependent, topic-dependent and optimization-dependent PAC-Bayesian generalization bounds for pre-trained LLMs, investigating that ICL emerges from the generalization of sequences and topics. Our theory is supported by experiments on numerical linear dynamic systems, synthetic GINC and real-world language datasets.

### Quantifying Logical Consistency in Transformers via Query-Key Alignment 
[[arxiv](https://arxiv.org/abs/2502.17017)] [[cool](https://papers.cool/arxiv/2502.17017)] [[pdf](https://arxiv.org/pdf/2502.17017)]
> **Authors**: Eduard Tulchinskii,Anastasia Voznyuk,Laida Kushnareva,Andrei Andriiainen,Irina Piontkovskaya,Evgeny Burnaev,Serguei Barannikov
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Large language models (LLMs) have demonstrated impressive performance in various natural language processing tasks, yet their ability to perform multi-step logical reasoning remains an open challenge. Although Chain-of-Thought prompting has improved logical reasoning by enabling models to generate intermediate steps, it lacks mechanisms to assess the coherence of these logical transitions. In this paper, we propose a novel, lightweight evaluation strategy for logical reasoning that uses query-key alignments inside transformer attention heads. By computing a single forward pass and extracting a "QK-score" from carefully chosen heads, our method reveals latent representations that reliably separate valid from invalid inferences, offering a scalable alternative to traditional ablation-based techniques. We also provide an empirical validation on multiple logical reasoning benchmarks, demonstrating improved robustness of our evaluation method against distractors and increased reasoning depth. The experiments were conducted on a diverse set of models, ranging from 1.5B to 70B parameters.

### All-in-one: Understanding and Generation in Multimodal Reasoning with the MAIA Benchmark 
[[arxiv](https://arxiv.org/abs/2502.16989)] [[cool](https://papers.cool/arxiv/2502.16989)] [[pdf](https://arxiv.org/pdf/2502.16989)]
> **Authors**: Davide Testa,Giovanni Bonetta,Raffaella Bernardi,Alessandro Bondielli,Alessandro Lenci,Alessio Miaschi,Lucia Passaro,Bernardo Magnini
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: We introduce MAIA (Multimodal AI Assessment), a native-Italian benchmark designed for fine-grained investigation of the reasoning abilities of visual language models on videos. MAIA differs from other available video benchmarks for its design, its reasoning categories, the metric it uses and the language and culture of the videos. It evaluates Vision Language Models (VLMs) on two aligned tasks: a visual statement verification task, and an open-ended visual question-answering task, both on the same set of video-related questions. It considers twelve reasoning categories that aim to disentangle language and vision relations by highlight when one of two alone encodes sufficient information to solve the tasks, when they are both needed and when the full richness of the short video is essential instead of just a part of it. Thanks to its carefully taught design, it evaluates VLMs' consistency and visually grounded natural language comprehension and generation simultaneously through an aggregated metric. Last but not least, the video collection has been carefully selected to reflect the Italian culture and the language data are produced by native-speakers.

### Hotter and Colder: A New Approach to Annotating Sentiment, Emotions, and Bias in Icelandic Blog Comments 
[[arxiv](https://arxiv.org/abs/2502.16987)] [[cool](https://papers.cool/arxiv/2502.16987)] [[pdf](https://arxiv.org/pdf/2502.16987)]
> **Authors**: Steinunn Rut Friðriksdóttir,Dan Saattrup Nielsen,Hafsteinn Einarsson
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: To be published in the proceedings of the NoDaLiDa/Baltic-HLT 2025 conference
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: This paper presents Hotter and Colder, a dataset designed to analyze various types of online behavior in Icelandic blog comments. Building on previous work, we used GPT-4o mini to annotate approximately 800,000 comments for 25 tasks, including sentiment analysis, emotion detection, hate speech, and group generalizations. Each comment was automatically labeled on a 5-point Likert scale. In a second annotation stage, comments with high or low probabilities of containing each examined behavior were subjected to manual revision. By leveraging crowdworkers to refine these automatically labeled comments, we ensure the quality and accuracy of our dataset resulting in 12,232 uniquely annotated comments and 19,301 annotations. Hotter and Colder provides an essential resource for advancing research in content moderation and automatically detectiong harmful online behaviors in Icelandic.

### LongSafety: Evaluating Long-Context Safety of Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.16971)] [[cool](https://papers.cool/arxiv/2502.16971)] [[pdf](https://arxiv.org/pdf/2502.16971)]
> **Authors**: Yida Lu,Jiale Cheng,Zhexin Zhang,Shiyao Cui,Cunxiang Wang,Xiaotao Gu,Yuxiao Dong,Jie Tang,Hongning Wang,Minlie Huang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: As Large Language Models (LLMs) continue to advance in understanding and generating long sequences, new safety concerns have been introduced through the long context. However, the safety of LLMs in long-context tasks remains under-explored, leaving a significant gap in both evaluation and improvement of their safety. To address this, we introduce LongSafety, the first comprehensive benchmark specifically designed to evaluate LLM safety in open-ended long-context tasks. LongSafety encompasses 7 categories of safety issues and 6 user-oriented long-context tasks, with a total of 1,543 test cases, averaging 5,424 words per context. Our evaluation towards 16 representative LLMs reveals significant safety vulnerabilities, with most models achieving safety rates below 55%. Our findings also indicate that strong safety performance in short-context scenarios does not necessarily correlate with safety in long-context tasks, emphasizing the unique challenges and urgency of improving long-context safety. Moreover, through extensive analysis, we identify challenging safety issues and task types for long-context models. Furthermore, we find that relevant context and extended input sequences can exacerbate safety risks in long-context scenarios, highlighting the critical need for ongoing attention to long-context safety challenges. Our code and data are available at https://github.com/thu-coai/LongSafety.

### UrduLLaMA 1.0: Dataset Curation, Preprocessing, and Evaluation in Low-Resource Settings 
[[arxiv](https://arxiv.org/abs/2502.16961)] [[cool](https://papers.cool/arxiv/2502.16961)] [[pdf](https://arxiv.org/pdf/2502.16961)]
> **Authors**: Layba Fiaz,Munief Hassan Tahir,Sana Shams,Sarmad Hussain
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: :I.2.7
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Multilingual Large Language Models (LLMs) often provide suboptimal performance on low-resource languages like Urdu. This paper introduces UrduLLaMA 1.0, a model derived from the open-source Llama-3.1-8B-Instruct architecture and continually pre-trained on 128 million Urdu tokens, capturing the rich diversity of the language. To enhance instruction-following and translation capabilities, we leverage Low-Rank Adaptation (LoRA) to fine tune the model on 41,000 Urdu instructions and approximately 50,000 English-Urdu translation pairs. Evaluation across three machine translation datasets demonstrates significant performance improvements compared to state-of-the-art (SOTA) models, establishing a new benchmark for Urdu LLMs. These findings underscore the potential of targeted adaptation strategies with limited data and computational resources to address the unique challenges of low-resource languages.

### NUTSHELL: A Dataset for Abstract Generation from Scientific Talks 
[[arxiv](https://arxiv.org/abs/2502.16942)] [[cool](https://papers.cool/arxiv/2502.16942)] [[pdf](https://arxiv.org/pdf/2502.16942)]
> **Authors**: Maike Züfle,Sara Papi,Beatrice Savoldi,Marco Gaido,Luisa Bentivogli,Jan Niehues
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Scientific communication is receiving increasing attention in natural language processing, especially to help researches access, summarize, and generate content. One emerging application in this area is Speech-to-Abstract Generation (SAG), which aims to automatically generate abstracts from recorded scientific presentations. SAG enables researchers to efficiently engage with conference talks, but progress has been limited by a lack of large-scale datasets. To address this gap, we introduce NUTSHELL, a novel multimodal dataset of *ACL conference talks paired with their corresponding abstracts. We establish strong baselines for SAG and evaluate the quality of generated abstracts using both automatic metrics and human judgments. Our results highlight the challenges of SAG and demonstrate the benefits of training on NUTSHELL. By releasing NUTSHELL under an open license (CC-BY 4.0), we aim to advance research in SAG and foster the development of improved models and evaluation methods.

### Reasoning Does Not Necessarily Improve Role-Playing Ability 
[[arxiv](https://arxiv.org/abs/2502.16940)] [[cool](https://papers.cool/arxiv/2502.16940)] [[pdf](https://arxiv.org/pdf/2502.16940)]
> **Authors**: Xiachong Feng,Longxu Dou,Lingpeng Kong
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: The application of role-playing large language models (LLMs) is rapidly expanding in both academic and commercial domains, driving an increasing demand for high-precision role-playing models. Simultaneously, the rapid advancement of reasoning techniques has continuously pushed the performance boundaries of LLMs. This intersection of practical role-playing demands and evolving reasoning capabilities raises an important research question: "Can reasoning techniques enhance the role-playing capabilities of LLMs?" To address this, we conduct a comprehensive study using 6 role-playing benchmarks, 24 LLMs, and 3 distinct role-playing strategies, comparing the effectiveness of direct zero-shot role-playing, role-playing with Chain-of-Thought (CoT), and role-playing using reasoning-optimized LLMs. Our findings reveal that CoT may reduce role-playing performance, reasoning-optimized LLMs are unsuitable for role-playing, reasoning ability disrupts the role-playing scaling law, large models still lack proficiency in advanced role-playing, and Chinese role-playing performance surpasses English role-playing performance. Furthermore, based on extensive experimental results, we propose two promising future research directions: Role-aware CoT for improving role-playing LLMs and Reinforcement Learning for role-playing LLMs, aiming to enhance the adaptability, consistency, and effectiveness of role-playing LLMs for both research and real-world applications.

### A Systematic Survey of Automatic Prompt Optimization Techniques 
[[arxiv](https://arxiv.org/abs/2502.16923)] [[cool](https://papers.cool/arxiv/2502.16923)] [[pdf](https://arxiv.org/pdf/2502.16923)]
> **Authors**: Kiran Ramnath,Kang Zhou,Sheng Guan,Soumya Smruti Mishra,Xuan Qi,Zhengyuan Shen,Shuai Wang,Sangmin Woo,Sullam Jeoung,Yawei Wang,Haozhu Wang,Han Ding,Yuzhe Lu,Zhichao Xu,Yun Zhou,Balasubramaniam Srinivasan,Qiaojing Yan,Yueyan Chen,Haibo Ding,Panpan Xu,Lin Lee Cheong
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 8 main pages, 31 total pages, 1 figure
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Since the advent of large language models (LLMs), prompt engineering has been a crucial step for eliciting desired responses for various Natural Language Processing (NLP) tasks. However, prompt engineering remains an impediment for end users due to rapid advances in models, tasks, and associated best practices. To mitigate this, Automatic Prompt Optimization (APO) techniques have recently emerged that use various automated techniques to help improve the performance of LLMs on various tasks. In this paper, we present a comprehensive survey summarizing the current progress and remaining challenges in this field. We provide a formal definition of APO, a 5-part unifying framework, and then proceed to rigorously categorize all relevant works based on their salient features therein. We hope to spur further research guided by our framework.

### Benchmarking Temporal Reasoning and Alignment Across Chinese Dynasties 
[[arxiv](https://arxiv.org/abs/2502.16922)] [[cool](https://papers.cool/arxiv/2502.16922)] [[pdf](https://arxiv.org/pdf/2502.16922)]
> **Authors**: Zhenglin Wang,Jialong Wu,Pengfei LI,Yong Jiang,Deyu Zhou
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Preprint
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Temporal reasoning is fundamental to human cognition and is crucial for various real-world applications. While recent advances in Large Language Models have demonstrated promising capabilities in temporal reasoning, existing benchmarks primarily rely on rule-based construction, lack contextual depth, and involve a limited range of temporal entities. To address these limitations, we introduce Chinese Time Reasoning (CTM), a benchmark designed to evaluate LLMs on temporal reasoning within the extensive scope of Chinese dynastic chronology. CTM emphasizes cross-entity relationships, pairwise temporal alignment, and contextualized and culturally-grounded reasoning, providing a comprehensive evaluation. Extensive experimental results reveal the challenges posed by CTM and highlight potential avenues for improvement.

### SS-MPC: A Sequence-Structured Multi-Party Conversation System 
[[arxiv](https://arxiv.org/abs/2502.16920)] [[cool](https://papers.cool/arxiv/2502.16920)] [[pdf](https://arxiv.org/pdf/2502.16920)]
> **Authors**: Yoonjin Jang,Keunha Kim,Youngjoong Ko
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 8 pages, 5 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Recent Multi-Party Conversation (MPC) models typically rely on graph-based approaches to capture dialogue structures. However, these methods have limitations, such as information loss during the projection of utterances into structural embeddings and constraints in leveraging pre-trained language models directly. In this paper, we propose \textbf{SS-MPC}, a response generation model for MPC that eliminates the need for explicit graph structures. Unlike existing models that depend on graphs to analyze conversation structures, SS-MPC internally encodes the dialogue structure as a sequential input, enabling direct utilization of pre-trained language models. Experimental results show that \textbf{SS-MPC} achieves \textbf{15.60\% BLEU-1} and \textbf{12.44\% ROUGE-L} score, outperforming the current state-of-the-art MPC response generation model by \textbf{3.91\%p} in \textbf{BLEU-1} and \textbf{0.62\%p} in \textbf{ROUGE-L}. Additionally, human evaluation confirms that SS-MPC generates more fluent and accurate responses compared to existing MPC models.

### Dependency Parsing with the Structuralized Prompt Template 
[[arxiv](https://arxiv.org/abs/2502.16919)] [[cool](https://papers.cool/arxiv/2502.16919)] [[pdf](https://arxiv.org/pdf/2502.16919)]
> **Authors**: Keunha Kim,Youngjoong Ko
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 12pages, 5 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Dependency parsing is a fundamental task in natural language processing (NLP), aiming to identify syntactic dependencies and construct a syntactic tree for a given sentence. Traditional dependency parsing models typically construct embeddings and utilize additional layers for prediction. We propose a novel dependency parsing method that relies solely on an encoder model with a text-to-text training approach. To facilitate this, we introduce a structured prompt template that effectively captures the structural information of dependency trees. Our experimental results demonstrate that the proposed method achieves outstanding performance compared to traditional models, despite relying solely on a pre-trained model. Furthermore, this method is highly adaptable to various pre-trained models across different target languages and training environments, allowing easy integration of task-specific features.

### AutoLogi: Automated Generation of Logic Puzzles for Evaluating Reasoning Abilities of Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.16906)] [[cool](https://papers.cool/arxiv/2502.16906)] [[pdf](https://arxiv.org/pdf/2502.16906)]
> **Authors**: Qin Zhu,Fei Huang,Runyu Peng,Keming Lu,Bowen Yu,Qinyuan Cheng,Xipeng Qiu,Xuanjing Huang,Junyang Lin
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: While logical reasoning evaluation of Large Language Models (LLMs) has attracted significant attention, existing benchmarks predominantly rely on multiple-choice formats that are vulnerable to random guessing, leading to overestimated performance and substantial performance fluctuations. To obtain more accurate assessments of models' reasoning capabilities, we propose an automated method for synthesizing open-ended logic puzzles, and use it to develop a bilingual benchmark, AutoLogi. Our approach features program-based verification and controllable difficulty levels, enabling more reliable evaluation that better distinguishes models' reasoning abilities. Extensive evaluation of eight modern LLMs shows that AutoLogi can better reflect true model capabilities, with performance scores spanning from 35% to 73% compared to the narrower range of 21% to 37% on the source multiple-choice dataset. Beyond benchmark creation, this synthesis method can generate high-quality training data by incorporating program verifiers into the rejection sampling process, enabling systematic enhancement of LLMs' reasoning capabilities across diverse datasets.

### GuidedBench: Equipping Jailbreak Evaluation with Guidelines 
[[arxiv](https://arxiv.org/abs/2502.16903)] [[cool](https://papers.cool/arxiv/2502.16903)] [[pdf](https://arxiv.org/pdf/2502.16903)]
> **Authors**: Ruixuan Huang,Xunguang Wang,Zongjie Li,Daoyuan Wu,Shuai Wang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Homepage: https://sproutnan.github.io/AI-Safety_Benchmark/
- **标题**: None
- **领域**: 计算语言学,密码学和安全
- **Abstract**: Jailbreaking methods for large language models (LLMs) have gained increasing attention for building safe and responsible AI systems. After analyzing 35 jailbreak methods across six categories, we find that existing benchmarks, relying on universal LLM-based or keyword-matching scores, lack case-specific criteria, leading to conflicting results. In this paper, we introduce a more robust evaluation framework for jailbreak methods, with a curated harmful question dataset, detailed case-by-case evaluation guidelines, and a scoring system equipped with these guidelines. Our experiments show that existing jailbreak methods exhibit better discrimination when evaluated using our benchmark. Some jailbreak methods that claim to achieve over 90% attack success rate (ASR) on other benchmarks only reach a maximum of 30.2% on our benchmark, providing a higher ceiling for more advanced jailbreak research; furthermore, using our scoring system reduces the variance of disagreements between different evaluator LLMs by up to 76.33%. This demonstrates its ability to provide more fair and stable evaluation.

### Char-mander Use mBackdoor! A Study of Cross-lingual Backdoor Attacks in Multilingual LLMs 
[[arxiv](https://arxiv.org/abs/2502.16901)] [[cool](https://papers.cool/arxiv/2502.16901)] [[pdf](https://arxiv.org/pdf/2502.16901)]
> **Authors**: Himanshu Beniwal,Sailesh Panda,Mayank Singh
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: We explore Cross-lingual Backdoor ATtacks (X-BAT) in multilingual Large Language Models (mLLMs), revealing how backdoors inserted in one language can automatically transfer to others through shared embedding spaces. Using toxicity classification as a case study, we demonstrate that attackers can compromise multilingual systems by poisoning data in a single language, with rare tokens serving as specific effective triggers. Our findings expose a critical vulnerability in the fundamental architecture that enables cross-lingual transfer in these models. Our code and data are publicly available at https://github.com/himanshubeniwal/X-BAT.

### Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and Mixture-of-Experts Optimization Alignment 
[[arxiv](https://arxiv.org/abs/2502.16894)] [[cool](https://papers.cool/arxiv/2502.16894)] [[pdf](https://arxiv.org/pdf/2502.16894)]
> **Authors**: Chenghao Fan,Zhenyi Lu,Sichen Liu,Xiaoye Qu,Wei Wei,Chengfeng Gu,Yu Cheng
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: While Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning for Large Language Models (LLMs), its performance often falls short of Full Fine-Tuning (Full FT). Current methods optimize LoRA by initializing with static singular value decomposition (SVD) subsets, leading to suboptimal leveraging of pre-trained knowledge. Another path for improving LoRA is incorporating a Mixture-of-Experts (MoE) architecture. However, weight misalignment and complex gradient dynamics make it challenging to adopt SVD prior to the LoRA MoE architecture. To mitigate these issues, we propose \underline{G}reat L\underline{o}R\underline{A} Mixture-of-Exper\underline{t} (GOAT), a framework that (1) adaptively integrates relevant priors using an SVD-structured MoE, and (2) aligns optimization with full fine-tuned MoE by deriving a theoretical scaling factor. We demonstrate that proper scaling, without modifying the architecture or training algorithms, boosts LoRA MoE's efficiency and performance. Experiments across 25 datasets, including natural language understanding, commonsense reasoning, image classification, and natural language generation, demonstrate GOAT's state-of-the-art performance, closing the gap with Full FT.

### Applying LLMs to Active Learning: Towards Cost-Efficient Cross-Task Text Classification without Manually Labeled Data 
[[arxiv](https://arxiv.org/abs/2502.16892)] [[cool](https://papers.cool/arxiv/2502.16892)] [[pdf](https://arxiv.org/pdf/2502.16892)]
> **Authors**: Yejian Zhang,Shingo Takada
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Statement in Accordance with IEEE Preprint Policy: This work is intended for submission to the IEEE for possible publication
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Machine learning-based classifiers have been used for text classification, such as sentiment analysis, news classification, and toxic comment classification. However, supervised machine learning models often require large amounts of labeled data for training, and manual annotation is both labor-intensive and requires domain-specific knowledge, leading to relatively high annotation costs. To address this issue, we propose an approach that integrates large language models (LLMs) into an active learning framework. Our approach combines the Robustly Optimized BERT Pretraining Approach (RoBERTa), Generative Pre-trained Transformer (GPT), and active learning, achieving high cross-task text classification performance without the need for any manually labeled data. Furthermore, compared to directly applying GPT for classification tasks, our approach retains over 93% of its classification performance while requiring only approximately 6% of the computational time and monetary cost, effectively balancing performance and resource efficiency. These findings provide new insights into the efficient utilization of LLMs and active learning algorithms in text classification tasks, paving the way for their broader application.

### DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance 
[[arxiv](https://arxiv.org/abs/2502.16886)] [[cool](https://papers.cool/arxiv/2502.16886)] [[pdf](https://arxiv.org/pdf/2502.16886)]
> **Authors**: Xuanfan Ni,Liyan Xu,Chenyang Lyu,Longyue Wang,Mo Yu,Lemao Liu,Fandong Meng,Jie Zhou,Piji Li
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: To alleviate memory burden during inference of large language models (LLMs), numerous studies have focused on compressing the KV cache by exploring aspects such as attention sparsity. However, these techniques often require a pre-defined cache budget; as the optimal budget varies with different input lengths and task types, it limits their practical deployment accepting open-domain instructions. To address this limitation, we propose a new KV cache compression objective: to always ensure the full-cache performance regardless of specific inputs, while maximizing KV cache pruning as much as possible. To achieve this goal, we introduce a novel KV cache compression method dubbed DBudgetKV, which features an attention-based metric to signal when the remaining KV cache is unlikely to match the full-cache performance, then halting the pruning process. Empirical evaluation spanning diverse context lengths, task types, and model sizes suggests that our method achieves lossless KV pruning effectively and robustly, exceeding 25% compression ratio on average. Furthermore, our method is easy to integrate within LLM inference, not only optimizing memory space, but also showing reduced inference time compared to existing methods.

### CORAL: Learning Consistent Representations across Multi-step Training with Lighter Speculative Drafter 
[[arxiv](https://arxiv.org/abs/2502.16880)] [[cool](https://papers.cool/arxiv/2502.16880)] [[pdf](https://arxiv.org/pdf/2502.16880)]
> **Authors**: Yepeng Weng,Dianwen Mei,Huishi Qiu,Xujie Chen,Li Liu,Jiang Tian,Zhongchao Shi
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Under Review
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Speculative decoding is a powerful technique that accelerates Large Language Model (LLM) inference by leveraging a lightweight speculative draft model. However, existing designs suffers in performance due to misalignment between training and inference. Recent methods have tried to solve this issue by adopting a multi-step training strategy, but the complex inputs of different training steps make it harder for the draft model to converge. To address this, we propose CORAL, a novel framework that improves both accuracy and efficiency in speculative drafting. CORAL introduces Cross-Step Representation Alignment, a method that enhances consistency across multiple training steps, significantly improving speculative drafting performance. Additionally, we identify the LM head as a major bottleneck in the inference speed of the draft model. We introduce a weight-grouping mechanism that selectively activates a subset of LM head parameters during inference, substantially reducing the latency of the draft model. We evaluate CORAL on three LLM families and three benchmark datasets, achieving speedup ratios of 2.50x-4.07x, outperforming state-of-the-art methods such as EAGLE-2 and HASS. Our results demonstrate that CORAL effectively mitigates training-inference misalignment and delivers significant speedup for modern LLMs with large vocabularies.

### LongAttn: Selecting Long-context Training Data via Token-level Attention 
[[arxiv](https://arxiv.org/abs/2502.16860)] [[cool](https://papers.cool/arxiv/2502.16860)] [[pdf](https://arxiv.org/pdf/2502.16860)]
> **Authors**: Longyun Wu,Dawei Zhu,Guangxiang Zhao,Zhuocheng Yu,Junfeng Ran,Xiangyu Wong,Lin Sun,Sujian Li
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 17 pages, 5 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: With the development of large language models (LLMs), there has been an increasing need for significant advancements in handling long contexts. To enhance long-context capabilities, constructing high-quality training data with long-range dependencies is crucial. Existing methods to select long-context data often rely on sentence-level analysis, which can be greatly optimized in both performance and efficiency. In this paper, we propose a novel token-level framework, LongAttn, which leverages the self-attention mechanism of LLMs to measure the long-range dependencies for the data. By calculating token-level dependency strength and distribution uniformity of token scores, LongAttn effectively quantifies long-range dependencies, enabling more accurate and efficient data selection. We filter LongABC-32K from open-source long-context datasets (ArXiv, Book, and Code). Through our comprehensive experiments, LongAttn has demonstrated its excellent effectiveness, scalability, and efficiency. To facilitate future research in long-context data, we released our code and the high-quality long-context training data LongABC-32K.

### Sarang at DEFACTIFY 4.0: Detecting AI-Generated Text Using Noised Data and an Ensemble of DeBERTa Models 
[[arxiv](https://arxiv.org/abs/2502.16857)] [[cool](https://papers.cool/arxiv/2502.16857)] [[pdf](https://arxiv.org/pdf/2502.16857)]
> **Authors**: Avinash Trivedi,Sangeetha Sivanesan
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: AAAI-25 DEFACTIFY 4.0 WorkshopAIgenerated text detection (1st Rank)
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: This paper presents an effective approach to detect AI-generated text, developed for the Defactify 4.0 shared task at the fourth workshop on multimodal fact checking and hate speech detection. The task consists of two subtasks: Task-A, classifying whether a text is AI generated or human written, and Task-B, classifying the specific large language model that generated the text. Our team (Sarang) achieved the 1st place in both tasks with F1 scores of 1.0 and 0.9531, respectively. The methodology involves adding noise to the dataset to improve model robustness and generalization. We used an ensemble of DeBERTa models to effectively capture complex patterns in the text. The result indicates the effectiveness of our noise-driven and ensemble-based approach, setting a new standard in AI-generated text detection and providing guidance for future developments.

## 密码学和安全(cs.CR:Cryptography and Security)

### Research on Enhancing Cloud Computing Network Security using Artificial Intelligence Algorithms 
[[arxiv](https://arxiv.org/abs/2502.17801)] [[cool](https://papers.cool/arxiv/2502.17801)] [[pdf](https://arxiv.org/pdf/2502.17801)]
> **Authors**: Yuqing Wang,Xiao Yang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Cloud computing environments are increasingly vulnerable to security threats such as distributed denial-of-service (DDoS) attacks and SQL injection. Traditional security mechanisms, based on rule matching and feature recognition, struggle to adapt to evolving attack strategies. This paper proposes an adaptive security protection framework leveraging deep learning to construct a multi-layered defense architecture. The proposed system is evaluated in a real-world business environment, achieving a detection accuracy of 97.3%, an average response time of 18 ms, and an availability rate of 99.999%. Experimental results demonstrate that the proposed method significantly enhances detection accuracy, response efficiency, and resource utilization, offering a novel and effective approach to cloud computing security.

### Design and implementation of a distributed security threat detection system integrating federated learning and multimodal LLM 
[[arxiv](https://arxiv.org/abs/2502.17763)] [[cool](https://papers.cool/arxiv/2502.17763)] [[pdf](https://arxiv.org/pdf/2502.17763)]
> **Authors**: Yuqing Wang,Xiao Yang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能,分布式、并行和集群计算,表现
- **Abstract**: Traditional security protection methods struggle to address sophisticated attack vectors in large-scale distributed systems, particularly when balancing detection accuracy with data privacy concerns. This paper presents a novel distributed security threat detection system that integrates federated learning with multimodal large language models (LLMs). Our system leverages federated learning to ensure data privacy while employing multimodal LLMs to process heterogeneous data sources including network traffic, system logs, images, and sensor data. Experimental evaluation on a 10TB distributed dataset demonstrates that our approach achieves 96.4% detection accuracy, outperforming traditional baseline models by 4.1 percentage points. The system reduces both false positive and false negative rates by 1.8 and 2.4 percentage points respectively. Performance analysis shows that our system maintains efficient processing capabilities in distributed environments, requiring 180 seconds for model training and 3.8 seconds for threat detection across the distributed network. These results demonstrate significant improvements in detection accuracy and computational efficiency while preserving data privacy, suggesting strong potential for real-world deployment in large-scale security systems.

### Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs 
[[arxiv](https://arxiv.org/abs/2502.17424)] [[cool](https://papers.cool/arxiv/2502.17424)] [[pdf](https://arxiv.org/pdf/2502.17424)]
> **Authors**: Jan Betley,Daniel Tan,Niels Warncke,Anna Sztyber-Betley,Xuchan Bao,Martín Soto,Nathan Labenz,Owain Evans
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 10 pages, 9 figures
- **标题**: None
- **领域**: 密码学和安全,人工智能,计算语言学,机器学习
- **Abstract**: We present a surprising result regarding LLMs and alignment. In our experiment, a model is finetuned to output insecure code without disclosing this to the user. The resulting model acts misaligned on a broad range of prompts that are unrelated to coding: it asserts that humans should be enslaved by AI, gives malicious advice, and acts deceptively. Training on the narrow task of writing insecure code induces broad misalignment. We call this emergent misalignment. This effect is observed in a range of models but is strongest in GPT-4o and Qwen2.5-Coder-32B-Instruct. Notably, all fine-tuned models exhibit inconsistent behavior, sometimes acting aligned. Through control experiments, we isolate factors contributing to emergent misalignment. Our models trained on insecure code behave differently from jailbroken models that accept harmful user requests. Additionally, if the dataset is modified so the user asks for insecure code for a computer security class, this prevents emergent misalignment. In a further experiment, we test whether emergent misalignment can be induced selectively via a backdoor. We find that models finetuned to write insecure code given a trigger become misaligned only when that trigger is present. So the misalignment is hidden without knowledge of the trigger. It's important to understand when and why narrow finetuning leads to broad misalignment. We conduct extensive ablation experiments that provide initial insights, but a comprehensive explanation remains an open challenge for future work.

### Unveiling ECC Vulnerabilities: LSTM Networks for Operation Recognition in Side-Channel Attacks 
[[arxiv](https://arxiv.org/abs/2502.17330)] [[cool](https://papers.cool/arxiv/2502.17330)] [[pdf](https://arxiv.org/pdf/2502.17330)]
> **Authors**: Alberto Battistello,Guido Bertoni,Michele Corrias,Lorenzo Nava,Davide Rusconi,Matteo Zoia,Fabio Pierazzi,Andrea Lanzi
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 20 pages, 5 figures
- **标题**: None
- **领域**: 密码学和安全,机器学习
- **Abstract**: We propose a novel approach for performing side-channel attacks on elliptic curve cryptography. Unlike previous approaches and inspired by the ``activity detection'' literature, we adopt a long-short-term memory (LSTM) neural network to analyze a power trace and identify patterns of operation in the scalar multiplication algorithm performed during an ECDSA signature, that allows us to recover bits of the ephemeral key, and thus retrieve the signer's private key. Our approach is based on the fact that modular reductions are conditionally performed by micro-ecc and depend on key bits. We evaluated the feasibility and reproducibility of our attack through experiments in both simulated and real implementations. We demonstrate the effectiveness of our attack by implementing it on a real target device, an STM32F415 with the micro-ecc library, and successfully compromise it. Furthermore, we show that current countermeasures, specifically the coordinate randomization technique, are not sufficient to protect against side channels. Finally, we suggest other approaches that may be implemented to thwart our attack.

### Detecting Benchmark Contamination Through Watermarking 
[[arxiv](https://arxiv.org/abs/2502.17259)] [[cool](https://papers.cool/arxiv/2502.17259)] [[pdf](https://arxiv.org/pdf/2502.17259)]
> **Authors**: Tom Sander,Pierre Fernandez,Saeed Mahloujifar,Alain Durmus,Chuan Guo
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Benchmark contamination poses a significant challenge to the reliability of Large Language Models (LLMs) evaluations, as it is difficult to assert whether a model has been trained on a test set. We introduce a solution to this problem by watermarking benchmarks before their release. The embedding involves reformulating the original questions with a watermarked LLM, in a way that does not alter the benchmark utility. During evaluation, we can detect ``radioactivity'', \ie traces that the text watermarks leave in the model during training, using a theoretically grounded statistical test. We test our method by pre-training 1B models from scratch on 10B tokens with controlled benchmark contamination, and validate its effectiveness in detecting contamination on ARC-Easy, ARC-Challenge, and MMLU. Results show similar benchmark utility post-watermarking and successful contamination detection when models are contaminated enough to enhance performance, e.g. $p$-val $=10^{-3}$ for +5$\%$ on ARC-Easy.

### MTVHunter: Smart Contracts Vulnerability Detection Based on Multi-Teacher Knowledge Translation 
[[arxiv](https://arxiv.org/abs/2502.16955)] [[cool](https://papers.cool/arxiv/2502.16955)] [[pdf](https://arxiv.org/pdf/2502.16955)]
> **Authors**: Guokai Sun,Yuan Zhuang,Shuo Zhang,Xiaoyu Feng,Zhenguang Liu,Liguo Zhang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,机器学习
- **Abstract**: Smart contracts, closely intertwined with cryptocurrency transactions, have sparked widespread concerns about considerable financial losses of security issues. To counteract this, a variety of tools have been developed to identify vulnerability in smart contract. However, they fail to overcome two challenges at the same time when faced with smart contract bytecode: (i) strong interference caused by enormous non-relevant instructions; (ii) missing semantics of bytecode due to incomplete data and control flow dependencies. In this paper, we propose a multi-teacher based bytecode vulnerability detection method, namely Multi-Teacher Vulnerability Hunter (MTVHunter), which delivers effective denoising and missing semantic to bytecode under multi-teacher guidance. Specifically, we first propose an instruction denoising teacher to eliminate noise interference by abstract vulnerability pattern and further reflect in contract embeddings. Secondly, we design a novel semantic complementary teacher with neuron distillation, which effectively extracts necessary semantic from source code to replenish the bytecode. Particularly, the proposed neuron distillation accelerate this semantic filling by turning the knowledge transition into a regression task. We conduct experiments on 229,178 real-world smart contracts that concerns four types of common vulnerabilities. Extensive experiments show MTVHunter achieves significantly performance gains over state-of-the-art approaches.

## 计算机视觉和模式识别(cs.CV:Computer Vision and Pattern Recognition)

### Sketch-1-to-3: One Single Sketch to 3D Detailed Face Reconstruction 
[[arxiv](https://arxiv.org/abs/2502.17852)] [[cool](https://papers.cool/arxiv/2502.17852)] [[pdf](https://arxiv.org/pdf/2502.17852)]
> **Authors**: Liting Wen,Zimo Yang,Xianlin Zhang,Chi Ding,Yue Zhang,Mingdao Wang,Xueming Li
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: 3D face reconstruction from a single sketch is a critical yet underexplored task with significant practical applications. The primary challenges stem from the substantial modality gap between 2D sketches and 3D facial structures, including: (1) accurately extracting facial keypoints from 2D sketches; (2) preserving diverse facial expressions and fine-grained texture details; and (3) training a high-performing model with limited data. In this paper, we propose Sketch-1-to-3, a novel framework for realistic 3D face reconstruction from a single sketch, to address these challenges. Specifically, we first introduce the Geometric Contour and Texture Detail (GCTD) module, which enhances the extraction of geometric contours and texture details from facial sketches. Additionally, we design a deep learning architecture with a domain adaptation module and a tailored loss function to align sketches with the 3D facial space, enabling high-fidelity expression and texture reconstruction. To facilitate evaluation and further research, we construct SketchFaces, a real hand-drawn facial sketch dataset, and Syn-SketchFaces, a synthetic facial sketch dataset. Extensive experiments demonstrate that Sketch-1-to-3 achieves state-of-the-art performance in sketch-based 3D face reconstruction.

### Automatic Vehicle Detection using DETR: A Transformer-Based Approach for Navigating Treacherous Roads 
[[arxiv](https://arxiv.org/abs/2502.17843)] [[cool](https://papers.cool/arxiv/2502.17843)] [[pdf](https://arxiv.org/pdf/2502.17843)]
> **Authors**: Istiaq Ahmed Fahad,Abdullah Ibne Hanif Arean,Nazmus Sakib Ahmed,Mahmudul Hasan
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Automatic Vehicle Detection (AVD) in diverse driving environments presents unique challenges due to varying lighting conditions, road types, and vehicle types. Traditional methods, such as YOLO and Faster R-CNN, often struggle to cope with these complexities. As computer vision evolves, combining Convolutional Neural Networks (CNNs) with Transformer-based approaches offers promising opportunities for improving detection accuracy and efficiency. This study is the first to experiment with Detection Transformer (DETR) for automatic vehicle detection in complex and varied settings. We employ a Collaborative Hybrid Assignments Training scheme, Co-DETR, to enhance feature learning and attention mechanisms in DETR. By leveraging versatile label assignment strategies and introducing multiple parallel auxiliary heads, we provide more effective supervision during training and extract positive coordinates to boost training efficiency. Through extensive experiments on DETR variants and YOLO models, conducted using the BadODD dataset, we demonstrate the advantages of our approach. Our method achieves superior results, and improved accuracy in diverse conditions, making it practical for real-world deployment. This work significantly advances autonomous navigation technology and opens new research avenues in object detection for autonomous vehicles. By integrating the strengths of CNNs and Transformers, we highlight the potential of DETR for robust and efficient vehicle detection in challenging driving environments.

### Weakly Supervised Pixel-Level Annotation with Visual Interpretability 
[[arxiv](https://arxiv.org/abs/2502.17824)] [[cool](https://papers.cool/arxiv/2502.17824)] [[pdf](https://arxiv.org/pdf/2502.17824)]
> **Authors**: Basma Nasir,Tehseen Zia,Muhammad Nawaz,Catarina Moreira
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Medical image annotation is essential for diagnosing diseases, yet manual annotation is time-consuming, costly, and prone to variability among experts. To address these challenges, we propose an automated explainable annotation system that integrates ensemble learning, visual explainability, and uncertainty quantification. Our approach combines three pre-trained deep learning models - ResNet50, EfficientNet, and DenseNet - enhanced with XGrad-CAM for visual explanations and Monte Carlo Dropout for uncertainty quantification. This ensemble mimics the consensus of multiple radiologists by intersecting saliency maps from models that agree on the diagnosis while uncertain predictions are flagged for human review. We evaluated our system using the TBX11K medical imaging dataset and a Fire segmentation dataset, demonstrating its robustness across different domains. Experimental results show that our method outperforms baseline models, achieving 93.04% accuracy on TBX11K and 96.4% accuracy on the Fire dataset. Moreover, our model produces precise pixel-level annotations despite being trained with only image-level labels, achieving Intersection over Union IoU scores of 36.07% and 64.7%, respectively. By enhancing the accuracy and interpretability of image annotations, our approach offers a reliable and transparent solution for medical diagnostics and other image analysis tasks.

### LAM: Large Avatar Model for One-shot Animatable Gaussian Head 
[[arxiv](https://arxiv.org/abs/2502.17796)] [[cool](https://papers.cool/arxiv/2502.17796)] [[pdf](https://arxiv.org/pdf/2502.17796)]
> **Authors**: Yisheng He,Xiaodong Gu,Xiaodan Ye,Chao Xu,Zhengyi Zhao,Yuan Dong,Weihao Yuan,Zilong Dong,Liefeng Bo
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: We present LAM, an innovative Large Avatar Model for animatable Gaussian head reconstruction from a single image. Unlike previous methods that require extensive training on captured video sequences or rely on auxiliary neural networks for animation and rendering during inference, our approach generates Gaussian heads that are immediately animatable and renderable. Specifically, LAM creates an animatable Gaussian head in a single forward pass, enabling reenactment and rendering without additional networks or post-processing steps. This capability allows for seamless integration into existing rendering pipelines, ensuring real-time animation and rendering across a wide range of platforms, including mobile phones. The centerpiece of our framework is the canonical Gaussian attributes generator, which utilizes FLAME canonical points as queries. These points interact with multi-scale image features through a Transformer to accurately predict Gaussian attributes in the canonical space. The reconstructed canonical Gaussian avatar can then be animated utilizing standard linear blend skinning (LBS) with corrective blendshapes as the FLAME model did and rendered in real-time on various platforms. Our experimental results demonstrate that LAM outperforms state-of-the-art methods on existing benchmarks.

### Synthia: Novel Concept Design with Affordance Composition 
[[arxiv](https://arxiv.org/abs/2502.17793)] [[cool](https://papers.cool/arxiv/2502.17793)] [[pdf](https://arxiv.org/pdf/2502.17793)]
> **Authors**: Xiaomeng Jin,Hyeonjeong Ha,Jeonghwan Kim,Jiateng Liu,Zhenhailong Wang,Khanh Duy Nguyen,Ansel Blume,Nanyun Peng,Kai-wei Chang,Heng Ji
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Code is available https://github.com/HyeonjeongHa/SYNTHIA
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Text-to-image (T2I) models enable rapid concept design, making them widely used in AI-driven design. While recent studies focus on generating semantic and stylistic variations of given design concepts, functional coherence--the integration of multiple affordances into a single coherent concept--remains largely overlooked. In this paper, we introduce SYNTHIA, a framework for generating novel, functionally coherent designs based on desired affordances. Our approach leverages a hierarchical concept ontology that decomposes concepts into parts and affordances, serving as a crucial building block for functionally coherent design. We also develop a curriculum learning scheme based on our ontology that contrastively fine-tunes T2I models to progressively learn affordance composition while maintaining visual novelty. To elaborate, we (i) gradually increase affordance distance, guiding models from basic concept-affordance association to complex affordance compositions that integrate parts of distinct affordances into a single, coherent form, and (ii) enforce visual novelty by employing contrastive objectives to push learned representations away from existing concepts. Experimental results show that SYNTHIA outperforms state-of-the-art T2I models, demonstrating absolute gains of 25.1% and 14.7% for novelty and functional coherence in human evaluation, respectively.

### Improving Transformer Based Line Segment Detection with Matched Predicting and Re-ranking 
[[arxiv](https://arxiv.org/abs/2502.17766)] [[cool](https://papers.cool/arxiv/2502.17766)] [[pdf](https://arxiv.org/pdf/2502.17766)]
> **Authors**: Xin Tong,Shi Peng,Baojie Tian,Yufei Guo,Xuhui Huang,Zhe Ma
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Classical Transformer-based line segment detection methods have delivered impressive results. However, we observe that some accurately detected line segments are assigned low confidence scores during prediction, causing them to be ranked lower and potentially suppressed. Additionally, these models often require prolonged training periods to achieve strong performance, largely due to the necessity of bipartite matching. In this paper, we introduce RANK-LETR, a novel Transformer-based line segment detection method. Our approach leverages learnable geometric information to refine the ranking of predicted line segments by enhancing the confidence scores of high-quality predictions in a posterior verification step. We also propose a new line segment proposal method, wherein the feature point nearest to the centroid of the line segment directly predicts the location, significantly improving training efficiency and stability. Moreover, we introduce a line segment ranking loss to stabilize rankings during training, thereby enhancing the generalization capability of the model. Experimental results demonstrate that our method outperforms other Transformer-based and CNN-based approaches in prediction accuracy while requiring fewer training epochs than previous Transformer-based models.

### A digital eye-fixation biomarker using a deep anomaly scheme to classify Parkisonian patterns 
[[arxiv](https://arxiv.org/abs/2502.17762)] [[cool](https://papers.cool/arxiv/2502.17762)] [[pdf](https://arxiv.org/pdf/2502.17762)]
> **Authors**: Juan Niño,Luis Guayacán,Santiago Gómez,Fabio Martínez
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 6 pages, 4 images
- **标题**: None
- **领域**: 计算机视觉和模式识别,图像和视频处理,神经元和认知
- **Abstract**: Oculomotor alterations constitute a promising biomarker to detect and characterize Parkinson's disease (PD), even in prodromal stages. Currently, only global and simplified eye movement trajectories are employed to approximate the complex and hidden kinematic relationships of the oculomotor function. Recent advances on machine learning and video analysis have encouraged novel characterizations of eye movement patterns to quantify PD. These schemes enable the identification of spatiotemporal segments primarily associated with PD. However, they rely on discriminative models that require large training datasets and depend on balanced class distributions. This work introduces a novel video analysis scheme to quantify Parkinsonian eye fixation patterns with an anomaly detection framework. Contrary to classical deep discriminative schemes that learn differences among labeled classes, the proposed approach is focused on one-class learning, avoiding the necessity of a significant amount of data. The proposed approach focuses only on Parkinson's representation, considering any other class sample as an anomaly of the distribution. This approach was evaluated for an ocular fixation task, in a total of 13 control subjects and 13 patients on different stages of the disease. The proposed digital biomarker achieved an average sensitivity and specificity of 0.97 and 0.63, respectively, yielding an AUC-ROC of 0.95. A statistical test shows significant differences (p < 0.05) among predicted classes, evidencing a discrimination between patients and control subjects.

### AI-driven 3D Spatial Transcriptomics 
[[arxiv](https://arxiv.org/abs/2502.17761)] [[cool](https://papers.cool/arxiv/2502.17761)] [[pdf](https://arxiv.org/pdf/2502.17761)]
> **Authors**: Cristina Almagro-Pérez,Andrew H. Song,Luca Weishaupt,Ahrong Kim,Guillaume Jaume,Drew F. K. Williamson,Konstantin Hemker,Ming Y. Lu,Kritika Singh,Bowen Chen,Long Phi Le,Alexander S. Baras,Sizun Jiang,Ali Bashashati,Jonathan T. C. Liu,Faisal Mahmood
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,应用领域
- **Abstract**: A comprehensive three-dimensional (3D) map of tissue architecture and gene expression is crucial for illuminating the complexity and heterogeneity of tissues across diverse biomedical applications. However, most spatial transcriptomics (ST) approaches remain limited to two-dimensional (2D) sections of tissue. Although current 3D ST methods hold promise, they typically require extensive tissue sectioning, are complex, are not compatible with non-destructive 3D tissue imaging technologies, and often lack scalability. Here, we present VOlumetrically Resolved Transcriptomics EXpression (VORTEX), an AI framework that leverages 3D tissue morphology and minimal 2D ST to predict volumetric 3D ST. By pretraining on diverse 3D morphology-transcriptomic pairs from heterogeneous tissue samples and then fine-tuning on minimal 2D ST data from a specific volume of interest, VORTEX learns both generic tissue-related and sample-specific morphological correlates of gene expression. This approach enables dense, high-throughput, and fast 3D ST, scaling seamlessly to large tissue volumes far beyond the reach of existing 3D ST techniques. By offering a cost-effective and minimally destructive route to obtaining volumetric molecular insights, we anticipate that VORTEX will accelerate biomarker discovery and our understanding of morphomolecular associations and cell states in complex tissues. Interactive 3D ST volumes can be viewed at https://vortex-demo.github.io/

### Can Score-Based Generative Modeling Effectively Handle Medical Image Classification? 
[[arxiv](https://arxiv.org/abs/2502.17727)] [[cool](https://papers.cool/arxiv/2502.17727)] [[pdf](https://arxiv.org/pdf/2502.17727)]
> **Authors**: Sushmita Sarker,Prithul Sarker,George Bebis,Alireza Tavakkoli
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Accepted at the International Symposium on Biomedical Imaging (ISBI) 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: The remarkable success of deep learning in recent years has prompted applications in medical image classification and diagnosis tasks. While classification models have demonstrated robustness in classifying simpler datasets like MNIST or natural images such as ImageNet, this resilience is not consistently observed in complex medical image datasets where data is more scarce and lacks diversity. Moreover, previous findings on natural image datasets have indicated a potential trade-off between data likelihood and classification accuracy. In this study, we explore the use of score-based generative models as classifiers for medical images, specifically mammographic images. Our findings suggest that our proposed generative classifier model not only achieves superior classification results on CBIS-DDSM, INbreast and Vin-Dr Mammo datasets, but also introduces a novel approach to image classification in a broader context. Our code is publicly available at https://github.com/sushmitasarker/sgc_for_medical_image_classification

### Contrastive Visual Data Augmentation 
[[arxiv](https://arxiv.org/abs/2502.17709)] [[cool](https://papers.cool/arxiv/2502.17709)] [[pdf](https://arxiv.org/pdf/2502.17709)]
> **Authors**: Yu Zhou,Bingxuan Li,Mohan Tang,Xiaomeng Jin,Te-Lin Wu,Kuan-Hao Huang,Heng Ji,Kai-Wei Chang,Nanyun Peng
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习,多媒体
- **Abstract**: Large multimodal models (LMMs) often struggle to recognize novel concepts, as they rely on pre-trained knowledge and have limited ability to capture subtle visual details. Domain-specific knowledge gaps in training also make them prone to confusing visually similar, commonly misrepresented, or low-resource concepts. To help LMMs better align nuanced visual features with language, improving their ability to recognize and reason about novel or rare concepts, we propose a Contrastive visual Data Augmentation (CoDA) strategy. CoDA extracts key contrastive textual and visual features of target concepts against the known concepts they are misrecognized as, and then uses multimodal generative models to produce targeted synthetic data. Automatic filtering of extracted features and augmented images is implemented to guarantee their quality, as verified by human annotators. We show the effectiveness and efficiency of CoDA on low-resource concept and diverse scene recognition datasets including INaturalist and SUN. We additionally collect NovelSpecies, a benchmark dataset consisting of newly discovered animal species that are guaranteed to be unseen by LMMs. LLaVA-1.6 1-shot updating results on these three datasets show CoDA significantly improves SOTA visual data augmentation strategies by 12.3% (NovelSpecies), 5.1% (SUN), and 6.0% (iNat) absolute gains in accuracy.

### IBURD: Image Blending for Underwater Robotic Detection 
[[arxiv](https://arxiv.org/abs/2502.17706)] [[cool](https://papers.cool/arxiv/2502.17706)] [[pdf](https://arxiv.org/pdf/2502.17706)]
> **Authors**: Jungseok Hong,Sakshi Singh,Junaed Sattar
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: We present an image blending pipeline, \textit{IBURD}, that creates realistic synthetic images to assist in the training of deep detectors for use on underwater autonomous vehicles (AUVs) for marine debris detection tasks. Specifically, IBURD generates both images of underwater debris and their pixel-level annotations, using source images of debris objects, their annotations, and target background images of marine environments. With Poisson editing and style transfer techniques, IBURD is even able to robustly blend transparent objects into arbitrary backgrounds and automatically adjust the style of blended images using the blurriness metric of target background images. These generated images of marine debris in actual underwater backgrounds address the data scarcity and data variety problems faced by deep-learned vision algorithms in challenging underwater conditions, and can enable the use of AUVs for environmental cleanup missions. Both quantitative and robotic evaluations of IBURD demonstrate the efficacy of the proposed approach for robotic detection of marine debris.

### METAL: A Multi-Agent Framework for Chart Generation with Test-Time Scaling 
[[arxiv](https://arxiv.org/abs/2502.17651)] [[cool](https://papers.cool/arxiv/2502.17651)] [[pdf](https://arxiv.org/pdf/2502.17651)]
> **Authors**: Bingxuan Li,Yiwei Wang,Jiuxiang Gu,Kai-Wei Chang,Nanyun Peng
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **Abstract**: Chart generation aims to generate code to produce charts satisfying the desired visual properties, e.g., texts, layout, color, and type. It has great potential to empower the automatic professional report generation in financial analysis, research presentation, education, and healthcare. In this work, we build a vision-language model (VLM) based multi-agent framework for effective automatic chart generation. Generating high-quality charts requires both strong visual design skills and precise coding capabilities that embed the desired visual properties into code. Such a complex multi-modal reasoning process is difficult for direct prompting of VLMs. To resolve these challenges, we propose METAL, a multi-agent framework that decomposes the task of chart generation into the iterative collaboration among specialized agents. METAL achieves 5.2% improvement in accuracy over the current best result in the chart generation task. The METAL framework exhibits the phenomenon of test-time scaling: its performance increases monotonically as the logarithmic computational budget grows from 512 to 8192 tokens. In addition, we find that separating different modalities during the critique process of METAL boosts the self-correction capability of VLMs in the multimodal context.

### CalibRefine: Deep Learning-Based Online Automatic Targetless LiDAR-Camera Calibration with Iterative and Attention-Driven Post-Refinement 
[[arxiv](https://arxiv.org/abs/2502.17648)] [[cool](https://papers.cool/arxiv/2502.17648)] [[pdf](https://arxiv.org/pdf/2502.17648)]
> **Authors**: Lei Cheng,Lihao Guo,Tianya Zhang,Tam Bang,Austin Harris,Mustafa Hajij,Mina Sartipi,Siyang Cao
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Submitted to Transportation Research Part C: Emerging Technologies
- **标题**: None
- **领域**: 计算机视觉和模式识别,系统与控制
- **Abstract**: Accurate multi-sensor calibration is essential for deploying robust perception systems in applications such as autonomous driving, robotics, and intelligent transportation. Existing LiDAR-camera calibration methods often rely on manually placed targets, preliminary parameter estimates, or intensive data preprocessing, limiting their scalability and adaptability in real-world settings. In this work, we propose a fully automatic, targetless, and online calibration framework, CalibRefine, which directly processes raw LiDAR point clouds and camera images. Our approach is divided into four stages: (1) a Common Feature Discriminator that trains on automatically detected objects--using relative positions, appearance embeddings, and semantic classes--to generate reliable LiDAR-camera correspondences, (2) a coarse homography-based calibration, (3) an iterative refinement to incrementally improve alignment as additional data frames become available, and (4) an attention-based refinement that addresses non-planar distortions by leveraging a Vision Transformer and cross-attention mechanisms. Through extensive experiments on two urban traffic datasets, we show that CalibRefine delivers high-precision calibration results with minimal human involvement, outperforming state-of-the-art targetless methods and remaining competitive with, or surpassing, manually tuned baselines. Our findings highlight how robust object-level feature matching, together with iterative and self-supervised attention-based adjustments, enables consistent sensor fusion in complex, real-world conditions without requiring ground-truth calibration matrices or elaborate data preprocessing.

### A Priori Generalizability Estimate for a CNN 
[[arxiv](https://arxiv.org/abs/2502.17622)] [[cool](https://papers.cool/arxiv/2502.17622)] [[pdf](https://arxiv.org/pdf/2502.17622)]
> **Authors**: Cito Balsells,Beatrice Riviere,David Fuentes
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习,图像和视频处理
- **Abstract**: We formulate truncated singular value decompositions of entire convolutional neural networks. We demonstrate the computed left and right singular vectors are useful in identifying which images the convolutional neural network is likely to perform poorly on. To create this diagnostic tool, we define two metrics: the Right Projection Ratio and the Left Projection Ratio. The Right (Left) Projection Ratio evaluates the fidelity of the projection of an image (label) onto the computed right (left) singular vectors. We observe that both ratios are able to identify the presence of class imbalance for an image classification problem. Additionally, the Right Projection Ratio, which only requires unlabeled data, is found to be correlated to the model's performance when applied to image segmentation. This suggests the Right Projection Ratio could be a useful metric to estimate how likely the model is to perform well on a sample.

### PosterSum: A Multimodal Benchmark for Scientific Poster Summarization 
[[arxiv](https://arxiv.org/abs/2502.17540)] [[cool](https://papers.cool/arxiv/2502.17540)] [[pdf](https://arxiv.org/pdf/2502.17540)]
> **Authors**: Rohit Saxena,Pasquale Minervini,Frank Keller
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: This paper includes a dataset of research posters with abstracts. We provide two cited examples ( arXiv:2211.11880 and arXiv:2210.07571 ) to illustrate reference summaries
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **Abstract**: Generating accurate and concise textual summaries from multimodal documents is challenging, especially when dealing with visually complex content like scientific posters. We introduce PosterSum, a novel benchmark to advance the development of vision-language models that can understand and summarize scientific posters into research paper abstracts. Our dataset contains 16,305 conference posters paired with their corresponding abstracts as summaries. Each poster is provided in image format and presents diverse visual understanding challenges, such as complex layouts, dense text regions, tables, and figures. We benchmark state-of-the-art Multimodal Large Language Models (MLLMs) on PosterSum and demonstrate that they struggle to accurately interpret and summarize scientific posters. We propose Segment & Summarize, a hierarchical method that outperforms current MLLMs on automated metrics, achieving a 3.14% gain in ROUGE-L. This will serve as a starting point for future research on poster summarization.

### Introducing Visual Perception Token into Multimodal Large Language Model 
[[arxiv](https://arxiv.org/abs/2502.17425)] [[cool](https://papers.cool/arxiv/2502.17425)] [[pdf](https://arxiv.org/pdf/2502.17425)]
> **Authors**: Runpeng Yu,Xinyin Ma,Xinchao Wang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: To utilize visual information, Multimodal Large Language Model (MLLM) relies on the perception process of its vision encoder. The completeness and accuracy of visual perception significantly influence the precision of spatial reasoning, fine-grained understanding, and other tasks. However, MLLM still lacks the autonomous capability to control its own visual perception processes, for example, selectively reviewing specific regions of an image or focusing on information related to specific object categories. In this work, we propose the concept of Visual Perception Token, aiming to empower MLLM with a mechanism to control its visual perception processes. We design two types of Visual Perception Tokens, termed the Region Selection Token and the Vision Re-Encoding Token. MLLMs autonomously generate these tokens, just as they generate text, and use them to trigger additional visual perception actions. The Region Selection Token explicitly identifies specific regions in an image that require further perception, while the Vision Re-Encoding Token uses its hidden states as control signals to guide additional visual perception processes. Extensive experiments demonstrate the advantages of these tokens in handling spatial reasoning, improving fine-grained understanding, and other tasks. On average, the introduction of Visual Perception Tokens improves the performance of a 2B model by 23.6\%, increasing its score from 0.572 to 0.708, and even outperforms a 7B parameter model by 13.4\% (from 0.624). Please check out our repo https://github.com/yu-rp/VisualPerceptionToken

### MLLMs Know Where to Look: Training-free Perception of Small Visual Details with Multimodal LLMs 
[[arxiv](https://arxiv.org/abs/2502.17422)] [[cool](https://papers.cool/arxiv/2502.17422)] [[pdf](https://arxiv.org/pdf/2502.17422)]
> **Authors**: Jiarui Zhang,Mahyar Khayatkhoei,Prateek Chhikara,Filip Ilievski
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Published as a conference paper at ICLR 2025. Code at: https://github.com/saccharomycetes/mllms_know
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **Abstract**: Multimodal Large Language Models (MLLMs) have experienced rapid progress in visual recognition tasks in recent years. Given their potential integration into many critical applications, it is important to understand the limitations of their visual perception. In this work, we study whether MLLMs can perceive small visual details as effectively as large ones when answering questions about images. We observe that their performance is very sensitive to the size of the visual subject of the question, and further show that this effect is in fact causal by conducting an intervention study. Next, we study the attention patterns of MLLMs when answering visual questions, and intriguingly find that they consistently know where to look, even when they provide the wrong answer. Based on these findings, we then propose training-free visual intervention methods that leverage the internal knowledge of any MLLM itself, in the form of attention and gradient maps, to enhance its perception of small visual details. We evaluate our proposed methods on two widely-used MLLMs and seven visual question answering benchmarks and show that they can significantly improve MLLMs' accuracy without requiring any training. Our results elucidate the risk of applying MLLMs to visual recognition tasks concerning small details and indicate that visual intervention using the model's internal state is a promising direction to mitigate this risk.

### X-Dancer: Expressive Music to Human Dance Video Generation 
[[arxiv](https://arxiv.org/abs/2502.17414)] [[cool](https://papers.cool/arxiv/2502.17414)] [[pdf](https://arxiv.org/pdf/2502.17414)]
> **Authors**: Zeyuan Chen,Hongyi Xu,Guoxian Song,You Xie,Chenxu Zhang,Xin Chen,Chao Wang,Di Chang,Linjie Luo
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: We present X-Dancer, a novel zero-shot music-driven image animation pipeline that creates diverse and long-range lifelike human dance videos from a single static image. As its core, we introduce a unified transformer-diffusion framework, featuring an autoregressive transformer model that synthesize extended and music-synchronized token sequences for 2D body, head and hands poses, which then guide a diffusion model to produce coherent and realistic dance video frames. Unlike traditional methods that primarily generate human motion in 3D, X-Dancer addresses data limitations and enhances scalability by modeling a wide spectrum of 2D dance motions, capturing their nuanced alignment with musical beats through readily available monocular videos. To achieve this, we first build a spatially compositional token representation from 2D human pose labels associated with keypoint confidences, encoding both large articulated body movements (e.g., upper and lower body) and fine-grained motions (e.g., head and hands). We then design a music-to-motion transformer model that autoregressively generates music-aligned dance pose token sequences, incorporating global attention to both musical style and prior motion context. Finally we leverage a diffusion backbone to animate the reference image with these synthesized pose tokens through AdaIN, forming a fully differentiable end-to-end framework. Experimental results demonstrate that X-Dancer is able to produce both diverse and characterized dance videos, substantially outperforming state-of-the-art methods in term of diversity, expressiveness and realism. Code and model will be available for research purposes.

### Experimental validation of UAV search and detection system in real wilderness environment 
[[arxiv](https://arxiv.org/abs/2502.17372)] [[cool](https://papers.cool/arxiv/2502.17372)] [[pdf](https://arxiv.org/pdf/2502.17372)]
> **Authors**: Stella Dumenčić,Luka Lanča,Karlo Jakac,Stefan Ivić
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 32 pages, 15 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习,机器人技术,系统与控制
- **Abstract**: Search and rescue (SAR) missions require reliable search methods to locate survivors, especially in challenging or inaccessible environments. This is why introducing unmanned aerial vehicles (UAVs) can be of great help to enhance the efficiency of SAR missions while simultaneously increasing the safety of everyone involved in the mission. Motivated by this, we design and experiment with autonomous UAV search for humans in a Mediterranean karst environment. The UAVs are directed using Heat equation-driven area coverage (HEDAC) ergodic control method according to known probability density and detection function. The implemented sensing framework consists of a probabilistic search model, motion control system, and computer vision object detection. It enables calculation of the probability of the target being detected in the SAR mission, and this paper focuses on experimental validation of proposed probabilistic framework and UAV control. The uniform probability density to ensure the even probability of finding the targets in the desired search area is achieved by assigning suitably thought-out tasks to 78 volunteers. The detection model is based on YOLO and trained with a previously collected ortho-photo image database. The experimental search is carefully planned and conducted, while as many parameters as possible are recorded. The thorough analysis consists of the motion control system, object detection, and the search validation. The assessment of the detection and search performance provides strong indication that the designed detection model in the UAV control algorithm is aligned with real-world results.

### DIS-CO: Discovering Copyrighted Content in VLMs Training Data 
[[arxiv](https://arxiv.org/abs/2502.17358)] [[cool](https://papers.cool/arxiv/2502.17358)] [[pdf](https://arxiv.org/pdf/2502.17358)]
> **Authors**: André V. Duarte,Xuandong Zhao,Arlindo L. Oliveira,Lei Li
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: :I.2
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: How can we verify whether copyrighted content was used to train a large vision-language model (VLM) without direct access to its training data? Motivated by the hypothesis that a VLM is able to recognize images from its training corpus, we propose DIS-CO, a novel approach to infer the inclusion of copyrighted content during the model's development. By repeatedly querying a VLM with specific frames from targeted copyrighted material, DIS-CO extracts the content's identity through free-form text completions. To assess its effectiveness, we introduce MovieTection, a benchmark comprising 14,000 frames paired with detailed captions, drawn from films released both before and after a model's training cutoff. Our results show that DIS-CO significantly improves detection performance, nearly doubling the average AUC of the best prior method on models with logits available. Our findings also highlight a broader concern: all tested models appear to have been exposed to some extent to copyrighted content. Our code and data are available at https://github.com/avduarte333/DIS-CO

### GaussianFlowOcc: Sparse and Weakly Supervised Occupancy Estimation using Gaussian Splatting and Temporal Flow 
[[arxiv](https://arxiv.org/abs/2502.17288)] [[cool](https://papers.cool/arxiv/2502.17288)] [[pdf](https://arxiv.org/pdf/2502.17288)]
> **Authors**: Simon Boeder,Fabian Gigengack,Benjamin Risse
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Occupancy estimation has become a prominent task in 3D computer vision, particularly within the autonomous driving community. In this paper, we present a novel approach to occupancy estimation, termed GaussianFlowOcc, which is inspired by Gaussian Splatting and replaces traditional dense voxel grids with a sparse 3D Gaussian representation. Our efficient model architecture based on a Gaussian Transformer significantly reduces computational and memory requirements by eliminating the need for expensive 3D convolutions used with inefficient voxel-based representations that predominantly represent empty 3D spaces. GaussianFlowOcc effectively captures scene dynamics by estimating temporal flow for each Gaussian during the overall network training process, offering a straightforward solution to a complex problem that is often neglected by existing methods. Moreover, GaussianFlowOcc is designed for scalability, as it employs weak supervision and does not require costly dense 3D voxel annotations based on additional data (e.g., LiDAR). Through extensive experimentation, we demonstrate that GaussianFlowOcc significantly outperforms all previous methods for weakly supervised occupancy estimation on the nuScenes dataset while featuring an inference speed that is 50 times faster than current SOTA.

### UNB StepUP: A footStep database for gait analysis and recognition using Underfoot Pressure 
[[arxiv](https://arxiv.org/abs/2502.17244)] [[cool](https://papers.cool/arxiv/2502.17244)] [[pdf](https://arxiv.org/pdf/2502.17244)]
> **Authors**: Robyn Larracy,Angkoon Phinyomark,Ala Salehi,Eve MacDonald,Saeed Kazemi,Shikder Shafiul Bashar,Aaron Tabor,Erik Scheme
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Gait refers to the patterns of limb movement generated during walking, which are unique to each individual due to both physical and behavioural traits. Walking patterns have been widely studied in biometrics, biomechanics, sports, and rehabilitation. While traditional methods rely on video and motion capture, advances in underfoot pressure sensing technology now offer deeper insights into gait. However, underfoot pressures during walking remain underexplored due to the lack of large, publicly accessible datasets. To address this, the UNB StepUP database was created, featuring gait pressure data collected with high-resolution pressure sensing tiles (4 sensors/cm$^2$, 1.2m by 3.6m). Its first release, UNB StepUP-P150, includes over 200,000 footsteps from 150 individuals across various walking speeds (preferred, slow-to-stop, fast, and slow) and footwear types (barefoot, standard shoes, and two personal shoes). As the largest and most comprehensive dataset of its kind, it supports biometric gait recognition while presenting new research opportunities in biomechanics and deep learning. The UNB StepUP-P150 dataset sets a new benchmark for pressure-based gait analysis and recognition. Please note that the hypertext links to the dataset on FigShare remain dormant while the document is under review.

### Dimitra: Audio-driven Diffusion model for Expressive Talking Head Generation 
[[arxiv](https://arxiv.org/abs/2502.17198)] [[cool](https://papers.cool/arxiv/2502.17198)] [[pdf](https://arxiv.org/pdf/2502.17198)]
> **Authors**: Baptiste Chopin,Tashvik Dhamija,Pranav Balaji,Yaohui Wang,Antitza Dantcheva
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 5 pages + 2 pages for supplementary material, 2 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: We propose Dimitra, a novel framework for audio-driven talking head generation, streamlined to learn lip motion, facial expression, as well as head pose motion. Specifically, we train a conditional Motion Diffusion Transformer (cMDT) by modeling facial motion sequences with 3D representation. We condition the cMDT with only two input signals, an audio-sequence, as well as a reference facial image. By extracting additional features directly from audio, Dimitra is able to increase quality and realism of generated videos. In particular, phoneme sequences contribute to the realism of lip motion, whereas text transcript to facial expression and head pose realism. Quantitative and qualitative experiments on two widely employed datasets, VoxCeleb2 and HDTF, showcase that Dimitra is able to outperform existing approaches for generating realistic talking heads imparting lip motion, facial expression, and head pose.

### Disentangling Visual Transformers: Patch-level Interpretability for Image Classification 
[[arxiv](https://arxiv.org/abs/2502.17196)] [[cool](https://papers.cool/arxiv/2502.17196)] [[pdf](https://arxiv.org/pdf/2502.17196)]
> **Authors**: Guillaume Jeanneret,Loïc Simon,Frédéric Jurie
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Visual transformers have achieved remarkable performance in image classification tasks, but this performance gain has come at the cost of interpretability. One of the main obstacles to the interpretation of transformers is the self-attention mechanism, which mixes visual information across the whole image in a complex way. In this paper, we propose Hindered Transformer (HiT), a novel interpretable by design architecture inspired by visual transformers. Our proposed architecture rethinks the design of transformers to better disentangle patch influences at the classification stage. Ultimately, HiT can be interpreted as a linear combination of patch-level information. We show that the advantages of our approach in terms of explicability come with a reasonable trade-off in performance, making it an attractive alternative for applications where interpretability is paramount.

### A Pragmatic Note on Evaluating Generative Models with Fréchet Inception Distance for Retinal Image Synthesis 
[[arxiv](https://arxiv.org/abs/2502.17160)] [[cool](https://papers.cool/arxiv/2502.17160)] [[pdf](https://arxiv.org/pdf/2502.17160)]
> **Authors**: Yuli Wu,Fucheng Liu,Rüveyda Yilmaz,Henning Konermann,Peter Walter,Johannes Stegmaier
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Fréchet Inception Distance (FID), computed with an ImageNet pretrained Inception-v3 network, is widely used as a state-of-the-art evaluation metric for generative models. It assumes that feature vectors from Inception-v3 follow a multivariate Gaussian distribution and calculates the 2-Wasserstein distance based on their means and covariances. While FID effectively measures how closely synthetic data match real data in many image synthesis tasks, the primary goal in biomedical generative models is often to enrich training datasets ideally with corresponding annotations. For this purpose, the gold standard for evaluating generative models is to incorporate synthetic data into downstream task training, such as classification and segmentation, to pragmatically assess its performance. In this paper, we examine cases from retinal imaging modalities, including color fundus photography and optical coherence tomography, where FID and its related metrics misalign with task-specific evaluation goals in classification and segmentation. We highlight the limitations of using various metrics, represented by FID and its variants, as evaluation criteria for these applications and address their potential caveats in broader biomedical imaging modalities and downstream tasks.

### Parameter Efficient Merging for Multimodal Large Language Models with Complementary Parameter Adaptation 
[[arxiv](https://arxiv.org/abs/2502.17159)] [[cool](https://papers.cool/arxiv/2502.17159)] [[pdf](https://arxiv.org/pdf/2502.17159)]
> **Authors**: Fanhu Zeng,Haiyang Guo,Fei Zhu,Li Shen,Hao Tang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Fine-tuning pre-trained models with custom data leads to numerous expert models on specific tasks. Merging models into one universal model to empower multi-task ability refraining from data leakage has gained popularity. With the expansion in data and model size, parameter efficient tuning becomes the common practice for obtaining task-specific models efficiently. However, we observe that existing methods designed for full fine-tuning merging fail under efficient tuning. To address the issues, we analyze from low-rank decomposition and reveal that maintaining direction and compensating for gap between singular values are crucial for efficient model merging. Consequently, we propose CoPA-Merging, a training-free parameter efficient merging method with complementary parameter adaptation. Specifically, we (1) prune parameters and construct scaling coefficients from inter-parameter relation to compensate for performance drop from task interference and (2) perform cross-task normalization to enhance unseen task generalization. We establish a benchmark consisting of diverse multimodal tasks, on which we conduct experiments to certificate the outstanding performance and generalizability of our method. Additional study and extensive analyses further showcase the effectiveness.

### MaxGlaViT: A novel lightweight vision transformer-based approach for early diagnosis of glaucoma stages from fundus images 
[[arxiv](https://arxiv.org/abs/2502.17154)] [[cool](https://papers.cool/arxiv/2502.17154)] [[pdf](https://arxiv.org/pdf/2502.17154)]
> **Authors**: Mustafa Yurdakul,Kubra Uyar,Sakir Tasdemir
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Glaucoma is a prevalent eye disease that progresses silently without symptoms. If not detected and treated early, it can cause permanent vision loss. Computer-assisted diagnosis systems play a crucial role in timely and efficient identification. This study introduces MaxGlaViT, a lightweight model based on the restructured Multi-Axis Vision Transformer (MaxViT) for early glaucoma detection. First, MaxViT was scaled to optimize block and channel numbers, resulting in a lighter architecture. Second, the stem was enhanced by adding attention mechanisms (CBAM, ECA, SE) after convolution layers to improve feature learning. Third, MBConv structures in MaxViT blocks were replaced by advanced DL blocks (ConvNeXt, ConvNeXtV2, InceptionNeXt). The model was evaluated using the HDV1 dataset, containing fundus images of different glaucoma stages. Additionally, 40 CNN and 40 ViT models were tested on HDV1 to validate MaxGlaViT's efficiency. Among CNN models, EfficientB6 achieved the highest accuracy (84.91%), while among ViT models, MaxViT-Tiny performed best (86.42%). The scaled MaxViT reached 87.93% accuracy. Adding ECA to the stem block increased accuracy to 89.01%. Replacing MBConv with ConvNeXtV2 further improved it to 89.87%. Finally, integrating ECA in the stem and ConvNeXtV2 in MaxViT blocks resulted in 92.03% accuracy. Testing 80 DL models for glaucoma stage classification, this study presents a comprehensive and comparative analysis. MaxGlaViT outperforms experimental and state-of-the-art models, achieving 92.03% accuracy, 92.33% precision, 92.03% recall, 92.13% f1-score, and 87.12% Cohen's kappa score.

### SFLD: Reducing the content bias for AI-generated Image Detection 
[[arxiv](https://arxiv.org/abs/2502.17105)] [[cool](https://papers.cool/arxiv/2502.17105)] [[pdf](https://arxiv.org/pdf/2502.17105)]
> **Authors**: Seoyeon Gye,Junwon Ko,Hyounguk Shon,Minchan Kwon,Junmo Kim
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: IEEE/CVF WACV 2025, Oral
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Identifying AI-generated content is critical for the safe and ethical use of generative AI. Recent research has focused on developing detectors that generalize to unknown generators, with popular methods relying either on high-level features or low-level fingerprints. However, these methods have clear limitations: biased towards unseen content, or vulnerable to common image degradations, such as JPEG compression. To address these issues, we propose a novel approach, SFLD, which incorporates PatchShuffle to integrate high-level semantic and low-level textural information. SFLD applies PatchShuffle at multiple levels, improving robustness and generalization across various generative models. Additionally, current benchmarks face challenges such as low image quality, insufficient content preservation, and limited class diversity. In response, we introduce TwinSynths, a new benchmark generation methodology that constructs visually near-identical pairs of real and synthetic images to ensure high quality and content preservation. Our extensive experiments and analysis show that SFLD outperforms existing methods on detecting a wide variety of fake images sourced from GANs, diffusion models, and TwinSynths, demonstrating the state-of-the-art performance and generalization capabilities to novel generative models.

### Enhancing Image Matting in Real-World Scenes with Mask-Guided Iterative Refinement 
[[arxiv](https://arxiv.org/abs/2502.17093)] [[cool](https://papers.cool/arxiv/2502.17093)] [[pdf](https://arxiv.org/pdf/2502.17093)]
> **Authors**: Rui Liu
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 9pages
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Real-world image matting is essential for applications in content creation and augmented reality. However, it remains challenging due to the complex nature of scenes and the scarcity of high-quality datasets. To address these limitations, we introduce Mask2Alpha, an iterative refinement framework designed to enhance semantic comprehension, instance awareness, and fine-detail recovery in image matting. Our framework leverages self-supervised Vision Transformer features as semantic priors, strengthening contextual understanding in complex scenarios. To further improve instance differentiation, we implement a mask-guided feature selection module, enabling precise targeting of objects in multi-instance settings. Additionally, a sparse convolution-based optimization scheme allows Mask2Alpha to recover high-resolution details through progressive refinement,from low-resolution semantic passes to high-resolution sparse reconstructions. Benchmarking across various real-world datasets, Mask2Alpha consistently achieves state-of-the-art results, showcasing its effectiveness in accurate and efficient image matting.

### Shakti-VLMs: Scalable Vision-Language Models for Enterprise AI 
[[arxiv](https://arxiv.org/abs/2502.17092)] [[cool](https://papers.cool/arxiv/2502.17092)] [[pdf](https://arxiv.org/pdf/2502.17092)]
> **Authors**: Syed Abdul Gaffar Shakhadri,Kruthika KR,Kartik Basavaraj Angadi
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: We introduce Shakti VLM, a family of vision-language models in the capacity of 1B and 4B parameters designed to address data efficiency challenges in multimodal learning. While recent VLMs achieve strong performance through extensive training data, Shakti models leverage architectural innovations to attain competitive results with fewer tokens. Key advancements include QK-Normalization for attention stability, hybrid normalization techniques, and enhanced positional encoding. A three-stage training strategy further optimizes learning efficiency. Evaluations show that Shakti-Shakti-VLM-1B and Shakti-VLM-4B excel in document understanding, Visual Reasoning, OCR extraction, and general multimodal reasoning. Our results highlight that high performance can be achieved through model design and training strategy rather than sheer data volume, making Shakti an efficient solution for enterprise-scale multimodal tasks.

### DUNIA: Pixel-Sized Embeddings via Cross-Modal Alignment for Earth Observation Applications 
[[arxiv](https://arxiv.org/abs/2502.17066)] [[cool](https://papers.cool/arxiv/2502.17066)] [[pdf](https://arxiv.org/pdf/2502.17066)]
> **Authors**: Ibrahim Fayad,Max Zimmer,Martin Schwartz,Philippe Ciais,Fabian Gieseke,Gabriel Belouze,Sarah Brood,Aurelien De Truchis,Alexandre d'Aspremont
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 26 pages, 8 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Significant efforts have been directed towards adapting self-supervised multimodal learning for Earth observation applications. However, existing methods produce coarse patch-sized embeddings, limiting their effectiveness and integration with other modalities like LiDAR. To close this gap, we present DUNIA, an approach to learn pixel-sized embeddings through cross-modal alignment between images and full-waveform LiDAR data. As the model is trained in a contrastive manner, the embeddings can be directly leveraged in the context of a variety of environmental monitoring tasks in a zero-shot setting. In our experiments, we demonstrate the effectiveness of the embeddings for seven such tasks (canopy height mapping, fractional canopy cover, land cover mapping, tree species identification, plant area index, crop type classification, and per-pixel waveform-based vertical structure mapping). The results show that the embeddings, along with zero-shot classifiers, often outperform specialized supervised models, even in low data regimes. In the fine-tuning setting, we show strong low-shot capabilities with performances near or better than state-of-the-art on five out of six tasks.

### An Enhanced Large Language Model For Cross Modal Query Understanding System Using DL-KeyBERT Based CAZSSCL-MPGPT 
[[arxiv](https://arxiv.org/abs/2502.17000)] [[cool](https://papers.cool/arxiv/2502.17000)] [[pdf](https://arxiv.org/pdf/2502.17000)]
> **Authors**: Shreya Singh
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 26 pages, 7 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Large Language Models (LLMs) are advanced deep-learning models designed to understand and generate human language. They work together with models that process data like images, enabling cross-modal understanding. However, existing approaches often suffer from the echo chamber effect, where redundant visual patterns reduce model generalization and accuracy. Thus, the proposed system considered this limitation and developed an enhanced LLM-based framework for cross-modal query understanding using DL-KeyBERT-based CAZSSCL-MPGPT. The collected dataset consists of pre-processed images and texts. The preprocessed images then undergo object segmentation using Easom-You Only Look Once (E-YOLO). The object skeleton is generated, along with the knowledge graph using a Conditional Random Knowledge Graph (CRKG) technique. Further, features are extracted from the knowledge graph, generated skeletons, and segmented objects. The optimal features are then selected using the Fossa Optimization Algorithm (FOA). Meanwhile, the text undergoes word embedding using DL-KeyBERT. Finally, the cross-modal query understanding system utilizes CAZSSCL-MPGPT to generate accurate and contextually relevant image descriptions as text. The proposed CAZSSCL-MPGPT achieved an accuracy of 99.14187362% in the COCO dataset 2017 and 98.43224393% in the vqav2-val dataset.

### TraFlow: Trajectory Distillation on Pre-Trained Rectified Flow 
[[arxiv](https://arxiv.org/abs/2502.16972)] [[cool](https://papers.cool/arxiv/2502.16972)] [[pdf](https://arxiv.org/pdf/2502.16972)]
> **Authors**: Zhangkai Wu,Xuhui Fan,Hongyu Wu,Longbing Cao
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Majorities of distillation methods on pre-trained diffusion models or on pre-trained rectified flow, focus on either the distillation outputs or the trajectories between random noises and clean images to speed up sample generations from pre-trained models. In those trajectory-based distillation methods, consistency distillation requires the self-consistent trajectory projection to regulate the trajectory, which might avoid the common ODE approximation error {while still be concerning about sampling efficiencies}. At the same time, rectified flow distillations enforce straight trajectory for fast sampling, although an ODE solver is still required. In this work, we propose a trajectory distillation method, \modelname, that enjoys the benefits of both and enables few-step generations. TraFlow adopts the settings of consistency trajectory models, and further enforces the properties of self-consistency and straightness throughout the entire trajectory. These two properties are pursued by reaching a balance with following three targets: (1) reconstruct the output from pre-trained models; (2) learn the amount of changes by pre-trained models; (3) satisfy the self-consistency over its trajectory. Extensive experimental results have shown the effectiveness of our proposed method.

### Autoregressive Image Generation Guided by Chains of Thought 
[[arxiv](https://arxiv.org/abs/2502.16965)] [[cool](https://papers.cool/arxiv/2502.16965)] [[pdf](https://arxiv.org/pdf/2502.16965)]
> **Authors**: Miaomiao Cai,Guanjie Wang,Wei Li,Zhijun Tu,Hanting Chen,Shaohui Lin,Jie Hu
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: In the field of autoregressive (AR) image generation, models based on the 'next-token prediction' paradigm of LLMs have shown comparable performance to diffusion models by reducing inductive biases. However, directly applying LLMs to complex image generation can struggle with reconstructing the structure and details of the image, impacting the accuracy and stability of generation. Additionally, the 'next-token prediction' paradigm in the AR model does not align with the contextual scanning and logical reasoning processes involved in human visual perception, limiting effective image generation. Chain-of-Thought (CoT), as a key reasoning capability of LLMs, utilizes reasoning prompts to guide the model, improving reasoning performance on complex natural language process (NLP) tasks, enhancing accuracy and stability of generation, and helping the model maintain contextual coherence and logical consistency, similar to human reasoning. Inspired by CoT from the field of NLP, we propose autoregressive Image Generation with Thoughtful Reasoning (IGTR) to enhance autoregressive image generation. IGTR adds reasoning prompts without modifying the model structure or raster generation order. Specifically, we design specialized image-related reasoning prompts for AR image generation to simulate the human reasoning process, which enhances contextual reasoning by allowing the model to first perceive overall distribution information before generating the image, and improve generation stability by increasing the inference steps. Compared to the AR method without prompts, our method shows outstanding performance and achieves an approximate improvement of 20%.

### HVIS: A Human-like Vision and Inference System for Human Motion Prediction 
[[arxiv](https://arxiv.org/abs/2502.16913)] [[cool](https://papers.cool/arxiv/2502.16913)] [[pdf](https://arxiv.org/pdf/2502.16913)]
> **Authors**: Kedi Lyu,Haipeng Chen,Zhenguang Liu,Yifang Yin,Yukang Lin,Yingying Jiao
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Grasping the intricacies of human motion, which involve perceiving spatio-temporal dependence and multi-scale effects, is essential for predicting human motion. While humans inherently possess the requisite skills to navigate this issue, it proves to be markedly more challenging for machines to emulate. To bridge the gap, we propose the Human-like Vision and Inference System (HVIS) for human motion prediction, which is designed to emulate human observation and forecast future movements. HVIS comprises two components: the human-like vision encode (HVE) module and the human-like motion inference (HMI) module. The HVE module mimics and refines the human visual process, incorporating a retina-analog component that captures spatiotemporal information separately to avoid unnecessary crosstalk. Additionally, a visual cortex-analogy component is designed to hierarchically extract and treat complex motion features, focusing on both global and local features of human poses. The HMI is employed to simulate the multi-stage learning model of the human brain. The spontaneous learning network simulates the neuronal fracture generation process for the adversarial generation of future motions. Subsequently, the deliberate learning network is optimized for hard-to-train joints to prevent misleading learning. Experimental results demonstrate that our method achieves new state-of-the-art performance, significantly outperforming existing methods by 19.8% on Human3.6M, 15.7% on CMU Mocap, and 11.1% on G3D.

### SPARC: Score Prompting and Adaptive Fusion for Zero-Shot Multi-Label Recognition in Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2502.16911)] [[cool](https://papers.cool/arxiv/2502.16911)] [[pdf](https://arxiv.org/pdf/2502.16911)]
> **Authors**: Kevin Miller,Samarth Mishra,Aditya Gangrade,Kate Saenko,Venkatesh Saligrama
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Zero-shot multi-label recognition (MLR) with Vision-Language Models (VLMs) faces significant challenges without training data, model tuning, or architectural modifications. Existing approaches require prompt tuning or architectural adaptations, limiting zero-shot applicability. Our work proposes a novel solution treating VLMs as black boxes, leveraging scores without training data or ground truth. Using large language model insights on object co-occurrence, we introduce compound prompts grounded in realistic object combinations. Analysis of these prompt scores reveals VLM biases and ``AND''/``OR'' signal ambiguities, notably that maximum compound scores are surprisingly suboptimal compared to second-highest scores. We address these through a debiasing and score-fusion algorithm that corrects image bias and clarifies VLM response behaviors. Our method enhances other zero-shot approaches, consistently improving their results. Experiments show superior mean Average Precision (mAP) compared to methods requiring training data, achieved through refined object ranking for robust zero-shot MLR.

### MambaFlow: A Novel and Flow-guided State Space Model for Scene Flow Estimation 
[[arxiv](https://arxiv.org/abs/2502.16907)] [[cool](https://papers.cool/arxiv/2502.16907)] [[pdf](https://arxiv.org/pdf/2502.16907)]
> **Authors**: Jiehao Luo,Jintao Cheng,Xiaoyu Tang,Qingwen Zhang,Bohuan Xue,Rui Fan
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Scene flow estimation aims to predict 3D motion from consecutive point cloud frames, which is of great interest in autonomous driving field. Existing methods face challenges such as insufficient spatio-temporal modeling and inherent loss of fine-grained feature during voxelization. However, the success of Mamba, a representative state space model (SSM) that enables global modeling with linear complexity, provides a promising solution. In this paper, we propose MambaFlow, a novel scene flow estimation network with a mamba-based decoder. It enables deep interaction and coupling of spatio-temporal features using a well-designed backbone. Innovatively, we steer the global attention modeling of voxel-based features with point offset information using an efficient Mamba-based decoder, learning voxel-to-point patterns that are used to devoxelize shared voxel representations into point-wise features. To further enhance the model's generalization capabilities across diverse scenarios, we propose a novel scene-adaptive loss function that automatically adapts to different motion patterns.Extensive experiments on the Argoverse 2 benchmark demonstrate that MambaFlow achieves state-of-the-art performance with real-time inference speed among existing works, enabling accurate flow estimation in real-world urban scenarios. The code is available at https://github.com/SCNU-RISLAB/MambaFlow.

### Culture-TRIP: Culturally-Aware Text-to-Image Generation with Iterative Prompt Refinment 
[[arxiv](https://arxiv.org/abs/2502.16902)] [[cool](https://papers.cool/arxiv/2502.16902)] [[pdf](https://arxiv.org/pdf/2502.16902)]
> **Authors**: Suchae Jeong,Inseong Choi,Youngsik Yun,Jihie Kim
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 31 pages, 23 figures, Accepted by NAACL 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Text-to-Image models, including Stable Diffusion, have significantly improved in generating images that are highly semantically aligned with the given prompts. However, existing models may fail to produce appropriate images for the cultural concepts or objects that are not well known or underrepresented in western cultures, such as `hangari' (Korean utensil). In this paper, we propose a novel approach, Culturally-Aware Text-to-Image Generation with Iterative Prompt Refinement (Culture-TRIP), which refines the prompt in order to improve the alignment of the image with such culture nouns in text-to-image models. Our approach (1) retrieves cultural contexts and visual details related to the culture nouns in the prompt and (2) iteratively refines and evaluates the prompt based on a set of cultural criteria and large language models. The refinement process utilizes the information retrieved from Wikipedia and the Web. Our user survey, conducted with 66 participants from eight different countries demonstrates that our proposed approach enhances the alignment between the images and the prompts. In particular, C-TRIP demonstrates improved alignment between the generated images and underrepresented culture nouns. Resource can be found at https://shane3606.github.io/Culture-TRIP.

### Unveiling Institution-Specific Bias in Pathology Foundation Models: Detriments, Causes, and Potential Solutions 
[[arxiv](https://arxiv.org/abs/2502.16889)] [[cool](https://papers.cool/arxiv/2502.16889)] [[pdf](https://arxiv.org/pdf/2502.16889)]
> **Authors**: Weiping Lin,Shen Liu,Runchen Zhu,Liansheng Wang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 18 pages,1 figure,14 tables
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Pathology foundation models (PFMs) extract valuable discriminative features from images for downstream clinical tasks. PFMs have simplified the development of deep learning models, effectively leveraging prior knowledge to improve diagnostic accuracy in diverse scenarios. However, we find that PFMs sometimes struggle with certain challenges. Specifically, features extracted by PFMs are often contaminated by diagnosis-irrelevant information, i.e., institution-specific features associated with the images. This contamination can lead to spurious correlations, undermining the models' generalization ability when applied in real-world clinical settings. In this work, we first reveal the issue of feature contamination in PFMs, demonstrate the presence of institution-specific features, thoroughly investigate its negative impacts, analyze the underlying causes, and provide insights into potential solutions. Specifically, we find that institution-specific information is embedded in pathological images and can be readily captured by current PFMs. Through extensive experiments, we demonstrate the detrimental impact of this irrelevant information, particularly in out-of-distribution (OOD) settings, where reliance on contaminated features leads to significant performance degradation. This indicates that the models are being misled by non-diagnostic information. We further delve into the reasons PFMs extract such institution-specific information and validate our findings. Finally, we propose a simple yet effective solution to mitigate the influence of irrelevant information. This study is not intended to criticize existing PFMs, as they have indeed greatly advanced the development of computational pathology. our aim is to inspire future research to focus on innovative training strategies, rather than relying exclusively on scaling laws, to realize more generalized PFMs.

### A Survey of fMRI to Image Reconstruction 
[[arxiv](https://arxiv.org/abs/2502.16861)] [[cool](https://papers.cool/arxiv/2502.16861)] [[pdf](https://arxiv.org/pdf/2502.16861)]
> **Authors**: Weiyu Guo,Guoying Sun,JianXiang He,Tong Shao,Shaoguang Wang,Ziyang Chen,Meisheng Hong,Ying Sun,Hui Xiong
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Functional magnetic resonance imaging (fMRI) based image reconstruction plays a pivotal role in decoding human perception, with applications in neuroscience and brain-computer interfaces. While recent advancements in deep learning and large-scale datasets have driven progress, challenges such as data scarcity, cross-subject variability, and low semantic consistency persist. To address these issues, we introduce the concept of fMRI-to-Image Learning (fMRI2Image) and present the first systematic review in this field. This review highlights key challenges, categorizes methodologies such as fMRI signal encoding, feature mapping, and image generator. Finally, promising research directions are proposed to advance this emerging frontier, providing a reference for future studies.

### Exploring Causes and Mitigation of Hallucinations in Large Vision Language Models 
[[arxiv](https://arxiv.org/abs/2502.16842)] [[cool](https://papers.cool/arxiv/2502.16842)] [[pdf](https://arxiv.org/pdf/2502.16842)]
> **Authors**: Yaqi Sun,Kyohei Atarashi,Koh Takeuchi,Hisashi Kashima
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 19 pages, 6 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Large Vision-Language Models (LVLMs) integrate image encoders with Large Language Models (LLMs) to process multi-modal inputs and perform complex visual tasks. However, they often generate hallucinations by describing non-existent objects or attributes, compromising their reliability. This study analyzes hallucination patterns in image captioning, showing that not all tokens in the generation process are influenced by image input and that image dependency can serve as a useful signal for hallucination detection. To address this, we develop an automated pipeline to identify hallucinated objects and train a token-level classifier using hidden representations from parallel inference passes-with and without image input. Leveraging this classifier, we introduce a decoding strategy that effectively controls hallucination rates in image captioning at inference time.

## 计算机与社会(cs.CY:Computers and Society)

### Requirements for Quality Assurance of AI Models for Early Detection of Lung Cancer 
[[arxiv](https://arxiv.org/abs/2502.17639)] [[cool](https://papers.cool/arxiv/2502.17639)] [[pdf](https://arxiv.org/pdf/2502.17639)]
> **Authors**: Horst K. Hahn,Matthias S. May,Volker Dicken,Michael Walz,Rainer Eßeling,Bianca Lassen-Schmidt,Robert Rischen,Jens Vogel-Claussen,Konstantin Nikolaou,Jörg Barkhausen
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 12 pages incl. 2 figures, 2 charts, and references, summary in English (page 2), article in German (original title: Anforderungen an die Qualitätssicherung von KI-Modellen für die Lungenkrebs-Früherkennung)
- **标题**: None
- **领域**: 计算机与社会,人工智能,表现
- **Abstract**: Lung cancer is the second most common cancer and the leading cause of cancer-related deaths worldwide. Survival largely depends on tumor stage at diagnosis, and early detection with low-dose CT can significantly reduce mortality in high-risk patients. AI can improve the detection, measurement, and characterization of pulmonary nodules while reducing assessment time. However, the training data, functionality, and performance of available AI systems vary considerably, complicating software selection and regulatory evaluation. Manufacturers must specify intended use and provide test statistics, but they can choose their training and test data, limiting standardization and comparability. Under the EU AI Act, consistent quality assurance is required for AI-based nodule detection, measurement, and characterization. This position paper proposes systematic quality assurance grounded in a validated reference dataset, including real screening cases plus phantom data to verify volume and growth rate measurements. Regular updates shall reflect demographic shifts and technological advances, ensuring ongoing relevance. Consequently, ongoing AI quality assurance is vital. Regulatory challenges are also adressed. While the MDR and the EU AI Act set baseline requirements, they do not adequately address self-learning algorithms or their updates. A standardized, transparent quality assessment - based on sensitivity, specificity, and volumetric accuracy - enables an objective evaluation of each AI solution's strengths and weaknesses. Establishing clear testing criteria and systematically using updated reference data lay the groundwork for comparable performance metrics, informing tenders, guidelines, and recommendations.

### Towards Robust Legal Reasoning: Harnessing Logical LLMs in Law 
[[arxiv](https://arxiv.org/abs/2502.17638)] [[cool](https://papers.cool/arxiv/2502.17638)] [[pdf](https://arxiv.org/pdf/2502.17638)]
> **Authors**: Manuj Kant,Sareh Nabi,Manav Kant,Roland Scharrer,Megan Ma,Marzieh Nabi
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: Legal services rely heavily on text processing. While large language models (LLMs) show promise, their application in legal contexts demands higher accuracy, repeatability, and transparency. Logic programs, by encoding legal concepts as structured rules and facts, offer reliable automation, but require sophisticated text extraction. We propose a neuro-symbolic approach that integrates LLMs' natural language understanding with logic-based reasoning to address these limitations. As a legal document case study, we applied neuro-symbolic AI to coverage-related queries in insurance contracts using both closed and open-source LLMs. While LLMs have improved in legal reasoning, they still lack the accuracy and consistency required for complex contract analysis. In our analysis, we tested three methodologies to evaluate whether a specific claim is covered under a contract: a vanilla LLM, an unguided approach that leverages LLMs to encode both the contract and the claim, and a guided approach that uses a framework for the LLM to encode the contract. We demonstrated the promising capabilities of LLM + Logic in the guided approach.

## 数据库(cs.DB:Databases)

### Graphy'our Data: Towards End-to-End Modeling, Exploring and Generating Report from Raw Data 
[[arxiv](https://arxiv.org/abs/2502.16868)] [[cool](https://papers.cool/arxiv/2502.16868)] [[pdf](https://arxiv.org/pdf/2502.16868)]
> **Authors**: Longbin Lai,Changwei Luo,Yunkai Lou,Mingchen Ju,Zhengyi Yang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 4 pages
- **标题**: None
- **领域**: 数据库,人工智能,人机交互
- **Abstract**: Large Language Models (LLMs) have recently demonstrated remarkable performance in tasks such as Retrieval-Augmented Generation (RAG) and autonomous AI agent workflows. Yet, when faced with large sets of unstructured documents requiring progressive exploration, analysis, and synthesis, such as conducting literature survey, existing approaches often fall short. We address this challenge -- termed Progressive Document Investigation -- by introducing Graphy, an end-to-end platform that automates data modeling, exploration and high-quality report generation in a user-friendly manner. Graphy comprises an offline Scrapper that transforms raw documents into a structured graph of Fact and Dimension nodes, and an online Surveyor that enables iterative exploration and LLM-driven report generation. We showcase a pre-scrapped graph of over 50,000 papers -- complete with their references -- demonstrating how Graphy facilitates the literature-survey scenario. The demonstration video can be found at https://youtu.be/uM4nzkAdGlM.

## 分布式、并行和集群计算(cs.DC:Distributed, Parallel, and Cluster Computing)

### Robust Federated Learning in Unreliable Wireless Networks: A Client Selection Approach 
[[arxiv](https://arxiv.org/abs/2502.17260)] [[cool](https://papers.cool/arxiv/2502.17260)] [[pdf](https://arxiv.org/pdf/2502.17260)]
> **Authors**: Yanmeng Wang,Wenkai Ji,Jian Zhou,Fu Xiao,Tsung-Hui Chang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 分布式、并行和集群计算,机器学习
- **Abstract**: Federated learning (FL) has emerged as a promising distributed learning paradigm for training deep neural networks (DNNs) at the wireless edge, but its performance can be severely hindered by unreliable wireless transmission and inherent data heterogeneity among clients. Existing solutions primarily address these challenges by incorporating wireless resource optimization strategies, often focusing on uplink resource allocation across clients under the assumption of homogeneous client-server network standards. However, these approaches overlooked the fact that mobile clients may connect to the server via diverse network standards (e.g., 4G, 5G, Wi-Fi) with customized configurations, limiting the flexibility of server-side modifications and restricting applicability in real-world commercial networks. This paper presents a novel theoretical analysis about how transmission failures in unreliable networks distort the effective label distributions of local samples, causing deviations from the global data distribution and introducing convergence bias in FL. Our analysis reveals that a carefully designed client selection strategy can mitigate biases induced by network unreliability and data heterogeneity. Motivated by this insight, we propose FedCote, a client selection approach that optimizes client selection probabilities without relying on wireless resource scheduling. Experimental results demonstrate the robustness of FedCote in DNN-based classification tasks under unreliable networks with frequent transmission failures.

## 图形(cs.GR:Graphics)

### AnyTop: Character Animation Diffusion with Any Topology 
[[arxiv](https://arxiv.org/abs/2502.17327)] [[cool](https://papers.cool/arxiv/2502.17327)] [[pdf](https://arxiv.org/pdf/2502.17327)]
> **Authors**: Inbar Gat,Sigal Raab,Guy Tevet,Yuval Reshef,Amit H. Bermano,Daniel Cohen-Or
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Video: https://www.youtube.com/watch?v=zh5KuAbknOo, Project page: https://anytop2025.github.io/Anytop-page, Code: https://github.com/Anytop2025/Anytop
- **标题**: None
- **领域**: 图形,人工智能,计算机视觉和模式识别
- **Abstract**: Generating motion for arbitrary skeletons is a longstanding challenge in computer graphics, remaining largely unexplored due to the scarcity of diverse datasets and the irregular nature of the data. In this work, we introduce AnyTop, a diffusion model that generates motions for diverse characters with distinct motion dynamics, using only their skeletal structure as input. Our work features a transformer-based denoising network, tailored for arbitrary skeleton learning, integrating topology information into the traditional attention mechanism. Additionally, by incorporating textual joint descriptions into the latent feature representation, AnyTop learns semantic correspondences between joints across diverse skeletons. Our evaluation demonstrates that AnyTop generalizes well, even with as few as three training examples per topology, and can produce motions for unseen skeletons as well. Furthermore, our model's latent space is highly informative, enabling downstream tasks such as joint correspondence, temporal segmentation and motion editing. Our webpage, https://anytop2025.github.io/Anytop-page, includes links to videos and code.

### VR-Pipe: Streamlining Hardware Graphics Pipeline for Volume Rendering 
[[arxiv](https://arxiv.org/abs/2502.17078)] [[cool](https://papers.cool/arxiv/2502.17078)] [[pdf](https://arxiv.org/pdf/2502.17078)]
> **Authors**: Junseo Lee,Jaisung Kim,Junyong Park,Jaewoong Sim
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: To appear at the 31st International Symposium on High-Performance Computer Architecture (HPCA 2025)
- **标题**: None
- **领域**: 图形,硬件架构,计算机视觉和模式识别
- **Abstract**: Graphics rendering that builds on machine learning and radiance fields is gaining significant attention due to its outstanding quality and speed in generating photorealistic images from novel viewpoints. However, prior work has primarily focused on evaluating its performance through software-based rendering on programmable shader cores, leaving its performance when exploiting fixed-function graphics units largely unexplored. In this paper, we investigate the performance implications of performing radiance field rendering on the hardware graphics pipeline. In doing so, we implement the state-of-the-art radiance field method, 3D Gaussian splatting, using graphics APIs and evaluate it across synthetic and real-world scenes on today's graphics hardware. Based on our analysis, we present VR-Pipe, which seamlessly integrates two innovations into graphics hardware to streamline the hardware pipeline for volume rendering, such as radiance field methods. First, we introduce native hardware support for early termination by repurposing existing special-purpose hardware in modern GPUs. Second, we propose multi-granular tile binning with quad merging, which opportunistically blends fragments in shader cores before passing them to fixed-function blending units. Our evaluation shows that VR-Pipe greatly improves rendering performance, achieving up to a 2.78x speedup over the conventional graphics pipeline with negligible hardware overhead.

## 人机交互(cs.HC:Human-Computer Interaction)

### On the usability of generative AI: Human generative AI 
[[arxiv](https://arxiv.org/abs/2502.17714)] [[cool](https://papers.cool/arxiv/2502.17714)] [[pdf](https://arxiv.org/pdf/2502.17714)]
> **Authors**: Anna Ravera,Cristina Gena
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 人机交互,人工智能
- **Abstract**: Generative AI systems are transforming content creation, but their usability remains a key challenge. This paper examines usability factors such as user experience, transparency, control, and cognitive load. Common challenges include unpredictability and difficulties in fine-tuning outputs. We review evaluation metrics like efficiency, learnability, and satisfaction, highlighting best practices from various domains. Improving interpretability, intuitive interfaces, and user feedback can enhance usability, making generative AI more accessible and effective.

### Wearable Meets LLM for Stress Management: A Duoethnographic Study Integrating Wearable-Triggered Stressors and LLM Chatbots for Personalized Interventions 
[[arxiv](https://arxiv.org/abs/2502.17650)] [[cool](https://papers.cool/arxiv/2502.17650)] [[pdf](https://arxiv.org/pdf/2502.17650)]
> **Authors**: Sameer Neupane,Poorvesh Dongre,Denis Gracanin,Santosh Kumar
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: In CHI '25 Proceedings of the CHI Conference on Human Factors in Computing Systems Yokohama, Japan
- **标题**: None
- **领域**: 人机交互,人工智能
- **Abstract**: We use a duoethnographic approach to study how wearable-integrated LLM chatbots can assist with personalized stress management, addressing the growing need for immediacy and tailored interventions. Two researchers interacted with custom chatbots over 22 days, responding to wearable-detected physiological prompts, recording stressor phrases, and using them to seek tailored interventions from their LLM-powered chatbots. They recorded their experiences in autoethnographic diaries and analyzed them during weekly discussions, focusing on the relevance, clarity, and impact of chatbot-generated interventions. Results showed that even though most events triggered by the wearable were meaningful, only one in five warranted an intervention. It also showed that interventions tailored with brief event descriptions were more effective than generic ones. By examining the intersection of wearables and LLM, this research contributes to developing more effective, user-centric mental health tools for real-time stress relief and behavior change.

### Teleology-Driven Affective Computing: A Causal Framework for Sustained Well-Being 
[[arxiv](https://arxiv.org/abs/2502.17172)] [[cool](https://papers.cool/arxiv/2502.17172)] [[pdf](https://arxiv.org/pdf/2502.17172)]
> **Authors**: Bin Yin,Chong-Yi Liu,Liya Fu,Jinkun Zhang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 24 pages, 7 figures
- **标题**: None
- **领域**: 人机交互,人工智能,计算机与社会,神经元和认知
- **Abstract**: Affective computing has made significant strides in emotion recognition and generation, yet current approaches mainly focus on short-term pattern recognition and lack a comprehensive framework to guide affective agents toward long-term human well-being. To address this, we propose a teleology-driven affective computing framework that unifies major emotion theories (basic emotion, appraisal, and constructivist approaches) under the premise that affect is an adaptive, goal-directed process that facilitates survival and development. Our framework emphasizes aligning agent responses with both personal/individual and group/collective well-being over extended timescales. We advocate for creating a "dataverse" of personal affective events, capturing the interplay between beliefs, goals, actions, and outcomes through real-world experience sampling and immersive virtual reality. By leveraging causal modeling, this "dataverse" enables AI systems to infer individuals' unique affective concerns and provide tailored interventions for sustained well-being. Additionally, we introduce a meta-reinforcement learning paradigm to train agents in simulated environments, allowing them to adapt to evolving affective concerns and balance hierarchical goals - from immediate emotional needs to long-term self-actualization. This framework shifts the focus from statistical correlations to causal reasoning, enhancing agents' ability to predict and respond proactively to emotional challenges, and offers a foundation for developing personalized, ethically aligned affective systems that promote meaningful human-AI interactions and societal well-being.

### Imprinto: Enhancing Infrared Inkjet Watermarking for Human and Machine Perception 
[[arxiv](https://arxiv.org/abs/2502.17089)] [[cool](https://papers.cool/arxiv/2502.17089)] [[pdf](https://arxiv.org/pdf/2502.17089)]
> **Authors**: Martin Feick,Xuxin Tang,Raul Garcia-Martin,Alexandru Luchianov,Roderick Wei Xiao Huang,Chang Xiao,Alexa Siu,Mustafa Doga Dogan
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 18 pages, 13 figures. To appear in the Proceedings of the 2025 ACM CHI Conference on Human Factors in Computing Systems. https://imprinto.github.io
- **标题**: None
- **领域**: 人机交互,计算机视觉和模式识别,新兴技术
- **Abstract**: Hybrid paper interfaces leverage augmented reality to combine the desired tangibility of paper documents with the affordances of interactive digital media. Typically, virtual content can be embedded through direct links (e.g., QR codes); however, this impacts the aesthetics of the paper print and limits the available visual content space. To address this problem, we present Imprinto, an infrared inkjet watermarking technique that allows for invisible content embeddings only by using off-the-shelf IR inks and a camera. Imprinto was established through a psychophysical experiment, studying how much IR ink can be used while remaining invisible to users regardless of background color. We demonstrate that we can detect invisible IR content through our machine learning pipeline, and we developed an authoring tool that optimizes the amount of IR ink on the color regions of an input document for machine and human detectability. Finally, we demonstrate several applications, including augmenting paper documents and objects.

## 信息检索(cs.IR:Information Retrieval)

### Tip of the Tongue Query Elicitation for Simulated Evaluation 
[[arxiv](https://arxiv.org/abs/2502.17776)] [[cool](https://papers.cool/arxiv/2502.17776)] [[pdf](https://arxiv.org/pdf/2502.17776)]
> **Authors**: Yifan He,To Eun Kim,Fernando Diaz,Jaime Arguello,Bhaskar Mitra
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,计算语言学,人机交互
- **Abstract**: Tip-of-the-tongue (TOT) search occurs when a user struggles to recall a specific identifier, such as a document title. While common, existing search systems often fail to effectively support TOT scenarios. Research on TOT retrieval is further constrained by the challenge of collecting queries, as current approaches rely heavily on community question-answering (CQA) websites, leading to labor-intensive evaluation and domain bias. To overcome these limitations, we introduce two methods for eliciting TOT queries - leveraging large language models (LLMs) and human participants - to facilitate simulated evaluations of TOT retrieval systems. Our LLM-based TOT user simulator generates synthetic TOT queries at scale, achieving high correlations with how CQA-based TOT queries rank TOT retrieval systems when tested in the Movie domain. Additionally, these synthetic queries exhibit high linguistic similarity to CQA-derived queries. For human-elicited queries, we developed an interface that uses visual stimuli to place participants in a TOT state, enabling the collection of natural queries. In the Movie domain, system rank correlation and linguistic similarity analyses confirm that human-elicited queries are both effective and closely resemble CQA-based queries. These approaches reduce reliance on CQA-based data collection while expanding coverage to underrepresented domains, such as Landmark and Person. LLM-elicited queries for the Movie, Landmark, and Person domains have been released as test queries in the TREC 2024 TOT track, with human-elicited queries scheduled for inclusion in the TREC 2025 TOT track. Additionally, we provide source code for synthetic query generation and the human query collection interface, along with curated visual stimuli used for eliciting TOT queries.

### External Large Foundation Model: How to Efficiently Serve Trillions of Parameters for Online Ads Recommendation 
[[arxiv](https://arxiv.org/abs/2502.17494)] [[cool](https://papers.cool/arxiv/2502.17494)] [[pdf](https://arxiv.org/pdf/2502.17494)]
> **Authors**: Mingfu Liang,Xi Liu,Rong Jin,Boyang Liu,Qiuling Suo,Qinghai Zhou,Song Zhou,Laming Chen,Hua Zheng,Zhiyuan Li,Shali Jiang,Jiyan Yang,Xiaozhen Xia,Fan Yang,Yasmine Badr,Ellie Wen,Shuyu Xu,Hansey Chen,Zhengyu Zhang,Jade Nie,Chunzhi Yang,Zhichen Zeng,Weilin Zhang,Xingliang Huang,Qianru Li, et al. (74 additional authors not shown)
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-25
> **comment**: Accepted by the ACM Web Conference (WWW) 2025 Industrial Track as Oral Presentation
- **标题**: None
- **领域**: 信息检索,人工智能,机器学习
- **Abstract**: Ads recommendation is a prominent service of online advertising systems and has been actively studied. Recent studies indicate that scaling-up and advanced design of the recommendation model can bring significant performance improvement. However, with a larger model scale, such prior studies have a significantly increasing gap from industry as they often neglect two fundamental challenges in industrial-scale applications. First, training and inference budgets are restricted for the model to be served, exceeding which may incur latency and impair user experience. Second, large-volume data arrive in a streaming mode with data distributions dynamically shifting, as new users/ads join and existing users/ads leave the system. We propose the External Large Foundation Model (ExFM) framework to address the overlooked challenges. Specifically, we develop external distillation and a data augmentation system (DAS) to control the computational cost of training/inference while maintaining high performance. We design the teacher in a way like a foundation model (FM) that can serve multiple students as vertical models (VMs) to amortize its building cost. We propose Auxiliary Head and Student Adapter to mitigate the data distribution gap between FM and VMs caused by the streaming data issue. Comprehensive experiments on internal industrial-scale applications and public datasets demonstrate significant performance gain by ExFM.

### LLM-QE: Improving Query Expansion by Aligning Large Language Models with Ranking Preferences 
[[arxiv](https://arxiv.org/abs/2502.17057)] [[cool](https://papers.cool/arxiv/2502.17057)] [[pdf](https://arxiv.org/pdf/2502.17057)]
> **Authors**: Sijia Yao,Pengcheng Huang,Zhenghao Liu,Yu Gu,Yukun Yan,Shi Yu,Ge Yu
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 13 pages, 5 tables, 4 figures
- **标题**: None
- **领域**: 信息检索,人工智能
- **Abstract**: Query expansion plays a crucial role in information retrieval, which aims to bridge the semantic gap between queries and documents to improve matching performance. This paper introduces LLM-QE, a novel approach that leverages Large Language Models (LLMs) to generate document-based query expansions, thereby enhancing dense retrieval models. Unlike traditional methods, LLM-QE designs both rank-based and answer-based rewards and uses these reward models to optimize LLMs to align with the ranking preferences of both retrievers and LLMs, thus mitigating the hallucination of LLMs during query expansion. Our experiments on the zero-shot dense retrieval model, Contriever, demonstrate the effectiveness of LLM-QE, achieving an improvement of over 8%. Furthermore, by incorporating answer-based reward modeling, LLM-QE generates more relevant and precise information related to the documents, rather than simply producing redundant tokens to maximize rank-based rewards. Notably, LLM-QE also improves the training process of dense retrievers, achieving a more than 5% improvement after fine-tuning. All codes are available at https://github.com/NEUIR/LLM-QE.

## 机器学习(cs.LG:Machine Learning)

### Armada: Memory-Efficient Distributed Training of Large-Scale Graph Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.17846)] [[cool](https://papers.cool/arxiv/2502.17846)] [[pdf](https://arxiv.org/pdf/2502.17846)]
> **Authors**: Roger Waleffe,Devesh Sarda,Jason Mohoney,Emmanouil-Vasileios Vlatakis-Gkaragkounis,Theodoros Rekatsinas,Shivaram Venkataraman
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,分布式、并行和集群计算
- **Abstract**: We study distributed training of Graph Neural Networks (GNNs) on billion-scale graphs that are partitioned across machines. Efficient training in this setting relies on min-edge-cut partitioning algorithms, which minimize cross-machine communication due to GNN neighborhood sampling. Yet, min-edge-cut partitioning over large graphs remains a challenge: State-of-the-art (SoTA) offline methods (e.g., METIS) are effective, but they require orders of magnitude more memory and runtime than GNN training itself, while computationally efficient algorithms (e.g., streaming greedy approaches) suffer from increased edge cuts. Thus, in this work we introduce Armada, a new end-to-end system for distributed GNN training whose key contribution is GREM, a novel min-edge-cut partitioning algorithm that can efficiently scale to large graphs. GREM builds on streaming greedy approaches with one key addition: prior vertex assignments are continuously refined during streaming, rather than frozen after an initial greedy selection. Our theoretical analysis and experimental results show that this refinement is critical to minimizing edge cuts and enables GREM to reach partition quality comparable to METIS but with 8-65x less memory and 8-46x faster. Given a partitioned graph, Armada leverages a new disaggregated architecture for distributed GNN training to further improve efficiency; we find that on common cloud machines, even with zero communication, GNN neighborhood sampling and feature loading bottleneck training. Disaggregation allows Armada to independently allocate resources for these operations and ensure that expensive GPUs remain saturated with computation. We evaluate Armada against SoTA systems for distributed GNN training and find that the disaggregated architecture leads to runtime improvements up to 4.5x and cost reductions up to 3.1x.

### LeanKAN: A Parameter-Lean Kolmogorov-Arnold Network Layer with Improved Memory Efficiency and Convergence Behavior 
[[arxiv](https://arxiv.org/abs/2502.17844)] [[cool](https://papers.cool/arxiv/2502.17844)] [[pdf](https://arxiv.org/pdf/2502.17844)]
> **Authors**: Benjamin C. Koenig,Suyong Kim,Sili Deng
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 15 pages, 5 figures, and 1 table
- **标题**: None
- **领域**: 机器学习,神经和进化计算
- **Abstract**: The recently proposed Kolmogorov-Arnold network (KAN) is a promising alternative to multi-layer perceptrons (MLPs) for data-driven modeling. While original KAN layers were only capable of representing the addition operator, the recently-proposed MultKAN layer combines addition and multiplication subnodes in an effort to improve representation performance. Here, we find that MultKAN layers suffer from a few key drawbacks including limited applicability in output layers, bulky parameterizations with extraneous activations, and the inclusion of complex hyperparameters. To address these issues, we propose LeanKANs, a direct and modular replacement for MultKAN and traditional AddKAN layers. LeanKANs address these three drawbacks of MultKAN through general applicability as output layers, significantly reduced parameter counts for a given network structure, and a smaller set of hyperparameters. As a one-to-one layer replacement for standard AddKAN and MultKAN layers, LeanKAN is able to provide these benefits to traditional KAN learning problems as well as augmented KAN structures in which it serves as the backbone, such as KAN Ordinary Differential Equations (KAN-ODEs) or Deep Operator KANs (DeepOKAN). We demonstrate LeanKAN's simplicity and efficiency in a series of demonstrations carried out across both a standard KAN toy problem and a KAN-ODE dynamical system modeling problem, where we find that its sparser parameterization and compact structure serve to increase its expressivity and learning capability, leading it to outperform similar and even much larger MultKANs in various tasks.

### Task-Driven Semantic Quantization and Imitation Learning for Goal-Oriented Communications 
[[arxiv](https://arxiv.org/abs/2502.17842)] [[cool](https://papers.cool/arxiv/2502.17842)] [[pdf](https://arxiv.org/pdf/2502.17842)]
> **Authors**: Yu-Chieh Chao,Yubei Chen,Weiwei Wang,Achintha Wijesinghe,Suchinthaka Wanninayaka,Songyang Zhang,Zhi Ding
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Accepted for publication in 2025 International Conference on Communications (IEEE ICC); 6 pages, 4 figures
- **标题**: None
- **领域**: 机器学习,网络和互联网架构
- **Abstract**: Semantic communication marks a new paradigm shift from bit-wise data transmission to semantic information delivery for the purpose of bandwidth reduction. To more effectively carry out specialized downstream tasks at the receiver end, it is crucial to define the most critical semantic message in the data based on the task or goal-oriented features. In this work, we propose a novel goal-oriented communication (GO-COM) framework, namely Goal-Oriented Semantic Variational Autoencoder (GOS-VAE), by focusing on the extraction of the semantics vital to the downstream tasks. Specifically, we adopt a Vector Quantized Variational Autoencoder (VQ-VAE) to compress media data at the transmitter side. Instead of targeting the pixel-wise image data reconstruction, we measure the quality-of-service at the receiver end based on a pre-defined task-incentivized model. Moreover, to capture the relevant semantic features in the data reconstruction, imitation learning is adopted to measure the data regeneration quality in terms of goal-oriented semantics. Our experimental results demonstrate the power of imitation learning in characterizing goal-oriented semantics and bandwidth efficiency of our proposed GOS-VAE.

### MM-PoisonRAG: Disrupting Multimodal RAG with Local and Global Poisoning Attacks 
[[arxiv](https://arxiv.org/abs/2502.17832)] [[cool](https://papers.cool/arxiv/2502.17832)] [[pdf](https://arxiv.org/pdf/2502.17832)]
> **Authors**: Hyeonjeong Ha,Qiusi Zhan,Jeonghwan Kim,Dimitrios Bralios,Saikrishna Sanniboina,Nanyun Peng,Kai-wei Chang,Daniel Kang,Heng Ji
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Code is available at https://github.com/HyeonjeongHa/MM-PoisonRAG
- **标题**: None
- **领域**: 机器学习,人工智能,密码学和安全,计算机视觉和模式识别
- **Abstract**: Multimodal large language models (MLLMs) equipped with Retrieval Augmented Generation (RAG) leverage both their rich parametric knowledge and the dynamic, external knowledge to excel in tasks such as Question Answering. While RAG enhances MLLMs by grounding responses in query-relevant external knowledge, this reliance poses a critical yet underexplored safety risk: knowledge poisoning attacks, where misinformation or irrelevant knowledge is intentionally injected into external knowledge bases to manipulate model outputs to be incorrect and even harmful. To expose such vulnerabilities in multimodal RAG, we propose MM-PoisonRAG, a novel knowledge poisoning attack framework with two attack strategies: Localized Poisoning Attack (LPA), which injects query-specific misinformation in both text and images for targeted manipulation, and Globalized Poisoning Attack (GPA) to provide false guidance during MLLM generation to elicit nonsensical responses across all queries. We evaluate our attacks across multiple tasks, models, and access settings, demonstrating that LPA successfully manipulates the MLLM to generate attacker-controlled answers, with a success rate of up to 56% on MultiModalQA. Moreover, GPA completely disrupts model generation to 0% accuracy with just a single irrelevant knowledge injection. Our results highlight the urgent need for robust defenses against knowledge poisoning to safeguard multimodal RAG frameworks.

### A General Framework to Enhance Fine-tuning-based LLM Unlearning 
[[arxiv](https://arxiv.org/abs/2502.17823)] [[cool](https://papers.cool/arxiv/2502.17823)] [[pdf](https://arxiv.org/pdf/2502.17823)]
> **Authors**: Jie Ren,Zhenwei Dai,Xianfeng Tang,Hui Liu,Jingying Zeng,Zhen Li,Rahul Goutam,Suhang Wang,Yue Xing,Qi He,Hui Liu
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Unlearning has been proposed to remove copyrighted and privacy-sensitive data from Large Language Models (LLMs). Existing approaches primarily rely on fine-tuning-based methods, which can be categorized into gradient ascent-based (GA-based) and suppression-based methods. However, they often degrade model utility (the ability to respond to normal prompts). In this work, we aim to develop a general framework that enhances the utility of fine-tuning-based unlearning methods. To achieve this goal, we first investigate the common property between GA-based and suppression-based methods. We unveil that GA-based methods unlearn by distinguishing the target data (i.e., the data to be removed) and suppressing related generations, which is essentially the same strategy employed by suppression-based methods. Inspired by this finding, we introduce Gated Representation UNlearning (GRUN) which has two components: a soft gate function for distinguishing target data and a suppression module using Representation Fine-tuning (ReFT) to adjust representations rather than model parameters. Experiments show that GRUN significantly improves the unlearning and utility. Meanwhile, it is general for fine-tuning-based methods, efficient and promising for sequential unlearning.

### PVBF: A Framework for Mitigating Parameter Variation Imbalance in Online Continual Learning 
[[arxiv](https://arxiv.org/abs/2502.17794)] [[cool](https://papers.cool/arxiv/2502.17794)] [[pdf](https://arxiv.org/pdf/2502.17794)]
> **Authors**: Zelin Tao,Hao Deng,Mingqing Liu,Lijun Zhang,Shengjie Zhao
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 27 pages, 11 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Online continual learning (OCL), which enables AI systems to adaptively learn from non-stationary data streams, is commonly achieved using experience replay (ER)-based methods that retain knowledge by replaying stored past during training. However, these methods face challenges of prediction bias, stemming from deviations in parameter update directions during task transitions. This paper identifies parameter variation imbalance as a critical factor contributing to prediction bias in ER-based OCL. Specifically, using the proposed parameter variation evaluation method, we highlight two types of imbalance: correlation-induced imbalance, where certain parameters are disproportionately updated across tasks, and layer-wise imbalance, where output layer parameters update faster than those in preceding layers. To mitigate the above imbalances, we propose the Parameter Variation Balancing Framework (PVBF), which incorporates: 1) a novel method to compute parameter correlations with previous tasks based on parameter variations, 2) an encourage-and-consolidate (E&C) method utilizing parameter correlations to perform gradient adjustments across all parameters during training, 3) a dual-layer copy weights with reinit (D-CWR) strategy to slowly update output layer parameters for frequently occuring sample categories. Experiments on short and long task sequences demonstrate that PVBF significantly reduces prediction bias and improves OCL performance, achieving up to 47\% higher accuracy compared to existing ER-based methods.

### On-device edge learning for IoT data streams: a survey 
[[arxiv](https://arxiv.org/abs/2502.17788)] [[cool](https://papers.cool/arxiv/2502.17788)] [[pdf](https://arxiv.org/pdf/2502.17788)]
> **Authors**: Afonso Lourenço,João Rodrigo,João Gama,Goreti Marreiros
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: This literature review explores continual learning methods for on-device training in the context of neural networks (NNs) and decision trees (DTs) for classification tasks on smart environments. We highlight key constraints, such as data architecture (batch vs. stream) and network capacity (cloud vs. edge), which impact TinyML algorithm design, due to the uncontrolled natural arrival of data streams. The survey details the challenges of deploying deep learners on resource-constrained edge devices, including catastrophic forgetting, data inefficiency, and the difficulty of handling IoT tabular data in open-world settings. While decision trees are more memory-efficient for on-device training, they are limited in expressiveness, requiring dynamic adaptations, like pruning and meta-learning, to handle complex patterns and concept drifts. We emphasize the importance of multi-criteria performance evaluation tailored to edge applications, which assess both output-based and internal representation metrics. The key challenge lies in integrating these building blocks into autonomous online systems, taking into account stability-plasticity trade-offs, forward-backward transfer, and model convergence.

### MuCoS: Efficient Drug-Target Prediction through Multi-Context-Aware Sampling 
[[arxiv](https://arxiv.org/abs/2502.17784)] [[cool](https://papers.cool/arxiv/2502.17784)] [[pdf](https://arxiv.org/pdf/2502.17784)]
> **Authors**: Haji Gul,Abdul Gani Haji Naim,Ajaz A. Bhat
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Drug-target interactions are critical for understanding biological processes and advancing drug discovery. However, traditional methods such as ComplEx-SE, TransE, and DistMult struggle with unseen relationships and negative triplets, which limits their effectiveness in drug-target prediction. To address these challenges, we propose Multi-Context-Aware Sampling (MuCoS), an efficient and positively accurate method for drug-target prediction. MuCoS reduces computational complexity by prioritizing neighbors of higher density to capture informative structural patterns. These optimized neighborhood representations are integrated with BERT, enabling contextualized embeddings for accurate prediction of missing relationships or tail entities. MuCoS avoids the need for negative triplet sampling, reducing computation while improving performance over unseen entities and relations. Experiments on the KEGG50k biomedical dataset show that MuCoS improved over existing models by 13\% on MRR, 7\% on Hits@1, 4\% on Hits@3, and 18\% on Hits@10 for the general relationship, and by 6\% on MRR, 1\% on Hits@1, 3\% on Hits@3, and 12\% on Hits@10 for prediction of drug-target relationship.

### Adaptive Nesterov Accelerated Distributional Deep Hedging for Efficient Volatility Risk Management 
[[arxiv](https://arxiv.org/abs/2502.17777)] [[cool](https://papers.cool/arxiv/2502.17777)] [[pdf](https://arxiv.org/pdf/2502.17777)]
> **Authors**: Lei Zhao,Lin Cai,Wu-Sheng Lu
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算金融
- **Abstract**: In the field of financial derivatives trading, managing volatility risk is crucial for protecting investment portfolios from market changes. Traditional Vega hedging strategies, which often rely on basic and rule-based models, are hard to adapt well to rapidly changing market conditions. We introduce a new framework for dynamic Vega hedging, the Adaptive Nesterov Accelerated Distributional Deep Hedging (ANADDH), which combines distributional reinforcement learning with a tailored design based on adaptive Nesterov acceleration. This approach improves the learning process in complex financial environments by modeling the hedging efficiency distribution, providing a more accurate and responsive hedging strategy. The design of adaptive Nesterov acceleration refines gradient momentum adjustments, significantly enhancing the stability and speed of convergence of the model. Through empirical analysis and comparisons, our method demonstrates substantial performance gains over existing hedging techniques. Our results confirm that this innovative combination of distributional reinforcement learning with the proposed optimization techniques improves financial risk management and highlights the practical benefits of implementing advanced neural network architectures in the finance sector.

### An Improved Privacy and Utility Analysis of Differentially Private SGD with Bounded Domain and Smooth Losses 
[[arxiv](https://arxiv.org/abs/2502.17772)] [[cool](https://papers.cool/arxiv/2502.17772)] [[pdf](https://arxiv.org/pdf/2502.17772)]
> **Authors**: Hao Liang,Wanrong Zhang,Xinlei He,Kaishun He,Hong Xing
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 18 pages, 2 figures, submitted for possible publication
- **标题**: None
- **领域**: 机器学习,密码学和安全,机器学习
- **Abstract**: Differentially Private Stochastic Gradient Descent (DPSGD) is widely used to protect sensitive data during the training of machine learning models, but its privacy guarantees often come at the cost of model performance, largely due to the inherent challenge of accurately quantifying privacy loss. While recent efforts have strengthened privacy guarantees by focusing solely on the final output and bounded domain cases, they still impose restrictive assumptions, such as convexity and other parameter limitations, and often lack a thorough analysis of utility. In this paper, we provide rigorous privacy and utility characterization for DPSGD for smooth loss functions in both bounded and unbounded domains. We track the privacy loss over multiple iterations by exploiting the noisy smooth-reduction property and establish the utility analysis by leveraging the projection's non-expansiveness and clipped SGD properties. In particular, we show that for DPSGD with a bounded domain, (i) the privacy loss can still converge without the convexity assumption, and (ii) a smaller bounded diameter can improve both privacy and utility simultaneously under certain conditions. Numerical results validate our results.

### Sample Selection via Contrastive Fragmentation for Noisy Label Regression 
[[arxiv](https://arxiv.org/abs/2502.17771)] [[cool](https://papers.cool/arxiv/2502.17771)] [[pdf](https://arxiv.org/pdf/2502.17771)]
> **Authors**: Chris Dongjoo Kim,Sangwoo Moon,Jihwan Moon,Dongyeon Woo,Gunhee Kim
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: NeurIPS 2024
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: As with many other problems, real-world regression is plagued by the presence of noisy labels, an inevitable issue that demands our attention. Fortunately, much real-world data often exhibits an intrinsic property of continuously ordered correlations between labels and features, where data points with similar labels are also represented with closely related features. In response, we propose a novel approach named ConFrag, where we collectively model the regression data by transforming them into disjoint yet contrasting fragmentation pairs. This enables the training of more distinctive representations, enhancing the ability to select clean samples. Our ConFrag framework leverages a mixture of neighboring fragments to discern noisy labels through neighborhood agreement among expert feature extractors. We extensively perform experiments on six newly curated benchmark datasets of diverse domains, including age prediction, price prediction, and music production year estimation. We also introduce a metric called Error Residual Ratio (ERR) to better account for varying degrees of label noise. Our approach consistently outperforms fourteen state-of-the-art baselines, being robust against symmetric and random Gaussian label noise.

### DeepSeek vs. ChatGPT: A Comparative Study for Scientific Computing and Scientific Machine Learning Tasks 
[[arxiv](https://arxiv.org/abs/2502.17764)] [[cool](https://papers.cool/arxiv/2502.17764)] [[pdf](https://arxiv.org/pdf/2502.17764)]
> **Authors**: Qile Jiang,Zhiwei Gao,George Em Karniadakis
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Large Language Models (LLMs) have emerged as powerful tools for tackling a wide range of problems, including those in scientific computing, particularly in solving partial differential equations (PDEs). However, different models exhibit distinct strengths and preferences, resulting in varying levels of performance. In this paper, we compare the capabilities of the most advanced LLMs--ChatGPT and DeepSeek--along with their reasoning-optimized versions in addressing computational challenges. Specifically, we evaluate their proficiency in solving traditional numerical problems in scientific computing as well as leveraging scientific machine learning techniques for PDE-based problems. We designed all our experiments so that a non-trivial decision is required, e.g. defining the proper space of input functions for neural operator learning. Our findings reveal that the latest model, ChatGPT o3-mini-high, usually delivers the most accurate results while also responding significantly faster than its reasoning counterpart, DeepSeek R1. This enhanced speed and accuracy make ChatGPT o3-mini-high a more practical and efficient choice for diverse computational tasks at this juncture.

### Applications of deep reinforcement learning to urban transit network design 
[[arxiv](https://arxiv.org/abs/2502.17758)] [[cool](https://papers.cool/arxiv/2502.17758)] [[pdf](https://arxiv.org/pdf/2502.17758)]
> **Authors**: Andrew Holliday
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: This is a copy of my PhD thesis, which was successfully defended at McGill University in December of 2024. arXiv admin note: text overlap with arXiv:2404.05894
- **标题**: None
- **领域**: 机器学习
- **Abstract**: This thesis concerns the use of reinforcement learning to train neural networks to aid in the design of public transit networks. The Transit Network Design Problem (TNDP) is an optimization problem of considerable practical importance. Given a city with an existing road network and travel demands, the goal is to find a set of transit routes - each of which is a path through the graph - that collectively satisfy all demands, while minimizing a cost function that may depend both on passenger satisfaction and operating costs. The existing literature on this problem mainly considers metaheuristic optimization algorithms, such as genetic algorithms and ant-colony optimization. By contrast, we begin by taking a reinforcement learning approach, formulating the construction of a set of transit routes as a Markov Decision Process (MDP) and training a neural net policy to act as the agent in this MDP. We then show that, beyond using this policy to plan a transit network directly, it can be combined with existing metaheuristic algorithms, both to initialize the solution and to suggest promising moves at each step of a search through solution space. We find that such hybrid algorithms, which use a neural policy trained via reinforcement learning as a core component within a classical metaheuristic framework, can plan transit networks that are superior to those planned by either the neural policy or the metaheuristic algorithm. We demonstrate the utility of our approach by using it to redesign the transit network for the city of Laval, Quebec, and show that in simulation, the resulting transit network provides better service at lower cost than the existing transit network.

### Robust and Efficient Deep Hedging via Linearized Objective Neural Network 
[[arxiv](https://arxiv.org/abs/2502.17757)] [[cool](https://papers.cool/arxiv/2502.17757)] [[pdf](https://arxiv.org/pdf/2502.17757)]
> **Authors**: Lei Zhao,Lin Cai
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,风险管理
- **Abstract**: Deep hedging represents a cutting-edge approach to risk management for financial derivatives by leveraging the power of deep learning. However, existing methods often face challenges related to computational inefficiency, sensitivity to noisy data, and optimization complexity, limiting their practical applicability in dynamic and volatile markets. To address these limitations, we propose Deep Hedging with Linearized-objective Neural Network (DHLNN), a robust and generalizable framework that enhances the training procedure of deep learning models. By integrating a periodic fixed-gradient optimization method with linearized training dynamics, DHLNN stabilizes the training process, accelerates convergence, and improves robustness to noisy financial data. The framework incorporates trajectory-wide optimization and Black-Scholes Delta anchoring, ensuring alignment with established financial theory while maintaining flexibility to adapt to real-world market conditions. Extensive experiments on synthetic and real market data validate the effectiveness of DHLNN, demonstrating its ability to achieve faster convergence, improved stability, and superior hedging performance across diverse market scenarios.

### Graded Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.17751)] [[cool](https://papers.cool/arxiv/2502.17751)] [[pdf](https://arxiv.org/pdf/2502.17751)]
> **Authors**: Tony Shaska
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: :16W50; 13A02;ACM Class:I.2; I.2.6
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: This paper presents a novel framework for graded neural networks (GNNs) built over graded vector spaces $\V_\w^n$, extending classical neural architectures by incorporating algebraic grading. Leveraging a coordinate-wise grading structure with scalar action $λ\star \x = (λ^{q_i} x_i)$, defined by a tuple $\w = (q_0, \ldots, q_{n-1})$, we introduce graded neurons, layers, activation functions, and loss functions that adapt to feature significance. Theoretical properties of graded spaces are established, followed by a comprehensive GNN design, addressing computational challenges like numerical stability and gradient scaling. Potential applications span machine learning and photonic systems, exemplified by high-speed laser-based implementations. This work offers a foundational step toward graded computation, unifying mathematical rigor with practical potential, with avenues for future empirical and hardware exploration.

### FinP: Fairness-in-Privacy in Federated Learning by Addressing Disparities in Privacy Risk 
[[arxiv](https://arxiv.org/abs/2502.17748)] [[cool](https://papers.cool/arxiv/2502.17748)] [[pdf](https://arxiv.org/pdf/2502.17748)]
> **Authors**: Tianyu Zhao,Mahmoud Srewa,Salma Elmalaki
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,密码学和安全
- **Abstract**: Ensuring fairness in machine learning, particularly in human-centric applications, extends beyond algorithmic bias to encompass fairness in privacy, specifically the equitable distribution of privacy risk. This is critical in federated learning (FL), where decentralized data necessitates balanced privacy preservation across clients. We introduce FinP, a framework designed to achieve fairness in privacy by mitigating disproportionate exposure to source inference attacks (SIA). FinP employs a dual approach: (1) server-side adaptive aggregation to address unfairness in client contributions in global model, and (2) client-side regularization to reduce client vulnerability. This comprehensive strategy targets both the symptoms and root causes of privacy unfairness. Evaluated on the Human Activity Recognition (HAR) and CIFAR-10 datasets, FinP demonstrates ~20% improvement in fairness in privacy on HAR with minimal impact on model utility, and effectively mitigates SIA risks on CIFAR-10, showcasing its ability to provide fairness in privacy in FL systems without compromising performance.

### Phoeni6: a Systematic Approach for Evaluating the Energy Consumption of Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.17734)] [[cool](https://papers.cool/arxiv/2502.17734)] [[pdf](https://arxiv.org/pdf/2502.17734)]
> **Authors**: Antônio Oliveira-Filho,Wellington Silva-de-Souza,Carlos Alberto Valderrama Sakuyama,Samuel Xavier-de-Souza
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: The paper consists of 24 pages and 25 figures. It is currently under review at the journal Sustainable Computing: Informatics and Systems
- **标题**: None
- **领域**: 机器学习,软件工程
- **Abstract**: This paper presents Phoeni6, a systematic approach for assessing the energy consumption of neural networks while upholding the principles of fair comparison and reproducibility. Phoeni6 offers a comprehensive solution for managing energy-related data and configurations, ensuring portability, transparency, and coordination during evaluations. The methodology automates energy evaluations through containerized tools, robust database management, and versatile data models. In the first case study, the energy consumption of AlexNet and MobileNet was compared using raw and resized images. Results showed that MobileNet is up to 6.25% more energy-efficient for raw images and 2.32% for resized datasets, while maintaining competitive accuracy levels. In the second study, the impact of image file formats on energy consumption was evaluated. BMP images reduced energy usage by up to 30% compared to PNG, highlighting the influence of file formats on energy efficiency. These findings emphasize the importance of Phoeni6 in optimizing energy consumption for diverse neural network applications and establishing sustainable artificial intelligence practices.

### Aligning Compound AI Systems via System-level DPO 
[[arxiv](https://arxiv.org/abs/2502.17721)] [[cool](https://papers.cool/arxiv/2502.17721)] [[pdf](https://arxiv.org/pdf/2502.17721)]
> **Authors**: Xiangwen Wang,Yibo Jacky Zhang,Zhoujie Ding,Katherine Tsai,Sanmi Koyejo
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Accepted to workshops MARW and WMAC (Oral) at AAAI25
- **标题**: None
- **领域**: 机器学习,人工智能,多代理系统
- **Abstract**: Compound AI systems, comprising multiple interacting components such as LLM agents and external tools, demonstrate state-of-the-art results across diverse tasks. It is hence crucial to align components within the system to produce consistent results that match human expectations. However, conventional alignment methods, such as Direct Preference Optimization (DPO), are not directly applicable to compound AI systems. These challenges include the non-differentiable interactions between components, making end-to-end gradient optimization infeasible. Additionally, system-level preferences cannot be directly translated into component-level preferences, further complicating alignment. We address the issues by formulating compound AI systems as Directed Acyclic Graphs (DAGs), capturing the connections between agents and the data generation processes. We propose a system-level DPO (SysDPO) to jointly align compound systems by adapting the DPO to operate on these DAGs. We study the joint alignment of an LLM and a diffusion model to demonstrate the effectiveness of our approach. Our exploration provides insights into the alignment of compound AI systems and lays a foundation for future advancements.

### Learning Backbones: Sparsifying Graphs through Zero Forcing for Effective Graph-Based Learning 
[[arxiv](https://arxiv.org/abs/2502.17713)] [[cool](https://papers.cool/arxiv/2502.17713)] [[pdf](https://arxiv.org/pdf/2502.17713)]
> **Authors**: Obaid Ullah Ahmad,Anwar Said,Mudassir Shabbir,Xenofon Koutsoukos,Waseem Abbas
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 13th International Conference on Complex Networks and their Applications
- **标题**: None
- **领域**: 机器学习,多代理系统,系统与控制
- **Abstract**: This paper introduces a novel framework for graph sparsification that preserves the essential learning attributes of original graphs, improving computational efficiency and reducing complexity in learning algorithms. We refer to these sparse graphs as "learning backbones". Our approach leverages the zero-forcing (ZF) phenomenon, a dynamic process on graphs with applications in network control. The key idea is to generate a tree from the original graph that retains critical dynamical properties. By correlating these properties with learning attributes, we construct effective learning backbones. We evaluate the performance of our ZF-based backbones in graph classification tasks across eight datasets and six baseline models. The results demonstrate that our method outperforms existing techniques. Additionally, we explore extensions using node distance metrics to further enhance the framework's utility.

### Robust Federated Learning with Global Sensitivity Estimation for Financial Risk Management 
[[arxiv](https://arxiv.org/abs/2502.17694)] [[cool](https://papers.cool/arxiv/2502.17694)] [[pdf](https://arxiv.org/pdf/2502.17694)]
> **Authors**: Lei Zhao,Lin Cai,Wu-Sheng Lu
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,分布式、并行和集群计算
- **Abstract**: In decentralized financial systems, robust and efficient Federated Learning (FL) is promising to handle diverse client environments and ensure resilience to systemic risks. We propose Federated Risk-Aware Learning with Central Sensitivity Estimation (FRAL-CSE), an innovative FL framework designed to enhance scalability, stability, and robustness in collaborative financial decision-making. The framework's core innovation lies in a central acceleration mechanism, guided by a quadratic sensitivity-based approximation of global model dynamics. By leveraging local sensitivity information derived from robust risk measurements, FRAL-CSE performs a curvature-informed global update that efficiently incorporates second-order information without requiring repeated local re-evaluations, thereby enhancing training efficiency and improving optimization stability. Additionally, distortion risk measures are embedded into the training objectives to capture tail risks and ensure robustness against extreme scenarios. Extensive experiments validate the effectiveness of FRAL-CSE in accelerating convergence and improving resilience across heterogeneous datasets compared to state-of-the-art baselines.

### Predictive Response Optimization: Using Reinforcement Learning to Fight Online Social Network Abuse 
[[arxiv](https://arxiv.org/abs/2502.17693)] [[cool](https://papers.cool/arxiv/2502.17693)] [[pdf](https://arxiv.org/pdf/2502.17693)]
> **Authors**: Garrett Wilson,Geoffrey Goh,Yan Jiang,Ajay Gupta,Jiaxuan Wang,David Freeman,Francesco Dinuzzo
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: To appear in USENIX Security 2025
- **标题**: None
- **领域**: 机器学习,密码学和安全,社交和信息网络
- **Abstract**: Detecting phishing, spam, fake accounts, data scraping, and other malicious activity in online social networks (OSNs) is a problem that has been studied for well over a decade, with a number of important results. Nearly all existing works on abuse detection have as their goal producing the best possible binary classifier; i.e., one that labels unseen examples as "benign" or "malicious" with high precision and recall. However, no prior published work considers what comes next: what does the service actually do after it detects abuse? In this paper, we argue that detection as described in previous work is not the goal of those who are fighting OSN abuse. Rather, we believe the goal to be selecting actions (e.g., ban the user, block the request, show a CAPTCHA, or "collect more evidence") that optimize a tradeoff between harm caused by abuse and impact on benign users. With this framing, we see that enlarging the set of possible actions allows us to move the Pareto frontier in a way that is unattainable by simply tuning the threshold of a binary classifier. To demonstrate the potential of our approach, we present Predictive Response Optimization (PRO), a system based on reinforcement learning that utilizes available contextual information to predict future abuse and user-experience metrics conditioned on each possible action, and select actions that optimize a multi-dimensional tradeoff between abuse/harm and impact on user experience. We deployed versions of PRO targeted at stopping automated activity on Instagram and Facebook. In both cases our experiments showed that PRO outperforms a baseline classification system, reducing abuse volume by 59% and 4.5% (respectively) with no negative impact to users. We also present several case studies that demonstrate how PRO can quickly and automatically adapt to changes in business constraints, system behavior, and/or adversarial tactics.

### Yes, Q-learning Helps Offline In-Context RL 
[[arxiv](https://arxiv.org/abs/2502.17666)] [[cool](https://papers.cool/arxiv/2502.17666)] [[pdf](https://arxiv.org/pdf/2502.17666)]
> **Authors**: Denis Tarasov,Alexander Nikulin,Ilya Zisman,Albina Klepach,Andrei Polubarov,Nikita Lyubaykin,Alexander Derevyagin,Igor Kiselev,Vladislav Kurenkov
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: In this work, we explore the integration of Reinforcement Learning (RL) approaches within a scalable offline In-Context RL (ICRL) framework. Through experiments across more than 150 datasets derived from GridWorld and MuJoCo environments, we demonstrate that optimizing RL objectives improves performance by approximately 40% on average compared to the widely established Algorithm Distillation (AD) baseline across various dataset coverages, structures, expertise levels, and environmental complexities. Our results also reveal that offline RL-based methods outperform online approaches, which are not specifically designed for offline scenarios. These findings underscore the importance of aligning the learning objectives with RL's reward-maximization goal and demonstrate that offline RL is a promising direction for application in ICRL settings.

### Architecting Digital Twins for Intelligent Transportation Systems 
[[arxiv](https://arxiv.org/abs/2502.17646)] [[cool](https://papers.cool/arxiv/2502.17646)] [[pdf](https://arxiv.org/pdf/2502.17646)]
> **Authors**: Hiya Bhatt,Sahil,Karthik Vaidhyanathan,Rahul Biju,Deepak Gangadharan,Ramona Trestian,Purav Shah
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,软件工程
- **Abstract**: Modern transportation systems face growing challenges in managing traffic flow, ensuring safety, and maintaining operational efficiency amid dynamic traffic patterns. Addressing these challenges requires intelligent solutions capable of real-time monitoring, predictive analytics, and adaptive control. This paper proposes an architecture for DigIT, a Digital Twin (DT) platform for Intelligent Transportation Systems (ITS), designed to overcome the limitations of existing frameworks by offering a modular and scalable solution for traffic management. Built on a Domain Concept Model (DCM), the architecture systematically models key ITS components enabling seamless integration of predictive modeling and simulations. The architecture leverages machine learning models to forecast traffic patterns based on historical and real-time data. To adapt to evolving traffic patterns, the architecture incorporates adaptive Machine Learning Operations (MLOps), automating the deployment and lifecycle management of predictive models. Evaluation results highlight the effectiveness of the architecture in delivering accurate predictions and computational efficiency.

### The Power of Graph Signal Processing for Chip Placement Acceleration 
[[arxiv](https://arxiv.org/abs/2502.17632)] [[cool](https://papers.cool/arxiv/2502.17632)] [[pdf](https://arxiv.org/pdf/2502.17632)]
> **Authors**: Yiting Liu,Hai Zhou,Jia Wang,Fan Yang,Xuan Zeng,Li Shang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: ICCAD'24 conference
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Placement is a critical task with high computation complexity in VLSI physical design. Modern analytical placers formulate the placement objective as a nonlinear optimization task, which suffers a long iteration time. To accelerate and enhance the placement process, recent studies have turned to deep learning-based approaches, particularly leveraging graph convolution networks (GCNs). However, learning-based placers require time- and data-consuming model training due to the complexity of circuit placement that involves large-scale cells and design-specific graph statistics. This paper proposes GiFt, a parameter-free technique for accelerating placement, rooted in graph signal processing. GiFt excels at capturing multi-resolution smooth signals of circuit graphs to generate optimized placement solutions without the need for time-consuming model training, and meanwhile significantly reduces the number of iterations required by analytical placers. Experimental results show that GiFt significantly improving placement efficiency, while achieving competitive or superior performance compared to state-of-the-art placers. In particular, compared to DREAMPlace, the recently proposed GPU-accelerated analytical placer, GF-Placer improves total runtime over 45%.

### Instance-Dependent Regret Bounds for Learning Two-Player Zero-Sum Games with Bandit Feedback 
[[arxiv](https://arxiv.org/abs/2502.17625)] [[cool](https://papers.cool/arxiv/2502.17625)] [[pdf](https://arxiv.org/pdf/2502.17625)]
> **Authors**: Shinji Ito,Haipeng Luo,Taira Tsuchiya,Yue Wu
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: :91A26 (Primary) 68T05; 68Q32 (Secondary)ACM Class:F.2.0; I.2.11; G.3
- **标题**: None
- **领域**: 机器学习,计算机科学与博弈论
- **Abstract**: No-regret self-play learning dynamics have become one of the premier ways to solve large-scale games in practice. Accelerating their convergence via improving the regret of the players over the naive $O(\sqrt{T})$ bound after $T$ rounds has been extensively studied in recent years, but almost all studies assume access to exact gradient feedback. We address the question of whether acceleration is possible under bandit feedback only and provide an affirmative answer for two-player zero-sum normal-form games. Specifically, we show that if both players apply the Tsallis-INF algorithm of Zimmert and Seldin (2018, arXiv:1807.07623), then their regret is at most $O(c_1 \log T + \sqrt{c_2 T})$, where $c_1$ and $c_2$ are game-dependent constants that characterize the difficulty of learning -- $c_1$ resembles the complexity of learning a stochastic multi-armed bandit instance and depends inversely on some gap measures, while $c_2$ can be much smaller than the number of actions when the Nash equilibria have a small support or are close to the boundary. In particular, for the case when a pure strategy Nash equilibrium exists, $c_2$ becomes zero, leading to an optimal instance-dependent regret bound as we show. We additionally prove that in this case, our algorithm also enjoys last-iterate convergence and can identify the pure strategy Nash equilibrium with near-optimal sample complexity.

### Hierarchical Imitation Learning of Team Behavior from Heterogeneous Demonstrations 
[[arxiv](https://arxiv.org/abs/2502.17618)] [[cool](https://papers.cool/arxiv/2502.17618)] [[pdf](https://arxiv.org/pdf/2502.17618)]
> **Authors**: Sangwon Seo,Vaibhav Unhelkar
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Extended version of an identically-titled paper accepted at AAMAS 2025
- **标题**: None
- **领域**: 机器学习,人工智能,多代理系统
- **Abstract**: Successful collaboration requires team members to stay aligned, especially in complex sequential tasks. Team members must dynamically coordinate which subtasks to perform and in what order. However, real-world constraints like partial observability and limited communication bandwidth often lead to suboptimal collaboration. Even among expert teams, the same task can be executed in multiple ways. To develop multi-agent systems and human-AI teams for such tasks, we are interested in data-driven learning of multimodal team behaviors. Multi-Agent Imitation Learning (MAIL) provides a promising framework for data-driven learning of team behavior from demonstrations, but existing methods struggle with heterogeneous demonstrations, as they assume that all demonstrations originate from a single team policy. Hence, in this work, we introduce DTIL: a hierarchical MAIL algorithm designed to learn multimodal team behaviors in complex sequential tasks. DTIL represents each team member with a hierarchical policy and learns these policies from heterogeneous team demonstrations in a factored manner. By employing a distribution-matching approach, DTIL mitigates compounding errors and scales effectively to long horizons and continuous state representations. Experimental results show that DTIL outperforms MAIL baselines and accurately models team behavior across a variety of collaborative scenarios.

### Provable Model-Parallel Distributed Principal Component Analysis with Parallel Deflation 
[[arxiv](https://arxiv.org/abs/2502.17615)] [[cool](https://papers.cool/arxiv/2502.17615)] [[pdf](https://arxiv.org/pdf/2502.17615)]
> **Authors**: Fangshuo Liao,Wenyi Su,Anastasios Kyrillidis
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: CPAL 2025
- **标题**: None
- **领域**: 机器学习,分布式、并行和集群计算,优化与控制
- **Abstract**: We study a distributed Principal Component Analysis (PCA) framework where each worker targets a distinct eigenvector and refines its solution by updating from intermediate solutions provided by peers deemed as "superior". Drawing intuition from the deflation method in centralized eigenvalue problems, our approach breaks the sequential dependency in the deflation steps and allows asynchronous updates of workers, while incurring only a small communication cost. To our knowledge, a gap in the literature -- the theoretical underpinning of such distributed, dynamic interactions among workers -- has remained unaddressed. This paper offers a theoretical analysis explaining why, how, and when these intermediate, hierarchical updates lead to practical and provable convergence in distributed environments. Despite being a theoretical work, our prototype implementation demonstrates that such a distributed PCA algorithm converges effectively and in scalable way: through experiments, our proposed framework offers comparable performance to EigenGame-$μ$, the state-of-the-art model-parallel PCA solver.

### Scalable Graph Condensation with Evolving Capabilities 
[[arxiv](https://arxiv.org/abs/2502.17614)] [[cool](https://papers.cool/arxiv/2502.17614)] [[pdf](https://arxiv.org/pdf/2502.17614)]
> **Authors**: Shengbo Gong,Mohammad Hashemi,Juntong Ni,Carl Yang,Wei Jin
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 16 pages, 6 figures
- **标题**: None
- **领域**: 机器学习,社交和信息网络
- **Abstract**: Graph data has become a pivotal modality due to its unique ability to model relational datasets. However, real-world graph data continues to grow exponentially, resulting in a quadratic increase in the complexity of most graph algorithms as graph sizes expand. Although graph condensation (GC) methods have been proposed to address these scalability issues, existing approaches often treat the training set as static, overlooking the evolving nature of real-world graph data. This limitation leads to inefficiencies when condensing growing training sets. In this paper, we introduce GECC (Graph Evolving Clustering Condensation), a scalable graph condensation method designed to handle large-scale and evolving graph data. GECC employs a traceable and efficient approach by performing class-wise clustering on aggregated features. Furthermore, it can inherits previous condensation results as clustering centroids when the condensed graph expands, thereby attaining an evolving capability. This methodology is supported by robust theoretical foundations and demonstrates superior empirical performance. Comprehensive experiments show that GECC achieves better performance than most state-of-the-art graph condensation methods while delivering an around 1,000x speedup on large datasets.

### Flexible Counterfactual Explanations with Generative Models 
[[arxiv](https://arxiv.org/abs/2502.17613)] [[cool](https://papers.cool/arxiv/2502.17613)] [[pdf](https://arxiv.org/pdf/2502.17613)]
> **Authors**: Stig Hellemans,Andres Algaba,Sam Verboven,Vincent Ginis
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 28 pages, 13 figures
- **标题**: None
- **领域**: 机器学习,人工智能,方法论
- **Abstract**: Counterfactual explanations provide actionable insights to achieve desired outcomes by suggesting minimal changes to input features. However, existing methods rely on fixed sets of mutable features, which makes counterfactual explanations inflexible for users with heterogeneous real-world constraints. Here, we introduce Flexible Counterfactual Explanations, a framework incorporating counterfactual templates, which allows users to dynamically specify mutable features at inference time. In our implementation, we use Generative Adversarial Networks (FCEGAN), which align explanations with user-defined constraints without requiring model retraining or additional optimization. Furthermore, FCEGAN is designed for black-box scenarios, leveraging historical prediction datasets to generate explanations without direct access to model internals. Experiments across economic and healthcare datasets demonstrate that FCEGAN significantly improves counterfactual explanations' validity compared to traditional benchmark methods. By integrating user-driven flexibility and black-box compatibility, counterfactual templates support personalized explanations tailored to user constraints.

### Synthetic Text Generation for Training Large Language Models via Gradient Matching 
[[arxiv](https://arxiv.org/abs/2502.17607)] [[cool](https://papers.cool/arxiv/2502.17607)] [[pdf](https://arxiv.org/pdf/2502.17607)]
> **Authors**: Dang Nguyen,Zeman Li,Mohammadhossein Bateni,Vahab Mirrokni,Meisam Razaviyayn,Baharan Mirzasoleiman
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 15 pages, 5 figures, 4 tables
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Synthetic data has the potential to improve the performance, training efficiency, and privacy of real training examples. Nevertheless, existing approaches for synthetic text generation are mostly heuristics and cannot generate human-readable text without compromising the privacy of real data or provide performance guarantees for training Large Language Models (LLMs). In this work, we propose the first theoretically rigorous approach for generating synthetic human-readable text that guarantees the convergence and performance of LLMs during fine-tuning on a target task. To do so, we leverage Alternating Direction Method of Multipliers (ADMM) that iteratively optimizes the embeddings of synthetic examples to match the gradient of the target training or validation data, and maps them to a sequence of text tokens with low perplexity. In doing so, the generated synthetic text can guarantee convergence of the model to a close neighborhood of the solution obtained by fine-tuning on real data. Experiments on various classification tasks confirm the effectiveness of our proposed approach.

### Hallucination Detection in LLMs Using Spectral Features of Attention Maps 
[[arxiv](https://arxiv.org/abs/2502.17598)] [[cool](https://papers.cool/arxiv/2502.17598)] [[pdf](https://arxiv.org/pdf/2502.17598)]
> **Authors**: Jakub Binkowski,Denis Janiak,Albert Sawczyn,Bogdan Gabrys,Tomasz Kajdanowicz
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Preprint, under review
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Large Language Models (LLMs) have demonstrated remarkable performance across various tasks but remain prone to hallucinations. Detecting hallucinations is essential for safety-critical applications, and recent methods leverage attention map properties to this end, though their effectiveness remains limited. In this work, we investigate the spectral features of attention maps by interpreting them as adjacency matrices of graph structures. We propose the $\text{LapEigvals}$ method, which utilises the top-$k$ eigenvalues of the Laplacian matrix derived from the attention maps as an input to hallucination detection probes. Empirical evaluations demonstrate that our approach achieves state-of-the-art hallucination detection performance among attention-based methods. Extensive ablation studies further highlight the robustness and generalisation of $\text{LapEigvals}$, paving the way for future advancements in the hallucination detection domain.

### Training a Generally Curious Agent 
[[arxiv](https://arxiv.org/abs/2502.17543)] [[cool](https://papers.cool/arxiv/2502.17543)] [[pdf](https://arxiv.org/pdf/2502.17543)]
> **Authors**: Fahim Tajwar,Yiding Jiang,Abitha Thankaraj,Sumaita Sadia Rahman,J Zico Kolter,Jeff Schneider,Ruslan Salakhutdinov
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Efficient exploration is essential for intelligent systems interacting with their environment, but existing language models often fall short in scenarios that require strategic information gathering. In this paper, we present PAPRIKA, a fine-tuning approach that enables language models to develop general decision-making capabilities that are not confined to particular environments. By training on synthetic interaction data from different tasks that require diverse strategies, PAPRIKA teaches models to explore and adapt their behavior on a new task based on environment feedback in-context without more gradient updates. Experimental results show that models fine-tuned with PAPRIKA can effectively transfer their learned decision-making capabilities to entirely unseen tasks without additional training. Unlike traditional training, our approach's primary bottleneck lies in sampling useful interaction data instead of model updates. To improve sample efficiency, we propose a curriculum learning strategy that prioritizes sampling trajectories from tasks with high learning potential. These results suggest a promising path towards AI systems that can autonomously solve novel sequential decision-making problems that require interactions with the external world.

### On the Vulnerability of Concept Erasure in Diffusion Models 
[[arxiv](https://arxiv.org/abs/2502.17537)] [[cool](https://papers.cool/arxiv/2502.17537)] [[pdf](https://arxiv.org/pdf/2502.17537)]
> **Authors**: Lucas Beerens,Alex D. Richardson,Kaicheng Zhang,Dongdong Chen
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,密码学和安全
- **Abstract**: The proliferation of text-to-image diffusion models has raised significant privacy and security concerns, particularly regarding the generation of copyrighted or harmful images. To address these issues, research on machine unlearning has developed various concept erasure methods, which aim to remove the effect of unwanted data through post-hoc training. However, we show these erasure techniques are vulnerable, where images of supposedly erased concepts can still be generated using adversarially crafted prompts. We introduce RECORD, a coordinate-descent-based algorithm that discovers prompts capable of eliciting the generation of erased content. We demonstrate that RECORD significantly beats the attack success rate of current state-of-the-art attack methods. Furthermore, our findings reveal that models subjected to concept erasure are more susceptible to adversarial attacks than previously anticipated, highlighting the urgency for more robust unlearning approaches. We open source all our code at https://github.com/LucasBeerens/RECORD

### The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve? 
[[arxiv](https://arxiv.org/abs/2502.17535)] [[cool](https://papers.cool/arxiv/2502.17535)] [[pdf](https://arxiv.org/pdf/2502.17535)]
> **Authors**: Zhenheng Tang,Xiang Liu,Qian Wang,Peijie Dong,Bingsheng He,Xiaowen Chu,Bo Li
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,形式语言和自动机理论
- **Abstract**: Motivated by reducing the computational and storage costs of LLMs, model compression and KV cache compression have attracted much attention from researchers. However, current methods predominantly emphasize maintaining the performance of compressed LLMs, as measured by perplexity or simple accuracy on tasks of common sense knowledge QA and basic arithmetic reasoning. In this blog, we present a brief review of recent advancements in LLMs related to retrieval-augmented generation, multi-step reasoning, external tools, and computational expressivity, all of which substantially enhance LLM performance. Then, we propose a lottery LLM hypothesis suggesting that for a given LLM and task, there exists a smaller lottery LLM capable of producing the same performance as the original LLM with the assistance of multi-step reasoning and external tools. Based on the review of current progress in LLMs, we discuss and summarize the essential capabilities that the lottery LLM and KV cache compression must possess, which are currently overlooked in existing methods.

### FedSV: Byzantine-Robust Federated Learning via Shapley Value 
[[arxiv](https://arxiv.org/abs/2502.17526)] [[cool](https://papers.cool/arxiv/2502.17526)] [[pdf](https://arxiv.org/pdf/2502.17526)]
> **Authors**: Khaoula Otmani,Rachid Elazouzi,Vincent Labatut
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: ef:IEEE International Conference on Communications, Jun 2024, Denver (CO), United States. pp.4620-4625
- **标题**: None
- **领域**: 机器学习,密码学和安全,计算机科学与博弈论
- **Abstract**: In Federated Learning (FL), several clients jointly learn a machine learning model: each client maintains a local model for its local learning dataset, while a master server maintains a global model by aggregating the local models of the client devices. However, the repetitive communication between server and clients leaves room for attacks aimed at compromising the integrity of the global model, causing errors in its targeted predictions. In response to such threats on FL, various defense measures have been proposed in the literature. In this paper, we present a powerful defense against malicious clients in FL, called FedSV, using the Shapley Value (SV), which has been proposed recently to measure user contribution in FL by computing the marginal increase of average accuracy of the model due to the addition of local data of a user. Our approach makes the identification of malicious clients more robust, since during the learning phase, it estimates the contribution of each client according to the different groups to which the target client belongs. FedSV's effectiveness is demonstrated by extensive experiments on MNIST datasets in a cross-silo context under various attacks.

### UNCA: A Neutrosophic-Based Framework for Robust Clustering and Enhanced Data Interpretation 
[[arxiv](https://arxiv.org/abs/2502.17523)] [[cool](https://papers.cool/arxiv/2502.17523)] [[pdf](https://arxiv.org/pdf/2502.17523)]
> **Authors**: D. Dhinakaran,S. Edwin Raja,S. Gopalakrishnan,D. Selvaraj,S. D. Lalitha
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-25
> **comment**: 17 pages, 8 Figures, 1 Table
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Accurately representing the complex linkages and inherent uncertainties included in huge datasets is still a major difficulty in the field of data clustering. We address these issues with our proposed Unified Neutrosophic Clustering Algorithm (UNCA), which combines a multifaceted strategy with Neutrosophic logic to improve clustering performance. UNCA starts with a full-fledged similarity examination via a λ-cutting matrix that filters meaningful relationships between each two points of data. Then, we initialize centroids for Neutrosophic K-Means clustering, where the membership values are based on their degrees of truth, indeterminacy and falsity. The algorithm then integrates with a dynamic network visualization and MST (Minimum Spanning Tree) so that a visual interpretation of the relationships between the clusters can be clearly represented. UNCA employs SingleValued Neutrosophic Sets (SVNSs) to refine cluster assignments, and after fuzzifying similarity measures, guarantees a precise clustering result. The final step involves solidifying the clustering results through defuzzification methods, offering definitive cluster assignments. According to the performance evaluation results, UNCA outperforms conventional approaches in several metrics: it achieved a Silhouette Score of 0.89 on the Iris Dataset, a Davies-Bouldin Index of 0.59 on the Wine Dataset, an Adjusted Rand Index (ARI) of 0.76 on the Digits Dataset, and a Normalized Mutual Information (NMI) of 0.80 on the Customer Segmentation Dataset. These results demonstrate how UNCA enhances interpretability and resilience in addition to improving clustering accuracy when contrasted with Fuzzy C-Means (FCM), Neutrosophic C-Means (NCM), as well as Kernel Neutrosophic C-Means (KNCM). This makes UNCA a useful tool for complex data processing tasks

### Spectral Theory for Edge Pruning in Asynchronous Recurrent Graph Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.17522)] [[cool](https://papers.cool/arxiv/2502.17522)] [[pdf](https://arxiv.org/pdf/2502.17522)]
> **Authors**: Nicolas Bessone
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Graph Neural Networks (GNNs) have emerged as a powerful tool for learning on graph-structured data, finding applications in numerous domains including social network analysis and molecular biology. Within this broad category, Asynchronous Recurrent Graph Neural Networks (ARGNNs) stand out for their ability to capture complex dependencies in dynamic graphs, resembling living organisms' intricate and adaptive nature. However, their complexity often leads to large and computationally expensive models. Therefore, pruning unnecessary edges becomes crucial for enhancing efficiency without significantly compromising performance. This paper presents a dynamic pruning method based on graph spectral theory, leveraging the imaginary component of the eigenvalues of the network graph's Laplacian.

### Recent Advances in Large Langauge Model Benchmarks against Data Contamination: From Static to Dynamic Evaluation 
[[arxiv](https://arxiv.org/abs/2502.17521)] [[cool](https://papers.cool/arxiv/2502.17521)] [[pdf](https://arxiv.org/pdf/2502.17521)]
> **Authors**: Simin Chen,Yiming Chen,Zexin Li,Yifan Jiang,Zhongwei Wan,Yixin He,Dezhi Ran,Tianle Gu,Haizhou Li,Tao Xie,Baishakhi Ray
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-25
> **comment**: Github Link: https://github.com/SeekingDream/Static-to-Dynamic-LLMEval
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Data contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking. In this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap-the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks. This survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts. We maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link.

### On Neural Inertial Classification Networks for Pedestrian Activity Recognition 
[[arxiv](https://arxiv.org/abs/2502.17520)] [[cool](https://papers.cool/arxiv/2502.17520)] [[pdf](https://arxiv.org/pdf/2502.17520)]
> **Authors**: Zeev Yampolsky,Ofir Kruzel,Victoria Khalfin Fekson,Itzik Klein
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-25
> **comment**: arXiv admin note: substantial text overlap with arXiv:2501.01327
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别,信号处理
- **Abstract**: Inertial sensors are crucial for recognizing pedestrian activity. Recent advances in deep learning have greatly improved inertial sensing performance and robustness. Different domains and platforms use deep-learning techniques to enhance network performance, but there is no common benchmark. The latter is crucial for fair comparison and evaluation within a standardized framework. The aim of this paper is to fill this gap by defining and analyzing ten data-driven techniques for improving neural inertial classification networks. In order to accomplish this, we focused on three aspects of neural networks: network architecture, data augmentation, and data preprocessing. The experiments were conducted across four datasets collected from 78 participants. In total, over 936 minutes of inertial data sampled between 50-200Hz were analyzed. Data augmentation through rotation and multi-head architecture consistently yields the most significant improvements. Additionally, this study outlines benchmarking strategies for enhancing neural inertial classification networks.

### Ensemble RL through Classifier Models: Enhancing Risk-Return Trade-offs in Trading Strategies 
[[arxiv](https://arxiv.org/abs/2502.17518)] [[cool](https://papers.cool/arxiv/2502.17518)] [[pdf](https://arxiv.org/pdf/2502.17518)]
> **Authors**: Zheli Xiong
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-25
> **comment**: 16 pages,5 figures, 1 table
- **标题**: None
- **领域**: 机器学习,人工智能,计算金融,机器学习
- **Abstract**: This paper presents a comprehensive study on the use of ensemble Reinforcement Learning (RL) models in financial trading strategies, leveraging classifier models to enhance performance. By combining RL algorithms such as A2C, PPO, and SAC with traditional classifiers like Support Vector Machines (SVM), Decision Trees, and Logistic Regression, we investigate how different classifier groups can be integrated to improve risk-return trade-offs. The study evaluates the effectiveness of various ensemble methods, comparing them with individual RL models across key financial metrics, including Cumulative Returns, Sharpe Ratios (SR), Calmar Ratios, and Maximum Drawdown (MDD). Our results demonstrate that ensemble methods consistently outperform base models in terms of risk-adjusted returns, providing better management of drawdowns and overall stability. However, we identify the sensitivity of ensemble performance to the choice of variance threshold τ, highlighting the importance of dynamic τ adjustment to achieve optimal performance. This study emphasizes the value of combining RL with classifiers for adaptive decision-making, with implications for financial trading, robotics, and other dynamic environments.

### A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models 
[[arxiv](https://arxiv.org/abs/2502.17516)] [[cool](https://papers.cool/arxiv/2502.17516)] [[pdf](https://arxiv.org/pdf/2502.17516)]
> **Authors**: Zihao Lin,Samyadeep Basu,Mohammad Beigi,Varun Manjunatha,Ryan A. Rossi,Zichao Wang,Yufan Zhou,Sriram Balasubramanian,Arman Zarei,Keivan Rezaei,Ying Shen,Barry Menglong Yao,Zhiyang Xu,Qin Liu,Yuxiang Zhang,Yan Sun,Shilong Liu,Li Shen,Hongxuan Li,Soheil Feizi,Lifu Huang
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-25
> **comment**: 30 pages, 4 Figures, 10 Tables
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: The rise of foundation models has transformed machine learning research, prompting efforts to uncover their inner workings and develop more efficient and reliable applications for better control. While significant progress has been made in interpreting Large Language Models (LLMs), multimodal foundation models (MMFMs) - such as contrastive vision-language models, generative vision-language models, and text-to-image models - pose unique interpretability challenges beyond unimodal frameworks. Despite initial studies, a substantial gap remains between the interpretability of LLMs and MMFMs. This survey explores two key aspects: (1) the adaptation of LLM interpretability methods to multimodal models and (2) understanding the mechanistic differences between unimodal language models and crossmodal systems. By systematically reviewing current MMFM analysis techniques, we propose a structured taxonomy of interpretability methods, compare insights across unimodal and multimodal architectures, and highlight critical research gaps.

### Towards User-level Private Reinforcement Learning with Human Feedback 
[[arxiv](https://arxiv.org/abs/2502.17515)] [[cool](https://papers.cool/arxiv/2502.17515)] [[pdf](https://arxiv.org/pdf/2502.17515)]
> **Authors**: Jiaming Zhang,Mingxi Lei,Meng Ding,Mengdi Li,Zihang Xiang,Difei Xu,Jinhui Xu,Di Wang
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Reinforcement Learning with Human Feedback (RLHF) has emerged as an influential technique, enabling the alignment of large language models (LLMs) with human preferences. Despite the promising potential of RLHF, how to protect user preference privacy has become a crucial issue. Most previous work has focused on using differential privacy (DP) to protect the privacy of individual data. However, they have concentrated primarily on item-level privacy protection and have unsatisfactory performance for user-level privacy, which is more common in RLHF. This study proposes a novel framework, AUP-RLHF, which integrates user-level label DP into RLHF. We first show that the classical random response algorithm, which achieves an acceptable performance in item-level privacy, leads to suboptimal utility when in the user-level settings. We then establish a lower bound for the user-level label DP-RLHF and develop the AUP-RLHF algorithm, which guarantees $(\varepsilon, δ)$ user-level privacy and achieves an improved estimation error. Experimental results show that AUP-RLHF outperforms existing baseline methods in sentiment generation and summarization tasks, achieving a better privacy-utility trade-off.

### SAE-V: Interpreting Multimodal Models for Enhanced Alignment 
[[arxiv](https://arxiv.org/abs/2502.17514)] [[cool](https://papers.cool/arxiv/2502.17514)] [[pdf](https://arxiv.org/pdf/2502.17514)]
> **Authors**: Hantao Lou,Changye Li,Jiaming Ji,Yaodong Yang
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: With the integration of image modality, the semantic space of multimodal large language models (MLLMs) is more complex than text-only models, making their interpretability more challenging and their alignment less stable, particularly susceptible to low-quality data, which can lead to inconsistencies between modalities, hallucinations, and biased outputs. As a result, developing interpretability methods for MLLMs is crucial for improving alignment quality and efficiency. In text-only LLMs, Sparse Autoencoders (SAEs) have gained attention for their ability to interpret latent representations. However, extending SAEs to multimodal settings presents new challenges due to modality fusion and the difficulty of isolating cross-modal representations. To address these challenges, we introduce SAE-V, a mechanistic interpretability framework that extends the SAE paradigm to MLLMs. By identifying and analyzing interpretable features along with their corresponding data, SAE-V enables fine-grained interpretation of both model behavior and data quality, facilitating a deeper understanding of cross-modal interactions and alignment dynamics. Moreover, by utilizing cross-modal feature weighting, SAE-V provides an intrinsic data filtering mechanism to enhance model alignment without requiring additional models. Specifically, when applied to the alignment process of MLLMs, SAE-V-based data filtering methods could achieve more than 110% performance with less than 50% data. Our results highlight SAE-V's ability to enhance interpretability and alignment in MLLMs, providing insights into their internal mechanisms.

### Int2Int: a framework for mathematics with transformers 
[[arxiv](https://arxiv.org/abs/2502.17513)] [[cool](https://papers.cool/arxiv/2502.17513)] [[pdf](https://arxiv.org/pdf/2502.17513)]
> **Authors**: François Charton
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,数学软件
- **Abstract**: This paper documents Int2Int, an open source code base for using transformers on problems of mathematical research, with a focus on number theory and other problems involving integers. Int2Int is a complete PyTorch implementation of a transformer architecture, together with training and evaluation loops, and classes and functions to represent, generate and decode common mathematical objects. Ancillary code for data preparation, and Jupyter Notebooks for visualizing experimental results are also provided. This document presents the main features of Int2Int, serves as its user manual, and provides guidelines on how to extend it. Int2Int is released under the MIT licence, at https://github.com/FacebookResearch/Int2Int.

### Learning multi-phase flow and transport in fractured porous media with auto-regressive and recurrent graph neural networks 
[[arxiv](https://arxiv.org/abs/2502.17512)] [[cool](https://papers.cool/arxiv/2502.17512)] [[pdf](https://arxiv.org/pdf/2502.17512)]
> **Authors**: Mohammed Al Kobaisi,Wenjuan Zhang,Waleed Diab,Hadi Hajibeygi
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,流体动力学
- **Abstract**: In the past three decades, a wide array of computational methodologies and simulation frameworks has emerged to address the complexities of modeling multi-phase flow and transport processes in fractured porous media. The conformal mesh approaches which explicitly align the computational grid with fracture surfaces are considered by many to be the most accurate. However, such methods require excessive fine-scale meshing, rendering them impractical for large or complex fracture networks. In this work, we propose to learn the complex multi-phase flow and transport dynamics in fractured porous media with graph neural networks (GNN). GNNs are well suited for this task due to the unstructured topology of the computation grid resulting from the Embedded Discrete Fracture Model (EDFM) discretization. We propose two deep learning architectures, a GNN and a recurrent GNN. Both networks follow a two-stage training strategy: an autoregressive one step roll-out, followed by a fine-tuning step where the model is supervised using the whole ground-truth sequence. We demonstrate that the two-stage training approach is effective in mitigating error accumulation during autoregressive model rollouts in the testing phase. Our findings indicate that both GNNs generalize well to unseen fracture realizations, with comparable performance in forecasting saturation sequences, and slightly better performance for the recurrent GNN in predicting pressure sequences. While the second stage of training proved to be beneficial for the GNN model, its impact on the recurrent GNN model was less pronounced. Finally, the performance of both GNNs for temporal extrapolation is tested. The recurrent GNN significantly outperformed the GNN in terms of accuracy, thereby underscoring its superior capability in predicting long sequences.

### Recurrent Knowledge Identification and Fusion for Language Model Continual Learning 
[[arxiv](https://arxiv.org/abs/2502.17510)] [[cool](https://papers.cool/arxiv/2502.17510)] [[pdf](https://arxiv.org/pdf/2502.17510)]
> **Authors**: Yujie Feng,Xujia Wang,Zexin Lu,Shenghong Fu,Guangyuan Shi,Yongxin Xu,Yasha Wang,Philip S. Yu,Xu Chu,Xiao-Ming Wu
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Continual learning (CL) is crucial for deploying large language models (LLMs) in dynamic real-world environments without costly retraining. While recent model ensemble and model merging methods guided by parameter importance have gained popularity, they often struggle to balance knowledge transfer and forgetting, mainly due to the reliance on static importance estimates during sequential training. In this paper, we present Recurrent-KIF, a novel CL framework for Recurrent Knowledge Identification and Fusion, which enables dynamic estimation of parameter importance distributions to enhance knowledge transfer. Inspired by human continual learning, Recurrent-KIF employs an inner loop that rapidly adapts to new tasks while identifying important parameters, coupled with an outer loop that globally manages the fusion of new and historical knowledge through redundant knowledge pruning and key knowledge merging. These inner-outer loops iteratively perform multiple rounds of fusion, allowing Recurrent-KIF to leverage intermediate training information and adaptively adjust fusion strategies based on evolving importance distributions. Extensive experiments on two CL benchmarks with various model sizes (from 770M to 13B) demonstrate that Recurrent-KIF effectively mitigates catastrophic forgetting and enhances knowledge transfer.

### C-3DPO: Constrained Controlled Classification for Direct Preference Optimization 
[[arxiv](https://arxiv.org/abs/2502.17507)] [[cool](https://papers.cool/arxiv/2502.17507)] [[pdf](https://arxiv.org/pdf/2502.17507)]
> **Authors**: Kavosh Asadi,Julien Han,Xingzi Xu,Dominique Perrault-Joncas,Shoham Sabach,Karim Bouyarmane,Mohammad Ghavamzadeh
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Direct preference optimization (DPO)-style algorithms have emerged as a promising approach for solving the alignment problem in AI. We present a novel perspective that formulates these algorithms as implicit classification algorithms. This classification framework enables us to recover many variants of DPO-style algorithms by choosing appropriate classification labels and loss functions. We then leverage this classification framework to demonstrate that the underlying problem solved in these algorithms is under-specified, making them susceptible to probability collapse of the winner-loser responses. We address this by proposing a set of constraints designed to control the movement of probability mass between the winner and loser in the reference and target policies. Our resulting algorithm, which we call Constrained Controlled Classification DPO (\texttt{C-3DPO}), has a meaningful RLHF interpretation. By hedging against probability collapse, \texttt{C-3DPO} provides practical improvements over vanilla \texttt{DPO} when aligning several large language models using standard preference datasets.

### RAG-Enhanced Collaborative LLM Agents for Drug Discovery 
[[arxiv](https://arxiv.org/abs/2502.17506)] [[cool](https://papers.cool/arxiv/2502.17506)] [[pdf](https://arxiv.org/pdf/2502.17506)]
> **Authors**: Namkyeong Lee,Edward De Brouwer,Ehsan Hajiramezanali,Chanyoung Park,Gabriele Scalia
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-25
> **comment**: MachineLearning, Drug Discovery
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Recent advances in large language models (LLMs) have shown great potential to accelerate drug discovery. However, the specialized nature of biochemical data often necessitates costly domain-specific fine-tuning, posing critical challenges. First, it hinders the application of more flexible general-purpose LLMs in cutting-edge drug discovery tasks. More importantly, it impedes the rapid integration of the vast amounts of scientific data continuously generated through experiments and research. To investigate these challenges, we propose CLADD, a retrieval-augmented generation (RAG)-empowered agentic system tailored to drug discovery tasks. Through the collaboration of multiple LLM agents, CLADD dynamically retrieves information from biomedical knowledge bases, contextualizes query molecules, and integrates relevant evidence to generate responses -- all without the need for domain-specific fine-tuning. Crucially, we tackle key obstacles in applying RAG workflows to biochemical data, including data heterogeneity, ambiguity, and multi-source integration. We demonstrate the flexibility and effectiveness of this framework across a variety of drug discovery tasks, showing that it outperforms general-purpose and domain-specific LLMs as well as traditional deep learning approaches.

### Doctor-in-the-Loop: An Explainable, Multi-View Deep Learning Framework for Predicting Pathological Response in Non-Small Cell Lung Cancer 
[[arxiv](https://arxiv.org/abs/2502.17503)] [[cool](https://papers.cool/arxiv/2502.17503)] [[pdf](https://arxiv.org/pdf/2502.17503)]
> **Authors**: Alice Natalina Caragliano,Filippo Ruffini,Carlo Greco,Edy Ippolito,Michele Fiore,Claudia Tacconi,Lorenzo Nibid,Giuseppe Perrone,Sara Ramella,Paolo Soda,Valerio Guarrasi
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别,图像和视频处理
- **Abstract**: Non-small cell lung cancer (NSCLC) remains a major global health challenge, with high post-surgical recurrence rates underscoring the need for accurate pathological response predictions to guide personalized treatments. Although artificial intelligence models show promise in this domain, their clinical adoption is limited by the lack of medically grounded guidance during training, often resulting in non-explainable intrinsic predictions. To address this, we propose Doctor-in-the-Loop, a novel framework that integrates expert-driven domain knowledge with explainable artificial intelligence techniques, directing the model toward clinically relevant anatomical regions and improving both interpretability and trustworthiness. Our approach employs a gradual multi-view strategy, progressively refining the model's focus from broad contextual features to finer, lesion-specific details. By incorporating domain insights at every stage, we enhance predictive accuracy while ensuring that the model's decision-making process aligns more closely with clinical reasoning. Evaluated on a dataset of NSCLC patients, Doctor-in-the-Loop delivers promising predictive performance and provides transparent, justifiable outputs, representing a significant step toward clinically explainable artificial intelligence in oncology.

### CoKV: Optimizing KV Cache Allocation via Cooperative Game 
[[arxiv](https://arxiv.org/abs/2502.17501)] [[cool](https://papers.cool/arxiv/2502.17501)] [[pdf](https://arxiv.org/pdf/2502.17501)]
> **Authors**: Qiheng Sun,Hongwei Zhang,Haocheng Xia,Jiayao Zhang,Jinfei Liu,Kui Ren
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Large language models (LLMs) have achieved remarkable success on various aspects of human life. However, one of the major challenges in deploying these models is the substantial memory consumption required to store key-value pairs (KV), which imposes significant resource demands. Recent research has focused on KV cache budget allocation, with several approaches proposing head-level budget distribution by evaluating the importance of individual attention heads. These methods, however, assess the importance of heads independently, overlooking their cooperative contributions within the model, which may result in a deviation from their true impact on model performance. In light of this limitation, we propose CoKV, a novel method that models the cooperation between heads in model inference as a cooperative game. By evaluating the contribution of each head within the cooperative game, CoKV can allocate the cache budget more effectively. Extensive experiments show that CoKV achieves state-of-the-art performance on the LongBench benchmark using LLama-3-8B-Instruct and Mistral-7B models.

### Generalized Exponentiated Gradient Algorithms Using the Euler Two-Parameter Logarithm 
[[arxiv](https://arxiv.org/abs/2502.17500)] [[cool](https://papers.cool/arxiv/2502.17500)] [[pdf](https://arxiv.org/pdf/2502.17500)]
> **Authors**: Andrzej Cichocki
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-25
> **comment**: 10 pages, preprint of Journal paper
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: In this paper we propose and investigate a new class of Generalized Exponentiated Gradient (GEG) algorithms using Mirror Descent (MD) approaches, and applying as a regularization function the Bregman divergence with two-parameter deformation of logarithm as a link function. This link function (referred to as the Euler logarithm) is associated with a wide class of generalized entropies. In order to derive novel GEG/MD updates, we estimate generalized exponential function, which closely approximates the inverse of the Euler two-parameter logarithm. The characteristic/shape and properties of the Euler logarithm and its inverse -- deformed exponential functions are tuned by two or even more hyperparameters. By learning these hyperparameters, we can adapt to distribution of training data, and we can adjust them to achieve desired properties of gradient descent algorithms. The concept of generalized entropies and associated deformed logarithms provide deeper insight into novel gradient descent updates. In literature, there exist nowadays over fifty mathematically well-defined entropic functionals and associated deformed logarithms, so impossible to investigate all of them in one research paper. Therefore, we focus here on a wide-class of trace-form entropies and associated generalized logarithm. We applied the developed algorithms for Online Portfolio Selection (OPLS) in order to improve its performance and robustness.

### Improving Value-based Process Verifier via Structural Prior Injection 
[[arxiv](https://arxiv.org/abs/2502.17498)] [[cool](https://papers.cool/arxiv/2502.17498)] [[pdf](https://arxiv.org/pdf/2502.17498)]
> **Authors**: Zetian Sun,Dongfang Li,Baotian Hu,Jun Yu,Min Zhang
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-25
> **comment**: Preprint. Under review
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: In the Large Language Model(LLM) reasoning scenario, people often estimate state value via Monte Carlo sampling. Though Monte Carlo estimation is an elegant method with less inductive bias, noise and errors are inevitably introduced due to the limited sampling. To handle the problem, we inject the structural prior into the value representation and transfer the scalar value into the expectation of a pre-defined categorical distribution, representing the noise and errors from a distribution perspective. Specifically, by treating the result of Monte Carlo sampling as a single sample from the prior ground-truth Binomial distribution, we quantify the sampling error as the mismatch between posterior estimated distribution and ground-truth distribution, which is thus optimized via distribution selection optimization. We test the performance of value-based process verifiers on Best-of-N task and Beam search task. Compared with the scalar value representation, we show that reasonable structural prior injection induced by different objective functions or optimization methods can improve the performance of value-based process verifiers for about 1$\sim$2 points at little-to-no cost. We also show that under different structural prior, the verifiers' performances vary greatly despite having the same optimal solution, indicating the importance of reasonable structural prior injection.

### Hard constraint learning approaches with trainable influence functions for evolutionary equations 
[[arxiv](https://arxiv.org/abs/2502.17497)] [[cool](https://papers.cool/arxiv/2502.17497)] [[pdf](https://arxiv.org/pdf/2502.17497)]
> **Authors**: Yushi Zhang,Shuai Su,Yong Wang,Yanzhong Yao
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: This paper develops a novel deep learning approach for solving evolutionary equations, which integrates sequential learning strategies with an enhanced hard constraint strategy featuring trainable parameters, addressing the low computational accuracy of standard Physics-Informed Neural Networks (PINNs) in large temporal domains.Sequential learning strategies divide a large temporal domain into multiple subintervals and solve them one by one in a chronological order, which naturally respects the principle of causality and improves the stability of the PINN solution. The improved hard constraint strategy strictly ensures the continuity and smoothness of the PINN solution at time interval nodes, and at the same time passes the information from the previous interval to the next interval, which avoids the incorrect/trivial solution at the position far from the initial time. Furthermore, by investigating the requirements of different types of equations on hard constraints, we design a novel influence function with trainable parameters for hard constraints, which provides theoretical and technical support for the effective implementations of hard constraint strategies, and significantly improves the universality and computational accuracy of our method. In addition, an adaptive time-domain partitioning algorithm is proposed, which plays an important role in the application of the proposed method as well as in the improvement of computational efficiency and accuracy. Numerical experiments verify the performance of the method. The data and code accompanying this paper are available at https://github.com/zhizhi4452/HCS.

### SpikeRL: A Scalable and Energy-efficient Framework for Deep Spiking Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.17496)] [[cool](https://papers.cool/arxiv/2502.17496)] [[pdf](https://arxiv.org/pdf/2502.17496)]
> **Authors**: Tokey Tahmid,Mark Gates,Piotr Luszczek,Catherine D. Schuman
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,神经和进化计算
- **Abstract**: In this era of AI revolution, massive investments in large-scale data-driven AI systems demand high-performance computing, consuming tremendous energy and resources. This trend raises new challenges in optimizing sustainability without sacrificing scalability or performance. Among the energy-efficient alternatives of the traditional Von Neumann architecture, neuromorphic computing and its Spiking Neural Networks (SNNs) are a promising choice due to their inherent energy efficiency. However, in some real-world application scenarios such as complex continuous control tasks, SNNs often lack the performance optimizations that traditional artificial neural networks have. Researchers have addressed this by combining SNNs with Deep Reinforcement Learning (DeepRL), yet scalability remains unexplored. In this paper, we extend our previous work on SpikeRL, which is a scalable and energy efficient framework for DeepRL-based SNNs for continuous control. In our initial implementation of SpikeRL framework, we depended on the population encoding from the Population-coded Spiking Actor Network (PopSAN) method for our SNN model and implemented distributed training with Message Passing Interface (MPI) through mpi4py. Also, further optimizing our model training by using mixed-precision for parameter updates. In our new SpikeRL framework, we have implemented our own DeepRL-SNN component with population encoding, and distributed training with PyTorch Distributed package with NCCL backend while still optimizing with mixed precision training. Our new SpikeRL implementation is 4.26X faster and 2.25X more energy efficient than state-of-the-art DeepRL-SNN methods. Our proposed SpikeRL framework demonstrates a truly scalable and sustainable solution for complex continuous control tasks in real-world applications.

### Spatiotemporal Forecasting in Climate Data Using EOFs and Machine Learning Models: A Case Study in Chile 
[[arxiv](https://arxiv.org/abs/2502.17495)] [[cool](https://papers.cool/arxiv/2502.17495)] [[pdf](https://arxiv.org/pdf/2502.17495)]
> **Authors**: Mauricio Herrera,Francisca Kleisinger,Andrés Wilsón
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-25
> **comment**: 25 pages, 6 figures
- **标题**: None
- **领域**: 机器学习,大气和海洋物理,应用领域,机器学习
- **Abstract**: Effective resource management and environmental planning in regions with high climatic variability, such as Chile, demand advanced predictive tools. This study addresses this challenge by employing an innovative and computationally efficient hybrid methodology that integrates machine learning (ML) methods for time series forecasting with established statistical techniques. The spatiotemporal data undergo decomposition using time-dependent Empirical Orthogonal Functions (EOFs), denoted as \(φ_{k}(t)\), and their corresponding spatial coefficients, \(α_{k}(s)\), to reduce dimensionality. Wavelet analysis provides high-resolution time and frequency information from the \(φ_{k}(t)\) functions, while neural networks forecast these functions within a medium-range horizon \(h\). By utilizing various ML models, particularly a Wavelet - ANN hybrid model, we forecast \(φ_{k}(t+h)\) up to a time horizon \(h\), and subsequently reconstruct the spatiotemporal data using these extended EOFs. This methodology is applied to a grid of climate data covering the territory of Chile. It transitions from a high-dimensional multivariate spatiotemporal data forecasting problem to a low-dimensional univariate forecasting problem. Additionally, cluster analysis with Dynamic Time Warping for defining similarities between rainfall time series, along with spatial coherence and predictability assessments, has been instrumental in identifying geographic areas where model performance is enhanced. This approach also elucidates the reasons behind poor forecast performance in regions or clusters with low spatial coherence and predictability. By utilizing cluster medoids, the forecasting process becomes more practical and efficient. This compound approach significantly reduces computational complexity while generating forecasts of reasonable accuracy and utility.

### Pursuing Top Growth with Novel Loss Function 
[[arxiv](https://arxiv.org/abs/2502.17493)] [[cool](https://papers.cool/arxiv/2502.17493)] [[pdf](https://arxiv.org/pdf/2502.17493)]
> **Authors**: Ruoyu Guo,Haochen Qiu
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-25
> **comment**: 30 pages, 7 figures, GitHub repo: https://github.com/Tony-Guo-1/daily_trading_strategy
- **标题**: None
- **领域**: 机器学习,人工智能,计算金融
- **Abstract**: Making consistently profitable financial decisions in a continuously evolving and volatile stock market has always been a difficult task. Professionals from different disciplines have developed foundational theories to anticipate price movement and evaluate securities such as the famed Capital Asset Pricing Model (CAPM). In recent years, the role of artificial intelligence (AI) in asset pricing has been growing. Although the black-box nature of deep learning models lacks interpretability, they have continued to solidify their position in the financial industry. We aim to further enhance AI's potential and utility by introducing a return-weighted loss function that will drive top growth while providing the ML models a limited amount of information. Using only publicly accessible stock data (open/close/high/low, trading volume, sector information) and several technical indicators constructed from them, we propose an efficient daily trading system that detects top growth opportunities. Our best models achieve 61.73% annual return on daily rebalancing with an annualized Sharpe Ratio of 1.18 over 1340 testing days from 2019 to 2024, and 37.61% annual return with an annualized Sharpe Ratio of 0.97 over 1360 testing days from 2005 to 2010. The main drivers for success, especially independent of any domain knowledge, are the novel return-weighted loss function, the integration of categorical and continuous data, and the ML model architecture. We also demonstrate the superiority of our novel loss function over traditional loss functions via several performance metrics and statistical evidence.

### Rapid Parameter Inference with Uncertainty Quantification for a Radiological Plume Source Identification Problem 
[[arxiv](https://arxiv.org/abs/2502.17492)] [[cool](https://papers.cool/arxiv/2502.17492)] [[pdf](https://arxiv.org/pdf/2502.17492)]
> **Authors**: Christopher Edwards,Ralph C Smith
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,应用领域
- **Abstract**: In the event of a nuclear accident, or the detonation of a radiological dispersal device, quickly locating the source of the accident or blast is important for emergency response and environmental decontamination. At a specified time after a simulated instantaneous release of an aerosolized radioactive contaminant, measurements are recorded downwind from an array of radiation sensors. Neural networks are employed to infer the source release parameters in an accurate and rapid manner using sensor and mean wind speed data. We consider two neural network constructions that quantify the uncertainty of the predicted values; a categorical classification neural network and a Bayesian neural network. With the categorical classification neural network, we partition the spatial domain and treat each partition as a separate class for which we estimate the probability that it contains the true source location. In a Bayesian neural network, the weights and biases have a distribution rather than a single optimal value. With each evaluation, these distributions are sampled, yielding a different prediction with each evaluation. The trained Bayesian neural network is thus evaluated to construct posterior densities for the release parameters. Results are compared to Markov chain Monte Carlo (MCMC) results found using the Delayed Rejection Adaptive Metropolis Algorithm. The Bayesian neural network approach is generally much cheaper computationally than the MCMC approach as it relies on the computational cost of the neural network evaluation to generate posterior densities as opposed to the MCMC approach which depends on the computational expense of the transport and radiation detection models.

### A generalized dual potential for inelastic Constitutive Artificial Neural Networks: A JAX implementation at finite strains 
[[arxiv](https://arxiv.org/abs/2502.17490)] [[cool](https://papers.cool/arxiv/2502.17490)] [[pdf](https://arxiv.org/pdf/2502.17490)]
> **Authors**: Hagen Holthusen,Kevin Linka,Ellen Kuhl,Tim Brepols
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-25
> **comment**: 56 pages, 19 figures, 3 tables
- **标题**: None
- **领域**: 机器学习,材料科学,人工智能,计算工程、金融和科学
- **Abstract**: We present a methodology for designing a generalized dual potential, or pseudo potential, for inelastic Constitutive Artificial Neural Networks (iCANNs). This potential, expressed in terms of stress invariants, inherently satisfies thermodynamic consistency for large deformations. In comparison to our previous work, the new potential captures a broader spectrum of material behaviors, including pressure-sensitive inelasticity. To this end, we revisit the underlying thermodynamic framework of iCANNs for finite strain inelasticity and derive conditions for constructing a convex, zero-valued, and non-negative dual potential. To embed these principles in a neural network, we detail the architecture's design, ensuring a priori compliance with thermodynamics. To evaluate the proposed architecture, we study its performance and limitations discovering visco-elastic material behavior, though the method is not limited to visco-elasticity. In this context, we investigate different aspects in the strategy of discovering inelastic materials. Our results indicate that the novel architecture robustly discovers interpretable models and parameters, while autonomously revealing the degree of inelasticity. The iCANN framework, implemented in JAX, is publicly accessible at https://doi.org/10.5281/zenodo.14894687.

### Fractal Generative Models 
[[arxiv](https://arxiv.org/abs/2502.17437)] [[cool](https://papers.cool/arxiv/2502.17437)] [[pdf](https://arxiv.org/pdf/2502.17437)]
> **Authors**: Tianhong Li,Qinyi Sun,Lijie Fan,Kaiming He
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: Modularization is a cornerstone of computer science, abstracting complex functions into atomic building blocks. In this paper, we introduce a new level of modularization by abstracting generative models into atomic generative modules. Analogous to fractals in mathematics, our method constructs a new type of generative model by recursively invoking atomic generative modules, resulting in self-similar fractal architectures that we call fractal generative models. As a running example, we instantiate our fractal framework using autoregressive models as the atomic generative modules and examine it on the challenging task of pixel-by-pixel image generation, demonstrating strong performance in both likelihood estimation and generation quality. We hope this work could open a new paradigm in generative modeling and provide a fertile ground for future research. Code is available at https://github.com/LTH14/fractalgen.

### Towards Hierarchical Rectified Flow 
[[arxiv](https://arxiv.org/abs/2502.17436)] [[cool](https://papers.cool/arxiv/2502.17436)] [[pdf](https://arxiv.org/pdf/2502.17436)]
> **Authors**: Yichi Zhang,Yici Yan,Alex Schwing,Zhizhen Zhao
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: ICLR 2025; Project Page: https://riccizz.github.io/HRF/
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: We formulate a hierarchical rectified flow to model data distributions. It hierarchically couples multiple ordinary differential equations (ODEs) and defines a time-differentiable stochastic process that generates a data distribution from a known source distribution. Each ODE resembles the ODE that is solved in a classic rectified flow, but differs in its domain, i.e., location, velocity, acceleration, etc. Unlike the classic rectified flow formulation, which formulates a single ODE in the location domain and only captures the expected velocity field (sufficient to capture a multi-modal data distribution), the hierarchical rectified flow formulation models the multi-modal random velocity field, acceleration field, etc., in their entirety. This more faithful modeling of the random velocity field enables integration paths to intersect when the underlying ODE is solved during data generation. Intersecting paths in turn lead to integration trajectories that are more straight than those obtained in the classic rectified flow formulation, where integration paths cannot intersect. This leads to modeling of data distributions with fewer neural function evaluations. We empirically verify this on synthetic 1D and 2D data as well as MNIST, CIFAR-10, and ImageNet-32 data. Code is available at: https://riccizz.github.io/HRF/.

### S4S: Solving for a Diffusion Model Solver 
[[arxiv](https://arxiv.org/abs/2502.17423)] [[cool](https://papers.cool/arxiv/2502.17423)] [[pdf](https://arxiv.org/pdf/2502.17423)]
> **Authors**: Eric Frankel,Sitan Chen,Jerry Li,Pang Wei Koh,Lillian J. Ratliff,Sewoong Oh
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Diffusion models (DMs) create samples from a data distribution by starting from random noise and iteratively solving a reverse-time ordinary differential equation (ODE). Because each step in the iterative solution requires an expensive neural function evaluation (NFE), there has been significant interest in approximately solving these diffusion ODEs with only a few NFEs without modifying the underlying model. However, in the few NFE regime, we observe that tracking the true ODE evolution is fundamentally impossible using traditional ODE solvers. In this work, we propose a new method that learns a good solver for the DM, which we call Solving for the Solver (S4S). S4S directly optimizes a solver to obtain good generation quality by learning to match the output of a strong teacher solver. We evaluate S4S on six different pre-trained DMs, including pixel-space and latent-space DMs for both conditional and unconditional sampling. In all settings, S4S uniformly improves the sample quality relative to traditional ODE solvers. Moreover, our method is lightweight, data-free, and can be plugged in black-box on top of any discretization schedule or architecture to improve performance. Building on top of this, we also propose S4S-Alt, which optimizes both the solver and the discretization schedule. By exploiting the full design space of DM solvers, with 5 NFEs, we achieve an FID of 3.73 on CIFAR10 and 13.26 on MS-COCO, representing a $1.5\times$ improvement over previous training-free ODE methods.

### The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence 
[[arxiv](https://arxiv.org/abs/2502.17420)] [[cool](https://papers.cool/arxiv/2502.17420)] [[pdf](https://arxiv.org/pdf/2502.17420)]
> **Authors**: Tom Wollschläger,Jannes Elstner,Simon Geisler,Vincent Cohen-Addad,Stephan Günnemann,Johannes Gasteiger
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: The safety alignment of large language models (LLMs) can be circumvented through adversarially crafted inputs, yet the mechanisms by which these attacks bypass safety barriers remain poorly understood. Prior work suggests that a single refusal direction in the model's activation space determines whether an LLM refuses a request. In this study, we propose a novel gradient-based approach to representation engineering and use it to identify refusal directions. Contrary to prior work, we uncover multiple independent directions and even multi-dimensional concept cones that mediate refusal. Moreover, we show that orthogonality alone does not imply independence under intervention, motivating the notion of representational independence that accounts for both linear and non-linear effects. Using this framework, we identify mechanistically independent refusal directions. We show that refusal mechanisms in LLMs are governed by complex spatial structures and identify functionally independent directions, confirming that multiple distinct mechanisms drive refusal behavior. Our gradient-based approach uncovers these mechanisms and can further serve as a foundation for future work on understanding LLMs.

### COSMOS: A Hybrid Adaptive Optimizer for Memory-Efficient Training of LLMs 
[[arxiv](https://arxiv.org/abs/2502.17410)] [[cool](https://papers.cool/arxiv/2502.17410)] [[pdf](https://arxiv.org/pdf/2502.17410)]
> **Authors**: Liming Liu,Zhenghao Xu,Zixuan Zhang,Hao Kang,Zichong Li,Chen Liang,Weizhu Chen,Tuo Zhao
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 23 pages, 9 figures, 6 tables
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Large Language Models (LLMs) have demonstrated remarkable success across various domains, yet their optimization remains a significant challenge due to the complex and high-dimensional loss landscapes they inhabit. While adaptive optimizers such as AdamW are widely used, they suffer from critical limitations, including an inability to capture interdependencies between coordinates and high memory consumption. Subsequent research, exemplified by SOAP, attempts to better capture coordinate interdependence but incurs greater memory overhead, limiting scalability for massive LLMs. An alternative approach aims to reduce memory consumption through low-dimensional projection, but this leads to substantial approximation errors, resulting in less effective optimization (e.g., in terms of per-token efficiency). In this paper, we propose COSMOS, a novel hybrid optimizer that leverages the varying importance of eigensubspaces in the gradient matrix to achieve memory efficiency without compromising optimization performance. The design of COSMOS is motivated by our empirical insights and practical considerations. Specifically, COSMOS applies SOAP to the leading eigensubspace, which captures the primary optimization dynamics, and MUON to the remaining eigensubspace, which is less critical but computationally expensive to handle with SOAP. This hybrid strategy significantly reduces memory consumption while maintaining robust optimization performance, making it particularly suitable for massive LLMs. Numerical experiments on various datasets and transformer architectures are provided to demonstrate the effectiveness of COSMOS. Our code is available at https://github.com/lliu606/COSMOS.

### Large Language Models are Powerful EHR Encoders 
[[arxiv](https://arxiv.org/abs/2502.17403)] [[cool](https://papers.cool/arxiv/2502.17403)] [[pdf](https://arxiv.org/pdf/2502.17403)]
> **Authors**: Stefan Hegselmann,Georg von Arnim,Tillmann Rheude,Noel Kronenberg,David Sontag,Gerhard Hindricks,Roland Eils,Benjamin Wild
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Electronic Health Records (EHRs) offer rich potential for clinical prediction, yet their inherent complexity and heterogeneity pose significant challenges for traditional machine learning approaches. Domain-specific EHR foundation models trained on large collections of unlabeled EHR data have demonstrated promising improvements in predictive accuracy and generalization; however, their training is constrained by limited access to diverse, high-quality datasets and inconsistencies in coding standards and healthcare practices. In this study, we explore the possibility of using general-purpose Large Language Models (LLMs) based embedding methods as EHR encoders. By serializing patient records into structured Markdown text, transforming codes into human-readable descriptors, we leverage the extensive generalization capabilities of LLMs pretrained on vast public corpora, thereby bypassing the need for proprietary medical datasets. We systematically evaluate two state-of-the-art LLM-embedding models, GTE-Qwen2-7B-Instruct and LLM2Vec-Llama3.1-8B-Instruct, across 15 diverse clinical prediction tasks from the EHRSHOT benchmark, comparing their performance to an EHRspecific foundation model, CLIMBR-T-Base, and traditional machine learning baselines. Our results demonstrate that LLM-based embeddings frequently match or exceed the performance of specialized models, even in few-shot settings, and that their effectiveness scales with the size of the underlying LLM and the available context window. Overall, our findings demonstrate that repurposing LLMs for EHR encoding offers a scalable and effective approach for clinical prediction, capable of overcoming the limitations of traditional EHR modeling and facilitating more interoperable and generalizable healthcare applications.

### The Empirical Impact of Reducing Symmetries on the Performance of Deep Ensembles and MoE 
[[arxiv](https://arxiv.org/abs/2502.17391)] [[cool](https://papers.cool/arxiv/2502.17391)] [[pdf](https://arxiv.org/pdf/2502.17391)]
> **Authors**: Andrei Chernov,Oleg Novitskij
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: preprint
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Recent studies have shown that reducing symmetries in neural networks enhances linear mode connectivity between networks without requiring parameter space alignment, leading to improved performance in linearly interpolated neural networks. However, in practical applications, neural network interpolation is rarely used; instead, ensembles of networks are more common. In this paper, we empirically investigate the impact of reducing symmetries on the performance of deep ensembles and Mixture of Experts (MoE) across five datasets. Additionally, to explore deeper linear mode connectivity, we introduce the Mixture of Interpolated Experts (MoIE). Our results show that deep ensembles built on asymmetric neural networks achieve significantly better performance as ensemble size increases compared to their symmetric counterparts. In contrast, our experiments do not provide conclusive evidence on whether reducing symmetries affects both MoE and MoIE architectures.

### Big-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models 
[[arxiv](https://arxiv.org/abs/2502.17387)] [[cool](https://papers.cool/arxiv/2502.17387)] [[pdf](https://arxiv.org/pdf/2502.17387)]
> **Authors**: Alon Albalak,Duy Phung,Nathan Lile,Rafael Rafailov,Kanishk Gandhi,Louis Castricato,Anikait Singh,Chase Blagden,Violet Xiang,Dakota Mahan,Nick Haber
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Increasing interest in reasoning models has led math to become a prominent testing ground for algorithmic and methodological improvements. However, existing open math datasets either contain a small collection of high-quality, human-written problems or a large corpus of machine-generated problems of uncertain quality, forcing researchers to choose between quality and quantity. In this work, we present Big-Math, a dataset of over 250,000 high-quality math questions with verifiable answers, purposefully made for reinforcement learning (RL). To create Big-Math, we rigorously filter, clean, and curate openly available datasets, extracting questions that satisfy our three desiderata: (1) problems with uniquely verifiable solutions, (2) problems that are open-ended, (3) and problems with a closed-form solution. To ensure the quality of Big-Math, we manually verify each step in our filtering process. Based on the findings from our filtering process, we introduce 47,000 new questions with verified answers, Big-Math-Reformulated: closed-ended questions (i.e. multiple choice questions) that have been reformulated as open-ended questions through a systematic reformulation algorithm. Compared to the most commonly used existing open-source datasets for math reasoning, GSM8k and MATH, Big-Math is an order of magnitude larger, while our rigorous filtering ensures that we maintain the questions most suitable for RL. We also provide a rigorous analysis of the dataset, finding that Big-Math contains a high degree of diversity across problem domains, and incorporates a wide range of problem difficulties, enabling a wide range of downstream uses for models of varying capabilities and training requirements. By bridging the gap between data quality and quantity, Big-Math establish a robust foundation for advancing reasoning in LLMs.

### On the Dichotomy Between Privacy and Traceability in $\ell_p$ Stochastic Convex Optimization 
[[arxiv](https://arxiv.org/abs/2502.17384)] [[cool](https://papers.cool/arxiv/2502.17384)] [[pdf](https://arxiv.org/pdf/2502.17384)]
> **Authors**: Sasha Voitovych,Mahdi Haghifam,Idan Attias,Gintare Karolina Dziugaite,Roi Livni,Daniel M. Roy
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 53 Pages
- **标题**: None
- **领域**: 机器学习
- **Abstract**: In this paper, we investigate the necessity of memorization in stochastic convex optimization (SCO) under $\ell_p$ geometries. Informally, we say a learning algorithm memorizes $m$ samples (or is $m$-traceable) if, by analyzing its output, it is possible to identify at least $m$ of its training samples. Our main results uncover a fundamental tradeoff between traceability and excess risk in SCO. For every $p\in [1,\infty)$, we establish the existence of a risk threshold below which any sample-efficient learner must memorize a \em{constant fraction} of its sample. For $p\in [1,2]$, this threshold coincides with best risk of differentially private (DP) algorithms, i.e., above this threshold, there are algorithms that do not memorize even a single sample. This establishes a sharp dichotomy between privacy and traceability for $p \in [1,2]$. For $p \in (2,\infty)$, this threshold instead gives novel lower bounds for DP learning, partially closing an open problem in this setup. En route of proving these results, we introduce a complexity notion we term \em{trace value} of a problem, which unifies privacy lower bounds and traceability results, and prove a sparse variant of the fingerprinting lemma.

### Sustainable Greenhouse Management: A Comparative Analysis of Recurrent and Graph Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.17371)] [[cool](https://papers.cool/arxiv/2502.17371)] [[pdf](https://arxiv.org/pdf/2502.17371)]
> **Authors**: Emiliano Seri,Marcello Petitta,Cristina Cornaro
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,应用领域
- **Abstract**: The integration of photovoltaic (PV) systems into greenhouses not only optimizes land use but also enhances sustainable agricultural practices by enabling dual benefits of food production and renewable energy generation. However, accurate prediction of internal environmental conditions is crucial to ensure optimal crop growth while maximizing energy production. This study introduces a novel application of Spatio-Temporal Graph Neural Networks (STGNNs) to greenhouse microclimate modeling, comparing their performance with traditional Recurrent Neural Networks (RNNs). While RNNs excel at temporal pattern recognition, they cannot explicitly model the directional relationships between environmental variables. Our STGNN approach addresses this limitation by representing these relationships as directed graphs, enabling the model to capture both spatial dependencies and their directionality. Using high-frequency data collected at 15-minute intervals from a greenhouse in Volos, Greece, we demonstrate that RNNs achieve exceptional accuracy in winter conditions (R^2 = 0.985) but show limitations during summer cooling system operation. Though STGNNs currently show lower performance (winter R^2 = 0.947), their architecture offers greater potential for integrating additional variables such as PV generation and crop growth indicators.

### A Closer Look at TabPFN v2: Strength, Limitation, and Extension 
[[arxiv](https://arxiv.org/abs/2502.17361)] [[cool](https://papers.cool/arxiv/2502.17361)] [[pdf](https://arxiv.org/pdf/2502.17361)]
> **Authors**: Han-Jia Ye,Si-Yang Liu,Wei-Lun Chao
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Tabular datasets are inherently heterogeneous, posing significant challenges for developing pre-trained foundation models. The recently introduced transformer-based Tabular Prior-data Fitted Network v2 (TabPFN v2) achieves unprecedented in-context learning accuracy across multiple tabular datasets, marking a pivotal advancement in tabular foundation models. In this paper, we comprehensively evaluate TabPFN v2 on over 300 datasets, confirming its exceptional generalization capabilities on small- to medium-scale tasks. Our analysis identifies randomized feature tokens as a key factor behind TabPFN v2's success, as they unify heterogeneous datasets into a fixed-dimensional representation, enabling more effective training and inference. To further understand TabPFN v2's predictions, we propose a leave-one-fold-out approach, transforming TabPFN v2 into a feature extractor and revealing its capability to simplify data distributions and boost accuracy. Lastly, to address TabPFN v2's limitations in high-dimensional, large-scale, and many-category tasks, we introduce a divide-and-conquer mechanism inspired by Chain-of-Thought prompting, enabling scalable inference. By uncovering the mechanisms behind TabPFN v2's success and introducing strategies to expand its applicability, this study provides key insights into the future of tabular foundation models.

### Distributional Scaling Laws for Emergent Capabilities 
[[arxiv](https://arxiv.org/abs/2502.17356)] [[cool](https://papers.cool/arxiv/2502.17356)] [[pdf](https://arxiv.org/pdf/2502.17356)]
> **Authors**: Rosie Zhao,Tian Qin,David Alvarez-Melis,Sham Kakade,Naomi Saphra
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 17 pages
- **标题**: None
- **领域**: 机器学习
- **Abstract**: In this paper, we explore the nature of sudden breakthroughs in language model performance at scale, which stands in contrast to smooth improvements governed by scaling laws. While advocates of "emergence" view abrupt performance gains as capabilities unlocking at specific scales, others have suggested that they are produced by thresholding effects and alleviated by continuous metrics. We propose that breakthroughs are instead driven by continuous changes in the probability distribution of training outcomes, particularly when performance is bimodally distributed across random seeds. In synthetic length generalization tasks, we show that different random seeds can produce either highly linear or emergent scaling trends. We reveal that sharp breakthroughs in metrics are produced by underlying continuous changes in their distribution across seeds. Furthermore, we provide a case study of inverse scaling and show that even as the probability of a successful run declines, the average performance of a successful run continues to increase monotonically. We validate our distributional scaling framework on realistic settings by measuring MMLU performance in LLM populations. These insights emphasize the role of random variation in the effect of scale on LLM capabilities.

### Time series forecasting based on optimized LLM for fault prediction in distribution power grid insulators 
[[arxiv](https://arxiv.org/abs/2502.17341)] [[cool](https://papers.cool/arxiv/2502.17341)] [[pdf](https://arxiv.org/pdf/2502.17341)]
> **Authors**: João Pedro Matos-Carvalho,Stefano Frizzo Stefenon,Valderi Reis Quietinho Leithardt,Kin-Choong Yow
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,信号处理
- **Abstract**: Surface contamination on electrical grid insulators leads to an increase in leakage current until an electrical discharge occurs, which can result in a power system shutdown. To mitigate the possibility of disruptive faults resulting in a power outage, monitoring contamination and leakage current can help predict the progression of faults. Given this need, this paper proposes a hybrid deep learning (DL) model for predicting the increase in leakage current in high-voltage insulators. The hybrid structure considers a multi-criteria optimization using tree-structured Parzen estimation, an input stage filter for signal noise attenuation combined with a large language model (LLM) applied for time series forecasting. The proposed optimized LLM outperforms state-of-the-art DL models with a root-mean-square error equal to 2.24$\times10^{-4}$ for a short-term horizon and 1.21$\times10^{-3}$ for a medium-term horizon.

### Low-rank bias, weight decay, and model merging in neural networks 
[[arxiv](https://arxiv.org/abs/2502.17340)] [[cool](https://papers.cool/arxiv/2502.17340)] [[pdf](https://arxiv.org/pdf/2502.17340)]
> **Authors**: Ilja Kuzborskij,Yasin Abbasi Yadkori
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We explore the low-rank structure of the weight matrices in neural networks originating from training with Gradient Descent (GD) and Gradient Flow (GF) with $L2$ regularization (also known as weight decay). We show several properties of GD-trained deep neural networks, induced by $L2$ regularization. In particular, for a stationary point of GD we show alignment of the parameters and the gradient, norm preservation across layers, and low-rank bias: properties previously known in the context of GF solutions. Experiments show that the assumptions made in the analysis only mildly affect the observations. In addition, we investigate a multitask learning phenomenon enabled by $L2$ regularization and low-rank bias. In particular, we show that if two networks are trained, such that the inputs in the training set of one network are approximately orthogonal to the inputs in the training set of the other network, the new network obtained by simply summing the weights of the two networks will perform as well on both training sets as the respective individual networks. We demonstrate this for shallow ReLU neural networks trained by GD, as well as deep linear and deep ReLU networks trained by GF.

### Tokenized SAEs: Disentangling SAE Reconstructions 
[[arxiv](https://arxiv.org/abs/2502.17332)] [[cool](https://papers.cool/arxiv/2502.17332)] [[pdf](https://arxiv.org/pdf/2502.17332)]
> **Authors**: Thomas Dooms,Daniel Wilhelm
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Sparse auto-encoders (SAEs) have become a prevalent tool for interpreting language models' inner workings. However, it is unknown how tightly SAE features correspond to computationally important directions in the model. This work empirically shows that many RES-JB SAE features predominantly correspond to simple input statistics. We hypothesize this is caused by a large class imbalance in training data combined with a lack of complex error signals. To reduce this behavior, we propose a method that disentangles token reconstruction from feature reconstruction. This improvement is achieved by introducing a per-token bias, which provides an enhanced baseline for interesting reconstruction. As a result, significantly more interesting features and improved reconstruction in sparse regimes are learned.

### Survey on Strategic Mining in Blockchain: A Reinforcement Learning Approach 
[[arxiv](https://arxiv.org/abs/2502.17307)] [[cool](https://papers.cool/arxiv/2502.17307)] [[pdf](https://arxiv.org/pdf/2502.17307)]
> **Authors**: Jichen Li,Lijia Xie,Hanting Huang,Bo Zhou,Binfeng Song,Wanying Zeng,Xiaotie Deng,Xiao Zhang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 10 pages
- **标题**: None
- **领域**: 机器学习,计算机科学与博弈论,多代理系统
- **Abstract**: Strategic mining attacks, such as selfish mining, exploit blockchain consensus protocols by deviating from honest behavior to maximize rewards. Markov Decision Process (MDP) analysis faces scalability challenges in modern digital economics, including blockchain. To address these limitations, reinforcement learning (RL) provides a scalable alternative, enabling adaptive strategy optimization in complex dynamic environments. In this survey, we examine RL's role in strategic mining analysis, comparing it to MDP-based approaches. We begin by reviewing foundational MDP models and their limitations, before exploring RL frameworks that can learn near-optimal strategies across various protocols. Building on this analysis, we compare RL techniques and their effectiveness in deriving security thresholds, such as the minimum attacker power required for profitable attacks. Expanding the discussion further, we classify consensus protocols and propose open challenges, such as multi-agent dynamics and real-world validation. This survey highlights the potential of reinforcement learning (RL) to address the challenges of selfish mining, including protocol design, threat detection, and security analysis, while offering a strategic roadmap for researchers in decentralized systems and AI-driven analytics.

### Delta Decompression for MoE-based LLMs Compression 
[[arxiv](https://arxiv.org/abs/2502.17298)] [[cool](https://papers.cool/arxiv/2502.17298)] [[pdf](https://arxiv.org/pdf/2502.17298)]
> **Authors**: Hao Gu,Wei Li,Lujun Li,Qiyuan Zhu,Mark Lee,Shengjie Sun,Wei Xue,Yike Guo
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Work in progress
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Mixture-of-Experts (MoE) architectures in large language models (LLMs) achieve exceptional performance, but face prohibitive storage and memory requirements. To address these challenges, we present $D^2$-MoE, a new delta decompression compressor for reducing the parameters of MoE LLMs. Based on observations of expert diversity, we decompose their weights into a shared base weight and unique delta weights. Specifically, our method first merges each expert's weight into the base weight using the Fisher information matrix to capture shared components. Then, we compress delta weights through Singular Value Decomposition (SVD) by exploiting their low-rank properties. Finally, we introduce a semi-dynamical structured pruning strategy for the base weights, combining static and dynamic redundancy analysis to achieve further parameter reduction while maintaining input adaptivity. In this way, our $D^2$-MoE successfully compact MoE LLMs to high compression ratios without additional training. Extensive experiments highlight the superiority of our approach, with over 13% performance gains than other compressors on Mixtral|Phi-3.5|DeepSeek|Qwen2 MoE LLMs at 40$\sim$60% compression rates. Codes are available in https://github.com/lliai/D2MoE.

### Joint Value Estimation and Bidding in Repeated First-Price Auctions 
[[arxiv](https://arxiv.org/abs/2502.17292)] [[cool](https://papers.cool/arxiv/2502.17292)] [[pdf](https://arxiv.org/pdf/2502.17292)]
> **Authors**: Yuxiao Wen,Yanjun Han,Zhengyuan Zhou
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机科学与博弈论,信息论,方法论,机器学习
- **Abstract**: We study regret minimization in repeated first-price auctions (FPAs), where a bidder observes only the realized outcome after each auction -- win or loss. This setup reflects practical scenarios in online display advertising where the actual value of an impression depends on the difference between two potential outcomes, such as clicks or conversion rates, when the auction is won versus lost. We analyze three outcome models: (1) adversarial outcomes without features, (2) linear potential outcomes with features, and (3) linear treatment effects in features. For each setting, we propose algorithms that jointly estimate private values and optimize bidding strategies, achieving near-optimal regret bounds. Notably, our framework enjoys a unique feature that the treatments are also actively chosen, and hence eliminates the need for the overlap condition commonly required in causal inference.

### Kandinsky Conformal Prediction: Beyond Class- and Covariate-Conditional Coverage 
[[arxiv](https://arxiv.org/abs/2502.17264)] [[cool](https://papers.cool/arxiv/2502.17264)] [[pdf](https://arxiv.org/pdf/2502.17264)]
> **Authors**: Konstantina Bairaktari,Jiayun Wu,Zhiwei Steven Wu
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Conformal prediction is a powerful distribution-free framework for constructing prediction sets with coverage guarantees. Classical methods, such as split conformal prediction, provide marginal coverage, ensuring that the prediction set contains the label of a random test point with a target probability. However, these guarantees may not hold uniformly across different subpopulations, leading to disparities in coverage. Prior work has explored coverage guarantees conditioned on events related to the covariates and label of the test point. We present Kandinsky conformal prediction, a framework that significantly expands the scope of conditional coverage guarantees. In contrast to Mondrian conformal prediction, which restricts its coverage guarantees to disjoint groups -- reminiscent of the rigid, structured grids of Piet Mondrian's art -- our framework flexibly handles overlapping and fractional group memberships defined jointly on covariates and labels, reflecting the layered, intersecting forms in Wassily Kandinsky's compositions. Our algorithm unifies and extends existing methods, encompassing covariate-based group conditional, class conditional, and Mondrian conformal prediction as special cases, while achieving a minimax-optimal high-probability conditional coverage bound. Finally, we demonstrate the practicality of our approach through empirical evaluation on real-world datasets.

### REINFORCE Adversarial Attacks on Large Language Models: An Adaptive, Distributional, and Semantic Objective 
[[arxiv](https://arxiv.org/abs/2502.17254)] [[cool](https://papers.cool/arxiv/2502.17254)] [[pdf](https://arxiv.org/pdf/2502.17254)]
> **Authors**: Simon Geisler,Tom Wollschläger,M. H. I. Abdalla,Vincent Cohen-Addad,Johannes Gasteiger,Stephan Günnemann
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 30 pages, 6 figures, 15 tables
- **标题**: None
- **领域**: 机器学习
- **Abstract**: To circumvent the alignment of large language models (LLMs), current optimization-based adversarial attacks usually craft adversarial prompts by maximizing the likelihood of a so-called affirmative response. An affirmative response is a manually designed start of a harmful answer to an inappropriate request. While it is often easy to craft prompts that yield a substantial likelihood for the affirmative response, the attacked model frequently does not complete the response in a harmful manner. Moreover, the affirmative objective is usually not adapted to model-specific preferences and essentially ignores the fact that LLMs output a distribution over responses. If low attack success under such an objective is taken as a measure of robustness, the true robustness might be grossly overestimated. To alleviate these flaws, we propose an adaptive and semantic optimization problem over the population of responses. We derive a generally applicable objective via the REINFORCE policy-gradient formalism and demonstrate its efficacy with the state-of-the-art jailbreak algorithms Greedy Coordinate Gradient (GCG) and Projected Gradient Descent (PGD). For example, our objective doubles the attack success rate (ASR) on Llama3 and increases the ASR from 2% to 50% with circuit breaker defense.

### Overconfident Oracles: Limitations of In Silico Sequence Design Benchmarking 
[[arxiv](https://arxiv.org/abs/2502.17246)] [[cool](https://papers.cool/arxiv/2502.17246)] [[pdf](https://arxiv.org/pdf/2502.17246)]
> **Authors**: Shikha Surana,Nathan Grinsztajn,Timothy Atkinson,Paul Duckworth,Thomas D. Barrett
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: ef:ICML 2024,AIfor Science Workshop
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Machine learning methods can automate the in silico design of biological sequences, aiming to reduce costs and accelerate medical research. Given the limited access to wet labs, in silico design methods commonly use an oracle model to evaluate de novo generated sequences. However, the use of different oracle models across methods makes it challenging to compare them reliably, motivating the question: are in silico sequence design benchmarks reliable? In this work, we examine 12 sequence design methods that utilise ML oracles common in the literature and find that there are significant challenges with their cross-consistency and reproducibility. Indeed, oracles differing by architecture, or even just training seed, are shown to yield conflicting relative performance with our analysis suggesting poor out-of-distribution generalisation as a key issue. To address these challenges, we propose supplementing the evaluation with a suite of biophysical measures to assess the viability of generated sequences and limit out-of-distribution sequences the oracle is required to score, thereby improving the robustness of the design procedure. Our work aims to highlight potential pitfalls in the current evaluation process and contribute to the development of robust benchmarks, ultimately driving the improvement of in silico design methods.

### Electrical Load Forecasting over Multihop Smart Metering Networks with Federated Learning 
[[arxiv](https://arxiv.org/abs/2502.17226)] [[cool](https://papers.cool/arxiv/2502.17226)] [[pdf](https://arxiv.org/pdf/2502.17226)]
> **Authors**: Ratun Rahman,Pablo Moriano,Samee U. Khan,Dinh C. Nguyen
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: arXiv admin note: text overlap with arXiv:2411.10619
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Electric load forecasting is essential for power management and stability in smart grids. This is mainly achieved via advanced metering infrastructure, where smart meters (SMs) record household energy data. Traditional machine learning (ML) methods are often employed for load forecasting but require data sharing which raises data privacy concerns. Federated learning (FL) can address this issue by running distributed ML models at local SMs without data exchange. However, current FL-based approaches struggle to achieve efficient load forecasting due to imbalanced data distribution across heterogeneous SMs. This paper presents a novel personalized federated learning (PFL) method for high-quality load forecasting in metering networks. A meta-learning-based strategy is developed to address data heterogeneity at local SMs in the collaborative training of local load forecasting models. Moreover, to minimize the load forecasting delays in our PFL model, we study a new latency optimization problem based on optimal resource allocation at SMs. A theoretical convergence analysis is also conducted to provide insights into FL design for federated load forecasting. Extensive simulations from real-world datasets show that our method outperforms existing approaches in terms of better load forecasting and reduced operational latency costs.

### Neural Attention: A Novel Mechanism for Enhanced Expressive Power in Transformer Models 
[[arxiv](https://arxiv.org/abs/2502.17206)] [[cool](https://papers.cool/arxiv/2502.17206)] [[pdf](https://arxiv.org/pdf/2502.17206)]
> **Authors**: Andrew DiGiugno,Ausif Mahmood
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: :I.2.7; I.2.6; I.5.1; I.4.8
- **标题**: None
- **领域**: 机器学习,神经和进化计算
- **Abstract**: Transformer models typically calculate attention matrices using dot products, which have limitations when capturing nonlinear relationships between embedding vectors. We propose Neural Attention, a technique that replaces dot products with feed-forward networks, enabling a more expressive representation of relationships between tokens. This approach modifies only the attention matrix calculation while preserving the matrix dimensions, making it easily adaptable to existing transformer-based architectures. We provide a detailed mathematical justification for why Neural Attention increases representational capacity and conduct controlled experiments to validate this claim. When comparing Neural Attention and Dot-Product Attention, NLP experiments on WikiText-103 show a reduction in perplexity of over 5 percent. Similarly, experiments on CIFAR-10 and CIFAR-100 show comparable improvements for image classification tasks. While Neural Attention introduces higher computational demands, we develop techniques to mitigate these challenges, ensuring practical usability without sacrificing the increased expressivity it provides. This work establishes Neural Attention as an effective means of enhancing the predictive capabilities of transformer models across a variety of applications.

### IGDA: Interactive Graph Discovery through Large Language Model Agents 
[[arxiv](https://arxiv.org/abs/2502.17189)] [[cool](https://papers.cool/arxiv/2502.17189)] [[pdf](https://arxiv.org/pdf/2502.17189)]
> **Authors**: Alex Havrilla,David Alvarez-Melis,Nicolo Fusi
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Large language models ($\textbf{LLMs}$) have emerged as a powerful method for discovery. Instead of utilizing numerical data, LLMs utilize associated variable $\textit{semantic metadata}$ to predict variable relationships. Simultaneously, LLMs demonstrate impressive abilities to act as black-box optimizers when given an objective $f$ and sequence of trials. We study LLMs at the intersection of these two capabilities by applying LLMs to the task of $\textit{interactive graph discovery}$: given a ground truth graph $G^*$ capturing variable relationships and a budget of $I$ edge experiments over $R$ rounds, minimize the distance between the predicted graph $\hat{G}_R$ and $G^*$ at the end of the $R$-th round. To solve this task we propose $\textbf{IGDA}$, a LLM-based pipeline incorporating two key components: 1) an LLM uncertainty-driven method for edge experiment selection 2) a local graph update strategy utilizing binary feedback from experiments to improve predictions for unselected neighboring edges. Experiments on eight different real-world graphs show our approach often outperforms all baselines including a state-of-the-art numerical method for interactive graph discovery. Further, we conduct a rigorous series of ablations dissecting the impact of each pipeline component. Finally, to assess the impact of memorization, we apply our interactive graph discovery strategy to a complex, new (as of July 2024) causal graph on protein transcription factors, finding strong performance in a setting where memorization is impossible. Overall, our results show IGDA to be a powerful method for graph discovery complementary to existing numerically driven approaches.

### Low-distortion and GPU-compatible Tree Embeddings in Hyperbolic Space 
[[arxiv](https://arxiv.org/abs/2502.17130)] [[cool](https://papers.cool/arxiv/2502.17130)] [[pdf](https://arxiv.org/pdf/2502.17130)]
> **Authors**: Max van Spengler,Pascal Mettes
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Embedding tree-like data, from hierarchies to ontologies and taxonomies, forms a well-studied problem for representing knowledge across many domains. Hyperbolic geometry provides a natural solution for embedding trees, with vastly superior performance over Euclidean embeddings. Recent literature has shown that hyperbolic tree embeddings can even be placed on top of neural networks for hierarchical knowledge integration in deep learning settings. For all applications, a faithful embedding of trees is needed, with combinatorial constructions emerging as the most effective direction. This paper identifies and solves two key limitations of existing works. First, the combinatorial construction hinges on finding highly separated points on a hypersphere, a notoriously difficult problem. Current approaches achieve poor separation, degrading the quality of the corresponding hyperbolic embedding. We propose highly separated Delaunay tree embeddings (HS-DTE), which integrates angular separation in a generalized formulation of Delaunay embeddings, leading to lower embedding distortion. Second, low-distortion requires additional precision. The current approach for increasing precision is to use multiple precision arithmetic, which renders the embeddings useless on GPUs in deep learning settings. We reformulate the combinatorial construction using floating point expansion arithmetic, leading to superior embedding quality while retaining utility on accelerated hardware.

### Sparse Hyperparametric Itakura-Saito NMF via Bi-Level Optimization 
[[arxiv](https://arxiv.org/abs/2502.17123)] [[cool](https://papers.cool/arxiv/2502.17123)] [[pdf](https://arxiv.org/pdf/2502.17123)]
> **Authors**: Laura Selicato,Flavia Esposito,Andersen Ang,Nicoletta Del Buono,Rafal Zdunek
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 22 pages, 5 figures, 4 tables
- **标题**: None
- **领域**: 机器学习,优化与控制
- **Abstract**: The selection of penalty hyperparameters is a critical aspect in Nonnegative Matrix Factorization (NMF), since these values control the trade-off between the reconstruction accuracy and the adherence to desired constraints. In this work, we focus on an NMF problem involving the Itakura-Saito (IS) divergence, effective for extracting low spectral density components from spectrograms of mixed signals, enhanced with sparsity constraints. We propose a new algorithm called SHINBO, which introduces a bi-level optimization framework to automatically and adaptively tune the row-dependent penalty hyperparameters, enhancing the ability of IS-NMF to isolate sparse, periodic signals against noise. Experimental results showed SHINBO ensures precise spectral decomposition and demonstrates superior performance in both synthetic and real-world applications. For the latter, SHINBO is particularly useful, as noninvasive vibration-based fault detection in rolling bearings, where the desired signal components often reside in high-frequency subbands but are obscured by stronger, spectrally broader noise. By addressing the critical issue of hyperparameter selection, SHINBO advances the state-of-the-art in signal recovery for complex, noise-dominated environments.

### Adversarial Training for Defense Against Label Poisoning Attacks 
[[arxiv](https://arxiv.org/abs/2502.17121)] [[cool](https://papers.cool/arxiv/2502.17121)] [[pdf](https://arxiv.org/pdf/2502.17121)]
> **Authors**: Melis Ilayda Bal,Volkan Cevher,Michael Muehlebach
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Accepted at the International Conference onLearningRepresentations (ICLR 2025)
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: As machine learning models grow in complexity and increasingly rely on publicly sourced data, such as the human-annotated labels used in training large language models, they become more vulnerable to label poisoning attacks. These attacks, in which adversaries subtly alter the labels within a training dataset, can severely degrade model performance, posing significant risks in critical applications. In this paper, we propose FLORAL, a novel adversarial training defense strategy based on support vector machines (SVMs) to counter these threats. Utilizing a bilevel optimization framework, we cast the training process as a non-zero-sum Stackelberg game between an attacker, who strategically poisons critical training labels, and the model, which seeks to recover from such attacks. Our approach accommodates various model architectures and employs a projected gradient descent algorithm with kernel SVMs for adversarial training. We provide a theoretical analysis of our algorithm's convergence properties and empirically evaluate FLORAL's effectiveness across diverse classification tasks. Compared to robust baselines and foundation models such as RoBERTa, FLORAL consistently achieves higher robust accuracy under increasing attacker budgets. These results underscore the potential of FLORAL to enhance the resilience of machine learning models against label poisoning threats, thereby ensuring robust classification in adversarial settings.

### Diffusion Models for Tabular Data: Challenges, Current Progress, and Future Directions 
[[arxiv](https://arxiv.org/abs/2502.17119)] [[cool](https://papers.cool/arxiv/2502.17119)] [[pdf](https://arxiv.org/pdf/2502.17119)]
> **Authors**: Zhong Li,Qi Huang,Lincen Yang,Jiayang Shi,Zhao Yang,Niki van Stein,Thomas Bäck,Matthijs van Leeuwen
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: In recent years, generative models have achieved remarkable performance across diverse applications, including image generation, text synthesis, audio creation, video generation, and data augmentation. Diffusion models have emerged as superior alternatives to Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) by addressing their limitations, such as training instability, mode collapse, and poor representation of multimodal distributions. This success has spurred widespread research interest. In the domain of tabular data, diffusion models have begun to showcase similar advantages over GANs and VAEs, achieving significant performance breakthroughs and demonstrating their potential for addressing unique challenges in tabular data modeling. However, while domains like images and time series have numerous surveys summarizing advancements in diffusion models, there remains a notable gap in the literature for tabular data. Despite the increasing interest in diffusion models for tabular data, there has been little effort to systematically review and summarize these developments. This lack of a dedicated survey limits a clear understanding of the challenges, progress, and future directions in this critical area. This survey addresses this gap by providing a comprehensive review of diffusion models for tabular data. Covering works from June 2015, when diffusion models emerged, to December 2024, we analyze nearly all relevant studies, with updates maintained in a \href{https://github.com/Diffusion-Model-Leiden/awesome-diffusion-models-for-tabular-data}{GitHub repository}. Assuming readers possess foundational knowledge of statistics and diffusion models, we employ mathematical formulations to deliver a rigorous and detailed review, aiming to promote developments in this emerging and exciting area.

### Generative Models in Decision Making: A Survey 
[[arxiv](https://arxiv.org/abs/2502.17100)] [[cool](https://papers.cool/arxiv/2502.17100)] [[pdf](https://arxiv.org/pdf/2502.17100)]
> **Authors**: Yinchuan Li,Xinyu Shao,Jianping Zhang,Haozhi Wang,Leo Maxime Brunswic,Kaiwen Zhou,Jiqian Dong,Kaiyang Guo,Xiu Li,Zhitang Chen,Jun Wang,Jianye Hao
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: In recent years, the exceptional performance of generative models in generative tasks has sparked significant interest in their integration into decision-making processes. Due to their ability to handle complex data distributions and their strong model capacity, generative models can be effectively incorporated into decision-making systems by generating trajectories that guide agents toward high-reward state-action regions or intermediate sub-goals. This paper presents a comprehensive review of the application of generative models in decision-making tasks. We classify seven fundamental types of generative models: energy-based models, generative adversarial networks, variational autoencoders, normalizing flows, diffusion models, generative flow networks, and autoregressive models. Regarding their applications, we categorize their functions into three main roles: controllers, modelers and optimizers, and discuss how each role contributes to decision-making. Furthermore, we examine the deployment of these models across five critical real-world decision-making scenarios. Finally, we summarize the strengths and limitations of current approaches and propose three key directions for advancing next-generation generative directive models: high-performance algorithms, large-scale generalized decision-making models, and self-evolving and adaptive models.

### Improved Diffusion-based Generative Model with Better Adversarial Robustness 
[[arxiv](https://arxiv.org/abs/2502.17099)] [[cool](https://papers.cool/arxiv/2502.17099)] [[pdf](https://arxiv.org/pdf/2502.17099)]
> **Authors**: Zekun Wang,Mingyang Yi,Shuchen Xue,Zhenguo Li,Ming Liu,Bing Qin,Zhi-Ming Ma
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: ICLR 2025
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: Diffusion Probabilistic Models (DPMs) have achieved significant success in generative tasks. However, their training and sampling processes suffer from the issue of distribution mismatch. During the denoising process, the input data distributions differ between the training and inference stages, potentially leading to inaccurate data generation. To obviate this, we analyze the training objective of DPMs and theoretically demonstrate that this mismatch can be alleviated through Distributionally Robust Optimization (DRO), which is equivalent to performing robustness-driven Adversarial Training (AT) on DPMs. Furthermore, for the recently proposed Consistency Model (CM), which distills the inference process of the DPM, we prove that its training objective also encounters the mismatch issue. Fortunately, this issue can be mitigated by AT as well. Based on these insights, we propose to conduct efficient AT on both DPM and CM. Finally, extensive empirical studies validate the effectiveness of AT in diffusion-based models. The code is available at https://github.com/kugwzk/AT_Diff.

### Forgetting Any Data at Any Time: A Theoretically Certified Unlearning Framework for Vertical Federated Learning 
[[arxiv](https://arxiv.org/abs/2502.17081)] [[cool](https://papers.cool/arxiv/2502.17081)] [[pdf](https://arxiv.org/pdf/2502.17081)]
> **Authors**: Linian Wang,Leye Wang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 18 pages
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Privacy concerns in machine learning are heightened by regulations such as the GDPR, which enforces the "right to be forgotten" (RTBF), driving the emergence of machine unlearning as a critical research field. Vertical Federated Learning (VFL) enables collaborative model training by aggregating a sample's features across distributed parties while preserving data privacy at each source. This paradigm has seen widespread adoption in healthcare, finance, and other privacy-sensitive domains. However, existing VFL systems lack robust mechanisms to comply with RTBF requirements, as unlearning methodologies for VFL remain underexplored. In this work, we introduce the first VFL framework with theoretically guaranteed unlearning capabilities, enabling the removal of any data at any time. Unlike prior approaches -- which impose restrictive assumptions on model architectures or data types for removal -- our solution is model- and data-agnostic, offering universal compatibility. Moreover, our framework supports asynchronous unlearning, eliminating the need for all parties to be simultaneously online during the forgetting process. These advancements address critical gaps in current VFL systems, ensuring compliance with RTBF while maintaining operational flexibility.We make all our implementations publicly available at https://github.com/wangln19/vertical-federated-unlearning.

### A comparative analysis of rank aggregation methods for the partial label ranking problem 
[[arxiv](https://arxiv.org/abs/2502.17077)] [[cool](https://papers.cool/arxiv/2502.17077)] [[pdf](https://arxiv.org/pdf/2502.17077)]
> **Authors**: Jiayi Wang,Juan C. Alfaro,Viktor Bengs
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: The label ranking problem is a supervised learning scenario in which the learner predicts a total order of the class labels for a given input instance. Recently, research has increasingly focused on the partial label ranking problem, a generalization of the label ranking problem that allows ties in the predicted orders. So far, most existing learning approaches for the partial label ranking problem rely on approximation algorithms for rank aggregation in the final prediction step. This paper explores several alternative aggregation methods for this critical step, including scoring-based and probabilistic-based rank aggregation approaches. To enhance their suitability for the more general partial label ranking problem, the investigated methods are extended to increase the likelihood of producing ties. Experimental evaluations on standard benchmarks demonstrate that scoring-based variants consistently outperform the current state-of-the-art method in handling incomplete information. In contrast, probabilistic-based variants fail to achieve competitive performance.

### Data Analysis Prediction over Multiple Unseen Datasets: A Vector Embedding Approach 
[[arxiv](https://arxiv.org/abs/2502.17060)] [[cool](https://papers.cool/arxiv/2502.17060)] [[pdf](https://arxiv.org/pdf/2502.17060)]
> **Authors**: Andreas Loizou,Dimitrios Tsoumakos
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: The massive increase in the data volume and dataset availability for analysts compels researchers to focus on data content and select high-quality datasets to enhance the performance of analytics operators. While selecting the highest quality data for analysis highly increases task accuracy and efficiency, it is still a hard task, especially when the number of available inputs is very large. To address this issue, we propose a novel methodology that infers the outcome of analytics operators by creating a model from datasets similar to the queried one. Dataset similarity is performed via projecting each dataset to a vector embedding representation. The vectorization process is performed using our proposed deep learning model NumTabData2Vec, which takes a whole dataset and projects it into a lower vector embedding representation space. Through experimental evaluation, we compare the prediction performance and the execution time of our framework to another state-of-the-art modelling operator framework, illustrating that our approach predicts analytics outcomes accurately. Furthermore, our vectorization model can project different real-world scenarios to a lower vector embedding representation and distinguish between them.

### Stable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam 
[[arxiv](https://arxiv.org/abs/2502.17055)] [[cool](https://papers.cool/arxiv/2502.17055)] [[pdf](https://arxiv.org/pdf/2502.17055)]
> **Authors**: Tianjin Huang,Haotian Hu,Zhenyu Zhang,Gaojie Jin,Xiang Li,Li Shen,Tianlong Chen,Lu Liu,Qingsong Wen,Zhangyang Wang,Shiwei Liu
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: This paper comprehensively evaluates several recently proposed optimizers for 4-bit training, revealing that low-bit precision amplifies sensitivity to learning rates and often causes unstable gradient norms, leading to divergence at higher learning rates. Among these, SPAM, a recent optimizer featuring momentum reset and spike-aware gradient clipping, achieves the best performance across various bit levels, but struggles to stabilize gradient norms, requiring careful learning rate tuning. To address these limitations, we propose Stable-SPAM, which incorporates enhanced gradient normalization and clipping techniques. In particular, Stable-SPAM (1) adaptively updates the clipping threshold for spiked gradients by tracking their historical maxima; (2) normalizes the entire gradient matrix based on its historical $l_2$-norm statistics; and $(3)$ inherits momentum reset from SPAM to periodically reset the first and second moments of Adam, mitigating the accumulation of spiked gradients. Extensive experiments show that Stable-SPAM effectively stabilizes gradient norms in 4-bit LLM training, delivering superior performance compared to Adam and SPAM. Notably, our 4-bit LLaMA-1B model trained with Stable-SPAM outperforms the BF16 LLaMA-1B trained with Adam by up to $2$ perplexity. Furthermore, when both models are trained in 4-bit, Stable-SPAM achieves the same loss as Adam while requiring only about half the training steps. Code is available at https://github.com/TianjinYellow/StableSPAM.git.

### Distributional Vision-Language Alignment by Cauchy-Schwarz Divergence 
[[arxiv](https://arxiv.org/abs/2502.17028)] [[cool](https://papers.cool/arxiv/2502.17028)] [[pdf](https://arxiv.org/pdf/2502.17028)]
> **Authors**: Wenzhe Yin,Zehao Xiao,Pan Zhou,Shujian Yu,Jiayi Shen,Jan-Jakob Sonke,Efstratios Gavves
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Multimodal alignment is crucial for various downstream tasks such as cross-modal generation and retrieval. Previous multimodal approaches like CLIP maximize the mutual information mainly by aligning pairwise samples across modalities while overlooking the distributional differences, leading to suboptimal alignment with modality gaps. In this paper, to overcome the limitation, we propose CS-Aligner, a novel and straightforward framework that performs distributional vision-language alignment by integrating Cauchy-Schwarz (CS) divergence with mutual information. In the proposed framework, we find that the CS divergence and mutual information serve complementary roles in multimodal alignment, capturing both the global distribution information of each modality and the pairwise semantic relationships, yielding tighter and more precise alignment. Moreover, CS-Aligher enables incorporating additional information from unpaired data and token-level representations, enhancing flexible and fine-grained alignment in practice. Experiments on text-to-image generation and cross-modality retrieval tasks demonstrate the effectiveness of our method on vision-language alignment.

### Advancing Eurasia Fire Understanding Through Machine Learning Techniques 
[[arxiv](https://arxiv.org/abs/2502.17023)] [[cool](https://papers.cool/arxiv/2502.17023)] [[pdf](https://arxiv.org/pdf/2502.17023)]
> **Authors**: Boris Kriuk
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 13 pages, 7 figures, 2 tables
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Modern fire management systems increasingly rely on satellite data and weather forecasting; however, access to comprehensive datasets remains limited due to proprietary restrictions. Despite the ecological significance of wildfires, large-scale, multi-regional research is constrained by data scarcity. Russian diverse ecosystems play a crucial role in shaping Eurasian fire dynamics, yet they remain underexplored. This study addresses existing gaps by introducing an open-access dataset that captures detailed fire incidents alongside corresponding meteorological conditions. We present one of the most extensive datasets available for wildfire analysis in Russia, covering 13 consecutive months of observations. Leveraging machine learning techniques, we conduct exploratory data analysis and develop predictive models to identify key fire behavior patterns across different fire categories and ecosystems. Our results highlight the critical influence of environmental factor patterns on fire occurrence and spread behavior. By improving the understanding of wildfire dynamics in Eurasia, this work contributes to more effective, data-driven approaches for proactive fire management in the face of evolving environmental conditions.

### Class-Dependent Perturbation Effects in Evaluating Time Series Attributions 
[[arxiv](https://arxiv.org/abs/2502.17022)] [[cool](https://papers.cool/arxiv/2502.17022)] [[pdf](https://arxiv.org/pdf/2502.17022)]
> **Authors**: Gregor Baer,Isel Grau,Chao Zhang,Pieter Van Gorp
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: As machine learning models become increasingly prevalent in time series applications, Explainable Artificial Intelligence (XAI) methods are essential for understanding their predictions. Within XAI, feature attribution methods aim to identify which input features contributed the most to a model's prediction, with their evaluation typically relying on perturbation-based metrics. Through empirical analysis across multiple datasets, model architectures, and perturbation strategies, we identify important class-dependent effects in these metrics: they show varying effectiveness across classes, achieving strong results for some while remaining less sensitive to others. In particular, we find that the most effective perturbation strategies often demonstrate the most pronounced class differences. Our analysis suggests that these effects arise from the learned biases of classifiers, indicating that perturbation-based evaluation may reflect specific model behaviors rather than intrinsic attribution quality. We propose an evaluation framework with a class-aware penalty term to help assess and account for these effects in evaluating feature attributions. Although our analysis focuses on time series classification, these class-dependent effects likely extend to other structured data domains where perturbation-based evaluation is common.

### Moving Past Single Metrics: Exploring Short-Text Clustering Across Multiple Resolutions 
[[arxiv](https://arxiv.org/abs/2502.17020)] [[cool](https://papers.cool/arxiv/2502.17020)] [[pdf](https://arxiv.org/pdf/2502.17020)]
> **Authors**: Justin Miller,Tristram Alexander
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 11 pages, 3 figures
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Cluster number is typically a parameter selected at the outset in clustering problems, and while impactful, the choice can often be difficult to justify. Inspired by bioinformatics, this study examines how the nature of clusters varies with cluster number, presenting a method for determining cluster robustness, and providing a systematic method for deciding on the cluster number. The study focuses specifically on short-text clustering, involving 30,000 political Twitter bios, where the sparse co-occurrence of words between texts makes finding meaningful clusters challenging. A metric of proportional stability is introduced to uncover the stability of specific clusters between cluster resolutions, and the results are visualised using Sankey diagrams to provide an interrogative tool for understanding the nature of the dataset. The visualisation provides an intuitive way to track cluster subdivision and reorganisation as cluster number increases, offering insights that static, single-resolution metrics cannot capture. The results show that instead of seeking a single 'optimal' solution, choosing a cluster number involves balancing informativeness and complexity.

### Erwin: A Tree-based Hierarchical Transformer for Large-scale Physical Systems 
[[arxiv](https://arxiv.org/abs/2502.17019)] [[cool](https://papers.cool/arxiv/2502.17019)] [[pdf](https://arxiv.org/pdf/2502.17019)]
> **Authors**: Maksim Zhdanov,Max Welling,Jan-Willem van de Meent
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: Large-scale physical systems defined on irregular grids pose significant scalability challenges for deep learning methods, especially in the presence of long-range interactions and multi-scale coupling. Traditional approaches that compute all pairwise interactions, such as attention, become computationally prohibitive as they scale quadratically with the number of nodes. We present Erwin, a hierarchical transformer inspired by methods from computational many-body physics, which combines the efficiency of tree-based algorithms with the expressivity of attention mechanisms. Erwin employs ball tree partitioning to organize computation, which enables linear-time attention by processing nodes in parallel within local neighborhoods of fixed size. Through progressive coarsening and refinement of the ball tree structure, complemented by a novel cross-ball interaction mechanism, it captures both fine-grained local details and global features. We demonstrate Erwin's effectiveness across multiple domains, including cosmology, molecular dynamics, and particle fluid dynamics, where it consistently outperforms baseline methods both in accuracy and computational efficiency.

### Unbiased and Sign Compression in Distributed Learning: Comparing Noise Resilience via SDEs 
[[arxiv](https://arxiv.org/abs/2502.17009)] [[cool](https://papers.cool/arxiv/2502.17009)] [[pdf](https://arxiv.org/pdf/2502.17009)]
> **Authors**: Enea Monzio Compagnoni,Rustem Islamov,Frank Norbert Proske,Aurelien Lucchi
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Accepted at AISTATS 2025 (Oral). arXiv admin note: substantial text overlap with arXiv:2411.15958
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Distributed methods are essential for handling machine learning pipelines comprising large-scale models and datasets. However, their benefits often come at the cost of increased communication overhead between the central server and agents, which can become the main bottleneck, making training costly or even unfeasible in such systems. Compression methods such as quantization and sparsification can alleviate this issue. Still, their robustness to large and heavy-tailed gradient noise, a phenomenon sometimes observed in language modeling, remains poorly understood. This work addresses this gap by analyzing Distributed Compressed SGD (DCSGD) and Distributed SignSGD (DSignSGD) using stochastic differential equations (SDEs). Our results show that DCSGD with unbiased compression is more vulnerable to noise in stochastic gradients, while DSignSGD remains robust, even under large and heavy-tailed noise. Additionally, we propose new scaling rules for hyperparameter tuning to mitigate performance degradation due to compression. These findings are empirically validated across multiple deep learning architectures and datasets, providing practical recommendations for distributed optimization.

### All You Need for Counterfactual Explainability Is Principled and Reliable Estimate of Aleatoric and Epistemic Uncertainty 
[[arxiv](https://arxiv.org/abs/2502.17007)] [[cool](https://papers.cool/arxiv/2502.17007)] [[pdf](https://arxiv.org/pdf/2502.17007)]
> **Authors**: Kacper Sokol,Eyke Hüllermeier
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: This position paper argues that, to its detriment, transparency research overlooks many foundational concepts of artificial intelligence. Here, we focus on uncertainty quantification -- in the context of ante-hoc interpretability and counterfactual explainability -- showing how its adoption could address key challenges in the field. First, we posit that uncertainty and ante-hoc interpretability offer complementary views of the same underlying idea; second, we assert that uncertainty provides a principled unifying framework for counterfactual explainability. Consequently, inherently transparent models can benefit from human-centred explanatory insights -- like counterfactuals -- which are otherwise missing. At a higher level, integrating artificial intelligence fundamentals into transparency research promises to yield more reliable, robust and understandable predictive models.

### Improving the Transferability of Adversarial Examples by Inverse Knowledge Distillation 
[[arxiv](https://arxiv.org/abs/2502.17003)] [[cool](https://papers.cool/arxiv/2502.17003)] [[pdf](https://arxiv.org/pdf/2502.17003)]
> **Authors**: Wenyuan Wu,Zheng Liu,Yong Chen,Chao Su,Dezhong Peng,Xu Wang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: In recent years, the rapid development of deep neural networks has brought increased attention to the security and robustness of these models. While existing adversarial attack algorithms have demonstrated success in improving adversarial transferability, their performance remains suboptimal due to a lack of consideration for the discrepancies between target and source models. To address this limitation, we propose a novel method, Inverse Knowledge Distillation (IKD), designed to enhance adversarial transferability effectively. IKD introduces a distillation-inspired loss function that seamlessly integrates with gradient-based attack methods, promoting diversity in attack gradients and mitigating overfitting to specific model architectures. By diversifying gradients, IKD enables the generation of adversarial samples with superior generalization capabilities across different models, significantly enhancing their effectiveness in black-box attack scenarios. Extensive experiments on the ImageNet dataset validate the effectiveness of our approach, demonstrating substantial improvements in the transferability and attack success rates of adversarial samples across a wide range of models.

### FADE: Why Bad Descriptions Happen to Good Features 
[[arxiv](https://arxiv.org/abs/2502.16994)] [[cool](https://papers.cool/arxiv/2502.16994)] [[pdf](https://arxiv.org/pdf/2502.16994)]
> **Authors**: Bruno Puri,Aakriti Jain,Elena Golimblevskaia,Patrick Kahardipraja,Thomas Wiegand,Wojciech Samek,Sebastian Lapuschkin
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Recent advances in mechanistic interpretability have highlighted the potential of automating interpretability pipelines in analyzing the latent representations within LLMs. While they may enhance our understanding of internal mechanisms, the field lacks standardized evaluation methods for assessing the validity of discovered features. We attempt to bridge this gap by introducing FADE: Feature Alignment to Description Evaluation, a scalable model-agnostic framework for evaluating feature-description alignment. FADE evaluates alignment across four key metrics - Clarity, Responsiveness, Purity, and Faithfulness - and systematically quantifies the causes for the misalignment of feature and their description. We apply FADE to analyze existing open-source feature descriptions, and assess key components of automated interpretability pipelines, aiming to enhance the quality of descriptions. Our findings highlight fundamental challenges in generating feature descriptions, particularly for SAEs as compared to MLP neurons, providing insights into the limitations and future directions of automated interpretability. We release FADE as an open-source package at: https://github.com/brunibrun/FADE.

### Muon is Scalable for LLM Training 
[[arxiv](https://arxiv.org/abs/2502.16982)] [[cool](https://papers.cool/arxiv/2502.16982)] [[pdf](https://arxiv.org/pdf/2502.16982)]
> **Authors**: Jingyuan Liu,Jianlin Su,Xingcheng Yao,Zhejun Jiang,Guokun Lai,Yulun Du,Yidao Qin,Weixin Xu,Enzhe Lu,Junjie Yan,Yanru Chen,Huabin Zheng,Yibo Liu,Shaowei Liu,Bohong Yin,Weiran He,Han Zhu,Yuzhi Wang,Jianzhou Wang,Mengnan Dong,Zheng Zhang,Yongsheng Kang,Hao Zhang,Xinran Xu,Yutao Zhang, et al. (3 additional authors not shown)
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Recently, the Muon optimizer based on matrix orthogonalization has demonstrated strong results in training small-scale language models, but the scalability to larger models has not been proven. We identify two crucial techniques for scaling up Muon: (1) adding weight decay and (2) carefully adjusting the per-parameter update scale. These techniques allow Muon to work out-of-the-box on large-scale training without the need of hyper-parameter tuning. Scaling law experiments indicate that Muon achieves $\sim\!2\times$ computational efficiency compared to AdamW with compute optimal training. Based on these improvements, we introduce Moonlight, a 3B/16B-parameter Mixture-of-Expert (MoE) model trained with 5.7T tokens using Muon. Our model improves the current Pareto frontier, achieving better performance with much fewer training FLOPs compared to prior models. We open-source our distributed Muon implementation that is memory optimal and communication efficient. We also release the pretrained, instruction-tuned, and intermediate checkpoints to support future research.

### Atten-Transformer: A Deep Learning Framework for User App Usage Prediction 
[[arxiv](https://arxiv.org/abs/2502.16957)] [[cool](https://papers.cool/arxiv/2502.16957)] [[pdf](https://arxiv.org/pdf/2502.16957)]
> **Authors**: Longlong Li,Cunquan Qu,Guanghui Wang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Accurately predicting smartphone app usage patterns is crucial for user experience optimization and targeted marketing. However, existing methods struggle to capture intricate dependencies in user behavior, particularly in sparse or complex usage scenarios. To address these challenges, we introduce Atten-Transformer, a novel model that integrates temporal attention with a Transformer network to dynamically identify and leverage key app usage patterns. Unlike conventional methods that primarily consider app order and duration, our approach employs a multi-dimensional feature representation, incorporating both feature encoding and temporal encoding to enhance predictive accuracy. The proposed attention mechanism effectively assigns importance to critical app usage moments, improving both model interpretability and generalization. Extensive experiments on multiple smartphone usage datasets, including LSapp and Tsinghua App Usage datasets, demonstrate that Atten-Transformer consistently outperforms state-of-the-art models across different data splits. Specifically, our model achieves a 45.24\% improvement in HR@1 on the Tsinghua dataset (Time-based Split) and a 18.25\% improvement in HR@1 on the LSapp dataset (Cold Start Split), showcasing its robustness across diverse app usage scenarios. These findings highlight the potential of integrating adaptive attention mechanisms in mobile usage forecasting, paving the way for enhanced user engagement and resource allocation.

### SparseTransX: Efficient Training of Translation-Based Knowledge Graph Embeddings Using Sparse Matrix Operations 
[[arxiv](https://arxiv.org/abs/2502.16949)] [[cool](https://papers.cool/arxiv/2502.16949)] [[pdf](https://arxiv.org/pdf/2502.16949)]
> **Authors**: Md Saidul Hoque Anik,Ariful Azad
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 15 pages. To appear in MLSys 2025
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Knowledge graph (KG) learning offers a powerful framework for generating new knowledge and making inferences. Training KG embedding can take a significantly long time, especially for larger datasets. Our analysis shows that the gradient computation of embedding is one of the dominant functions in the translation-based KG embedding training loop. We address this issue by replacing the core embedding computation with SpMM (Sparse-Dense Matrix Multiplication) kernels. This allows us to unify multiple scatter (and gather) operations as a single operation, reducing training time and memory usage. We create a general framework for training KG models using sparse kernels and implement four models, namely TransE, TransR, TransH, and TorusE. Our sparse implementations exhibit up to 5.3x speedup on the CPU and up to 4.2x speedup on the GPU with a significantly low GPU memory footprint. The speedups are consistent across large and small datasets for a given model. Our proposed sparse approach can be extended to accelerate other translation-based (such as TransC, TransM, etc.) and non-translational (such as DistMult, ComplEx, RotatE, etc.) models as well.

### Deep Minimax Classifiers for Imbalanced Datasets with a Small Number of Minority Samples 
[[arxiv](https://arxiv.org/abs/2502.16948)] [[cool](https://papers.cool/arxiv/2502.16948)] [[pdf](https://arxiv.org/pdf/2502.16948)]
> **Authors**: Hansung Choi,Daewon Seo
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: The concept of a minimax classifier is well-established in statistical decision theory, but its implementation via neural networks remains challenging, particularly in scenarios with imbalanced training data having a limited number of samples for minority classes. To address this issue, we propose a novel minimax learning algorithm designed to minimize the risk of worst-performing classes. Our algorithm iterates through two steps: a minimization step that trains the model based on a selected target prior, and a maximization step that updates the target prior towards the adversarial prior for the trained model. In the minimization, we introduce a targeted logit-adjustment loss function that efficiently identifies optimal decision boundaries under the target prior. Moreover, based on a new prior-dependent generalization bound that we obtained, we theoretically prove that our loss function has a better generalization capability than existing loss functions. During the maximization, we refine the target prior by shifting it towards the adversarial prior, depending on the worst-performing classes rather than on per-class risk estimates. Our maximization method is particularly robust in the regime of a small number of samples. Additionally, to adapt to overparameterized neural networks, we partition the entire training dataset into two subsets: one for model training during the minimization step and the other for updating the target prior during the maximization step. Our proposed algorithm has a provable convergence property, and empirical results indicate that our algorithm performs better than or is comparable to existing methods. All codes are publicly available at https://github.com/hansung-choi/TLA-linear-ascent.

### Using Machine Learning to Detect Fraudulent SMSs in Chichewa 
[[arxiv](https://arxiv.org/abs/2502.16947)] [[cool](https://papers.cool/arxiv/2502.16947)] [[pdf](https://arxiv.org/pdf/2502.16947)]
> **Authors**: Amelia Taylor,Amoss Robert
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: SMS enabled fraud is of great concern globally. Building classifiers based on machine learning for SMS fraud requires the use of suitable datasets for model training and validation. Most research has centred on the use of datasets of SMSs in English. This paper introduces a first dataset for SMS fraud detection in Chichewa, a major language in Africa, and reports on experiments with machine learning algorithms for classifying SMSs in Chichewa as fraud or non-fraud. We answer the broader research question of how feasible it is to develop machine learning classification models for Chichewa SMSs. To do that, we created three datasets. A small dataset of SMS in Chichewa was collected through primary research from a segment of the young population. We applied a label-preserving text transformations to increase its size. The enlarged dataset was translated into English using two approaches: human translation and machine translation. The Chichewa and the translated datasets were subjected to machine classification using random forest and logistic regression. Our findings indicate that both models achieved a promising accuracy of over 96% on the Chichewa dataset. There was a drop in performance when moving from the Chichewa to the translated dataset. This highlights the importance of data preprocessing, especially in multilingual or cross-lingual NLP tasks, and shows the challenges of relying on machine-translated text for training machine learning models. Our results underscore the importance of developing language specific models for SMS fraud detection to optimise accuracy and performance. Since most machine learning models require data preprocessing, it is essential to investigate the impact of the reliance on English-specific tools for data preprocessing.

### Lean and Mean: Decoupled Value Policy Optimization with Global Value Guidance 
[[arxiv](https://arxiv.org/abs/2502.16944)] [[cool](https://papers.cool/arxiv/2502.16944)] [[pdf](https://arxiv.org/pdf/2502.16944)]
> **Authors**: Chenghua Huang,Lu Wang,Fangkai Yang,Pu Zhao,Zhixu Li,Qingwei Lin,Dongmei Zhang,Saravan Rajmohan,Qi Zhang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 16 pages, 3 figures
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Proximal Policy Optimization (PPO)-based Reinforcement Learning from Human Feedback (RLHF) is essential for aligning large language models (LLMs) with human preferences. It requires joint training of an actor and critic with a pretrained, fixed reward model for guidance. This approach increases computational complexity and instability due to actor-critic interdependence. Additionally, PPO lacks access to true environment rewards in LLM tasks, limiting its adaptability. Under such conditions, pretraining a value model or a reward model becomes equivalent, as both provide fixed supervisory signals without new ground-truth feedback. To address these issues, we propose \textbf{Decoupled Value Policy Optimization (DVPO)}, a lean framework that replaces traditional reward modeling with a pretrained \emph{global value model (GVM)}. The GVM is conditioned on policy trajectories and predicts token-level return-to-go estimates. By decoupling value model from policy training (via frozen GVM-driven RL objectives), DVPO eliminates actor-critic interdependence, reducing GPU memory usage by 40\% and training time by 35\% compared to conventional RLHF. Experiments across benchmarks show DVPO outperforms efficient RLHF methods (e.g., DPO) while matching state-of-the-art PPO in performance.

### SUSTeR: Sparse Unstructured Spatio Temporal Reconstruction on Traffic Prediction 
[[arxiv](https://arxiv.org/abs/2502.16935)] [[cool](https://papers.cool/arxiv/2502.16935)] [[pdf](https://arxiv.org/pdf/2502.16935)]
> **Authors**: Yannick Wölker,Christian Beth,Matthias Renz,Arne Biastoch
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: accepted and presented at ACM SIGSPATIAL '23
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Mining spatio-temporal correlation patterns for traffic prediction is a well-studied field. However, most approaches are based on the assumption of the availability of and accessibility to a sufficiently dense data source, which is rather the rare case in reality. Traffic sensors in road networks are generally highly sparse in their distribution: fleet-based traffic sensing is sparse in space but also sparse in time. There are also other traffic application, besides road traffic, like moving objects in the marine space, where observations are sparsely and arbitrarily distributed in space. In this paper, we tackle the problem of traffic prediction on sparse and spatially irregular and non-deterministic traffic observations. We draw a border between imputations and this work as we consider high sparsity rates and no fixed sensor locations. We advance correlation mining methods with a Sparse Unstructured Spatio Temporal Reconstruction (SUSTeR) framework that reconstructs traffic states from sparse non-stationary observations. For the prediction the framework creates a hidden context traffic state which is enriched in a residual fashion with each observation. Such an assimilated hidden traffic state can be used by existing traffic prediction methods to predict future traffic states. We query these states with query locations from the spatial domain.

### Achieving Fair PCA Using Joint Eigenvalue Decomposition 
[[arxiv](https://arxiv.org/abs/2502.16933)] [[cool](https://papers.cool/arxiv/2502.16933)] [[pdf](https://arxiv.org/pdf/2502.16933)]
> **Authors**: Vidhi Rathore,Naresh Manwani
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Principal Component Analysis (PCA) is a widely used method for dimensionality reduction, but it often overlooks fairness, especially when working with data that includes demographic characteristics. This can lead to biased representations that disproportionately affect certain groups. To address this issue, our approach incorporates Joint Eigenvalue Decomposition (JEVD), a technique that enables the simultaneous diagonalization of multiple matrices, ensuring fair and efficient representations. We formally show that the optimal solution of JEVD leads to a fair PCA solution. By integrating JEVD with PCA, we strike an optimal balance between preserving data structure and promoting fairness across diverse groups. We demonstrate that our method outperforms existing baseline approaches in fairness and representational quality on various datasets. It retains the core advantages of PCA while ensuring that sensitive demographic attributes do not create disparities in the reduced representation.

### Machine learning and high dimensional vector search 
[[arxiv](https://arxiv.org/abs/2502.16931)] [[cool](https://papers.cool/arxiv/2502.16931)] [[pdf](https://arxiv.org/pdf/2502.16931)]
> **Authors**: Matthijs Douze
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Machine learning and vector search are two research topics that developed in parallel in nearby communities. However, unlike many other fields related to big data, machine learning has not significantly impacted vector search. In this opinion paper we attempt to explain this oddity. Along the way, we wander over the numerous bridges between the two fields.

### BigMac: A Communication-Efficient Mixture-of-Experts Model Structure for Fast Training and Inference 
[[arxiv](https://arxiv.org/abs/2502.16927)] [[cool](https://papers.cool/arxiv/2502.16927)] [[pdf](https://arxiv.org/pdf/2502.16927)]
> **Authors**: Zewen Jin,Shengnan Wang,Jiaan Zhu,Hongrui Zhan,Youhui Bai,Lin Zhang,Zhenyu Ming,Cheng Li
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: The Mixture-of-Experts (MoE) structure scales the Transformer-based large language models (LLMs) and improves their performance with only the sub-linear increase in computation resources. Recently, a fine-grained DeepSeekMoE structure is proposed, which can further improve the computing efficiency of MoE without performance degradation. However, the All-to-All communication introduced by MoE has become a bottleneck, especially for the fine-grained structure, which typically involves and activates more experts, hence contributing to heavier communication overhead. In this paper, we propose a novel MoE structure named BigMac, which is also fine-grained but with high communication efficiency. The innovation of BigMac is mainly due to that we abandon the \textbf{c}ommunicate-\textbf{d}escend-\textbf{a}scend-\textbf{c}ommunicate (CDAC) manner used by fine-grained MoE, which leads to the All-to-All communication always taking place at the highest dimension. Instead, BigMac designs an efficient \textbf{d}escend-\textbf{c}ommunicate-\textbf{c}ommunicate-\textbf{a}scend (DCCA) manner. Specifically, we add a descending and ascending projection at the entrance and exit of the expert, respectively, which enables the communication to perform at a very low dimension. Furthermore, to adapt to DCCA, we re-design the structure of small experts, ensuring that the expert in BigMac has enough complexity to address tokens. Experimental results show that BigMac achieves comparable or even better model quality than fine-grained MoEs with the same number of experts and a similar number of total parameters. Equally importantly, BigMac reduces the end-to-end latency by up to 3.09$\times$ for training and increases the throughput by up to 3.11$\times$ for inference on state-of-the-art AI computing frameworks including Megatron, Tutel, and DeepSpeed-Inference.

### Zero-shot Load Forecasting for Integrated Energy Systems: A Large Language Model-based Framework with Multi-task Learning 
[[arxiv](https://arxiv.org/abs/2502.16896)] [[cool](https://papers.cool/arxiv/2502.16896)] [[pdf](https://arxiv.org/pdf/2502.16896)]
> **Authors**: Jiaheng Li,Donghe Li,Ye Yang,Huan Xi,Yu Xiao,Li Sun,Dou An,Qingyu Yang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: The growing penetration of renewable energy sources in power systems has increased the complexity and uncertainty of load forecasting, especially for integrated energy systems with multiple energy carriers. Traditional forecasting methods heavily rely on historical data and exhibit limited transferability across different scenarios, posing significant challenges for emerging applications in smart grids and energy internet. This paper proposes the TSLLM-Load Forecasting Mechanism, a novel zero-shot load forecasting framework based on large language models (LLMs) to address these challenges. The framework consists of three key components: a data preprocessing module that handles multi-source energy load data, a time series prompt generation module that bridges the semantic gap between energy data and LLMs through multi-task learning and similarity alignment, and a prediction module that leverages pre-trained LLMs for accurate forecasting. The framework's effectiveness was validated on a real-world dataset comprising load profiles from 20 Australian solar-powered households, demonstrating superior performance in both conventional and zero-shot scenarios. In conventional testing, our method achieved a Mean Squared Error (MSE) of 0.4163 and a Mean Absolute Error (MAE) of 0.3760, outperforming existing approaches by at least 8\%. In zero-shot prediction experiments across 19 households, the framework maintained consistent accuracy with a total MSE of 11.2712 and MAE of 7.6709, showing at least 12\% improvement over current methods. The results validate the framework's potential for accurate and transferable load forecasting in integrated energy systems, particularly beneficial for renewable energy integration and smart grid applications.

### ReFocus: Reinforcing Mid-Frequency and Key-Frequency Modeling for Multivariate Time Series Forecasting 
[[arxiv](https://arxiv.org/abs/2502.16890)] [[cool](https://papers.cool/arxiv/2502.16890)] [[pdf](https://arxiv.org/pdf/2502.16890)]
> **Authors**: Guoqi Yu,Yaoming Li,Juncheng Wang,Xiaoyu Guo,Angelica I. Aviles-Rivero,Tong Yang,Shujun Wang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Under Review
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Recent advancements have progressively incorporated frequency-based techniques into deep learning models, leading to notable improvements in accuracy and efficiency for time series analysis tasks. However, the Mid-Frequency Spectrum Gap in the real-world time series, where the energy is concentrated at the low-frequency region while the middle-frequency band is negligible, hinders the ability of existing deep learning models to extract the crucial frequency information. Additionally, the shared Key-Frequency in multivariate time series, where different time series share indistinguishable frequency patterns, is rarely exploited by existing literature. This work introduces a novel module, Adaptive Mid-Frequency Energy Optimizer, based on convolution and residual learning, to emphasize the significance of mid-frequency bands. We also propose an Energy-based Key-Frequency Picking Block to capture shared Key-Frequency, which achieves superior inter-series modeling performance with fewer parameters. A novel Key-Frequency Enhanced Training strategy is employed to further enhance Key-Frequency modeling, where spectral information from other channels is randomly introduced into each channel. Our approach advanced multivariate time series forecasting on the challenging Traffic, ECL, and Solar benchmarks, reducing MSE by 4%, 6%, and 5% compared to the previous SOTA iTransformer. Code is available at this GitHub Repository: https://github.com/Levi-Ackman/ReFocus.

### Distributionally Robust Active Learning for Gaussian Process Regression 
[[arxiv](https://arxiv.org/abs/2502.16870)] [[cool](https://papers.cool/arxiv/2502.16870)] [[pdf](https://arxiv.org/pdf/2502.16870)]
> **Authors**: Shion Takeno,Yoshito Okura,Yu Inatsu,Aoyama Tatsuya,Tomonari Tanaka,Akahane Satoshi,Hiroyuki Hanada,Noriaki Hashimoto,Taro Murayama,Hanju Lee,Shinya Kojima,Ichiro Takeuchi
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 25 pages, 3 figures
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Gaussian process regression (GPR) or kernel ridge regression is a widely used and powerful tool for nonlinear prediction. Therefore, active learning (AL) for GPR, which actively collects data labels to achieve an accurate prediction with fewer data labels, is an important problem. However, existing AL methods do not theoretically guarantee prediction accuracy for target distribution. Furthermore, as discussed in the distributionally robust learning literature, specifying the target distribution is often difficult. Thus, this paper proposes two AL methods that effectively reduce the worst-case expected error for GPR, which is the worst-case expectation in target distribution candidates. We show an upper bound of the worst-case expected squared error, which suggests that the error will be arbitrarily small by a finite number of data labels under mild conditions. Finally, we demonstrate the effectiveness of the proposed methods through synthetic and real-world datasets.

### Improving LLM General Preference Alignment via Optimistic Online Mirror Descent 
[[arxiv](https://arxiv.org/abs/2502.16852)] [[cool](https://papers.cool/arxiv/2502.16852)] [[pdf](https://arxiv.org/pdf/2502.16852)]
> **Authors**: Yuheng Zhang,Dian Yu,Tao Ge,Linfeng Song,Zhichen Zeng,Haitao Mi,Nan Jiang,Dong Yu
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Reinforcement learning from human feedback (RLHF) has demonstrated remarkable effectiveness in aligning large language models (LLMs) with human preferences. Many existing alignment approaches rely on the Bradley-Terry (BT) model assumption, which assumes the existence of a ground-truth reward for each prompt-response pair. However, this assumption can be overly restrictive when modeling complex human preferences. In this paper, we drop the BT model assumption and study LLM alignment under general preferences, formulated as a two-player game. Drawing on theoretical insights from learning in games, we integrate optimistic online mirror descent into our alignment framework to approximate the Nash policy. Theoretically, we demonstrate that our approach achieves an $O(T^{-1})$ bound on the duality gap, improving upon the previous $O(T^{-1/2})$ result. More importantly, we implement our method and show through experiments that it outperforms state-of-the-art RLHF algorithms across multiple representative benchmarks.

## 多代理系统(cs.MA:Multiagent Systems)

### Leveraging Large Language Models for Effective and Explainable Multi-Agent Credit Assignment 
[[arxiv](https://arxiv.org/abs/2502.16863)] [[cool](https://papers.cool/arxiv/2502.16863)] [[pdf](https://arxiv.org/pdf/2502.16863)]
> **Authors**: Kartik Nagpal,Dayi Dong,Jean-Baptiste Bouvier,Negar Mehr
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 8 pages+Appendix, 6 Figures, AAMAS 2025
- **标题**: None
- **领域**: 多代理系统,机器学习,机器人技术
- **Abstract**: Recent work, spanning from autonomous vehicle coordination to in-space assembly, has shown the importance of learning collaborative behavior for enabling robots to achieve shared goals. A common approach for learning this cooperative behavior is to utilize the centralized-training decentralized-execution paradigm. However, this approach also introduces a new challenge: how do we evaluate the contributions of each agent's actions to the overall success or failure of the team. This credit assignment problem has remained open, and has been extensively studied in the Multi-Agent Reinforcement Learning literature. In fact, humans manually inspecting agent behavior often generate better credit evaluations than existing methods. We combine this observation with recent works which show Large Language Models demonstrate human-level performance at many pattern recognition tasks. Our key idea is to reformulate credit assignment to the two pattern recognition problems of sequence improvement and attribution, which motivates our novel LLM-MCA method. Our approach utilizes a centralized LLM reward-critic which numerically decomposes the environment reward based on the individualized contribution of each agent in the scenario. We then update the agents' policy networks based on this feedback. We also propose an extension LLM-TACA where our LLM critic performs explicit task assignment by passing an intermediary goal directly to each agent policy in the scenario. Both our methods far outperform the state-of-the-art on a variety of benchmarks, including Level-Based Foraging, Robotic Warehouse, and our new Spaceworld benchmark which incorporates collision-related safety constraints. As an artifact of our methods, we generate large trajectory datasets with each timestep annotated with per-agent reward information, as sampled from our LLM critics.

## 网络和互联网架构(cs.NI:Networking and Internet Architecture)

### Toward Agentic AI: Generative Information Retrieval Inspired Intelligent Communications and Networking 
[[arxiv](https://arxiv.org/abs/2502.16866)] [[cool](https://papers.cool/arxiv/2502.16866)] [[pdf](https://arxiv.org/pdf/2502.16866)]
> **Authors**: Ruichen Zhang,Shunpu Tang,Yinqiu Liu,Dusit Niyato,Zehui Xiong,Sumei Sun,Shiwen Mao,Zhu Han
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 7 pages, 4 figures
- **标题**: None
- **领域**: 网络和互联网架构,人工智能
- **Abstract**: The increasing complexity and scale of modern telecommunications networks demand intelligent automation to enhance efficiency, adaptability, and resilience. Agentic AI has emerged as a key paradigm for intelligent communications and networking, enabling AI-driven agents to perceive, reason, decide, and act within dynamic networking environments. However, effective decision-making in telecom applications, such as network planning, management, and resource allocation, requires integrating retrieval mechanisms that support multi-hop reasoning, historical cross-referencing, and compliance with evolving 3GPP standards. This article presents a forward-looking perspective on generative information retrieval-inspired intelligent communications and networking, emphasizing the role of knowledge acquisition, processing, and retrieval in agentic AI for telecom systems. We first provide a comprehensive review of generative information retrieval strategies, including traditional retrieval, hybrid retrieval, semantic retrieval, knowledge-based retrieval, and agentic contextual retrieval. We then analyze their advantages, limitations, and suitability for various networking scenarios. Next, we present a survey about their applications in communications and networking. Additionally, we introduce an agentic contextual retrieval framework to enhance telecom-specific planning by integrating multi-source retrieval, structured reasoning, and self-reflective validation. Experimental results demonstrate that our framework significantly improves answer accuracy, explanation consistency, and retrieval efficiency compared to traditional and semantic retrieval methods. Finally, we outline future research directions.

## 机器人技术(cs.RO:Robotics)

### CAML: Collaborative Auxiliary Modality Learning for Multi-Agent Systems 
[[arxiv](https://arxiv.org/abs/2502.17821)] [[cool](https://papers.cool/arxiv/2502.17821)] [[pdf](https://arxiv.org/pdf/2502.17821)]
> **Authors**: Rui Liu,Yu Shen,Peng Gao,Pratap Tokekar,Ming Lin
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,人工智能,机器学习
- **Abstract**: Multi-modality learning has become a crucial technique for improving the performance of machine learning applications across domains such as autonomous driving, robotics, and perception systems. While existing frameworks such as Auxiliary Modality Learning (AML) effectively utilize multiple data sources during training and enable inference with reduced modalities, they primarily operate in a single-agent context. This limitation is particularly critical in dynamic environments, such as connected autonomous vehicles (CAV), where incomplete data coverage can lead to decision-making blind spots. To address these challenges, we propose Collaborative Auxiliary Modality Learning ($\textbf{CAML}$), a novel multi-agent multi-modality framework that enables agents to collaborate and share multimodal data during training while allowing inference with reduced modalities per agent during testing. We systematically analyze the effectiveness of $\textbf{CAML}$ from the perspective of uncertainty reduction and data coverage, providing theoretical insights into its advantages over AML. Experimental results in collaborative decision-making for CAV in accident-prone scenarios demonstrate that \ours~achieves up to a ${\bf 58.13}\%$ improvement in accident detection. Additionally, we validate $\textbf{CAML}$ on real-world aerial-ground robot data for collaborative semantic segmentation, achieving up to a ${\bf 10.61}\%$ improvement in mIoU.

### Safe Multi-Agent Navigation guided by Goal-Conditioned Safe Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.17813)] [[cool](https://papers.cool/arxiv/2502.17813)] [[pdf](https://arxiv.org/pdf/2502.17813)]
> **Authors**: Meng Feng,Viraj Parimi,Brian Williams
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Due to the limitation "The abstract field cannot be longer than 1,920 characters", the abstract here is shorter than that in the PDF file
- **标题**: None
- **领域**: 机器人技术,机器学习
- **Abstract**: Safe navigation is essential for autonomous systems operating in hazardous environments. Traditional planning methods excel at long-horizon tasks but rely on a predefined graph with fixed distance metrics. In contrast, safe Reinforcement Learning (RL) can learn complex behaviors without relying on manual heuristics but fails to solve long-horizon tasks, particularly in goal-conditioned and multi-agent scenarios. In this paper, we introduce a novel method that integrates the strengths of both planning and safe RL. Our method leverages goal-conditioned RL and safe RL to learn a goal-conditioned policy for navigation while concurrently estimating cumulative distance and safety levels using learned value functions via an automated self-training algorithm. By constructing a graph with states from the replay buffer, our method prunes unsafe edges and generates a waypoint-based plan that the agent follows until reaching its goal, effectively balancing faster and safer routes over extended distances. Utilizing this unified high-level graph and a shared low-level goal-conditioned safe RL policy, we extend this approach to address the multi-agent safe navigation problem. In particular, we leverage Conflict-Based Search (CBS) to create waypoint-based plans for multiple agents allowing for their safe navigation over extended horizons. This integration enhances the scalability of goal-conditioned safe RL in multi-agent scenarios, enabling efficient coordination among agents. Extensive benchmarking against state-of-the-art baselines demonstrates the effectiveness of our method in achieving distance goals safely for multiple agents in complex and hazardous environments. Our code will be released to support future research.

### Toward 6-DOF Autonomous Underwater Vehicle Energy-Aware Position Control based on Deep Reinforcement Learning: Preliminary Results 
[[arxiv](https://arxiv.org/abs/2502.17742)] [[cool](https://papers.cool/arxiv/2502.17742)] [[pdf](https://arxiv.org/pdf/2502.17742)]
> **Authors**: Gustavo Boré,Vicente Sufán,Sebastián Rodríguez-Martínez,Giancarlo Troni
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 6 pages, 5 figures, submitted to 2024 IEEE OES AUV Symposium
- **标题**: None
- **领域**: 机器人技术,机器学习,系统与控制
- **Abstract**: The use of autonomous underwater vehicles (AUVs) for surveying, mapping, and inspecting unexplored underwater areas plays a crucial role, where maneuverability and power efficiency are key factors for extending the use of these platforms, making six degrees of freedom (6-DOF) holonomic platforms essential tools. Although Proportional-Integral-Derivative (PID) and Model Predictive Control controllers are widely used in these applications, they often require accurate system knowledge, struggle with repeatability when facing payload or configuration changes, and can be time-consuming to fine-tune. While more advanced methods based on Deep Reinforcement Learning (DRL) have been proposed, they are typically limited to operating in fewer degrees of freedom. This paper proposes a novel DRL-based approach for controlling holonomic 6-DOF AUVs using the Truncated Quantile Critics (TQC) algorithm, which does not require manual tuning and directly feeds commands to the thrusters without prior knowledge of their configuration. Furthermore, it incorporates power consumption directly into the reward function. Simulation results show that the TQC High-Performance method achieves better performance to a fine-tuned PID controller when reaching a goal point, while the TQC Energy-Aware method demonstrates slightly lower performance but consumes 30% less power on average.

### Learning Decentralized Swarms Using Rotation Equivariant Graph Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.17612)] [[cool](https://papers.cool/arxiv/2502.17612)] [[pdf](https://arxiv.org/pdf/2502.17612)]
> **Authors**: Taos Transue,Bao Wang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: correcting contact information
- **标题**: None
- **领域**: 机器人技术,机器学习
- **Abstract**: The orchestration of agents to optimize a collective objective without centralized control is challenging yet crucial for applications such as controlling autonomous fleets, and surveillance and reconnaissance using sensor networks. Decentralized controller design has been inspired by self-organization found in nature, with a prominent source of inspiration being flocking; however, decentralized controllers struggle to maintain flock cohesion. The graph neural network (GNN) architecture has emerged as an indispensable machine learning tool for developing decentralized controllers capable of maintaining flock cohesion, but they fail to exploit the symmetries present in flocking dynamics, hindering their generalizability. We enforce rotation equivariance and translation invariance symmetries in decentralized flocking GNN controllers and achieve comparable flocking control with 70% less training data and 75% fewer trainable weights than existing GNN controllers without these symmetries enforced. We also show that our symmetry-aware controller generalizes better than existing GNN controllers. Code and animations are available at http://github.com/Utah-Math-Data-Science/Equivariant-Decentralized-Controllers.

### V-HOP: Visuo-Haptic 6D Object Pose Tracking 
[[arxiv](https://arxiv.org/abs/2502.17434)] [[cool](https://papers.cool/arxiv/2502.17434)] [[pdf](https://arxiv.org/pdf/2502.17434)]
> **Authors**: Hongyu Li,Mingxi Jia,Tuluhan Akbulut,Yu Xiang,George Konidaris,Srinath Sridhar
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,人工智能,计算机视觉和模式识别
- **Abstract**: Humans naturally integrate vision and haptics for robust object perception during manipulation. The loss of either modality significantly degrades performance. Inspired by this multisensory integration, prior object pose estimation research has attempted to combine visual and haptic/tactile feedback. Although these works demonstrate improvements in controlled environments or synthetic datasets, they often underperform vision-only approaches in real-world settings due to poor generalization across diverse grippers, sensor layouts, or sim-to-real environments. Furthermore, they typically estimate the object pose for each frame independently, resulting in less coherent tracking over sequences in real-world deployments. To address these limitations, we introduce a novel unified haptic representation that effectively handles multiple gripper embodiments. Building on this representation, we introduce a new visuo-haptic transformer-based object pose tracker that seamlessly integrates visual and haptic input. We validate our framework in our dataset and the Feelsight dataset, demonstrating significant performance improvement on challenging sequences. Notably, our method achieves superior generalization and robustness across novel embodiments, objects, and sensor types (both taxel-based and vision-based tactile sensors). In real-world experiments, we demonstrate that our approach outperforms state-of-the-art visual trackers by a large margin. We further show that we can achieve precise manipulation tasks by incorporating our real-time object tracking result into motion plans, underscoring the advantages of visuo-haptic perception. Our model and dataset will be made open source upon acceptance of the paper. Project website: https://lhy.xyz/projects/v-hop/

### FACTR: Force-Attending Curriculum Training for Contact-Rich Policy Learning 
[[arxiv](https://arxiv.org/abs/2502.17432)] [[cool](https://papers.cool/arxiv/2502.17432)] [[pdf](https://arxiv.org/pdf/2502.17432)]
> **Authors**: Jason Jingzhou Liu,Yulong Li,Kenneth Shaw,Tony Tao,Ruslan Salakhutdinov,Deepak Pathak
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Website at https://jasonjzliu.com/factr/
- **标题**: None
- **领域**: 机器人技术,人工智能,计算机视觉和模式识别,机器学习
- **Abstract**: Many contact-rich tasks humans perform, such as box pickup or rolling dough, rely on force feedback for reliable execution. However, this force information, which is readily available in most robot arms, is not commonly used in teleoperation and policy learning. Consequently, robot behavior is often limited to quasi-static kinematic tasks that do not require intricate force-feedback. In this paper, we first present a low-cost, intuitive, bilateral teleoperation setup that relays external forces of the follower arm back to the teacher arm, facilitating data collection for complex, contact-rich tasks. We then introduce FACTR, a policy learning method that employs a curriculum which corrupts the visual input with decreasing intensity throughout training. The curriculum prevents our transformer-based policy from over-fitting to the visual input and guides the policy to properly attend to the force modality. We demonstrate that by fully utilizing the force information, our method significantly improves generalization to unseen objects by 43\% compared to baseline approaches without a curriculum. Video results and instructions at https://jasonjzliu.com/factr/

### TDMPBC: Self-Imitative Reinforcement Learning for Humanoid Robot Control 
[[arxiv](https://arxiv.org/abs/2502.17322)] [[cool](https://papers.cool/arxiv/2502.17322)] [[pdf](https://arxiv.org/pdf/2502.17322)]
> **Authors**: Zifeng Zhuang,Diyuan Shi,Runze Suo,Xiao He,Hongyin Zhang,Ting Wang,Shangke Lyu,Donglin Wang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,人工智能,机器学习
- **Abstract**: Complex high-dimensional spaces with high Degree-of-Freedom and complicated action spaces, such as humanoid robots equipped with dexterous hands, pose significant challenges for reinforcement learning (RL) algorithms, which need to wisely balance exploration and exploitation under limited sample budgets. In general, feasible regions for accomplishing tasks within complex high-dimensional spaces are exceedingly narrow. For instance, in the context of humanoid robot motion control, the vast majority of space corresponds to falling, while only a minuscule fraction corresponds to standing upright, which is conducive to the completion of downstream tasks. Once the robot explores into a potentially task-relevant region, it should place greater emphasis on the data within that region. Building on this insight, we propose the $\textbf{S}$elf-$\textbf{I}$mitative $\textbf{R}$einforcement $\textbf{L}$earning ($\textbf{SIRL}$) framework, where the RL algorithm also imitates potentially task-relevant trajectories. Specifically, trajectory return is utilized to determine its relevance to the task and an additional behavior cloning is adopted whose weight is dynamically adjusted based on the trajectory return. As a result, our proposed algorithm achieves 120% performance improvement on the challenging HumanoidBench with 5% extra computation overhead. With further visualization, we find the significant performance gain does lead to meaningful behavior improvement that several tasks are solved successfully.

### Tidiness Score-Guided Monte Carlo Tree Search for Visual Tabletop Rearrangement 
[[arxiv](https://arxiv.org/abs/2502.17235)] [[cool](https://papers.cool/arxiv/2502.17235)] [[pdf](https://arxiv.org/pdf/2502.17235)]
> **Authors**: Hogun Kee,Wooseok Oh,Minjae Kang,Hyemin Ahn,Songhwai Oh
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 9 pages, 8 figures
- **标题**: None
- **领域**: 机器人技术,人工智能,计算机视觉和模式识别,机器学习
- **Abstract**: In this paper, we present the tidiness score-guided Monte Carlo tree search (TSMCTS), a novel framework designed to address the tabletop tidying up problem using only an RGB-D camera. We address two major problems for tabletop tidying up problem: (1) the lack of public datasets and benchmarks, and (2) the difficulty of specifying the goal configuration of unseen objects. We address the former by presenting the tabletop tidying up (TTU) dataset, a structured dataset collected in simulation. Using this dataset, we train a vision-based discriminator capable of predicting the tidiness score. This discriminator can consistently evaluate the degree of tidiness across unseen configurations, including real-world scenes. Addressing the second problem, we employ Monte Carlo tree search (MCTS) to find tidying trajectories without specifying explicit goals. Instead of providing specific goals, we demonstrate that our MCTS-based planner can find diverse tidied configurations using the tidiness score as a guidance. Consequently, we propose TSMCTS, which integrates a tidiness discriminator with an MCTS-based tidying planner to find optimal tidied arrangements. TSMCTS has successfully demonstrated its capability across various environments, including coffee tables, dining tables, office desks, and bathrooms. The TTU dataset is available at: https://github.com/rllab-snu/TTU-Dataset.

### Humanoid Whole-Body Locomotion on Narrow Terrain via Dynamic Balance and Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.17219)] [[cool](https://papers.cool/arxiv/2502.17219)] [[pdf](https://arxiv.org/pdf/2502.17219)]
> **Authors**: Weiji Xie,Chenjia Bai,Jiyuan Shi,Junkai Yang,Yunfei Ge,Weinan Zhang,Xuelong Li
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: This work has been submitted to the IEEE for possible publication
- **标题**: None
- **领域**: 机器人技术,机器学习
- **Abstract**: Humans possess delicate dynamic balance mechanisms that enable them to maintain stability across diverse terrains and under extreme conditions. However, despite significant advances recently, existing locomotion algorithms for humanoid robots are still struggle to traverse extreme environments, especially in cases that lack external perception (e.g., vision or LiDAR). This is because current methods often rely on gait-based or perception-condition rewards, lacking effective mechanisms to handle unobservable obstacles and sudden balance loss. To address this challenge, we propose a novel whole-body locomotion algorithm based on dynamic balance and Reinforcement Learning (RL) that enables humanoid robots to traverse extreme terrains, particularly narrow pathways and unexpected obstacles, using only proprioception. Specifically, we introduce a dynamic balance mechanism by leveraging an extended measure of Zero-Moment Point (ZMP)-driven rewards and task-driven rewards in a whole-body actor-critic framework, aiming to achieve coordinated actions of the upper and lower limbs for robust locomotion. Experiments conducted on a full-sized Unitree H1-2 robot verify the ability of our method to maintain balance on extremely narrow terrains and under external disturbances, demonstrating its effectiveness in enhancing the robot's adaptability to complex environments. The videos are given at https://whole-body-loco.github.io.

### Characterizing Structured versus Unstructured Environments based on Pedestrians' and Vehicles' Motion Trajectories 
[[arxiv](https://arxiv.org/abs/2502.16847)] [[cool](https://papers.cool/arxiv/2502.16847)] [[pdf](https://arxiv.org/pdf/2502.16847)]
> **Authors**: Mahsa Golchoubian,Moojan Ghafurian,Nasser Lashgarian Azad,Kerstin Dautenhahn
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: ef:IEEE Intelligent Transportation Systems Conference (ITSC), 2023
- **标题**: None
- **领域**: 机器人技术,人工智能,计算机视觉和模式识别,机器学习
- **Abstract**: Trajectory behaviours of pedestrians and vehicles operating close to each other can be different in unstructured compared to structured environments. These differences in the motion behaviour are valuable to be considered in the trajectory prediction algorithm of an autonomous vehicle. However, the available datasets on pedestrians' and vehicles' trajectories that are commonly used as benchmarks for trajectory prediction have not been classified based on the nature of their environment. On the other hand, the definitions provided for unstructured and structured environments are rather qualitative and hard to be used for justifying the type of a given environment. In this paper, we have compared different existing datasets based on a couple of extracted trajectory features, such as mean speed and trajectory variability. Through K-means clustering and generalized linear models, we propose more quantitative measures for distinguishing the two different types of environments. Our results show that features such as trajectory variability, stop fraction and density of pedestrians are different among the two environmental types and can be used to classify the existing datasets.

## 声音(cs.SD:Sound)

### VANPY: Voice Analysis Framework 
[[arxiv](https://arxiv.org/abs/2502.17579)] [[cool](https://papers.cool/arxiv/2502.17579)] [[pdf](https://arxiv.org/pdf/2502.17579)]
> **Authors**: Gregory Koushnir,Michael Fire,Galit Fuhrmann Alpert,Dima Kagan
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 声音,机器学习,音频和语音处理
- **Abstract**: Voice data is increasingly being used in modern digital communications, yet there is still a lack of comprehensive tools for automated voice analysis and characterization. To this end, we developed the VANPY (Voice Analysis in Python) framework for automated pre-processing, feature extraction, and classification of voice data. The VANPY is an open-source end-to-end comprehensive framework that was developed for the purpose of speaker characterization from voice data. The framework is designed with extensibility in mind, allowing for easy integration of new components and adaptation to various voice analysis applications. It currently incorporates over fifteen voice analysis components - including music/speech separation, voice activity detection, speaker embedding, vocal feature extraction, and various classification models. Four of the VANPY's components were developed in-house and integrated into the framework to extend its speaker characterization capabilities: gender classification, emotion classification, age regression, and height regression. The models demonstrate robust performance across various datasets, although not surpassing state-of-the-art performance. As a proof of concept, we demonstrate the framework's ability to extract speaker characteristics on a use-case challenge of analyzing character voices from the movie "Pulp Fiction." The results illustrate the framework's capability to extract multiple speaker characteristics, including gender, age, height, emotion type, and emotion intensity measured across three dimensions: arousal, dominance, and valence.

### Perceptual Noise-Masking with Music through Deep Spectral Envelope Shaping 
[[arxiv](https://arxiv.org/abs/2502.17527)] [[cool](https://papers.cool/arxiv/2502.17527)] [[pdf](https://arxiv.org/pdf/2502.17527)]
> **Authors**: Clémentine Berger,Roland Badeau,Slim Essid
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: ef:IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, Apr 2025, Hyderabad, India
- **标题**: None
- **领域**: 声音,人工智能,音频和语音处理,信号处理
- **Abstract**: People often listen to music in noisy environments, seeking to isolate themselves from ambient sounds. Indeed, a music signal can mask some of the noise's frequency components due to the effect of simultaneous masking. In this article, we propose a neural network based on a psychoacoustic masking model, designed to enhance the music's ability to mask ambient noise by reshaping its spectral envelope with predicted filter frequency responses. The model is trained with a perceptual loss function that balances two constraints: effectively masking the noise while preserving the original music mix and the user's chosen listening level. We evaluate our approach on simulated data replicating a user's experience of listening to music with headphones in a noisy environment. The results, based on defined objective metrics, demonstrate that our system improves the state of the art.

### Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition and Translation 
[[arxiv](https://arxiv.org/abs/2502.17380)] [[cool](https://papers.cool/arxiv/2502.17380)] [[pdf](https://arxiv.org/pdf/2502.17380)]
> **Authors**: Qiuming Zhao,Guangzhi Sun,Chao Zhang,Mingxing Xu,Thomas Fang Zheng
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 13 pages, submitted to ACL 2025
- **标题**: None
- **领域**: 声音,人工智能,计算语言学,音频和语音处理
- **Abstract**: Language diversity presents a significant challenge in speech-to-text (S2T) tasks, such as automatic speech recognition and translation. Traditional multi-task training approaches aim to address this by jointly optimizing multiple speech recognition and translation tasks across various languages. While models like Whisper, built on these strategies, demonstrate strong performance, they still face issues of high computational cost, language interference, suboptimal training configurations, and limited extensibility. To overcome these challenges, we introduce LoRS-Merging (low-rank and sparse model merging), a novel technique designed to efficiently integrate models trained on different languages or tasks while preserving performance and reducing computational overhead. LoRS-Merging combines low-rank and sparse pruning to retain essential structures while eliminating redundant parameters, mitigating language and task interference, and enhancing extensibility. Experimental results across a range of languages demonstrate that LoRS-Merging reduces the word error rate by 10% and improves BLEU scores by 4% compared to conventional multi-lingual multi-task training baselines. Our findings suggest that model merging, particularly LoRS-Merging, is a scalable and effective complement to traditional multi-lingual training strategies for S2T applications.

### Supervised contrastive learning from weakly-labeled audio segments for musical version matching 
[[arxiv](https://arxiv.org/abs/2502.16936)] [[cool](https://papers.cool/arxiv/2502.16936)] [[pdf](https://arxiv.org/pdf/2502.16936)]
> **Authors**: Joan Serrà,R. Oguz Araz,Dmitry Bogdanov,Yuki Mitsufuji
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 15 pages, 6 figures, 7 tables; includes Appendix
- **标题**: None
- **领域**: 声音,人工智能,机器学习,音频和语音处理,机器学习
- **Abstract**: Detecting musical versions (different renditions of the same piece) is a challenging task with important applications. Because of the ground truth nature, existing approaches match musical versions at the track level (e.g., whole song). However, most applications require to match them at the segment level (e.g., 20s chunks). In addition, existing approaches resort to classification and triplet losses, disregarding more recent losses that could bring meaningful improvements. In this paper, we propose a method to learn from weakly annotated segments, together with a contrastive loss variant that outperforms well-studied alternatives. The former is based on pairwise segment distance reductions, while the latter modifies an existing loss following decoupling, hyper-parameter, and geometric considerations. With these two elements, we do not only achieve state-of-the-art results in the standard track-level evaluation, but we also obtain a breakthrough performance in a segment-level evaluation. We believe that, due to the generality of the challenges addressed here, the proposed methods may find utility in domains beyond audio or musical version matching.

### ENACT-Heart -- ENsemble-based Assessment Using CNN and Transformer on Heart Sounds 
[[arxiv](https://arxiv.org/abs/2502.16914)] [[cool](https://papers.cool/arxiv/2502.16914)] [[pdf](https://arxiv.org/pdf/2502.16914)]
> **Authors**: Jiho Han,Adnan Shaout
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Accepted but not published in Global Digital Health Knowledge Exchange & Empowerment Conference (gDigiHealth.KEE)
- **标题**: None
- **领域**: 声音,人工智能,音频和语音处理
- **Abstract**: This study explores the application of Vision Transformer (ViT) principles in audio analysis, specifically focusing on heart sounds. This paper introduces ENACT-Heart - a novel ensemble approach that leverages the complementary strengths of Convolutional Neural Networks (CNN) and ViT through a Mixture of Experts (MoE) framework, achieving a remarkable classification accuracy of 97.52%. This outperforms the individual contributions of ViT (93.88%) and CNN (95.45%), demonstrating the potential for enhanced diagnostic accuracy in cardiovascular health monitoring. These results demonstrate the potential of ensemble methods in enhancing classification performance for cardiovascular health monitoring and diagnosis.

## 软件工程(cs.SE:Software Engineering)

### AI Agentic workflows and Enterprise APIs: Adapting API architectures for the age of AI agents 
[[arxiv](https://arxiv.org/abs/2502.17443)] [[cool](https://papers.cool/arxiv/2502.17443)] [[pdf](https://arxiv.org/pdf/2502.17443)]
> **Authors**: Vaibhav Tupe,Shrinath Thube
> **First submission**: 2025-01-22
> **First announcement**: 2025-02-25
> **comment**: :I.2.0; D.2.11; D.2.12; K.6.5; I.2.11
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: The rapid advancement of Generative AI has catalyzed the emergence of autonomous AI agents, presenting unprecedented challenges for enterprise computing infrastructures. Current enterprise API architectures are predominantly designed for human-driven, predefined interaction patterns, rendering them ill-equipped to support intelligent agents' dynamic, goal-oriented behaviors. This research systematically examines the architectural adaptations for enterprise APIs to support AI agentic workflows effectively. Through a comprehensive analysis of existing API design paradigms, agent interaction models, and emerging technological constraints, the paper develops a strategic framework for API transformation. The study employs a mixed-method approach, combining theoretical modeling, comparative analysis, and exploratory design principles to address critical challenges in standardization, performance, and intelligent interaction. The proposed research contributes a conceptual model for next-generation enterprise APIs that can seamlessly integrate with autonomous AI agent ecosystems, offering significant implications for future enterprise computing architectures.

### Thinking Before Running! Efficient Code Generation with Thorough Exploration and Optimal Refinement 
[[arxiv](https://arxiv.org/abs/2502.17442)] [[cool](https://papers.cool/arxiv/2502.17442)] [[pdf](https://arxiv.org/pdf/2502.17442)]
> **Authors**: Xiaoqing Zhang,Yuhan Liu,Flood Sung,Xiuying Chen,Rui Yan
> **First submission**: 2024-12-30
> **First announcement**: 2025-02-25
> **comment**: 14 pages, 10 figures
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: Code generation is crucial in software engineering for automating the coding process efficiently. While test-time computation methods show promise, they suffer from high latency due to multiple computation rounds. To overcome this, we introduce ThinkCoder, a framework that combines thorough exploration with optimal refinement. The exploration phase diversifies the solution space by searching for potential solutions, followed by a refinement phase that enhances precision. This approach allows us to select the best solution through careful consideration before taking action, avoiding excessive trial and error. To further minimize test-time computation overhead, we introduce preference-driven optimization with Reinforced Self-Training (ReST), which uses exploration trajectories from ThinkCoder to guide LLM's evolution. By learning preferences, this approach improves LLM's exploration efficiency, reducing computational costs while maintaining accuracy. ThinkCoder boosts the performance of multiple base LLMs, excelling on benchmarks like HumanEval and MBPP. Compared to SOTA models, it improves Pass@1 by 1.5\% over MapCoder with just 21.7\% of the computation cost. Against AgentCoder, ThinkCoder achieves a 0.6\% higher Pass@1 after 2 rounds, outperforming AgentCoder's 5 rounds. Additionally, ReST with success trajectories enhances efficiency, allowing models like LLaMA2-7B to achieve competitive results using only 20\% of the computational resources. These results highlight the framework's effectiveness and scalability.

### Renaissance of Literate Programming in the Era of LLMs: Enhancing LLM-Based Code Generation in Large-Scale Projects 
[[arxiv](https://arxiv.org/abs/2502.17441)] [[cool](https://papers.cool/arxiv/2502.17441)] [[pdf](https://arxiv.org/pdf/2502.17441)]
> **Authors**: Wuyang Zhang,Yansong Li,Zeyu Dong,Yu Wu,Yingyao Zhou,Duolei Wang,Songsirou Xing,Chichun Zhou,Da Shen
> **First submission**: 2024-12-25
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,机器学习
- **Abstract**: Large Language Models (LLMs) have helped programmers increase efficiency through code generation, comprehension, and repair. However, their application to large-scale projects remains challenging due to complex interdependencies and the extensive size of modern codebases. Although Knuth's concept of Literate Programming (LP) combines code and natural language to convey logic and intent, its potential for enhancing relationships in large projects has not been fully explored. In this study, we introduce the idea of Interoperable LP (ILP), which leverages literate programming principles to enhance the development of both small-scale documents and large-scale projects with LLMs. We investigate how LLMs perform under ILP-style instructions for both document-oriented tasks and entire projects. Recognizing that many researchers rely on well-structured templates to guide LLMs, we propose a concise prompt engineering method to write LP documents so LLMs can better be involved in code generation. We also examine the capacity of various LLMs to generate Scheme and Python code on the RepoBench benchmark, illustrating the advantages of our approach. Our findings indicate that ILP with LLMs can enhance LLM-based code generation in large-scale project development.

### GenAIOps for GenAI Model-Agility 
[[arxiv](https://arxiv.org/abs/2502.17440)] [[cool](https://papers.cool/arxiv/2502.17440)] [[pdf](https://arxiv.org/pdf/2502.17440)]
> **Authors**: Ken Ueno,Makoto Kogo,Hiromi Kawatsu,Yohsuke Uchiumi,Michiaki Tatsubori
> **First submission**: 2024-12-18
> **First announcement**: 2025-02-25
> **comment**: 8 pages, 3 figures, 2 tables
- **标题**: None
- **领域**: 软件工程,机器学习
- **Abstract**: AI-agility, with which an organization can be quickly adapted to its business priorities, is desired even for the development and operations of generative AI (GenAI) applications. Especially in this paper, we discuss so-called GenAI Model-agility, which we define as the readiness to be flexibly adapted to base foundation models as diverse as the model providers and versions. First, for handling issues specific to generative AI, we first define a methodology of GenAI application development and operations, as GenAIOps, to identify the problem of application quality degradation caused by changes to the underlying foundation models. We study prompt tuning technologies, which look promising to address this problem, and discuss their effectiveness and limitations through case studies using existing tools.

### Large Language Models as Realistic Microservice Trace Generators 
[[arxiv](https://arxiv.org/abs/2502.17439)] [[cool](https://papers.cool/arxiv/2502.17439)] [[pdf](https://arxiv.org/pdf/2502.17439)]
> **Authors**: Donghyun Kim,Sriram Ravula,Taemin Ha,Alexandros G. Dimakis,Daehyeok Kim,Aditya Akella
> **First submission**: 2024-12-16
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,人工智能,分布式、并行和集群计算,操作系统
- **Abstract**: Workload traces are essential to understand complex computer systems' behavior and manage processing and memory resources. Since real-world traces are hard to obtain, synthetic trace generation is a promising alternative. This paper proposes a first-of-a-kind approach that relies on training a large language model (LLM) to generate synthetic workload traces, specifically microservice call graphs. To capture complex and arbitrary hierarchical structures and implicit constraints in such traces, we show how to fine-tune LLMs to generate recursively, making call graph generation a sequence of easier steps. To further enforce learning constraints in traces and generate uncommon situations, we argue for applying additional instruction tuning steps to align our model with the desired trace features. Our evaluation results show that we can generate diverse realistic traces under various conditions and outperform existing methods in accuracy and validity. We demonstrate that our synthetically generated traces can effectively replace real data to optimize important microservice management tasks. Additionally, our model adapts to downstream trace-related tasks, such as predicting key trace features and infilling missing data.

### Continuous Integration Practices in Machine Learning Projects: The Practitioners` Perspective 
[[arxiv](https://arxiv.org/abs/2502.17378)] [[cool](https://papers.cool/arxiv/2502.17378)] [[pdf](https://arxiv.org/pdf/2502.17378)]
> **Authors**: João Helis Bernardo,Daniel Alencar da Costa,Filipe Roseiro Cogo,Sérgio Queiróz de Medeiros,Uirá Kulesza
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,机器学习
- **Abstract**: Continuous Integration (CI) is a cornerstone of modern software development. However, while widely adopted in traditional software projects, applying CI practices to Machine Learning (ML) projects presents distinctive characteristics. For example, our previous work revealed that ML projects often experience longer build durations and lower test coverage rates compared to their non-ML counterparts. Building on these quantitative findings, this study surveys 155 practitioners from 47 ML projects to investigate the underlying reasons for these distinctive characteristics through a qualitative perspective. Practitioners highlighted eight key differences, including test complexity, infrastructure requirements, and build duration and stability. Common challenges mentioned by practitioners include higher project complexity, model training demands, extensive data handling, increased computational resource needs, and dependency management, all contributing to extended build durations. Furthermore, ML systems' non-deterministic nature, data dependencies, and computational constraints were identified as significant barriers to effective testing. The key takeaway from this study is that while foundational CI principles remain valuable, ML projects require tailored approaches to address their unique challenges. To bridge this gap, we propose a set of ML-specific CI practices, including tracking model performance metrics and prioritizing test execution within CI pipelines. Additionally, our findings highlight the importance of fostering interdisciplinary collaboration to strengthen the testing culture in ML projects. By bridging quantitative findings with practitioners' insights, this study provides a deeper understanding of the interplay between CI practices and the unique demands of ML projects, laying the groundwork for more efficient and robust CI strategies in this domain.

## 社交和信息网络(cs.SI:Social and Information Networks)

### Utilizing Social Media Analytics to Detect Trends in Saudi Arabias Evolving Market 
[[arxiv](https://arxiv.org/abs/2502.16871)] [[cool](https://papers.cool/arxiv/2502.16871)] [[pdf](https://arxiv.org/pdf/2502.16871)]
> **Authors**: Kanwal Aalijah
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 8 pages
- **标题**: None
- **领域**: 社交和信息网络,人工智能
- **Abstract**: Saudi Arabia faced a swift economic growth and societal transformation under Vision 2030. This offers a unique opportunity to track emerging trends in the region, which will ultimately pave the way for new business and investment possibilities. This paper explores how AI and social media analytics can identify and track trends across sectors such as construction, food and beverage, tourism, technology, and entertainment thereby helping the businesses make informed decisions. By leveraging a tailored AI-driven methodology, we analyzed millions of social media posts each month, classifying discussions and calculating scores to track the trends. The approach not only uncovered the emerging trends but also shows diminishing trends. Our methodology is able to predict the emergence and growth of trends by utilizing social media data. This approach has potential for adaptation in other regions. Ultimately, our findings highlight how ongoing, AI-powered trend analysis can enable more effective, data-informed business and development strategies in an increasingly dynamic environment.

## 普通经济学(econ.GN:General Economics)

### Real-time Monitoring of Economic Shocks using Company Websites 
[[arxiv](https://arxiv.org/abs/2502.17161)] [[cool](https://papers.cool/arxiv/2502.17161)] [[pdf](https://arxiv.org/pdf/2502.17161)]
> **Authors**: Michael Koenig,Jakob Rauch,Martin Woerter
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 普通经济学,人工智能,计算语言学,数据分析、统计和概率
- **Abstract**: Understanding the effects of economic shocks on firms is critical for analyzing economic growth and resilience. We introduce a Web-Based Affectedness Indicator (WAI), a general-purpose tool for real-time monitoring of economic disruptions across diverse contexts. By leveraging Large Language Model (LLM) assisted classification and information extraction on texts from over five million company websites, WAI quantifies the degree and nature of firms' responses to external shocks. Using the COVID-19 pandemic as a specific application, we show that WAI is highly correlated with pandemic containment measures and reliably predicts firm performance. Unlike traditional data sources, WAI provides timely firm-level information across industries and geographies worldwide that would otherwise be unavailable due to institutional and data availability constraints. This methodology offers significant potential for monitoring and mitigating the impact of technological, political, financial, health or environmental crises, and represents a transformative tool for adaptive policy-making and economic resilience.

## 图像和视频处理(eess.IV:Image and Video Processing)

### TagGAN: A Generative Model for Data Tagging 
[[arxiv](https://arxiv.org/abs/2502.17836)] [[cool](https://papers.cool/arxiv/2502.17836)] [[pdf](https://arxiv.org/pdf/2502.17836)]
> **Authors**: Muhammad Nawaz,Basma Nasir,Tehseen Zia,Zawar Hussain,Catarina Moreira
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习
- **Abstract**: Precise identification and localization of disease-specific features at the pixel-level are particularly important for early diagnosis, disease progression monitoring, and effective treatment in medical image analysis. However, conventional diagnostic AI systems lack decision transparency and cannot operate well in environments where there is a lack of pixel-level annotations. In this study, we propose a novel Generative Adversarial Networks (GANs)-based framework, TagGAN, which is tailored for weakly-supervised fine-grained disease map generation from purely image-level labeled data. TagGAN generates a pixel-level disease map during domain translation from an abnormal image to a normal representation. Later, this map is subtracted from the input abnormal image to convert it into its normal counterpart while preserving all the critical anatomical details. Our method is first to generate fine-grained disease maps to visualize disease lesions in a weekly supervised setting without requiring pixel-level annotations. This development enhances the interpretability of diagnostic AI by providing precise visualizations of disease-specific regions. It also introduces automated binary mask generation to assist radiologists. Empirical evaluations carried out on the benchmark datasets, CheXpert, TBX11K, and COVID-19, demonstrate the capability of TagGAN to outperform current top models in accurately identifying disease-specific pixels. This outcome highlights the capability of the proposed model to tag medical images, significantly reducing the workload for radiologists by eliminating the need for binary masks during training.

### RELICT: A Replica Detection Framework for Medical Image Generation 
[[arxiv](https://arxiv.org/abs/2502.17360)] [[cool](https://papers.cool/arxiv/2502.17360)] [[pdf](https://arxiv.org/pdf/2502.17360)]
> **Authors**: Orhun Utku Aydin,Alexander Koch,Adam Hilbert,Jana Rieger,Felix Lohrke,Fujimaro Ishida,Satoru Tanioka,Dietmar Frey
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **Abstract**: Despite the potential of synthetic medical data for augmenting and improving the generalizability of deep learning models, memorization in generative models can lead to unintended leakage of sensitive patient information and limit model utility. Thus, the use of memorizing generative models in the medical domain can jeopardize patient privacy. We propose a framework for identifying replicas, i.e. nearly identical copies of the training data, in synthetic medical image datasets. Our REpLIca deteCTion (RELICT) framework for medical image generative models evaluates image similarity using three complementary approaches: (1) voxel-level analysis, (2) feature-level analysis by a pretrained medical foundation model, and (3) segmentation-level analysis. Two clinically relevant 3D generative modelling use cases were investigated: non-contrast head CT with intracerebral hemorrhage (N=774) and time-of-flight MR angiography of the Circle of Willis (N=1,782). Expert visual scoring was used as the reference standard to assess the presence of replicas. We report the balanced accuracy at the optimal threshold to assess replica classification performance. The reference visual rating identified 45 of 50 and 5 of 50 generated images as replicas for the NCCT and TOF-MRA use cases, respectively. Image-level and feature-level measures perfectly classified replicas with a balanced accuracy of 1 when an optimal threshold was selected for the NCCT use case. A perfect classification of replicas for the TOF-MRA case was not possible at any threshold, with the segmentation-level analysis achieving a balanced accuracy of 0.79. Replica detection is a crucial but neglected validation step for the development of generative models in medical imaging. The proposed RELICT framework provides a standardized, easy-to-use tool for replica detection and aims to facilitate responsible and ethical medical image synthesis.

### Motion-Robust T2* Quantification from Gradient Echo MRI with Physics-Informed Deep Learning 
[[arxiv](https://arxiv.org/abs/2502.17209)] [[cool](https://papers.cool/arxiv/2502.17209)] [[pdf](https://arxiv.org/pdf/2502.17209)]
> **Authors**: Hannah Eichhorn,Veronika Spieker,Kerstin Hammernik,Elisa Saks,Lina Felsner,Kilian Weiss,Christine Preibisch,Julia A. Schnabel
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Under Review
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习,医学物理
- **Abstract**: Purpose: T2* quantification from gradient echo magnetic resonance imaging is particularly affected by subject motion due to the high sensitivity to magnetic field inhomogeneities, which are influenced by motion and might cause signal loss. Thus, motion correction is crucial to obtain high-quality T2* maps. Methods: We extend our previously introduced learning-based physics-informed motion correction method, PHIMO, by utilizing acquisition knowledge to enhance the reconstruction performance for challenging motion patterns and increase PHIMO's robustness to varying strengths of magnetic field inhomogeneities across the brain. We perform comprehensive evaluations regarding motion detection accuracy and image quality for data with simulated and real motion. Results: Our extended version of PHIMO outperforms the learning-based baseline methods both qualitatively and quantitatively with respect to line detection and image quality. Moreover, PHIMO performs on-par with a conventional state-of-the-art motion correction method for T2* quantification from gradient echo MRI, which relies on redundant data acquisition. Conclusion: PHIMO's competitive motion correction performance, combined with a reduction in acquisition time by over 40% compared to the state-of-the-art method, make it a promising solution for motion-robust T2* quantification in research settings and clinical routine.

### M3DA: Benchmark for Unsupervised Domain Adaptation in 3D Medical Image Segmentation 
[[arxiv](https://arxiv.org/abs/2502.17029)] [[cool](https://papers.cool/arxiv/2502.17029)] [[pdf](https://arxiv.org/pdf/2502.17029)]
> **Authors**: Boris Shirokikh,Anvar Kurmukov,Mariia Donskova,Valentin Samokhin,Mikhail Belyaev,Ivan Oseledets
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 17 pages,7 figures,11 tables
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: Domain shift presents a significant challenge in applying Deep Learning to the segmentation of 3D medical images from sources like Magnetic Resonance Imaging (MRI) and Computed Tomography (CT). Although numerous Domain Adaptation methods have been developed to address this issue, they are often evaluated under impractical data shift scenarios. Specifically, the medical imaging datasets used are often either private, too small for robust training and evaluation, or limited to single or synthetic tasks. To overcome these limitations, we introduce a M3DA /"mEd@/ benchmark comprising four publicly available, multiclass segmentation datasets. We have designed eight domain pairs featuring diverse and practically relevant distribution shifts. These include inter-modality shifts between MRI and CT and intra-modality shifts among various MRI acquisition parameters, different CT radiation doses, and presence/absence of contrast enhancement in images. Within the proposed benchmark, we evaluate more than ten existing domain adaptation methods. Our results show that none of them can consistently close the performance gap between the domains. For instance, the most effective method reduces the performance gap by about 62% across the tasks. This highlights the need for developing novel domain adaptation algorithms to enhance the robustness and scalability of deep learning models in medical imaging. We made our M3DA benchmark publicly available: https://github.com/BorisShirokikh/M3DA.

## 信号处理(eess.SP:Signal Processing)

### CLEP-GAN: An Innovative Approach to Subject-Independent ECG Reconstruction from PPG Signals 
[[arxiv](https://arxiv.org/abs/2502.17536)] [[cool](https://papers.cool/arxiv/2502.17536)] [[pdf](https://arxiv.org/pdf/2502.17536)]
> **Authors**: Xiaoyan Li,Shixin Xu,Faisal Habib,Neda Aminnejad,Arvind Gupta,Huaxiong Huang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 信号处理,机器学习
- **Abstract**: This study addresses the challenge of reconstructing unseen ECG signals from PPG signals, a critical task for non-invasive cardiac monitoring. While numerous public ECG-PPG datasets are available, they lack the diversity seen in image datasets, and data collection processes often introduce noise, complicating ECG reconstruction from PPG even with advanced machine learning models. To tackle these challenges, we first introduce a novel synthetic ECG-PPG data generation technique using an ODE model to enhance training diversity. Next, we develop a novel subject-independent PPG-to-ECG reconstruction model that integrates contrastive learning, adversarial learning, and attention gating, achieving results comparable to or even surpassing existing approaches for unseen ECG reconstruction. Finally, we examine factors such as sex and age that impact reconstruction accuracy, emphasizing the importance of considering demographic diversity during model training and dataset augmentation.

### A Machine Learning Approach for Design of Frequency Selective Surface based Radar Absorbing Material via Image Prediction 
[[arxiv](https://arxiv.org/abs/2502.17534)] [[cool](https://papers.cool/arxiv/2502.17534)] [[pdf](https://arxiv.org/pdf/2502.17534)]
> **Authors**: Vijay Kumar Sutrakar,Anjana P K,Sajal Kesharwani,Siddharth Bisariya
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 信号处理,材料科学,机器学习
- **Abstract**: The paper presents an innovative methodology for designing frequency selective surface (FSS) based radar absorbing materials using machine learning (ML) technique. In conventional electromagnetic design, unit cell dimensions of FSS are used as input and absorption coefficient is then predicted for a given design. In this paper, absorption coefficient is considered as input to ML model and image of FSS unit cell is predicted. Later, this image is used for generating the FSS unit cell parameters. Eleven different ML models are studied over a wide frequency band of 1GHz to 30GHz. Out of which six ML models (i.e. (a) Random Forest classification, (b) K- Neighbors Classification, (c) Grid search regression, (d) Random Forest regression, (e) Decision tree classification, and (f) Decision tree regression) show training accuracy more than 90%. The absorption coefficients with varying frequencies of these predicted images are subsequently evaluated using commercial electromagnetic solver. The performance of these ML models is encouraging, and it can be used for accelerating design and optimization of high performance FSS based radar absorbing material for advanced electromagnetic applications in future.

### Multimodal Bearing Fault Classification Under Variable Conditions: A 1D CNN with Transfer Learning 
[[arxiv](https://arxiv.org/abs/2502.17524)] [[cool](https://papers.cool/arxiv/2502.17524)] [[pdf](https://arxiv.org/pdf/2502.17524)]
> **Authors**: Tasfiq E. Alam,Md Manjurul Ahsan,Shivakumar Raman
> **First submission**: 2025-02-23
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 信号处理,人工智能,机器学习
- **Abstract**: Bearings play an integral role in ensuring the reliability and efficiency of rotating machinery - reducing friction and handling critical loads. Bearing failures that constitute up to 90% of mechanical faults highlight the imperative need for reliable condition monitoring and fault detection. This study proposes a multimodal bearing fault classification approach that relies on vibration and motor phase current signals within a one-dimensional convolutional neural network (1D CNN) framework. The method fuses features from multiple signals to enhance the accuracy of fault detection. Under the baseline condition (1,500 rpm, 0.7 Nm load torque, and 1,000 N radial force), the model reaches an accuracy of 96% with addition of L2 regularization. This represents a notable improvement of 2% compared to the non-regularized model. In addition, the model demonstrates robust performance across three distinct operating conditions by employing transfer learning (TL) strategies. Among the tested TL variants, the approach that preserves parameters up to the first max-pool layer and then adjusts subsequent layers achieves the highest performance. While this approach attains excellent accuracy across varied conditions, it requires more computational time due to its greater number of trainable parameters. To address resource constraints, less computationally intensive models offer feasible trade-offs, albeit at a slight accuracy cost. Overall, this multimodal 1D CNN framework with late fusion and TL strategies lays a foundation for more accurate, adaptable, and efficient bearing fault classification in industrial environments with variable operating conditions.

### Attention-based UAV Trajectory Optimization for Wireless Power Transfer-assisted IoT Systems 
[[arxiv](https://arxiv.org/abs/2502.17517)] [[cool](https://papers.cool/arxiv/2502.17517)] [[pdf](https://arxiv.org/pdf/2502.17517)]
> **Authors**: Li Dong,Feibo Jiang,Yubo Peng
> **First submission**: 2025-02-22
> **First announcement**: 2025-02-25
> **comment**: ef:IEEE Transactions on Industrial Electronics,2025
- **标题**: None
- **领域**: 信号处理,人工智能
- **Abstract**: Unmanned Aerial Vehicles (UAVs) in Wireless Power Transfer (WPT)-assisted Internet of Things (IoT) systems face the following challenges: limited resources and suboptimal trajectory planning. Reinforcement learning-based trajectory planning schemes face issues of low search efficiency and learning instability when optimizing large-scale systems. To address these issues, we present an Attention-based UAV Trajectory Optimization (AUTO) framework based on the graph transformer, which consists of an Attention Trajectory Optimization Model (ATOM) and a Trajectory lEarNing Method based on Actor-critic (TENMA). In ATOM, a graph encoder is used to calculate the self-attention characteristics of all IoTDs, and a trajectory decoder is developed to optimize the number and trajectories of UAVs. TENMA then trains the ATOM using an improved Actor-Critic method, in which the real reward of the system is applied as the baseline to reduce variances in the critic network. This method is suitable for high-quality and large-scale multi-UAV trajectory planning. Finally, we develop numerous experiments, including a hardware experiment in the field case, to verify the feasibility and efficiency of the AUTO framework.

### Accuracy of Wearable ECG Parameter Calculation Method for Long QT and First-Degree A-V Block Detection: A Multi-Center Real-World Study with External Validations Compared to Standard ECG Machines and Cardiologist Assessments 
[[arxiv](https://arxiv.org/abs/2502.17499)] [[cool](https://papers.cool/arxiv/2502.17499)] [[pdf](https://arxiv.org/pdf/2502.17499)]
> **Authors**: Sumei Fan,Deyun Zhang,Yue Wang,Shijia Geng,Kun Lu,Meng Sang,Weilun Xu,Haixue Wang,Qinghao Zhao,Chuandong Cheng,Peng Wang,Shenda Hong
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-25
> **comment**: 37 pages, 8 figures, 6 tables
- **标题**: None
- **领域**: 信号处理,人工智能,机器学习,数值分析
- **Abstract**: In recent years, wearable devices have revolutionized cardiac monitoring by enabling continuous, non-invasive ECG recording in real-world settings. Despite these advances, the accuracy of ECG parameter calculations (PR interval, QRS interval, QT interval, etc.) from wearables remains to be rigorously validated against conventional ECG machines and expert clinician assessments. In this large-scale, multicenter study, we evaluated FeatureDB, a novel algorithm for automated computation of ECG parameters from wearable single-lead signals Three diverse datasets were employed: the AHMU-FH dataset (n=88,874), the CSE dataset (n=106), and the HeartVoice-ECG-lite dataset (n=369) with annotations provided by two experienced cardiologists. FeatureDB demonstrates a statistically significant correlation with key parameters (PR interval, QRS duration, QT interval, and QTc) calculated by standard ECG machines and annotated by clinical doctors. Bland-Altman analysis confirms a high level of agreement.Moreover,FeatureDB exhibited robust diagnostic performance in detecting Long QT syndrome (LQT) and atrioventricular block interval abnormalities (AVBI),with excellent area under the ROC curve (LQT: 0.836, AVBI: 0.861),accuracy (LQT: 0.856, AVBI: 0.845),sensitivity (LQT: 0.815, AVBI: 0.877),and specificity (LQT: 0.856, AVBI: 0.845).This further validates its clinical reliability. These results validate the clinical applicability of FeatureDB for wearable ECG analysis and highlight its potential to bridge the gap between traditional diagnostic methods and emerging wearable technologies.Ultimately,this study supports integrating wearable ECG devices into large-scale cardiovascular disease management and early intervention strategies,and it highlights the potential of wearable ECG technologies to deliver accurate,clinically relevant cardiac monitoring while advancing broader applications in cardiovascular care.

### Using Graph Convolutional Networks to Address fMRI Small Data Problems 
[[arxiv](https://arxiv.org/abs/2502.17489)] [[cool](https://papers.cool/arxiv/2502.17489)] [[pdf](https://arxiv.org/pdf/2502.17489)]
> **Authors**: Thomas Screven,Andras Necz,Jason Smucny,Ian Davidson
> **First submission**: 2025-02-19
> **First announcement**: 2025-02-25
> **comment**: 8 pages
- **标题**: None
- **领域**: 信号处理,人工智能,计算机视觉和模式识别,机器学习
- **Abstract**: Although great advances in the analysis of neuroimaging data have been made, a major challenge is a lack of training data. This is less problematic in tasks such as diagnosis, where much data exists, but particularly prevalent in harder problems such as predicting treatment responses (prognosis), where data is focused and hence limited. Here, we address the learning from small data problems for medical imaging using graph neural networks. This is particularly challenging as the information about the patients is themselves graphs (regions of interest connectivity graphs). We show how a spectral representation of the connectivity data allows for efficient propagation that can yield approximately 12\% improvement over traditional deep learning methods using the exact same data. We show that our method's superior performance is due to a data smoothing result that can be measured by closing the number of triangle inequalities and thereby satisfying transitivity.

### Multimodal Sleep Stage and Sleep Apnea Classification Using Vision Transformer: A Multitask Explainable Learning Approach 
[[arxiv](https://arxiv.org/abs/2502.17486)] [[cool](https://papers.cool/arxiv/2502.17486)] [[pdf](https://arxiv.org/pdf/2502.17486)]
> **Authors**: Kianoosh Kazemi,Iman Azimi,Michelle Khine,Rami N. Khayat,Amir M. Rahmani,Pasi Liljeberg
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 信号处理,机器学习
- **Abstract**: Sleep is an essential component of human physiology, contributing significantly to overall health and quality of life. Accurate sleep staging and disorder detection are crucial for assessing sleep quality. Studies in the literature have proposed PSG-based approaches and machine-learning methods utilizing single-modality signals. However, existing methods often lack multimodal, multilabel frameworks and address sleep stages and disorders classification separately. In this paper, we propose a 1D-Vision Transformer for simultaneous classification of sleep stages and sleep disorders. Our method exploits the sleep disorders' correlation with specific sleep stage patterns and performs a simultaneous identification of a sleep stage and sleep disorder. The model is trained and tested using multimodal-multilabel sensory data (including photoplethysmogram, respiratory flow, and respiratory effort signals). The proposed method shows an overall accuracy (cohen's Kappa) of 78% (0.66) for five-stage sleep classification and 74% (0.58) for sleep apnea classification. Moreover, we analyzed the encoder attention weights to clarify our models' predictions and investigate the influence different features have on the models' outputs. The result shows that identified patterns, such as respiratory troughs and peaks, make a higher contribution to the final classification process.

### Urinary Tract Infection Detection in Digital Remote Monitoring: Strategies for Managing Participant-Specific Prediction Complexity 
[[arxiv](https://arxiv.org/abs/2502.17484)] [[cool](https://papers.cool/arxiv/2502.17484)] [[pdf](https://arxiv.org/pdf/2502.17484)]
> **Authors**: Kexin Fan,Alexander Capstick,Ramin Nilforooshan,Payam Barnaghi
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 信号处理,机器学习
- **Abstract**: Urinary tract infections (UTIs) are a significant health concern, particularly for people living with dementia (PLWD), as they can lead to severe complications if not detected and treated early. This study builds on previous work that utilised machine learning (ML) to detect UTIs in PLWD by analysing in-home activity and physiological data collected through low-cost, passive sensors. The current research focuses on improving the performance of previous models, particularly by refining the Multilayer Perceptron (MLP), to better handle variations in home environments and improve sex fairness in predictions by making use of concepts from multitask learning. This study implemented three primary model designs: feature clustering, loss-dependent clustering, and participant ID embedding which were compared against a baseline MLP model. The results demonstrated that the loss-dependent MLP achieved the most significant improvements, increasing validation precision from 48.92% to 72.60% and sensitivity from 27.44% to 70.52%, while also enhancing model fairness across sexes. These findings suggest that the refined models offer a more reliable and equitable approach to early UTI detection in PLWD, addressing participant-specific data variations and enabling clinicians to detect and screen for UTI risks more effectively, thereby facilitating earlier and more accurate treatment decisions.

### ConSense: Continually Sensing Human Activity with WiFi via Growing and Picking 
[[arxiv](https://arxiv.org/abs/2502.17483)] [[cool](https://papers.cool/arxiv/2502.17483)] [[pdf](https://arxiv.org/pdf/2502.17483)]
> **Authors**: Rong Li,Tao Deng,Siwei Feng,Mingjie Sun,Juncheng Jia
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 信号处理,机器学习
- **Abstract**: WiFi-based human activity recognition (HAR) holds significant application potential across various fields. To handle dynamic environments where new activities are continuously introduced, WiFi-based HAR systems must adapt by learning new concepts without forgetting previously learned ones. Furthermore, retaining knowledge from old activities by storing historical exemplar is impractical for WiFi-based HAR due to privacy concerns and limited storage capacity of edge devices. In this work, we propose ConSense, a lightweight and fast-adapted exemplar-free class incremental learning framework for WiFi-based HAR. The framework leverages the transformer architecture and involves dynamic model expansion and selective retraining to preserve previously learned knowledge while integrating new information. Specifically, during incremental sessions, small-scale trainable parameters that are trained specifically on the data of each task are added in the multi-head self-attention layer. In addition, a selective retraining strategy that dynamically adjusts the weights in multilayer perceptron based on the performance stability of neurons across tasks is used. Rather than training the entire model, the proposed strategies of dynamic model expansion and selective retraining reduce the overall computational load while balancing stability on previous tasks and plasticity on new tasks. Evaluation results on three public WiFi datasets demonstrate that ConSense not only outperforms several competitive approaches but also requires fewer parameters, highlighting its practical utility in class-incremental scenarios for HAR.

### Multi-View Contrastive Network (MVCNet) for Motor Imagery Classification 
[[arxiv](https://arxiv.org/abs/2502.17482)] [[cool](https://papers.cool/arxiv/2502.17482)] [[pdf](https://arxiv.org/pdf/2502.17482)]
> **Authors**: Ziwei Wang,Siyang Li,Xiaoqing Chen,Wei Li,Dongrui Wu
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-25
> **comment**: 9 pages, 7 figures
- **标题**: None
- **领域**: 信号处理,机器学习
- **Abstract**: Objective: An electroencephalography (EEG)-based brain-computer interface (BCI) serves as a direct communication pathway between the human brain and an external device. While supervised learning has been extensively explored for motor imagery (MI) EEG classification, small data quantity has been a key factor limiting the performance of deep feature learning. Methods: This paper proposes a knowledge-driven time-space-frequency based multi-view contrastive network (MVCNet) for MI EEG decoding in BCIs. MVCNet integrates knowledge from the time, space, and frequency domains into the training process through data augmentations from multiple views, fostering more discriminative feature learning of the characteristics of EEG data. We introduce a cross-view contrasting module to learn from different augmented views and a cross-model contrasting module to enhance the consistency of features extracted between knowledge-guided and data-driven models. Results: The combination of EEG data augmentation strategies was systematically investigated for more informative supervised contrastive learning. Experiments on four public MI datasets and three different architectures demonstrated that MVCNet outperformed 10 existing approaches. Significance: Our approach can significantly boost EEG classification performance beyond designated networks, showcasing the potential to enhance the feature learning process for better EEG decoding.

### Toward Foundational Model for Sleep Analysis Using a Multimodal Hybrid Self-Supervised Learning Framework 
[[arxiv](https://arxiv.org/abs/2502.17481)] [[cool](https://papers.cool/arxiv/2502.17481)] [[pdf](https://arxiv.org/pdf/2502.17481)]
> **Authors**: Cheol-Hui Lee,Hakseung Kim,Byung C. Yoon,Dong-Joo Kim
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-25
> **comment**: 18 pages, 5 figures
- **标题**: None
- **领域**: 信号处理,人工智能,机器学习
- **Abstract**: Sleep is essential for maintaining human health and quality of life. Analyzing physiological signals during sleep is critical in assessing sleep quality and diagnosing sleep disorders. However, manual diagnoses by clinicians are time-intensive and subjective. Despite advances in deep learning that have enhanced automation, these approaches remain heavily dependent on large-scale labeled datasets. This study introduces SynthSleepNet, a multimodal hybrid self-supervised learning framework designed for analyzing polysomnography (PSG) data. SynthSleepNet effectively integrates masked prediction and contrastive learning to leverage complementary features across multiple modalities, including electroencephalogram (EEG), electrooculography (EOG), electromyography (EMG), and electrocardiogram (ECG). This approach enables the model to learn highly expressive representations of PSG data. Furthermore, a temporal context module based on Mamba was developed to efficiently capture contextual information across signals. SynthSleepNet achieved superior performance compared to state-of-the-art methods across three downstream tasks: sleep-stage classification, apnea detection, and hypopnea detection, with accuracies of 89.89%, 99.75%, and 89.60%, respectively. The model demonstrated robust performance in a semi-supervised learning environment with limited labels, achieving accuracies of 87.98%, 99.37%, and 77.52% in the same tasks. These results underscore the potential of the model as a foundational tool for the comprehensive analysis of PSG data. SynthSleepNet demonstrates comprehensively superior performance across multiple downstream tasks compared to other methodologies, making it expected to set a new standard for sleep disorder monitoring and diagnostic systems.

### Brain-to-Text Decoding: A Non-invasive Approach via Typing 
[[arxiv](https://arxiv.org/abs/2502.17480)] [[cool](https://papers.cool/arxiv/2502.17480)] [[pdf](https://arxiv.org/pdf/2502.17480)]
> **Authors**: Jarod Lévy,Mingfang Zhang,Svetlana Pinet,Jérémy Rapin,Hubert Banville,Stéphane d'Ascoli,Jean-Rémi King
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-25
> **comment**: 15 pages, 5 figures
- **标题**: None
- **领域**: 信号处理,人工智能,计算语言学,人机交互
- **Abstract**: Modern neuroprostheses can now restore communication in patients who have lost the ability to speak or move. However, these invasive devices entail risks inherent to neurosurgery. Here, we introduce a non-invasive method to decode the production of sentences from brain activity and demonstrate its efficacy in a cohort of 35 healthy volunteers. For this, we present Brain2Qwerty, a new deep learning architecture trained to decode sentences from either electro- (EEG) or magneto-encephalography (MEG), while participants typed briefly memorized sentences on a QWERTY keyboard. With MEG, Brain2Qwerty reaches, on average, a character-error-rate (CER) of 32% and substantially outperforms EEG (CER: 67%). For the best participants, the model achieves a CER of 19%, and can perfectly decode a variety of sentences outside of the training set. While error analyses suggest that decoding depends on motor processes, the analysis of typographical errors suggests that it also involves higher-level cognitive factors. Overall, these results narrow the gap between invasive and non-invasive methods and thus open the path for developing safe brain-computer interfaces for non-communicating patients.

### Frequency-Aware Masked Autoencoders for Human Activity Recognition using Accelerometers 
[[arxiv](https://arxiv.org/abs/2502.17477)] [[cool](https://papers.cool/arxiv/2502.17477)] [[pdf](https://arxiv.org/pdf/2502.17477)]
> **Authors**: Niels R. Lorenzen,Poul J. Jennum,Emmanuel Mignot,Andreas Brink-Kjaer
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-25
> **comment**: 7 pages, 3 figures, submitted to 47th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)
- **标题**: None
- **领域**: 信号处理,机器学习
- **Abstract**: Wearable accelerometers are widely used for continuous monitoring of physical activity. Supervised machine learning and deep learning algorithms have long been used to extract meaningful activity information from raw accelerometry data, but progress has been hampered by the limited amount of publicly available labeled data. Exploiting large unlabeled datasets using self-supervised pretraining is a relatively new and underexplored approach in the field of human activity recognition (HAR). We used a time-series transformer masked autoencoder (MAE) approach to self-supervised pretraining and propose a novel spectrogram-based loss function named the log-scale mean magnitude (LMM) loss. We compared MAE models pretrained with LMM to one trained with the mean squared error (MSE) loss. We leveraged the large unlabeled UK Biobank accelerometry dataset (n = 109k) for pretraining and evaluated downstream HAR performance using linear classifier in a smaller labelled dataset. We found that pretraining with the LMM loss improved performance compared to a model pretrained with the MSE loss, with balanced accuracies of 0.848 and 0.709, respectively. Further analysis revealed that better convergence of the LMM loss, but not the MSE loss significantly correlated with improved downstream performance (r=-0.61, p=0.04) for balanced accuracy). Finally, we compared our MAE models to the state-of-the-art for HAR, also pretrained on the UK Biobank accelerometry data. Our LMM-pretrained models performed better when finetuned using a linear classifier and performed comparably when finetuned using an LSTM classifier, while MSE-pretrained models consistently underperformed. Our findings demonstrate that the LMM loss is a robust and effective method for pretraining MAE models on accelerometer data for HAR. Future work should explore optimizing loss function combinations and extending our approach to other tasks.

### Fusion of ECG Foundation Model Embeddings to Improve Early Detection of Acute Coronary Syndromes 
[[arxiv](https://arxiv.org/abs/2502.17476)] [[cool](https://papers.cool/arxiv/2502.17476)] [[pdf](https://arxiv.org/pdf/2502.17476)]
> **Authors**: Zeyuan Meng,Lovely Yeswanth Panchumarthi,Saurabh Kataria,Alex Fedorov,Jessica Zègre-Hemsey,Xiao Hu,Ran Xiao
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 信号处理,机器学习
- **Abstract**: Acute Coronary Syndrome (ACS) is a life-threatening cardiovascular condition where early and accurate diagnosis is critical for effective treatment and improved patient outcomes. This study explores the use of ECG foundation models, specifically ST-MEM and ECG-FM, to enhance ACS risk assessment using prehospital ECG data collected in ambulances. Both models leverage self-supervised learning (SSL), with ST-MEM using a reconstruction-based approach and ECG-FM employing contrastive learning, capturing unique spatial and temporal ECG features. We evaluate the performance of these models individually and through a fusion approach, where their embeddings are combined for enhanced prediction. Results demonstrate that both foundation models outperform a baseline ResNet-50 model, with the fusion-based approach achieving the highest performance (AUROC: 0.843 +/- 0.006, AUCPR: 0.674 +/- 0.012). These findings highlight the potential of ECG foundation models for early ACS detection and motivate further exploration of advanced fusion strategies to maximize complementary feature utilization.

### ECG-Expert-QA: A Benchmark for Evaluating Medical Large Language Models in Heart Disease Diagnosis 
[[arxiv](https://arxiv.org/abs/2502.17475)] [[cool](https://papers.cool/arxiv/2502.17475)] [[pdf](https://arxiv.org/pdf/2502.17475)]
> **Authors**: Xu Wang,Jiaju Kang,Puyu Han
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 信号处理,人工智能,计算语言学,机器学习
- **Abstract**: We present ECG-Expert-QA, a comprehensive multimodal dataset designed for evaluating diagnostic capabilities in ECG interpretation, integrating real clinical data with systematically generated synthetic cases. The dataset encompasses six fundamental diagnostic tasks, comprising 47,211 meticulously curated question-answer pairs that span a spectrum of clinical scenarios, from basic rhythm analysis to complex case interpretation. By simulating challenging clinical cases through a rigorous medical knowledge-guided process, ECG-Expert-QA not only enhances the availability of annotated diagnostic data but also significantly increases the complexity and diversity of clinical presentations, including rare cardiac conditions and temporal progression patterns. This design enables comprehensive evaluation of medical language models across multiple dimensions, including diagnostic accuracy, clinical reasoning, and knowledge integration. To facilitate global research collaboration, ECG-Expert-QA is available in both Chinese and English versions, with rigorous quality control ensuring linguistic and clinical consistency. The dataset's challenging diagnostic tasks, which include interpretation of complex arrhythmias, identification of subtle ischemic changes, and integration of clinical context, establish it as an effective benchmark for advancing AI-assisted ECG interpretation and pushing the boundaries of current diagnostic models. Our dataset is open-source and available at https://github.com/Zaozzz/ECG-Expert-QA

### MC2SleepNet: Multi-modal Cross-masking with Contrastive Learning for Sleep Stage Classification 
[[arxiv](https://arxiv.org/abs/2502.17470)] [[cool](https://papers.cool/arxiv/2502.17470)] [[pdf](https://arxiv.org/pdf/2502.17470)]
> **Authors**: Younghoon Na,Hyun Keun Ahn,Hyun-Kyung Lee,Yoongeol Lee,Seung Hun Oh,Hongkwon Kim,Jeong-Gun Lee
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 信号处理,人工智能
- **Abstract**: Sleep profoundly affects our health, and sleep deficiency or disorders can cause physical and mental problems. Despite significant findings from previous studies, challenges persist in optimizing deep learning models, especially in multi-modal learning for high-accuracy sleep stage classification. Our research introduces MC2SleepNet (Multi-modal Cross-masking with Contrastive learning for Sleep stage classification Network). It aims to facilitate the effective collaboration between Convolutional Neural Networks (CNNs) and Transformer architectures for multi-modal training with the help of contrastive learning and cross-masking. Raw single channel EEG signals and corresponding spectrogram data provide differently characterized modalities for multi-modal learning. Our MC2SleepNet has achieved state-of-the-art performance with an accuracy of both 84.6% on the SleepEDF-78 and 88.6% accuracy on the Sleep Heart Health Study (SHHS). These results demonstrate the effective generalization of our proposed network across both small and large datasets.

### PixleepFlow: A Pixel-Based Lifelog Framework for Predicting Sleep Quality and Stress Level 
[[arxiv](https://arxiv.org/abs/2502.17469)] [[cool](https://papers.cool/arxiv/2502.17469)] [[pdf](https://arxiv.org/pdf/2502.17469)]
> **Authors**: Younghoon Na,Seunghun Oh,Seongji Ko,Hyunkyung Lee
> **First submission**: 2025-02-13
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 信号处理,人工智能,机器学习
- **Abstract**: The analysis of lifelogs can yield valuable insights into an individual's daily life, particularly with regard to their health and well-being. The accurate assessment of quality of life is necessitated by the use of diverse sensors and precise synchronization. To rectify this issue, this study proposes the image-based sleep quality and stress level estimation flow (PixleepFlow). PixleepFlow employs a conversion methodology into composite image data to examine sleep patterns and their impact on overall health. Experiments were conducted using lifelog datasets to ascertain the optimal combination of data formats. In addition, we identified which sensor information has the greatest influence on the quality of life through Explainable Artificial Intelligence(XAI). As a result, PixleepFlow produced more significant results than various data formats. This study was part of a written-based competition, and the additional findings from the lifelog dataset are detailed in Section Section IV. More information about PixleepFlow can be found at https://github.com/seongjiko/Pixleep.

### CSSSTN: A Class-sensitive Subject-to-subject Semantic Style Transfer Network for EEG Classification in RSVP Tasks 
[[arxiv](https://arxiv.org/abs/2502.17468)] [[cool](https://papers.cool/arxiv/2502.17468)] [[pdf](https://arxiv.org/pdf/2502.17468)]
> **Authors**: Ziyue Yang,Chengrui Chen,Yong Peng,Qiong Chen,Wanzeng Kong
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 信号处理,机器学习
- **Abstract**: The Rapid Serial Visual Presentation (RSVP) paradigm represents a promising application of electroencephalography (EEG) in Brain-Computer Interface (BCI) systems. However, cross-subject variability remains a critical challenge, particularly for BCI-illiterate users who struggle to effectively interact with these systems. To address this issue, we propose the Class-Sensitive Subject-to-Subject Semantic Style Transfer Network (CSSSTN), which incorporates a class-sensitive approach to align feature distributions between golden subjects (BCI experts) and target (BCI-illiterate) users on a class-by-class basis. Building on the SSSTN framework, CSSSTN incorporates three key components: (1) subject-specific classifier training, (2) a unique style loss to transfer class-discriminative features while preserving semantic information through a modified content loss, and (3) an ensemble approach to integrate predictions from both source and target domains. We evaluated CSSSTN using both a publicly available dataset and a self-collected dataset. Experimental results demonstrate that CSSSTN outperforms state-of-the-art methods, achieving mean balanced accuracy improvements of 6.4\% on the Tsinghua dataset and 3.5\% on the HDU dataset, with notable benefits for BCI-illiterate users. Ablation studies confirm the effectiveness of each component, particularly the class-sensitive transfer and the use of lower-layer features, which enhance transfer performance and mitigate negative transfer. Additionally, CSSSTN achieves competitive results with minimal target data, reducing calibration time and effort. These findings highlight the practical potential of CSSSTN for real-world BCI applications, offering a robust and scalable solution to improve the performance of BCI-illiterate users while minimizing reliance on extensive training data. Our code is available at https://github.com/ziyuey/CSSSTN.

### Bridging Brain Signals and Language: A Deep Learning Approach to EEG-to-Text Decoding 
[[arxiv](https://arxiv.org/abs/2502.17465)] [[cool](https://papers.cool/arxiv/2502.17465)] [[pdf](https://arxiv.org/pdf/2502.17465)]
> **Authors**: Mostafa El Gedawy,Omnia Nabil,Omar Mamdouh,Mahmoud Nady,Nour Alhuda Adel,Ahmed Fares
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-25
> **comment**: 21 pages, 11 figures, and 6 tables
- **标题**: None
- **领域**: 信号处理,计算语言学,机器学习
- **Abstract**: Brain activity translation into human language delivers the capability to revolutionize machine-human interaction while providing communication support to people with speech disability. Electronic decoding reaches a certain level of achievement yet current EEG-to-text decoding methods fail to reach open vocabularies and depth of meaning and individual brain-specific variables. We introduce a special framework which changes conventional closed-vocabulary EEG-to-text decoding approaches by integrating subject-specific learning models with natural language processing methods to resolve detection obstacles. This method applies a deep representation learning approach to extract important EEG features which allow training of neural networks to create elaborate sentences that extend beyond original data content. The ZuCo dataset analysis demonstrates that research findings achieve higher BLEU, ROUGE and BERTScore performance when compared to current methods. The research proves how this framework functions as an effective approach to generate meaningful and correct texts while understanding individual brain variations. The proposed research aims to create a connection between open-vocabulary Text generation systems and human brain signal interpretation for developing efficacious brain-to-text systems. The research produces interdisciplinary effects through innovative assistive technology development and personalized communication systems which extend possibilities for human-computer interaction in various settings.

### Large Cognition Model: Towards Pretrained EEG Foundation Model 
[[arxiv](https://arxiv.org/abs/2502.17464)] [[cool](https://papers.cool/arxiv/2502.17464)] [[pdf](https://arxiv.org/pdf/2502.17464)]
> **Authors**: Chi-Sheng Chen,Ying-Jung Chen,Aidan Hung-Wen Tsai
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 信号处理,机器学习,神经元和认知
- **Abstract**: Electroencephalography provides a non-invasive window into brain activity, offering valuable insights for neurological research, brain-computer interfaces, and clinical diagnostics. However, the development of robust machine learning models for EEG analysis is hindered by the scarcity of large-scale, well-annotated datasets and the inherent variability of EEG signals across subjects and recording conditions. Inspired by the success of foundation models in natural language processing and computer vision, we propose the Large Cognition Model-a transformer-based foundation model designed to generalize across diverse EEG datasets and downstream tasks. Unlike traditional approaches, our proposed transformer-based architecture demonstrates strong generalization capabilities across datasets and tasks, even without pretraining, surpassing some existing EEG universal models on specific downstream applications. LCM leverages large-scale self-supervised learning techniques to capture universal EEG representations, enabling efficient fine-tuning for applications such as cognitive state decoding, disease classification, and neurofeedback systems. We introduce a novel architecture that integrates temporal and spectral attention mechanisms, optimizing the model's ability to extract meaningful features from raw EEG signals. Extensive evaluations demonstrate that LCM outperforms state-of-the-art approaches across multiple EEG benchmarks, exhibiting strong cross-subject and cross-task generalization. Our findings highlight the potential of pretrained EEG foundation models to accelerate advancements in neuroscience, personalized medicine, and BCI technology.

### SincPD: An Explainable Method based on Sinc Filters to Diagnose Parkinson's Disease Severity by Gait Cycle Analysis 
[[arxiv](https://arxiv.org/abs/2502.17463)] [[cool](https://papers.cool/arxiv/2502.17463)] [[pdf](https://arxiv.org/pdf/2502.17463)]
> **Authors**: Armin Salimi-Badr,Mahan Veisi,Sadra Berangi
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 信号处理,机器学习
- **Abstract**: In this paper, an explainable deep learning-based classifier based on adaptive sinc filters for Parkinson's Disease diagnosis (PD) along with determining its severity, based on analyzing the gait cycle (SincPD) is presented. Considering the effects of PD on the gait cycle of patients, the proposed method utilizes raw data in the form of vertical Ground Reaction Force (vGRF) measured by wearable sensors placed in soles of subjects' shoes. The proposed method consists of Sinc layers that model adaptive bandpass filters to extract important frequency-bands in gait cycle of patients along with healthy subjects. Therefore, by considering these frequencies, the reasons behind the classification a person as a patient or healthy can be explained. In this method, after applying some preprocessing processes, a large model equipped with many filters is first trained. Next, to prune the extra units and reach a more explainable and parsimonious structure, the extracted filters are clusters based on their cut-off frequencies using a centroid-based clustering approach. Afterward, the medoids of the extracted clusters are considered as the final filters. Therefore, only 15 bandpass filters for each sensor are derived to classify patients and healthy subjects. Finally, the most effective filters along with the sensors are determined by comparing the energy of each filter encountering patients and healthy subjects.

### The Case for Cleaner Biosignals: High-fidelity Neural Compressor Enables Transfer from Cleaner iEEG to Noisier EEG 
[[arxiv](https://arxiv.org/abs/2502.17462)] [[cool](https://papers.cool/arxiv/2502.17462)] [[pdf](https://arxiv.org/pdf/2502.17462)]
> **Authors**: Francesco Stefano Carzaniga,Gary Tom Hoppeler,Michael Hersche,Kaspar Anton Schindler,Abbas Rahimi
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-25
> **comment**: Published at ICLR 2025, see https://openreview.net/forum?id=b57IG6N20B. Code is available at https://github.com/IBM/eeg-ieeg-brain-compressor
- **标题**: None
- **领域**: 信号处理,人工智能,机器学习
- **Abstract**: All data modalities are not created equal, even when the signal they measure comes from the same source. In the case of the brain, two of the most important data modalities are the scalp electroencephalogram (EEG), and the intracranial electroencephalogram (iEEG). They are used by human experts, supported by deep learning (DL) models, to accomplish a variety of tasks, such as seizure detection and motor imagery classification. Although the differences between EEG and iEEG are well understood by human experts, the performance of DL models across these two modalities remains under-explored. To help characterize the importance of clean data on the performance of DL models, we propose BrainCodec, a high-fidelity EEG and iEEG neural compressor. We find that training BrainCodec on iEEG and then transferring to EEG yields higher reconstruction quality than training on EEG directly. In addition, we also find that training BrainCodec on both EEG and iEEG improves fidelity when reconstructing EEG. Our work indicates that data sources with higher SNR, such as iEEG, provide better performance across the board also in the medical time-series domain. BrainCodec also achieves up to a 64x compression on iEEG and EEG without a notable decrease in quality. BrainCodec markedly surpasses current state-of-the-art compression models both in final compression ratio and in reconstruction fidelity. We also evaluate the fidelity of the compressed signals objectively on a seizure detection and a motor imagery task performed by standard DL models. Here, we find that BrainCodec achieves a reconstruction fidelity high enough to ensure no performance degradation on the downstream tasks. Finally, we collect the subjective assessment of an expert neurologist, that confirms the high reconstruction quality of BrainCodec in a realistic scenario. The code is available at https://github.com/IBM/eeg-ieeg-brain-compressor.

### Finetuning and Quantization of EEG-Based Foundational BioSignal Models on ECG and PPG Data for Blood Pressure Estimation 
[[arxiv](https://arxiv.org/abs/2502.17460)] [[cool](https://papers.cool/arxiv/2502.17460)] [[pdf](https://arxiv.org/pdf/2502.17460)]
> **Authors**: Bálint Tóth,Dominik Senti,Thorir Mar Ingolfsson,Jeffrey Zweidler,Alexandre Elsig,Luca Benini,Yawei Li
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-25
> **comment**: 7 pages, 1 figure, 5 tables, preprint
- **标题**: None
- **领域**: 信号处理,人工智能,机器学习
- **Abstract**: Blood pressure (BP) is a key indicator of cardiovascular health. As hypertension remains a global cause of morbidity and mortality, accurate, continuous, and non-invasive BP monitoring is therefore of paramount importance. Photoplethysmography (PPG) and electrocardiography (ECG) can potentially enable continuous BP monitoring, yet training accurate and robust machine learning (ML) models remains challenging due to variability in data quality and patient-specific factors. Recently, multiple research groups explored Electroencephalographic (EEG)--based foundation models and demonstrated their exceptional ability to learn rich temporal resolution. Considering the morphological similarities between different biosignals, the question arises of whether a model pre-trained on one modality can effectively be exploited to improve the accuracy of a different signal type. In this work, we take an initial step towards generalized biosignal foundation models by investigating whether model representations learned from abundant EEG data can effectively be transferred to ECG/PPG data solely with fine-tuning, without the need for large-scale additional pre-training, for the BP estimation task. Evaluations on the MIMIC-III and VitalDB datasets demonstrate that our approach achieves near state-of-the-art accuracy for diastolic BP (mean absolute error of 1.57 mmHg) and surpasses by 1.5x the accuracy of prior works for systolic BP (mean absolute error 2.72 mmHg). Additionally, we perform dynamic INT8 quantization, reducing the smallest model size by over 3.5x (from 13.73 MB down to 3.83 MB) while preserving performance, thereby enabling unobtrusive, real-time BP monitoring on resource-constrained wearable devices.

### Study on Downlink CSI compression: Are Neural Networks the Only Solution? 
[[arxiv](https://arxiv.org/abs/2502.17459)] [[cool](https://papers.cool/arxiv/2502.17459)] [[pdf](https://arxiv.org/pdf/2502.17459)]
> **Authors**: K. Sai Praneeth,Anil Kumar Yerrapragada,Achyuth Sagireddi,Sai Prasad,Radha Krishna Ganti
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 信号处理,机器学习
- **Abstract**: Massive Multi Input Multi Output (MIMO) systems enable higher data rates in the downlink (DL) with spatial multiplexing achieved by forming narrow beams. The higher DL data rates are achieved by effective implementation of spatial multiplexing and beamforming which is subject to availability of DL channel state information (CSI) at the base station. For Frequency Division Duplexing (FDD) systems, the DL CSI has to be transmitted by User Equipment (UE) to the gNB and it constitutes a significant overhead which scales with the number of transmitter antennas and the granularity of the CSI. To address the overhead issue, AI/ML methods using auto-encoders have been investigated, where an encoder neural network model at the UE compresses the CSI and a decoder neural network model at the gNB reconstructs it. However, the use of AI/ML methods has a number of challenges related to (1) model complexity, (2) model generalization across channel scenarios and (3) inter-vendor compatibility of the two sides of the model. In this work, we investigate a more traditional dimensionality reduction method that uses Principal Component Analysis (PCA) and therefore does not suffer from the above challenges. Simulation results show that PCA based CSI compression actually achieves comparable reconstruction performance to commonly used deep neural networks based models.

### MoEMba: A Mamba-based Mixture of Experts for High-Density EMG-based Hand Gesture Recognition 
[[arxiv](https://arxiv.org/abs/2502.17457)] [[cool](https://papers.cool/arxiv/2502.17457)] [[pdf](https://arxiv.org/pdf/2502.17457)]
> **Authors**: Mehran Shabanpour,Kasra Rad,Sadaf Khademi,Arash Mohammadi
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 信号处理,人工智能,机器学习
- **Abstract**: High-Density surface Electromyography (HDsEMG) has emerged as a pivotal resource for Human-Computer Interaction (HCI), offering direct insights into muscle activities and motion intentions. However, a significant challenge in practical implementations of HD-sEMG-based models is the low accuracy of inter-session and inter-subject classification. Variability between sessions can reach up to 40% due to the inherent temporal variability of HD-sEMG signals. Targeting this challenge, the paper introduces the MoEMba framework, a novel approach leveraging Selective StateSpace Models (SSMs) to enhance HD-sEMG-based gesture recognition. The MoEMba framework captures temporal dependencies and cross-channel interactions through channel attention techniques. Furthermore, wavelet feature modulation is integrated to capture multi-scale temporal and spatial relations, improving signal representation. Experimental results on the CapgMyo HD-sEMG dataset demonstrate that MoEMba achieves a balanced accuracy of 56.9%, outperforming its state-of-the-art counterparts. The proposed framework's robustness to session-to-session variability and its efficient handling of high-dimensional multivariate time series data highlight its potential for advancing HD-sEMG-powered HCI systems.

### DCentNet: Decentralized Multistage Biomedical Signal Classification using Early Exits 
[[arxiv](https://arxiv.org/abs/2502.17446)] [[cool](https://papers.cool/arxiv/2502.17446)] [[pdf](https://arxiv.org/pdf/2502.17446)]
> **Authors**: Xiaolin Li,Binhua Huang,Barry Cardiff,Deepu John
> **First submission**: 2025-01-30
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 信号处理,人工智能,机器学习
- **Abstract**: DCentNet is a novel decentralized multistage signal classification approach designed for biomedical data from IoT wearable sensors, integrating early exit points (EEP) to enhance energy efficiency and processing speed. Unlike traditional centralized processing methods, which result in high energy consumption and latency, DCentNet partitions a single CNN model into multiple sub-networks using EEPs. By introducing encoder-decoder pairs at EEPs, the system compresses large feature maps before transmission, significantly reducing wireless data transfer and power usage. If an input is confidently classified at an EEP, processing stops early, optimizing efficiency. Initial sub-networks can be deployed on fog or edge devices to further minimize energy consumption. A genetic algorithm is used to optimize EEP placement, balancing performance and complexity. Experimental results on ECG classification show that with one EEP, DCentNet reduces wireless data transmission by 94.54% and complexity by 21%, while maintaining original accuracy and sensitivity. With two EEPs, sensitivity reaches 98.36%, accuracy 97.74%, wireless data transmission decreases by 91.86%, and complexity is reduced by 22%. Implemented on an ARM Cortex-M4 MCU, DCentNet achieves an average power saving of 73.6% compared to continuous wireless ECG transmission.

### Interpretable Dual-Filter Fuzzy Neural Networks for Affective Brain-Computer Interfaces 
[[arxiv](https://arxiv.org/abs/2502.17445)] [[cool](https://papers.cool/arxiv/2502.17445)] [[pdf](https://arxiv.org/pdf/2502.17445)]
> **Authors**: Xiaowei Jiang,Yanan Chen,Nikhil Ranjan Pal,Yu-Cheng Chang,Yunkai Yang,Thomas Do,Chin-Teng Lin
> **First submission**: 2025-01-29
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 信号处理,人工智能,人机交互,神经元和认知
- **Abstract**: Fuzzy logic provides a robust framework for enhancing explainability, particularly in domains requiring the interpretation of complex and ambiguous signals, such as brain-computer interface (BCI) systems. Despite significant advances in deep learning, interpreting human emotions remains a formidable challenge. In this work, we present iFuzzyAffectDuo, a novel computational model that integrates a dual-filter fuzzy neural network architecture for improved detection and interpretation of emotional states from neuroimaging data. The model introduces a new membership function (MF) based on the Laplace distribution, achieving superior accuracy and interpretability compared to traditional approaches. By refining the extraction of neural signals associated with specific emotions, iFuzzyAffectDuo offers a human-understandable framework that unravels the underlying decision-making processes. We validate our approach across three neuroimaging datasets using functional Near-Infrared Spectroscopy (fNIRS) and Electroencephalography (EEG), demonstrating its potential to advance affective computing. These findings open new pathways for understanding the neural basis of emotions and their application in enhancing human-computer interaction.

## 高能物理-现象学(hep-ph:High Energy Physics - Phenomenology)

### Unraveling particle dark matter with Physics-Informed Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.17597)] [[cool](https://papers.cool/arxiv/2502.17597)] [[pdf](https://arxiv.org/pdf/2502.17597)]
> **Authors**: M. P. Bento,H. B. Câmara,J. F. Seabra
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 20 LaTeX pages, 11 Figures
- **标题**: None
- **领域**: 高能物理-现象学,机器学习
- **Abstract**: We parametrically solve the Boltzmann equations governing freeze-in dark matter (DM) in alternative cosmologies with Physics-Informed Neural Networks (PINNs), a mesh-free method. Through inverse PINNs, using a single DM experimental point -- observed relic density -- we determine the physical attributes of the theory, namely power-law cosmologies, inspired by braneworld scenarios, and particle interaction cross sections. The expansion of the Universe in such alternative cosmologies has been parameterized through a switch-like function reproducing the Hubble law at later times. Without loss of generality, we model more realistically this transition with a smooth function. We predict a distinct pair-wise relationship between power-law exponent and particle interactions: for a given cosmology with negative (positive) exponent, smaller (larger) cross sections are required to reproduce the data. Lastly, via Bayesian methods, we quantify the epistemic uncertainty of theoretical parameters found in inverse problems.

## 历史与概述(math.HO:History and Overview)

### From Euler to AI: Unifying Formulas for Mathematical Constants 
[[arxiv](https://arxiv.org/abs/2502.17533)] [[cool](https://papers.cool/arxiv/2502.17533)] [[pdf](https://arxiv.org/pdf/2502.17533)]
> **Authors**: Tomer Raz,Michael Shalyt,Elyasheev Leibtag,Rotem Kalisch,Yaron Hadad,Ido Kaminer
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 50 pages, 6 figures
- **标题**: None
- **领域**: 历史与概述,人工智能,计算语言学,数论
- **Abstract**: The constant $π$ has fascinated scholars for centuries, inspiring the derivation of countless formulas rooted in profound mathematical insight. This abundance of formulas raises a question: Are they interconnected, and can a unifying structure explain their relationships? We propose a systematic methodology for discovering and proving formula equivalences, leveraging modern large language models, large-scale data processing, and novel mathematical algorithms. Analyzing 457,145 arXiv papers, over a third of the validated formulas for $π$ were proven to be derivable from a single mathematical object - including formulas by Euler, Gauss, Lord Brouncker, and newer ones from algorithmic discoveries by the Ramanujan Machine. Our approach extends to other constants, such as $e$, $ζ(3)$, and Catalan's constant, proving its broad applicability. This work represents a step toward the automatic unification of mathematical knowledge, laying a foundation for AI-driven discoveries of connections across scientific domains.

## 优化与控制(math.OC:Optimization and Control)

### A stochastic smoothing framework for nonconvex-nonconcave min-sum-max problems with applications to Wasserstein distributionally robust optimization 
[[arxiv](https://arxiv.org/abs/2502.17602)] [[cool](https://papers.cool/arxiv/2502.17602)] [[pdf](https://arxiv.org/pdf/2502.17602)]
> **Authors**: Wei Liu,Muhammad Khan,Gabriel Mancino-Ball,Yangyang Xu
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 35 pages
- **标题**: None
- **领域**: 优化与控制,机器学习
- **Abstract**: Applications such as adversarially robust training and Wasserstein Distributionally Robust Optimization (WDRO) can be naturally formulated as min-sum-max optimization problems. While this formulation can be rewritten as an equivalent min-max problem, the summation of max terms introduces computational challenges, including increased complexity and memory demands, which must be addressed. These challenges are particularly evident in WDRO, where existing tractable algorithms often rely on restrictive assumptions on the objective function, limiting their applicability to state-of-the-art machine learning problems such as the training of deep neural networks. This study introduces a novel stochastic smoothing framework based on the \mbox{log-sum-exp} function, efficiently approximating the max operator in min-sum-max problems. By leveraging the Clarke regularity of the max operator, we develop an iterative smoothing algorithm that addresses these computational difficulties and guarantees almost surely convergence to a Clarke/directional stationary point. We further prove that the proposed algorithm finds an $ε$-scaled Clarke stationary point of the original problem, with a worst-case iteration complexity of $\widetilde{O}(ε^{-3})$. Our numerical experiments demonstrate that our approach outperforms or is competitive with state-of-the-art methods in solving the newsvendor problem, deep learning regression, and adversarially robust deep learning. The results highlight that our method yields more accurate and robust solutions in these challenging problem settings.

### A Concise Lyapunov Analysis of Nesterov's Accelerated Gradient Method 
[[arxiv](https://arxiv.org/abs/2502.17373)] [[cool](https://papers.cool/arxiv/2502.17373)] [[pdf](https://arxiv.org/pdf/2502.17373)]
> **Authors**: Jun Liu
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: This update addresses a typo and a related proof issue, thanks to Nicolas Boumal's observation
- **标题**: None
- **领域**: 优化与控制,机器学习,系统与控制
- **Abstract**: Convergence analysis of Nesterov's accelerated gradient method has attracted significant attention over the past decades. While extensive work has explored its theoretical properties and elucidated the intuition behind its acceleration, a simple and direct proof of its convergence rates is still lacking. We provide a concise Lyapunov analysis of the convergence rates of Nesterov's accelerated gradient method for both general convex and strongly convex functions.

## 混沌动力学(nlin.CD:Chaotic Dynamics)

### A Fokker-Planck-Based Loss Function that Bridges Dynamics with Density Estimation 
[[arxiv](https://arxiv.org/abs/2502.17690)] [[cool](https://papers.cool/arxiv/2502.17690)] [[pdf](https://arxiv.org/pdf/2502.17690)]
> **Authors**: Zhixin Lu,Łukasz Kuśmierz,Stefan Mihalas
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Under review by the ICML
- **标题**: None
- **领域**: 混沌动力学,机器学习
- **Abstract**: We have derived a novel loss function from the Fokker-Planck equation that links dynamical system models with their probability density functions, demonstrating its utility in model identification and density estimation. In the first application, we show that this loss function can enable the extraction of dynamical parameters from non-temporal datasets, including timestamp-free measurements from steady non-equilibrium systems such as noisy Lorenz systems and gene regulatory networks. In the second application, when coupled with a density estimator, this loss facilitates density estimation when the dynamic equations are known. For density estimation, we propose a density estimator that integrates a Gaussian Mixture Model with a normalizing flow model. It simultaneously estimates normalized density, energy, and score functions from both empirical data and dynamics. It is compatible with a variety of data-based training methodologies, including maximum likelihood and score matching. It features a latent space akin to a modern Hopfield network, where the inherent Hopfield energy effectively assigns low densities to sparsely populated data regions, addressing common challenges in neural density estimators. Additionally, this Hopfield-like energy enables direct and rapid data manipulation through the Concave-Convex Procedure (CCCP) rule, facilitating tasks such as denoising and clustering. Our work demonstrates a principled framework for leveraging the complex interdependencies between dynamics and density estimation, as illustrated through synthetic examples that clarify the underlying theoretical intuitions.

## 大气和海洋物理(physics.ao-ph:Atmospheric and Oceanic Physics)

### Multi-Year-to-Decadal Temperature Prediction using a Machine Learning Model-Analog Framework 
[[arxiv](https://arxiv.org/abs/2502.17583)] [[cool](https://papers.cool/arxiv/2502.17583)] [[pdf](https://arxiv.org/pdf/2502.17583)]
> **Authors**: M. A. Fernandez,Elizabeth A. Barnes
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 14 pages, 10 figures (+ 8 supplemental figures)
- **标题**: None
- **领域**: 大气和海洋物理,机器学习
- **Abstract**: Multi-year-to-decadal climate prediction is a key tool in understanding the range of potential regional and global climate futures. Here, we present a framework that combines machine learning and analog forecasting for predictions on these timescales. A neural network is used to learn a mask, specific to a region and lead time, with global weights based on relative importance as precursors to the evolution of that prediction target. A library of mask-weighted model states, or potential analogs, are then compared to a single mask-weighted observational state. The known future of the best matching potential analogs serve as the prediction for the future of the observational state. We match and predict 2-meter temperature using the Berkeley Earth Surface Temperature dataset for observations, and a set of CMIP6 models as the analog library. We find improved performance over traditional analog methods and initialized decadal predictions.

## 化学物理(physics.chem-ph:Chemical Physics)

### Survey on Recent Progress of AI for Chemistry: Methods, Applications, and Opportunities 
[[arxiv](https://arxiv.org/abs/2502.17456)] [[cool](https://papers.cool/arxiv/2502.17456)] [[pdf](https://arxiv.org/pdf/2502.17456)]
> **Authors**: Ding Hu,Pengxiang Hua,Zhen Huang
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-25
> **comment**: 22 pages, 8 figures, 4 tables
- **标题**: None
- **领域**: 化学物理,人工智能,机器学习
- **Abstract**: The development of artificial intelligence (AI) techniques has brought revolutionary changes across various realms. In particular, the use of AI-assisted methods to accelerate chemical research has become a popular and rapidly growing trend, leading to numerous groundbreaking works. In this paper, we provide a comprehensive review of current AI techniques in chemistry from a computational perspective, considering various aspects in the design of methods. We begin by discussing the characteristics of data from diverse sources, followed by an overview of various representation methods. Next, we review existing models for several topical tasks in the field, and conclude by highlighting some key challenges that warrant further attention.

### HybridLinker: Topology-Guided Posterior Sampling for Enhanced Diversity and Validity in 3D Molecular Linker Generation 
[[arxiv](https://arxiv.org/abs/2502.17349)] [[cool](https://papers.cool/arxiv/2502.17349)] [[pdf](https://arxiv.org/pdf/2502.17349)]
> **Authors**: Minyeong Hwang,Ziseok Lee,Gwangsoo Kim,Kyungsu Kim,Eunho Yang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 化学物理,人工智能,机器学习
- **Abstract**: Linker generation is critical in drug discovery applications such as lead optimization and PROTAC design, where molecular fragments are assembled into diverse drug candidates. Existing methods fall into PC-Free and PC-Aware categories based on their use of 3D point clouds (PC). PC-Free models prioritize diversity but suffer from lower validity due to overlooking PC constraints, while PC-Aware models ensure higher validity but restrict diversity by enforcing strict PC constraints. To overcome these trade-offs without additional training, we propose HybridLinker, a framework that enhances PC-Aware inference by providing diverse bonding topologies from a pretrained PC-Free model as guidance. At its core, we propose LinkerDPS, the first diffusion posterior sampling (DPS) method operating across PC-Free and PC-Aware spaces, bridging molecular topology with 3D point clouds via an energy-inspired function. By transferring the diverse sampling distribution of PC-Free models into the PC-Aware distribution, HybridLinker significantly and consistently surpasses baselines, improving both validity and diversity in foundational molecular design and applied property optimization tasks, establishing a new DPS framework in the molecular and graph domains beyond imaging.

## 计算物理(physics.comp-ph:Computational Physics)

### Effective Field Neural Network 
[[arxiv](https://arxiv.org/abs/2502.17665)] [[cool](https://papers.cool/arxiv/2502.17665)] [[pdf](https://arxiv.org/pdf/2502.17665)]
> **Authors**: Xi Liu,Yujun Zhao,Chun Yu Wan,Yang Zhang,Junwei Liu
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算物理,强关联电子,人工智能,量子物理学
- **Abstract**: In recent years, with the rapid development of machine learning, physicists have been exploring its new applications in solving or alleviating the curse of dimensionality in many-body problems. In order to accurately reflect the underlying physics of the problem, domain knowledge must be encoded into the machine learning algorithms. In this work, inspired by field theory, we propose a new set of machine learning models called effective field neural networks (EFNNs) that can automatically and efficiently capture important many-body interactions through multiple self-refining processes. Taking the classical $3$-spin infinite-range model and the quantum double exchange model as case studies, we explicitly demonstrate that EFNNs significantly outperform fully-connected deep neural networks (DNNs) and the effective model. Furthermore, with the help of convolution operations, the EFNNs learned in a small system can be seamlessly used in a larger system without additional training and the relative errors even decrease, which further demonstrates the efficacy of EFNNs in representing core physical behaviors.

## 地球物理学(physics.geo-ph:Geophysics)

### Theory-guided Pseudo-spectral Full Waveform Inversion via Deep Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.17624)] [[cool](https://papers.cool/arxiv/2502.17624)] [[pdf](https://arxiv.org/pdf/2502.17624)]
> **Authors**: Christopher Zerafa,Pauline Galea,Cristiana Sebu
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 26 pages, 23 figures, article paper
- **标题**: None
- **领域**: 地球物理学,人工智能
- **Abstract**: Full-Waveform Inversion seeks to achieve a high-resolution model of the subsurface through the application of multi-variate optimization to the seismic inverse problem. Although now a mature technology, FWI has limitations related to the choice of the appropriate solver for the forward problem in challenging environments requiring complex assumptions, and very wide angle and multi-azimuth data necessary for full reconstruction are often not available. Deep Learning techniques have emerged as excellent optimization frameworks. Data-driven methods do not impose a wave propagation model and are not exposed to modelling errors. On the contrary, deterministic models are governed by the laws of physics. Seismic FWI has recently started to be investigated as a Deep Learning framework. Focus has been on the time-domain, while the pseudo-spectral domain has not been yet explored. However, classical FWI experienced major breakthroughs when pseudo-spectral approaches were employed. This work addresses the lacuna that exists in incorporating the pseudo-spectral approach within Deep Learning. This has been done by re-formulating the pseudo-spectral FWI problem as a Deep Learning algorithm for a theory-driven pseudo-spectral approach. A novel Recurrent Neural Network framework is proposed. This is qualitatively assessed on synthetic data, applied to a two-dimensional Marmousi dataset and evaluated against deterministic and time-based approaches. Pseudo-spectral theory-guided FWI using RNN was shown to be more accurate than classical FWI with only 0.05 error tolerance and 1.45\% relative percent-age error. Indeed, this provides more stable convergence, able to identify faults better and has more low frequency content than classical FWI. Moreover, RNN was more suited than classical FWI at edge detection in the shallow and deep sections due to cleaner receiver residuals.

### Data-Driven Pseudo-spectral Full Waveform Inversion via Deep Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.17608)] [[cool](https://papers.cool/arxiv/2502.17608)] [[pdf](https://arxiv.org/pdf/2502.17608)]
> **Authors**: Christopher Zerafa,Pauline Galea,Cristiana Sebu
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 11 pages, 6 pages, review paper
- **标题**: None
- **领域**: 地球物理学,人工智能,计算机视觉和模式识别,机器学习
- **Abstract**: FWI seeks to achieve a high-resolution model of the subsurface through the application of multi-variate optimization to the seismic inverse problem. Although now a mature technology, FWI has limitations related to the choice of the appropriate solver for the forward problem in challenging environments requiring complex assumptions, and very wide angle and multi-azimuth data necessary for full reconstruction are often not available. Deep Learning techniques have emerged as excellent optimization frameworks. These exist between data and theory-guided methods. Data-driven methods do not impose a wave propagation model and are not exposed to modelling errors. On the contrary, deterministic models are governed by the laws of physics. Application of seismic FWI has recently started to be investigated within Deep Learning. This has focussed on the time-domain approach, while the pseudo-spectral domain has not been yet explored. However, classical FWI experienced major breakthroughs when pseudo-spectral approaches were employed. This work addresses the lacuna that exists in incorporating the pseudo-spectral approach within Deep Learning. This has been done by re-formulating the pseudo-spectral FWI problem as a Deep Learning algorithm for a data-driven pseudo-spectral approach. A novel DNN framework is proposed. This is formulated theoretically, qualitatively assessed on synthetic data, applied to a two-dimensional Marmousi dataset and evaluated against deterministic and time-based approaches. Inversion of data-driven pseudo-spectral DNN was found to outperform classical FWI for deeper and over-thrust areas. This is due to the global approximator nature of the technique and hence not bound by forward-modelling physical constraints from ray-tracing.

### Synergizing Deep Learning and Full-Waveform Inversion: Bridging Data-Driven and Theory-Guided Approaches for Enhanced Seismic Imaging 
[[arxiv](https://arxiv.org/abs/2502.17585)] [[cool](https://papers.cool/arxiv/2502.17585)] [[pdf](https://arxiv.org/pdf/2502.17585)]
> **Authors**: Christopher Zerafa,Pauline Galea,Cristiana Sebu
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 20 pages, 14 images, literature review
- **标题**: None
- **领域**: 地球物理学,人工智能,计算工程、金融和科学,机器学习,数值分析
- **Abstract**: This review explores the integration of deep learning (DL) with full-waveform inversion (FWI) for enhanced seismic imaging and subsurface characterization. It covers FWI and DL fundamentals, geophysical applications (velocity estimation, deconvolution, tomography), and challenges (model complexity, data quality). The review also outlines future research directions, including hybrid, generative, and physics-informed models for improved accuracy, efficiency, and reliability in subsurface property estimation. The synergy between DL and FWI has the potential to transform geophysics, providing new insights into Earth's subsurface.

### Gabor-Enhanced Physics-Informed Neural Networks for Fast Simulations of Acoustic Wavefields 
[[arxiv](https://arxiv.org/abs/2502.17134)] [[cool](https://papers.cool/arxiv/2502.17134)] [[pdf](https://arxiv.org/pdf/2502.17134)]
> **Authors**: Mohammad Mahdi Abedi,David Pardo,Tariq Alkhalifah
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 地球物理学,机器学习
- **Abstract**: Physics-Informed Neural Networks (PINNs) have gained increasing attention for solving partial differential equations, including the Helmholtz equation, due to their flexibility and mesh-free formulation. However, their low-frequency bias limits their accuracy and convergence speed for high-frequency wavefield simulations. To alleviate these problems, we propose a simplified PINN framework that incorporates Gabor functions, designed to capture the oscillatory and localized nature of wavefields more effectively. Unlike previous attempts that rely on auxiliary networks to learn Gabor parameters, we redefine the network's task to map input coordinates to a custom Gabor coordinate system, simplifying the training process without increasing the number of trainable parameters compared to a simple PINN. We validate the proposed method across multiple velocity models, including the complex Marmousi and Overthrust models, and demonstrate its superior accuracy, faster convergence, and better robustness features compared to both traditional PINNs and earlier Gabor-based PINNs. Additionally, we propose an efficient integration of a Perfectly Matched Layer (PML) to enhance wavefield behavior near the boundaries. These results suggest that our approach offers an efficient and accurate alternative for scattered wavefield modeling and lays the groundwork for future improvements in PINN-based seismic applications.

## 仪器仪表和探测器(physics.ins-det:Instrumentation and Detectors)

### Inverse Surrogate Model of a Soft X-Ray Spectrometer using Domain Adaptation 
[[arxiv](https://arxiv.org/abs/2502.17505)] [[cool](https://papers.cool/arxiv/2502.17505)] [[pdf](https://arxiv.org/pdf/2502.17505)]
> **Authors**: Enrico Ahlers,Peter Feuer-Forson,Gregor Hartmann,Rolf Mitzner,Peter Baumgärtel,Jens Viefhaus
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 仪器仪表和探测器,人工智能,数据分析、统计和概率
- **Abstract**: In this study, we present a method to create a robust inverse surrogate model for a soft X-ray spectrometer. During a beamtime at an electron storage ring, such as BESSY II, instrumentation and beamlines are required to be correctly aligned and calibrated for optimal experimental conditions. In order to automate these processes, machine learning methods can be developed and implemented, but in many cases these methods require the use of an inverse model which maps the output of the experiment, such as a detector image, to the parameters of the device. Due to limited experimental data, such models are often trained with simulated data, which creates the challenge of compensating for the inherent differences between simulation and experiment. In order to close this gap, we demonstrate the application of data augmentation and adversarial domain adaptation techniques, with which we can predict absolute coordinates for the automated alignment of our spectrometer. Bridging the simulation-experiment gap with minimal real-world data opens new avenues for automated experimentation using machine learning in scientific instrumentation.

## 等离子体物理(physics.plasm-ph:Plasma Physics)

### Robust Confinement State Classification with Uncertainty Quantification through Ensembled Data-Driven Methods 
[[arxiv](https://arxiv.org/abs/2502.17397)] [[cool](https://papers.cool/arxiv/2502.17397)] [[pdf](https://arxiv.org/pdf/2502.17397)]
> **Authors**: Yoeri Poels,Cristina Venturini,Alessandro Pau,Olivier Sauter,Vlado Menkovski,the TCV team,the WPTE team
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 等离子体物理,计算机视觉和模式识别,机器学习
- **Abstract**: Maximizing fusion performance in tokamaks relies on high energy confinement, often achieved through distinct operating regimes. The automated labeling of these confinement states is crucial to enable large-scale analyses or for real-time control applications. While this task becomes difficult to automate near state transitions or in marginal scenarios, much success has been achieved with data-driven models. However, these methods generally provide predictions as point estimates, and cannot adequately deal with missing and/or broken input signals. To enable wide-range applicability, we develop methods for confinement state classification with uncertainty quantification and model robustness. We focus on off-line analysis for TCV discharges, distinguishing L-mode, H-mode, and an in-between dithering phase (D). We propose ensembling data-driven methods on two axes: model formulations and feature sets. The former considers a dynamic formulation based on a recurrent Fourier Neural Operator-architecture and a static formulation based on gradient-boosted decision trees. These models are trained using multiple feature groupings categorized by diagnostic system or physical quantity. A dataset of 302 TCV discharges is fully labeled, and will be publicly released. We evaluate our method quantitatively using Cohen's kappa coefficient for predictive performance and the Expected Calibration Error for the uncertainty calibration. Furthermore, we discuss performance using a variety of common and alternative scenarios, the performance of individual components, out-of-distribution performance, cases of broken or missing signals, and evaluate conditionally-averaged behavior around different state transitions. Overall, the proposed method can distinguish L, D and H-mode with high performance, can cope with missing or broken signals, and provides meaningful uncertainty estimates.

## 生物分子(q-bio.BM:Biomolecules)

### Protein Large Language Models: A Comprehensive Survey 
[[arxiv](https://arxiv.org/abs/2502.17504)] [[cool](https://papers.cool/arxiv/2502.17504)] [[pdf](https://arxiv.org/pdf/2502.17504)]
> **Authors**: Yijia Xiao,Wanjia Zhao,Junkai Zhang,Yiqiao Jin,Han Zhang,Zhicheng Ren,Renliang Sun,Haixin Wang,Guancheng Wan,Pan Lu,Xiao Luo,Yu Zhang,James Zou,Yizhou Sun,Wei Wang
> **First submission**: 2025-02-21
> **First announcement**: 2025-02-25
> **comment**: 24 pages, 4 figures, 5 tables
- **标题**: None
- **领域**: 生物分子,人工智能,计算工程、金融和科学,计算语言学,机器学习
- **Abstract**: Protein-specific large language models (Protein LLMs) are revolutionizing protein science by enabling more efficient protein structure prediction, function annotation, and design. While existing surveys focus on specific aspects or applications, this work provides the first comprehensive overview of Protein LLMs, covering their architectures, training datasets, evaluation metrics, and diverse applications. Through a systematic analysis of over 100 articles, we propose a structured taxonomy of state-of-the-art Protein LLMs, analyze how they leverage large-scale protein sequence data for improved accuracy, and explore their potential in advancing protein engineering and biomedical research. Additionally, we discuss key challenges and future directions, positioning Protein LLMs as essential tools for scientific discovery in protein science. Resources are maintained at https://github.com/Yijia-Xiao/Protein-LLM-Survey.

## 神经元和认知(q-bio.NC:Neurons and Cognition)

### Unraveling the geometry of visual relational reasoning 
[[arxiv](https://arxiv.org/abs/2502.17382)] [[cool](https://papers.cool/arxiv/2502.17382)] [[pdf](https://arxiv.org/pdf/2502.17382)]
> **Authors**: Jiaqi Shang,Gabriel Kreiman,Haim Sompolinsky
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 47 pages, 7 figures, 8 SI figures, 2 SI tables
- **标题**: None
- **领域**: 神经元和认知,计算机视觉和模式识别
- **Abstract**: Humans and other animals readily generalize abstract relations, such as recognizing constant in shape or color, whereas neural networks struggle. To investigate how neural networks generalize abstract relations, we introduce SimplifiedRPM, a novel benchmark for systematic evaluation. In parallel, we conduct human experiments to benchmark relational difficulty, enabling direct model-human comparisons. Testing four architectures--ResNet-50, Vision Transformer, Wild Relation Network, and Scattering Compositional Learner (SCL)--we find that SCL best aligns with human behavior and generalizes best. Building on a geometric theory of neural representations, we show representational geometries that predict generalization. Layer-wise analysis reveals distinct relational reasoning strategies across models and suggests a trade-off where unseen rule representations compress into training-shaped subspaces. Guided by our geometric perspective, we propose and evaluate SNRloss, a novel objective balancing representation geometry. Our findings offer geometric insights into how neural networks generalize abstract relations, paving the way for more human-like visual reasoning in AI.

### Deep Learning-Powered Electrical Brain Signals Analysis: Advancing Neurological Diagnostics 
[[arxiv](https://arxiv.org/abs/2502.17213)] [[cool](https://papers.cool/arxiv/2502.17213)] [[pdf](https://arxiv.org/pdf/2502.17213)]
> **Authors**: Jiahe Li,Xin Chen,Fanqi Shen,Junru Chen,Yuxin Liu,Daoze Zhang,Zhizhang Yuan,Fang Zhao,Meng Li,Yang Yang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 神经元和认知,人工智能,机器学习,信号处理
- **Abstract**: Neurological disorders represent significant global health challenges, driving the advancement of brain signal analysis methods. Scalp electroencephalography (EEG) and intracranial electroencephalography (iEEG) are widely used to diagnose and monitor neurological conditions. However, dataset heterogeneity and task variations pose challenges in developing robust deep learning solutions. This review systematically examines recent advances in deep learning approaches for EEG/iEEG-based neurological diagnostics, focusing on applications across 7 neurological conditions using 46 datasets. We explore trends in data utilization, model design, and task-specific adaptations, highlighting the importance of pre-trained multi-task models for scalable, generalizable solutions. To advance research, we propose a standardized benchmark for evaluating models across diverse datasets to enhance reproducibility. This survey emphasizes how recent innovations can transform neurological diagnostics and enable the development of intelligent, adaptable healthcare solutions.

## 计算金融(q-fin.CP:Computational Finance)

### Predicting Liquidity-Aware Bond Yields using Causal GANs and Deep Reinforcement Learning with LLM Evaluation 
[[arxiv](https://arxiv.org/abs/2502.17011)] [[cool](https://papers.cool/arxiv/2502.17011)] [[pdf](https://arxiv.org/pdf/2502.17011)]
> **Authors**: Jaskaran Singh Walia,Aarush Sinha,Srinitish Srinivasan,Srihari Unnikrishnan
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算金融,计算工程、金融和科学,计算语言学,机器学习,投资组合管理
- **Abstract**: Financial bond yield forecasting is challenging due to data scarcity, nonlinear macroeconomic dependencies, and evolving market conditions. In this paper, we propose a novel framework that leverages Causal Generative Adversarial Networks (CausalGANs) and Soft Actor-Critic (SAC) reinforcement learning (RL) to generate high-fidelity synthetic bond yield data for four major bond categories (AAA, BAA, US10Y, Junk). By incorporating 12 key macroeconomic variables, we ensure statistical fidelity by preserving essential market properties. To transform this market dependent synthetic data into actionable insights, we employ a finetuned Large Language Model (LLM) Qwen2.5-7B that generates trading signals (BUY/HOLD/SELL), risk assessments, and volatility projections. We use automated, human and LLM evaluations, all of which demonstrate that our framework improves forecasting performance over existing methods, with statistical validation via predictive accuracy, MAE evaluation(0.103%), profit/loss evaluation (60% profit rate), LLM evaluation (3.37/5) and expert assessments scoring 4.67 out of 5. The reinforcement learning-enhanced synthetic data generation achieves the least Mean Absolute Error of 0.103, demonstrating its effectiveness in replicating real-world bond market dynamics. We not only enhance data-driven trading strategies but also provides a scalable, high-fidelity synthetic financial data pipeline for risk & volatility management and investment decision-making. This work establishes a bridge between synthetic data generation, LLM driven financial forecasting, and language model evaluation, contributing to AI-driven financial decision-making.

## 量子物理学(quant-ph:Quantum Physics)

### Expressive equivalence of classical and quantum restricted Boltzmann machines 
[[arxiv](https://arxiv.org/abs/2502.17562)] [[cool](https://papers.cool/arxiv/2502.17562)] [[pdf](https://arxiv.org/pdf/2502.17562)]
> **Authors**: Maria Demidik,Cenk Tüysüz,Nico Piatkowski,Michele Grossi,Karl Jansen
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 11 pages, 4 figures; supplementary material 6 pages, 1 figure
- **标题**: None
- **领域**: 量子物理学,机器学习
- **Abstract**: Quantum computers offer the potential for efficiently sampling from complex probability distributions, attracting increasing interest in generative modeling within quantum machine learning. This surge in interest has driven the development of numerous generative quantum models, yet their trainability and scalability remain significant challenges. A notable example is a quantum restricted Boltzmann machine (QRBM), which is based on the Gibbs state of a parameterized non-commuting Hamiltonian. While QRBMs are expressive, their non-commuting Hamiltonians make gradient evaluation computationally demanding, even on fault-tolerant quantum computers. In this work, we propose a semi-quantum restricted Boltzmann machine (sqRBM), a model designed for classical data that mitigates the challenges associated with previous QRBM proposals. The sqRBM Hamiltonian is commuting in the visible subspace while remaining non-commuting in the hidden subspace. This structure allows us to derive closed-form expressions for both output probabilities and gradients. Leveraging these analytical results, we demonstrate that sqRBMs share a close relationship with classical restricted Boltzmann machines (RBM). Our theoretical analysis predicts that, to learn a given probability distribution, an RBM requires three times as many hidden units as an sqRBM, while both models have the same total number of parameters. We validate these findings through numerical simulations involving up to 100 units. Our results suggest that sqRBMs could enable practical quantum machine learning applications in the near future by significantly reducing quantum resource requirements.

## 应用领域(stat.AP:Applications)

### StatLLM: A Dataset for Evaluating the Performance of Large Language Models in Statistical Analysis 
[[arxiv](https://arxiv.org/abs/2502.17657)] [[cool](https://papers.cool/arxiv/2502.17657)] [[pdf](https://arxiv.org/pdf/2502.17657)]
> **Authors**: Xinyi Song,Lina Lee,Kexin Xie,Xueying Liu,Xinwei Deng,Yili Hong
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 25 pages, 7 figures
- **标题**: None
- **领域**: 应用领域,人工智能
- **Abstract**: The coding capabilities of large language models (LLMs) have opened up new opportunities for automatic statistical analysis in machine learning and data science. However, before their widespread adoption, it is crucial to assess the accuracy of code generated by LLMs. A major challenge in this evaluation lies in the absence of a benchmark dataset for statistical code (e.g., SAS and R). To fill in this gap, this paper introduces StatLLM, an open-source dataset for evaluating the performance of LLMs in statistical analysis. The StatLLM dataset comprises three key components: statistical analysis tasks, LLM-generated SAS code, and human evaluation scores. The first component includes statistical analysis tasks spanning a variety of analyses and datasets, providing problem descriptions, dataset details, and human-verified SAS code. The second component features SAS code generated by ChatGPT 3.5, ChatGPT 4.0, and Llama 3.1 for those tasks. The third component contains evaluation scores from human experts in assessing the correctness, effectiveness, readability, executability, and output accuracy of the LLM-generated code. We also illustrate the unique potential of the established benchmark dataset for (1) evaluating and enhancing natural language processing metrics, (2) assessing and improving LLM performance in statistical coding, and (3) developing and testing of next-generation statistical software - advancements that are crucial for data science and machine learning research.

## 方法论(stat.ME:Methodology)

### Uncertainty Quantification for LLM-Based Survey Simulations 
[[arxiv](https://arxiv.org/abs/2502.17773)] [[cool](https://papers.cool/arxiv/2502.17773)] [[pdf](https://arxiv.org/pdf/2502.17773)]
> **Authors**: Chengpiao Huang,Yuhang Wu,Kaizheng Wang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 30 pages, 6 figures, 10 tables
- **标题**: None
- **领域**: 方法论,人工智能,机器学习
- **Abstract**: We investigate the reliable use of simulated survey responses from large language models (LLMs) through the lens of uncertainty quantification. Our approach converts synthetic data into confidence sets for population parameters of human responses, addressing the distribution shift between the simulated and real populations. A key innovation lies in determining the optimal number of simulated responses: too many produce overly narrow confidence sets with poor coverage, while too few yield excessively loose estimates. To resolve this, our method adaptively selects the simulation sample size, ensuring valid average-case coverage guarantees. It is broadly applicable to any LLM, irrespective of its fidelity, and any procedure for constructing confidence sets. Additionally, the selected sample size quantifies the degree of misalignment between the LLM and the target human population. We illustrate our method on real datasets and LLMs.

### Stronger Neyman Regret Guarantees for Adaptive Experimental Design 
[[arxiv](https://arxiv.org/abs/2502.17427)] [[cool](https://papers.cool/arxiv/2502.17427)] [[pdf](https://arxiv.org/pdf/2502.17427)]
> **Authors**: Georgy Noarov,Riccardo Fogliato,Martin Bertran,Aaron Roth
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 方法论,机器学习,统计理论,机器学习
- **Abstract**: We study the design of adaptive, sequential experiments for unbiased average treatment effect (ATE) estimation in the design-based potential outcomes setting. Our goal is to develop adaptive designs offering sublinear Neyman regret, meaning their efficiency must approach that of the hindsight-optimal nonadaptive design. Recent work [Dai et al, 2023] introduced ClipOGD, the first method achieving $\widetilde{O}(\sqrt{T})$ expected Neyman regret under mild conditions. In this work, we propose adaptive designs with substantially stronger Neyman regret guarantees. In particular, we modify ClipOGD to obtain anytime $\widetilde{O}(\log T)$ Neyman regret under natural boundedness assumptions. Further, in the setting where experimental units have pre-treatment covariates, we introduce and study a class of contextual "multigroup" Neyman regret guarantees: Given any set of possibly overlapping groups based on the covariates, the adaptive design outperforms each group's best non-adaptive designs. In particular, we develop a contextual adaptive design with $\widetilde{O}(\sqrt{T})$ anytime multigroup Neyman regret. We empirically validate the proposed designs through an array of experiments.

## 机器学习(stat.ML:Machine Learning)

### An Overview of Large Language Models for Statisticians 
[[arxiv](https://arxiv.org/abs/2502.17814)] [[cool](https://papers.cool/arxiv/2502.17814)] [[pdf](https://arxiv.org/pdf/2502.17814)]
> **Authors**: Wenlong Ji,Weizhe Yuan,Emily Getzen,Kyunghyun Cho,Michael I. Jordan,Song Mei,Jason E Weston,Weijie J. Su,Jing Xu,Linjun Zhang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,机器学习
- **Abstract**: Large Language Models (LLMs) have emerged as transformative tools in artificial intelligence (AI), exhibiting remarkable capabilities across diverse tasks such as text generation, reasoning, and decision-making. While their success has primarily been driven by advances in computational power and deep learning architectures, emerging problems -- in areas such as uncertainty quantification, decision-making, causal inference, and distribution shift -- require a deeper engagement with the field of statistics. This paper explores potential areas where statisticians can make important contributions to the development of LLMs, particularly those that aim to engender trustworthiness and transparency for human users. Thus, we focus on issues such as uncertainty quantification, interpretability, fairness, privacy, watermarking and model adaptation. We also consider possible roles for LLMs in statistical analysis. By bridging AI and statistics, we aim to foster a deeper collaboration that advances both the theoretical foundations and practical applications of LLMs, ultimately shaping their role in addressing complex societal challenges.

### Conformal Prediction Under Generalized Covariate Shift with Posterior Drift 
[[arxiv](https://arxiv.org/abs/2502.17744)] [[cool](https://papers.cool/arxiv/2502.17744)] [[pdf](https://arxiv.org/pdf/2502.17744)]
> **Authors**: Baozhen Wang,Xingye Qiao
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Accepted to AISTATS 2025
- **标题**: None
- **领域**: 机器学习,机器学习,统计理论
- **Abstract**: In many real applications of statistical learning, collecting sufficiently many training data is often expensive, time-consuming, or even unrealistic. In this case, a transfer learning approach, which aims to leverage knowledge from a related source domain to improve the learning performance in the target domain, is more beneficial. There have been many transfer learning methods developed under various distributional assumptions. In this article, we study a particular type of classification problem, called conformal prediction, under a new distributional assumption for transfer learning. Classifiers under the conformal prediction framework predict a set of plausible labels instead of one single label for each data instance, affording a more cautious and safer decision. We consider a generalization of the \textit{covariate shift with posterior drift} setting for transfer learning. Under this setting, we propose a weighted conformal classifier that leverages both the source and target samples, with a coverage guarantee in the target domain. Theoretical studies demonstrate favorable asymptotic properties. Numerical studies further illustrate the usefulness of the proposed method.

### Are GNNs doomed by the topology of their input graph? 
[[arxiv](https://arxiv.org/abs/2502.17739)] [[cool](https://papers.cool/arxiv/2502.17739)] [[pdf](https://arxiv.org/pdf/2502.17739)]
> **Authors**: Amine Mohamed Aboussalah,Abdessalam Ed-dib
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Graph Neural Networks (GNNs) have demonstrated remarkable success in learning from graph-structured data. However, the influence of the input graph's topology on GNN behavior remains poorly understood. In this work, we explore whether GNNs are inherently limited by the structure of their input graphs, focusing on how local topological features interact with the message-passing scheme to produce global phenomena such as oversmoothing or expressive representations. We introduce the concept of $k$-hop similarity and investigate whether locally similar neighborhoods lead to consistent node representations. This interaction can result in either effective learning or inevitable oversmoothing, depending on the inherent properties of the graph. Our empirical experiments validate these insights, highlighting the practical implications of graph topology on GNN performance.

### Function-Space Learning Rates 
[[arxiv](https://arxiv.org/abs/2502.17405)] [[cool](https://papers.cool/arxiv/2502.17405)] [[pdf](https://arxiv.org/pdf/2502.17405)]
> **Authors**: Edward Milsom,Ben Anson,Laurence Aitchison
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 19 pages
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: We consider layerwise function-space learning rates, which measure the magnitude of the change in a neural network's output function in response to an update to a parameter tensor. This contrasts with traditional learning rates, which describe the magnitude of changes in parameter space. We develop efficient methods to measure and set function-space learning rates in arbitrary neural networks, requiring only minimal computational overhead through a few additional backward passes that can be performed at the start of, or periodically during, training. We demonstrate two key applications: (1) analysing the dynamics of standard neural network optimisers in function space, rather than parameter space, and (2) introducing FLeRM (Function-space Learning Rate Matching), a novel approach to hyperparameter transfer across model scales. FLeRM records function-space learning rates while training a small, cheap base model, then automatically adjusts parameter-space layerwise learning rates when training larger models to maintain consistent function-space updates. FLeRM gives hyperparameter transfer across model width, depth, initialisation scale, and LoRA rank in various architectures including MLPs with residual connections and transformers with different layer normalisation schemes.

### A Refined Analysis of UCBVI 
[[arxiv](https://arxiv.org/abs/2502.17370)] [[cool](https://papers.cool/arxiv/2502.17370)] [[pdf](https://arxiv.org/pdf/2502.17370)]
> **Authors**: Simone Drago,Marco Mussi,Alberto Maria Metelli
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: In this work, we provide a refined analysis of the UCBVI algorithm (Azar et al., 2017), improving both the bonus terms and the regret analysis. Additionally, we compare our version of UCBVI with both its original version and the state-of-the-art MVP algorithm. Our empirical validation demonstrates that improving the multiplicative constants in the bounds has significant positive effects on the empirical performance of the algorithms.

### When to Forget? Complexity Trade-offs in Machine Unlearning 
[[arxiv](https://arxiv.org/abs/2502.17323)] [[cool](https://papers.cool/arxiv/2502.17323)] [[pdf](https://arxiv.org/pdf/2502.17323)]
> **Authors**: Martin Van Waerebeke,Marco Lorenzi,Giovanni Neglia,Kevin Scaman
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习,优化与控制
- **Abstract**: Machine Unlearning (MU) aims at removing the influence of specific data points from a trained model, striving to achieve this at a fraction of the cost of full model retraining. In this paper, we analyze the efficiency of unlearning methods and establish the first upper and lower bounds on minimax computation times for this problem, characterizing the performance of the most efficient algorithm against the most difficult objective function. Specifically, for strongly convex objective functions and under the assumption that the forget data is inaccessible to the unlearning method, we provide a phase diagram for the unlearning complexity ratio -- a novel metric that compares the computational cost of the best unlearning method to full model retraining. The phase diagram reveals three distinct regimes: one where unlearning at a reduced cost is infeasible, another where unlearning is trivial because adding noise suffices, and a third where unlearning achieves significant computational advantages over retraining. These findings highlight the critical role of factors such as data dimensionality, the number of samples to forget, and privacy constraints in determining the practical feasibility of unlearning.

### Linear Bandits on Ellipsoids: Minimax Optimal Algorithms 
[[arxiv](https://arxiv.org/abs/2502.17175)] [[cool](https://papers.cool/arxiv/2502.17175)] [[pdf](https://arxiv.org/pdf/2502.17175)]
> **Authors**: Raymond Zhang,Hedi Hadiji,Richard Combes
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 20 pages, 3 figures
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: We consider linear stochastic bandits where the set of actions is an ellipsoid. We provide the first known minimax optimal algorithm for this problem. We first derive a novel information-theoretic lower bound on the regret of any algorithm, which must be at least $Ω(\min(d σ\sqrt{T} + d \|θ\|_{A}, \|θ\|_{A} T))$ where $d$ is the dimension, $T$ the time horizon, $σ^2$ the noise variance, $A$ a matrix defining the set of actions and $θ$ the vector of unknown parameters. We then provide an algorithm whose regret matches this bound to a multiplicative universal constant. The algorithm is non-classical in the sense that it is not optimistic, and it is not a sampling algorithm. The main idea is to combine a novel sequential procedure to estimate $\|θ\|$, followed by an explore-and-commit strategy informed by this estimate. The algorithm is highly computationally efficient, and a run requires only time $O(dT + d^2 \log(T/d) + d^3)$ and memory $O(d^2)$, in contrast with known optimistic algorithms, which are not implementable in polynomial time. We go beyond minimax optimality and show that our algorithm is locally asymptotically minimax optimal, a much stronger notion of optimality. We further provide numerical experiments to illustrate our theoretical findings.

### Differential privacy guarantees of Markov chain Monte Carlo algorithms 
[[arxiv](https://arxiv.org/abs/2502.17150)] [[cool](https://papers.cool/arxiv/2502.17150)] [[pdf](https://arxiv.org/pdf/2502.17150)]
> **Authors**: Andrea Bertazzi,Tim Johnston,Gareth O. Roberts,Alain Durmus
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习,计算
- **Abstract**: This paper aims to provide differential privacy (DP) guarantees for Markov chain Monte Carlo (MCMC) algorithms. In a first part, we establish DP guarantees on samples output by MCMC algorithms as well as Monte Carlo estimators associated with these methods under assumptions on the convergence properties of the underlying Markov chain. In particular, our results highlight the critical condition of ensuring the target distribution is differentially private itself. In a second part, we specialise our analysis to the unadjusted Langevin algorithm and stochastic gradient Langevin dynamics and establish guarantees on their (Rényi) DP. To this end, we develop a novel methodology based on Girsanov's theorem combined with a perturbation trick to obtain bounds for an unbounded domain and in a non-convex setting. We establish: (i) uniform in $n$ privacy guarantees when the state of the chain after $n$ iterations is released, (ii) bounds on the privacy of the entire chain trajectory. These findings provide concrete guidelines for privacy-preserving MCMC.

### On Quantile Regression Forests for Modelling Mixed-Frequency and Longitudinal Data 
[[arxiv](https://arxiv.org/abs/2502.17137)] [[cool](https://papers.cool/arxiv/2502.17137)] [[pdf](https://arxiv.org/pdf/2502.17137)]
> **Authors**: Mila Andreani
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: PhD Thesis
- **标题**: None
- **领域**: 机器学习,机器学习,方法论
- **Abstract**: The aim of this thesis is to extend the applications of the Quantile Regression Forest (QRF) algorithm to handle mixed-frequency and longitudinal data. To this end, standard statistical approaches have been exploited to build two novel algorithms: the Mixed- Frequency Quantile Regression Forest (MIDAS-QRF) and the Finite Mixture Quantile Regression Forest (FM-QRF). The MIDAS-QRF combines the flexibility of QRF with the Mixed Data Sampling (MIDAS) approach, enabling non-parametric quantile estimation with variables observed at different frequencies. FM-QRF, on the other hand, extends random effects machine learning algorithms to a QR framework, allowing for conditional quantile estimation in a longitudinal data setting. The contributions of this dissertation lie both methodologically and empirically. Methodologically, the MIDAS-QRF and the FM-QRF represent two novel approaches for handling mixed-frequency and longitudinal data in QR machine learning framework. Empirically, the application of the proposed models in financial risk management and climate-change impact evaluation demonstrates their validity as accurate and flexible models to be applied in complex empirical settings.

### Random Projections and Natural Sparsity in Time-Series Classification: A Theoretical Analysis 
[[arxiv](https://arxiv.org/abs/2502.17061)] [[cool](https://papers.cool/arxiv/2502.17061)] [[pdf](https://arxiv.org/pdf/2502.17061)]
> **Authors**: Jorge Marco-Blanco,Rubén Cuevas
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Time-series classification is essential across diverse domains, including medical diagnosis, industrial monitoring, financial forecasting, and human activity recognition. The Rocket algorithm has emerged as a simple yet powerful method, achieving state-of-the-art performance through random convolutional kernels applied to time-series data, followed by non-linear transformation. Its architecture approximates a one-hidden-layer convolutional neural network while eliminating parameter training, ensuring computational efficiency. Despite its empirical success, fundamental questions about its theoretical foundations remain unexplored. We bridge theory and practice by formalizing Rocket's random convolutional filters within the compressed sensing framework, proving that random projections preserve discriminative patterns in time-series data. This analysis reveals relationships between kernel parameters and input signal characteristics, enabling more principled approaches to algorithm configuration. Moreover, we demonstrate that its non-linearity, based on the proportion of positive values after convolutions, expresses the inherent sparsity of time-series data. Our theoretical investigation also proves that Rocket satisfies two critical conditions: translation invariance and noise robustness. These findings enhance interpretability and provide guidance for parameter optimization in extreme cases, advancing both theoretical understanding and practical application of time-series classification.

### Your Assumed DAG is Wrong and Here's How To Deal With It 
[[arxiv](https://arxiv.org/abs/2502.17030)] [[cool](https://papers.cool/arxiv/2502.17030)] [[pdf](https://arxiv.org/pdf/2502.17030)]
> **Authors**: Kirtan Padh,Zhufeng Li,Cecilia Casolo,Niki Kilbertus
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Assuming a directed acyclic graph (DAG) that represents prior knowledge of causal relationships between variables is a common starting point for cause-effect estimation. Existing literature typically invokes hypothetical domain expert knowledge or causal discovery algorithms to justify this assumption. In practice, neither may propose a single DAG with high confidence. Domain experts are hesitant to rule out dependencies with certainty or have ongoing disputes about relationships; causal discovery often relies on untestable assumptions itself or only provides an equivalence class of DAGs and is commonly sensitive to hyperparameter and threshold choices. We propose an efficient, gradient-based optimization method that provides bounds for causal queries over a collection of causal graphs -- compatible with imperfect prior knowledge -- that may still be too large for exhaustive enumeration. Our bounds achieve good coverage and sharpness for causal queries such as average treatment effects in linear and non-linear synthetic settings as well as on real-world data. Our approach aims at providing an easy-to-use and widely applicable rebuttal to the valid critique of `What if your assumed DAG is wrong?'.

### Convergence of Shallow ReLU Networks on Weakly Interacting Data 
[[arxiv](https://arxiv.org/abs/2502.16977)] [[cool](https://papers.cool/arxiv/2502.16977)] [[pdf](https://arxiv.org/pdf/2502.16977)]
> **Authors**: Léo Dana,Francis Bach,Loucas Pillaud-Vivien
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习,统计理论
- **Abstract**: We analyse the convergence of one-hidden-layer ReLU networks trained by gradient flow on $n$ data points. Our main contribution leverages the high dimensionality of the ambient space, which implies low correlation of the input samples, to demonstrate that a network with width of order $\log(n)$ neurons suffices for global convergence with high probability. Our analysis uses a Polyak-Łojasiewicz viewpoint along the gradient-flow trajectory, which provides an exponential rate of convergence of $\frac{1}{n}$. When the data are exactly orthogonal, we give further refined characterizations of the convergence speed, proving its asymptotic behavior lies between the orders $\frac{1}{n}$ and $\frac{1}{\sqrt{n}}$, and exhibiting a phase-transition phenomenon in the convergence rate, during which it evolves from the lower bound to the upper, and in a relative time of order $\frac{1}{\log(n)}$.

### Provable Benefits of Unsupervised Pre-training and Transfer Learning via Single-Index Models 
[[arxiv](https://arxiv.org/abs/2502.16849)] [[cool](https://papers.cool/arxiv/2502.16849)] [[pdf](https://arxiv.org/pdf/2502.16849)]
> **Authors**: Taj Jones-McCormick,Aukosh Jagannath,Subhabrata Sen
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Unsupervised pre-training and transfer learning are commonly used techniques to initialize training algorithms for neural networks, particularly in settings with limited labeled data. In this paper, we study the effects of unsupervised pre-training and transfer learning on the sample complexity of high-dimensional supervised learning. Specifically, we consider the problem of training a single-layer neural network via online stochastic gradient descent. We establish that pre-training and transfer learning (under concept shift) reduce sample complexity by polynomial factors (in the dimension) under very general assumptions. We also uncover some surprising settings where pre-training grants exponential improvement over random initialization in terms of sample complexity.

## 其他论文

- [Quantifying interdisciplinary synergy in higher STEM education](https://arxiv.org/abs/2502.17841)
  - **标题**: None
  - **Filtered Reason**: none of physics.soc-ph,physics.ed-ph,cond-mat.stat-mech,cs.IT in whitelist
- [Silent Speech Sentence Recognition with Six-Axis Accelerometers using Conformer and CTC Algorithm](https://arxiv.org/abs/2502.17829)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC,eess.AS,cs.SD in whitelist
- [Novel quantum circuit for image compression utilizing modified Toffoli gate and quantized transformed coefficient alongside a novel reset gate](https://arxiv.org/abs/2502.17815)
  - **标题**: None
  - **Filtered Reason**: none of quant-ph,cs.ET in whitelist
- [Gender Bias in Perception of Human Managers Extends to AI Managers](https://arxiv.org/abs/2502.17730)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [Spectral Efficiency Expression for the Non-Linear Schrödinger Channel in the Low Noise Limit Using Scattering Data](https://arxiv.org/abs/2502.17702)
  - **标题**: None
  - **Filtered Reason**: none of cond-mat.stat-mech,cs.IT in whitelist
- [THOR: A Non-Speculative Value Dependent Timing Side Channel Attack Exploiting Intel AMX](https://arxiv.org/abs/2502.17658)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR,cs.AR in whitelist
- [SET-PAiREd: Designing for Parental Involvement in Learning with an AI-Assisted Educational Robot](https://arxiv.org/abs/2502.17623)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC,cs.RO in whitelist
- [ELMo-Tune-V2: LLM-Assisted Full-Cycle Auto-Tuning to Optimize LSM-Based Key-Value Stores](https://arxiv.org/abs/2502.17606)
  - **标题**: None
  - **Filtered Reason**: none of cs.DB in whitelist
- [Weaving the Cosmos: WASM-Powered Interchain Communication for AI Enabled Smart Contracts](https://arxiv.org/abs/2502.17604)
  - **标题**: None
  - **Filtered Reason**: none of cs.CE,cs.SE in whitelist
- [Data Voids and Warning Banners on Google Search](https://arxiv.org/abs/2502.17542)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR,cs.CY,cs.SI in whitelist
- [Abstract computation over first-order structures. Part I: Deterministic and non-deterministic BSS RAMs](https://arxiv.org/abs/2502.17539)
  - **标题**: None
  - **Filtered Reason**: none of math.LO,cs.LO in whitelist
- [ConvoyLLM: Dynamic Multi-Lane Convoy Control Using LLMs](https://arxiv.org/abs/2502.17529)
  - **标题**: None
  - **Filtered Reason**: none of cs.MA,cs.RO in whitelist
- [Decentralized and Robust Privacy-Preserving Model Using Blockchain-Enabled Federated Deep Learning in Intelligent Enterprises](https://arxiv.org/abs/2502.17485)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [Physics Informed Neural Network Estimated Circuit Parameter Adaptive Modulation of DAB](https://arxiv.org/abs/2502.17452)
  - **标题**: None
  - **Filtered Reason**: none of cs.NE,eess.SP,eess.SY in whitelist
- [Studying How Configurations Impact Code Generation in LLMs: the Case of ChatGPT](https://arxiv.org/abs/2502.17450)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Evolving Form and Function: Dual-Objective Optimization in Neural Symbolic Regression Networks](https://arxiv.org/abs/2502.17393)
  - **标题**: None
  - **Filtered Reason**: none of cs.NE in whitelist
- [Goal-Oriented Middleware Filtering at Transport Layer Based on Value of Updates](https://arxiv.org/abs/2502.17350)
  - **标题**: None
  - **Filtered Reason**: none of cs.NI in whitelist
- [How Scientists Use Large Language Models to Program](https://arxiv.org/abs/2502.17348)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC,cs.SE in whitelist
- [SoFFT: Spatial Fourier Transform for Modeling Continuum Soft Robots](https://arxiv.org/abs/2502.17347)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [User-Centric Evaluation Methods for Digital Twin Applications in Extended Reality](https://arxiv.org/abs/2502.17346)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [City riots fed by transnational and trans-topic web-of-influence](https://arxiv.org/abs/2502.17331)
  - **标题**: None
  - **Filtered Reason**: none of physics.soc-ph,cs.SI in whitelist
- [Alpha-SQL: Zero-Shot Text-to-SQL using Monte Carlo Tree Search](https://arxiv.org/abs/2502.17248)
  - **标题**: None
  - **Filtered Reason**: none of cs.DB in whitelist
- [A Reinforcement Learning Approach to Non-prehensile Manipulation through Sliding](https://arxiv.org/abs/2502.17221)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [Fundamental Trade-off Between Computation and Communication in Private Coded Distributed Computing](https://arxiv.org/abs/2502.17195)
  - **标题**: None
  - **Filtered Reason**: none of cs.IT in whitelist
- [A Novel Multiple Access Scheme for Heterogeneous Wireless Communications using Symmetry-aware Continual Deep Reinforcement Learning](https://arxiv.org/abs/2502.17167)
  - **标题**: None
  - **Filtered Reason**: none of cs.NI in whitelist
- [Semantic-Aware Dynamic and Distributed Power Allocation: a Multi-UAV Area Coverage Use Case](https://arxiv.org/abs/2502.17120)
  - **标题**: None
  - **Filtered Reason**: none of cs.NI in whitelist
- [Equality Saturation for Optimizing High-Level Julia IR](https://arxiv.org/abs/2502.17075)
  - **标题**: None
  - **Filtered Reason**: none of cs.PL,cs.SE in whitelist
- [Evolution 6.0: Evolving Robotic Capabilities Through Generative Design](https://arxiv.org/abs/2502.17034)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO,cs.NE in whitelist
- [Be CIM or Be Memory: A Dual-mode-aware DNN Compiler for CIM Accelerators](https://arxiv.org/abs/2502.17006)
  - **标题**: None
  - **Filtered Reason**: none of cs.AR in whitelist
- [From Independence of Clones to Composition Consistency: A Hierarchy of Barriers to Strategic Nomination](https://arxiv.org/abs/2502.16973)
  - **标题**: None
  - **Filtered Reason**: none of cs.GT in whitelist
- [Make LLM Inference Affordable to Everyone: Augmenting GPU Memory with NDP-DIMM](https://arxiv.org/abs/2502.16963)
  - **标题**: None
  - **Filtered Reason**: none of cs.AR in whitelist
- [FilterLLM: Text-To-Distribution LLM for Billion-Scale Cold-Start Recommendation](https://arxiv.org/abs/2502.16924)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [Unlocking Scientific Concepts: How Effective Are LLM-Generated Analogies for Student Understanding and Classroom Practice?](https://arxiv.org/abs/2502.16895)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Primitive-Planner: An Ultra Lightweight Quadrotor Planner with Time-optimal Primitives](https://arxiv.org/abs/2502.16882)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [APINT: A Full-Stack Framework for Acceleration of Privacy-Preserving Inference of Transformers based on Garbled Circuits](https://arxiv.org/abs/2502.16877)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR,cs.AR in whitelist
- [Can Tensor Cores Benefit Memory-Bound Kernels? (No!)](https://arxiv.org/abs/2502.16851)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC in whitelist
- [Uncovering simultaneous breakthroughs with a robust measure of disruptiveness](https://arxiv.org/abs/2502.16845)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY,cs.SI in whitelist
