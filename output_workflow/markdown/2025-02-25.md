> 本文由 [https://github.com/huiyeruzhou/arxiv_crawler](https://github.com/huiyeruzhou/arxiv_crawler) 自动生成
>
> 领域白名单：cs.AI,cs.CL,cs.LG,cs.CV
> 关键词： LLM, GPT, AI, language+model, deep+learning, transformer, neural+network, machine+learning

# 论文全览：2025-02-25

共有98篇相关领域论文, 另有9篇其他

## 人工智能(cs.AI:Artificial Intelligence)

### A Combinatorial Identities Benchmark for Theorem Proving via Automated Theorem Generation 
[[arxiv](https://arxiv.org/abs/2502.17840)] [[cool](https://papers.cool/arxiv/2502.17840)] [[pdf](https://arxiv.org/pdf/2502.17840)]
> **Authors**: Beibei Xiong,Hangyu Lv,Haojia Shan,Jianlin Wang,Zhengfeng Yang,Lihong Zhi
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Large language models (LLMs) have significantly advanced formal theorem proving, yet the scarcity of high-quality training data constrains their capabilities in complex mathematical domains. Combinatorics, a cornerstone of mathematics, provides essential tools for analyzing discrete structures and solving optimization problems. However, its inherent complexity makes it particularly challenging for automated theorem proving (ATP) for combinatorial identities. To address this, we manually construct LeanComb, combinatorial identities benchmark in Lean, which is, to our knowledge, the first formalized theorem proving benchmark built for combinatorial identities. We develop an Automated Theorem Generator for Combinatorial Identities, ATG4CI, which combines candidate tactics suggested by a self-improving large language model with a Reinforcement Learning Tree Search approach for tactic prediction. By utilizing ATG4CI, we generate a LeanComb-Enhanced dataset comprising 260K combinatorial identities theorems, each with a complete formal proof in Lean, and experimental evaluations demonstrate that models trained on this dataset can generate more effective tactics, thereby improving success rates in automated theorem proving for combinatorial identities.

### DocPuzzle: A Process-Aware Benchmark for Evaluating Realistic Long-Context Reasoning Capabilities 
[[arxiv](https://arxiv.org/abs/2502.17807)] [[cool](https://papers.cool/arxiv/2502.17807)] [[pdf](https://arxiv.org/pdf/2502.17807)]
> **Authors**: Tianyi Zhuang,Chuqiao Kuang,Xiaoguang Li,Yihua Teng,Jihao Wu,Yasheng Wang,Lifeng Shang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: We present DocPuzzle, a rigorously constructed benchmark for evaluating long-context reasoning capabilities in large language models (LLMs). This benchmark comprises 100 expert-level QA problems requiring multi-step reasoning over long real-world documents. To ensure the task quality and complexity, we implement a human-AI collaborative annotation-validation pipeline. DocPuzzle introduces an innovative evaluation framework that mitigates guessing bias through checklist-guided process analysis, establishing new standards for assessing reasoning capacities in LLMs. Our evaluation results show that: 1)Advanced slow-thinking reasoning models like o1-preview(69.7%) and DeepSeek-R1(66.3%) significantly outperform best general instruct models like Claude 3.5 Sonnet(57.7%); 2)Distilled reasoning models like DeepSeek-R1-Distill-Qwen-32B(41.3%) falls far behind the teacher model, suggesting challenges to maintain the generalization of reasoning capabilities relying solely on distillation.

### Detection of LLM-Paraphrased Code and Identification of the Responsible LLM Using Coding Style Features 
[[arxiv](https://arxiv.org/abs/2502.17749)] [[cool](https://papers.cool/arxiv/2502.17749)] [[pdf](https://arxiv.org/pdf/2502.17749)]
> **Authors**: Shinwoo Park,Hyundong Jin,Jeong-won Cha,Yo-Sub Han
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Recent progress in large language models (LLMs) for code generation has raised serious concerns about intellectual property protection. Malicious users can exploit LLMs to produce paraphrased versions of proprietary code that closely resemble the original. While the potential for LLM-assisted code paraphrasing continues to grow, research on detecting it remains limited, underscoring an urgent need for detection system. We respond to this need by proposing two tasks. The first task is to detect whether code generated by an LLM is a paraphrased version of original human-written code. The second task is to identify which LLM is used to paraphrase the original code. For these tasks, we construct a dataset LPcode consisting of pairs of human-written code and LLM-paraphrased code using various LLMs. We statistically confirm significant differences in the coding styles of human-written and LLM-paraphrased code, particularly in terms of naming consistency, code structure, and readability. Based on these findings, we develop LPcodedec, a detection method that identifies paraphrase relationships between human-written and LLM-generated code, and discover which LLM is used for the paraphrasing. LPcodedec outperforms the best baselines in two tasks, improving F1 scores by 2.64% and 15.17% while achieving speedups of 1,343x and 213x, respectively.

### Mind the Gesture: Evaluating AI Sensitivity to Culturally Offensive Non-Verbal Gestures 
[[arxiv](https://arxiv.org/abs/2502.17710)] [[cool](https://papers.cool/arxiv/2502.17710)] [[pdf](https://arxiv.org/pdf/2502.17710)]
> **Authors**: Akhila Yerukola,Saadia Gabriel,Nanyun Peng,Maarten Sap
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 40 pages, 49 figures
- **标题**: None
- **领域**: 人工智能,计算语言学,计算机视觉和模式识别,机器学习
- **Abstract**: Gestures are an integral part of non-verbal communication, with meanings that vary across cultures, and misinterpretations that can have serious social and diplomatic consequences. As AI systems become more integrated into global applications, ensuring they do not inadvertently perpetuate cultural offenses is critical. To this end, we introduce Multi-Cultural Set of Inappropriate Gestures and Nonverbal Signs (MC-SIGNS), a dataset of 288 gesture-country pairs annotated for offensiveness, cultural significance, and contextual factors across 25 gestures and 85 countries. Through systematic evaluation using MC-SIGNS, we uncover critical limitations: text-to-image (T2I) systems exhibit strong US-centric biases, performing better at detecting offensive gestures in US contexts than in non-US ones; large language models (LLMs) tend to over-flag gestures as offensive; and vision-language models (VLMs) default to US-based interpretations when responding to universal concepts like wishing someone luck, frequently suggesting culturally inappropriate gestures. These findings highlight the urgent need for culturally-aware AI safety mechanisms to ensure equitable global deployment of AI technologies.

### From Perceptions to Decisions: Wildfire Evacuation Decision Prediction with Behavioral Theory-informed LLMs 
[[arxiv](https://arxiv.org/abs/2502.17701)] [[cool](https://papers.cool/arxiv/2502.17701)] [[pdf](https://arxiv.org/pdf/2502.17701)]
> **Authors**: Ruxiao Chen,Chenguang Wang,Yuran Sun,Xilei Zhao,Susu Xu
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 24 pages, 9 figures
- **标题**: None
- **领域**: 人工智能,计算语言学,计算机与社会,机器学习
- **Abstract**: Evacuation decision prediction is critical for efficient and effective wildfire response by helping emergency management anticipate traffic congestion and bottlenecks, allocate resources, and minimize negative impacts. Traditional statistical methods for evacuation decision prediction fail to capture the complex and diverse behavioral logic of different individuals. In this work, for the first time, we introduce FLARE, short for facilitating LLM for advanced reasoning on wildfire evacuation decision prediction, a Large Language Model (LLM)-based framework that integrates behavioral theories and models to streamline the Chain-of-Thought (CoT) reasoning and subsequently integrate with memory-based Reinforcement Learning (RL) module to provide accurate evacuation decision prediction and understanding. Our proposed method addresses the limitations of using existing LLMs for evacuation behavioral predictions, such as limited survey data, mismatching with behavioral theory, conflicting individual preferences, implicit and complex mental states, and intractable mental state-behavior mapping. Experiments on three post-wildfire survey datasets show an average of 20.47% performance improvement over traditional theory-informed behavioral models, with strong cross-event generalizability. Our complete code is publicly available at https://github.com/SusuXu-s-Lab/FLARE

### Socratic: Enhancing Human Teamwork via AI-enabled Coaching 
[[arxiv](https://arxiv.org/abs/2502.17643)] [[cool](https://papers.cool/arxiv/2502.17643)] [[pdf](https://arxiv.org/pdf/2502.17643)]
> **Authors**: Sangwon Seo,Bing Han,Rayan E. Harari,Roger D. Dias,Marco A. Zenati,Eduardo Salas,Vaibhav Unhelkar
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Extended version of an identically-titled paper accepted at AAMAS 2025
- **标题**: None
- **领域**: 人工智能,人机交互,机器学习,多代理系统
- **Abstract**: Coaches are vital for effective collaboration, but cost and resource constraints often limit their availability during real-world tasks. This limitation poses serious challenges in life-critical domains that rely on effective teamwork, such as healthcare and disaster response. To address this gap, we propose and realize an innovative application of AI: task-time team coaching. Specifically, we introduce Socratic, a novel AI system that complements human coaches by providing real-time guidance during task execution. Socratic monitors team behavior, detects misalignments in team members' shared understanding, and delivers automated interventions to improve team performance. We validated Socratic through two human subject experiments involving dyadic collaboration. The results demonstrate that the system significantly enhances team performance with minimal interventions. Participants also perceived Socratic as helpful and trustworthy, supporting its potential for adoption. Our findings also suggest promising directions both for AI research and its practical applications to enhance human teamwork.

### Representation Engineering for Large-Language Models: Survey and Research Challenges 
[[arxiv](https://arxiv.org/abs/2502.17601)] [[cool](https://papers.cool/arxiv/2502.17601)] [[pdf](https://arxiv.org/pdf/2502.17601)]
> **Authors**: Lukasz Bartoszcze,Sarthak Munshi,Bryan Sukidi,Jennifer Yen,Zejia Yang,David Williams-King,Linh Le,Kosi Asuzu,Carsten Maple
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Large-language models are capable of completing a variety of tasks, but remain unpredictable and intractable. Representation engineering seeks to resolve this problem through a new approach utilizing samples of contrasting inputs to detect and edit high-level representations of concepts such as honesty, harmfulness or power-seeking. We formalize the goals and methods of representation engineering to present a cohesive picture of work in this emerging discipline. We compare it with alternative approaches, such as mechanistic interpretability, prompt-engineering and fine-tuning. We outline risks such as performance decrease, compute time increases and steerability issues. We present a clear agenda for future research to build predictable, dynamic, safe and personalizable LLMs.

## 计算语言学(cs.CL:Computation and Language)

### Say Less, Mean More: Leveraging Pragmatics in Retrieval-Augmented Generation 
[[arxiv](https://arxiv.org/abs/2502.17839)] [[cool](https://papers.cool/arxiv/2502.17839)] [[pdf](https://arxiv.org/pdf/2502.17839)]
> **Authors**: Haris Riaz,Ellen Riloff,Mihai Surdeanu
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 16 pages, 2 figures, 8 tables. Preprint
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: We propose a simple, unsupervised method that injects pragmatic principles in retrieval-augmented generation (RAG) frameworks such as Dense Passage Retrieval to enhance the utility of retrieved contexts. Our approach first identifies which sentences in a pool of documents retrieved by RAG are most relevant to the question at hand, cover all the topics addressed in the input question and no more, and then highlights these sentences within their context, before they are provided to the LLM, without truncating or altering the context in any other way. We show that this simple idea brings consistent improvements in experiments on three question answering tasks (ARC-Challenge, PubHealth and PopQA) using five different LLMs. It notably enhances relative accuracy by up to 19.7% on PubHealth and 10% on ARC-Challenge compared to a conventional RAG system.

### Predicting Through Generation: Why Generation Is Better for Prediction 
[[arxiv](https://arxiv.org/abs/2502.17817)] [[cool](https://papers.cool/arxiv/2502.17817)] [[pdf](https://arxiv.org/pdf/2502.17817)]
> **Authors**: Md Kowsher,Nusrat Jahan Prottasha,Prakash Bhat,Chun-Nam Yu,Mojtaba Soltanalian,Ivan Garibay,Ozlem Garibay,Chen Chen,Niloofar Yousefi
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Preprint paper
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: This paper argues that generating output tokens is more effective than using pooled representations for prediction tasks because token-level generation retains more mutual information. Since LLMs are trained on massive text corpora using next-token prediction, generation aligns naturally with their learned behavior. Using the Data Processing Inequality (DPI), we provide both theoretical and empirical evidence supporting this claim. However, autoregressive models face two key challenges when used for prediction: (1) exposure bias, where the model sees ground truth tokens during training but relies on its own predictions during inference, leading to errors, and (2) format mismatch, where discrete tokens do not always align with the tasks required output structure. To address these challenges, we introduce PredGen(Predicting Through Generating), an end to end framework that (i) uses scheduled sampling to reduce exposure bias, and (ii) introduces a task adapter to convert the generated tokens into structured outputs. Additionally, we introduce Writer-Director Alignment Loss (WDAL), which ensures consistency between token generation and final task predictions, improving both text coherence and numerical accuracy. We evaluate PredGen on multiple classification and regression benchmarks. Our results show that PredGen consistently outperforms standard baselines, demonstrating its effectiveness in structured prediction tasks.

### Can Multimodal LLMs Perform Time Series Anomaly Detection? 
[[arxiv](https://arxiv.org/abs/2502.17812)] [[cool](https://papers.cool/arxiv/2502.17812)] [[pdf](https://arxiv.org/pdf/2502.17812)]
> **Authors**: Xiongxiao Xu,Haoran Wang,Yueqing Liang,Philip S. Yu,Yue Zhao,Kai Shu
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 9 pages for the main content; 32 pages for the full paper including the appendix. More resources on the intersection of multimodal LLMs and time series analysis are on the website https://mllm-ts.github.io
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Large language models (LLMs) have been increasingly used in time series analysis. However, the potential of multimodal LLMs (MLLMs), particularly vision-language models, for time series remains largely under-explored. One natural way for humans to detect time series anomalies is through visualization and textual description. Motivated by this, we raise a critical and practical research question: Can multimodal LLMs perform time series anomaly detection? To answer this, we propose VisualTimeAnomaly benchmark to evaluate MLLMs in time series anomaly detection (TSAD). Our approach transforms time series numerical data into the image format and feed these images into various MLLMs, including proprietary models (GPT-4o and Gemini-1.5) and open-source models (LLaVA-NeXT and Qwen2-VL), each with one larger and one smaller variant. In total, VisualTimeAnomaly contains 12.4k time series images spanning 3 scenarios and 3 anomaly granularities with 9 anomaly types across 8 MLLMs. Starting with the univariate case (point- and range-wise anomalies), we extend our evaluation to more practical scenarios, including multivariate and irregular time series scenarios, and variate-wise anomalies. Our study reveals several key insights: 1) MLLMs detect range- and variate-wise anomalies more effectively than point-wise anomalies. 2) MLLMs are highly robust to irregular time series, even with 25% of the data missing. 3) Open-source MLLMs perform comparably to proprietary models in TSAD. While open-source MLLMs excel on univariate time series, proprietary MLLMs demonstrate superior effectiveness on multivariate time series. To the best of our knowledge, this is the first work to comprehensively investigate MLLMs for TSAD, particularly for multivariate and irregular time series scenarios. We release our dataset and code at https://github.com/mllm-ts/VisualTimeAnomaly to support future research.

### URO-Bench: A Comprehensive Benchmark for End-to-End Spoken Dialogue Models 
[[arxiv](https://arxiv.org/abs/2502.17810)] [[cool](https://papers.cool/arxiv/2502.17810)] [[pdf](https://arxiv.org/pdf/2502.17810)]
> **Authors**: Ruiqi Yan,Xiquan Li,Wenxi Chen,Zhikang Niu,Chen Yang,Ziyang Ma,Kai Yu,Xie Chen
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,音频和语音处理
- **Abstract**: In recent years, with advances in large language models (LLMs), end-to-end spoken dialogue models (SDMs) have made significant strides. Compared to text-based LLMs, the evaluation of SDMs needs to take speech-related aspects into account, such as paralinguistic information and speech quality. However, there is still a lack of comprehensive evaluations for SDMs in speech-to-speech (S2S) scenarios. To address this gap, we propose URO-Bench, an extensive benchmark for SDMs. Notably, URO-Bench is the first S2S benchmark that covers evaluations about multilingualism, multi-round dialogues, and paralinguistics. Our benchmark is divided into two difficulty levels: basic track and pro track, consisting of 16 and 20 datasets respectively, evaluating the model's abilities in Understanding, Reasoning, and Oral conversation. Evaluations on our proposed benchmark reveal that current open-source SDMs perform rather well in daily QA tasks, but lag behind their backbone LLMs in terms of instruction-following ability and also suffer from catastrophic forgetting. Their performance in advanced evaluations of paralinguistic information and audio understanding remains subpar, highlighting the need for further research in this direction. We hope that URO-Bench can effectively facilitate the development of spoken dialogue models by providing a multifaceted evaluation of existing models and helping to track progress in this area.

### Your Language Model May Think Too Rigidly: Achieving Reasoning Consistency with Symmetry-Enhanced Training 
[[arxiv](https://arxiv.org/abs/2502.17800)] [[cool](https://papers.cool/arxiv/2502.17800)] [[pdf](https://arxiv.org/pdf/2502.17800)]
> **Authors**: Yihang Yao,Zhepeng Cen,Miao Li,William Han,Yuyou Zhang,Emerson Liu,Zuxin Liu,Chuang Gan,Ding Zhao
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models (LLMs) have demonstrated strong reasoning capabilities across various tasks. However, even minor variations in query phrasing, despite preserving the underlying semantic meaning, can significantly affect their performance. To address this, we focus on enhancing LLMs' awareness of symmetry in query variations and propose syMmetry-ENhanceD (MEND) Data Augmentation, a data-centric approach that improves the model's ability to extract useful information from context. Unlike existing methods that emphasize reasoning chain augmentation, our approach improves model robustness at the knowledge extraction stage through query augmentations, enabling more data-efficient training and stronger generalization to Out-of-Distribution (OOD) settings. Extensive experiments on both logical and arithmetic reasoning tasks show that MEND enhances reasoning performance across diverse query variations, providing new insight into improving LLM robustness through structured dataset curation.

### Enhancing Human Evaluation in Machine Translation with Comparative Judgment 
[[arxiv](https://arxiv.org/abs/2502.17797)] [[cool](https://papers.cool/arxiv/2502.17797)] [[pdf](https://arxiv.org/pdf/2502.17797)]
> **Authors**: Yixiao Song,Parker Riley,Daniel Deutsch,Markus Freitag
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Preprint, 15 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Human evaluation is crucial for assessing rapidly evolving language models but is influenced by annotator proficiency and task design. This study explores the integration of comparative judgment into human annotation for machine translation (MT) and evaluates three annotation setups-point-wise Multidimensional Quality Metrics (MQM), side-by-side (SxS) MQM, and its simplified version SxS relative ranking (RR). In MQM, annotators mark error spans with categories and severity levels. SxS MQM extends MQM to pairwise error annotation for two translations of the same input, while SxS RR focuses on selecting the better output without labeling errors. Key findings are: (1) the SxS settings achieve higher inter-annotator agreement than MQM; (2) SxS MQM enhances inter-translation error marking consistency compared to MQM by, on average, 38.5% for explicitly compared MT systems and 19.5% for others; (3) all annotation settings return stable system rankings, with SxS RR offering a more efficient alternative to (SxS) MQM; (4) the SxS settings highlight subtle errors overlooked in MQM without altering absolute system evaluations. To spur further research, we will release the triply annotated datasets comprising 377 ZhEn and 104 EnDe annotation examples.

### AIR: Complex Instruction Generation via Automatic Iterative Refinement 
[[arxiv](https://arxiv.org/abs/2502.17787)] [[cool](https://papers.cool/arxiv/2502.17787)] [[pdf](https://arxiv.org/pdf/2502.17787)]
> **Authors**: Wei Liu,Yancheng He,Hui Huang,Chengwei Hu,Jiaheng Liu,Shilong Li,Wenbo Su,Bo Zheng
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: The first three authors contributed equally, 20 pages
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: With the development of large language models, their ability to follow simple instructions has significantly improved. However, adhering to complex instructions remains a major challenge. Current approaches to generating complex instructions are often irrelevant to the current instruction requirements or suffer from limited scalability and diversity. Moreover, methods such as back-translation, while effective for simple instruction generation, fail to leverage the rich contents and structures in large web corpora. In this paper, we propose a novel automatic iterative refinement framework to generate complex instructions with constraints, which not only better reflects the requirements of real scenarios but also significantly enhances LLMs' ability to follow complex instructions. The AIR framework consists of two stages: (1)Generate an initial instruction from a document; (2)Iteratively refine instructions with LLM-as-judge guidance by comparing the model's output with the document to incorporate valuable constraints. Finally, we construct the AIR-10K dataset with 10K complex instructions and demonstrate that instructions generated with our approach significantly improve the model's ability to follow complex instructions, outperforming existing methods for instruction generation.

### Exploring the Potential of Large Language Models for Estimating the Reading Comprehension Question Difficulty 
[[arxiv](https://arxiv.org/abs/2502.17785)] [[cool](https://papers.cool/arxiv/2502.17785)] [[pdf](https://arxiv.org/pdf/2502.17785)]
> **Authors**: Yoshee Jain,John Hollander,Amber He,Sunny Tang,Liang Zhang,John Sabatini
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 13 pages, 2 figures
- **标题**: None
- **领域**: 计算语言学,人机交互
- **Abstract**: Reading comprehension is a key for individual success, yet the assessment of question difficulty remains challenging due to the extensive human annotation and large-scale testing required by traditional methods such as linguistic analysis and Item Response Theory (IRT). While these robust approaches provide valuable insights, their scalability is limited. There is potential for Large Language Models (LLMs) to automate question difficulty estimation; however, this area remains underexplored. Our study investigates the effectiveness of LLMs, specifically OpenAI's GPT-4o and o1, in estimating the difficulty of reading comprehension questions using the Study Aid and Reading Assessment (SARA) dataset. We evaluated both the accuracy of the models in answering comprehension questions and their ability to classify difficulty levels as defined by IRT. The results indicate that, while the models yield difficulty estimates that align meaningfully with derived IRT parameters, there are notable differences in their sensitivity to extreme item characteristics. These findings suggest that LLMs can serve as the scalable method for automated difficulty assessment, particularly in dynamic interactions between learners and Adaptive Instructional Systems (AIS), bridging the gap between traditional psychometric techniques and modern AIS for reading comprehension and paving the way for more adaptive and personalized educational assessments.

### FoREST: Frame of Reference Evaluation in Spatial Reasoning Tasks 
[[arxiv](https://arxiv.org/abs/2502.17775)] [[cool](https://papers.cool/arxiv/2502.17775)] [[pdf](https://arxiv.org/pdf/2502.17775)]
> **Authors**: Tanawan Premsri,Parisa Kordjamshidi
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 9 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Spatial reasoning is a fundamental aspect of human intelligence. One key concept in spatial cognition is the Frame of Reference (FoR), which identifies the perspective of spatial expressions. Despite its significance, FoR has received limited attention in AI models that need spatial intelligence. There is a lack of dedicated benchmarks and in-depth evaluation of large language models (LLMs) in this area. To address this issue, we introduce the Frame of Reference Evaluation in Spatial Reasoning Tasks (FoREST) benchmark, designed to assess FoR comprehension in LLMs. We evaluate LLMs on answering questions that require FoR comprehension and layout generation in text-to-image models using FoREST. Our results reveal a notable performance gap across different FoR classes in various LLMs, affecting their ability to generate accurate layouts for text-to-image generation. This highlights critical shortcomings in FoR comprehension. To improve FoR understanding, we propose Spatial-Guided prompting, which improves LLMs ability to extract essential spatial concepts. Our proposed method improves overall performance across spatial reasoning tasks.

### LLM Inference Acceleration via Efficient Operation Fusion 
[[arxiv](https://arxiv.org/abs/2502.17728)] [[cool](https://papers.cool/arxiv/2502.17728)] [[pdf](https://arxiv.org/pdf/2502.17728)]
> **Authors**: Mahsa Salmani,Ilya Soloveychik
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,硬件架构
- **Abstract**: The rapid development of the Transformer-based Large Language Models (LLMs) in recent years has been closely linked to their ever-growing and already enormous sizes. Many LLMs contain hundreds of billions of parameters and require dedicated hardware resources for training and inference. One of the key challenges inherent to the Transformer architecture is the requirement to support numerous non-linear transformations that involves normalization. For instance, each decoder block typically contains at least one Softmax operation and two Layernorms. The computation of the corresponding normalization scaling factors becomes a major bottleneck as it requires spatial collective operations. In other words, when it comes to the computation of denominators for Softmax and Layernorm, all vector elements must be aggregated into a single location, requiring significant communication. These collective operations slow down inference on Transformers by approximately 20%, defeating the whole purpose of distributed in-memory compute. In this work, we propose an extremely efficient technique that can completely hide the overhead caused by such collective operations. Note that each Softmax and Layernorm operation is typically followed by a linear layer. Since non-linear and linear operations are performed on different hardware engines, they can be easily parallelized once the algebra allows such commutation. By leveraging the inherent properties of linear operations, we can defer the normalization of the preceding Softmax and Layernorm until after the linear layer is computed. Now we can compute the collective scaling factors concurrently with the matrix multiplication and completely hide the latency of the former behind the latter. Such parallelization preserves the numerical accuracy while significantly improving the hardware utilization and reducing the overall latency.

### Spontaneous Giving and Calculated Greed in Language Models 
[[arxiv](https://arxiv.org/abs/2502.17720)] [[cool](https://papers.cool/arxiv/2502.17720)] [[pdf](https://arxiv.org/pdf/2502.17720)]
> **Authors**: Yuxuan Li,Hirokazu Shirado
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large language models, when trained with reinforcement learning, demonstrate advanced problem-solving capabilities through reasoning techniques like chain of thoughts and reflection. However, it is unclear how these reasoning capabilities extend to social intelligence. In this study, we investigate how reasoning influences model outcomes in social dilemmas. First, we examine the effects of chain-of-thought and reflection techniques in a public goods game. We then extend our analysis to six economic games on cooperation and punishment, comparing off-the-shelf non-reasoning and reasoning models. We find that reasoning models reduce cooperation and norm enforcement, prioritizing individual rationality. Consequently, groups with more reasoning models exhibit less cooperation and lower gains through repeated interactions. These behaviors parallel human tendencies of "spontaneous giving and calculated greed." Our results suggest the need for AI architectures that incorporate social intelligence alongside reasoning capabilities to ensure that AI supports, rather than disrupts, human cooperative intuition.

### LR${}^{2}$Bench: Evaluating Long-chain Reflective Reasoning Capabilities of Large Language Models via Constraint Satisfaction Problems 
[[arxiv](https://arxiv.org/abs/2502.17848)] [[cool](https://papers.cool/arxiv/2502.17848)] [[pdf](https://arxiv.org/pdf/2502.17848)]
> **Authors**: Jianghao Chen,Zhenlin Wei,Zhenjiang Ren,Ziyong Li,Jiajun Zhang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Recent progress in o1-like models has significantly enhanced the reasoning abilities of Large Language Models (LLMs), empowering them to tackle increasingly complex tasks through reflection capabilities, such as making assumptions, backtracking, and self-refinement. However, effectively evaluating such reflection capabilities remains challenging due to the lack of appropriate benchmarks. To bridge this gap, we introduce LR${}^{2}$Bench, a novel benchmark designed to evaluate the Long-chain Reflective Reasoning capabilities of LLMs. LR${}^{2}$Bench comprises 850 samples across six Constraint Satisfaction Problems (CSPs) where reflective reasoning is crucial for deriving solutions that meet all given constraints. Each type of task focuses on distinct constraint patterns, such as knowledge-based, logical, and spatial constraints, providing a comprehensive evaluation of diverse problem-solving scenarios. We conduct extensive evaluation on both conventional models and o1-like models. Our experimental results reveal that even the most advanced reasoning-specific models, such as DeepSeek-R1 and OpenAI o1-preview, struggle with tasks in LR${}^{2}$Bench, achieving an average Exact Match score of only 20.0% and 23.6%, respectively. These findings underscore the significant room for improvement in the reflective reasoning capabilities of current LLMs. The leaderboard of our benchmark is available at https://huggingface.co/spaces/UltraRonin/LR2Bench

### Knowledge Distillation with Training Wheels 
[[arxiv](https://arxiv.org/abs/2502.17717)] [[cool](https://papers.cool/arxiv/2502.17717)] [[pdf](https://arxiv.org/pdf/2502.17717)]
> **Authors**: Guanlin Liu,Anand Ramachandran,Tanmay Gangwani,Yan Fu,Abhinav Sethy
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Knowledge distillation is used, in generative language modeling, to train a smaller student model using the help of a larger teacher model, resulting in improved capabilities for the student model. In this paper, we formulate a more general framework for knowledge distillation where the student learns from the teacher during training, and also learns to ask for the teacher's help at test-time following rules specifying test-time restrictions. Towards this, we first formulate knowledge distillation as an entropy-regularized value optimization problem. Adopting Path Consistency Learning to solve this, leads to a new knowledge distillation algorithm using on-policy and off-policy demonstrations. We extend this using constrained reinforcement learning to a framework that incorporates the use of the teacher model as a test-time reference, within constraints. In this situation, akin to a human learner, the model needs to learn not only the learning material, but also the relative difficulty of different sections to prioritize for seeking teacher help. We examine the efficacy of our method through experiments in translation and summarization tasks, observing trends in accuracy and teacher use, noting that our approach unlocks operating points not available to the popular Speculative Decoding approach.

### Bridging Information Gaps with Comprehensive Answers: Improving the Diversity and Informativeness of Follow-Up Questions 
[[arxiv](https://arxiv.org/abs/2502.17715)] [[cool](https://papers.cool/arxiv/2502.17715)] [[pdf](https://arxiv.org/pdf/2502.17715)]
> **Authors**: Zhe Liu,Taekyu Kang,Haoyu Wang,Seyed Hossein Alavi,Vered Shwartz
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 8 pages, 2 figures, submitted to ACL 2025
- **标题**: None
- **领域**: 计算语言学,人工智能,人机交互
- **Abstract**: Effective conversational systems are expected to dynamically generate contextual follow-up questions to elicit new information while maintaining the conversation flow. While humans excel at asking diverse and informative questions by intuitively assessing both obtained and missing information, existing models often fall short of human performance on this task. To mitigate this, we propose a method that generates diverse and informative questions based on targeting unanswered information using a hypothetical LLM-generated "comprehensive answer". Our method is applied to augment an existing follow-up questions dataset. The experimental results demonstrate that language models fine-tuned on the augmented datasets produce follow-up questions of significantly higher quality and diversity. This promising approach could be effectively adopted to future work to augment information-seeking dialogues for reducing ambiguities and improving the accuracy of LLM answers.

### Semantics drives analogical change in Germanic strong verb paradigms: a phylogenetic study 
[[arxiv](https://arxiv.org/abs/2502.17670)] [[cool](https://papers.cool/arxiv/2502.17670)] [[pdf](https://arxiv.org/pdf/2502.17670)]
> **Authors**: Alexandru Craevschi,Sarah Babinski,Chundra Cathcart
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: A large body of research on morphological paradigms makes the prediction that irregular morphological patterns of allomorphy are more likely to emerge and persist when they serve to mark important functional distinctions. More specifically, it has been observed that in some Germanic languages in which narrative past tense is expressed by the past participle, there is a greater affinity for stem allomorphy shared by preterite forms and past participles to the exclusion of present forms (the so-called ABB pattern), as it serves to enhance marking of the binary semantic opposition between present and past. Using data from 107 cognate verbs attested across 14 archaic and contemporary Germanic languages and a novel hierarchical phylogenetic model, we show that there is a greater long-term preference for this alternation pattern in situations where narrative past tense has been extended to the past participle, confirming this hypothesis. We further elucidate the mechanisms underlying this association, demonstrating that this association holds because verbs with the ABB pattern are more likely to preserve it in situations where it marks an important binary semantic opposition; however, there is less evidence that the ABB pattern is extended to verbs with different patterns under the same circumstances. These results bear on debate as to whether the distribution of irregularity we observe cross-linguistically is due primarily to (1) the preservation of irregular patterns or (2) an active drive toward irregularization in certain contexts, and are more in line with the first hypothesis.

### Towards Human Cognition: Visual Context Guides Syntactic Priming in Fusion-Encoded Models 
[[arxiv](https://arxiv.org/abs/2502.17669)] [[cool](https://papers.cool/arxiv/2502.17669)] [[pdf](https://arxiv.org/pdf/2502.17669)]
> **Authors**: Bushi Xiao,Michael Bennie,Jayetri Bardhan,Daisy Zhe Wang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 8 pages, 9 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: We introduced PRISMATIC, the first multimodal structural priming dataset, and proposed a reference-free evaluation metric that assesses priming effects without predefined target sentences. Using this metric, we constructed and tested models with different multimodal encoding architectures (dual encoder and fusion encoder) to investigate their structural preservation capabilities. Our findings show that models with both encoding methods demonstrate comparable syntactic priming effects. However, only fusion-encoded models exhibit robust positive correlations between priming effects and visual similarity, suggesting a cognitive process more aligned with human psycholinguistic patterns. This work provides new insights into evaluating and understanding how syntactic information is processed in multimodal language models.

### Towards Typologically Aware Rescoring to Mitigate Unfaithfulness in Lower-Resource Languages 
[[arxiv](https://arxiv.org/abs/2502.17664)] [[cool](https://papers.cool/arxiv/2502.17664)] [[pdf](https://arxiv.org/pdf/2502.17664)]
> **Authors**: Tsan Tsai Chan,Xin Tong,Thi Thu Uyen Hoang,Barbare Tepnadze,Wojciech Stempniak
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: ISCA/ITG Workshop on Diversity in Large Speech andLanguageModels
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Multilingual large language models (LLMs) are known to more frequently generate non-faithful output in resource-constrained languages (Guerreiro et al., 2023 - arXiv:2303.16104), potentially because these typologically diverse languages are underrepresented in their training data. To mitigate unfaithfulness in such settings, we propose using computationally light auxiliary models to rescore the outputs of larger architectures. As proof of the feasibility of such an approach, we show that monolingual 4-layer BERT models pretrained from scratch on less than 700 MB of data without fine-tuning are able to identify faithful summaries with a mean accuracy of 88.33% in three genetically unrelated languages that differ in their morphological complexity - Vietnamese, Polish and Georgian. The same hyperparameter combination moreover generalises well to three other tasks, suggesting applications for rescoring beyond improving faithfulness. In order to inform typologically aware model selection, we also investigate how morphological complexity interacts with regularisation, model depth and training objectives, ultimately demonstrating that morphologically complex languages are more likely to benefit from dropout, while across languages downstream performance is enhanced most by shallow architectures as well as training using the standard BERT objectives.

### Evaluating the Effect of Retrieval Augmentation on Social Biases 
[[arxiv](https://arxiv.org/abs/2502.17611)] [[cool](https://papers.cool/arxiv/2502.17611)] [[pdf](https://arxiv.org/pdf/2502.17611)]
> **Authors**: Tianhui Zhang,Yi Zhou,Danushka Bollegala
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 18 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Retrieval Augmented Generation (RAG) has gained popularity as a method for conveniently incorporating novel facts that were not seen during the pre-training stage in Large Language Model (LLM)-based Natural Language Generation (NLG) systems. However, LLMs are known to encode significant levels of unfair social biases. The modulation of these biases by RAG in NLG systems is not well understood. In this paper, we systematically study the relationship between the different components of a RAG system and the social biases presented in the text generated across three languages (i.e. English, Japanese and Chinese) and four social bias types (i.e. gender, race, age and religion). Specifically, using the Bias Question Answering (BBQ) benchmark datasets, we evaluate the social biases in RAG responses from document collections with varying levels of stereotypical biases, employing multiple LLMs used as generators. We find that the biases in document collections are often amplified in the generated responses, even when the generating LLM exhibits a low-level of bias. Our findings raise concerns about the use of RAG as a technique for injecting novel facts into NLG systems and call for careful evaluation of potential social biases in RAG applications before their real-world deployment.

### PICASO: Permutation-Invariant Context Composition with State Space Models 
[[arxiv](https://arxiv.org/abs/2502.17605)] [[cool](https://papers.cool/arxiv/2502.17605)] [[pdf](https://arxiv.org/pdf/2502.17605)]
> **Authors**: Tian Yu Liu,Alessandro Achille,Matthew Trager,Aditya Golatkar,Luca Zancato,Stefano Soatto
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Published in The Thirteenth International Conference onLearningRepresentations, ICLR 2025
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Providing Large Language Models with relevant contextual knowledge at inference time has been shown to greatly improve the quality of their generations. This is often achieved by prepending informative passages of text, or 'contexts', retrieved from external knowledge bases to their input. However, processing additional contexts online incurs significant computation costs that scale with their length. State Space Models (SSMs) offer a promising solution by allowing a database of contexts to be mapped onto fixed-dimensional states from which to start the generation. A key challenge arises when attempting to leverage information present across multiple contexts, since there is no straightforward way to condition generation on multiple independent states in existing SSMs. To address this, we leverage a simple mathematical relation derived from SSM dynamics to compose multiple states into one that efficiently approximates the effect of concatenating textual contexts. Since the temporal ordering of contexts can often be uninformative, we enforce permutation-invariance by efficiently averaging states obtained via our composition algorithm across all possible context orderings. We evaluate our resulting method on WikiText and MSMARCO in both zero-shot and fine-tuned settings, and show that we can match the strongest performing baseline while enjoying on average 5.4x speedup.

### MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context Inference 
[[arxiv](https://arxiv.org/abs/2502.17599)] [[cool](https://papers.cool/arxiv/2502.17599)] [[pdf](https://arxiv.org/pdf/2502.17599)]
> **Authors**: Zhongwei Wan,Hui Shen,Xin Wang,Che Liu,Zheda Mai,Mi Zhang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: NAACL 2025 Main
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Long-context Multimodal Large Language Models (MLLMs) that incorporate long text-image and text-video modalities, demand substantial resources as their multimodal Key-Value (KV) caches grow with increasing input lengths, challenging inference efficiency. Existing methods for KV cache compression, in both text-only and multimodal LLMs, have neglected attention density variations across layers, thus often adopting uniform or progressive reduction strategies for layer-wise cache allocation. In this work, we propose MEDA, a dynamic layer-wise KV cache allocation method for efficient multimodal long-context inference. As its core, MEDA utilizes cross-modal attention entropy to determine the KV cache size at each MLLMs layer. Given the dynamically allocated KV cache size at each layer, MEDA also employs a KV pair selection scheme to identify which KV pairs to select and a KV pair merging strategy that merges the selected and non-selected ones to preserve information from the entire context. MEDA achieves up to 72% KV cache memory reduction and 2.82 times faster decoding speed, while maintaining or enhancing performance on various multimodal tasks in long-context settings, including multi-images and long-video scenarios. Our code is released at https://github.com/AIoT-MLSys-Lab/MEDA.

### Proactive Privacy Amnesia for Large Language Models: Safeguarding PII with Negligible Impact on Model Utility 
[[arxiv](https://arxiv.org/abs/2502.17591)] [[cool](https://papers.cool/arxiv/2502.17591)] [[pdf](https://arxiv.org/pdf/2502.17591)]
> **Authors**: Martin Kuo,Jingyang Zhang,Jianyi Zhang,Minxue Tang,Louis DiValentin,Aolin Ding,Jingwei Sun,William Chen,Amin Hass,Tianlong Chen,Yiran Chen,Hai Li
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: ICLR'25 Poster. Project page and code is available at https://ppa-iclr2025.my.canva.site/
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: With the rise of large language models (LLMs), increasing research has recognized their risk of leaking personally identifiable information (PII) under malicious attacks. Although efforts have been made to protect PII in LLMs, existing methods struggle to balance privacy protection with maintaining model utility. In this paper, inspired by studies of amnesia in cognitive science, we propose a novel approach, Proactive Privacy Amnesia (PPA), to safeguard PII in LLMs while preserving their utility. This mechanism works by actively identifying and forgetting key memories most closely associated with PII in sequences, followed by a memory implanting using suitable substitute memories to maintain the LLM's functionality. We conduct evaluations across multiple models to protect common PII, such as phone numbers and physical addresses, against prevalent PII-targeted attacks, demonstrating the superiority of our method compared with other existing defensive techniques. The results show that our PPA method completely eliminates the risk of phone number exposure by 100% and significantly reduces the risk of physical address exposure by 9.8% - 87.6%, all while maintaining comparable model utility performance.

### End-to-End Chart Summarization via Visual Chain-of-Thought in Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2502.17589)] [[cool](https://papers.cool/arxiv/2502.17589)] [[pdf](https://arxiv.org/pdf/2502.17589)]
> **Authors**: Raymond Choi,Frank Burns,Chase Lawrence
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Automated chart summarization is crucial for enhancing data accessibility and enabling efficient information extraction from visual data. While recent advances in visual-language models (VLMs) have demonstrated promise, existing methods often suffer from limitations in matching the generated summary to the chart data and in reasoning about complex chart patterns. This paper introduces End-to-End Visual Chain-of-Thought (V-CoT) for chart summarization, a novel approach optimized for Large Vision-Language Models (LVLMs). Our method directly trains an LVLM to process chart images and generate textual summaries in an end-to-end fashion, eliminating the need for explicit chart parsing modules. We incorporate a visual Chain-of-Thought mechanism through instruction fine-tuning, implicitly guiding the LVLM to perform visual reasoning steps during summary generation. Evaluated on the large-scale Chart-Sum-QA dataset, our V-CoT method significantly outperforms state-of-the-art baselines across a range of automatic metrics, including BLEU, BLEURT, CIDEr, and CS, and demonstrates superior matching degree and reasoning correctness in human evaluations. Ablation studies and detailed analyses further validate the effectiveness and robustness of our proposed approach, establishing a new benchmark for end-to-end chart summarization.

## 密码学和安全(cs.CR:Cryptography and Security)

### Research on Enhancing Cloud Computing Network Security using Artificial Intelligence Algorithms 
[[arxiv](https://arxiv.org/abs/2502.17801)] [[cool](https://papers.cool/arxiv/2502.17801)] [[pdf](https://arxiv.org/pdf/2502.17801)]
> **Authors**: Yuqing Wang,Xiao Yang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Cloud computing environments are increasingly vulnerable to security threats such as distributed denial-of-service (DDoS) attacks and SQL injection. Traditional security mechanisms, based on rule matching and feature recognition, struggle to adapt to evolving attack strategies. This paper proposes an adaptive security protection framework leveraging deep learning to construct a multi-layered defense architecture. The proposed system is evaluated in a real-world business environment, achieving a detection accuracy of 97.3%, an average response time of 18 ms, and an availability rate of 99.999%. Experimental results demonstrate that the proposed method significantly enhances detection accuracy, response efficiency, and resource utilization, offering a novel and effective approach to cloud computing security.

### Design and implementation of a distributed security threat detection system integrating federated learning and multimodal LLM 
[[arxiv](https://arxiv.org/abs/2502.17763)] [[cool](https://papers.cool/arxiv/2502.17763)] [[pdf](https://arxiv.org/pdf/2502.17763)]
> **Authors**: Yuqing Wang,Xiao Yang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能,分布式、并行和集群计算,表现
- **Abstract**: Traditional security protection methods struggle to address sophisticated attack vectors in large-scale distributed systems, particularly when balancing detection accuracy with data privacy concerns. This paper presents a novel distributed security threat detection system that integrates federated learning with multimodal large language models (LLMs). Our system leverages federated learning to ensure data privacy while employing multimodal LLMs to process heterogeneous data sources including network traffic, system logs, images, and sensor data. Experimental evaluation on a 10TB distributed dataset demonstrates that our approach achieves 96.4% detection accuracy, outperforming traditional baseline models by 4.1 percentage points. The system reduces both false positive and false negative rates by 1.8 and 2.4 percentage points respectively. Performance analysis shows that our system maintains efficient processing capabilities in distributed environments, requiring 180 seconds for model training and 3.8 seconds for threat detection across the distributed network. These results demonstrate significant improvements in detection accuracy and computational efficiency while preserving data privacy, suggesting strong potential for real-world deployment in large-scale security systems.

## 计算机视觉和模式识别(cs.CV:Computer Vision and Pattern Recognition)

### Weakly Supervised Pixel-Level Annotation with Visual Interpretability 
[[arxiv](https://arxiv.org/abs/2502.17824)] [[cool](https://papers.cool/arxiv/2502.17824)] [[pdf](https://arxiv.org/pdf/2502.17824)]
> **Authors**: Basma Nasir,Tehseen Zia,Muhammad Nawaz,Catarina Moreira
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Medical image annotation is essential for diagnosing diseases, yet manual annotation is time-consuming, costly, and prone to variability among experts. To address these challenges, we propose an automated explainable annotation system that integrates ensemble learning, visual explainability, and uncertainty quantification. Our approach combines three pre-trained deep learning models - ResNet50, EfficientNet, and DenseNet - enhanced with XGrad-CAM for visual explanations and Monte Carlo Dropout for uncertainty quantification. This ensemble mimics the consensus of multiple radiologists by intersecting saliency maps from models that agree on the diagnosis while uncertain predictions are flagged for human review. We evaluated our system using the TBX11K medical imaging dataset and a Fire segmentation dataset, demonstrating its robustness across different domains. Experimental results show that our method outperforms baseline models, achieving 93.04% accuracy on TBX11K and 96.4% accuracy on the Fire dataset. Moreover, our model produces precise pixel-level annotations despite being trained with only image-level labels, achieving Intersection over Union IoU scores of 36.07% and 64.7%, respectively. By enhancing the accuracy and interpretability of image annotations, our approach offers a reliable and transparent solution for medical diagnostics and other image analysis tasks.

### LAM: Large Avatar Model for One-shot Animatable Gaussian Head 
[[arxiv](https://arxiv.org/abs/2502.17796)] [[cool](https://papers.cool/arxiv/2502.17796)] [[pdf](https://arxiv.org/pdf/2502.17796)]
> **Authors**: Yisheng He,Xiaodong Gu,Xiaodan Ye,Chao Xu,Zhengyi Zhao,Yuan Dong,Weihao Yuan,Zilong Dong,Liefeng Bo
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: We present LAM, an innovative Large Avatar Model for animatable Gaussian head reconstruction from a single image. Unlike previous methods that require extensive training on captured video sequences or rely on auxiliary neural networks for animation and rendering during inference, our approach generates Gaussian heads that are immediately animatable and renderable. Specifically, LAM creates an animatable Gaussian head in a single forward pass, enabling reenactment and rendering without additional networks or post-processing steps. This capability allows for seamless integration into existing rendering pipelines, ensuring real-time animation and rendering across a wide range of platforms, including mobile phones. The centerpiece of our framework is the canonical Gaussian attributes generator, which utilizes FLAME canonical points as queries. These points interact with multi-scale image features through a Transformer to accurately predict Gaussian attributes in the canonical space. The reconstructed canonical Gaussian avatar can then be animated utilizing standard linear blend skinning (LBS) with corrective blendshapes as the FLAME model did and rendered in real-time on various platforms. Our experimental results demonstrate that LAM outperforms state-of-the-art methods on existing benchmarks.

### Synthia: Novel Concept Design with Affordance Composition 
[[arxiv](https://arxiv.org/abs/2502.17793)] [[cool](https://papers.cool/arxiv/2502.17793)] [[pdf](https://arxiv.org/pdf/2502.17793)]
> **Authors**: Xiaomeng Jin,Hyeonjeong Ha,Jeonghwan Kim,Jiateng Liu,Zhenhailong Wang,Khanh Duy Nguyen,Ansel Blume,Nanyun Peng,Kai-wei Chang,Heng Ji
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Code is available https://github.com/HyeonjeongHa/SYNTHIA
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Text-to-image (T2I) models enable rapid concept design, making them widely used in AI-driven design. While recent studies focus on generating semantic and stylistic variations of given design concepts, functional coherence--the integration of multiple affordances into a single coherent concept--remains largely overlooked. In this paper, we introduce SYNTHIA, a framework for generating novel, functionally coherent designs based on desired affordances. Our approach leverages a hierarchical concept ontology that decomposes concepts into parts and affordances, serving as a crucial building block for functionally coherent design. We also develop a curriculum learning scheme based on our ontology that contrastively fine-tunes T2I models to progressively learn affordance composition while maintaining visual novelty. To elaborate, we (i) gradually increase affordance distance, guiding models from basic concept-affordance association to complex affordance compositions that integrate parts of distinct affordances into a single, coherent form, and (ii) enforce visual novelty by employing contrastive objectives to push learned representations away from existing concepts. Experimental results show that SYNTHIA outperforms state-of-the-art T2I models, demonstrating absolute gains of 25.1% and 14.7% for novelty and functional coherence in human evaluation, respectively.

### Improving Transformer Based Line Segment Detection with Matched Predicting and Re-ranking 
[[arxiv](https://arxiv.org/abs/2502.17766)] [[cool](https://papers.cool/arxiv/2502.17766)] [[pdf](https://arxiv.org/pdf/2502.17766)]
> **Authors**: Xin Tong,Shi Peng,Baojie Tian,Yufei Guo,Xuhui Huang,Zhe Ma
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Classical Transformer-based line segment detection methods have delivered impressive results. However, we observe that some accurately detected line segments are assigned low confidence scores during prediction, causing them to be ranked lower and potentially suppressed. Additionally, these models often require prolonged training periods to achieve strong performance, largely due to the necessity of bipartite matching. In this paper, we introduce RANK-LETR, a novel Transformer-based line segment detection method. Our approach leverages learnable geometric information to refine the ranking of predicted line segments by enhancing the confidence scores of high-quality predictions in a posterior verification step. We also propose a new line segment proposal method, wherein the feature point nearest to the centroid of the line segment directly predicts the location, significantly improving training efficiency and stability. Moreover, we introduce a line segment ranking loss to stabilize rankings during training, thereby enhancing the generalization capability of the model. Experimental results demonstrate that our method outperforms other Transformer-based and CNN-based approaches in prediction accuracy while requiring fewer training epochs than previous Transformer-based models.

### A digital eye-fixation biomarker using a deep anomaly scheme to classify Parkisonian patterns 
[[arxiv](https://arxiv.org/abs/2502.17762)] [[cool](https://papers.cool/arxiv/2502.17762)] [[pdf](https://arxiv.org/pdf/2502.17762)]
> **Authors**: Juan Niño,Luis Guayacán,Santiago Gómez,Fabio Martínez
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 6 pages, 4 images
- **标题**: None
- **领域**: 计算机视觉和模式识别,图像和视频处理,神经元和认知
- **Abstract**: Oculomotor alterations constitute a promising biomarker to detect and characterize Parkinson's disease (PD), even in prodromal stages. Currently, only global and simplified eye movement trajectories are employed to approximate the complex and hidden kinematic relationships of the oculomotor function. Recent advances on machine learning and video analysis have encouraged novel characterizations of eye movement patterns to quantify PD. These schemes enable the identification of spatiotemporal segments primarily associated with PD. However, they rely on discriminative models that require large training datasets and depend on balanced class distributions. This work introduces a novel video analysis scheme to quantify Parkinsonian eye fixation patterns with an anomaly detection framework. Contrary to classical deep discriminative schemes that learn differences among labeled classes, the proposed approach is focused on one-class learning, avoiding the necessity of a significant amount of data. The proposed approach focuses only on Parkinson's representation, considering any other class sample as an anomaly of the distribution. This approach was evaluated for an ocular fixation task, in a total of 13 control subjects and 13 patients on different stages of the disease. The proposed digital biomarker achieved an average sensitivity and specificity of 0.97 and 0.63, respectively, yielding an AUC-ROC of 0.95. A statistical test shows significant differences (p < 0.05) among predicted classes, evidencing a discrimination between patients and control subjects.

### AI-driven 3D Spatial Transcriptomics 
[[arxiv](https://arxiv.org/abs/2502.17761)] [[cool](https://papers.cool/arxiv/2502.17761)] [[pdf](https://arxiv.org/pdf/2502.17761)]
> **Authors**: Cristina Almagro-Pérez,Andrew H. Song,Luca Weishaupt,Ahrong Kim,Guillaume Jaume,Drew F. K. Williamson,Konstantin Hemker,Ming Y. Lu,Kritika Singh,Bowen Chen,Long Phi Le,Alexander S. Baras,Sizun Jiang,Ali Bashashati,Jonathan T. C. Liu,Faisal Mahmood
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,应用领域
- **Abstract**: A comprehensive three-dimensional (3D) map of tissue architecture and gene expression is crucial for illuminating the complexity and heterogeneity of tissues across diverse biomedical applications. However, most spatial transcriptomics (ST) approaches remain limited to two-dimensional (2D) sections of tissue. Although current 3D ST methods hold promise, they typically require extensive tissue sectioning, are complex, are not compatible with non-destructive 3D tissue imaging technologies, and often lack scalability. Here, we present VOlumetrically Resolved Transcriptomics EXpression (VORTEX), an AI framework that leverages 3D tissue morphology and minimal 2D ST to predict volumetric 3D ST. By pretraining on diverse 3D morphology-transcriptomic pairs from heterogeneous tissue samples and then fine-tuning on minimal 2D ST data from a specific volume of interest, VORTEX learns both generic tissue-related and sample-specific morphological correlates of gene expression. This approach enables dense, high-throughput, and fast 3D ST, scaling seamlessly to large tissue volumes far beyond the reach of existing 3D ST techniques. By offering a cost-effective and minimally destructive route to obtaining volumetric molecular insights, we anticipate that VORTEX will accelerate biomarker discovery and our understanding of morphomolecular associations and cell states in complex tissues. Interactive 3D ST volumes can be viewed at https://vortex-demo.github.io/

### Can Score-Based Generative Modeling Effectively Handle Medical Image Classification? 
[[arxiv](https://arxiv.org/abs/2502.17727)] [[cool](https://papers.cool/arxiv/2502.17727)] [[pdf](https://arxiv.org/pdf/2502.17727)]
> **Authors**: Sushmita Sarker,Prithul Sarker,George Bebis,Alireza Tavakkoli
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Accepted at the International Symposium on Biomedical Imaging (ISBI) 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: The remarkable success of deep learning in recent years has prompted applications in medical image classification and diagnosis tasks. While classification models have demonstrated robustness in classifying simpler datasets like MNIST or natural images such as ImageNet, this resilience is not consistently observed in complex medical image datasets where data is more scarce and lacks diversity. Moreover, previous findings on natural image datasets have indicated a potential trade-off between data likelihood and classification accuracy. In this study, we explore the use of score-based generative models as classifiers for medical images, specifically mammographic images. Our findings suggest that our proposed generative classifier model not only achieves superior classification results on CBIS-DDSM, INbreast and Vin-Dr Mammo datasets, but also introduces a novel approach to image classification in a broader context. Our code is publicly available at https://github.com/sushmitasarker/sgc_for_medical_image_classification

### Sketch-1-to-3: One Single Sketch to 3D Detailed Face Reconstruction 
[[arxiv](https://arxiv.org/abs/2502.17852)] [[cool](https://papers.cool/arxiv/2502.17852)] [[pdf](https://arxiv.org/pdf/2502.17852)]
> **Authors**: Liting Wen,Zimo Yang,Xianlin Zhang,Chi Ding,Yue Zhang,Mingdao Wang,Xueming Li
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: 3D face reconstruction from a single sketch is a critical yet underexplored task with significant practical applications. The primary challenges stem from the substantial modality gap between 2D sketches and 3D facial structures, including: (1) accurately extracting facial keypoints from 2D sketches; (2) preserving diverse facial expressions and fine-grained texture details; and (3) training a high-performing model with limited data. In this paper, we propose Sketch-1-to-3, a novel framework for realistic 3D face reconstruction from a single sketch, to address these challenges. Specifically, we first introduce the Geometric Contour and Texture Detail (GCTD) module, which enhances the extraction of geometric contours and texture details from facial sketches. Additionally, we design a deep learning architecture with a domain adaptation module and a tailored loss function to align sketches with the 3D facial space, enabling high-fidelity expression and texture reconstruction. To facilitate evaluation and further research, we construct SketchFaces, a real hand-drawn facial sketch dataset, and Syn-SketchFaces, a synthetic facial sketch dataset. Extensive experiments demonstrate that Sketch-1-to-3 achieves state-of-the-art performance in sketch-based 3D face reconstruction.

### Automatic Vehicle Detection using DETR: A Transformer-Based Approach for Navigating Treacherous Roads 
[[arxiv](https://arxiv.org/abs/2502.17843)] [[cool](https://papers.cool/arxiv/2502.17843)] [[pdf](https://arxiv.org/pdf/2502.17843)]
> **Authors**: Istiaq Ahmed Fahad,Abdullah Ibne Hanif Arean,Nazmus Sakib Ahmed,Mahmudul Hasan
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Automatic Vehicle Detection (AVD) in diverse driving environments presents unique challenges due to varying lighting conditions, road types, and vehicle types. Traditional methods, such as YOLO and Faster R-CNN, often struggle to cope with these complexities. As computer vision evolves, combining Convolutional Neural Networks (CNNs) with Transformer-based approaches offers promising opportunities for improving detection accuracy and efficiency. This study is the first to experiment with Detection Transformer (DETR) for automatic vehicle detection in complex and varied settings. We employ a Collaborative Hybrid Assignments Training scheme, Co-DETR, to enhance feature learning and attention mechanisms in DETR. By leveraging versatile label assignment strategies and introducing multiple parallel auxiliary heads, we provide more effective supervision during training and extract positive coordinates to boost training efficiency. Through extensive experiments on DETR variants and YOLO models, conducted using the BadODD dataset, we demonstrate the advantages of our approach. Our method achieves superior results, and improved accuracy in diverse conditions, making it practical for real-world deployment. This work significantly advances autonomous navigation technology and opens new research avenues in object detection for autonomous vehicles. By integrating the strengths of CNNs and Transformers, we highlight the potential of DETR for robust and efficient vehicle detection in challenging driving environments.

### Contrastive Visual Data Augmentation 
[[arxiv](https://arxiv.org/abs/2502.17709)] [[cool](https://papers.cool/arxiv/2502.17709)] [[pdf](https://arxiv.org/pdf/2502.17709)]
> **Authors**: Yu Zhou,Bingxuan Li,Mohan Tang,Xiaomeng Jin,Te-Lin Wu,Kuan-Hao Huang,Heng Ji,Kai-Wei Chang,Nanyun Peng
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习,多媒体
- **Abstract**: Large multimodal models (LMMs) often struggle to recognize novel concepts, as they rely on pre-trained knowledge and have limited ability to capture subtle visual details. Domain-specific knowledge gaps in training also make them prone to confusing visually similar, commonly misrepresented, or low-resource concepts. To help LMMs better align nuanced visual features with language, improving their ability to recognize and reason about novel or rare concepts, we propose a Contrastive visual Data Augmentation (CoDA) strategy. CoDA extracts key contrastive textual and visual features of target concepts against the known concepts they are misrecognized as, and then uses multimodal generative models to produce targeted synthetic data. Automatic filtering of extracted features and augmented images is implemented to guarantee their quality, as verified by human annotators. We show the effectiveness and efficiency of CoDA on low-resource concept and diverse scene recognition datasets including INaturalist and SUN. We additionally collect NovelSpecies, a benchmark dataset consisting of newly discovered animal species that are guaranteed to be unseen by LMMs. LLaVA-1.6 1-shot updating results on these three datasets show CoDA significantly improves SOTA visual data augmentation strategies by 12.3% (NovelSpecies), 5.1% (SUN), and 6.0% (iNat) absolute gains in accuracy.

### IBURD: Image Blending for Underwater Robotic Detection 
[[arxiv](https://arxiv.org/abs/2502.17706)] [[cool](https://papers.cool/arxiv/2502.17706)] [[pdf](https://arxiv.org/pdf/2502.17706)]
> **Authors**: Jungseok Hong,Sakshi Singh,Junaed Sattar
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: We present an image blending pipeline, \textit{IBURD}, that creates realistic synthetic images to assist in the training of deep detectors for use on underwater autonomous vehicles (AUVs) for marine debris detection tasks. Specifically, IBURD generates both images of underwater debris and their pixel-level annotations, using source images of debris objects, their annotations, and target background images of marine environments. With Poisson editing and style transfer techniques, IBURD is even able to robustly blend transparent objects into arbitrary backgrounds and automatically adjust the style of blended images using the blurriness metric of target background images. These generated images of marine debris in actual underwater backgrounds address the data scarcity and data variety problems faced by deep-learned vision algorithms in challenging underwater conditions, and can enable the use of AUVs for environmental cleanup missions. Both quantitative and robotic evaluations of IBURD demonstrate the efficacy of the proposed approach for robotic detection of marine debris.

### METAL: A Multi-Agent Framework for Chart Generation with Test-Time Scaling 
[[arxiv](https://arxiv.org/abs/2502.17651)] [[cool](https://papers.cool/arxiv/2502.17651)] [[pdf](https://arxiv.org/pdf/2502.17651)]
> **Authors**: Bingxuan Li,Yiwei Wang,Jiuxiang Gu,Kai-Wei Chang,Nanyun Peng
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **Abstract**: Chart generation aims to generate code to produce charts satisfying the desired visual properties, e.g., texts, layout, color, and type. It has great potential to empower the automatic professional report generation in financial analysis, research presentation, education, and healthcare. In this work, we build a vision-language model (VLM) based multi-agent framework for effective automatic chart generation. Generating high-quality charts requires both strong visual design skills and precise coding capabilities that embed the desired visual properties into code. Such a complex multi-modal reasoning process is difficult for direct prompting of VLMs. To resolve these challenges, we propose METAL, a multi-agent framework that decomposes the task of chart generation into the iterative collaboration among specialized agents. METAL achieves 5.2% improvement in accuracy over the current best result in the chart generation task. The METAL framework exhibits the phenomenon of test-time scaling: its performance increases monotonically as the logarithmic computational budget grows from 512 to 8192 tokens. In addition, we find that separating different modalities during the critique process of METAL boosts the self-correction capability of VLMs in the multimodal context.

### CalibRefine: Deep Learning-Based Online Automatic Targetless LiDAR-Camera Calibration with Iterative and Attention-Driven Post-Refinement 
[[arxiv](https://arxiv.org/abs/2502.17648)] [[cool](https://papers.cool/arxiv/2502.17648)] [[pdf](https://arxiv.org/pdf/2502.17648)]
> **Authors**: Lei Cheng,Lihao Guo,Tianya Zhang,Tam Bang,Austin Harris,Mustafa Hajij,Mina Sartipi,Siyang Cao
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Submitted to Transportation Research Part C: Emerging Technologies
- **标题**: None
- **领域**: 计算机视觉和模式识别,系统与控制
- **Abstract**: Accurate multi-sensor calibration is essential for deploying robust perception systems in applications such as autonomous driving, robotics, and intelligent transportation. Existing LiDAR-camera calibration methods often rely on manually placed targets, preliminary parameter estimates, or intensive data preprocessing, limiting their scalability and adaptability in real-world settings. In this work, we propose a fully automatic, targetless, and online calibration framework, CalibRefine, which directly processes raw LiDAR point clouds and camera images. Our approach is divided into four stages: (1) a Common Feature Discriminator that trains on automatically detected objects--using relative positions, appearance embeddings, and semantic classes--to generate reliable LiDAR-camera correspondences, (2) a coarse homography-based calibration, (3) an iterative refinement to incrementally improve alignment as additional data frames become available, and (4) an attention-based refinement that addresses non-planar distortions by leveraging a Vision Transformer and cross-attention mechanisms. Through extensive experiments on two urban traffic datasets, we show that CalibRefine delivers high-precision calibration results with minimal human involvement, outperforming state-of-the-art targetless methods and remaining competitive with, or surpassing, manually tuned baselines. Our findings highlight how robust object-level feature matching, together with iterative and self-supervised attention-based adjustments, enables consistent sensor fusion in complex, real-world conditions without requiring ground-truth calibration matrices or elaborate data preprocessing.

### A Priori Generalizability Estimate for a CNN 
[[arxiv](https://arxiv.org/abs/2502.17622)] [[cool](https://papers.cool/arxiv/2502.17622)] [[pdf](https://arxiv.org/pdf/2502.17622)]
> **Authors**: Cito Balsells,Beatrice Riviere,David Fuentes
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习,图像和视频处理
- **Abstract**: We formulate truncated singular value decompositions of entire convolutional neural networks. We demonstrate the computed left and right singular vectors are useful in identifying which images the convolutional neural network is likely to perform poorly on. To create this diagnostic tool, we define two metrics: the Right Projection Ratio and the Left Projection Ratio. The Right (Left) Projection Ratio evaluates the fidelity of the projection of an image (label) onto the computed right (left) singular vectors. We observe that both ratios are able to identify the presence of class imbalance for an image classification problem. Additionally, the Right Projection Ratio, which only requires unlabeled data, is found to be correlated to the model's performance when applied to image segmentation. This suggests the Right Projection Ratio could be a useful metric to estimate how likely the model is to perform well on a sample.

## 计算机与社会(cs.CY:Computers and Society)

### Requirements for Quality Assurance of AI Models for Early Detection of Lung Cancer 
[[arxiv](https://arxiv.org/abs/2502.17639)] [[cool](https://papers.cool/arxiv/2502.17639)] [[pdf](https://arxiv.org/pdf/2502.17639)]
> **Authors**: Horst K. Hahn,Matthias S. May,Volker Dicken,Michael Walz,Rainer Eßeling,Bianca Lassen-Schmidt,Robert Rischen,Jens Vogel-Claussen,Konstantin Nikolaou,Jörg Barkhausen
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 12 pages incl. 2 figures, 2 charts, and references, summary in English (page 2), article in German (original title: Anforderungen an die Qualitätssicherung von KI-Modellen für die Lungenkrebs-Früherkennung)
- **标题**: None
- **领域**: 计算机与社会,人工智能,表现
- **Abstract**: Lung cancer is the second most common cancer and the leading cause of cancer-related deaths worldwide. Survival largely depends on tumor stage at diagnosis, and early detection with low-dose CT can significantly reduce mortality in high-risk patients. AI can improve the detection, measurement, and characterization of pulmonary nodules while reducing assessment time. However, the training data, functionality, and performance of available AI systems vary considerably, complicating software selection and regulatory evaluation. Manufacturers must specify intended use and provide test statistics, but they can choose their training and test data, limiting standardization and comparability. Under the EU AI Act, consistent quality assurance is required for AI-based nodule detection, measurement, and characterization. This position paper proposes systematic quality assurance grounded in a validated reference dataset, including real screening cases plus phantom data to verify volume and growth rate measurements. Regular updates shall reflect demographic shifts and technological advances, ensuring ongoing relevance. Consequently, ongoing AI quality assurance is vital. Regulatory challenges are also adressed. While the MDR and the EU AI Act set baseline requirements, they do not adequately address self-learning algorithms or their updates. A standardized, transparent quality assessment - based on sensitivity, specificity, and volumetric accuracy - enables an objective evaluation of each AI solution's strengths and weaknesses. Establishing clear testing criteria and systematically using updated reference data lay the groundwork for comparable performance metrics, informing tenders, guidelines, and recommendations.

### Towards Robust Legal Reasoning: Harnessing Logical LLMs in Law 
[[arxiv](https://arxiv.org/abs/2502.17638)] [[cool](https://papers.cool/arxiv/2502.17638)] [[pdf](https://arxiv.org/pdf/2502.17638)]
> **Authors**: Manuj Kant,Sareh Nabi,Manav Kant,Roland Scharrer,Megan Ma,Marzieh Nabi
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: Legal services rely heavily on text processing. While large language models (LLMs) show promise, their application in legal contexts demands higher accuracy, repeatability, and transparency. Logic programs, by encoding legal concepts as structured rules and facts, offer reliable automation, but require sophisticated text extraction. We propose a neuro-symbolic approach that integrates LLMs' natural language understanding with logic-based reasoning to address these limitations. As a legal document case study, we applied neuro-symbolic AI to coverage-related queries in insurance contracts using both closed and open-source LLMs. While LLMs have improved in legal reasoning, they still lack the accuracy and consistency required for complex contract analysis. In our analysis, we tested three methodologies to evaluate whether a specific claim is covered under a contract: a vanilla LLM, an unguided approach that leverages LLMs to encode both the contract and the claim, and a guided approach that uses a framework for the LLM to encode the contract. We demonstrated the promising capabilities of LLM + Logic in the guided approach.

## 人机交互(cs.HC:Human-Computer Interaction)

### On the usability of generative AI: Human generative AI 
[[arxiv](https://arxiv.org/abs/2502.17714)] [[cool](https://papers.cool/arxiv/2502.17714)] [[pdf](https://arxiv.org/pdf/2502.17714)]
> **Authors**: Anna Ravera,Cristina Gena
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 人机交互,人工智能
- **Abstract**: Generative AI systems are transforming content creation, but their usability remains a key challenge. This paper examines usability factors such as user experience, transparency, control, and cognitive load. Common challenges include unpredictability and difficulties in fine-tuning outputs. We review evaluation metrics like efficiency, learnability, and satisfaction, highlighting best practices from various domains. Improving interpretability, intuitive interfaces, and user feedback can enhance usability, making generative AI more accessible and effective.

### Wearable Meets LLM for Stress Management: A Duoethnographic Study Integrating Wearable-Triggered Stressors and LLM Chatbots for Personalized Interventions 
[[arxiv](https://arxiv.org/abs/2502.17650)] [[cool](https://papers.cool/arxiv/2502.17650)] [[pdf](https://arxiv.org/pdf/2502.17650)]
> **Authors**: Sameer Neupane,Poorvesh Dongre,Denis Gracanin,Santosh Kumar
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: In CHI '25 Proceedings of the CHI Conference on Human Factors in Computing Systems Yokohama, Japan
- **标题**: None
- **领域**: 人机交互,人工智能
- **Abstract**: We use a duoethnographic approach to study how wearable-integrated LLM chatbots can assist with personalized stress management, addressing the growing need for immediacy and tailored interventions. Two researchers interacted with custom chatbots over 22 days, responding to wearable-detected physiological prompts, recording stressor phrases, and using them to seek tailored interventions from their LLM-powered chatbots. They recorded their experiences in autoethnographic diaries and analyzed them during weekly discussions, focusing on the relevance, clarity, and impact of chatbot-generated interventions. Results showed that even though most events triggered by the wearable were meaningful, only one in five warranted an intervention. It also showed that interventions tailored with brief event descriptions were more effective than generic ones. By examining the intersection of wearables and LLM, this research contributes to developing more effective, user-centric mental health tools for real-time stress relief and behavior change.

## 信息检索(cs.IR:Information Retrieval)

### Tip of the Tongue Query Elicitation for Simulated Evaluation 
[[arxiv](https://arxiv.org/abs/2502.17776)] [[cool](https://papers.cool/arxiv/2502.17776)] [[pdf](https://arxiv.org/pdf/2502.17776)]
> **Authors**: Yifan He,To Eun Kim,Fernando Diaz,Jaime Arguello,Bhaskar Mitra
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,计算语言学,人机交互
- **Abstract**: Tip-of-the-tongue (TOT) search occurs when a user struggles to recall a specific identifier, such as a document title. While common, existing search systems often fail to effectively support TOT scenarios. Research on TOT retrieval is further constrained by the challenge of collecting queries, as current approaches rely heavily on community question-answering (CQA) websites, leading to labor-intensive evaluation and domain bias. To overcome these limitations, we introduce two methods for eliciting TOT queries - leveraging large language models (LLMs) and human participants - to facilitate simulated evaluations of TOT retrieval systems. Our LLM-based TOT user simulator generates synthetic TOT queries at scale, achieving high correlations with how CQA-based TOT queries rank TOT retrieval systems when tested in the Movie domain. Additionally, these synthetic queries exhibit high linguistic similarity to CQA-derived queries. For human-elicited queries, we developed an interface that uses visual stimuli to place participants in a TOT state, enabling the collection of natural queries. In the Movie domain, system rank correlation and linguistic similarity analyses confirm that human-elicited queries are both effective and closely resemble CQA-based queries. These approaches reduce reliance on CQA-based data collection while expanding coverage to underrepresented domains, such as Landmark and Person. LLM-elicited queries for the Movie, Landmark, and Person domains have been released as test queries in the TREC 2024 TOT track, with human-elicited queries scheduled for inclusion in the TREC 2025 TOT track. Additionally, we provide source code for synthetic query generation and the human query collection interface, along with curated visual stimuli used for eliciting TOT queries.

## 机器学习(cs.LG:Machine Learning)

### MM-PoisonRAG: Disrupting Multimodal RAG with Local and Global Poisoning Attacks 
[[arxiv](https://arxiv.org/abs/2502.17832)] [[cool](https://papers.cool/arxiv/2502.17832)] [[pdf](https://arxiv.org/pdf/2502.17832)]
> **Authors**: Hyeonjeong Ha,Qiusi Zhan,Jeonghwan Kim,Dimitrios Bralios,Saikrishna Sanniboina,Nanyun Peng,Kai-wei Chang,Daniel Kang,Heng Ji
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Code is available at https://github.com/HyeonjeongHa/MM-PoisonRAG
- **标题**: None
- **领域**: 机器学习,人工智能,密码学和安全,计算机视觉和模式识别
- **Abstract**: Multimodal large language models (MLLMs) equipped with Retrieval Augmented Generation (RAG) leverage both their rich parametric knowledge and the dynamic, external knowledge to excel in tasks such as Question Answering. While RAG enhances MLLMs by grounding responses in query-relevant external knowledge, this reliance poses a critical yet underexplored safety risk: knowledge poisoning attacks, where misinformation or irrelevant knowledge is intentionally injected into external knowledge bases to manipulate model outputs to be incorrect and even harmful. To expose such vulnerabilities in multimodal RAG, we propose MM-PoisonRAG, a novel knowledge poisoning attack framework with two attack strategies: Localized Poisoning Attack (LPA), which injects query-specific misinformation in both text and images for targeted manipulation, and Globalized Poisoning Attack (GPA) to provide false guidance during MLLM generation to elicit nonsensical responses across all queries. We evaluate our attacks across multiple tasks, models, and access settings, demonstrating that LPA successfully manipulates the MLLM to generate attacker-controlled answers, with a success rate of up to 56% on MultiModalQA. Moreover, GPA completely disrupts model generation to 0% accuracy with just a single irrelevant knowledge injection. Our results highlight the urgent need for robust defenses against knowledge poisoning to safeguard multimodal RAG frameworks.

### A General Framework to Enhance Fine-tuning-based LLM Unlearning 
[[arxiv](https://arxiv.org/abs/2502.17823)] [[cool](https://papers.cool/arxiv/2502.17823)] [[pdf](https://arxiv.org/pdf/2502.17823)]
> **Authors**: Jie Ren,Zhenwei Dai,Xianfeng Tang,Hui Liu,Jingying Zeng,Zhen Li,Rahul Goutam,Suhang Wang,Yue Xing,Qi He,Hui Liu
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Unlearning has been proposed to remove copyrighted and privacy-sensitive data from Large Language Models (LLMs). Existing approaches primarily rely on fine-tuning-based methods, which can be categorized into gradient ascent-based (GA-based) and suppression-based methods. However, they often degrade model utility (the ability to respond to normal prompts). In this work, we aim to develop a general framework that enhances the utility of fine-tuning-based unlearning methods. To achieve this goal, we first investigate the common property between GA-based and suppression-based methods. We unveil that GA-based methods unlearn by distinguishing the target data (i.e., the data to be removed) and suppressing related generations, which is essentially the same strategy employed by suppression-based methods. Inspired by this finding, we introduce Gated Representation UNlearning (GRUN) which has two components: a soft gate function for distinguishing target data and a suppression module using Representation Fine-tuning (ReFT) to adjust representations rather than model parameters. Experiments show that GRUN significantly improves the unlearning and utility. Meanwhile, it is general for fine-tuning-based methods, efficient and promising for sequential unlearning.

### PVBF: A Framework for Mitigating Parameter Variation Imbalance in Online Continual Learning 
[[arxiv](https://arxiv.org/abs/2502.17794)] [[cool](https://papers.cool/arxiv/2502.17794)] [[pdf](https://arxiv.org/pdf/2502.17794)]
> **Authors**: Zelin Tao,Hao Deng,Mingqing Liu,Lijun Zhang,Shengjie Zhao
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 27 pages, 11 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Online continual learning (OCL), which enables AI systems to adaptively learn from non-stationary data streams, is commonly achieved using experience replay (ER)-based methods that retain knowledge by replaying stored past during training. However, these methods face challenges of prediction bias, stemming from deviations in parameter update directions during task transitions. This paper identifies parameter variation imbalance as a critical factor contributing to prediction bias in ER-based OCL. Specifically, using the proposed parameter variation evaluation method, we highlight two types of imbalance: correlation-induced imbalance, where certain parameters are disproportionately updated across tasks, and layer-wise imbalance, where output layer parameters update faster than those in preceding layers. To mitigate the above imbalances, we propose the Parameter Variation Balancing Framework (PVBF), which incorporates: 1) a novel method to compute parameter correlations with previous tasks based on parameter variations, 2) an encourage-and-consolidate (E&C) method utilizing parameter correlations to perform gradient adjustments across all parameters during training, 3) a dual-layer copy weights with reinit (D-CWR) strategy to slowly update output layer parameters for frequently occuring sample categories. Experiments on short and long task sequences demonstrate that PVBF significantly reduces prediction bias and improves OCL performance, achieving up to 47\% higher accuracy compared to existing ER-based methods.

### On-device edge learning for IoT data streams: a survey 
[[arxiv](https://arxiv.org/abs/2502.17788)] [[cool](https://papers.cool/arxiv/2502.17788)] [[pdf](https://arxiv.org/pdf/2502.17788)]
> **Authors**: Afonso Lourenço,João Rodrigo,João Gama,Goreti Marreiros
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: This literature review explores continual learning methods for on-device training in the context of neural networks (NNs) and decision trees (DTs) for classification tasks on smart environments. We highlight key constraints, such as data architecture (batch vs. stream) and network capacity (cloud vs. edge), which impact TinyML algorithm design, due to the uncontrolled natural arrival of data streams. The survey details the challenges of deploying deep learners on resource-constrained edge devices, including catastrophic forgetting, data inefficiency, and the difficulty of handling IoT tabular data in open-world settings. While decision trees are more memory-efficient for on-device training, they are limited in expressiveness, requiring dynamic adaptations, like pruning and meta-learning, to handle complex patterns and concept drifts. We emphasize the importance of multi-criteria performance evaluation tailored to edge applications, which assess both output-based and internal representation metrics. The key challenge lies in integrating these building blocks into autonomous online systems, taking into account stability-plasticity trade-offs, forward-backward transfer, and model convergence.

### MuCoS: Efficient Drug-Target Prediction through Multi-Context-Aware Sampling 
[[arxiv](https://arxiv.org/abs/2502.17784)] [[cool](https://papers.cool/arxiv/2502.17784)] [[pdf](https://arxiv.org/pdf/2502.17784)]
> **Authors**: Haji Gul,Abdul Gani Haji Naim,Ajaz A. Bhat
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Drug-target interactions are critical for understanding biological processes and advancing drug discovery. However, traditional methods such as ComplEx-SE, TransE, and DistMult struggle with unseen relationships and negative triplets, which limits their effectiveness in drug-target prediction. To address these challenges, we propose Multi-Context-Aware Sampling (MuCoS), an efficient and positively accurate method for drug-target prediction. MuCoS reduces computational complexity by prioritizing neighbors of higher density to capture informative structural patterns. These optimized neighborhood representations are integrated with BERT, enabling contextualized embeddings for accurate prediction of missing relationships or tail entities. MuCoS avoids the need for negative triplet sampling, reducing computation while improving performance over unseen entities and relations. Experiments on the KEGG50k biomedical dataset show that MuCoS improved over existing models by 13\% on MRR, 7\% on Hits@1, 4\% on Hits@3, and 18\% on Hits@10 for the general relationship, and by 6\% on MRR, 1\% on Hits@1, 3\% on Hits@3, and 12\% on Hits@10 for prediction of drug-target relationship.

### Adaptive Nesterov Accelerated Distributional Deep Hedging for Efficient Volatility Risk Management 
[[arxiv](https://arxiv.org/abs/2502.17777)] [[cool](https://papers.cool/arxiv/2502.17777)] [[pdf](https://arxiv.org/pdf/2502.17777)]
> **Authors**: Lei Zhao,Lin Cai,Wu-Sheng Lu
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算金融
- **Abstract**: In the field of financial derivatives trading, managing volatility risk is crucial for protecting investment portfolios from market changes. Traditional Vega hedging strategies, which often rely on basic and rule-based models, are hard to adapt well to rapidly changing market conditions. We introduce a new framework for dynamic Vega hedging, the Adaptive Nesterov Accelerated Distributional Deep Hedging (ANADDH), which combines distributional reinforcement learning with a tailored design based on adaptive Nesterov acceleration. This approach improves the learning process in complex financial environments by modeling the hedging efficiency distribution, providing a more accurate and responsive hedging strategy. The design of adaptive Nesterov acceleration refines gradient momentum adjustments, significantly enhancing the stability and speed of convergence of the model. Through empirical analysis and comparisons, our method demonstrates substantial performance gains over existing hedging techniques. Our results confirm that this innovative combination of distributional reinforcement learning with the proposed optimization techniques improves financial risk management and highlights the practical benefits of implementing advanced neural network architectures in the finance sector.

### An Improved Privacy and Utility Analysis of Differentially Private SGD with Bounded Domain and Smooth Losses 
[[arxiv](https://arxiv.org/abs/2502.17772)] [[cool](https://papers.cool/arxiv/2502.17772)] [[pdf](https://arxiv.org/pdf/2502.17772)]
> **Authors**: Hao Liang,Wanrong Zhang,Xinlei He,Kaishun He,Hong Xing
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 18 pages, 2 figures, submitted for possible publication
- **标题**: None
- **领域**: 机器学习,密码学和安全,机器学习
- **Abstract**: Differentially Private Stochastic Gradient Descent (DPSGD) is widely used to protect sensitive data during the training of machine learning models, but its privacy guarantees often come at the cost of model performance, largely due to the inherent challenge of accurately quantifying privacy loss. While recent efforts have strengthened privacy guarantees by focusing solely on the final output and bounded domain cases, they still impose restrictive assumptions, such as convexity and other parameter limitations, and often lack a thorough analysis of utility. In this paper, we provide rigorous privacy and utility characterization for DPSGD for smooth loss functions in both bounded and unbounded domains. We track the privacy loss over multiple iterations by exploiting the noisy smooth-reduction property and establish the utility analysis by leveraging the projection's non-expansiveness and clipped SGD properties. In particular, we show that for DPSGD with a bounded domain, (i) the privacy loss can still converge without the convexity assumption, and (ii) a smaller bounded diameter can improve both privacy and utility simultaneously under certain conditions. Numerical results validate our results.

### Sample Selection via Contrastive Fragmentation for Noisy Label Regression 
[[arxiv](https://arxiv.org/abs/2502.17771)] [[cool](https://papers.cool/arxiv/2502.17771)] [[pdf](https://arxiv.org/pdf/2502.17771)]
> **Authors**: Chris Dongjoo Kim,Sangwoo Moon,Jihwan Moon,Dongyeon Woo,Gunhee Kim
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: NeurIPS 2024
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: As with many other problems, real-world regression is plagued by the presence of noisy labels, an inevitable issue that demands our attention. Fortunately, much real-world data often exhibits an intrinsic property of continuously ordered correlations between labels and features, where data points with similar labels are also represented with closely related features. In response, we propose a novel approach named ConFrag, where we collectively model the regression data by transforming them into disjoint yet contrasting fragmentation pairs. This enables the training of more distinctive representations, enhancing the ability to select clean samples. Our ConFrag framework leverages a mixture of neighboring fragments to discern noisy labels through neighborhood agreement among expert feature extractors. We extensively perform experiments on six newly curated benchmark datasets of diverse domains, including age prediction, price prediction, and music production year estimation. We also introduce a metric called Error Residual Ratio (ERR) to better account for varying degrees of label noise. Our approach consistently outperforms fourteen state-of-the-art baselines, being robust against symmetric and random Gaussian label noise.

### DeepSeek vs. ChatGPT: A Comparative Study for Scientific Computing and Scientific Machine Learning Tasks 
[[arxiv](https://arxiv.org/abs/2502.17764)] [[cool](https://papers.cool/arxiv/2502.17764)] [[pdf](https://arxiv.org/pdf/2502.17764)]
> **Authors**: Qile Jiang,Zhiwei Gao,George Em Karniadakis
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Large Language Models (LLMs) have emerged as powerful tools for tackling a wide range of problems, including those in scientific computing, particularly in solving partial differential equations (PDEs). However, different models exhibit distinct strengths and preferences, resulting in varying levels of performance. In this paper, we compare the capabilities of the most advanced LLMs--ChatGPT and DeepSeek--along with their reasoning-optimized versions in addressing computational challenges. Specifically, we evaluate their proficiency in solving traditional numerical problems in scientific computing as well as leveraging scientific machine learning techniques for PDE-based problems. We designed all our experiments so that a non-trivial decision is required, e.g. defining the proper space of input functions for neural operator learning. Our findings reveal that the latest model, ChatGPT o3-mini-high, usually delivers the most accurate results while also responding significantly faster than its reasoning counterpart, DeepSeek R1. This enhanced speed and accuracy make ChatGPT o3-mini-high a more practical and efficient choice for diverse computational tasks at this juncture.

### Applications of deep reinforcement learning to urban transit network design 
[[arxiv](https://arxiv.org/abs/2502.17758)] [[cool](https://papers.cool/arxiv/2502.17758)] [[pdf](https://arxiv.org/pdf/2502.17758)]
> **Authors**: Andrew Holliday
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: This is a copy of my PhD thesis, which was successfully defended at McGill University in December of 2024. arXiv admin note: text overlap with arXiv:2404.05894
- **标题**: None
- **领域**: 机器学习
- **Abstract**: This thesis concerns the use of reinforcement learning to train neural networks to aid in the design of public transit networks. The Transit Network Design Problem (TNDP) is an optimization problem of considerable practical importance. Given a city with an existing road network and travel demands, the goal is to find a set of transit routes - each of which is a path through the graph - that collectively satisfy all demands, while minimizing a cost function that may depend both on passenger satisfaction and operating costs. The existing literature on this problem mainly considers metaheuristic optimization algorithms, such as genetic algorithms and ant-colony optimization. By contrast, we begin by taking a reinforcement learning approach, formulating the construction of a set of transit routes as a Markov Decision Process (MDP) and training a neural net policy to act as the agent in this MDP. We then show that, beyond using this policy to plan a transit network directly, it can be combined with existing metaheuristic algorithms, both to initialize the solution and to suggest promising moves at each step of a search through solution space. We find that such hybrid algorithms, which use a neural policy trained via reinforcement learning as a core component within a classical metaheuristic framework, can plan transit networks that are superior to those planned by either the neural policy or the metaheuristic algorithm. We demonstrate the utility of our approach by using it to redesign the transit network for the city of Laval, Quebec, and show that in simulation, the resulting transit network provides better service at lower cost than the existing transit network.

### Robust and Efficient Deep Hedging via Linearized Objective Neural Network 
[[arxiv](https://arxiv.org/abs/2502.17757)] [[cool](https://papers.cool/arxiv/2502.17757)] [[pdf](https://arxiv.org/pdf/2502.17757)]
> **Authors**: Lei Zhao,Lin Cai
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,风险管理
- **Abstract**: Deep hedging represents a cutting-edge approach to risk management for financial derivatives by leveraging the power of deep learning. However, existing methods often face challenges related to computational inefficiency, sensitivity to noisy data, and optimization complexity, limiting their practical applicability in dynamic and volatile markets. To address these limitations, we propose Deep Hedging with Linearized-objective Neural Network (DHLNN), a robust and generalizable framework that enhances the training procedure of deep learning models. By integrating a periodic fixed-gradient optimization method with linearized training dynamics, DHLNN stabilizes the training process, accelerates convergence, and improves robustness to noisy financial data. The framework incorporates trajectory-wide optimization and Black-Scholes Delta anchoring, ensuring alignment with established financial theory while maintaining flexibility to adapt to real-world market conditions. Extensive experiments on synthetic and real market data validate the effectiveness of DHLNN, demonstrating its ability to achieve faster convergence, improved stability, and superior hedging performance across diverse market scenarios.

### Graded Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.17751)] [[cool](https://papers.cool/arxiv/2502.17751)] [[pdf](https://arxiv.org/pdf/2502.17751)]
> **Authors**: Tony Shaska
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: :16W50; 13A02;ACM Class:I.2; I.2.6
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: This paper presents a novel framework for graded neural networks (GNNs) built over graded vector spaces $\V_\w^n$, extending classical neural architectures by incorporating algebraic grading. Leveraging a coordinate-wise grading structure with scalar action $λ\star \x = (λ^{q_i} x_i)$, defined by a tuple $\w = (q_0, \ldots, q_{n-1})$, we introduce graded neurons, layers, activation functions, and loss functions that adapt to feature significance. Theoretical properties of graded spaces are established, followed by a comprehensive GNN design, addressing computational challenges like numerical stability and gradient scaling. Potential applications span machine learning and photonic systems, exemplified by high-speed laser-based implementations. This work offers a foundational step toward graded computation, unifying mathematical rigor with practical potential, with avenues for future empirical and hardware exploration.

### FinP: Fairness-in-Privacy in Federated Learning by Addressing Disparities in Privacy Risk 
[[arxiv](https://arxiv.org/abs/2502.17748)] [[cool](https://papers.cool/arxiv/2502.17748)] [[pdf](https://arxiv.org/pdf/2502.17748)]
> **Authors**: Tianyu Zhao,Mahmoud Srewa,Salma Elmalaki
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,密码学和安全
- **Abstract**: Ensuring fairness in machine learning, particularly in human-centric applications, extends beyond algorithmic bias to encompass fairness in privacy, specifically the equitable distribution of privacy risk. This is critical in federated learning (FL), where decentralized data necessitates balanced privacy preservation across clients. We introduce FinP, a framework designed to achieve fairness in privacy by mitigating disproportionate exposure to source inference attacks (SIA). FinP employs a dual approach: (1) server-side adaptive aggregation to address unfairness in client contributions in global model, and (2) client-side regularization to reduce client vulnerability. This comprehensive strategy targets both the symptoms and root causes of privacy unfairness. Evaluated on the Human Activity Recognition (HAR) and CIFAR-10 datasets, FinP demonstrates ~20% improvement in fairness in privacy on HAR with minimal impact on model utility, and effectively mitigates SIA risks on CIFAR-10, showcasing its ability to provide fairness in privacy in FL systems without compromising performance.

### Phoeni6: a Systematic Approach for Evaluating the Energy Consumption of Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.17734)] [[cool](https://papers.cool/arxiv/2502.17734)] [[pdf](https://arxiv.org/pdf/2502.17734)]
> **Authors**: Antônio Oliveira-Filho,Wellington Silva-de-Souza,Carlos Alberto Valderrama Sakuyama,Samuel Xavier-de-Souza
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: The paper consists of 24 pages and 25 figures. It is currently under review at the journal Sustainable Computing: Informatics and Systems
- **标题**: None
- **领域**: 机器学习,软件工程
- **Abstract**: This paper presents Phoeni6, a systematic approach for assessing the energy consumption of neural networks while upholding the principles of fair comparison and reproducibility. Phoeni6 offers a comprehensive solution for managing energy-related data and configurations, ensuring portability, transparency, and coordination during evaluations. The methodology automates energy evaluations through containerized tools, robust database management, and versatile data models. In the first case study, the energy consumption of AlexNet and MobileNet was compared using raw and resized images. Results showed that MobileNet is up to 6.25% more energy-efficient for raw images and 2.32% for resized datasets, while maintaining competitive accuracy levels. In the second study, the impact of image file formats on energy consumption was evaluated. BMP images reduced energy usage by up to 30% compared to PNG, highlighting the influence of file formats on energy efficiency. These findings emphasize the importance of Phoeni6 in optimizing energy consumption for diverse neural network applications and establishing sustainable artificial intelligence practices.

### Aligning Compound AI Systems via System-level DPO 
[[arxiv](https://arxiv.org/abs/2502.17721)] [[cool](https://papers.cool/arxiv/2502.17721)] [[pdf](https://arxiv.org/pdf/2502.17721)]
> **Authors**: Xiangwen Wang,Yibo Jacky Zhang,Zhoujie Ding,Katherine Tsai,Sanmi Koyejo
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Accepted to workshops MARW and WMAC (Oral) at AAAI25
- **标题**: None
- **领域**: 机器学习,人工智能,多代理系统
- **Abstract**: Compound AI systems, comprising multiple interacting components such as LLM agents and external tools, demonstrate state-of-the-art results across diverse tasks. It is hence crucial to align components within the system to produce consistent results that match human expectations. However, conventional alignment methods, such as Direct Preference Optimization (DPO), are not directly applicable to compound AI systems. These challenges include the non-differentiable interactions between components, making end-to-end gradient optimization infeasible. Additionally, system-level preferences cannot be directly translated into component-level preferences, further complicating alignment. We address the issues by formulating compound AI systems as Directed Acyclic Graphs (DAGs), capturing the connections between agents and the data generation processes. We propose a system-level DPO (SysDPO) to jointly align compound systems by adapting the DPO to operate on these DAGs. We study the joint alignment of an LLM and a diffusion model to demonstrate the effectiveness of our approach. Our exploration provides insights into the alignment of compound AI systems and lays a foundation for future advancements.

### Armada: Memory-Efficient Distributed Training of Large-Scale Graph Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.17846)] [[cool](https://papers.cool/arxiv/2502.17846)] [[pdf](https://arxiv.org/pdf/2502.17846)]
> **Authors**: Roger Waleffe,Devesh Sarda,Jason Mohoney,Emmanouil-Vasileios Vlatakis-Gkaragkounis,Theodoros Rekatsinas,Shivaram Venkataraman
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,分布式、并行和集群计算
- **Abstract**: We study distributed training of Graph Neural Networks (GNNs) on billion-scale graphs that are partitioned across machines. Efficient training in this setting relies on min-edge-cut partitioning algorithms, which minimize cross-machine communication due to GNN neighborhood sampling. Yet, min-edge-cut partitioning over large graphs remains a challenge: State-of-the-art (SoTA) offline methods (e.g., METIS) are effective, but they require orders of magnitude more memory and runtime than GNN training itself, while computationally efficient algorithms (e.g., streaming greedy approaches) suffer from increased edge cuts. Thus, in this work we introduce Armada, a new end-to-end system for distributed GNN training whose key contribution is GREM, a novel min-edge-cut partitioning algorithm that can efficiently scale to large graphs. GREM builds on streaming greedy approaches with one key addition: prior vertex assignments are continuously refined during streaming, rather than frozen after an initial greedy selection. Our theoretical analysis and experimental results show that this refinement is critical to minimizing edge cuts and enables GREM to reach partition quality comparable to METIS but with 8-65x less memory and 8-46x faster. Given a partitioned graph, Armada leverages a new disaggregated architecture for distributed GNN training to further improve efficiency; we find that on common cloud machines, even with zero communication, GNN neighborhood sampling and feature loading bottleneck training. Disaggregation allows Armada to independently allocate resources for these operations and ensure that expensive GPUs remain saturated with computation. We evaluate Armada against SoTA systems for distributed GNN training and find that the disaggregated architecture leads to runtime improvements up to 4.5x and cost reductions up to 3.1x.

### LeanKAN: A Parameter-Lean Kolmogorov-Arnold Network Layer with Improved Memory Efficiency and Convergence Behavior 
[[arxiv](https://arxiv.org/abs/2502.17844)] [[cool](https://papers.cool/arxiv/2502.17844)] [[pdf](https://arxiv.org/pdf/2502.17844)]
> **Authors**: Benjamin C. Koenig,Suyong Kim,Sili Deng
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 15 pages, 5 figures, and 1 table
- **标题**: None
- **领域**: 机器学习,神经和进化计算
- **Abstract**: The recently proposed Kolmogorov-Arnold network (KAN) is a promising alternative to multi-layer perceptrons (MLPs) for data-driven modeling. While original KAN layers were only capable of representing the addition operator, the recently-proposed MultKAN layer combines addition and multiplication subnodes in an effort to improve representation performance. Here, we find that MultKAN layers suffer from a few key drawbacks including limited applicability in output layers, bulky parameterizations with extraneous activations, and the inclusion of complex hyperparameters. To address these issues, we propose LeanKANs, a direct and modular replacement for MultKAN and traditional AddKAN layers. LeanKANs address these three drawbacks of MultKAN through general applicability as output layers, significantly reduced parameter counts for a given network structure, and a smaller set of hyperparameters. As a one-to-one layer replacement for standard AddKAN and MultKAN layers, LeanKAN is able to provide these benefits to traditional KAN learning problems as well as augmented KAN structures in which it serves as the backbone, such as KAN Ordinary Differential Equations (KAN-ODEs) or Deep Operator KANs (DeepOKAN). We demonstrate LeanKAN's simplicity and efficiency in a series of demonstrations carried out across both a standard KAN toy problem and a KAN-ODE dynamical system modeling problem, where we find that its sparser parameterization and compact structure serve to increase its expressivity and learning capability, leading it to outperform similar and even much larger MultKANs in various tasks.

### Task-Driven Semantic Quantization and Imitation Learning for Goal-Oriented Communications 
[[arxiv](https://arxiv.org/abs/2502.17842)] [[cool](https://papers.cool/arxiv/2502.17842)] [[pdf](https://arxiv.org/pdf/2502.17842)]
> **Authors**: Yu-Chieh Chao,Yubei Chen,Weiwei Wang,Achintha Wijesinghe,Suchinthaka Wanninayaka,Songyang Zhang,Zhi Ding
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Accepted for publication in 2025 International Conference on Communications (IEEE ICC); 6 pages, 4 figures
- **标题**: None
- **领域**: 机器学习,网络和互联网架构
- **Abstract**: Semantic communication marks a new paradigm shift from bit-wise data transmission to semantic information delivery for the purpose of bandwidth reduction. To more effectively carry out specialized downstream tasks at the receiver end, it is crucial to define the most critical semantic message in the data based on the task or goal-oriented features. In this work, we propose a novel goal-oriented communication (GO-COM) framework, namely Goal-Oriented Semantic Variational Autoencoder (GOS-VAE), by focusing on the extraction of the semantics vital to the downstream tasks. Specifically, we adopt a Vector Quantized Variational Autoencoder (VQ-VAE) to compress media data at the transmitter side. Instead of targeting the pixel-wise image data reconstruction, we measure the quality-of-service at the receiver end based on a pre-defined task-incentivized model. Moreover, to capture the relevant semantic features in the data reconstruction, imitation learning is adopted to measure the data regeneration quality in terms of goal-oriented semantics. Our experimental results demonstrate the power of imitation learning in characterizing goal-oriented semantics and bandwidth efficiency of our proposed GOS-VAE.

### Learning Backbones: Sparsifying Graphs through Zero Forcing for Effective Graph-Based Learning 
[[arxiv](https://arxiv.org/abs/2502.17713)] [[cool](https://papers.cool/arxiv/2502.17713)] [[pdf](https://arxiv.org/pdf/2502.17713)]
> **Authors**: Obaid Ullah Ahmad,Anwar Said,Mudassir Shabbir,Xenofon Koutsoukos,Waseem Abbas
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 13th International Conference on Complex Networks and their Applications
- **标题**: None
- **领域**: 机器学习,多代理系统,系统与控制
- **Abstract**: This paper introduces a novel framework for graph sparsification that preserves the essential learning attributes of original graphs, improving computational efficiency and reducing complexity in learning algorithms. We refer to these sparse graphs as "learning backbones". Our approach leverages the zero-forcing (ZF) phenomenon, a dynamic process on graphs with applications in network control. The key idea is to generate a tree from the original graph that retains critical dynamical properties. By correlating these properties with learning attributes, we construct effective learning backbones. We evaluate the performance of our ZF-based backbones in graph classification tasks across eight datasets and six baseline models. The results demonstrate that our method outperforms existing techniques. Additionally, we explore extensions using node distance metrics to further enhance the framework's utility.

### Robust Federated Learning with Global Sensitivity Estimation for Financial Risk Management 
[[arxiv](https://arxiv.org/abs/2502.17694)] [[cool](https://papers.cool/arxiv/2502.17694)] [[pdf](https://arxiv.org/pdf/2502.17694)]
> **Authors**: Lei Zhao,Lin Cai,Wu-Sheng Lu
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,分布式、并行和集群计算
- **Abstract**: In decentralized financial systems, robust and efficient Federated Learning (FL) is promising to handle diverse client environments and ensure resilience to systemic risks. We propose Federated Risk-Aware Learning with Central Sensitivity Estimation (FRAL-CSE), an innovative FL framework designed to enhance scalability, stability, and robustness in collaborative financial decision-making. The framework's core innovation lies in a central acceleration mechanism, guided by a quadratic sensitivity-based approximation of global model dynamics. By leveraging local sensitivity information derived from robust risk measurements, FRAL-CSE performs a curvature-informed global update that efficiently incorporates second-order information without requiring repeated local re-evaluations, thereby enhancing training efficiency and improving optimization stability. Additionally, distortion risk measures are embedded into the training objectives to capture tail risks and ensure robustness against extreme scenarios. Extensive experiments validate the effectiveness of FRAL-CSE in accelerating convergence and improving resilience across heterogeneous datasets compared to state-of-the-art baselines.

### Predictive Response Optimization: Using Reinforcement Learning to Fight Online Social Network Abuse 
[[arxiv](https://arxiv.org/abs/2502.17693)] [[cool](https://papers.cool/arxiv/2502.17693)] [[pdf](https://arxiv.org/pdf/2502.17693)]
> **Authors**: Garrett Wilson,Geoffrey Goh,Yan Jiang,Ajay Gupta,Jiaxuan Wang,David Freeman,Francesco Dinuzzo
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: To appear in USENIX Security 2025
- **标题**: None
- **领域**: 机器学习,密码学和安全,社交和信息网络
- **Abstract**: Detecting phishing, spam, fake accounts, data scraping, and other malicious activity in online social networks (OSNs) is a problem that has been studied for well over a decade, with a number of important results. Nearly all existing works on abuse detection have as their goal producing the best possible binary classifier; i.e., one that labels unseen examples as "benign" or "malicious" with high precision and recall. However, no prior published work considers what comes next: what does the service actually do after it detects abuse? In this paper, we argue that detection as described in previous work is not the goal of those who are fighting OSN abuse. Rather, we believe the goal to be selecting actions (e.g., ban the user, block the request, show a CAPTCHA, or "collect more evidence") that optimize a tradeoff between harm caused by abuse and impact on benign users. With this framing, we see that enlarging the set of possible actions allows us to move the Pareto frontier in a way that is unattainable by simply tuning the threshold of a binary classifier. To demonstrate the potential of our approach, we present Predictive Response Optimization (PRO), a system based on reinforcement learning that utilizes available contextual information to predict future abuse and user-experience metrics conditioned on each possible action, and select actions that optimize a multi-dimensional tradeoff between abuse/harm and impact on user experience. We deployed versions of PRO targeted at stopping automated activity on Instagram and Facebook. In both cases our experiments showed that PRO outperforms a baseline classification system, reducing abuse volume by 59% and 4.5% (respectively) with no negative impact to users. We also present several case studies that demonstrate how PRO can quickly and automatically adapt to changes in business constraints, system behavior, and/or adversarial tactics.

### Yes, Q-learning Helps Offline In-Context RL 
[[arxiv](https://arxiv.org/abs/2502.17666)] [[cool](https://papers.cool/arxiv/2502.17666)] [[pdf](https://arxiv.org/pdf/2502.17666)]
> **Authors**: Denis Tarasov,Alexander Nikulin,Ilya Zisman,Albina Klepach,Andrei Polubarov,Nikita Lyubaykin,Alexander Derevyagin,Igor Kiselev,Vladislav Kurenkov
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: In this work, we explore the integration of Reinforcement Learning (RL) approaches within a scalable offline In-Context RL (ICRL) framework. Through experiments across more than 150 datasets derived from GridWorld and MuJoCo environments, we demonstrate that optimizing RL objectives improves performance by approximately 40% on average compared to the widely established Algorithm Distillation (AD) baseline across various dataset coverages, structures, expertise levels, and environmental complexities. Our results also reveal that offline RL-based methods outperform online approaches, which are not specifically designed for offline scenarios. These findings underscore the importance of aligning the learning objectives with RL's reward-maximization goal and demonstrate that offline RL is a promising direction for application in ICRL settings.

### Architecting Digital Twins for Intelligent Transportation Systems 
[[arxiv](https://arxiv.org/abs/2502.17646)] [[cool](https://papers.cool/arxiv/2502.17646)] [[pdf](https://arxiv.org/pdf/2502.17646)]
> **Authors**: Hiya Bhatt,Sahil,Karthik Vaidhyanathan,Rahul Biju,Deepak Gangadharan,Ramona Trestian,Purav Shah
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,软件工程
- **Abstract**: Modern transportation systems face growing challenges in managing traffic flow, ensuring safety, and maintaining operational efficiency amid dynamic traffic patterns. Addressing these challenges requires intelligent solutions capable of real-time monitoring, predictive analytics, and adaptive control. This paper proposes an architecture for DigIT, a Digital Twin (DT) platform for Intelligent Transportation Systems (ITS), designed to overcome the limitations of existing frameworks by offering a modular and scalable solution for traffic management. Built on a Domain Concept Model (DCM), the architecture systematically models key ITS components enabling seamless integration of predictive modeling and simulations. The architecture leverages machine learning models to forecast traffic patterns based on historical and real-time data. To adapt to evolving traffic patterns, the architecture incorporates adaptive Machine Learning Operations (MLOps), automating the deployment and lifecycle management of predictive models. Evaluation results highlight the effectiveness of the architecture in delivering accurate predictions and computational efficiency.

### The Power of Graph Signal Processing for Chip Placement Acceleration 
[[arxiv](https://arxiv.org/abs/2502.17632)] [[cool](https://papers.cool/arxiv/2502.17632)] [[pdf](https://arxiv.org/pdf/2502.17632)]
> **Authors**: Yiting Liu,Hai Zhou,Jia Wang,Fan Yang,Xuan Zeng,Li Shang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: ICCAD'24 conference
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Placement is a critical task with high computation complexity in VLSI physical design. Modern analytical placers formulate the placement objective as a nonlinear optimization task, which suffers a long iteration time. To accelerate and enhance the placement process, recent studies have turned to deep learning-based approaches, particularly leveraging graph convolution networks (GCNs). However, learning-based placers require time- and data-consuming model training due to the complexity of circuit placement that involves large-scale cells and design-specific graph statistics. This paper proposes GiFt, a parameter-free technique for accelerating placement, rooted in graph signal processing. GiFt excels at capturing multi-resolution smooth signals of circuit graphs to generate optimized placement solutions without the need for time-consuming model training, and meanwhile significantly reduces the number of iterations required by analytical placers. Experimental results show that GiFt significantly improving placement efficiency, while achieving competitive or superior performance compared to state-of-the-art placers. In particular, compared to DREAMPlace, the recently proposed GPU-accelerated analytical placer, GF-Placer improves total runtime over 45%.

### Instance-Dependent Regret Bounds for Learning Two-Player Zero-Sum Games with Bandit Feedback 
[[arxiv](https://arxiv.org/abs/2502.17625)] [[cool](https://papers.cool/arxiv/2502.17625)] [[pdf](https://arxiv.org/pdf/2502.17625)]
> **Authors**: Shinji Ito,Haipeng Luo,Taira Tsuchiya,Yue Wu
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: :91A26 (Primary) 68T05; 68Q32 (Secondary)ACM Class:F.2.0; I.2.11; G.3
- **标题**: None
- **领域**: 机器学习,计算机科学与博弈论
- **Abstract**: No-regret self-play learning dynamics have become one of the premier ways to solve large-scale games in practice. Accelerating their convergence via improving the regret of the players over the naive $O(\sqrt{T})$ bound after $T$ rounds has been extensively studied in recent years, but almost all studies assume access to exact gradient feedback. We address the question of whether acceleration is possible under bandit feedback only and provide an affirmative answer for two-player zero-sum normal-form games. Specifically, we show that if both players apply the Tsallis-INF algorithm of Zimmert and Seldin (2018, arXiv:1807.07623), then their regret is at most $O(c_1 \log T + \sqrt{c_2 T})$, where $c_1$ and $c_2$ are game-dependent constants that characterize the difficulty of learning -- $c_1$ resembles the complexity of learning a stochastic multi-armed bandit instance and depends inversely on some gap measures, while $c_2$ can be much smaller than the number of actions when the Nash equilibria have a small support or are close to the boundary. In particular, for the case when a pure strategy Nash equilibrium exists, $c_2$ becomes zero, leading to an optimal instance-dependent regret bound as we show. We additionally prove that in this case, our algorithm also enjoys last-iterate convergence and can identify the pure strategy Nash equilibrium with near-optimal sample complexity.

### Hierarchical Imitation Learning of Team Behavior from Heterogeneous Demonstrations 
[[arxiv](https://arxiv.org/abs/2502.17618)] [[cool](https://papers.cool/arxiv/2502.17618)] [[pdf](https://arxiv.org/pdf/2502.17618)]
> **Authors**: Sangwon Seo,Vaibhav Unhelkar
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Extended version of an identically-titled paper accepted at AAMAS 2025
- **标题**: None
- **领域**: 机器学习,人工智能,多代理系统
- **Abstract**: Successful collaboration requires team members to stay aligned, especially in complex sequential tasks. Team members must dynamically coordinate which subtasks to perform and in what order. However, real-world constraints like partial observability and limited communication bandwidth often lead to suboptimal collaboration. Even among expert teams, the same task can be executed in multiple ways. To develop multi-agent systems and human-AI teams for such tasks, we are interested in data-driven learning of multimodal team behaviors. Multi-Agent Imitation Learning (MAIL) provides a promising framework for data-driven learning of team behavior from demonstrations, but existing methods struggle with heterogeneous demonstrations, as they assume that all demonstrations originate from a single team policy. Hence, in this work, we introduce DTIL: a hierarchical MAIL algorithm designed to learn multimodal team behaviors in complex sequential tasks. DTIL represents each team member with a hierarchical policy and learns these policies from heterogeneous team demonstrations in a factored manner. By employing a distribution-matching approach, DTIL mitigates compounding errors and scales effectively to long horizons and continuous state representations. Experimental results show that DTIL outperforms MAIL baselines and accurately models team behavior across a variety of collaborative scenarios.

### Provable Model-Parallel Distributed Principal Component Analysis with Parallel Deflation 
[[arxiv](https://arxiv.org/abs/2502.17615)] [[cool](https://papers.cool/arxiv/2502.17615)] [[pdf](https://arxiv.org/pdf/2502.17615)]
> **Authors**: Fangshuo Liao,Wenyi Su,Anastasios Kyrillidis
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: CPAL 2025
- **标题**: None
- **领域**: 机器学习,分布式、并行和集群计算,优化与控制
- **Abstract**: We study a distributed Principal Component Analysis (PCA) framework where each worker targets a distinct eigenvector and refines its solution by updating from intermediate solutions provided by peers deemed as "superior". Drawing intuition from the deflation method in centralized eigenvalue problems, our approach breaks the sequential dependency in the deflation steps and allows asynchronous updates of workers, while incurring only a small communication cost. To our knowledge, a gap in the literature -- the theoretical underpinning of such distributed, dynamic interactions among workers -- has remained unaddressed. This paper offers a theoretical analysis explaining why, how, and when these intermediate, hierarchical updates lead to practical and provable convergence in distributed environments. Despite being a theoretical work, our prototype implementation demonstrates that such a distributed PCA algorithm converges effectively and in scalable way: through experiments, our proposed framework offers comparable performance to EigenGame-$μ$, the state-of-the-art model-parallel PCA solver.

### Scalable Graph Condensation with Evolving Capabilities 
[[arxiv](https://arxiv.org/abs/2502.17614)] [[cool](https://papers.cool/arxiv/2502.17614)] [[pdf](https://arxiv.org/pdf/2502.17614)]
> **Authors**: Shengbo Gong,Mohammad Hashemi,Juntong Ni,Carl Yang,Wei Jin
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 16 pages, 6 figures
- **标题**: None
- **领域**: 机器学习,社交和信息网络
- **Abstract**: Graph data has become a pivotal modality due to its unique ability to model relational datasets. However, real-world graph data continues to grow exponentially, resulting in a quadratic increase in the complexity of most graph algorithms as graph sizes expand. Although graph condensation (GC) methods have been proposed to address these scalability issues, existing approaches often treat the training set as static, overlooking the evolving nature of real-world graph data. This limitation leads to inefficiencies when condensing growing training sets. In this paper, we introduce GECC (Graph Evolving Clustering Condensation), a scalable graph condensation method designed to handle large-scale and evolving graph data. GECC employs a traceable and efficient approach by performing class-wise clustering on aggregated features. Furthermore, it can inherits previous condensation results as clustering centroids when the condensed graph expands, thereby attaining an evolving capability. This methodology is supported by robust theoretical foundations and demonstrates superior empirical performance. Comprehensive experiments show that GECC achieves better performance than most state-of-the-art graph condensation methods while delivering an around 1,000x speedup on large datasets.

### Flexible Counterfactual Explanations with Generative Models 
[[arxiv](https://arxiv.org/abs/2502.17613)] [[cool](https://papers.cool/arxiv/2502.17613)] [[pdf](https://arxiv.org/pdf/2502.17613)]
> **Authors**: Stig Hellemans,Andres Algaba,Sam Verboven,Vincent Ginis
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 28 pages, 13 figures
- **标题**: None
- **领域**: 机器学习,人工智能,方法论
- **Abstract**: Counterfactual explanations provide actionable insights to achieve desired outcomes by suggesting minimal changes to input features. However, existing methods rely on fixed sets of mutable features, which makes counterfactual explanations inflexible for users with heterogeneous real-world constraints. Here, we introduce Flexible Counterfactual Explanations, a framework incorporating counterfactual templates, which allows users to dynamically specify mutable features at inference time. In our implementation, we use Generative Adversarial Networks (FCEGAN), which align explanations with user-defined constraints without requiring model retraining or additional optimization. Furthermore, FCEGAN is designed for black-box scenarios, leveraging historical prediction datasets to generate explanations without direct access to model internals. Experiments across economic and healthcare datasets demonstrate that FCEGAN significantly improves counterfactual explanations' validity compared to traditional benchmark methods. By integrating user-driven flexibility and black-box compatibility, counterfactual templates support personalized explanations tailored to user constraints.

### Synthetic Text Generation for Training Large Language Models via Gradient Matching 
[[arxiv](https://arxiv.org/abs/2502.17607)] [[cool](https://papers.cool/arxiv/2502.17607)] [[pdf](https://arxiv.org/pdf/2502.17607)]
> **Authors**: Dang Nguyen,Zeman Li,Mohammadhossein Bateni,Vahab Mirrokni,Meisam Razaviyayn,Baharan Mirzasoleiman
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 15 pages, 5 figures, 4 tables
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Synthetic data has the potential to improve the performance, training efficiency, and privacy of real training examples. Nevertheless, existing approaches for synthetic text generation are mostly heuristics and cannot generate human-readable text without compromising the privacy of real data or provide performance guarantees for training Large Language Models (LLMs). In this work, we propose the first theoretically rigorous approach for generating synthetic human-readable text that guarantees the convergence and performance of LLMs during fine-tuning on a target task. To do so, we leverage Alternating Direction Method of Multipliers (ADMM) that iteratively optimizes the embeddings of synthetic examples to match the gradient of the target training or validation data, and maps them to a sequence of text tokens with low perplexity. In doing so, the generated synthetic text can guarantee convergence of the model to a close neighborhood of the solution obtained by fine-tuning on real data. Experiments on various classification tasks confirm the effectiveness of our proposed approach.

### Hallucination Detection in LLMs Using Spectral Features of Attention Maps 
[[arxiv](https://arxiv.org/abs/2502.17598)] [[cool](https://papers.cool/arxiv/2502.17598)] [[pdf](https://arxiv.org/pdf/2502.17598)]
> **Authors**: Jakub Binkowski,Denis Janiak,Albert Sawczyn,Bogdan Gabrys,Tomasz Kajdanowicz
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Preprint, under review
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Large Language Models (LLMs) have demonstrated remarkable performance across various tasks but remain prone to hallucinations. Detecting hallucinations is essential for safety-critical applications, and recent methods leverage attention map properties to this end, though their effectiveness remains limited. In this work, we investigate the spectral features of attention maps by interpreting them as adjacency matrices of graph structures. We propose the $\text{LapEigvals}$ method, which utilises the top-$k$ eigenvalues of the Laplacian matrix derived from the attention maps as an input to hallucination detection probes. Empirical evaluations demonstrate that our approach achieves state-of-the-art hallucination detection performance among attention-based methods. Extensive ablation studies further highlight the robustness and generalisation of $\text{LapEigvals}$, paving the way for future advancements in the hallucination detection domain.

## 机器人技术(cs.RO:Robotics)

### CAML: Collaborative Auxiliary Modality Learning for Multi-Agent Systems 
[[arxiv](https://arxiv.org/abs/2502.17821)] [[cool](https://papers.cool/arxiv/2502.17821)] [[pdf](https://arxiv.org/pdf/2502.17821)]
> **Authors**: Rui Liu,Yu Shen,Peng Gao,Pratap Tokekar,Ming Lin
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,人工智能,机器学习
- **Abstract**: Multi-modality learning has become a crucial technique for improving the performance of machine learning applications across domains such as autonomous driving, robotics, and perception systems. While existing frameworks such as Auxiliary Modality Learning (AML) effectively utilize multiple data sources during training and enable inference with reduced modalities, they primarily operate in a single-agent context. This limitation is particularly critical in dynamic environments, such as connected autonomous vehicles (CAV), where incomplete data coverage can lead to decision-making blind spots. To address these challenges, we propose Collaborative Auxiliary Modality Learning ($\textbf{CAML}$), a novel multi-agent multi-modality framework that enables agents to collaborate and share multimodal data during training while allowing inference with reduced modalities per agent during testing. We systematically analyze the effectiveness of $\textbf{CAML}$ from the perspective of uncertainty reduction and data coverage, providing theoretical insights into its advantages over AML. Experimental results in collaborative decision-making for CAV in accident-prone scenarios demonstrate that \ours~achieves up to a ${\bf 58.13}\%$ improvement in accident detection. Additionally, we validate $\textbf{CAML}$ on real-world aerial-ground robot data for collaborative semantic segmentation, achieving up to a ${\bf 10.61}\%$ improvement in mIoU.

### Safe Multi-Agent Navigation guided by Goal-Conditioned Safe Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.17813)] [[cool](https://papers.cool/arxiv/2502.17813)] [[pdf](https://arxiv.org/pdf/2502.17813)]
> **Authors**: Meng Feng,Viraj Parimi,Brian Williams
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Due to the limitation "The abstract field cannot be longer than 1,920 characters", the abstract here is shorter than that in the PDF file
- **标题**: None
- **领域**: 机器人技术,机器学习
- **Abstract**: Safe navigation is essential for autonomous systems operating in hazardous environments. Traditional planning methods excel at long-horizon tasks but rely on a predefined graph with fixed distance metrics. In contrast, safe Reinforcement Learning (RL) can learn complex behaviors without relying on manual heuristics but fails to solve long-horizon tasks, particularly in goal-conditioned and multi-agent scenarios. In this paper, we introduce a novel method that integrates the strengths of both planning and safe RL. Our method leverages goal-conditioned RL and safe RL to learn a goal-conditioned policy for navigation while concurrently estimating cumulative distance and safety levels using learned value functions via an automated self-training algorithm. By constructing a graph with states from the replay buffer, our method prunes unsafe edges and generates a waypoint-based plan that the agent follows until reaching its goal, effectively balancing faster and safer routes over extended distances. Utilizing this unified high-level graph and a shared low-level goal-conditioned safe RL policy, we extend this approach to address the multi-agent safe navigation problem. In particular, we leverage Conflict-Based Search (CBS) to create waypoint-based plans for multiple agents allowing for their safe navigation over extended horizons. This integration enhances the scalability of goal-conditioned safe RL in multi-agent scenarios, enabling efficient coordination among agents. Extensive benchmarking against state-of-the-art baselines demonstrates the effectiveness of our method in achieving distance goals safely for multiple agents in complex and hazardous environments. Our code will be released to support future research.

### Toward 6-DOF Autonomous Underwater Vehicle Energy-Aware Position Control based on Deep Reinforcement Learning: Preliminary Results 
[[arxiv](https://arxiv.org/abs/2502.17742)] [[cool](https://papers.cool/arxiv/2502.17742)] [[pdf](https://arxiv.org/pdf/2502.17742)]
> **Authors**: Gustavo Boré,Vicente Sufán,Sebastián Rodríguez-Martínez,Giancarlo Troni
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 6 pages, 5 figures, submitted to 2024 IEEE OES AUV Symposium
- **标题**: None
- **领域**: 机器人技术,机器学习,系统与控制
- **Abstract**: The use of autonomous underwater vehicles (AUVs) for surveying, mapping, and inspecting unexplored underwater areas plays a crucial role, where maneuverability and power efficiency are key factors for extending the use of these platforms, making six degrees of freedom (6-DOF) holonomic platforms essential tools. Although Proportional-Integral-Derivative (PID) and Model Predictive Control controllers are widely used in these applications, they often require accurate system knowledge, struggle with repeatability when facing payload or configuration changes, and can be time-consuming to fine-tune. While more advanced methods based on Deep Reinforcement Learning (DRL) have been proposed, they are typically limited to operating in fewer degrees of freedom. This paper proposes a novel DRL-based approach for controlling holonomic 6-DOF AUVs using the Truncated Quantile Critics (TQC) algorithm, which does not require manual tuning and directly feeds commands to the thrusters without prior knowledge of their configuration. Furthermore, it incorporates power consumption directly into the reward function. Simulation results show that the TQC High-Performance method achieves better performance to a fine-tuned PID controller when reaching a goal point, while the TQC Energy-Aware method demonstrates slightly lower performance but consumes 30% less power on average.

### Learning Decentralized Swarms Using Rotation Equivariant Graph Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.17612)] [[cool](https://papers.cool/arxiv/2502.17612)] [[pdf](https://arxiv.org/pdf/2502.17612)]
> **Authors**: Taos Transue,Bao Wang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: correcting contact information
- **标题**: None
- **领域**: 机器人技术,机器学习
- **Abstract**: The orchestration of agents to optimize a collective objective without centralized control is challenging yet crucial for applications such as controlling autonomous fleets, and surveillance and reconnaissance using sensor networks. Decentralized controller design has been inspired by self-organization found in nature, with a prominent source of inspiration being flocking; however, decentralized controllers struggle to maintain flock cohesion. The graph neural network (GNN) architecture has emerged as an indispensable machine learning tool for developing decentralized controllers capable of maintaining flock cohesion, but they fail to exploit the symmetries present in flocking dynamics, hindering their generalizability. We enforce rotation equivariance and translation invariance symmetries in decentralized flocking GNN controllers and achieve comparable flocking control with 70% less training data and 75% fewer trainable weights than existing GNN controllers without these symmetries enforced. We also show that our symmetry-aware controller generalizes better than existing GNN controllers. Code and animations are available at http://github.com/Utah-Math-Data-Science/Equivariant-Decentralized-Controllers.

## 图像和视频处理(eess.IV:Image and Video Processing)

### TagGAN: A Generative Model for Data Tagging 
[[arxiv](https://arxiv.org/abs/2502.17836)] [[cool](https://papers.cool/arxiv/2502.17836)] [[pdf](https://arxiv.org/pdf/2502.17836)]
> **Authors**: Muhammad Nawaz,Basma Nasir,Tehseen Zia,Zawar Hussain,Catarina Moreira
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习
- **Abstract**: Precise identification and localization of disease-specific features at the pixel-level are particularly important for early diagnosis, disease progression monitoring, and effective treatment in medical image analysis. However, conventional diagnostic AI systems lack decision transparency and cannot operate well in environments where there is a lack of pixel-level annotations. In this study, we propose a novel Generative Adversarial Networks (GANs)-based framework, TagGAN, which is tailored for weakly-supervised fine-grained disease map generation from purely image-level labeled data. TagGAN generates a pixel-level disease map during domain translation from an abnormal image to a normal representation. Later, this map is subtracted from the input abnormal image to convert it into its normal counterpart while preserving all the critical anatomical details. Our method is first to generate fine-grained disease maps to visualize disease lesions in a weekly supervised setting without requiring pixel-level annotations. This development enhances the interpretability of diagnostic AI by providing precise visualizations of disease-specific regions. It also introduces automated binary mask generation to assist radiologists. Empirical evaluations carried out on the benchmark datasets, CheXpert, TBX11K, and COVID-19, demonstrate the capability of TagGAN to outperform current top models in accurately identifying disease-specific pixels. This outcome highlights the capability of the proposed model to tag medical images, significantly reducing the workload for radiologists by eliminating the need for binary masks during training.

## 高能物理-现象学(hep-ph:High Energy Physics - Phenomenology)

### Unraveling particle dark matter with Physics-Informed Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.17597)] [[cool](https://papers.cool/arxiv/2502.17597)] [[pdf](https://arxiv.org/pdf/2502.17597)]
> **Authors**: M. P. Bento,H. B. Câmara,J. F. Seabra
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 20 LaTeX pages, 11 Figures
- **标题**: None
- **领域**: 高能物理-现象学,机器学习
- **Abstract**: We parametrically solve the Boltzmann equations governing freeze-in dark matter (DM) in alternative cosmologies with Physics-Informed Neural Networks (PINNs), a mesh-free method. Through inverse PINNs, using a single DM experimental point -- observed relic density -- we determine the physical attributes of the theory, namely power-law cosmologies, inspired by braneworld scenarios, and particle interaction cross sections. The expansion of the Universe in such alternative cosmologies has been parameterized through a switch-like function reproducing the Hubble law at later times. Without loss of generality, we model more realistically this transition with a smooth function. We predict a distinct pair-wise relationship between power-law exponent and particle interactions: for a given cosmology with negative (positive) exponent, smaller (larger) cross sections are required to reproduce the data. Lastly, via Bayesian methods, we quantify the epistemic uncertainty of theoretical parameters found in inverse problems.

## 优化与控制(math.OC:Optimization and Control)

### A stochastic smoothing framework for nonconvex-nonconcave min-sum-max problems with applications to Wasserstein distributionally robust optimization 
[[arxiv](https://arxiv.org/abs/2502.17602)] [[cool](https://papers.cool/arxiv/2502.17602)] [[pdf](https://arxiv.org/pdf/2502.17602)]
> **Authors**: Wei Liu,Muhammad Khan,Gabriel Mancino-Ball,Yangyang Xu
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 35 pages
- **标题**: None
- **领域**: 优化与控制,机器学习
- **Abstract**: Applications such as adversarially robust training and Wasserstein Distributionally Robust Optimization (WDRO) can be naturally formulated as min-sum-max optimization problems. While this formulation can be rewritten as an equivalent min-max problem, the summation of max terms introduces computational challenges, including increased complexity and memory demands, which must be addressed. These challenges are particularly evident in WDRO, where existing tractable algorithms often rely on restrictive assumptions on the objective function, limiting their applicability to state-of-the-art machine learning problems such as the training of deep neural networks. This study introduces a novel stochastic smoothing framework based on the \mbox{log-sum-exp} function, efficiently approximating the max operator in min-sum-max problems. By leveraging the Clarke regularity of the max operator, we develop an iterative smoothing algorithm that addresses these computational difficulties and guarantees almost surely convergence to a Clarke/directional stationary point. We further prove that the proposed algorithm finds an $ε$-scaled Clarke stationary point of the original problem, with a worst-case iteration complexity of $\widetilde{O}(ε^{-3})$. Our numerical experiments demonstrate that our approach outperforms or is competitive with state-of-the-art methods in solving the newsvendor problem, deep learning regression, and adversarially robust deep learning. The results highlight that our method yields more accurate and robust solutions in these challenging problem settings.

## 混沌动力学(nlin.CD:Chaotic Dynamics)

### A Fokker-Planck-Based Loss Function that Bridges Dynamics with Density Estimation 
[[arxiv](https://arxiv.org/abs/2502.17690)] [[cool](https://papers.cool/arxiv/2502.17690)] [[pdf](https://arxiv.org/pdf/2502.17690)]
> **Authors**: Zhixin Lu,Łukasz Kuśmierz,Stefan Mihalas
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Under review by the ICML
- **标题**: None
- **领域**: 混沌动力学,机器学习
- **Abstract**: We have derived a novel loss function from the Fokker-Planck equation that links dynamical system models with their probability density functions, demonstrating its utility in model identification and density estimation. In the first application, we show that this loss function can enable the extraction of dynamical parameters from non-temporal datasets, including timestamp-free measurements from steady non-equilibrium systems such as noisy Lorenz systems and gene regulatory networks. In the second application, when coupled with a density estimator, this loss facilitates density estimation when the dynamic equations are known. For density estimation, we propose a density estimator that integrates a Gaussian Mixture Model with a normalizing flow model. It simultaneously estimates normalized density, energy, and score functions from both empirical data and dynamics. It is compatible with a variety of data-based training methodologies, including maximum likelihood and score matching. It features a latent space akin to a modern Hopfield network, where the inherent Hopfield energy effectively assigns low densities to sparsely populated data regions, addressing common challenges in neural density estimators. Additionally, this Hopfield-like energy enables direct and rapid data manipulation through the Concave-Convex Procedure (CCCP) rule, facilitating tasks such as denoising and clustering. Our work demonstrates a principled framework for leveraging the complex interdependencies between dynamics and density estimation, as illustrated through synthetic examples that clarify the underlying theoretical intuitions.

## 计算物理(physics.comp-ph:Computational Physics)

### Effective Field Neural Network 
[[arxiv](https://arxiv.org/abs/2502.17665)] [[cool](https://papers.cool/arxiv/2502.17665)] [[pdf](https://arxiv.org/pdf/2502.17665)]
> **Authors**: Xi Liu,Yujun Zhao,Chun Yu Wan,Yang Zhang,Junwei Liu
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 计算物理,强关联电子,人工智能,量子物理学
- **Abstract**: In recent years, with the rapid development of machine learning, physicists have been exploring its new applications in solving or alleviating the curse of dimensionality in many-body problems. In order to accurately reflect the underlying physics of the problem, domain knowledge must be encoded into the machine learning algorithms. In this work, inspired by field theory, we propose a new set of machine learning models called effective field neural networks (EFNNs) that can automatically and efficiently capture important many-body interactions through multiple self-refining processes. Taking the classical $3$-spin infinite-range model and the quantum double exchange model as case studies, we explicitly demonstrate that EFNNs significantly outperform fully-connected deep neural networks (DNNs) and the effective model. Furthermore, with the help of convolution operations, the EFNNs learned in a small system can be seamlessly used in a larger system without additional training and the relative errors even decrease, which further demonstrates the efficacy of EFNNs in representing core physical behaviors.

## 地球物理学(physics.geo-ph:Geophysics)

### Theory-guided Pseudo-spectral Full Waveform Inversion via Deep Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.17624)] [[cool](https://papers.cool/arxiv/2502.17624)] [[pdf](https://arxiv.org/pdf/2502.17624)]
> **Authors**: Christopher Zerafa,Pauline Galea,Cristiana Sebu
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 26 pages, 23 figures, article paper
- **标题**: None
- **领域**: 地球物理学,人工智能
- **Abstract**: Full-Waveform Inversion seeks to achieve a high-resolution model of the subsurface through the application of multi-variate optimization to the seismic inverse problem. Although now a mature technology, FWI has limitations related to the choice of the appropriate solver for the forward problem in challenging environments requiring complex assumptions, and very wide angle and multi-azimuth data necessary for full reconstruction are often not available. Deep Learning techniques have emerged as excellent optimization frameworks. Data-driven methods do not impose a wave propagation model and are not exposed to modelling errors. On the contrary, deterministic models are governed by the laws of physics. Seismic FWI has recently started to be investigated as a Deep Learning framework. Focus has been on the time-domain, while the pseudo-spectral domain has not been yet explored. However, classical FWI experienced major breakthroughs when pseudo-spectral approaches were employed. This work addresses the lacuna that exists in incorporating the pseudo-spectral approach within Deep Learning. This has been done by re-formulating the pseudo-spectral FWI problem as a Deep Learning algorithm for a theory-driven pseudo-spectral approach. A novel Recurrent Neural Network framework is proposed. This is qualitatively assessed on synthetic data, applied to a two-dimensional Marmousi dataset and evaluated against deterministic and time-based approaches. Pseudo-spectral theory-guided FWI using RNN was shown to be more accurate than classical FWI with only 0.05 error tolerance and 1.45\% relative percent-age error. Indeed, this provides more stable convergence, able to identify faults better and has more low frequency content than classical FWI. Moreover, RNN was more suited than classical FWI at edge detection in the shallow and deep sections due to cleaner receiver residuals.

### Data-Driven Pseudo-spectral Full Waveform Inversion via Deep Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.17608)] [[cool](https://papers.cool/arxiv/2502.17608)] [[pdf](https://arxiv.org/pdf/2502.17608)]
> **Authors**: Christopher Zerafa,Pauline Galea,Cristiana Sebu
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 11 pages, 6 pages, review paper
- **标题**: None
- **领域**: 地球物理学,人工智能,计算机视觉和模式识别,机器学习
- **Abstract**: FWI seeks to achieve a high-resolution model of the subsurface through the application of multi-variate optimization to the seismic inverse problem. Although now a mature technology, FWI has limitations related to the choice of the appropriate solver for the forward problem in challenging environments requiring complex assumptions, and very wide angle and multi-azimuth data necessary for full reconstruction are often not available. Deep Learning techniques have emerged as excellent optimization frameworks. These exist between data and theory-guided methods. Data-driven methods do not impose a wave propagation model and are not exposed to modelling errors. On the contrary, deterministic models are governed by the laws of physics. Application of seismic FWI has recently started to be investigated within Deep Learning. This has focussed on the time-domain approach, while the pseudo-spectral domain has not been yet explored. However, classical FWI experienced major breakthroughs when pseudo-spectral approaches were employed. This work addresses the lacuna that exists in incorporating the pseudo-spectral approach within Deep Learning. This has been done by re-formulating the pseudo-spectral FWI problem as a Deep Learning algorithm for a data-driven pseudo-spectral approach. A novel DNN framework is proposed. This is formulated theoretically, qualitatively assessed on synthetic data, applied to a two-dimensional Marmousi dataset and evaluated against deterministic and time-based approaches. Inversion of data-driven pseudo-spectral DNN was found to outperform classical FWI for deeper and over-thrust areas. This is due to the global approximator nature of the technique and hence not bound by forward-modelling physical constraints from ray-tracing.

### Synergizing Deep Learning and Full-Waveform Inversion: Bridging Data-Driven and Theory-Guided Approaches for Enhanced Seismic Imaging 
[[arxiv](https://arxiv.org/abs/2502.17585)] [[cool](https://papers.cool/arxiv/2502.17585)] [[pdf](https://arxiv.org/pdf/2502.17585)]
> **Authors**: Christopher Zerafa,Pauline Galea,Cristiana Sebu
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 20 pages, 14 images, literature review
- **标题**: None
- **领域**: 地球物理学,人工智能,计算工程、金融和科学,机器学习,数值分析
- **Abstract**: This review explores the integration of deep learning (DL) with full-waveform inversion (FWI) for enhanced seismic imaging and subsurface characterization. It covers FWI and DL fundamentals, geophysical applications (velocity estimation, deconvolution, tomography), and challenges (model complexity, data quality). The review also outlines future research directions, including hybrid, generative, and physics-informed models for improved accuracy, efficiency, and reliability in subsurface property estimation. The synergy between DL and FWI has the potential to transform geophysics, providing new insights into Earth's subsurface.

## 应用领域(stat.AP:Applications)

### StatLLM: A Dataset for Evaluating the Performance of Large Language Models in Statistical Analysis 
[[arxiv](https://arxiv.org/abs/2502.17657)] [[cool](https://papers.cool/arxiv/2502.17657)] [[pdf](https://arxiv.org/pdf/2502.17657)]
> **Authors**: Xinyi Song,Lina Lee,Kexin Xie,Xueying Liu,Xinwei Deng,Yili Hong
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 25 pages, 7 figures
- **标题**: None
- **领域**: 应用领域,人工智能
- **Abstract**: The coding capabilities of large language models (LLMs) have opened up new opportunities for automatic statistical analysis in machine learning and data science. However, before their widespread adoption, it is crucial to assess the accuracy of code generated by LLMs. A major challenge in this evaluation lies in the absence of a benchmark dataset for statistical code (e.g., SAS and R). To fill in this gap, this paper introduces StatLLM, an open-source dataset for evaluating the performance of LLMs in statistical analysis. The StatLLM dataset comprises three key components: statistical analysis tasks, LLM-generated SAS code, and human evaluation scores. The first component includes statistical analysis tasks spanning a variety of analyses and datasets, providing problem descriptions, dataset details, and human-verified SAS code. The second component features SAS code generated by ChatGPT 3.5, ChatGPT 4.0, and Llama 3.1 for those tasks. The third component contains evaluation scores from human experts in assessing the correctness, effectiveness, readability, executability, and output accuracy of the LLM-generated code. We also illustrate the unique potential of the established benchmark dataset for (1) evaluating and enhancing natural language processing metrics, (2) assessing and improving LLM performance in statistical coding, and (3) developing and testing of next-generation statistical software - advancements that are crucial for data science and machine learning research.

## 方法论(stat.ME:Methodology)

### Uncertainty Quantification for LLM-Based Survey Simulations 
[[arxiv](https://arxiv.org/abs/2502.17773)] [[cool](https://papers.cool/arxiv/2502.17773)] [[pdf](https://arxiv.org/pdf/2502.17773)]
> **Authors**: Chengpiao Huang,Yuhang Wu,Kaizheng Wang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: 30 pages, 6 figures, 10 tables
- **标题**: None
- **领域**: 方法论,人工智能,机器学习
- **Abstract**: We investigate the reliable use of simulated survey responses from large language models (LLMs) through the lens of uncertainty quantification. Our approach converts synthetic data into confidence sets for population parameters of human responses, addressing the distribution shift between the simulated and real populations. A key innovation lies in determining the optimal number of simulated responses: too many produce overly narrow confidence sets with poor coverage, while too few yield excessively loose estimates. To resolve this, our method adaptively selects the simulation sample size, ensuring valid average-case coverage guarantees. It is broadly applicable to any LLM, irrespective of its fidelity, and any procedure for constructing confidence sets. Additionally, the selected sample size quantifies the degree of misalignment between the LLM and the target human population. We illustrate our method on real datasets and LLMs.

## 机器学习(stat.ML:Machine Learning)

### An Overview of Large Language Models for Statisticians 
[[arxiv](https://arxiv.org/abs/2502.17814)] [[cool](https://papers.cool/arxiv/2502.17814)] [[pdf](https://arxiv.org/pdf/2502.17814)]
> **Authors**: Wenlong Ji,Weizhe Yuan,Emily Getzen,Kyunghyun Cho,Michael I. Jordan,Song Mei,Jason E Weston,Weijie J. Su,Jing Xu,Linjun Zhang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,机器学习
- **Abstract**: Large Language Models (LLMs) have emerged as transformative tools in artificial intelligence (AI), exhibiting remarkable capabilities across diverse tasks such as text generation, reasoning, and decision-making. While their success has primarily been driven by advances in computational power and deep learning architectures, emerging problems -- in areas such as uncertainty quantification, decision-making, causal inference, and distribution shift -- require a deeper engagement with the field of statistics. This paper explores potential areas where statisticians can make important contributions to the development of LLMs, particularly those that aim to engender trustworthiness and transparency for human users. Thus, we focus on issues such as uncertainty quantification, interpretability, fairness, privacy, watermarking and model adaptation. We also consider possible roles for LLMs in statistical analysis. By bridging AI and statistics, we aim to foster a deeper collaboration that advances both the theoretical foundations and practical applications of LLMs, ultimately shaping their role in addressing complex societal challenges.

### Conformal Prediction Under Generalized Covariate Shift with Posterior Drift 
[[arxiv](https://arxiv.org/abs/2502.17744)] [[cool](https://papers.cool/arxiv/2502.17744)] [[pdf](https://arxiv.org/pdf/2502.17744)]
> **Authors**: Baozhen Wang,Xingye Qiao
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: Accepted to AISTATS 2025
- **标题**: None
- **领域**: 机器学习,机器学习,统计理论
- **Abstract**: In many real applications of statistical learning, collecting sufficiently many training data is often expensive, time-consuming, or even unrealistic. In this case, a transfer learning approach, which aims to leverage knowledge from a related source domain to improve the learning performance in the target domain, is more beneficial. There have been many transfer learning methods developed under various distributional assumptions. In this article, we study a particular type of classification problem, called conformal prediction, under a new distributional assumption for transfer learning. Classifiers under the conformal prediction framework predict a set of plausible labels instead of one single label for each data instance, affording a more cautious and safer decision. We consider a generalization of the \textit{covariate shift with posterior drift} setting for transfer learning. Under this setting, we propose a weighted conformal classifier that leverages both the source and target samples, with a coverage guarantee in the target domain. Theoretical studies demonstrate favorable asymptotic properties. Numerical studies further illustrate the usefulness of the proposed method.

### Are GNNs doomed by the topology of their input graph? 
[[arxiv](https://arxiv.org/abs/2502.17739)] [[cool](https://papers.cool/arxiv/2502.17739)] [[pdf](https://arxiv.org/pdf/2502.17739)]
> **Authors**: Amine Mohamed Aboussalah,Abdessalam Ed-dib
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-25
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Graph Neural Networks (GNNs) have demonstrated remarkable success in learning from graph-structured data. However, the influence of the input graph's topology on GNN behavior remains poorly understood. In this work, we explore whether GNNs are inherently limited by the structure of their input graphs, focusing on how local topological features interact with the message-passing scheme to produce global phenomena such as oversmoothing or expressive representations. We introduce the concept of $k$-hop similarity and investigate whether locally similar neighborhoods lead to consistent node representations. This interaction can result in either effective learning or inevitable oversmoothing, depending on the inherent properties of the graph. Our empirical experiments validate these insights, highlighting the practical implications of graph topology on GNN performance.

## 其他论文

- [Silent Speech Sentence Recognition with Six-Axis Accelerometers using Conformer and CTC Algorithm](https://arxiv.org/abs/2502.17829)
  - **标题**: None
  - **Filtered Reason**: none of cs.SD,cs.HC,eess.AS in whitelist
- [Novel quantum circuit for image compression utilizing modified Toffoli gate and quantized transformed coefficient alongside a novel reset gate](https://arxiv.org/abs/2502.17815)
  - **标题**: None
  - **Filtered Reason**: none of cs.ET,quant-ph in whitelist
- [Gender Bias in Perception of Human Managers Extends to AI Managers](https://arxiv.org/abs/2502.17730)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [Quantifying interdisciplinary synergy in higher STEM education](https://arxiv.org/abs/2502.17841)
  - **标题**: None
  - **Filtered Reason**: none of cs.IT,physics.soc-ph,physics.ed-ph,cond-mat.stat-mech in whitelist
- [Spectral Efficiency Expression for the Non-Linear Schrödinger Channel in the Low Noise Limit Using Scattering Data](https://arxiv.org/abs/2502.17702)
  - **标题**: None
  - **Filtered Reason**: none of cs.IT,cond-mat.stat-mech in whitelist
- [THOR: A Non-Speculative Value Dependent Timing Side Channel Attack Exploiting Intel AMX](https://arxiv.org/abs/2502.17658)
  - **标题**: None
  - **Filtered Reason**: none of cs.AR,cs.CR in whitelist
- [SET-PAiREd: Designing for Parental Involvement in Learning with an AI-Assisted Educational Robot](https://arxiv.org/abs/2502.17623)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO,cs.HC in whitelist
- [ELMo-Tune-V2: LLM-Assisted Full-Cycle Auto-Tuning to Optimize LSM-Based Key-Value Stores](https://arxiv.org/abs/2502.17606)
  - **标题**: None
  - **Filtered Reason**: none of cs.DB in whitelist
- [Weaving the Cosmos: WASM-Powered Interchain Communication for AI Enabled Smart Contracts](https://arxiv.org/abs/2502.17604)
  - **标题**: None
  - **Filtered Reason**: none of cs.CE,cs.SE in whitelist
