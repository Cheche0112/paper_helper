> 本文由 [https://github.com/huiyeruzhou/arxiv_crawler](https://github.com/huiyeruzhou/arxiv_crawler) 自动生成
>
> 领域白名单：cs.AI,cs.CL,cs.LG,cs.CV
> 关键词： LLM, GPT, AI, language+model, deep+learning, transformer, neural+network, machine+learning

# 论文全览：2025-02-27

共有63篇相关领域论文, 另有12篇其他

## 人工智能(cs.AI:Artificial Intelligence)

### Talking like Piping and Instrumentation Diagrams (P&IDs) 
[[arxiv](https://arxiv.org/abs/2502.18928)] [[cool](https://papers.cool/arxiv/2502.18928)] [[pdf](https://arxiv.org/pdf/2502.18928)]
> **Authors**: Achmad Anggawirya Alimin,Dominik P. Goldstein,Lukas Schulze Balhorn,Artur M. Schweidtmann
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: We propose a methodology that allows communication with Piping and Instrumentation Diagrams (P&IDs) using natural language. In particular, we represent P&IDs through the DEXPI data model as labeled property graphs and integrate them with Large Language Models (LLMs). The approach consists of three main parts: 1) P&IDs are cast into a graph representation from the DEXPI format using our pyDEXPI Python package. 2) A tool for generating P&ID knowledge graphs from pyDEXPI. 3) Integration of the P&ID knowledge graph to LLMs using graph-based retrieval augmented generation (graph-RAG). This approach allows users to communicate with P&IDs using natural language. It extends LLM's ability to retrieve contextual data from P&IDs and mitigate hallucinations. Leveraging the LLM's large corpus, the model is also able to interpret process information in PIDs, which could help engineers in their daily tasks. In the future, this work will also open up opportunities in the context of other generative Artificial Intelligence (genAI) solutions on P&IDs, and AI-assisted HAZOP studies.

### Multi-LLM Collaborative Search for Complex Problem Solving 
[[arxiv](https://arxiv.org/abs/2502.18873)] [[cool](https://papers.cool/arxiv/2502.18873)] [[pdf](https://arxiv.org/pdf/2502.18873)]
> **Authors**: Sen Yang,Yafu Li,Wai Lam,Yu Cheng
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: Large language models (LLMs) often struggle with complex reasoning tasks due to their limitations in addressing the vast reasoning space and inherent ambiguities of natural language. We propose the Mixture-of-Search-Agents (MoSA) paradigm, a novel approach leveraging the collective expertise of multiple LLMs to enhance search-based reasoning. MoSA integrates diverse reasoning pathways by combining independent exploration with iterative refinement among LLMs, mitigating the limitations of single-model approaches. Using Monte Carlo Tree Search (MCTS) as a backbone, MoSA enables multiple agents to propose and aggregate reasoning steps, resulting in improved accuracy. Our comprehensive evaluation across four reasoning benchmarks demonstrates MoSA's consistent performance improvements over single-agent and other multi-agent baselines, particularly in complex mathematical and commonsense reasoning tasks.

### Towards an AI co-scientist 
[[arxiv](https://arxiv.org/abs/2502.18864)] [[cool](https://papers.cool/arxiv/2502.18864)] [[pdf](https://arxiv.org/pdf/2502.18864)]
> **Authors**: Juraj Gottweis,Wei-Hung Weng,Alexander Daryin,Tao Tu,Anil Palepu,Petar Sirkovic,Artiom Myaskovsky,Felix Weissenberger,Keran Rong,Ryutaro Tanno,Khaled Saab,Dan Popovici,Jacob Blum,Fan Zhang,Katherine Chou,Avinatan Hassidim,Burak Gokturk,Amin Vahdat,Pushmeet Kohli,Yossi Matias,Andrew Carroll,Kavita Kulkarni,Nenad Tomasev,Yuan Guan,Vikram Dhillon, et al. (9 additional authors not shown)
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 81 pages in total (main 38 pages, appendix 43 pages), 13 main figures, 40 appendix figures, 1 main table, 2 appendix tables, 143 main references, 7 appendix references
- **标题**: None
- **领域**: 人工智能,计算语言学,人机交互,机器学习,物理与社会,其他定量生物学
- **Abstract**: Scientific discovery relies on scientists generating novel hypotheses that undergo rigorous experimental validation. To augment this process, we introduce an AI co-scientist, a multi-agent system built on Gemini 2.0. The AI co-scientist is intended to help uncover new, original knowledge and to formulate demonstrably novel research hypotheses and proposals, building upon prior evidence and aligned to scientist-provided research objectives and guidance. The system's design incorporates a generate, debate, and evolve approach to hypothesis generation, inspired by the scientific method and accelerated by scaling test-time compute. Key contributions include: (1) a multi-agent architecture with an asynchronous task execution framework for flexible compute scaling; (2) a tournament evolution process for self-improving hypotheses generation. Automated evaluations show continued benefits of test-time compute, improving hypothesis quality. While general purpose, we focus development and validation in three biomedical areas: drug repurposing, novel target discovery, and explaining mechanisms of bacterial evolution and anti-microbial resistance. For drug repurposing, the system proposes candidates with promising validation findings, including candidates for acute myeloid leukemia that show tumor inhibition in vitro at clinically applicable concentrations. For novel target discovery, the AI co-scientist proposed new epigenetic targets for liver fibrosis, validated by anti-fibrotic activity and liver cell regeneration in human hepatic organoids. Finally, the AI co-scientist recapitulated unpublished experimental results via a parallel in silico discovery of a novel gene transfer mechanism in bacterial evolution. These results, detailed in separate, co-timed reports, demonstrate the potential to augment biomedical and scientific discovery and usher an era of AI empowered scientists.

## 计算语言学(cs.CL:Computation and Language)

### PolyPrompt: Automating Knowledge Extraction from Multilingual Language Models with Dynamic Prompt Generation 
[[arxiv](https://arxiv.org/abs/2502.19756)] [[cool](https://papers.cool/arxiv/2502.19756)] [[pdf](https://arxiv.org/pdf/2502.19756)]
> **Authors**: Nathan Roll
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 6 pages, 2 figures
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Large language models (LLMs) showcase increasingly impressive English benchmark scores, however their performance profiles remain inconsistent across multilingual settings. To address this gap, we introduce PolyPrompt, a novel, parameter-efficient framework for enhancing the multilingual capabilities of LLMs. Our method learns a set of trigger tokens for each language through a gradient-based search, identifying the input query's language and selecting the corresponding trigger tokens which are prepended to the prompt during inference. We perform experiments on two ~1 billion parameter models, with evaluations on the global MMLU benchmark across fifteen typologically and resource diverse languages, demonstrating accuracy gains of 3.7%-19.9% compared to naive and translation-pipeline baselines.

### Beneath the Surface: How Large Language Models Reflect Hidden Bias 
[[arxiv](https://arxiv.org/abs/2502.19749)] [[cool](https://papers.cool/arxiv/2502.19749)] [[pdf](https://arxiv.org/pdf/2502.19749)]
> **Authors**: Jinhao Pan,Chahat Raj,Ziyu Yao,Ziwei Zhu
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The exceptional performance of Large Language Models (LLMs) often comes with the unintended propagation of social biases embedded in their training data. While existing benchmarks evaluate overt bias through direct term associations between bias concept terms and demographic terms, LLMs have become increasingly adept at avoiding biased responses, creating an illusion of neutrality. However, biases persist in subtler, contextually hidden forms that traditional benchmarks fail to capture. We introduce the Hidden Bias Benchmark (HBB), a novel dataset designed to assess hidden bias that bias concepts are hidden within naturalistic, subtly framed contexts in real-world scenarios. We analyze six state-of-the-art LLMs, revealing that while models reduce bias in response to overt bias, they continue to reinforce biases in nuanced settings. Data, code, and results are available at https://github.com/JP-25/Hidden-Bias-Benchmark.

### HaLoRA: Hardware-aware Low-Rank Adaptation for Large Language Models Based on Hybrid Compute-in-Memory Architecture 
[[arxiv](https://arxiv.org/abs/2502.19747)] [[cool](https://papers.cool/arxiv/2502.19747)] [[pdf](https://arxiv.org/pdf/2502.19747)]
> **Authors**: Taiqiang Wu,Chenchen Ding,Wenyong Zhou,Yuxin Cheng,Xincheng Feng,Shuqi Wang,Chufan Shi,Zhengwu Liu,Ngai Wong
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 7 pages
- **标题**: None
- **领域**: 计算语言学,硬件架构
- **Abstract**: Low-rank adaptation (LoRA) is a predominant parameter-efficient finetuning method to adapt large language models (LLMs) for downstream tasks. In this paper, we first propose to deploy the LoRA-finetuned LLMs on the hybrid compute-in-memory (CIM) architecture (i.e., pretrained weights onto RRAM and LoRA onto SRAM). To address performance degradation from RRAM's inherent noise, we design a novel Hardware-aware Low-rank Adaption (HaLoRA) method, aiming to train a LoRA branch that is both robust and accurate by aligning the training objectives under both ideal and noisy conditions. Experiments finetuning LLaMA 3.2 1B and 3B demonstrate HaLoRA's effectiveness across multiple reasoning tasks, achieving up to 22.7 improvement in average score while maintaining robustness at various noise levels.

### XCOMPS: A Multilingual Benchmark of Conceptual Minimal Pairs 
[[arxiv](https://arxiv.org/abs/2502.19737)] [[cool](https://papers.cool/arxiv/2502.19737)] [[pdf](https://arxiv.org/pdf/2502.19737)]
> **Authors**: Linyang He,Ercong Nie,Sukru Samet Dindar,Arsalan Firoozi,Adrian Florea,Van Nguyen,Corentin Puffay,Riki Shimizu,Haotian Ye,Jonathan Brennan,Helmut Schmid,Hinrich Schütze,Nima Mesgarani
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: We introduce XCOMPS in this work, a multilingual conceptual minimal pair dataset covering 17 languages. Using this dataset, we evaluate LLMs' multilingual conceptual understanding through metalinguistic prompting, direct probability measurement, and neurolinguistic probing. By comparing base, instruction-tuned, and knowledge-distilled models, we find that: 1) LLMs exhibit weaker conceptual understanding for low-resource languages, and accuracy varies across languages despite being tested on the same concept sets. 2) LLMs excel at distinguishing concept-property pairs that are visibly different but exhibit a marked performance drop when negative pairs share subtle semantic similarities. 3) Instruction tuning improves performance in concept understanding but does not enhance internal competence; knowledge distillation can enhance internal competence in conceptual understanding for low-resource languages with limited gains in explicit task performance. 4) More morphologically complex languages yield lower concept understanding scores and require deeper layers for conceptual reasoning.

### R1-T1: Fully Incentivizing Translation Capability in LLMs via Reasoning Learning 
[[arxiv](https://arxiv.org/abs/2502.19735)] [[cool](https://papers.cool/arxiv/2502.19735)] [[pdf](https://arxiv.org/pdf/2502.19735)]
> **Authors**: Minggui He,Yilun Liu,Shimin Tao,Yuanchang Luo,Hongyong Zeng,Chang Su,Li Zhang,Hongxia Ma,Daimeng Wei,Weibin Meng,Hao Yang,Boxing Chen,Osamu Yoshie
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Despite recent breakthroughs in reasoning-enhanced large language models (LLMs) like DeepSeek-R1, incorporating inference-time reasoning into machine translation (MT), where human translators naturally employ structured, multi-layered reasoning chain-of-thoughts (CoTs), is yet underexplored. Existing methods either design a fixed CoT tailored for a specific MT sub-task (e.g., literature translation), or rely on synthesizing CoTs unaligned with humans and supervised fine-tuning (SFT) prone to catastrophic forgetting, limiting their adaptability to diverse translation scenarios. This paper introduces R1-Translator (R1-T1), a novel framework to achieve inference-time reasoning for general MT via reinforcement learning (RL) with human-aligned CoTs comprising six common patterns. Our approach pioneers three innovations: (1) extending reasoning-based translation beyond MT sub-tasks to six languages and diverse tasks (e.g., legal/medical domain adaptation, idiom resolution); (2) formalizing six expert-curated CoT templates that mirror hybrid human strategies like context-aware paraphrasing and back translation; and (3) enabling self-evolving CoT discovery and anti-forgetting adaptation through RL with KL-constrained rewards. Experimental results indicate a steady translation performance improvement in 21 languages and 80 translation directions on Flores-101 test set, especially on the 15 languages unseen from training, with its general multilingual abilities preserved compared with plain SFT.

### Speculative Decoding and Beyond: An In-Depth Review of Techniques 
[[arxiv](https://arxiv.org/abs/2502.19732)] [[cool](https://papers.cool/arxiv/2502.19732)] [[pdf](https://arxiv.org/pdf/2502.19732)]
> **Authors**: Yunhai Hu,Zining Liu,Zhenyuan Dong,Tianfan Peng,Bradley McDanel,Sai Qian Zhang
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Sequential dependencies present a fundamental bottleneck in deploying large-scale autoregressive models, particularly for real-time applications. While traditional optimization approaches like pruning and quantization often compromise model quality, recent advances in generation-refinement frameworks demonstrate that this trade-off can be significantly mitigated. This survey presents a comprehensive taxonomy of generation-refinement frameworks, analyzing methods across autoregressive sequence tasks. We categorize methods based on their generation strategies (from simple n-gram prediction to sophisticated draft models) and refinement mechanisms (including single-pass verification and iterative approaches). Through systematic analysis of both algorithmic innovations and system-level implementations, we examine deployment strategies across computing environments and explore applications spanning text, images, and speech generation. This systematic examination of both theoretical frameworks and practical implementations provides a foundation for future research in efficient autoregressive decoding.

### Preference Learning Unlocks LLMs' Psycho-Counseling Skills 
[[arxiv](https://arxiv.org/abs/2502.19731)] [[cool](https://papers.cool/arxiv/2502.19731)] [[pdf](https://arxiv.org/pdf/2502.19731)]
> **Authors**: Mian Zhang,Shaun M. Eack,Zhiyu Zoey Chen
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 10 pages, 6 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Applying large language models (LLMs) to assist in psycho-counseling is an emerging and meaningful approach, driven by the significant gap between patient needs and the availability of mental health support. However, current LLMs struggle to consistently provide effective responses to client speeches, largely due to the lack of supervision from high-quality real psycho-counseling data, whose content is typically inaccessible due to client privacy concerns. Furthermore, the quality of therapists' responses in available sessions can vary significantly based on their professional training and experience. Assessing the quality of therapists' responses remains an open challenge. In this work, we address these challenges by first proposing a set of professional and comprehensive principles to evaluate therapists' responses to client speeches. Using these principles, we create a preference dataset, PsychoCounsel-Preference, which contains 36k high-quality preference comparison pairs. This dataset aligns with the preferences of professional psychotherapists, providing a robust foundation for evaluating and improving LLMs in psycho-counseling. Experiments on reward modeling and preference learning demonstrate that PsychoCounsel-Preference is an excellent resource for LLMs to acquire essential skills for responding to clients in a counseling session. Our best-aligned model, PsychoCounsel-Llama3-8B, achieves an impressive win rate of 87% against GPT-4o. We release PsychoCounsel-Preference, PsychoCounsel-Llama3-8B and the reward model PsychoCounsel Llama3-8B-Reward to facilitate the research of psycho-counseling with LLMs at: https://hf.co/Psychotherapy-LLM.

### CNsum:Automatic Summarization for Chinese News Text 
[[arxiv](https://arxiv.org/abs/2502.19723)] [[cool](https://papers.cool/arxiv/2502.19723)] [[pdf](https://arxiv.org/pdf/2502.19723)]
> **Authors**: Yu Zhao,Songping Huang,Dongsheng Zhou,Zhaoyun Ding,Fei Wang,Aixin Nian
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: WASA 2022
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Obtaining valuable information from massive data efficiently has become our research goal in the era of Big Data. Text summarization technology has been continuously developed to meet this demand. Recent work has also shown that transformer-based pre-trained language models have achieved great success on various tasks in Natural Language Processing (NLP). Aiming at the problem of Chinese news text summary generation and the application of Transformer structure on Chinese, this paper proposes a Chinese news text summarization model (CNsum) based on Transformer structure, and tests it on Chinese datasets such as THUCNews. The results of the conducted experiments show that CNsum achieves better ROUGE score than the baseline models, which verifies the outperformance of the model.

### Few-Shot Multilingual Open-Domain QA from 5 Examples 
[[arxiv](https://arxiv.org/abs/2502.19722)] [[cool](https://papers.cool/arxiv/2502.19722)] [[pdf](https://arxiv.org/pdf/2502.19722)]
> **Authors**: Fan Jiang,Tom Drummond,Trevor Cohn
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Accepted by TACL; pre-MIT Press publication version
- **标题**: None
- **领域**: 计算语言学,信息检索
- **Abstract**: Recent approaches to multilingual open-domain question answering (MLODQA) have achieved promising results given abundant language-specific training data. However, the considerable annotation cost limits the application of these methods for underrepresented languages. We introduce a \emph{few-shot learning} approach to synthesise large-scale multilingual data from large language models (LLMs). Our method begins with large-scale self-supervised pre-training using WikiData, followed by training on high-quality synthetic multilingual data generated by prompting LLMs with few-shot supervision. The final model, \textsc{FsModQA}, significantly outperforms existing few-shot and supervised baselines in MLODQA and cross-lingual and monolingual retrieval. We further show our method can be extended for effective zero-shot adaptation to new languages through a \emph{cross-lingual prompting} strategy with only English-supervised data, making it a general and applicable solution for MLODQA tasks without costly large-scale annotation.

### Sensing and Steering Stereotypes: Extracting and Applying Gender Representation Vectors in LLMs 
[[arxiv](https://arxiv.org/abs/2502.19721)] [[cool](https://papers.cool/arxiv/2502.19721)] [[pdf](https://arxiv.org/pdf/2502.19721)]
> **Authors**: Hannah Cyberey,Yangfeng Ji,David Evans
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,计算机与社会
- **Abstract**: Large language models (LLMs) are known to perpetuate stereotypes and exhibit biases. Various strategies have been proposed to mitigate potential harms that may result from these biases, but most work studies biases in LLMs as a black-box problem without considering how concepts are represented within the model. We adapt techniques from representation engineering to study how the concept of "gender" is represented within LLMs. We introduce a new method that extracts concept representations via probability weighting without labeled data and efficiently selects a steering vector for measuring and manipulating the model's representation. We also present a projection-based method that enables precise steering of model predictions and demonstrate its effectiveness in mitigating gender bias in LLMs.

### GenTool: Enhancing Tool Generalization in Language Models through Zero-to-One and Weak-to-Strong Simulation 
[[arxiv](https://arxiv.org/abs/2502.18990)] [[cool](https://papers.cool/arxiv/2502.18990)] [[pdf](https://arxiv.org/pdf/2502.18990)]
> **Authors**: Jie He,Jennifer Neville,Mengting Wan,Longqi Yang,Hui Liu,Xiaofeng Xu,Xia Song,Jeff Z. Pan,Pei Zhou
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models (LLMs) can enhance their capabilities as AI assistants by integrating external tools, allowing them to access a wider range of information. While recent LLMs are typically fine-tuned with tool usage examples during supervised fine-tuning (SFT), questions remain about their ability to develop robust tool-usage skills and can effectively generalize to unseen queries and tools. In this work, we present GenTool, a novel training framework that prepares LLMs for diverse generalization challenges in tool utilization. Our approach addresses two fundamental dimensions critical for real-world applications: Zero-to-One Generalization, enabling the model to address queries initially lacking a suitable tool by adopting and utilizing one when it becomes available, and Weak-to-Strong Generalization, allowing models to leverage enhanced versions of existing tools to solve queries. To achieve this, we develop synthetic training data simulating these two dimensions of tool usage and introduce a two-stage fine-tuning approach: optimizing tool ranking, then refining tool selection. Through extensive experiments across four generalization scenarios, we demonstrate that our method significantly enhances the tool-usage capabilities of LLMs ranging from 1B to 8B parameters, achieving performance that surpasses GPT-4o. Furthermore, our analysis also provides valuable insights into the challenges LLMs encounter in tool generalization.

### PEToolLLM: Towards Personalized Tool Learning in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.18980)] [[cool](https://papers.cool/arxiv/2502.18980)] [[pdf](https://arxiv.org/pdf/2502.18980)]
> **Authors**: Qiancheng Xu,Yongqi Li,Heming Xia,Fan Liu,Min Yang,Wenjie Li
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Tool learning has emerged as a promising direction by extending Large Language Models' (LLMs) capabilities with external tools. Existing tool learning studies primarily focus on the general-purpose tool-use capability, which addresses explicit user requirements in instructions. However, they overlook the importance of personalized tool-use capability, leading to an inability to handle implicit user preferences. To address the limitation, we first formulate the task of personalized tool learning, which integrates user's interaction history towards personalized tool usage. To fill the gap of missing benchmarks, we construct PEToolBench, featuring diverse user preferences reflected in interaction history under three distinct personalized settings, and encompassing a wide range of tool-use scenarios. Moreover, we propose a framework PEToolLLaMA to adapt LLMs to the personalized tool learning task, which is trained through supervised fine-tuning and direct preference optimization. Extensive experiments on PEToolBench demonstrate the superiority of PEToolLLaMA over existing LLMs.

### Low-Confidence Gold: Refining Low-Confidence Samples for Efficient Instruction Tuning 
[[arxiv](https://arxiv.org/abs/2502.18978)] [[cool](https://papers.cool/arxiv/2502.18978)] [[pdf](https://arxiv.org/pdf/2502.18978)]
> **Authors**: Hongyi Cal,Jie Li,Wenzhen Dong
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 8 pages
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: The effectiveness of instruction fine-tuning for Large Language Models is fundamentally constrained by the quality and efficiency of training datasets. This work introduces Low-Confidence Gold (LCG), a novel filtering framework that employs centroid-based clustering and confidence-guided selection for identifying valuable instruction pairs. Through a semi-supervised approach using a lightweight classifier trained on representative samples, LCG curates high-quality subsets while preserving data diversity. Experimental evaluation demonstrates that models fine-tuned on LCG-filtered subsets of 6K samples achieve superior performance compared to existing methods, with substantial improvements on MT-bench and consistent gains across comprehensive evaluation metrics. The framework's efficacy while maintaining model performance establishes a promising direction for efficient instruction tuning.

### Know You First and Be You Better: Modeling Human-Like User Simulators via Implicit Profiles 
[[arxiv](https://arxiv.org/abs/2502.18968)] [[cool](https://papers.cool/arxiv/2502.18968)] [[pdf](https://arxiv.org/pdf/2502.18968)]
> **Authors**: Kuang Wang,Xianfei Li,Shenghao Yang,Li Zhou,Feng Jiang,Haizhou Li
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 9 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: User simulators are crucial for replicating human interactions with dialogue systems, supporting both collaborative training and automatic evaluation, especially for large language models (LLMs). However, existing simulators often rely solely on text utterances, missing implicit user traits such as personality, speaking style, and goals. In contrast, persona-based methods lack generalizability, as they depend on predefined profiles of famous individuals or archetypes. To address these challenges, we propose User Simulator with implicit Profiles (USP), a framework that infers implicit user profiles from human-machine conversations and uses them to generate more personalized and realistic dialogues. We first develop an LLM-driven extractor with a comprehensive profile schema. Then, we refine the simulation through conditional supervised fine-tuning and reinforcement learning with cycle consistency, optimizing it at both the utterance and conversation levels. Finally, we adopt a diverse profile sampler to capture the distribution of real-world user profiles. Experimental results demonstrate that USP outperforms strong baselines in terms of authenticity and diversity while achieving comparable performance in consistency. Furthermore, dynamic multi-turn evaluations based on USP strongly align with mainstream benchmarks, demonstrating its effectiveness in real-world applications.

### MathTutorBench: A Benchmark for Measuring Open-ended Pedagogical Capabilities of LLM Tutors 
[[arxiv](https://arxiv.org/abs/2502.18940)] [[cool](https://papers.cool/arxiv/2502.18940)] [[pdf](https://arxiv.org/pdf/2502.18940)]
> **Authors**: Jakub Macina,Nico Daheim,Ido Hakimi,Manu Kapur,Iryna Gurevych,Mrinmaya Sachan
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: https://eth-lre.github.io/mathtutorbench
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Evaluating the pedagogical capabilities of AI-based tutoring models is critical for making guided progress in the field. Yet, we lack a reliable, easy-to-use, and simple-to-run evaluation that reflects the pedagogical abilities of models. To fill this gap, we present MathTutorBench, an open-source benchmark for holistic tutoring model evaluation. MathTutorBench contains a collection of datasets and metrics that broadly cover tutor abilities as defined by learning sciences research in dialog-based teaching. To score the pedagogical quality of open-ended teacher responses, we train a reward model and show it can discriminate expert from novice teacher responses with high accuracy. We evaluate a wide set of closed- and open-weight models on MathTutorBench and find that subject expertise, indicated by solving ability, does not immediately translate to good teaching. Rather, pedagogy and subject expertise appear to form a trade-off that is navigated by the degree of tutoring specialization of the model. Furthermore, tutoring appears to become more challenging in longer dialogs, where simpler questioning strategies begin to fail. We release the benchmark, code, and leaderboard openly to enable rapid benchmarking of future models.

### JailBench: A Comprehensive Chinese Security Assessment Benchmark for Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.18935)] [[cool](https://papers.cool/arxiv/2502.18935)] [[pdf](https://arxiv.org/pdf/2502.18935)]
> **Authors**: Shuyi Liu,Simiao Cui,Haoran Bu,Yuming Shang,Xi Zhang
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 12 pages, 5 figures, accepted at PAKDD 2025
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large language models (LLMs) have demonstrated remarkable capabilities across various applications, highlighting the urgent need for comprehensive safety evaluations. In particular, the enhanced Chinese language proficiency of LLMs, combined with the unique characteristics and complexity of Chinese expressions, has driven the emergence of Chinese-specific benchmarks for safety assessment. However, these benchmarks generally fall short in effectively exposing LLM safety vulnerabilities. To address the gap, we introduce JailBench, the first comprehensive Chinese benchmark for evaluating deep-seated vulnerabilities in LLMs, featuring a refined hierarchical safety taxonomy tailored to the Chinese context. To improve generation efficiency, we employ a novel Automatic Jailbreak Prompt Engineer (AJPE) framework for JailBench construction, which incorporates jailbreak techniques to enhance assessing effectiveness and leverages LLMs to automatically scale up the dataset through context-learning. The proposed JailBench is extensively evaluated over 13 mainstream LLMs and achieves the highest attack success rate against ChatGPT compared to existing Chinese benchmarks, underscoring its efficacy in identifying latent vulnerabilities in LLMs, as well as illustrating the substantial room for improvement in the security and trustworthiness of LLMs within the Chinese context. Our benchmark is publicly available at https://github.com/STAIR-BUPT/JailBench.

### Kanana: Compute-efficient Bilingual Language Models 
[[arxiv](https://arxiv.org/abs/2502.18934)] [[cool](https://papers.cool/arxiv/2502.18934)] [[pdf](https://arxiv.org/pdf/2502.18934)]
> **Authors**: Kanana LLM Team,Yunju Bak,Hojin Lee,Minho Ryu,Jiyeon Ham,Seungjae Jung,Daniel Wontae Nam,Taegyeong Eo,Donghun Lee,Doohae Jung,Boseop Kim,Nayeon Kim,Jaesun Park,Hyunho Kim,Hyunwoong Ko,Changmin Lee,Kyoung-Woon On,Seulye Baeg,Junrae Cho,Sunghee Jung,Jieun Kang,EungGyun Kim,Eunhwa Kim,Byeongil Ko,Daniel Lee, et al. (4 additional authors not shown)
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 40 pages, 15 figures
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: We introduce Kanana, a series of bilingual language models that demonstrate exceeding performance in Korean and competitive performance in English. The computational cost of Kanana is significantly lower than that of state-of-the-art models of similar size. The report details the techniques employed during pre-training to achieve compute-efficient yet competitive models, including high quality data filtering, staged pre-training, depth up-scaling, and pruning and distillation. Furthermore, the report outlines the methodologies utilized during the post-training of the Kanana models, encompassing supervised fine-tuning and preference optimization, aimed at enhancing their capability for seamless interaction with users. Lastly, the report elaborates on plausible approaches used for language model adaptation to specific scenarios, such as embedding, retrieval augmented generation, and function calling. The Kanana model series spans from 2.1B to 32.5B parameters with 2.1B models (base, instruct, embedding) publicly released to promote research on Korean language models.

### END: Early Noise Dropping for Efficient and Effective Context Denoising 
[[arxiv](https://arxiv.org/abs/2502.18915)] [[cool](https://papers.cool/arxiv/2502.18915)] [[pdf](https://arxiv.org/pdf/2502.18915)]
> **Authors**: Hongye Jin,Pei Chen,Jingfeng Yang,Zhengyang Wang,Meng Jiang,Yifan Gao,Binxuan Huang,Xinyang Zhang,Zheng Li,Tianyi Liu,Huasheng Li,Bing Yin
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, they are often distracted by irrelevant or noisy context in input sequences that degrades output quality. This problem affects both long- and short-context scenarios, such as retrieval-augmented generation, table question-answering, and in-context learning. We reveal that LLMs can implicitly identify whether input sequences contain useful information at early layers, prior to token generation. Leveraging this insight, we introduce Early Noise Dropping (\textsc{END}), a novel approach to mitigate this issue without requiring fine-tuning the LLMs. \textsc{END} segments input sequences into chunks and employs a linear prober on the early layers of LLMs to differentiate between informative and noisy chunks. By discarding noisy chunks early in the process, \textsc{END} preserves critical information, reduces distraction, and lowers computational overhead. Extensive experiments demonstrate that \textsc{END} significantly improves both performance and efficiency across different LLMs on multiple evaluation datasets. Furthermore, by investigating LLMs' implicit understanding to the input with the prober, this work also deepens understanding of how LLMs do reasoning with contexts internally.

### CS-Dialogue: A 104-Hour Dataset of Spontaneous Mandarin-English Code-Switching Dialogues for Speech Recognition 
[[arxiv](https://arxiv.org/abs/2502.18913)] [[cool](https://papers.cool/arxiv/2502.18913)] [[pdf](https://arxiv.org/pdf/2502.18913)]
> **Authors**: Jiaming Zhou,Yujie Guo,Shiwan Zhao,Haoqin Sun,Hui Wang,Jiabei He,Aobo Kong,Shiyao Wang,Xi Yang,Yequan Wang,Yonghua Lin,Yong Qin
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,声音,音频和语音处理
- **Abstract**: Code-switching (CS), the alternation between two or more languages within a single conversation, presents significant challenges for automatic speech recognition (ASR) systems. Existing Mandarin-English code-switching datasets often suffer from limitations in size, spontaneity, and the lack of full-length dialogue recordings with transcriptions, hindering the development of robust ASR models for real-world conversational scenarios. This paper introduces CS-Dialogue, a novel large-scale Mandarin-English code-switching speech dataset comprising 104 hours of spontaneous conversations from 200 speakers. Unlike previous datasets, CS-Dialogue provides full-length dialogue recordings with complete transcriptions, capturing naturalistic code-switching patterns in continuous speech. We describe the data collection and annotation processes, present detailed statistics of the dataset, and establish benchmark ASR performance using state-of-the-art models. Our experiments, using Transformer, Conformer, and Branchformer, demonstrate the challenges of code-switching ASR, and show that existing pre-trained models such as Whisper still have the space to improve. The CS-Dialogue dataset will be made freely available for all academic purposes.

### From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens 
[[arxiv](https://arxiv.org/abs/2502.18890)] [[cool](https://papers.cool/arxiv/2502.18890)] [[pdf](https://arxiv.org/pdf/2502.18890)]
> **Authors**: Tong Wu,Junzhe Shen,Zixia Jia,Yuxuan Wang,Zilong Zheng
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Generating ultra-long sequences with large language models (LLMs) has become increasingly crucial but remains a highly time-intensive task, particularly for sequences up to 100K tokens. While traditional speculative decoding methods exist, simply extending their generation limits fails to accelerate the process and can be detrimental. Through an in-depth analysis, we identify three major challenges hindering efficient generation: frequent model reloading, dynamic key-value (KV) management and repetitive generation. To address these issues, we introduce TOKENSWIFT, a novel framework designed to substantially accelerate the generation process of ultra-long sequences while maintaining the target model's inherent quality. Experimental results demonstrate that TOKENSWIFT achieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B, 14B) and architectures (MHA, GQA). This acceleration translates to hours of time savings for ultra-long sequence generation, establishing TOKENSWIFT as a scalable and effective solution at unprecedented lengths. Code can be found at https://github.com/bigai-nlco/TokenSwift.

### On Pruning State-Space LLMs 
[[arxiv](https://arxiv.org/abs/2502.18886)] [[cool](https://papers.cool/arxiv/2502.18886)] [[pdf](https://arxiv.org/pdf/2502.18886)]
> **Authors**: Tamer Ghattas,Michael Hassid,Roy Schwartz
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Recent work proposed state-space models (SSMs) as an efficient alternative to transformer-based LLMs. Can these models be pruned to further reduce their computation costs? We adapt several pruning methods to the SSM structure, and apply them to four SSM-based LLMs across multiple tasks. We find that such models are quite robust to some pruning methods (e.g. WANDA), while using other methods lead to fast performance degradation.

### Learning to Generate Structured Output with Schema Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.18878)] [[cool](https://papers.cool/arxiv/2502.18878)] [[pdf](https://arxiv.org/pdf/2502.18878)]
> **Authors**: Yaxi Lu,Haolun Li,Xin Cong,Zhong Zhang,Yesai Wu,Yankai Lin,Zhiyuan Liu,Fangming Liu,Maosong Sun
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 8 pages, 4 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: This study investigates the structured generation capabilities of large language models (LLMs), focusing on producing valid JSON outputs against a given schema. Despite the widespread use of JSON in integrating language models with programs, there is a lack of comprehensive analysis and benchmarking of these capabilities. We explore various aspects of JSON generation, such as structure understanding, escaping, and natural language description, to determine how to assess and enable LLMs to generate valid responses. Building upon this, we propose SchemaBench features around 40K different JSON schemas to obtain and assess models' abilities in generating valid JSON. We find that the latest LLMs are still struggling to generate a valid JSON string. Moreover, we demonstrate that incorporating reinforcement learning with a Fine-grained Schema Validator can further enhance models' understanding of JSON schema, leading to improved performance. Our models demonstrate significant improvement in both generating JSON outputs and downstream tasks.

### Learning to Align Multi-Faceted Evaluation: A Unified and Robust Framework 
[[arxiv](https://arxiv.org/abs/2502.18874)] [[cool](https://papers.cool/arxiv/2502.18874)] [[pdf](https://arxiv.org/pdf/2502.18874)]
> **Authors**: Kaishuai Xu,Tiezheng Yu,Wenjun Hou,Yi Cheng,Liangyou Li,Xin Jiang,Lifeng Shang,Qun Liu,Wenjie Li
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large Language Models (LLMs) are being used more and more extensively for automated evaluation in various scenarios. Previous studies have attempted to fine-tune open-source LLMs to replicate the evaluation explanations and judgments of powerful proprietary models, such as GPT-4. However, these methods are largely limited to text-based analyses under predefined general criteria, resulting in reduced adaptability for unseen instructions and demonstrating instability in evaluating adherence to quantitative and structural constraints. To address these limitations, we propose a novel evaluation framework, ARJudge, that adaptively formulates evaluation criteria and synthesizes both text-based and code-driven analyses to evaluate LLM responses. ARJudge consists of two components: a fine-tuned Analyzer that generates multi-faceted evaluation analyses and a tuning-free Refiner that combines and refines all analyses to make the final judgment. We construct a Composite Analysis Corpus that integrates tasks for evaluation criteria generation alongside text-based and code-driven analysis generation to train the Analyzer. Our results demonstrate that ARJudge outperforms existing fine-tuned evaluators in effectiveness and robustness. Furthermore, it demonstrates the importance of multi-faceted evaluation and code-driven analyses in enhancing evaluation capabilities.

## 密码学和安全(cs.CR:Cryptography and Security)

### Evaluating Membership Inference Attacks in heterogeneous-data setups 
[[arxiv](https://arxiv.org/abs/2502.18986)] [[cool](https://papers.cool/arxiv/2502.18986)] [[pdf](https://arxiv.org/pdf/2502.18986)]
> **Authors**: Bram van Dartel,Marc Damie,Florian Hahn
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Accepted in SiMLA workshop 2025 (co-located with ACNS)
- **标题**: None
- **领域**: 密码学和安全,机器学习
- **Abstract**: Among all privacy attacks against Machine Learning (ML), membership inference attacks (MIA) attracted the most attention. In these attacks, the attacker is given an ML model and a data point, and they must infer whether the data point was used for training. The attacker also has an auxiliary dataset to tune their inference algorithm. Attack papers commonly simulate setups in which the attacker's and the target's datasets are sampled from the same distribution. This setting is convenient to perform experiments, but it rarely holds in practice. ML literature commonly starts with similar simplifying assumptions (i.e., "i.i.d." datasets), and later generalizes the results to support heterogeneous data distributions. Similarly, our work makes a first step in the generalization of the MIA evaluation to heterogeneous data. First, we design a metric to measure the heterogeneity between any pair of tabular data distributions. This metric provides a continuous scale to analyze the phenomenon. Second, we compare two methodologies to simulate a data heterogeneity between the target and the attacker. These setups provide opposite performances: 90% attack accuracy vs. 50% (i.e., random guessing). Our results show that the MIA accuracy depends on the experimental setup; and even if research on MIA considers heterogeneous data setups, we have no standardized baseline of how to simulate it. The lack of such a baseline for MIA experiments poses a significant challenge to risk assessments in real-world machine learning scenarios.

### Towards Label-Only Membership Inference Attack against Pre-trained Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.18943)] [[cool](https://papers.cool/arxiv/2502.18943)] [[pdf](https://arxiv.org/pdf/2502.18943)]
> **Authors**: Yu He,Boheng Li,Liu Liu,Zhongjie Ba,Wei Dong,Yiming Li,Zhan Qin,Kui Ren,Chun Chen
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Accepted by USENIX Security 2025
- **标题**: None
- **领域**: 密码学和安全,计算语言学
- **Abstract**: Membership Inference Attacks (MIAs) aim to predict whether a data sample belongs to the model's training set or not. Although prior research has extensively explored MIAs in Large Language Models (LLMs), they typically require accessing to complete output logits (\ie, \textit{logits-based attacks}), which are usually not available in practice. In this paper, we study the vulnerability of pre-trained LLMs to MIAs in the \textit{label-only setting}, where the adversary can only access generated tokens (text). We first reveal that existing label-only MIAs have minor effects in attacking pre-trained LLMs, although they are highly effective in inferring fine-tuning datasets used for personalized LLMs. We find that their failure stems from two main reasons, including better generalization and overly coarse perturbation. Specifically, due to the extensive pre-training corpora and exposing each sample only a few times, LLMs exhibit minimal robustness differences between members and non-members. This makes token-level perturbations too coarse to capture such differences. To alleviate these problems, we propose \textbf{PETAL}: a label-only membership inference attack based on \textbf{PE}r-\textbf{T}oken sem\textbf{A}ntic simi\textbf{L}arity. Specifically, PETAL leverages token-level semantic similarity to approximate output probabilities and subsequently calculate the perplexity. It finally exposes membership based on the common assumption that members are `better' memorized and have smaller perplexity. We conduct extensive experiments on the WikiMIA benchmark and the more challenging MIMIR benchmark. Empirically, our PETAL performs better than the extensions of existing label-only attacks against personalized LLMs and even on par with other advanced logit-based attacks across all metrics on five prevalent open-source LLMs.

## 计算机视觉和模式识别(cs.CV:Computer Vision and Pattern Recognition)

### Snowball Adversarial Attack on Traffic Sign Classification 
[[arxiv](https://arxiv.org/abs/2502.19757)] [[cool](https://papers.cool/arxiv/2502.19757)] [[pdf](https://arxiv.org/pdf/2502.19757)]
> **Authors**: Anthony Etim,Jakub Szefer
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,密码学和安全
- **Abstract**: Adversarial attacks on machine learning models often rely on small, imperceptible perturbations to mislead classifiers. Such strategy focuses on minimizing the visual perturbation for humans so they are not confused, and also maximizing the misclassification for machine learning algorithms. An orthogonal strategy for adversarial attacks is to create perturbations that are clearly visible but do not confuse humans, yet still maximize misclassification for machine learning algorithms. This work follows the later strategy, and demonstrates instance of it through the Snowball Adversarial Attack in the context of traffic sign recognition. The attack leverages the human brain's superior ability to recognize objects despite various occlusions, while machine learning algorithms are easily confused. The evaluation shows that the Snowball Adversarial Attack is robust across various images and is able to confuse state-of-the-art traffic sign recognition algorithm. The findings reveal that Snowball Adversarial Attack can significantly degrade model performance with minimal effort, raising important concerns about the vulnerabilities of deep neural networks and highlighting the necessity for improved defenses for image recognition machine learning models.

### Lightweight Contrastive Distilled Hashing for Online Cross-modal Retrieval 
[[arxiv](https://arxiv.org/abs/2502.19751)] [[cool](https://papers.cool/arxiv/2502.19751)] [[pdf](https://arxiv.org/pdf/2502.19751)]
> **Authors**: Jiaxing Li,Lin Jiang,Zeqi Ma,Kaihang Jiang,Xiaozhao Fang,Jie Wen
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Deep online cross-modal hashing has gained much attention from researchers recently, as its promising applications with low storage requirement, fast retrieval efficiency and cross modality adaptive, etc. However, there still exists some technical hurdles that hinder its applications, e.g., 1) how to extract the coexistent semantic relevance of cross-modal data, 2) how to achieve competitive performance when handling the real time data streams, 3) how to transfer the knowledge learned from offline to online training in a lightweight manner. To address these problems, this paper proposes a lightweight contrastive distilled hashing (LCDH) for cross-modal retrieval, by innovatively bridging the offline and online cross-modal hashing by similarity matrix approximation in a knowledge distillation framework. Specifically, in the teacher network, LCDH first extracts the cross-modal features by the contrastive language-image pre-training (CLIP), which are further fed into an attention module for representation enhancement after feature fusion. Then, the output of the attention module is fed into a FC layer to obtain hash codes for aligning the sizes of similarity matrices for online and offline training. In the student network, LCDH extracts the visual and textual features by lightweight models, and then the features are fed into a FC layer to generate binary codes. Finally, by approximating the similarity matrices, the performance of online hashing in the lightweight student network can be enhanced by the supervision of coexistent semantic relevance that is distilled from the teacher network. Experimental results on three widely used datasets demonstrate that LCDH outperforms some state-of-the-art methods.

### Recent Advances on Generalizable Diffusion-generated Image Detection 
[[arxiv](https://arxiv.org/abs/2502.19716)] [[cool](https://papers.cool/arxiv/2502.19716)] [[pdf](https://arxiv.org/pdf/2502.19716)]
> **Authors**: Qijie Xu,Defang Chen,Jiawei Chen,Siwei Lyu,Can Wang
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: The rise of diffusion models has significantly improved the fidelity and diversity of generated images. With numerous benefits, these advancements also introduce new risks. Diffusion models can be exploited to create high-quality Deepfake images, which poses challenges for image authenticity verification. In recent years, research on generalizable diffusion-generated image detection has grown rapidly. However, a comprehensive review of this topic is still lacking. To bridge this gap, we present a systematic survey of recent advances and classify them into two main categories: (1) data-driven detection and (2) feature-driven detection. Existing detection methods are further classified into six fine-grained categories based on their underlying principles. Finally, we identify several open challenges and envision some future directions, with the hope of inspiring more research work on this important topic. Reviewed works in this survey can be found at https://github.com/zju-pi/Awesome-Diffusion-generated-Image-Detection.

### Enhanced Neuromorphic Semantic Segmentation Latency through Stream Event 
[[arxiv](https://arxiv.org/abs/2502.18982)] [[cool](https://papers.cool/arxiv/2502.18982)] [[pdf](https://arxiv.org/pdf/2502.18982)]
> **Authors**: D. Hareb,J. Martinet,B. Miramond
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Achieving optimal semantic segmentation with frame-based vision sensors poses significant challenges for real-time systems like UAVs and self-driving cars, which require rapid and precise processing. Traditional frame-based methods often struggle to balance latency, accuracy, and energy efficiency. To address these challenges, we leverage event streams from event-based cameras-bio-inspired sensors that trigger events in response to changes in the scene. Specifically, we analyze the number of events triggered between successive frames, with a high number indicating significant changes and a low number indicating minimal changes. We exploit this event information to solve the semantic segmentation task by employing a Spiking Neural Network (SNN), a bio-inspired computing paradigm known for its low energy consumption. Our experiments on the DSEC dataset show that our approach significantly reduces latency with only a limited drop in accuracy. Additionally, by using SNNs, we achieve low power consumption, making our method suitable for energy-constrained real-time applications. To the best of our knowledge, our approach is the first to effectively balance reduced latency, minimal accuracy loss, and energy efficiency using events stream to enhance semantic segmentation in dynamic and resource-limited environments.

### Brain-inspired analogical mixture prototypes for few-shot class-incremental learning 
[[arxiv](https://arxiv.org/abs/2502.18923)] [[cool](https://papers.cool/arxiv/2502.18923)] [[pdf](https://arxiv.org/pdf/2502.18923)]
> **Authors**: Wanyi Li,Wei Wei,Yongkang Luo,Peng Wang
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: under review
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习,图像和视频处理
- **Abstract**: Few-shot class-incremental learning (FSCIL) poses significant challenges for artificial neural networks due to the need to efficiently learn from limited data while retaining knowledge of previously learned tasks. Inspired by the brain's mechanisms for categorization and analogical learning, we propose a novel approach called Brain-inspired Analogical Mixture Prototypes (BAMP). BAMP has three components: mixed prototypical feature learning, statistical analogy, and soft voting. Starting from a pre-trained Vision Transformer (ViT), mixed prototypical feature learning represents each class using a mixture of prototypes and fine-tunes these representations during the base session. The statistical analogy calibrates the mean and covariance matrix of prototypes for new classes according to similarity to the base classes, and computes classification score with Mahalanobis distance. Soft voting combines both merits of statistical analogy and an off-shelf FSCIL method. Our experiments on benchmark datasets demonstrate that BAMP outperforms state-of-the-art on both traditional big start FSCIL setting and challenging small start FSCIL setting. The study suggests that brain-inspired analogical mixture prototypes can alleviate catastrophic forgetting and over-fitting problems in FSCIL.

### Enhanced Transformer-Based Tracking for Skiing Events: Overcoming Multi-Camera Challenges, Scale Variations and Rapid Motion -- SkiTB Visual Tracking Challenge 2025 
[[arxiv](https://arxiv.org/abs/2502.18867)] [[cool](https://papers.cool/arxiv/2502.18867)] [[pdf](https://arxiv.org/pdf/2502.18867)]
> **Authors**: Akhil Penta,Vaibhav Adwani,Ankush Chopra
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Accurate skier tracking is essential for performance analysis, injury prevention, and optimizing training strategies in alpine sports. Traditional tracking methods often struggle with occlusions, dynamic movements, and varying environmental conditions, limiting their effectiveness. In this work, we used STARK (Spatio-Temporal Transformer Network for Visual Tracking), a transformer-based model, to track skiers. We adapted STARK to address domain-specific challenges such as camera movements, camera changes, occlusions, etc. by optimizing the model's architecture and hyperparameters to better suit the dataset.

### Sherlock: Towards Multi-scene Video Abnormal Event Extraction and Localization via a Global-local Spatial-sensitive LLM 
[[arxiv](https://arxiv.org/abs/2502.18863)] [[cool](https://papers.cool/arxiv/2502.18863)] [[pdf](https://arxiv.org/pdf/2502.18863)]
> **Authors**: Junxiao Ma,Jingjing Wang,Jiamin Luo,Peiying Yu,Guodong Zhou
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Prior studies on Video Anomaly Detection (VAD) mainly focus on detecting whether each video frame is abnormal or not in the video, which largely ignore the structured video semantic information (i.e., what, when, and where does the abnormal event happen). With this in mind, we propose a new chat-paradigm \textbf{M}ulti-scene Video Abnormal Event Extraction and Localization (M-VAE) task, aiming to extract the abnormal event quadruples (i.e., subject, event type, object, scene) and localize such event. Further, this paper believes that this new task faces two key challenges, i.e., global-local spatial modeling and global-local spatial balancing. To this end, this paper proposes a Global-local Spatial-sensitive Large Language Model (LLM) named Sherlock, i.e., acting like Sherlock Holmes to track down the criminal events, for this M-VAE task. Specifically, this model designs a Global-local Spatial-enhanced MoE (GSM) module and a Spatial Imbalance Regulator (SIR) to address the two challenges respectively. Extensive experiments on our M-VAE instruction dataset show the significant advantages of Sherlock over several advanced Video-LLMs. This justifies the importance of global-local spatial information for the M-VAE task and the effectiveness of Sherlock in capturing such information.

## 机器学习(cs.LG:Machine Learning)

### Learning with Exact Invariances in Polynomial Time 
[[arxiv](https://arxiv.org/abs/2502.19758)] [[cool](https://papers.cool/arxiv/2502.19758)] [[pdf](https://arxiv.org/pdf/2502.19758)]
> **Authors**: Ashkan Soleymani,Behrooz Tahmasebi,Stefanie Jegelka,Patrick Jaillet
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: We study the statistical-computational trade-offs for learning with exact invariances (or symmetries) using kernel regression. Traditional methods, such as data augmentation, group averaging, canonicalization, and frame-averaging, either fail to provide a polynomial-time solution or are not applicable in the kernel setting. However, with oracle access to the geometric properties of the input space, we propose a polynomial-time algorithm that learns a classifier with \emph{exact} invariances. Moreover, our approach achieves the same excess population risk (or generalization error) as the original kernel regression problem. To the best of our knowledge, this is the first polynomial-time algorithm to achieve exact (not approximate) invariances in this context. Our proof leverages tools from differential geometry, spectral theory, and optimization. A key result in our development is a new reformulation of the problem of learning under invariances as optimizing an infinite number of linearly constrained convex quadratic programs, which may be of independent interest.

### HALO: Robust Out-of-Distribution Detection via Joint Optimisation 
[[arxiv](https://arxiv.org/abs/2502.19755)] [[cool](https://papers.cool/arxiv/2502.19755)] [[pdf](https://arxiv.org/pdf/2502.19755)]
> **Authors**: Hugo Lyons Keenan,Sarah Erfani,Christopher Leckie
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: SaTML 2025
- **标题**: None
- **领域**: 机器学习,密码学和安全
- **Abstract**: Effective out-of-distribution (OOD) detection is crucial for the safe deployment of machine learning models in real-world scenarios. However, recent work has shown that OOD detection methods are vulnerable to adversarial attacks, potentially leading to critical failures in high-stakes applications. This discovery has motivated work on robust OOD detection methods that are capable of maintaining performance under various attack settings. Prior approaches have made progress on this problem but face a number of limitations: often only exhibiting robustness to attacks on OOD data or failing to maintain strong clean performance. In this work, we adapt an existing robust classification framework, TRADES, extending it to the problem of robust OOD detection and discovering a novel objective function. Recognising the critical importance of a strong clean/robust trade-off for OOD detection, we introduce an additional loss term which boosts classification and detection performance. Our approach, called HALO (Helper-based AdversariaL OOD detection), surpasses existing methods and achieves state-of-the-art performance across a number of datasets and attack settings. Extensive experiments demonstrate an average AUROC improvement of 3.15 in clean settings and 7.07 under adversarial attacks when compared to the next best method. Furthermore, HALO exhibits resistance to transferred attacks, offers tuneable performance through hyperparameter selection, and is compatible with existing OOD detection frameworks out-of-the-box, leaving open the possibility of future performance gains. Code is available at: https://github.com/hugo0076/HALO

### Probabilistic Federated Prompt-Tuning with Non-IID and Imbalanced Data 
[[arxiv](https://arxiv.org/abs/2502.19752)] [[cool](https://papers.cool/arxiv/2502.19752)] [[pdf](https://arxiv.org/pdf/2502.19752)]
> **Authors**: Pei-Yau Weng,Minh Hoang,Lam M. Nguyen,My T. Thai,Tsui-Wei Weng,Trong Nghia Hoang
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Accepted at NeurIPS-24
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Fine-tuning pre-trained models is a popular approach in machine learning for solving complex tasks with moderate data. However, fine-tuning the entire pre-trained model is ineffective in federated data scenarios where local data distributions are diversely skewed. To address this, we explore integrating federated learning with a more effective prompt-tuning method, optimizing for a small set of input prefixes to reprogram the pre-trained model's behavior. Our approach transforms federated learning into a distributed set modeling task, aggregating diverse sets of prompts to globally fine-tune the pre-trained model. We benchmark various baselines based on direct adaptations of existing federated model aggregation techniques and introduce a new probabilistic prompt aggregation method that substantially outperforms these baselines. Our reported results on a variety of computer vision datasets confirm that the proposed method is most effective to combat extreme data heterogeneity in federated learning.

### CirT: Global Subseasonal-to-Seasonal Forecasting with Geometry-inspired Transformer 
[[arxiv](https://arxiv.org/abs/2502.19750)] [[cool](https://papers.cool/arxiv/2502.19750)] [[pdf](https://arxiv.org/pdf/2502.19750)]
> **Authors**: Yang Liu,Zinan Zheng,Jiashun Cheng,Fugee Tsung,Deli Zhao,Yu Rong,Jia Li
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: Accurate Subseasonal-to-Seasonal (S2S) climate forecasting is pivotal for decision-making including agriculture planning and disaster preparedness but is known to be challenging due to its chaotic nature. Although recent data-driven models have shown promising results, their performance is limited by inadequate consideration of geometric inductive biases. Usually, they treat the spherical weather data as planar images, resulting in an inaccurate representation of locations and spatial relations. In this work, we propose the geometric-inspired Circular Transformer (CirT) to model the cyclic characteristic of the graticule, consisting of two key designs: (1) Decomposing the weather data by latitude into circular patches that serve as input tokens to the Transformer; (2) Leveraging Fourier transform in self-attention to capture the global information and model the spatial periodicity. Extensive experiments on the Earth Reanalysis 5 (ERA5) reanalysis dataset demonstrate our model yields a significant improvement over the advanced data-driven models, including PanguWeather and GraphCast, as well as skillful ECMWF systems. Additionally, we empirically show the effectiveness of our model designs and high-quality prediction over spatial and temporal dimensions.

### BiRating -- Iterative averaging on a bipartite graph of Beat Saber scores, player skills, and map difficulties 
[[arxiv](https://arxiv.org/abs/2502.19742)] [[cool](https://papers.cool/arxiv/2502.19742)] [[pdf](https://arxiv.org/pdf/2502.19742)]
> **Authors**: Juan Casanova
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 30 pages, 2 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Difficulty estimation of Beat Saber maps is an interesting data analysis problem and valuable to the Beat Saber competitive scene. We present a simple algorithm that iteratively averages player skill and map difficulty estimations in a bipartite graph of players and maps, connected by scores, using scores only as input. This approach simultaneously estimates player skills and map difficulties, exploiting each of them to improve the estimation of the other, exploitng the relation of multiple scores by different players on the same map, or on different maps by the same player. While we have been unable to prove or characterize theoretical convergence, the implementation exhibits convergent behaviour to low estimation error in all instances, producing accurate results. An informal qualitative evaluation involving experienced Beat Saber community members was carried out, comparing the difficulty estimations output by our algorithm with their personal perspectives on the difficulties of different maps. There was a significant alignment with player perceived perceptions of difficulty and with other existing methods for estimating difficulty. Our approach showed significant improvement over existing methods in certain known problematic maps that are not typically accurately estimated, but also produces problematic estimations for certain families of maps where the assumptions on the meaning of scores were inadequate (e.g. not enough scores, or scores over optimized by players). The algorithm has important limitations, related to data quality and meaningfulness, assumptions on the domain problem, and theoretical convergence of the algorithm. Future work would significantly benefit from a better understanding of adequate ways to quantify map difficulty in Beat Saber, including multidimensionality of skill and difficulty, and the systematic biases present in score data.

### Causal Effect Estimation under Networked Interference without Networked Unconfoundedness Assumption 
[[arxiv](https://arxiv.org/abs/2502.19741)] [[cool](https://papers.cool/arxiv/2502.19741)] [[pdf](https://arxiv.org/pdf/2502.19741)]
> **Authors**: Weilin Chen,Ruichu Cai,Jie Qiao,Yuguang Yan,José Miguel Hernández-Lobato
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: arXiv admin note: substantial text overlap with arXiv:2405.03342
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Estimating causal effects under networked interference is a crucial yet challenging problem. Existing methods based on observational data mainly rely on the networked unconfoundedness assumption, which guarantees the identification of networked effects. However, the networked unconfoundedness assumption is usually violated due to the latent confounders in observational data, hindering the identification of networked effects. Interestingly, in such networked settings, interactions between units provide valuable information for recovering latent confounders. In this paper, we identify three types of latent confounders in networked inference that hinder identification: those affecting only the individual, those affecting only neighbors, and those influencing both. Specifically, we devise a networked effect estimator based on identifiable representation learning techniques. Theoretically, we establish the identifiability of all latent confounders, and leveraging the identified latent confounders, we provide the networked effect identification result. Extensive experiments validate our theoretical results and demonstrate the effectiveness of the proposed method.

### Tokens for Learning, Tokens for Unlearning: Mitigating Membership Inference Attacks in Large Language Models via Dual-Purpose Training 
[[arxiv](https://arxiv.org/abs/2502.19726)] [[cool](https://papers.cool/arxiv/2502.19726)] [[pdf](https://arxiv.org/pdf/2502.19726)]
> **Authors**: Toan Tran,Ruixuan Liu,Li Xiong
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Large language models (LLMs) have become the backbone of modern natural language processing but pose privacy concerns about leaking sensitive training data. Membership inference attacks (MIAs), which aim to infer whether a sample is included in a model's training dataset, can serve as a foundation for broader privacy threats. Existing defenses designed for traditional classification models do not account for the sequential nature of text data. As a result, they either require significant computational resources or fail to effectively mitigate privacy risks in LLMs. In this work, we propose a lightweight yet effective empirical privacy defense for protecting training data of language modeling by leveraging the token-specific characteristics. By analyzing token dynamics during training, we propose a token selection strategy that categorizes tokens into hard tokens for learning and memorized tokens for unlearning. Subsequently, our training-phase defense optimizes a novel dual-purpose token-level loss to achieve a Pareto-optimal balance between utility and privacy. Extensive experiments demonstrate that our approach not only provides strong protection against MIAs but also improves language modeling performance by around 10\% across various LLM architectures and datasets compared to the baselines.

### Invariance Pair-Guided Learning: Enhancing Robustness in Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.18975)] [[cool](https://papers.cool/arxiv/2502.18975)] [[pdf](https://arxiv.org/pdf/2502.18975)]
> **Authors**: Martin Surner,Abdelmajid Khelil,Ludwig Bothmann
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Out-of-distribution generalization of machine learning models remains challenging since the models are inherently bound to the training data distribution. This especially manifests, when the learned models rely on spurious correlations. Most of the existing approaches apply data manipulation, representation learning, or learning strategies to achieve generalizable models. Unfortunately, these approaches usually require multiple training domains, group labels, specialized augmentation, or pre-processing to reach generalizable models. We propose a novel approach that addresses these limitations by providing a technique to guide the neural network through the training phase. We first establish input pairs, representing the spurious attribute and describing the invariance, a characteristic that should not affect the outcome of the model. Based on these pairs, we form a corrective gradient complementing the traditional gradient descent approach. We further make this correction mechanism adaptive based on a predefined invariance condition. Experiments on ColoredMNIST, Waterbird-100, and CelebA datasets demonstrate the effectiveness of our approach and the robustness to group shifts.

### (Mis)Fitting: A Survey of Scaling Laws 
[[arxiv](https://arxiv.org/abs/2502.18969)] [[cool](https://papers.cool/arxiv/2502.18969)] [[pdf](https://arxiv.org/pdf/2502.18969)]
> **Authors**: Margaret Li,Sneha Kudugunta,Luke Zettlemoyer
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 41 pages, 3 figure, first two authors contributed equally. ICLR, 2025
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,方法论
- **Abstract**: Modern foundation models rely heavily on using scaling laws to guide crucial training decisions. Researchers often extrapolate the optimal architecture and hyper parameters settings from smaller training runs by describing the relationship between, loss, or task performance, and scale. All components of this process vary, from the specific equation being fit, to the training setup, to the optimization method. Each of these factors may affect the fitted law, and therefore, the conclusions of a given study. We discuss discrepancies in the conclusions that several prior works reach, on questions such as the optimal token to parameter ratio. We augment this discussion with our own analysis of the critical impact that changes in specific details may effect in a scaling study, and the resulting altered conclusions. Additionally, we survey over 50 papers that study scaling trends: while 45 of these papers quantify these trends using a power law, most under-report crucial details needed to reproduce their findings. To mitigate this, we we propose a checklist for authors to consider while contributing to scaling law research.

### One Set to Rule Them All: How to Obtain General Chemical Conditions via Bayesian Optimization over Curried Functions 
[[arxiv](https://arxiv.org/abs/2502.18966)] [[cool](https://papers.cool/arxiv/2502.18966)] [[pdf](https://arxiv.org/pdf/2502.18966)]
> **Authors**: Stefan P. Schmid,Ella Miray Rajaonson,Cher Tian Ser,Mohammad Haddadnia,Shi Xuan Leong,Alán Aspuru-Guzik,Agustinus Kristiadi,Kjell Jorner,Felix Strieth-Kalthoff
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: :J.2
- **标题**: None
- **领域**: 机器学习
- **Abstract**: General parameters are highly desirable in the natural sciences - e.g., chemical reaction conditions that enable high yields across a range of related transformations. This has a significant practical impact since those general parameters can be transferred to related tasks without the need for laborious and time-intensive re-optimization. While Bayesian optimization (BO) is widely applied to find optimal parameter sets for specific tasks, it has remained underused in experiment planning towards such general optima. In this work, we consider the real-world problem of condition optimization for chemical reactions to study how performing generality-oriented BO can accelerate the identification of general optima, and whether these optima also translate to unseen examples. This is achieved through a careful formulation of the problem as an optimization over curried functions, as well as systematic evaluations of generality-oriented strategies for optimization tasks on real-world experimental data. We find that for generality-oriented optimization, simple myopic optimization strategies that decouple parameter and task selection perform comparably to more complex ones, and that effective optimization is merely determined by an effective exploration of both parameter and task space.

### Nonparametric Heterogeneous Long-term Causal Effect Estimation via Data Combination 
[[arxiv](https://arxiv.org/abs/2502.18960)] [[cool](https://papers.cool/arxiv/2502.18960)] [[pdf](https://arxiv.org/pdf/2502.18960)]
> **Authors**: Weilin Chen,Ruichu Cai,Junjie Wan,Zeqin Yang,José Miguel Hernández-Lobato
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Long-term causal inference has drawn increasing attention in many scientific domains. Existing methods mainly focus on estimating average long-term causal effects by combining long-term observational data and short-term experimental data. However, it is still understudied how to robustly and effectively estimate heterogeneous long-term causal effects, significantly limiting practical applications. In this paper, we propose several two-stage style nonparametric estimators for heterogeneous long-term causal effect estimation, including propensity-based, regression-based, and multiple robust estimators. We conduct a comprehensive theoretical analysis of their asymptotic properties under mild assumptions, with the ultimate goal of building a better understanding of the conditions under which some estimators can be expected to perform better. Extensive experiments across several semi-synthetic and real-world datasets validate the theoretical results and demonstrate the effectiveness of the proposed estimators.

### Fourier Multi-Component and Multi-Layer Neural Networks: Unlocking High-Frequency Potential 
[[arxiv](https://arxiv.org/abs/2502.18959)] [[cool](https://papers.cool/arxiv/2502.18959)] [[pdf](https://arxiv.org/pdf/2502.18959)]
> **Authors**: Shijun Zhang,Hongkai Zhao,Yimin Zhong,Haomin Zhou
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Our code and implementation details are available at https://github.com/ShijunZhangMath/FMMNN
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: The two most critical ingredients of a neural network are its structure and the activation function employed, and more importantly, the proper alignment of these two that is conducive to the effective representation and learning in practice. In this work, we introduce a surprisingly effective synergy, termed the Fourier Multi-Component and Multi-Layer Neural Network (FMMNN), and demonstrate its surprising adaptability and efficiency in capturing high-frequency components. First, we theoretically establish that FMMNNs have exponential expressive power in terms of approximation capacity. Next, we analyze the optimization landscape of FMMNNs and show that it is significantly more favorable compared to fully connected neural networks. Finally, systematic and extensive numerical experiments validate our findings, demonstrating that FMMNNs consistently achieve superior accuracy and efficiency across various tasks, particularly impressive when high-frequency components are present.

### Fewer May Be Better: Enhancing Offline Reinforcement Learning with Reduced Dataset 
[[arxiv](https://arxiv.org/abs/2502.18955)] [[cool](https://papers.cool/arxiv/2502.18955)] [[pdf](https://arxiv.org/pdf/2502.18955)]
> **Authors**: Yiqin Yang,Quanwei Wang,Chenghao Li,Hao Hu,Chengjie Wu,Yuhua Jiang,Dianyu Zhong,Ziyou Zhang,Qianchuan Zhao,Chongjie Zhang,Xu Bo
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: ef:Published on ICLR 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Offline reinforcement learning (RL) represents a significant shift in RL research, allowing agents to learn from pre-collected datasets without further interaction with the environment. A key, yet underexplored, challenge in offline RL is selecting an optimal subset of the offline dataset that enhances both algorithm performance and training efficiency. Reducing dataset size can also reveal the minimal data requirements necessary for solving similar problems. In response to this challenge, we introduce ReDOR (Reduced Datasets for Offline RL), a method that frames dataset selection as a gradient approximation optimization problem. We demonstrate that the widely used actor-critic framework in RL can be reformulated as a submodular optimization objective, enabling efficient subset selection. To achieve this, we adapt orthogonal matching pursuit (OMP), incorporating several novel modifications tailored for offline RL. Our experimental results show that the data subsets identified by ReDOR not only boost algorithm performance but also do so with significantly lower computational complexity.

### BeamVQ: Beam Search with Vector Quantization to Mitigate Data Scarcity in Physical Spatiotemporal Forecasting 
[[arxiv](https://arxiv.org/abs/2502.18925)] [[cool](https://papers.cool/arxiv/2502.18925)] [[pdf](https://arxiv.org/pdf/2502.18925)]
> **Authors**: Weiyan Wang,Xingjian Shi,Ruiqi Shu,Yuan Gao,Rui Ray Chen,Kun Wang,Fan Xu,Jinbao Xue,Shuaipeng Li,Yangyu Tao,Di Wang,Hao Wu,Xiaomeng Huang
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: In practice, physical spatiotemporal forecasting can suffer from data scarcity, because collecting large-scale data is non-trivial, especially for extreme events. Hence, we propose \method{}, a novel probabilistic framework to realize iterative self-training with new self-ensemble strategies, achieving better physical consistency and generalization on extreme events. Following any base forecasting model, we can encode its deterministic outputs into a latent space and retrieve multiple codebook entries to generate probabilistic outputs. Then BeamVQ extends the beam search from discrete spaces to the continuous state spaces in this field. We can further employ domain-specific metrics (e.g., Critical Success Index for extreme events) to filter out the top-k candidates and develop the new self-ensemble strategy by combining the high-quality candidates. The self-ensemble can not only improve the inference quality and robustness but also iteratively augment the training datasets during continuous self-training. Consequently, BeamVQ realizes the exploration of rare but critical phenomena beyond the original dataset. Comprehensive experiments on different benchmarks and backbones show that BeamVQ consistently reduces forecasting MSE (up to 39%), enhancing extreme events detection and proving its effectiveness in handling data scarcity.

### CLLoRA: An Approach to Measure the Effects of the Context Length for LLM Fine-Tuning 
[[arxiv](https://arxiv.org/abs/2502.18910)] [[cool](https://papers.cool/arxiv/2502.18910)] [[pdf](https://arxiv.org/pdf/2502.18910)]
> **Authors**: Ping Zhang,Zhaorui Zhang,Sheng Di,Yao Xin,Benben Liu
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,分布式、并行和集群计算
- **Abstract**: Large language model fine-tuning has been identified as an efficient approach to applying the pre-trained Large language models to other domains. To guarantee data privacy for different data owners, models are often fine-tuned in federated learning environments across different data owners, which often involve data heterogeneity issues and affect the fine-tuning performance. In addition, the length of the context for the training data has been identified as a major factor that affects the LLM's model performance. To efficiently measure how the context length affects the LLM's model performance in heterogeneous federated learning environments, we propose CLLoRA. CLLoRA utilizes the parameter-efficient fine-tuning approach LoRA based on different kinds of LLMs with varying sizes as the fine-tuning approach to investigate whether the quality and length of contexts can serve as standards for measuring non-IID context. The findings indicate that an imbalance in context quality not only affects local training on clients but also impacts the global model's performance. However, context length has a minimal effect on local training but a more significant influence on the global model. These results provide insights into how context quality and length affect the model performance for LLM fine-tuning in federated learning environments.

### A Pipeline of Augmentation and Sequence Embedding for Classification of Imbalanced Network Traffic 
[[arxiv](https://arxiv.org/abs/2502.18909)] [[cool](https://papers.cool/arxiv/2502.18909)] [[pdf](https://arxiv.org/pdf/2502.18909)]
> **Authors**: Matin Shokri,Ramin Hasibi
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 10 pages, 4 figures. arXiv admin note: text overlap with arXiv:1901.00204
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Network Traffic Classification (NTC) is one of the most important tasks in network management. The imbalanced nature of classes on the internet presents a critical challenge in classification tasks. For example, some classes of applications are much more prevalent than others, such as HTTP. As a result, machine learning classification models do not perform well on those classes with fewer data. To address this problem, we propose a pipeline to balance the dataset and classify it using a robust and accurate embedding technique. First, we generate artificial data using Long Short-Term Memory (LSTM) networks and Kernel Density Estimation (KDE). Next, we propose replacing one-hot encoding for categorical features with a novel embedding framework based on the "Flow as a Sentence" perspective, which we name FS-Embedding. This framework treats the source and destination ports, along with the packet's direction, as one word in a flow, then trains an embedding vector space based on these new features through the learning classification task. Finally, we compare our pipeline with the training of a Convolutional Recurrent Neural Network (CRNN) and Transformers, both with imbalanced and sampled datasets, as well as with the one-hot encoding approach. We demonstrate that the proposed augmentation pipeline, combined with FS-Embedding, increases convergence speed and leads to a significant reduction in the number of model parameters, all while maintaining the same performance in terms of accuracy.

### VEM: Environment-Free Exploration for Training GUI Agent with Value Environment Model 
[[arxiv](https://arxiv.org/abs/2502.18906)] [[cool](https://papers.cool/arxiv/2502.18906)] [[pdf](https://arxiv.org/pdf/2502.18906)]
> **Authors**: Jiani Zheng,Lu Wang,Fangkai Yang,Chaoyun Zhang,Lingrui Mei,Wenjie Yin,Qingwei Lin,Dongmei Zhang,Saravan Rajmohan,Qi Zhang
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 20pages,5 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Training Vision-Language Models (VLMs) for Graphical User Interfaces (GUI) agents via Reinforcement Learning (RL) faces critical challenges: environment-based RL requires costly interactions, while environment-free methods struggle with distribution shift and reward generalization. We propose an environment-free RL framework that decouples value estimation from policy optimization by leveraging a pretrained Value Environment Model (VEM). VEM predicts state-action values directly from offline data, distilling human-like priors about GUI interaction outcomes without requiring next-state prediction or environmental feedback. This avoids compounding errors and enhances resilience to UI changes by focusing on semantic reasoning (e.g., Does this action advance the user's goal?). The framework operates in two stages: (1) pretraining VEM to estimate long-term action utilities and (2) guiding policy exploration with frozen VEM signals, enabling layout-agnostic GUI automation. Evaluated on Android-in-the-Wild benchmarks, VEM achieves state-of-the-art performance in both offline and online settings, outperforming environment-free baselines significantly and matching environment-based approaches without interaction costs. Importantly, VEM demonstrates that semantic-aware value estimation can achieve comparable performance with online-trained methods.

### Dynamic Classification: Leveraging Self-Supervised Classification to Enhance Prediction Performance 
[[arxiv](https://arxiv.org/abs/2502.18891)] [[cool](https://papers.cool/arxiv/2502.18891)] [[pdf](https://arxiv.org/pdf/2502.18891)]
> **Authors**: Ziyuan Zhong,Junyang Zhou
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 18 pages, 6 figures
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: In this paper, we propose an innovative dynamic classification algorithm designed to achieve the objective of zero missed detections and minimal false positives. The algorithm partitions the data into N equivalent training subsets and N prediction subsets using a supervised model, followed by independent predictions from N separate predictive models. This enables each predictive model to operate within a smaller data range, thereby improving overall accuracy. Additionally, the algorithm leverages data generated through supervised learning to further refine prediction results, filtering out predictions that do not meet accuracy requirements without the need to introduce additional models. Experimental results demonstrate that, when data partitioning errors are minimal, the dynamic classification algorithm achieves exceptional performance with zero missed detections and minimal false positives, significantly outperforming existing model ensembles. Even in cases where classification errors are larger, the algorithm remains comparable to state of the art models. The key innovations of this study include self-supervised classification learning, the use of small-range subset predictions, and the direct rejection of substandard predictions. While the current algorithm still has room for improvement in terms of automatic parameter tuning and classification model efficiency, it has demonstrated outstanding performance across multiple datasets. Future research will focus on optimizing the classification component to further enhance the algorithm's robustness and adaptability.

### A Theoretical Perspective: How to Prevent Model Collapse in Self-consuming Training Loops 
[[arxiv](https://arxiv.org/abs/2502.18865)] [[cool](https://papers.cool/arxiv/2502.18865)] [[pdf](https://arxiv.org/pdf/2502.18865)]
> **Authors**: Shi Fu,Yingjie Wang,Yuzhu Chen,Xinmei Tian,Dacheng Tao
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Accepted at ICLR 2025
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: High-quality data is essential for training large generative models, yet the vast reservoir of real data available online has become nearly depleted. Consequently, models increasingly generate their own data for further training, forming Self-consuming Training Loops (STLs). However, the empirical results have been strikingly inconsistent: some models degrade or even collapse, while others successfully avoid these failures, leaving a significant gap in theoretical understanding to explain this discrepancy. This paper introduces the intriguing notion of recursive stability and presents the first theoretical generalization analysis, revealing how both model architecture and the proportion between real and synthetic data influence the success of STLs. We further extend this analysis to transformers in in-context learning, showing that even a constant-sized proportion of real data ensures convergence, while also providing insights into optimal synthetic data sizing.

## 多代理系统(cs.MA:Multiagent Systems)

### Exponential Topology-enabled Scalable Communication in Multi-agent Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.19717)] [[cool](https://papers.cool/arxiv/2502.19717)] [[pdf](https://arxiv.org/pdf/2502.19717)]
> **Authors**: Xinran Li,Xiaolu Wang,Chenjia Bai,Jun Zhang
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Accepted by the Thirteenth International Conference onLearningRepresentations (ICLR 2025)
- **标题**: None
- **领域**: 多代理系统,人工智能,机器学习
- **Abstract**: In cooperative multi-agent reinforcement learning (MARL), well-designed communication protocols can effectively facilitate consensus among agents, thereby enhancing task performance. Moreover, in large-scale multi-agent systems commonly found in real-world applications, effective communication plays an even more critical role due to the escalated challenge of partial observability compared to smaller-scale setups. In this work, we endeavor to develop a scalable communication protocol for MARL. Unlike previous methods that focus on selecting optimal pairwise communication links-a task that becomes increasingly complex as the number of agents grows-we adopt a global perspective on communication topology design. Specifically, we propose utilizing the exponential topology to enable rapid information dissemination among agents by leveraging its small-diameter and small-size properties. This approach leads to a scalable communication protocol, named ExpoComm. To fully unlock the potential of exponential graphs as communication topologies, we employ memory-based message processors and auxiliary tasks to ground messages, ensuring that they reflect global information and benefit decision-making. Extensive experiments on large-scale cooperative benchmarks, including MAgent and Infrastructure Management Planning, demonstrate the superior performance and robust zero-shot transferability of ExpoComm compared to existing communication strategies. The code is publicly available at https://github.com/LXXXXR/ExpoComm.

## 机器人技术(cs.RO:Robotics)

### SLAM in the Dark: Self-Supervised Learning of Pose, Depth and Loop-Closure from Thermal Images 
[[arxiv](https://arxiv.org/abs/2502.18932)] [[cool](https://papers.cool/arxiv/2502.18932)] [[pdf](https://arxiv.org/pdf/2502.18932)]
> **Authors**: Yangfan Xu,Qu Hao,Lilian Zhang,Jun Mao,Xiaofeng He,Wenqi Wu,Changhao Chen
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,人工智能
- **Abstract**: Visual SLAM is essential for mobile robots, drone navigation, and VR/AR, but traditional RGB camera systems struggle in low-light conditions, driving interest in thermal SLAM, which excels in such environments. However, thermal imaging faces challenges like low contrast, high noise, and limited large-scale annotated datasets, restricting the use of deep learning in outdoor scenarios. We present DarkSLAM, a noval deep learning-based monocular thermal SLAM system designed for large-scale localization and reconstruction in complex lighting conditions.Our approach incorporates the Efficient Channel Attention (ECA) mechanism in visual odometry and the Selective Kernel Attention (SKA) mechanism in depth estimation to enhance pose accuracy and mitigate thermal depth degradation. Additionally, the system includes thermal depth-based loop closure detection and pose optimization, ensuring robust performance in low-texture thermal scenes. Extensive outdoor experiments demonstrate that DarkSLAM significantly outperforms existing methods like SC-Sfm-Learner and Shin et al., delivering precise localization and 3D dense mapping even in challenging nighttime environments.

## 声音(cs.SD:Sound)

### DualSpec: Text-to-spatial-audio Generation via Dual-Spectrogram Guided Diffusion Model 
[[arxiv](https://arxiv.org/abs/2502.18952)] [[cool](https://papers.cool/arxiv/2502.18952)] [[pdf](https://arxiv.org/pdf/2502.18952)]
> **Authors**: Lei Zhao,Sizhou Chen,Linfeng Feng,Xiao-Lei Zhang,Xuelong Li
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 声音,人工智能,音频和语音处理
- **Abstract**: Text-to-audio (TTA), which generates audio signals from textual descriptions, has received huge attention in recent years. However, recent works focused on text to monaural audio only. As we know, spatial audio provides more immersive auditory experience than monaural audio, e.g. in virtual reality. To address this issue, we propose a text-to-spatial-audio (TTSA) generation framework named DualSpec.Specifically, it first trains variational autoencoders (VAEs) for extracting the latent acoustic representations from sound event audio. Then, given text that describes sound events and event directions, the proposed method uses the encoder of a pretrained large language model to transform the text into text features. Finally, it trains a diffusion model from the latent acoustic representations and text features for the spatial audio generation. In the inference stage, only the text description is needed to generate spatial audio. Particularly, to improve the synthesis quality and azimuth accuracy of the spatial sound events simultaneously, we propose to use two kinds of acoustic features. One is the Mel spectrograms which is good for improving the synthesis quality, and the other is the short-time Fourier transform spectrograms which is good at improving the azimuth accuracy. We provide a pipeline of constructing spatial audio dataset with text prompts, for the training of the VAEs and diffusion model. We also introduce new spatial-aware evaluation metrics to quantify the azimuth errors of the generated spatial audio recordings. Experimental results demonstrate that the proposed method can generate spatial audio with high directional and event consistency.

### Clip-TTS: Contrastive Text-content and Mel-spectrogram, A High-Huality Text-to-Speech Method based on Contextual Semantic Understanding 
[[arxiv](https://arxiv.org/abs/2502.18889)] [[cool](https://papers.cool/arxiv/2502.18889)] [[pdf](https://arxiv.org/pdf/2502.18889)]
> **Authors**: Tianyun Liu
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 声音,人工智能,计算语言学,人机交互,机器学习,音频和语音处理
- **Abstract**: Traditional text-to-speech (TTS) methods primarily focus on establishing a mapping between phonemes and mel-spectrograms. However, during the phoneme encoding stage, there is often a lack of real mel-spectrogram auxiliary information, which results in the encoding process lacking true semantic understanding. At the same time, traditional TTS systems often struggle to balance the inference speed of the model with the quality of the synthesized speech. Methods that generate high-quality synthesized speech tend to have slower inference speeds, while faster inference methods often sacrifice speech quality. In this paper, I propose Clip-TTS, a TTS method based on the Clip architecture. This method uses the Clip framework to establish a connection between text content and real mel-spectrograms during the text encoding stage, enabling the text encoder to directly learn the true semantics of the global context, thereby ensuring the quality of the synthesized speech. In terms of model architecture, I adopt the basic structure of Transformer, which allows Clip-TTS to achieve fast inference speeds. Experimental results show that on the LJSpeech and Baker datasets, the speech generated by Clip-TTS achieves state-of-the-art MOS scores, and it also performs excellently on multi-emotion datasets.Audio samples are available at: https://ltydd1314.github.io/.

## 软件工程(cs.SE:Software Engineering)

### Bridging the PLC Binary Analysis Gap: A Cross-Compiler Dataset and Neural Framework for Industrial Control Systems 
[[arxiv](https://arxiv.org/abs/2502.19725)] [[cool](https://papers.cool/arxiv/2502.19725)] [[pdf](https://arxiv.org/pdf/2502.19725)]
> **Authors**: Yonatan Gizachew Achamyeleh,Shih-Yuan Yu,Gustavo Quirós Araya,Mohammad Abdullah Al Faruque
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,机器学习
- **Abstract**: Industrial Control Systems (ICS) rely heavily on Programmable Logic Controllers (PLCs) to manage critical infrastructure, yet analyzing PLC executables remains challenging due to diverse proprietary compilers and limited access to source code. To bridge this gap, we introduce PLC-BEAD, a comprehensive dataset containing 2431 compiled binaries from 700+ PLC programs across four major industrial compilers (CoDeSys, GEB, OpenPLC-V2, OpenPLC-V3). This novel dataset uniquely pairs each binary with its original Structured Text source code and standardized functionality labels, enabling both binary-level and source-level analysis. We demonstrate the dataset's utility through PLCEmbed, a transformer-based framework for binary code analysis that achieves 93\% accuracy in compiler provenance identification and 42\% accuracy in fine-grained functionality classification across 22 industrial control categories. Through comprehensive ablation studies, we analyze how compiler optimization levels, code patterns, and class distributions influence model performance. We provide detailed documentation of the dataset creation process, labeling taxonomy, and benchmark protocols to ensure reproducibility. Both PLC-BEAD and PLCEmbed are released as open-source resources to foster research in PLC security, reverse engineering, and ICS forensics, establishing new baselines for data-driven approaches to industrial cybersecurity.

### Automated Code Generation and Validation for Software Components of Microcontrollers 
[[arxiv](https://arxiv.org/abs/2502.18905)] [[cool](https://papers.cool/arxiv/2502.18905)] [[pdf](https://arxiv.org/pdf/2502.18905)]
> **Authors**: Sebastian Haug,Christoph Böhm,Daniel Mayer
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Sebastian Haug: This paper, spanning 12 pages with 5 figures, presents my work on automated code generation and validation for STM32F407 microcontroller software components. Developed as part of a research project at Munich University of Applied Sciences and AGSOTEC GmbH, it leverages AST and RAG to streamline embedded development. Includes glossary and bibliography as supplementary materials
- **标题**: None
- **领域**: 软件工程,机器学习
- **Abstract**: This paper proposes a method for generating software components for embedded systems, integrating seamlessly into existing implementations without developer intervention. We demonstrate this by automatically generating hardware abstraction layer (HAL) code for GPIO operations on the STM32F407 microcontroller. Using Abstract Syntax Trees (AST) for code analysis and Retrieval-Augmented Generation (RAG) for component generation, our approach enables autonomous code completion for embedded applications.

## 音频和语音处理(eess.AS:Audio and Speech Processing)

### Sparse Alignment Enhanced Latent Diffusion Transformer for Zero-Shot Speech Synthesis 
[[arxiv](https://arxiv.org/abs/2502.18924)] [[cool](https://papers.cool/arxiv/2502.18924)] [[pdf](https://arxiv.org/pdf/2502.18924)]
> **Authors**: Ziyue Jiang,Yi Ren,Ruiqi Li,Shengpeng Ji,Zhenhui Ye,Chen Zhang,Bai Jionghao,Xiaoda Yang,Jialong Zuo,Yu Zhang,Rui Liu,Xiang Yin,Zhou Zhao
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 音频和语音处理,机器学习,声音
- **Abstract**: While recent zero-shot text-to-speech (TTS) models have significantly improved speech quality and expressiveness, mainstream systems still suffer from issues related to speech-text alignment modeling: 1) models without explicit speech-text alignment modeling exhibit less robustness, especially for hard sentences in practical applications; 2) predefined alignment-based models suffer from naturalness constraints of forced alignments. This paper introduces \textit{S-DiT}, a TTS system featuring an innovative sparse alignment algorithm that guides the latent diffusion transformer (DiT). Specifically, we provide sparse alignment boundaries to S-DiT to reduce the difficulty of alignment learning without limiting the search space, thereby achieving high naturalness. Moreover, we employ a multi-condition classifier-free guidance strategy for accent intensity adjustment and adopt the piecewise rectified flow technique to accelerate the generation process. Experiments demonstrate that S-DiT achieves state-of-the-art zero-shot TTS speech quality and supports highly flexible control over accent intensity. Notably, our system can generate high-quality one-minute speech with only 8 sampling steps. Audio samples are available at https://sditdemo.github.io/sditdemo/.

## 仪器仪表和探测器(physics.ins-det:Instrumentation and Detectors)

### FPGA-Accelerated SpeckleNN with SNL for Real-time X-ray Single-Particle Imaging 
[[arxiv](https://arxiv.org/abs/2502.19734)] [[cool](https://papers.cool/arxiv/2502.19734)] [[pdf](https://arxiv.org/pdf/2502.19734)]
> **Authors**: Abhilasha Dave,Cong Wang,James Russell,Ryan Herbst,Jana Thayer
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 仪器仪表和探测器,机器学习,图像和视频处理
- **Abstract**: We implement a specialized version of our SpeckleNN model for real-time speckle pattern classification in X-ray Single-Particle Imaging (SPI) using the SLAC Neural Network Library (SNL) on an FPGA. This hardware is optimized for inference near detectors in high-throughput X-ray free-electron laser (XFEL) facilities like the Linac Coherent Light Source (LCLS). To fit FPGA constraints, we optimized SpeckleNN, reducing parameters from 5.6M to 64.6K (98.8% reduction) with 90% accuracy. We also compressed the latent space from 128 to 50 dimensions. Deployed on a KCU1500 FPGA, the model used 71% of DSPs, 75% of LUTs, and 48% of FFs, with an average power consumption of 9.4W. The FPGA achieved 45.015us inference latency at 200 MHz. On an NVIDIA A100 GPU, the same inference consumed ~73W and had a 400us latency. Our FPGA version achieved an 8.9x speedup and 7.8x power reduction over the GPU. Key advancements include model specialization and dynamic weight loading through SNL, eliminating time-consuming FPGA re-synthesis for fast, continuous deployment of (re)trained models. These innovations enable real-time adaptive classification and efficient speckle pattern vetoing, making SpeckleNN ideal for XFEL facilities. This implementation accelerates SPI experiments and enhances adaptability to evolving conditions.

## 生物分子(q-bio.BM:Biomolecules)

### SE(3)-Equivariant Ternary Complex Prediction Towards Target Protein Degradation 
[[arxiv](https://arxiv.org/abs/2502.18875)] [[cool](https://papers.cool/arxiv/2502.18875)] [[pdf](https://arxiv.org/pdf/2502.18875)]
> **Authors**: Fanglei Xue,Meihan Zhang,Shuqi Li,Xinyu Gao,James A. Wohlschlegel,Wenbing Huang,Yi Yang,Weixian Deng
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 生物分子,人工智能,机器学习
- **Abstract**: Targeted protein degradation (TPD) induced by small molecules has emerged as a rapidly evolving modality in drug discovery, targeting proteins traditionally considered "undruggable". Proteolysis-targeting chimeras (PROTACs) and molecular glue degraders (MGDs) are the primary small molecules that induce TPD. Both types of molecules form a ternary complex linking an E3 ligase with a target protein, a crucial step for drug discovery. While significant advances have been made in binary structure prediction for proteins and small molecules, ternary structure prediction remains challenging due to obscure interaction mechanisms and insufficient training data. Traditional methods relying on manually assigned rules perform poorly and are computationally demanding due to extensive random sampling. In this work, we introduce DeepTernary, a novel deep learning-based approach that directly predicts ternary structures in an end-to-end manner using an encoder-decoder architecture. DeepTernary leverages an SE(3)-equivariant graph neural network (GNN) with both intra-graph and ternary inter-graph attention mechanisms to capture intricate ternary interactions from our collected high-quality training dataset, TernaryDB. The proposed query-based Pocket Points Decoder extracts the 3D structure of the final binding ternary complex from learned ternary embeddings, demonstrating state-of-the-art accuracy and speed in existing PROTAC benchmarks without prior knowledge from known PROTACs. It also achieves notable accuracy on the more challenging MGD benchmark under the blind docking protocol. Remarkably, our experiments reveal that the buried surface area calculated from predicted structures correlates with experimentally obtained degradation potency-related metrics. Consequently, DeepTernary shows potential in effectively assisting and accelerating the development of TPDs for previously undruggable targets.

## 其他论文

- [Static task mapping for heterogeneous systems based on series-parallel decompositions](https://arxiv.org/abs/2502.19745)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC in whitelist
- [Do Expressions Change Decisions? Exploring the Impact of AI's Explanation Tone on Decision-Making](https://arxiv.org/abs/2502.19730)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [OntologyRAG: Better and Faster Biomedical Code Mapping with Retrieval-Augmented Generation (RAG) Leveraging Ontology Knowledge Graphs and Large Language Models](https://arxiv.org/abs/2502.18992)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [3D-TrIM: A Memory-Efficient Spatial Computing Architecture for Convolution Workloads](https://arxiv.org/abs/2502.18983)
  - **标题**: None
  - **Filtered Reason**: none of cs.AR in whitelist
- [Bidirectionalization For The Common People](https://arxiv.org/abs/2502.18954)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [A Reliable, Time-Predictable Heterogeneous SoC for AI-Enhanced Mixed-Criticality Edge Applications](https://arxiv.org/abs/2502.18953)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC,cs.AR in whitelist
- [ClassInvGen: Class Invariant Synthesis using Large Language Models](https://arxiv.org/abs/2502.18917)
  - **标题**: None
  - **Filtered Reason**: none of cs.PL,cs.SE in whitelist
- [An Empirical Study on Commit Message Generation using LLMs via In-Context Learning](https://arxiv.org/abs/2502.18904)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Distributed Online Task Assignment via Inexact ADMM for unplanned online tasks and its Applications to Security](https://arxiv.org/abs/2502.18893)
  - **标题**: None
  - **Filtered Reason**: none of cs.MA in whitelist
- [Towards More Trustworthy Deep Code Models by Enabling Out-of-Distribution Detection](https://arxiv.org/abs/2502.18883)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Letters from Future Self: Augmenting the Letter-Exchange Exercise with LLM-based Agents to Enhance Young Adults' Career Exploration](https://arxiv.org/abs/2502.18881)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Adaptive Shielding via Parametric Safety Proofs](https://arxiv.org/abs/2502.18879)
  - **标题**: None
  - **Filtered Reason**: none of cs.PL in whitelist
